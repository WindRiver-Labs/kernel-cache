From a11fd9b7021e99c5c5143ee96853aa4d9267d382 Mon Sep 17 00:00:00 2001
From: andrei varvara <andrei.varvara@freescale.com>
Date: Fri, 10 Feb 2012 00:43:08 +0000
Subject: [PATCH 092/518] Added DPA IPSec component

The DPA IPSec module exports an API which enables offloading of parts of the IPSec specific processing using DPAA.

Signed-off-by: Andrei Varvara <andrei.varvara@freescale.com>
Signed-off-by: Mihai Serb <mihai.serb@freescale.com>
[Grabbed from the branch, LINUX_IR5.2.0, of
https://git.freescale.com/git-private/cgit.cgi/ppc/alu-b4860/linux.git.]
Signed-off-by: Tiejun Chen <tiejun.chen@windriver.com>
---
 drivers/staging/fsl_dpa_offload/Makefile         |    8 +-
 drivers/staging/fsl_dpa_offload/cq.c             |  471 ++++
 drivers/staging/fsl_dpa_offload/cq.h             |  285 +++
 drivers/staging/fsl_dpa_offload/dpa_ipsec.c      | 2864 ++++++++++++++++++++++
 drivers/staging/fsl_dpa_offload/dpa_ipsec.h      |  321 +++
 drivers/staging/fsl_dpa_offload/dpa_ipsec_desc.c |  311 +++
 drivers/staging/fsl_dpa_offload/dpa_ipsec_desc.h |   88 +
 include/linux/fsl_dpa_ipsec.h                    |  429 ++++
 8 files changed, 4775 insertions(+), 2 deletions(-)
 create mode 100644 drivers/staging/fsl_dpa_offload/cq.c
 create mode 100644 drivers/staging/fsl_dpa_offload/cq.h
 create mode 100644 drivers/staging/fsl_dpa_offload/dpa_ipsec.c
 create mode 100644 drivers/staging/fsl_dpa_offload/dpa_ipsec.h
 create mode 100644 drivers/staging/fsl_dpa_offload/dpa_ipsec_desc.c
 create mode 100644 drivers/staging/fsl_dpa_offload/dpa_ipsec_desc.h
 create mode 100644 include/linux/fsl_dpa_ipsec.h

diff --git a/drivers/staging/fsl_dpa_offload/Makefile b/drivers/staging/fsl_dpa_offload/Makefile
index 5683163..0c2bf90 100644
--- a/drivers/staging/fsl_dpa_offload/Makefile
+++ b/drivers/staging/fsl_dpa_offload/Makefile
@@ -4,10 +4,14 @@ include $(srctree)/drivers/net/ethernet/freescale/dpa/NetCommSw/ncsw_config.mk
 
 
 EXTRA_CFLAGS += \
-	-Idrivers/staging/fsl_dpa_offload/integration
+	-Idrivers/staging/fsl_dpa_offload/integration \
+	-Idrivers/crypto/caam
 
 
 obj-$(CONFIG_FSL_DPA_OFFLOAD) +=    \
         crc8.o                      \
         dpa_classifier.o            \
-        dpa_compat.o
+        dpa_compat.o \
+        dpa_ipsec.o \
+        dpa_ipsec_desc.o \
+        cq.o
diff --git a/drivers/staging/fsl_dpa_offload/cq.c b/drivers/staging/fsl_dpa_offload/cq.c
new file mode 100644
index 0000000..51ed853
--- /dev/null
+++ b/drivers/staging/fsl_dpa_offload/cq.c
@@ -0,0 +1,471 @@
+/* Copyright 2008-2012 Freescale Semiconductor, Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/*
+ * Source file for the Circular Queue object.
+ * Elements are put in and taken out of the queue in FIFO order.
+ * In addition any position in the queue may be read (without
+ * affecting the contents of the queue). The size of each item
+ * in the queue is set when the queue is initialized.
+ */
+
+#include "cq.h"
+
+/******************************************************************************/
+struct cq *cq_new(uint16_t max_items, uint16_t item_size)
+{
+	struct cq *cq;
+
+	xx_assert(max_items != 0);
+	xx_assert(item_size != 0);
+
+	cq = (struct cq *)xx_malloc((uint32_t)
+				(sizeof(struct cq) + (max_items * item_size)));
+	if (cq) {
+		cq->max_items = max_items;
+		cq->item_size = item_size;
+		cq->items_in_queue = 0;
+		cq->first = 0;
+	}
+
+	return cq;
+}
+
+/******************************************************************************/
+void cq_delete(struct cq *cq)
+{
+	xx_assert(cq != 0);
+
+	xx_free(cq);
+}
+
+/******************************************************************************/
+int cq_flush(struct cq *cq)
+{
+	int items_in_queue;
+
+	xx_assert(cq != 0);
+
+	items_in_queue = cq->items_in_queue;
+	cq->first = 0;
+	cq->items_in_queue = 0;
+
+	return items_in_queue;
+}
+
+/******************************************************************************/
+int cq_put(struct cq *cq, void *item)
+{
+	int put_pos, max_items, first, items_in_queue, item_size, i;
+	uint8_t *byte = item;
+
+	xx_assert(cq != 0);
+
+	first = cq->first;
+	max_items = cq->max_items;
+	items_in_queue = cq->items_in_queue;
+	item_size = cq->item_size;
+
+	/* Check if queue is full */
+	if (items_in_queue == max_items)
+		return -1;
+
+	if ((first + items_in_queue) < max_items)
+		put_pos = ((first + items_in_queue) * item_size);
+	else
+		put_pos = ((first + items_in_queue - max_items) * item_size);
+
+	/* add element to queue */
+	for (i = 0; i < item_size; i++)
+		cq->items[put_pos + i] = *(byte + i);
+
+	cq->items_in_queue++;
+
+	return items_in_queue + 1;
+}
+
+/******************************************************************************/
+int cq_put_unsafe(struct cq *cq, void *item)
+{
+	int put_pos, max_items, first, items_in_queue, item_size, i;
+	uint8_t *byte = item;
+
+	xx_assert(cq != 0);
+
+	first = cq->first;
+	max_items = cq->max_items;
+	items_in_queue = cq->items_in_queue;
+	item_size = cq->item_size;
+
+	/* Check if queue is full */
+	if (items_in_queue == max_items)
+		return -1;
+
+	if ((first + items_in_queue) < max_items)
+		put_pos = ((first + items_in_queue) * item_size);
+	else
+		put_pos = ((first + items_in_queue - max_items) * item_size);
+
+	/* add element to queue */
+	for (i = 0; i < item_size; i++)
+		cq->items[put_pos + i] = *(byte + i);
+
+	cq->items_in_queue++;
+
+	return items_in_queue + 1;
+}
+
+/******************************************************************************/
+int cq_put_1byte(struct cq *cq, uint8_t item)
+{
+	int put_pos, max_items, first, items_in_queue;
+
+	xx_assert(cq != 0);
+	xx_assert(cq->item_size == 1);
+
+	first = cq->first;
+	max_items = cq->max_items;
+	items_in_queue = cq->items_in_queue;
+
+	/* Check if queue is full */
+	if (items_in_queue == max_items)
+		return -1;
+
+	if ((first + items_in_queue) < max_items)
+		put_pos = first + items_in_queue;
+	else
+		put_pos = first + items_in_queue - max_items;
+
+	xx_assert((put_pos >= 0) && (put_pos < max_items));
+
+	/* add element to queue */
+	cq->items[put_pos] = item;
+	cq->items_in_queue++;
+
+	return items_in_queue + 1;
+}
+
+/******************************************************************************/
+int cq_put_2bytes(struct cq *cq, uint16_t item)
+{
+	int put_pos, max_items, first, items_in_queue;
+
+	xx_assert(cq);
+	xx_assert(cq->item_size == 2);
+
+	first = cq->first;
+	max_items = cq->max_items;
+	items_in_queue = cq->items_in_queue;
+
+	/* Check if queue is full */
+	if (items_in_queue == max_items)
+		return -1;
+
+	if ((first + items_in_queue) < max_items)
+		put_pos = first + items_in_queue;
+	else
+		put_pos = first + items_in_queue - max_items;
+
+	xx_assert((put_pos >= 0) && (put_pos < max_items));
+
+	/* add element to queue */
+	*((uint16_t *) cq->items + put_pos) = item;
+	cq->items_in_queue++;
+
+	return items_in_queue + 1;
+}
+
+/******************************************************************************/
+int cq_put_4bytes(struct cq *cq, uint32_t item)
+{
+	int put_pos, max_items, first, items_in_queue;
+
+	xx_assert(cq);
+	xx_assert(cq->item_size == 4);
+
+	first = cq->first;
+	max_items = cq->max_items;
+	items_in_queue = cq->items_in_queue;
+
+	/* Check if queue is full */
+	if (items_in_queue == max_items)
+		return -1;
+
+	if ((first + items_in_queue) < max_items)
+		put_pos = first + items_in_queue;
+	else
+		put_pos = first + items_in_queue - max_items;
+
+	xx_assert((put_pos >= 0) && (put_pos < max_items));
+
+	/* add element to queue */
+	*((uint32_t *) cq->items + put_pos) = item;
+	cq->items_in_queue++;
+
+	return items_in_queue + 1;
+}
+
+/******************************************************************************/
+int cq_put_8bytes(struct cq *cq, uint64_t item)
+{
+	int put_pos, max_items, first, items_in_queue;
+
+	xx_assert(cq);
+	xx_assert(cq->item_size == 8);
+
+	first = cq->first;
+	max_items = cq->max_items;
+	items_in_queue = cq->items_in_queue;
+
+	/* Check if queue is full */
+	if (items_in_queue == max_items)
+		return -1;
+
+	if ((first + items_in_queue) < max_items)
+		put_pos = first + items_in_queue;
+	else
+		put_pos = first + items_in_queue - max_items;
+
+	xx_assert((put_pos >= 0) && (put_pos < max_items));
+
+	/* add element to queue */
+	*((uint64_t *) cq->items + put_pos) = item;
+	cq->items_in_queue++;
+
+	return items_in_queue + 1;
+}
+
+/******************************************************************************/
+int cq_get(struct cq *cq, void *item)
+{
+	int get_pos, items_in_queue, item_size, first, i;
+	uint8_t *byte = item;
+
+	xx_assert(cq);
+	xx_assert(item);
+
+	items_in_queue = cq->items_in_queue;
+	/* Check if queue is empty */
+	if (items_in_queue == 0)
+		return -1;
+
+	first = cq->first;
+	item_size = cq->item_size;
+	get_pos = first * item_size;
+
+	/* Get item from queue */
+	for (i = 0; i < item_size; i++)
+		*((byte) + i) = (cq->items[i + get_pos]);
+	cq->items_in_queue--;
+
+	if (++first >= cq->max_items)
+		first = 0;
+	cq->first = (uint16_t) first;
+
+	return items_in_queue - 1;
+}
+
+/******************************************************************************/
+int cq_get_unsafe(struct cq *cq, void *item)
+{
+	int get_pos, items_in_queue, item_size, first, i;
+	uint8_t *byte = item;
+
+	xx_assert(cq);
+	xx_assert(item);
+
+	items_in_queue = cq->items_in_queue;
+	/* Check if queue is empty */
+	if (items_in_queue == 0)
+		return -1;
+
+	first = cq->first;
+	item_size = cq->item_size;
+	get_pos = first * item_size;
+
+	/* Get item from queue */
+	for (i = 0; i < item_size; i++)
+		*((byte) + i) = (cq->items[i + get_pos]);
+	cq->items_in_queue--;
+
+	if (++first >= cq->max_items)
+		first = 0;
+	cq->first = (uint16_t) first;
+
+	return items_in_queue - 1;
+}
+
+/******************************************************************************/
+int cq_get_1byte(struct cq *cq, uint8_t * item)
+{
+	int first, items_in_queue;
+
+	xx_assert(cq);
+	xx_assert(item);
+	xx_assert(cq->item_size == 1);
+
+	items_in_queue = cq->items_in_queue;
+	/* Check if queue is empty */
+	if (items_in_queue == 0)
+		return -1;
+
+	first = cq->first;
+
+	/* Get item from queue */
+	*item = (cq->items[first]);
+	cq->items_in_queue--;
+
+	if (++first >= cq->max_items)
+		first = 0;
+	cq->first = (uint16_t) first;
+
+	return items_in_queue - 1;
+}
+
+/******************************************************************************/
+int cq_get_2bytes(struct cq *cq, uint16_t * item)
+{
+	int first, items_in_queue;
+
+	xx_assert(cq);
+	xx_assert(item);
+	xx_assert(cq->item_size == 2);
+
+	items_in_queue = cq->items_in_queue;
+	/* Check if queue is empty */
+	if (items_in_queue == 0)
+		return -1;
+
+	first = cq->first;
+
+	/* Get item from queue */
+	cq->items_in_queue--;
+	*item = *((uint16_t *) cq->items + first);
+
+	if (++first >= cq->max_items)
+		first = 0;
+	cq->first = (uint16_t) first;
+
+	return items_in_queue - 1;
+}
+
+/******************************************************************************/
+int cq_get_4bytes(struct cq *cq, uint32_t * item)
+{
+	int first, items_in_queue;
+
+	xx_assert(cq);
+	xx_assert(item);
+	xx_assert(cq->item_size == 4);
+
+	items_in_queue = cq->items_in_queue;
+	/* Check if queue is empty */
+	if (items_in_queue == 0)
+		return -1;
+
+	first = cq->first;
+
+	/* Get item from queue */
+	cq->items_in_queue--;
+	*item = *((uint32_t *) cq->items + first);
+
+	if (++first >= cq->max_items)
+		first = 0;
+	cq->first = (uint16_t) first;
+
+	return items_in_queue - 1;
+}
+
+/******************************************************************************/
+int cq_get_8bytes(struct cq *cq, uint64_t * item)
+{
+	int first, items_in_queue;
+
+	xx_assert(cq);
+	xx_assert(item);
+	xx_assert(cq->item_size == 8);
+
+	items_in_queue = cq->items_in_queue;
+	/* Check if queue is empty */
+	if (items_in_queue == 0)
+		return -1;
+
+	first = cq->first;
+
+	/* Get item from queue */
+	cq->items_in_queue--;
+	*item = *((uint64_t *) cq->items + first);
+
+	if (++first >= cq->max_items)
+		first = 0;
+	cq->first = (uint16_t) first;
+
+	return items_in_queue - 1;
+}
+
+/******************************************************************************/
+int cq_items_in_queue(struct cq *cq)
+{
+	xx_assert(cq);
+
+	return ((struct cq *)cq)->items_in_queue;
+}
+
+/******************************************************************************/
+int cq_read(struct cq *cq, void *read_item, uint16_t position)
+{
+	int item_size, first, items_in_queue, bytePosition, i;
+	uint8_t *read_byte = read_item;
+
+	xx_assert(cq);
+	xx_assert(read_item);
+
+	items_in_queue = cq->items_in_queue;
+	/* Check if queue is empty and if position is valid */
+	if ((items_in_queue == 0) || (position > items_in_queue))
+		return -1;
+
+	item_size = cq->item_size;
+	first = cq->first;
+
+	/* Find byte position */
+	if (first + position - 1 > cq->max_items)
+		bytePosition =
+		    (first + position - 1 - cq->max_items) * item_size;
+	else
+		bytePosition = (first + position - 1) * item_size;
+
+	/*read from queue */
+	for (i = 0; i < item_size; i++)
+		*(read_byte + i) = (cq->items[bytePosition + i]);
+
+	return items_in_queue;
+}
diff --git a/drivers/staging/fsl_dpa_offload/cq.h b/drivers/staging/fsl_dpa_offload/cq.h
new file mode 100644
index 0000000..7efc07c
--- /dev/null
+++ b/drivers/staging/fsl_dpa_offload/cq.h
@@ -0,0 +1,285 @@
+/* Copyright 2008-2012 Freescale Semiconductor, Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/*
+ * External prototypes for the circular queue object.
+ */
+
+#ifndef __CQ_H
+#define __CQ_H
+
+#include "dpa_compat.h"
+
+/**************************************************************************//**
+   @Description   Circular queue structure
+*//***************************************************************************/
+struct cq {
+	uint16_t max_items;		/* Size of queue */
+	uint16_t item_size;		/* Size of each item in the queue */
+	uint16_t items_in_queue;	/* Number of items in the queue */
+	uint16_t first;			/* Index of first item in queue */
+	uint8_t items[1];		/* Holds the elements in the queue -
+					fake size; (this field must be
+					4-bytes aligned) */
+};
+
+/**************************************************************************//**
+ @Function      cq_new
+
+ @Description   Allocates and initializes a new circular queue.
+
+ @Param[in]     max_items    - Maximum number of items in queue.
+ @Param[in]     item_size    - Size, in bytes, of each item in the queue.
+
+ @Return        Handle to the new object on success;
+		NULL on failure (out of memory).
+*//***************************************************************************/
+struct cq *cq_new(uint16_t max_items, uint16_t item_size);
+
+/**************************************************************************//**
+ @Function      cq_delete
+
+ @Description   Deletes a circular queue.
+
+ @Param[in]     cq - Handle to the circular queue.
+*//***************************************************************************/
+void cq_delete(struct cq *cq);
+
+/**************************************************************************//**
+ @Function      cq_flush
+
+ @Description   Empties the circular queue (clears all items).
+
+ @Param[in]     cq - Handle to the circular queue.
+
+ @Return        The number of items that were flushed from the queue.
+*//***************************************************************************/
+int cq_flush(struct cq *cq);
+
+/**************************************************************************//**
+ @Function      cq_put
+
+ @Description   Puts a new item in the circular queue.
+
+ @Param[in]     cq      - Handle to the circular queue.
+ @Param[in]     item    - Pointer to the item that should be added.
+
+ @Return        Number of items in the queue (including the added item);
+		Returns (-1) if failed to put the item (queue is full).
+*//***************************************************************************/
+int cq_put(struct cq *cq, void *item);
+
+/**************************************************************************//**
+ @Function      cq_put_unsafe
+
+ @Description   Puts a new item in the circular queue.
+
+ @Param[in]     cq      - Handle to the circular queue.
+ @Param[in]     item    - Pointer to the item that should be added.
+
+ @Return        Number of items in the queue (including the added item);
+		Returns (-1) if failed to put the item (queue is full).
+
+ @Cautions      This routine is not interrupt-safe, but is faster than cq_put();
+		The user must prevent concurrent accesses to the queue.
+*//***************************************************************************/
+int cq_put_unsafe(struct cq *cq, void *item);
+
+/**************************************************************************//**
+ @Function      cq_put_1byte
+
+ @Description   Puts a new 1-byte item in the circular queue.
+		The routine is more efficient than cq_put.
+
+ @Param[in]     cq      - Handle to the circular queue.
+ @Param[in]     item    - The item that should be added.
+
+ @Return        Number of items in the queue (including the added item);
+		Returns (-1) if failed to put the item (queue is full).
+*//***************************************************************************/
+int cq_put_1byte(struct cq *cq, uint8_t item);
+
+/**************************************************************************//**
+ @Function      cq_put_2bytes
+
+ @Description   Puts a new 2-byte item in the circular queue.
+		The routine is more efficient than cq_put.
+
+ @Param[in]     cq      - Handle to the circular queue.
+ @Param[in]     item    - The item that should be added.
+
+ @Return        Number of items in the queue (including the added item);
+		Returns (-1) if failed to put the item (queue is full).
+*//***************************************************************************/
+int cq_put_2bytes(struct cq *cq, uint16_t item);
+
+/**************************************************************************//**
+ @Function      cq_put_4bytes
+
+ @Description   Puts a new 4-byte item in the circular queue.
+		The routine is more efficient than cq_put.
+
+ @Param[in]     cq      - Handle to the circular queue.
+ @Param[in]     item    - The item that should be added.
+
+ @Return        Number of items in the queue (including the added item);
+		Returns (-1) if failed to put the item (queue is full).
+*//***************************************************************************/
+int cq_put_4bytes(struct cq *cq, uint32_t item);
+
+/**************************************************************************//**
+ @Function      cq_put_8bytes
+
+ @Description   Puts a new 8-byte item in the circular queue.
+		The routine is more efficient than cq_put.
+
+ @Param[in]     cq      - Handle to the circular queue.
+ @Param[in]     item    - The item that should be added.
+
+ @Return        Number of items in the queue (including the added item);
+		Returns (-1) if failed to put the item (queue is full).
+*//***************************************************************************/
+int cq_put_8bytes(struct cq *cq, uint64_t item);
+
+/**************************************************************************//**
+ @Function      cq_get
+
+ @Description   Gets (and removes) an item from the circular queue.
+
+ @Param[in]     cq      - Handle to the circular queue.
+ @Param[out]    item    - Pointer that will receive the item from the queue.
+
+ @Return        Number of items left in the queue;
+		Returns (-1) if failed to get an item (queue is empty).
+*//***************************************************************************/
+int cq_get(struct cq *cq, void *item);
+
+/**************************************************************************//**
+ @Function      cq_get_unsafe
+
+ @Description   Gets (and removes) an item from the circular queue.
+
+ @Param[in]     cq      - Handle to the circular queue.
+ @Param[in]     item    - Pointer that will receive the item from the queue.
+
+ @Return        Number of items left in the queue;
+		Returns (-1) if failed to get an item (queue is empty).
+
+ @Cautions      This routine is not interrupt-safe, but is faster than cq_get();
+		The user must prevent concurrent accesses to the queue.
+*//***************************************************************************/
+int cq_get_unsafe(struct cq *cq, void *item);
+
+/**************************************************************************//**
+ @Function      cq_get_1byte
+
+ @Description   Gets and removes a 1-byte item from the circular queue.
+		The routine is more efficient than cq_get.
+
+ @Param[in]     cq      - Handle to the circular queue.
+ @Param[out]    item    - Pointer that will receive the item from the queue.
+
+ @Return        Number of items left in the queue;
+		Returns (-1) if failed to get an item (queue is empty).
+*//***************************************************************************/
+int cq_get_1byte(struct cq *cq, uint8_t * item);
+
+/**************************************************************************//**
+ @Function      cq_get_2bytes
+
+ @Description   Gets and removes a 2-byte item from the circular queue.
+		The routine is more efficient than cq_get.
+
+ @Param[in]     cq      - Handle to the circular queue.
+ @Param[out]    item    - Pointer that will receive the item from the queue.
+
+ @Return        Number of items left in the queue;
+		Returns (-1) if failed to get an item (queue is empty).
+*//***************************************************************************/
+int cq_get_2bytes(struct cq *cq, uint16_t * item);
+
+/**************************************************************************//**
+ @Function      cq_get_4bytes
+
+ @Description   Gets and removes a 4-byte item from the circular queue.
+		The routine is more efficient than cq_get.
+
+ @Param[in]     cq      - Handle to the circular queue.
+ @Param[out]    item    - Pointer that will receive the item from the queue.
+
+ @Return        Number of items left in the queue;
+		Returns (-1) if failed to get an item (queue is empty).
+*//***************************************************************************/
+int cq_get_4bytes(struct cq *cq, uint32_t * item);
+
+/**************************************************************************//**
+ @Function      cq_get_8bytes
+
+ @Description   Gets and removes a 8-byte item from the circular queue.
+		The routine is more efficient than cq_get.
+
+ @Param[in]     cq      - Handle to the circular queue.
+ @Param[out]    item    - Pointer that will receive the item from the queue.
+
+ @Return        Number of items left in the queue;
+		Returns (-1) if failed to get an item (queue is empty).
+*//***************************************************************************/
+int cq_get_8bytes(struct cq *cq, uint64_t * item);
+
+/**************************************************************************//**
+ @Function      cq_items_in_queue
+
+ @Description   Returns the number of items that are currently in the queue.
+
+ @Param[in]     cq   - Handle to the circular queue.
+
+ @Return        The number of items in the queue.
+*//***************************************************************************/
+int cq_items_in_queue(struct cq *cq);
+
+/**************************************************************************//**
+ @Function      cq_read
+
+ @Description   Reads an item in the queue. The contents of the queue are not
+		altered.
+
+ @Param[in]     cq          - Handle to the circular queue.
+ @Param[out]    item	    - Pointer that will receive the item from the queue.
+ @Param[in]     position    - The position of the item which should be read
+				(1 is the first position in the queue)
+
+ @Return        The number of items in the queue; Returns (-1) if failed to
+		read the item (either the queue is empty, or the requested
+		position exceeds number of items).
+*//***************************************************************************/
+int cq_read(struct cq *cq, void *item, uint16_t position);
+
+#endif /* __CQ_H */
diff --git a/drivers/staging/fsl_dpa_offload/dpa_ipsec.c b/drivers/staging/fsl_dpa_offload/dpa_ipsec.c
new file mode 100644
index 0000000..31c492d
--- /dev/null
+++ b/drivers/staging/fsl_dpa_offload/dpa_ipsec.c
@@ -0,0 +1,2864 @@
+/* Copyright 2008-2012 Freescale Semiconductor, Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <linux/netdevice.h>
+#include <linux/delay.h>
+#include <linux/in.h>
+
+#include "dpa_ipsec.h"
+#include "dpa_ipsec_desc.h"
+
+/* DPA IPsec Mapping between API DF bit action and FM PCD DF bit action */
+e_FmPcdManipDontFragAction	pcd_df_action[] = {
+	/* DPA_IPSEC_DF_ACTION_DISCARD */
+	e_FM_PCD_MANIP_ENQ_TO_ERR_Q_OR_DISCARD_PACKET,
+	/* DPA_IPSEC_DF_ACTION_OVERRIDE */
+	e_FM_PCD_MANIP_FRAGMENT_PACKECT,
+	/* DPA_IPSEC_DF_ACTION_CONTINUE */
+	e_FM_PCD_MANIP_CONTINUE_WITHOUT_FRAG
+};
+
+/* DPA IPsec Mapping between API algorithm suites and SEC algorithm IDs */
+struct ipsec_alg_suite	ipsec_algs[] = IPSEC_ALGS;
+
+/* globally allocated because of performance constraints */
+static t_FmPcdCcNodeParams cc_node_prms;
+static t_FmPcdManipParams pcd_manip_params;
+
+/* Global dpa_ipsec component */
+struct dpa_ipsec *gbl_dpa_ipsec;
+
+/* Debug support functions */
+
+#ifdef DEBUG_PARAM
+int print_sa_sec_param(struct dpa_ipsec_sa *sa)
+{
+	int i;
+	struct dpa_ipsec_policy_entry *policy_entry, *tmp_policy_entry;
+	struct dpa_ipsec_policy_selectors *policy_selectors;
+
+	if (!sa)
+		xx_pr_err(("Invalid input: null SA handle\n"));
+
+	pr_info("\n Printing SA SEC PARAM for sa %p\n", sa);
+	pr_info("\n sa_dir = %d\n", sa->sa_dir);
+	pr_info("\n id = %d\n", sa->id);
+	pr_info(" dpa_ipsec addr = %p\n", sa->dpa_ipsec);
+	pr_info(" from_sec_fq addr = %p\n", sa->from_sec_fq);
+
+	pr_info("\n auth_data.auth_type = %d\n", sa->auth_data.auth_type);
+	pr_info("auth_data.auth_key_len = %d\n", sa->auth_data.auth_key_len);
+	pr_info("auth_data.auth_key is\n");
+	for (i = 0; i < sa->auth_data.auth_key_len; i++)
+		pr_info(("%x, ", sa->auth_data.auth_key[i]));
+
+	pr_info("\n cipher_data.cipher_type = %d\n",
+		sa->cipher_data.cipher_type);
+	pr_info("cipher_data.cipher_key_len = %d\n",
+		sa->cipher_data.cipher_key_len);
+	pr_info("cipher_data.cipher_key is\n");
+	for (i = 0; i < sa->cipher_data.cipher_key_len; i++)
+		pr_info(("%x, ", sa->cipher_data.cipher_key[i]));
+
+	pr_info("\n sa_bpid = %d\n", sa->sa_bpid);
+	pr_info(" spi = %d\n", sa->spi);
+	pr_info(" sa_wqid = %d\n", sa->sa_wqid);
+	pr_info(" out_flow_id = %d\n", sa->out_flow_id);
+
+	pr_info("dest_addr.addr_type = %d\n", sa->dest_addr.addr_type);
+	pr_info("dest_addr = %x.%x.%x.%x\n",
+		sa->dest_addr.ipv4.byte[0],
+		sa->dest_addr.ipv4.byte[1],
+		sa->dest_addr.ipv4.byte[2], sa->dest_addr.ipv4.byte[3]);
+	pr_info("src_addr.addr_type = %d\n", sa->src_addr.addr_type);
+	pr_info("src_addr = %x.%x.%x.%x\n",
+		sa->src_addr.ipv4.byte[0],
+		sa->src_addr.ipv4.byte[1],
+		sa->src_addr.ipv4.byte[2], sa->src_addr.ipv4.byte[3]);
+
+	if (sa->sa_dir == DPA_IPSEC_OUTBOUND) {
+		uint8_t *out_hdr;
+		out_hdr = &sa->sec_desc->pdb_en.ip_hdr[0];
+		pr_info("Outer Header length  %d\n",
+			sa->sec_desc->pdb_en.ip_hdr_len);
+		pr_info("Outer Header is:\n");
+		for (i = 0; i < sa->sec_desc->pdb_en.ip_hdr_len; i++)
+			pr_info("%x, ", *(out_hdr + i));
+
+		pr_info("pdb_en.ip_hdr_len %d\n",
+			sa->sec_desc->pdb_en.ip_hdr_len);
+		pr_info("pdb_en.spi = %d\n", sa->sec_desc->pdb_en.spi);
+		pr_info("pdb_en.seq_num = %d\n", sa->sec_desc->pdb_en.seq_num);
+		pr_info("pdb_en.options = 0x%x\n",
+			sa->sec_desc->pdb_en.options);
+		pr_info("pdb_en.desc_hdr = 0x%x\n",
+			sa->sec_desc->pdb_en.desc_hdr);
+		pr_info("pdb_en.ip_nh = 0x%x\n", sa->sec_desc->pdb_en.ip_nh);
+	} else {
+		pr_info("pdb_dec.hmo_ip_hdr_len %d\n",
+			sa->sec_desc->pdb_dec.hmo_ip_hdr_len);
+		pr_info("pdb_dec.options %d\n", sa->sec_desc->pdb_dec.options);
+		pr_info("pdb_dec.seq_num %d\n", sa->sec_desc->pdb_dec.seq_num);
+	}
+
+	pr_info("\n Printing all policies from this SA policy_list\n");
+	list_for_each_entry_safe(policy_entry, tmp_policy_entry,
+				 &sa->policy_headlist, node) {
+		policy_selectors = &policy_entry->policy_selectors;
+		pr_info("policy_selectors src_addr.addr_type = %d\n",
+			policy_selectors->src_addr.addr_type);
+		pr_info("policy_selectors src_addr = %x.%x.%x.%x\n",
+			policy_selectors->src_addr.ipv4.byte[0],
+			policy_selectors->src_addr.ipv4.byte[1],
+			policy_selectors->src_addr.ipv4.byte[2],
+			policy_selectors->src_addr.ipv4.byte[3]);
+		pr_info("\n policy_selectors dest_addr.addr_type = %d\n",
+			policy_selectors->dest_addr.addr_type);
+		pr_info("policy_selectors dest_addr = %x.%x.%x.%x\n",
+			policy_selectors->dest_addr.ipv4.byte[0],
+			policy_selectors->dest_addr.ipv4.byte[1],
+			policy_selectors->dest_addr.ipv4.byte[2],
+			policy_selectors->dest_addr.ipv4.byte[3]);
+
+		pr_info("\n policy_selectors dest_port = %d\n",
+			policy_selectors->dest_port);
+		pr_info(" policy_selectors src_port = %d\n",
+			policy_selectors->src_port);
+		pr_info(" policy_selectors dest_port = %d\n",
+			policy_selectors->dest_port_mask);
+		pr_info(" policy_selectors dest_port = %d\n",
+			policy_selectors->src_port_mask);
+		pr_info(" policy_selectors proto = %d\n",
+			policy_selectors->protocol);
+		pr_info(" policy_selectors dest_prefix_len = %d\n",
+			policy_selectors->dest_prefix_len);
+		pr_info(" policy_selectors src_prefix_len = %d\n",
+			policy_selectors->src_prefix_len);
+	}
+	pr_info("\n Done printing SA SEC PARAM for sa %p\n", sa);
+
+	return 0;
+}
+#endif
+
+/* Initialization functions */
+
+/* store params needed during runtime or free */
+static inline void store_ipsec_params(struct dpa_ipsec *dpa_ipsec,
+				      const struct dpa_ipsec_params *params)
+{
+	xx_assert(dpa_ipsec);
+	xx_assert(params);
+
+	/* copy config params */
+	dpa_ipsec->config = *params;
+}
+
+/* check that the provided params are valid */
+static int check_ipsec_params(const struct dpa_ipsec_params *prms)
+{
+	struct dpa_cls_tbl_params table_params;
+	int i, err, valid_tables = 0;
+
+	xx_assert(prms);
+
+	if ((prms->post_sec_in_params.do_pol_check) && (!prms->fm_pcd)) {
+		xx_pr_err(("Provide a valid PCD handle to enable inbound policy check!\n"));
+		return -EINVAL;
+	}
+
+	/*
+	 * check that all required table descriptors were provided:
+	 * - at least one table for outbound policy lookup
+	 * - one table for index lookup after decryption
+	 * - one table for SA lookup
+	 */
+	for (i = 0; i < DPA_IPSEC_MAX_SUPPORTED_PROTOS; i++)
+		if (prms->pre_sec_out_params.dpa_cls_td[i] !=
+						DPA_CLS_INVALID_TABLE_DESC) {
+			/* verify that it is not an indexed table */
+			err = dpa_classif_table_get_params(
+				       prms->pre_sec_out_params.dpa_cls_td[i],
+				       &table_params);
+			if (err < 0) {
+				xx_pr_err(("Couldn't check type of outbound policy lookup table\n"));
+				return -EINVAL;
+			}
+
+			if (table_params.type == DPA_CLS_TBL_INDEXED) {
+				xx_pr_err(("Outbound policy lookup table cannot be of type INDEXED\n"));
+				return -EINVAL;
+			}
+			valid_tables++;
+		}
+
+	if (!valid_tables) {
+		xx_pr_err(("Specify at least one table for outbound policy lookup\n"));
+		return -EINVAL;
+	}
+
+	/* post decryption SA classification table */
+	if (prms->post_sec_in_params.dpa_cls_td ==
+						  DPA_CLS_INVALID_TABLE_DESC) {
+		xx_pr_err(("Specify a valid table for post decryption classification\n"));
+		return -EINVAL;
+	}
+
+	/* verify that it is an indexed table */
+	err = dpa_classif_table_get_params(prms->post_sec_in_params.dpa_cls_td,
+					   &table_params);
+	if (err < 0) {
+		xx_pr_err(("Could not check type of post decryption table\n"));
+		return -EINVAL;
+	}
+
+	if (table_params.type != DPA_CLS_TBL_INDEXED) {
+		xx_pr_err(("Post decryption table must be of type INDEXED\n"));
+		return -EINVAL;
+	}
+
+	/* pre decryption SA lookup table */
+	if (prms->pre_sec_in_params.dpa_cls_td < 0) {
+		xx_pr_err(("Specify a valid table for SA lookup\n"));
+		return -EINVAL;
+	}
+
+	/* verify that it is not an indexed table */
+	err = dpa_classif_table_get_params(prms->pre_sec_in_params.dpa_cls_td,
+					   &table_params);
+	if (err < 0) {
+		xx_pr_err(("Could not check type of pre decryption table\n"));
+		return -EINVAL;
+	}
+
+	if (table_params.type == DPA_CLS_TBL_INDEXED) {
+		xx_pr_err(("Pre decryption table mustn't be of type index\n"));
+		return -EINVAL;
+	}
+
+	/*verify that at least one field was selected for building policy keys*/
+	if (prms->pre_sec_out_params.key_fields == 0 ||
+	   (prms->post_sec_in_params.do_pol_check &&
+	    prms->post_sec_in_params.key_fields == 0)) {
+		xx_pr_err(("At least one field must be specified for building policy keys\n"));
+		return -EINVAL;
+	}
+
+	/*
+	 * verify that the instance is configured
+	 * for offloading at least one SA pair
+	 */
+	if (prms->max_sa_pairs == 0) {
+		xx_pr_err(("The instance must be configured for offloading at least one SA pair\n"));
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static uint8_t calc_in_pol_key_size(struct dpa_ipsec *dpa_ipsec)
+{
+	uint8_t key_fields, field_mask = 0;
+	uint8_t key_size = 0, i;
+
+	xx_assert(dpa_ipsec);
+
+	key_fields = dpa_ipsec->config.post_sec_in_params.key_fields;
+
+	for (i = 0; i < DPA_IPSEC_MAX_KEY_FIELDS; i++) {
+		field_mask = (uint8_t)(1 << i);
+		switch (key_fields & field_mask) {
+		case DPA_IPSEC_KEY_FIELD_SIP:
+			if (dpa_ipsec->config.post_sec_in_params.use_ipv6_pol)
+				key_size += IPv6_ADDR_SIZE_IN_BYTES;
+			else
+				key_size += IPv4_ADDR_SIZE_IN_BYTES;
+			break;
+
+		case DPA_IPSEC_KEY_FIELD_DIP:
+			if (dpa_ipsec->config.post_sec_in_params.use_ipv6_pol)
+				key_size += IPv6_ADDR_SIZE_IN_BYTES;
+			else
+				key_size += IPv4_ADDR_SIZE_IN_BYTES;
+			break;
+
+		case DPA_IPSEC_KEY_FIELD_PROTO:
+			key_size += IP_PROTO_FIELD_LEN;
+			break;
+
+		case DPA_IPSEC_KEY_FIELD_SPORT:
+			key_size += PORT_FIELD_LEN;
+			break;
+
+		case DPA_IPSEC_KEY_FIELD_DPORT:
+			key_size += PORT_FIELD_LEN;
+			break;
+		}
+	}
+
+	return key_size;
+}
+
+static void *create_inpol_node(struct dpa_ipsec *dpa_ipsec)
+{
+	void *cc_node;
+	t_FmPcdCcNextEngineParams *next_engine_miss_action;
+
+	xx_assert(dpa_ipsec->sa_mng.inpol_key_size != 0);
+
+	memset(&cc_node_prms, 0, sizeof(cc_node_prms));
+	cc_node_prms.extractCcParams.type = e_FM_PCD_EXTRACT_NON_HDR;
+	cc_node_prms.extractCcParams.extractNonHdr.src =
+						e_FM_PCD_EXTRACT_FROM_KEY;
+	cc_node_prms.extractCcParams.extractNonHdr.action =
+						e_FM_PCD_ACTION_EXACT_MATCH;
+	cc_node_prms.extractCcParams.extractNonHdr.offset = 0;
+	cc_node_prms.extractCcParams.extractNonHdr.size =
+					      dpa_ipsec->sa_mng.inpol_key_size;
+
+	cc_node_prms.keysParams.numOfKeys = 0;
+	cc_node_prms.keysParams.keySize = dpa_ipsec->sa_mng.inpol_key_size;
+
+	next_engine_miss_action =
+			     &cc_node_prms.keysParams.ccNextEngineParamsForMiss;
+	next_engine_miss_action->nextEngine = e_FM_PCD_DONE;
+
+	cc_node = FM_PCD_CcSetNode(dpa_ipsec->config.fm_pcd, &cc_node_prms);
+
+	return cc_node;
+}
+
+static int get_inbound_flowid(struct dpa_ipsec *dpa_ipsec, uint16_t * flowid)
+{
+	if (!dpa_ipsec) {
+		xx_pr_err(("The DPA IPSec instance was not initialized\n"));
+		return -EINVAL;
+	}
+
+	if (!dpa_ipsec->sa_mng.inbound_flowid_cq) {
+		xx_pr_err(("Null inbould policy flowid circular queue\n"));
+		return -EINVAL;
+	}
+
+	if (cq_get_2bytes(dpa_ipsec->sa_mng.inbound_flowid_cq, flowid) < 0) {
+		xx_pr_err(("Could not get valid inbound flow id\n"));
+		return -ENOMEM;
+	}
+
+	return 0;
+}
+
+static int put_inbound_flowid(struct dpa_ipsec *dpa_ipsec, uint16_t flowid)
+{
+	if (!dpa_ipsec) {
+		xx_pr_err(("The DPA IPSec instance was not initialized\n"));
+		return -EFAULT;
+	}
+
+	if (!dpa_ipsec->sa_mng.inbound_flowid_cq) {
+		xx_pr_err(("Null inbound policy flowid circular queue\n"));
+		return -EFAULT;
+	}
+
+	if (cq_put_2bytes(dpa_ipsec->sa_mng.inbound_flowid_cq, flowid) < 0) {
+		xx_pr_err(("Could not put inbound flow id\n"));
+		return -ERANGE;
+	}
+
+	return 0;
+}
+
+static int create_inbound_flowid_cq(struct dpa_ipsec *dpa_ipsec)
+{
+	void *cq;
+	int i, err;
+
+	cq = cq_new((uint16_t) (dpa_ipsec->sa_mng.max_num_sa / 2),
+		    (uint16_t)sizeof(uint16_t));
+	if (!cq) {
+		xx_pr_err(("Could not create inbound flow ID management CQ\n"));
+		return -ENOMEM;
+	}
+
+	dpa_ipsec->sa_mng.inbound_flowid_cq = cq;
+
+	/* Populate the created CQ with flow ids */
+	for (i = 0; i < dpa_ipsec->sa_mng.max_num_sa / 2; i++) {
+		err = put_inbound_flowid(dpa_ipsec, (uint16_t) i);
+		if (err < 0) {
+			xx_pr_err(("Couldn't fill flow id management queue\n"));
+			cq_delete(cq);
+			dpa_ipsec->sa_mng.inbound_flowid_cq = NULL;
+			return err;
+		}
+	}
+
+	return 0;
+}
+
+static int destroy_inbound_flowid_cq(struct dpa_ipsec *dpa_ipsec)
+{
+	if (!dpa_ipsec) {
+		xx_pr_err(("The DPA IPSec instance was not initialized\n"));
+		return -EINVAL;
+	}
+
+	if (!dpa_ipsec->sa_mng.inbound_flowid_cq) {
+		xx_pr_err(("Null inbould policy flowid circular queue\n"));
+		return -EINVAL;
+	}
+
+	cq_delete(dpa_ipsec->sa_mng.inbound_flowid_cq);
+
+	return 0;
+}
+
+static int get_free_inbpol_tbl(struct dpa_ipsec *dpa_ipsec, int *table_desc)
+{
+	struct inpol_tbl *inpol_tbl;
+	struct list_head *head;
+
+	if ((!dpa_ipsec) || (!table_desc)) {
+		xx_pr_err(("Invalid input parameters"));
+		return -EINVAL;
+	}
+
+	head = &dpa_ipsec->sa_mng.inpol_tables;
+
+	list_for_each_entry(inpol_tbl, head, table_list)
+		if (inpol_tbl->used == FALSE)
+			break;
+
+	if (inpol_tbl->used == FALSE)
+		inpol_tbl->used = TRUE;
+	else {
+		xx_pr_err(("No more free EM tables for inbound policy verification\n"));
+		return -ENOMEM;
+	}
+
+	if (inpol_tbl->td < 0) {
+		xx_pr_err(("Invalid table reference\n"));
+		return -EINVAL;
+	}
+
+	*table_desc = inpol_tbl->td;
+
+	return 0;
+}
+
+static int put_free_inbpol_tbl(struct dpa_ipsec *dpa_ipsec, int table_desc)
+{
+	struct inpol_tbl *inpol_tbl;
+	struct list_head *head;
+
+	if ((!dpa_ipsec) || (table_desc < 0)) {
+		xx_pr_err(("Invalid input parameters"));
+		return -EINVAL;
+	}
+
+	head = &dpa_ipsec->sa_mng.inpol_tables;
+
+	list_for_each_entry(inpol_tbl, head, table_list)
+	    if (inpol_tbl->td == table_desc)
+		break;
+
+	if (inpol_tbl->used == FALSE) {
+		xx_pr_warn(("Exact match table %d is not used\n", table_desc));
+		return 0;
+	} else
+		inpol_tbl->used = FALSE;
+
+	return 0;
+}
+
+/*
+ * Create a circular queue with id's for aquiring SA's handles
+ * Allocate a maximum number of SA internal structures to be used at runtime.
+ * Param[in]	dpa_ipsec - Instance for which SaMng is initialized
+ * Return value	0 on success. Error code otherwise.
+ * Cleanup provided by FreeSaMng().
+ */
+static int init_sa_manager(struct dpa_ipsec *dpa_ipsec)
+{
+	struct dpa_ipsec_sa_mng *sa_mng;
+	int i = 0, err;
+
+	sa_mng = &dpa_ipsec->sa_mng;
+	sa_mng->max_num_sa = dpa_ipsec->config.max_sa_pairs * 2;
+
+	/* create queue that holds free queues' ids */
+	sa_mng->sa_id_cq = cq_new(sa_mng->max_num_sa, (uint16_t) sizeof(int));
+	if (!sa_mng->sa_id_cq) {
+		xx_pr_err(("Could not create SA IDs circular queue\n"));
+		return -ENOMEM;
+	}
+
+	/* fill with ids */
+	for (i = 0; i < sa_mng->max_num_sa; i++)
+		cq_put_4bytes(sa_mng->sa_id_cq, i);
+
+	/* alloc SA array */
+	sa_mng->sa = xx_malloc(sa_mng->max_num_sa * sizeof(*sa_mng->sa));
+	if (!sa_mng->sa) {
+		xx_pr_err(("Could not allocate memory for SAs\n"));
+		return -ENOMEM;
+	}
+	memset(sa_mng->sa, 0, sa_mng->max_num_sa * sizeof(*sa_mng->sa));
+
+	/* alloc cipher/auth stuff */
+	for (i = 0; i < sa_mng->max_num_sa; i++) {
+		sa_mng->sa[i].cipher_data.cipher_key =
+						 xx_malloc(MAX_CIPHER_KEY_LEN);
+		if (!sa_mng->sa[i].cipher_data.cipher_key) {
+			xx_pr_err(("Could not allocate memory for cipher key\n"));
+			return -ENOMEM;
+		}
+		memset(sa_mng->sa[i].cipher_data.cipher_key, 0,
+		       MAX_CIPHER_KEY_LEN);
+
+		sa_mng->sa[i].auth_data.auth_key = xx_malloc(MAX_AUTH_KEY_LEN);
+		if (!sa_mng->sa[i].auth_data.auth_key) {
+			xx_pr_err(("Could not allocate memory for authentication key\n"));
+			return -ENOMEM;
+		}
+		memset(sa_mng->sa[i].auth_data.auth_key, 0, MAX_AUTH_KEY_LEN);
+
+		sa_mng->sa[i].auth_data.split_key = xx_malloc(MAX_AUTH_KEY_LEN);
+		if (!sa_mng->sa[i].auth_data.split_key) {
+			xx_pr_err(("Could not allocate memory for authentication split key\n"));
+			return -ENOMEM;
+		}
+		memset(sa_mng->sa[i].auth_data.split_key, 0, MAX_AUTH_KEY_LEN);
+
+		sa_mng->sa[i].from_sec_fq = xx_malloc(sizeof(struct qman_fq));
+		if (!sa_mng->sa[i].from_sec_fq) {
+			xx_pr_err(("Can't allocate space for 'from SEC FQ'\n"));
+			return -ENOMEM;
+		}
+
+		sa_mng->sa[i].to_sec_fq = xx_malloc(sizeof(struct qman_fq));
+		if (!sa_mng->sa[i].to_sec_fq) {
+			xx_pr_err(("Can't allocate space for 'to SEC FQ'\n"));
+			return -ENOMEM;
+		}
+
+		/* Allocate space for the sec descriptor which is holding the
+		 * preheader information and the share descriptor. 64 bit align.
+		 */
+		sa_mng->sa[i].sec_desc =
+			xx_malloc_smart(sizeof(struct sec_descriptor), 0, 64);
+		if (!sa_mng->sa[i].sec_desc) {
+			xx_pr_err(("Could not allocate memory for SEC descriptor\n"));
+			return -ENOMEM;
+		}
+		memset(sa_mng->sa[i].sec_desc, 0,
+		       sizeof(struct sec_descriptor));
+
+		/* Initialize the policy parameter list which will hold all
+		 * inbound or outbound policy parameters which were use to
+		 * generate PCD entries */
+		INIT_LIST_HEAD(&sa_mng->sa[i].policy_headlist);
+	}
+
+	err = create_inbound_flowid_cq(dpa_ipsec);
+	if (err < 0) {
+		xx_pr_err(("Could not create inbound policy flow id cq\n"));
+		return err;
+	}
+
+	/* If policy check is enabled than for every possible inbound SA create
+	 * an Exact Match Table and link it to the Inbound Index Table */
+	if (dpa_ipsec->config.post_sec_in_params.do_pol_check == TRUE) {
+		struct inpol_tbl *pol_table;
+		struct dpa_cls_tbl_params params;
+		void *cc_node;
+
+		/* calculate key size for policy verification tables */
+		dpa_ipsec->sa_mng.inpol_key_size =
+						calc_in_pol_key_size(dpa_ipsec);
+
+		INIT_LIST_HEAD(&sa_mng->inpol_tables);
+		for (i = 0; i < dpa_ipsec->config.max_sa_pairs; i++) {
+			pol_table = xx_malloc(sizeof(*pol_table));
+			if (!pol_table) {
+				xx_pr_err(("Could not allocate memory for policy table"));
+				return -ENOMEM;
+			}
+			memset(pol_table, 0, sizeof(*pol_table));
+
+			/* create cc node for inbound policy */
+			cc_node = create_inpol_node(dpa_ipsec);
+			if (!cc_node) {
+				xx_pr_err(("Could not create cc node for EM table\n"));
+				return -ENOEXEC;
+			}
+			pol_table->cc_node = cc_node;
+
+			memset(&params, 0, sizeof(params));
+			params.fm_pcd = dpa_ipsec->config.fm_pcd;
+			params.entry_mgmt = DPA_CLS_TBL_MANAGE_BY_KEY;
+			params.type = DPA_CLS_TBL_EXACT_MATCH;
+			params.exact_match_params.entries_cnt =
+						    DPA_IPSEC_MAX_IN_POL_PER_SA;
+			params.exact_match_params.key_size =
+					       dpa_ipsec->sa_mng.inpol_key_size;
+			params.exact_match_params.use_priorities = 0;
+			params.cc_node = cc_node;
+			err = dpa_classif_table_create(&params, &pol_table->td);
+			if (err < 0) {
+				xx_pr_err(("Could not create exact match tbl"));
+				return err;
+			}
+			list_add(&pol_table->table_list,
+				 &dpa_ipsec->sa_mng.inpol_tables);
+		}
+	}
+
+	/* Initialize the sa rekeying list of inboud SA's in rekeying process */
+	INIT_LIST_HEAD(&dpa_ipsec->sa_mng.sa_rekeying_headlist);
+
+	/* Creating a single thread work queue used to defer work when there are
+	 * inbound SA's in rekeying process */
+	dpa_ipsec->sa_mng.sa_rekeying_wq =
+		create_singlethread_workqueue("sa_rekeying_wq");
+	if (!dpa_ipsec->sa_mng.sa_rekeying_wq) {
+		xx_pr_err(("Creating SA rekeying work queue failed\n"));
+		return -ENOSPC;
+	}
+
+	/* Initialize the work needed to be done rekeying inbound process */
+	INIT_DELAYED_WORK(&dpa_ipsec->sa_mng.sa_rekeying_work,
+			  sa_rekeying_work_func);
+
+	return 0;
+}
+
+/* cleanup SA manager */
+static void free_sa_mng(struct dpa_ipsec *dpa_ipsec)
+{
+	struct dpa_ipsec_sa_mng *sa_mng =
+				(struct dpa_ipsec_sa_mng *)&dpa_ipsec->sa_mng;
+	struct inpol_tbl *pol_tbl, *tmp;
+	struct list_head *head;
+	int i = 0, err;
+	t_Error	fmd_err;
+
+	/* Remove the DPA IPsec created tables for policy verification */
+	head = &sa_mng->inpol_tables;
+	list_for_each_entry_safe(pol_tbl, tmp, head, table_list) {
+		err = dpa_classif_table_free(pol_tbl->td);
+		if (err < 0)
+			xx_pr_err(("Could not free policy check EM table\n"));
+		list_del(&pol_tbl->table_list);
+		fmd_err = FM_PCD_CcDeleteTree(dpa_ipsec->config.fm_pcd,
+				pol_tbl->cc_node);
+		if (fmd_err != E_OK)
+			xx_pr_fmd_err(fmd_err, "FM_PCD_CcDeleteTree");
+		xx_free(pol_tbl);
+	}
+
+	if (!sa_mng->sa) {
+		xx_pr_err(("Sa vector null"));
+		return;
+	}
+
+
+	/* dealloc cipher/auth stuff */
+	for (i = 0; i < sa_mng->max_num_sa; i++) {
+		if (sa_mng->sa[i].cipher_data.cipher_key) {
+			xx_free(sa_mng->sa[i].cipher_data.cipher_key);
+			sa_mng->sa[i].cipher_data.cipher_key = NULL;
+		}
+		if (sa_mng->sa[i].auth_data.auth_key) {
+			xx_free(sa_mng->sa[i].auth_data.auth_key);
+			sa_mng->sa[i].auth_data.auth_key = NULL;
+		}
+		if (sa_mng->sa[i].auth_data.split_key) {
+			xx_free(sa_mng->sa[i].auth_data.split_key);
+			sa_mng->sa[i].auth_data.split_key = NULL;
+		}
+		if (sa_mng->sa[i].from_sec_fq) {
+			xx_free(sa_mng->sa[i].from_sec_fq);
+			sa_mng->sa[i].from_sec_fq = NULL;
+		}
+		if (sa_mng->sa[i].to_sec_fq) {
+			xx_free(sa_mng->sa[i].to_sec_fq);
+			sa_mng->sa[i].to_sec_fq = NULL;
+		}
+		if (sa_mng->sa[i].sec_desc) {
+			xx_free(sa_mng->sa[i].sec_desc);
+			sa_mng->sa[i].sec_desc = NULL;
+		}
+	}
+
+	cq_delete(sa_mng->sa_id_cq);
+	sa_mng->sa_id_cq = NULL;
+
+	destroy_inbound_flowid_cq(dpa_ipsec);
+
+	if (sa_mng->sa_rekeying_wq)
+		destroy_workqueue(sa_mng->sa_rekeying_wq);
+
+	xx_free(sa_mng->sa);
+	sa_mng->sa = NULL;
+
+	return;
+}
+
+/* cleanup Ipsec */
+static void free_resources(void)
+{
+	struct dpa_ipsec *dpa_ipsec;
+
+	if (!gbl_dpa_ipsec) {
+		xx_pr_err(("There is no dpa_ipsec instance initialized\n"));
+		return;
+	}
+	dpa_ipsec = gbl_dpa_ipsec;
+
+	/* free all SA related stuff */
+	free_sa_mng(dpa_ipsec);
+
+	xx_free(dpa_ipsec->used_sa_ids);
+	xx_free(dpa_ipsec);
+	gbl_dpa_ipsec = NULL;
+}
+
+/* Runtime functions */
+
+/* Convert prefixLen into IP address's netmask. */
+static void set_ip_addr_mask(uint8_t *mask, uint8_t prefix_len,
+			     uint8_t mask_len)
+{
+	static const uint8_t mask_bits[] = {0x00, 0x80, 0xc0, 0xe0, 0xf0, 0xf8,
+					    0xfc, 0xfe, 0xff};
+	uint8_t bit, off;
+
+	xx_assert(mask);
+	memset(mask, 0, mask_len);
+
+	off = prefix_len / 8;
+	bit = prefix_len % 8;
+	while (off--)
+		*mask++ = 0xff;
+	if (bit)
+		*mask = mask_bits[bit];
+}
+
+static int set_in_sa_default_action(struct dpa_ipsec_sa *sa)
+{
+	struct dpa_ipsec *dpa_ipsec;
+	int table;
+	struct dpa_cls_tbl_key tbl_key;
+	struct dpa_cls_tbl_action *action;
+	int err, id;
+
+	xx_assert(sa);
+
+	id = sa->id;
+
+	dpa_ipsec = (struct dpa_ipsec *)sa->dpa_ipsec;
+	/* AV's note: TODO - case when inbound_flowid is greater than 255 */
+	memset(tbl_key.byte, 0, DPA_CLS_TBL_MAXENTRYKEYSIZE);
+	memset(tbl_key.mask, 0, DPA_CLS_TBL_MAXENTRYKEYSIZE);
+	tbl_key.byte[0] = (uint8_t) sa->inbound_flowid;
+
+	action = &sa->def_sa_action;
+	table = dpa_ipsec->config.post_sec_in_params.dpa_cls_td;
+	err = dpa_classif_table_insert_entry(table, &tbl_key, action, 0,
+					     &sa->inbound_indx_entry);
+	if (err < 0) {
+		xx_pr_err(("Could not set default action for SA id %d\n", id));
+		return err;
+	}
+
+	return 0;
+}
+
+static int fill_policy_key(int td,
+			   struct dpa_ipsec_policy_selectors *policy_selectors,
+			   uint8_t key_fields,
+			   uint8_t *key, uint8_t *mask, uint8_t *key_len)
+{
+	struct dpa_cls_tbl_params tbl_params;
+	uint8_t offset = 0, field_mask = 0, tbl_key_size = 0;
+	int err = 0, i;
+
+	/* Fill in the key components */
+	for (i = 0; i < DPA_IPSEC_MAX_KEY_FIELDS; i++) {
+		field_mask = (uint8_t) (1 << i);
+		switch (key_fields & field_mask) {
+		case DPA_IPSEC_KEY_FIELD_SIP:
+			memcpy(key + offset,
+			       IP_ADDR(policy_selectors->src_addr),
+			       IP_ADDR_LEN(policy_selectors->src_addr));
+			set_ip_addr_mask(mask + offset,
+				       policy_selectors->src_prefix_len,
+				       IP_ADDR_LEN(policy_selectors->src_addr));
+			offset += IP_ADDR_LEN(policy_selectors->src_addr);
+			break;
+
+		case DPA_IPSEC_KEY_FIELD_DIP:
+			memcpy(key + offset,
+			       IP_ADDR(policy_selectors->dest_addr),
+			       IP_ADDR_LEN(policy_selectors->dest_addr));
+			set_ip_addr_mask(mask + offset,
+				      policy_selectors->dest_prefix_len,
+				      IP_ADDR_LEN(policy_selectors->dest_addr));
+			offset += IP_ADDR_LEN(policy_selectors->dest_addr);
+			break;
+
+		case DPA_IPSEC_KEY_FIELD_PROTO:
+			if (policy_selectors->protocol) {
+				SET_IP_PROTO_IN_KEY(key, offset,
+						    policy_selectors->protocol);
+				SET_IP_PROTO_MASK(mask, offset, 0xFF);
+			} else {
+				/* ignore protocol field */
+				SET_IP_PROTO_IN_KEY(key, offset,
+						    DPA_IPSEC_DEF_PAD_VAL);
+				SET_IP_PROTO_MASK(mask, offset, 0x00);
+			}
+			offset += IP_PROTO_FIELD_LEN;
+			break;
+
+		case DPA_IPSEC_KEY_FIELD_SPORT:
+			switch (policy_selectors->protocol) {
+			case IPPROTO_TCP:
+			case IPPROTO_UDP:
+			case IPPROTO_SCTP:
+				memcpy(key + offset,
+				      (uint8_t *) &(policy_selectors->src_port),
+				       PORT_FIELD_LEN);
+				SET_L4_PORT_MASK(mask, offset,
+					       policy_selectors->src_port_mask);
+				offset += PORT_FIELD_LEN;
+				break;
+			}
+			break;
+
+		case DPA_IPSEC_KEY_FIELD_DPORT:
+			switch (policy_selectors->protocol) {
+			case IPPROTO_TCP:
+			case IPPROTO_UDP:
+			case IPPROTO_SCTP:
+				memcpy(key + offset,
+				     (uint8_t *) &(policy_selectors->dest_port),
+				      PORT_FIELD_LEN);
+				SET_L4_PORT_MASK(mask, offset,
+					      policy_selectors->dest_port_mask);
+				offset += PORT_FIELD_LEN;
+				break;
+			}
+			break;
+		}
+	}
+
+	/*
+	 * Add padding to compensate difference in size between table maximum
+	 * key size and computed key size.
+	 */
+
+	/* get table params (including maximum key size) */
+	err = dpa_classif_table_get_params(td, &tbl_params);
+	if (err < 0) {
+		xx_pr_err(("Could not retrieve table maximum key size\n"));
+		return -EINVAL;
+	}
+	tbl_key_size = TABLE_KEY_SIZE(tbl_params);
+
+	if (tbl_key_size < offset) {
+		xx_pr_err(("Policy key is greater than maximum table key size\n"));
+		return err;
+	}
+
+	if (tbl_key_size > offset) {
+		for (i = 0; i < tbl_key_size - offset; i++) {
+			*(key + offset + i) = DPA_IPSEC_DEF_PAD_VAL;
+			/* ignore padding during classification (mask it) */
+			*(mask + offset + i) = 0x00;
+		}
+		offset = tbl_key_size;
+	}
+
+	/* Store key length */
+	*key_len = offset;
+
+	return 0;
+}
+
+static int update_inbound_policy(struct dpa_ipsec_sa *sa,
+				 struct dpa_ipsec_policy_entry *policy_entry,
+				 enum mng_op_type op_type)
+{
+	struct dpa_ipsec *dpa_ipsec;
+	struct dpa_ipsec_policy_selectors *policy_selectors;
+	uint8_t keyLength;
+	uint8_t key[MAX_SIZE_POLICY_KEY];
+	uint8_t mask[MAX_SIZE_POLICY_KEY];
+	struct dpa_cls_tbl_action *action;
+	struct dpa_cls_tbl_key tbl_key;
+	int entry_id;
+	int err;
+
+	xx_assert(sa);
+	xx_assert(policy_entry);
+
+	dpa_ipsec = (struct dpa_ipsec *)sa->dpa_ipsec;
+	policy_selectors = &(policy_entry->policy_selectors);
+
+	if (sa->em_inpol_td < 0) {
+		xx_pr_err(("Invalid exact match table for SA %d.\n", sa->id));
+		return -EINVAL;
+	}
+
+	switch (op_type) {
+	case MNG_OP_ADD:
+		memset(key, 0, MAX_SIZE_POLICY_KEY);
+		memset(mask, 0, MAX_SIZE_POLICY_KEY);
+
+		/* Key contains:
+		 * IP SRC ADDR  - from Policy handle
+		 * IP DST ADDR  - from Policy handle
+		 * IP_PROTO     - from Policy handle
+		 * SRC_PORT             - from Policy handle (for UDP & TCP)
+		 * DST_PORT             - from Policy handle (for UDP & TCP)
+		 */
+		err = fill_policy_key(sa->em_inpol_td,
+				      policy_selectors,
+				      dpa_ipsec->config.post_sec_in_params.
+				      key_fields, key, mask, &keyLength);
+		if (err < 0)
+			return err;
+
+		action = &sa->def_sa_action;
+		memcpy(tbl_key.byte, key, MAX_SIZE_POLICY_KEY);
+		memcpy(tbl_key.mask, mask, MAX_SIZE_POLICY_KEY);
+		err = dpa_classif_table_insert_entry(sa->em_inpol_td, &tbl_key,
+						     action, 0, &entry_id);
+		if (err < 0) {
+			xx_pr_err(("Could not insert key in EM table\n"));
+			return err;
+		}
+		policy_entry->entry_id = entry_id;
+		break;
+	case MNG_OP_REMOVE:
+		entry_id = policy_entry->entry_id;
+		err = dpa_classif_table_delete_entry_by_ref(sa->em_inpol_td,
+							    entry_id);
+		if (err < 0) {
+			xx_pr_err(("Could not remove key in EM table\n"));
+			return err;
+		}
+		break;
+	case MNG_OP_MODIFY:
+		xx_pr_err(("Modify operation unsupported for IN Policy PCD\n"));
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int update_outbound_policy(struct dpa_ipsec_sa *sa,
+				  struct dpa_ipsec_policy_entry *policy_entry,
+				  enum mng_op_type op_type)
+{
+	struct dpa_ipsec *dpa_ipsec;
+	struct dpa_ipsec_policy_selectors *policy_selectors;
+	uint8_t keyLength, table_idx;
+	uint8_t key[MAX_SIZE_POLICY_KEY];
+	uint8_t mask[MAX_SIZE_POLICY_KEY];
+	int table;
+	struct dpa_cls_tbl_key tbl_key;
+	struct dpa_cls_tbl_action action;
+	struct dpa_cls_tbl_header_manip *hm = NULL;
+	struct dpa_cls_tbl_entry_mod_params params;
+	int err;
+
+	xx_assert(sa);
+	xx_assert(policy_entry);
+
+	dpa_ipsec = (struct dpa_ipsec *)sa->dpa_ipsec;
+	policy_selectors = &(policy_entry->policy_selectors);
+
+	table_idx = GET_POL_CC_NODE_INDX(policy_selectors->protocol);
+	table = dpa_ipsec->config.pre_sec_out_params.dpa_cls_td[table_idx];
+
+	if (table == DPA_CLS_INVALID_TABLE_DESC) {
+		struct dpa_ipsec_pre_sec_out_params *pre_sec_out_params;
+		pre_sec_out_params = &dpa_ipsec->config.pre_sec_out_params;
+
+		if ((table_idx != DPA_IPSEC_PROTO_ANY) &&
+		    (pre_sec_out_params->dpa_cls_td[DPA_IPSEC_PROTO_ANY] !=
+				    DPA_CLS_INVALID_TABLE_DESC)) {
+			table =
+			    pre_sec_out_params->dpa_cls_td[DPA_IPSEC_PROTO_ANY];
+		} else {
+			xx_pr_err(("No suitable table found for this policy type!\n"));
+			return -EINVAL;
+		}
+	}
+
+	switch (op_type) {
+	case MNG_OP_ADD:
+		memset(key, 0, MAX_SIZE_POLICY_KEY);
+		memset(mask, 0, MAX_SIZE_POLICY_KEY);
+		/* Key may contain:
+		 * IP SRC ADDR  - from Policy handle
+		 * IP DST ADDR  - from Policy handle
+		 * IP_PROTO     - from Policy handle
+		 * SRC_PORT     - from Policy handle (for UDP & TCP & SCTP)
+		 * DST_PORT     - from Policy handle (for UDP & TCP & SCTP)
+		 */
+		err = fill_policy_key(table, policy_selectors,
+				dpa_ipsec->config.pre_sec_out_params.key_fields,
+				key, mask, &keyLength);
+		if (err < 0)
+			return err;
+
+		/* Configure fragmentation */
+		if (policy_entry->mtu) {
+			t_FmPcdManipFragOrReasmParams *frag_param;
+
+			memset(&pcd_manip_params, 0, sizeof(pcd_manip_params));
+			frag_param = &pcd_manip_params.fragOrReasmParams;
+			pcd_manip_params.fragOrReasm = TRUE;
+			frag_param->frag = TRUE;
+			frag_param->hdr  = HEADER_TYPE_IPv4;
+			frag_param->ipFragParams.sizeForFragmentation =
+							policy_entry->mtu;
+			frag_param->ipFragParams.scratchBpid =
+						dpa_ipsec->config.ipf_bpid;
+			frag_param->ipFragParams.dontFragAction =
+					pcd_df_action[policy_entry->df_action];
+			hm = FM_PCD_ManipSetNode(dpa_ipsec->config.fm_pcd,
+						 &pcd_manip_params);
+			if (!hm) {
+				xx_pr_err(("Could not create Manip node for fragmentation!\n"));
+				return -EAGAIN;
+			}
+		}
+
+		memcpy(&tbl_key.byte, key, keyLength);
+		memcpy(&tbl_key.mask, mask, keyLength);
+		memset(&action, 0, sizeof(action));
+		action.type = DPA_CLS_TBL_ACTION_ENQ;
+		action.enable_statistics = FALSE;
+		action.enq_params.new_fqid = qman_fq_fqid((sa->to_sec_fq));
+		action.enq_params.override_fqid = TRUE;
+		action.enq_params.policer_params = NULL;
+		action.enq_params.hm = hm;
+		/* Store the header manipulation into the policy entry */
+		policy_entry->hm = hm;
+
+		err = dpa_classif_table_insert_entry(table, &tbl_key,
+					   &action, 0, &policy_entry->entry_id);
+		if (err < 0) {
+			xx_pr_err(("Could not add key in exact match table\n"));
+			return err;
+		}
+		break;
+	case MNG_OP_REMOVE:
+		err = dpa_classif_table_delete_entry_by_ref(table,
+							policy_entry->entry_id);
+		if (err < 0) {
+			xx_pr_err(("Could not remove key from EM table\n"));
+			return err;
+		}
+		break;
+	case MNG_OP_MODIFY:
+		memset(&action, 0, sizeof(action));
+		action.type = DPA_CLS_TBL_ACTION_ENQ;
+		action.enable_statistics = FALSE;
+		action.enq_params.hm = policy_entry->hm;
+		action.enq_params.new_fqid = qman_fq_fqid(sa->to_sec_fq);
+		action.enq_params.override_fqid = TRUE;
+		action.enq_params.policer_params = NULL;
+
+		memset(&params, 0, sizeof(params));
+		params.type = DPA_CLS_TBL_MODIFY_ACTION;
+		params.key = NULL;
+		params.action = &action;
+
+		err = dpa_classif_table_modify_entry_by_ref(table,
+							 policy_entry->entry_id,
+							 &params);
+		if (err < 0) {
+			xx_pr_err(("Could not modify key in EM table\n"));
+			return err;
+		}
+		break;
+	}
+
+	return 0;
+}
+
+static int update_pre_sec_inbound_table(struct dpa_ipsec_sa *sa,
+					enum mng_op_type op_type)
+{
+	struct dpa_ipsec *dpa_ipsec;
+	uint8_t key[MAX_SIZE_IP_UDP_SPI_KEY];
+	uint8_t mask[MAX_SIZE_IP_UDP_SPI_KEY];
+	int table, entry_id;
+	struct dpa_cls_tbl_key tbl_key;
+	struct dpa_cls_tbl_action action;
+	struct dpa_cls_tbl_params tbl_params;
+	int offset, err = 0, tbl_key_size = 0, i;
+
+	xx_assert(sa);
+
+	dpa_ipsec = (struct dpa_ipsec *)sa->dpa_ipsec;
+	table = dpa_ipsec->config.pre_sec_in_params.dpa_cls_td;
+
+	switch (op_type) {
+	case MNG_OP_ADD:
+		/*Mark classifier entry id as invalid until it's properly
+		 * inserted*/
+		sa->inbound_hash_entry = -1;
+
+		memset(key, 0, MAX_SIZE_IP_UDP_SPI_KEY);
+		memset(mask, 0, MAX_SIZE_IP_UDP_SPI_KEY);
+
+		/* Key contains:
+		 * IP DST ADDR  - from SA handle
+		 * IP_PROTO     - always ESP (SEC limitation)
+		 * UDP_SPORT    - in case of UDP encapsulated ESP
+		 * UDP_DPORT    - in case of UDP encapsulated ESP
+		 * SPI          - from SA handle
+		 */
+
+		/* Fill in the key components */
+		memcpy(key, IP_ADDR(sa->dest_addr), IP_ADDR_LEN(sa->dest_addr));
+
+		offset = IP_ADDR_LEN(sa->dest_addr);
+		if (sa->use_udp_encap) {
+			SET_IP_PROTO_IN_KEY(key, offset, IPPROTO_UDP);
+			offset += IP_PROTO_FIELD_LEN;
+			memcpy(key + offset, (uint8_t *) &(sa->udp_src_port),
+			       PORT_FIELD_LEN);
+			offset += PORT_FIELD_LEN;
+			memcpy(key + offset, (uint8_t *) &(sa->udp_dest_port),
+			       PORT_FIELD_LEN);
+			offset += PORT_FIELD_LEN;
+		} else {
+			SET_IP_PROTO_IN_KEY(key, offset, IPPROTO_ESP);
+			offset += IP_PROTO_FIELD_LEN;
+		}
+
+		memcpy(key + offset, (uint8_t *) &sa->spi, ESP_SPI_FIELD_LEN);
+		offset += ESP_SPI_FIELD_LEN;
+
+		/* determine padding length based on the table params */
+		err = dpa_classif_table_get_params(table, &tbl_params);
+		if (err < 0) {
+			xx_pr_err(("Could not get table maximum key size\n"));
+			return err;
+		}
+		tbl_key_size = TABLE_KEY_SIZE(tbl_params);
+
+		if (tbl_key_size < offset) {
+			xx_pr_err(("SA lookup key is greater than maximum table key size\n"));
+			return err;
+		}
+
+		if (tbl_key_size > offset) {
+			for (i = 0; i < tbl_key_size - offset; i++)
+				*(key + offset + i) = DPA_IPSEC_DEF_PAD_VAL;
+			offset = tbl_key_size;
+		}
+
+		/* Fill the mask. All the key components are used.
+		 * Offset is now equal to key length
+		 */
+		memset(mask, 0xFF, offset);
+
+		/* Complete the parameters for table insert function */
+		memcpy(tbl_key.byte, key, MAX_SIZE_IP_UDP_SPI_KEY);
+		memcpy(tbl_key.mask, mask, MAX_SIZE_IP_UDP_SPI_KEY);
+
+		memset(&action, 0, sizeof(action));
+		action.type = DPA_CLS_TBL_ACTION_ENQ;
+		action.enable_statistics = FALSE;
+		action.enq_params.new_fqid = qman_fq_fqid(sa->to_sec_fq);
+		action.enq_params.override_fqid = TRUE;
+		action.enq_params.hm = NULL;
+		action.enq_params.policer_params = NULL;
+
+		err = dpa_classif_table_insert_entry(table, &tbl_key, &action,
+						     0, &entry_id);
+		if (err < 0) {
+			xx_pr_err(("Could not add key for inbound SA!\n"));
+			return err;
+		}
+		sa->inbound_hash_entry = entry_id;
+		break;
+
+	case MNG_OP_REMOVE:
+		entry_id = sa->inbound_hash_entry;
+		err = dpa_classif_table_delete_entry_by_ref(table, entry_id);
+		if (err < 0) {
+			xx_pr_err(("Could not remove key for inbound SA!\n"));
+			return err;
+		}
+		sa->inbound_hash_entry = -1;
+		break;
+
+	case MNG_OP_MODIFY:
+		xx_pr_err(("Modify operation not supported for IN SA PCD!\n"));
+		return -EINVAL;
+	}
+
+	return err;
+}
+
+static int get_new_fqid(struct dpa_ipsec *dpa_ipsec, uint32_t * fqid)
+{
+	int err = 0;
+
+	if (dpa_ipsec->config.fqid_pool != NULL) {
+		err = qman_fqid_pool_alloc(dpa_ipsec->config.fqid_pool, fqid);
+		if (err < 0)
+			xx_pr_err(("FQID allocation (from pool) failure."
+				   "QMan error code %d\n", err));
+		return err;
+	}
+
+	/* No pool defined. Get FQID from default allocator. */
+	*fqid = qm_fq_new();
+	if (*fqid == 0) {
+		xx_pr_err(("FQID allocation (no pool) failure.\n"));
+		return -ERANGE;
+	}
+
+	return 0;
+}
+
+static void put_free_fqid(uint32_t fqid)
+{
+	struct dpa_ipsec *dpa_ipsec;
+	int err;
+
+	dpa_ipsec = gbl_dpa_ipsec;
+
+	/* recycle the FQID */
+	if (dpa_ipsec->config.fqid_pool != NULL)
+		qman_fqid_pool_free(dpa_ipsec->config.fqid_pool, fqid);
+	else {
+		err = qm_fq_free_flags(fqid, 0);	/* only warn of error */
+		if (err < 0)
+			xx_pr_warn(("Could not release FQID# %u\n", fqid));
+	}
+}
+
+static int wait_until_fq_empty(struct qman_fq *fq, int *timeout)
+{
+	struct qm_mcr_queryfq_np queryfq_np;
+
+	if ((!fq) || (!timeout)) {
+		xx_pr_err(("Invalid input arguments\n"));
+		return -EINVAL;
+	}
+
+	do {
+		qman_query_fq_np(fq, &queryfq_np);
+		cpu_relax();
+		udelay(1);
+		*timeout = *timeout - 1;
+	} while (queryfq_np.frm_cnt && *timeout);
+
+	if (*timeout == 0) {
+		xx_pr_err(("Timeout. Fq with id %d not empty.\n", fq->fqid));
+		return -EBUSY;
+	}
+
+	return 0;
+}
+
+static int remove_sa_sec_fq(struct qman_fq *sec_fq)
+{
+	int err, flags, timeout = 10000;
+
+	if (!sec_fq) {
+		xx_pr_err(("Invalid FQ pointer address specified!\n"));
+		return -EINVAL;
+	}
+
+	err = qman_retire_fq(sec_fq, &flags);
+	if (err < 0) {
+		xx_pr_err(("Failed to retire FQ %d\n", sec_fq->fqid));
+		return err;
+	}
+
+	err = wait_until_fq_empty(sec_fq, &timeout);
+	if (err < 0)
+		return -EBUSY;
+
+	err = qman_oos_fq(sec_fq);
+	if (err < 0) {
+		xx_pr_err(("Failed to OOS FQ %d\n", sec_fq->fqid));
+		return -EBUSY;
+	}
+
+	qman_destroy_fq(sec_fq, 0);
+
+	/* release FQID */
+	put_free_fqid(qman_fq_fqid(sec_fq));
+
+	/* Clean the FQ structure for reuse */
+	memset((void *)sec_fq, 0, sizeof(struct qman_fq));
+
+	return 0;
+}
+
+static int remove_sa_fq_pair(struct dpa_ipsec_sa *sa)
+{
+	struct qman_fq *to_sec_fq, *from_sec_fq;
+	int err;
+
+	to_sec_fq = sa->to_sec_fq;
+	from_sec_fq = sa->from_sec_fq;
+
+	if (qman_fq_fqid(to_sec_fq) != 0) {
+		err = remove_sa_sec_fq(to_sec_fq);
+		if (err < 0)
+			return err;
+	}
+
+	if (qman_fq_fqid(from_sec_fq) != 0) {
+		err = remove_sa_sec_fq(from_sec_fq);
+		if (err < 0)
+			return err;
+	}
+
+	return 0;
+}
+
+static int create_sec_frame_queue(uint32_t *fq_id, uint16_t channel,
+				  uint16_t wq_id, uint32_t ctx_a_hi,
+				  uint32_t ctx_a_lo, uint32_t ctxB, bool parked,
+				  void *sec_fq)
+{
+	struct qm_mcc_initfq fq_opts;
+	struct qman_fq *fq;
+	uint32_t flags;
+	int err = 0;
+
+	xx_assert(fq_id);
+	xx_assert(sec_fq);
+
+	fq = (struct qman_fq *)sec_fq;
+	memset(fq, 0, sizeof(struct qman_fq));
+
+	flags = QMAN_FQ_FLAG_LOCKED | QMAN_FQ_FLAG_TO_DCPORTAL;
+
+	err = qman_create_fq(*fq_id, flags, fq);
+	if (unlikely(err < 0)) {
+		xx_pr_err(("Could not create FQ with ID: %u\n", *fq_id));
+		goto create_sec_fq_err;
+	}
+
+	/* generate a parked queue or a scheduled one depending on the function
+	 * input parameters. */
+	flags = (parked == TRUE) ? 0 : QMAN_INITFQ_FLAG_SCHED;
+	memset(&fq_opts, 0, sizeof(fq_opts));
+	fq_opts.we_mask = QM_INITFQ_WE_DESTWQ | QM_INITFQ_WE_CONTEXTA;
+	if (ctx_a_lo) {
+		fq_opts.we_mask |= QM_INITFQ_WE_CONTEXTB;
+		fq_opts.fqd.context_a.hi = ctx_a_hi;
+		fq_opts.fqd.context_a.lo = ctx_a_lo;
+		fq_opts.fqd.context_b = ctxB;
+	} else {
+		uint32_t ctx_a_excl, ctx_a_len;
+		ctx_a_excl = (QM_STASHING_EXCL_DATA | QM_STASHING_EXCL_CTX);
+		ctx_a_len = (1 << 2) | 1;
+		fq_opts.fqd.context_a.hi =
+					(ctx_a_excl << 24) | (ctx_a_len << 16);
+
+		/*
+		 * configure forwarding / flowID info:
+		 * - enable ctxB;
+		 * - set ctxA override;
+		 * - set in ctxB FQID or FlowID.
+		 */
+		fq_opts.we_mask |= QM_INITFQ_WE_CONTEXTB;
+		fq_opts.fqd.context_a.hi |= (1 << 31);
+		FM_CONTEXTB_SET_FQID(&(fq_opts.fqd.context_b), ctxB);
+	}
+
+	fq_opts.fqd.dest.wq = wq_id;
+	fq_opts.fqd.dest.channel = channel;
+
+	err = qman_init_fq(fq, flags, &fq_opts);
+	if (unlikely(err < 0)) {
+		xx_pr_err(("Could not init FQ with ID: %u\n", fq->fqid));
+		goto create_sec_fq_err;
+	}
+
+	return 0;
+
+ create_sec_fq_err:
+	/*Reset all fields of FQ structure (including FQID) to mark it invalid*/
+	memset(fq, 0, sizeof(struct qman_fq));
+
+	return err;
+}
+
+static int create_sa_fq_pair(struct dpa_ipsec_sa *sa,
+			     bool reuse_from_secfq, bool parked_to_secfq)
+{
+	void *ctxtA;
+	uint32_t ctxtA_hi, ctxtA_lo;
+	phys_addr_t addr;
+	struct qman_fq *from_Sec;
+	struct dpa_ipsec *dpa_ipsec;
+	uint32_t fqid_from_sec = 0, fqid_to_sec = 0;
+	int err;
+
+	xx_assert(sa);
+
+	dpa_ipsec = (struct dpa_ipsec *)sa->dpa_ipsec;
+	if (!dpa_ipsec) {
+		xx_pr_err(("Null IPSec Port pointer\n"));
+		return -EFAULT;
+	}
+
+	err = create_sec_descriptor(sa);
+	if (err < 0) {
+		xx_pr_err(("Could not create sec descriptor\n"));
+		return err;
+	}
+
+	ctxtA = sa->sec_desc;
+	addr = virt_to_phys(ctxtA);
+	ctxtA_hi = (uint32_t) (addr >> 32);
+	ctxtA_lo = (uint32_t) (addr);
+
+	if (sa->sa_dir == DPA_IPSEC_OUTBOUND) {
+		/* If reuse FROM SEC FQ is false than create other FROM SEC FQ
+		 * and set it as output frame queue for this SA. Otherwise
+		 * profit that you poses a valid FROM SEC FQ from the OLD SA
+		 * and use it accordingly.
+		 */
+		if (!reuse_from_secfq) {
+			/* acquire fqid for 'FROM SEC' fq */
+			err = get_new_fqid(dpa_ipsec, &fqid_from_sec);
+			if (err < 0)
+				return err;
+
+			err = create_sec_frame_queue(&fqid_from_sec,
+				 dpa_ipsec->config.post_sec_out_params.qm_tx_ch,
+				 sa->sa_wqid, 0, 0, /* ctxA */
+				 sa->out_flow_id, /*ctxB forwarding info*/
+				 FALSE, sa->from_sec_fq);
+			if (err < 0) {
+				xx_pr_err(("From SEC FQ couldn't be done\n"));
+				goto create_fq_pair_err;
+			}
+		}
+
+		/* acquire fqid for 'TO SEC' fq */
+		err = get_new_fqid(dpa_ipsec, &fqid_to_sec);
+		if (err < 0)
+			goto create_fq_pair_err;
+
+		from_Sec = sa->from_sec_fq;
+		err = create_sec_frame_queue(&fqid_to_sec,
+				dpa_ipsec->config.qm_sec_ch,
+				sa->sa_wqid, ctxtA_hi, ctxtA_lo, /* ctxA */
+				qman_fq_fqid(from_Sec), /*ctxB - output SEC fq*/
+				parked_to_secfq, sa->to_sec_fq);
+		if (err < 0) {
+			xx_pr_err(("Encrypt FQ(to SEC) couldn't be created\n"));
+			goto create_fq_pair_err;
+		}
+	} else { /* DPA_IPSEC_INBOUND */
+		/* If reuse FROM SEC FQ is false than create other FROM SEC FQ
+		 * and set it as output frame queue for this SA. Otherwise
+		 * profit that you poses a valid FROM SEC FQ from the OLD SA
+		 * and use it accordingly.
+		 */
+		if (!reuse_from_secfq) {
+			/* acquire fqid for 'FROM SEC' fq */
+			err = get_new_fqid(dpa_ipsec, &fqid_from_sec);
+			if (err < 0)
+				return err;
+			err = create_sec_frame_queue(&fqid_from_sec,
+				  dpa_ipsec->config.post_sec_in_params.qm_tx_ch,
+				  sa->sa_wqid, 0, 0,	/* ctxA */
+				  sa->inbound_flowid,	/* flowID */
+				  FALSE, sa->from_sec_fq);
+			if (err < 0) {
+				xx_pr_err(("Decrypt FQ(from SEC) couldn't be created\n"));
+				goto create_fq_pair_err;
+			}
+		}
+
+		/* acquire fqid for 'TO SEC' fq */
+		err = get_new_fqid(dpa_ipsec, &fqid_to_sec);
+		if (err < 0)
+			goto create_fq_pair_err;
+
+		from_Sec = sa->from_sec_fq;
+		err = create_sec_frame_queue(&fqid_to_sec,
+					dpa_ipsec->config.qm_sec_ch,
+					sa->sa_wqid, ctxtA_hi, ctxtA_lo,
+					from_Sec->fqid,	/*ctxB - output SEC fq*/
+					parked_to_secfq, sa->to_sec_fq);
+		if (err < 0) {
+			xx_pr_err(("Decrypt FQ(to SEC) couldn't be created"));
+			goto create_fq_pair_err;
+		}
+	}
+
+	return 0;
+
+ create_fq_pair_err:
+	if (qman_fq_fqid(sa->from_sec_fq) != 0)
+		remove_sa_sec_fq(sa->from_sec_fq);
+	else
+		put_free_fqid(fqid_from_sec);	/*just recycle the FQID*/
+
+	if (fqid_to_sec != 0)
+		put_free_fqid(fqid_to_sec); /*a FQID was allocated;recycle it*/
+
+	return err;
+
+}
+
+static int set_cipher_auth_alg(enum dpa_ipsec_cipher_alg alg_suite,
+			       uint16_t *cipher, uint16_t * auth)
+{
+	xx_assert(cipher);
+	xx_assert(auth);
+
+	*cipher = ipsec_algs[alg_suite].enc_alg;
+	*auth = ipsec_algs[alg_suite].auth_alg;
+
+	if (*cipher == OP_PCL_IPSEC_INVALID_ALG_ID ||
+	    *auth == OP_PCL_IPSEC_INVALID_ALG_ID)
+		return -EINVAL;
+
+	return 0;
+}
+
+static int copy_sa_params_to_out_sa(struct dpa_ipsec_sa *sa,
+				    struct dpa_ipsec_sa_params *sa_params)
+{
+	struct ip_header *outer_ip_hdr;
+	int err;
+
+	xx_assert(sa);
+	xx_assert(sa_params);
+
+	sa->sa_dir = DPA_IPSEC_OUTBOUND;
+	sa->parent_sa = NULL;
+	sa->sa_bpid = sa_params->sa_bpid;
+	sa->sa_wqid = sa_params->sa_wqid;
+
+	err = set_cipher_auth_alg(sa_params->crypto_params.alg_suite,
+				  &sa->cipher_data.cipher_type,
+				  &sa->auth_data.auth_type);
+	if (err < 0)
+		return err;
+
+	sa->auth_data.auth_key_len = sa_params->crypto_params.auth_key_len;
+	memcpy(sa->auth_data.auth_key,
+	       sa_params->crypto_params.auth_key,
+	       sa_params->crypto_params.auth_key_len);
+
+	sa->cipher_data.cipher_key_len =
+		sa_params->crypto_params.cipher_key_len;
+	memcpy(sa->cipher_data.cipher_key,
+	       sa_params->crypto_params.cipher_key,
+	       sa_params->crypto_params.cipher_key_len);
+	sa->sec_desc->pdb_en.spi = sa_params->spi;
+	sa->sec_desc->pdb_en.options = PDBOPTS_ESP_TUNNEL |
+				       PDBOPTS_ESP_INCIPHDR |
+				       PDBOPTS_ESP_IPHDRSRC |
+				       PDBOPTS_ESP_UPDATE_CSUM;
+	if (sa_params->use_ext_seq_num) {
+		sa->sec_desc->pdb_en.seq_num_ext_hi =
+			(sa_params->start_seq_num & SEQ_NUM_HI_MASK) >> 32;
+		sa->sec_desc->pdb_en.options |= PDBOPTS_ESP_ESN;
+	}
+	sa->sec_desc->pdb_en.seq_num =
+				sa_params->start_seq_num & SEQ_NUM_LOW_MASK;
+
+	if (!sa_params->sa_out_params.init_vector)
+		sa->sec_desc->pdb_en.options |= PDBOPTS_ESP_IVSRC;
+	else
+		memcpy(&sa->sec_desc->pdb_en.cbc,
+		       sa_params->sa_out_params.init_vector->init_vector,
+		       sa_params->sa_out_params.init_vector->length);
+
+	sa->sec_desc->pdb_en.ip_nh = NEXT_HEADER_IS_IPv4;
+
+	sa->out_flow_id = sa_params->sa_out_params.post_sec_flow_id;
+
+	/* Copy the outer header and generate the original header checksum */
+	memcpy(&sa->sec_desc->pdb_en.ip_hdr[0],
+	       sa_params->sa_out_params.outer_ip_header,
+	       sa_params->sa_out_params.ip_hdr_size);
+
+	if (sa_params->sa_out_params.outer_udp_header) {
+		uint8_t *tmp;
+		tmp = (uint8_t *) &sa->sec_desc->pdb_en.ip_hdr[0];
+		memcpy(tmp + sa_params->sa_out_params.ip_hdr_size,
+		       sa_params->sa_out_params.outer_udp_header,
+		       UDP_HEADER_LEN);
+		sa->sec_desc->pdb_en.ip_hdr_len =
+			sa_params->sa_out_params.ip_hdr_size + UDP_HEADER_LEN;
+	} else {
+		sa->sec_desc->pdb_en.ip_hdr_len =
+				sa_params->sa_out_params.ip_hdr_size;
+	}
+
+	outer_ip_hdr = (struct ip_header *) &sa->sec_desc->pdb_en.ip_hdr[0];
+	outer_ip_hdr->check = ip_fast_csum((unsigned char *)outer_ip_hdr,
+					   outer_ip_hdr->ihl);
+
+#ifdef DEBUG_PARAM
+	/* Printing all the parameters */
+	print_sa_sec_param(sa);
+#endif
+
+	return 0;
+}
+
+static int copy_sa_params_to_in_sa(struct dpa_ipsec_sa *sa,
+				   struct dpa_ipsec_sa_params *sa_params,
+				   bool rekeying)
+{
+	struct dpa_ipsec *dpa_ipsec;
+	int err;
+
+	xx_assert(sa);
+	xx_assert(sa_params);
+
+	dpa_ipsec = (struct dpa_ipsec *)sa->dpa_ipsec;
+
+	/* reserve a FlowID for this SA only if we are not rekeying */
+	if (!rekeying) {
+		err = get_inbound_flowid(dpa_ipsec, &sa->inbound_flowid);
+		if (err < 0) {
+			xx_pr_err(("Can't get valid inbound flow id\n"));
+			return -EINVAL;
+		}
+	}
+
+	sa->sa_dir = DPA_IPSEC_INBOUND;
+	sa->sa_bpid = sa_params->sa_bpid;
+	sa->sa_wqid = sa_params->sa_wqid;
+	sa->spi = sa_params->spi;
+
+	err = set_cipher_auth_alg(sa_params->crypto_params.alg_suite,
+				  &sa->cipher_data.cipher_type,
+				  &sa->auth_data.auth_type);
+	if (err < 0)
+		return err;
+
+	sa->auth_data.auth_key_len = sa_params->crypto_params.auth_key_len;
+	memcpy(sa->auth_data.auth_key,
+	       sa_params->crypto_params.auth_key,
+	       sa_params->crypto_params.auth_key_len);
+
+	sa->cipher_data.cipher_key_len =
+			sa_params->crypto_params.cipher_key_len;
+	memcpy(sa->cipher_data.cipher_key,
+	       sa_params->crypto_params.cipher_key,
+	       sa_params->crypto_params.cipher_key_len);
+
+	sa->use_udp_encap = sa_params->sa_in_params.use_udp_encap;
+	sa->udp_src_port  = sa_params->sa_in_params.src_port;
+	sa->udp_dest_port = sa_params->sa_in_params.dest_port;
+
+	memcpy(&sa->def_sa_action,
+	       &sa_params->sa_in_params.post_ipsec_action,
+	       sizeof(struct dpa_cls_tbl_action));
+
+	sa->sec_desc->pdb_dec.seq_num =
+			sa_params->start_seq_num & SEQ_NUM_LOW_MASK;
+	sa->sec_desc->pdb_dec.options = PDBOPTS_ESP_TUNNEL |
+					PDBOPTS_ESP_OUTFMT;
+	if (sa_params->use_ext_seq_num) {
+		sa->sec_desc->pdb_en.seq_num_ext_hi =
+			(sa_params->start_seq_num & SEQ_NUM_HI_MASK) >> 32;
+		sa->sec_desc->pdb_dec.options |= PDBOPTS_ESP_ESN;
+	}
+
+	switch (sa_params->sa_in_params.arw) {
+	case DPA_IPSEC_ARSNONE:
+		sa->sec_desc->pdb_dec.options |= PDBOPTS_ESP_ARSNONE;
+		break;
+	case DPA_IPSEC_ARS32:
+		sa->sec_desc->pdb_dec.options |= PDBOPTS_ESP_ARS32;
+		break;
+	case DPA_IPSEC_ARS64:
+		sa->sec_desc->pdb_dec.options |= PDBOPTS_ESP_ARS64;
+		break;
+	default:
+		xx_pr_err(("Invalid ARS mode specified\n"));
+		return -EINVAL;
+	}
+
+	/* Updated the offset to the point in frame were the encrypted
+	 * stuff starts.*/
+	sa->sec_desc->pdb_dec.hmo_ip_hdr_len = (uint16_t) sizeof(struct ip_header);
+	if (sa->use_udp_encap)
+		sa->sec_desc->pdb_dec.hmo_ip_hdr_len += UDP_HEADER_LEN;
+
+	memcpy(&sa->src_addr,
+	       &sa_params->sa_in_params.src_addr,
+	       sizeof(struct dpa_ipsec_ip_address));
+
+	memcpy(&sa->dest_addr,
+	       &sa_params->sa_in_params.dest_addr,
+	       sizeof(struct dpa_ipsec_ip_address));
+
+	sa->policy_miss_fqid = sa_params->sa_in_params.policy_miss_fqid;
+#ifdef DEBUG_PARAM
+	/* Printing all the parameters */
+	print_sa_sec_param(sa);
+#endif
+
+	return 0;
+}
+
+static void copy_policy_selectors(struct dpa_ipsec_policy_params *policy_params,
+			struct dpa_ipsec_policy_selectors *policy_selectors)
+{
+	/* TODO: find a better way to do this */
+	policy_selectors->src_addr = policy_params->src_addr;
+	policy_selectors->src_prefix_len = policy_params->src_prefix_len;
+	policy_selectors->dest_addr = policy_params->dest_addr;
+	policy_selectors->dest_prefix_len = policy_params->dest_prefix_len;
+	policy_selectors->protocol = policy_params->protocol;
+	policy_selectors->src_port = policy_params->src_port;
+	policy_selectors->src_port_mask = policy_params->src_port_mask;
+	policy_selectors->dest_port = policy_params->dest_port;
+	policy_selectors->dest_port_mask = policy_params->dest_port_mask;
+
+	return;
+}
+
+static struct dpa_ipsec_policy_entry *store_policy_param_to_sa_pol_list(
+				struct dpa_ipsec_sa *sa,
+				struct dpa_ipsec_policy_params *policy_params)
+{
+	struct dpa_ipsec_policy_entry *policy_entry;
+
+	xx_assert(sa);
+	xx_assert(policy_params);
+
+	policy_entry = xx_malloc(sizeof(struct dpa_ipsec_policy_entry));
+	if (!policy_entry) {
+		xx_pr_err(("Could not allocate memory for policy\n"));
+		return NULL;
+	}
+	copy_policy_selectors(policy_params, &policy_entry->policy_selectors);
+	policy_entry->mtu = policy_params->mtu;
+	policy_entry->df_action = policy_params->df_action;
+	policy_entry->priority = policy_params->priority;
+	list_add(&policy_entry->node, &sa->policy_headlist);
+	return policy_entry;
+}
+
+static struct dpa_ipsec_policy_entry *find_policy_in_sa_policy_list(
+				struct dpa_ipsec_sa  *sa,
+				struct dpa_ipsec_policy_params *policy_params)
+{
+	struct dpa_ipsec_policy_entry *policy_entry, *tmp_policy_entry;
+	struct dpa_ipsec_policy_selectors policy_selectors;
+
+	xx_assert(sa);
+	xx_assert(policy_params);
+
+	if (list_empty(&sa->policy_headlist)) {
+		xx_pr_err(("Policy parameter list is empty\n"));
+		return NULL;
+	}
+
+	list_for_each_entry_safe(policy_entry, tmp_policy_entry,
+				 &sa->policy_headlist, node) {
+		copy_policy_selectors(policy_params, &policy_selectors);
+		if (!memcmp(&policy_entry->policy_selectors, &policy_selectors,
+			    sizeof(struct dpa_ipsec_policy_selectors))) {
+			/* found the entry that matches the input policy
+			 * parameters */
+			return policy_entry;
+		}
+	}
+
+	/* did not find the entry that matches the input policy parameters */
+	return NULL;
+}
+
+static int get_policy_count_for_sa(struct dpa_ipsec_sa *sa)
+{
+	struct dpa_ipsec_policy_entry *policy_entry, *tmp_policy_entry;
+	int pol_count = 0;
+
+	if (list_empty(&sa->policy_headlist)) {
+		xx_pr_err(("Policy parameter list is empty\n"));
+		return 0;
+	}
+
+	list_for_each_entry_safe(policy_entry, tmp_policy_entry,
+				 &sa->policy_headlist, node)
+		pol_count++;
+
+	return pol_count;
+}
+
+static int copy_all_policies(struct dpa_ipsec_sa *sa,
+			     struct dpa_ipsec_policy_params *policy_params,
+			     int num_pol)
+{
+	struct dpa_ipsec_policy_entry *policy_entry, *tmp_policy_entry;
+	int pol_count = 0;
+
+	if (list_empty(&sa->policy_headlist)) {
+		xx_pr_err(("Policy parameter list is empty\n"));
+		return 0;
+	}
+
+	list_for_each_entry_safe(policy_entry, tmp_policy_entry,
+				 &sa->policy_headlist, node) {
+		struct dpa_ipsec_policy_params *pol_param;
+		struct dpa_ipsec_policy_selectors *pol_sel;
+
+		pol_count++;
+		if (pol_count > num_pol) {
+			xx_pr_err(("Num policies in this SA greater than %d",
+				num_pol));
+			return -EAGAIN;
+		}
+
+		pol_param = (struct dpa_ipsec_policy_params *)policy_entry;
+		pol_sel = (struct dpa_ipsec_policy_selectors *)
+				&policy_params[pol_count - 1];
+		copy_policy_selectors(pol_param, pol_sel);
+		policy_params[pol_count - 1].mtu = policy_entry->mtu;
+		policy_params[pol_count - 1].df_action =
+						policy_entry->df_action;
+		policy_params[pol_count - 1].priority = policy_entry->priority;
+	}
+
+	return 0;
+}
+
+static int remove_policy_from_sa_policy_list(struct dpa_ipsec_sa *sa,
+					     struct dpa_ipsec_policy_entry
+					     *policy_entry)
+{
+	xx_assert(sa);
+	xx_assert(policy_entry);
+
+	if (list_empty(&sa->policy_headlist)) {
+		xx_pr_err(("Policy parameter list is empty\n"));
+		return -EINVAL;
+	}
+
+	list_del(&policy_entry->node);
+	xx_free(policy_entry);
+
+	return 0;
+}
+
+static int remove_policy(struct dpa_ipsec_sa *sa,
+			 struct dpa_ipsec_policy_entry *policy_entry)
+{
+	int err;
+
+	if (sa->sa_dir == DPA_IPSEC_INBOUND) {
+		err = update_inbound_policy(sa, policy_entry, MNG_OP_REMOVE);
+		if (err < 0) {
+			xx_pr_err(("Could not remove the inbound policy\n"));
+			return err;
+		}
+		err = remove_policy_from_sa_policy_list(sa, policy_entry);
+		if (err < 0) {
+			xx_pr_err(("Couldn't remove inbound policy from SA policy list\n"));
+			return err;
+		}
+	} else {  /* DPA_IPSEC_OUTBOUND */
+		err = update_outbound_policy(sa, policy_entry, MNG_OP_REMOVE);
+		if (err < 0) {
+			xx_pr_err(("Could not remove the outbound policy\n"));
+			return err;
+		}
+		err = remove_policy_from_sa_policy_list(sa, policy_entry);
+		if (err < 0) {
+			xx_pr_err(("Could not remove outbound policy from SA policy list\n"));
+			return err;
+		}
+	}
+
+	return 0;
+}
+
+static struct dpa_ipsec_sa *get_sa_from_sa_id(int sa_id)
+{
+	struct dpa_ipsec *dpa_ipsec;
+	struct dpa_ipsec_sa_mng *sa_mng;
+	struct dpa_ipsec_sa *sa = NULL;
+
+	if (!gbl_dpa_ipsec) {
+		xx_pr_err(("There is no dpa_ipsec component initialized\n"));
+		return NULL;
+	}
+
+	dpa_ipsec = gbl_dpa_ipsec;
+	sa_mng = &dpa_ipsec->sa_mng;
+
+	if ((sa_id < 0) || (sa_id > sa_mng->max_num_sa)) {
+		xx_pr_err(("Invalid SA id %d provided\n", sa_id));
+		return NULL;
+	}
+	sa = &sa_mng->sa[sa_id];
+
+	return sa;
+}
+
+static int check_sa_params(struct dpa_ipsec_sa_params *sa_params)
+{
+	uint16_t cipher_alg, auth_alg;
+	int err = 0;
+
+	xx_assert(sa_params);
+
+	/*
+	 * check crypto params:
+	 * - an authentication key must always be provided
+	 * - a cipher key must be provided if alg != NULL encryption
+	 */
+
+	err = set_cipher_auth_alg(sa_params->crypto_params.alg_suite,
+				  &cipher_alg, &auth_alg);
+	if (err < 0)
+		return err;
+
+	if (sa_params->crypto_params.auth_key == NULL) {
+		xx_pr_err(("A valid authentication key must be provided\n"));
+		return -EINVAL;
+	}
+
+	/* TODO: check cipher_key ONLY if alg != null encryption */
+	if (sa_params->crypto_params.cipher_key == NULL) {
+		xx_pr_err(("A valid cipher key must be provided\n"));
+		return -EINVAL;
+	}
+
+	if (sa_params->sa_dir == DPA_IPSEC_OUTBOUND) {
+		if ((sa_params->sa_out_params.ip_hdr_size == 0) ||
+		    (sa_params->sa_out_params.outer_ip_header == NULL)) {
+			xx_pr_err(("Transport mode is not currently supported."
+				   "Specify a valid encapsulation header\n"));
+			return -EINVAL;
+		}
+	} else {
+		/* Inbound SA */
+		if (sa_params->sa_in_params.src_addr.addr_type !=
+		    sa_params->sa_in_params.dest_addr.addr_type) {
+			xx_pr_err(("Source and destination IP address must be of same type\n"));
+			return -EINVAL;
+		}
+	}
+
+	/* check buffer pool ID validity */
+	if (sa_params->sa_bpid > MAX_BUFFER_POOL_ID) {
+		xx_pr_err(("Invalid SA buffer pool ID.\n"));
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+int dpa_ipsec_init(const struct dpa_ipsec_params *params, int *dpa_ipsec_id)
+{
+	struct dpa_ipsec *dpa_ipsec = NULL;
+	uint32_t max_num_sa;
+	int err = 0;
+
+	/* multiple DPA IPSec instances are not currently supported */
+	unused(dpa_ipsec_id);
+
+	if (gbl_dpa_ipsec) {
+		xx_pr_err(("There is already an initialized dpa_ipsec component.\n"));
+		xx_pr_err(("Multiple DPA IPSec Instances aren't currently supported.\n"));
+		return -EAGAIN;
+	}
+
+	xx_sanity_check_return_value(params, "initialization parameters",
+				     -EINVAL);
+
+	/* make sure all user params are ok and init can start */
+	err = check_ipsec_params(params);
+	if (err < 0)
+		return err;
+
+	/* alloc control block */
+	dpa_ipsec = (struct dpa_ipsec *)xx_malloc(sizeof(struct dpa_ipsec));
+	if (!dpa_ipsec) {
+		xx_pr_err(("Could not allocate memory for control block.\n"));
+		return -ENOMEM;
+	}
+	memset(dpa_ipsec, 0, sizeof(struct dpa_ipsec));
+
+	/* store parameters */
+	store_ipsec_params(dpa_ipsec, params);
+
+	/* init SA manager */
+	err = init_sa_manager(dpa_ipsec);
+	if (err < 0) {
+		free_resources();
+		return err;
+	}
+
+	/* Init used sa vector */
+	max_num_sa = dpa_ipsec->sa_mng.max_num_sa;
+	dpa_ipsec->used_sa_ids = xx_malloc(max_num_sa * sizeof(uint32_t));
+	if (!dpa_ipsec->used_sa_ids) {
+		xx_pr_err(("No more memory for used sa id's vector "));
+		return -ENOMEM;
+	}
+	memset(dpa_ipsec->used_sa_ids, DPA_IPSEC_INVALID_SA_ID,
+	       max_num_sa * sizeof(uint32_t));
+	dpa_ipsec->num_used_sas = 0;
+
+	gbl_dpa_ipsec = dpa_ipsec;
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(dpa_ipsec_init);
+
+int dpa_ipsec_free(int dpa_ipsec_id)
+{
+	int err = 0;
+
+	/* multiple DPA IPSec instances are not currently supported */
+	unused(dpa_ipsec_id);
+
+	/* destroy all SAs offloaded in this DPA IPSec instance */
+	err = dpa_ipsec_flush_all_sa(0);
+	if (err < 0) {
+		xx_pr_err(("Could not remove all SAs from this instance!\n"));
+		return -EINVAL;
+	}
+
+	free_resources();
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(dpa_ipsec_free);
+
+int dpa_ipsec_create_sa(int dpa_ipsec_id,
+			struct dpa_ipsec_sa_params *sa_params, int *sa_id)
+{
+	struct dpa_ipsec *dpa_ipsec;
+	struct dpa_ipsec_sa_mng *sa_mng;
+	struct dpa_ipsec_sa *sa;
+	uint32_t id;
+	int err = 0, err_rb = 0, i;
+
+	/* multiple DPA IPSec instances are not currently supported */
+	unused(dpa_ipsec_id);
+
+	if (!gbl_dpa_ipsec) {
+		xx_pr_err(("There is no dpa_ipsec component initialized\n"));
+		return -EPERM;
+	}
+
+	if (!sa_params) {
+		xx_pr_err(("Invalid input params\n"));
+		return -EINVAL;
+	}
+
+	err = check_sa_params(sa_params);
+	if (err < 0)
+		return err;
+
+	*sa_id = DPA_IPSEC_INVALID_SA_ID;
+	dpa_ipsec = gbl_dpa_ipsec;
+	sa_mng = &dpa_ipsec->sa_mng;
+
+	/* Get an id for new SA */
+	if (cq_get_4bytes(sa_mng->sa_id_cq, &id) < 0) {
+		xx_pr_err(("No more unused SA handles"));
+		return -ERANGE;
+	}
+
+	/* Acquire a preallocated SA structure */
+	sa = &sa_mng->sa[id];
+	/* AV's TODO: create a clean function for preallocated SA structure
+	 * and call here that function */
+
+	/* Update internal SA structure */
+	sa->id = id;
+	sa->sa_dir = sa_params->sa_dir;
+	sa->dpa_ipsec = dpa_ipsec;
+
+	sa->used_sa_index = -1;
+	for (i = 0; i < sa->dpa_ipsec->sa_mng.max_num_sa; i++)
+		if (sa->dpa_ipsec->used_sa_ids[i] == DPA_IPSEC_INVALID_SA_ID)
+			break;
+	if (i == sa->dpa_ipsec->sa_mng.max_num_sa) {
+		xx_pr_err(("No more unused SAs"));
+		goto create_sa_err;
+	}
+	sa->used_sa_index = i;
+	sa->dpa_ipsec->used_sa_ids[i] = sa->id;
+	sa->dpa_ipsec->num_used_sas++;
+
+	/* Copy SA params into the internal SA structure */
+	if (sa->sa_dir == DPA_IPSEC_OUTBOUND) {
+		sa->parent_sa = NULL;
+		err = copy_sa_params_to_out_sa(sa, sa_params);
+	} else { /* DPA_IPSEC_INBOUND */
+		sa->parent_sa = NULL;
+		err = copy_sa_params_to_in_sa(sa, sa_params, FALSE);
+	}
+
+	if (err < 0) {
+		xx_pr_err(("Could not copy SA parameters into SA\n"));
+		goto create_sa_err;
+	}
+
+	/* Generate the split key from the normal auth key */
+	err = generate_split_key(&sa->auth_data);
+	if (err < 0)
+		goto create_sa_err;
+
+	/* Call internal function to create SEC FQ according to SA parameters */
+	err = create_sa_fq_pair(sa, FALSE, FALSE);
+	if (err < 0) {
+		xx_pr_err(("Could not create SEC frame queues\n"));
+		goto create_sa_err;
+	}
+
+	if (sa->sa_dir == DPA_IPSEC_INBOUND) {
+		/*Mark classifier entries as invalid (useful for rolling back)*/
+		sa->inbound_hash_entry = -1;
+		sa->inbound_indx_entry = -1;
+
+		/* Call update_pre_sec_inbound_table to place an entry in the
+		 *  PCD of the Rx Port */
+		err = update_pre_sec_inbound_table(sa, MNG_OP_ADD);
+		if (err < 0) {
+			xx_pr_err(("Could not update PCD entry\n"));
+			goto create_sa_err;
+		}
+
+		if (dpa_ipsec->config.post_sec_in_params.do_pol_check == TRUE) {
+			int inbpol_td;
+			int inbindx_td;
+			struct dpa_cls_tbl_action miss_action, action;
+			struct dpa_cls_tbl_key inbindx_key;
+
+			err = get_free_inbpol_tbl(dpa_ipsec, &inbpol_td);
+			if (err < 0) {
+				xx_pr_err(("Could not get a free EM table\n"));
+				goto create_sa_err;
+			}
+			sa->em_inpol_td = inbpol_td;
+
+			/* Link Exact Match table with the index table on
+			 * inbound_flowid */
+			inbindx_td =
+				dpa_ipsec->config.post_sec_in_params.dpa_cls_td;
+
+			/*TODO - case when flow id is greater than 255 */
+			inbindx_key.byte[0] = (uint8_t) sa->inbound_flowid;
+			memset(&action, 0, sizeof(action));
+			action.type = DPA_CLS_TBL_ACTION_NEXT_TABLE;
+			action.next_table_params.next_td = inbpol_td;
+			action.enable_statistics = FALSE;
+			err = dpa_classif_table_insert_entry(inbindx_td,
+						&inbindx_key,
+						&action, 0,
+						       &sa->inbound_indx_entry);
+			if (err < 0) {
+				xx_pr_err(("Can't link EM table with index table\n"));
+				goto create_sa_err;
+			}
+
+			memset(&miss_action, 0, sizeof(miss_action));
+			miss_action.type = DPA_CLS_TBL_ACTION_ENQ;
+			miss_action.enq_params.hm = NULL;
+			miss_action.enq_params.new_fqid = sa->policy_miss_fqid;
+			miss_action.enq_params.override_fqid = TRUE;
+			miss_action.enq_params.policer_params = NULL;
+			miss_action.enable_statistics = FALSE;
+			err = dpa_classif_table_modify_miss_action(inbpol_td,
+								 &miss_action);
+			if (err < 0) {
+				xx_pr_err(("Can't set policy miss action\n"));
+				goto create_sa_err;
+			}
+		} else {
+			/* Set the post decryption default action */
+			err = set_in_sa_default_action(sa);
+			if (err < 0) {
+				xx_pr_err(("Could not set default action for post decryption\n"));
+				goto create_sa_err;
+			}
+		}
+	}
+
+	/* SA done ok. Return the SA id */
+	*sa_id = id;
+
+	return 0;
+
+	/* Something went wrong. Begin rollback */
+ create_sa_err:
+
+	/* Prepare to return a invalid SA if rollback succeeds */
+	*sa_id = DPA_IPSEC_INVALID_SA_ID;
+
+	if ((sa->sa_dir == DPA_IPSEC_INBOUND) &&
+	    (sa->inbound_hash_entry != -1)) {
+		err_rb = update_pre_sec_inbound_table(sa, MNG_OP_REMOVE);
+		if (err_rb < 0) {
+			xx_pr_err(("Could not remove SA entry from lookup table.\n"));
+			*sa_id = id;
+		}
+	}
+	sa->inbound_hash_entry = -1;
+
+	if ((sa->sa_dir == DPA_IPSEC_INBOUND) &&
+	    (dpa_ipsec->config.post_sec_in_params.do_pol_check == TRUE) &&
+	    (sa->inbound_indx_entry != -1)) {
+		err_rb = dpa_classif_table_delete_entry_by_ref(
+				dpa_ipsec->config.post_sec_in_params.dpa_cls_td,
+				sa->inbound_indx_entry);
+		if (err_rb < 0) {
+			xx_pr_err(("Could not remove entry in post decryption table\n"));
+			*sa_id = id;
+		}
+	}
+	sa->inbound_indx_entry = -1;
+
+	err_rb = remove_sa_fq_pair(sa);
+	if (err_rb < 0) {
+		xx_pr_err(("Could not remove SA FQs.\n"));
+		*sa_id = id;
+	}
+
+	/* RollBack succeeded. Free the SA id and FlowID (for inbound SAs only).
+	 * If any of these operations fail, do nothing, because this will
+	 * result in blocking the IDs (not a critical resource)*/
+	if (*sa_id == DPA_IPSEC_INVALID_SA_ID) {
+		put_inbound_flowid(dpa_ipsec, sa->inbound_flowid);
+		if (sa->used_sa_index != -1) {
+			sa->dpa_ipsec->used_sa_ids[sa->used_sa_index] =
+					DPA_IPSEC_INVALID_SA_ID;
+			sa->used_sa_index = -1;
+			sa->dpa_ipsec->num_used_sas--;
+		}
+		cq_put_4bytes(sa_mng->sa_id_cq, id);
+	}
+
+	return err;
+}
+EXPORT_SYMBOL_GPL(dpa_ipsec_create_sa);
+
+/*
+ * The steps implemented here are:
+ * 1. Remove the PCD entries that make traffic to go to SEC
+ * 2. Wait in order to consume all frames in
+ *    the TO_SEC queue and to be correctly distributed by the PCD entries
+ *    that reside in the offline port post SEC.
+ * 3. Remove all the PCD entries from the offline port post SEC
+ * 4. Destroy the TO_SEC and FROM_SEC queues
+ * 5. Free all memory used for this SA
+ */
+int dpa_ipsec_remove_sa(int sa_id)
+{
+	struct dpa_ipsec *dpa_ipsec;
+	struct dpa_ipsec_sa_mng *sa_mng;
+	struct dpa_ipsec_sa *sa;
+	struct dpa_ipsec_policy_entry *policy_entry, *tmp_policy_entry;
+	int timeout = 10000, err = 0;
+
+	if (!gbl_dpa_ipsec) {
+		xx_pr_err(("There is no dpa_ipsec instance initialized\n"));
+		return -EINVAL;
+	}
+
+	dpa_ipsec = gbl_dpa_ipsec;
+	sa_mng = &dpa_ipsec->sa_mng;
+
+	sa = get_sa_from_sa_id(sa_id);
+	if (!sa) {
+		xx_pr_err(("Invalid SA handle\n"));
+		return -EINVAL;
+	}
+
+	if (sa->sa_dir == DPA_IPSEC_INBOUND) {
+		/* Remove the PCD entry that makes traffic to go to SEC
+		 * First check that we have a valid entry reference.
+		 * In case previous add/remove operations failed, this entry
+		 * may already have been removed
+		 */
+		if (sa->inbound_hash_entry != -1) {
+			err = update_pre_sec_inbound_table(sa, MNG_OP_REMOVE);
+			if (err < 0) {
+				xx_pr_err(("Could not remove the PCD entry for this SA\n"));
+				return -EAGAIN;
+			}
+		}
+
+		/* AV's note: Wait until all the frames from the TO_SEC FQ are
+		 * consumed */
+		wait_until_fq_empty(sa->to_sec_fq, &timeout);
+		if (timeout == 0) {
+			/* AV's note: TODO Add this SA to the SA garbage
+			 * collector list because we can't remove it until
+			 *  it's to SEC FQ is empty. */
+			return -EBUSY;
+		}
+
+		/* AV's note: Wait until all the frames from the FROM_SEC FQ are
+		 * consumed */
+		timeout = 10000;
+		wait_until_fq_empty(sa->from_sec_fq, &timeout);
+		if (timeout == 0) {
+			/* AV's note: TODO Add this SA to the SA garbage
+			 * collector list because we can't remove it until it's
+			 * to SEC FQ is empty. */
+			return -EBUSY;
+		}
+
+		/* Remove all the PCD entries from the offline port post SEC */
+		if (!list_empty(&sa->policy_headlist)) {
+			list_for_each_entry_safe(policy_entry,
+						 tmp_policy_entry,
+						 &sa->policy_headlist, node) {
+				err = remove_policy(sa, policy_entry);
+				if (err < 0) {
+					xx_pr_err(("Could not remove PCD entry for policy\n"));
+					return -EAGAIN;
+				}
+			}
+		}
+
+		/* Destroy the TO_SEC and FROM_SEC queues */
+		err = remove_sa_fq_pair(sa);
+		if (err != 0) {
+			xx_pr_err(("Could not remove the SEC frame queues\n"));
+			return -EAGAIN;
+		}
+
+		/* Free all memory used for this SA */
+		if ((dpa_ipsec->config.post_sec_in_params.do_pol_check == TRUE)
+		    && (sa->inbound_indx_entry != -1)) {
+			int td;
+			td = dpa_ipsec->config.post_sec_in_params.dpa_cls_td;
+			err = dpa_classif_table_delete_entry_by_ref(td,
+							sa->inbound_indx_entry);
+			if (err < 0) {
+				xx_pr_err(("Could not remove SA entry in indexed table\n"));
+				return -EAGAIN;
+			}
+			put_free_inbpol_tbl(dpa_ipsec, sa->em_inpol_td);
+		}
+		put_inbound_flowid(dpa_ipsec, sa->inbound_flowid);
+
+		/* Mark as free index in used sa ids vector of this dpa ipsec
+		 * instance */
+		dpa_ipsec->used_sa_ids[sa->used_sa_index] =
+		    DPA_IPSEC_INVALID_SA_ID;
+		dpa_ipsec->num_used_sas--;
+		cq_put_4bytes(sa_mng->sa_id_cq, sa->id);
+	} else {  /* DPA_IPSEC_OUTBOUND */
+		/* Remove the PCD entries that make traffic to go to SEC */
+		if (!list_empty(&sa->policy_headlist)) {
+			list_for_each_entry_safe(policy_entry,
+						 tmp_policy_entry,
+						 &sa->policy_headlist, node) {
+				err = remove_policy(sa, policy_entry);
+				if (err < 0) {
+					xx_pr_err(("Could not remove policy PCD entry\n"));
+					return -EAGAIN;
+				}
+			}
+		}
+
+		/* Destroy the TO_SEC and FROM_SEC queues */
+		err = remove_sa_fq_pair(sa);
+		if (err < 0) {
+			xx_pr_err(("Could not remove the SEC frame queues\n"));
+			return -EAGAIN;
+		}
+
+		/* Mark as free index in used sa ids vector of this dpa ipsec
+		 * instance */
+		dpa_ipsec->used_sa_ids[sa->used_sa_index] =
+						DPA_IPSEC_INVALID_SA_ID;
+		dpa_ipsec->num_used_sas--;
+		/* Free all memory used for this SA */
+		cq_put_4bytes(sa_mng->sa_id_cq, sa->id);
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(dpa_ipsec_remove_sa);
+
+int dpa_ipsec_sa_add_policy(int sa_id,
+			    struct dpa_ipsec_policy_params *policy_params)
+{
+	struct dpa_ipsec_policy_entry *policy_entry;
+	struct dpa_ipsec_sa *sa;
+	int err;
+
+	xx_sanity_check_return_value(policy_params, "policy params", -EINVAL);
+
+	sa = get_sa_from_sa_id(sa_id);
+	if (!sa) {
+		xx_pr_err(("Invalid SA handle\n"));
+		return -EINVAL;
+	}
+
+	/* AV's note: one SA could have more in/out policies
+	 * Store all the in/out policies into the SA policy param list in order
+	 * to know what to remove when SA expires.
+	 */
+	policy_entry = store_policy_param_to_sa_pol_list(sa, policy_params);
+	if (!policy_entry) {
+		xx_pr_err(("Could not store the policy in the SA\n"));
+		return -EINVAL;
+	}
+
+	/*Insert inbound or outbound policy for this SA depending on it's type*/
+	if (sa->sa_dir == DPA_IPSEC_INBOUND) {
+		err = update_inbound_policy(sa, policy_entry, MNG_OP_ADD);
+		if (err < 0) {
+			remove_policy_from_sa_policy_list(sa, policy_entry);
+			xx_pr_err(("Could not add the inbound policy\n"));
+		}
+	} else {  /* DPA_IPSEC_OUTBOUND */
+		err = update_outbound_policy(sa, policy_entry, MNG_OP_ADD);
+		if (err < 0) {
+			remove_policy_from_sa_policy_list(sa, policy_entry);
+			xx_pr_err(("Could not add the outbound policy\n"));
+		}
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(dpa_ipsec_sa_add_policy);
+
+int dpa_ipsec_sa_remove_policy(int sa_id,
+			       struct dpa_ipsec_policy_params *policy_params)
+{
+	struct dpa_ipsec_policy_entry *policy_entry;
+	struct dpa_ipsec_sa *sa;
+	int err = 0;
+
+	xx_sanity_check_return_value(policy_params, "policy params", -EINVAL);
+
+	sa = get_sa_from_sa_id(sa_id);
+	if (!sa) {
+		xx_pr_err(("Invalid SA handle provided\n"));
+		return -EINVAL;
+	}
+
+	policy_entry = find_policy_in_sa_policy_list(sa, policy_params);
+	if (!policy_entry) {
+		xx_pr_err(("Could not find policy entry in SA policy list\n"));
+		return -EAGAIN;
+	}
+
+	/* found the policy entry in SA policy parameter list;
+	 * depending on the type of the SA remove the PCD entry for this policy
+	 * and afterwards remove the policy param from SA policy param list */
+	err = remove_policy(sa, policy_entry);
+
+	return err;
+}
+EXPORT_SYMBOL_GPL(dpa_ipsec_sa_remove_policy);
+
+int dpa_ipsec_sa_rekeying(int sa_id,
+			  struct dpa_ipsec_sa_params *sa_params,
+			  dpa_ipsec_rekey_event_cb *rekey_event_cb,
+			  int auto_rmv_old_sa,
+			  int *new_sa_id)
+{
+	struct dpa_ipsec *dpa_ipsec;
+	struct dpa_ipsec_sa_mng *sa_mng;
+	struct dpa_ipsec_sa *old_sa, *new_sa;
+	struct dpa_ipsec_policy_entry *policy_entry, *tmp_policy_entry;
+	uint32_t id, i;
+	int err = 0, timeout = 10000;
+
+	unused(rekey_event_cb);
+	unused(auto_rmv_old_sa);
+
+	if ((!sa_params) || (sa_params->sa_bpid > MAX_BUFFER_POOL_ID)) {
+		xx_pr_err(("Invalid input params\n"));
+		return -EINVAL;
+	}
+
+	*new_sa_id = DPA_IPSEC_INVALID_SA_ID;
+	if (!gbl_dpa_ipsec) {
+		xx_pr_err(("There is no dpa_ipsec instance initialized\n"));
+		return -EINVAL;
+	}
+	dpa_ipsec = gbl_dpa_ipsec;
+
+	old_sa = get_sa_from_sa_id(sa_id);
+	if (!old_sa) {
+		xx_pr_err(("Invalid SA handle provided\n"));
+		return -EINVAL;
+	}
+
+	sa_mng = &dpa_ipsec->sa_mng;
+	/* Get an id for new SA */
+	if (cq_get_4bytes(sa_mng->sa_id_cq, &id) < 0) {
+		xx_pr_err(("No more unused SA's handles\n"));
+		return -ENOMEM;
+	}
+
+	/* Aquire an preallocated SA structure  */
+	new_sa = &sa_mng->sa[id];
+	/* Update the new SA structure */
+	new_sa->dpa_ipsec = old_sa->dpa_ipsec;
+	new_sa->id = id;
+	new_sa->inbound_flowid = old_sa->inbound_flowid;
+	new_sa->used_sa_index = -1;
+	for (i = 0; i < new_sa->dpa_ipsec->sa_mng.max_num_sa; i++)
+		if (new_sa->dpa_ipsec->used_sa_ids[i] == -1)
+			break;
+	if (i == new_sa->dpa_ipsec->sa_mng.max_num_sa) {
+		xx_pr_err(("No more unused SAs"));
+		return -ERANGE;
+	}
+	new_sa->used_sa_index = i;
+	new_sa->dpa_ipsec->used_sa_ids[i] = new_sa->id;
+	new_sa->dpa_ipsec->num_used_sas++;
+
+	/* Copy SA params into the internal SA structure */
+	if (old_sa->sa_dir == DPA_IPSEC_OUTBOUND) {
+		err = copy_sa_params_to_out_sa(new_sa, sa_params);
+		new_sa->parent_sa = old_sa;
+	} else {  /* DPA_IPSEC_INBOUND */
+		err = copy_sa_params_to_in_sa(new_sa, sa_params, TRUE);
+		new_sa->parent_sa = old_sa;
+	}
+
+	if (err < 0) {
+		xx_pr_err(("Could not copy SA parameters into SA\n"));
+		goto rekey_sa_err;
+	}
+
+	/* Generate the split key from the normal auth key */
+	err = generate_split_key(&new_sa->auth_data);
+	if (err < 0)
+		goto rekey_sa_err;
+
+	/* Update the new SA with information from the old SA */
+	/* The from SEC frame queue of the old SA will be used by the new SA */
+	memcpy(new_sa->from_sec_fq, old_sa->from_sec_fq,
+	       sizeof(struct qman_fq));
+
+	/* Exact match table will be reused by the new SA. */
+	new_sa->em_inpol_td = old_sa->em_inpol_td;
+
+	/* Link the entries of the policy list to the policy list on the new SA.
+	 * The policy list from the old SA becomes void.
+	 */
+	list_splice_init(&old_sa->policy_headlist, &new_sa->policy_headlist);
+
+	/* Call internal function to create SEC queues according to SA
+	 * parameters */
+	err = create_sa_fq_pair(new_sa, TRUE, TRUE);
+	if (err < 0) {
+		xx_pr_err(("Could not create SEC frame queues\n"));
+		goto rekey_sa_err;
+	}
+
+	/* AV's note: Since we have reused the FROM SEC FQ it is not needed to
+	 * make another entry in the PCD node of the post encryption OH PORT. */
+	if (new_sa->sa_dir == DPA_IPSEC_OUTBOUND) {
+		/* Need to update the outbound policy if we have policies */
+		if (!list_empty(&new_sa->policy_headlist)) {
+			list_for_each_entry_safe(policy_entry, tmp_policy_entry,
+						 &new_sa->policy_headlist,
+						 node) {
+				err = update_outbound_policy(new_sa,
+							     policy_entry,
+							     MNG_OP_MODIFY);
+				if (err < 0) {
+					/* Keep both SAs. Both must be removed
+					 * using remove_sa */
+					*new_sa_id = id;
+					xx_pr_err(("Could't modify outbound policy"));
+					goto rekey_sa_err;
+				}
+			}
+		}
+		/* AV's note: Wait until all the frames from the old TO SEC FQ
+		 * are consumed and then schedule the new TO SEC FQ */
+		wait_until_fq_empty(old_sa->to_sec_fq, &timeout);
+
+		qman_schedule_fq(new_sa->to_sec_fq);
+
+		/* Free the old SA structure and all its resources */
+		err = remove_sa_sec_fq(old_sa->to_sec_fq);
+		if (err < 0) {
+			/* Mark SA as rekeyed */
+			/* AV's note: TODO Add this SA to the SA garbage
+			 * collector list because we can't remove it until
+			 * it's to SEC FQ is empty. */
+		} else {
+			/* Mark as free index in used sa ids vector of this
+			 * dpa ipsec instance */
+			dpa_ipsec->used_sa_ids[old_sa->used_sa_index] =
+			    DPA_IPSEC_INVALID_SA_ID;
+			dpa_ipsec->num_used_sas--;
+			cq_put_4bytes(sa_mng->sa_id_cq, old_sa->id);
+		}
+	} else {		/* DPA_IPSEC_INBOUND */
+		/* Need to update the IN SA PCD entry */
+		err = update_pre_sec_inbound_table(new_sa, MNG_OP_ADD);
+		if (err < 0) {
+			xx_pr_err(("Could not add PCD entry for new SA\n"));
+			goto rekey_sa_err;
+		}
+
+		/* Add new SA into the sa_rekeying_headlist */
+		list_add(&new_sa->sa_rekeying_node,
+			 &sa_mng->sa_rekeying_headlist);
+
+		/* schedule inbound SA's rekeying */
+		queue_delayed_work(sa_mng->sa_rekeying_wq,
+				   &sa_mng->sa_rekeying_work, 1);
+	}
+
+	*new_sa_id = new_sa->id;
+
+	return 0;
+
+	/* Something went wrong. Begin rollback */
+ rekey_sa_err:
+	if (*new_sa_id == DPA_IPSEC_INVALID_SA_ID) {
+		remove_sa_fq_pair(new_sa);
+		if (new_sa->used_sa_index != -1) {
+			new_sa->dpa_ipsec->used_sa_ids[new_sa->used_sa_index] =
+					DPA_IPSEC_INVALID_SA_ID;
+			new_sa->used_sa_index = -1;
+			new_sa->dpa_ipsec->num_used_sas--;
+		}
+		cq_put_4bytes(sa_mng->sa_id_cq, new_sa->id);
+	}
+
+	return err;
+}
+EXPORT_SYMBOL_GPL(dpa_ipsec_sa_rekeying);
+
+void sa_rekeying_work_func(struct work_struct *work)
+{
+	struct dpa_ipsec_sa_mng *sa_mng;
+	struct dpa_ipsec_sa *new_sa, *old_sa, *tmp_sa;
+	struct dpa_ipsec *dpa_ipsec;
+	struct list_head *head_list;
+	struct qm_mcr_queryfq_np queryfq_np;
+	int err;
+
+	sa_mng = container_of((struct delayed_work *)work,
+			      struct dpa_ipsec_sa_mng, sa_rekeying_work);
+	head_list = &sa_mng->sa_rekeying_headlist;
+
+	list_for_each_entry_safe(new_sa, tmp_sa, head_list, sa_rekeying_node) {
+		/* Verify if the new SA TO SEC FQ has something enqueued in it*/
+		qman_query_fq_np(new_sa->to_sec_fq, &queryfq_np);
+		if (queryfq_np.frm_cnt > 0) {
+			/* timeout in microseconds */
+			uint32_t timeout = 10000;
+
+			old_sa = new_sa->parent_sa;
+			BUG_ON(!old_sa);
+
+			dpa_ipsec = old_sa->dpa_ipsec;
+			if (old_sa->inbound_hash_entry != -1) {
+				err = update_pre_sec_inbound_table(old_sa,
+								 MNG_OP_REMOVE);
+				if (err < 0) {
+					xx_pr_err(("Could not remove PCD entry for SA\n"));
+					/*Remove the new SA from rekeying list*/
+					list_del(&new_sa->sa_rekeying_node);
+					err = dpa_ipsec_remove_sa(new_sa->id);
+					if (err < 0)
+						xx_pr_err(("Could not remove new sa\n"));
+					continue;
+				}
+			}
+
+			/* AV's note: Wait until all the frames from the old
+			 * TO SEC FQ are consumed and then schedule the new TO
+			 * SEC FQ */
+			wait_until_fq_empty(old_sa->to_sec_fq, &timeout);
+
+			/* If timeout has reached 0 it means that there are
+			 * still frames inside OLD SA TO SEC Frame Queue,
+			 * therefore we continue with other SA in progress of
+			 * rekeying and let the current one for a later time
+			 * when the work will be rescheduled. Do not remove
+			 * this new SA from rekeying list.
+			 * We can not receive on inbound old SA because the
+			 * old SA was removed from the PCD Entry which is
+			 * perfect */
+			if (timeout == 0)
+				continue;
+
+			/* schedule new inbound SA */
+			qman_schedule_fq(new_sa->to_sec_fq);
+
+			/* Free the old SA structure and all its resources */
+			err = remove_sa_sec_fq(old_sa->to_sec_fq);
+			if (err < 0) {
+				xx_pr_err(("Could not remove SA FQ pair during rekeying!\n"));
+				/* TODO: add this SA to garbage collector */
+				/*Remove the new SA from rekeying list*/
+				list_del(&new_sa->sa_rekeying_node);
+				continue;
+			}
+			/* Mark as free index in used sa ids vector of this
+			 * dpa ipsec instance */
+			dpa_ipsec->used_sa_ids[old_sa->used_sa_index] =
+				DPA_IPSEC_INVALID_SA_ID;
+			dpa_ipsec->num_used_sas--;
+			cq_put_4bytes(sa_mng->sa_id_cq, old_sa->id);
+
+			/*Remove the new SA from rekeying list*/
+			list_del(&new_sa->sa_rekeying_node);
+		} else {
+			continue;
+		}
+	}
+
+	if (!list_empty(head_list)) {
+		struct timeval timeval;
+		unsigned long jiffies_to_wait;
+
+		timeval.tv_sec = 0;
+		timeval.tv_usec = 100;
+		jiffies_to_wait = timeval_to_jiffies(&timeval);
+
+		queue_delayed_work(sa_mng->sa_rekeying_wq,
+				   &sa_mng->sa_rekeying_work,
+				   jiffies_to_wait);	/* 100 microseconds. */
+	}
+
+	return;
+}
+
+int dpa_ipsec_disable_sa(int sa_id)
+{
+	xx_pr_err(("This function is currently not supported\n"));
+	return -EINVAL;
+}
+EXPORT_SYMBOL_GPL(dpa_ipsec_disable_sa);
+
+int dpa_ipsec_flush_all_sa(int dpa_ipsec_id)
+{
+	struct dpa_ipsec *dpa_ipsec;
+	uint32_t i, sa_id;
+	int err;
+
+	/* multiple DPA IPSec instances are not currently supported */
+	unused(dpa_ipsec_id);
+
+	if (!gbl_dpa_ipsec) {
+		xx_pr_err(("There is no dpa_ipsec component initialized\n"));
+		return -EPERM;
+	}
+	dpa_ipsec = gbl_dpa_ipsec;
+
+	for (i = 0; i < dpa_ipsec->sa_mng.max_num_sa; i++) {
+		sa_id = dpa_ipsec->used_sa_ids[i];
+		if (sa_id != DPA_IPSEC_INVALID_SA_ID) {
+			err = dpa_ipsec_remove_sa(sa_id);
+			if (err < 0) {
+				xx_pr_err(("Could not remove SA %d\n", sa_id));
+				return err;
+			}
+		}
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(dpa_ipsec_flush_all_sa);
+
+int dpa_ipsec_sa_get_policies(int sa_id,
+			      struct dpa_ipsec_policy_params *policy_params,
+			      int *num_pol)
+{
+	struct dpa_ipsec_sa *sa;
+	int err;
+
+	if (sa_id < 0) {
+		xx_pr_err(("Invalid SA id"));
+		return -EINVAL;
+	}
+
+	sa = get_sa_from_sa_id(sa_id);
+	if (!sa) {
+		pr_err("Invalid SA handle\n");
+		return -EINVAL;
+	}
+
+	if (!policy_params) {
+		if (!num_pol) {
+			xx_pr_err(("Invalid input parameters"));
+			return -EINVAL;
+		} else {
+			/* get the number of policies for SA with id sa_id */
+			*num_pol = get_policy_count_for_sa(sa);
+			return 0;
+		}
+	}
+
+	err = copy_all_policies(sa, policy_params, *num_pol);
+	if (err < 0)
+		return err;
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(dpa_ipsec_sa_get_policies);
+
+int dpa_ipsec_sa_flush_policies(int sa_id)
+{
+	xx_pr_err(("This function is currently not supported\n"));
+	return -EINVAL;
+}
+EXPORT_SYMBOL_GPL(dpa_ipsec_sa_flush_policies);
diff --git a/drivers/staging/fsl_dpa_offload/dpa_ipsec.h b/drivers/staging/fsl_dpa_offload/dpa_ipsec.h
new file mode 100644
index 0000000..ad9aa37
--- /dev/null
+++ b/drivers/staging/fsl_dpa_offload/dpa_ipsec.h
@@ -0,0 +1,321 @@
+/* Copyright 2008-2012 Freescale Semiconductor, Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef __DPA_IPSEC_H__
+#define __DPA_IPSEC_H__
+
+#include <linux/fsl_qman.h>
+#include <linux/fsl_dpa_ipsec.h>
+
+/* From Linux for Shared Descriptor auxiliary structures */
+#include <linux/bitops.h>
+#include <linux/compiler.h>
+#include <linux/workqueue.h>
+
+#include "desc.h"
+
+#include "dpa_compat.h"
+
+#include "fm_pcd_ext.h"
+#include "cq.h"
+
+
+/* MACRO declaration */
+#define OP_PCL_IPSEC_INVALID_ALG_ID	0xFFFF
+
+#define IPSEC_ALGS_ENTRY(enc, auth)	{		\
+		.enc_alg = OP_PCL_IPSEC_ ## enc,	\
+		.auth_alg = OP_PCL_IPSEC_ ## auth	\
+	}
+
+#define IPSEC_ALGS	{					\
+	/* DPA_IPSEC_CIPHER_ALG_3DES_CBC_HMAC_96_MD5_128 */	\
+	IPSEC_ALGS_ENTRY(3DES, HMAC_MD5_96),			\
+	/* DPA_IPSEC_CIPHER_ALG_3DES_CBC_HMAC_96_SHA_160 */	\
+	IPSEC_ALGS_ENTRY(3DES, HMAC_SHA1_96),			\
+	/* DPA_IPSEC_CIPHER_ALG_3DES_CBC_HMAC_MD5_128 */	\
+	IPSEC_ALGS_ENTRY(3DES, HMAC_MD5_128),			\
+	/* DPA_IPSEC_CIPHER_ALG_3DES_CBC_HMAC_SHA_160 */	\
+	IPSEC_ALGS_ENTRY(3DES, HMAC_SHA1_160),			\
+	/* DPA_IPSEC_CIPHER_ALG_3DES_CBC_HMAC_SHA_256_128 */	\
+	IPSEC_ALGS_ENTRY(3DES, HMAC_SHA2_256_128),		\
+	/* DPA_IPSEC_CIPHER_ALG_3DES_CBC_HMAC_SHA_384_192 */	\
+	IPSEC_ALGS_ENTRY(3DES, HMAC_SHA2_384_192),		\
+	/* DPA_IPSEC_CIPHER_ALG_3DES_CBC_HMAC_SHA_512_256 */	\
+	IPSEC_ALGS_ENTRY(3DES, HMAC_SHA2_512_256),		\
+	/* DPA_IPSEC_CIPHER_ALG_NULL_ENC_HMAC_96_MD5_128 */	\
+	IPSEC_ALGS_ENTRY(INVALID_ALG_ID, HMAC_MD5_96),		\
+	/* DPA_IPSEC_CIPHER_ALG_NULL_ENC_HMAC_96_SHA_160 */	\
+	IPSEC_ALGS_ENTRY(INVALID_ALG_ID, HMAC_SHA1_96),		\
+	/* DPA_IPSEC_CIPHER_ALG_NULL_ENC_AES_XCBC_MAC_96 */	\
+	IPSEC_ALGS_ENTRY(INVALID_ALG_ID, AES_XCBC_MAC_96),	\
+	/* DPA_IPSEC_CIPHER_ALG_NULL_ENC_HMAC_MD5_128 */	\
+	IPSEC_ALGS_ENTRY(INVALID_ALG_ID, HMAC_MD5_128),		\
+	/* DPA_IPSEC_CIPHER_ALG_NULL_ENC_HMAC_SHA_160 */	\
+	IPSEC_ALGS_ENTRY(INVALID_ALG_ID, HMAC_SHA1_160),	\
+	/* DPA_IPSEC_CIPHER_ALG_NULL_ENC_HMAC_SHA_256_128 */	\
+	IPSEC_ALGS_ENTRY(INVALID_ALG_ID, HMAC_SHA2_256_128),	\
+	/* DPA_IPSEC_CIPHER_ALG_NULL_ENC_HMAC_SHA_384_192 */	\
+	IPSEC_ALGS_ENTRY(INVALID_ALG_ID, HMAC_SHA2_384_192),	\
+	/* DPA_IPSEC_CIPHER_ALG_NULL_ENC_HMAC_SHA_512_256 */	\
+	IPSEC_ALGS_ENTRY(INVALID_ALG_ID, HMAC_SHA2_512_256),	\
+	/* DPA_IPSEC_CIPHER_ALG_AES_CBC_HMAC_96_MD5_128 */	\
+	IPSEC_ALGS_ENTRY(AES_CBC, HMAC_MD5_96),			\
+	/* DPA_IPSEC_CIPHER_ALG_AES_CBC_HMAC_96_SHA_160 */	\
+	IPSEC_ALGS_ENTRY(AES_CBC, HMAC_SHA1_96),		\
+	/* DPA_IPSEC_CIPHER_ALG_AES_CBC_AES_XCBC_MAC_96 */	\
+	IPSEC_ALGS_ENTRY(AES_CBC, AES_XCBC_MAC_96),		\
+	/* DPA_IPSEC_CIPHER_ALG_AES_CBC_HMAC_MD5_128 */		\
+	IPSEC_ALGS_ENTRY(AES_CBC, HMAC_MD5_128),		\
+	/* DPA_IPSEC_CIPHER_ALG_AES_CBC_HMAC_SHA_160 */		\
+	IPSEC_ALGS_ENTRY(AES_CBC, HMAC_SHA1_160),		\
+	/* DPA_IPSEC_CIPHER_ALG_AES_CBC_HMAC_SHA_256_128 */	\
+	IPSEC_ALGS_ENTRY(AES_CBC, HMAC_SHA2_256_128),		\
+	/* DPA_IPSEC_CIPHER_ALG_AES_CBC_HMAC_SHA_384_192 */	\
+	IPSEC_ALGS_ENTRY(AES_CBC, HMAC_SHA2_384_192),		\
+	/* DPA_IPSEC_CIPHER_ALG_AES_CBC_HMAC_SHA_512_256 */	\
+	IPSEC_ALGS_ENTRY(AES_CBC, HMAC_SHA2_512_256),		\
+	/* DPA_IPSEC_CIPHER_ALG_AES_CTR_HMAC_96_MD5_128 */	\
+	IPSEC_ALGS_ENTRY(AES_CTR, HMAC_MD5_96),			\
+	/* DPA_IPSEC_CIPHER_ALG_AES_CTR_HMAC_96_SHA_160 */	\
+	IPSEC_ALGS_ENTRY(AES_CTR, HMAC_SHA1_96),		\
+	/* DPA_IPSEC_CIPHER_ALG_AES_CTR_AES_XCBC_MAC_96 */	\
+	IPSEC_ALGS_ENTRY(AES_CTR, AES_XCBC_MAC_96),		\
+	/* DPA_IPSEC_CIPHER_ALG_AES_CTR_HMAC_MD5_128 */		\
+	IPSEC_ALGS_ENTRY(AES_CTR, HMAC_MD5_128),		\
+	/* DPA_IPSEC_CIPHER_ALG_AES_CTR_HMAC_SHA_160 */		\
+	IPSEC_ALGS_ENTRY(AES_CTR, HMAC_SHA1_160),		\
+	/* DPA_IPSEC_CIPHER_ALG_AES_CTR_HMAC_SHA_256_128 */	\
+	IPSEC_ALGS_ENTRY(AES_CTR, HMAC_SHA2_256_128),		\
+	/* DPA_IPSEC_CIPHER_ALG_AES_CTR_HMAC_SHA_384_192 */	\
+	IPSEC_ALGS_ENTRY(AES_CTR, HMAC_SHA2_384_192),		\
+	/* DPA_IPSEC_CIPHER_ALG_AES_CTR_HMAC_SHA_512_256 */	\
+	IPSEC_ALGS_ENTRY(AES_CTR, HMAC_SHA2_512_256)		\
+}
+
+#define GET_POL_CC_NODE_INDX(_proto) \
+	((_proto == IPPROTO_TCP)  ? DPA_IPSEC_PROTO_TCP :  \
+	 (_proto == IPPROTO_UDP)  ? DPA_IPSEC_PROTO_UDP :  \
+	 (_proto == IPPROTO_ICMP) ? DPA_IPSEC_PROTO_ICMP : \
+	 (_proto == IPPROTO_SCTP) ? DPA_IPSEC_PROTO_SCTP : \
+	  DPA_IPSEC_PROTO_ANY)
+
+#define IP_ADDR(_ipAddr) \
+	((_ipAddr.addr_type == DPA_IPSEC_ADDR_T_IPv4) ? \
+	(_ipAddr.ipv4.byte) : (_ipAddr.ipv6.byte))
+#define IP_ADDR_LEN(_ipAddr) \
+	((_ipAddr.addr_type == DPA_IPSEC_ADDR_T_IPv4) ? \
+	(IPv4_ADDR_SIZE_IN_BYTES) : (IPv6_ADDR_SIZE_IN_BYTES))
+
+#define SET_IP_PROTO_IN_KEY(_key, _off, _val) (_key[_off] = _val)
+#define SET_IP_PROTO_MASK(_mask, _off, _val) (_mask[_off] = _val)
+#define SET_L4_PORT_MASK(_mask, _off, _val) \
+		(*(uint16_t *) &(_mask[_off]) = _val)
+
+#define TABLE_KEY_SIZE(_tbl_params) \
+	((_tbl_params.type == DPA_CLS_TBL_HASH) ? \
+		tbl_params.hash_params.key_size : \
+		(_tbl_params.type == DPA_CLS_TBL_EXACT_MATCH) ?	\
+			 tbl_params.exact_match_params.key_size : 0)
+
+#define SEQ_NUM_HI_MASK		0xFFFFFFFF00000000
+#define SEQ_NUM_LOW_MASK	0x00000000FFFFFFFF
+
+#define MAX_NUM_OF_SA       2000
+#define MAX_CIPHER_KEY_LEN  100
+#define MAX_AUTH_KEY_LEN    256
+#define MAX_BUFFER_POOL_ID  63
+
+#define UDP_HEADER_LEN		8
+#define NEXT_HEADER_IS_IPv4	0x04
+
+/* DPA IPSec Encryption & authentication algorithm identifiers */
+struct ipsec_alg_suite {
+	uint16_t	enc_alg;
+	uint16_t	auth_alg;
+};
+
+/* DPA IPsec PCD management operation types */
+enum mng_op_type {
+	MNG_OP_ADD = 0,
+	MNG_OP_REMOVE,
+	MNG_OP_MODIFY
+};
+
+/* DPA IPsec Cipher Parameters */
+struct cipher_params {
+	uint16_t cipher_type;	 /* Algorithm type as defined by SEC driver   */
+	uint8_t *cipher_key;	 /* Address to the encryption key	      */
+	uint32_t cipher_key_len; /* Length in bytes of the normal key         */
+};
+
+/* DPA IPsec Authentication Parameters */
+struct auth_params {
+	uint16_t auth_type;	/* Algorithm type as defined by SEC driver    */
+	uint8_t *auth_key;	/* Address to the normal key		      */
+	uint32_t auth_key_len;	/* Length in bytes of the normal key          */
+	uint8_t *split_key;	/* Address to the generated split key         */
+	uint32_t split_key_len;	/* Length in bytes of the split key           */
+	uint32_t split_key_pad_len;/* Length in bytes of the padded split key */
+};
+
+/* DPA IPsec IP Header */
+struct ip_header {
+	uint8_t version:4, ihl:4;
+	uint8_t tos;
+	uint16_t tot_len;
+	uint16_t id;
+	uint16_t frag_off;
+	uint8_t ttl;
+	uint8_t protocol;
+	uint16_t check;
+	uint32_t saddr;
+	uint32_t daddr;
+};
+
+/*
+ * DPA IPsec Security Association
+ * This structure will represent a SA. All SA structures will be allocated
+ * in the initialization part for performance reasons.
+ */
+struct dpa_ipsec_sa {
+	struct dpa_ipsec *dpa_ipsec;	    /* Pointer to DPA_IPSEC           */
+	enum dpa_ipsec_direction sa_dir;    /* SA direction		      */
+	uint32_t id;			    /* Used to index in circular queue*/
+	struct cipher_params cipher_data;   /* Encryption parameters	      */
+	struct auth_params auth_data;	    /* Authentication key parameters  */
+	struct sec_descriptor *sec_desc;    /* Computed SEC 4.x descriptor    */
+			  /*Allocated at initialization time.
+			   * Holds valid information after creating the
+			   * shared descriptor according to the SA information*/
+	struct qman_fq *to_sec_fq; /*From this Frame Queue SEC consumes frames*/
+	struct qman_fq *from_sec_fq; /*In this Frame Queue SEC will enqueue the
+				encryption/decryption result (FD).            */
+	uint16_t sa_wqid; /* Work queue id in which the TO SEC FQ will be put */
+	uint8_t sa_bpid;  /* Buffer pool id used by SEC for acquiring buffers,
+			     comes from user. Default buffer pool 63	      */
+	uint32_t spi;	/* IPsec Security parameter index		      */
+	struct dpa_ipsec_ip_address src_addr;  /* Source IP address           */
+	struct dpa_ipsec_ip_address dest_addr;	/* Destination IP address     */
+	uint32_t out_flow_id; /* Value used to classify frames encrypted
+				 with this SA				      */
+	bool use_udp_encap;   /* NAT-T is activated for this SA.
+				Only for inbound  SAs			      */
+	uint16_t udp_src_port;	/* Source UDP port (for UDP encapsulated ESP)
+				   Only for inbound  SAs.		      */
+	uint16_t udp_dest_port;	/* Destination UDP port (for UDP encap ESP)
+				   Only for inbound  SAs.                     */
+	uint16_t inbound_flowid; /* Value used for identifying a inbound SA.  */
+	int inbound_indx_entry;	/* Entry in the index table correspondent
+				   with inbound_flowid			      */
+	int inbound_hash_entry;	/* Entry in the hash table
+				   corresponding to SPI extended key	      */
+	struct dpa_cls_tbl_action def_sa_action;
+	struct list_head policy_headlist; /* Head of the policy param list
+			 used to store all the in/out policy parameters in order
+			 to know how to remove the corresponding PCD entries  */
+	uint32_t policy_miss_fqid; /* FQID where frames that fail inbound policy
+				      verification should be enqueued	      */
+	int em_inpol_td; /* Exact match table descriptor for inbound policy
+			    check					      */
+	struct dpa_ipsec_sa *parent_sa;	/* Address of the parent SA or NULL
+			if this is an SA created with DPA_IPSEC_CreateSA.
+			Used only when rekeying an inbound SA		      */
+	struct list_head sa_rekeying_node; /* Only used for inbound rekeying  */
+	int used_sa_index; /* Index in the used_sa_ids vector of the dpa ipsec
+			      instance this SA is part of.		      */
+};
+
+/* Parameters for inbound policy verification tables */
+struct inpol_tbl {
+	void *cc_node; /* Cc node handle on top of which the table is created */
+	int td;	 /* Exact match table used for inbound policy verification    */
+	bool used;
+	struct list_head table_list;
+};
+
+/* DPA IPSEC - Security Associations Management */
+struct dpa_ipsec_sa_mng {
+	struct dpa_ipsec_sa *sa; /* Array of SAs. Use indexes from sa_id_cq   */
+	struct cq *sa_id_cq;	/* Circular Queue with id's for SAs           */
+	uint32_t max_num_sa;	/* Maximum number of SAs                      */
+	struct cq *inbound_flowid_cq; /* Circular queue with flow IDs for
+					 identifing an inbound SA             */
+	uint8_t inpol_key_size; /* Inbound policy verification tables
+					key size.*/
+	struct list_head inpol_tables;	/* Head list of tables used for inbound
+		policy verification. List of inpol_tbl structures.
+		Populated only if inbound policy verification is enabled      */
+	struct delayed_work sa_rekeying_work;
+	struct workqueue_struct *sa_rekeying_wq; /* Single threaded work
+				queue used to defer the work to be done in the
+				inbound rekeying process. */
+	struct list_head sa_rekeying_headlist;	/* Head list with inbound SA's
+						   currently in the rekeying
+						   process                    */
+};
+
+/* DPA IPSEC - Control Block */
+struct dpa_ipsec {
+	struct dpa_ipsec_params config;	/* Configuration parameters as
+					provided in dap_ipsec_config_and_init */
+	struct dpa_ipsec_sa_mng sa_mng;	/* Internal DPA IPsec SA manager      */
+	uint32_t *used_sa_ids;	/* Sa ids used by this dpa ipsec instance     */
+	int num_used_sas;  /* The current number of sa's used by this instance*/
+};
+
+/* DPA IPSEC - Security Policy Selectors */
+struct dpa_ipsec_policy_selectors {
+	struct dpa_ipsec_ip_address src_addr;	/* Source IP address          */
+	uint8_t src_prefix_len;	/* Source network prefix		      */
+	struct dpa_ipsec_ip_address dest_addr;	/* Destination IP address     */
+	uint8_t dest_prefix_len;	/* Destination network prefix	      */
+	uint8_t protocol;	/* Protocol				      */
+	uint16_t dest_port;	/* Destination port			      */
+	uint16_t dest_port_mask;	/* Destination port mask	      */
+	uint16_t src_port;	/* Source port				      */
+	uint16_t src_port_mask;	/* Source port mask			      */
+};
+
+/* DPA IPSEC - Security Policy Parameter Entry */
+struct dpa_ipsec_policy_entry {
+	struct dpa_ipsec_policy_selectors policy_selectors;/* Policy selectors*/
+	uint16_t mtu;	/* Maximum size of packets matching policy selectors  */
+	enum dpa_ipsec_df_action df_action;	/* Action when DF bit is set  */
+	int priority;		/* Policy priority                            */
+	int entry_id;		/* Set by dpa_classif_table_insert_entry      */
+	struct dpa_cls_tbl_header_manip *hm;	/* Header manipulation        */
+	struct list_head node;	/* Node in linked list			      */
+};
+
+void sa_rekeying_work_func(struct work_struct *work);
+
+#endif	/* __DPA_IPSEC_H__ */
diff --git a/drivers/staging/fsl_dpa_offload/dpa_ipsec_desc.c b/drivers/staging/fsl_dpa_offload/dpa_ipsec_desc.c
new file mode 100644
index 0000000..c46fc37
--- /dev/null
+++ b/drivers/staging/fsl_dpa_offload/dpa_ipsec_desc.c
@@ -0,0 +1,311 @@
+/* Copyright 2008-2012 Freescale Semiconductor, Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <linux/version.h>
+#include <linux/platform_device.h>
+
+#include "compat.h"
+#include "desc.h"
+#include "error.h"
+#include "jr.h"
+
+#include "dpa_ipsec_desc.h"
+
+static const struct of_device_id sec_match[] = {
+	{
+	 .compatible = "fsl,sec-v4.0"
+	}
+};
+
+static const struct of_device_id sec_jr_match[] = {
+	{
+	 .compatible = "fsl,sec-v4.0-job-ring"
+	}
+};
+
+static struct device *get_jrdev(void)
+{
+	struct device_node *sec_jr_node;
+	struct platform_device *sec_of_jr_dev;
+	struct device *sec_jr_dev;
+
+	sec_jr_node = of_find_matching_node(NULL, &sec_jr_match[0]);
+	if (sec_jr_node == NULL) {
+		xx_pr_err(("Couln't find the device_node SEC job-ring, check the device tree\n"));
+		return NULL;
+	}
+
+	sec_of_jr_dev = of_find_device_by_node(sec_jr_node);
+	if (sec_of_jr_dev == NULL) {
+		xx_pr_err(("SEC job-ring of_device null\n"));
+		return NULL;
+	}
+
+	sec_jr_dev = &sec_of_jr_dev->dev;
+	if (sec_jr_dev == NULL) {
+		xx_pr_err(("SEC Job Ring Device is null\n"));
+		return NULL;
+	}
+
+	sec_jr_node = of_find_matching_node(sec_jr_node, &sec_jr_match[0]);
+	if (sec_jr_node == NULL) {
+		xx_pr_err(("Couln't find the device_node SEC job-ring, check the device tree\n"));
+		return NULL;
+	}
+
+	sec_of_jr_dev = of_find_device_by_node(sec_jr_node);
+	if (sec_of_jr_dev == NULL) {
+		xx_pr_err((KERN_ERR "SEC job-ring of_device null\n"));
+		return NULL;
+	}
+
+	sec_jr_dev = &sec_of_jr_dev->dev;
+	if (sec_jr_dev == NULL) {
+		xx_pr_err((KERN_ERR "SEC Job Ring Device is null\n"));
+		return NULL;
+	}
+
+	return sec_jr_dev;
+}
+
+static inline u32 get_ipsec_op_type(enum dpa_ipsec_direction sa_dir)
+{
+	return sa_dir == DPA_IPSEC_INBOUND ?  OP_TYPE_DECAP_PROTOCOL :
+					      OP_TYPE_ENCAP_PROTOCOL;
+}
+
+int build_shared_descriptor(struct dpa_ipsec_sa *sa,
+			    dma_addr_t auth_key_dma,
+			    dma_addr_t crypto_key_dma, u8 bytes_to_copy)
+{
+	u32 *desc, *key_jump_cmd;
+	int opthdrsz;
+
+	desc = (u32 *) sa->sec_desc->desc;
+	if (sa->sa_dir == DPA_IPSEC_OUTBOUND) {
+		/* Compute optional header size, rounded up to descriptor
+		 * word size */
+		opthdrsz = (sa->sec_desc->pdb_en.ip_hdr_len + 3) & ~3;
+		init_sh_desc_pdb(desc, HDR_SAVECTX | HDR_SHARE_SERIAL,
+				 sizeof(struct ipsec_encap_pdb) + opthdrsz);
+	} else {
+		/* Since ipsec_decap_pdb has cbc which stores ESN
+		*into its second field and there is also seq_num_ext_hi we
+		*must subtract 1 word for ESN */
+		init_sh_desc_pdb(desc, HDR_SAVECTX | HDR_SHARE_SERIAL,
+				 sizeof(struct ipsec_decap_pdb) - sizeof(u32));
+	}
+
+	/* Key jump */
+	key_jump_cmd = append_jump(desc, CLASS_BOTH | JUMP_TEST_ALL |
+				   JUMP_COND_SHRD | JUMP_COND_SELF);
+
+	/* Append split authentication key */
+	append_key(desc, auth_key_dma, sa->auth_data.split_key_len,
+		   CLASS_2 | KEY_ENC | KEY_DEST_MDHA_SPLIT);
+
+	/* Append cipher key */
+	append_key(desc, crypto_key_dma, sa->cipher_data.cipher_key_len,
+		   CLASS_1 | KEY_DEST_CLASS_REG);
+
+	set_jump_tgt_here(desc, key_jump_cmd);
+
+	/*
+	 * actual move
+	 * ld: deco-deco-ctrl len=0 offs=8 imm -auto-nfifo-entries
+	 */
+	append_cmd(desc, CMD_LOAD | DISABLE_AUTO_INFO_FIFO);
+
+	/* seqfifold: both msgdata-last2-last1-flush1 len=4 */
+	append_seq_fifo_load(desc, bytes_to_copy, FIFOLD_TYPE_MSG |
+			     FIFOLD_CLASS_BOTH | FIFOLD_TYPE_LAST1 |
+			     FIFOLD_TYPE_LAST2 | FIFOLD_TYPE_FLUSH1);
+
+	/* ld: deco-deco-ctrl len=0 offs=4 imm +auto-nfifo-entries */
+	append_cmd(desc, CMD_LOAD | ENABLE_AUTO_INFO_FIFO);
+
+	/* move: ififo->deco-alnblk -> ofifo, len=4 */
+	append_move(desc, MOVE_SRC_INFIFO | MOVE_DEST_OUTFIFO | bytes_to_copy);
+
+	/* seqfifostr: msgdata len=4 */
+	append_seq_fifo_store(desc, FIFOST_TYPE_MESSAGE_DATA, bytes_to_copy);
+
+	append_operation(desc, OP_PCLID_IPSEC |
+			 get_ipsec_op_type(sa->sa_dir) |
+			 sa->cipher_data.cipher_type | sa->auth_data.auth_type);
+	return 0;
+}
+
+int create_sec_descriptor(struct dpa_ipsec_sa *sa)
+{
+	struct sec_descriptor *sec_desc;
+	struct device *jrdev;
+	dma_addr_t auth_key_dma;
+	dma_addr_t crypto_key_dma;
+
+	/* get the jr device  */
+	jrdev = get_jrdev();
+	if (!jrdev) {
+		xx_pr_err(("Failed to get the job ring devic, check the dts\n"));
+		return -EINVAL;
+	}
+
+	auth_key_dma = dma_map_single(jrdev, sa->auth_data.split_key,
+				      sa->auth_data.split_key_pad_len,
+				      DMA_TO_DEVICE);
+	if (!auth_key_dma) {
+		xx_pr_err(("Could not DMA map authentication key\n"));
+		return -EINVAL;
+	}
+
+	crypto_key_dma = dma_map_single(jrdev, sa->cipher_data.cipher_key,
+					sa->cipher_data.cipher_key_len,
+					DMA_TO_DEVICE);
+	if (!crypto_key_dma) {
+		xx_pr_err(("Could not DMA map cipher key\n"));
+		return -EINVAL;
+	}
+
+	/* build the shared descriptor */
+	build_shared_descriptor(sa, auth_key_dma, crypto_key_dma, ETH_HLEN);
+
+	sec_desc = sa->sec_desc;
+	/* setup preheader */
+	sec_desc->preheader.hi.field.idlen = desc_len((u32 *) sec_desc->desc);
+	sec_desc->preheader.lo.field.pool_id = sa->sa_bpid;
+	sec_desc->preheader.lo.field.pool_buffer_size = 0;
+	/* offset output by one burst size (64 bytes) */
+	sec_desc->preheader.lo.field.offset = 1;
+
+	dma_unmap_single(jrdev, auth_key_dma,
+			 sa->auth_data.split_key_pad_len, DMA_TO_DEVICE);
+	dma_unmap_single(jrdev, crypto_key_dma,
+			 sa->cipher_data.cipher_key_len, DMA_TO_DEVICE);
+	return 0;
+}
+
+static void split_key_done(struct device *dev, u32 * desc, u32 err,
+			   void *context)
+{
+	register atomic_t *done = context;
+
+	if (err) {
+		char tmp[256];
+		dev_err(dev, "%s\n", caam_jr_strstatus(tmp, err));
+	}
+	atomic_set(done, 1);
+}
+
+/******************************************************************************/
+int generate_split_key(struct auth_params *auth_param)
+{
+	struct device *jrdev;
+	dma_addr_t dma_addr_in, dma_addr_out;
+	u32 *desc, timeout = 1000000;
+	atomic_t done;
+	int ret = 0;
+	/* Sizes for MDHA pads (*not* keys): MD5, SHA1, 224, 256, 384, 512 */
+	static const u8 mdpadlen[] = { 16, 20, 32, 32, 64, 64 };
+
+	jrdev = get_jrdev();
+	if (!jrdev) {
+		xx_pr_err(("Could not get job ring device, please check dts\n"));
+		return -ENODEV;
+	}
+
+	desc = kmalloc(CAAM_CMD_SZ * 6 + CAAM_PTR_SZ * 2, GFP_KERNEL | GFP_DMA);
+	if (!desc) {
+		xx_pr_err(("Allocate memory failed for split key desc\n"));
+		return -ENOMEM;
+	}
+
+	auth_param->split_key_len =
+	    mdpadlen[((OP_ALG_ALGSEL_SHA1 | OP_ALG_AAI_HMAC) &
+		      OP_ALG_ALGSEL_SUBMASK) >> OP_ALG_ALGSEL_SHIFT] * 2;
+	auth_param->split_key_pad_len = ALIGN(auth_param->split_key_len, 16);
+
+	dma_addr_in = dma_map_single(jrdev, auth_param->auth_key,
+				     auth_param->auth_key_len, DMA_TO_DEVICE);
+	if (dma_mapping_error(jrdev, dma_addr_in)) {
+		dev_err(jrdev, "Unable to DMA map the input key address\n");
+		kfree(desc);
+		return -ENOMEM;
+	}
+
+	dma_addr_out = dma_map_single(jrdev, auth_param->split_key,
+				      auth_param->split_key_pad_len,
+				      DMA_FROM_DEVICE);
+	if (dma_mapping_error(jrdev, dma_addr_out)) {
+		dev_err(jrdev, "Unable to DMA map the output key address\n");
+		dma_unmap_single(jrdev, dma_addr_in, auth_param->auth_key_len,
+				 DMA_TO_DEVICE);
+		kfree(desc);
+		return -ENOMEM;
+	}
+
+	init_job_desc(desc, 0);
+
+	append_key(desc, dma_addr_in, auth_param->auth_key_len,
+		   CLASS_2 | KEY_DEST_CLASS_REG);
+
+	/* Sets MDHA up into an HMAC-INIT */
+	append_operation(desc, (OP_ALG_TYPE_CLASS2 << OP_ALG_TYPE_SHIFT) |
+			 OP_ALG_ALGSEL_SHA1 | OP_ALG_AAI_HMAC |
+			 OP_ALG_DECRYPT | OP_ALG_AS_INIT);
+
+	/* Do a FIFO_LOAD of zero, this will trigger the internal key expansion
+	   into both pads inside MDHA */
+	append_fifo_load_as_imm(desc, NULL, 0, LDST_CLASS_2_CCB |
+				FIFOLD_TYPE_MSG | FIFOLD_TYPE_LAST2);
+
+	/* FIFO_STORE with the explicit split-key content store
+	 * (0x26 output type) */
+	append_fifo_store(desc, dma_addr_out, auth_param->split_key_len,
+			  LDST_CLASS_2_CCB | FIFOST_TYPE_SPLIT_KEK);
+
+	atomic_set(&done, 0);
+	ret = caam_jr_enqueue(jrdev, desc, split_key_done, &done);
+
+	while (!atomic_read(&done) && --timeout) {
+		udelay(1);
+		cpu_relax();
+	}
+
+	if (timeout == 0)
+		xx_pr_err(("Timeout waiting for job ring to complete\n"));
+
+	dma_unmap_single(jrdev, dma_addr_out, auth_param->split_key_pad_len,
+			 DMA_FROM_DEVICE);
+	dma_unmap_single(jrdev, dma_addr_in, auth_param->auth_key_len,
+			 DMA_TO_DEVICE);
+	kfree(desc);
+	return ret;
+}
diff --git a/drivers/staging/fsl_dpa_offload/dpa_ipsec_desc.h b/drivers/staging/fsl_dpa_offload/dpa_ipsec_desc.h
new file mode 100644
index 0000000..ae90945
--- /dev/null
+++ b/drivers/staging/fsl_dpa_offload/dpa_ipsec_desc.h
@@ -0,0 +1,88 @@
+/* Copyright 2008-2012 Freescale Semiconductor, Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef _DPA_IPSEC_DESC_H_
+#define _DPA_IPSEC_DESC_H_
+
+#include "pdb.h"
+#include "dpa_ipsec.h"
+#include "desc_constr.h"
+
+/* preheader */
+struct preheader {
+	union {
+		uint32_t word;
+		struct {
+			uint16_t rsvd63_48;
+			unsigned int rsvd47_39:9;
+			unsigned int idlen:7;
+		} field;
+	} __packed hi;
+
+	union {
+		uint32_t word;
+		struct {
+			unsigned int rsvd31_30:2;
+			unsigned int fsgt:1;
+			unsigned int lng:1;
+			unsigned int offset:2;
+			unsigned int abs:1;
+			unsigned int add_buf:1;
+			uint8_t pool_id;
+			uint16_t pool_buffer_size;
+		} field;
+	} __packed lo;
+} __packed;
+
+struct desc_hdr {
+	uint32_t hdr_word;
+	union {
+		struct ipsec_encap_pdb pdb_en;
+		struct ipsec_decap_pdb pdb_dec;
+	};
+};
+
+struct sec_descriptor {
+	struct preheader preheader;	/* SEC preheader */
+	/* SEC Shared Descriptor */
+	union {
+		uint32_t desc[CAAM_DESC_BYTES_MAX];
+		struct desc_hdr desc_hdr;
+#define hdr_word	desc_hdr.hdr_word
+#define pdb_en		desc_hdr.pdb_en
+#define pdb_dec		desc_hdr.pdb_dec
+	};
+};
+
+int create_sec_descriptor(struct dpa_ipsec_sa *sa);
+int generate_split_key(struct auth_params *auth_param);
+
+#endif	/* _DPA_IPSEC_DESC_H_ */
diff --git a/include/linux/fsl_dpa_ipsec.h b/include/linux/fsl_dpa_ipsec.h
new file mode 100644
index 0000000..2fbbcae
--- /dev/null
+++ b/include/linux/fsl_dpa_ipsec.h
@@ -0,0 +1,429 @@
+/* Copyright 2008-2012 Freescale Semiconductor, Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/*
+ * DPA-IPSec Application Programming Interface.
+ */
+
+#ifndef __FSL_DPA_IPSEC_H
+#define __FSL_DPA_IPSEC_H
+
+#include "fsl_dpa_classifier.h"
+
+/* General DPA-IPSec defines */
+#define IPv4_ADDR_SIZE_IN_BYTES		4
+#define IPv6_ADDR_SIZE_IN_BYTES		16
+#define IPv6_ADDR_SIZE_IN_WORDS		4
+#define IPv6_ADDR_SIZE_IN_LONG		2
+
+#define IP_PROTO_FIELD_LEN		1
+#define ESP_SPI_FIELD_LEN		4
+#define PORT_FIELD_LEN			2
+
+#define MAX_SIZE_IP_UDP_SPI_KEY	\
+		(1 * IPv6_ADDR_SIZE_IN_BYTES + \
+		IP_PROTO_FIELD_LEN + \
+		2 * PORT_FIELD_LEN + \
+		ESP_SPI_FIELD_LEN)
+
+#define MAX_SIZE_IP_UDP_SPI_KEY_IPV4 \
+		(1 * IPv4_ADDR_SIZE_IN_BYTES + \
+		IP_PROTO_FIELD_LEN + \
+		2 * PORT_FIELD_LEN + \
+		ESP_SPI_FIELD_LEN)
+
+#define MAX_SIZE_POLICY_KEY \
+		(2 * IPv6_ADDR_SIZE_IN_BYTES + \
+		IP_PROTO_FIELD_LEN + \
+		2 * PORT_FIELD_LEN)
+
+#define MAX_SIZE_POLICY_KEY_IPV4 \
+		(2 * IPv4_ADDR_SIZE_IN_BYTES + \
+		IP_PROTO_FIELD_LEN + \
+		2 * PORT_FIELD_LEN)
+
+#define DPA_IPSEC_MAX_IN_POL_PER_SA  255  /* Maximum supported number of
+					   * inbound policies per SA	      */
+
+#define DPA_IPSEC_INVALID_SA_ID	-1
+
+#define DPA_IPSEC_HDR_COPY_DSCP		0x01 /* Copy DSCP bit from inner /
+					      * outer header to outer / inner
+					      * header                        */
+#define	DPA_IPSEC_HDR_COPY_DF		0x02 /* Copy DF bit from outer header
+					      * to outer / inner header       */
+#define DPA_IPSEC_HDR_DEC_TTL		0x04 /* Automatically decrment the TTL
+					      * value in the inner / outer hdr*/
+
+#define DPA_IPSEC_KEY_FIELD_SIP		0x01 /* Use source IP address in key  */
+#define DPA_IPSEC_KEY_FIELD_DIP		0x02 /* Use destination IP in key     */
+#define	DPA_IPSEC_KEY_FIELD_PROTO	0x04 /* Use IP protocol field in key  */
+#define DPA_IPSEC_KEY_FIELD_SPORT	0x08 /* Use source port in key        */
+#define DPA_IPSEC_KEY_FIELD_DPORT	0x10 /* Use destination port in key   */
+#define	DPA_IPSEC_MAX_KEY_FIELDS	5    /* Maximum key components        */
+
+#define DPA_IPSEC_DEF_PAD_VAL		0xAA /* Value to be used as padding in
+					      * classification keys           */
+
+/* DPA-IPSec Supported Protocols (for policy offloading) */
+enum dpa_ipsec_proto {
+	DPA_IPSEC_PROTO_TCP = 0,
+	DPA_IPSEC_PROTO_UDP,
+	DPA_IPSEC_PROTO_ICMP,
+	DPA_IPSEC_PROTO_SCTP,
+	DPA_IPSEC_PROTO_ANY,
+	DPA_IPSEC_MAX_SUPPORTED_PROTOS
+};
+
+/*
+ * DPA-IPSec Post SEC Data Offsets. 1 BURST = 32 or 64 bytes
+ * depending on SEC configuration. Default BURST size = 64 bytes
+ */
+enum dpa_ipsec_data_off {
+	DPA_IPSEC_DATA_OFF_NONE = 0,
+	DPA_IPSEC_DATA_OFF_1_BURST,
+	DPA_IPSEC_DATA_OFF_2_BURST,
+	DPA_IPSEC_DATA_OFF_3_BURST
+};
+
+/* DPA-IPSec Pre-Sec Inbound Parameters */
+struct dpa_ipsec_pre_sec_in_params {
+	int dpa_cls_td;		/* SA lookup table descriptor		      */
+};
+
+/* DPA-IPSec Pre-Sec Outbound Parameters */
+struct dpa_ipsec_pre_sec_out_params {
+	int dpa_cls_td[DPA_IPSEC_MAX_SUPPORTED_PROTOS];	/* Oubound policy
+							 * lookup table
+							 * descriptors	      */
+	uint8_t key_fields; /* Flags indicating policy key components.
+			     * (use DPA_IPSEC_KEY_FIELD* macros to configure) */
+};
+
+/* DPA-IPSec Post-Sec-Inbound Parameters */
+struct dpa_ipsec_post_sec_in_params {
+	enum dpa_ipsec_data_off data_off;/*Data offset in the decrypted buffer*/
+	uint16_t qm_tx_ch;   /* QMan channel of the post decryption OH port   */
+	int dpa_cls_td;	     /* Index table descriptor			      */
+	int do_pol_check;    /* Enable inbound policy verification	      */
+	uint8_t key_fields;  /* Flags indicating policy key components.
+			      * (use DPA_IPSEC_KEY_FIELD* macros to configure)
+			      *  Relevant only if do_pol_check = TRUE	      */
+	int use_ipv6_pol;    /* Activate support for IPv6 policies. Allows
+			      * better MURAM management. Relevant only if
+			      * do_pol_check = TRUE			      */
+};
+
+/* DPA-IPSec Post-Sec-Inbound Parameters */
+struct dpa_ipsec_post_sec_out_params {
+	enum dpa_ipsec_data_off data_off;/*Data offset in the decrypted buffer*/
+	uint16_t qm_tx_ch; /* QMan channel of the post encrytion OH port      */
+};
+
+/* IPsec parameters used to configure the DPA IPsec instance */
+struct dpa_ipsec_params {
+	struct dpa_ipsec_pre_sec_in_params pre_sec_in_params;
+	struct dpa_ipsec_post_sec_in_params post_sec_in_params;
+	struct dpa_ipsec_pre_sec_out_params pre_sec_out_params;
+	struct dpa_ipsec_post_sec_out_params post_sec_out_params;
+	void *fm_pcd;		/* Handle of the PCD object		      */
+	uint16_t qm_sec_ch;	/* QMan channel# for the SEC		      */
+	uint16_t max_sa_pairs;	/* Maximum number of SA pairs
+				 * (1 SA Pair = 1 In SA + 1 Out SA)	      */
+	struct qman_fqid_pool *fqid_pool; /* FQID pool to be used by DPA IPSec
+					   * for allocating FQIDs for internal
+					   * frame queues		      */
+	uint8_t ipf_bpid;	/* Scratch buffer pool for IP Frag.	      */
+};
+
+/* Initialize a DPA-IPSec instance. */
+int dpa_ipsec_init(const struct dpa_ipsec_params *params, int *dpa_ipsec_id);
+
+/* Free a DPA-IPSec instance */
+int dpa_ipsec_free(int dpa_ipsec_id);
+
+/* DPA-IPSec IP Address Type */
+enum dpa_ipsec_ip_addr_type {
+	DPA_IPSEC_ADDR_T_IPv4 = 0,	/* IPv4 addresses		      */
+	DPA_IPSEC_ADDR_T_IPv6		/* IPv6 addresses		      */
+};
+
+/* DPA-IPSec data flow source specification */
+enum dpa_ipsec_direction {
+	DPA_IPSEC_INBOUND = 0,	/* Inbound				      */
+	DPA_IPSEC_OUTBOUND	/* Outbound				      */
+};
+
+/* DPA-IPSec IPv4 Address Descriptor */
+struct dpa_ipsec_ipv4_address {
+	union {
+		uint32_t word;	/* Address as 32bit word		      */
+		uint8_t byte[IPv4_ADDR_SIZE_IN_BYTES];/* Address as byte array*/
+	};
+};
+
+/* DPA-IPSec IPv6 Address Descriptor */
+struct dpa_ipsec_ipv6_address {
+	union {
+		uint8_t byte[IPv6_ADDR_SIZE_IN_BYTES];/* Address as byte array*/
+		uint32_t word[IPv6_ADDR_SIZE_IN_WORDS];/*Address as word array*/
+		uint64_t lword[IPv6_ADDR_SIZE_IN_LONG];/* Address as long word
+							* array		      */
+	};
+};
+
+/* DPA IPSEC IP Address Descriptor */
+struct dpa_ipsec_ip_address {
+	enum dpa_ipsec_ip_addr_type addr_type;
+	union {
+		struct dpa_ipsec_ipv4_address ipv4;  /* IPv4 address format   */
+		struct dpa_ipsec_ipv6_address ipv6;  /* IPv6 address format   */
+	};
+};
+
+/* DPA-IPSec Supported Cipher Suites */
+enum dpa_ipsec_cipher_alg {
+	DPA_IPSEC_CIPHER_ALG_3DES_CBC_HMAC_96_MD5_128,
+	DPA_IPSEC_CIPHER_ALG_3DES_CBC_HMAC_96_SHA_160,
+	DPA_IPSEC_CIPHER_ALG_3DES_CBC_HMAC_MD5_128,
+	DPA_IPSEC_CIPHER_ALG_3DES_CBC_HMAC_SHA_160,
+	DPA_IPSEC_CIPHER_ALG_3DES_CBC_HMAC_SHA_256_128,
+	DPA_IPSEC_CIPHER_ALG_3DES_CBC_HMAC_SHA_384_192,
+	DPA_IPSEC_CIPHER_ALG_3DES_CBC_HMAC_SHA_512_256,
+	DPA_IPSEC_CIPHER_ALG_NULL_ENC_HMAC_96_MD5_128,
+	DPA_IPSEC_CIPHER_ALG_NULL_ENC_HMAC_96_SHA_160,
+	DPA_IPSEC_CIPHER_ALG_NULL_ENC_AES_XCBC_MAC_96,
+	DPA_IPSEC_CIPHER_ALG_NULL_ENC_HMAC_MD5_128,
+	DPA_IPSEC_CIPHER_ALG_NULL_ENC_HMAC_SHA_160,
+	DPA_IPSEC_CIPHER_ALG_NULL_ENC_HMAC_SHA_256_128,
+	DPA_IPSEC_CIPHER_ALG_NULL_ENC_HMAC_SHA_384_192,
+	DPA_IPSEC_CIPHER_ALG_NULL_ENC_HMAC_SHA_512_256,
+	DPA_IPSEC_CIPHER_ALG_AES_CBC_HMAC_96_MD5_128,
+	DPA_IPSEC_CIPHER_ALG_AES_CBC_HMAC_96_SHA_160,
+	DPA_IPSEC_CIPHER_ALG_AES_CBC_AES_XCBC_MAC_96,
+	DPA_IPSEC_CIPHER_ALG_AES_CBC_HMAC_MD5_128,
+	DPA_IPSEC_CIPHER_ALG_AES_CBC_HMAC_SHA_160,
+	DPA_IPSEC_CIPHER_ALG_AES_CBC_HMAC_SHA_256_128,
+	DPA_IPSEC_CIPHER_ALG_AES_CBC_HMAC_SHA_384_192,
+	DPA_IPSEC_CIPHER_ALG_AES_CBC_HMAC_SHA_512_256,
+	DPA_IPSEC_CIPHER_ALG_AES_CTR_HMAC_96_MD5_128,
+	DPA_IPSEC_CIPHER_ALG_AES_CTR_HMAC_96_SHA_160,
+	DPA_IPSEC_CIPHER_ALG_AES_CTR_AES_XCBC_MAC_96,
+	DPA_IPSEC_CIPHER_ALG_AES_CTR_HMAC_MD5_128,
+	DPA_IPSEC_CIPHER_ALG_AES_CTR_HMAC_SHA_160,
+	DPA_IPSEC_CIPHER_ALG_AES_CTR_HMAC_SHA_256_128,
+	DPA_IPSEC_CIPHER_ALG_AES_CTR_HMAC_SHA_384_192,
+	DPA_IPSEC_CIPHER_ALG_AES_CTR_HMAC_SHA_512_256
+};
+
+/* DPA-IPSec Initialization Vector */
+struct dpa_ipsec_init_vector {
+	uint8_t *init_vector;	/* Pointer to the initialization vector	      */
+	uint8_t length;		/* Length in bytes. May be 8 or 16 bytes      */
+};
+
+/* DPA IPSEC Anti Replay Window Size */
+enum dpa_ipsec_arw {
+	DPA_IPSEC_ARSNONE = 0,	/* No Anti Replay Protection		      */
+	DPA_IPSEC_ARS32,	/* 32 bit Anti Replay Window size	      */
+	DPA_IPSEC_ARS64,	/* 64 bit Anti Replay Window size	      */
+};
+
+/* DPA-IPSec Security Association Cryptographic Parameters */
+struct dpa_ipsec_sa_crypto_params {
+	enum dpa_ipsec_cipher_alg alg_suite;	/* Algorithm suite specifying
+						 * encryption and authentication
+						 * algorithms to be used      */
+	uint8_t *cipher_key;	/* Address of the encryption key	      */
+	uint8_t cipher_key_len;	/* Length of the encryption key in bytes      */
+	uint8_t *auth_key;	/* Address of the authentication key	      */
+	uint8_t auth_key_len;	/* Length of the authentication key in bytes  */
+};
+
+/* DPA-IPSec SA Modes */
+enum dpa_ipsec_sa_mode {
+	DPA_IPSEC_SA_MODE_TUNNEL = 0,
+	DPA_IPSEC_SA_MODE_TRANSPORT
+};
+
+/* DPA-IPSec SA Protocols */
+enum dpa_ipsec_sa_proto {
+	DPA_IPSEC_SA_PROTO_ESP = 0,
+	DPA_IPSEC_SA_PROTO_AH
+};
+
+/* DPA-IPSec Security Association Out Parameters */
+struct dpa_ipsec_sa_out_params {
+	struct dpa_ipsec_init_vector *init_vector; /* Initialization vector
+						    * (IV). Null for using the
+						    * internal random number
+						    * generator               */
+	enum dpa_ipsec_ip_addr_type addr_type;	/* IPv4 or IPv6 address type  */
+	uint16_t ip_hdr_size;	/* IP header size including any IP options    */
+	void *outer_ip_header;	/* IP encapsulation header		      */
+	void *outer_udp_header;	/* UDP encapsulation header
+				 * (for SAs using NAT-T)		      */
+	uint16_t post_sec_flow_id;  /* Flow ID used to mark frames encrypted
+				     * using this SA                          */
+};
+
+/* DPA-IPSec Security Association In Parameters */
+struct dpa_ipsec_sa_in_params {
+	enum dpa_ipsec_arw arw;	/* Anti replay window			      */
+	struct dpa_ipsec_ip_address src_addr;	/* Source IP address	      */
+	struct dpa_ipsec_ip_address dest_addr;	/* Destination IP address     */
+	int use_udp_encap;	/* NAT-T is activated (UDP encapsulated ESP)  */
+	uint16_t src_port;	/* Source UDP port (UDP encapsulated ESP)     */
+	uint16_t dest_port;	/* Destination UDP port (UDP encapsulated ESP)*/
+	uint32_t policy_miss_fqid; /* FQID where frames that fail inbound
+				    * policy verification should be enqueued  */
+	struct dpa_cls_tbl_action post_ipsec_action; /* Action to be performed
+						      * on the frames after
+						      * inbound IPSec processing
+						      * is completed          */
+};
+
+/* DPA-IPSec Security Association Parameters */
+struct dpa_ipsec_sa_params {
+	uint32_t spi;		/* IPSec Security parameter index	      */
+	int use_ext_seq_num;	/* Enable extended sequence number	      */
+	uint64_t start_seq_num;	/* Sequence number to start with	      */
+	uint32_t l2_hdr_size;	/* Size of the Ethernet header, including any
+				 * VLAN information.			      */
+	enum dpa_ipsec_sa_mode sa_mode;	/* Tunnel or transport mode selection */
+	enum dpa_ipsec_sa_proto sa_proto; /* Protocol to be used (AH or ESP)-
+					   * Only ESP supported currently     */
+	uint8_t hdr_upd_flags;	/* Flags for propagating information from inner
+				 * to outer header and vice versa	      */
+	uint8_t sa_wqid;	/* Work queue Id for all the queues in this SA*/
+	uint8_t sa_bpid;	/* Buffer Pool ID to be used with this SA     */
+	struct dpa_ipsec_sa_crypto_params crypto_params;/* IPSec crypto params*/
+	enum dpa_ipsec_direction sa_dir;  /* SA direction: Outbound/Inbound   */
+	union {
+		struct dpa_ipsec_sa_in_params sa_in_params; /* Inb SA params  */
+		struct dpa_ipsec_sa_out_params sa_out_params; /* Out SA params*/
+	};
+};
+
+/* DPA-IPSEC Rekeying error callback */
+typedef int (*dpa_ipsec_rekey_event_cb) (int dpa_ipsec_id, int sa_id,
+					 int error);
+
+/* Offload an SA. */
+int dpa_ipsec_create_sa(int dpa_ipsec_id,
+			struct dpa_ipsec_sa_params *sa_params, int *sa_id);
+
+/* This function will be used when rekeying a SA.
+ *	- The new SA will inherit the old SA's policies.
+ *	- To SEC FQ of the new SA will be created in parked mode and
+ *	  will be scheduled after the to SEC FQ of the old SA is empty.
+ *	  This will ensure the preservation of the frame order
+ *	- To SEC FQ of the old SA will be retired and destroyed when it
+ *	  has no purpose.
+ *	- Memory allocated for old SA will be returned to the SA memory
+ *	  pool
+ *	- Currently the OLD SA is removed when encrypted traffic starts
+ *	  flowing on the new SA regardless of the value auto_rmv_old_sa
+ *	- auto_rmv_old_sa (UNUSED parameter)
+ *	- rekey_event_cb (UNUSED parameter)
+ */
+int dpa_ipsec_sa_rekeying(int sa_id,
+			  struct dpa_ipsec_sa_params *sa_params,
+			  dpa_ipsec_rekey_event_cb *rekey_event_cb,
+			  int auto_rmv_old_sa, int *new_sa_id);
+
+/*
+ * Disables a SA before removal (no more packets will be processed
+ * using this SA). The resource associated with this SA are not
+ * freed until dpa_ipsec_remove_sa is called.
+ */
+int dpa_ipsec_disable_sa(int sa_id);
+
+/* Unregister a SA and destroys the accelerated path. */
+int dpa_ipsec_remove_sa(int sa_id);
+
+/*
+ * This function will remove all SAs (in a specified DPA IPSec
+ * instance)that were offloaded using the DPA IPsec API
+ */
+int dpa_ipsec_flush_all_sa(int dpa_ipsec_id);
+
+/* DPA-IPSec Action For DF Bit */
+enum dpa_ipsec_df_action {
+	DPA_IPSEC_DF_ACTION_DISCARD = 0,	/* Discard the frame	      */
+	DPA_IPSEC_DF_ACTION_OVERRIDE,		/* Override DF bit	      */
+	DPA_IPSEC_DF_ACTION_CONTINUE		/* Continue processing without
+						 * performing IP Fragmentation*/
+};
+
+/* DPA-IPSec Security Policy Parameters */
+struct dpa_ipsec_policy_params {
+	struct dpa_ipsec_ip_address src_addr;	/* Source IP address	      */
+	uint8_t src_prefix_len;	/* Source network prefix		      */
+	struct dpa_ipsec_ip_address dest_addr;	/**< Destination IP address   */
+	uint8_t dest_prefix_len; /* Destination network prefix		      */
+	uint8_t protocol;	/* Protocol				      */
+	uint16_t dest_port;	/* Destination port			      */
+	uint16_t dest_port_mask;/* Destination port mask		      */
+	uint16_t src_port;	/* Source port				      */
+	uint16_t src_port_mask;	/* Source port mask			      */
+	uint16_t mtu;		/* Maximum size of unencrypted fragments.
+				 * Packets with a bigger size will be
+				 * automatically fragmented. 0 = disabled     */
+	enum dpa_ipsec_df_action df_action; /* Action to be performed when IP
+					     * fragmentation is configured for
+					     * this flow and the DF bit is set
+					     * in the input frame             */
+	int priority;		/* Policy priority			      */
+};
+
+/* Add a new rule for policy verification / lookup. */
+int dpa_ipsec_sa_add_policy(int sa_id,
+			    struct dpa_ipsec_policy_params *policy_params);
+
+/* Removes a rule for policy verification / lookup. */
+int dpa_ipsec_sa_remove_policy(int sa_id,
+			       struct dpa_ipsec_policy_params *policy_params);
+
+/*
+ * Retrieves all the policies linked to the specified SA. In order
+ * to determine the size of the policy_params array, the function
+ * must first be called with policy_params = NULL. In this case it
+ * will only return the number of policy entries linked to the SA.
+ */
+int dpa_ipsec_sa_get_policies(int sa_id,
+			      struct dpa_ipsec_policy_params *policy_params,
+			      int *num_pol);
+
+/* This function will remove all policies associated with the specified SA */
+int dpa_ipsec_sa_flush_policies(int sa_id);
+
+#endif	/* __FSL_DPA_IPSEC_H */
-- 
1.7.5.4

