From 283de9beac5075f020ba7e6876997ff247f19f59 Mon Sep 17 00:00:00 2001
From: Ioana Radulescu <ruxandra.radulescu@freescale.com>
Date: Thu, 7 Mar 2013 23:59:32 +0000
Subject: [PATCH 504/518] dpaa_eth: Allow separate buffer layouts per rx/tx
 port

Until now all ports expected the same (hardcoded) buffer layout,
where buffer laout means: private data size, presence of parse results,
hash results, timestamp. Buffer layout also influences the size of the
buffers for private ports.

Create a framework for configuring the buffer layout on a per rx/tx
port basis. There is no change in functionality at the moment (the same
values for buffer layout are kept).

Signed-off-by: Ioana Radulescu <ruxandra.radulescu@freescale.com>
[Grabbed from the branch, LINUX_IR5.2.0, of
https://git.freescale.com/git-private/cgit.cgi/ppc/alu-b4860/linux.git.]
Signed-off-by: Tiejun Chen <tiejun.chen@windriver.com>
---
 .../net/ethernet/freescale/dpa/dpaa_eth-common.h   |   22 ++-
 drivers/net/ethernet/freescale/dpa/dpaa_eth.c      |  147 +++++++++++++-------
 drivers/net/ethernet/freescale/dpa/dpaa_eth.h      |   65 +++++++---
 drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c   |   25 ++--
 drivers/net/ethernet/freescale/dpa/offline_port.c  |   13 ++-
 5 files changed, 185 insertions(+), 87 deletions(-)

diff --git a/drivers/net/ethernet/freescale/dpa/dpaa_eth-common.h b/drivers/net/ethernet/freescale/dpa/dpaa_eth-common.h
index 17c2382..f26e089 100644
--- a/drivers/net/ethernet/freescale/dpa/dpaa_eth-common.h
+++ b/drivers/net/ethernet/freescale/dpa/dpaa_eth-common.h
@@ -58,22 +58,28 @@ enum dpa_fq_type {
 #endif
 };
 
+struct dpa_buffer_layout_s {
+	uint16_t	priv_data_size;
+	bool		parse_results;
+	bool		time_stamp;
+	bool		hash_results;
+};
 
+#define DPA_TX_PRIV_DATA_SIZE	16
 #define DPA_PARSE_RESULTS_SIZE sizeof(t_FmPrsResult)
-#define DPA_HASH_RESULTS_SIZE 16
+#define DPA_TIME_STAMP_SIZE 8
+#define DPA_HASH_RESULTS_SIZE 8
 
-#define DPA_TX_PRIV_DATA_SIZE	16
 
-#define dpaa_eth_init_port(type, port, param, errq_id, defq_id, priv_size, \
-			   has_timer) \
+#define dpaa_eth_init_port(type, port, param, errq_id, defq_id, buf_layout) \
 { \
 	param.errq = errq_id; \
 	param.defq = defq_id; \
-	param.priv_data_size = priv_size; \
-	param.parse_results = true; \
-	param.hash_results = true; \
+	param.priv_data_size = buf_layout->priv_data_size; \
+	param.parse_results = buf_layout->parse_results; \
+	param.hash_results = buf_layout->hash_results; \
 	param.frag_enable = false; \
-	param.time_stamp = has_timer; \
+	param.time_stamp = buf_layout->time_stamp; \
 	fm_set_##type##_port_params(port, &param); \
 }
 
diff --git a/drivers/net/ethernet/freescale/dpa/dpaa_eth.c b/drivers/net/ethernet/freescale/dpa/dpaa_eth.c
index 3b1f685..e8e56b7 100644
--- a/drivers/net/ethernet/freescale/dpa/dpaa_eth.c
+++ b/drivers/net/ethernet/freescale/dpa/dpaa_eth.c
@@ -106,7 +106,8 @@
 #define DPA_FQ_TD		0x200000
 
 /* S/G table requires at least 256 bytes */
-#define SGT_BUFFER_SIZE		DPA_BP_SIZE(256)
+#define sgt_buffer_size(priv) \
+	dpa_get_buffer_size(&priv->buf_layout[TX], 256)
 
 /* Maximum frame size on Tx for which skb copying is preferrable to
  * creating a S/G frame */
@@ -119,12 +120,20 @@
 #define DPA_MAX_FD_OFFSET	((1 << 9) - 1)
 
 /*
- * Maximum size of a buffer that is to be recycled back to the buffer pool.
+ * Extra size of a buffer (beyond the size of the buffers that are seeded into
+ * the global pool) for which recycling is allowed.
  * The value is arbitrary, but tries to reach a balance such that originating
  * frames may get recycled, while forwarded skbs that get reallocated on Tx
  * aren't allowed to grow unboundedly.
  */
-#define DPA_BP_MAX_BUF_SIZE	(DEFAULT_BUF_SIZE + 256)
+#define DPA_RECYCLE_EXTRA_SIZE	256
+
+/*
+ * For MAC-based interfaces, we compute the tx needed headroom from the
+ * associated Tx port's buffer layout settings.
+ * For MACless interfaces just use a default value.
+ */
+#define DPA_DEFAULT_TX_HEADROOM	64
 
 #define DPA_DESCRIPTION "FSL DPAA Ethernet driver"
 
@@ -382,7 +391,8 @@ static void dpaa_eth_seed_pool(struct dpa_bp *bp)
  * Add buffers/pages/skbuffs for Rx processing whenever bpool count falls below
  * REFILL_THRESHOLD.
  */
-static void dpaa_eth_refill_bpools(struct dpa_percpu_priv_s *percpu_priv)
+static void dpaa_eth_refill_bpools(struct dpa_priv_s *priv,
+				   struct dpa_percpu_priv_s *percpu_priv)
 {
 	int *countptr = percpu_priv->dpa_bp_count;
 	int count = *countptr;
@@ -404,7 +414,7 @@ static void dpaa_eth_refill_bpools(struct dpa_percpu_priv_s *percpu_priv)
 	/* Add skbs to the percpu skb list, reuse var count */
 	count = percpu_priv->skb_count;
 	if (unlikely(count < DEFAULT_SKB_COUNT / 4))
-		dpa_list_add_skbs(percpu_priv,
+		dpa_list_add_skbs(priv, percpu_priv,
 				  DEFAULT_SKB_COUNT - count);
 #endif
 }
@@ -644,7 +654,7 @@ _dpa_fq_alloc(struct list_head *list, struct dpa_fq *dpa_fq)
 			initfq.we_mask |= QM_INITFQ_WE_OAC;
 			initfq.fqd.oac_init.oac = QM_OAC_CG;
 			initfq.fqd.oac_init.oal = min(sizeof(struct sk_buff) +
-				DPA_BP_HEAD, (size_t)FSL_QMAN_MAX_OAL);
+				priv->tx_headroom, (size_t)FSL_QMAN_MAX_OAL);
 		}
 
 		/*
@@ -927,7 +937,7 @@ struct sk_buff *_dpa_cleanup_tx_fd(const struct dpa_priv_s *priv,
 		void *vaddr = phys_to_virt(addr);
 
 		/* Unmap first buffer (contains S/G table) */
-		dma_unmap_single(bp->dev, addr, SGT_BUFFER_SIZE,
+		dma_unmap_single(bp->dev, addr, sgt_buffer_size(priv),
 				 DMA_TO_DEVICE);
 
 		/* Unmap data buffer */
@@ -1481,14 +1491,20 @@ static struct dpa_bp *dpa_size2pool(struct dpa_priv_s *priv, size_t size)
 	int i;
 
 	for (i = 0; i < priv->bp_count; i++)
-		if (DPA_BP_SIZE(size) <= priv->dpa_bp[i].size)
+		if ((size + priv->tx_headroom) <= priv->dpa_bp[i].size)
 			return dpa_bpid2pool(priv->dpa_bp[i].bpid);
 	return ERR_PTR(-ENODEV);
 }
 
-static uint32_t dpa_bp_size(struct fm_port *rx_port)
+static void dpa_set_buffer_layout(struct dpa_priv_s *priv, struct fm_port *port,
+				  struct dpa_buffer_layout_s *layout, int type)
 {
-	return DEFAULT_BUF_SIZE;
+	layout->priv_data_size = (type == RX ?
+			DPA_RX_PRIV_DATA_SIZE : DPA_TX_PRIV_DATA_SIZE);
+	layout->parse_results = true;
+	layout->hash_results = true;
+	if (priv && priv->tsu && priv->tsu->valid)
+		layout->time_stamp = true;
 }
 
 /**
@@ -1628,7 +1644,7 @@ static int __hot dpa_shared_tx(struct sk_buff *skb, struct net_device *net_dev)
 	fd.length20 = skb_headlen(skb);
 	fd.addr_hi = bmb.hi;
 	fd.addr_lo = bmb.lo;
-	fd.offset = DPA_BP_HEAD;
+	fd.offset = priv->tx_headroom;
 
 	/*
 	 * The virtual address of the buffer pool is expected to be NULL
@@ -1690,7 +1706,7 @@ static int skb_to_sg_fd(struct dpa_priv_s *priv,
 	int err;
 
 	/* Allocate the first buffer in the FD (used for storing S/G table) */
-	vaddr = kmalloc(SGT_BUFFER_SIZE, GFP_ATOMIC);
+	vaddr = kmalloc(sgt_buffer_size(priv), GFP_ATOMIC);
 	if (unlikely(vaddr == NULL)) {
 		if (netif_msg_tx_err(priv) && net_ratelimit())
 			netdev_err(net_dev, "Memory allocation failed\n");
@@ -1702,7 +1718,7 @@ static int skb_to_sg_fd(struct dpa_priv_s *priv,
 
 	/* Fill in FD */
 	fd->format = qm_fd_sg;
-	fd->offset = DPA_BP_HEAD;
+	fd->offset = priv->tx_headroom;
 	fd->length20 = skb->len;
 
 	/* Enable hardware checksum computation */
@@ -1716,7 +1732,7 @@ static int skb_to_sg_fd(struct dpa_priv_s *priv,
 	}
 
 	/* Map the buffer and store its address in the FD */
-	paddr = dma_map_single(dpa_bp->dev, vaddr, SGT_BUFFER_SIZE,
+	paddr = dma_map_single(dpa_bp->dev, vaddr, sgt_buffer_size(priv),
 			       DMA_TO_DEVICE);
 	if (unlikely(dma_mapping_error(dpa_bp->dev, paddr))) {
 		if (netif_msg_tx_err(priv) && net_ratelimit())
@@ -1739,10 +1755,10 @@ static int skb_to_sg_fd(struct dpa_priv_s *priv,
 	 * Put the same offset in the data buffer as in the SGT (first) buffer.
 	 * This is the format for S/G frames generated by FMan; the manual is
 	 * not clear if same is required of Tx S/G frames, but since we know
-	 * for sure we have at least DPA_BP_HEAD bytes of skb headroom, lets not
-	 * take any chances.
+	 * for sure we have at least tx_headroom bytes of skb headroom,
+	 * lets not take any chances.
 	 */
-	sg_entry->offset = DPA_BP_HEAD;
+	sg_entry->offset = priv->tx_headroom;
 
 	paddr = dma_map_single(dpa_bp->dev, skb->data - sg_entry->offset,
 			       dpa_bp->size, DMA_TO_DEVICE);
@@ -1771,11 +1787,11 @@ static int skb_to_contig_fd(struct dpa_priv_s *priv,
 	int err;
 
 	/*
-	 * We are guaranteed that we have at least DPA_BP_HEAD of headroom.
+	 * We are guaranteed that we have at least tx_headroom bytes.
 	 * Buffers we allocated are padded to improve cache usage. In order
 	 * to increase buffer re-use, we aim to keep any such buffers the
-	 * same. This means the address passed to the FM should be DPA_BP_HEAD
-	 * before the data for forwarded frames.
+	 * same. This means the address passed to the FM should be
+	 * tx_headroom bytes before the data for forwarded frames.
 	 *
 	 * However, offer some flexibility in fd layout, to allow originating
 	 * (termination) buffers to be also recycled when possible.
@@ -1790,23 +1806,24 @@ static int skb_to_contig_fd(struct dpa_priv_s *priv,
 	 * - there's enough room in the buffer pool
 	 */
 	if (likely(skb_is_recycleable(skb, dpa_bp->size) &&
-		   (skb_end_pointer(skb) - skb->head <= DPA_BP_MAX_BUF_SIZE) &&
+		   (skb_end_pointer(skb) - skb->head <=
+			dpa_bp->size + DPA_RECYCLE_EXTRA_SIZE) &&
 		   (*percpu_priv->dpa_bp_count < dpa_bp->target_count))) {
 		/* Compute the minimum necessary fd offset */
 		offset = dpa_bp->size - skb->len - skb_tailroom(skb);
 
 		/*
-		 * And make sure the offset is no lower than DPA_BP_HEAD,
-		 * as required by FMan
+		 * And make sure the offset is no lower than the offset
+		 * required by FMan
 		 */
-		offset = max(offset, (int)DPA_BP_HEAD);
+		offset = max(offset, (int)priv->tx_headroom);
 
 		/*
 		 * We also need to align the buffer address to 16, such that
 		 * Fman will be able to reuse it on Rx.
 		 * Since the buffer going to FMan starts at (skb->data - offset)
 		 * this is what we'll try to align. We already know that
-		 * headroom is at least DPA_BP_HEAD bytes long, but with
+		 * headroom is at least tx_headroom bytes long, but with
 		 * the extra offset needed for alignment we may go beyond
 		 * the beginning of the buffer.
 		 *
@@ -1836,7 +1853,7 @@ static int skb_to_contig_fd(struct dpa_priv_s *priv,
 		 * No recycling here, so we don't care about address alignment.
 		 * Just use the smallest offset required by FMan
 		 */
-		offset = DPA_BP_HEAD;
+		offset = priv->tx_headroom;
 	}
 
 	skbh = (struct sk_buff **)(skb->data - offset);
@@ -1894,10 +1911,10 @@ int __hot dpa_tx(struct sk_buff *skb, struct net_device *net_dev)
 	clear_fd(&fd);
 	queue_mapping = dpa_get_queue_mapping(skb);
 
-	if (unlikely(skb_headroom(skb) < DPA_BP_HEAD)) {
+	if (unlikely(skb_headroom(skb) < priv->tx_headroom)) {
 		struct sk_buff *skb_new;
 
-		skb_new = skb_realloc_headroom(skb, DPA_BP_HEAD);
+		skb_new = skb_realloc_headroom(skb, priv->tx_headroom);
 		if (unlikely(!skb_new)) {
 			percpu_stats->tx_errors++;
 			kfree_skb(skb);
@@ -2018,7 +2035,7 @@ ingress_rx_error_dqrr(struct qman_portal		*portal,
 		return qman_cb_dqrr_stop;
 	}
 
-	dpaa_eth_refill_bpools(percpu_priv);
+	dpaa_eth_refill_bpools(priv, percpu_priv);
 	_dpa_rx_error(net_dev, priv, percpu_priv, &dq->fd, fq->fqid);
 
 	return qman_cb_dqrr_consume;
@@ -2056,7 +2073,7 @@ shared_rx_dqrr(struct qman_portal *portal, struct qman_fq *fq,
 	}
 
 	skb = __netdev_alloc_skb(net_dev,
-				 DPA_BP_HEAD + dpa_fd_length(fd),
+				 priv->tx_headroom + dpa_fd_length(fd),
 				 GFP_ATOMIC);
 	if (unlikely(skb == NULL)) {
 		if (netif_msg_rx_err(priv) && net_ratelimit())
@@ -2067,7 +2084,7 @@ shared_rx_dqrr(struct qman_portal *portal, struct qman_fq *fq,
 		goto out;
 	}
 
-	skb_reserve(skb, DPA_BP_HEAD);
+	skb_reserve(skb, priv->tx_headroom);
 
 	if (fd->format == qm_fd_sg) {
 		if (dpa_bp->vaddr) {
@@ -2177,7 +2194,7 @@ ingress_rx_default_dqrr(struct qman_portal		*portal,
 	}
 
 	/* Vale of plenty: make sure we didn't run out of buffers */
-	dpaa_eth_refill_bpools(percpu_priv);
+	dpaa_eth_refill_bpools(priv, percpu_priv);
 	_dpa_rx(net_dev, priv, percpu_priv, &dq->fd, fq->fqid);
 
 	return qman_cb_dqrr_consume;
@@ -2613,7 +2630,8 @@ static int __devinit dpa_tx_unit_test(struct net_device *net_dev)
 
 	/* Try packet sizes from 64-bytes to just above the maximum */
 	for (size = 64; size <= 9600 + 128; size += 64) {
-		for (headroom = DPA_BP_HEAD; headroom < 0x800; headroom += 16) {
+		for (headroom = priv->tx_headroom; headroom < 0x800;
+		     headroom += 16) {
 			int ret;
 			struct sk_buff *skb;
 
@@ -2745,10 +2763,11 @@ static int __cold dpa_start(struct net_device *net_dev)
 	}
 	for_each_online_cpu(i) {
 		percpu_priv = per_cpu_ptr(priv->percpu_priv, i);
-		if (!priv->shared && !percpu_priv->dpa_bp)
+		if (!priv->shared && !percpu_priv->dpa_bp) {
 			percpu_priv->dpa_bp = priv->dpa_bp;
 			percpu_priv->dpa_bp_count =
 				per_cpu_ptr(priv->dpa_bp->percpu_count, i);
+		}
 	}
 
 	dpaa_eth_napi_enable(priv);
@@ -3095,12 +3114,13 @@ static int __cold dpa_debugfs_show(struct seq_file *file, void *offset)
 		"CPU           irqs        rx        tx   recycle" \
 		"   confirm     tx sg    tx err    rx err   l4 hxs drp    bp count\n",
 		priv->net_dev->name);
+
 	for_each_online_cpu(i) {
 		percpu_priv = per_cpu_ptr(priv->percpu_priv, i);
 
 		/* Only private interfaces have an associated counter for bp
 		 * buffers */
-		if (!priv->shared)
+		if (!priv->shared && percpu_priv->dpa_bp_count)
 			dpa_bp_count = *percpu_priv->dpa_bp_count;
 
 		total.in_interrupt += percpu_priv->in_interrupt;
@@ -3591,17 +3611,18 @@ static void dpa_setup_ingress_queues(struct dpa_priv_s *priv,
 
 static void __devinit
 dpaa_eth_init_tx_port(struct fm_port *port, struct dpa_fq *errq,
-		struct dpa_fq *defq, bool has_timer)
+		struct dpa_fq *defq, struct dpa_buffer_layout_s *buf_layout)
 {
 	struct fm_port_params tx_port_param;
 
 	dpaa_eth_init_port(tx, port, tx_port_param, errq->fqid, defq->fqid,
-			DPA_TX_PRIV_DATA_SIZE, has_timer);
+			   buf_layout);
 }
 
 static void __devinit
 dpaa_eth_init_rx_port(struct fm_port *port, struct dpa_bp *bp, size_t count,
-		struct dpa_fq *errq, struct dpa_fq *defq, bool has_timer)
+		      struct dpa_fq *errq, struct dpa_fq *defq,
+		      struct dpa_buffer_layout_s *buf_layout)
 {
 	struct fm_port_params rx_port_param;
 	int i;
@@ -3620,7 +3641,7 @@ dpaa_eth_init_rx_port(struct fm_port *port, struct dpa_bp *bp, size_t count,
 	rx_port_param.data_align = ???
 */
 	dpaa_eth_init_port(rx, port, rx_port_param, errq->fqid, defq->fqid,
-			DPA_RX_PRIV_DATA_SIZE, has_timer);
+			   buf_layout);
 }
 
 static void dpa_rx_fq_init(struct dpa_priv_s *priv, struct list_head *head,
@@ -3728,7 +3749,12 @@ static int dpa_netdev_init(struct device_node *dpa_node,
 	memcpy(net_dev->dev_addr, mac_addr, net_dev->addr_len);
 
 	SET_ETHTOOL_OPS(net_dev, &dpa_ethtool_ops);
-	net_dev->needed_headroom = DPA_BP_HEAD;
+
+	/*
+	 * TODO: May want to remove tx_headroom from priv altogether and use
+	 * priv->net_dev->needed_headroom instead
+	 */
+	net_dev->needed_headroom = priv->tx_headroom;
 	net_dev->watchdog_timeo = msecs_to_jiffies(tx_timeout);
 
 	err = register_netdev(net_dev);
@@ -3782,7 +3808,7 @@ static int dpa_private_netdev_init(struct device_node *dpa_node,
 		/* init the percpu list and add some skbs */
 		skb_queue_head_init(&percpu_priv->skb_list);
 
-		dpa_list_add_skbs(percpu_priv, DEFAULT_SKB_COUNT);
+		dpa_list_add_skbs(priv, percpu_priv, DEFAULT_SKB_COUNT);
 #endif
 		netif_napi_add(net_dev, &percpu_priv->napi, dpaa_eth_poll,
 			       DPA_NAPI_WEIGHT);
@@ -3875,7 +3901,7 @@ static const struct of_device_id dpa_match[] __devinitconst ;
 static int __devinit
 dpaa_eth_probe(struct platform_device *_of_dev)
 {
-	int err, i;
+	int err = 0, i;
 	struct device *dev;
 	struct device_node *dpa_node;
 	struct dpa_bp *dpa_bp;
@@ -3885,6 +3911,7 @@ dpaa_eth_probe(struct platform_device *_of_dev)
 	size_t count;
 	struct net_device *net_dev = NULL;
 	struct dpa_priv_s *priv = NULL;
+	struct dpa_percpu_priv_s *percpu_priv;
 	struct dpa_fq *rxdefault = NULL;
 	struct dpa_fq *txdefault = NULL;
 	struct dpa_fq *rxerror = NULL;
@@ -3895,7 +3922,7 @@ dpaa_eth_probe(struct platform_device *_of_dev)
 	struct dpa_fq *txrecycle = NULL;
 	struct fm_port *rxport = NULL;
 	struct fm_port *txport = NULL;
-	bool has_timer = FALSE;
+	struct dpa_buffer_layout_s *buf_layout = NULL;
 	bool is_shared = false;
 	struct mac_device *mac_dev;
 	int proxy_enet;
@@ -3953,6 +3980,19 @@ dpaa_eth_probe(struct platform_device *_of_dev)
 	} else if (mac_dev) {
 		rxport = mac_dev->port_dev[RX];
 		txport = mac_dev->port_dev[TX];
+
+		/*
+		 * We have physical ports, so we need to establish
+		 * the buffer layout.
+		 */
+		buf_layout = devm_kzalloc(dev, 2 * sizeof(*buf_layout),
+					  GFP_KERNEL);
+		if (!buf_layout) {
+			dev_err(dev, "devm_kzalloc() failed\n");
+			goto alloc_failed;
+		}
+		dpa_set_buffer_layout(priv, rxport, &buf_layout[RX], RX);
+		dpa_set_buffer_layout(priv, txport, &buf_layout[TX], TX);
 	}
 
 	if (!dpa_bp->kernel_pool)
@@ -3963,7 +4003,7 @@ dpaa_eth_probe(struct platform_device *_of_dev)
 		 * buffer pool, based on FMan port buffer layout;also update
 		 * the maximum buffer size for private ports if necessary
 		 */
-		dpa_bp->size = dpa_bp_size(rxport);
+		dpa_bp->size = dpa_bp_size(&buf_layout[RX]);
 		if (dpa_bp->size > default_buf_size)
 			default_buf_size = dpa_bp->size;
 	}
@@ -4070,17 +4110,23 @@ dpaa_eth_probe(struct platform_device *_of_dev)
 				goto fq_alloc_failed;
 		}
 
-		if (priv->tsu && priv->tsu->valid)
-			has_timer = TRUE;
+		if (mac_dev) {
+			priv->buf_layout = buf_layout;
+			priv->tx_headroom =
+				dpa_get_headroom(&priv->buf_layout[TX]);
+		} else {
+			priv->tx_headroom = DPA_DEFAULT_TX_HEADROOM;
+		}
 	}
 
 	/* All real interfaces need their ports initialized */
 	if (mac_dev) {
 		struct fm_port_pcd_param rx_port_pcd_param;
 
-		dpaa_eth_init_tx_port(txport, txerror, txdefault, has_timer);
+		dpaa_eth_init_tx_port(txport, txerror, txdefault,
+				      &buf_layout[TX]);
 		dpaa_eth_init_rx_port(rxport, dpa_bp, count, rxerror,
-				rxdefault, has_timer);
+				      rxdefault, &buf_layout[RX]);
 
 		rx_port_pcd_param.cba = dpa_alloc_pcd_fqids;
 		rx_port_pcd_param.cbf = dpa_free_pcd_fqids;
@@ -4117,6 +4163,10 @@ dpaa_eth_probe(struct platform_device *_of_dev)
 		err = -ENOMEM;
 		goto alloc_percpu_failed;
 	}
+	for_each_online_cpu(i) {
+		percpu_priv = per_cpu_ptr(priv->percpu_priv, i);
+		memset(percpu_priv, 0, sizeof(*percpu_priv));
+	}
 
 	if (priv->shared)
 		err = dpa_shared_netdev_init(dpa_node, net_dev);
@@ -4157,6 +4207,7 @@ get_channel_failed:
 bp_create_failed:
 tx_fq_probe_failed:
 rx_fq_probe_failed:
+alloc_failed:
 mac_probe_failed:
 bp_probe_failed:
 	dev_set_drvdata(dev, NULL);
diff --git a/drivers/net/ethernet/freescale/dpa/dpaa_eth.h b/drivers/net/ethernet/freescale/dpa/dpaa_eth.h
index 34b99d5..122bb9d 100644
--- a/drivers/net/ethernet/freescale/dpa/dpaa_eth.h
+++ b/drivers/net/ethernet/freescale/dpa/dpaa_eth.h
@@ -71,6 +71,23 @@
 #define dpa_get_max_mtu()	\
 	(dpa_get_max_frm() - (VLAN_ETH_HLEN + ETH_FCS_LEN))
 
+
+#ifdef CONFIG_DPAA_ETH_SG_SUPPORT
+/*
+ * We may want this value configurable. Must be <= PAGE_SIZE
+ * A lower value may help with recycling rates, at least on forwarding
+ */
+#define dpa_bp_size(buffer_layout)	PAGE_SIZE
+#else
+/*
+ * Default buffer size is based on L2 MAX_FRM value, minus the FCS which
+ * is stripped down by hardware.
+ */
+#define dpa_bp_size(buffer_layout) \
+	dpa_get_buffer_size(buffer_layout, (dpa_get_max_frm() - ETH_FCS_LEN))
+#endif
+
+
 #define DPA_RX_PRIV_DATA_SIZE   (DPA_TX_PRIV_DATA_SIZE + \
 					dpa_get_rx_extra_headroom())
 /* number of Tx queues to FMan */
@@ -146,32 +163,16 @@ struct dpaa_eth_hooks_s {
 
 void fsl_dpaa_eth_set_hooks(struct dpaa_eth_hooks_s *hooks);
 
-/* The netdevice's needed_headroom */
-#define DPA_BP_HEAD (DPA_TX_PRIV_DATA_SIZE + DPA_PARSE_RESULTS_SIZE + \
-			DPA_HASH_RESULTS_SIZE)
-#define DPA_BP_SIZE(s)	(DPA_BP_HEAD + dpa_get_rx_extra_headroom() + (s))
-
 #define DPA_SGT_MAX_ENTRIES 16 /* maximum number of entries in SG Table */
 
 #ifdef CONFIG_DPAA_ETH_SG_SUPPORT
 #define DEFAULT_SKB_COUNT 64 /* maximum number of SKBs in each percpu list */
 /*
- * We may want this value configurable. Must be <= PAGE_SIZE
- * A lower value may help with recycling rates, at least on forwarding
- */
-#define DEFAULT_BUF_SIZE	PAGE_SIZE
-/*
  * Default amount data to be copied from the beginning of a frame into the
  * linear part of the skb, in case we aren't using the hardware parser.
  */
 #define DPA_COPIED_HEADERS_SIZE 128
 
-#else
-/*
- * Default buffer size is based on L2 MAX_FRM value, minus the FCS which is
- * stripped down by hardware.
- */
-#define DEFAULT_BUF_SIZE DPA_BP_SIZE(dpa_get_max_frm() - ETH_FCS_LEN)
 #endif /* CONFIG_DPAA_ETH_SG_SUPPORT */
 
 /*
@@ -397,6 +398,12 @@ struct dpa_priv_s {
 		 */
 		u32 cgr_congested_count;
 	} cgr_data;
+	/*
+	 * Store here the needed Tx headroom for convenience and speed
+	 * (even though it can be computed based on the fields of buf_layout)
+	 */
+	uint16_t tx_headroom;
+	struct dpa_buffer_layout_s *buf_layout;
 };
 
 extern const struct ethtool_ops dpa_ethtool_ops;
@@ -429,7 +436,8 @@ int __hot _dpa_process_parse_results(const t_FmPrsResult *parse_results,
 void dpa_bp_add_8_pages(const struct dpa_bp *dpa_bp, int cpu_id);
 int _dpa_bp_add_8_pages(const struct dpa_bp *dpa_bp);
 
-void dpa_list_add_skbs(struct dpa_percpu_priv_s *cpu_priv, int count);
+void dpa_list_add_skbs(struct dpa_priv_s *, struct dpa_percpu_priv_s *cpu_priv,
+		       int count);
 #endif
 
 /*
@@ -481,6 +489,29 @@ static inline int dpa_check_rx_mtu(struct sk_buff *skb, int mtu)
 	return 0;
 }
 
+static inline uint16_t dpa_get_headroom(struct dpa_buffer_layout_s *bl)
+{
+	/*
+	 * The frame headroom must accomodate:
+	 * - the driver private data area
+	 * - parse results, hash results, timestamp if selected
+	 * If either hash results or time stamp are selected, both will
+	 * be copied to/from the frame headroom, as TS is located between PR and
+	 * HR in the IC and IC copy size has a granularity of 16bytes
+	 * (see description of FMBM_RICP and FMBM_TICP registers in DPAARM)
+	 */
+	return bl->priv_data_size +
+		(bl->parse_results ? DPA_PARSE_RESULTS_SIZE : 0) +
+		(bl->hash_results || bl->time_stamp ?
+		 DPA_TIME_STAMP_SIZE + DPA_HASH_RESULTS_SIZE : 0);
+}
+
+static inline uint16_t dpa_get_buffer_size(struct dpa_buffer_layout_s *bl,
+					   uint16_t data_size)
+{
+	return dpa_get_headroom(bl) + data_size;
+}
+
 void fm_mac_dump_regs(struct mac_device *mac_dev);
 
 void dpaa_eth_sysfs_remove(struct device *dev);
diff --git a/drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c b/drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c
index ec8a371..ecb2e75 100644
--- a/drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c
+++ b/drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c
@@ -179,13 +179,14 @@ static struct sk_buff *dpa_list_get_skb(struct dpa_percpu_priv_s *cpu_priv)
 	return new_skb;
 }
 
-void dpa_list_add_skbs(struct dpa_percpu_priv_s *cpu_priv, int count)
+void dpa_list_add_skbs(struct dpa_priv_s *priv,
+		       struct dpa_percpu_priv_s *cpu_priv, int count)
 {
 	struct sk_buff *new_skb;
 	int i;
 
 	for (i = 0; i < count; i++) {
-		new_skb = dev_alloc_skb(DPA_BP_HEAD +
+		new_skb = dev_alloc_skb(priv->tx_headroom +
 				dpa_get_rx_extra_headroom() +
 				DPA_COPIED_HEADERS_SIZE);
 		if (unlikely(!new_skb)) {
@@ -467,8 +468,8 @@ void __hot _dpa_rx(struct net_device *net_dev,
 
 	if (unlikely(skb == NULL)) {
 		/* List is empty, so allocate a new skb */
-		skb = dev_alloc_skb(DPA_BP_HEAD + dpa_get_rx_extra_headroom() +
-			DPA_COPIED_HEADERS_SIZE);
+		skb = dev_alloc_skb(priv->tx_headroom +
+			dpa_get_rx_extra_headroom() + DPA_COPIED_HEADERS_SIZE);
 		if (unlikely(skb == NULL)) {
 			if (netif_msg_rx_err(priv) && net_ratelimit())
 				netdev_err(net_dev,
@@ -484,7 +485,7 @@ void __hot _dpa_rx(struct net_device *net_dev,
 	 * Make sure forwarded skbs will have enough space on Tx,
 	 * if extra headers are added.
 	 */
-	skb_reserve(skb, DPA_BP_HEAD + dpa_get_rx_extra_headroom());
+	skb_reserve(skb, priv->tx_headroom + dpa_get_rx_extra_headroom());
 
 	dpa_bp_removed_one_page(dpa_bp, addr);
 
@@ -560,8 +561,8 @@ static int __hot skb_to_contig_fd(struct dpa_priv_s *priv,
 	struct net_device *net_dev = priv->net_dev;
 	int err;
 
-	/* We are guaranteed that we have at least DPA_BP_HEAD of headroom. */
-	skbh = (struct sk_buff **)(skb->data - DPA_BP_HEAD);
+	/* We are guaranteed that we have at least tx_headroom bytes */
+	skbh = (struct sk_buff **)(skb->data - priv->tx_headroom);
 
 	*skbh = skb;
 
@@ -584,7 +585,7 @@ static int __hot skb_to_contig_fd(struct dpa_priv_s *priv,
 	/* Fill in the FD */
 	fd->format = qm_fd_contig;
 	fd->length20 = skb->len;
-	fd->offset = DPA_BP_HEAD; /* This is now guaranteed */
+	fd->offset = priv->tx_headroom; /* This is now guaranteed */
 
 	addr = dma_map_single(dpa_bp->dev, skbh, dpa_bp->size, DMA_TO_DEVICE);
 	if (unlikely(dma_mapping_error(dpa_bp->dev, addr))) {
@@ -637,7 +638,7 @@ static int __hot skb_to_sg_fd(struct dpa_priv_s *priv,
 		goto csum_failed;
 	}
 
-	sgt = (struct qm_sg_entry *)(sgt_page + DPA_BP_HEAD);
+	sgt = (struct qm_sg_entry *)(sgt_page + priv->tx_headroom);
 	sgt[0].bpid = dpa_bp->bpid;
 	sgt[0].offset = 0;
 	sgt[0].length = skb_headlen(skb);
@@ -680,7 +681,7 @@ static int __hot skb_to_sg_fd(struct dpa_priv_s *priv,
 	sgt[i - 1].final = 1;
 
 	fd->length20 = skb->len;
-	fd->offset = DPA_BP_HEAD;
+	fd->offset = priv->tx_headroom;
 
 	/* DMA map the SGT page */
 	buffer_start = (void *)sgt - dpa_fd_offset(fd);
@@ -744,10 +745,10 @@ int __hot dpa_tx(struct sk_buff *skb, struct net_device *net_dev)
 		 * data, parse results, etc. Normally this shouldn't happen if
 		 * we're here via the standard kernel stack.
 		 */
-		if (unlikely(skb_headroom(skb) < DPA_BP_HEAD)) {
+		if (unlikely(skb_headroom(skb) < priv->tx_headroom)) {
 			struct sk_buff *skb_new;
 
-			skb_new = skb_realloc_headroom(skb, DPA_BP_HEAD);
+			skb_new = skb_realloc_headroom(skb, priv->tx_headroom);
 			if (unlikely(!skb_new)) {
 				dev_kfree_skb(skb);
 				percpu_stats->tx_errors++;
diff --git a/drivers/net/ethernet/freescale/dpa/offline_port.c b/drivers/net/ethernet/freescale/dpa/offline_port.c
index 691cd48..600d94c 100644
--- a/drivers/net/ethernet/freescale/dpa/offline_port.c
+++ b/drivers/net/ethernet/freescale/dpa/offline_port.c
@@ -97,6 +97,14 @@ static int __devinit __cold oh_free_pcd_fqids(struct device *dev, uint32_t base_
 	return 0;
 }
 
+static void oh_set_buffer_layout(struct dpa_buffer_layout_s *layout)
+{
+	layout->priv_data_size = DPA_TX_PRIV_DATA_SIZE;
+	layout->parse_results = true;
+	layout->hash_results = true;
+	layout->time_stamp = false;
+}
+
 static int __devinit
 oh_port_probe(struct platform_device *_of_dev)
 {
@@ -114,6 +122,7 @@ oh_port_probe(struct platform_device *_of_dev)
 	uint32_t		 crt_fq_count;
 	struct fm_port_params	 oh_port_tx_params;
 	struct fm_port_pcd_param	oh_port_pcd_params;
+	struct dpa_buffer_layout_s buf_layout;
 	/* True if the current partition owns the OH port. */
 	bool init_oh_port;
 	const struct of_device_id *match;
@@ -263,10 +272,10 @@ oh_port_probe(struct platform_device *_of_dev)
 		goto return_kfree;
 	}
 
+	oh_set_buffer_layout(&buf_layout);
 	/* Set Tx params */
 	dpaa_eth_init_port(tx, oh_config->oh_port, oh_port_tx_params,
-		oh_config->error_fqid, oh_config->default_fqid,
-		DPA_TX_PRIV_DATA_SIZE, FALSE);
+		oh_config->error_fqid, oh_config->default_fqid, (&buf_layout));
 	/* Set PCD params */
 	oh_port_pcd_params.cba = oh_alloc_pcd_fqids;
 	oh_port_pcd_params.cbf = oh_free_pcd_fqids;
-- 
1.7.5.4

