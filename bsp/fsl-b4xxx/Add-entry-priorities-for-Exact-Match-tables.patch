From 27ddbd855edd583b50b73f129bef95cbbd0679dd Mon Sep 17 00:00:00 2001
From: Marian Chereji <marian.chereji@freescale.com>
Date: Fri, 9 Mar 2012 21:25:49 +0000
Subject: [PATCH 110/518] Add entry priorities for Exact Match tables

Needed to update the [entry_id] to [cc table index] mapping so that the
mapping array can be sorted in the order of [priority] values (in case
the table is an exact match table and entry priorities are used) or [cc
table index] values (in all other cases). The Cc table index management
is now global across all internal Cc nodes, instead of local for each
internal Cc node. This allows the [entry_id] to be constant regardless
whether an entry needs to move into a different internal Cc node. It has
also optimised the speed of updating the table entry indexes in case of
removing or adding table entries.

The entry priorities are supported only for small exact match tables at
this time, which means exact match tables with less than 256 entries. An
enhancement request was logged to extend entry priorities also to large
exact match table.

Signed-off-by: Marian Chereji <marian.chereji@freescale.com>
[Grabbed from the branch, LINUX_IR5.2.0, of
https://git.freescale.com/git-private/cgit.cgi/ppc/alu-b4860/linux.git.]
Signed-off-by: Tiejun Chen <tiejun.chen@windriver.com>
---
 drivers/staging/fsl_dpa_offload/dpa_classifier.c |  628 +++++++++++-----------
 drivers/staging/fsl_dpa_offload/dpa_classifier.h |   72 ++--
 include/linux/fsl_dpa_classifier.h               |    2 +-
 3 files changed, 345 insertions(+), 357 deletions(-)

diff --git a/drivers/staging/fsl_dpa_offload/dpa_classifier.c b/drivers/staging/fsl_dpa_offload/dpa_classifier.c
index e742f36..6c4d8a3d 100644
--- a/drivers/staging/fsl_dpa_offload/dpa_classifier.c
+++ b/drivers/staging/fsl_dpa_offload/dpa_classifier.c
@@ -51,17 +51,6 @@
  */
 #define DPA_CLS_TBL_ARRAYSIZEGRANULARITY			10
 
-#define DPA_CLS_TBL_ENTRYIDNODEMASK				0xff0000
-#define DPA_CLS_TBL_ENTRYIDINDEXMASK				0x00ffff
-#define DPA_CLS_TBL_ENTRYIDNODESHIFT				16
-
-#define ENTRY_ID_GET_NODE(entry_id)				\
-	(((entry_id) & DPA_CLS_TBL_ENTRYIDNODEMASK) >>		\
-	DPA_CLS_TBL_ENTRYIDNODESHIFT)
-
-#define ENTRY_ID_GET_INDEX(entry_id)				\
-	((entry_id) & DPA_CLS_TBL_ENTRYIDINDEXMASK)
-
 
 /* Array for mapping table descriptors to table control structures */
 static struct dpa_cls_table **table;
@@ -245,6 +234,11 @@ int dpa_classif_table_free(int td)
 		break;
 	}
 
+	/* Free entry index management */
+	xx_free(ptable->entry);
+	ptable->entry		= NULL;
+	ptable->entries_cnt	= 0;
+
 	free_table_management(ptable);
 
 	xx_free(table[td]);
@@ -376,6 +370,7 @@ int dpa_classif_table_insert_entry(int				td,
 		err = table_insert_entry_exact_match(table[td],
 						key,
 						action,
+						priority,
 						entry_id);
 		break;
 #ifdef DPA_OFFLOAD_DEBUG
@@ -433,18 +428,18 @@ int dpa_classif_table_modify_entry_by_ref(int			td,
 	uint8_t key_data[DPA_CLS_TBL_MAXENTRYKEYSIZE];
 	uint8_t mask_data[DPA_CLS_TBL_MAXENTRYKEYSIZE];
 	uint8_t entry_index;
-	uint8_t shadow_table_index;
 	unsigned int cc_node_index;
 	int errno;
 	t_Error err;
 	struct dpa_cls_table *ptable;
 	t_Handle fm_pcd, cc_node;
-	uint8_t index;
-	struct list_head *bucket_head;
+	struct list_head *shadow_list_entry;
 
 	xx_sanity_check_return_value(((td >= 0) && (td < num_tables)), "td",
 		-EINVAL);
 	xx_sanity_check_return_value(table[td], "td", -EINVAL);
+	xx_sanity_check_return_value(((entry_id >= 0) &&
+		(entry_id < table[td]->entries_cnt)), "entry_id", -EINVAL);
 	xx_sanity_check_return_value(mod_params, "mod_params", -EINVAL);
 
 	/* Check for unsupported modifications */
@@ -454,26 +449,9 @@ int dpa_classif_table_modify_entry_by_ref(int			td,
 		return -ENOSYS;
 	}
 
-	ptable = table[td];
-	if (ptable->params.type == DPA_CLS_TBL_INDEXED) {
-		cc_node_index	= 0;
-		entry_index	= (uint8_t)entry_id;
-		index		= entry_index;
-	} else {
-		cc_node_index	= ENTRY_ID_GET_NODE(entry_id);
-		entry_index	= (uint8_t)ENTRY_ID_GET_INDEX(entry_id);
-		index		= ptable->int_cc_node[cc_node_index].
-					entry[entry_index].entry_index;
-	}
-
-	xx_sanity_check_return_value((cc_node_index <
-				ptable->int_cc_nodes_count),
-			"entry_id",
-			-EINVAL);
-	xx_sanity_check_return_value((entry_index <
-				ptable->int_cc_node[cc_node_index].table_size),
-			"entry_id",
-			-EINVAL);
+	ptable		= table[td];
+	cc_node_index	= ptable->entry[entry_id].int_cc_node_index;
+	entry_index	= ptable->entry[entry_id].entry_index;
 
 	fm_pcd	= (t_Handle)ptable->params.fm_pcd;
 	cc_node	= (t_Handle)ptable->int_cc_node[cc_node_index].cc_node;
@@ -486,7 +464,7 @@ int dpa_classif_table_modify_entry_by_ref(int			td,
 
 		err = FM_PCD_CcNodeModifyNextEngine(fm_pcd,
 						cc_node,
-						index,
+						entry_index,
 						&next_engine_params);
 		if (err != E_OK) {
 			xx_pr_fmd_err(err, "FM_PCD_CcNodeModifyNextEngine");
@@ -511,7 +489,7 @@ int dpa_classif_table_modify_entry_by_ref(int			td,
 
 		err = FM_PCD_CcNodeModifyKey(fm_pcd,
 				cc_node,
-				index,
+				entry_index,
 				ptable->params.exact_match_params.key_size,
 				key_data,
 				mask_data);
@@ -524,7 +502,6 @@ int dpa_classif_table_modify_entry_by_ref(int			td,
 		break;
 	case DPA_CLS_TBL_MODIFY_KEY_AND_ACTION:
 		/* Only exact match tables support this type of modification. */
-
 		errno = action_to_next_engine_params(mod_params->action,
 						&key_params.ccNextEngineParams);
 		if (errno < 0)
@@ -547,7 +524,7 @@ int dpa_classif_table_modify_entry_by_ref(int			td,
 
 		err = FM_PCD_CcNodeModifyKeyAndNextEngine(fm_pcd,
 				cc_node,
-				index,
+				entry_index,
 				ptable->params.exact_match_params.key_size,
 				&key_params);
 		if (err != E_OK) {
@@ -564,10 +541,10 @@ int dpa_classif_table_modify_entry_by_ref(int			td,
 	if (ptable->shadow_table) {
 		if (ptable->params.type == DPA_CLS_TBL_INDEXED) {
 
-			bucket_head =
+			shadow_list_entry =
 				ptable->shadow_table->
 					shadow_entry[entry_index].next;
-			shadow_entry_indexed = list_entry(bucket_head,
+			shadow_entry_indexed = list_entry(shadow_list_entry,
 				struct dpa_cls_tbl_shadow_entry_indexed,
 				list_node);
 
@@ -575,32 +552,14 @@ int dpa_classif_table_modify_entry_by_ref(int			td,
 			action	= &shadow_entry_indexed->action;
 		} else {
 
-			bucket_head =
-				ptable->int_cc_node[cc_node_index].
-					entry[entry_index].shadow_entry;
-			shadow_entry = list_entry(bucket_head,
+			shadow_list_entry =
+					ptable->entry[entry_id].shadow_entry;
+			shadow_entry = list_entry(shadow_list_entry,
 					struct dpa_cls_tbl_shadow_entry,
 					list_node);
 
 			key	= &shadow_entry->key;
 			action	= &shadow_entry->action;
-
-			/*
-			 * The entry needs to be re-hashed and relocated
-			 * according to the new key
-			 */
-
-			/* Detach from current bucket: */
-			list_del(&shadow_entry->list_node);
-			/* Recompute bucket: */
-			shadow_table_index = crc8(
-				mod_params->key->byte,
-				ptable->params.exact_match_params.key_size);
-			/* Add to the new bucket: */
-			list_add_tail(
-				&shadow_entry->list_node,
-				&ptable->shadow_table->
-					shadow_entry[shadow_table_index]);
 		}
 
 		if ((mod_params->type == DPA_CLS_TBL_MODIFY_KEY) ||
@@ -647,36 +606,22 @@ int dpa_classif_table_delete_entry_by_ref(int td, int entry_id)
 	struct dpa_cls_tbl_shadow_entry *shadow_entry;
 	struct dpa_cls_tbl_shadow_entry_indexed *shadow_entry_indexed;
 	uint8_t entry_index;
-	unsigned int i, cc_node_index;
+	unsigned int cc_node_index;
 	struct dpa_cls_table *ptable;
 	t_Handle fm_pcd, cc_node;
-	uint8_t index;
-	struct list_head *bucket_head;
+	struct list_head *shadow_list_entry, *current;
 	struct dpa_cls_tbl_cc_node_info *int_cc_node;
+	struct dpa_cls_tbl_entry *index_entry;
 
 	xx_sanity_check_return_value(((td >= 0) && (td < num_tables)), "td",
 		-EINVAL);
 	xx_sanity_check_return_value(table[td], "td", -EINVAL);
+	xx_sanity_check_return_value(((entry_id >= 0) &&
+		(entry_id < table[td]->entries_cnt)), "entry_id", -EINVAL);
 
-	ptable = table[td];
-
-	if (ptable->params.type == DPA_CLS_TBL_INDEXED) {
-		cc_node_index	= 0;
-		entry_index	= (uint8_t)entry_id;
-	} else {
-		cc_node_index	= ENTRY_ID_GET_NODE(entry_id);
-		entry_index	= (uint8_t)ENTRY_ID_GET_INDEX(entry_id);
-	}
-
-	xx_sanity_check_return_value((cc_node_index <
-				ptable->int_cc_nodes_count),
-			"entry_id",
-			-EINVAL);
-
-	xx_sanity_check_return_value((entry_index <
-				ptable->int_cc_node[cc_node_index].table_size),
-			"entry_id",
-			-EINVAL);
+	ptable		= table[td];
+	cc_node_index	= ptable->entry[entry_id].int_cc_node_index;
+	entry_index	= ptable->entry[entry_id].entry_index;
 
 	fm_pcd	= (t_Handle)ptable->params.fm_pcd;
 	cc_node	= (t_Handle)ptable->int_cc_node[cc_node_index].cc_node;
@@ -705,43 +650,43 @@ int dpa_classif_table_delete_entry_by_ref(int td, int entry_id)
 		/* No indexes updates are necessary for the indexed table */
 	} else {
 		/* For all the other tables types we can remove the key */
-
-		index = int_cc_node->entry[entry_index].entry_index;
 		err = FM_PCD_CcNodeRemoveKey(fm_pcd,
 					cc_node,
-					index);
+					entry_index);
 		if (err != E_OK) {
 			xx_pr_fmd_err(err, "FM_PCD_CcNodeRemoveKey");
 			xx_pr_err("FMan driver call failed.");
 			return -EBUSY;
 		}
 
-
 		/*
 		 * Update the index management for the Cc node that this entry
-		 * was removed from. The linked lists were allowing us to
-		 * process only the entries with higher indexes than the one
-		 * that was removed. Arrays are forcing us to process the
-		 * entire set of entries (because they are not sorted in any
-		 * way).
+		 * was removed from.
 		 */
-	for (i = 0; i < int_cc_node->table_size; i++) {
-		if ((int_cc_node->entry[i].valid) &&
-			(int_cc_node->entry[i].entry_index >
-				int_cc_node->entry[entry_index].entry_index))
-				int_cc_node->entry[i].entry_index--;
-	}
+		current = ptable->entry[entry_id].list_node.next;
+		while (current != &ptable->entry_list) {
+			index_entry = list_entry(current,
+					struct dpa_cls_tbl_entry,
+					list_node);
+			if (index_entry->int_cc_node_index >
+					cc_node_index)
+				break;
+			index_entry->entry_index--;
+			current = current->next;
+		}
 
-		int_cc_node->entry[entry_index].valid = 0;
+		list_del(&ptable->entry[entry_id].list_node);
 	}
+
+	ptable->entry[entry_id].valid = 0;
+
 	int_cc_node->used--;
 
 	if (ptable->shadow_table) {
 		if (ptable->params.type == DPA_CLS_TBL_INDEXED) {
-
-			bucket_head = ptable->shadow_table->
+			shadow_list_entry = ptable->shadow_table[0].
 					shadow_entry[entry_index].next;
-			shadow_entry_indexed = list_entry(bucket_head,
+			shadow_entry_indexed = list_entry(shadow_list_entry,
 					struct dpa_cls_tbl_shadow_entry_indexed,
 					list_node);
 
@@ -749,10 +694,9 @@ int dpa_classif_table_delete_entry_by_ref(int td, int entry_id)
 
 			xx_free(shadow_entry_indexed);
 		} else {
-
-			bucket_head = ptable->int_cc_node[cc_node_index].
-					entry[entry_index].shadow_entry,
-			shadow_entry = list_entry(bucket_head,
+			shadow_list_entry =
+					ptable->entry[entry_id].shadow_entry;
+			shadow_entry = list_entry(shadow_list_entry,
 						struct dpa_cls_tbl_shadow_entry,
 						list_node);
 
@@ -814,13 +758,13 @@ int dpa_classif_table_lookup_by_ref(int				td,
 {
 	struct dpa_cls_tbl_shadow_entry *shadow_entry;
 	struct dpa_cls_tbl_shadow_entry_indexed *shadow_entry_indexed;
-	unsigned int cc_node_index;
-	uint8_t entry_index;
-	struct list_head *bucket_head;
+	struct list_head *shadow_list_entry;
 
 	xx_sanity_check_return_value(((td >= 0) && (td < num_tables)), "td",
 		-EINVAL);
 	xx_sanity_check_return_value(table[td], "td", -EINVAL);
+	xx_sanity_check_return_value(((entry_id >= 0) &&
+		(entry_id < table[td]->entries_cnt)), "entry_id", -EINVAL);
 	xx_sanity_check_return_value(action, "action", -EINVAL);
 
 	if (!table[td]->shadow_table) {
@@ -829,38 +773,17 @@ int dpa_classif_table_lookup_by_ref(int				td,
 	}
 
 	if (table[td]->params.type == DPA_CLS_TBL_INDEXED) {
-		cc_node_index	= 0;
-		entry_index	= (uint8_t)entry_id;
-	} else {
-		cc_node_index	= ENTRY_ID_GET_NODE(entry_id);
-		entry_index	= (uint8_t)ENTRY_ID_GET_INDEX(entry_id);
-	}
-
-	xx_sanity_check_return_value((cc_node_index <
-			table[td]->int_cc_nodes_count),
-		"entry_id",
-		-EINVAL);
-
-	xx_sanity_check_return_value((entry_index <
-			table[td]->int_cc_node[cc_node_index].table_size),
-		"entry_id",
-		-EINVAL);
-
-	if (table[td]->params.type == DPA_CLS_TBL_INDEXED) {
-
-		bucket_head = table[td]->shadow_table->
-					shadow_entry[entry_index].next;
-		shadow_entry_indexed = list_entry(bucket_head,
+		shadow_list_entry = table[td]->shadow_table->
+					shadow_entry[entry_id].next;
+		shadow_entry_indexed = list_entry(shadow_list_entry,
 					struct dpa_cls_tbl_shadow_entry_indexed,
 					list_node);
 
 		memcpy(action, &shadow_entry_indexed->action,
 			sizeof(struct dpa_cls_tbl_action));
 	} else {
-
-		bucket_head = table[td]->int_cc_node[cc_node_index].
-					entry[entry_index].shadow_entry;
-		shadow_entry = list_entry(bucket_head,
+		shadow_list_entry = table[td]->entry[entry_id].shadow_entry;
+		shadow_entry = list_entry(shadow_list_entry,
 					struct dpa_cls_tbl_shadow_entry,
 					list_node);
 
@@ -877,19 +800,22 @@ int dpa_classif_table_flush(int td)
 	struct dpa_cls_tbl_shadow_entry *shadow_entry;
 	struct dpa_cls_tbl_shadow_entry_indexed *shadow_entry_indexed;
 	t_FmPcdCcNextEngineParams next_engine_params;
-	unsigned int i, j, k;
+	unsigned int cc_node_index;
 	t_Error err;
 	struct dpa_cls_table *ptable;
 	t_Handle fm_pcd, cc_node;
-	uint8_t index;
-	struct list_head *bucket_head;
+	struct list_head *current, *tmp;
 	struct dpa_cls_tbl_cc_node_info *int_cc_node;
-	struct dpa_cls_tbl_shadow_table *shadow_table;
+	struct dpa_cls_tbl_entry *index_entry;
 
 	xx_sanity_check_return_value(((td >= 0) && (td < num_tables)), "td",
 		-EINVAL);
 	xx_sanity_check_return_value(table[td], "td", -EINVAL);
 
+	if (list_empty(&table[td]->entry_list))
+		/* Table is already empty. Nothing to do */
+		return 0;
+
 	ptable = table[td];
 	fm_pcd = (t_Handle)ptable->params.fm_pcd;
 	if (ptable->params.type == DPA_CLS_TBL_INDEXED) {
@@ -901,28 +827,25 @@ int dpa_classif_table_flush(int td)
 		next_engine_params.params.enqueueParams.statisticsEn = FALSE;
 
 		cc_node = (t_Handle)ptable->int_cc_node[0].cc_node;
-		for (i = 0; i < ptable->int_cc_node[0].table_size; i++) {
+		list_for_each_entry(index_entry,
+				&ptable->entry_list,
+				list_node) {
 			err = FM_PCD_CcNodeModifyNextEngine(fm_pcd,
-							cc_node,
-							(uint8_t)i,
-							&next_engine_params);
+						cc_node,
+						index_entry->entry_index,
+						&next_engine_params);
 			if (err != E_OK) {
 				xx_pr_fmd_err(err,
 					"FM_PCD_CcNodeModifyNextEngine");
 				xx_pr_err("FMan driver call failed.");
 				return -EBUSY;
 			}
-		}
-		ptable->int_cc_node[0].used = 0;
+			index_entry->valid = 0;
 
-	/* Clean up shadow table if it exists */
-	if (ptable->shadow_table) {
-		for (i = 0; i < ptable->int_cc_node[0].table_size; i++) {
-			shadow_table = ptable->shadow_table;
-			if (!list_empty(&shadow_table->shadow_entry[i])) {
+			/* Clean up associated shadow entry if it exists */
+			if (index_entry->shadow_entry) {
 				shadow_entry_indexed =
-					list_entry(shadow_table->
-				shadow_entry[i].next,
+					list_entry(index_entry->shadow_entry,
 				struct dpa_cls_tbl_shadow_entry_indexed,
 				list_node);
 
@@ -930,64 +853,47 @@ int dpa_classif_table_flush(int td)
 				xx_free(shadow_entry_indexed);
 			}
 		}
-	}
+		INIT_LIST_HEAD(&ptable->entry_list);
+		ptable->int_cc_node[0].used = 0;
 	} else {
-		for (i = 0; i < ptable->int_cc_nodes_count; i++) {
-			cc_node = (t_Handle)ptable->int_cc_node[i].cc_node;
-			int_cc_node = &ptable->int_cc_node[i];
-			for (j = 0; j < int_cc_node->table_size; j++) {
-				if (int_cc_node->entry[j].valid) {
-					if (int_cc_node->entry[j].
-						shadow_entry) {
-						/*
-						 * Clean up shadow
-						 * entry as well
-						 */
-						bucket_head =
-					int_cc_node->entry[j].shadow_entry;
-
-						shadow_entry =
-							list_entry(bucket_head,
-					struct dpa_cls_tbl_shadow_entry,
-					list_node);
-
-						list_del(&shadow_entry->
-								list_node);
+		/* Flush the table from tail to head to avoid having to update
+		 * the remaining entry indexes all the time */
+		current = ptable->entry_list.prev;
+		while (current != &ptable->entry_list) {
+			index_entry = list_entry(current,
+						struct dpa_cls_tbl_entry,
+						list_node);
+			if (index_entry->shadow_entry) {
+				/* Clean up shadow entry as well */
+				shadow_entry =
+					list_entry(index_entry->shadow_entry,
+				struct dpa_cls_tbl_shadow_entry,
+				list_node);
 
-						xx_free(shadow_entry);
-					}
+				list_del(&shadow_entry->list_node);
 
-				index = int_cc_node->entry[j].entry_index;
-				err = FM_PCD_CcNodeRemoveKey(fm_pcd,
-							cc_node,
-							index);
-				if (err != E_OK) {
-					xx_pr_fmd_err(err, "FM_PCD_CcNodeRemoveKey");
-					xx_pr_err("FMan driver call failed.");
-					return -EBUSY;
-				}
+				xx_free(shadow_entry);
+			}
 
-				int_cc_node->used--;
-				int_cc_node->entry[j].valid = 0;
+			cc_node_index = index_entry->int_cc_node_index;
+			cc_node = (t_Handle)ptable->int_cc_node[cc_node_index].
+								cc_node;
+			int_cc_node = &ptable->int_cc_node[cc_node_index];
 
-		/*
-		 * Update the index management for the Cc node that this entry
-		 * was removed from. The linked lists were allowing us to
-		 * process only the entries with higher indexes than the one
-		 * that was removed. Arrays are forcing us to process the
-		 * entire set of entries (because they are not sorted in any
-		 * way).
-		 */
-		for (k = 0; k < int_cc_node->table_size; k++) {
-			if ((int_cc_node->entry[k].valid) &&
-				(int_cc_node->entry[k].entry_index >
-				int_cc_node->entry[j].entry_index))
-					int_cc_node->entry[k].entry_index--;
-		}
-		}
-		}
+			err = FM_PCD_CcNodeRemoveKey(fm_pcd,
+						cc_node,
+						index_entry->entry_index);
+			if (err != E_OK) {
+				xx_pr_fmd_err(err, "FM_PCD_CcNodeRemoveKey");
+				xx_pr_err("FMan driver call failed.");
+				return -EBUSY;
+			}
 
-		xx_assert(int_cc_node->used == 0);
+			int_cc_node->used--;
+			index_entry->valid = 0;
+			tmp	= current;
+			current	= current->prev;
+			list_del(tmp);
 		}
 	}
 
@@ -1035,32 +941,20 @@ int dpa_classif_table_get_entry_stats_by_ref(int		td,
 	xx_sanity_check_return_value(((td >= 0) && (td < num_tables)), "td",
 		-EINVAL);
 	xx_sanity_check_return_value(table[td], "td", -EINVAL);
+	xx_sanity_check_return_value(((entry_id >= 0) &&
+		(entry_id < table[td]->entries_cnt)), "entry_id", -EINVAL);
 	xx_sanity_check_return_value(stats, "stats", -EINVAL);
 
-	ptable = table[td];
-	if (ptable->params.type == DPA_CLS_TBL_INDEXED) {
-		cc_node_index	= 0;
-		entry_index	= (uint8_t)entry_id;
-	} else {
-		cc_node_index	= ENTRY_ID_GET_NODE(entry_id);
-		entry_index	= (uint8_t)ENTRY_ID_GET_INDEX(entry_id);
-	}
-
-	xx_sanity_check_return_value((cc_node_index <
-			ptable->int_cc_nodes_count),
-		"entry_id",
-		-EINVAL);
-	xx_sanity_check_return_value((entry_index <
-			ptable->int_cc_node[cc_node_index].table_size),
-		"entry_id",
-		-EINVAL);
+	ptable		= table[td];
+	cc_node_index	= ptable->entry[entry_id].int_cc_node_index;
+	index_entry	= &ptable->entry[entry_id];
+	entry_index	= index_entry->entry_index;
 
 	fm_pcd = (t_Handle)ptable->params.fm_pcd;
 	cc_node = (t_Handle)ptable->int_cc_node[cc_node_index].cc_node;
-	index_entry = &ptable->int_cc_node[cc_node_index].entry[entry_index];
 	stats->total_pkts = (unsigned long) FM_PCD_CcNodeGetKeyCounter(fm_pcd,
 						cc_node,
-						index_entry->entry_index);
+						entry_index);
 
 	if (stats->total_pkts < index_entry->last_stats.total_pkts)
 		/* Roll over */
@@ -1110,27 +1004,15 @@ int dpa_classif_table_reset_entry_stats_by_ref(int		td,
 	xx_sanity_check_return_value(((td >= 0) && (td < num_tables)), "td",
 		-EINVAL);
 	xx_sanity_check_return_value(table[td], "td", -EINVAL);
+	xx_sanity_check_return_value(((entry_id >= 0) &&
+		(entry_id < table[td]->entries_cnt)), "entry_id", -EINVAL);
 
-	if (table[td]->params.type == DPA_CLS_TBL_INDEXED) {
-		cc_node_index	= 0;
-		entry_index	= (uint8_t)entry_id;
-	} else {
-		cc_node_index	= ENTRY_ID_GET_NODE(entry_id);
-		entry_index	= (uint8_t)ENTRY_ID_GET_INDEX(entry_id);
-	}
-
-	xx_sanity_check_return_value((cc_node_index <
-			table[td]->int_cc_nodes_count),
-		"entry_id",
-		-EINVAL);
-	xx_sanity_check_return_value((entry_index <
-			table[td]->int_cc_node[cc_node_index].table_size),
-		"entry_id",
-		-EINVAL);
+	cc_node_index	= table[td]->entry[entry_id].int_cc_node_index;
+	index_entry	= &table[td]->entry[entry_id];
+	entry_index	= index_entry->entry_index;
 
 	fm_pcd = (t_Handle)table[td]->params.fm_pcd;
 	cc_node = (t_Handle)table[td]->int_cc_node[cc_node_index].cc_node;
-	index_entry = &table[td]->int_cc_node[cc_node_index].entry[entry_index];
 	index_entry->last_stats.total_pkts =
 		(unsigned long) FM_PCD_CcNodeGetKeyCounter(fm_pcd,
 						cc_node,
@@ -1180,17 +1062,8 @@ alloc_table_mgmt_error:
 
 static void free_table_management(struct dpa_cls_table *cls_table)
 {
-	unsigned int i;
-
 	xx_assert(cls_table);
 
-	if (cls_table->params.type != DPA_CLS_TBL_INDEXED)
-		/* Release index management */
-		for (i = 0; i < cls_table->int_cc_nodes_count; i++) {
-			if (cls_table->int_cc_node[i].entry)
-				xx_free(cls_table->int_cc_node[i].entry);
-		}
-
 	if (cls_table->int_cc_node)
 		xx_free(cls_table->int_cc_node);
 
@@ -1207,6 +1080,7 @@ static int table_init_indexed(struct dpa_cls_table *cls_table)
 
 	xx_assert(cls_table);
 	xx_assert(cls_table->params.type == DPA_CLS_TBL_INDEXED);
+	xx_assert(cls_table->int_cc_nodes_count == 1);
 
 	errno = alloc_table_management(cls_table);
 	if (errno < 0)
@@ -1237,13 +1111,16 @@ static int table_init_indexed(struct dpa_cls_table *cls_table)
 	}
 
 	/* Allocate the index management array */
-	cls_table->int_cc_node[0].entry = (struct dpa_cls_tbl_entry *)
-		xx_zalloc(cls_table->int_cc_node[0].table_size *
+	cls_table->entries_cnt = cls_table->params.indexed_params.entries_cnt;
+	cls_table->entry = (struct dpa_cls_tbl_entry *)
+		xx_zalloc(cls_table->entries_cnt *
 			sizeof(struct dpa_cls_tbl_entry));
-	if (!cls_table->int_cc_node[0].entry) {
+	if (!cls_table->entry) {
 		xx_pr_err("No more memory for DPA Classifier table index management.");
+		cls_table->entries_cnt = 0;
 		return -ENOMEM;
 	}
+	INIT_LIST_HEAD(&cls_table->entry_list);
 
 	return 0;
 }
@@ -1323,17 +1200,21 @@ static int table_init_hash(struct dpa_cls_table *cls_table)
 			err = -EBUSY;
 			goto table_init_hash_error;
 		}
+	}
 
-		/* Allocate the index management array */
-		cls_table->int_cc_node[i].entry = (struct dpa_cls_tbl_entry *)
-			xx_zalloc(cls_table->int_cc_node[i].table_size *
-				sizeof(struct dpa_cls_tbl_entry));
-		if (!cls_table->int_cc_node[i].entry) {
-			xx_pr_err("No more memory for DPA Classifier table index management.");
-			err = -ENOMEM;
-			goto table_init_hash_error;
-		}
+	/* Allocate the index management array */
+	cls_table->entries_cnt = cls_table->params.hash_params.num_sets *
+		cls_table->params.hash_params.max_ways;
+	cls_table->entry = (struct dpa_cls_tbl_entry *)
+		xx_zalloc(cls_table->entries_cnt *
+			sizeof(struct dpa_cls_tbl_entry));
+	if (!cls_table->entry) {
+		xx_pr_err("No more memory for DPA Classifier table index management.");
+		cls_table->entries_cnt	= 0;
+		err			= -ENOMEM;
+		goto table_init_hash_error;
 	}
+	INIT_LIST_HEAD(&cls_table->entry_list);
 
 	cls_table->hash_mask =
 		(uint64_t)(cls_table->params.hash_params.num_sets - 1) <<
@@ -1422,17 +1303,21 @@ static int table_init_exact_match(struct dpa_cls_table *cls_table)
 			cls_table->int_cc_node[i].table_size =
 			cls_table->params.exact_match_params.entries_cnt -
 			(i * FM_PCD_MAX_NUM_OF_KEYS);
+	}
 
-		/* Allocate the index management array */
-		cls_table->int_cc_node[i].entry = (struct dpa_cls_tbl_entry *)
-			xx_zalloc(cls_table->int_cc_node[i].table_size *
+	/* Allocate the index management array */
+	cls_table->entries_cnt =
+		cls_table->params.exact_match_params.entries_cnt;
+	cls_table->entry = (struct dpa_cls_tbl_entry *)
+		xx_zalloc(cls_table->entries_cnt *
 			sizeof(struct dpa_cls_tbl_entry));
-		if (!cls_table->int_cc_node[i].entry) {
-			xx_pr_err("No more memory for DPA Classifier table index management.");
-			err = -ENOMEM;
-			goto table_init_exact_match_error;
-		}
+	if (!cls_table->entry) {
+		xx_pr_err("No more memory for DPA Classifier table index management.");
+		cls_table->entries_cnt	= 0;
+		err			= -ENOMEM;
+		goto table_init_exact_match_error;
 	}
+	INIT_LIST_HEAD(&cls_table->entry_list);
 
 	xx_free(cc_node_params);
 
@@ -1561,8 +1446,11 @@ static int verify_table_params(const struct dpa_cls_tbl_params *params)
 			break;
 		}
 
-		if (params->exact_match_params.use_priorities) {
-			xx_pr_err("Entry priorities for exact match tables are not yet supported.");
+		if ((params->exact_match_params.use_priorities) &&
+				(params->exact_match_params.entries_cnt >
+				FM_PCD_MAX_NUM_OF_KEYS)) {
+			xx_pr_err("Entry priorities for exact match tables are supported only for small tables (<= %d entries).",
+				FM_PCD_MAX_NUM_OF_KEYS);
 			err = -ENOSYS;
 			break;
 		}
@@ -1711,7 +1599,7 @@ static int table_insert_entry_indexed(struct dpa_cls_table	*cls_table,
 	xx_assert(cls_table->params.type == DPA_CLS_TBL_INDEXED);
 
 	/* Check the index management array if the entry is already used */
-	if (cls_table->int_cc_node[0].entry[key->byte[0]].valid)
+	if (cls_table->entry[key->byte[0]].valid)
 		return -EEXIST;
 
 	memset(&next_engine_params, 0 , sizeof(next_engine_params));
@@ -1757,11 +1645,15 @@ static int table_insert_entry_indexed(struct dpa_cls_table	*cls_table,
 	}
 
 	/* Clean up and prepare the index entry */
-	memset(&cls_table->int_cc_node[0].entry[key->byte[0]], 0,
+	memset(&cls_table->entry[key->byte[0]], 0,
 		sizeof(struct dpa_cls_tbl_entry));
-	cls_table->int_cc_node[0].entry[key->byte[0]].valid = 1;
-	cls_table->int_cc_node[0].entry[key->byte[0]].entry_index =
-		key->byte[0];
+	cls_table->entry[key->byte[0]].valid		= 1;
+	cls_table->entry[key->byte[0]].entry_index	= key->byte[0];
+
+	/*
+	 * Index management list is not necessary to be updated for indexed
+	 * tables.
+	 */
 
 	if (entry_id)
 		*entry_id = key->byte[0];
@@ -1778,20 +1670,22 @@ table_insert_entry_indexed_error:
 static int table_insert_entry_exact_match(struct dpa_cls_table	*cls_table,
 				const struct dpa_cls_tbl_key	*key,
 				const struct dpa_cls_tbl_action	*action,
+				int				priority,
 				int				*entry_id)
 {
 	t_Error err;
 	int errno = 0;
-	int id;
 	struct dpa_cls_tbl_shadow_entry *shadow_entry = NULL;
 	t_FmPcdCcKeyParams key_params;
 	t_FmPcdCcNextEngineParams cc_miss_engine_params;
-	unsigned int i, j;
+	int i = 0, j, k;
 	uint8_t shadow_table_index;
 	uint8_t key_data[DPA_CLS_TBL_MAXENTRYKEYSIZE];
 	uint8_t mask_data[DPA_CLS_TBL_MAXENTRYKEYSIZE];
 	t_Handle fm_pcd;
 	struct dpa_cls_tbl_shadow_table *shadow_table;
+	struct dpa_cls_tbl_entry *index_entry;
+	struct list_head *current;
 
 	xx_assert(cls_table);
 	xx_assert(key);
@@ -1817,13 +1711,84 @@ static int table_insert_entry_exact_match(struct dpa_cls_table	*cls_table,
 	if (errno < 0)
 		goto table_insert_entry_exact_match_error;
 
-	/* Find an empty spot for this entry, in the existing Cc nodes */
-	i = 0;
+	/* Find an empty spot for this entry, at the end of an existing Cc
+	 * node */
 	while ((i < cls_table->int_cc_nodes_count) &&
 			(cls_table->int_cc_node[i].used >=
 			cls_table->int_cc_node[i].table_size)) {
 		i++;
 	}
+	if (i >= cls_table->int_cc_nodes_count) {
+		/* No more space to add a new entry */
+		xx_pr_err("DPA Classifier exact match table is full. Unable to add a new entry.");
+		errno = -ENOSPC;
+		goto table_insert_entry_exact_match_error;
+	}
+
+	/* Find an empty index management entry */
+	for (k = 0; k < cls_table->entries_cnt; k++)
+		if (!cls_table->entry[k].valid)
+			break;
+
+	xx_assert(k < cls_table->entries_cnt);
+
+	/* Clean up and prepare the index entry */
+	memset(&cls_table->entry[k], 0,
+		sizeof(struct dpa_cls_tbl_entry));
+	cls_table->entry[k].int_cc_node_index	= (unsigned)i;
+	cls_table->entry[k].priority		= priority;
+	cls_table->entry[k].entry_index =
+				(uint8_t)cls_table->int_cc_node[i].used;
+
+	/* Calculate the position in the index management list where this entry
+	 * should go */
+	if (list_empty(&cls_table->entry_list))
+		/* List is empty. Just add to its tail. */
+		current = &cls_table->entry_list;
+	else {
+		if (cls_table->params.exact_match_params.use_priorities) {
+			xx_assert(cls_table->entry[k].int_cc_node_index == 0);
+
+			/*
+			 * Have to recalculate the position of this entry based
+			 * on its priority.
+			 */
+			/*
+			 * Find the first entry with a priority value which is
+			 * higher than or equal to the one to add
+			 */
+			list_for_each(current, &cls_table->entry_list) {
+				index_entry = list_entry(current,
+						struct dpa_cls_tbl_entry,
+						list_node);
+				if (index_entry->priority >= priority)
+					break;
+			}
+			/* If there are such entries in the list */
+			if (current != &cls_table->entry_list) {
+				/* Shall add this entry in the position of
+				 * the [current] one */
+				cls_table->entry[k].entry_index =
+					index_entry->entry_index;
+			}
+			/* Otherwise let the entry be added at the end of the
+			 * table */
+		} else
+			/* If priorities are not used, sort the index
+			 * management list based on [cc_node_index] and
+			 * [entry_index]. In other words, add the current entry
+			 * before the first entry of the next cc node */
+			if (i < cls_table->int_cc_nodes_count - 1)
+				list_for_each(current, &cls_table->entry_list) {
+					index_entry = list_entry(current,
+						struct dpa_cls_tbl_entry,
+						list_node);
+					if (index_entry->int_cc_node_index > i)
+						break;
+				}
+			else
+				current = &cls_table->entry_list;
+	}
 
 	fm_pcd = (t_Handle)cls_table->params.fm_pcd;
 	if ((i > 0) && (!cls_table->int_cc_node[i].used)) {
@@ -1870,33 +1835,10 @@ static int table_insert_entry_exact_match(struct dpa_cls_table	*cls_table,
 		}
 	}
 
-	if (i > cls_table->int_cc_nodes_count) {
-		/* No more space to add a new entry */
-		xx_pr_err("DPA Classifier exact match table is full. Unable to add a new entry.");
-		errno = -ENOSPC;
-		goto table_insert_entry_exact_match_error;
-	}
-
-	/* Find an empty index entry */
-	for (j = 0; j < cls_table->int_cc_node[i].table_size; j++)
-		if (!cls_table->int_cc_node[i].entry[j].valid)
-			break;
-
-	xx_assert(j < cls_table->int_cc_node[i].table_size);
-
-	/* Clean up and prepare the index entry */
-	memset(&cls_table->int_cc_node[i].entry[j], 0,
-		sizeof(struct dpa_cls_tbl_entry));
-	cls_table->int_cc_node[i].entry[j].valid = 1;
-	cls_table->int_cc_node[i].entry[j].entry_index =
-		(uint8_t)cls_table->int_cc_node[i].used;
-
-	id = (i << DPA_CLS_TBL_ENTRYIDNODESHIFT) + j;
-
 	/* Add the key to the selected Cc node */
 	err = FM_PCD_CcNodeAddKey(fm_pcd,
 		(t_Handle)cls_table->int_cc_node[i].cc_node,
-		cls_table->int_cc_node[i].entry[j].entry_index,
+		cls_table->entry[k].entry_index,
 		cls_table->params.exact_match_params.key_size,
 		&key_params);
 	if (err != E_OK) {
@@ -1906,6 +1848,25 @@ static int table_insert_entry_exact_match(struct dpa_cls_table	*cls_table,
 		goto table_insert_entry_exact_match_error;
 	}
 
+	/* Add the index entry to the index management list */
+	list_add_tail(&cls_table->entry[k].list_node, current);
+
+	cls_table->entry[k].valid = 1;
+
+	/* Increment all entry indexes in the current cc node starting from
+	 * [current] on */
+	while (current != &cls_table->entry_list) {
+		index_entry = list_entry(current,
+					struct dpa_cls_tbl_entry,
+					list_node);
+		if (index_entry->int_cc_node_index != i)
+			break;
+
+		index_entry->entry_index++;
+
+		current = current->next;
+	}
+
 	cls_table->int_cc_node[i].used++;
 
 	/* If shadow table exists, add the entry to them */
@@ -1924,9 +1885,8 @@ static int table_insert_entry_exact_match(struct dpa_cls_table	*cls_table,
 			sizeof(struct dpa_cls_tbl_key));
 
 		/* Connect index management entry with the shadow table entry */
-		shadow_entry->entry_id = id;
-		cls_table->int_cc_node[i].entry[j].shadow_entry =
-			&shadow_entry->list_node;
+		shadow_entry->entry_id = k;
+		cls_table->entry[k].shadow_entry = &shadow_entry->list_node;
 
 		/* Add entry to the proper shadow table. */
 		key_apply_mask(key, key_data,
@@ -1939,7 +1899,7 @@ static int table_insert_entry_exact_match(struct dpa_cls_table	*cls_table,
 	}
 
 	if (entry_id)
-		*entry_id = id;
+		*entry_id = k;
 
 	return errno;
 
@@ -1957,15 +1917,16 @@ static int table_insert_entry_hash(struct dpa_cls_table		*cls_table,
 {
 	t_Error err;
 	int errno = 0;
-	int id;
 	struct dpa_cls_tbl_shadow_entry *shadow_entry = NULL;
 	t_FmPcdCcKeyParams key_params;
 	uint8_t shadow_table_index;
 	uint64_t hash_set_index;
 	uint8_t key_data[DPA_CLS_TBL_MAXENTRYKEYSIZE];
 	uint8_t mask_data[DPA_CLS_TBL_MAXENTRYKEYSIZE];
-	unsigned int j;
+	int j;
 	struct dpa_cls_tbl_shadow_table *shadow_table;
+	struct list_head *current;
+	struct dpa_cls_tbl_entry *index_entry;
 
 	xx_assert(cls_table);
 	xx_assert(key);
@@ -2001,31 +1962,49 @@ static int table_insert_entry_hash(struct dpa_cls_table		*cls_table,
 	/* Check if there are entries still available in the selected set */
 	if (cls_table->int_cc_node[hash_set_index].used >=
 			cls_table->int_cc_node[hash_set_index].table_size) {
-		xx_pr_err("DPA Classifier hash table is full. Unable to add a new entry.");
+		xx_pr_err("Hash set is full. Unable to add this entry.");
 		errno = -ENOSPC;
 		goto table_insert_entry_hash_error;
 	}
 
 	/* Find an empty index entry */
-	for (j = 0; j < cls_table->int_cc_node[hash_set_index].table_size; j++)
-		if (!cls_table->int_cc_node[hash_set_index].entry[j].valid)
+	for (j = 0; j < cls_table->entries_cnt; j++)
+		if (!cls_table->entry[j].valid)
 			break;
 
-	xx_assert(j < cls_table->int_cc_node[hash_set_index].table_size);
+	xx_assert(j < cls_table->entries_cnt);
 
 	/* Clean up and prepare the index entry */
-	memset(&cls_table->int_cc_node[hash_set_index].entry[j], 0,
+	memset(&cls_table->entry[j], 0,
 		sizeof(struct dpa_cls_tbl_entry));
-	cls_table->int_cc_node[hash_set_index].entry[j].valid = 1;
-	cls_table->int_cc_node[hash_set_index].entry[j].entry_index =
+	cls_table->entry[j].valid = 1;
+	cls_table->entry[j].int_cc_node_index = (unsigned int)hash_set_index;
+	cls_table->entry[j].entry_index =
 			(uint8_t)cls_table->int_cc_node[hash_set_index].used;
 
-	id = ((int)hash_set_index << DPA_CLS_TBL_ENTRYIDNODESHIFT) + j;
+	/* Calculate the position in the index management list where this entry
+	 * should go */
+	if ((list_empty(&cls_table->entry_list)) ||
+		(hash_set_index >= cls_table->int_cc_nodes_count - 1))
+		/* Just add to the tail of the list. */
+		current = &cls_table->entry_list;
+	else {
+		/* Sort the index management list based on [cc_node_index] and
+		 * [entry_index]. In other words, add the current entry
+		 * before the first entry of the next cc node */
+		list_for_each(current, &cls_table->entry_list) {
+			index_entry = list_entry(current,
+						struct dpa_cls_tbl_entry,
+						list_node);
+			if (index_entry->int_cc_node_index > hash_set_index)
+				break;
+		}
+	}
 
 	/* Add the key to the selected Cc node */
 	err = FM_PCD_CcNodeAddKey((t_Handle)cls_table->params.fm_pcd,
 		(t_Handle)cls_table->int_cc_node[hash_set_index].cc_node,
-		cls_table->int_cc_node[hash_set_index].entry[j].entry_index,
+		cls_table->entry[j].entry_index,
 		cls_table->params.hash_params.key_size,
 		&key_params);
 	if (err != E_OK) {
@@ -2035,6 +2014,9 @@ static int table_insert_entry_hash(struct dpa_cls_table		*cls_table,
 		goto table_insert_entry_hash_error;
 	}
 
+	/* Add the index entry to the index management list */
+	list_add_tail(&cls_table->entry[j].list_node, current);
+
 	cls_table->int_cc_node[hash_set_index].used++;
 
 	/* If shadow tables exist, add the entry to them */
@@ -2053,9 +2035,8 @@ static int table_insert_entry_hash(struct dpa_cls_table		*cls_table,
 			sizeof(struct dpa_cls_tbl_key));
 
 		/* Connect index management entry with the shadow table entry */
-		shadow_entry->entry_id = id;
-		cls_table->int_cc_node[hash_set_index].entry[j].shadow_entry =
-			&shadow_entry->list_node;
+		shadow_entry->entry_id = j;
+		cls_table->entry[j].shadow_entry = &shadow_entry->list_node;
 
 		/* Add entry to the proper shadow table. */
 		key_apply_mask(key, key_data,
@@ -2068,7 +2049,7 @@ static int table_insert_entry_hash(struct dpa_cls_table		*cls_table,
 	}
 
 	if (entry_id)
-		*entry_id = id;
+		*entry_id = j;
 
 	return errno;
 
@@ -2219,7 +2200,6 @@ static int get_new_table_descriptor(int *td)
 	return 0;
 }
 
-
 static inline void put_table_descriptor(int td)
 {
 	if (table[td] != NULL) {
diff --git a/drivers/staging/fsl_dpa_offload/dpa_classifier.h b/drivers/staging/fsl_dpa_offload/dpa_classifier.h
index 9e87861..df91d3f 100644
--- a/drivers/staging/fsl_dpa_offload/dpa_classifier.h
+++ b/drivers/staging/fsl_dpa_offload/dpa_classifier.h
@@ -57,37 +57,42 @@
  */
 #define DPA_CLS_TBL_MAXSHADOWTABLESIZE				256
 
-
 /* Index management entry */
 struct dpa_cls_tbl_entry {
 
-	/**
-	 * \brief	Nonzero if this entry is valid
-	 */
+	/* Nonzero if this entry is valid */
 	int				valid;
 
-	/**
-	 * \brief	The index of this entry in the Cc node table
-	 */
+	/* Internal Cc node index where this entry resides */
+	unsigned int			int_cc_node_index;
+
+	/* The index of this entry in the Cc node table */
 	uint8_t				entry_index;
 
-	/**
-	 * \brief	Pointer to the shadow entry (if there is one) associated
-	 *		with this index management entry
+	/* The priority value of this entry in the table */
+	int				priority;
+
+	/*
+	 * Pointer to the shadow entry (if there is one) associated
+	 * with this index management entry
 	 */
 	struct list_head		*shadow_entry;
 
-	/**
-	 * \brief	Last packet statistics for this entry.
-	 */
+	/* Last packet statistics for this entry. */
 	struct dpa_cls_tbl_entry_stats	stats;
 
-	/**
-	 * \brief	Last statistics counters provided by the low level
-	 *		driver. This is used for low level driver counter roll
-	 *		over protection.
+	/*
+	 * Last statistics counters provided by the low level
+	 * driver. This is used for low level driver counter roll
+	 * over protection.
 	 */
 	struct dpa_cls_tbl_entry_stats	last_stats;
+
+	/*
+	 * List node which allows linking this entry in the index
+	 * management list.
+	 */
+	struct list_head		list_node;
 };
 
 /*
@@ -132,27 +137,17 @@ struct dpa_cls_tbl_shadow_table {
 /* Internal FMan Cc Node Management Info */
 struct dpa_cls_tbl_cc_node_info {
 
-	/**
-	 * \brief	Low level driver (FMD) handle of the Cc node
-	 */
+	/* Low level driver (FMD) handle of the Cc node */
 	void				*cc_node;
 
-	/**
-	 * \brief	The size of this Cc node's lookup table
-	 */
+	/* The size of this Cc node's lookup table */
 	unsigned int			table_size;
 
-	/**
-	 * \brief	Number of entries in the lookup table that are
-	 *		currently in use
+	/*
+	 * Number of entries in the lookup table that are
+	 * currently in use
 	 */
 	unsigned int			used;
-
-	/**
-	 * \brief	Index management array. The size of this array is
-	 *		<i>table_size</i>
-	 */
-	struct dpa_cls_tbl_entry	*entry;
 };
 
 /* DPA Classifier Table Control Data Structure */
@@ -182,6 +177,18 @@ struct dpa_cls_table {
 	 */
 	uint64_t				hash_mask;
 
+	/* Index management array. */
+	struct dpa_cls_tbl_entry		*entry;
+
+	/* Number of entries in the index management array. */
+	unsigned int				entries_cnt;
+
+	/*
+	 * Linked list storing the index management entries
+	 * which are in use.
+	 */
+	struct list_head			entry_list;
+
 	/* (Initial) parameters of the DPA Classifier table. */
 	struct dpa_cls_tbl_params		params;
 
@@ -248,6 +255,7 @@ static int table_insert_entry_indexed(struct dpa_cls_table	*cls_table,
 static int table_insert_entry_exact_match(struct dpa_cls_table	*cls_table,
 				const struct dpa_cls_tbl_key	*key,
 				const struct dpa_cls_tbl_action	*action,
+				int				priority,
 				int				*entry_id);
 
 /* Add a new entry in a hash table. */
diff --git a/include/linux/fsl_dpa_classifier.h b/include/linux/fsl_dpa_classifier.h
index 9a76d6a..86d5a30 100644
--- a/include/linux/fsl_dpa_classifier.h
+++ b/include/linux/fsl_dpa_classifier.h
@@ -42,7 +42,7 @@
 #include "fsl_dpa_compat.h"
 
 /* Other includes */
-#include "linux/types.h"
+#include <linux/types.h>
 
 
 /* API functions, definitions and enums */
-- 
1.7.5.4

