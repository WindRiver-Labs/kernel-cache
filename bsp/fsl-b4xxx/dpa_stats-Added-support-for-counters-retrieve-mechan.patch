From b244b069a9122a0f56707259550f80208c6f38bf Mon Sep 17 00:00:00 2001
From: Anca-Jeanina Floarea <anca.floarea@freescale.com>
Date: Fri, 3 Aug 2012 22:58:58 +0000
Subject: [PATCH 259/518] dpa_stats: Added support for counters retrieve
 mechanism

Counters can be retrieved using a synchronous or an asynchronous
mechanism. The calling application is notified through a callback
when the operation has finished.

Signed-off-by: Anca Jeanina FLOAREA <anca.floarea@freescale.com>
[Grabbed from the branch, LINUX_IR5.2.0, of
https://git.freescale.com/git-private/cgit.cgi/ppc/alu-b4860/linux.git.]
Signed-off-by: Tiejun Chen <tiejun.chen@windriver.com>
---
 drivers/staging/fsl_dpa_offload/dpa_stats.c |  377 +++++++++++++++++++++++++++
 drivers/staging/fsl_dpa_offload/dpa_stats.h |   15 +
 include/linux/fsl_dpa_stats.h               |  100 +++++++
 3 files changed, 492 insertions(+), 0 deletions(-)

diff --git a/drivers/staging/fsl_dpa_offload/dpa_stats.c b/drivers/staging/fsl_dpa_offload/dpa_stats.c
index 22f423a..c24a1e0 100644
--- a/drivers/staging/fsl_dpa_offload/dpa_stats.c
+++ b/drivers/staging/fsl_dpa_offload/dpa_stats.c
@@ -117,6 +117,45 @@ static int get_new_cnt(struct dpa_stats *dpa_stats,
 	return 0;
 }
 
+static int get_new_req(struct dpa_stats *dpa_stats,
+		int *dpa_stats_req_id, struct dpa_stats_req_cb **req_cb)
+{
+	struct dpa_stats_req_cb *new_req;
+	uint32_t id;
+	int i;
+
+	/* Get an id for a new request */
+	if (cq_get_4bytes(dpa_stats->req_id_cq, &id) < 0) {
+		pr_err("No more unused counter ids\n");
+		return -EDOM;
+	}
+
+	/* Use 'used_req_ids' array in order to store requests ids that are
+	 * 'in use' . Array can be further used to remove requests */
+	for (i = 0; i < DPA_STATS_MAX_NUM_OF_REQUESTS; i++)
+		if (dpa_stats->used_req_ids[i] == DPA_OFFLD_INVALID_OBJECT_ID)
+			break;
+
+	if (i == DPA_STATS_MAX_NUM_OF_REQUESTS) {
+		pr_err("All requests have been used");
+		cq_put_4bytes(dpa_stats->req_id_cq, id);
+		return -EDOM;
+	}
+
+	/* Acquire a preallocated Request Control Block  */
+	new_req = &(dpa_stats->reqs_cb[id]);
+	new_req->req_id = id;
+	new_req->req_index = i;
+
+	/* Store on the current position the request id */
+	dpa_stats->used_req_ids[i] = id;
+
+	*req_cb = new_req;
+	*dpa_stats_req_id = id;
+
+	return 0;
+}
+
 static int put_cnt(struct dpa_stats *dpa_stats, struct dpa_stats_cnt_cb *cnt_cb)
 {
 	int err = 0;
@@ -139,6 +178,28 @@ static int put_cnt(struct dpa_stats *dpa_stats, struct dpa_stats_cnt_cb *cnt_cb)
 	return 0;
 }
 
+static int put_req(struct dpa_stats *dpa_stats, struct dpa_stats_req_cb *req_cb)
+{
+	int err = 0;
+
+	/* Release the Counter id in the Counter IDs circular queue */
+	err = cq_put_4bytes(dpa_stats->req_id_cq, req_cb->req_id);
+	if (err < 0) {
+		pr_err("Could not release the counter id %d\n", req_cb->req_id);
+		return -EDOM;
+	}
+
+	/* Mark the Counter id as 'not used' */
+	dpa_stats->used_req_ids[req_cb->req_index] =
+						DPA_OFFLD_INVALID_OBJECT_ID;
+
+	/* Clear all 'cnt_cb' information */
+	req_cb->req_index = DPA_OFFLD_INVALID_OBJECT_ID;
+	req_cb->req_id = DPA_OFFLD_INVALID_OBJECT_ID;
+
+	return 0;
+}
+
 static int init_cnts_cb(struct dpa_stats *dpa_stats)
 {
 	struct dpa_stats_params config = dpa_stats->config;
@@ -221,6 +282,85 @@ static int free_cnts_cb(struct dpa_stats *dpa_stats)
 	return 0;
 }
 
+static int init_reqs_cb(struct dpa_stats *dpa_stats)
+{
+	int i;
+
+	/* Create circular queue that holds free counter request IDs */
+	dpa_stats->req_id_cq = cq_new(
+			DPA_STATS_MAX_NUM_OF_REQUESTS, sizeof(int));
+	if (!dpa_stats->req_id_cq) {
+		pr_err("Could not create Request IDs circular queue\n");
+		return -ENOMEM;
+	}
+
+	/* Fill the circular queue with ids */
+	for (i = 0; i < DPA_STATS_MAX_NUM_OF_REQUESTS; i++) {
+		if (cq_put_4bytes(dpa_stats->req_id_cq, i) < 0) {
+			pr_err("Could not fill Request IDs circular queue\n");
+			return -EDOM;
+		}
+	}
+
+	/* Allocate array to store requests ids that are 'in use' */
+	dpa_stats->used_req_ids = kmalloc(DPA_STATS_MAX_NUM_OF_REQUESTS *
+			sizeof(uint32_t), GFP_KERNEL);
+	if (!dpa_stats->used_req_ids) {
+		pr_err("No more memory for used counter ids array\n");
+		return -ENOMEM;
+	}
+	memset(dpa_stats->used_req_ids, DPA_OFFLD_INVALID_OBJECT_ID,
+			DPA_STATS_MAX_NUM_OF_REQUESTS * sizeof(uint32_t));
+
+	/* Allocate array to store requests control blocks */
+	dpa_stats->reqs_cb = kzalloc(DPA_STATS_MAX_NUM_OF_REQUESTS *
+				sizeof(struct dpa_stats_req_cb), GFP_KERNEL);
+	if (!dpa_stats->reqs_cb) {
+		pr_err("No more memory for requests control blocks\n");
+		return -ENOMEM;
+	}
+
+	return 0;
+}
+
+static int free_reqs_cb(struct dpa_stats *dpa_stats)
+{
+	struct dpa_stats_req_cb *req_cb = NULL;
+	uint32_t id, i;
+	int err = 0;
+
+	for (i = 0; i <  DPA_STATS_MAX_NUM_OF_REQUESTS; i++) {
+		id = dpa_stats->used_req_ids[i];
+
+		if (id != DPA_OFFLD_INVALID_OBJECT_ID) {
+			req_cb = &(dpa_stats->reqs_cb[id]);
+
+			/* Release the request id in the Requests IDs cq */
+			err = put_req(dpa_stats, req_cb);
+			if (err < 0) {
+				pr_err("Failed to release a request id\n");
+				return err;
+			}
+		}
+	}
+
+	/* Release requests IDs circular queue */
+	if (dpa_stats->req_id_cq) {
+		cq_delete(dpa_stats->req_id_cq);
+		dpa_stats->req_id_cq = NULL;
+	}
+
+	/* Release requests control blocks */
+	kfree(dpa_stats->reqs_cb);
+	dpa_stats->reqs_cb = NULL;
+
+	/* Release requests 'used ids' array */
+	kfree(dpa_stats->used_req_ids);
+	dpa_stats->used_req_ids = NULL;
+
+	return 0;
+}
+
 /* cleanup DPA Stats */
 static void free_resources(void)
 {
@@ -236,10 +376,49 @@ static void free_resources(void)
 	/* free counters control blocks related stuff */
 	free_cnts_cb(dpa_stats);
 
+	/* free requests control blocks related stuff */
+	free_reqs_cb(dpa_stats);
+
+	/* destroy asynchronous requests workqueue */
+	if (dpa_stats->async_req_workqueue) {
+		destroy_workqueue(dpa_stats->async_req_workqueue);
+		dpa_stats->async_req_workqueue = NULL;
+	}
+
 	kfree(dpa_stats);
 	gbl_dpa_stats = NULL;
 }
 
+static int treat_cnts_request(struct dpa_stats *dpa_stats,
+				struct dpa_stats_req_cb *req_cb)
+{
+	struct dpa_stats_cnt_request_params params = req_cb->config;
+	struct dpa_stats_cnt_cb *cnt_cb = NULL;
+	int cnt_id = 0, err = 0;
+	uint32_t i = 0;
+
+	for (i = 0; i < params.cnts_ids_len; i++) {
+		cnt_id = params.cnts_ids[i];
+
+		/* Get counter's control block */
+		cnt_cb = &(dpa_stats->cnts_cb[cnt_id]);
+
+		/* Call counter's retrieve function */
+		err = cnt_cb->f_get_cnt_stats(req_cb, cnt_cb);
+		if (err < 0) {
+			pr_err("Failed to retrieve counter values\n");
+			return err;
+		}
+
+		/* Update number of bytes and number of counters
+		 * successfully written so far */
+		req_cb->bytes_num += cnt_cb->bytes_num;
+		req_cb->cnts_num += 1;
+	}
+
+	return 0;
+}
+
 static void create_cnt_eth_stats(struct dpa_stats *dpa_stats)
 {
 	struct t_FmMacStatistics stats;
@@ -1523,6 +1702,8 @@ static int get_cnt_eth_stats(struct dpa_stats_req_cb *req_cb,
 	uint32_t i = 0;
 	int err = 0;
 
+	cnt_cb->gen_cb.info.reset = req_cb->config.reset_cnts;
+
 	for (i = 0; i < cnt_cb->gen_cb.objs_num; i++) {
 		err = FM_MAC_GetStatistics(cnt_cb->gen_cb.objs[i], &stats);
 		if (err != 0) {
@@ -1544,6 +1725,8 @@ static int get_cnt_reass_stats(struct dpa_stats_req_cb *req_cb,
 	uint32_t i = 0;
 	int err = 0;
 
+	cnt_cb->gen_cb.info.reset = req_cb->config.reset_cnts;
+
 	for (i = 0; i < cnt_cb->gen_cb.objs_num; i++) {
 		err = FM_PCD_ManipGetStatistics(cnt_cb->gen_cb.objs[i], &stats);
 		if (err < 0) {
@@ -1565,6 +1748,8 @@ static int get_cnt_frag_stats(struct dpa_stats_req_cb *req_cb,
 	uint32_t i = 0;
 	int err = 0;
 
+	cnt_cb->gen_cb.info.reset = req_cb->config.reset_cnts;
+
 	for (i = 0; i < cnt_cb->gen_cb.objs_num; i++) {
 		err = FM_PCD_ManipGetStatistics(cnt_cb->gen_cb.objs[i], &stats);
 		if (err < 0) {
@@ -1586,6 +1771,8 @@ static int get_cnt_plcr_stats(struct dpa_stats_req_cb *req_cb,
 	uint64_t stats_val = 0;
 	uint32_t i = 0, j = 0;
 
+	cnt_cb->gen_cb.info.reset = req_cb->config.reset_cnts;
+
 	for (i = 0; i < cnt_cb->gen_cb.objs_num; i++) {
 		for (j = 0; j < info->stats_num; j++) {
 			stats_val = (uint64_t)FM_PCD_PlcrProfileGetCounter(
@@ -1625,6 +1812,8 @@ static int get_cnt_cls_tbl_stats(struct dpa_stats_req_cb *req_cb,
 	uint32_t i = 0;
 	int err = 0;
 
+	cnt_cb->tbl_cb.info.reset = req_cb->config.reset_cnts;
+
 	for (i = 0; i < cnt_cb->tbl_cb.keys_num; i++) {
 		if (!cnt_cb->tbl_cb.keys[i].valid) {
 			/* Write the memory location */
@@ -1659,6 +1848,8 @@ static int get_cnt_ccnode_stats(struct dpa_stats_req_cb *req_cb,
 	uint32_t i = 0;
 	int err = 0;
 
+	cnt_cb->ccnode_cb.info.reset = req_cb->config.reset_cnts;
+
 	for (i = 0; i < cnt_cb->ccnode_cb.keys_num; i++) {
 		err = FM_PCD_MatchTableFindNGetKeyStatistics(
 				cnt_cb->ccnode_cb.cc_node,
@@ -1684,6 +1875,8 @@ static int get_cnt_ipsec_stats(struct dpa_stats_req_cb *req_cb,
 	uint32_t i = 0;
 	int err = 0;
 
+	cnt_cb->ipsec_cb.info.reset = req_cb->config.reset_cnts;
+
 	for (i = 0; i < cnt_cb->ipsec_cb.sa_id_num; i++) {
 		if (!cnt_cb->ipsec_cb.valid[i]) {
 			/* Write the memory location */
@@ -1707,6 +1900,67 @@ static int get_cnt_ipsec_stats(struct dpa_stats_req_cb *req_cb,
 	return 0;
 }
 
+void async_req_work_func(struct work_struct *work)
+{
+	struct dpa_stats *dpa_stats = NULL;
+	struct dpa_stats_req_cb *req_cb = NULL, *pos = NULL, *new_rq_cb = NULL;
+	struct list_head *head;
+	int err = 0;
+
+	dpa_stats = container_of((struct work_struct *)work,
+			      struct dpa_stats, req_async_work);
+
+	/* Acquire protective lock for the asynchronous request list */
+	mutex_lock(&dpa_stats->async_req_hlist_lock);
+	head = &dpa_stats->async_req_hlist;
+	pos = container_of(head, struct dpa_stats_req_cb, async_req_node);
+
+	if (pos->async_req_node.next != head) {
+		req_cb = list_entry(pos->async_req_node.next,
+				struct dpa_stats_req_cb,
+				async_req_node);
+	} else {
+		req_cb = NULL;
+	}
+
+	/* Release the list lock so other threads may use it */
+	mutex_unlock(&dpa_stats->async_req_hlist_lock);
+
+	while (req_cb) {
+		err = treat_cnts_request(dpa_stats, req_cb);
+		if (err < 0) {
+			pr_err("Failed to retrieve counter values\n");
+			req_cb->bytes_num = err;
+		}
+
+		err = put_req(dpa_stats, req_cb);
+
+		/* Notify the application */
+		req_cb->request_done(0, req_cb->config.storage_area_offset,
+				req_cb->cnts_num, req_cb->bytes_num);
+
+		/* Acquire protective lock for the asynchronous request list */
+		mutex_lock(&dpa_stats->async_req_hlist_lock);
+
+		if (req_cb->async_req_node.next != head) {
+			new_rq_cb = list_entry(req_cb->async_req_node.next,
+					struct dpa_stats_req_cb,
+					async_req_node);
+		} else {
+			new_rq_cb = NULL;
+		}
+
+		list_del(&req_cb->async_req_node);
+
+		/* Release the list lock so other threads may use it */
+		mutex_unlock(&dpa_stats->async_req_hlist_lock);
+
+		req_cb = new_rq_cb;
+	}
+
+	return;
+}
+
 int dpa_stats_init(const struct dpa_stats_params *params, int *dpa_stats_id)
 {
 	struct dpa_stats *dpa_stats = NULL;
@@ -1744,6 +1998,13 @@ int dpa_stats_init(const struct dpa_stats_params *params, int *dpa_stats_id)
 		return err;
 	}
 
+	/* Allocate and initialize requests control block  */
+	err = init_reqs_cb(dpa_stats);
+	if (err < 0) {
+		free_resources();
+		return err;
+	}
+
 	/* Map each Ethernet counter selection to a FM-MAC statistics */
 	create_cnt_eth_stats(dpa_stats);
 
@@ -1762,6 +2023,22 @@ int dpa_stats_init(const struct dpa_stats_params *params, int *dpa_stats_id)
 	/* Map IPSec counters  */
 	create_cnt_ipsec_stats(dpa_stats);
 
+	/* Initialize the asynchronous requests list and its protective lock */
+	INIT_LIST_HEAD(&dpa_stats->async_req_hlist);
+	mutex_init(&dpa_stats->async_req_hlist_lock);
+
+	/* Create a single thread work queue used to defer work when
+	 * asynchronous counters requests are received  */
+	dpa_stats->async_req_workqueue =
+		create_singlethread_workqueue("async_req_workqueue");
+	if (!dpa_stats->async_req_workqueue) {
+		pr_err("Creating async request work queue failed\n");
+		return -ENOSPC;
+	}
+
+	/* Initialize the work needed to be done in asynchronous request */
+	INIT_WORK(&dpa_stats->req_async_work, async_req_work_func);
+
 	gbl_dpa_stats = dpa_stats;
 
 	return 0;
@@ -1863,6 +2140,9 @@ int dpa_stats_create_counter(int dpa_stats_id,
 		cnt_cb->type = DPA_STATS_CNT_IPSEC;
 		cnt_cb->f_get_cnt_stats = get_cnt_ipsec_stats;
 		break;
+	case DPA_STATS_CNT_TRAFFIC_MNG:
+		pr_err("Counter type not supported\n");
+		return -EINVAL;
 	default:
 		pr_err("Invalid counter type\n");
 		return -EINVAL;
@@ -1987,6 +2267,9 @@ int dpa_stats_create_class_counter(int dpa_stats_id,
 		cnt_cb->type = DPA_STATS_CNT_IPSEC;
 		cnt_cb->f_get_cnt_stats = get_cnt_ipsec_stats;
 		break;
+	case DPA_STATS_CNT_TRAFFIC_MNG:
+		pr_err("Counter type not supported\n");
+		return -EINVAL;
 	default:
 		pr_err("Invalid counter type\n");
 		return -EINVAL;
@@ -2015,6 +2298,7 @@ int dpa_stats_remove_counter(int dpa_stats_cnt_id)
 	struct dpa_stats *dpa_stats = NULL;
 	struct dpa_stats_cnt_cb *cnt_cb = NULL;
 	int err = 0;
+	uint32_t i;
 
 	if (!gbl_dpa_stats) {
 		pr_err("dpa_stats component is not initialized\n");
@@ -2038,6 +2322,22 @@ int dpa_stats_remove_counter(int dpa_stats_cnt_id)
 		return -EINVAL;
 	}
 
+	/* Remove the allocated memory for keys bytes and masks */
+	if (cnt_cb->type == DPA_STATS_CNT_CLASSIF_NODE) {
+		for (i = 0; i < cnt_cb->ccnode_cb.keys_num; i++) {
+			kfree(cnt_cb->ccnode_cb.keys[i].byte);
+			kfree(cnt_cb->ccnode_cb.keys[i].mask);
+		}
+	}
+
+	/* Remove the allocated memory for keys bytes and masks */
+	if (cnt_cb->type == DPA_STATS_CNT_CLASSIF_TBL) {
+		for (i = 0; i < cnt_cb->tbl_cb.keys_num; i++) {
+			kfree(cnt_cb->tbl_cb.keys[i].key.byte);
+			kfree(cnt_cb->tbl_cb.keys[i].key.mask);
+		}
+	}
+
 	/* Release the counter id in the Counter IDs circular queue */
 	err = put_cnt(dpa_stats, cnt_cb);
 	if (err < 0) {
@@ -2049,6 +2349,83 @@ int dpa_stats_remove_counter(int dpa_stats_cnt_id)
 }
 EXPORT_SYMBOL(dpa_stats_remove_counter);
 
+int dpa_stats_get_counters(struct dpa_stats_cnt_request_params params,
+			int *cnts_len,
+			dpa_stats_request_cb request_done)
+{
+	struct dpa_stats *dpa_stats = NULL;
+	struct dpa_stats_req_cb *req_cb = NULL;
+	struct dpa_stats_cnt_cb *cnt_cb = NULL;
+	int err = 0, req_id = 0, cnt_id = 0;
+	uint32_t i = 0;
+
+	if (!gbl_dpa_stats) {
+		pr_err("dpa_stats component is not initialized\n");
+		return -EPERM;
+	}
+
+	dpa_stats = gbl_dpa_stats;
+
+	*cnts_len = 0;
+
+	/* Calculate number of bytes occupied by the counters */
+	for (i = 0; i < params.cnts_ids_len; i++) {
+		cnt_id = params.cnts_ids[i];
+
+		if (cnt_id == DPA_OFFLD_INVALID_OBJECT_ID) {
+			pr_err("Invalid Counter id %d provided\n", cnt_id);
+			return -EINVAL;
+		}
+
+		/* Get counter's control block */
+		cnt_cb = &(dpa_stats->cnts_cb[cnt_id]);
+
+		*cnts_len += cnt_cb->bytes_num;
+	}
+
+	/* Check user-provided parameters */
+	if ((params.storage_area_offset + *cnts_len) >
+		dpa_stats->config.storage_area_len) {
+		pr_err("Invalid offset %d provided\n",
+				params.storage_area_offset);
+		return -EINVAL;
+	}
+
+	/* Create a new request */
+	err = get_new_req(dpa_stats, &req_id, &req_cb);
+	if (err < 0) {
+		pr_err("Failed retrieving a preallocated request\n");
+		return err;
+	}
+	req_cb->config = params;
+	req_cb->request_done = request_done;
+
+	/* Set memory area where the request should write */
+	req_cb->request_area = dpa_stats->config.storage_area +
+					params.storage_area_offset;
+
+	if (!req_cb->request_done) {
+		/* Call is synchronous */
+		err = treat_cnts_request(dpa_stats, req_cb);
+		if (err < 0)
+			pr_err("Failed to retrieve counter values\n");
+
+		err = put_req(dpa_stats, req_cb);
+
+		return err;
+	} else {
+		/* Call is asynchronous */
+		mutex_lock(&dpa_stats->async_req_hlist_lock);
+		list_add_tail(&req_cb->async_req_node,
+				&dpa_stats->async_req_hlist);
+		mutex_unlock(&dpa_stats->async_req_hlist_lock);
+		queue_work(dpa_stats->async_req_workqueue,
+				&dpa_stats->req_async_work);
+	}
+	return 0;
+}
+EXPORT_SYMBOL(dpa_stats_get_counters);
+
 int dpa_stats_free(int dpa_stats_id)
 {
 	/* multiple DPA Stats instances are not currently supported */
diff --git a/drivers/staging/fsl_dpa_offload/dpa_stats.h b/drivers/staging/fsl_dpa_offload/dpa_stats.h
index 5218dbe..df02c3d 100644
--- a/drivers/staging/fsl_dpa_offload/dpa_stats.h
+++ b/drivers/staging/fsl_dpa_offload/dpa_stats.h
@@ -54,19 +54,34 @@ struct dpa_stats {
 	struct cq *cnt_id_cq;	/* Circular Queue with ids for stats counters */
 	uint32_t *used_cnt_ids;	/* Counter ids used by this dpa_stats instance*/
 	struct dpa_stats_cnt_cb *cnts_cb; /* Array of counters control blocks */
+
+	uint32_t *used_req_ids;	/* Request ids used by this dpa_stats instance*/
+	struct dpa_stats_req_cb *reqs_cb; /* Array of counter requests */
+	struct cq *req_id_cq; /* Circular Queue with ids for counters request */
+
 	int stats_sel[NUM_OF_CNT_TYPES][MAX_NUM_OF_STATS]; /* Array that stores
 					the mapping between counter selection
 					and statistics values */
+	struct work_struct req_async_work; /* Asynchronous request work */
+	struct workqueue_struct *async_req_workqueue; /* Single threaded work
+				queue used to defer the work to be done when
+				an asynchronous counters request is received */
+	struct list_head async_req_hlist;  /* Head list with asynchronous
+				 counters requests currently being processed */
+	struct mutex async_req_hlist_lock; /* Lock for async requests list */
 };
 
 /* DPA Stats  request control block */
 struct dpa_stats_req_cb {
+	struct dpa_stats_cnt_request_params config;
+				/* Parameters provided to the request */
 	uint32_t req_id; /* Request id */
 	uint32_t req_index; /* Request index in the 'used_req_ids'*/
 	void *request_area; /* Address in the storage area
 				associated with this request */
 	uint32_t bytes_num; /* Number of bytes written by this request */
 	uint32_t cnts_num; /* Number of counters written by this request */
+	dpa_stats_request_cb request_done; /* Callback to notify upper layer */
 	struct list_head async_req_node; /* For linking async req list */
 };
 
diff --git a/include/linux/fsl_dpa_stats.h b/include/linux/fsl_dpa_stats.h
index 4b8861f..24d2cac 100644
--- a/include/linux/fsl_dpa_stats.h
+++ b/include/linux/fsl_dpa_stats.h
@@ -43,6 +43,9 @@
 /* Other includes */
 #include <linux/types.h>
 
+/* Maximum number simultaneous counters requests */
+#define DPA_STATS_MAX_NUM_OF_REQUESTS      256
+
 /* Maximum number of single and class counters */
 #define DPA_STATS_MAX_NUM_OF_COUNTERS      128
 
@@ -63,6 +66,33 @@ struct dpa_stats_params {
 	unsigned int storage_area_len;
 };
 
+/*
+ * Callback used to notify the upper layer that the requested counters values
+ * were written in the storage area. The 'storage_area_offset' is the offset
+ * in the storage_area and the 'cnts_written' represents the number of counters
+ * successfully written. The 'bytes_written' parameter can have a positive value
+ * and in this case it's value the size of the memory area written or it can
+ * have a negative value and contain the code of the error that occurred.
+ */
+typedef void (*dpa_stats_request_cb)(int dpa_stats_id,
+		unsigned int storage_area_offset, unsigned int cnts_written,
+		int bytes_written);
+
+/* DPA Stats Request parameters */
+struct dpa_stats_cnt_request_params {
+	/* Array of counter IDs to retrieve values for */
+	int *cnts_ids;
+
+	/* Size of array of counters to retrieve values for */
+	unsigned int cnts_ids_len;
+
+	/* Reset counters after the retrieve operation */
+	bool reset_cnts;
+
+	/* Storage area offset, expressed in bytes */
+	unsigned int storage_area_offset;
+};
+
 /* DPA Stats counter types */
 enum dpa_stats_cnt_type {
 	DPA_STATS_CNT_ETH = 0,		/* Ethernet counter		*/
@@ -386,6 +416,35 @@ struct dpa_stats_cnt_ipsec {
 	enum dpa_stats_cnt_sel cnt_sel;
 };
 
+/* DPA Stats Traffic Manager counter source */
+enum dpa_stats_cnt_traffic_mng_src {
+	/* Traffic Manager Class counter */
+	DPA_STATS_CNT_TRAFFIC_CLASS = 0,
+	/* Traffic Manager Congestion Group counter */
+	DPA_STATS_CNT_TRAFFIC_CG
+};
+
+/* DPA Stats Traffic Manager counter parameters */
+struct dpa_stats_cnt_traffic_mng {
+	/* Traffic Manager counter source */
+	enum dpa_stats_cnt_traffic_mng_src src;
+
+	/*
+	 * Depending on the Traffic Manager source, the 'src_id' has a
+	 * different meaning: it represents the 'class id' or the
+	 * 'congestion group id'
+	 */
+	uint8_t src_id;
+
+	/*
+	 * Traffic Manager Class: Number of bytes/frames dequeued from a Class
+	 * Traffic Manager Congestion Group: Number of bytes/frames whose
+	 * enqueues was rejected in all Class queues that belong to the
+	 * Congestion Group
+	 */
+	enum dpa_stats_cnt_sel cnt_sel;
+};
+
 /* DPA Stats counter parameters */
 struct dpa_stats_cnt_params {
 
@@ -413,6 +472,9 @@ struct dpa_stats_cnt_params {
 
 		/* Parameters for IPSec counter */
 		struct dpa_stats_cnt_ipsec ipsec_params;
+
+		/* Parameters for Traffic Manager counter */
+		struct dpa_stats_cnt_traffic_mng traffic_mng_params;
 	};
 };
 
@@ -555,6 +617,26 @@ struct dpa_stats_cls_cnt_ipsec {
 	enum dpa_stats_cnt_sel cnt_sel;
 };
 
+/* DPA Stats Traffic Manager class counter parameters */
+struct dpa_stats_cls_cnt_traffic_mng {
+
+	/* Traffic Manager source */
+	enum dpa_stats_cnt_traffic_mng_src src;
+
+	/*
+	 * Depending on the Traffic Manager source, the 'src_id' has a
+	 * different meaning: it represents the array of 'class ids' or the
+	 * array of 'congestion group ids' */
+	uint8_t *src_id;
+
+	/* Traffic Manager Class: Number of bytes/frames dequeued from a Class
+	 * Traffic Manager Congestion Group: Number of bytes/frames whose
+	 * enqueues was rejected in all Class queues that belong to the
+	 * Congestion Group
+	 */
+	enum dpa_stats_cnt_sel cnt_sel;
+};
+
 /* DPA Stats class counter parameters */
 struct dpa_stats_cls_cnt_params {
 
@@ -585,6 +667,9 @@ struct dpa_stats_cls_cnt_params {
 
 		/* Parameters for IPSec class counter */
 		struct dpa_stats_cls_cnt_ipsec ipsec_params;
+
+		/* Parameters for Traffic Manager class counter */
+		struct dpa_stats_cls_cnt_traffic_mng traffic_mng_params;
 	};
 };
 
@@ -607,6 +692,21 @@ int dpa_stats_create_class_counter(int dpa_stats_id,
 int dpa_stats_remove_counter(int dpa_stats_cnt_id);
 
 /*
+ * Create a request to retrieve the values of one or multiple single or class
+ * of counters. Counters that are in the 'requested_cnts' array will be
+ * retrieved in the order given by the position in the array. The counters
+ * values are written in the storage area, at offset defined by
+ * 'storage_area_offset' and the user is notified through the callback
+ * 'request_done'.
+ */
+int dpa_stats_get_counters(struct dpa_stats_cnt_request_params params,
+			int *cnts_len,
+			dpa_stats_request_cb request_done);
+
+/* Reset the statistics for a group of counters */
+int dpa_stats_reset_counters(int *cnts_ids,
+				unsigned int cnts_ids_len);
+/*
  * Releases all resources associated with a DPA Stats instance
  * and destroys it.
  */
-- 
1.7.5.4

