From a56ca66c181cc7ce9a00e88de415607ee00e7663 Mon Sep 17 00:00:00 2001
From: Zhong Hongbo <hongbo.zhong@windriver.com>
Date: Tue, 16 Jul 2013 16:00:09 +0800
Subject: [PATCH 35/38] fsl-imx6q: arm: add dma_alloc_noncacheable interface

- After USB driver prime a bulk transfer(whatever IN or OUT, take
  OUT for example) on ep1, only one dTD is primed, an USB Interrupt
  (bit 0 of USBSTS) will be issued, and find that endptcomplete
  register is 0x2 which means an OUT transfer on ep1 is completed,
  at this time the ep1 out queue head status is 0x1e18000, and next
  dtd pointer is 0x1 which means transfer is done and everything is
  OK, while the dTD token status is 0x2008080 which means this dTD
  is still active, not completed yet.
- Audio SDMA and Ethernet have the similar issue
- work around:
  change the non-cacheable bufferable memory to non-cacheable
  non-bufferable memory to make this issue disappear.

Source: Extract from vendor-drop package, L3.0.35_4.0.0_130424_source.tar.gz

Signed-off-by: Zhong Hongbo <hongbo.zhong@windriver.com>
---
 arch/arm/include/asm/dma-mapping.h   |   19 ++++++++
 arch/arm/mm/dma-mapping.c            |   10 +++-
 drivers/dma/imx-sdma.c               |    4 +-
 drivers/net/ethernet/freescale/fec.c |    2 +-
 include/linux/dma-attrs.h            |    1 +
 include/linux/dmapool.h              |    3 +
 mm/dmapool.c                         |   76 ++++++++++++++++++++++++++++++++++
 sound/soc/imx/imx-hdmi-dma.c         |    2 +-
 8 files changed, 110 insertions(+), 7 deletions(-)

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index b5745a8..e26abb7 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -185,6 +185,25 @@ static inline void dma_free_attrs(struct device *dev, size_t size,
 extern int arm_dma_mmap(struct device *dev, struct vm_area_struct *vma,
 			void *cpu_addr, dma_addr_t dma_addr, size_t size,
 			struct dma_attrs *attrs);
+/*
+ *  Allocate noncacheable memory space and return both the kernel remapped
+ * virtual and bus address for that space.
+ */
+static inline void *dma_alloc_noncacheable(struct device *dev, size_t size,
+				       dma_addr_t *dma_handle, gfp_t flag)
+{
+	DEFINE_DMA_ATTRS(attrs);
+	dma_set_attr(DMA_ATTR_NON_CACHEABLE, &attrs);
+	return dma_alloc_attrs(dev, size, dma_handle, flag, &attrs);
+}
+
+static inline void dma_free_noncacheable(struct device *dev, size_t size,
+				     void *cpu_addr, dma_addr_t dma_handle)
+{
+	DEFINE_DMA_ATTRS(attrs);
+	dma_set_attr(DMA_ATTR_NON_CACHEABLE, &attrs);
+	return dma_free_attrs(dev, size, cpu_addr, dma_handle, &attrs);
+}
 
 static inline void *dma_alloc_writecombine(struct device *dev, size_t size,
 				       dma_addr_t *dma_handle, gfp_t flag)
diff --git a/arch/arm/mm/dma-mapping.c b/arch/arm/mm/dma-mapping.c
index eec54aa..d79f488 100644
--- a/arch/arm/mm/dma-mapping.c
+++ b/arch/arm/mm/dma-mapping.c
@@ -548,9 +548,13 @@ static void __free_from_contiguous(struct device *dev, struct page *page,
 
 static inline pgprot_t __get_dma_pgprot(struct dma_attrs *attrs, pgprot_t prot)
 {
-	prot = dma_get_attr(DMA_ATTR_WRITE_COMBINE, attrs) ?
-			    pgprot_writecombine(prot) :
-			    pgprot_dmacoherent(prot);
+	if (dma_get_attr(DMA_ATTR_WRITE_COMBINE, attrs))
+		prot = pgprot_writecombine(prot);
+	else if (dma_get_attr(DMA_ATTR_NON_CACHEABLE, attrs))
+		prot = pgprot_noncached(prot);
+	else
+		prot = pgprot_dmacoherent(prot);
+
 	return prot;
 }
 
diff --git a/drivers/dma/imx-sdma.c b/drivers/dma/imx-sdma.c
index b08bd1e..2eb5c65 100644
--- a/drivers/dma/imx-sdma.c
+++ b/drivers/dma/imx-sdma.c
@@ -883,7 +883,7 @@ static int sdma_request_channel(struct sdma_channel *sdmac)
 	int channel = sdmac->channel;
 	int ret = -EBUSY;
 
-	sdmac->bd = dma_alloc_coherent(NULL, PAGE_SIZE, &sdmac->bd_phys, GFP_KERNEL);
+	sdmac->bd = dma_alloc_noncacheable(NULL, PAGE_SIZE, &sdmac->bd_phys, GFP_KERNEL);
 	if (!sdmac->bd) {
 		ret = -ENOMEM;
 		goto out;
@@ -1046,7 +1046,7 @@ static void sdma_free_chan_resources(struct dma_chan *chan)
 
 	sdma_set_channel_priority(sdmac, 0);
 
-	dma_free_coherent(NULL, PAGE_SIZE, sdmac->bd, sdmac->bd_phys);
+	dma_free_noncacheable(NULL, PAGE_SIZE, sdmac->bd, sdmac->bd_phys);
 
 	clk_disable(sdma->clk);
 }
diff --git a/drivers/net/ethernet/freescale/fec.c b/drivers/net/ethernet/freescale/fec.c
index 14ee0de..7981767 100644
--- a/drivers/net/ethernet/freescale/fec.c
+++ b/drivers/net/ethernet/freescale/fec.c
@@ -1506,7 +1506,7 @@ static int fec_enet_init(struct net_device *ndev)
 	int i;
 
 	/* Allocate memory for buffer descriptors. */
-	cbd_base = dma_alloc_coherent(NULL, BUFDES_SIZE, &fep->bd_dma,
+	cbd_base = dma_alloc_noncacheable(NULL, BUFDES_SIZE, &fep->bd_dma,
 			GFP_KERNEL);
 	if (!cbd_base) {
 		printk("FEC: allocate descriptor memory failed?\n");
diff --git a/include/linux/dma-attrs.h b/include/linux/dma-attrs.h
index 547ab56..df57da7 100644
--- a/include/linux/dma-attrs.h
+++ b/include/linux/dma-attrs.h
@@ -15,6 +15,7 @@ enum dma_attr {
 	DMA_ATTR_WEAK_ORDERING,
 	DMA_ATTR_WRITE_COMBINE,
 	DMA_ATTR_NON_CONSISTENT,
+	DMA_ATTR_NON_CACHEABLE,
 	DMA_ATTR_MAX,
 };
 
diff --git a/include/linux/dmapool.h b/include/linux/dmapool.h
index 022e34f..c3c41ef 100644
--- a/include/linux/dmapool.h
+++ b/include/linux/dmapool.h
@@ -22,6 +22,9 @@ void dma_pool_destroy(struct dma_pool *pool);
 void *dma_pool_alloc(struct dma_pool *pool, gfp_t mem_flags,
 		     dma_addr_t *handle);
 
+void *dma_pool_alloc_nonbufferable(struct dma_pool *pool, gfp_t mem_flags,
+		     dma_addr_t *handle);
+
 void dma_pool_free(struct dma_pool *pool, void *vaddr, dma_addr_t addr);
 
 /*
diff --git a/mm/dmapool.c b/mm/dmapool.c
index da1b0f0..d131124 100644
--- a/mm/dmapool.c
+++ b/mm/dmapool.c
@@ -209,6 +209,30 @@ static void pool_initialise_page(struct dma_pool *pool, struct dma_page *page)
 	} while (offset < pool->allocation);
 }
 
+static struct dma_page *pool_alloc_page_nonbufferable(struct dma_pool *pool, gfp_t mem_flags)
+{
+	struct dma_page *page;
+
+	page = kmalloc(sizeof(*page), mem_flags);
+	if (!page)
+		return NULL;
+	page->vaddr = dma_alloc_noncacheable(pool->dev, pool->allocation,
+					 &page->dma, mem_flags);
+	if (page->vaddr) {
+#ifdef	DMAPOOL_DEBUG
+		memset(page->vaddr, POOL_POISON_FREED, pool->allocation);
+#endif
+		pool_initialise_page(pool, page);
+		list_add(&page->page_list, &pool->page_list);
+		page->in_use = 0;
+		page->offset = 0;
+	} else {
+		kfree(page);
+		page = NULL;
+	}
+	return page;
+}
+
 static struct dma_page *pool_alloc_page(struct dma_pool *pool, gfp_t mem_flags)
 {
 	struct dma_page *page;
@@ -339,6 +363,58 @@ void *dma_pool_alloc(struct dma_pool *pool, gfp_t mem_flags,
 }
 EXPORT_SYMBOL(dma_pool_alloc);
 
+#ifdef CONFIG_FSL_UTP
+/**
+ * dma_pool_alloc_nonbufferable - get a block of consistent memory
+ * @pool: dma pool that will produce the block
+ * @mem_flags: GFP_* bitmask
+ * @handle: pointer to dma address of block
+ *
+ * This returns the kernel virtual address of a currently unused block,
+ * and reports its dma address through the handle.
+ * If such a memory block can't be allocated, %NULL is returned.
+ */
+void *dma_pool_alloc_nonbufferable(struct dma_pool *pool, gfp_t mem_flags,
+				   dma_addr_t *handle)
+{
+	unsigned long flags;
+	struct dma_page *page;
+	size_t offset;
+	void *retval;
+
+	might_sleep_if(mem_flags & __GFP_WAIT);
+
+	spin_lock_irqsave(&pool->lock, flags);
+	list_for_each_entry(page, &pool->page_list, page_list) {
+		if (page->offset < pool->allocation)
+			goto ready;
+	}
+
+	/* pool_alloc_page() might sleep, so temporarily drop &pool->lock */
+	spin_unlock_irqrestore(&pool->lock, flags);
+
+	page = pool_alloc_page_nonbufferable(pool, GFP_ATOMIC);
+	if (!page)
+		return NULL;
+
+	spin_lock_irqsave(&pool->lock, flags);
+
+	list_add(&page->page_list, &pool->page_list);
+ ready:
+	page->in_use++;
+	offset = page->offset;
+	page->offset = *(int *)(page->vaddr + offset);
+	retval = offset + page->vaddr;
+	*handle = offset + page->dma;
+#ifdef	DMAPOOL_DEBUG
+	memset(retval, POOL_POISON_ALLOCATED, pool->size);
+#endif
+	spin_unlock_irqrestore(&pool->lock, flags);
+	return retval;
+}
+EXPORT_SYMBOL(dma_pool_alloc_nonbufferable);
+#endif
+
 static struct dma_page *pool_find_page(struct dma_pool *pool, dma_addr_t dma)
 {
 	struct dma_page *page;
diff --git a/sound/soc/imx/imx-hdmi-dma.c b/sound/soc/imx/imx-hdmi-dma.c
index 7040954..ced8e06 100644
--- a/sound/soc/imx/imx-hdmi-dma.c
+++ b/sound/soc/imx/imx-hdmi-dma.c
@@ -1349,7 +1349,7 @@ static int __devinit imx_soc_platform_probe(struct platform_device *pdev)
 	if (hdmi_SDMA_check()) {
 		/*To alloc a buffer non cacheable for hdmi script use*/
 		hdmi_dma_priv->hdmi_sdma_t =
-			dma_alloc_coherent(NULL,
+			dma_alloc_noncacheable(NULL,
 				sizeof(struct hdmi_sdma_script_data),
 				&hdmi_dma_priv->phy_hdmi_sdma_t,
 				GFP_KERNEL);
-- 
1.7.5.4

