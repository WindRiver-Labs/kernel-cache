From 6bb0e81569f045d60234fa073ca1375f4b53e5ae Mon Sep 17 00:00:00 2001
From: Liu Changhui <changhui.liu@windriver.com>
Date: Fri, 29 Jan 2010 13:27:57 +0800
Subject: [PATCH] Huge TLB: Add hugetlb support for rmi xls

Add rmi xls hugetlb support for mips.

source: from RMI SDK1.7

Signed-off-by: Liu Changhui <changhui.liu@windriver.com>
---
 arch/mips/Kconfig               |   26 +++
 arch/mips/mm/Makefile           |    1 +
 arch/mips/mm/hugetlbpage.c      |  320 +++++++++++++++++++++++++++++++++++++++
 arch/mips/mm/tlb-r4k.c          |   11 ++
 arch/mips/mm/tlbex.c            |   61 ++++++++
 fs/Kconfig                      |    3 +-
 include/asm-mips/hugetlb.h      |   79 ++++++++++
 include/asm-mips/page.h         |   33 ++++
 include/asm-mips/pgtable-64.h   |    6 +-
 include/asm-mips/pgtable-bits.h |    4 +
 include/asm-mips/pgtable.h      |    7 +
 11 files changed, 549 insertions(+), 2 deletions(-)
 create mode 100644 arch/mips/mm/hugetlbpage.c
 create mode 100644 include/asm-mips/hugetlb.h

diff --git a/arch/mips/Kconfig b/arch/mips/Kconfig
index 2c75827..a9a098d 100644
--- a/arch/mips/Kconfig
+++ b/arch/mips/Kconfig
@@ -624,6 +624,32 @@ config RMI_PTR
 
 endchoice
 
+choice
+	prompt "Hugetlb Page Size"
+	depends on HUGETLB_PAGE
+	default HUGETLB_PAGE_SIZE_2MB
+
+config HUGETLB_PAGE_SIZE_32KB
+	bool "32KB"
+	depends on !PAGE_SIZE_16KB && !PAGE_SIZE_64KB
+config HUGETLB_PAGE_SIZE_128KB
+	bool "128KB"
+	depends on !PAGE_SIZE_64KB
+config HUGETLB_PAGE_SIZE_512KB
+	bool "512KB"
+config HUGETLB_PAGE_SIZE_2MB
+	bool "2MB"
+config HUGETLB_PAGE_SIZE_8MB
+	bool "8MB"
+config HUGETLB_PAGE_SIZE_32MB
+	bool "32MB"
+config HUGETLB_PAGE_SIZE_128MB
+	bool "128MB"
+config HUGETLB_PAGE_SIZE_512MB
+	bool "512MB"
+
+
+endchoice
 source "arch/mips/au1000/Kconfig"
 source "arch/mips/basler/excite/Kconfig"
 source "arch/mips/jazz/Kconfig"
diff --git a/arch/mips/mm/Makefile b/arch/mips/mm/Makefile
index b6b0909..2afc4fa 100644
--- a/arch/mips/mm/Makefile
+++ b/arch/mips/mm/Makefile
@@ -34,4 +34,5 @@ obj-$(CONFIG_CPU_PHOENIX)	+= c-phoenix.o tlb-r4k.o \
                                cex-gen.o cerr-phoenix.o 
 obj-$(CONFIG_RM7000_CPU_SCACHE)	+= sc-rm7k.o
 obj-$(CONFIG_MIPS_CPU_SCACHE)	+= sc-mips.o
+obj-$(CONFIG_HUGETLB_PAGE) += hugetlbpage.o
 EXTRA_CFLAGS += -Werror
diff --git a/arch/mips/mm/hugetlbpage.c b/arch/mips/mm/hugetlbpage.c
new file mode 100644
index 0000000..e86d1cf
--- /dev/null
+++ b/arch/mips/mm/hugetlbpage.c
@@ -0,0 +1,320 @@
+/*********************************************************************
+
+  Copyright 2003-2006 Raza Microelectronics, Inc. (RMI). All rights
+  reserved.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+  1. Redistributions of source code must retain the above copyright
+  notice, this list of conditions and the following disclaimer.
+  2. Redistributions in binary form must reproduce the above copyright
+  notice, this list of conditions and the following disclaimer in
+  the documentation and/or other materials provided with the
+  distribution.
+
+  THIS SOFTWARE IS PROVIDED BY Raza Microelectronics, Inc. ``AS IS'' AND
+  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL RMI OR CONTRIBUTORS BE LIABLE
+  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+  INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+  CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+  THE POSSIBILITY OF SUCH DAMAGE.
+
+  *****************************#RMI_2#**********************************/
+/*
+ * MIPS Huge TLB page support.
+ *
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/fs.h>
+#include <linux/mm.h>
+#include <linux/hugetlb.h>
+#include <linux/pagemap.h>
+#include <linux/smp_lock.h>
+#include <linux/slab.h>
+#include <linux/sysctl.h>
+
+#include <asm/mman.h>
+#include <asm/pgalloc.h>
+#include <asm/tlb.h>
+#include <asm/tlbflush.h>
+#include <asm/cacheflush.h>
+#include <asm/mmu_context.h>
+
+#define Message(a,b...)		//printk("\n%s:%d: "a"\n",__FUNCTION__,__LINE__,##b)
+
+static pte_t *get_pte_offset(struct mm_struct *mm, unsigned long addr)
+{
+	pgd_t *pgd;
+	pud_t *pud;
+	pmd_t *pmd;
+	pte_t *pte = NULL;
+
+	pgd = pgd_offset(mm, addr);
+	if (!pgd_none(*pgd)) {
+		pud = pud_offset(pgd, addr);
+		if (!pud_none(*pud)) {
+			pmd = pmd_offset(pud, addr);
+			if (!pmd_none(*pmd))
+				pte = pte_offset_map(pmd, addr);
+		}
+	}
+	return pte;
+}
+
+pte_t *huge_pte_offset(struct mm_struct * mm, unsigned long addr)
+{
+	pgd_t *pgd;
+	pud_t *pud;
+	pmd_t *pmd;
+	pte_t *pte = NULL;
+
+	addr &= HPAGE_MASK;
+
+	pgd = pgd_offset(mm, addr);
+	if (!pgd_none(*pgd)) {
+		pud = pud_offset(pgd, addr);
+		if (!pud_none(*pud)) {
+			pmd = pmd_offset(pud, addr);
+			if (!pmd_none(*pmd))
+				pte = pte_offset_map(pmd, addr);
+		}
+	}
+	return pte;
+}
+
+static pte_t *huge_pte_alloc_single(struct mm_struct *mm,
+				    unsigned long addr)
+{
+	pgd_t *pgd;
+	pud_t *pud;
+	pmd_t *pmd;
+	pte_t *pte = NULL;
+
+	pgd = pgd_offset(mm, addr);
+	pud = pud_alloc(mm, pgd, addr);
+	if (pud) {
+		pmd = pmd_alloc(mm, pud, addr);
+		if (pmd)
+			pte = pte_alloc_map(mm, pmd, addr);
+	}
+	return pte;
+}
+
+pte_t *huge_pte_alloc(struct mm_struct * mm, unsigned long addr,
+		      unsigned long sz)
+{
+	pte_t *first_pte = NULL;
+	pte_t *pte = NULL;
+	int i = 0;
+	uint32_t htlb_entries = 1 << HUGETLB_PAGE_ORDER;
+	uint32_t total_pte_required = htlb_entries / PTRS_PER_PTE;
+
+	/*increment number of pte required if htlb_entries 
+	   is not multiple of PTRS_PER_PTE
+	 */
+	if (htlb_entries % PTRS_PER_PTE)
+		total_pte_required++;
+
+	Message(" allocating pte for before mask - addr %#lx", addr);
+	addr &= HPAGE_MASK;
+	Message(" allocating pte for after mask - addr %#lx", addr);
+	Message(" total pte required %d", total_pte_required);
+	for (i = 0; i < total_pte_required; i++) {
+		pte = huge_pte_alloc_single(mm, addr);
+		if (!pte)
+			return NULL;
+		if (!first_pte)
+			first_pte = pte;
+		addr = addr + (PTRS_PER_PTE * PAGE_SIZE);
+		Message("New Addr %#lx", addr);
+	}
+	Message("Returning first_pte %#lx", (unsigned long) first_pte);
+	return first_pte;
+}
+
+void set_huge_pte_at(struct mm_struct *mm, unsigned long addr,
+		     pte_t * ptep, pte_t entry)
+{
+	int i;
+	uint32_t htlb_entries = 1 << HUGETLB_PAGE_ORDER;
+	struct vm_area_struct *vma = NULL;
+	uint32_t len = htlb_entries * PAGE_SIZE;
+	pte_t first_entry = entry;
+	unsigned long orig_addr;
+
+	/* We must align the address, because our caller will run
+	 * set_huge_pte_at() on whatever we return, which writes out
+	 * all of the sub-ptes for the hugepage range.  So we have
+	 * to give it the first such sub-pte.
+	 */
+	addr &= HPAGE_MASK;
+	orig_addr = addr;
+	Message("\nSetting hugepte for address %#lx\n", addr);
+	Message("htlb entries %d", htlb_entries);
+	/*Fill each entry with its own physical address map */
+	for (i = 0; i < htlb_entries; i++) {
+		/*Get the pte offset, we may cross the pte table */
+		ptep = get_pte_offset(mm, addr);
+
+		if (!(i % 1024))
+			Message
+			    ("Entry [%d] Virt Addr [%#lx] Phys Addr[%#llx]\n",
+			     i, (unsigned long) addr,
+			     (unsigned long long) ((pte_val(entry))));
+
+		set_pte_at(mm, addr, ptep, entry);
+		addr += PAGE_SIZE;
+		pte_val(entry) += PAGE_SIZE;
+	}
+	addr = orig_addr;
+	vma = find_vma(mm, addr);
+
+	/*addr must belong to this vma */
+	if (!((addr >= vma->vm_start) && ((addr + len) <= vma->vm_end)))
+		panic("set_huge_pte_at: No vma found for hugtlb page!!");
+
+	/*Don't know below loop is required or not. */
+	pte_val(entry) = pte_val(first_entry);
+	for (i = 0; i < htlb_entries; i++) {
+		__update_cache(vma, addr, entry);
+		addr += PAGE_SIZE;
+		pte_val(entry) += PAGE_SIZE;
+	}
+	Message("Returning...");
+}
+
+pte_t huge_ptep_get_and_clear(struct mm_struct *mm, unsigned long addr,
+			      pte_t * ptep)
+{
+	pte_t entry;
+	int i;
+
+	entry = *ptep;
+	pte_val(entry) = pte_val(entry) & ~(_PAGE_HUGE);
+
+	addr &= HPAGE_MASK;
+	Message("Called");
+	for (i = 0; i < (1 << HUGETLB_PAGE_ORDER); i++) {
+		ptep = get_pte_offset(mm, addr);
+		if (!(i % 1024))
+			Message("\nClearing ptep %d\n", i);
+		pte_clear(mm, addr, ptep);
+		addr += PAGE_SIZE;
+	}
+	Message("Returning");
+	return entry;
+}
+
+struct page *follow_huge_addr(struct mm_struct *mm,
+			      unsigned long address, int write)
+{
+	return ERR_PTR(-EINVAL);
+}
+
+int pmd_huge(pmd_t pmd)
+{
+	return 0;
+}
+
+struct page *follow_huge_pmd(struct mm_struct *mm, unsigned long address,
+			     pmd_t * pmd, int write)
+{
+	return NULL;
+}
+
+int huge_pmd_unshare(struct mm_struct *mm, unsigned long *addr,
+		     pte_t * ptep)
+{
+	return 0;
+}
+
+/*
+ * If the arch doesn't supply something else, assume that hugepage
+ * size aligned regions are ok without further preparation.
+ */
+static inline int prepare_hugepage_range(struct file *file,
+					 unsigned long addr,
+					 unsigned long len)
+{
+	struct hstate *h = hstate_file(file);
+	if (len & ~huge_page_mask(h))
+		return -EINVAL;
+	if (addr & ~huge_page_mask(h))
+		return -EINVAL;
+	return 0;
+}
+
+unsigned long
+hugetlb_get_unmapped_area(struct file *file, unsigned long addr,
+			  unsigned long len, unsigned long pgoff,
+			  unsigned long flags)
+{
+	struct mm_struct *mm = current->mm;
+	struct vm_area_struct *vma;
+	unsigned long start_addr;
+	unsigned long task_size = TASK_SIZE;
+#ifdef CONFIG_64BIT
+	if (test_thread_flag(TIF_32BIT)) {
+		task_size = TASK_SIZE32;
+	}
+#endif
+
+	if (len & ~HPAGE_MASK)
+		return -EINVAL;
+
+	if (len > task_size)
+		return -ENOMEM;
+
+	if (flags & MAP_FIXED) {
+		if (prepare_hugepage_range(file, addr, len))
+			return -EINVAL;
+		return addr;
+	}
+
+	if (addr) {
+		addr = ALIGN(addr, HPAGE_SIZE);
+		vma = find_vma(mm, addr);
+		if (task_size - len >= addr &&
+		    (!vma || addr + len <= vma->vm_start)) {
+			return addr;
+		}
+	}
+
+	start_addr = mm->free_area_cache;
+
+	if (len <= mm->cached_hole_size)
+		start_addr = TASK_UNMAPPED_BASE;
+
+full_search:
+	addr = ALIGN(start_addr, HPAGE_SIZE);
+
+	for (vma = find_vma(mm, addr);; vma = vma->vm_next) {
+		/* At this point:  (!vma || addr < vma->vm_end). */
+		if (task_size - len < addr) {
+			/*
+			 * Start a new search - just in case we missed
+			 * some holes.
+			 */
+			if (start_addr != TASK_UNMAPPED_BASE) {
+				start_addr = TASK_UNMAPPED_BASE;
+				goto full_search;
+			}
+			return -ENOMEM;
+		}
+
+		if (!vma || addr + len <= vma->vm_start) {
+			return addr;
+		}
+		addr = ALIGN(vma->vm_end, HPAGE_SIZE);
+	}
+}
+
diff --git a/arch/mips/mm/tlb-r4k.c b/arch/mips/mm/tlb-r4k.c
index 2df68ff..940d24c 100644
--- a/arch/mips/mm/tlb-r4k.c
+++ b/arch/mips/mm/tlb-r4k.c
@@ -476,6 +476,14 @@ void rmi_tlb_stats_init(void)
 	rmi_write_os_scratch_2(0ULL);
 }
 
+#ifdef CONFIG_HUGETLBFS
+void rmi_tlb_entrylo0_mask_init(void);
+void rmi_tlb_entrylo0_mask_init()
+{
+	unsigned long long mask = ~(((1ULL<<HUGETLB_PAGE_ORDER)-1)<<6);
+	rmi_write_os_scratch_3(mask);
+}
+#endif
 
 /* This is called from smp_call_function (IPI) context */
 extern unsigned long long phnx_tlb_stats[];
@@ -522,6 +530,9 @@ void __cpuinit tlb_init(void)
 
 #ifdef CONFIG_RMI_PHOENIX
     rmi_tlb_stats_init();
+#ifdef CONFIG_HUGETLBFS
+	rmi_tlb_entrylo0_mask_init();
+#endif
 #endif
     build_tlb_refill_handler();
 
diff --git a/arch/mips/mm/tlbex.c b/arch/mips/mm/tlbex.c
index ffd7c39..da377d2 100644
--- a/arch/mips/mm/tlbex.c
+++ b/arch/mips/mm/tlbex.c
@@ -645,8 +645,14 @@ static void __cpuinit build_get_ptep(u32 **p, unsigned int tmp, unsigned int ptr
 	UASM_i_ADDU(p, ptr, ptr, tmp); /* add in offset */
 }
 
+#ifdef CONFIG_HUGETLBFS
+static void __cpuinit build_update_entries(u32 **p, struct uasm_label **l,
+               struct uasm_reloc **r, unsigned int tmp,
+               unsigned int ptep)
+#else
 static void __cpuinit build_update_entries(u32 **p, unsigned int tmp,
 					unsigned int ptep)
+#endif
 {
 	/*
 	 * 64bit address support (36bit on a 32bit CPU) in a 32bit
@@ -655,11 +661,44 @@ static void __cpuinit build_update_entries(u32 **p, unsigned int tmp,
 #ifdef CONFIG_64BIT_PHYS_ADDR
 	if (cpu_has_64bits) {
 		uasm_i_ld(p, tmp, 0, ptep); /* get even pte */
+#ifdef CONFIG_HUGETLBFS
+		/*continue if htlb found otherwise jump to label_nohpage_tlbs*/
+		uasm_i_dsrl32(p, tmp, tmp, 31); 
+		uasm_il_beqz(p, r, tmp, label_nohpage_tlbs);
+		uasm_i_ld(p, tmp, 0, ptep); /*load tmp with even entry again*/
+		uasm_i_dsrl(p, tmp, tmp, 6);    /*shift by 6 to remove s/w bits*/
+		/*get the entrylo0 mask*/
+		uasm_i_dmfc0(p, ptep, OS_SCRATCH_REG3);
+		/*mask lower paddr bits*/
+		uasm_i_and(p, tmp, tmp, ptep);
+		uasm_i_dmtc0(p, tmp, C0_ENTRYLO0); /* load it */
+
+		/*Add (HUGE_PAGE_SIZE/2)>>6 to entrylo0 to compute entrlylo1*/
+		uasm_i_daddiu(p, ptep, ZERO, 1);
+		uasm_i_sll(p, ptep, ptep, HPAGE_SHIFT-7);
+		uasm_i_daddu(p, tmp, tmp, ptep); /*compute entrylo1*/
+
+		uasm_i_dmtc0(p, tmp, C0_ENTRYLO1);
+		uasm_i_daddiu(p, ptep, ZERO, HUGETLB_PAGE_MASK);
+		uasm_i_sll(p, ptep, ptep, 13);
+#ifndef CONFIG_PAGE_SIZE_4KB
+		uasm_i_mfc0(p, tmp, C0_PAGEMASK);
+#endif
+		uasm_il_b(p, r, label_hpage_tlb_leave);
+		uasm_i_mtc0(p, ptep, C0_PAGEMASK);  /*delay slot*/
+		uasm_l_nohpage_tlbs(l, *p);
+#endif
 		uasm_i_ld(p, ptep, sizeof(pte_t), ptep); /* get odd pte */
 		uasm_i_dsrl(p, tmp, tmp, 6); /* convert to entrylo0 */
 		uasm_i_mtc0(p, tmp, C0_ENTRYLO0); /* load it */
 		uasm_i_dsrl(p, ptep, ptep, 6); /* convert to entrylo1 */
 		uasm_i_mtc0(p, ptep, C0_ENTRYLO1); /* load it */
+#ifdef CONFIG_HUGETLBFS
+#ifndef CONFIG_PAGE_SIZE_4KB
+		uasm_i_mfc0(p, tmp, C0_PAGEMASK);
+#endif
+		uasm_l_hpage_tlb_leave(l, *p);
+#endif
 	} else {
 		int pte_off_even = sizeof(pte_t) / 2;
 		int pte_off_odd = pte_off_even + sizeof(pte_t);
@@ -720,8 +759,19 @@ static void __cpuinit build_r4000_tlb_refill_handler(void)
 #endif
 
 	build_get_ptep(&p, K0, K1);
+#ifdef CONFIG_HUGETLBFS
+	build_update_entries(&p, &l, &r, K0, K1);
+#else
 	build_update_entries(&p, K0, K1);
+#endif
 	build_tlb_write_entry(&p, &l, &r, tlb_random);
+#ifdef CONFIG_HUGETLBFS
+	#ifdef CONFIG_PAGE_SIZE_4KB
+	    uasm_i_mtc0(&p, ZERO, C0_PAGEMASK);
+	#else
+	    uasm_i_mtc0(&p, K0, C0_PAGEMASK);
+	#endif
+#endif
 	uasm_l_leave(&l, p);
 
 	if (rmi_xls_war()){
@@ -1157,8 +1207,19 @@ build_r4000_tlbchange_handler_tail(u32 **p, struct uasm_label **l,
 {
 	uasm_i_ori(p, ptr, ptr, sizeof(pte_t));
 	uasm_i_xori(p, ptr, ptr, sizeof(pte_t));
+#ifdef CONFIG_HUGETLBFS
+	build_update_entries(p, l, r, tmp, ptr);
+#else
 	build_update_entries(p, tmp, ptr);
+#endif
 	build_tlb_write_entry(p, l, r, tlb_indexed);
+#ifdef CONFIG_HUGETLBFS
+	#ifdef CONFIG_PAGE_SIZE_4KB
+	    uasm_i_mtc0(p, ZERO, C0_PAGEMASK);
+	#else
+		uasm_i_mtc0(p, K0, C0_PAGEMASK);
+	#endif
+#endif
 	uasm_l_leave(l, *p);
 	uasm_i_eret(p); /* return from trap */
 
diff --git a/fs/Kconfig b/fs/Kconfig
index 5af291a..5a05e58 100644
--- a/fs/Kconfig
+++ b/fs/Kconfig
@@ -956,7 +956,8 @@ config TMPFS_POSIX_ACL
 config HUGETLBFS
 	bool "HugeTLB file system support"
 	depends on X86 || IA64 || PPC64 || SPARC64 || (SUPERH && MMU) || \
-		   (S390 && 64BIT) || BROKEN
+		   (S390 && 64BIT) || BROKEN || \
+		   (RMI_PHOENIX && (64BIT_PHYS_ADDR || 64BIT))
 	help
 	  hugetlbfs is a filesystem backing for HugeTLB pages, based on
 	  ramfs. For architectures that support it, say Y here and read
diff --git a/include/asm-mips/hugetlb.h b/include/asm-mips/hugetlb.h
new file mode 100644
index 0000000..40cfc3c
--- /dev/null
+++ b/include/asm-mips/hugetlb.h
@@ -0,0 +1,79 @@
+/*********************************************************************
+
+  Copyright 2003-2006 Raza Microelectronics, Inc. (RMI). All rights
+  reserved.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+  1. Redistributions of source code must retain the above copyright
+  notice, this list of conditions and the following disclaimer.
+  2. Redistributions in binary form must reproduce the above copyright
+  notice, this list of conditions and the following disclaimer in
+  the documentation and/or other materials provided with the
+  distribution.
+
+  THIS SOFTWARE IS PROVIDED BY Raza Microelectronics, Inc. ``AS IS'' AND
+  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL RMI OR CONTRIBUTORS BE LIABLE
+  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+  INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+  CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+  THE POSSIBILITY OF SUCH DAMAGE.
+
+  *****************************#RMI_2#**********************************/
+#ifndef _ASM_MIPS_HUGETLB_H
+#define _ASM_MIPS_HUGETLB_H
+
+#include <asm/page.h>
+#include <asm/pgtable.h>
+
+#define is_hugepage_only_range(mm, addr, len)	0
+#define hugetlb_free_pgd_range			free_pgd_range
+#define arch_release_hugepage(page)		0
+#define arch_prepare_hugepage(page)		0
+#define huge_pte_none				pte_none
+#define pud_huge(x)				0
+#define hugetlb_prefault_arch_hook(mm)		do { } while (0)
+
+extern void set_huge_pte_at(struct mm_struct *mm, unsigned long addr, 
+				pte_t *ptep, pte_t entry);
+extern pte_t huge_ptep_get_and_clear(struct mm_struct *mm, unsigned long addr,
+			      pte_t *ptep);
+static inline pte_t huge_pte_wrprotect(pte_t pte)
+{
+	return pte_wrprotect(pte);
+}
+
+		
+static inline void huge_ptep_clear_flush(struct vm_area_struct *vma,
+					 unsigned long addr, pte_t *ptep)
+{
+}
+
+
+static inline pte_t huge_ptep_get(pte_t *ptep)
+{
+	return *ptep;
+}
+
+static inline void huge_ptep_set_wrprotect(struct mm_struct *mm,
+					   unsigned long addr, pte_t *ptep)
+{
+	set_huge_pte_at(mm, addr, ptep, pte_wrprotect(huge_ptep_get(ptep)));
+}
+
+static inline int huge_ptep_set_access_flags(struct vm_area_struct *vma, 
+		unsigned long addr, pte_t *ptep, pte_t entry, int dirty)
+{
+	set_huge_pte_at(vma->vm_mm, addr, ptep, entry);	
+	flush_tlb_page(vma, addr);
+	return 1;
+}
+
+#endif
diff --git a/include/asm-mips/page.h b/include/asm-mips/page.h
index abd5ef5..f3e352f 100644
--- a/include/asm-mips/page.h
+++ b/include/asm-mips/page.h
@@ -29,6 +29,39 @@
 #define PAGE_SIZE	(1UL << PAGE_SHIFT)
 #define PAGE_MASK       (~((1 << PAGE_SHIFT) - 1))
 
+
+#ifdef CONFIG_HUGETLB_PAGE
+#define HAVE_ARCH_HUGETLB_UNMAPPED_AREA
+#if defined(CONFIG_HUGETLB_PAGE_SIZE_32KB)
+#define HPAGE_SHIFT		15
+#elif defined(CONFIG_HUGETLB_PAGE_SIZE_128KB)
+#define HPAGE_SHIFT		17
+#elif defined(CONFIG_HUGETLB_PAGE_SIZE_512KB)
+#define HPAGE_SHIFT		19
+#elif defined(CONFIG_HUGETLB_PAGE_SIZE_2MB)
+#define HPAGE_SHIFT		21
+#elif defined(CONFIG_HUGETLB_PAGE_SIZE_8MB)
+#define CONFIG_FORCE_MAX_ZONEORDER 12
+#define HPAGE_SHIFT		23
+#elif defined(CONFIG_HUGETLB_PAGE_SIZE_32MB)
+#define CONFIG_FORCE_MAX_ZONEORDER 14
+#define HPAGE_SHIFT		25
+#elif defined(CONFIG_HUGETLB_PAGE_SIZE_128MB)
+#define CONFIG_FORCE_MAX_ZONEORDER 16
+#define HPAGE_SHIFT		27
+#elif defined(CONFIG_HUGETLB_PAGE_SIZE_512MB)
+#define CONFIG_FORCE_MAX_ZONEORDER 18
+#define HPAGE_SHIFT		29
+#endif
+
+#define HPAGE_SIZE		((1UL) << HPAGE_SHIFT)
+#define HPAGE_MASK		(~(HPAGE_SIZE - 1UL))
+#define HUGETLB_PAGE_ORDER	(HPAGE_SHIFT - PAGE_SHIFT)
+
+#define HUGETLB_PAGE_MASK	(((HPAGE_SIZE/8192)-1))
+#define ARCH_HAS_SETCLEAR_HUGE_PTE 1
+#endif
+
 #ifndef __ASSEMBLY__
 
 #include <linux/pfn.h>
diff --git a/include/asm-mips/pgtable-64.h b/include/asm-mips/pgtable-64.h
index 943515f..71eb2cd 100644
--- a/include/asm-mips/pgtable-64.h
+++ b/include/asm-mips/pgtable-64.h
@@ -173,7 +173,11 @@ static inline void pud_clear(pud_t *pudp)
 #define pte_pfn(x)		((unsigned long)((x).pte >> (PAGE_SHIFT + 2)))
 #define pfn_pte(pfn, prot)	__pte(((pfn) << (PAGE_SHIFT + 2)) | pgprot_val(prot))
 #else
-#define pte_pfn(x)		((unsigned long)((x).pte >> PAGE_SHIFT))
+#ifdef CONFIG_HUGETLBFS
+#define pte_pfn(x)		((unsigned long)((((x).pte & ~(_PAGE_HUGE)) >> PAGE_SHIFT)))
+#else
+#define pte_pfn(x)		((unsigned long)(((x).pte >> PAGE_SHIFT)))
+#endif
 #define pfn_pte(pfn, prot)	__pte(((pfn) << PAGE_SHIFT) | pgprot_val(prot))
 #endif
 
diff --git a/include/asm-mips/pgtable-bits.h b/include/asm-mips/pgtable-bits.h
index 1efa42b..96dd85d 100644
--- a/include/asm-mips/pgtable-bits.h
+++ b/include/asm-mips/pgtable-bits.h
@@ -83,6 +83,10 @@
 #endif
 #endif /* defined(CONFIG_64BIT_PHYS_ADDR && defined(CONFIG_CPU_MIPS32) */
 
+#ifdef CONFIG_HUGETLB_PAGE
+#define HUGE_BIT		    63
+#define _PAGE_HUGE		    (1ULL<<HUGE_BIT) /* mark pte as a hugepage */
+#endif
 
 /*
  * Cache attributes
diff --git a/include/asm-mips/pgtable.h b/include/asm-mips/pgtable.h
index 48aa104..ada25a3 100644
--- a/include/asm-mips/pgtable.h
+++ b/include/asm-mips/pgtable.h
@@ -294,6 +294,13 @@ static inline pte_t pte_mkyoung(pte_t pte)
 		pte_val(pte) |= _PAGE_SILENT_READ;
 	return pte;
 }
+#ifdef CONFIG_HUGETLBFS
+static inline pte_t pte_mkhuge(pte_t pte)
+{
+	pte_val(pte) |= _PAGE_HUGE; 
+	return pte; 
+}
+#endif
 #endif
 static inline int pte_special(pte_t pte)	{ return 0; }
 static inline pte_t pte_mkspecial(pte_t pte)	{ return pte; }
-- 
1.6.0.4

