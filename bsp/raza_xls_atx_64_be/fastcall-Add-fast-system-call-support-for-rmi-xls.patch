From 1f601fe681b022122827675a22a9728314cb14b5 Mon Sep 17 00:00:00 2001
From: Liu Changhui <changhui.liu@windriver.com>
Date: Fri, 29 Jan 2010 13:27:54 +0800
Subject: [PATCH] fastcall: Add fast system call support for rmi xls

Add fast system call support for rmi xls.

source: from RMI SDK1.7

Signed-off-by: shuo.kang <shuo.kang@windriver.com>
---
 arch/mips/kernel/scall32-o32.S               |   24 +++
 arch/mips/kernel/scall64-o32.S               |   26 +++
 arch/mips/kernel/syscall.c                   |   53 +++++
 arch/mips/kernel/xlr_fast_sys_call_handler.S |  285 ++++++++++++++++++++++++++
 4 files changed, 388 insertions(+), 0 deletions(-)
 create mode 100644 arch/mips/kernel/xlr_fast_sys_call_handler.S

diff --git a/arch/mips/kernel/scall32-o32.S b/arch/mips/kernel/scall32-o32.S
index d9e07aa..d3e7039 100644
--- a/arch/mips/kernel/scall32-o32.S
+++ b/arch/mips/kernel/scall32-o32.S
@@ -26,6 +26,30 @@
 
 	.align  5
 NESTED(handle_sys, PT_SIZE, sp)
+#ifdef CONFIG_32BIT
+#ifdef CONFIG_RMI_PHOENIX 
+	.set push
+	.set mips64
+    /* XLR Specific fast system calls */
+	mfc0	k1, CP0_EPC
+	lw	k0, 0(k1)
+	dsll32	k0, k0, 0
+	dsrl32  k0, k0, 6
+	beqz	k0, 1f
+	nop
+	sll	k0, k0, 2
+	lw	k1, xlr_fast_sys_call_table(k0)
+	jr	k1
+	nop
+	/* should never come here */
+2:	wait
+	b	2b
+	nop
+	.set pop
+1:		
+#endif
+#endif
+
 	.set	noat
 	SAVE_SOME
 	TRACE_IRQS_ON_RELOAD
diff --git a/arch/mips/kernel/scall64-o32.S b/arch/mips/kernel/scall64-o32.S
index b2052dc..c1b5c5d 100644
--- a/arch/mips/kernel/scall64-o32.S
+++ b/arch/mips/kernel/scall64-o32.S
@@ -26,6 +26,32 @@
 
 	.align  5
 NESTED(handle_sys, PT_SIZE, sp)
+#ifdef CONFIG_64BIT
+#ifdef CONFIG_RMI_PHOENIX
+	.set push
+	.set    noat
+	.set mips64
+	/* XLR Specific fast system calls */
+	dmfc0   k1, CP0_EPC
+	lw  k0, 0(k1)
+	dsll32  k0, k0, 0
+	dsrl32  k0, k0, 6
+	beqz    k0, 1f
+	nop
+	sll k0, k0, 3
+	PTR_LA  k1, xlr_fast_sys_call_table
+	PTR_ADDU k1,k0,k1
+	ld  k1, 0(k1)
+	jr  k1
+	nop
+	/* should never come here */
+2: wait
+	b   2b
+	nop
+1:
+	.set pop
+#endif
+#endif
 	.set	noat
 	SAVE_SOME
 	TRACE_IRQS_ON_RELOAD
diff --git a/arch/mips/kernel/syscall.c b/arch/mips/kernel/syscall.c
index a5b3108..b08afeb 100644
--- a/arch/mips/kernel/syscall.c
+++ b/arch/mips/kernel/syscall.c
@@ -41,6 +41,39 @@
 #include <asm/sysmips.h>
 #include <asm/uaccess.h>
 
+#ifdef CONFIG_RMI_PHOENIX
+extern void xlr_fast_syscall_msgsnd(void);
+extern void xlr_fast_syscall_msgld(void);
+extern void xlr_fast_syscall_c0_count(void);
+extern void xlr_fast_syscall_processorId(void);
+extern void xlr_fast_syscall_iomem_read(void);
+extern void xlr_fast_syscall_iomem_write(void);
+extern void xlr_fast_syscall_msg_write(void);
+extern void xlr_fast_syscall_msg_read(void);
+extern void xlr_fast_syscall_perf_ctr_start(void);
+extern void xlr_fast_syscall_perf_ctr_stop(void);
+extern void xlr_fast_syscall_get_cpumasks(void);
+extern void xlr_fast_syscall_prominfo(void);
+static void xlr_fast_syscall_unused(void) {}
+unsigned long xlr_fast_sys_call_table[] = {
+	(unsigned long)xlr_fast_syscall_unused,
+	(unsigned long)xlr_fast_syscall_msgsnd,
+	(unsigned long)xlr_fast_syscall_msgld,
+	(unsigned long)xlr_fast_syscall_c0_count,
+	(unsigned long)xlr_fast_syscall_iomem_read,
+	(unsigned long)xlr_fast_syscall_iomem_write,
+	(unsigned long)xlr_fast_syscall_msg_write,
+	(unsigned long)xlr_fast_syscall_msg_read,
+	(unsigned long)xlr_fast_syscall_perf_ctr_start,
+	(unsigned long)xlr_fast_syscall_perf_ctr_stop,
+	(unsigned long)xlr_fast_syscall_get_cpumasks,
+	(unsigned long)xlr_fast_syscall_processorId,
+	(unsigned long)xlr_fast_syscall_prominfo,
+	0
+};
+
+#endif
+
 /*
  * For historic reasons the pipe(2) syscall on MIPS has an unusual calling
  * convention.  It returns results in registers $v0 / $v1 which means there
@@ -111,6 +144,7 @@ unsigned long arch_get_unmapped_area(struct file *filp, unsigned long addr,
 		    (!vmm || addr + len <= vmm->vm_start))
 			return addr;
 	}
+
 	addr = TASK_UNMAPPED_BASE;
 	if (do_color_align)
 		addr = COLOUR_ALIGN(addr, pgoff);
@@ -129,6 +163,25 @@ unsigned long arch_get_unmapped_area(struct file *filp, unsigned long addr,
 	}
 }
 
+#if defined(CONFIG_RMI_PHOENIX) && defined(CONFIG_64BIT)
+int mips_mmap_check(unsigned long addr, unsigned long len,
+		unsigned long flags)
+{
+	int app_is_32bit = test_thread_flag(TIF_32BIT_ADDR);
+
+	if(app_is_32bit){
+		if(len >= TASK_SIZE32){
+			return -EINVAL;
+		}
+		if ((flags & MAP_FIXED) && addr > (TASK_SIZE32 - len)){
+			return -EINVAL;
+		}
+		
+	}
+	return 0;
+}
+#endif
+
 /* common code for old and new mmaps */
 static inline unsigned long
 do_mmap2(unsigned long addr, unsigned long len, unsigned long prot,
diff --git a/arch/mips/kernel/xlr_fast_sys_call_handler.S b/arch/mips/kernel/xlr_fast_sys_call_handler.S
new file mode 100644
index 0000000..2eb4e12
--- /dev/null
+++ b/arch/mips/kernel/xlr_fast_sys_call_handler.S
@@ -0,0 +1,285 @@
+/*********************************************************************
+
+  Copyright 2003-2006 RMI Corporation, Inc. (RMI). All rights
+  reserved.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+  1. Redistributions of source code must retain the above copyright
+  notice, this list of conditions and the following disclaimer.
+  2. Redistributions in binary form must reproduce the above copyright
+  notice, this list of conditions and the following disclaimer in
+  the documentation and/or other materials provided with the
+  distribution.
+
+  THIS SOFTWARE IS PROVIDED BY RMI Corporation, Inc. ``AS IS'' AND
+  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL RMI OR CONTRIBUTORS BE LIABLE
+  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+  INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+  CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+  THE POSSIBILITY OF SUCH DAMAGE.
+
+  *****************************#RMI_2#**********************************/
+
+
+#include <asm/addrspace.h>
+#include <asm/asm.h>
+#include <asm/mipsregs.h>
+#include <asm/regdef.h>
+#include <asm/stackframe.h>
+#include <asm/rmi/perf_ctr.h>
+#include <asm/rmi/interrupt.h>
+
+#ifdef CONFIG_32BIT
+	#define T0 t0
+	#define T1 t1
+	#define T2 t2
+	#define T3 t3
+	#define T4 t4
+	#define T5 t5
+	#define T6 t6
+	#define T7 t7
+#else
+	#define T0 ta0
+	#define T1 ta1
+	#define T2 ta2
+	#define T3 ta3
+	#define T4 t0
+	#define T5 t1
+	#define T6 t2
+	#define T7 t3
+#endif
+	.text
+	.set	push
+	.set	noreorder
+	.set	mips64
+	.align	5
+
+	NESTED(xlr_fast_syscall_msgsnd, PT_SIZE, sp)
+
+	/* msgsnd arg0 is in (t1, t2) */
+	dsll32  k0, T1, 0
+	dsll32	k1, T2, 0
+	dsrl32	T2, k1, 0
+	or      T1, k0, T2
+	dmtc2	T1, $0, 0
+
+	/* msgsnd arg1 is in (t3, t4) */
+	dsll32  k0, T3, 0
+	dsll32	k1, T4, 0
+	dsrl32	T4, k1, 0
+	or      T3, k0, T4
+	dmtc2   T3, $0, 1
+
+	/* msgsnd dst is in t0 */
+1:	c2	0x80001
+	mfc2	T1, $2
+	andi	T1, T1, 0x06
+	bnez	T1, 1b
+	nop
+
+	/* skip the syscall instruction */
+	MFC0	k0, CP0_EPC
+	PTR_ADDIU	k0, 4
+	MTC0	k0, CP0_EPC
+	eret
+	END(xlr_fast_syscall_msgsnd)
+
+	NESTED(xlr_fast_syscall_msgld, PT_SIZE, sp)
+	/* t0 has the bucket arg */
+	c2	0x80002
+1:	mfc2	T3, $2
+	andi	k0, T3, 0x08
+	bnez	k0, 1b
+	nop
+	andi    T0, T3, 0x30
+	bnez	T0, 2f
+	nop
+	dmfc2	T2, $1, 0
+	dmfc2   T5, $1, 1
+	nop
+	dsra32  T1, T2, 0
+	dsra32  T4, T5, 0
+2:
+
+	/* move    t0, k0 */
+
+	/* skip the syscall instruction */
+	MFC0	k0, CP0_EPC
+	PTR_ADDIU	k0, 4
+	MTC0	k0, CP0_EPC
+	eret
+	END(xlr_fast_syscall_msgld)
+
+	NESTED(xlr_fast_syscall_c0_count, PT_SIZE, sp)
+
+	mfc0    T0, $9, 0
+
+	/* skip the syscall instruction */
+	MFC0	k0, CP0_EPC
+	PTR_ADDIU	k0, 4
+	MTC0	k0, CP0_EPC
+	eret
+
+	END(xlr_fast_syscall_c0_count)
+
+	NESTED(xlr_fast_syscall_iomem_read, PT_SIZE, sp)
+
+	/* t0 has the address */
+	lw      T1, (T0)
+
+	/* skip the syscall instruction */
+	MFC0    k0, CP0_EPC
+	PTR_ADDIU   k0, 4
+	MTC0    k0, CP0_EPC
+	eret
+
+	END(xlr_fast_syscall_iomem_read)
+
+	NESTED(xlr_fast_syscall_iomem_write, PT_SIZE, sp)
+
+	/* t0 has the address, t1 has the data */
+	sw      T1, (T0)
+
+	/* skip the syscall instruction */
+	MFC0    k0, CP0_EPC
+	PTR_ADDIU   k0, 4
+	MTC0    k0, CP0_EPC
+	eret
+
+	END(xlr_fast_syscall_iomem_write)
+
+
+	NESTED(xlr_fast_syscall_msg_write, PT_SIZE, sp)
+
+	/* disable the message ring interrupts and enable 64 bits operations */
+	mfc0    k0, CP0_STATUS
+	li      k1, 1
+	dsll    k1, k1, 30
+	or      k0, k0, k1
+	li      k1, 1
+	dsll    k1, k1, 23
+	or      k0, k0, k1
+	mtc0    k0, CP0_STATUS
+	bnez	T1, 1f
+	nop
+	mtc2    T1, $3, 0
+1:
+
+	/* skip the syscall instruction */
+	MFC0    k0, CP0_EPC
+	PTR_ADDIU   k0, 4
+	MTC0    k0, CP0_EPC
+	eret
+
+	END(xlr_fast_syscall_msg_write)
+
+
+	NESTED(xlr_fast_syscall_msg_read, PT_SIZE, sp)
+
+	/* read C0 and C2 registers */
+	mfc0    T0, CP0_STATUS
+	mfc2    T1, $2, 0
+	mfc2    T2, $2, 1
+	mfc2    T3, $3, 0
+	mfc2    T4, $3, 1
+
+	/* skip the syscall instruction */
+	MFC0    k0, CP0_EPC
+	PTR_ADDIU   k0, 4
+	MTC0    k0, CP0_EPC
+	eret
+
+	END(xlr_fast_syscall_msg_read)
+
+	NESTED(xlr_fast_syscall_perf_ctr_start, PT_SIZE, sp)
+
+	mtc0    $0, CP0_PERF_CTR, PERF_CTR_EVENT0_VALUE
+	mtc0    $0, CP0_PERF_CTR, PERF_CTR_EVENT1_VALUE
+	mtc0    T0, CP0_PERF_CTR, PERF_CTR_EVENT0
+	mtc0    T1, CP0_PERF_CTR, PERF_CTR_EVENT1
+
+	/* skip the syscall instruction */
+	MFC0    k0, CP0_EPC
+	PTR_ADDIU   k0, 4
+	MTC0    k0, CP0_EPC
+	eret
+
+	END(xlr_fast_syscall_perf_ctr_start)
+
+	NESTED(xlr_fast_syscall_perf_ctr_stop, PT_SIZE, sp)
+
+	mtc0    T0, CP0_PERF_CTR, PERF_CTR_EVENT0
+	mtc0    T0, CP0_PERF_CTR, PERF_CTR_EVENT1
+	mfc0    T1, CP0_PERF_CTR, PERF_CTR_EVENT0_VALUE
+	mfc0    T2, CP0_PERF_CTR, PERF_CTR_EVENT1_VALUE
+
+	/* skip the syscall instruction */
+	MFC0    k0, CP0_EPC
+	PTR_ADDIU   k0, 4
+	MTC0    k0, CP0_EPC
+	eret
+
+	END(xlr_fast_syscall_perf_ctr_stop)
+
+
+	NESTED(xlr_fast_syscall_get_cpumasks, PT_SIZE, sp)
+
+		.word 0x40088007
+		move k0, T0
+		srl  T0, T0, 4
+		andi T0, T0, 0x3f
+		sll  T0, T0, 2
+		andi k0, k0, 0x0f
+		or T0, T0, k0
+
+		PTR_LA k0, fast_syscall_cpumask_phy		;
+		lw T1, (k0)
+
+		/* skip the syscall instruction */
+		MFC0	k0, CP0_EPC
+		PTR_ADDIU	k0, 4
+		MTC0	k0, CP0_EPC
+
+		eret
+
+	END(xlr_fast_syscall_get_cpumasks)
+
+
+	NESTED(xlr_fast_syscall_processorId, PT_SIZE, sp)
+
+	mfc0    T1, $15, 0
+
+	/* skip the syscall instruction */
+	MFC0	k0, CP0_EPC
+	PTR_ADDIU	k0, 4
+	MTC0	k0, CP0_EPC
+	eret
+
+	END(xlr_fast_syscall_processorId)
+
+	NESTED(xlr_fast_syscall_prominfo, PT_SIZE, sp)
+
+	PTR_LA	k0, prom_info_copy
+	move	k1, T0
+	sll	k1, k1, 3
+	addu	k0, k0, k1
+	lw	T1, (k0)
+	lw	T2, 4(k0)
+	
+	/* skip the syscall instruction */
+	MFC0    k0, CP0_EPC
+	PTR_ADDIU       k0, 4
+	MTC0    k0, CP0_EPC
+	eret
+
+	END(xlr_fast_syscall_prominfo)
+
+	.set pop
-- 
1.6.0.4

