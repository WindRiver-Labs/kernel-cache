From afc0d3b85fe207723193a6bd3b12af36f494174d Mon Sep 17 00:00:00 2001
From: Liu Changhui <changhui.liu@windriver.com>
Date: Fri, 29 Jan 2010 13:27:43 +0800
Subject: [PATCH] Platform: Add cache operation for rmi xls

RMI XLS is a multi-core processor. It has hardware machanism to make each
core's cache coherent. In spite of it's a MIPS64 implementation it has
a different cache style against R4K. So it need own cache flush operations.

source: from RMI SDK1.7

Signed-off-by: Liu Changhui <changhui.liu@windriver.com>
---
 arch/mips/mm/c-phoenix.c      |  501 +++++++++++++++++++++++++++++++++++++++++
 arch/mips/mm/cache.c          |   56 ++++-
 include/asm-mips/cache.h      |    2 +
 include/asm-mips/cacheflush.h |    5 +
 4 files changed, 553 insertions(+), 11 deletions(-)
 create mode 100644 arch/mips/mm/c-phoenix.c

diff --git a/arch/mips/mm/c-phoenix.c b/arch/mips/mm/c-phoenix.c
new file mode 100644
index 0000000..6cadc77
--- /dev/null
+++ b/arch/mips/mm/c-phoenix.c
@@ -0,0 +1,501 @@
+/************************************************************************
+
+  Copyright 2003-2006 RMI Corporation, Inc.(RMI).
+
+  This is a derived work from software originally provided by the external
+  entity identified below. The licensing terms and warranties specified in
+  the header of the original work apply to this derived work.
+
+  Contribution by RMI: 
+
+  *****************************#RMI_1#************************************/
+/*
+ * Copyright (C) 1996 David S. Miller (dm@engr.sgi.com)
+ * Copyright (C) 1997, 2001 Ralf Baechle (ralf@gnu.org)
+ * 
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version 2
+ * of the License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ * 
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
+ */ 
+#include <linux/init.h>
+#include <asm/asm.h>
+#include <asm/mmu_context.h>
+#include <asm/bootinfo.h>
+#include <asm/cacheops.h>
+#include <asm/cpu.h>
+#include <asm/uaccess.h>
+#include <linux/smp.h>
+#include <linux/kallsyms.h>
+#include <linux/mm.h>
+#include <linux/module.h>
+
+#include <asm/rmi/debug.h>
+
+static unsigned int icache_linesz;
+static unsigned int icache_lines;
+
+#ifdef CONFIG_RMI_VMIPS
+extern void rmi_vmips_temp_xkphys_tlb_add(phys_t start, phys_t end, int *tlbs, int *tlbe);
+extern void rmi_vmips_wired_entry_remove(int index);
+#endif
+
+
+#define cacheop(op, base) __asm__ __volatile__ (".set push\n.set mips4\ncache %0, 0(%1)\n.set pop\n" : : "i"(op), "r"(base))
+
+#define cacheop_extable(op, base) do {                    \
+  __asm__ __volatile__(                                    \
+		       "    .set push                \n"   \
+		       "    .set noreorder           \n"   \
+		       "    .set mips4               \n"   \
+		       "1:  cache %0, 0(%1)           \n"  \
+		       "2:  .set pop                 \n"   \
+		       "    .section __ex_table,\"a\"\n"   \
+		            STR(PTR)"\t1b, 2b\n\t"        \
+		       "     .previous               \n"   \
+		       : : "i" (op), "r" (base));          \
+  } while (0) 
+
+static __inline__ void sync_istream(void)
+{
+  __asm__ __volatile__ (                                     
+                       ".set push                     \n"    
+                       ".set noreorder                \n"    
+		       //                       " la     $8, 1f                \n"    
+                       //" mtc0   $8, $14               \n"    
+                       //"eret                          \n"    
+		       //"1:nop                         \n"
+		       "nop                           \n"
+		       "nop                           \n"
+		       "nop                           \n"
+		       "nop                           \n"
+		       "nop                           \n"
+		       "nop                           \n"
+		       "nop                           \n"
+		       "nop                           \n"
+		       "nop                           \n"
+		       "nop                           \n"
+                       ".set pop                      \n"    
+                       : : : "$8"
+		       );
+}
+
+static __inline__ void cacheop_hazard(void)
+{
+  __asm__ __volatile__ (                                     
+                       ".set push                     \n"    
+                       ".set noreorder                \n"    
+                       " nop;nop;nop;nop              \n"    
+                       " nop;nop;nop;nop              \n"    
+                       ".set pop                      \n"    
+                       );  
+}
+
+static __inline__ void cacheop_sync_istream(void)
+{
+  cacheop_hazard();
+  sync_istream();
+}
+
+#define optimize_thread_flush()
+
+extern unsigned long phnx_ebase;
+/*****************************************************************************************
+ * 
+ * These routines support Generic Kernel cache flush requirements
+ *
+ *****************************************************************************************/
+void phoenix_flush_dcache_page(struct page *page)
+{
+	ClearPageDcacheDirty(page);    
+}
+
+EXPORT_SYMBOL(phoenix_flush_dcache_page);
+
+static void phoenix_local_flush_icache_range(unsigned long start, unsigned long end)
+{
+	unsigned long addr;
+  
+	for(addr = (start & ~((unsigned long)(icache_linesz - 1))); addr < end; 
+		addr += icache_linesz) {
+		cacheop_extable(Hit_Invalidate_I, addr);
+	}
+
+	cacheop_sync_istream();
+}
+
+struct flush_icache_range_args {
+	unsigned long start;
+	unsigned long end;
+};
+struct flush_icache_range_args_paddr {
+	phys_t start;
+	phys_t end;
+};
+
+static void phoenix_flush_icache_range_ipi(void *info)
+{
+	struct flush_icache_range_args *args = info;
+
+	optimize_thread_flush();
+
+  	phoenix_local_flush_icache_range(args->start, args->end);
+}
+
+void phoenix_flush_icache_range(unsigned long start, unsigned long end)
+{
+	struct flush_icache_range_args args;
+  
+	if ((end - start) > PAGE_SIZE) {
+		dbg_msg("flushing more than page size of icache addresses starting @ %lx\n", start);
+	}
+  
+	args.start = start;
+	args.end = end;
+  /* TODO: don't even send ipi to non-zero thread ids 
+   * This may require some changes to smp_call_function interface, for now just avoid 
+   * redundant cache ops
+   */
+	on_each_cpu(phoenix_flush_icache_range_ipi, &args, 1);
+}
+
+static void phoenix_flush_cache_sigtramp_ipi(void *info)
+{
+	unsigned long addr = (unsigned long)info;
+
+	optimize_thread_flush();
+
+	addr = addr & ~(icache_linesz - 1);
+	cacheop_extable(Hit_Invalidate_I, addr);
+	cacheop_sync_istream();
+}
+
+static void phoenix_flush_cache_sigtramp(unsigned long addr)
+{
+	on_each_cpu(phoenix_flush_cache_sigtramp_ipi, (void *) addr, 1);
+}
+
+/*****************************************************************************************
+ * 
+ * These routines support MIPS specific cache flush requirements.
+ * These are called only during bootup or special system calls 
+ *
+ *****************************************************************************************/
+
+static void phoenix_local_flush_icache(void)
+{
+	int i=0;
+  	unsigned long base = CKSEG0;
+
+	/* Index Invalidate all the lines and the ways */
+	for (i = 0; i < icache_lines; i++){
+		cacheop(Index_Invalidate_I, base);
+		base += icache_linesz;
+	}
+
+	cacheop_sync_istream(); 
+}
+
+static void phoenix_local_flush_dcache(void)
+{
+	int i=0;
+	unsigned long base = CKSEG0;
+	unsigned int lines;
+
+	lines = current_cpu_data.dcache.ways * current_cpu_data.dcache.sets;
+  
+	/* Index Invalidate all the lines and the ways */  
+	for (i = 0; i < lines; i++) {
+		cacheop(Index_Writeback_Inv_D, base);
+		base += current_cpu_data.dcache.linesz;
+	}
+
+	cacheop_hazard(); 
+}
+
+#ifdef CONFIG_KGDB
+void phoenix_flush_l1_icache_ipi(void *info)
+{
+	phoenix_local_flush_icache();
+}
+#endif
+
+#ifdef CONFIG_KGDB
+void phoenix_flush_l1_caches_ipi(void *info)
+#else
+static void phoenix_flush_l1_caches_ipi(void *info)
+#endif
+{
+	optimize_thread_flush();
+ 
+	phoenix_local_flush_dcache();
+	phoenix_local_flush_icache();
+}
+
+static void phoenix_flush_l1_caches(void)
+{
+	on_each_cpu(phoenix_flush_l1_caches_ipi, (void *)NULL, 1);
+}
+
+/*****************************************************************************************/
+
+static void phoenix_noflush(void) { /* do nothing */ }
+
+static __init void probe_l1_cache(void)
+{
+	struct cpuinfo_mips *c = &current_cpu_data;
+	unsigned int config1 = read_c0_config1();
+	int lsize = 0;
+	int icache_size=0, dcache_size=0;
+
+	if ((lsize = ((config1 >> 19) & 7)))
+		c->icache.linesz = 2 << lsize;
+	else
+		c->icache.linesz = lsize;
+	c->icache.sets = 64 << ((config1 >> 22) & 7);
+	c->icache.ways = 1 + ((config1 >> 16) & 7);
+
+	icache_size = c->icache.sets *
+		c->icache.ways *
+		c->icache.linesz;
+	c->icache.waybit = ffs(icache_size/c->icache.ways) - 1;
+
+	c->dcache.flags = 0;
+
+	if ((lsize = ((config1 >> 10) & 7)))
+		c->dcache.linesz = 2 << lsize;
+	else
+		c->dcache.linesz= lsize;
+	c->dcache.sets = 64 << ((config1 >> 13) & 7);
+	c->dcache.ways = 1 + ((config1 >> 7) & 7);
+
+	dcache_size = c->dcache.sets *
+		c->dcache.ways *
+		c->dcache.linesz;
+	c->dcache.waybit = ffs(dcache_size/c->dcache.ways) - 1;
+
+	if (smp_processor_id()==0) {
+		printk("Primary instruction cache %dkB, %d-way, linesize %d bytes.\n",
+		icache_size >> 10,
+		c->icache.ways, c->icache.linesz);
+    
+	printk("Primary data cache %dkB %d-way, linesize %d bytes.\n",
+		dcache_size >> 10, c->dcache.ways, c->dcache.linesz);
+	}
+}
+
+static __inline__ void install_cerr_handler(void)
+{
+	extern char except_vec2_generic;
+
+	memcpy((void *)(phnx_ebase + 0x100), &except_vec2_generic, 0x80);
+}
+
+static void update_kseg0_coherency(void)
+{
+	int attr = read_c0_config() & CONF_CM_CMASK;
+
+	if (attr != 0x3) {
+
+		phoenix_local_flush_dcache();
+		phoenix_local_flush_icache();
+
+		change_c0_config(CONF_CM_CMASK, 0x3);
+
+		sync_istream();
+	}
+	_page_cachable_default = (0x3 << _CACHE_SHIFT);
+
+}
+
+void ld_mmu_phoenix(void)
+{
+	extern void build_clear_page(void);
+	extern void build_copy_page(void);
+	/* update cpu_data */
+
+	probe_l1_cache();
+
+	if (smp_processor_id()) {  
+
+		phoenix_local_flush_icache();
+
+		update_kseg0_coherency();
+
+		return;
+	}
+
+	/* These values are assumed to be the same for all cores */
+	icache_lines = current_cpu_data.icache.ways * current_cpu_data.icache.sets;
+	icache_linesz = current_cpu_data.icache.linesz;
+
+	/* When does this function get called? Looks like MIPS has some syscalls
+	 * to flush the caches. 
+	 */
+	__flush_cache_all = phoenix_flush_l1_caches;
+
+	/* flush_cache_all: makes all kernel data coherent.
+	 * This gets called just before changing or removing
+	 * a mapping in the page-table-mapped kernel segment (kmap). 
+	 * Physical Cache -> do nothing
+	 */
+	flush_cache_all = phoenix_noflush;
+
+	/* flush_icache_range: makes the range of addresses coherent w.r.t I-cache and D-cache 
+	 * This gets called after the instructions are written to memory
+	 * All addresses are valid kernel or mapped user-space virtual addresses
+	 */
+	flush_icache_range = phoenix_flush_icache_range;
+
+	/* flush_cache_{mm, range, page}: make these memory locations, that may have been written
+	 *                                by a user process, coherent
+	 * These get called when virtual->physical translation of a user address space is about
+	 * to be changed. These are closely related to TLB coherency (flush_tlb_{mm, range, page})
+	 */
+	flush_cache_mm = (void (*)(struct mm_struct *))phoenix_noflush;
+	flush_cache_range = (void *) phoenix_noflush;
+	flush_cache_page = (void *) phoenix_noflush;
+
+	/* flush_icache_page: flush_dcache_page + update_mmu_cache takes care of this
+	 * 
+	 */
+	flush_data_cache_page = (void *) phoenix_noflush;
+
+	/* flush_cache_sigtramp: flush the single I-cache line with the proper fixup code
+	 */
+	flush_cache_sigtramp = phoenix_flush_cache_sigtramp;
+
+	/* flush_icache_all: This should get called only for Virtuall Tagged I-Caches
+	 */
+	flush_icache_all = (void *)phoenix_noflush;
+
+	local_flush_icache_range = phoenix_local_flush_icache_range;
+	local_flush_data_cache_page	= (void *)phoenix_noflush;
+
+	__flush_cache_vmap = (void *)phoenix_noflush;
+	__flush_cache_vunmap = (void *)phoenix_noflush;
+
+	install_cerr_handler();
+
+	build_clear_page();
+	build_copy_page();
+
+	phoenix_local_flush_icache();
+
+	update_kseg0_coherency();
+}
+
+#define cacheop_paddr(op, base) __asm__ __volatile__ ( \
+                         ".set push\n"           \
+                         ".set noreorder\n"      \
+                         ".set mips64\n"          \
+                         "dli $8, 0x9800000000000000\n"              \
+                         "daddu $8, $8, %1\n"       \
+                         "cache %0, 0($8)\n"     \
+                         ".set pop\n"            \
+                         : : "i"(op), "r"(base) : "$8")
+
+#define enable_KX(flags)   \
+ preempt_disable(); \
+ __asm__ __volatile__ (          \
+	".set push\n"              \
+	".set noat\n"               \
+	".set noreorder\n"     \
+	"mfc0 %0, $12\n\t"             \
+	"ori $1, %0, 0x81\n\t"   \
+	"xori $1, 1\n\t"      \
+	"mtc0 $1, $12\n"       \
+        ".set pop\n"          \
+        : "=r"(flags) ); \
+  preempt_enable();
+	
+#define disable_KX(flags)   \
+ __asm__ __volatile__ (          \
+	".set push\n"              \
+	"mtc0 %0, $12\n"       \
+        ".set pop\n"          \
+        : : "r"(flags) )
+	
+
+static void phoenix_local_flush_icache_range_paddr(phys_t start, phys_t end)
+{
+	phys_t addr;
+#ifdef CONFIG_32BIT
+	unsigned long flags;
+	phys_t temp;
+#endif
+#ifdef CONFIG_RMI_VMIPS
+	int tlbs = 0, tlbe = 0;
+	rmi_vmips_temp_xkphys_tlb_add(start, end, &tlbs, &tlbe);
+#endif
+
+#ifdef CONFIG_64BIT
+	for(addr = (start & ~(phys_t)(icache_linesz - 1)); addr < end;
+		addr += icache_linesz) {
+		cacheop_paddr(Hit_Invalidate_I, addr);
+	}
+#else
+	enable_KX(flags);
+	for(addr = (start & ~(phys_t)(icache_linesz - 1)); addr < end; 
+					addr += icache_linesz) {
+		temp = addr | 0x9800000000000000ULL;
+		__asm__ volatile(
+				".set push\n"
+				".set noreorder\n"
+				".set mips64\n"
+				"dsll32 %L1, %L1, 0\n"
+				"dsrl32 %L1, %L1, 0\n"
+				"dsll32 %M1, %M1, 0\n"
+				"or %L1, %L1, %M1\n"
+				"cache %0, 0(%L1)\n"
+				".set pop\n"
+				".set reorder\n"
+				:: "i"(Hit_Invalidate_I), "r"(temp)
+				);
+	}
+	disable_KX(flags);
+#endif
+
+#ifdef CONFIG_RMI_VMIPS
+	for(;tlbe >= tlbs; tlbe--)
+        rmi_vmips_wired_entry_remove(tlbe); 
+
+#endif
+	cacheop_sync_istream();
+}
+
+static void phoenix_flush_icache_range_paddr_ipi(void *info)
+{
+	struct flush_icache_range_args_paddr *args = info;
+
+	optimize_thread_flush();
+
+	phoenix_local_flush_icache_range_paddr(args->start, args->end);
+}
+
+void phoenix_flush_icache_range_paddr(phys_t start)
+{
+	struct flush_icache_range_args_paddr args;
+
+#ifdef CONFIG_PHOENIX_VM_DEBUG
+	dbg_msg("return address: ");
+	print_symbol("ra[0]=%s\n", (unsigned long) __builtin_return_address(0));
+#endif
+  
+	args.start = start;
+	args.end = start + PAGE_SIZE;
+	/* TODO: don't even send ipi to non-zero thread ids 
+	 * This may require some changes to smp_call_function interface, for now just avoid 
+	 * redundant cache ops
+	 */
+	on_each_cpu(phoenix_flush_icache_range_paddr_ipi, &args, 1);
+}
diff --git a/arch/mips/mm/cache.c b/arch/mips/mm/cache.c
index 789c97a..70426ef 100644
--- a/arch/mips/mm/cache.c
+++ b/arch/mips/mm/cache.c
@@ -113,26 +113,47 @@ void __flush_anon_page(struct page *page, unsigned long vmaddr)
 
 EXPORT_SYMBOL(__flush_anon_page);
 
+extern void phoenix_flush_icache_range_paddr(phys_t start);
 void __update_cache(struct vm_area_struct *vma, unsigned long address,
 	pte_t pte)
 {
 	struct page *page;
-	unsigned long pfn, addr;
-	int exec = (vma->vm_flags & VM_EXEC) && !cpu_has_ic_fills_f_dc;
 
-	pfn = pte_pfn(pte);
-	if (unlikely(!pfn_valid(pfn)))
-		return;
-	page = pfn_to_page(pfn);
-	if (page_mapping(page) && Page_dcache_dirty(page)) {
-		addr = (unsigned long) page_address(page);
-		if (exec || pages_do_alias(addr, address & PAGE_MASK))
-			flush_data_cache_page(addr);
-		ClearPageDcacheDirty(page);
+	if (cpu_has_phoenix_cache) {
+		phys_t start;
+		if (!(vma->vm_flags & VM_EXEC)) 
+			return;
+		page = pte_page(pte);
+		if(test_bit(PG_dcache_dirty, &(page)->flags)) 
+			return;
+		start = (phys_t)pte_pfn(pte);
+		phoenix_flush_icache_range_paddr(start << PAGE_SHIFT);
+		SetPageDcacheDirty(page);
+	}else{
+		unsigned long pfn, addr;
+		int exec = (vma->vm_flags & VM_EXEC) && !cpu_has_ic_fills_f_dc;
+
+		pfn = pte_pfn(pte);
+		if (unlikely(!pfn_valid(pfn)))
+			return;
+		page = pfn_to_page(pfn);
+		if (page_mapping(page) && Page_dcache_dirty(page)) {
+			addr = (unsigned long) page_address(page);
+			if (exec || pages_do_alias(addr, address & PAGE_MASK))
+				flush_data_cache_page(addr);
+			ClearPageDcacheDirty(page);
+		}
 	}
 }
 
+#ifdef CONFIG_RMI_PHOENIX
+/* This variable needs to be initialized before setup_arch() if this is not
+   initialized like below
+   */
+unsigned long _page_cachable_default = (0x3 << _CACHE_SHIFT);
+#else
 unsigned long _page_cachable_default;
+#endif
 EXPORT_SYMBOL_GPL(_page_cachable_default);
 
 static inline void setup_protection_map(void)
@@ -182,6 +203,11 @@ void __devinit cpu_cache_init(void)
 
 		tx39_cache_init();
 	}
+	if (cpu_has_phoenix_cache) {
+		extern void __weak ld_mmu_phoenix(void);
+
+		ld_mmu_phoenix();
+	}
 
 	setup_protection_map();
 }
@@ -190,6 +216,14 @@ int __weak __uncached_access(struct file *file, unsigned long addr)
 {
 	if (file->f_flags & O_SYNC)
 		return 1;
+  
+	if (cpu_has_phoenix_cache) {	
+		extern int phnx_get_pgprot(unsigned long address);
+	  	/* check the address region, return uncached pages for IO space and 
+	     	cached page for memory space. */
+		return phnx_get_pgprot(addr);
+	}
+
 
 	return addr >= __pa(high_memory);
 }
diff --git a/include/asm-mips/cache.h b/include/asm-mips/cache.h
index 37f175c..9f32a4e 100644
--- a/include/asm-mips/cache.h
+++ b/include/asm-mips/cache.h
@@ -17,4 +17,6 @@
 #define SMP_CACHE_SHIFT		L1_CACHE_SHIFT
 #define SMP_CACHE_BYTES		L1_CACHE_BYTES
 
+#define ARCH_KMALLOC_MINALIGN   8
+
 #endif /* _ASM_CACHE_H */
diff --git a/include/asm-mips/cacheflush.h b/include/asm-mips/cacheflush.h
index 03b1d69..62c0dd8 100644
--- a/include/asm-mips/cacheflush.h
+++ b/include/asm-mips/cacheflush.h
@@ -40,9 +40,14 @@ extern void __flush_dcache_page(struct page *page);
 
 static inline void flush_dcache_page(struct page *page)
 {
+#ifdef CONFIG_RMI_PHOENIX
+	extern void phoenix_flush_dcache_page(struct page *page);
+	phoenix_flush_dcache_page(page);  
+#else
 	if (cpu_has_dc_aliases || !cpu_has_ic_fills_f_dc)
 		__flush_dcache_page(page);
 
+#endif
 }
 
 #define flush_dcache_mmap_lock(mapping)		do { } while (0)
-- 
1.6.0.4

