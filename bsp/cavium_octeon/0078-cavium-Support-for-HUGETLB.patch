From e22d5e69cc15da0e6a9c2b1d2fdaae3dccc3dc93 Mon Sep 17 00:00:00 2001
From: Wally Gleemer <Wally.Gleemer@windriver.com>
Date: Tue, 25 Mar 2008 16:21:41 -0700
Subject: [PATCH] cavium: Support for HUGETLB

Add in MIPS support for HUGETLB; currently only accesible for
Cavium via the Kconfigs.

Signed-off-by: Wally Gleemer <Wally.Gleemer@windriver.com>
---
 arch/mips/Kconfig              |   14 ++-
 arch/mips/mm/Makefile          |    1 +
 arch/mips/mm/hugetlbpage.c     |   97 +++++++++++
 arch/mips/mm/tlbex.c           |  360 +++++++++++++++++++++++++++++++++++++++-
 include/asm-mips/mipsregs.h    |    9 +-
 include/asm-mips/page.h        |   10 +
 include/asm-mips/pgtable.h     |   40 +++++
 include/asm-mips/sparsemem.h   |    5 +
 include/asm-mips/thread_info.h |    3 +
 9 files changed, 531 insertions(+), 8 deletions(-)
 create mode 100644 arch/mips/mm/hugetlbpage.c

diff --git a/arch/mips/Kconfig b/arch/mips/Kconfig
index 61460f7..063c776 100644
--- a/arch/mips/Kconfig
+++ b/arch/mips/Kconfig
@@ -1439,11 +1439,11 @@ config PAGE_SIZE_4KB
 
 config PAGE_SIZE_8KB
 	bool "8kB"
-	depends on EXPERIMENTAL && CPU_R8000
+	depends on EXPERIMENTAL && (CPU_R8000 || CPU_CAVIUM_OCTEON)
 	help
 	  Using 8kB page size will result in higher performance kernel at
 	  the price of higher memory consumption.  This option is available
-	  only on the R8000 processor.  Not that at the time of this writing
+	  only on the R8000 & Octeon processor.  Not that at the time of this writing
 	  this option is still high experimental; there are also issues with
 	  compatibility of user applications.
 
@@ -1456,6 +1456,16 @@ config PAGE_SIZE_16KB
 	  all non-R3000 family processors.  Note that you will need a suitable
 	  Linux distribution to support this.
 
+config PAGE_SIZE_32KB
+	bool "32kB"
+	depends on EXPERIMENTAL && CPU_CAVIUM_OCTEON
+	help
+	  Using 32kB page size will result in higher performance kernel at
+	  the price of higher memory consumption.  This option is available on
+	  all Octeon processors.  Not that at the time of this writing this
+	  option is still high experimental; there are also issues with
+	  compatibility of user applications.
+
 config PAGE_SIZE_64KB
 	bool "64kB"
 	depends on EXPERIMENTAL && !CPU_R3000 && !CPU_TX39XX
diff --git a/arch/mips/mm/Makefile b/arch/mips/mm/Makefile
index 6994d84..ee3d170 100644
--- a/arch/mips/mm/Makefile
+++ b/arch/mips/mm/Makefile
@@ -18,6 +18,7 @@ endif
 obj-$(CONFIG_32BIT)		+= ioremap.o pgtable-32.o
 obj-$(CONFIG_64BIT)		+= pgtable-64.o
 obj-$(CONFIG_HIGHMEM)		+= highmem.o
+obj-$(CONFIG_HUGETLB_PAGE)	+= hugetlbpage.o
 
 obj-$(CONFIG_CPU_LOONGSON2)	+= c-r4k.o cex-gen.o pg-r4k.o tlb-r4k.o
 obj-$(CONFIG_CPU_MIPS32)	+= c-r4k.o cex-gen.o pg-r4k.o tlb-r4k.o
diff --git a/arch/mips/mm/hugetlbpage.c b/arch/mips/mm/hugetlbpage.c
new file mode 100644
index 0000000..e42fc37
--- /dev/null
+++ b/arch/mips/mm/hugetlbpage.c
@@ -0,0 +1,97 @@
+/*
+ * MIPS Huge TLB Page Support for Kernel.
+ *
+ * Copyright (C) 2002, Rohit Seth <rohit.seth@intel.com>
+ * Copyright 2005, Embedded Alley Solutions, Inc.
+ * Matt Porter <mporter@embeddedalley.com>
+ */
+
+#include <linux/init.h>
+#include <linux/fs.h>
+#include <linux/mm.h>
+#include <linux/hugetlb.h>
+#include <linux/pagemap.h>
+#include <linux/smp_lock.h>
+#include <linux/slab.h>
+#include <linux/err.h>
+#include <linux/sysctl.h>
+#include <asm/mman.h>
+#include <asm/tlb.h>
+#include <asm/tlbflush.h>
+
+pte_t *huge_pte_alloc(struct mm_struct *mm, unsigned long addr)
+{
+	pgd_t *pgd;
+	pud_t *pud;
+	pte_t *pte = NULL;
+
+
+	pgd = pgd_offset(mm, addr);
+	pud = pud_alloc(mm, pgd, addr);
+	if (pud)
+		pte = (pte_t *)pmd_alloc(mm, pud, addr);
+
+	if (!pmd_none(*(pmd_t*)pte) && !pte_none(*pte))
+		return pte;
+
+	if (pte)
+		pte_clear(mm, addr, pte);
+
+	return pte;
+}
+
+pte_t *huge_pte_offset(struct mm_struct *mm, unsigned long addr)
+{
+	pgd_t *pgd;
+	pud_t *pud;
+	pmd_t *pmd = NULL;
+
+	pgd = pgd_offset(mm, addr);
+	if (pgd_present(*pgd)) {
+		pud = pud_offset(pgd, addr);
+		if (pud_present(*pud))
+			pmd = pmd_offset(pud, addr);
+	}
+	return (pte_t *) pmd;
+}
+
+int huge_pmd_unshare(struct mm_struct *mm, unsigned long *addr, pte_t *ptep)
+{
+        return 0;
+}
+
+/*
+ * This function checks for proper alignment of input addr and len parameters.
+ */
+int is_aligned_hugepage_range(unsigned long addr, unsigned long len)
+{
+	if (len & ~HPAGE_MASK)
+		return -EINVAL;
+	if (addr & ~HPAGE_MASK)
+		return -EINVAL;
+	return 0;
+}
+
+struct page *
+follow_huge_addr(struct mm_struct *mm, unsigned long address, int write)
+{
+	return ERR_PTR(-EINVAL);
+}
+
+int pmd_huge(pmd_t pmd)
+{
+	return !!(pmd_val(pmd) & _PAGE_HUGE);
+}
+
+struct page *
+follow_huge_pmd(struct mm_struct *mm, unsigned long address,
+		pmd_t *pmd, int write)
+{
+	struct page *page;
+
+	page = pte_page(*(pte_t *)pmd);
+	if (page)
+		page += ((address & ~HPAGE_MASK) >> PAGE_SHIFT);
+	return page;
+}
+
diff --git a/arch/mips/mm/tlbex.c b/arch/mips/mm/tlbex.c
index 4ed7291..080f1b6 100644
--- a/arch/mips/mm/tlbex.c
+++ b/arch/mips/mm/tlbex.c
@@ -82,6 +82,9 @@ enum label_id {
 	label_nopage_tlbm,
 	label_smp_pgtable_change,
 	label_r3000_write_probe_fail,
+#ifdef CONFIG_HUGETLB_PAGE
+	label_tlb_huge_update,
+#endif
 };
 
 UASM_L_LA(_second_part)
@@ -98,6 +101,9 @@ UASM_L_LA(_nopage_tlbs)
 UASM_L_LA(_nopage_tlbm)
 UASM_L_LA(_smp_pgtable_change)
 UASM_L_LA(_r3000_write_probe_fail)
+#ifdef CONFIG_HUGETLB_PAGE
+UASM_L_LA(_tlb_huge_update)
+#endif
 
 /*
  * For debug purposes.
@@ -125,9 +131,7 @@ static inline void dump_handler(const u32 *handler, int count)
 #define C0_TCBIND	2, 2
 #define C0_ENTRYLO1	3, 0
 #define C0_CONTEXT	4, 0
-#ifdef CONFIG_CPU_CAVIUM_OCTEON
 #define C0_PAGEMASK	5, 0
-#endif
 #define C0_BADVADDR	8, 0
 #define C0_ENTRYHI	10, 0
 #define C0_EPC		14, 0
@@ -389,6 +393,167 @@ static void __cpuinit build_tlb_write_entry(u32 **p, struct uasm_label **l,
 	}
 }
 
+#ifdef CONFIG_HUGETLB_PAGE
+
+static __init void build_huge_tlb_write_entry(u32 **p, struct label **l,
+					 struct reloc **r,
+					 unsigned int tmp,
+					 enum tlb_write_entry wmode)
+{
+	void(*tlbw)(u32 **) = NULL;
+
+	switch (wmode) {
+	case tlb_random: tlbw = i_tlbwr; break;
+	case tlb_indexed: tlbw = i_tlbwi; break;
+	}
+
+	/* Set huge page tlb entry size */
+	uasm_i_lui(p, tmp, PM_HUGE_MASK >> 16);
+	uasm_i_ori(p, tmp, tmp, PM_HUGE_MASK & 0xffff);
+	uasm_i_mtc0(p, tmp, C0_PAGEMASK);
+
+	switch (current_cpu_data.cputype) {
+	case CPU_R4000PC:
+	case CPU_R4000SC:
+	case CPU_R4000MC:
+	case CPU_R4400PC:
+	case CPU_R4400SC:
+	case CPU_R4400MC:
+		/*
+		 * This branch uses up a mtc0 hazard nop slot and saves
+		 * two nops after the tlbw instruction.
+		 */
+		uasm_il_bgezl(p, r, 0, label_tlbw_hazard);
+		tlbw(p);
+		uasm_l_tlbw_hazard(l, *p);
+		uasm_i_nop(p);
+		break;
+
+	case CPU_R4600:
+	case CPU_R4700:
+	case CPU_R5000:
+	case CPU_R5000A:
+		uasm_i_nop(p);
+		tlbw(p);
+		uasm_i_nop(p);
+		break;
+
+	case CPU_R4300:
+	case CPU_5KC:
+	case CPU_TX49XX:
+	case CPU_AU1000:
+	case CPU_AU1100:
+	case CPU_AU1500:
+	case CPU_AU1550:
+	case CPU_AU1200:
+	case CPU_PR4450:
+		uasm_i_nop(p);
+		tlbw(p);
+		break;
+
+	case CPU_R10000:
+	case CPU_R12000:
+	case CPU_4KC:
+	case CPU_SB1:
+	case CPU_4KSC:
+	case CPU_20KC:
+	case CPU_25KF:
+	case CPU_CAVIUM_OCTEON:
+		tlbw(p);
+		break;
+
+	case CPU_NEVADA:
+		uasm_i_nop(p); /* QED specifies 2 nops hazard */
+		/*
+		 * This branch uses up a mtc0 hazard nop slot and saves
+		 * a nop after the tlbw instruction.
+		 */
+		uasm_il_bgezl(p, r, 0, label_tlbw_hazard);
+		tlbw(p);
+		uasm_l_tlbw_hazard(l, *p);
+		break;
+
+	case CPU_RM7000:
+		uasm_i_nop(p);
+		uasm_i_nop(p);
+		uasm_i_nop(p);
+		uasm_i_nop(p);
+		tlbw(p);
+		break;
+
+	case CPU_4KEC:
+	case CPU_24K:
+	case CPU_34K:
+		uasm_i_ehb(p);
+		tlbw(p);
+		break;
+
+	case CPU_RM9000:
+		/*
+		 * When the JTLB is updated by tlbwi or tlbwr, a subsequent
+		 * use of the JTLB for instructions should not occur for 4
+		 * cpu cycles and use for data translations should not occur
+		 * for 3 cpu cycles.
+		 */
+		uasm_i_ssnop(p);
+		uasm_i_ssnop(p);
+		uasm_i_ssnop(p);
+		uasm_i_ssnop(p);
+		tlbw(p);
+		uasm_i_ssnop(p);
+		uasm_i_ssnop(p);
+		uasm_i_ssnop(p);
+		uasm_i_ssnop(p);
+		break;
+
+	case CPU_VR4111:
+	case CPU_VR4121:
+	case CPU_VR4122:
+	case CPU_VR4181:
+	case CPU_VR4181A:
+		uasm_i_nop(p);
+		uasm_i_nop(p);
+		tlbw(p);
+		uasm_i_nop(p);
+		uasm_i_nop(p);
+		break;
+
+	case CPU_VR4131:
+	case CPU_VR4133:
+	case CPU_R5432:
+		uasm_i_nop(p);
+		uasm_i_nop(p);
+		tlbw(p);
+		break;
+
+	default:
+		panic("No TLB refill handler yet (CPU type: %d)",
+		      current_cpu_data.cputype);
+		break;
+	}
+
+	/* Reset default page size */
+	if (PM_DEFAULT_MASK>>16)
+	{
+		uasm_i_lui(p, tmp, PM_DEFAULT_MASK >> 16);
+		uasm_i_ori(p, tmp, tmp, PM_DEFAULT_MASK & 0xffff);
+		uasm_i_mtc0(p, tmp, C0_PAGEMASK);
+	}
+	else if (PM_DEFAULT_MASK)
+	{
+		uasm_i_ori(p, tmp, 0, PM_DEFAULT_MASK);
+		uasm_i_mtc0(p, tmp, C0_PAGEMASK);
+	}
+	else
+		uasm_i_mtc0(p, 0, C0_PAGEMASK);
+
+	/* Jump past default page size TLB load */
+	uasm_il_b(p, r, label_leave);
+	uasm_i_nop(p);
+}
+
+#endif
+
 #ifdef CONFIG_64BIT
 /*
  * TMP and PTR are scratch.
@@ -656,6 +821,54 @@ static void __cpuinit build_update_entries(u32 **p, unsigned int tmp,
 #endif
 }
 
+#ifdef CONFIG_HUGETLB_PAGE
+
+/*
+ * Check if Huge PTE is present, if so then jump to LABEL.
+ */
+static void __init
+build_is_huge_pte(u32 **p, struct reloc **r, unsigned int tmp,
+		unsigned int pmd, int lid)
+{
+	UASM_i_LW(p, tmp, 0, pmd);
+#ifdef CONFIG_CPU_CAVIUM_OCTEON
+	uasm_il_bbit1(p, r, tmp, __ilog2(_PAGE_HUGE), lid);
+#else
+	uasm_i_andi(p, tmp, tmp, _PAGE_HUGE);
+	uasm_il_bnez(p, r, tmp, lid);
+#endif
+}
+
+static __init void build_huge_update_entries(u32 **p, struct label **l,
+		unsigned int tmp, unsigned int pmd)
+{
+	uasm_l_tlb_huge_update(l, *p);
+
+	/*
+	 * A huge PTE describes an area the size of the
+	 * configured huge page size. This is twice the
+	 * of the large TLB entry size we intend to use.
+	 * A TLB entry half the size of the configured
+	 * huge page size is configured into entrylo0
+	 * and entrylo1 to cover the contiguous huge PTE
+	 * address space.
+	 */
+	UASM_i_LW(p, tmp, 0, pmd);
+	uasm_i_SRL(p, tmp, tmp, 6); /* convert to entrylo0 */
+	uasm_i_mtc0(p, tmp, C0_ENTRYLO0); /* load it */
+	if (HPAGE_SIZE>>7 < 0x10000)
+		uasm_i_ADDIU(p, tmp, tmp, HPAGE_SIZE >> 7); /* convert to entrylo1 */
+	else
+	{
+		uasm_i_nor(p, tmp, tmp, 0);
+		uasm_i_dins(p, tmp, 0, HPAGE_SHIFT-7, 1);
+		uasm_i_nor(p, tmp, tmp, 0);
+	}
+	uasm_i_mtc0(p, tmp, C0_ENTRYLO1); /* load it */
+}
+
+#endif
+
 static void __cpuinit build_r4000_tlb_refill_handler(void)
 {
 	u32 *p = tlb_handler;
@@ -687,6 +900,10 @@ static void __cpuinit build_r4000_tlb_refill_handler(void)
 	build_get_pgde32(&p, K0, K1); /* get pgd in K1 */
 #endif
 
+#ifdef CONFIG_HUGETLB_PAGE
+	build_is_huge_pte(&p, &r, K0, K1, label_tlb_huge_update);
+#endif
+
 	build_get_ptep(&p, K0, K1);
 	build_update_entries(&p, K0, K1);
 	build_tlb_write_entry(&p, &l, &r, tlb_random);
@@ -696,6 +913,11 @@ static void __cpuinit build_r4000_tlb_refill_handler(void)
 #endif
 	uasm_i_eret(&p); /* return from trap */
 
+#ifdef CONFIG_HUGETLB_PAGE
+	build_huge_update_entries(&p, &l, K0, K1);
+	build_huge_tlb_write_entry(&p, &l, &r, K0, tlb_random);
+#endif
+
 #ifdef CONFIG_64BIT
 	build_get_pgd_vmalloc64(&p, &l, &r, K0, K1);
 #endif
@@ -720,8 +942,10 @@ static void __cpuinit build_r4000_tlb_refill_handler(void)
 
 	/*
 	 * Now fold the handler in the TLB refill handler space.
+	 * Note: We always do this on Octeon since IO addresses are at
+	 * 64bit addresses. CP0_STATUS[ST0_KX] is always set.
 	 */
-#if defined(CONFIG_32BIT) || defined(CONFIG_CPU_LOONGSON2)
+#if (defined(CONFIG_32BIT) || defined(CONFIG_CPU_LOONGSON2)) && !defined(CONFIG_CPU_CAVIUM_OCTEON)
 	f = final_handler;
 	/* Simplest case, just copy the handler. */
 	uasm_copy_handler(relocs, labels, tlb_handler, p, f);
@@ -875,10 +1099,18 @@ static void __cpuinit
 build_pte_present(u32 **p, struct uasm_label **l, struct uasm_reloc **r,
 		  unsigned int pte, unsigned int ptr, enum label_id lid)
 {
+#if defined(_PAGE_NO_READ) || defined(_PAGE_NO_EXEC)
+	/* FIXME: We need to check if the read or execute failed because of
+		an inhibit bit or the valid bit being off. Right now we assume
+		it was because of the valid bit, so might do the wrong thing */
+	uasm_il_bbit0(p, r, pte, __ilog2(_PAGE_PRESENT), lid);
+	uasm_i_nop(p);
+#else
 	uasm_i_andi(p, pte, pte, _PAGE_PRESENT | _PAGE_READ);
 	uasm_i_xori(p, pte, pte, _PAGE_PRESENT | _PAGE_READ);
 	uasm_il_bnez(p, r, pte, lid);
 	iPTE_LW(p, l, pte, ptr);
+#endif
 }
 
 /* Make PTE valid, store result in PTR. */
@@ -926,9 +1158,14 @@ static void __cpuinit
 build_pte_modifiable(u32 **p, struct uasm_label **l, struct uasm_reloc **r,
 		     unsigned int pte, unsigned int ptr, enum label_id lid)
 {
+#ifdef CONFIG_CPU_CAVIUM_OCTEON
+	uasm_il_bbit0(p, r, pte, __ilog2(_PAGE_WRITE), lid);
+	uasm_i_nop(p);
+#else
 	uasm_i_andi(p, pte, pte, _PAGE_WRITE);
 	uasm_il_beqz(p, r, pte, lid);
 	iPTE_LW(p, l, pte, ptr);
+#endif
 }
 
 /*
@@ -1097,6 +1334,15 @@ build_r4000_tlbchange_handler_head(u32 **p, struct uasm_label **l,
 	build_get_pgde32(p, pte, ptr); /* get pgd in ptr */
 #endif
 
+#ifdef CONFIG_HUGETLB_PAGE
+	/* For huge tlb entries, pmd doesn't contain an address but
+		instead contains the tlb pte. Check the PAGE_HUGE bit
+		and see if we need to jump to huge tlb processing */
+	UASM_i_LW(p, pte, 0, ptr);
+	uasm_il_bbit1(p, r, pte, __ilog2(_PAGE_HUGE), label_tlb_huge_update);
+	uasm_i_nop(p); /* delay slot */
+#endif
+
 	UASM_i_MFC0(p, pte, C0_BADVADDR);
 	UASM_i_LW(p, ptr, 0, ptr);
 	UASM_i_SRL(p, pte, pte, PAGE_SHIFT + PTE_ORDER - PTE_T_LOG2);
@@ -1133,6 +1379,10 @@ build_r4000_tlbchange_handler_tail(u32 **p, struct uasm_label **l,
 
 static void __cpuinit build_r4000_tlb_load_handler(void)
 {
+#ifdef CONFIG_HUGETLB_PAGE
+	const int PTE = K0;
+	const int PTR = K1;
+#endif
 	u32 *p = handle_tlbl;
 	struct uasm_label *l = labels;
 	struct uasm_reloc *r = relocs;
@@ -1157,6 +1407,43 @@ static void __cpuinit build_r4000_tlb_load_handler(void)
 	build_make_valid(&p, &r, K0, K1);
 	build_r4000_tlbchange_handler_tail(&p, &l, &r, K0, K1);
 
+#ifdef CONFIG_HUGETLB_PAGE
+	/* This is the entry point when build_r4000_tlbchange_handler_head
+		spots a huge page */
+	uasm_l_tlb_huge_update(&l, p);
+	iPTE_LW(&p, &l, PTE, PTR);
+	uasm_il_bbit0(&p, &r, PTE, __ilog2(_PAGE_PRESENT), label_nopage_tlbl);
+	build_tlb_probe_entry(&p);
+#if defined(_PAGE_NO_READ) || defined(_PAGE_NO_EXEC)
+	/* FIXME: We need to check if the read or execute failed because of
+		an inhibit bit or the valid bit being off. Right now we assume
+		it was because of the valid bit, so might do the wrong thing */
+	// Nothing for now?
+#else
+	uasm_il_bbit0(&p, &r, PTE, __ilog2(_PAGE_READ), label_nopage_tlbl);
+#endif
+	uasm_i_ori(&p, PTE, PTE, (_PAGE_ACCESSED | _PAGE_VALID));
+#ifdef CONFIG_SMP
+	uasm_i_SC(&p, PTE, 0, PTR);
+	uasm_il_beqz(&p, &r, PTE, label_tlb_huge_update);
+	UASM_i_LW(&p, PTE, 0, PTR); /* Needed because SC killed our PTE */
+#else
+	uasm_i_SW(&p, PTE, 0, PTR);
+#endif
+	uasm_i_SRL(&p, PTE, PTE, 6); /* convert to entrylo0 */
+	uasm_i_mtc0(&p, PTE, C0_ENTRYLO0); /* load it */
+	if (HPAGE_SIZE>>7 < 0x10000)
+		uasm_i_addiu(&p, PTE, PTE, HPAGE_SIZE>>7); /* convert to entrylo1 */
+	else
+	{
+		uasm_i_nor(&p, PTE, PTE, 0);
+		uasm_i_dins(&p, PTE, 0, HPAGE_SHIFT-7, 1);
+		uasm_i_nor(&p, PTE, PTE, 0);
+	}
+	uasm_i_mtc0(&p, PTE, C0_ENTRYLO1); /* load it */
+	build_huge_tlb_write_entry(&p, &l, &r, PTE, tlb_indexed);
+#endif
+
 	uasm_l_nopage_tlbl(&l, p);
 	uasm_i_j(&p, (unsigned long)tlb_do_page_fault_0 & 0x0fffffff);
 	uasm_i_nop(&p);
@@ -1173,6 +1460,10 @@ static void __cpuinit build_r4000_tlb_load_handler(void)
 
 static void __cpuinit build_r4000_tlb_store_handler(void)
 {
+#ifdef CONFIG_HUGETLB_PAGE
+	const int PTE = K0;
+	const int PTR = K1;
+#endif
 	u32 *p = handle_tlbs;
 	struct uasm_label *l = labels;
 	struct uasm_reloc *r = relocs;
@@ -1188,6 +1479,36 @@ static void __cpuinit build_r4000_tlb_store_handler(void)
 	build_make_write(&p, &r, K0, K1);
 	build_r4000_tlbchange_handler_tail(&p, &l, &r, K0, K1);
 
+#ifdef CONFIG_HUGETLB_PAGE
+	/* This is the entry point when build_r4000_tlbchange_handler_head
+		spots a huge page */
+	uasm_l_tlb_huge_update(&l, p);
+	uasm_iPTE_LW(&p, &l, PTE, PTR);
+	uasm_il_bbit0(&p, &r, PTE, __ilog2(_PAGE_PRESENT), label_nopage_tlbs);
+	build_tlb_probe_entry(&p);
+	uasm_il_bbit0(&p, &r, PTE, __ilog2(_PAGE_WRITE), label_nopage_tlbs);
+	uasm_i_ori(&p, PTE, PTE, (_PAGE_ACCESSED | _PAGE_MODIFIED | _PAGE_VALID | _PAGE_DIRTY));
+#ifdef CONFIG_SMP
+	uasm_i_SC(&p, PTE, 0, PTR);
+	uasm_il_beqz(&p, &r, PTE, label_tlb_huge_update);
+	UASM_i_LW(&p, PTE, 0, PTR); /* Needed because SC killed our PTE */
+#else
+	uasm_i_SW(&p, PTE, 0, PTR);
+#endif
+	uasm_i_SRL(&p, PTE, PTE, 6); /* convert to entrylo0 */
+	uasm_i_mtc0(&p, PTE, C0_ENTRYLO0); /* load it */
+	if (HPAGE_SIZE>>7 < 0x10000)
+		uasm_i_addiu(&p, PTE, PTE, HPAGE_SIZE>>7); /* convert to entrylo1 */
+	else
+	{
+		uasm_i_nor(&p, PTE, PTE, 0);
+		uasm_i_dins(&p, PTE, 0, HPAGE_SHIFT-7, 1);
+		uasm_i_nor(&p, PTE, PTE, 0);
+	}
+	uasm_i_mtc0(&p, PTE, C0_ENTRYLO1); /* load it */
+	build_huge_tlb_write_entry(&p, &l, &r, PTE, tlb_indexed);
+#endif
+
 	uasm_l_nopage_tlbs(&l, p);
 	uasm_i_j(&p, (unsigned long)tlb_do_page_fault_1 & 0x0fffffff);
 	uasm_i_nop(&p);
@@ -1204,6 +1525,10 @@ static void __cpuinit build_r4000_tlb_store_handler(void)
 
 static void __cpuinit build_r4000_tlb_modify_handler(void)
 {
+#ifdef CONFIG_HUGETLB_PAGE
+	const int PTE = K0;
+	const int PTR = K1;
+#endif
 	u32 *p = handle_tlbm;
 	struct uasm_label *l = labels;
 	struct uasm_reloc *r = relocs;
@@ -1220,6 +1545,35 @@ static void __cpuinit build_r4000_tlb_modify_handler(void)
 	build_make_write(&p, &r, K0, K1);
 	build_r4000_tlbchange_handler_tail(&p, &l, &r, K0, K1);
 
+#ifdef CONFIG_HUGETLB_PAGE
+	/* This is the entry point when build_r4000_tlbchange_handler_head
+		spots a huge page */
+	uasm_l_tlb_huge_update(&l, p);
+	uasm_iPTE_LW(&p, &l, PTE, PTR);
+	uasm_il_bbit0(&p, &r, PTE, __ilog2(_PAGE_WRITE), label_nopage_tlbm);
+	build_tlb_probe_entry(&p);
+	uasm_i_ori(&p, PTE, PTE, (_PAGE_ACCESSED | _PAGE_MODIFIED | _PAGE_VALID | _PAGE_DIRTY));
+#ifdef CONFIG_SMP
+	uasm_i_SC(&p, PTE, 0, PTR);
+	uasm_il_beqz(&p, &r, PTE, label_tlb_huge_update);
+	UASM_i_LW(&p, PTE, 0, PTR); /* Needed because SC killed our PTE */
+#else
+	uasm_i_SW(&p, PTE, 0, PTR);
+#endif
+	uasm_i_SRL(&p, PTE, PTE, 6); /* convert to entrylo0 */
+	uasm_i_mtc0(&p, PTE, C0_ENTRYLO0); /* load it */
+	if (HPAGE_SIZE>>7 < 0x10000)
+		uasm_i_addiu(&p, PTE, PTE, HPAGE_SIZE>>7); /* convert to entrylo1 */
+	else
+	{
+		uasm_i_nor(&p, PTE, PTE, 0);
+		uasm_i_dins(&p, PTE, 0, HPAGE_SHIFT-7, 1);
+		uasm_i_nor(&p, PTE, PTE, 0);
+	}
+	uasm_i_mtc0(&p, PTE, C0_ENTRYLO1); /* load it */
+	build_huge_tlb_write_entry(&p, &l, &r, PTE, tlb_indexed);
+#endif
+
 	uasm_l_nopage_tlbm(&l, p);
 	uasm_i_j(&p, (unsigned long)tlb_do_page_fault_1 & 0x0fffffff);
 	uasm_i_nop(&p);
diff --git a/include/asm-mips/mipsregs.h b/include/asm-mips/mipsregs.h
index 9a73634..4b89a8e 100644
--- a/include/asm-mips/mipsregs.h
+++ b/include/asm-mips/mipsregs.h
@@ -184,9 +184,9 @@
 #else
 
 #define PM_4K		0x00000000
-#define PM_8K		0x00002000
+#define PM_8K		0x00002000  /* Used for Cavium Octeon */
 #define PM_16K		0x00006000
-#define PM_32K		0x0000e000
+#define PM_32K		0x0000e000  /* Used for Cavium Octeon */
 #define PM_64K		0x0001e000
 #define PM_256K		0x0007e000
 #define PM_1M		0x001fe000
@@ -229,7 +229,7 @@
 #elif defined(CONFIG_PAGE_SIZE_64KB)
 #define PM_HUGE_MASK	PM_256M
 #else
-#error Bad page size configuration (only 4KB supported for hugetlbfs)!
+#error Bad page size configuration (only 4KB, 8KB, 16KB, 32KB, and 64KB supported for hugetlbfs)!
 #endif
 #endif
 
@@ -817,6 +817,9 @@ do {									\
 #define read_c0_pagemask()	__read_32bit_c0_register($5, 0)
 #define write_c0_pagemask(val)	__write_32bit_c0_register($5, 0, val)
 
+#define read_c0_pagegrain()	__read_32bit_c0_register($5, 1)
+#define write_c0_pagegrain(val)	__write_32bit_c0_register($5, 1, val)
+
 #define read_c0_wired()		__read_32bit_c0_register($6, 0)
 #define write_c0_wired(val)	__write_32bit_c0_register($6, 0, val)
 
diff --git a/include/asm-mips/page.h b/include/asm-mips/page.h
index 8735aa0..25796fd 100644
--- a/include/asm-mips/page.h
+++ b/include/asm-mips/page.h
@@ -23,12 +23,22 @@
 #ifdef CONFIG_PAGE_SIZE_16KB
 #define PAGE_SHIFT	14
 #endif
+#ifdef CONFIG_PAGE_SIZE_32KB
+#define PAGE_SHIFT	15
+#endif
 #ifdef CONFIG_PAGE_SIZE_64KB
 #define PAGE_SHIFT	16
 #endif
 #define PAGE_SIZE	(1UL << PAGE_SHIFT)
 #define PAGE_MASK       (~((1 << PAGE_SHIFT) - 1))
 
+#ifdef CONFIG_HUGETLB_PAGE
+#define HPAGE_SHIFT	(PAGE_SHIFT + PAGE_SHIFT - 3)
+#define HPAGE_SIZE	((1UL) << HPAGE_SHIFT)
+#define HPAGE_MASK	(~(HPAGE_SIZE - 1))
+#define HUGETLB_PAGE_ORDER	(HPAGE_SHIFT - PAGE_SHIFT)
+#endif
+
 #ifndef __ASSEMBLY__
 
 #include <linux/pfn.h>
diff --git a/include/asm-mips/pgtable.h b/include/asm-mips/pgtable.h
index 17a7703..78ab801 100644
--- a/include/asm-mips/pgtable.h
+++ b/include/asm-mips/pgtable.h
@@ -21,6 +21,28 @@
 struct mm_struct;
 struct vm_area_struct;
 
+#ifdef _PAGE_NO_EXEC
+#define PAGE_BASE_FLAGS (_PAGE_PRESENT | PAGE_CACHABLE_DEFAULT)
+#define PAGE_KERNEL __pgprot(PAGE_BASE_FLAGS | __READABLE | __WRITEABLE | _PAGE_GLOBAL)
+#define PAGE_COPY __pgprot(PAGE_BASE_FLAGS | _PAGE_NO_EXEC)
+#define __P000	__pgprot(PAGE_BASE_FLAGS | _PAGE_NO_EXEC | _PAGE_NO_READ | _PAGE_NO_EXEC)
+#define __P001	__pgprot(PAGE_BASE_FLAGS | _PAGE_NO_EXEC)
+#define __P010	__pgprot(PAGE_BASE_FLAGS | _PAGE_NO_EXEC | _PAGE_NO_READ)
+#define __P011	__pgprot(PAGE_BASE_FLAGS | _PAGE_NO_EXEC)
+#define __P100	__pgprot(PAGE_BASE_FLAGS /* | _PAGE_NO_READ */)
+#define __P101	__pgprot(PAGE_BASE_FLAGS)
+#define __P110	__pgprot(PAGE_BASE_FLAGS /* | _PAGE_NO_READ */)
+#define __P111	__pgprot(PAGE_BASE_FLAGS)
+
+#define __S000	__pgprot(PAGE_BASE_FLAGS | _PAGE_NO_EXEC | _PAGE_NO_READ)
+#define __S001	__pgprot(PAGE_BASE_FLAGS | _PAGE_NO_EXEC)
+#define __S010	__pgprot(PAGE_BASE_FLAGS | _PAGE_NO_EXEC | _PAGE_WRITE | _PAGE_NO_READ)
+#define __S011	__pgprot(PAGE_BASE_FLAGS | _PAGE_NO_EXEC | _PAGE_WRITE)
+#define __S100	__pgprot(PAGE_BASE_FLAGS /* | _PAGE_NO_READ */)
+#define __S101	__pgprot(PAGE_BASE_FLAGS)
+#define __S110	__pgprot(PAGE_BASE_FLAGS | _PAGE_WRITE /* | _PAGE_NO_READ */)
+#define __S111	__pgprot(PAGE_BASE_FLAGS | _PAGE_WRITE)
+#else
 #define PAGE_NONE	__pgprot(_PAGE_PRESENT | _CACHE_CACHABLE_NONCOHERENT)
 #define PAGE_SHARED	__pgprot(_PAGE_PRESENT | _PAGE_READ | _PAGE_WRITE | \
 			PAGE_CACHABLE_DEFAULT)
@@ -57,6 +79,7 @@ struct vm_area_struct;
 #define __S101	PAGE_READONLY
 #define __S110	PAGE_SHARED
 #define __S111	PAGE_SHARED
+#endif
 
 /*
  * ZERO_PAGE is a global shared page that is always zero; used
@@ -242,6 +265,9 @@ static inline int pte_write(pte_t pte)	{ return pte_val(pte) & _PAGE_WRITE; }
 static inline int pte_dirty(pte_t pte)	{ return pte_val(pte) & _PAGE_MODIFIED; }
 static inline int pte_young(pte_t pte)	{ return pte_val(pte) & _PAGE_ACCESSED; }
 static inline int pte_file(pte_t pte)	{ return pte_val(pte) & _PAGE_FILE; }
+#ifdef CONFIG_HUGETLB_PAGE
+static inline int pte_huge(pte_t pte)	{ return pte_val(pte) & _PAGE_HUGE; }
+#endif
 
 static inline pte_t pte_wrprotect(pte_t pte)
 {
@@ -279,12 +305,26 @@ static inline pte_t pte_mkdirty(pte_t pte)
 
 static inline pte_t pte_mkyoung(pte_t pte)
 {
+#ifdef _PAGE_NO_READ
+	pte_val(pte) |= _PAGE_ACCESSED;
+	if (!(pte_val(pte) & _PAGE_NO_READ))
+		pte_val(pte) |= _PAGE_SILENT_READ;
+#else
 	pte_val(pte) |= _PAGE_ACCESSED;
 	if (pte_val(pte) & _PAGE_READ)
 		pte_val(pte) |= _PAGE_SILENT_READ;
+#endif
+	return pte;
+}
+
+#ifdef CONFIG_HUGETLB_PAGE
+static inline pte_t pte_mkhuge(pte_t pte)
+{
+	pte_val(pte) |= _PAGE_HUGE;
 	return pte;
 }
 #endif
+#endif
 
 /*
  * Macro to make mark a page protection value as "uncacheable".  Note
diff --git a/include/asm-mips/sparsemem.h b/include/asm-mips/sparsemem.h
index 795ac6c..849b74c 100644
--- a/include/asm-mips/sparsemem.h
+++ b/include/asm-mips/sparsemem.h
@@ -6,7 +6,12 @@
  * SECTION_SIZE_BITS		2^N: how big each section will be
  * MAX_PHYSMEM_BITS		2^N: how much memory we can have in that space
  */
+#if defined(CONFIG_PAGE_SIZE_64KB) && defined(CONFIG_HUGETLB_PAGE)
+/* We need bigger sections with 64KB in order to support 512MB hugetlb */
+#define SECTION_SIZE_BITS       29
+#else
 #define SECTION_SIZE_BITS       28
+#endif
 #define MAX_PHYSMEM_BITS        35
 
 #endif /* CONFIG_SPARSEMEM */
diff --git a/include/asm-mips/thread_info.h b/include/asm-mips/thread_info.h
index b2772df..141fa74 100644
--- a/include/asm-mips/thread_info.h
+++ b/include/asm-mips/thread_info.h
@@ -75,6 +75,9 @@ register struct thread_info *__current_thread_info __asm__("$28");
 #ifdef CONFIG_PAGE_SIZE_16KB
 #define THREAD_SIZE_ORDER (0)
 #endif
+#ifdef CONFIG_PAGE_SIZE_32KB
+#define THREAD_SIZE_ORDER (0)
+#endif
 #ifdef CONFIG_PAGE_SIZE_64KB
 #define THREAD_SIZE_ORDER (0)
 #endif
-- 
1.5.5.1

