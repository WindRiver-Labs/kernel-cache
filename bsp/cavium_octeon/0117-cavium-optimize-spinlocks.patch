From a7ac4be91e1867914d9a894f16435e1c5a5a150c Mon Sep 17 00:00:00 2001
From: Wally Gleemer <Wally.Gleemer@windriver.com>
Date: Tue, 25 Mar 2008 16:21:46 -0700
Subject: [PATCH] cavium: optimize spinlocks

Add ticket based mechanism to spinlock handling, add in some
syncw for proper data integrity.

Signed-off-by: Wally Gleemer <Wally.Gleemer@windriver.com>
---
 include/asm-mips/spinlock.h       |  128 +++++++++++++++++++++++++++++++++++++
 include/asm-mips/spinlock_types.h |    9 +++
 2 files changed, 137 insertions(+), 0 deletions(-)

diff --git a/include/asm-mips/spinlock.h b/include/asm-mips/spinlock.h
index bb89701..19720c1 100644
--- a/include/asm-mips/spinlock.h
+++ b/include/asm-mips/spinlock.h
@@ -16,10 +16,17 @@
  * Your basic SMP spinlocks, allowing only a single CPU anywhere
  */
 
+#ifdef CONFIG_CPU_CAVIUM_OCTEON
+#define __raw_spin_is_locked(x) ((x)->now_serving != (x)->ticket)
+#define __raw_spin_lock_flags(lock, flags) __raw_spin_lock(lock)
+#define __raw_spin_unlock_wait(x) \
+	do { cpu_relax(); } while (__raw_spin_is_locked(x))
+#else
 #define __raw_spin_is_locked(x)       ((x)->lock != 0)
 #define __raw_spin_lock_flags(lock, flags) __raw_spin_lock(lock)
 #define __raw_spin_unlock_wait(x) \
 	do { cpu_relax(); } while ((x)->lock)
+#endif
 
 /*
  * Simple spin lock operations.  There are two variants, one clears IRQ's
@@ -30,6 +37,44 @@
 
 static inline void __raw_spin_lock(raw_spinlock_t *lock)
 {
+#ifdef CONFIG_CPU_CAVIUM_OCTEON
+	int tmp;
+	int my_ticket;
+	__asm__ __volatile__ (
+	"	.set push					\n"
+	"	.set noreorder					\n"
+	"1:							\n"
+	"	ll	%[my_ticket], %[ticket_ptr]		\n"
+	"	li	%[ticket], 1				\n"
+	"	addu	%[ticket], %[my_ticket]			\n"
+	"	sc	%[ticket], %[ticket_ptr]		\n"
+	"	beqz	%[ticket], 1b				\n"
+	"	 syncw						\n"
+	"	syncw						\n"
+	"	lw	%[ticket], %[now_serving]		\n"
+	"	bne	%[ticket], %[my_ticket], 2f		\n"
+	"	 subu	%[ticket], %[my_ticket], %[ticket]	\n"
+	"4:							\n"
+	"	.subsection 2					\n"
+	"2:							\n"
+	"	subu	%[ticket], 1				\n"
+	"	sll	%[ticket], 5				\n"
+	"3:							\n"
+	"	bnez	%[ticket], 3b				\n"
+	"	 subu	%[ticket], 1				\n"
+	"	lw	%[ticket], %[now_serving]		\n"
+	"	beq	%[ticket], %[my_ticket], 4b		\n"
+	"	 subu	%[ticket], %[my_ticket], %[ticket]	\n"
+	"	subu	%[ticket], 1				\n"
+	"	b	3b					\n"
+	"	 sll	%[ticket], 5				\n"
+	"	.previous					\n"
+	"	.set pop					\n"
+	: [ticket_ptr] "=m" (lock->ticket),
+	[now_serving] "=m" (lock->now_serving),
+	[ticket] "=r" (tmp),
+	[my_ticket] "=r" (my_ticket));
+#else
 	unsigned int tmp;
 
 	if (R10000_LLSC_WAR) {
@@ -68,10 +113,18 @@ static inline void __raw_spin_lock(raw_spinlock_t *lock)
 	}
 
 	smp_llsc_mb();
+#endif
 }
 
 static inline void __raw_spin_unlock(raw_spinlock_t *lock)
 {
+#ifdef CONFIG_CPU_CAVIUM_OCTEON
+	int now_serving = lock->now_serving;
+	wmb();
+	if (likely(now_serving != lock->ticket))
+		lock->now_serving = now_serving+1;
+	wmb();
+#else
 	smp_mb();
 
 	__asm__ __volatile__(
@@ -81,10 +134,17 @@ static inline void __raw_spin_unlock(raw_spinlock_t *lock)
 	: "=m" (lock->lock)
 	: "m" (lock->lock)
 	: "memory");
+#endif
 }
 
 static inline unsigned int __raw_spin_trylock(raw_spinlock_t *lock)
 {
+#ifdef CONFIG_CPU_CAVIUM_OCTEON
+	if (__raw_spin_is_locked(lock))
+		return 0;
+	__raw_spin_lock(lock);
+	return 1;
+#else
 	unsigned int temp, res;
 
 	if (R10000_LLSC_WAR) {
@@ -121,6 +181,7 @@ static inline unsigned int __raw_spin_trylock(raw_spinlock_t *lock)
 	smp_llsc_mb();
 
 	return res == 0;
+#endif
 }
 
 /*
@@ -162,6 +223,21 @@ static inline void __raw_read_lock(raw_rwlock_t *rw)
 		: "m" (rw->lock)
 		: "memory");
 	} else {
+#ifdef CONFIG_CPU_CAVIUM_OCTEON
+		__asm__ __volatile__(
+		"	.set	noreorder	# _raw_read_lock	\n"
+		OCTEON_SYNCW_STR
+		"1:	ll	%1, %2					\n"
+		"	bltz	%1, 1b					\n"
+		"	 addu	%1, 1					\n"
+		"	sc	%1, %0			# This will go out after the syncw completes\n"
+		"	beqz	%1, 1b					\n"
+		"	 nop # No syncw is needed here since the sc goes out immediately\n"
+		"	.set	reorder					\n"
+		: "=m" (rw->lock), "=&r" (tmp)
+		: "m" (rw->lock)
+		: "memory");
+#else
 		__asm__ __volatile__(
 		"	.set	noreorder	# __raw_read_lock	\n"
 		"1:	ll	%1, %2					\n"
@@ -181,6 +257,7 @@ static inline void __raw_read_lock(raw_rwlock_t *rw)
 		: "=m" (rw->lock), "=&r" (tmp)
 		: "m" (rw->lock)
 		: "memory");
+#endif
 	}
 
 	smp_llsc_mb();
@@ -205,6 +282,20 @@ static inline void __raw_read_unlock(raw_rwlock_t *rw)
 		: "m" (rw->lock)
 		: "memory");
 	} else {
+#ifdef CONFIG_CPU_CAVIUM_OCTEON
+		__asm__ __volatile__(
+		"	.set	noreorder	# _raw_read_unlock	\n"
+		OCTEON_SYNCW_STR
+		"1:	ll	%1, %2					\n"
+		"	sub	%1, 1					\n"
+		"	sc	%1, %0					\n"
+		"	beqz	%1, 1b					\n"
+		"	 nop						\n"
+		"	.set	reorder					\n"
+		: "=m" (rw->lock), "=&r" (tmp)
+		: "m" (rw->lock)
+		: "memory");
+#else
 		__asm__ __volatile__(
 		"	.set	noreorder	# __raw_read_unlock	\n"
 		"1:	ll	%1, %2					\n"
@@ -220,6 +311,7 @@ static inline void __raw_read_unlock(raw_rwlock_t *rw)
 		: "=m" (rw->lock), "=&r" (tmp)
 		: "m" (rw->lock)
 		: "memory");
+#endif
 	}
 }
 
@@ -241,6 +333,21 @@ static inline void __raw_write_lock(raw_rwlock_t *rw)
 		: "m" (rw->lock)
 		: "memory");
 	} else {
+#ifdef CONFIG_CPU_CAVIUM_OCTEON
+		__asm__ __volatile__(
+		"	.set	noreorder	# _raw_write_lock	\n"
+		OCTEON_SYNCW_STR
+		"1:	ll	%1, %2					\n"
+		"	bnez	%1, 1b					\n"
+		"	 lui	%1, 0x8000				\n"
+		"	sc	%1, %0			# This will go out after the syncw completes\n"
+		"	beqz	%1, 1b					\n"
+		"	 nop # No syncw is needed here since the sc goes out immediately\n"
+		"	.set	reorder					\n"
+		: "=m" (rw->lock), "=&r" (tmp)
+		: "m" (rw->lock)
+		: "memory");
+#else
 		__asm__ __volatile__(
 		"	.set	noreorder	# __raw_write_lock	\n"
 		"1:	ll	%1, %2					\n"
@@ -260,6 +367,7 @@ static inline void __raw_write_lock(raw_rwlock_t *rw)
 		: "=m" (rw->lock), "=&r" (tmp)
 		: "m" (rw->lock)
 		: "memory");
+#endif
 	}
 
 	smp_llsc_mb();
@@ -272,6 +380,7 @@ static inline void __raw_write_unlock(raw_rwlock_t *rw)
 	__asm__ __volatile__(
 	"				# __raw_write_unlock	\n"
 	"	sw	$0, %0					\n"
+	OCTEON_SYNCW_STR
 	: "=m" (rw->lock)
 	: "m" (rw->lock)
 	: "memory");
@@ -344,6 +453,24 @@ static inline int __raw_write_trylock(raw_rwlock_t *rw)
 		: "m" (rw->lock)
 		: "memory");
 	} else {
+#ifdef CONFIG_CPU_CAVIUM_OCTEON
+		__asm__ __volatile__(
+		"	.set	noreorder	# _raw_write_trylock	\n"
+		OCTEON_SYNCW_STR
+		"	li	%2, 0					\n"
+		"1:	ll	%1, %3					\n"
+		"	bnez	%1, 2f					\n"
+		"	lui	%1, 0x8000				\n"
+		"	sc	%1, %0			# This will go out after the syncw completes\n"
+		"	beqz	%1, 1b					\n"
+		"	 nop # No syncw is needed here since the sc goes out immediately\n"
+		"	li	%2, 1					\n"
+		"	.set	reorder					\n"
+		"2:							\n"
+		: "=m" (rw->lock), "=&r" (tmp), "=&r" (ret)
+		: "m" (rw->lock)
+		: "memory");
+#else
 		__asm__ __volatile__(
 		"	.set	noreorder	# __raw_write_trylock	\n"
 		"	li	%2, 0					\n"
@@ -363,6 +490,7 @@ static inline int __raw_write_trylock(raw_rwlock_t *rw)
 		: "=m" (rw->lock), "=&r" (tmp), "=&r" (ret)
 		: "m" (rw->lock)
 		: "memory");
+#endif
 	}
 
 	return ret;
diff --git a/include/asm-mips/spinlock_types.h b/include/asm-mips/spinlock_types.h
index ce26c50..362f3e2 100644
--- a/include/asm-mips/spinlock_types.h
+++ b/include/asm-mips/spinlock_types.h
@@ -5,11 +5,20 @@
 # error "please don't include this file directly"
 #endif
 
+#ifdef CONFIG_CPU_CAVIUM_OCTEON
+typedef struct {
+	volatile unsigned int ticket;
+	volatile unsigned int now_serving;
+} raw_spinlock_t;
+
+#define __RAW_SPIN_LOCK_UNLOCKED	{ 0, 0 }
+#else
 typedef struct {
 	volatile unsigned int lock;
 } raw_spinlock_t;
 
 #define __RAW_SPIN_LOCK_UNLOCKED	{ 0 }
+#endif
 
 typedef struct {
 	volatile unsigned int lock;
-- 
1.5.5.1

