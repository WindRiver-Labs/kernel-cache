From c18f5110f87ba029ddd7e136102d1bf90d92ff27 Mon Sep 17 00:00:00 2001
From: Wally Gleemer <Wally.Gleemer@windriver.com>
Date: Thu, 13 Mar 2008 11:00:32 -0700
Subject: [PATCH] cavium: add Cavium platform specific files

Bring in the new platform specific files that are only used by
the Cavium boards.  Direct import of files as found in the drop
of OCTEON-LINUX-1.7.1-240, with the exception of some trailing
whitespace removal.

Signed-off-by: Wally Gleemer <Wally.Gleemer@windriver.com>
---
 arch/mips/cavium-octeon/Kconfig                    |  260 ++++
 arch/mips/cavium-octeon/Makefile                   |   29 +
 arch/mips/cavium-octeon/dma-octeon.c               |  489 ++++++++
 arch/mips/cavium-octeon/ebt3000_cf.c               |  653 ++++++++++
 arch/mips/cavium-octeon/ethernet-mgmt-port.c       |  308 +++++
 arch/mips/cavium-octeon/ethernet-pow.c             |  734 +++++++++++
 arch/mips/cavium-octeon/flash_setup.c              |   76 ++
 arch/mips/cavium-octeon/hal.c                      |  532 ++++++++
 arch/mips/cavium-octeon/hal.h                      |  138 ++
 arch/mips/cavium-octeon/i8259.c                    |  178 +++
 arch/mips/cavium-octeon/irq.c                      |   72 ++
 arch/mips/cavium-octeon/msi.c                      |  140 +++
 arch/mips/cavium-octeon/octeon_info.c              |  126 ++
 arch/mips/cavium-octeon/pci-common.c               |   74 ++
 arch/mips/cavium-octeon/pci-common.h               |   39 +
 arch/mips/cavium-octeon/pci.c                      |  511 ++++++++
 arch/mips/cavium-octeon/pci_chips.c                |  480 +++++++
 arch/mips/cavium-octeon/pci_console.c              |  327 +++++
 arch/mips/cavium-octeon/pcie.c                     |  318 +++++
 arch/mips/cavium-octeon/perf_counters.c            |  786 ++++++++++++
 arch/mips/cavium-octeon/serial.c                   |  147 +++
 arch/mips/cavium-octeon/setup.c                    | 1307 ++++++++++++++++++++
 arch/mips/cavium-octeon/simulator.c                |   28 +
 arch/mips/cavium-octeon/smp.c                      |  200 +++
 arch/mips/cavium-octeon/userio.c                   |  162 +++
 arch/mips/cavium-octeon/watchdog.c                 |  136 ++
 arch/mips/cavium-octeon/watchdog_nmi.S             |   32 +
 arch/mips/kernel/irq-octeon.c                      |  377 ++++++
 arch/mips/kernel/octeon_switch.S                   |  511 ++++++++
 arch/mips/lib/octeon-memcpy.S                      |  628 ++++++++++
 arch/mips/mm/c-octeon.c                            |  293 +++++
 arch/mips/mm/cex-oct.S                             |   73 ++
 arch/mips/mm/pg-octeon.c                           |  113 ++
 .../mach-cavium-octeon/cpu-feature-overrides.h     |   68 +
 include/asm-mips/mach-cavium-octeon/irq.h          |  178 +++
 .../mach-cavium-octeon/kernel-entry-init.h         |  116 ++
 include/asm-mips/mach-cavium-octeon/mc146818rtc.h  |   14 +
 .../mach-cavium-octeon/octeon-hal-read-write.h     |   38 +
 include/asm-mips/mach-cavium-octeon/param.h        |   13 +
 .../asm-mips/mach-cavium-octeon/perf_counters.h    |   24 +
 40 files changed, 10728 insertions(+), 0 deletions(-)
 create mode 100644 arch/mips/cavium-octeon/Kconfig
 create mode 100644 arch/mips/cavium-octeon/Makefile
 create mode 100644 arch/mips/cavium-octeon/dma-octeon.c
 create mode 100644 arch/mips/cavium-octeon/ebt3000_cf.c
 create mode 100644 arch/mips/cavium-octeon/ethernet-mgmt-port.c
 create mode 100644 arch/mips/cavium-octeon/ethernet-pow.c
 create mode 100644 arch/mips/cavium-octeon/flash_setup.c
 create mode 100644 arch/mips/cavium-octeon/hal.c
 create mode 100644 arch/mips/cavium-octeon/hal.h
 create mode 100644 arch/mips/cavium-octeon/i8259.c
 create mode 100644 arch/mips/cavium-octeon/irq.c
 create mode 100644 arch/mips/cavium-octeon/msi.c
 create mode 100644 arch/mips/cavium-octeon/octeon_info.c
 create mode 100644 arch/mips/cavium-octeon/pci-common.c
 create mode 100644 arch/mips/cavium-octeon/pci-common.h
 create mode 100644 arch/mips/cavium-octeon/pci.c
 create mode 100644 arch/mips/cavium-octeon/pci_chips.c
 create mode 100644 arch/mips/cavium-octeon/pci_console.c
 create mode 100644 arch/mips/cavium-octeon/pcie.c
 create mode 100644 arch/mips/cavium-octeon/perf_counters.c
 create mode 100644 arch/mips/cavium-octeon/serial.c
 create mode 100644 arch/mips/cavium-octeon/setup.c
 create mode 100644 arch/mips/cavium-octeon/simulator.c
 create mode 100644 arch/mips/cavium-octeon/smp.c
 create mode 100644 arch/mips/cavium-octeon/userio.c
 create mode 100644 arch/mips/cavium-octeon/watchdog.c
 create mode 100644 arch/mips/cavium-octeon/watchdog_nmi.S
 create mode 100644 arch/mips/kernel/irq-octeon.c
 create mode 100644 arch/mips/kernel/octeon_switch.S
 create mode 100644 arch/mips/lib/octeon-memcpy.S
 create mode 100644 arch/mips/mm/c-octeon.c
 create mode 100644 arch/mips/mm/cex-oct.S
 create mode 100644 arch/mips/mm/pg-octeon.c
 create mode 100644 include/asm-mips/mach-cavium-octeon/cpu-feature-overrides.h
 create mode 100644 include/asm-mips/mach-cavium-octeon/irq.h
 create mode 100644 include/asm-mips/mach-cavium-octeon/kernel-entry-init.h
 create mode 100644 include/asm-mips/mach-cavium-octeon/mc146818rtc.h
 create mode 100644 include/asm-mips/mach-cavium-octeon/octeon-hal-read-write.h
 create mode 100644 include/asm-mips/mach-cavium-octeon/param.h
 create mode 100644 include/asm-mips/mach-cavium-octeon/perf_counters.h

diff --git a/arch/mips/cavium-octeon/Kconfig b/arch/mips/cavium-octeon/Kconfig
new file mode 100644
index 0000000..fa5e705
--- /dev/null
+++ b/arch/mips/cavium-octeon/Kconfig
@@ -0,0 +1,260 @@
+config CAVIUM_OCTEON_SPECIFIC_OPTIONS
+	bool "Enable Octeon specific options"
+	depends on CPU_CAVIUM_OCTEON
+	default "y"
+
+config USE_RI_XI_PAGE_BITS
+	bool "Enable RI/XI extended page table bits"
+	depends on CPU_CAVIUM_OCTEON
+	default "y"
+	help
+	  This option enables the use of the Read Inhibit (RI) and Execute
+	  Inhibit (XI) on page table entries. These bits are only effective
+	  on processors that support them. Currently, only the CN5XXX series
+	  of Octeon processors support them.
+
+config CAVIUM_OCTEON_2ND_KERNEL
+	bool "Build the kernel to be used as a 2nd kernel on the same chip"
+	depends on CAVIUM_OCTEON_SPECIFIC_OPTIONS
+	default "n"
+	help
+	  This option configures this kernel to be linked at a different
+	  address and use the 2nd uart for output. This allows a kernel built
+	  with this option to be run at the same time as one built without this
+	  option.
+
+config CAVIUM_OCTEON_BOOTBUS_COMPACT_FLASH
+	bool "Enable support for Compact flash hooked to the Octeon Boot Bus"
+	depends on CAVIUM_OCTEON_SPECIFIC_OPTIONS
+	default "y"
+	help
+	  This option enables a polled compact flash driver for use with
+	  compact flash cards attached to the Octeon boot bus. This interface
+	  is quite slow. It has a maximum rate of about 1.5 MB/s.
+
+config CAVIUM_OCTEON_HW_FIX_UNALIGNED
+	bool "Enable hardware fixups of unaligned loads and stores"
+	depends on CAVIUM_OCTEON_SPECIFIC_OPTIONS
+	default "y"
+	help
+	  Configure the Octeon hardware to automatically fix unaligned loads
+	  and stores. Normally unaligned accesses are fixed using a kernel
+	  exception handler. This option enables the hardware automatic fixups,
+	  which requires only an extra 3 cycles. Disable this option if you
+	  are running code that relies on address exceptions on unaligned
+	  accesses.
+
+config FAST_ACCESS_TO_THREAD_POINTER
+	bool "Enable fast access to the thread pointer"
+	depends on CAVIUM_OCTEON_SPECIFIC_OPTIONS
+	default "y"
+	help
+	  For Mips, normally the TLS thread pointer is accessed by the
+	  userspace program executing a "rdhwr" from register $29. This
+	  register doesn't exist, so the kernel emulates the instruction
+	  assigning the thread pointer to the value register. This option
+	  supplies an alternate, faster access to the thread pointer. A
+	  side effect of this option is that the highest 8 bytes of CVMSEG
+	  is used by the kernel to save and restore the thread pointer during
+	  the TLB fault handlers. This CVMSEG address isn't available to user
+	  applications.
+
+config REPLACE_EMULATED_ACCESS_TO_THREAD_POINTER
+	bool "Support dynamically replacing emulated thread pointer accesses"
+	depends on FAST_ACCESS_TO_THREAD_POINTER
+	default "y"
+	help
+	  When this option is set, the kernel can dynamically replace slower
+	  references to the thread pointer with fast accesses. This involves
+	  replacing userspace instructions at runtime, so it may not work with
+	  all programs. It is advised to use a toolchain that creates code for
+	  FAST_ACCESS_TO_THREAD_POINTER instead of this option. If you have
+	  code compiled with a Cavium compiler prior to release 1.5, or are
+	  using a non Cavium compiler, this option may allow you to receive
+	  most of the benefit of direct access to the thread pointer. It may
+	  also cause programs to fail.
+
+	  Instruction replacement is disabled on boot. It can be controlled by
+	  writing a mode to /sys/module/traps/parameters/thread_pointer_mode.
+	  The supported modes are:
+
+	  0 - Use the normal kernel emulation without any changes.
+	  1 - Replace emulated instructions with direct accesses to the thread
+		register.
+	  2 - Replace emulated instructions and log the replacement PC.
+	  3 - Replace emulated instructions with break instructions. This will
+		cause programs to fail, but makes it easy to stop gdb on the
+		instruction.
+
+config CAVIUM_OCTEON_CVMSEG_SIZE
+	int "Number of L1 cache lines reserved for CVMSEG memory"
+	depends on CAVIUM_OCTEON_SPECIFIC_OPTIONS
+	range 0 54
+	default 1
+	help
+          CVMSEG LM is a segment that accesses portions of the dcache as a
+	  local memory; the larger CVMSEG is, the smaller the cache is.
+	  This selects the size of CVMSEG LM, which is in cache blocks. The
+	  legally range is from zero to 54 cache blocks (i.e. CVMSEG LM is
+	  between zero and 6192 bytes).
+
+config CAVIUM_OCTEON_LOCK_L2
+	bool "Lock often used kernel code in the L2"
+	depends on CAVIUM_OCTEON_SPECIFIC_OPTIONS
+	default "y"
+	help
+	  Enable locking parts of the kernel into the L2 cache.
+
+config CAVIUM_OCTEON_LOCK_L2_TLB
+	bool "Lock the TLB handler in L2"
+	depends on CAVIUM_OCTEON_LOCK_L2
+	default "y"
+	help
+	  Lock the low level TLB fast path into L2.
+
+config CAVIUM_OCTEON_LOCK_L2_EXCEPTION
+	bool "Lock the exception handler in L2"
+	depends on CAVIUM_OCTEON_LOCK_L2
+	default "y"
+	help
+	  Lock the low level exception handler into L2.
+
+config CAVIUM_OCTEON_LOCK_L2_LOW_LEVEL_INTERRUPT
+	bool "Lock the interrupt handler in L2"
+	depends on CAVIUM_OCTEON_LOCK_L2
+	default "y"
+	help
+	  Lock the low level interrupt handler into L2.
+
+config CAVIUM_OCTEON_LOCK_L2_INTERRUPT
+	bool "Lock the 2nd level interrupt handler in L2"
+	depends on CAVIUM_OCTEON_LOCK_L2
+	default "y"
+	help
+	  Lock the 2nd level interrupt handler in L2.
+
+config CAVIUM_OCTEON_LOCK_L2_MEMCPY
+	bool "Lock memcpy() in L2"
+	depends on CAVIUM_OCTEON_LOCK_L2
+	default "y"
+	help
+	  Lock the kernel's implementation of memcpy() into L2.
+
+config CAVIUM_OCTEON_USER_IO
+	bool "Allow User space to access hardware IO directly"
+	depends on CAVIUM_OCTEON_SPECIFIC_OPTIONS
+	help
+	  Allows user applications to directly access the Octeon hardware
+	  IO addresses (0x1000000000000 - 0x1ffffffffffff). This allows high
+	  performance networking applications to run in user space with minimal
+	  performance penalties. This also means a user application can bring
+	  down the entire system. Only use this option on embedded devices
+	  where all user applications are strictly controlled.
+
+config CAVIUM_OCTEON_USER_MEM
+	bool "Allow User space to access memory directly"
+	depends on CAVIUM_OCTEON_SPECIFIC_OPTIONS
+	depends on CAVIUM_OCTEON_USER_IO
+	help
+	  Allows user applications to use XKPHYS addresses directly to memory.
+	  This allows user space direct access to shared memory not in use by
+	  Linux. This memory is suitable for use with the Octeon hardware.
+	  Cavium simple executive applications also share this memory. Since
+	  this bypass all of the Linux memory protection, only use this option
+	  on embedded devices where all user applications are strictly
+	  controlled.
+
+config CAVIUM_RESERVE32
+	int "Memory to reserve for user processes shared region (MB)"
+	range 0 1536
+	depends on CAVIUM_OCTEON_SPECIFIC_OPTIONS
+	depends on CAVIUM_OCTEON_USER_IO
+	default "0"
+	help
+	  Reserve a shared memory region for user processes to use for hardware
+	  memory buffers. This is required for 32bit applications to be able to
+	  send and receive packets directly. Applications access this memory by
+	  memory mapping /dev/mem for the addresses in /proc/octeon_info. For
+	  optimal performance with HugeTLBs, keep this size an even number of
+	  megabytes.
+
+config CAVIUM_RESERVE32_USE_WIRED_TLB
+	bool "Use wired TLB entries to access the reserved memory region"
+	depends on CAVIUM_OCTEON_SPECIFIC_OPTIONS
+	depends on CAVIUM_OCTEON_USER_IO
+	default "n"
+	help
+	  When this option is set, the memory reserved for the user process
+	  shared region (CONFIG_CAVIUM_RESERVE32) is globally mapped to all
+	  userspace programs using wired TLB entries. The userspace addresses
+	  beginning at 2GB-CONFIG_CAVIUM_RESERVE32 and ending at 2GB are hard
+	  wired to always map to the reserved memory region. This has the
+	  benefit of completely eliminating TLB overhead to the region, but may
+	  cause unwanted side affect. Since all memory in this region is shared
+	  across all userspace applications, any application attempting to
+	  mmap() these virtual address will fail in strange ways. In
+	  particular, be careful of shared libraries being mapped into this
+	  region. If this happens, the entire system may become unstable.
+
+	  Note: When this option is selected, CONFIG_CAVIUM_RESERVE32 must be
+	  512MB, 1024MB, or 1536MB. Any other valid will fail on boot.
+
+config CAVIUM_OCTEON_NUM_PACKET_BUFFERS
+	int "Number of packet buffers (and work queue entries) for the Ethernet driver"
+	range 0 8192
+	depends on CAVIUM_OCTEON_SPECIFIC_OPTIONS
+	default "1024"
+	help
+	  Number of packet buffers (and work queue entries) to allocate for
+	  the ethernet driver. Zero is treated as 1024.
+
+config ARCH_SPARSEMEM_ENABLE
+	def_bool y
+	select SPARSEMEM_STATIC
+	depends on CPU_CAVIUM_OCTEON
+
+config CAVIUM_OCTEON_POW_ONLY_ETHERNET
+	tristate "POW based internal only ethernet driver"
+	default n
+	help
+	  This option enables a very simple ethernet driver for internal core
+	  to core traffic. It relies on another driver, cavium-ethernet,
+	  to perform all hardware setup. This driver's purpose is to supply
+	  basic networking between different Linux images running on the same
+	  chip. A single core loads the cavium-ethernet module, all other cores
+	  load this driver. On load, the driver waits for some other core to
+	  perform hardware setup.
+
+config CAVIUM_OCTEON_MGMT_PORT_ETHERNET
+	tristate "Management port ethernet driver (CN5XXX)"
+	default y
+	help
+	  This option enables the ethernet driver for the management port on
+	  CN57XX, CN56XX, CN55XX, and CN54XX chips.
+
+config CAVIUM_OCTEON_WATCHDOG
+	tristate "Octeon watchdog driver"
+	default y
+	help
+	  This option enables a watchdog driver for all cores running Linux. It
+	  installs a NMI handler and pokes the watchdog with core timers.
+	  On first expiration of the watchdog, a masked interrupt fires.
+	  The second expiration causes an NMI that prints a message and resets
+	  the chip. The third expiration causes a global soft reset.
+
+config CAVIUM_OCTEON_IPSEC
+   bool "Enable enhancements to the IPSec stack to allow procotol offload."
+   depends on CAVIUM_OCTEON_SPECIFIC_OPTIONS
+   default "n"
+   help
+     This enables enhancements to the IPSec stack to allow some of the
+     processing required for IPSec to be performed on another processor
+     which must be running the ipsec-filter application.
+
+config FORCE_MAX_ZONEORDER
+	int
+	depends on HUGETLB_PAGE && (PAGE_SIZE_16KB || PAGE_SIZE_32KB || PAGE_SIZE_64KB)
+	default 12 if PAGE_SIZE_16KB
+	default 13 if PAGE_SIZE_32KB
+	default 14 if PAGE_SIZE_32KB
+
diff --git a/arch/mips/cavium-octeon/Makefile b/arch/mips/cavium-octeon/Makefile
new file mode 100644
index 0000000..0d5e8b9
--- /dev/null
+++ b/arch/mips/cavium-octeon/Makefile
@@ -0,0 +1,29 @@
+#
+# Makefile for the Cavium Octeon specific kernel interface routines
+# under Linux.
+#
+# This file is subject to the terms and conditions of the GNU General Public
+# License.  See the file "COPYING" in the main directory of this archive
+# for more details.
+#
+# Copyright (C) 2005-2007 Cavium Networks
+#
+
+obj-y := setup.o serial.o irq.o hal.o perf_counters.o octeon_info.o
+obj-y += dma-octeon.o userio.o flash_setup.o
+obj-y += pci_console.o
+obj-y += simulator.o
+
+obj-$(CONFIG_SMP)                     += smp.o
+obj-$(CONFIG_CAVIUM_OCTEON_BOOTBUS_COMPACT_FLASH) += ebt3000_cf.o
+obj-$(CONFIG_PCI)                     += pci-common.o
+obj-$(CONFIG_PCI)                     += pci.o pci_chips.o i8259.o
+obj-$(CONFIG_PCI_MSI)                 += msi.o
+obj-$(CONFIG_PCI)                     += pcie.o
+obj-$(CONFIG_CAVIUM_OCTEON_POW_ONLY_ETHERNET) += ethernet-pow.o
+obj-$(CONFIG_CAVIUM_OCTEON_WATCHDOG) += watchdog.o watchdog_nmi.o
+obj-$(CONFIG_CAVIUM_OCTEON_MGMT_PORT_ETHERNET) += ethernet-mgmt-port.o
+
+clean:
+	rm -f *.o
+
diff --git a/arch/mips/cavium-octeon/dma-octeon.c b/arch/mips/cavium-octeon/dma-octeon.c
new file mode 100644
index 0000000..a609baf
--- /dev/null
+++ b/arch/mips/cavium-octeon/dma-octeon.c
@@ -0,0 +1,489 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2000  Ani Joshi <ajoshi@unixbox.com>
+ * Copyright (C) 2000, 2001  Ralf Baechle <ralf@gnu.org>
+ * Copyright (C) 2005 Ilya A. Volynets-Evenbakh <ilya@total-knowledge.com>
+ * swiped from i386, and cloned for MIPS by Geert, polished by Ralf.
+ * IP32 changes by Ilya.
+ * Cavium Networks: Create new dma setup for Cavium Networks Octeon based on
+ * the kernels original.
+ */
+#include <linux/types.h>
+#include <linux/mm.h>
+#include <linux/module.h>
+#include <linux/string.h>
+#include <linux/dma-mapping.h>
+#include <linux/platform_device.h>
+
+#include <asm/cache.h>
+#include <asm/io.h>
+#include <asm/ip32/crime.h>
+
+#include "hal.h"
+#include "pci-common.h"
+
+#define BAR2_PCI_ADDRESS 0x8000000000ul
+
+typedef struct {
+	int16_t ref_count;	/* Number of PCI mappings using this index */
+	int16_t address_bits;	/* Upper bits of physical address. This is
+				   shifted 22 bits */
+} bar1_index_state_t;
+
+#ifdef CONFIG_PCI
+static spinlock_t bar1_lock = SPIN_LOCK_UNLOCKED;
+static bar1_index_state_t bar1_state[32] = { {0, 0}, };
+#endif
+
+void *dma_alloc_noncoherent(struct device *dev, size_t size,
+			    dma_addr_t * dma_handle, unsigned int __nocast gfp)
+	__attribute__ ((alias("dma_alloc_coherent")));
+
+EXPORT_SYMBOL(dma_alloc_noncoherent);
+
+
+void *dma_alloc_coherent(struct device *dev, size_t size,
+			 dma_addr_t * dma_handle, unsigned int __nocast gfp)
+{
+	void *ret;
+	/* Ignore region specifiers. The Octeon Bar1 register can be used to
+	   map any memory on the system. As long as we don't fill up the 32
+	   entry table, memory can be anywhere */
+	gfp &= ~(__GFP_DMA | __GFP_HIGHMEM);
+
+	ret = (void *) __get_free_pages(gfp, get_order(size));
+	if (likely(ret != NULL)) {
+		memset(ret, 0, size);
+		/* By default, PCI devices can't get to memory. dma_map_single
+		   changes the Bar mapping to allow access to this region */
+		*dma_handle = dma_map_single(dev, ret, size, DMA_BIDIRECTIONAL);
+	}
+
+	return ret;
+}
+
+EXPORT_SYMBOL(dma_alloc_coherent);
+
+
+void dma_free_noncoherent(struct device *dev, size_t size, void *vaddr,
+			  dma_addr_t dma_handle)
+	__attribute__ ((alias("dma_free_coherent")));
+
+EXPORT_SYMBOL(dma_free_noncoherent);
+
+
+void dma_free_coherent(struct device *dev, size_t size, void *vaddr,
+		       dma_addr_t dma_handle)
+{
+	/* Free the Bar mappings */
+	dma_unmap_single(dev, dma_handle, size, DMA_BIDIRECTIONAL);
+	free_pages((unsigned long) vaddr, get_order(size));
+}
+
+EXPORT_SYMBOL(dma_free_coherent);
+
+
+dma_addr_t dma_map_single(struct device *dev, void *ptr, size_t size,
+			  enum dma_data_direction direction)
+{
+#ifndef CONFIG_PCI
+	/* Without PCI/PCIe this function can be called for Octeon internal
+	   devices such as USB. These devices all support 64bit addressing */
+	return virt_to_phys(ptr);
+#else
+	unsigned long flags;
+	uint64_t dma_mask;
+	int64_t start_index;
+	dma_addr_t result = -1;
+	uint64_t physical = virt_to_phys(ptr);
+	int64_t index;
+
+	BUG_ON(direction == DMA_NONE);
+
+	/* Use the DMA masks to determine the allowed memory region. For us it
+	   doesn't limit the actual memory, just the address visible over PCI.
+	   Devices with limits need to use lower indexed Bar1 entries. */
+	if (dev) {
+		dma_mask = dev->coherent_dma_mask;
+		if (dev->dma_mask)
+			dma_mask = *dev->dma_mask;
+	} else
+		dma_mask = 0xfffffffful;
+
+	/* Platform devices, such as the internal USB, skip all translation and
+	   use Octeon physical addresses directly */
+	if (dev->bus == &platform_bus_type)
+		return physical;
+
+	switch (octeon_dma_bar_type) {
+	case OCTEON_DMA_BAR_TYPE_PCIE:
+		if (unlikely(physical < (16ul << 10)))
+			panic("dma_map_single: Not allowed to map first 16KB. It interferes with BAR0 special area\n");
+		else if ((physical + size >= (256ul << 20)) &&
+			 (physical < (512ul << 20)))
+			panic("dma_map_single: Not allowed to map bootbus\n");
+		else if ((physical + size >= 0x400000000ull) &&
+			 physical < 0x410000000ull)
+			panic("dma_map_single: Attempt to map illegal memory address 0x%lx\n", physical);
+		else if (physical >= 0x420000000ull)
+			panic("dma_map_single: Attempt to map illegal memory address 0x%lx\n", physical);
+		/* The 2nd 256MB is mapped at 256<<20 instead of 0x410000000 */
+		if ((physical >= 0x410000000ull) && physical < 0x420000000ull)
+			result = physical - 0x400000000ull;
+		else
+			result = physical;
+		goto done;
+
+	case OCTEON_DMA_BAR_TYPE_BIG:
+#ifdef CONFIG_64BIT
+		/* If the device supports 64bit addressing, then use BAR2 */
+		if (dma_mask > BAR2_PCI_ADDRESS) {
+			result = physical + BAR2_PCI_ADDRESS;
+			goto done;
+		}
+#endif
+		if (unlikely(physical < (4ul << 10))) {
+			panic("dma_map_single: Not allowed to map first 4KB. It interferes with BAR0 special area\n");
+		} else if (physical < (256ul << 20)) {
+			if (unlikely(physical + size > (256ul << 20)))
+				panic("dma_map_single: Requested memory spans Bar0 0:256MB and bootbus\n");
+			result = physical;
+			goto done;
+		} else if (unlikely(physical < (512ul << 20))) {
+			panic("dma_map_single: Not allowed to map bootbus\n");
+		} else if (physical < (2ul << 30)) {
+			if (unlikely(physical + size > (2ul << 30)))
+				panic("dma_map_single: Requested memory spans Bar0 512MB:2GB and BAR1\n");
+			result = physical;
+			goto done;
+		} else if (physical < (2ul << 30) + (128 << 20)) {
+			/* Fall through */
+		} else if (physical <
+			   (4ul << 30) - (OCTEON_PCI_BAR1_HOLE_SIZE << 20)) {
+			if (unlikely
+			    (physical + size >
+			     (4ul << 30) - (OCTEON_PCI_BAR1_HOLE_SIZE << 20)))
+				panic("dma_map_single: Requested memory extends past Bar1 (4GB-%luMB)\n", OCTEON_PCI_BAR1_HOLE_SIZE);
+			result = physical;
+			goto done;
+		} else if ((physical >= 0x410000000ull) &&
+			   (physical < 0x420000000ull)) {
+			if (unlikely(physical + size > 0x420000000ull))
+				panic("dma_map_single: Requested memory spans non existant memory\n");
+			result = physical - 0x400000000ull;	/* BAR0 fixed
+								   mapping
+								   256MB:512MB
+								   ->
+								   16GB+256MB:16GB+512MB
+								 */
+			goto done;
+		} else {
+			/* Continued below switch statement */
+		}
+		break;
+
+	case OCTEON_DMA_BAR_TYPE_SMALL:
+#ifdef CONFIG_64BIT
+		/* If the device supports 64bit addressing, then use BAR2 */
+		if (dma_mask > BAR2_PCI_ADDRESS) {
+			result = physical + BAR2_PCI_ADDRESS;
+			goto done;
+		}
+#endif
+		/* Continued below switch statement */
+		break;
+
+	default:
+		panic("dma_map_single: Invalid octeon_dma_bar_type\n");
+	}
+
+	/* Don't allow mapping to span multiple Bar entries. The hardware guys
+	   won't guarantee that DMA across boards work */
+	if (unlikely((physical >> 22) != ((physical + size - 1) >> 22)))
+		panic("dma_map_single: Requested memory spans more than one Bar1 entry\n");
+
+	if (octeon_dma_bar_type == OCTEON_DMA_BAR_TYPE_BIG)
+		start_index = 31;
+	else if (unlikely(dma_mask < (1ul << 27)))
+		start_index = (dma_mask >> 22);
+	else
+		start_index = 31;
+
+	/* Only one processor can access the Bar register at once */
+	spin_lock_irqsave(&bar1_lock, flags);
+
+	/* Look through Bar1 for existing mapping that will work */
+	for (index = start_index; index >= 0; index--) {
+		if ((bar1_state[index].address_bits == physical >> 22) &&
+		    (bar1_state[index].ref_count)) {
+			/* An existing mapping will work, use it */
+			bar1_state[index].ref_count++;
+			if (unlikely(bar1_state[index].ref_count < 0))
+				panic("dma_map_single: Bar1[%d] reference count overflowed\n", (int) index);
+			result = (index << 22) | (physical & ((1 << 22) - 1));
+			/* Large BAR1 is offset at 2GB */
+			if (octeon_dma_bar_type == OCTEON_DMA_BAR_TYPE_BIG)
+				result += 2ul << 30;
+			goto done_unlock;
+		}
+	}
+
+	/* No existing mappings, look for a free entry */
+	for (index = start_index; index >= 0; index--) {
+		if (unlikely(bar1_state[index].ref_count == 0)) {
+			cvmx_pci_bar1_indexx_t bar1_index;
+			/* We have a free entry, use it */
+			bar1_state[index].ref_count = 1;
+			bar1_state[index].address_bits = physical >> 22;
+			bar1_index.u32 = 0;
+			bar1_index.s.addr_idx = physical >> 22;	/* Address bits
+								   [35:22] sent
+								   to L2C */
+			bar1_index.s.ca = 1;	/* Don't put PCI accesses in
+						   L2. */
+			bar1_index.s.end_swp = 1;	/* Endian Swap Mode */
+			bar1_index.s.addr_v = 1;	/* Set '1' when the
+							   selected address
+							   range is valid. */
+			octeon_npi_write32(CVMX_NPI_PCI_BAR1_INDEXX(index),
+					   bar1_index.u32);
+			/* An existing mapping will work, use it */
+			result = (index << 22) | (physical & ((1 << 22) - 1));
+			/* Large BAR1 is offset at 2GB */
+			if (octeon_dma_bar_type == OCTEON_DMA_BAR_TYPE_BIG)
+				result += 2ul << 30;
+			goto done_unlock;
+		}
+	}
+
+	printk("dma_map_single: Can't find empty BAR1 index for physical mapping 0x%llx\n", (unsigned long long) physical);
+
+      done_unlock:
+	spin_unlock_irqrestore(&bar1_lock, flags);
+      done:
+	// printk("dma_map_single 0x%lx->0x%lx\n", physical, result);
+	return result;
+#endif
+}
+
+EXPORT_SYMBOL(dma_map_single);
+
+
+void dma_unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
+		      enum dma_data_direction direction)
+{
+#ifndef CONFIG_PCI
+	/* Without PCI/PCIe this function can be called for Octeon internal
+	   devices such as USB. These devices all support 64bit addressing */
+	return;
+#else
+	unsigned long flags;
+	uint64_t index;
+
+	BUG_ON(direction == DMA_NONE);
+
+	/* Platform devices, such as the internal USB, skip all translation and
+	   use Octeon physical addresses directly */
+	if (dev->bus == &platform_bus_type)
+		return;
+
+	switch (octeon_dma_bar_type) {
+	case OCTEON_DMA_BAR_TYPE_PCIE:
+		/* Nothing to do, all mappings are static */
+		goto done;
+
+	case OCTEON_DMA_BAR_TYPE_BIG:
+#ifdef CONFIG_64BIT
+		/* Nothing to do for addresses using BAR2 */
+		if (dma_addr >= BAR2_PCI_ADDRESS)
+			goto done;
+#endif
+		if (unlikely(dma_addr < (4ul << 10)))
+			panic("dma_unmap_single: Unexpect DMA address 0x%lx\n",
+			      dma_addr);
+		else if (dma_addr < (2ul << 30))
+			goto done;	/* Nothing to do for addresses using
+					   BAR0 */
+		else if (dma_addr < (2ul << 30) + (128ul << 20))
+			index = (dma_addr - (2ul << 30)) >> 22;	/* Need to
+								   unmap, fall
+								   through */
+		else if (dma_addr <
+			 (4ul << 30) - (OCTEON_PCI_BAR1_HOLE_SIZE << 20))
+			goto done;	/* Nothing to do for the rest of BAR1 */
+		else
+			panic("dma_unmap_single: Unexpect DMA address 0x%lx\n",
+			      dma_addr);
+		/* Continued below switch statement */
+		break;
+
+	case OCTEON_DMA_BAR_TYPE_SMALL:
+#ifdef CONFIG_64BIT
+		/* Nothing to do for addresses using BAR2 */
+		if (dma_addr >= BAR2_PCI_ADDRESS)
+			goto done;
+#endif
+		index = dma_addr >> 22;
+		/* Continued below switch statement */
+		break;
+
+	default:
+		panic("dma_unmap_single: Invalid octeon_dma_bar_type\n");
+	}
+
+	if (unlikely(index > 31))
+		panic("dma_unmap_single: Attempt to unmap an invalid address (0x%llx)\n", (unsigned long long) dma_addr);
+
+	spin_lock_irqsave(&bar1_lock, flags);
+	bar1_state[index].ref_count--;
+	if (bar1_state[index].ref_count == 0)
+		octeon_npi_write32(CVMX_NPI_PCI_BAR1_INDEXX(index), 0);
+	else if (unlikely(bar1_state[index].ref_count < 0))
+		panic("dma_unmap_single: Bar1[%u] reference count < 0\n",
+		      (int) index);
+	spin_unlock_irqrestore(&bar1_lock, flags);
+      done:
+	// printk("dma_unmap_single 0x%lx\n", dma_addr);
+	return;
+#endif
+}
+
+EXPORT_SYMBOL(dma_unmap_single);
+
+
+int dma_map_sg(struct device *dev, struct scatterlist *sg, int nents,
+	       enum dma_data_direction direction)
+{
+	int i;
+	for (i = 0; i < nents; i++, sg++)
+		sg->dma_address =
+			dma_map_page(dev, sg->page, sg->offset, sg->length,
+				     direction);
+
+	return nents;
+}
+
+EXPORT_SYMBOL(dma_map_sg);
+
+
+dma_addr_t dma_map_page(struct device * dev, struct page * page,
+			unsigned long offset, size_t size,
+			enum dma_data_direction direction)
+{
+	return dma_map_single(dev, page_address(page) + offset, size,
+			      direction);
+}
+
+EXPORT_SYMBOL(dma_map_page);
+
+
+void dma_unmap_page(struct device *dev, dma_addr_t dma_address, size_t size,
+		    enum dma_data_direction direction)
+{
+	dma_unmap_single(dev, dma_address, size, direction);
+}
+
+EXPORT_SYMBOL(dma_unmap_page);
+
+
+void dma_unmap_sg(struct device *dev, struct scatterlist *sg, int nhwentries,
+		  enum dma_data_direction direction)
+{
+	int i;
+	for (i = 0; i < nhwentries; i++, sg++)
+		dma_unmap_page(dev, sg->dma_address, sg->length, direction);
+}
+
+EXPORT_SYMBOL(dma_unmap_sg);
+
+
+void dma_sync_single_for_cpu(struct device *dev, dma_addr_t dma_handle,
+			     size_t size, enum dma_data_direction direction)
+{
+	mb();
+}
+
+EXPORT_SYMBOL(dma_sync_single_for_cpu);
+
+
+void dma_sync_single_for_device(struct device *dev, dma_addr_t dma_handle,
+				size_t size, enum dma_data_direction direction)
+{
+	mb();
+}
+
+EXPORT_SYMBOL(dma_sync_single_for_device);
+
+
+void dma_sync_single_range_for_cpu(struct device *dev, dma_addr_t dma_handle,
+				   unsigned long offset, size_t size,
+				   enum dma_data_direction direction)
+{
+	mb();
+}
+
+EXPORT_SYMBOL(dma_sync_single_range_for_cpu);
+
+
+void dma_sync_single_range_for_device(struct device *dev, dma_addr_t dma_handle,
+				      unsigned long offset, size_t size,
+				      enum dma_data_direction direction)
+{
+	mb();
+}
+
+EXPORT_SYMBOL(dma_sync_single_range_for_device);
+
+
+void dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg, int nelems,
+			 enum dma_data_direction direction)
+{
+	mb();
+}
+
+EXPORT_SYMBOL(dma_sync_sg_for_cpu);
+
+
+void dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg,
+			    int nelems, enum dma_data_direction direction)
+{
+	mb();
+}
+
+EXPORT_SYMBOL(dma_sync_sg_for_device);
+
+
+int dma_mapping_error(dma_addr_t dma_addr)
+{
+	return (dma_addr == -1);
+}
+
+EXPORT_SYMBOL(dma_mapping_error);
+
+
+int dma_supported(struct device *dev, u64 mask)
+{
+	return 1;
+}
+
+EXPORT_SYMBOL(dma_supported);
+
+
+int dma_is_consistent(struct device *dev, dma_addr_t dma_addr)
+{
+	return 1;
+}
+
+EXPORT_SYMBOL(dma_is_consistent);
+
+
+void dma_cache_sync(struct device *dev, void *vaddr, size_t size,
+		    enum dma_data_direction direction)
+{
+	mb();
+}
+
+EXPORT_SYMBOL(dma_cache_sync);
diff --git a/arch/mips/cavium-octeon/ebt3000_cf.c b/arch/mips/cavium-octeon/ebt3000_cf.c
new file mode 100644
index 0000000..f4af553
--- /dev/null
+++ b/arch/mips/cavium-octeon/ebt3000_cf.c
@@ -0,0 +1,653 @@
+/*
+ * Extra-simple block driver for the Octeon bootbus compact flash. This
+ * driver is based on the excellent article and example code from LWM.
+ * http://lwn.net/Articles/58719/
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2005-2007 Cavium Networks
+ */
+
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <linux/init.h>
+
+#include <linux/kernel.h>	/* printk() */
+#include <linux/fs.h>		/* everything... */
+#include <linux/errno.h>	/* error codes */
+#include <linux/types.h>	/* size_t */
+#include <linux/vmalloc.h>
+#include <linux/genhd.h>
+#include <linux/blkdev.h>
+#include <linux/hdreg.h>
+#include <linux/ide.h>
+#include <asm/delay.h>
+#include <linux/completion.h>
+
+#include "cvmx-app-init.h"
+#include "hal.h"
+
+#define VERSION "1.1"
+#define DEVICE_NAME "cf"
+MODULE_LICENSE("GPL");
+
+/*
+ * We can tweak our hardware sector size, but the kernel talks to us
+ * in terms of small sectors, always.
+ */
+#define KERNEL_SECTOR_SIZE 512
+
+/*
+ * The internal representation of our device.
+ */
+
+typedef struct {
+	void *base_ptr;
+	unsigned long num_sectors;
+	unsigned long sector_size;
+	struct gendisk *gd;
+	struct hd_geometry geo;
+	spinlock_t lock;
+	request_queue_t *queue;
+	struct completion comp;
+	int is16bit;
+	int is_true_ide;	/* is16bit must also be set */
+} cf_device_t;
+
+static cf_device_t STATIC_DEVICE;
+
+
+/**
+ * Check if the Compact flash is idle and doesn't have any
+ * errors.
+ *
+ * @param cf     Device to check
+ * @return Zero on success
+ */
+static inline int ata_wait_idle(cf_device_t * cf)
+{
+	unsigned char status;
+	unsigned long timeout = jiffies + HZ;
+
+	// octeon_led_set(0, 30);
+
+	if (cf->is_true_ide) {
+		status = __raw_readw(cf->base_ptr + 14) & 0xff;
+		while (status & 0x80) {
+			if (unlikely(time_after(jiffies, timeout)))
+				break;
+			udelay(5);
+			status = __raw_readw(cf->base_ptr + 14) & 0xff;
+		}
+	} else if (cf->is16bit) {
+		status = __raw_readw(cf->base_ptr + 6) >> 8;
+		while (status & 0x80) {
+			if (unlikely(time_after(jiffies, timeout)))
+				break;
+			udelay(5);
+			status = __raw_readw(cf->base_ptr + 6) >> 8;
+		}
+	} else {
+		status = __raw_readb(cf->base_ptr + 7);
+		while (status & 0x80) {
+			if (unlikely(time_after(jiffies, timeout)))
+				break;
+			udelay(5);
+			status = __raw_readb(cf->base_ptr + 7);
+		}
+	}
+	// octeon_led_clear(0, 30);
+
+	if (unlikely(status & 0x80)) {
+		if (cf->gd)
+			printk("%s: Timeout\n", cf->gd->disk_name);
+		return -1;
+	} else if (unlikely(status & 1)) {
+		if (cf->gd)
+			printk("%s: Error\n", cf->gd->disk_name);
+		return -1;
+	} else
+		return 0;
+}
+
+
+/**
+ * Wait for data request ready
+ *
+ * @param cf     Device to check
+ * @return Zero on success
+ */
+static inline int ata_wait_drq_ready(cf_device_t * cf)
+{
+	unsigned char status;
+	unsigned long timeout = jiffies + HZ;
+
+	// octeon_led_set(0, 30);
+	if (cf->is_true_ide) {
+		status = __raw_readw(cf->base_ptr + 14) & 0xff;
+		while ((status & 0x08) == 0) {
+			if (unlikely(time_after(jiffies, timeout)))
+				break;
+			udelay(1);
+			status = __raw_readw(cf->base_ptr + 14) & 0xff;
+		}
+	} else if (cf->is16bit) {
+		status = __raw_readw(cf->base_ptr + 6) >> 8;
+		while ((status & 0x08) == 0) {
+			if (unlikely(time_after(jiffies, timeout)))
+				break;
+			udelay(1);
+			status = __raw_readw(cf->base_ptr + 6) >> 8;
+		}
+	} else {
+		status = __raw_readb(cf->base_ptr + 7);
+		while ((status & 0x08) == 0) {
+			if (unlikely(time_after(jiffies, timeout)))
+				break;
+			udelay(1);
+			status = __raw_readb(cf->base_ptr + 7);
+		}
+	}
+	// octeon_led_clear(0, 30);
+
+
+	if (unlikely((status & 0x08) == 0)) {
+		if (cf->gd)
+			printk("%s: Timeout\n", cf->gd->disk_name);
+		return -1;
+	} else
+		return 0;
+}
+
+
+/**
+ * Send low level ATA command to the device
+ *
+ * @param cf      Device to send commadn to
+ * @param sectors Number of sectors for access
+ * @param lba     Logical block address
+ * @param command ATA command
+ * @return Zero on success
+ */
+static int ata_command(cf_device_t * cf, int sectors, unsigned long lba,
+		       int command)
+{
+	/* Wait to not be busy before we start */
+	if (ata_wait_idle(cf))
+		return -1;
+
+	if (cf->is_true_ide) {
+		__raw_writew(sectors, cf->base_ptr + 4);
+		__raw_writew(lba & 0xff, cf->base_ptr + 6);
+		__raw_writew((lba >> 8) & 0xff, cf->base_ptr + 8);
+		__raw_writew((lba >> 16) & 0xff, cf->base_ptr + 10);
+		__raw_writew(((lba >> 24) & 0x0f) | 0xe0, cf->base_ptr + 12);
+		__raw_writew(command, cf->base_ptr + 14);
+	} else if (cf->is16bit) {
+		__raw_writew(sectors | ((lba & 0xff) << 8), cf->base_ptr + 2);
+		__raw_writew(lba >> 8, cf->base_ptr + 4);
+		__raw_writew(((lba >> 24) & 0x0f) | 0xe0 | command << 8,
+			     cf->base_ptr + 6);
+	} else {
+		__raw_writeb(sectors, cf->base_ptr + 2);
+		__raw_writeb(lba & 0xff, cf->base_ptr + 3);
+		__raw_writeb((lba >> 8) & 0xff, cf->base_ptr + 4);
+		__raw_writeb((lba >> 16) & 0xff, cf->base_ptr + 5);
+		__raw_writeb(((lba >> 24) & 0x0f) | 0xe0, cf->base_ptr + 6);
+		__raw_writeb(command, cf->base_ptr + 7);
+	}
+
+	return 0;
+}
+
+
+/**
+ * Identify the compact flash
+ *
+ * @param cf         Device to access
+ * @param drive_info IDE drive information
+ * @return Zero on success
+ */
+static int ata_identify(cf_device_t * cf, struct hd_driveid *drive_info)
+{
+	if (ata_command(cf, 0, 0, WIN_IDENTIFY))
+		return -1;
+
+	/* Wait for read to complete (BSY clear) */
+	if (ata_wait_idle(cf))
+		return -1;
+	if (cf->is16bit) {
+		uint16_t *ptr = (uint16_t *) drive_info;
+		int count;
+		for (count = 0; count < sizeof(*drive_info); count += 2)
+			*ptr++ = readw(cf->base_ptr);	/* Swaps internally */
+	} else {
+		unsigned char *ptr = (unsigned char *) drive_info;
+		int count;
+		for (count = 0; count < sizeof(*drive_info); count++)
+			*ptr++ = readb(cf->base_ptr);
+	}
+	ide_fix_driveid(drive_info);
+	ide_fixstring(drive_info->model, sizeof(drive_info->model), 0);
+	ide_fixstring(drive_info->fw_rev, sizeof(drive_info->fw_rev), 0);
+	ide_fixstring(drive_info->serial_no, sizeof(drive_info->serial_no), 0);
+	return 0;
+}
+
+
+/**
+ * Read sectors from the device
+ *
+ * @param cf        Device to read from
+ * @param lba_start Start sector
+ * @param num_sectors
+ *                  Number of sectors to read
+ * @param buffer    Buffer to put the results in
+ * @return Number of sectors read
+ */
+static int ata_read(cf_device_t * cf, unsigned long lba_start,
+		    unsigned long num_sectors, char *buffer)
+{
+	int sectors_read = 0;
+
+	// octeon_led_set(0, 31);
+	while (sectors_read < num_sectors) {
+		int count;
+		int sectors_to_read = num_sectors;
+
+		if (sectors_to_read > 256)
+			sectors_to_read = 256;
+
+		if (ata_command
+		    (cf, sectors_to_read & 0xff, lba_start, WIN_READ))
+			goto done;
+		lba_start += sectors_to_read;
+
+		while (sectors_to_read--) {
+			/* Wait for read to complete (BSY clear) */
+			if (ata_wait_idle(cf))
+				goto done;
+			if (ata_wait_drq_ready(cf))
+				goto done;
+
+			if (cf->is16bit) {
+				uint16_t *ptr = (uint16_t *) buffer;
+				for (count = 0; count < cf->sector_size;
+				     count += 2)
+					*ptr++ = readw(cf->base_ptr);
+				buffer += cf->sector_size;
+			} else {
+				for (count = 0; count < cf->sector_size;
+				     count++)
+					*buffer++ = readb(cf->base_ptr);
+			}
+			sectors_read++;
+		}
+	}
+      done:
+	// octeon_led_clear(0, 31);
+
+	return sectors_read;
+}
+
+
+/**
+ * Write sectors to the device
+ *
+ * @param cf        Device to write to
+ * @param lba_start Starting sector number
+ * @param num_sectors
+ *                  Number of sectors to write
+ * @param buffer    Data buffer to write
+ * @return Number of sectors written
+ */
+static int ata_write(cf_device_t * cf, unsigned long lba_start,
+		     unsigned long num_sectors, const char *buffer)
+{
+	int sectors_written = 0;
+
+	// octeon_led_set(1, 31);
+	while (sectors_written < num_sectors) {
+		int count;
+		int sectors_to_write = num_sectors;
+
+		if (sectors_to_write > 256)
+			sectors_to_write = 256;
+
+		if (ata_command
+		    (cf, sectors_to_write & 0xff, lba_start, WIN_WRITE))
+			goto done;
+		lba_start += sectors_to_write;
+
+		while (sectors_to_write--) {
+			/* Wait for write command to be ready for data (BSY
+			   clear) */
+			if (ata_wait_idle(cf))
+				goto done;
+			if (ata_wait_drq_ready(cf))
+				goto done;
+
+			if (cf->is16bit) {
+				const uint16_t *ptr = (const uint16_t *) buffer;
+				for (count = 0; count < cf->sector_size;
+				     count += 2) {
+					writew(*ptr++, cf->base_ptr);
+					/* Every 16 writes do a read so the
+					   bootbus FIFO doesn't fill up */
+					if (count && ((count & 0x1f) == 0))
+						__raw_readw(cf->base_ptr + 6);
+				}
+				buffer += cf->sector_size;
+			} else {
+				for (count = 0; count < cf->sector_size;
+				     count++) {
+					writeb(*buffer++, cf->base_ptr);
+					/* Every 16 writes do a read so the
+					   bootbus FIFO doesn't fill up */
+					if (count && ((count & 0xf) == 0))
+						__raw_readb(cf->base_ptr + 7);
+				}
+			}
+			sectors_written++;
+		}
+	}
+      done:
+	// octeon_led_clear(1, 31);
+
+	return sectors_written;
+}
+
+
+/**
+ * Identify a compact flash disk
+ *
+ * @param cf     Device to check and update
+ * @return Zero on success. Failure will result in a device of zero
+ *         size and a -1 return code.
+ */
+static int ebt3000cf_identify(cf_device_t * cf)
+{
+	struct hd_driveid drive_info;
+	int result;
+
+	memset(&drive_info, 0, sizeof(drive_info));
+
+	result = ata_identify(cf, &drive_info);
+	if (result == 0) {
+		/* Sandisk 1G reports the wrong sector size */
+		drive_info.sector_bytes = 512;
+		printk("%s: %s Serial %s (%u sectors, %u bytes/sector)\n",
+		       (cf->gd) ? cf->gd->disk_name : DEVICE_NAME,
+		       drive_info.model, drive_info.serial_no,
+		       drive_info.lba_capacity, (int) drive_info.sector_bytes);
+		cf->num_sectors = drive_info.lba_capacity;
+		cf->sector_size = drive_info.sector_bytes;
+		cf->geo.cylinders = drive_info.cyls;
+		cf->geo.heads = drive_info.heads;
+		cf->geo.sectors = drive_info.sectors;
+		cf->geo.start = 0;
+	} else {
+		cf->num_sectors = 0;
+		cf->sector_size = KERNEL_SECTOR_SIZE;
+	}
+
+	return result;
+}
+
+
+/**
+ * Handle an I/O request.
+ *
+ * @param cf         Device to access
+ * @param lba_sector Starting sector
+ * @param num_sectors
+ *                   Number of sectors to transfer
+ * @param buffer     Data buffer
+ * @param write      Is the a write. Default to a read
+ */
+static int ebt3000_cf_transfer(cf_device_t * cf, unsigned long lba_sector,
+			       unsigned long num_sectors, char *buffer,
+			       int write)
+{
+	if ((lba_sector + num_sectors) > cf->num_sectors) {
+		printk("%s: Attempt to access beyond end of device (%ld > %ld)\n", cf->gd->disk_name, lba_sector + num_sectors - 1, cf->num_sectors - 1);
+		num_sectors = cf->num_sectors - lba_sector;
+		if (num_sectors <= 0)
+			return 0;
+	}
+
+	if (write)
+		return ata_write(cf, lba_sector, num_sectors, buffer);
+	else
+		return ata_read(cf, lba_sector, num_sectors, buffer);
+}
+
+
+/**
+ * Handle queued IO requests
+ *
+ * @param q      queue of requests
+ */
+static void ebt3000_cf_request(request_queue_t * q)
+{
+	/* For some unknown reason, sometimes the kernel calls us with
+	   interrupts disabled. Since the CF is very slow, we just use the
+	   kernel call to wakeup a thread. This way we never block for long
+	   periods of time */
+	struct request *req = elv_next_request(q);
+	if (req) {
+		cf_device_t *cf = req->rq_disk->private_data;
+		complete(&cf->comp);
+	}
+}
+
+
+/**
+ * Ioctl.
+ *
+ * @param inode
+ * @param filp
+ * @param cmd
+ * @param arg
+ * @return
+ */
+int ebt3000_cf_ioctl(struct inode *inode, struct file *filp,
+		     unsigned int cmd, unsigned long arg)
+{
+	struct block_device *bdev = inode->i_bdev;
+	struct gendisk *disk = bdev->bd_disk;
+	cf_device_t *cf = disk->private_data;
+	struct hd_geometry geo;
+	switch (cmd) {
+	case HDIO_GETGEO:
+		if (!arg)
+			return -EINVAL;
+		memcpy(&geo, &cf->geo, sizeof(geo));
+		geo.start = get_start_sect(bdev);
+		if (copy_to_user
+		    ((struct hd_geometry __user *) arg, &geo, sizeof(geo)))
+			return -EFAULT;
+		return 0;
+	}
+	return -ENOTTY;		/* unknown command */
+}
+
+
+/**
+ * Kernel thread that does the actual IO operations
+ *
+ * @param cf_obj The compact flash device
+ * @return Never returns
+ */
+int ebt3000_cf_work(void *cf_obj)
+{
+	cf_device_t *cf = cf_obj;
+	struct request *req;
+	unsigned long flags;
+
+	/* Give ourself a nice name and become a daemon */
+	daemonize("octeon_%s", cf->gd->disk_name);
+
+	/* Identify the compact flash. We need its size */
+	ebt3000cf_identify(cf);
+	set_capacity(cf->gd,
+		     cf->num_sectors * (cf->sector_size / KERNEL_SECTOR_SIZE));
+
+	/* Loop forever waiting for IO requests */
+	while (1) {
+		/* Wait for the queue request handler to signal us there are
+		   requests available */
+		wait_for_completion(&cf->comp);
+
+		/* We need the queue lock */
+		spin_lock_irqsave(&cf->lock, flags);
+
+		/* Loop through all the pending requests */
+		while ((req = elv_next_request(cf->queue)) != NULL) {
+			if (!blk_fs_request(req)) {
+				printk("%s: Skip non-CMD request\n",
+				       req->rq_disk->disk_name);
+				end_request(req, 0);
+			} else {
+				int count;
+				/* Give away the lock while we're doing the
+				   slow IOs */
+				spin_unlock_irqrestore(&cf->lock, flags);
+				count = ebt3000_cf_transfer(cf, req->sector,
+							    req->
+							    current_nr_sectors,
+							    req->buffer,
+							    rq_data_dir(req));
+				/* We need the lock again to signal completion */
+				spin_lock_irqsave(&cf->lock, flags);
+				if (count == req->current_nr_sectors)
+					end_request(req, 1);
+				else
+					end_request(req, -EIO);
+			}
+		}
+		spin_unlock_irqrestore(&cf->lock, flags);
+	}
+	return 0;
+}
+
+
+
+/*
+ * The device operations structure.
+ */
+static struct block_device_operations ebt3000_cf_ops = {
+	.owner = THIS_MODULE,
+	.ioctl = ebt3000_cf_ioctl
+};
+
+
+/**
+ * Initialization
+ *
+ * @return
+ */
+static int __init ebt3000_cf_init(void)
+{
+	extern cvmx_bootinfo_t *octeon_bootinfo;
+	cf_device_t *cf = &STATIC_DEVICE;
+	int major_num;
+	int region;
+
+	printk(KERN_NOTICE DEVICE_NAME
+	       ": Octeon bootbus compact flash driver version %s\n", VERSION);
+
+	memset(cf, 0, sizeof(*cf));
+
+#if 0
+	/* Force driver to use true IDE compact flash for testing. */
+	octeon_bootinfo->compact_flash_common_base_addr = 0x1d040000;
+#endif
+
+	if (octeon_bootinfo->major_version == 1
+	    && octeon_bootinfo->minor_version >= 1) {
+		if (octeon_bootinfo->compact_flash_common_base_addr)
+			cf->base_ptr =
+				cvmx_phys_to_ptr(octeon_bootinfo->
+						 compact_flash_common_base_addr);
+		else {
+			printk(KERN_NOTICE DEVICE_NAME
+			       ": Compact flash interface not present.\n");
+			goto out;
+		}
+	} else
+		cf->base_ptr = cvmx_phys_to_ptr(0x1d000800);
+
+
+	/* Determine from base address is in true IDE mode or not */
+	if (!(octeon_bootinfo->compact_flash_common_base_addr & 0xffff))
+		cf->is_true_ide = 1;
+
+	spin_lock_init(&cf->lock);
+	init_completion(&cf->comp);
+
+	/* Get a request queue. */
+	cf->queue = blk_init_queue(ebt3000_cf_request, &cf->lock);
+	if (cf->queue == NULL) {
+		printk(DEVICE_NAME
+		       ": unable to allocate block request queue\n");
+		goto out;
+	}
+	blk_queue_hardsect_size(cf->queue, KERNEL_SECTOR_SIZE);
+
+	/* Get registered. */
+	major_num = register_blkdev(0, DEVICE_NAME);
+	if (major_num <= 0) {
+		printk(DEVICE_NAME ": unable to get major number\n");
+		goto out;
+	}
+
+	/* And the gendisk structure. */
+	cf->gd = alloc_disk(64);
+	if (cf->gd == NULL) {
+		printk(DEVICE_NAME ": unable to allocate disk\n");
+		goto out_unregister;
+	}
+
+	/* Find the bootbus region for the CF to determine 16 or 8 bit */
+	for (region = 0; region < 8; region++) {
+		cvmx_mio_boot_reg_cfgx_t cfg;
+		cfg.u64 = cvmx_read_csr(CVMX_MIO_BOOT_REG_CFGX(region));
+		if (cfg.s.base ==
+		    octeon_bootinfo->compact_flash_common_base_addr >> 16) {
+			cf->is16bit = cfg.s.width;
+			printk(KERN_NOTICE DEVICE_NAME
+			       ": Compact flash found in bootbus region %d (%d bit%s).\n",
+			       region, (cf->is16bit) ? 16 : 8,
+			       (cf->is_true_ide) ? ", ide" : "");
+			break;
+		}
+	}
+
+	cf->gd->major = major_num;
+	cf->gd->first_minor = 0;
+	cf->gd->fops = &ebt3000_cf_ops;
+	cf->gd->private_data = cf;
+	cf->gd->queue = cf->queue;
+	strcpy(cf->gd->disk_name, DEVICE_NAME "a");
+
+	/* Set a size to make sure the kernel trys to find partitions. The real
+	   size will be set when the thread starts processing */
+	set_capacity(cf->gd, 16);
+
+	/* Create a kernel thread for doing the real IO operations */
+	kernel_thread(ebt3000_cf_work, cf, 0);
+
+	add_disk(cf->gd);
+
+	return 0;
+
+      out_unregister:
+	unregister_blkdev(major_num, DEVICE_NAME);
+      out:
+	return -ENOMEM;
+}
+
+late_initcall(ebt3000_cf_init);
diff --git a/arch/mips/cavium-octeon/ethernet-mgmt-port.c b/arch/mips/cavium-octeon/ethernet-mgmt-port.c
new file mode 100644
index 0000000..65a40f6
--- /dev/null
+++ b/arch/mips/cavium-octeon/ethernet-mgmt-port.c
@@ -0,0 +1,308 @@
+/*
+ *   Octeon Management Port Ethernet Driver
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2007 Cavium Networks
+ */
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/ip.h>
+#include <linux/string.h>
+#include <linux/delay.h>
+
+#undef OCTEON_MODEL
+#define USE_RUNTIME_MODEL_CHECKS 1
+#include "cvmx.h"
+#include "cvmx-mgmt-port.h"
+
+static struct net_device *global_dev = NULL;
+#define DEBUGPRINT(format, ...) do{if (__printk_ratelimit(HZ, 10)) printk(format, ##__VA_ARGS__);} while (0)
+
+/**
+ * This is the definition of the Ethernet driver's private
+ * driver state stored in dev->priv.
+ */
+typedef struct {
+	int port;
+	struct net_device_stats stats;	/* Device statistics */
+} device_private_t;
+
+
+/**
+ * Packet transmit
+ *
+ * @param skb    Packet to send
+ * @param dev    Device info structure
+ * @return Always returns zero
+ */
+static int packet_transmit(struct sk_buff *skb, struct net_device *dev)
+{
+	uint64_t flags;
+	device_private_t *priv = (device_private_t *) dev->priv;
+	cvmx_mgmt_port_result_t result;
+	local_irq_save(flags);
+	result = cvmx_mgmt_port_send(priv->port, skb->len, skb->data);
+	local_irq_restore(flags);
+	if (result == CVMX_MGMT_PORT_SUCCESS) {
+		priv->stats.tx_packets++;
+		priv->stats.tx_bytes += skb->len;
+	} else {
+		// DEBUGPRINT("ERROR: cvmx_mgmt_port_send() failed with %d\n",
+		// result);
+		priv->stats.tx_dropped++;
+	}
+	dev_kfree_skb(skb);
+	return 0;
+}
+
+
+/**
+ * Interrupt handler. The interrupt occurs whenever the POW
+ * transitions from 0->1 packets in our group.
+ *
+ * @param cpl
+ * @param dev_id
+ * @param regs
+ * @return
+ */
+static irqreturn_t do_interrupt(int cpl, void *dev_id)
+{
+	uint64_t flags;
+	struct sk_buff *skb;
+	int result;
+	char packet[2048];
+	struct net_device *dev = (struct net_device *) dev_id;
+	device_private_t *priv = (device_private_t *) dev->priv;
+
+	do {
+		local_irq_save(flags);
+		result = cvmx_mgmt_port_receive(priv->port, sizeof(packet),
+						packet);
+		local_irq_restore(flags);
+
+		/* Silently drop packets if we aren't up */
+		if ((dev->flags & IFF_UP) == 0)
+			continue;
+
+		if (result > 0) {
+			skb = dev_alloc_skb(result);
+			if (skb) {
+				memcpy(skb_put(skb, result), packet, result);
+				skb->protocol = eth_type_trans(skb, dev);
+				skb->dev = dev;
+				skb->ip_summed = CHECKSUM_NONE;
+				priv->stats.rx_bytes += skb->len;
+				priv->stats.rx_packets++;
+				netif_rx(skb);
+			} else {
+				DEBUGPRINT
+					("%s: Failed to allocate skbuff, packet dropped\n",
+					 dev->name);
+				priv->stats.rx_dropped++;
+			}
+		} else if (result < 0) {
+			DEBUGPRINT
+				("%s: Receive error code %d, packet dropped\n",
+				 dev->name, result);
+			priv->stats.rx_errors++;
+		}
+	} while (result != 0);
+
+	/* Clear any pending interrupts */
+	cvmx_write_csr(CVMX_MIXX_ISR(priv->port),
+		       cvmx_read_csr(CVMX_MIXX_ISR(priv->port)));
+	cvmx_read_csr(CVMX_MIXX_ISR(priv->port));
+
+	return IRQ_HANDLED;
+}
+
+
+#ifdef CONFIG_NET_POLL_CONTROLLER
+/**
+ * This is called when the kernel needs to manually poll the
+ * device. For Octeon, this is simply calling the interrupt
+ * handler. We actually poll all the devices, not just the
+ * one supplied.
+ *
+ * @param dev    Device to poll. Unused
+ */
+static void device_poll_controller(struct net_device *dev)
+{
+	do_interrupt(0, dev, NULL);
+}
+#endif
+
+
+/**
+ * Open a device for use. Device should be able to send and
+ * receive packets after this is called.
+ *
+ * @param dev    Device to bring up
+ * @return Zero on success
+ */
+static int device_open(struct net_device *dev)
+{
+	/* Clear the statistics whenever the interface is brought up */
+	device_private_t *priv = (device_private_t *) dev->priv;
+	memset(&priv->stats, 0, sizeof(priv->stats));
+	cvmx_mgmt_port_enable(priv->port);
+	return 0;
+}
+
+
+/**
+ * Stop an ethernet device. No more packets should be
+ * received from this device.
+ *
+ * @param dev    Device to bring down
+ * @return Zero on success
+ */
+static int device_close(struct net_device *dev)
+{
+	device_private_t *priv = (device_private_t *) dev->priv;
+	cvmx_mgmt_port_disable(priv->port);
+	return 0;
+}
+
+
+/**
+ * Get the low level ethernet statistics
+ *
+ * @param dev    Device to get the statistics from
+ * @return Pointer to the statistics
+ */
+static struct net_device_stats *device_get_stats(struct net_device *dev)
+{
+	device_private_t *priv = (device_private_t *) dev->priv;
+	return &priv->stats;
+}
+
+
+/**
+ * Per network device initialization
+ *
+ * @param dev    Device to initialize
+ * @return Zero on success
+ */
+static int device_init(struct net_device *dev)
+{
+	device_private_t *priv = (device_private_t *) dev->priv;
+	uint64_t mac = cvmx_mgmt_port_get_mac(priv->port);
+
+	dev->hard_start_xmit = packet_transmit;
+	dev->get_stats = device_get_stats;
+	dev->open = device_open;
+	dev->stop = device_close;
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	dev->poll_controller = device_poll_controller;
+#endif
+	dev->weight = 16;
+	dev->dev_addr[0] = (mac >> 40) & 0xff;
+	dev->dev_addr[1] = (mac >> 32) & 0xff;
+	dev->dev_addr[2] = (mac >> 24) & 0xff;
+	dev->dev_addr[3] = (mac >> 16) & 0xff;
+	dev->dev_addr[4] = (mac >> 8) & 0xff;
+	dev->dev_addr[5] = (mac >> 0) & 0xff;
+	return 0;
+}
+
+
+/**
+ * Module/ driver initialization. Creates the linux network
+ * devices.
+ *
+ * @return Zero on success
+ */
+static int __init ethernet_mgmt_port_init(void)
+{
+	struct net_device *dev;
+	device_private_t *priv;
+	cvmx_mixx_irhwm_t mix_irhwm;
+	cvmx_mixx_intena_t mix_intena;
+
+	if (!OCTEON_IS_MODEL(OCTEON_CN56XX))
+		return 0;
+
+	printk("Octeon management port ethernet driver\n");
+
+	if (cvmx_mgmt_port_initialize(0) != CVMX_MGMT_PORT_SUCCESS) {
+		printk("\n\nERROR: cvmx_mgmt_port_initialize() failed\n");
+		return -1;
+	}
+
+	/* Setup is complete, create the virtual ethernet devices */
+	dev = alloc_etherdev(sizeof(device_private_t));
+	if (dev == NULL) {
+		printk("\n\nERROR: Failed to allocate ethernet device\n");
+		return -1;
+	}
+
+	SET_MODULE_OWNER(dev);
+	dev->init = device_init;
+	strcpy(dev->name, "mgmt%d");
+
+	/* Initialize the device private structure. */
+	priv = (device_private_t *) dev->priv;
+	memset(priv, 0, sizeof(device_private_t));
+
+	if (register_netdev(dev) < 0) {
+		printk("\n\nERROR: Failed to register ethernet device\n");
+		kfree(dev);
+		return -1;
+	}
+
+	/* Clear any pending interrupts */
+	cvmx_write_csr(CVMX_MIXX_ISR(priv->port),
+		       cvmx_read_csr(CVMX_MIXX_ISR(priv->port)));
+
+	/* Register an IRQ hander for to receive interrupts */
+	request_irq(8 + 62, do_interrupt, SA_SHIRQ, "Mgmt Ethernet", dev);
+
+	/* Interrupt every single RX packet */
+	mix_irhwm.u64 = 0;
+	mix_irhwm.s.irhwm = 0;
+	cvmx_write_csr(CVMX_MIXX_IRHWM(priv->port), mix_irhwm.u64);
+
+	/* Enable receive interrupts */
+	mix_intena.u64 = 0;
+	mix_intena.s.ithena = 1;
+	cvmx_write_csr(CVMX_MIXX_INTENA(priv->port), mix_intena.u64);
+
+	global_dev = dev;
+	return 0;
+}
+
+
+/**
+ * Module / driver shutdown
+ *
+ * @return Zero on success
+ */
+static void __exit ethernet_mgmt_port_cleanup(void)
+{
+	if (global_dev) {
+		device_private_t *priv = (device_private_t *) global_dev->priv;
+		/* Disable interrupt */
+		cvmx_write_csr(CVMX_MIXX_IRHWM(priv->port), 0);
+		cvmx_write_csr(CVMX_MIXX_INTENA(priv->port), 0);
+
+		/* Free the interrupt handler */
+		free_irq(8 + 62, global_dev);
+
+		/* Free the ethernet devices */
+		unregister_netdev(global_dev);
+		kfree(global_dev);
+		cvmx_mgmt_port_shutdown(0);
+	}
+}
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Cavium Networks <support@caviumnetworks.com>");
+MODULE_DESCRIPTION("Cavium Networks Octeon management port ethernet driver.");
+module_init(ethernet_mgmt_port_init);
+module_exit(ethernet_mgmt_port_cleanup);
diff --git a/arch/mips/cavium-octeon/ethernet-pow.c b/arch/mips/cavium-octeon/ethernet-pow.c
new file mode 100644
index 0000000..be208a5
--- /dev/null
+++ b/arch/mips/cavium-octeon/ethernet-pow.c
@@ -0,0 +1,734 @@
+/*
+ *   Octeon POW Ethernet Driver
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2005-2007 Cavium Networks
+ */
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/ip.h>
+#include <linux/string.h>
+#include <linux/delay.h>
+#include "hal.h"
+
+#define NUM_GROUPS      16	/* Total number of groups in Octeon */
+#define VIRTUAL_PORT    36	/* Value to put in work->ipprt */
+#define IP_PROTOCOL_TCP 6	/* IP header protocol for TCP */
+#define IP_PROTOCOL_UDP 0x11	/* IP header protocol for UDP */
+
+#define DEBUGPRINT(format, ...) do{if (__printk_ratelimit(HZ, 10)) printk(format, ##__VA_ARGS__);} while (0)
+
+/* These are the Octeon CSR IO addresses we are going to need */
+#define OCTEON_POW_WQ_INT_THRX(offset)  (0x8001670000000080ull+((offset)*8))
+#define OCTEON_POW_WQ_INT               (0x8001670000000200ull)
+#define OCTEON_POW_PP_GRP_MSKX(offset)  (0x8001670000000000ull+((offset)*8))
+#define OCTEON_POW_WORK_SUBMIT(wqe)     (0x8001610000000000ull | (wqe))
+#define OCTEON_POW_WORK_REQUEST(wait)   (0x8001600000000000ull | (wait<<3))
+#define OCTEON_FPA_ALLOC(pool)          (0x8001280000000000ull | ((uint64_t)pool<<40))
+#define OCTEON_FPA_FREE(pool, address)  (0x8001280000000000ull | ((uint64_t)pool<<40) | (address))
+#define OCTEON_IPD_PACKET_MBUFF_SIZE    (0x80014F0000000010ull)
+#define OCTEON_IPD_WQE_FPA_QUEUE        (0x80014F0000000020ull)
+#define OCTEON_IPD_CTL_STATUS           (0x80014F0000000018ull)
+
+int receive_group = -1;
+module_param(receive_group, int, 0444);
+MODULE_PARM_DESC(receive_group,
+		 " 0-16 POW group to receive packets from. This must be unique in\n"
+		 "\t\tthe system. If you don't specify a value, the core ID will\n"
+		 "\t\tbe used.");
+
+int broadcast_groups = 0;
+module_param(broadcast_groups, int, 0644);
+MODULE_PARM_DESC(broadcast_groups,
+		 " Bitmask of groups to send broadcasts to. This MUST be specified.\n"
+		 "\t\tWARNING: Be careful to not send broadcasts to groups that aren't\n"
+		 "\t\tread otherwise you may fill the POW and stop receiving packets.\n");
+
+/**
+ * This is the definition of the Ethernet driver's private
+ * driver state stored in dev->priv.
+ */
+typedef struct {
+	struct net_device_stats stats;	/* Device statistics */
+} device_private_t;
+
+/**
+ * This structure defines a buffer pointer on Octeon
+ */
+typedef union {
+	void *ptr;
+	uint64_t u64;
+	struct {
+		uint64_t i:1;
+		uint64_t back:4;
+		uint64_t pool:3;
+		uint64_t size:16;
+		uint64_t addr:40;
+	} s;
+} octeon_buf_ptr_t;
+
+/**
+ * Work queue entry format
+ */
+typedef struct {
+	uint16_t hw_chksum;
+	uint8_t unused;
+	uint64_t next_ptr:40;
+	uint64_t len:16;
+	uint64_t ipprt:6;
+	uint64_t qos:3;
+	uint64_t grp:4;
+	uint64_t tag_type:3;
+	uint64_t tag:32;
+	union {
+		uint64_t u64;
+		struct {
+			uint64_t bufs:8;
+			uint64_t ip_offset:8;
+			uint64_t vlan_valid:1;
+			uint64_t unassigned:2;
+			uint64_t vlan_cfi:1;
+			uint64_t vlan_id:12;
+			uint64_t unassigned2:12;
+			uint64_t dec_ipcomp:1;
+			uint64_t tcp_or_udp:1;
+			uint64_t dec_ipsec:1;
+			uint64_t is_v6:1;
+			uint64_t software:1;
+			uint64_t L4_error:1;
+			uint64_t is_frag:1;
+			uint64_t IP_exc:1;
+			uint64_t is_bcast:1;
+			uint64_t is_mcast:1;
+			uint64_t not_IP:1;
+			uint64_t rcv_error:1;
+			uint64_t err_code:8;
+		} s;
+		struct {
+			uint64_t bufs:8;
+			uint64_t unused:8;
+			uint64_t vlan_valid:1;
+			uint64_t unassigned:2;
+			uint64_t vlan_cfi:1;
+			uint64_t vlan_id:12;
+			uint64_t unassigned2:16;
+			uint64_t software:1;
+			uint64_t unassigned3:1;
+			uint64_t is_rarp:1;
+			uint64_t is_arp:1;
+			uint64_t is_bcast:1;
+			uint64_t is_mcast:1;
+			uint64_t not_IP:1;
+			uint64_t rcv_error:1;
+			uint64_t err_code:8;
+		} snoip;
+	} word2;
+	octeon_buf_ptr_t packet_ptr;
+	uint8_t packet_data[96];
+} octeon_wqe_t;
+
+static int fpa_wqe_pool = 1;	/* HW FPA pool to use for work queue entries */
+static int fpa_packet_pool = 0;	/* HW FPA pool to use for packet buffers */
+static int fpa_packet_pool_size = 2048;	/* Size of the packet buffers */
+static struct net_device *global_device = NULL;
+
+
+/**
+ * Given a packet data address, return a pointer to the
+ * beginning of the packet buffer.
+ *
+ * @param packet_ptr Packet data hardware address
+ * @return Packet buffer pointer
+ */
+static inline void *get_buffer_ptr(octeon_buf_ptr_t packet_ptr)
+{
+	return phys_to_virt(((packet_ptr.s.addr >> 7) -
+			     packet_ptr.s.back) << 7);
+}
+
+
+/**
+ * Get a new block from the FPA
+ *
+ * @param pool   Pool to get the block from
+ * @return Pointer to the block or NULL on failure
+ */
+static inline void *fpa_alloc(uint64_t pool)
+{
+	uint64_t address = cvmx_read_csr(OCTEON_FPA_ALLOC(pool));
+	if (address)
+		return phys_to_virt(address);
+	else
+		return NULL;
+}
+
+
+/**
+ * Free a block allocated with a FPA pool.  Provides required memory
+ * ordering in cases where memory block was modified by core.
+ *
+ * @param ptr    Block to free
+ * @param pool   Pool to put it in
+ * @param num_cache_lines
+ *               Cache lines to invalidate
+ */
+static inline void fpa_free(void *ptr, int pool, int num_cache_lines)
+{
+	wmb();
+	cvmx_write_csr(OCTEON_FPA_FREE(pool, virt_to_phys(ptr)),
+		       num_cache_lines);
+}
+
+
+/**
+ * Submits work to an input queue.  This function updates the work queue entry in DRAM to match
+ * the arguments given.
+ * Note that the tag provided is for the work queue entry submitted, and is unrelated to the tag that
+ * the core currently holds.
+ *
+ * @param wqp      pointer to work queue entry to submit.  This entry is updated to match the other parameters
+ * @param tag      tag value to be assigned to work queue entry
+ * @param tag_type type of tag
+ * @param qos      Input queue to add to.
+ * @param grp      group value for the work queue entry.
+ */
+static inline void pow_work_submit(octeon_wqe_t * wqp, uint32_t tag,
+				   uint64_t tag_type, uint64_t qos,
+				   uint64_t grp)
+{
+	uint64_t tag_req =
+		(4ull << 44) | (qos << 39) | (grp << 35) | (tag_type << 32) |
+		tag;
+	wmb();
+	cvmx_write_csr(OCTEON_POW_WORK_SUBMIT(virt_to_phys(wqp)), tag_req);
+}
+
+
+/**
+ * Synchronous work request.  Requests work from the POW.
+ * This function does NOT wait for previous tag switches to complete,
+ * so the caller must ensure that there is not a pending tag switch.
+ *
+ * @param wait   When set, call stalls until work becomes avaiable, or times out.
+ *               If not set, returns immediately.
+ *
+ * @return Returns the WQE pointer from POW. Returns NULL if no work was available.
+ */
+static inline octeon_wqe_t *pow_work_request_sync(int wait)
+{
+	int64_t result = cvmx_read_csr(OCTEON_POW_WORK_REQUEST(wait));
+	if (result < 0)
+		return NULL;
+	else
+		return (octeon_wqe_t *) phys_to_virt(result);
+}
+
+
+/**
+ * Free a work queue entry received in a intercept callback.
+ *
+ * @param work_queue_entry
+ *               Work queue entry to free
+ * @return Zero on success, Negative on failure.
+ */
+static int free_work(octeon_wqe_t * work)
+{
+	int segments = work->word2.s.bufs;
+	octeon_buf_ptr_t segment_ptr = work->packet_ptr;
+
+	while (segments--) {
+		octeon_buf_ptr_t next_ptr =
+			*(octeon_buf_ptr_t *) phys_to_virt(segment_ptr.s.addr -
+							   8);
+		if (unlikely(!segment_ptr.s.i))
+			fpa_free(get_buffer_ptr(segment_ptr),
+				 segment_ptr.s.pool, 0);
+		segment_ptr = next_ptr;
+	}
+	fpa_free(work, fpa_wqe_pool, 0);
+
+	return 0;
+}
+
+
+/**
+ * Packet transmit to the POW
+ *
+ * @param skb    Packet to send
+ * @param dev    Device info structure
+ * @return Always returns zero
+ */
+static int packet_transmit(struct sk_buff *skb, struct net_device *dev)
+{
+	device_private_t *priv = (device_private_t *) dev->priv;
+	octeon_wqe_t *work = NULL;
+	void *packet_buffer = NULL;
+	void *copy_location;
+	int send_group_mask;
+	int send_group;
+
+	/* Any unknown MAC address goes to all groups in the module param
+	   broadcast_groups. Known MAC addresses use the low order dest mac
+	   byte as the group number */
+	if ((*(uint64_t *) (skb->data) >> 16) < 0x01ff)
+		send_group_mask = 1 << (skb->data[5] & (NUM_GROUPS - 1));
+	else
+		send_group_mask = broadcast_groups;
+	send_group_mask &= ~(1 << receive_group);
+
+	/* It is ugly, but we need to send multiple times for broadcast
+	   packets. The hardware doesn't support submitting work to multiple
+	   groups */
+	for (send_group = 0; send_group < NUM_GROUPS; send_group++) {
+		/* Don't transmit to groups not in our send_group_mask */
+		if (likely((send_group_mask & (1 << send_group)) == 0))
+			continue;
+
+		/* Get a work queue entry */
+		work = fpa_alloc(fpa_wqe_pool);
+		if (unlikely(work == NULL)) {
+			DEBUGPRINT
+				("%s: Failed to allocate a work queue entry\n",
+				 dev->name);
+			goto fail;
+		}
+
+		/* Get a packet buffer */
+		packet_buffer = fpa_alloc(fpa_packet_pool);
+		if (unlikely(packet_buffer == NULL)) {
+			DEBUGPRINT("%s: Failed to allocate a packet buffer\n",
+				   dev->name);
+			goto fail;
+		}
+
+		/* Calculate where we need to copy the data to. We need to
+		   leave 8 bytes for a next pointer (unused). Then we need to
+		   align the IP packet src and dest into the same 64bit word. */
+		copy_location = packet_buffer + sizeof(uint64_t) + 6;
+
+		/* Fail if the packet won't fit in a single buffer */
+		if (unlikely
+		    (copy_location + skb->len >
+		     packet_buffer + fpa_packet_pool_size)) {
+			DEBUGPRINT("%s: Packet too large for FPA buffer\n",
+				   dev->name);
+			goto fail;
+		}
+
+		memcpy(copy_location, skb->data, skb->len);
+
+		/* Fill in some of the work queue fields. We may need to add
+		   more if the software at the other end needs them */
+		work->hw_chksum = skb->csum;
+		work->len = skb->len;
+		work->ipprt = VIRTUAL_PORT;
+		work->qos = 0;
+		work->grp = send_group;
+		work->tag_type = 2;
+		work->tag = 0;
+		work->word2.u64 = 0;	/* Default to zero. Sets of zero later
+					   are commented out */
+		work->word2.s.bufs = 1;
+		work->packet_ptr.u64 = 0;
+		work->packet_ptr.s.addr = virt_to_phys(copy_location);
+		work->packet_ptr.s.pool = fpa_packet_pool;
+		work->packet_ptr.s.size = fpa_packet_pool_size;
+		work->packet_ptr.s.back = (copy_location - packet_buffer) >> 7;
+
+		if (skb->protocol == htons(ETH_P_IP)) {
+			work->word2.s.ip_offset = 14;
+			// work->word2.s.vlan_valid = 0; /* FIXME */
+			// work->word2.s.vlan_cfi = 0; /* FIXME */
+			// work->word2.s.vlan_id = 0; /* FIXME */
+			// work->word2.s.dec_ipcomp = 0; /* FIXME */
+			work->word2.s.tcp_or_udp =
+				(skb->nh.iph->protocol == IP_PROTOCOL_TCP) ||
+				(skb->nh.iph->protocol == IP_PROTOCOL_UDP);
+			// work->word2.s.dec_ipsec = 0; /* FIXME */
+			// work->word2.s.is_v6 = 0; /* We only support IPv4
+			// right now */
+			// work->word2.s.software = 0; /* Hardware would set to
+			// zero */
+			// work->word2.s.L4_error = 0; /* No error, packet is
+			// internal */
+			work->word2.s.is_frag = !((skb->nh.iph->frag_off == 0)
+						  || (skb->nh.iph->frag_off ==
+						      1 << 14));
+			// work->word2.s.IP_exc = 0; /* Assume Linux is sending
+			// a good packet */
+			work->word2.s.is_bcast =
+				(skb->pkt_type == PACKET_BROADCAST);
+			work->word2.s.is_mcast =
+				(skb->pkt_type == PACKET_MULTICAST);
+			// work->word2.s.not_IP = 0; /* This is an IP packet */
+			// work->word2.s.rcv_error = 0; /* No error, packet is
+			// internal */
+			// work->word2.s.err_code = 0; /* No error, packet is
+			// internal */
+
+			/* When copying the data, include 4 bytes of the
+			   ethernet header to align the same way hardware does */
+			memcpy(work->packet_data, skb->data + 10,
+			       sizeof(work->packet_data));
+		} else {
+			// work->word2.snoip.vlan_valid = 0; /* FIXME */
+			// work->word2.snoip.vlan_cfi = 0; /* FIXME */
+			// work->word2.snoip.vlan_id = 0; /* FIXME */
+			// work->word2.snoip.software = 0; /* Hardware would
+			// set to zero */
+			work->word2.snoip.is_rarp =
+				skb->protocol == htons(ETH_P_RARP);
+			work->word2.snoip.is_arp =
+				skb->protocol == htons(ETH_P_ARP);
+			work->word2.snoip.is_bcast =
+				(skb->pkt_type == PACKET_BROADCAST);
+			work->word2.snoip.is_mcast =
+				(skb->pkt_type == PACKET_MULTICAST);
+			work->word2.snoip.not_IP = 1;	/* IP was done up above
+							 */
+			// work->word2.snoip.rcv_error = 0; /* No error, packet
+			// is internal */
+			// work->word2.snoip.err_code = 0; /* No error, packet
+			// is internal */
+			memcpy(work->packet_data, skb->data,
+			       sizeof(work->packet_data));
+		}
+
+		/* Submit the packet to the POW */
+		pow_work_submit(work, work->tag, work->tag_type, work->qos,
+				work->grp);
+		work = NULL;
+		packet_buffer = NULL;
+	}
+
+	priv->stats.tx_packets++;
+	priv->stats.tx_bytes += skb->len;
+	dev_kfree_skb(skb);
+	return 0;
+
+      fail:
+	if (work)
+		fpa_free(work, fpa_wqe_pool, 0);
+	if (packet_buffer)
+		fpa_free(packet_buffer, fpa_packet_pool, 0);
+	priv->stats.tx_dropped++;
+	dev_kfree_skb(skb);
+	return 0;
+}
+
+
+/**
+ * Interrupt handler. The interrupt occurs whenever the POW
+ * transitions from 0->1 packets in our group.
+ *
+ * @param cpl
+ * @param dev_id
+ * @param regs
+ * @return
+ */
+static irqreturn_t do_interrupt(int cpl, void *dev_id)
+{
+	const uint64_t coreid = cvmx_get_core_num();
+	struct net_device *dev = (struct net_device *) dev_id;
+	device_private_t *priv = (device_private_t *) dev->priv;
+	uint64_t old_group_mask;
+	octeon_wqe_t *work;
+	struct sk_buff *skb;
+
+	/* Make sure any userspace operations are complete */
+	asm volatile ("synciobdma":::"memory");
+
+	/* Acknowledge the interrupt */
+	cvmx_write_csr(OCTEON_POW_WQ_INT, 0x10001 << receive_group);
+
+	/* Only allow work for our group */
+	old_group_mask = cvmx_read_csr(OCTEON_POW_PP_GRP_MSKX(coreid));
+	cvmx_write_csr(OCTEON_POW_PP_GRP_MSKX(coreid), 1 << receive_group);
+
+	while (1) {
+		work = pow_work_request_sync(0);
+		if (work == NULL)
+			break;
+
+		/* Silently drop packets that have the wrong input port */
+		if (work->ipprt != VIRTUAL_PORT) {
+			free_work(work);
+			continue;
+		}
+
+		/* Silently drop packets if we aren't up */
+		if ((dev->flags & IFF_UP) == 0) {
+			free_work(work);
+			continue;
+		}
+
+		/* Throw away all packets with receive errors */
+		if (unlikely(work->word2.snoip.rcv_error)) {
+			DEBUGPRINT
+				("%s: Receive error code %d, packet dropped\n",
+				 dev->name, work->word2.snoip.err_code);
+			free_work(work);
+			priv->stats.rx_errors++;
+			continue;
+		}
+
+		/* We have to copy the packet. First allocate an skbuff for it */
+		skb = dev_alloc_skb(work->len);
+		if (!skb) {
+			DEBUGPRINT
+				("%s: Failed to allocate skbuff, packet dropped\n",
+				 dev->name);
+			free_work(work);
+			priv->stats.rx_dropped++;
+			continue;
+		}
+
+		/* Check if we've received a packet that was entirely stored
+		   the work entry. This is untested */
+		if (unlikely(work->word2.s.bufs == 0)) {
+			DEBUGPRINT
+				("%s: Received a work with work->word2.s.bufs=0, untested\n",
+				 dev->name);
+			memcpy(skb_put(skb, work->len), work->packet_data,
+			       work->len);
+		} else {
+			int segments = work->word2.s.bufs;
+			octeon_buf_ptr_t segment_ptr = work->packet_ptr;
+			int len = work->len;
+			while (segments--) {
+				octeon_buf_ptr_t next_ptr =
+					*(octeon_buf_ptr_t *)
+					phys_to_virt(segment_ptr.s.addr - 8);
+				/* Octeon Errata PKI-100: The segment size is
+				   wrong. Until it is fixed, calculate the
+				   segment size based on the packet pool buffer
+				   size. When it is fixed, the following line
+				   should be replaced with this one: int
+				   segment_size = segment_ptr.s.size; */
+				int segment_size =
+					fpa_packet_pool_size -
+					(segment_ptr.s.addr -
+					 (((segment_ptr.s.addr >> 7) -
+					   segment_ptr.s.back) << 7));
+				/* Don't copy more than what is left in the
+				   packet */
+				if (segment_size > len)
+					segment_size = len;
+				/* Copy the data into the packet */
+				memcpy(skb_put(skb, segment_size),
+				       phys_to_virt(segment_ptr.s.addr),
+				       segment_size);
+				/* Reduce the amount of bytes left to copy */
+				len -= segment_size;
+				segment_ptr = next_ptr;
+			}
+		}
+		free_work(work);
+		skb->protocol = eth_type_trans(skb, dev);
+		skb->dev = dev;
+		skb->ip_summed = CHECKSUM_NONE;
+		priv->stats.rx_bytes += skb->len;
+		priv->stats.rx_packets++;
+		netif_rx(skb);
+	}
+
+	/* Restore the original POW group mask */
+	cvmx_write_csr(OCTEON_POW_PP_GRP_MSKX(coreid), old_group_mask);
+	return IRQ_HANDLED;
+}
+
+
+#ifdef CONFIG_NET_POLL_CONTROLLER
+/**
+ * This is called when the kernel needs to manually poll the
+ * device. For Octeon, this is simply calling the interrupt
+ * handler. We actually poll all the devices, not just the
+ * one supplied.
+ *
+ * @param dev    Device to poll. Unused
+ */
+static void device_poll_controller(struct net_device *dev)
+{
+	do_interrupt(0, dev, NULL);
+}
+#endif
+
+
+/**
+ * Open a device for use. Device should be able to send and
+ * receive packets after this is called.
+ *
+ * @param dev    Device to bring up
+ * @return Zero on success
+ */
+static int device_open(struct net_device *dev)
+{
+	/* Clear the statistics whenever the interface is brought up */
+	device_private_t *priv = (device_private_t *) dev->priv;
+	memset(&priv->stats, 0, sizeof(priv->stats));
+	return 0;
+}
+
+
+/**
+ * Stop an ethernet device. No more packets should be
+ * received from this device.
+ *
+ * @param dev    Device to bring down
+ * @return Zero on success
+ */
+static int device_close(struct net_device *dev)
+{
+	/* Nothing to do */
+	return 0;
+}
+
+
+/**
+ * Get the low level ethernet statistics
+ *
+ * @param dev    Device to get the statistics from
+ * @return Pointer to the statistics
+ */
+static struct net_device_stats *device_get_stats(struct net_device *dev)
+{
+	device_private_t *priv = (device_private_t *) dev->priv;
+	return &priv->stats;
+}
+
+
+/**
+ * Per network device initialization
+ *
+ * @param dev    Device to initialize
+ * @return Zero on success
+ */
+static int device_init(struct net_device *dev)
+{
+	dev->hard_start_xmit = packet_transmit;
+	dev->get_stats = device_get_stats;
+	dev->open = device_open;
+	dev->stop = device_close;
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	dev->poll_controller = device_poll_controller;
+#endif
+	dev->weight = 16;
+	dev->features |= NETIF_F_LLTX;	/* We do our own locking, Linux doesn't
+					   need to */
+	dev->dev_addr[0] = 0;
+	dev->dev_addr[1] = 0;
+	dev->dev_addr[2] = 0;
+	dev->dev_addr[3] = 0;
+	dev->dev_addr[4] = 1;
+	dev->dev_addr[5] = cvmx_get_core_num();
+	return 0;
+}
+
+
+/**
+ * Module/ driver initialization. Creates the linux network
+ * devices.
+ *
+ * @return Zero on success
+ */
+static int __init ethernet_pow_init(void)
+{
+	device_private_t *priv;
+
+	if ((receive_group >= NUM_GROUPS)) {
+		printk("\n\nERROR: Invalid receive group. Must be 0-%d\n",
+		       NUM_GROUPS - 1);
+		return -1;
+	}
+
+	if (!broadcast_groups) {
+		printk("\n\nERROR: You must specify a broadcast group mask.\n");
+		return -1;
+	}
+
+	if ((broadcast_groups & ((1 << NUM_GROUPS) - 1)) != broadcast_groups) {
+		printk("\n\nERROR: Invalid broadcast group mask.\n");
+		return -1;
+	}
+
+	printk("Octeon POW only ethernet driver\n");
+
+	/* If a receive group isn't specified, default to the core id */
+	if (receive_group < 0)
+		receive_group = cvmx_get_core_num();
+
+	/* Setup is complete, create the virtual ethernet devices */
+	global_device = alloc_etherdev(sizeof(device_private_t));
+	if (global_device == NULL) {
+		printk("\n\nERROR: Failed to allocate ethernet device\n");
+		return -1;
+	}
+
+	SET_MODULE_OWNER(global_device);
+	global_device->init = device_init;
+	strcpy(global_device->name, "oct%d");
+
+	/* Initialize the device private structure. */
+	priv = (device_private_t *) global_device->priv;
+	memset(priv, 0, sizeof(device_private_t));
+
+	/* Spin waiting for another core to setup all the hardware */
+	printk("Waiting for another core to setup the IPD hardware...");
+	while ((cvmx_read_csr(OCTEON_IPD_CTL_STATUS) & 1) == 0) {
+		mdelay(100);
+	}
+	printk("Done\n");
+
+	/* Read the configured size of the FPA packet buffers. This way we
+	   don't need changes if someone chooses to use a different buffer size
+	 */
+	fpa_packet_pool_size =
+		(cvmx_read_csr(OCTEON_IPD_PACKET_MBUFF_SIZE) & 0xfff) * 8;
+
+	/* Read the work queue pool */
+	fpa_wqe_pool = cvmx_read_csr(OCTEON_IPD_WQE_FPA_QUEUE) & 7;
+
+	if (register_netdev(global_device) < 0) {
+		printk("\n\nERROR: Failed to register ethernet device\n");
+		kfree(global_device);
+		return -1;
+	}
+
+	/* Register an IRQ hander for to receive POW interrupts */
+	request_irq(8 + receive_group, do_interrupt, SA_SHIRQ, "POW Ethernet",
+		    global_device);
+
+	/* Enable POW interrupt when our port has at least one packet */
+	cvmx_write_csr(OCTEON_POW_WQ_INT_THRX(receive_group), 0x1001);
+	return 0;
+}
+
+
+/**
+ * Module / driver shutdown
+ *
+ * @return Zero on success
+ */
+static void __exit ethernet_pow_cleanup(void)
+{
+	/* Disable POW interrupt */
+	cvmx_write_csr(OCTEON_POW_WQ_INT_THRX(receive_group), 0);
+
+	/* Free the interrupt handler */
+	free_irq(8 + receive_group, global_device);
+
+	/* Free the ethernet devices */
+	unregister_netdev(global_device);
+	kfree(global_device);
+}
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Cavium Networks <support@caviumnetworks.com>");
+MODULE_DESCRIPTION("Cavium Networks Octeon internal only POW ethernet driver.");
+module_init(ethernet_pow_init);
+module_exit(ethernet_pow_cleanup);
diff --git a/arch/mips/cavium-octeon/flash_setup.c b/arch/mips/cavium-octeon/flash_setup.c
new file mode 100644
index 0000000..544343a
--- /dev/null
+++ b/arch/mips/cavium-octeon/flash_setup.c
@@ -0,0 +1,76 @@
+/*
+ *   Octeon Bootbus flash setup
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2007 Cavium Networks
+ */
+#include <linux/kernel.h>
+#include <linux/mtd/mtd.h>
+#include <linux/mtd/map.h>
+#include <linux/mtd/partitions.h>
+#include "hal.h"
+
+static struct map_info flash_map;
+static struct mtd_info *mymtd;
+#ifdef CONFIG_MTD_PARTITIONS
+static int nr_parts;
+static struct mtd_partition *parts;
+static const char *part_probe_types[] = {
+	"cmdlinepart",
+#ifdef CONFIG_MTD_REDBOOT_PARTS
+	"RedBoot",
+#endif
+	NULL
+};
+#endif
+
+/**
+ * Module/ driver initialization.
+ *
+ * @return Zero on success
+ */
+static int __init flash_init(void)
+{
+	/* Read the bootbus region 0 setup to determine where the base of flash
+	   is set for */
+	cvmx_mio_boot_reg_cfgx_t region_cfg;
+	region_cfg.u64 = cvmx_read_csr(CVMX_MIO_BOOT_REG_CFG0);
+	if (region_cfg.s.en) {
+		/* The bootloader always takes the flash and sets its address
+		   so the entire flash fits below 0x1fc00000. This way the
+		   flash aliases to 0x1fc00000 for booting. Software can access
+		   the full flash at the true address, while core boot can
+		   access 4MB */
+		flash_map.name = "phys_mapped_flash";	/* Use this name so old
+							   part lines work */
+		flash_map.phys = region_cfg.s.base << 16;
+		flash_map.size = 0x1fc00000 - flash_map.phys;
+		flash_map.bankwidth = 1;
+		flash_map.virt = ioremap(flash_map.phys, flash_map.size);
+		printk("Bootbus flash: Setting flash for %luMB flash at 0x%08lx\n", flash_map.size >> 20, flash_map.phys);
+		simple_map_init(&flash_map);
+		mymtd = do_map_probe("cfi_probe", &flash_map);
+		if (mymtd) {
+			mymtd->owner = THIS_MODULE;
+
+#ifdef CONFIG_MTD_PARTITIONS
+			nr_parts =
+				parse_mtd_partitions(mymtd, part_probe_types,
+						     &parts, 0);
+			if (nr_parts > 0) {
+				add_mtd_partitions(mymtd, parts, nr_parts);
+			} else
+				add_mtd_device(mymtd);
+#else
+			add_mtd_device(mymtd);
+#endif
+		} else
+			printk("Failed to register MTD device for flash\n");
+	}
+	return 0;
+}
+
+late_initcall(flash_init);
diff --git a/arch/mips/cavium-octeon/hal.c b/arch/mips/cavium-octeon/hal.c
new file mode 100644
index 0000000..322d7f4
--- /dev/null
+++ b/arch/mips/cavium-octeon/hal.c
@@ -0,0 +1,532 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2004 - 2007 Cavium Networks
+ */
+#include <linux/kernel.h>
+#include <linux/string.h>
+#include <linux/module.h>
+#include <asm/time.h>
+#include <octeon-app-init.h>
+
+#ifndef CONFIG_CAVIUM_RESERVE32
+#define CONFIG_CAVIUM_RESERVE32 0
+#endif
+
+#include "hal.h"
+
+#include "cvmx-bootmem.h"
+#include "cvmx-app-init.h"
+#include "cvmx-sysinfo.h"
+#include "cvmx-l2c.h"
+#include "cvmx-spinlock.h"
+
+/* Set to non-zero, so it is not in .bss section and is not zeroed */
+volatile octeon_boot_descriptor_t *octeon_boot_desc_ptr = (void *) 0xEADBEEFULL;
+cvmx_bootinfo_t *octeon_bootinfo;
+
+/* This must not be static since inline functions access it */
+spinlock_t octeon_led_lock;
+
+#if CONFIG_CAVIUM_RESERVE32
+uint64_t octeon_reserve32_memory = 0;
+#endif
+
+
+/**
+ * Write to the LCD display connected to the bootbus. This display
+ * exists on most Cavium evaluation boards. If it doesn't exist, then
+ * this function doesn't do anything.
+ *
+ * @param s      String to write
+ */
+void octeon_write_lcd(const char *s)
+{
+	if (octeon_bootinfo->led_display_base_addr) {
+		volatile char *lcd_address =
+			cvmx_phys_to_ptr(octeon_bootinfo->
+					 led_display_base_addr);
+		int i;
+		for (i = 0; i < 8; i++) {
+			if (*s)
+				lcd_address[i] = *s++;
+			else
+				lcd_address[i] = ' ';
+		}
+	}
+}
+
+
+/**
+ * Check the hardware BIST results for a CPU
+ */
+void octeon_check_cpu_bist(void)
+{
+	const int coreid = cvmx_get_core_num();
+	unsigned long long mask;
+	unsigned long long bist_val;
+
+	/* Check BIST results for COP0 registers */
+	mask = 0x1f00000000ull;
+	bist_val = __read_64bit_c0_register($27, 0);
+	if (bist_val & mask)
+		printk("Core%d BIST Failure: CacheErr(icache) = 0x%llx\n",
+		       coreid, bist_val);
+
+	bist_val = __read_64bit_c0_register($27, 1);
+	if (bist_val & 1)
+		printk("Core%d L1 Dcache parity error: CacheErr(dcache) = 0x%llx\n", coreid, bist_val);
+
+	mask = 0xfc00000000000000ull;
+	bist_val = __read_64bit_c0_register($11, 7);
+	if (bist_val & mask)
+		printk("Core%d BIST Failure: COP0_CVM_MEM_CTL = 0x%llx\n",
+		       coreid, bist_val);
+
+	__write_64bit_c0_register($27, 1, 0);
+}
+
+
+/**
+ * Return non zero of we are currently running in the Octeon simulator
+ *
+ * @return
+ */
+int octeon_is_simulation(void)
+{
+	return (octeon_bootinfo->board_type == CVMX_BOARD_TYPE_SIM);
+}
+
+
+/**
+ * Return true if Octeon is in PCI Host mode. This means
+ * Linux can control the PCI bus.
+ *
+ * @return Non zero if Octeon in host mode
+ */
+int octeon_is_pci_host(void)
+{
+#ifdef CONFIG_PCI
+	return (octeon_bootinfo->
+		config_flags & CVMX_BOOTINFO_CFG_FLAG_PCI_HOST);
+#else
+	return 0;
+#endif
+}
+
+
+/**
+ * Get the clock rate of Octeon
+ *
+ * @return Clock rate in HZ
+ */
+uint64_t octeon_get_clock_rate(void)
+{
+	if (octeon_is_simulation())
+		octeon_bootinfo->eclock_hz = 6000000;
+	return octeon_bootinfo->eclock_hz;
+}
+
+
+/**
+ * Return the board name as a constant string
+ *
+ * @return board name
+ */
+const char *octeon_board_type_string(void)
+{
+	static char name[80];
+	sprintf(name, "%s (%s)",
+		cvmx_board_type_to_string(octeon_bootinfo->board_type),
+		octeon_model_get_string(read_c0_prid()));
+	return name;
+}
+
+
+/**
+ * Return the mapping of PCI device number to IRQ line. Each
+ * character in the return string represents the interrupt
+ * line for the device at that position. Device 1 maps to the
+ * first character, etc. The characters A-D are used for PCI
+ * interrupts.
+ *
+ * @return PCI interrupt mapping
+ */
+const char *octeon_get_pci_interrupts(void)
+{
+	/* Returning an empty string causes the interrupts to be routed based
+	   on the PCI specification. From the PCI spec:
+
+	   INTA# of Device Number 0 is connected to IRQW on the system board.
+	   (Device Number has no significance regarding being located on the
+	   system board or in a connector.) INTA# of Device Number 1 is
+	   connected to IRQX on the system board. INTA# of Device Number 2 is
+	   connected to IRQY on the system board. INTA# of Device Number 3 is
+	   connected to IRQZ on the system board. The table below describes how
+	   each agent's INTx# lines are connected to the system board interrupt
+	   lines. The following equation can be used to determine to which
+	   INTx# signal on the system board a given device's INTx# line(s) is
+	   connected.
+
+	   MB = (D + I) MOD 4 MB = System board Interrupt (IRQW = 0, IRQX = 1,
+	   IRQY = 2, and IRQZ = 3) D = Device Number I = Interrupt Number
+	   (INTA# = 0, INTB# = 1, INTC# = 2, and INTD# = 3) */
+	switch (octeon_bootinfo->board_type) {	/* Device ID
+		   1111111111222222222233 *//* 01234567890123456789012345678901 */
+		// case CVMX_BOARD_TYPE_NAO38: return
+		// "AAAAAADBAAAAAAAAAAAAAAAAAAAAAAAA";
+	case CVMX_BOARD_TYPE_NAO38:
+		return "AAAAADABAAAAAAAAAAAAAAAAAAAAAAAA";	/* This is
+								   really the
+								   NAC38 */
+	case CVMX_BOARD_TYPE_THUNDER:
+		return "";
+	case CVMX_BOARD_TYPE_EBH3000:
+		return "";
+	case CVMX_BOARD_TYPE_EBH3100:
+	case CVMX_BOARD_TYPE_CN3010_EVB_HS5:
+	case CVMX_BOARD_TYPE_CN3005_EVB_HS5:
+		return "AAABAAAAAAAAAAAAAAAAAAAAAAAAAAAA";
+	default:
+		return "";
+	}
+}
+
+
+/**
+ * Return the interrupt line for the i8259 in the southbridge
+ *
+ * @return
+ */
+int octeon_get_southbridge_interrupt(void)
+{
+	switch (octeon_bootinfo->board_type) {
+	case CVMX_BOARD_TYPE_EBH3000:
+		return 47;	/* PCI INDD */
+	case CVMX_BOARD_TYPE_NAC38:
+		return 39;	/* GPIO 15 */
+	default:
+		return 0;	/* No southbridge */
+	}
+}
+
+
+/**
+ * Get the coremask Linux was booted on.
+ *
+ * @return Core mask
+ */
+int octeon_get_boot_coremask(void)
+{
+	return octeon_boot_desc_ptr->core_mask;
+}
+
+
+/**
+ * Return the number of arguments we got from the bootloader
+ *
+ * @return argc
+ */
+int octeon_get_boot_num_arguments(void)
+{
+	return octeon_boot_desc_ptr->argc;
+}
+
+
+/**
+ * Return the console uart passed by the bootloader
+ *
+ * @return uart   (0 or 1)
+ */
+int octeon_get_boot_uart(void)
+{
+#if OCTEON_APP_INIT_H_VERSION >= 1	/* The UART1 flag is new */
+	return !!(octeon_boot_desc_ptr->flags & OCTEON_BL_FLAG_CONSOLE_UART1);
+#else
+	return 0;
+#endif
+}
+
+/**
+ * Return the debug flag passed by the bootloader
+ *
+ * @return debug flag (0 or 1)
+ */
+int octeon_get_boot_debug_flag(void)
+{
+	return !!(octeon_boot_desc_ptr->flags & OCTEON_BL_FLAG_DEBUG);
+}
+
+/**
+ * Get an argument from the bootloader
+ *
+ * @param arg    argument to get
+ * @return argument
+ */
+const char *octeon_get_boot_argument(int arg)
+{
+	return cvmx_phys_to_ptr(octeon_boot_desc_ptr->argv[arg]);
+}
+
+
+/**
+ * Called very early in the initial C code to initialize the Octeon
+ * HAL layer.
+ */
+void octeon_hal_init(void)
+{
+	cvmx_sysinfo_t *sysinfo;
+
+	/* Make sure we got the boot descriptor block */
+	if ((octeon_boot_desc_ptr == (void *) 0xEADBEEFULL))
+		panic("Boot descriptor block wasn't passed properly\n");
+
+	octeon_bootinfo =
+		cvmx_phys_to_ptr(octeon_boot_desc_ptr->cvmx_desc_vaddr);
+	cvmx_bootmem_init(cvmx_phys_to_ptr(octeon_bootinfo->phy_mem_desc_addr));
+
+	spin_lock_init(&octeon_led_lock);
+	/* Only enable the LED controller if we're running on a CN38XX, CN58XX,
+	   or CN56XX. The CN30XX and CN31XX don't have an LED controller */
+	if (!octeon_is_simulation() &&
+	    octeon_has_feature(OCTEON_FEATURE_LED_CONTROLLER)) {
+		cvmx_write_csr(CVMX_LED_EN, 0);
+		cvmx_write_csr(CVMX_LED_PRT, 0);
+		cvmx_write_csr(CVMX_LED_DBG, 0);
+		cvmx_write_csr(CVMX_LED_PRT_FMT, 0);
+		cvmx_write_csr(CVMX_LED_UDD_CNTX(0), 32);
+		cvmx_write_csr(CVMX_LED_UDD_CNTX(1), 32);
+		cvmx_write_csr(CVMX_LED_UDD_DATX(0), 0);
+		cvmx_write_csr(CVMX_LED_UDD_DATX(1), 0);
+		cvmx_write_csr(CVMX_LED_EN, 1);
+	}
+#if CONFIG_CAVIUM_RESERVE32
+	{
+		int64_t addr = -1;
+		/* We need to temporarily allocate all memory in the reserve32
+		   region. This makes sure the kernel doesn't allocate this
+		   memory when it is getting memory fro mthe bootloader. Later,
+		   after the memory allocations are complete, the reserve32
+		   will be freed in the call to octeon_hal_setup_reserved32() */
+#ifdef CONFIG_CAVIUM_RESERVE32_USE_WIRED_TLB
+		if (CONFIG_CAVIUM_RESERVE32 & 0x1ff)
+			printk("CAVIUM_RESERVE32 isn't a multiple of 512MB. This is required if CAVIUM_RESERVE32_USE_WIRED_TLB is set\n");
+		else
+			addr = cvmx_bootmem_phy_named_block_alloc
+				(CONFIG_CAVIUM_RESERVE32 << 20, 0, 0, 512 << 20,
+				 "CAVIUM_RESERVE32", 0);
+#else
+		/* Allocate memory for RESERVED32 aligned on 2MB boundary. This
+		   is in case we later use hugetlb entries with it */
+		addr = cvmx_bootmem_phy_named_block_alloc
+			(CONFIG_CAVIUM_RESERVE32 << 20, 0, 0, 2 << 20,
+			 "CAVIUM_RESERVE32", 0);
+#endif
+		if (addr < 0)
+			printk("Failed to allocate CAVIUM_RESERVE32 memory area\n");
+		else
+			octeon_reserve32_memory = addr;
+
+	}
+#endif
+
+#ifdef CONFIG_CAVIUM_OCTEON_LOCK_L2
+	if (cvmx_read_csr(CVMX_L2D_FUS3) & (3ull << 34)) {
+		printk("Skipping L2 locking due to reduced L2 cache size\n");
+	} else {
+		extern asmlinkage void handle_int(void);
+		extern asmlinkage void plat_irq_dispatch(void);
+		uint32_t ebase = read_c0_ebase() & 0x3ffff000;
+#ifdef CONFIG_CAVIUM_OCTEON_LOCK_L2_TLB
+		cvmx_l2c_lock_mem_region(ebase, 0x100);	/* TLB refill */
+#endif
+#ifdef CONFIG_CAVIUM_OCTEON_LOCK_L2_EXCEPTION
+		cvmx_l2c_lock_mem_region(ebase + 0x180, 0x80);	/* General
+								   exception */
+#endif
+#ifdef CONFIG_CAVIUM_OCTEON_LOCK_L2_LOW_LEVEL_INTERRUPT
+		cvmx_l2c_lock_mem_region(ebase + 0x200, 0x80);	/* Interrupt
+								   handler */
+#endif
+#ifdef CONFIG_CAVIUM_OCTEON_LOCK_L2_INTERRUPT
+		cvmx_l2c_lock_mem_region(0x1fffffff & (long) handle_int, 0x100);
+		cvmx_l2c_lock_mem_region(0x1fffffff & (long) plat_irq_dispatch,
+					 0x80);
+#endif
+#ifdef CONFIG_CAVIUM_OCTEON_LOCK_L2_MEMCPY
+		cvmx_l2c_lock_mem_region(0x1fffffff & (long) memcpy, 0x480);
+#endif
+	}
+#endif
+
+	sysinfo = cvmx_sysinfo_get();
+	memset(sysinfo, 0, sizeof(*sysinfo));
+	sysinfo->system_dram_size = octeon_bootinfo->dram_size << 20;
+	sysinfo->phy_mem_desc_ptr =
+		cvmx_phys_to_ptr(octeon_bootinfo->phy_mem_desc_addr);
+	sysinfo->core_mask = octeon_bootinfo->core_mask;
+	sysinfo->exception_base_addr = octeon_bootinfo->exception_base_addr;
+	sysinfo->cpu_clock_hz = octeon_bootinfo->eclock_hz;
+	sysinfo->dram_data_rate_hz = octeon_bootinfo->dclock_hz * 2;
+	sysinfo->board_type = octeon_bootinfo->board_type;
+	sysinfo->board_rev_major = octeon_bootinfo->board_rev_major;
+	sysinfo->board_rev_minor = octeon_bootinfo->board_rev_minor;
+	memcpy(sysinfo->mac_addr_base, octeon_bootinfo->mac_addr_base,
+	       sizeof(sysinfo->mac_addr_base));
+	sysinfo->mac_addr_count = octeon_bootinfo->mac_addr_count;
+	memcpy(sysinfo->board_serial_number,
+	       octeon_bootinfo->board_serial_number,
+	       sizeof(sysinfo->board_serial_number));
+	sysinfo->compact_flash_common_base_addr =
+		octeon_bootinfo->compact_flash_common_base_addr;
+	sysinfo->compact_flash_attribute_base_addr =
+		octeon_bootinfo->compact_flash_attribute_base_addr;
+	sysinfo->led_display_base_addr = octeon_bootinfo->led_display_base_addr;
+	sysinfo->dfa_ref_clock_hz = octeon_bootinfo->dfa_ref_clock_hz;
+	sysinfo->bootloader_config_flags = octeon_bootinfo->config_flags;
+}
+
+
+#ifdef CONFIG_CAVIUM_RESERVE32_USE_WIRED_TLB
+/**
+ * Called on every core to setup the wired tlb entry needed
+ * if CONFIG_CAVIUM_RESERVE32_USE_WIRED_TLB is set.
+ *
+ * @param unused
+ */
+static void octeon_hal_setup_per_cpu_reserved32(void *unused)
+{
+	/* The config has selected to wire the reserve32 memory for all
+	   userspace applications. We need to put a wired TLB entry in for each
+	   512MB of reserve32 memory. We only handle double 256MB pages here,
+	   so reserve32 must be multiple of 512MB */
+	extern void add_wired_entry(unsigned long entrylo0,
+				    unsigned long entrylo1,
+				    unsigned long entryhi,
+				    unsigned long pagemask);
+	uint32_t size = CONFIG_CAVIUM_RESERVE32;
+	uint32_t entrylo0 =
+		0x7 | ((octeon_reserve32_memory & ((1ul << 40) - 1)) >> 6);
+	uint32_t entrylo1 = entrylo0 + (256 << 14);
+	uint32_t entryhi = (0x80000000UL - (CONFIG_CAVIUM_RESERVE32 << 20));
+	while (size >= 512) {
+		// printk("CPU%d: Adding double wired TLB entry for 0x%lx\n",
+		// smp_processor_id(), entryhi);
+		add_wired_entry(entrylo0, entrylo1, entryhi, PM_256M);
+		entrylo0 += 512 << 14;
+		entrylo1 += 512 << 14;
+		entryhi += 512 << 20;
+		size -= 512;
+	}
+}
+#endif				/* CONFIG_CAVIUM_RESERVE32_USE_WIRED_TLB */
+
+/**
+ * Called to release the named block which was used to made sure
+ * that nobody used the memory for something else during
+ * init. Now we'll free it so userspace apps can use this
+ * memory region with bootmem_alloc.
+ *
+ * This function is called only once from prom_free_prom_memory().
+ */
+void octeon_hal_setup_reserved32(void)
+{
+#if CONFIG_CAVIUM_RESERVE32
+
+#ifdef CONFIG_CAVIUM_RESERVE32_USE_WIRED_TLB
+	on_each_cpu(octeon_hal_setup_per_cpu_reserved32, NULL, 0, 1);
+#endif				/* CONFIG_CAVIUM_RESERVE32 */
+
+	if (octeon_reserve32_memory)
+		cvmx_bootmem_free_named("CAVIUM_RESERVE32");
+
+#endif				/* CONFIG_CAVIUM_RESERVE32 */
+}
+
+
+/**
+ * Poweroff the Octeon board if possible.
+ */
+void octeon_poweroff(void)
+{
+	switch (octeon_bootinfo->board_type) {
+	case CVMX_BOARD_TYPE_NAO38:
+		/* Driving a 1 to GPIO 12 shuts off this board */
+		cvmx_write_csr(CVMX_GPIO_BIT_CFGX(12), 1);
+		cvmx_write_csr(CVMX_GPIO_TX_SET, 0x1000);
+		break;
+	default:
+		octeon_write_lcd("PowerOff");
+		break;
+	}
+}
+
+
+/**
+ * Enable access to Octeon's COP2 crypto hardware for kernel use.
+ * Wrap any crypto operations in calls to
+ * octeon_crypto_enable/disable in order to make sure the state of
+ * COP2 isn't corrupted if userspace is also performing hardware
+ * crypto operations. Allocate the state parameter on the stack.
+ *
+ * @param state  State structure to store current COP2 state in
+ *
+ * @return Flags to be passed to octeon_crypto_disable()
+ */
+unsigned long octeon_crypto_enable(struct octeon_cop2_state *state)
+{
+	extern void octeon_cop2_save(struct octeon_cop2_state *);
+	int status;
+	unsigned long flags;
+
+	local_irq_save(flags);
+	status = read_c0_status();
+	write_c0_status(status | ST0_CU2);
+	if (KSTK_STATUS(current) & ST0_CU2)
+	{
+		octeon_cop2_save(&(current->thread.cp2));
+		KSTK_STATUS(current) &= ~ST0_CU2;
+		status &= ~ST0_CU2;
+	} else if (status & ST0_CU2)
+		octeon_cop2_save(state);
+	local_irq_restore(flags);
+	return status & ST0_CU2;
+}
+EXPORT_SYMBOL(octeon_crypto_enable);
+
+
+/**
+ * Disable access to Octeon's COP2 crypto hardware in the kernel.
+ * This must be called after an octeon_crypto_enable() before any
+ * context switch or return to userspace.
+ *
+ * @param state  COP2 state to restore
+ * @param flags  Return value from octeon_crypto_enable()
+ */
+void octeon_crypto_disable(struct octeon_cop2_state *state, unsigned long crypto_flags)
+{
+	extern void octeon_cop2_restore(struct octeon_cop2_state *);
+	unsigned long flags;
+
+	local_irq_save(flags);
+	if (crypto_flags & ST0_CU2)
+		octeon_cop2_restore(state);
+	else
+                write_c0_status(read_c0_status() & ~ST0_CU2);
+	local_irq_restore(flags);
+}
+EXPORT_SYMBOL(octeon_crypto_disable);
+
+
+
+/* Misc exports */
+EXPORT_SYMBOL(octeon_is_simulation);
+EXPORT_SYMBOL(octeon_bootinfo);
+EXPORT_SYMBOL(mips_hpt_frequency);
+#if CONFIG_CAVIUM_RESERVE32
+EXPORT_SYMBOL(octeon_reserve32_memory);
+#endif
+
+EXPORT_SYMBOL(octeon_get_clock_rate);
diff --git a/arch/mips/cavium-octeon/hal.h b/arch/mips/cavium-octeon/hal.h
new file mode 100644
index 0000000..898d3bd
--- /dev/null
+++ b/arch/mips/cavium-octeon/hal.h
@@ -0,0 +1,138 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2004-2007 Cavium Networks
+ */
+#ifndef __CAVIUM_OCTEON_HAL_H
+#define __CAVIUM_OCTEON_HAL_H
+
+#include "octeon-hal-read-write.h"
+
+#ifndef __BYTE_ORDER
+#  error "__BYTE_ORDER not set"
+#endif
+#ifndef __BIG_ENDIAN
+#  error "__BIG_ENDIAN not set"
+#endif
+
+extern uint64_t octeon_bootmem_alloc_range_phys(uint64_t size,
+						uint64_t alignment,
+						uint64_t min_addr,
+						uint64_t max_addr,
+						int do_locking);
+extern void *octeon_bootmem_alloc(uint64_t size, uint64_t alignment,
+				  int do_locking);
+extern void *octeon_bootmem_alloc_range(uint64_t size, uint64_t alignment,
+					uint64_t min_addr, uint64_t max_addr,
+					int do_locking);
+extern void *octeon_bootmem_alloc_named(uint64_t size, uint64_t alignment,
+					char *name);
+extern void *octeon_bootmem_alloc_named_range(uint64_t size, uint64_t min_addr,
+					      uint64_t max_addr, uint64_t align,
+					      char *name);
+extern void *octeon_bootmem_alloc_named_address(uint64_t size, uint64_t address,
+						char *name);
+extern int octeon_bootmem_free_named(char *name);
+extern void octeon_bootmem_lock(void);
+extern void octeon_bootmem_unlock(void);
+
+extern int octeon_is_simulation(void);
+extern int octeon_is_pci_host(void);
+extern uint64_t octeon_get_clock_rate(void);
+extern const char *octeon_board_type_string(void);
+extern const char *octeon_get_pci_interrupts(void);
+extern int octeon_get_southbridge_interrupt(void);
+extern int octeon_get_boot_coremask(void);
+extern int octeon_get_boot_num_arguments(void);
+extern const char *octeon_get_boot_argument(int arg);
+extern void octeon_hal_setup_reserved32(void);
+extern unsigned long octeon_crypto_enable(struct octeon_cop2_state *state);
+extern void octeon_crypto_disable(struct octeon_cop2_state *state, unsigned long flags);
+
+typedef union {
+	uint64_t u64;
+	struct {
+		uint64_t tlbbist:1;	    /**< RO 1 = BIST fail, 0 = BIST pass */
+		uint64_t l1cbist:1;	    /**< RO 1 = BIST fail, 0 = BIST pass */
+		uint64_t l1dbist:1;	    /**< RO 1 = BIST fail, 0 = BIST pass */
+		uint64_t dcmbist:1;	    /**< RO 1 = BIST fail, 0 = BIST pass */
+		uint64_t ptgbist:1;	    /**< RO 1 = BIST fail, 0 = BIST pass */
+		uint64_t wbfbist:1;	    /**< RO 1 = BIST fail, 0 = BIST pass */
+		uint64_t reserved:22;	    /**< Reserved */
+		uint64_t dismarkwblongto:1; /**< R/W If set, marked write-buffer entries time out the same as
+                                                as other entries; if clear, marked write-buffer entries use the
+                                                maximum timeout. */
+		uint64_t dismrgclrwbto:1;   /**< R/W If set, a merged store does not clear the write-buffer entry
+                                                timeout state. */
+		uint64_t iobdmascrmsb:2;    /**< R/W Two bits that are the MSBs of the resultant CVMSEG LM word
+                                                location for an IOBDMA. The other 8 bits come from the SCRADDR
+                                                field of the IOBDMA. */
+		uint64_t syncwsmarked:1;    /**< R/W If set, SYNCWS and SYNCS only order marked stores; if clear,
+                                                SYNCWS and SYNCS only order unmarked stores. SYNCWSMARKED has no
+                                                effect when DISSYNCWS is set. */
+		uint64_t dissyncws:1;	    /**< R/W If set, SYNCWS acts as SYNCW and SYNCS acts as SYNC. */
+		uint64_t diswbfst:1;	    /**< R/W If set, no stall happens on write buffer full. */
+		uint64_t xkmemenas:1;	    /**< R/W If set (and SX set), supervisor-level loads/stores can use
+                                                XKPHYS addresses with VA<48>==0 */
+		uint64_t xkmemenau:1;	    /**< R/W If set (and UX set), user-level loads/stores can use XKPHYS
+                                                addresses with VA<48>==0 */
+		uint64_t xkioenas:1;	    /**< R/W If set (and SX set), supervisor-level loads/stores can use
+                                                XKPHYS addresses with VA<48>==1 */
+		uint64_t xkioenau:1;	    /**< R/W If set (and UX set), user-level loads/stores can use XKPHYS
+                                                addresses with VA<48>==1 */
+		uint64_t allsyncw:1;	    /**< R/W If set, all stores act as SYNCW (NOMERGE must be set when
+                                                this is set) RW, reset to 0. */
+		uint64_t nomerge:1;	    /**< R/W If set, no stores merge, and all stores reach the coherent
+                                                bus in order. */
+		uint64_t didtto:2;	    /**< R/W Selects the bit in the counter used for DID time-outs
+                                                0 = 231, 1 = 230, 2 = 229, 3 = 214. Actual time-out is between
+                                                1 and 2 this interval. For example, with DIDTTO=3, expiration
+                                                interval is between 16K and 32K. */
+		uint64_t csrckalwys:1;	    /**< R/W If set, the (mem) CSR clock never turns off. */
+		uint64_t mclkalwys:1;	    /**< R/W If set, mclk never turns off. */
+		uint64_t wbfltime:3;	    /**< R/W Selects the bit in the counter used for write buffer flush
+                                                time-outs (WBFLT+11) is the bit position in an internal counter
+                                                used to determine expiration. The write buffer expires between
+                                                1 and 2 this interval. For example, with WBFLT = 0, a write
+                                                buffer expires between 2K and 4K cycles after the write buffer
+                                                entry is allocated. */
+		uint64_t istrnol2:1;	    /**< R/W If set, do not put Istream in the L2 cache. */
+		uint64_t wbthresh:4;	    /**< R/W The write buffer threshold. */
+		uint64_t reserved2:2;	    /**< Reserved */
+		uint64_t cvmsegenak:1;	    /**< R/W If set, CVMSEG is available for loads/stores in kernel/debug mode. */
+		uint64_t cvmsegenas:1;	    /**< R/W If set, CVMSEG is available for loads/stores in supervisor mode. */
+		uint64_t cvmsegenau:1;	    /**< R/W If set, CVMSEG is available for loads/stores in user mode. */
+		uint64_t lmemsz:6;	    /**< R/W Size of local memory in cache blocks, 54 (6912 bytes) is max legal value. */
+	} s;
+} octeon_cvmemctl_t;
+
+static inline void octeon_led_write(int bank, uint32_t data)
+{
+	cvmx_write_csr(CVMX_LED_UDD_DATX(bank), data);
+}
+
+static inline uint32_t octeon_led_read(int bank)
+{
+	return cvmx_read_csr(CVMX_LED_UDD_DATX(bank));
+}
+
+static inline void octeon_led_set(int bank, int bit)
+{
+	cvmx_write_csr(CVMX_LED_UDD_DAT_SETX(bank), 1 << bit);
+}
+
+static inline void octeon_led_clear(int bank, int bit)
+{
+	cvmx_write_csr(CVMX_LED_UDD_DAT_CLRX(bank), 1 << bit);
+}
+
+extern void octeon_write_lcd(const char *s);
+extern void octeon_check_cpu_bist(void);
+extern void octeon_hal_init(void);
+extern int octeon_get_boot_uart(void);
+extern int octeon_get_boot_debug_flag(void);
+extern void octeon_poweroff(void);
+
+#endif
diff --git a/arch/mips/cavium-octeon/i8259.c b/arch/mips/cavium-octeon/i8259.c
new file mode 100644
index 0000000..a251771
--- /dev/null
+++ b/arch/mips/cavium-octeon/i8259.c
@@ -0,0 +1,178 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2006-2007 Cavium Networks
+ */
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/pci.h>
+#include <linux/interrupt.h>
+#include "hal.h"
+
+#define SLAVE           (0xa0 - 0x20)
+
+/* Initialization Command Word 1 (ICW1 address 0x20 or 0xa0) */
+/* 7:5 Interrupt Vector Addresses for MCS-80/85 Mode. */
+#define ICW1_ADDRESS    0x20
+#define ICW1            0x10	// 4 Must be set to 1 for ICW1
+#define ICW1_LEVEL_TRIG (1<<3)	// 3 1 Level Triggered Interrupts, 0 Edge
+				// Triggered Interrupts
+#define ICW1_INTERVAL4  (1<<2)	// 2 1 Call Address Interval of 4, 0 Call
+				// Address Interval of 8
+#define ICW1_SINGLE_PIC (1<<1)	// 1 1 Single PIC, 0 Cascaded PICs
+#define ICW1_NEED_ICW4  (1<<0)	// 0 1 Will be Sending ICW4, 0 Don't need ICW4
+
+/* Initialization Command Word 2 (ICW2 address 0x21 or 0xa1) */
+#define ICW2_ADDRESS    0x21
+/* Bit 8086/8080 Mode MCS 80/85 Mode 7 I7 A15 6 I6 A14 5 I5 A13 4 I4 A12 3 I3
+   A11 2 - A10 1 - A9 0 - A8 */
+
+/* Initialization Command Word 3 (ICW3 address 0x21 or 0xa1) For the master,
+   this is a bitfield saying which line is hooked to a slave. For a slave, this
+   is the slave's ID, the line it is hooked to */
+#define ICW3_ADDRESS    0x21
+
+/* Initialization Command Word 4 (ICW4 address 0x21 or 0xa1) */
+/* Bits 7-5 are reserved */
+#define ICW4_ADDRESS        0x21
+#define ICW4_FULLY_NESTED   (1<<4)	// 4 1 Special Fully Nested Mode, 0 Not
+					// Special Fully Nested Mode
+#define ICW4_BUFFERED       (3<<2)	// 3 1 Buffered Mode, 0 Unbuffered
+#define ICW4_MASTER         (2<<2)	// 2 1 Master, 0 Slave
+#define ICW4_AUTO_EOI       (1<<1)	// 1 1 Auto EOI, 0 Normal EOI
+#define ICW4_8086           (1<<0)	// 0 1 8086/8080 Mode, 0 MCS-80/85
+
+/* Operation Control Word 1 (OCW1 address 0x21 or 0xa1) This is a bitmask for
+   each interrupt */
+#define OCW1_ADDRESS    0x21
+
+/* Operation Control Word 2 (OCW2 address 0x20 or 0xa0) */
+#define OCW2_ADDRESS    0x20
+#define OCW2            0x00	// Bits 4:3 must be zero
+#define OCW2_ROTATE_AUTO_EOI_CLEAR  (0<<5)	// 7:5 000 Rotate in Auto EOI
+						// Mode (Clear)
+#define OCW2_NON_SPECIFIC_EOI       (1<<5)	// 001 Non Specific EOI
+#define OCW2_NOP                    (2<<5)	// 010 NOP
+#define OCW2_SPECIFIC_EOI           (3<<5)	// 011 Specific EOI
+#define OCW2_ROTATE_AUTO_EOI_SET    (4<<5)	// 100 Rotate in Auto EOI Mode
+						// (Set)
+#define OCW2_ROTATE_NON_SPECIFIC_EOI (5<<5)	// 101 Rotate on Non-Specific
+						// EOI
+#define OCW2_SET_PRIORITY           (6<<5)	// 110 Set Priority Command
+						// (Use Bits 2:0)
+#define OCW2_ROTATE_SPECIFIC_EOI    (7<<5)	// 111 Rotate on Specific EOI
+						// (Use Bits 2:0)
+
+/* Operation Control Word 3 (OCW3 address 0x20 or 0xa0) */
+/* Bit 7 Must be set to 0 */
+#define OCW3_ADDRESS    0x20
+#define OCW3            0x08	// 4:3 Must be set to 01
+#define OCW3_RESET_SPECIAL_MASK (2<<5)	// 6:5 00 Reserved, 01 Reserved, 10
+					// Reset Special Mask
+#define OCW3_SET_SPECIAL_MASK   (3<<5)	// 11 Set Special Mask
+#define OCW3_POLL               (1<<2)	// 2 1 Poll Command, 0 No Poll Command
+#define OCW3_READ_IRR           (2<<0)	// 1:0 00 Reserved, 01 Reserved, 10
+					// Next Read Returns Interrupt Request
+					// Register
+#define OCW3_READ_ISR           (3<<0)	// 11 Next Read Returns In-Service
+					// Register
+
+
+static irqreturn_t octeon_i8259_interrupt(int cpl, void *dev_id)
+{
+	u8 master_isr;
+	u8 slave_isr;
+
+	outb(OCW3 | OCW3_POLL, OCW3_ADDRESS);
+	master_isr = inb(OCW3_ADDRESS);
+	if (master_isr & 0x80) {	/* Top bit is set if the master
+					   requested the interrupt */
+		if ((master_isr & 0x7) == 2) {
+			outb(OCW3 | OCW3_POLL, OCW3_ADDRESS + SLAVE);
+			slave_isr = inb(OCW3_ADDRESS + SLAVE);
+			if (slave_isr & 0x80) {	/* Top bit is set if the slave
+						   requested the interrupt */
+				int irq = (slave_isr & 7) + 8 + 80;
+
+				// printk("8259: Interrupt %d from slave\n",
+				// irq);
+				if (irq_desc[irq].action)
+					do_IRQ(irq);
+
+				/* Ack the slave */
+				outb(OCW2 | OCW2_SPECIFIC_EOI | (slave_isr & 7),
+				     OCW2_ADDRESS + SLAVE);
+			} else
+				printk("8259: Spurious interrupt from master for slave\n");
+		} else {
+			int irq = (master_isr & 7) + 80;
+			// printk("8259: Interrupt %d from master\n", irq);
+			if (irq_desc[irq].action)
+				do_IRQ(irq);
+		}
+
+		/* Ack the master */
+		outb(OCW2 | OCW2_SPECIFIC_EOI | (master_isr & 7), OCW2_ADDRESS);
+
+		return IRQ_HANDLED;
+	} else {
+		printk("8259: Spurious interrupt from master\n");
+		return IRQ_NONE;
+	}
+}
+
+void octeon_i8259_setup(int irq_line)
+{
+	printk("8259: Initializing\n");
+
+	/* Setup the Master 8259 */
+	outb(ICW1 | ICW1_NEED_ICW4, ICW1_ADDRESS);	/* Begin the init
+							   sequence */
+	outb(0, ICW2_ADDRESS);	/* Master base address is zero, interrupts 0-7 */
+	outb(1 << 2, ICW3_ADDRESS);	/* Slave is connected to line 2 */
+	outb(ICW4_FULLY_NESTED | ICW4_MASTER | ICW4_BUFFERED | ICW4_8086, ICW4_ADDRESS);	/* Set
+												   the
+												   mode
+												   to
+												   buffered
+												   with
+												   edge
+												   triggering
+												 */
+	outb(OCW3 | OCW3_READ_ISR, OCW3_ADDRESS);	/* Read ISR */
+
+	/* Setup the Slave 8259 */
+	outb(ICW1 | ICW1_NEED_ICW4, ICW1_ADDRESS + SLAVE);	/* Begin the
+								   init
+								   sequence */
+	outb(8, ICW2_ADDRESS + SLAVE);	/* Slave base address is 8, interrupts
+					   8-15 */
+	outb(2, ICW3_ADDRESS + SLAVE);	/* Slave is connected to line 2 */
+	outb(ICW4_BUFFERED | ICW4_8086, ICW4_ADDRESS + SLAVE);	/* Set the mode
+								   to buffered
+								   with edge
+								   triggering */
+	outb(OCW3 | OCW3_READ_ISR, OCW3_ADDRESS + SLAVE);	/* Read ISR */
+
+	/* Set interrupt mask to disable all interrupts */
+	outb(0xfb, OCW1_ADDRESS);
+	outb(0xff, OCW1_ADDRESS + SLAVE);
+
+	/* Setup the GPIO pin if the interrupt is hooked to it */
+	if ((irq_line >= 24) && (irq_line <= 39)) {
+		printk("8259: Setting GPIO %d for the interrupt\n",
+		       irq_line - 24);
+		cvmx_write_csr(CVMX_GPIO_BIT_CFGX(irq_line - 24), 0x114);
+		request_irq(irq_line, octeon_i8259_interrupt, SA_SHIRQ, "8259",
+			    octeon_i8259_interrupt);
+	} else if ((irq_line >= 44) && (irq_line <= 47)) {
+		printk("8259: Using PCI INT-%c\n", irq_line - 44 + 'A');
+		request_irq(irq_line, octeon_i8259_interrupt, SA_SHIRQ, "8259",
+			    octeon_i8259_interrupt);
+	} else {
+		panic("8259: Don't know how to setup the interrupt IRQ %d\n",
+		      irq_line);
+	}
+}
diff --git a/arch/mips/cavium-octeon/irq.c b/arch/mips/cavium-octeon/irq.c
new file mode 100644
index 0000000..279eaee
--- /dev/null
+++ b/arch/mips/cavium-octeon/irq.c
@@ -0,0 +1,72 @@
+#include "linux/irq.h"
+#include "linux/hardirq.h"
+#include "linux/kernel_stat.h"
+#include "hal.h"
+
+#ifdef CONFIG_SMP
+static void mailbox_interrupt(void)
+{
+	const int coreid = cvmx_get_core_num();
+	uint64_t action;
+
+	/* Count the IRQs manually since we skip the normal process */
+	kstat_this_cpu.irqs[3]++;
+
+	/* Load the mailbox register to figure out what we're supposed to do */
+	action = cvmx_read_csr(CVMX_CIU_MBOX_CLRX(coreid));
+
+	/* Clear the mailbox to clear the interrupt */
+	cvmx_write_csr(CVMX_CIU_MBOX_CLRX(coreid), action);
+
+	if (action & SMP_CALL_FUNCTION)
+		smp_call_function_interrupt();
+
+	/* Check if we've been told to flush the icache */
+	if (action & SMP_ICACHE_FLUSH)
+		asm volatile ("synci 0($0)\n");
+}
+#endif
+
+asmlinkage void plat_irq_dispatch(void)
+{
+	const unsigned long core_id = cvmx_get_core_num();
+	const uint64_t ciu_sum0_address = CVMX_CIU_INTX_SUM0(core_id * 2);
+	const uint64_t ciu_en0_address = CVMX_CIU_INTX_EN0(core_id * 2);
+	unsigned long cop0_cause;
+	unsigned long cop0_status;
+	uint64_t ciu_en0;
+	uint64_t ciu_sum0;
+
+	while (1) {
+		asm volatile ("mfc0	%[cause], $13\n"
+			      "mfc0	%[status], $12\n"
+			      "ld	%[sum0], 0(%[sum0_address])\n"
+			      "ld	%[en0], 0(%[en0_address])\n":
+			      [cause] "=r"(cop0_cause),
+			      [status] "=r"(cop0_status),
+			      [sum0] "=r"(ciu_sum0),[en0] "=r"(ciu_en0)
+			      :[sum0_address] "r"(ciu_sum0_address),
+			      [en0_address] "r"(ciu_en0_address));
+		ciu_en0 |= 1ull << 44;
+		cop0_cause &= cop0_status;
+		ciu_sum0 &= ciu_en0;
+		cop0_cause &= ST0_IM;
+
+#ifdef CONFIG_SMP
+		if (unlikely(cop0_cause & STATUSF_IP3)) {
+			mailbox_interrupt();
+		} else if (unlikely(cop0_cause & STATUSF_IP2)) {
+#else
+		if (unlikely(cop0_cause & STATUSF_IP2)) {
+#endif
+			if (likely(ciu_sum0))
+				do_IRQ(fls64(ciu_sum0) + 7);
+			else
+				spurious_interrupt();
+		} else if (likely(cop0_cause)) {
+			do_IRQ(fls(cop0_cause) - 9);
+		} else {
+			break;
+		}
+	}
+}
diff --git a/arch/mips/cavium-octeon/msi.c b/arch/mips/cavium-octeon/msi.c
new file mode 100644
index 0000000..c98b7a0
--- /dev/null
+++ b/arch/mips/cavium-octeon/msi.c
@@ -0,0 +1,140 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2005-2007 Cavium Networks
+ */
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/msi.h>
+#include <linux/spinlock.h>
+#include <linux/interrupt.h>
+#include "hal.h"
+#include "pci-common.h"
+
+static uint64_t msi_free_irq_bitmask = 0;
+static DEFINE_SPINLOCK(msi_free_irq_bitmask_lock);
+
+int arch_setup_msi_irq(struct pci_dev *dev, struct msi_desc *desc)
+{
+	struct msi_msg msg;
+	int irq;
+
+	/* We're going to search msi_free_irq_bitmask_lock for a zero bit. This
+	   represents an MSI interrupt number that isn't in use */
+	spin_lock(&msi_free_irq_bitmask_lock);
+	for (irq = 0; irq < 64; irq++) {
+		if ((msi_free_irq_bitmask & (1ull << irq)) == 0) {
+			msi_free_irq_bitmask |= 1ull << irq;
+			break;
+		}
+	}
+	spin_unlock(&msi_free_irq_bitmask_lock);
+
+	/* If we went through all of the bits without finding a free one then
+	   panic. It may be possible to share MSI interrupts, but I can't image
+	   an Octeon needing more than 64 MSI interrupts */
+	if (irq == 64)
+		panic("Unable to find a free MSI interrupt");
+
+	/* MSI interrupts start at logical IRQ OCTEON_IRQ_MSI_BIT0 */
+	irq += OCTEON_IRQ_MSI_BIT0;
+
+	switch (octeon_dma_bar_type) {
+	case OCTEON_DMA_BAR_TYPE_SMALL:
+		/* When not using big bar, Bar 0 is based at 128MB */
+		msg.address_lo =
+			((128ul << 20) + CVMX_PCI_MSI_RCV) & 0xffffffff;
+		msg.address_hi = ((128ul << 20) + CVMX_PCI_MSI_RCV) >> 32;
+	case OCTEON_DMA_BAR_TYPE_BIG:
+		/* When using big bar, Bar 0 is based at 0 */
+		msg.address_lo = (0 + CVMX_PCI_MSI_RCV) & 0xffffffff;
+		msg.address_hi = (0 + CVMX_PCI_MSI_RCV) >> 32;
+		break;
+	case OCTEON_DMA_BAR_TYPE_PCIE:
+		/* When using PCIe, Bar 0 is based at 0 */
+		// FIXME CVMX_NPEI_MSI_RCV* other than 0?
+		msg.address_lo = (0 + CVMX_NPEI_PCIE_MSI_RCV) & 0xffffffff;
+		msg.address_hi = (0 + CVMX_NPEI_PCIE_MSI_RCV) >> 32;
+		break;
+	default:
+		panic("arch_setup_msi_irq: Invalid octeon_dma_bar_type\n");
+	}
+	msg.data = irq - OCTEON_IRQ_MSI_BIT0;
+
+	set_irq_msi(irq, desc);
+	write_msi_msg(irq, &msg);
+	return irq;
+}
+
+void arch_teardown_msi_irq(unsigned int irq)
+{
+	uint64_t bitmask;
+	printk("arch_teardown_msi_irq(irq=%d)\n", irq);
+
+	if ((irq < OCTEON_IRQ_MSI_BIT0) || (irq > OCTEON_IRQ_MSI_BIT63))
+		panic("Attempted to teardown illegal MSI interrupt (%d)", irq);
+
+	bitmask = 1ull << (irq - OCTEON_IRQ_MSI_BIT0);
+	if ((msi_free_irq_bitmask & bitmask) == 0)
+		panic("Attempted to teardown MSI interrupt (%d) not in use",
+		      irq);
+
+	spin_lock(&msi_free_irq_bitmask_lock);
+	msi_free_irq_bitmask ^= bitmask;
+	spin_unlock(&msi_free_irq_bitmask_lock);
+}
+
+static irqreturn_t octeon_msi_interrupt(int cpl, void *dev_id)
+{
+	uint64_t msi_bits;
+	int irq;
+
+	if (octeon_dma_bar_type == OCTEON_DMA_BAR_TYPE_PCIE)
+		msi_bits = cvmx_read_csr(CVMX_PEXP_NPEI_MSI_RCV0);
+	else
+		msi_bits = cvmx_read_csr(CVMX_NPI_NPI_MSI_RCV);
+	irq = fls64(msi_bits);
+	if (irq) {
+		irq += OCTEON_IRQ_MSI_BIT0 - 1;
+		if (irq_desc[irq].action) {
+			do_IRQ(irq);
+			return IRQ_HANDLED;
+		} else {
+			printk("Spurious MSI interrupt %d\n", irq);
+			if (octeon_has_feature(OCTEON_FEATURE_PCIE)) {
+				/* These chips have PCIe */
+				cvmx_write_csr(CVMX_PEXP_NPEI_MSI_RCV0,
+					       1ull << (irq -
+							OCTEON_IRQ_MSI_BIT0));
+			} else {
+				/* These chips have PCI */
+				cvmx_write_csr(CVMX_NPI_NPI_MSI_RCV,
+					       1ull << (irq -
+							OCTEON_IRQ_MSI_BIT0));
+			}
+		}
+	}
+	return IRQ_NONE;
+}
+
+int octeon_msi_initialize(void)
+{
+	if (octeon_has_feature(OCTEON_FEATURE_PCIE)) {
+		request_irq(OCTEON_IRQ_PCI_MSI0, octeon_msi_interrupt, SA_SHIRQ,
+			    "MSI[0:63]", octeon_msi_interrupt);
+	} else if (octeon_is_pci_host()) {
+		request_irq(OCTEON_IRQ_PCI_MSI0, octeon_msi_interrupt, SA_SHIRQ,
+			    "MSI[0:15]", octeon_msi_interrupt);
+		request_irq(OCTEON_IRQ_PCI_MSI1, octeon_msi_interrupt, SA_SHIRQ,
+			    "MSI[16:31]", octeon_msi_interrupt);
+		request_irq(OCTEON_IRQ_PCI_MSI2, octeon_msi_interrupt, SA_SHIRQ,
+			    "MSI[32:47]", octeon_msi_interrupt);
+		request_irq(OCTEON_IRQ_PCI_MSI3, octeon_msi_interrupt, SA_SHIRQ,
+			    "MSI[48:63]", octeon_msi_interrupt);
+	}
+	return 0;
+}
+
+subsys_initcall(octeon_msi_initialize);
diff --git a/arch/mips/cavium-octeon/octeon_info.c b/arch/mips/cavium-octeon/octeon_info.c
new file mode 100644
index 0000000..301e1a0
--- /dev/null
+++ b/arch/mips/cavium-octeon/octeon_info.c
@@ -0,0 +1,126 @@
+/*
+ * Simple /proc interface to Octeon Information
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2004-2007 Cavium Networks
+ */
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/fs.h>
+#include <linux/seq_file.h>
+#include <linux/proc_fs.h>
+#include "hal.h"
+#include "cvmx-app-init.h"
+
+/**
+ * User is reading /proc/octeon_info
+ *
+ * @param m
+ * @param v
+ * @return
+ */
+static int octeon_info_show(struct seq_file *m, void *v)
+{
+	extern cvmx_bootinfo_t *octeon_bootinfo;
+#if defined(CONFIG_CAVIUM_RESERVE32) && CONFIG_CAVIUM_RESERVE32
+	extern uint64_t octeon_reserve32_memory;
+#endif
+
+	seq_printf(m, "processor_id:        0x%x\n", read_c0_prid());
+	seq_printf(m, "boot_flags:          0x%x\n", octeon_bootinfo->flags);
+	seq_printf(m, "dram_size:           %u\n", octeon_bootinfo->dram_size);
+	seq_printf(m, "phy_mem_desc_addr:   0x%x\n",
+		   octeon_bootinfo->phy_mem_desc_addr);
+	seq_printf(m, "eclock_hz:           %u\n", octeon_bootinfo->eclock_hz);
+	seq_printf(m, "dclock_hz:           %u\n", octeon_bootinfo->dclock_hz);
+	seq_printf(m, "board_type:          %u\n", octeon_bootinfo->board_type);
+	seq_printf(m, "board_rev_major:     %u\n",
+		   octeon_bootinfo->board_rev_major);
+	seq_printf(m, "board_rev_minor:     %u\n",
+		   octeon_bootinfo->board_rev_minor);
+	seq_printf(m, "board_serial_number: %s\n",
+		   octeon_bootinfo->board_serial_number);
+	seq_printf(m, "mac_addr_base:       %02x:%02x:%02x:%02x:%02x:%02x\n",
+		   (int) octeon_bootinfo->mac_addr_base[0],
+		   (int) octeon_bootinfo->mac_addr_base[1],
+		   (int) octeon_bootinfo->mac_addr_base[2],
+		   (int) octeon_bootinfo->mac_addr_base[3],
+		   (int) octeon_bootinfo->mac_addr_base[4],
+		   (int) octeon_bootinfo->mac_addr_base[5]);
+	seq_printf(m, "mac_addr_count:      %u\n",
+		   octeon_bootinfo->mac_addr_count);
+#if CONFIG_CAVIUM_RESERVE32
+	seq_printf(m, "32bit_shared_mem_base: 0x%lx\n",
+		   octeon_reserve32_memory);
+	seq_printf(m, "32bit_shared_mem_size: 0x%x\n",
+		   octeon_reserve32_memory ? CONFIG_CAVIUM_RESERVE32 << 20 : 0);
+#else
+	seq_printf(m, "32bit_shared_mem_base: 0x%lx\n", 0ul);
+	seq_printf(m, "32bit_shared_mem_size: 0x%x\n", 0);
+#endif
+#ifdef CONFIG_CAVIUM_RESERVE32_USE_WIRED_TLB
+	seq_printf(m, "32bit_shared_mem_wired: 1\n");
+#else
+	seq_printf(m, "32bit_shared_mem_wired: 0\n");
+#endif
+	return 0;
+}
+
+
+/**
+ * /proc/octeon_info was openned. Use the single_open iterator
+ *
+ * @param inode
+ * @param file
+ * @return
+ */
+static int octeon_info_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, octeon_info_show, NULL);
+}
+
+
+static struct file_operations octeon_info_operations = {
+	.open = octeon_info_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+
+
+/**
+ * Module initialization
+ *
+ * @return
+ */
+static int __init octeon_info_init(void)
+{
+	struct proc_dir_entry *entry =
+		create_proc_entry("octeon_info", 0, NULL);
+	if (entry == NULL)
+		return -1;
+
+	entry->proc_fops = &octeon_info_operations;
+	return 0;
+}
+
+
+/**
+ * Module cleanup
+ *
+ * @return
+ */
+static void __exit octeon_info_cleanup(void)
+{
+	remove_proc_entry("octeon_info", NULL);
+}
+
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Cavium Networks <support@caviumnetworks.com>");
+MODULE_DESCRIPTION("Cavium Networks Octeon information interface.");
+module_init(octeon_info_init);
+module_exit(octeon_info_cleanup);
diff --git a/arch/mips/cavium-octeon/pci-common.c b/arch/mips/cavium-octeon/pci-common.c
new file mode 100644
index 0000000..8dcee5d
--- /dev/null
+++ b/arch/mips/cavium-octeon/pci-common.c
@@ -0,0 +1,74 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2005-2007 Cavium Networks
+ */
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/pci.h>
+#include <linux/interrupt.h>
+#include <asm/time.h>
+#include <asm/delay.h>
+#include "pci-common.h"
+#include "hal.h"
+
+typeof(pcibios_map_irq) * octeon_pcibios_map_irq = NULL;
+octeon_dma_bar_type_t octeon_dma_bar_type = OCTEON_DMA_BAR_TYPE_INVALID;
+
+/**
+ * Map a PCI device to the appropriate interrupt line
+ *
+ * @param dev    The Linux PCI device structure for the device to map
+ * @param slot   The slot number for this device on __BUS 0__. Linux
+ *               enumerates through all the bridges and figures out the
+ *               slot on Bus 0 where this device eventually hooks to.
+ * @param pin    The PCI interrupt pin read from the device, then swizzled
+ *               as it goes through each bridge.
+ * @return Interrupt number for the device
+ */
+int __init pcibios_map_irq(struct pci_dev *dev, u8 slot, u8 pin)
+{
+	if (octeon_pcibios_map_irq)
+		return octeon_pcibios_map_irq(dev, slot, pin);
+	else
+		panic("octeon_pcibios_map_irq doesn't point to a pcibios_map_irq() function");
+}
+
+
+/**
+ * Called to perform platform specific PCI setup
+ *
+ * @param dev
+ * @return
+ */
+int pcibios_plat_dev_init(struct pci_dev *dev)
+{
+	/* Force the Cache line setting to 64 bytes. The standard Linux bus
+	   scan doesn't seem to set it. Octeon really has 128 byte lines, but
+	   Intel bridges get really upset if you try and set values above 64
+	   bytes. Value is specified in 32bit words */
+	pci_write_config_byte(dev, PCI_CACHE_LINE_SIZE, 64 / 4);
+	/* Set latency timers for all devices */
+	pci_write_config_byte(dev, PCI_LATENCY_TIMER, 48);
+	if (dev->subordinate) {
+		uint16_t config;
+
+		/* Set latency timers on sub bridges */
+		pci_write_config_byte(dev, PCI_SEC_LATENCY_TIMER, 48);
+		/* Enable parity checking and error reporting */
+		pci_read_config_word(dev, PCI_COMMAND, &config);
+		config |= PCI_COMMAND_PARITY | PCI_COMMAND_SERR;
+		pci_write_config_word(dev, PCI_COMMAND, config);
+		/* More bridge error detection */
+		pci_read_config_word(dev, PCI_BRIDGE_CONTROL, &config);
+		config |= PCI_BRIDGE_CTL_PARITY | PCI_BRIDGE_CTL_SERR;
+		/* Reporting master aborts also causes SERR. Normally it
+		   creates too much noise, but it might be useful in the future
+		 */
+		// config |= PCI_BRIDGE_CTL_MASTER_ABORT;
+		pci_write_config_word(dev, PCI_BRIDGE_CONTROL, config);
+	}
+	return 0;
+}
diff --git a/arch/mips/cavium-octeon/pci-common.h b/arch/mips/cavium-octeon/pci-common.h
new file mode 100644
index 0000000..5256bca
--- /dev/null
+++ b/arch/mips/cavium-octeon/pci-common.h
@@ -0,0 +1,39 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2005-2007 Cavium Networks
+ */
+#ifndef __OCTEON_PCI_COMMON_H__
+#define __OCTEON_PCI_COMMON_H__
+
+#include <linux/pci.h>
+
+/* Some PCI cards require delays when accessing config space. */
+#define PCI_CONFIG_SPACE_DELAY 10000
+
+/* pcibios_map_irq() is defined inside pci-common.c. All it does is call the
+   Octeon specific version pointed to by this variable. This function needs to
+   change for PCI or PCIe based hosts */
+extern typeof(pcibios_map_irq) * octeon_pcibios_map_irq;
+
+/* The following defines are only used when octeon_dma_bar_type =
+   OCTEON_DMA_BAR_TYPE_BIG */
+#define OCTEON_PCI_BAR1_HOLE_BITS 5
+#define OCTEON_PCI_BAR1_HOLE_SIZE (1ul<<(OCTEON_PCI_BAR1_HOLE_BITS+3))
+
+typedef enum {
+	OCTEON_DMA_BAR_TYPE_INVALID,
+	OCTEON_DMA_BAR_TYPE_SMALL,
+	OCTEON_DMA_BAR_TYPE_BIG,
+	OCTEON_DMA_BAR_TYPE_PCIE
+} octeon_dma_bar_type_t;
+
+/**
+ * This is a variable to tell the DMA mapping system in dma-octeon.c
+ * how to map PCI DMA addresses.
+ */
+extern octeon_dma_bar_type_t octeon_dma_bar_type;
+
+#endif
diff --git a/arch/mips/cavium-octeon/pci.c b/arch/mips/cavium-octeon/pci.c
new file mode 100644
index 0000000..3549bb9
--- /dev/null
+++ b/arch/mips/cavium-octeon/pci.c
@@ -0,0 +1,511 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2005-2007 Cavium Networks
+ */
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/pci.h>
+#include <linux/interrupt.h>
+#include <asm/time.h>
+#include <asm/delay.h>
+#include "hal.h"
+#include "pci-common.h"
+
+#define USE_OCTEON_INTERNAL_ARBITER
+
+/* Octeon's PCI controller uses did=3, subdid=2 for PCI IO addresses. Use PCI
+   endian swapping 1 so no address swapping is necessary. The Linux io routines
+   will endian swap the data */
+#define OCTEON_PCI_IOSPACE_BASE     0x80011a0400000000ull
+#define OCTEON_PCI_IOSPACE_SIZE     (1ull<<32)
+
+/* Octeon't PCI controller uses did=3, subdid=3 for PCI memory. */
+#define OCTEON_PCI_MEMSPACE_OFFSET  (0x00011b0000000000ull)
+
+/**
+ * This is the bit decoding used for the Octeon PCI controller addresses
+ */
+typedef union {
+	uint64_t u64;
+	struct {
+		uint64_t upper:2;
+		uint64_t reserved:13;
+		uint64_t io:1;
+		uint64_t did:5;
+		uint64_t subdid:3;
+		uint64_t reserved2:4;
+		uint64_t endian_swap:2;
+		uint64_t reserved3:10;
+		uint64_t bus:8;
+		uint64_t dev:5;
+		uint64_t func:3;
+		uint64_t reg:8;
+	} s;
+} octeon_pci_address_t;
+
+
+/**
+ * Map a PCI device to the appropriate interrupt line
+ *
+ * @param dev    The Linux PCI device structure for the device to map
+ * @param slot   The slot number for this device on __BUS 0__. Linux
+ *               enumerates through all the bridges and figures out the
+ *               slot on Bus 0 where this device eventually hooks to.
+ * @param pin    The PCI interrupt pin read from the device, then swizzled
+ *               as it goes through each bridge.
+ * @return Interrupt number for the device
+ */
+int __init octeon_pci_pcibios_map_irq(struct pci_dev *dev, u8 slot, u8 pin)
+{
+	int irq_num;
+	const char *interrupts;
+	int dev_num;
+
+	/* Get the board specific interrupt mapping */
+	interrupts = octeon_get_pci_interrupts();
+
+	dev_num = dev->devfn >> 3;
+	if (dev_num < strlen(interrupts))
+		irq_num = ((interrupts[dev_num] - 'A' + pin - 1) & 3) + 44;	/* 44
+										   is
+										   the
+										   irq
+										   number
+										   of PCI
+										   INT-A */
+	else
+		irq_num = ((slot + pin - 3) & 3) + 44;	/* 44 is the irq number
+							   of PCI INT-A */
+	return irq_num;
+}
+
+
+/**
+ * Read a value from configuration space
+ *
+ * @param bus
+ * @param devfn
+ * @param reg
+ * @param size
+ * @param val
+ * @return
+ */
+static int octeon_read_config(struct pci_bus *bus, unsigned int devfn, int reg,
+			      int size, u32 * val)
+{
+	octeon_pci_address_t pci_addr;
+
+	pci_addr.u64 = 0;
+	pci_addr.s.upper = 2;
+	pci_addr.s.io = 1;
+	pci_addr.s.did = 3;
+	pci_addr.s.subdid = 1;
+	pci_addr.s.endian_swap = 1;
+	pci_addr.s.bus = bus->number;
+	pci_addr.s.dev = devfn >> 3;
+	pci_addr.s.func = devfn & 0x7;
+	pci_addr.s.reg = reg;
+
+#if PCI_CONFIG_SPACE_DELAY
+	udelay(PCI_CONFIG_SPACE_DELAY);
+#endif
+	switch (size) {
+	case 4:
+		*val = le32_to_cpu(cvmx_read64_uint32(pci_addr.u64));
+		return PCIBIOS_SUCCESSFUL;
+	case 2:
+		*val = le16_to_cpu(cvmx_read64_uint16(pci_addr.u64));
+		return PCIBIOS_SUCCESSFUL;
+	case 1:
+		*val = cvmx_read64_uint8(pci_addr.u64);
+		return PCIBIOS_SUCCESSFUL;
+	}
+	return PCIBIOS_FUNC_NOT_SUPPORTED;
+}
+
+
+/**
+ * Write a value to PCI configuration space
+ *
+ * @param bus
+ * @param devfn
+ * @param reg
+ * @param size
+ * @param val
+ * @return
+ */
+static int octeon_write_config(struct pci_bus *bus, unsigned int devfn, int reg,
+			       int size, u32 val)
+{
+	octeon_pci_address_t pci_addr;
+
+	pci_addr.u64 = 0;
+	pci_addr.s.upper = 2;
+	pci_addr.s.io = 1;
+	pci_addr.s.did = 3;
+	pci_addr.s.subdid = 1;
+	pci_addr.s.endian_swap = 1;
+	pci_addr.s.bus = bus->number;
+	pci_addr.s.dev = devfn >> 3;
+	pci_addr.s.func = devfn & 0x7;
+	pci_addr.s.reg = reg;
+
+#if PCI_CONFIG_SPACE_DELAY
+	udelay(PCI_CONFIG_SPACE_DELAY);
+#endif
+	switch (size) {
+	case 4:
+		cvmx_write64_uint32(pci_addr.u64, cpu_to_le32(val));
+		return PCIBIOS_SUCCESSFUL;
+	case 2:
+		cvmx_write64_uint16(pci_addr.u64, cpu_to_le16(val));
+		return PCIBIOS_SUCCESSFUL;
+	case 1:
+		cvmx_write64_uint8(pci_addr.u64, val);
+		return PCIBIOS_SUCCESSFUL;
+	}
+	return PCIBIOS_FUNC_NOT_SUPPORTED;
+}
+
+
+static struct pci_ops octeon_pci_ops = {
+	octeon_read_config,
+	octeon_write_config,
+};
+
+static struct resource octeon_pci_mem_resource = {
+	.start = 0,
+	.end = 0,
+	.name = "Octeon PCI MEM",
+	.flags = IORESOURCE_MEM,
+};
+
+/* PCI ports must be above 16KB so the ISA bus filtering in the PCI-X to PCI
+   bridge */
+static struct resource octeon_pci_io_resource = {
+	.start = 0x4000,
+	.end = OCTEON_PCI_IOSPACE_SIZE - 1,
+	.name = "Octeon PCI IO",
+	.flags = IORESOURCE_IO,
+};
+
+static struct pci_controller octeon_pci_controller = {
+	.pci_ops = &octeon_pci_ops,
+	.mem_resource = &octeon_pci_mem_resource,
+	.mem_offset = OCTEON_PCI_MEMSPACE_OFFSET,
+	.io_resource = &octeon_pci_io_resource,
+	.io_offset = 0,
+	.io_map_base = OCTEON_PCI_IOSPACE_BASE,
+};
+
+
+/**
+ * Low level initialize the Octeon PCI controller
+ *
+ * @return
+ */
+static void octeon_pci_initialize(void)
+{
+	cvmx_pci_cfg01_t cfg01;
+	cvmx_npi_ctl_status_t ctl_status;
+	cvmx_pci_ctl_status_2_t ctl_status_2;
+	cvmx_pci_cfg19_t cfg19;
+	cvmx_pci_cfg16_t cfg16;
+	cvmx_pci_cfg22_t cfg22;
+	cvmx_pci_cfg56_t cfg56;
+
+	/* Reset the PCI Bus */
+	cvmx_write_csr(CVMX_CIU_SOFT_PRST, 0x1);
+	cvmx_read_csr(CVMX_CIU_SOFT_PRST);
+
+	udelay(2000);		/* Hold PCI reset for 2 ms */
+
+	ctl_status.u64 = 0;	// cvmx_read_csr(CVMX_NPI_CTL_STATUS);
+	ctl_status.s.max_word = 1;
+	ctl_status.s.timer = 1;
+	cvmx_write_csr(CVMX_NPI_CTL_STATUS, ctl_status.u64);
+
+	/* Deassert PCI reset and advertize PCX Host Mode Device Capability
+	   (64b) */
+	cvmx_write_csr(CVMX_CIU_SOFT_PRST, 0x4);
+	cvmx_read_csr(CVMX_CIU_SOFT_PRST);
+
+	udelay(2000);		/* Wait 2 ms after deasserting PCI reset */
+
+	ctl_status_2.u32 = 0;
+	ctl_status_2.s.tsr_hwm = 1;	/* Initializes to 0.  Must be set
+					   before any PCI reads. */
+	ctl_status_2.s.bar2pres = 1;	/* Enable BAR2 */
+	ctl_status_2.s.bar2_enb = 1;
+	ctl_status_2.s.bar2_cax = 1;	/* Don't use L2 */
+	ctl_status_2.s.bar2_esx = 1;
+	ctl_status_2.s.pmo_amod = 1;	/* Round robin priority */
+	if (octeon_dma_bar_type == OCTEON_DMA_BAR_TYPE_BIG) {
+		ctl_status_2.s.bb1_hole = OCTEON_PCI_BAR1_HOLE_BITS;	/* BAR1
+									   hole */
+		ctl_status_2.s.bb1_siz = 1;	/* BAR1 is 2GB */
+		ctl_status_2.s.bb_ca = 1;	/* Don't use L2 with big bars */
+		ctl_status_2.s.bb_es = 1;	/* Big bar in byte swap mode */
+		ctl_status_2.s.bb1 = 1;	/* BAR1 is big */
+		ctl_status_2.s.bb0 = 1;	/* BAR0 is big */
+	}
+
+	octeon_npi_write32(CVMX_NPI_PCI_CTL_STATUS_2, ctl_status_2.u32);
+	udelay(2000);		/* Wait 2 ms before doing PCI reads */
+
+	ctl_status_2.u32 = octeon_npi_read32(CVMX_NPI_PCI_CTL_STATUS_2);
+	printk("PCI Status: %s %s-bit\n",
+	       ctl_status_2.s.ap_pcix ? "PCI-X" : "PCI",
+	       ctl_status_2.s.ap_64ad ? "64" : "32");
+
+	if (OCTEON_IS_MODEL(OCTEON_CN58XX)) {
+		cvmx_pci_cnt_reg_t pci_cnt_reg_start;
+		cvmx_pci_cnt_reg_t pci_cnt_reg_end;
+		cycles_t cycles;
+		pci_cnt_reg_start.u64 = cvmx_read_csr(CVMX_NPI_PCI_CNT_REG);
+		cycles = get_cycles();
+		udelay(1000);
+		pci_cnt_reg_end.u64 = cvmx_read_csr(CVMX_NPI_PCI_CNT_REG);
+		cycles = get_cycles() - cycles;
+		printk("PCI Clock: %lu MHz\n",
+		       (pci_cnt_reg_end.s.pcicnt -
+			pci_cnt_reg_start.s.pcicnt) / (cycles /
+						       (mips_hpt_frequency /
+							1000000)));
+	}
+
+	/*
+	 ** TDOMC must be set to one in PCI mode. TDOMC should be set to 4
+	 ** in PCI-X mode to allow four oustanding splits. Otherwise,
+	 ** should not change from its reset value. Don't write PCI_CFG19
+	 ** in PCI mode (0x82000001 reset value), write it to 0x82000004
+	 ** after PCI-X mode is known. MRBCI,MDWE,MDRE -> must be zero.
+	 ** MRBCM -> must be one.
+	 */
+	if (ctl_status_2.s.ap_pcix) {
+		cfg19.u32 = 0;
+		cfg19.s.tdomc = 4;	/* Target Delayed/Split request
+					   outstanding maximum count. [1..31]
+					   and 0=32.  NOTE: If the user
+					   programs these bits beyond the
+					   Designed Maximum outstanding count,
+					   then the designed maximum table
+					   depth will be used instead.  No
+					   additional Deferred/Split
+					   transactions will be accepted if
+					   this outstanding maximum count is
+					   reached. Furthermore, no additional
+					   deferred/split transactions will be
+					   accepted if the I/O delay/ I/O Split
+					   Request outstanding maximum is
+					   reached. */
+		cfg19.s.mdrrmc = 2;	/* Master Deferred Read Request
+					   Outstanding Max Count (PCI only).
+					   CR4C[26:24] Max SAC cycles MAX DAC
+					   cycles 000 8 4 001 1 0 010 2 1 011 3
+					   1 100 4 2 101 5 2 110 6 3 111 7 3 For
+					   example, if these bits are programmed
+					   to 100, the core can support 2 DAC
+					   cycles, 4 SAC cycles or a combination
+					   of 1 DAC and 2 SAC cycles. NOTE: For
+					   the PCI-X maximum outstanding split
+					   transactions, refer to CRE0[22:20] */
+
+		cfg19.s.mrbcm = 1;	/* Master Request (Memory Read) Byte
+					   Count/Byte Enable select. 0 = Byte
+					   Enables valid. In PCI mode, a burst
+					   transaction cannot be performed
+					   using Memory Read command=4?h6. 1 =
+					   DWORD Byte Count valid (default). In
+					   PCI Mode, the memory read byte
+					   enables are automatically generated
+					   by the core. Note: N3 Master Request
+					   transaction sizes are always
+					   determined through the
+					   am_attr[<35:32>|<7:0>] field.  */
+		octeon_npi_write32(CVMX_NPI_PCI_CFG19, cfg19.u32);
+	}
+
+
+	cfg01.u32 = 0;
+	cfg01.s.msae = 1;	/* Memory Space Access Enable */
+	cfg01.s.me = 1;		/* Master Enable */
+	cfg01.s.pee = 1;	/* PERR# Enable */
+	cfg01.s.see = 1;	/* System Error Enable */
+	cfg01.s.fbbe = 1;	/* Fast Back to Back Transaction Enable */
+
+	octeon_npi_write32(CVMX_NPI_PCI_CFG01, cfg01.u32);
+
+#ifdef USE_OCTEON_INTERNAL_ARBITER
+	/*
+	 ** When OCTEON is a PCI host, most systems will use OCTEON's
+	 ** internal arbiter, so must enable it before any PCI/PCI-X
+	 ** traffic can occur.
+	 */
+	{
+		cvmx_npi_pci_int_arb_cfg_t pci_int_arb_cfg;
+
+		pci_int_arb_cfg.u64 = 0;
+		pci_int_arb_cfg.s.en = 1;	/* Internal arbiter enable */
+		cvmx_write_csr(CVMX_NPI_PCI_INT_ARB_CFG, pci_int_arb_cfg.u64);
+	}
+#endif				/* USE_OCTEON_INTERNAL_ARBITER */
+
+	/*
+	 ** Preferrably written to 1 to set MLTD. [RDSATI,TRTAE,
+	 ** TWTAE,TMAE,DPPMR -> must be zero. TILT -> must not be set to
+	 ** 1..7.
+	 */
+	cfg16.u32 = 0;
+	cfg16.s.mltd = 1;	/* Master Latency Timer Disable */
+	octeon_npi_write32(CVMX_NPI_PCI_CFG16, cfg16.u32);
+
+	/*
+	 ** Should be written to 0x4ff00. MTTV -> must be zero.
+	 ** FLUSH -> must be 1. MRV -> should be 0xFF.
+	 */
+	cfg22.u32 = 0;
+	cfg22.s.mrv = 0xff;	/* Master Retry Value [1..255] and 0=infinite */
+	cfg22.s.flush = 1;	/* AM_DO_FLUSH_I control NOTE: This bit MUST BE
+				   ONE for proper N3K operation */
+	octeon_npi_write32(CVMX_NPI_PCI_CFG22, cfg22.u32);
+
+	/*
+	 ** MOST Indicates the maximum number of outstanding splits (in -1
+	 ** notation) when OCTEON is in PCI-X mode.  PCI-X performance is
+	 ** affected by the MOST selection.  Should generally be written
+	 ** with one of 0x3be807, 0x2be807, 0x1be807, or 0x0be807,
+	 ** depending on the desired MOST of 3, 2, 1, or 0, respectively.
+	 */
+	cfg56.u32 = 0;
+	cfg56.s.pxcid = 7;	/* RO - PCI-X Capability ID */
+	cfg56.s.ncp = 0xe8;	/* RO - Next Capability Pointer */
+	cfg56.s.dpere = 1;	/* Data Parity Error Recovery Enable */
+	cfg56.s.roe = 1;	/* Relaxed Ordering Enable */
+	cfg56.s.mmbc = 1;	/* Maximum Memory Byte Count
+				   [0=512B,1=1024B,2=2048B,3=4096B] */
+	cfg56.s.most = 3;	/* Maximum outstanding Split transactions [0=1
+				   .. 7=32] */
+
+	octeon_npi_write32(CVMX_NPI_PCI_CFG56, cfg56.u32);
+
+	/*
+	 ** Affects PCI performance when OCTEON services reads to its
+	 ** BAR1/BAR2. Refer to Section 10.6.1.  The recommended values are
+	 ** 0x22, 0x33, and 0x33 for PCI_READ_CMD_6, PCI_READ_CMD_C, and
+	 ** PCI_READ_CMD_E, respectively. Note that these values differ
+	 ** from their reset values.
+	 */
+	octeon_npi_write32(CVMX_NPI_PCI_READ_CMD_6, 0x22);
+	octeon_npi_write32(CVMX_NPI_PCI_READ_CMD_C, 0x33);
+	octeon_npi_write32(CVMX_NPI_PCI_READ_CMD_E, 0x33);
+}
+
+
+/**
+ * Initialize the Octeon PCI controller
+ *
+ * @return
+ */
+static int __init octeon_pci_setup(void)
+{
+	cvmx_npi_mem_access_subid_t mem_access;
+	int index;
+
+	/* Only these chips have PCI */
+	if (octeon_has_feature(OCTEON_FEATURE_PCIE))
+		return 0;
+
+	/* Point pcibios_map_irq() to the PCI version of it */
+	octeon_pcibios_map_irq = octeon_pci_pcibios_map_irq;
+
+	/* Only use the big bars on chips that support it */
+	if (OCTEON_IS_MODEL(OCTEON_CN31XX) ||
+	    OCTEON_IS_MODEL(OCTEON_CN38XX_PASS2) ||
+	    OCTEON_IS_MODEL(OCTEON_CN38XX_PASS1))
+		octeon_dma_bar_type = OCTEON_DMA_BAR_TYPE_SMALL;
+	else
+		octeon_dma_bar_type = OCTEON_DMA_BAR_TYPE_BIG;
+
+	/* PCI I/O and PCI MEM values */
+	set_io_port_base(OCTEON_PCI_IOSPACE_BASE);
+	ioport_resource.start = 0;
+	ioport_resource.end = OCTEON_PCI_IOSPACE_SIZE - 1;
+	iomem_resource.start = (1ull << 48);
+	iomem_resource.end = (2ull << 48) - 1;
+
+	if (!octeon_is_pci_host()) {
+		printk("Not in host mode, PCI Controller not initialized\n");
+		return 0;
+	}
+
+	printk("%s Octeon big bar support\n",
+	       (octeon_dma_bar_type ==
+		OCTEON_DMA_BAR_TYPE_BIG) ? "Enabling" : "Disabling");
+
+	octeon_pci_initialize();
+
+	mem_access.u64 = 0;
+	mem_access.s.esr = 1;	/* Endian-Swap on read. */
+	mem_access.s.esw = 1;	/* Endian-Swap on write. */
+	mem_access.s.nsr = 0;	/* No-Snoop on read. */
+	mem_access.s.nsw = 0;	/* No-Snoop on write. */
+	mem_access.s.ror = 0;	/* Relax Read on read. */
+	mem_access.s.row = 0;	/* Relax Order on write. */
+	mem_access.s.ba = 0;	/* PCI Address bits [63:36]. */
+	cvmx_write_csr(CVMX_NPI_MEM_ACCESS_SUBID3, mem_access.u64);
+
+	/* Remap the Octeon BAR 2 above all 32 bit devices (0x8000000000ul).
+	   This is done here so it is remapped before the readl()'s below. We
+	   don't want BAR2 overlapping with BAR0/BAR1 during these reads */
+	octeon_npi_write32(CVMX_NPI_PCI_CFG08, 0);
+	octeon_npi_write32(CVMX_NPI_PCI_CFG09, 0x80);
+
+	/* Disable the BAR1 movable mappings */
+	for (index = 0; index < 32; index++)
+		octeon_npi_write32(CVMX_NPI_PCI_BAR1_INDEXX(index), 0);
+
+	if (octeon_dma_bar_type == OCTEON_DMA_BAR_TYPE_BIG) {
+		/* Remap the Octeon BAR 0 to 0-2GB */
+		octeon_npi_write32(CVMX_NPI_PCI_CFG04, 0);
+		octeon_npi_write32(CVMX_NPI_PCI_CFG05, 0);
+
+		/* Remap the Octeon BAR 1 to map 2GB-4GB (minus the BAR 1 hole) */
+		octeon_npi_write32(CVMX_NPI_PCI_CFG06, 2ul << 30);
+		octeon_npi_write32(CVMX_NPI_PCI_CFG07, 0);
+
+		/* Devices go after BAR1 */
+		octeon_pci_mem_resource.start =
+			OCTEON_PCI_MEMSPACE_OFFSET + (4ul << 30) -
+			(OCTEON_PCI_BAR1_HOLE_SIZE << 20);
+		octeon_pci_mem_resource.end =
+			octeon_pci_mem_resource.start + (1ul << 30);
+	} else {
+		/* Remap the Octeon BAR 0 to map 128MB-(128MB+4KB) */
+		octeon_npi_write32(CVMX_NPI_PCI_CFG04, 128ul << 20);
+		octeon_npi_write32(CVMX_NPI_PCI_CFG05, 0);
+
+		/* Remap the Octeon BAR 1 to map 0-128MB */
+		octeon_npi_write32(CVMX_NPI_PCI_CFG06, 0);
+		octeon_npi_write32(CVMX_NPI_PCI_CFG07, 0);
+
+		/* Devices go after BAR0 */
+		octeon_pci_mem_resource.start =
+			OCTEON_PCI_MEMSPACE_OFFSET + (128ul << 20) +
+			(4ul << 10);
+		octeon_pci_mem_resource.end =
+			octeon_pci_mem_resource.start + (1ul << 30);
+	}
+
+	register_pci_controller(&octeon_pci_controller);
+
+	/* Clear any errors that might be pending from before the bus was setup
+	   properly */
+	cvmx_write_csr(CVMX_NPI_PCI_INT_SUM2, -1);
+	return 0;
+}
+
+arch_initcall(octeon_pci_setup);
diff --git a/arch/mips/cavium-octeon/pci_chips.c b/arch/mips/cavium-octeon/pci_chips.c
new file mode 100644
index 0000000..def156c
--- /dev/null
+++ b/arch/mips/cavium-octeon/pci_chips.c
@@ -0,0 +1,480 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2005-2007 Cavium Networks
+ */
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/pci.h>
+#include <linux/interrupt.h>
+#include <asm/delay.h>
+#include "hal.h"
+
+
+/**
+ * Fixup the Via southbridge to enable the IDE. By default
+ * it is disabled and doesn't show up in a bus scan.
+ *
+ * @param dev
+ */
+static void chip_vt82c686b_force_enable_ide(struct pci_dev *dev)
+{
+	uint8_t value;
+	pci_read_config_byte(dev, 0x48, &value);
+	/* Enable the IDE interface if it is disabled */
+	if (value & 2) {
+		value ^= 2;
+		pci_write_config_byte(dev, 0x48, value);
+	}
+}
+
+DECLARE_PCI_FIXUP_EARLY(PCI_VENDOR_ID_VIA, PCI_DEVICE_ID_VIA_82C686,
+			chip_vt82c686b_force_enable_ide);
+
+
+/**
+ * The Via southbridge requires ISA resources that are
+ * normally not forwarded through bridges. Modify the Via's
+ * parent bridge to forward these resource. We assume the Via
+ * is behind exactly one bridge.
+ *
+ * @param via_dev
+ */
+static void chip_vt82c686b_fix_parent_bridge(struct pci_dev *via_dev)
+{
+	extern void octeon_i8259_setup(int irq_line);
+	struct pci_dev *dev = via_dev->bus->self;
+	uint16_t val;
+	pci_write_config_word(dev, 0x04, 0x0026);
+	pci_read_config_word(dev, 0x3e, &val);
+	val &= ~(1 << 2);	/* Disable the ISA port filtering */
+	val |= 1 << 3;		/* Enable VGA */
+	val |= 1 << 4;		/* Enable VGA Alias Filter */
+	pci_write_config_word(dev, 0x3e, val);
+	pci_write_config_byte(dev, 0x1c, 0x0);	/* Force IO port range to start
+						   at 0 for Via 686 ISA devices
+						 */
+	pci_write_config_byte(dev, 0x30, 0x0);
+	pci_write_config_word(dev, 0x04, 0x0027);
+	octeon_i8259_setup(octeon_get_southbridge_interrupt());
+}
+
+DECLARE_PCI_FIXUP_FINAL(PCI_VENDOR_ID_VIA, PCI_DEVICE_ID_VIA_82C686,
+			chip_vt82c686b_fix_parent_bridge);
+
+
+/**
+ * Perform the convoluted setup needed for the Via southbridge
+ *
+ * @param dev
+ */
+static void chip_vt82c686b_setup(struct pci_dev *dev)
+{
+	uint8_t bvalue;
+	pci_write_config_byte(dev, 0x40, 0x08);	// I/O Recovery Time Enable
+	pci_write_config_byte(dev, 0x41, 0x41);	// I/O Recovery Time=8BCLKs,
+						// ISA Refresh
+	pci_write_config_byte(dev, 0x45, 0x80);	// ISA Master /DMA to PCI Line
+						// Buffer Enable
+	pci_write_config_byte(dev, 0x46, 0x60);	// GATE INTR Enable, Flush Line
+						// Buffer for Int or DMA IOR
+						// Cycle Enable
+	pci_write_config_byte(dev, 0x47, 0xa0);	// CPU Reset source: INIT, EISA
+						// 4d0 /4d1 port enable
+	/*
+	   Offset 48 - Miscellaneous Control 3 ................................
+	   RW 7-4 Reserved ........................................ always
+	   reads 0 3 Extra RTC Port 74/75 Enable 0 Disable
+	   ...................................................default 1 Enable
+	   2 Integrated USB Controller Disable 0 Enable
+	   ....................................................default 1
+	   Disable 1 Integrated IDE Controller Disable 0 Enable
+	   ....................................................default 1
+	   Disable 0 512K PCI Memory Decode 0 Use Rx4E[15-12] to select top of
+	   PCI memory 1 Use contents of Rx4E[15-12] plus 512K as top of PCI
+	   memory.......................................default */
+	pci_write_config_byte(dev, 0x48, 0x09);
+
+	/*
+	   Offset 4A - IDE Interrupt
+	   Routing..................................RW 7 Wait for PGNT Before
+	   Grant to ISA Master / DMA 0 Disable
+	   ...................................................default 1 Enable
+	   6 Bus Select for Access to I/O Devices Below 100h 0 Access ports
+	   00-FFh via XD bus ...........default 1 Access ports 00-FFh via SD
+	   bus (applies to external devices only; internal devices such as the
+	   mouse controller are not effected) 5-4 Reserved (do not
+	   program)..................... default = 0 3-2 IDE Second Channel IRQ
+	   Routing 00 IRQ14 01
+	   IRQ15.....................................................default 10
+	   IRQ10 11 IRQ11 1-0 IDE Primary Channel IRQ Routing 00
+	   IRQ14.....................................................default 01
+	   IRQ15 10 IRQ10 11 IRQ11 */
+	pci_write_config_byte(dev, 0x4a, 0x04);
+
+	/*
+	   Offset 4F-4E - ISA DMA/Master Mem Access Ctrl 3 ... RW 15-12 Top of
+	   PCI Memory for ISA DMA/Master accesses 0000 1M
+	   .................................................... default 0001 2M
+	   ... ... 1111 16M Note: All ISA DMA / Masters that access addresses
+	   higher than the top of PCI memory will not be directed to the PCI
+	   bus. 11 Forward E0000-EFFFF Accesses to PCI ....... def=0 10 Forward
+	   A0000-BFFFF Accesses to PCI ....... def=0 9 Forward 80000-9FFFF
+	   Accesses to PCI ........def=1 8 Forward 00000-7FFFF Accesses to PCI
+	   ........def=1 7 Forward DC000-DFFFF Accesses to PCI ...... def=0 6
+	   Forward D8000-DBFFF Accesses to PCI ......def=0 5 Forward D4000-D7FFF
+	   Accesses to PCI ......def=0 4 Forward D0000-D3FFF Accesses to PCI
+	   ......def=0 3 Forward CC000-CFFFF Accesses to PCI .....def=0 2
+	   Forward C8000-CBFFF Accesses to PCI ......def=0 1 Forward C4000-C7FFF
+	   Accesses to PCI ......def=0 0 Forward C0000-C3FFF Accesses to PCI
+	   ......def=0 */
+	pci_write_config_word(dev, 0x4e, 0xf000);
+
+	/*
+	   Offset 51 - PNP IRQ Routing
+	   1........................................RW 7-4 PnP Routing for
+	   Parallel Port IRQ (see PnP IRQ routing table) 3-0 PnP Routing for
+	   Floppy IRQ (see PnP IRQ routing table) */
+	pci_write_config_byte(dev, 0x51, 0x76);
+
+	/*
+	   Offset 52 - PNP IRQ Routing
+	   2........................................RW 7-4 PnP Routing for
+	   Serial Port 2 IRQ (see PnP IRQ routing table) 3-0 PnP Routing for
+	   Serial Port 1 IRQ (see PnP IRQ routing table) */
+	pci_write_config_byte(dev, 0x52, 0x34);
+
+	/*
+	   Offset 5A  KBC / RTC
+	   Control......................................RW Bits 7-4 of this
+	   register are latched from pins SD7-4 at power- up but are read/write
+	   accessible so may be changed after power-up to change the default
+	   strap setting: 7 Keyboard RP16........................... latched
+	   from SD7 6 Keyboard RP15 .......................... latched from SD6
+	   5 Keyboard RP14 .......................... latched from SD5 4
+	   Keyboard RP13 .......................... latched from SD4 3 Reserved
+	   ........................................ always reads 0 2 Internal
+	   RTC Enable 0 Disable 1 Enable
+	   ...................................................default 1 Internal
+	   PS2 Mouse Enable 0 Disable
+	   ..................................................default 1 Enable 0
+	   Internal KBC Enable 0 Disable
+	   ..................................................default 1 Enable */
+	pci_read_config_byte(dev, 0x5a, &bvalue);
+	pci_write_config_byte(dev, 0x5a, bvalue | 0x7);
+
+	/*
+	   Offset 61-60 - Distributed DMA Ch 0 Base / Enable ..... RW 15-4
+	   Channel 0 Base Address Bits 15-4.......... default = 0 3 Channel 0
+	   Enable 0 Disable
+	   ...................................................default 1 Enable
+	   2-0 Reserved ........................................ always reads 0 */
+	pci_write_config_word(dev, 0x60, 0x4);
+
+	/*
+	   Offset 63-62 - Distributed DMA Ch 1 Base / Enable ..... RW 15-4
+	   Channel 1 Base Address Bits 15-4.......... default = 0 3 Channel 1
+	   Enable 0 Disable
+	   ...................................................default 1 Enable
+	   2-0 Reserved ........................................ always reads 0 */
+	pci_write_config_word(dev, 0x62, 0x4);
+
+	/*
+	   Offset 65-64 - Distributed DMA Ch 2 Base / Enable ..... RW 15-4
+	   Channel 2 Base Address Bits 15-4.......... default = 0 3 Channel 2
+	   Enable 0 Disable
+	   ...................................................default 1 Enable
+	   2-0 Reserved ........................................ always reads 0 */
+	pci_write_config_word(dev, 0x64, 0x4);
+
+	/*
+	   Offset 67-66 - Distributed DMA Ch 3 Base / Enable ..... RW 15-4
+	   Channel 3 Base Address Bits 15-4.......... default = 0 3 Channel 3
+	   Enable 0 Disable
+	   ...................................................default 1 Enable
+	   2-0 Reserved ........................................ always reads 0 */
+	pci_write_config_word(dev, 0x66, 0x4);
+
+	/*
+	   Offset 77  GPIO Control 4 Control (10h)..................... RW 7
+	   DRQ / DACK# Pins are GPI / GPO 0
+	   Disable................................................... default 1
+	   Enable 6 Game Port XY Pins are GPI / GPO 0
+	   Disable................................................... default 1
+	   Enable 5 Reserved ........................................always
+	   reads 0 4 Internal APIC Enable 0 Disable 1 Enable (U10 = WSC#, V9 =
+	   APICD0, T10 =
+	   APICD1)................................................ default 3
+	   IRQ0 Output 0
+	   Disable................................................... default 1
+	   Enable IRQ0 output to GPIOC 2 RTC Rx32 Write Protect 0
+	   Disable................................................... default 1
+	   Enable 1 RTC Rx0D Write Protect 0
+	   Disable................................................... default 1
+	   Enable 0 GPO13 Enable (Pin U5) 0 Pin defined as
+	   SOE#.............................. default 1 Pin defined as GPO13 */
+	pci_write_config_byte(dev, 0x77, 0x40);	// Enable GPI22
+
+	/*
+	   Offset 81  ISA Positive Decoding Control 1..................RW 7
+	   On-Board I/O Port Positive Decoding 0 Disable
+	   ...................................................default 1 Enable
+	   6 Microsoft-Sound System I/O Port Positive Decoding 0 Disable
+	   ...................................................default 1 Enable
+	   5-4 Microsoft-Sound System I/O Decode Range 00 0530h-0537h
+	   ..........................................default 01 0604h-060Bh 10
+	   0E80-0E87h 11 0F40h-0F47h 3 APIC Positive Decoding 0 Disable
+	   ...................................................default 1 Enable
+	   2 BIOS ROM Positive Decoding 0 Disable
+	   ...................................................default 1 Enable
+	   1 Reserved ........................................ always reads 0 0
+	   PCS0 Positive Decoding 0 Disable
+	   ...................................................default 1 Enable */
+	pci_write_config_byte(dev, 0x81, 0xc0);
+
+	/*
+	   Offset 82  ISA Positive Decoding Control 2..................RW 7
+	   FDC Positive Decoding 0 Disable
+	   ...................................................default 1 Enable
+	   6 LPT Positive Decoding 0 Disable
+	   ...................................................default 1 Enable
+	   5-4 LPT Decode Range 00 3BCh-3BFh, 7BCh-7BEh
+	   ......................default 01 378h-37Fh, 778h-77Ah 10 278h-27Fh,
+	   678h-67Ah 11 -reserved- 3 Game Port Positive Decoding 0 Disable
+	   ...................................................default 1 Enable
+	   2 MIDI Positive Decoding 0 Disable
+	   ...................................................default 1 Enable
+	   1-0 MIDI Decode Range 00 300h-303h
+	   ..............................................default 01 310h-313h
+	   10 320h-323h 11 330h-333h */
+	pci_write_config_byte(dev, 0x82, 0xdc);
+
+	/*
+	   Offset 83  ISA Positive Decoding Control 3 ................. RW 7
+	   COM Port B Positive Decoding 0
+	   Disable................................................... default 1
+	   Enable 6-4 COM-Port B Decode Range 000 3F8h-3FFh
+	   (COM1)............................ default 001 2F8h-2FFh (COM2) 010
+	   220h-227h 011 228h-22Fh 100 238h-23Fh 101 2E8h-2EFh (COM4) 110
+	   338h-33Fh 111 3E8h-3EFh (COM3) 3 COM Port A Positive Decoding 0
+	   Disable................................................... default 1
+	   Enable 2-0 COM-Port A Decode Range 000 3F8h-3FFh
+	   (COM1)............................ default 001 2F8h-2FFh (COM2) 010
+	   220h-227h 011 228h-22Fh 100 238h-23Fh 101 2E8h-2EFh (COM4) 110
+	   338h-33Fh 111 3E8h-3EFh (COM3) */
+	pci_write_config_byte(dev, 0x83, 0x98);
+
+	/*
+	   Offset 84  ISA Positive Decoding Control 4 ................. RW 7-5
+	   Reserved ........................................always reads 0 4
+	   CD: Reserved.....................................always reads 0 CE:
+	   Port CF9 Positive Decoding 0 Disable 1
+	   Enable................................................... default 3
+	   FDC Decoding Range 0 Primary
+	   .................................................. default 1
+	   Secondary 2 Sound Blaster Positive Decoding 0
+	   Disable................................................... default 1
+	   Enable 1-0 Sound Blaster Decode Range 00 220h-22Fh, 230h-233h
+	   .......................... default 01 240h-24Fh, 250h-253h 10
+	   260h-26Fh, 270h-273h 11 280h-28Fh, 290h-293h */
+	pci_write_config_byte(dev, 0x84, 0x04);
+
+	/*
+	   Offset 85  Extended Function Enable............................RW
+	   7-6 PCI Master Grant Timeout Select 00 Disable
+	   ...................................................default 01 32 PCI
+	   Clocks 10 64 PCI Clocks 11 96 PCI Clocks 5 Keyboard Controller
+	   Configuration 0 Disable
+	   ...................................................default 1 Enable
+	   4 Function 3 USB Ports 2-3 0 Enable
+	   ....................................................default 1
+	   Disable 3 Function 6 Modem / Audio 0 Enable
+	   ....................................................default 1
+	   Disable 2 Function 5 Audio 0 Enable
+	   ....................................................default 1
+	   Disable 1 Super-I/O Configuration 0 Disable
+	   ...................................................default 1 Enable
+	   0 Super-I/O 0 Disable
+	   ...................................................default 1 Enable */
+	pci_write_config_byte(dev, 0x85, 0x23);
+
+	/*
+	   Index E0  Super-I/O Device ID (3Ch) ............................ RO
+	   7-0 Super-I/O ID ........................................ default =
+	   3Ch */
+	outb(0xe0, 0x3f0);
+	if (inb(0x3f1) != 0x3c)
+		printk("    ERROR: Super-I/O Device ID not found (read 0x%x, expected 0x3c)\n", inb(0x3f1));
+
+	/*
+	   Index E2  Super-I/O Function Select (03h)...................RW 7-5
+	   Reserved ........................................ always reads 0 4
+	   Floppy Controller Enable 0 Disable
+	   ...................................................default 1 Enable
+	   3 Serial Port 2 Enable 0 Disable
+	   ...................................................default 1 Enable
+	   2 Serial Port 1 Enable 0 Disable
+	   ...................................................default 1 Enable
+	   1-0 Parallel Port Mode / Enable 00 Unidirectional mode 01 ECP 10 EPP
+	   11 Parallel Port Disable ..............................default */
+	outb(0xe2, 0x3f0);
+	outb(0x1d, 0x3f1);
+
+	/* Set the floppy controller address */
+	outb(0xe3, 0x3f0);
+	outb(0x3f0 >> 2, 0x3f1);	/* PC Legacy default is 0x3f0 */
+
+	/* Set the LTP port address */
+	outb(0xe6, 0x3f0);
+	outb(0x378 >> 2, 0x3f1);	/* PC Legacy default is 0x378 */
+
+	/* Set the Serail Port 1 port address */
+	outb(0xe7, 0x3f0);
+	outb(0x3f8 >> 2, 0x3f1);	/* PC Legacy default is 0x3f8 */
+
+	/* Set the Serail Port 2 port address */
+	outb(0xe8, 0x3f0);
+	outb(0x2f8 >> 2, 0x3f1);	/* PC Legacy default is 0x2f8 */
+
+	/*
+	   Index F6  Floppy Controller Configuration................. RW 7-6
+	   Reserved ........................................always reads 0 5
+	   Floppy Drive On Parallel Port 0 Parallel Port (SPP) Mode
+	   ...................... default 1 FDC Mode 4 3-Mode FDD 0
+	   Disable................................................... default 1
+	   Enable 3 Reserved ........................................always
+	   reads 0 2 Four Floppy Drive Option 0 Internal 2-Drive
+	   Decoder....................... default 1 External 4-Drive Decoder 1
+	   FDC DMA Non-Burst 0 Burst
+	   .................................................... default 1
+	   Non-Burst 0 FDC Swap 0
+	   Disable................................................... default 1
+	   Enable */
+	outb(0xf6, 0x3f0);
+	outb(0x20, 0x3f1);
+
+	pci_write_config_byte(dev, 0x85, 0x21);
+	dev->irq = 80 + 10;
+}
+
+DECLARE_PCI_FIXUP_ENABLE(PCI_VENDOR_ID_VIA, PCI_DEVICE_ID_VIA_82C686,
+			 chip_vt82c686b_setup);
+
+
+/**
+ * Perform the setup for the Via IDE
+ *
+ * @param dev
+ */
+static void chip_vt82c686b_ide_setup(struct pci_dev *dev)
+{
+	uint8_t bvalue;
+	pci_read_config_byte(dev, 0x40, &bvalue);
+	pci_write_config_byte(dev, 0x40, bvalue | 3);
+	pci_write_config_byte(dev, 0x04, 0x87);	// Enable Bus Master I/O
+						// space,Memory space
+	pci_write_config_byte(dev, 0x0d, 0x40);	// Latency Timer
+	pci_write_config_byte(dev, 0x3c, 0x0e);	// Interrupt Line
+	pci_read_config_byte(dev, 0x41, &bvalue);
+	pci_write_config_byte(dev, 0x41, bvalue | 0xc0);	// enable read
+								// prefectch/post
+								// write buffer
+	pci_write_config_byte(dev, 0x43, 0x05);	// FIFO configuration 1/2
+	pci_write_config_byte(dev, 0x44, 0x1c);	// Miscellaneous Control
+	pci_write_config_byte(dev, 0x45, 0x00);	// Miscellaneous Control
+
+	pci_write_config_byte(dev, 0x46, 0xc0);	// Disable DMA FIFO flush
+	pci_write_config_dword(dev, 0x48, 0xa8a8a8a8);	// Drive Timing Control
+	pci_write_config_byte(dev, 0x4c, 0xff);	// Address Setup Time
+
+	pci_write_config_dword(dev, 0x50, 0x07070707);	// UltraDMA Extended
+							// Timing Control
+	pci_write_config_byte(dev, 0x54, 0x04);	// UltraDMA FIFO control
+
+	pci_write_config_word(dev, 0x04, 0x87);	// Eanable I/O space, Bus
+						// Master
+
+	pci_write_config_byte(dev, 0x3c, 80 + 14);
+	dev->irq = 80 + 14;
+}
+
+DECLARE_PCI_FIXUP_ENABLE(PCI_VENDOR_ID_VIA, PCI_DEVICE_ID_VIA_82C586_1,
+			 chip_vt82c686b_ide_setup);
+
+
+/**
+ * We need to change the Via USB to use the 8259 interrupt
+ * instead of the normal PCI interrrupts.
+ *
+ * @param dev
+ */
+static void chip_vt82c686b_usb_setup(struct pci_dev *dev)
+{
+	pci_write_config_byte(dev, 0x3c, 80 + 13);
+	dev->irq = 80 + 13;
+}
+
+DECLARE_PCI_FIXUP_FINAL(PCI_VENDOR_ID_VIA, PCI_DEVICE_ID_VIA_82C586_2,
+			chip_vt82c686b_usb_setup);
+
+
+/**
+ * Perform the Via IDE setup for DMA modes.
+ *
+ * @param dev
+ */
+static void chip_vt82c686b_pmio_setup(struct pci_dev *dev)
+{
+	uint32_t base;
+	u8 val;
+
+	pci_write_config_word(dev, 0x041, 0x80);	// Eanble PM/IO base
+	pci_write_config_word(dev, 0x048, 0x500);	// Set PM/IO base to
+							// 0x500
+
+	pci_read_config_dword(dev, 0x48, &base);	// read PM/IO base
+	base = base & 0xff00;
+	val = inb(base + 0x4a);	// bit 6 is IDE cable type 0:80w, 1:40w
+	outb(0x10, 0x70);	// write to CMOS offset=0x10
+	outb(val, 0x71);
+	outb(0x10, 0x70);
+	val = inb(0x71);
+}
+
+DECLARE_PCI_FIXUP_ENABLE(PCI_VENDOR_ID_VIA, PCI_DEVICE_ID_VIA_82C686_4,
+			 chip_vt82c686b_pmio_setup);
+
+
+/**
+ * Optimize the PLX6540 bridges to take advantage of their
+ * internal buffering.
+ *
+ * @param dev
+ */
+static void chip_plx6540_optimize(struct pci_dev *dev)
+{
+	uint8_t buffer_control;
+	pci_write_config_byte(dev, 0x48, (2 << 1) /* PCIX 4 cache lines */ |(5 << 3)	/* PCI
+											   20
+											   dwords
+					 */ );
+					/* Primary bus prefetch */
+	pci_write_config_byte(dev, 0x4a, (8 << 2) /* 32 dwords */ );	/* Primary
+									   Incremental
+									   Prefetch
+									   Count
+									 */
+	pci_write_config_byte(dev, 0x4c, 96 /* max dwords prefetch */ );	/* Primary
+										   Maximum
+										   Prefetch
+										   Count
+										 */
+	pci_read_config_byte(dev, 0x4f, &buffer_control);
+	buffer_control |= 1 << 1;	/* Enable smart prefetch */
+	buffer_control |= 1 << 2;	/* Divide fifos into four */
+	pci_write_config_byte(dev, 0x4f, buffer_control);
+}
+
+DECLARE_PCI_FIXUP_ENABLE(PCI_VENDOR_ID_PLX, 0x6540, chip_plx6540_optimize);
diff --git a/arch/mips/cavium-octeon/pci_console.c b/arch/mips/cavium-octeon/pci_console.c
new file mode 100644
index 0000000..69869f6
--- /dev/null
+++ b/arch/mips/cavium-octeon/pci_console.c
@@ -0,0 +1,327 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2006-2007 Cavium Networks
+ */
+#include <linux/console.h>
+#include <linux/tty.h>
+#include <linux/tty_driver.h>
+#include <linux/tty_flip.h>
+#undef OCTEON_MODEL
+#define USE_RUNTIME_MODEL_CHECKS 1
+#include "hal.h"
+#include "cvmx-bootmem.h"
+#include "octeon-pci-console.h"
+
+typedef struct {
+	struct console con;
+	struct tty_driver *ttydrv;
+	struct timer_list poll_timer;
+	int open_count;
+	int index;
+} pci_console_state_t;
+
+static pci_console_state_t pci_console_state;
+static struct tty_operations octeon_pci_tty_ops;
+static spinlock_t pci_console_lock;
+static uint64_t pci_console_base_address;
+
+
+/**
+ * Get the pci console state from a struct console
+ *
+ * @param con    struct console to get console for
+ *
+ * @return The console state
+ */
+static inline pci_console_state_t *get_state_con(struct console *con)
+{
+	return (pci_console_state_t *) con->data;
+}
+
+
+/**
+ * Get the pci console state from a tty
+ *
+ * @param tty    tty to get console for
+ *
+ * @return The console state
+ */
+static inline pci_console_state_t *get_state_tty(struct tty_struct *tty)
+{
+	return (pci_console_state_t *) tty->driver->driver_state;
+}
+
+
+/**
+ * Low level kernel write to the PCI console
+ *
+ * @param console_num
+ *               Console to write to
+ * @param str    String to write
+ * @param len    Length of the string
+ */
+static void pci_console_lowlevel_write(int console_num, const char *str,
+				       unsigned len)
+{
+	unsigned long flags;
+	spin_lock_irqsave(&pci_console_lock, flags);
+	while (len > 0) {
+		int written =
+			octeon_pci_console_write(pci_console_base_address,
+						 console_num, str, len,
+						 OCT_PCI_CON_FLAG_NONBLOCK);
+		if (written > 0) {
+			str += written;
+			len -= written;
+		}
+	}
+	spin_unlock_irqrestore(&pci_console_lock, flags);
+}
+
+
+/**
+ * Kernel write to the PCI console
+ *
+ * @param con    Console to write to
+ * @param str    String to write
+ * @param len    Length of the string
+ */
+static void pci_console_write(struct console *con, const char *str,
+			      unsigned len)
+{
+	pci_console_lowlevel_write(get_state_con(con)->index, str, len);
+}
+
+
+/**
+ * Get a TTY driver for the console device. Used to allow
+ * userspace to write to the kernel's console.
+ *
+ * @param con    Kernel's console
+ * @param index  Which console index
+ * @return TTY driver for userspace. NULL on failure.
+ */
+static struct tty_driver *pci_console_device(struct console *con, int *index)
+{
+	pci_console_state_t *console_state = get_state_con(con);
+	*index = 0;
+	return console_state->ttydrv;
+}
+
+
+/**
+ * Called by Linux when the console=string is parsed
+ *
+ * @param con    Kernel's console
+ * @param arg    Argument string
+ * @return Zero on success
+ */
+static int pci_console_setup(struct console *con, char *arg)
+{
+	octeon_write_lcd("pci cons");
+	if (pci_console_base_address == 0) {
+		cvmx_bootmem_named_block_desc_t *block_desc =
+			cvmx_bootmem_find_named_block
+			(OCTEON_PCI_CONSOLE_BLOCK_NAME);
+		if (block_desc == NULL) {
+			octeon_write_lcd("pci fail");
+			return -1;
+		}
+		pci_console_base_address = block_desc->base_addr;
+	}
+	return 0;
+}
+
+
+/**
+ * Initialize the PCI console for use
+ */
+void pci_console_init(const char *arg)
+{
+	memset(&pci_console_state, 0, sizeof(pci_console_state));
+	strcpy(pci_console_state.con.name, "pci");
+	pci_console_state.con.write = pci_console_write;
+	pci_console_state.con.device = pci_console_device;
+	pci_console_state.con.setup = pci_console_setup;
+	pci_console_state.con.data = &pci_console_state;
+	if (arg && (arg[3] >= '0') && (arg[3] <= '9'))
+		sscanf(arg + 3, "%d", &pci_console_state.index);
+	else
+		pci_console_state.index = 0;
+	register_console(&pci_console_state.con);
+}
+
+
+/**
+ * Called by a timer to poll the PCI device for input data
+ *
+ * @param arg    Pointer to the TTY structure
+ */
+static void pci_tty_read_poll(unsigned long arg)
+{
+	struct tty_struct *tty = (struct tty_struct *) arg;
+	int index = get_state_tty(tty)->index;
+	unsigned long flags;
+	int count;
+	spin_lock_irqsave(&pci_console_lock, flags);
+	count = octeon_pci_console_read_avail(pci_console_base_address, index);
+	if (count > 0) {
+		char buffer[count];
+		count = octeon_pci_console_read(pci_console_base_address, index,
+						buffer, sizeof(buffer),
+						OCT_PCI_CON_FLAG_NONBLOCK);
+		tty_insert_flip_string(tty, buffer, count);
+		tty_flip_buffer_push(tty);
+	}
+	spin_unlock_irqrestore(&pci_console_lock, flags);
+	mod_timer(&get_state_tty(tty)->poll_timer, jiffies + 1);
+}
+
+
+/**
+ * Called when userspace opens the TTY device. Can be called
+ * multiple times.
+ *
+ * @param tty    Device to open
+ * @param filp
+ * @return Zero on success
+ */
+static int pci_tty_open(struct tty_struct *tty, struct file *filp)
+{
+	pci_console_state_t *console_state = get_state_tty(tty);
+	console_state->open_count++;
+	if (console_state->open_count == 1) {
+		init_timer(&console_state->poll_timer);
+		console_state->poll_timer.data = (unsigned long) tty;
+		console_state->poll_timer.function = pci_tty_read_poll;
+		mod_timer(&console_state->poll_timer, jiffies + 1);
+	}
+	return 0;
+}
+
+
+/**
+ * Called when userspace closes the console TTY
+ *
+ * @param tty    TTY to close
+ * @param filp
+ */
+static void pci_tty_close(struct tty_struct *tty, struct file *filp)
+{
+	pci_console_state_t *console_state = get_state_tty(tty);
+	console_state->open_count--;
+	if (console_state->open_count == 0)
+		del_timer(&console_state->poll_timer);
+}
+
+
+/**
+ * Called when usersapce does a block write
+ *
+ * @param tty    TTY to write too
+ * @param buf    Data to write
+ * @param count  number of bytes
+ * @return Number of bytes written
+ */
+static int pci_tty_write(struct tty_struct *tty, const unsigned char *buf,
+			 int count)
+{
+	pci_console_lowlevel_write(get_state_tty(tty)->index, buf, count);
+	return count;
+}
+
+
+/**
+ * Write a single character
+ *
+ * @param tty    TTY to write to
+ * @param ch     Character to write
+ */
+static void pci_tty_put_char(struct tty_struct *tty, unsigned char ch)
+{
+	pci_console_lowlevel_write(get_state_tty(tty)->index, &ch, 1);
+}
+
+
+/**
+ * Write a single character
+ *
+ * @param tty    TTY to write to
+ * @param ch     Character to write
+ */
+static void pci_tty_send_xchar(struct tty_struct *tty, char ch)
+{
+	pci_console_lowlevel_write(get_state_tty(tty)->index, &ch, 1);
+}
+
+
+/**
+ * Determine the amount of room available for output
+ *
+ * @param tty    TTY structure
+ * @return Number of bytes
+ */
+static int pci_tty_write_room(struct tty_struct *tty)
+{
+	unsigned long flags;
+	int count;
+	spin_lock_irqsave(&pci_console_lock, flags);
+	count = octeon_pci_console_write_avail(pci_console_base_address,
+					       get_state_tty(tty)->index);
+	spin_unlock_irqrestore(&pci_console_lock, flags);
+	if (count)
+		return count;
+	else
+		return 0;
+}
+
+
+/**
+ * Return the number of characters pending. Needed for vi to work.
+ *
+ * @param tty    TTY structure
+ *
+ * @return Number of bytes
+ */
+static int pci_tty_chars_in_buffer(struct tty_struct *tty)
+{
+	return 0;
+}
+
+static __init int pci_console_module_init(void)
+{
+	pci_console_state.ttydrv = alloc_tty_driver(1);
+	if (!pci_console_state.ttydrv)
+		return 0;
+
+	pci_console_state.ttydrv->owner = THIS_MODULE;
+	pci_console_state.ttydrv->driver_name = "pci_console";
+	pci_console_state.ttydrv->name = "ttyPCI";
+	pci_console_state.ttydrv->type = TTY_DRIVER_TYPE_SERIAL;
+	pci_console_state.ttydrv->subtype = SERIAL_TYPE_NORMAL;
+	pci_console_state.ttydrv->flags = TTY_DRIVER_REAL_RAW;
+	pci_console_state.ttydrv->major = 4;
+	pci_console_state.ttydrv->minor_start = 96;
+	pci_console_state.ttydrv->init_termios = tty_std_termios;
+	pci_console_state.ttydrv->init_termios.c_cflag =
+		B9600 | CS8 | CREAD | HUPCL | CLOCAL;
+	pci_console_state.ttydrv->driver_state = &pci_console_state;
+	tty_set_operations(pci_console_state.ttydrv, &octeon_pci_tty_ops);
+	tty_register_driver(pci_console_state.ttydrv);
+	return 0;
+}
+
+module_init(pci_console_module_init);
+
+static struct tty_operations octeon_pci_tty_ops = {
+	.open = pci_tty_open,
+	.close = pci_tty_close,
+	.write = pci_tty_write,
+	.put_char = pci_tty_put_char,
+	.write_room = pci_tty_write_room,
+	.send_xchar = pci_tty_send_xchar,
+	.chars_in_buffer = pci_tty_chars_in_buffer,
+};
diff --git a/arch/mips/cavium-octeon/pcie.c b/arch/mips/cavium-octeon/pcie.c
new file mode 100644
index 0000000..47c4759
--- /dev/null
+++ b/arch/mips/cavium-octeon/pcie.c
@@ -0,0 +1,318 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2007 Cavium Networks
+ */
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/pci.h>
+#include <linux/interrupt.h>
+#include <asm/time.h>
+#include <asm/delay.h>
+#include "hal.h"
+#include "cvmx-pcie.h"
+#include "pci-common.h"
+
+
+/**
+ * Map a PCI device to the appropriate interrupt line
+ *
+ * @param dev    The Linux PCI device structure for the device to map
+ * @param slot   The slot number for this device on __BUS 0__. Linux
+ *               enumerates through all the bridges and figures out the
+ *               slot on Bus 0 where this device eventually hooks to.
+ * @param pin    The PCI interrupt pin read from the device, then swizzled
+ *               as it goes through each bridge.
+ * @return Interrupt number for the device
+ */
+int __init octeon_pcie_pcibios_map_irq(struct pci_dev *dev, u8 slot, u8 pin)
+{
+	return ((slot + pin - 3) & 3) + 44;	/* 44 is the irq number of PCI
+						   INT-A */
+}
+
+/**
+ * Read a value from configuration space
+ *
+ * @param bus
+ * @param devfn
+ * @param reg
+ * @param size
+ * @param val
+ * @return
+ */
+static inline int octeon_pcie_read_config(int pcie_port, struct pci_bus *bus,
+					  unsigned int devfn, int reg, int size,
+					  u32 * val)
+{
+	octeon_cvmemctl_t cvmmemctl;
+	octeon_cvmemctl_t cvmmemctl_save;
+	int bus_number = bus->number;
+	/* We need to force the bus number to be zero on the root bus. Linux
+	   numbers the 2nd root bus to start after all busses on root 0 */
+	if (bus->parent == NULL)
+		bus_number = 0;
+
+	/* PCIe only has a single device on the first bus */
+	if ((bus_number == 0) && (devfn >> 3 != 0))
+		return PCIBIOS_FUNC_NOT_SUPPORTED;
+
+	/* Hacks to get around config bus errors */
+	if ((bus_number == 0) && (devfn >= 2))
+		return PCIBIOS_FUNC_NOT_SUPPORTED;
+	if ((bus_number == 1) && (devfn >> 3 != 2))
+		return PCIBIOS_FUNC_NOT_SUPPORTED;
+
+#if PCI_CONFIG_SPACE_DELAY
+	udelay(PCI_CONFIG_SPACE_DELAY);
+#endif
+	// printk("octeon_pcie_read_config(pcie_port=%d, bus=%d, devfn=0x%x,
+	// reg=0x%x, size=%d, val=%p)\n",
+	// pcie_port, bus_number, devfn, reg, size, val);
+
+	/* Shorten the DID timeout so bus errors for PCIe config reads from non
+	   existent devices happen faster. CN57XX, CN56XX, CN55XX, and CN54XX
+	   pass 1.0 and 1.1 have an errata where reads from non existent
+	   devices can cause a bus errors instead of returning 0xffffffff */
+	cvmmemctl_save.u64 = __read_64bit_c0_register($11, 7);
+	cvmmemctl.u64 = cvmmemctl_save.u64;
+	cvmmemctl.s.didtto = 2;
+	__write_64bit_c0_register($11, 7, cvmmemctl.u64);
+
+	switch (size) {
+	case 4:
+		*val = cvmx_pcie_config_read32(pcie_port, bus_number,
+					       devfn >> 3, devfn & 0x7, reg);
+		// printk("octeon_pcie_read_config val=0x%x\n", *val);
+		__write_64bit_c0_register($11, 7, cvmmemctl_save.u64);
+		return PCIBIOS_SUCCESSFUL;
+	case 2:
+		*val = cvmx_pcie_config_read16(pcie_port, bus_number,
+					       devfn >> 3, devfn & 0x7, reg);
+		// printk("octeon_pcie_read_config val=0x%x\n", *val);
+		__write_64bit_c0_register($11, 7, cvmmemctl_save.u64);
+		return PCIBIOS_SUCCESSFUL;
+	case 1:
+		*val = cvmx_pcie_config_read8(pcie_port, bus_number, devfn >> 3,
+					      devfn & 0x7, reg);
+		// printk("octeon_pcie_read_config val=0x%x\n", *val);
+		__write_64bit_c0_register($11, 7, cvmmemctl_save.u64);
+		return PCIBIOS_SUCCESSFUL;
+	}
+	return PCIBIOS_FUNC_NOT_SUPPORTED;
+}
+
+static int octeon_pcie0_read_config(struct pci_bus *bus, unsigned int devfn,
+				    int reg, int size, u32 * val)
+{
+	return octeon_pcie_read_config(0, bus, devfn, reg, size, val);
+}
+
+static int octeon_pcie1_read_config(struct pci_bus *bus, unsigned int devfn,
+				    int reg, int size, u32 * val)
+{
+	return octeon_pcie_read_config(1, bus, devfn, reg, size, val);
+}
+
+
+
+/**
+ * Write a value to PCI configuration space
+ *
+ * @param bus
+ * @param devfn
+ * @param reg
+ * @param size
+ * @param val
+ * @return
+ */
+static inline int octeon_pcie_write_config(int pcie_port, struct pci_bus *bus,
+					   unsigned int devfn, int reg,
+					   int size, u32 val)
+{
+	int bus_number = bus->number;
+	/* We need to force the bus number to be zero on the root bus. Linux
+	   numbers the 2nd root bus to start after all busses on root 0 */
+	if (bus->parent == NULL)
+		bus_number = 0;
+
+#if PCI_CONFIG_SPACE_DELAY
+	udelay(PCI_CONFIG_SPACE_DELAY);
+#endif
+	// printk("octeon_pcie_write_config(pcie_port=%d, bus=%d, devfn=0x%x,
+	// reg=0x%x, size=%d, val=0x%x)\n",
+	// pcie_port, bus_number, devfn, reg, size, val);
+	switch (size) {
+	case 4:
+		cvmx_pcie_config_write32(pcie_port, bus_number, devfn >> 3,
+					 devfn & 0x7, reg, val);
+		return PCIBIOS_SUCCESSFUL;
+	case 2:
+		cvmx_pcie_config_write16(pcie_port, bus_number, devfn >> 3,
+					 devfn & 0x7, reg, val);
+		return PCIBIOS_SUCCESSFUL;
+	case 1:
+		cvmx_pcie_config_write8(pcie_port, bus_number, devfn >> 3,
+					devfn & 0x7, reg, val);
+		return PCIBIOS_SUCCESSFUL;
+	}
+	return PCIBIOS_FUNC_NOT_SUPPORTED;
+}
+
+static int octeon_pcie0_write_config(struct pci_bus *bus, unsigned int devfn,
+				     int reg, int size, u32 val)
+{
+	return octeon_pcie_write_config(0, bus, devfn, reg, size, val);
+}
+
+static int octeon_pcie1_write_config(struct pci_bus *bus, unsigned int devfn,
+				     int reg, int size, u32 val)
+{
+	return octeon_pcie_write_config(1, bus, devfn, reg, size, val);
+}
+
+static struct pci_ops octeon_pcie0_ops = {
+	octeon_pcie0_read_config,
+	octeon_pcie0_write_config,
+};
+
+static struct resource octeon_pcie0_mem_resource = {
+	.name = "Octeon PCIe0 MEM",
+	.flags = IORESOURCE_MEM,
+};
+
+static struct resource octeon_pcie0_io_resource = {
+	.name = "Octeon PCIe0 IO",
+	.flags = IORESOURCE_IO,
+};
+
+static struct pci_controller octeon_pcie0_controller = {
+	.pci_ops = &octeon_pcie0_ops,
+	.mem_resource = &octeon_pcie0_mem_resource,
+	.io_resource = &octeon_pcie0_io_resource,
+};
+
+static struct pci_ops octeon_pcie1_ops = {
+	octeon_pcie1_read_config,
+	octeon_pcie1_write_config,
+};
+
+static struct resource octeon_pcie1_mem_resource = {
+	.name = "Octeon PCIe1 MEM",
+	.flags = IORESOURCE_MEM,
+};
+
+static struct resource octeon_pcie1_io_resource = {
+	.name = "Octeon PCIe1 IO",
+	.flags = IORESOURCE_IO,
+};
+
+static struct pci_controller octeon_pcie1_controller = {
+	.pci_ops = &octeon_pcie1_ops,
+	.mem_resource = &octeon_pcie1_mem_resource,
+	.io_resource = &octeon_pcie1_io_resource,
+};
+
+
+/**
+ * Initialize the Octeon PCIe controllers
+ *
+ * @return
+ */
+static int __init octeon_pcie_setup(void)
+{
+	cvmx_npei_ctl_status_t npei_ctl_status;
+	int result;
+
+	/* These chips don't have PCIe */
+	if (!octeon_has_feature(OCTEON_FEATURE_PCIE))
+		return 0;
+
+	/* Point pcibios_map_irq() to the PCIe version of it */
+	octeon_pcibios_map_irq = octeon_pcie_pcibios_map_irq;
+
+	/* Use the PCIe based DMA mappings */
+	octeon_dma_bar_type = OCTEON_DMA_BAR_TYPE_PCIE;
+
+	/* Mark the Octeon CSR region as IO */
+	iomem_resource.start = (1ull << 48);
+	iomem_resource.end = (2ull << 48) - 1;
+
+	/* PCIe I/O range. It is based on port 0 but includes up until port 1's
+	   end */
+	set_io_port_base(CVMX_ADD_IO_SEG(cvmx_pcie_get_io_base_address(0)));
+	ioport_resource.start = 0;
+	ioport_resource.end =
+		cvmx_pcie_get_io_base_address(1) -
+		cvmx_pcie_get_io_base_address(0) + cvmx_pcie_get_io_size(1) - 1;
+
+	npei_ctl_status.u64 = cvmx_read_csr(CVMX_PEXP_NPEI_CTL_STATUS);
+	if (npei_ctl_status.s.host_mode) {
+		printk("PCIe: Initializing port 0\n");
+		result = cvmx_pcie_rc_initialize(0);
+		if (result == 0) {
+			/* Memory offsets are physical addresses */
+			octeon_pcie0_controller.mem_offset =
+				cvmx_pcie_get_mem_base_address(0);
+			/* IO offsets are Mips virtual addresses */
+			octeon_pcie0_controller.io_map_base =
+				CVMX_ADD_IO_SEG(cvmx_pcie_get_io_base_address
+						(0));
+			octeon_pcie0_controller.io_offset = 0;
+			/* To keep things similar to PCI, we start device
+			   addresses at the same place as PCI uisng big bar
+			   support. This normally translates to 4GB-256MB,
+			   which is the same as most x86 PCs */
+			octeon_pcie0_controller.mem_resource->start =
+				cvmx_pcie_get_mem_base_address(0) +
+				(4ul << 30) - (OCTEON_PCI_BAR1_HOLE_SIZE << 20);
+			octeon_pcie0_controller.mem_resource->end =
+				cvmx_pcie_get_mem_base_address(0) +
+				cvmx_pcie_get_mem_size(0) - 1;
+			/* Ports must be above 16KB for the ISA bus filtering
+			   in the PCI-X to PCI bridge */
+			octeon_pcie0_controller.io_resource->start = 4 << 10;
+			octeon_pcie0_controller.io_resource->end =
+				cvmx_pcie_get_io_size(0) - 1;
+			register_pci_controller(&octeon_pcie0_controller);
+		}
+	} else
+		printk("PCIe: Port 0 in endpoint mode, skipping.\n");
+
+	printk("PCIe: Initializing port 1\n");
+	result = cvmx_pcie_rc_initialize(1);
+	if (result == 0) {
+		/* Memory offsets are physical addresses */
+		octeon_pcie1_controller.mem_offset =
+			cvmx_pcie_get_mem_base_address(1);
+		/* IO offsets are Mips virtual addresses */
+		octeon_pcie1_controller.io_map_base =
+			CVMX_ADD_IO_SEG(cvmx_pcie_get_io_base_address(1));
+		octeon_pcie1_controller.io_offset =
+			cvmx_pcie_get_io_base_address(1) -
+			cvmx_pcie_get_io_base_address(0);
+		/* To keep things similar to PCI, we start device addresses at
+		   the same place as PCI uisng big bar support. This normally
+		   translates to 4GB-256MB, which is the same as most x86 PCs */
+		octeon_pcie1_controller.mem_resource->start =
+			cvmx_pcie_get_mem_base_address(1) + (4ul << 30) -
+			(OCTEON_PCI_BAR1_HOLE_SIZE << 20);
+		octeon_pcie1_controller.mem_resource->end =
+			cvmx_pcie_get_mem_base_address(1) +
+			cvmx_pcie_get_mem_size(1) - 1;
+		/* Ports must be above 16KB for the ISA bus filtering in the
+		   PCI-X to PCI bridge */
+		octeon_pcie1_controller.io_resource->start =
+			cvmx_pcie_get_io_base_address(1) -
+			cvmx_pcie_get_io_base_address(0);
+		octeon_pcie1_controller.io_resource->end =
+			octeon_pcie1_controller.io_resource->start +
+			cvmx_pcie_get_io_size(1) - 1;
+		register_pci_controller(&octeon_pcie1_controller);
+	}
+	return 0;
+}
+
+arch_initcall(octeon_pcie_setup);
diff --git a/arch/mips/cavium-octeon/perf_counters.c b/arch/mips/cavium-octeon/perf_counters.c
new file mode 100644
index 0000000..3b6939c
--- /dev/null
+++ b/arch/mips/cavium-octeon/perf_counters.c
@@ -0,0 +1,786 @@
+/*
+ * Simple /proc interface to the Octeon Performance Counters
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2004-2007 Cavium Networks
+ */
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/fs.h>
+#include <linux/seq_file.h>
+#include <linux/proc_fs.h>
+#include <asm/ioctl.h>
+#include <asm/mach-cavium-octeon/perf_counters.h>
+#include "hal.h"
+
+/**
+ * The types of counters supported per cpu
+ */
+typedef enum {
+	PROC_PERF_CORE_NONE = 0,	/* Turn off the performance counter */
+	PROC_PERF_CORE_CLK = 1,	/* Conditionally clocked cycles (as opposed to
+				   count/cvm_count which count even with no
+				   clocks) */
+	PROC_PERF_CORE_ISSUE = 2,	/* Instructions issued but not retired */
+	PROC_PERF_CORE_RET = 3,	/* Instructions retired */
+	PROC_PERF_CORE_NISSUE = 4,	/* Cycles no issue */
+	PROC_PERF_CORE_SISSUE = 5,	/* Cycles single issue */
+	PROC_PERF_CORE_DISSUE = 6,	/* Cycles dual issue */
+	PROC_PERF_CORE_IFI = 7,	/* Cycle ifetch issued (but not necessarily
+				   commit to pp_mem) */
+	PROC_PERF_CORE_BR = 8,	/* Branches retired */
+	PROC_PERF_CORE_BRMIS = 9,	/* Branch mispredicts */
+	PROC_PERF_CORE_J = 10,	/* Jumps retired */
+	PROC_PERF_CORE_JMIS = 11,	/* Jumps mispredicted */
+	PROC_PERF_CORE_REPLAY = 12,	/* Mem Replays */
+	PROC_PERF_CORE_IUNA = 13,	/* Cycles idle due to unaligned_replays
+					 */
+	PROC_PERF_CORE_TRAP = 14,	/* trap_6a signal */
+	PROC_PERF_CORE_UULOAD = 16,	/* Unexpected unaligned loads (REPUN=1)
+					 */
+	PROC_PERF_CORE_UUSTORE = 17,	/* Unexpected unaligned store (REPUN=1)
+					 */
+	PROC_PERF_CORE_ULOAD = 18,	/* Unaligned loads (REPUN=1 or USEUN=1)
+					 */
+	PROC_PERF_CORE_USTORE = 19,	/* Unaligned store (REPUN=1 or USEUN=1)
+					 */
+	PROC_PERF_CORE_EC = 20,	/* Exec clocks(must set CvmCtl[DISCE] for
+				   accurate timing) */
+	PROC_PERF_CORE_MC = 21,	/* Mul clocks(must set CvmCtl[DISCE] for
+				   accurate timing) */
+	PROC_PERF_CORE_CC = 22,	/* Crypto clocks(must set CvmCtl[DISCE] for
+				   accurate timing) */
+	PROC_PERF_CORE_CSRC = 23,	/* Issue_csr clocks(must set
+					   CvmCtl[DISCE] for accurate timing) */
+	PROC_PERF_CORE_CFETCH = 24,	/* Icache committed fetches
+					   (demand+prefetch) */
+	PROC_PERF_CORE_CPREF = 25,	/* Icache committed prefetches */
+	PROC_PERF_CORE_ICA = 26,	/* Icache aliases */
+	PROC_PERF_CORE_II = 27,	/* Icache invalidates */
+	PROC_PERF_CORE_IP = 28,	/* Icache parity error */
+	PROC_PERF_CORE_CIMISS = 29,	/* Cycles idle due to imiss (must set
+					   CvmCtl[DISCE] for accurate timing) */
+	PROC_PERF_CORE_WBUF = 32,	/* Number of write buffer entries
+					   created */
+	PROC_PERF_CORE_WDAT = 33,	/* Number of write buffer data cycles
+					   used (may need to set CvmCtl[DISCE]
+					   for accurate counts) */
+	PROC_PERF_CORE_WBUFLD = 34,	/* Number of write buffer entries
+					   forced out by loads */
+	PROC_PERF_CORE_WBUFFL = 35,	/* Number of cycles that there was no
+					   available write buffer entry (may
+					   need to set CvmCtl[DISCE] and
+					   CvmMemCtl[MCLK] for accurate counts)
+					 */
+	PROC_PERF_CORE_WBUFTR = 36,	/* Number of stores that found no
+					   available write buffer entries */
+	PROC_PERF_CORE_BADD = 37,	/* Number of address bus cycles used
+					   (may need to set CvmCtl[DISCE] for
+					   accurate counts) */
+	PROC_PERF_CORE_BADDL2 = 38,	/* Number of address bus cycles not
+					   reflected (i.e. destined for L2)
+					   (may need to set CvmCtl[DISCE] for
+					   accurate counts) */
+	PROC_PERF_CORE_BFILL = 39,	/* Number of fill bus cycles used (may
+					   need to set CvmCtl[DISCE] for
+					   accurate counts) */
+	PROC_PERF_CORE_DDIDS = 40,	/* Number of Dstream DIDs created */
+	PROC_PERF_CORE_IDIDS = 41,	/* Number of Istream DIDs created */
+	PROC_PERF_CORE_DIDNA = 42,	/* Number of cycles that no DIDs were
+					   available (may need to set
+					   CvmCtl[DISCE] and CvmMemCtl[MCLK]
+					   for accurate counts) */
+	PROC_PERF_CORE_LDS = 43,	/* Number of load issues */
+	PROC_PERF_CORE_LMLDS = 44,	/* Number of local memory load */
+	PROC_PERF_CORE_IOLDS = 45,	/* Number of I/O load issues */
+	PROC_PERF_CORE_DMLDS = 46,	/* Number of loads that were not
+					   prefetches and missed in the cache */
+	PROC_PERF_CORE_STS = 48,	/* Number of store issues */
+	PROC_PERF_CORE_LMSTS = 49,	/* Number of local memory store issues */
+	PROC_PERF_CORE_IOSTS = 50,	/* Number of I/O store issues */
+	PROC_PERF_CORE_IOBDMA = 51,	/* Number of IOBDMAs */
+	PROC_PERF_CORE_DTLB = 53,	/* Number of dstream TLB refill,
+					   invalid, or modified exceptions */
+	PROC_PERF_CORE_DTLBAD = 54,	/* Number of dstream TLB address errors
+					 */
+	PROC_PERF_CORE_ITLB = 55,	/* Number of istream TLB refill,
+					   invalid, or address error exceptions
+					 */
+	PROC_PERF_CORE_SYNC = 56,	/* Number of SYNC stall cycles (may
+					   need to set CvmCtl[DISCE] for
+					   accurate counts) */
+	PROC_PERF_CORE_SYNCIOB = 57,	/* Number of SYNCIOBDMA stall cycles
+					   (may need to set CvmCtl[DISCE] for
+					   accurate counts) */
+	PROC_PERF_CORE_SYNCW = 58,	/* Number of SYNCWs */
+	PROC_PERF_CORE_MAX
+} proc_perf_core_t;
+
+/**
+ * The types of counters supported for L2
+ */
+typedef enum {
+	PROC_PERF_L2_CYCLES,	/* Cycles */
+	PROC_PERF_L2_IMISS,	/* L2 Instruction Miss */
+	PROC_PERF_L2_IHIT,	/* L2 Instruction Hit */
+	PROC_PERF_L2_DMISS,	/* L2 Data Miss */
+	PROC_PERF_L2_DHIT,	/* L2 Data Hit */
+	PROC_PERF_L2_MISS,	/* L2 Miss (I/D) */
+	PROC_PERF_L2_HIT,	/* L2 Hit (I/D) */
+	PROC_PERF_L2_VICTIM_BUFFER_HIT,	/* L2 Victim Buffer Hit (Retry Probe) */
+	PROC_PERF_L2_LFB_NQ_INDEX_CONFLICT,	/* LFB-NQ Index Conflict */
+	PROC_PERF_L2_TAG_PROBE,	/* L2 Tag Probe (issued - could be VB-Retried) */
+	PROC_PERF_L2_TAG_UPDATE,	/* L2 Tag Update (completed). Note:
+					   Some CMD types do not update */
+	PROC_PERF_L2_TAG_PROBE_COMPLETED,	/* L2 Tag Probe Completed
+						   (beyond VB-RTY window) */
+	PROC_PERF_L2_TAG_DIRTY_VICTIM,	/* L2 Tag Dirty Victim */
+	PROC_PERF_L2_DATA_STORE_NOP,	/* L2 Data Store NOP */
+	PROC_PERF_L2_DATA_STORE_READ,	/* L2 Data Store READ */
+	PROC_PERF_L2_DATA_STORE_WRITE,	/* L2 Data Store WRITE */
+	PROC_PERF_L2_MEMORY_FILL_DATA_VALID,	/* Memory Fill Data valid */
+	PROC_PERF_L2_MEMORY_WRITE_REQUEST,	/* Memory Write Request */
+	PROC_PERF_L2_MEMORY_READ_REQUEST,	/* Memory Read Request */
+	PROC_PERF_L2_MEMORY_WRITE_DATA_VALID,	/* Memory Write Data valid */
+	PROC_PERF_L2_XMC_NOP,	/* XMC NOP */
+	PROC_PERF_L2_XMC_LDT,	/* XMC LDT */
+	PROC_PERF_L2_XMC_LDI,	/* XMC LDI */
+	PROC_PERF_L2_XMC_LDD,	/* XMC LDD */
+	PROC_PERF_L2_XMC_STF,	/* XMC STF */
+	PROC_PERF_L2_XMC_STT,	/* XMC STT */
+	PROC_PERF_L2_XMC_STP,	/* XMC STP */
+	PROC_PERF_L2_XMC_STC,	/* XMC STC */
+	PROC_PERF_L2_XMC_DWB,	/* XMC DWB */
+	PROC_PERF_L2_XMC_PL2,	/* XMC PL2 */
+	PROC_PERF_L2_XMC_PSL1,	/* XMC PSL1 */
+	PROC_PERF_L2_XMC_IOBLD,	/* XMC IOBLD */
+	PROC_PERF_L2_XMC_IOBST,	/* XMC IOBST */
+	PROC_PERF_L2_XMC_IOBDMA,	/* XMC IOBDMA */
+	PROC_PERF_L2_XMC_IOBRSP,	/* XMC IOBRSP */
+	PROC_PERF_L2_XMD_BUS_VALID,	/* XMD Bus valid (all) */
+	PROC_PERF_L2_XMD_BUS_VALID_DST_L2C,	/* XMD Bus valid (DST=L2C)
+						   Memory */
+	PROC_PERF_L2_XMD_BUS_VALID_DST_IOB,	/* XMD Bus valid (DST=IOB) REFL
+						   Data */
+	PROC_PERF_L2_XMD_BUS_VALID_DST_PP,	/* XMD Bus valid (DST=PP)
+						   IOBRSP Data */
+	PROC_PERF_L2_RSC_NOP,	/* RSC NOP */
+	PROC_PERF_L2_RSC_STDN,	/* RSC STDN */
+	PROC_PERF_L2_RSC_FILL,	/* RSC FILL */
+	PROC_PERF_L2_RSC_REFL,	/* RSC REFL */
+	PROC_PERF_L2_RSC_STIN,	/* RSC STIN */
+	PROC_PERF_L2_RSC_SCIN,	/* RSC SCIN */
+	PROC_PERF_L2_RSC_SCFL,	/* RSC SCFL */
+	PROC_PERF_L2_RSC_SCDN,	/* RSC SCDN */
+	PROC_PERF_L2_RSD_DATA_VALID,	/* RSD Data Valid */
+	PROC_PERF_L2_RSD_DATA_VALID_FILL,	/* RSD Data Valid (FILL) */
+	PROC_PERF_L2_RSD_DATA_VALID_STRSP,	/* RSD Data Valid (STRSP) */
+	PROC_PERF_L2_RSD_DATA_VALID_REFL,	/* RSD Data Valid (REFL) */
+	PROC_PERF_L2_LRF_REQ,	/* LRF-REQ (LFB-NQ) */
+	PROC_PERF_L2_DT_RD_ALLOC,	/* DT RD-ALLOC */
+	PROC_PERF_L2_DT_WR_INVA,	/* DT WR-INVA */
+	PROC_PERF_L2_MAX
+} proc_perf_l2_t;
+
+/**
+ * IO addresses for L2 registers
+ */
+#define  OCTEON_L2C_PFCTL   0x8001180080000090ull
+#define  OCTEON_L2C_PFC0    0x8001180080000098ull
+#define  OCTEON_L2C_PFC1    0x80011800800000A0ull
+#define  OCTEON_L2C_PFC2    0x80011800800000A8ull
+#define  OCTEON_L2C_PFC3    0x80011800800000B0ull
+#define  OCTEON_LMC_DCLK_CNT_HI 0x8001180088000070ull
+#define  OCTEON_LMC_DCLK_CNT_LO 0x8001180088000068ull
+#define  OCTEON_LMC_OPS_CNT_HI  0x8001180088000060ull
+#define  OCTEON_LMC_OPS_CNT_LO  0x8001180088000058ull
+
+/**
+ * Bit description of the core counters control register
+ */
+typedef union {
+	uint32_t u32;
+	struct {
+		uint32_t M:1;
+		uint32_t W:1;
+		uint32_t reserved:19;
+		proc_perf_core_t event:6;
+		uint32_t IE:1;
+		uint32_t U:1;
+		uint32_t S:1;
+		uint32_t K:1;
+		uint32_t EX:1;
+	} s;
+} proc_perf_core_control_t;
+
+/**
+ * Bit description of the L2 counters control register
+ */
+typedef union {
+	uint64_t u64;
+	struct {
+		uint64_t reserved:32;
+		uint64_t cnt3ena:1;
+		uint64_t cnt3clr:1;
+		proc_perf_l2_t cnt3sel:6;
+		uint64_t cnt2ena:1;
+		uint64_t cnt2clr:1;
+		proc_perf_l2_t cnt2sel:6;
+		uint64_t cnt1ena:1;
+		uint64_t cnt1clr:1;
+		proc_perf_l2_t cnt1sel:6;
+		uint64_t cnt0ena:1;
+		uint64_t cnt0clr:1;
+		proc_perf_l2_t cnt0sel:6;
+	} s;
+} proc_perf_l2_control_t;
+
+/**
+ * Module parameters used to control the counters. Can be
+ * changed on the fly through sysfs or ioctls.
+ */
+static char counter0[32] = "sissue";
+static char counter1[32] = "dissue";
+module_param_string(counter0, counter0, sizeof(counter0), 0644);
+module_param_string(counter1, counter1, sizeof(counter1), 0644);
+
+static char l2counter0[32] = "imiss";
+static char l2counter1[32] = "ihit";
+static char l2counter2[32] = "dmiss";
+static char l2counter3[32] = "dhit";
+module_param_string(l2counter0, l2counter0, sizeof(l2counter0), 0644);
+module_param_string(l2counter1, l2counter1, sizeof(l2counter1), 0644);
+module_param_string(l2counter2, l2counter2, sizeof(l2counter2), 0644);
+module_param_string(l2counter3, l2counter3, sizeof(l2counter3), 0644);
+
+static struct proc_dir_entry *proc_perf_entry;
+static uint64_t proc_perf_counter_control[2];
+static uint64_t proc_perf_counter_data[NR_CPUS][2];
+static uint64_t proc_perf_l2counter_control[4];
+static uint64_t proc_perf_l2counter_data[4];
+static const char *proc_perf_label[PROC_PERF_CORE_MAX];
+static const char *proc_perf_l2label[PROC_PERF_L2_MAX];
+static uint64_t proc_perf_dram_clocks;
+static uint64_t proc_perf_dram_operations;
+static int proc_perf_in_use = 0;
+
+
+/**
+ * Setup the core counters. Called on each core
+ *
+ * @param arg
+ */
+static void proc_perf_setup_counters(void *arg)
+{
+	proc_perf_core_control_t control;
+	uint64_t cvmctl;
+
+	if (proc_perf_in_use) {
+		/* Disable the issue and exec conditional clock support so we get
+			better results */
+		cvmctl = __read_64bit_c0_register($9, 7);
+		cvmctl |= 3 << 16;
+		__write_64bit_c0_register($9, 7, cvmctl);
+	}
+
+	control.u32 = 0;
+	control.s.event = proc_perf_counter_control[0];
+	control.s.U = 1;
+	control.s.S = 1;
+	control.s.K = 1;
+	control.s.EX = 1;
+	__write_32bit_c0_register($25, 0, control.u32);
+
+	control.s.event = proc_perf_counter_control[1];
+	__write_32bit_c0_register($25, 2, control.u32);
+
+	__write_32bit_c0_register($25, 1, 0);
+	__write_32bit_c0_register($25, 3, 0);
+}
+
+
+/**
+ * Update the counters for each core.
+ *
+ * @param arg
+ */
+static void proc_perf_update_counters(void *arg)
+{
+	int cpu = smp_processor_id();
+
+	proc_perf_counter_data[cpu][0] = __read_64bit_c0_register($25, 1);
+	proc_perf_counter_data[cpu][1] = __read_64bit_c0_register($25, 3);
+	mb();
+}
+
+
+/**
+ * Cleanup the input of sysfs
+ *
+ * @param str
+ * @param len
+ */
+static inline void clean_string(char *str, int len)
+{
+	int i;
+	for (i = 0; i < len; i++)
+		if (str[i] <= 32)
+			str[i] = 0;
+}
+
+
+/**
+ * Setup the counters using the current config
+ */
+static void proc_perf_setup(void)
+{
+	int i;
+	proc_perf_l2_control_t l2control;
+
+	proc_perf_counter_control[0] = 0;
+	proc_perf_counter_control[1] = 0;
+	proc_perf_l2counter_control[0] = 0;
+	proc_perf_l2counter_control[1] = 0;
+	proc_perf_l2counter_control[2] = 0;
+	proc_perf_l2counter_control[3] = 0;
+
+	/* Cleanup junk on end of param strings */
+	clean_string(counter0, sizeof(counter0));
+	clean_string(counter1, sizeof(counter1));
+	clean_string(l2counter0, sizeof(l2counter0));
+	clean_string(l2counter1, sizeof(l2counter1));
+	clean_string(l2counter2, sizeof(l2counter2));
+	clean_string(l2counter3, sizeof(l2counter3));
+
+	/* Set the core counters to match the string parameters */
+	for (i = 0; i < PROC_PERF_CORE_MAX; i++) {
+		if (proc_perf_label[i]) {
+			if (strcmp(proc_perf_label[i], counter0) == 0)
+				proc_perf_counter_control[0] = i;
+			if (strcmp(proc_perf_label[i], counter1) == 0)
+				proc_perf_counter_control[1] = i;
+		}
+	}
+
+	/* Set the L2 counters to match the string parameters */
+	for (i = 0; i < PROC_PERF_L2_MAX; i++) {
+		if (proc_perf_l2label[i]) {
+			if (strcmp(proc_perf_l2label[i], l2counter0) == 0)
+				proc_perf_l2counter_control[0] = i;
+			if (strcmp(proc_perf_l2label[i], l2counter1) == 0)
+				proc_perf_l2counter_control[1] = i;
+			if (strcmp(proc_perf_l2label[i], l2counter2) == 0)
+				proc_perf_l2counter_control[2] = i;
+			if (strcmp(proc_perf_l2label[i], l2counter3) == 0)
+				proc_perf_l2counter_control[3] = i;
+		}
+	}
+
+	/* Update strings to match final config */
+	strcpy(counter0, proc_perf_label[proc_perf_counter_control[0]]);
+	strcpy(counter1, proc_perf_label[proc_perf_counter_control[1]]);
+	strcpy(l2counter0, proc_perf_l2label[proc_perf_l2counter_control[0]]);
+	strcpy(l2counter1, proc_perf_l2label[proc_perf_l2counter_control[1]]);
+	strcpy(l2counter2, proc_perf_l2label[proc_perf_l2counter_control[2]]);
+	strcpy(l2counter3, proc_perf_l2label[proc_perf_l2counter_control[3]]);
+
+	on_each_cpu(proc_perf_setup_counters, NULL, 1, 1);
+
+	l2control.u64 = 0;
+	l2control.s.cnt3ena = 1;
+	l2control.s.cnt3clr = 1;
+	l2control.s.cnt3sel = proc_perf_l2counter_control[3];
+	l2control.s.cnt2ena = 1;
+	l2control.s.cnt2clr = 1;
+	l2control.s.cnt2sel = proc_perf_l2counter_control[2];
+	l2control.s.cnt1ena = 1;
+	l2control.s.cnt1clr = 1;
+	l2control.s.cnt1sel = proc_perf_l2counter_control[1];
+	l2control.s.cnt0ena = 1;
+	l2control.s.cnt0clr = 1;
+	l2control.s.cnt0sel = proc_perf_l2counter_control[0];
+
+	cvmx_write_csr(OCTEON_L2C_PFCTL, l2control.u64);
+}
+
+
+static void proc_perf_update(void)
+{
+	on_each_cpu(proc_perf_update_counters, NULL, 1, 1);
+	mb();
+	proc_perf_l2counter_data[0] = cvmx_read_csr(OCTEON_L2C_PFC0);
+	proc_perf_l2counter_data[1] = cvmx_read_csr(OCTEON_L2C_PFC1);
+	proc_perf_l2counter_data[2] = cvmx_read_csr(OCTEON_L2C_PFC2);
+	proc_perf_l2counter_data[3] = cvmx_read_csr(OCTEON_L2C_PFC3);
+}
+
+
+/**
+ * Show the counters to the user
+ *
+ * @param m
+ * @param v
+ * @return
+ */
+static int proc_perf_show(struct seq_file *m, void *v)
+{
+	int cpu;
+	int i;
+	uint64_t dram_clocks;
+	uint64_t dram_operations;
+	proc_perf_core_control_t control0;
+	proc_perf_core_control_t control1;
+
+	proc_perf_update();
+
+	control0.u32 = __read_32bit_c0_register($25, 0);
+	control1.u32 = __read_32bit_c0_register($25, 2);
+	seq_printf(m, "       %16s %16s\n",
+		   proc_perf_label[control0.s.event],
+		   proc_perf_label[control1.s.event]);
+	for (cpu = 0; cpu < NR_CPUS; cpu++) {
+		if (cpu_online(cpu))
+			seq_printf(m, "CPU%2d: %16llu %16llu\n", cpu,
+				   (unsigned long long)
+				   proc_perf_counter_data[cpu][0],
+				   (unsigned long long)
+				   proc_perf_counter_data[cpu][1]);
+	}
+
+	seq_printf(m, "\n");
+	for (i = 0; i < 4; i++)
+		seq_printf(m, "%s: %llu\n",
+			   proc_perf_l2label[proc_perf_l2counter_control[i]],
+			   (unsigned long long) proc_perf_l2counter_data[i]);
+
+	/* Compute DRAM utilization */
+	dram_operations =
+		(cvmx_read_csr(OCTEON_LMC_OPS_CNT_HI) << 32) |
+		cvmx_read_csr(OCTEON_LMC_OPS_CNT_LO);
+	dram_clocks =
+		(cvmx_read_csr(OCTEON_LMC_DCLK_CNT_HI) << 32) |
+		cvmx_read_csr(OCTEON_LMC_DCLK_CNT_LO);
+#ifndef _ABIO32
+	if (dram_clocks > proc_perf_dram_clocks) {
+		uint64_t delta_clocks = dram_clocks - proc_perf_dram_clocks;
+		uint64_t delta_operations =
+			dram_operations - proc_perf_dram_operations;
+		uint64_t percent_x100 = 10000 * delta_operations / delta_clocks;
+		seq_printf(m,
+			   "\nDRAM ops count: %lu, dclk count: %lu, utilization: %lu.%02lu%%\n",
+			   delta_operations, delta_clocks, percent_x100 / 100,
+			   percent_x100 % 100);
+	}
+#endif
+	proc_perf_dram_operations = dram_operations;
+	proc_perf_dram_clocks = dram_clocks;
+
+	seq_printf(m,
+		   "\n"
+		   "Configuration of the performance counters is controller by writing\n"
+		   "one of the following values to:\n"
+		   "    /sys/module/perf_counters/parameters/counter{0,1}\n"
+		   "    /sys/module/perf_counters/parameters/l2counter{0-3}\n"
+		   "\n" "Possible CPU counters:");
+	for (i = 0; i < PROC_PERF_CORE_MAX; i++) {
+		if ((i & 7) == 0)
+			seq_printf(m, "\n    ");
+		if (proc_perf_label[i])
+			seq_printf(m, "%s ", proc_perf_label[i]);
+	}
+
+	seq_printf(m, "\n\nPossible L2 counters:");
+	for (i = 0; i < PROC_PERF_L2_MAX; i++) {
+		if ((i & 3) == 0)
+			seq_printf(m, "\n    ");
+		if (proc_perf_l2label[i])
+			seq_printf(m, "%s ", proc_perf_l2label[i]);
+	}
+	seq_printf(m,
+		   "\nWarning: Counter configuration doesn't update till you access /proc/octeon_perf.\n");
+
+	proc_perf_setup();
+	return 0;
+}
+
+
+/**
+ * /proc/octeon_perf was openned. Use the single_open iterator
+ *
+ * @param inode
+ * @param file
+ * @return
+ */
+static int proc_perf_open(struct inode *inode, struct file *file)
+{
+	proc_perf_in_use = 1;
+	return single_open(file, proc_perf_show, NULL);
+}
+
+
+/**
+ * IOCTL on /proc/octeon_perf
+ *
+ * @param inode
+ * @param file
+ * @param cmd
+ * @param arg
+ * @return
+ */
+static int proc_perf_ioctl(struct inode *inode, struct file *file,
+			   unsigned int cmd, unsigned long arg)
+{
+	// printk("proc_perf_ioctl(cmd=0x%x(%u), arg=0x%lx)\n", cmd, cmd, arg);
+	switch (cmd) {
+	case PROC_PERF_IOCTL_SETUP_COUNTER0:
+		if ((arg <= PROC_PERF_CORE_MAX) && proc_perf_label[arg]) {
+			strcpy(counter0, proc_perf_label[arg]);
+			proc_perf_setup();
+			return 0;
+		}
+		return -EINVAL;
+	case PROC_PERF_IOCTL_SETUP_COUNTER1:
+		if ((arg <= PROC_PERF_CORE_MAX) && proc_perf_label[arg]) {
+			strcpy(counter1, proc_perf_label[arg]);
+			proc_perf_setup();
+			return 0;
+		}
+		return -EINVAL;
+	case PROC_PERF_IOCTL_SETUP_L2COUNTER0:
+		if ((arg <= PROC_PERF_L2_MAX) && proc_perf_l2label[arg]) {
+			strcpy(l2counter0, proc_perf_l2label[arg]);
+			proc_perf_setup();
+			return 0;
+		}
+		return -EINVAL;
+	case PROC_PERF_IOCTL_SETUP_L2COUNTER1:
+		if ((arg <= PROC_PERF_L2_MAX) && proc_perf_l2label[arg]) {
+			strcpy(l2counter1, proc_perf_l2label[arg]);
+			proc_perf_setup();
+			return 0;
+		}
+		return -EINVAL;
+	case PROC_PERF_IOCTL_SETUP_L2COUNTER2:
+		if ((arg <= PROC_PERF_L2_MAX) && proc_perf_l2label[arg]) {
+			strcpy(l2counter2, proc_perf_l2label[arg]);
+			proc_perf_setup();
+			return 0;
+		}
+		return -EINVAL;
+	case PROC_PERF_IOCTL_SETUP_L2COUNTER3:
+		if ((arg <= PROC_PERF_L2_MAX) && proc_perf_l2label[arg]) {
+			strcpy(l2counter3, proc_perf_l2label[arg]);
+			proc_perf_setup();
+			return 0;
+		}
+		return -EINVAL;
+	case PROC_PERF_IOCTL_READ_COUNTER0:
+		proc_perf_update();
+		copy_to_user((void *) arg,
+			     proc_perf_counter_data[smp_processor_id()] + 0,
+			     sizeof(long long));
+		return 0;
+	case PROC_PERF_IOCTL_READ_COUNTER1:
+		proc_perf_update();
+		copy_to_user((void *) arg,
+			     proc_perf_counter_data[smp_processor_id()] + 1,
+			     sizeof(long long));
+		return 0;
+	case PROC_PERF_IOCTL_READ_L2COUNTER0:
+		proc_perf_update();
+		copy_to_user((void *) arg, proc_perf_l2counter_data + 0,
+			     sizeof(long long));
+		return 0;
+	case PROC_PERF_IOCTL_READ_L2COUNTER1:
+		proc_perf_update();
+		copy_to_user((void *) arg, proc_perf_l2counter_data + 1,
+			     sizeof(long long));
+		return 0;
+	case PROC_PERF_IOCTL_READ_L2COUNTER2:
+		proc_perf_update();
+		copy_to_user((void *) arg, proc_perf_l2counter_data + 2,
+			     sizeof(long long));
+		return 0;
+	case PROC_PERF_IOCTL_READ_L2COUNTER3:
+		proc_perf_update();
+		copy_to_user((void *) arg, proc_perf_l2counter_data + 3,
+			     sizeof(long long));
+		return 0;
+	default:
+		return -EINVAL;
+	}
+}
+
+
+static struct file_operations proc_perf_operations = {
+	.open = proc_perf_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+	.ioctl = proc_perf_ioctl,
+};
+
+
+/**
+ * Module initialization
+ *
+ * @return
+ */
+static int __init proc_perf_init(void)
+{
+	printk("/proc/octeon_perf: Octeon performace counter interface loaded\n");
+
+	memset(proc_perf_label, 0, sizeof(proc_perf_label));
+	memset(proc_perf_l2label, 0, sizeof(proc_perf_l2label));
+
+	proc_perf_label[PROC_PERF_CORE_NONE] = "none";
+	proc_perf_label[PROC_PERF_CORE_CLK] = "clk";
+	proc_perf_label[PROC_PERF_CORE_ISSUE] = "issue";
+	proc_perf_label[PROC_PERF_CORE_RET] = "ret";
+	proc_perf_label[PROC_PERF_CORE_NISSUE] = "nissue";
+	proc_perf_label[PROC_PERF_CORE_SISSUE] = "sissue";
+	proc_perf_label[PROC_PERF_CORE_DISSUE] = "dissue";
+	proc_perf_label[PROC_PERF_CORE_IFI] = "ifi";
+	proc_perf_label[PROC_PERF_CORE_BR] = "br";
+	proc_perf_label[PROC_PERF_CORE_BRMIS] = "brmis";
+	proc_perf_label[PROC_PERF_CORE_J] = "j";
+	proc_perf_label[PROC_PERF_CORE_JMIS] = "jmis";
+	proc_perf_label[PROC_PERF_CORE_REPLAY] = "replay";
+	proc_perf_label[PROC_PERF_CORE_IUNA] = "iuna";
+	proc_perf_label[PROC_PERF_CORE_TRAP] = "trap";
+	proc_perf_label[PROC_PERF_CORE_UULOAD] = "uuload";
+	proc_perf_label[PROC_PERF_CORE_UUSTORE] = "uustore";
+	proc_perf_label[PROC_PERF_CORE_ULOAD] = "uload";
+	proc_perf_label[PROC_PERF_CORE_USTORE] = "ustore";
+	proc_perf_label[PROC_PERF_CORE_EC] = "ec";
+	proc_perf_label[PROC_PERF_CORE_MC] = "mc";
+	proc_perf_label[PROC_PERF_CORE_CC] = "cc";
+	proc_perf_label[PROC_PERF_CORE_CSRC] = "csrc";
+	proc_perf_label[PROC_PERF_CORE_CFETCH] = "cfetch";
+	proc_perf_label[PROC_PERF_CORE_CPREF] = "cpref";
+	proc_perf_label[PROC_PERF_CORE_ICA] = "ica";
+	proc_perf_label[PROC_PERF_CORE_II] = "ii";
+	proc_perf_label[PROC_PERF_CORE_IP] = "ip";
+	proc_perf_label[PROC_PERF_CORE_CIMISS] = "cimiss";
+	proc_perf_label[PROC_PERF_CORE_WBUF] = "wbuf";
+	proc_perf_label[PROC_PERF_CORE_WDAT] = "wdat";
+	proc_perf_label[PROC_PERF_CORE_WBUFLD] = "wbufld";
+	proc_perf_label[PROC_PERF_CORE_WBUFFL] = "wbuffl";
+	proc_perf_label[PROC_PERF_CORE_WBUFTR] = "wbuftr";
+	proc_perf_label[PROC_PERF_CORE_BADD] = "badd";
+	proc_perf_label[PROC_PERF_CORE_BADDL2] = "baddl2";
+	proc_perf_label[PROC_PERF_CORE_BFILL] = "bfill";
+	proc_perf_label[PROC_PERF_CORE_DDIDS] = "ddids";
+	proc_perf_label[PROC_PERF_CORE_IDIDS] = "idids";
+	proc_perf_label[PROC_PERF_CORE_DIDNA] = "didna";
+	proc_perf_label[PROC_PERF_CORE_LDS] = "lds";
+	proc_perf_label[PROC_PERF_CORE_LMLDS] = "lmlds";
+	proc_perf_label[PROC_PERF_CORE_IOLDS] = "iolds";
+	proc_perf_label[PROC_PERF_CORE_DMLDS] = "dmlds";
+	proc_perf_label[PROC_PERF_CORE_STS] = "sts";
+	proc_perf_label[PROC_PERF_CORE_LMSTS] = "lmsts";
+	proc_perf_label[PROC_PERF_CORE_IOSTS] = "iosts";
+	proc_perf_label[PROC_PERF_CORE_IOBDMA] = "iobdma";
+	proc_perf_label[PROC_PERF_CORE_DTLB] = "dtlb";
+	proc_perf_label[PROC_PERF_CORE_DTLBAD] = "dtlbad";
+	proc_perf_label[PROC_PERF_CORE_ITLB] = "itlb";
+	proc_perf_label[PROC_PERF_CORE_SYNC] = "sync";
+	proc_perf_label[PROC_PERF_CORE_SYNCIOB] = "synciob";
+	proc_perf_label[PROC_PERF_CORE_SYNCW] = "syncw";
+
+	proc_perf_l2label[PROC_PERF_L2_CYCLES] = "cycles";
+	proc_perf_l2label[PROC_PERF_L2_IMISS] = "imiss";
+	proc_perf_l2label[PROC_PERF_L2_IHIT] = "ihit";
+	proc_perf_l2label[PROC_PERF_L2_DMISS] = "dmiss";
+	proc_perf_l2label[PROC_PERF_L2_DHIT] = "dhit";
+	proc_perf_l2label[PROC_PERF_L2_MISS] = "miss";
+	proc_perf_l2label[PROC_PERF_L2_HIT] = "hit";
+	proc_perf_l2label[PROC_PERF_L2_VICTIM_BUFFER_HIT] = "victim-buffer-hit";
+	proc_perf_l2label[PROC_PERF_L2_LFB_NQ_INDEX_CONFLICT] =
+		"lfb-nq-index-conflict";
+	proc_perf_l2label[PROC_PERF_L2_TAG_PROBE] = "tag-probe";
+	proc_perf_l2label[PROC_PERF_L2_TAG_UPDATE] = "tag-update";
+	proc_perf_l2label[PROC_PERF_L2_TAG_PROBE_COMPLETED] =
+		"tag-probe-completed";
+	proc_perf_l2label[PROC_PERF_L2_TAG_DIRTY_VICTIM] = "tag-dirty-victim";
+	proc_perf_l2label[PROC_PERF_L2_DATA_STORE_NOP] = "data-store-nop";
+	proc_perf_l2label[PROC_PERF_L2_DATA_STORE_READ] = "data-store-read";
+	proc_perf_l2label[PROC_PERF_L2_DATA_STORE_WRITE] = "data-store-write";
+	proc_perf_l2label[PROC_PERF_L2_MEMORY_FILL_DATA_VALID] =
+		"memory-fill-data-valid";
+	proc_perf_l2label[PROC_PERF_L2_MEMORY_WRITE_REQUEST] =
+		"memory-write-request";
+	proc_perf_l2label[PROC_PERF_L2_MEMORY_READ_REQUEST] =
+		"memory-read-request";
+	proc_perf_l2label[PROC_PERF_L2_MEMORY_WRITE_DATA_VALID] =
+		"memory-write-data-valid";
+	proc_perf_l2label[PROC_PERF_L2_XMC_NOP] = "xmc-nop";
+	proc_perf_l2label[PROC_PERF_L2_XMC_LDT] = "xmc-ldt";
+	proc_perf_l2label[PROC_PERF_L2_XMC_LDI] = "xmc-ldi";
+	proc_perf_l2label[PROC_PERF_L2_XMC_LDD] = "xmc-ldd";
+	proc_perf_l2label[PROC_PERF_L2_XMC_STF] = "xmc-stf";
+	proc_perf_l2label[PROC_PERF_L2_XMC_STT] = "xmc-stt";
+	proc_perf_l2label[PROC_PERF_L2_XMC_STP] = "xmc-stp";
+	proc_perf_l2label[PROC_PERF_L2_XMC_STC] = "xmc-stc";
+	proc_perf_l2label[PROC_PERF_L2_XMC_DWB] = "xmc-dwb";
+	proc_perf_l2label[PROC_PERF_L2_XMC_PL2] = "xmc-pl2";
+	proc_perf_l2label[PROC_PERF_L2_XMC_PSL1] = "xmc-psl1";
+	proc_perf_l2label[PROC_PERF_L2_XMC_IOBLD] = "xmc-iobld";
+	proc_perf_l2label[PROC_PERF_L2_XMC_IOBST] = "xmc-iobst";
+	proc_perf_l2label[PROC_PERF_L2_XMC_IOBDMA] = "xmc-iobdma";
+	proc_perf_l2label[PROC_PERF_L2_XMC_IOBRSP] = "xmc-iobrsp";
+	proc_perf_l2label[PROC_PERF_L2_XMD_BUS_VALID] = "xmd-bus-valid";
+	proc_perf_l2label[PROC_PERF_L2_XMD_BUS_VALID_DST_L2C] =
+		"xmd-bus-valid-dst-l2c";
+	proc_perf_l2label[PROC_PERF_L2_XMD_BUS_VALID_DST_IOB] =
+		"xmd-bus-valid-dst-iob";
+	proc_perf_l2label[PROC_PERF_L2_XMD_BUS_VALID_DST_PP] =
+		"xmd-bus-valid-dst-pp";
+	proc_perf_l2label[PROC_PERF_L2_RSC_NOP] = "rsc-nop";
+	proc_perf_l2label[PROC_PERF_L2_RSC_STDN] = "rsc-stdn";
+	proc_perf_l2label[PROC_PERF_L2_RSC_FILL] = "rsc-fill";
+	proc_perf_l2label[PROC_PERF_L2_RSC_REFL] = "rsc-refl";
+	proc_perf_l2label[PROC_PERF_L2_RSC_STIN] = "rsc-stin";
+	proc_perf_l2label[PROC_PERF_L2_RSC_SCIN] = "rsc-scin";
+	proc_perf_l2label[PROC_PERF_L2_RSC_SCFL] = "rsc-scfl";
+	proc_perf_l2label[PROC_PERF_L2_RSC_SCDN] = "rsc-scdn";
+	proc_perf_l2label[PROC_PERF_L2_RSD_DATA_VALID] = "rsd-data-valid";
+	proc_perf_l2label[PROC_PERF_L2_RSD_DATA_VALID_FILL] =
+		"rsd-data-valid-fill";
+	proc_perf_l2label[PROC_PERF_L2_RSD_DATA_VALID_STRSP] =
+		"rsd-data-valid-strsp";
+	proc_perf_l2label[PROC_PERF_L2_RSD_DATA_VALID_REFL] =
+		"rsd-data-valid-refl";
+	proc_perf_l2label[PROC_PERF_L2_LRF_REQ] = "lrf-req";
+	proc_perf_l2label[PROC_PERF_L2_DT_RD_ALLOC] = "dt-rd-alloc";
+	proc_perf_l2label[PROC_PERF_L2_DT_WR_INVA] = "dt-wr-inva";
+
+	proc_perf_entry = create_proc_entry("octeon_perf", 0, NULL);
+	if (proc_perf_entry)
+		proc_perf_entry->proc_fops = &proc_perf_operations;
+
+	proc_perf_setup();
+	return 0;
+}
+
+
+/**
+ * Module cleanup
+ *
+ * @return
+ */
+static void __exit proc_perf_cleanup(void)
+{
+	if (proc_perf_entry)
+		remove_proc_entry("octeon_perf", NULL);
+}
+
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Cavium Networks <support@caviumnetworks.com>");
+MODULE_DESCRIPTION("Cavium Networks Octeon performance counter interface.");
+module_init(proc_perf_init);
+module_exit(proc_perf_cleanup);
diff --git a/arch/mips/cavium-octeon/serial.c b/arch/mips/cavium-octeon/serial.c
new file mode 100644
index 0000000..603b977
--- /dev/null
+++ b/arch/mips/cavium-octeon/serial.c
@@ -0,0 +1,147 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2004-2007 Cavium Networks
+ */
+#include <linux/console.h>
+#include <linux/serial.h>
+#include <linux/tty.h>
+#include <asm/time.h>
+#include <linux/serial_core.h>
+#include <asm/gdb-stub.h>
+#include "hal.h"
+
+extern int serial8250_register_port(struct uart_port *);
+
+#ifdef CONFIG_GDB_CONSOLE
+#define DEBUG_UART 0
+#else
+#define DEBUG_UART 1
+#endif
+
+#ifdef CONFIG_KGDB
+
+extern void breakpoint(void);
+
+char getDebugChar(void)
+{
+	unsigned long lsrval;
+
+	octeon_write_lcd("kgdb");
+
+	/* Spin until data is available */
+	do {
+		lsrval = cvmx_read_csr(CVMX_MIO_UARTX_LSR(DEBUG_UART));
+	} while ((lsrval & 0x1) == 0);
+
+	octeon_write_lcd("");
+
+	/* Read and return the data */
+	return cvmx_read_csr(CVMX_MIO_UARTX_RBR(DEBUG_UART));
+}
+
+void putDebugChar(char ch)
+{
+	unsigned long lsrval;
+
+	/* Spin until there is room */
+	do {
+		lsrval = cvmx_read_csr(CVMX_MIO_UARTX_LSR(DEBUG_UART));
+	}
+	while ((lsrval & 0x20) == 0);
+
+	/* Write the byte */
+	cvmx_write_csr(CVMX_MIO_UARTX_THR(DEBUG_UART), ch);
+}
+
+#endif
+
+#if defined(CONFIG_KGDB) || defined(CONFIG_CAVIUM_GDB)
+
+static irqreturn_t interruptDebugChar(int cpl, void *dev_id)
+{
+	unsigned long lsrval;
+	lsrval = cvmx_read_csr(CVMX_MIO_UARTX_LSR(1));
+	if (lsrval & 1) {
+#ifdef CONFIG_KGDB
+		struct pt_regs *regs = get_irq_regs();
+
+		putDebugChar(getDebugChar());
+		set_async_breakpoint(&regs->cp0_epc);
+#else
+		unsigned long tmp;
+		/* Pulse MCD0 signal on Ctrl-C to stop all the cores. Also set
+		   the MCD0 to be not masked by this core so we know the signal
+		   is received by someone */
+		octeon_write_lcd("brk");
+		asm volatile ("dmfc0 %0, $22\n"
+			      "ori   %0, %0, 0x10\n"
+			      "dmtc0 %0, $22\n":"=r" (tmp));
+		octeon_write_lcd("");
+#endif
+		return IRQ_HANDLED;
+	}
+	return IRQ_NONE;
+}
+
+#endif
+
+static int octeon_serial_init(void)
+{
+	int port;
+
+#if defined(CONFIG_KGDB) || defined(CONFIG_CAVIUM_GDB)
+	const int max_port = 1;
+	uint64_t ier;
+#else
+	/* Change the following to "2" to have both serial ports available in
+	   Linux. The second port isn't enabled by default because a simple
+	   exec application might be running on another core. Linux control of
+	   the second uart breaks the simple exec debugger interface through
+	   the second uart. */
+	const int max_port = 1 + octeon_get_boot_uart();	/* Init second
+								   serial port
+								   if console
+								   on uart 1 */
+#endif
+
+	struct uart_port octeon_port;
+	memset(&octeon_port, 0, sizeof(octeon_port));
+	octeon_port.flags = ASYNC_SKIP_TEST | UPF_SHARE_IRQ;
+	octeon_port.iotype = UPIO_MEM;
+	octeon_port.regshift = 3;	/* I/O addresses are every 8 bytes */
+	octeon_port.uartclk = mips_hpt_frequency;	/* Clock rate of the
+							   chip */
+	octeon_port.fifosize = 64;
+
+#ifdef CONFIG_CAVIUM_OCTEON_2ND_KERNEL
+	for (port = 1; port < 2; port++)
+#else
+	for (port = 0; port < max_port; port++)
+#endif
+	{
+		octeon_port.mapbase = 0x0001180000000800ull + (1024 * port);
+		octeon_port.membase = cvmx_phys_to_ptr(octeon_port.mapbase);
+		if (!OCTEON_IS_MODEL(OCTEON_CN38XX_PASS2))	/* Only
+								   CN38XXp{1,2}
+								   has errata
+								   with uart
+								   interrupt */
+			octeon_port.irq = 42 + port;
+		serial8250_register_port(&octeon_port);
+	}
+
+#if defined(CONFIG_KGDB) || defined(CONFIG_CAVIUM_GDB)
+	request_irq(42 + DEBUG_UART, interruptDebugChar, SA_SHIRQ, "KGDB",
+		    interruptDebugChar);
+
+	/* Enable uart1 interrupts for debugger Control-C processing */
+	ier = cvmx_read_csr(CVMX_MIO_UARTX_IER(DEBUG_UART));
+	cvmx_write_csr(CVMX_MIO_UARTX_IER(DEBUG_UART), ier | 1);
+#endif
+	return 0;
+}
+
+late_initcall(octeon_serial_init);
diff --git a/arch/mips/cavium-octeon/setup.c b/arch/mips/cavium-octeon/setup.c
new file mode 100644
index 0000000..91adbf3
--- /dev/null
+++ b/arch/mips/cavium-octeon/setup.c
@@ -0,0 +1,1307 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2004-2007 Cavium Networks
+ */
+#include <linux/init.h>
+#include <linux/delay.h>
+#include <linux/irq.h>
+#include <linux/interrupt.h>
+#include <linux/serial.h>
+#include <linux/types.h>
+#include <linux/string.h>	/* for memset */
+#include <linux/console.h>
+#include <linux/serial.h>
+#include <linux/tty.h>
+#include <asm/time.h>
+#include <linux/serial_core.h>
+#include <linux/string.h>
+
+#include <asm/reboot.h>
+#include <asm/io.h>
+#include <asm/time.h>
+#include <asm/processor.h>
+#include <asm/reboot.h>
+#include <asm/system.h>
+#include <asm/irq_cpu.h>
+#include <asm/mipsregs.h>
+#include <asm/bootinfo.h>
+#include <asm/gdb-stub.h>
+#include "hal.h"
+#include "cvmx-l2c.h"
+#include "cvmx-bootmem.h"
+
+extern void octeon_user_io_init(void);
+static unsigned long CYCLES_PER_JIFFY;
+static int ECC_REPORT_SINGLE_BIT_ERRORS = 0;
+static unsigned long long MAX_MEMORY = 512ull << 20;
+
+
+/**
+ * Reboot Octeon
+ *
+ * @param command Command to pass to the bootloader. Currently ignored.
+ */
+static void octeon_restart(char *command)
+{
+#ifdef CONFIG_SMP
+	int cpu;
+#endif
+	mb();
+	/* Disabled BIST on reset due to a chip errata G-200 for Cn38XX and
+	   CN31XX */
+	if (!OCTEON_IS_MODEL(OCTEON_CN38XX_PASS2) &&
+	    !OCTEON_IS_MODEL(OCTEON_CN31XX)) {
+		/* BIST should always be enabled when doing a soft reset. L2
+		   Cache locking for instance is not cleared unless BIST is
+		   enabled */
+		cvmx_write_csr(CVMX_CIU_SOFT_BIST, 1);
+		/* Read it back to make sure it is set before we reset the
+		   chip. */
+		cvmx_read_csr(CVMX_CIU_SOFT_BIST);
+	}
+
+	/* Disable all watchdogs before soft reset. They don't get cleared */
+#ifdef CONFIG_SMP
+	for (cpu = 0; cpu < NR_CPUS; cpu++)
+		if (cpu_online(cpu))
+			cvmx_write_csr(CVMX_CIU_WDOGX(cpu_logical_map(cpu)), 0);
+#else
+	cvmx_write_csr(CVMX_CIU_WDOGX(cvmx_get_core_num()), 0);
+#endif
+
+	while (1)
+		cvmx_write_csr(CVMX_CIU_SOFT_RST, 1);
+}
+
+
+/**
+ * Permanently stop a core.
+ *
+ * @param arg
+ */
+static void octeon_kill_core(void *arg)
+{
+	mb();
+	if (octeon_is_simulation()) {
+		/* The simulator needs the watchdog to stop for dead cores */
+		cvmx_write_csr(CVMX_CIU_WDOGX(cvmx_get_core_num()), 0);
+		/* A break instruction causes the simulator stop a core */
+		asm volatile ("sync\nbreak");
+	}
+}
+
+
+/**
+ * Halt the system
+ */
+static void octeon_halt(void)
+{
+	smp_call_function(octeon_kill_core, NULL, 0, 0);
+	octeon_poweroff();
+	octeon_kill_core(NULL);
+}
+
+
+/**
+ * Read the Octeon high performance counter
+ *
+ * @return The counter value. For some brain dead reason, the kernel
+ *         uses a 32bit number here.
+ */
+static cycles_t octeon_hpt_read(void)
+{
+	cycles_t cycles;
+      asm("rdhwr %0,$31":"=r"(cycles));
+	return cycles;
+}
+
+
+/**
+ * Acknowledge a timer tick. We don't use the standard Mips
+ * one because it confuses the timer ticks and the HPT clock.
+ */
+static void octeon_timer_ack(void)
+{
+	uint32_t count;
+	uint32_t next_compare = read_c0_compare() + CYCLES_PER_JIFFY;
+	write_c0_compare(next_compare);
+	count = read_c0_count();
+	if ((count - next_compare) < 0x7fffffff) {
+		next_compare = count + CYCLES_PER_JIFFY;
+		write_c0_compare(next_compare);
+	}
+}
+
+
+/**
+ * Interrupt entry point for timer ticks
+ *
+ * @param irq
+ * @param dev_id
+ * @return
+ */
+static irqreturn_t octeon_main_timer_interrupt(int irq, void *dev_id)
+{
+	if (read_c0_cause() & (1 << 30)) {
+		if (smp_processor_id() == 0) {
+			/* This function calls the timer ack internally */
+			timer_interrupt(irq, dev_id);
+		} else {
+			octeon_timer_ack();
+			local_timer_interrupt(irq, dev_id);
+		}
+		return IRQ_HANDLED;
+	} else
+		return IRQ_NONE;
+}
+
+
+/**
+ * Setup the first cores timer interrupt
+ *
+ * @param irq
+ * @return
+ */
+void __init plat_timer_setup(struct irqaction *irq)
+{
+	irq->handler = octeon_main_timer_interrupt;
+	irq->flags |= SA_SHIRQ;
+	setup_irq(7, irq);
+}
+
+
+/**
+ * Handle all the error condition interrupts that might occur.
+ *
+ * @param cpl
+ * @param dev_id
+ * @return
+ */
+static irqreturn_t octeon_rlm_interrupt(int cpl, void *dev_id)
+{
+	irqreturn_t result = IRQ_NONE;
+	cvmx_npei_rsl_int_blocks_t rsl_blocks;
+
+	uint64_t clr_val;
+	if (octeon_has_feature(OCTEON_FEATURE_PCIE))
+		rsl_blocks.u64 = cvmx_read_csr(CVMX_PEXP_NPEI_RSL_INT_BLOCKS);
+	else
+		rsl_blocks.u64 = cvmx_read_csr(CVMX_NPI_RSL_INT_BLOCKS);
+
+	/* rsl_blocks Bits 63-31 not used */
+
+	if (rsl_blocks.s.iob) {	/* 30 - IOB_INT_SUM */
+		cvmx_iob_int_sum_t err;
+
+		err.u64 = cvmx_read_csr(CVMX_IOB_INT_SUM);
+		cvmx_write_csr(CVMX_IOB_INT_SUM, err.u64);
+		if (err.u64) {
+			unsigned int port = cvmx_read_csr(CVMX_IOB_PKT_ERR);
+			if (err.s.np_sop)
+				printk("\nIOB: Port %u SOP error\n", port);
+			if (err.s.np_eop)
+				printk("\nIOB: Port %u EOP error\n", port);
+			if (err.s.p_sop)
+				printk("\nIOB: Passthrough Port %u SOP error\n",
+				       port);
+			if (err.s.p_eop)
+				printk("\nIOB: Passthrough Port %u EOP error\n",
+				       port);
+			if (err.s.np_dat)
+				printk("\nIOB: Data arrived before a SOP for the same port for a non-passthrough packet.\n");
+			if (err.s.p_dat)
+				printk("\nIOB: Data arrived before a SOP for the same port for a passthrough packet.\n");
+			result = IRQ_HANDLED;
+		}
+	}
+
+
+	if (rsl_blocks.s.lmc1) {	/* 29 - LMC_MEM_CFG1 */
+		static unsigned long single_bit_errors = 0;
+		static unsigned long double_bit_errors = 0;
+		const int ddr_controller = 1;
+		cvmx_lmcx_mem_cfg0_t mem_cfg0;
+                cvmx_lmc_fadr_t fadr;
+
+		mem_cfg0.u64 =cvmx_read_csr(CVMX_LMCX_MEM_CFG0(ddr_controller));
+                fadr.u64 = cvmx_read_csr(CVMX_LMCX_FADR (ddr_controller));
+		cvmx_write_csr(CVMX_LMCX_MEM_CFG0(ddr_controller),mem_cfg0.u64);
+		if (mem_cfg0.s.sec_err || mem_cfg0.s.ded_err) {
+			if (fadr.s.fbunk | fadr.s.frow | fadr.s.fcol) {
+				single_bit_errors += hweight64(mem_cfg0.s.sec_err);
+				double_bit_errors += hweight64(mem_cfg0.s.ded_err);
+				if ((mem_cfg0.s.ded_err) || (mem_cfg0.s.sec_err &&
+					ECC_REPORT_SINGLE_BIT_ERRORS)) {
+					printk("\nDDR%d ECC: %lu Single bit corrections, %lu Double bit errors\n" "DDR%d ECC:\tFailing dimm:   %u\n" "DDR%d ECC:\tFailing rank:   %u\n" "DDR%d ECC:\tFailing bank:   %u\n" "DDR%d ECC:\tFailing row:    0x%x\n" "DDR%d ECC:\tFailing column: 0x%x\n", ddr_controller, single_bit_errors, double_bit_errors, ddr_controller, fadr.s.fdimm, ddr_controller, fadr.s.fbunk, ddr_controller, fadr.s.fbank, ddr_controller, fadr.s.frow, ddr_controller, fadr.s.fcol);
+				}
+			}
+			result = IRQ_HANDLED;
+		}
+	}
+
+	/* rsl_blocks Bits 28-24 not used */
+
+	if (rsl_blocks.s.asxpcs1) {	/* 23 - ASX1_INT_REG */
+		if (OCTEON_IS_MODEL(OCTEON_CN3XXX) ||
+		    OCTEON_IS_MODEL(OCTEON_CN58XX) ||
+		    OCTEON_IS_MODEL(OCTEON_CN50XX)) {
+			cvmx_asxx_int_reg_t err;
+
+			err.u64 = cvmx_read_csr(CVMX_ASXX_INT_REG(1));
+			cvmx_write_csr(CVMX_ASXX_INT_REG(1), err.u64);
+			if (err.u64) {
+				int port;
+				for (port = 0; port < 4; port++) {
+					if (err.s.ovrflw & (1 << port))
+						printk("ASX1: RX FIFO overflow on RMGII port %d\n", port + 16);
+					if (err.s.txpop & (1 << port))
+						printk("ASX1: TX FIFO underflow on RMGII port %d\n", port + 16);
+					if (err.s.txpsh & (1 << port))
+						printk("ASX1: TX FIFO overflow on RMGII port %d\n", port + 16);
+				}
+				result = IRQ_HANDLED;
+			}
+		} else {
+			printk("FIXME: PCS0/1_INT(0..3)_REG needs decode\n");
+		}
+	}
+
+	if (rsl_blocks.s.asxpcs0) {	/* 22- ASX0_INT_REG */
+		if (OCTEON_IS_MODEL(OCTEON_CN3XXX) ||
+		    OCTEON_IS_MODEL(OCTEON_CN58XX) ||
+		    OCTEON_IS_MODEL(OCTEON_CN50XX)) {
+			cvmx_asxx_int_reg_t err;
+
+			err.u64 = cvmx_read_csr(CVMX_ASXX_INT_REG(0));
+			cvmx_write_csr(CVMX_ASXX_INT_REG(0), err.u64);
+			if (err.u64) {
+				int port;
+				for (port = 0; port < 4; port++) {
+					if (err.s.ovrflw & (1 << port))
+						printk("ASX0: RX FIFO overflow on RMGII port %d\n", port);
+					if (err.s.txpop & (1 << port))
+						printk("ASX0: TX FIFO underflow on RMGII port %d\n", port);
+					if (err.s.txpsh & (1 << port))
+						printk("ASX0: TX FIFO overflow on RMGII port %d\n", port);
+				}
+				result = IRQ_HANDLED;
+			}
+		} else {
+			printk("FIXME: PCS0/1_INT(0..3)_REG needs decode\n");
+		}
+	}
+
+	/* rsl_blocks Bit 21 not used */
+
+	if (rsl_blocks.s.pip) {	/* 20 - PIP_INT_REG. */
+		cvmx_pip_int_reg_t err;
+
+		err.u64 = cvmx_read_csr(CVMX_PIP_INT_REG);
+		cvmx_write_csr(CVMX_PIP_INT_REG, err.u64);
+		if (err.u64) {
+			/* Don't report disabled errors */
+			err.u64 &= cvmx_read_csr(CVMX_PIP_INT_EN);
+			if (err.s.beperr)
+				printk("PIP: Parity Error in back end memory\n");
+			if (err.s.feperr)
+				printk("PIP: Parity Error in front end memory\n");
+			if (err.s.todoovr)
+				printk("PIP: Todo list overflow (see PIP_BCK_PRS[HIWATER])\n");
+			if (err.s.skprunt)
+				printk("PIP: Packet was engulfed by skipper\n");
+			if (err.s.badtag)
+				printk("PIP: A bad tag was sent from IPD\n");
+			if (err.s.prtnxa)
+				printk("PIP: Non-existent port\n");
+			if (err.s.bckprs)
+				printk("PIP: PIP asserted backpressure\n");
+			if (err.s.crcerr)
+				printk("PIP: PIP calculated bad CRC\n");
+			if (err.s.pktdrp)
+				printk("PIP: Packet Dropped due to QOS\n");
+			result = IRQ_HANDLED;
+		}
+	}
+
+	/* 19 - SPX1_INT_REG & STX1_INT_REG */
+	/* Currently not checked. These should be checked by the ethernet
+	   driver */
+
+	/* 18 - SPX0_INT_REG & STX0_INT_REG */
+	/* Currently not checked. These should be checked by the ethernet
+	   driver */
+
+	if (rsl_blocks.s.lmc0) {	/* 17 - LMC_MEM_CFG0 */
+		static unsigned long single_bit_errors = 0;
+		static unsigned long double_bit_errors = 0;
+		const int ddr_controller = 0;
+		cvmx_lmcx_mem_cfg0_t mem_cfg0;
+                cvmx_lmc_fadr_t fadr;
+
+		mem_cfg0.u64 =cvmx_read_csr(CVMX_LMCX_MEM_CFG0(ddr_controller));
+                fadr.u64 = cvmx_read_csr(CVMX_LMCX_FADR(ddr_controller));
+		cvmx_write_csr(CVMX_LMCX_MEM_CFG0(ddr_controller),mem_cfg0.u64);
+		if (mem_cfg0.s.sec_err || mem_cfg0.s.ded_err) {
+			if (fadr.s.fbunk | fadr.s.frow | fadr.s.fcol) {
+				single_bit_errors += hweight64(mem_cfg0.s.sec_err);
+				double_bit_errors += hweight64(mem_cfg0.s.ded_err);
+				if ((mem_cfg0.s.ded_err) || (mem_cfg0.s.sec_err &&
+					ECC_REPORT_SINGLE_BIT_ERRORS)) {
+					printk("\nDDR%d ECC: %lu Single bit corrections, %lu Double bit errors\n" "DDR%d ECC:\tFailing dimm:   %u\n" "DDR%d ECC:\tFailing rank:   %u\n" "DDR%d ECC:\tFailing bank:   %u\n" "DDR%d ECC:\tFailing row:    0x%x\n" "DDR%d ECC:\tFailing column: 0x%x\n", ddr_controller, single_bit_errors, double_bit_errors, ddr_controller, fadr.s.fdimm, ddr_controller, fadr.s.fbunk, ddr_controller, fadr.s.fbank, ddr_controller, fadr.s.frow, ddr_controller, fadr.s.fcol);
+				}
+			}
+			result = IRQ_HANDLED;
+		}
+	}
+
+	if (rsl_blocks.s.l2c) {	/* 16 - L2T_ERR & L2D_ERR */
+		cvmx_l2t_err_t terr;
+		cvmx_l2d_err_t derr;
+
+		terr.u64 = cvmx_read_csr(CVMX_L2T_ERR);
+		cvmx_write_csr(CVMX_L2T_ERR, terr.u64);
+		if (terr.u64) {
+			if (terr.s.ded_err) {
+				printk("L2T ECC: double bit:\tfadr: 0x%x, fset: 0x%x, fsyn: 0x%x, jiffies: %lud\n", terr.s.fadr, terr.s.fset, terr.s.fsyn, jiffies);
+			}
+			if (terr.s.sec_err && ECC_REPORT_SINGLE_BIT_ERRORS) {
+				printk("L2T ECC: single bit:\tfadr: 0x%x, fset: 0x%x, fsyn: 0x%x, jiffies: %lud\n", terr.s.fadr, terr.s.fset, terr.s.fsyn, jiffies);
+			}
+			if (terr.s.ded_err || terr.s.sec_err) {
+				if (!terr.s.fsyn) {
+					/* Syndrome is zero, which means error
+					   was in non-hit line, so flush all
+					   associations */
+					int i;
+					int l2_assoc = cvmx_l2c_get_num_assoc();
+
+					for (i = 0; i < l2_assoc; i++)
+						cvmx_l2c_flush_line(i,
+								    terr.s.
+								    fadr);
+				} else
+					cvmx_l2c_flush_line(terr.s.fset,
+							    terr.s.fadr);
+
+			}
+			if (terr.s.lckerr2)
+				printk("L2T: HW detected a case where a Rd/Wr Miss from PP#n could not find an available/unlocked set (for replacement).\n");
+			if (terr.s.lckerr)
+				printk("L2T: SW attempted to LOCK DOWN the last available set of the INDEX (which is ignored by HW - but reported to SW).\n");
+			result = IRQ_HANDLED;
+		}
+
+		clr_val = derr.u64 = cvmx_read_csr(CVMX_L2D_ERR);
+		if (derr.u64) {
+
+			cvmx_l2d_fadr_t fadr;
+			result = IRQ_HANDLED;
+
+			if (derr.s.ded_err ||
+			    (derr.s.sec_err && ECC_REPORT_SINGLE_BIT_ERRORS)) {
+				const int coreid = (int) cvmx_get_core_num();
+
+				uint64_t syn0 = cvmx_read_csr(CVMX_L2D_FSYN0);
+				uint64_t syn1 = cvmx_read_csr(CVMX_L2D_FSYN1);
+				fadr.u64 = cvmx_read_csr(CVMX_L2D_FADR);
+
+				if (derr.s.ded_err) {
+					printk("\nL2D ECC double (core %d): fadr: 0x%llx, syn0:0x%llx, syn1: 0x%llx, jiffies: %lud\n", coreid, (unsigned long long) fadr.u64, (unsigned long long) syn0, (unsigned long long) syn1, jiffies);
+				} else {
+					printk("\nL2D ECC single (core %d): fadr: 0x%llx, syn0:0x%llx, syn1: 0x%llx, jiffies: %lud\n", coreid, (unsigned long long) fadr.u64, (unsigned long long) syn0, (unsigned long long) syn1, jiffies);
+				}
+				/* Flush the line that had the error */
+				if (derr.s.ded_err || derr.s.sec_err)
+					cvmx_l2c_flush_line(fadr.cn38xx.fset,
+							    fadr.cn38xx.
+							    fadr >> 1);
+			}
+		}
+		cvmx_write_csr(CVMX_L2D_ERR, clr_val);
+	}
+
+	/* rsl_blocks Bits 15-13 not used */
+
+	if (rsl_blocks.s.pow) {	/* 12 - POW_ECC_ERR */
+		cvmx_pow_ecc_err_t err;
+
+		err.u64 = cvmx_read_csr(CVMX_POW_ECC_ERR);
+		cvmx_write_csr(CVMX_POW_ECC_ERR, err.u64);
+		if (err.u64) {
+			if (err.s.sbe && ECC_REPORT_SINGLE_BIT_ERRORS)
+				printk("ECC: POW single bit error\n");
+			if (err.s.dbe)
+				printk("ECC: POW double bit error\n");
+			if (err.s.dbe ||
+			    (err.s.sbe && ECC_REPORT_SINGLE_BIT_ERRORS))
+				printk("ECC:\tFailing syndrome %u\n",
+				       err.s.syn);
+			if (err.s.rpe)
+				printk("POW: Remote pointer error\n");
+			if (err.s.iop & (1 << 0))
+				printk("POW: Received SWTAG/SWTAG_FULL/SWTAG_DESCH/DESCH/UPD_WQP from PP in NULL_NULL state\n");
+			if (err.s.iop & (1 << 1))
+				printk("POW: Received SWTAG/SWTAG_DESCH/DESCH/UPD_WQP from PP in NULL state\n");
+			if (err.s.iop & (1 << 2))
+				printk("POW: Received SWTAG/SWTAG_FULL/SWTAG_DESCH/GET_WORK from PP with pending tag switch to ORDERED or ATOMIC\n");
+			if (err.s.iop & (1 << 3))
+				printk("POW: Received SWTAG/SWTAG_FULL/SWTAG_DESCH from PP with tag specified as NULL_NULL\n");
+			if (err.s.iop & (1 << 4))
+				printk("POW: Received SWTAG_FULL/SWTAG_DESCH from PP with tag specified as NULL\n");
+			if (err.s.iop & (1 << 5))
+				printk("POW: Received SWTAG/SWTAG_FULL/SWTAG_DESCH/DESCH/UPD_WQP/GET_WORK/NULL_RD from PP with GET_WORK pending\n");
+			if (err.s.iop & (1 << 6))
+				printk("POW: Received SWTAG/SWTAG_FULL/SWTAG_DESCH/DESCH/UPD_WQP/GET_WORK/NULL_RD from PP with NULL_RD pending\n");
+			if (err.s.iop & (1 << 7))
+				printk("POW: Received CLR_NSCHED from PP with SWTAG_DESCH/DESCH/CLR_NSCHED pending\n");
+			if (err.s.iop & (1 << 8))
+				printk("POW: Received SWTAG/SWTAG_FULL/SWTAG_DESCH/DESCH/UPD_WQP/GET_WORK/NULL_RD from PP with CLR_NSCHED pending\n");
+			if (err.s.iop & (1 << 9))
+				printk("POW: Received illegal opcode\n");
+			if (err.s.iop & (1 << 10))
+				printk("POW: Received ADD_WORK with tag specified as NULL_NULL\n");
+			if (err.s.iop & (1 << 11))
+				printk("POW: Received DBG load from PP with DBG load pending\n");
+			if (err.s.iop & (1 << 12))
+				printk("POW: Received CSR load from PP with CSR load pending\n");
+			result = IRQ_HANDLED;
+		}
+	}
+
+	if (rsl_blocks.s.tim) {	/* 11 - TIM_REG_ERROR */
+		cvmx_tim_reg_error_t err;
+
+		err.u64 = cvmx_read_csr(CVMX_TIM_REG_ERROR);
+		cvmx_write_csr(CVMX_TIM_REG_ERROR, err.u64);
+		if (err.u64) {
+			int i;
+			for (i = 0; i < 16; i++)
+				if (err.s.mask & (1 << i))
+					printk("TIM: Timer wheel %d error\n",
+					       i);
+			result = IRQ_HANDLED;
+		}
+	}
+
+	if (rsl_blocks.s.pko) {	/* 10 - PKO_REG_ERROR */
+		cvmx_pko_reg_error_t err;
+
+		err.u64 = cvmx_read_csr(CVMX_PKO_REG_ERROR);
+		cvmx_write_csr(CVMX_PKO_REG_ERROR, err.u64);
+		if (err.u64) {
+			if (err.s.currzero)
+				printk("PKO: A packet data pointer has size=0\n");
+			if (err.s.doorbell)
+				printk("PKO: A doorbell count has overflowed\n");
+			if (err.s.parity)
+				printk("PKO: A read-parity error has occurred in the port data buffer\n");
+			result = IRQ_HANDLED;
+		}
+	}
+
+	if (rsl_blocks.s.ipd) {	/* 9 - IPD_INT_SUM */
+		cvmx_ipd_int_sum_t err;
+
+		err.u64 = cvmx_read_csr(CVMX_IPD_INT_SUM);
+		cvmx_write_csr(CVMX_IPD_INT_SUM, err.u64);
+		if (err.u64) {
+			if (err.s.pq_sub)
+				printk("IPD: Port-qos subtract causes the counter to wrap.\n");
+			if (err.s.pq_add)
+				printk("IPD: Port-qos add causes the counter to wrap.\n");
+			if (err.s.bc_ovr)
+				printk("IPD: Byte-count to send to IOB overflows.\n");
+			if (err.s.d_coll)
+				printk("IPD: Packet/WQE data to be sent to IOB collides.\n");
+			if (err.s.c_coll)
+				printk("IPD: Packet/WQE commands to be sent to IOB collides.\n");
+			if (err.s.cc_ovr)
+				printk("IPD: Command credits to the IOB overflow.\n");
+			if (err.s.dc_ovr)
+				printk("IPD: Data credits to the IOB overflow.\n");
+			if (err.s.bp_sub)
+				printk("IPD: Backpressure subtract supplied illegal value.\n");
+			if (err.s.prc_par3)
+				printk("IPD: Parity error detected for bits [127:96] of the PBM memory.\n");
+			if (err.s.prc_par2)
+				printk("IPD: Parity error detected for bits [95:64] of the PBM memory.\n");
+			if (err.s.prc_par1)
+				printk("IPD: Parity error detected for bits [63:32] of the PBM memory.\n");
+			if (err.s.prc_par0)
+				printk("IPD: Parity error detected for bits [31:0] of the PBM memory.\n");
+			result = IRQ_HANDLED;
+		}
+	}
+
+	/* rsl_blocks Bits 8 not used */
+
+	if (rsl_blocks.s.zip) {	/* 7 - ZIP_ERROR */
+		if (octeon_has_feature(OCTEON_FEATURE_ZIP)) {
+			cvmx_zip_error_t err;
+			err.u64 = cvmx_read_csr(CVMX_ZIP_ERROR);
+			cvmx_write_csr(CVMX_ZIP_ERROR, err.u64);
+			if (err.s.doorbell)
+				printk("ZIP: Doorbell overflow\n");
+			result = IRQ_HANDLED;
+		}
+	}
+
+	if (rsl_blocks.s.reserved_6_6) {	/* 6 - DFA_ERR */
+		cvmx_dfa_err_t err;
+
+		err.u64 = cvmx_read_csr(CVMX_DFA_ERR);
+		cvmx_write_csr(CVMX_DFA_ERR, err.u64);
+		if (err.u64) {
+			if (err.s.dblovf)
+				printk("DFA: Doorbell Overflow detected\n");
+			if (err.s.cp2perr)
+				printk("DFA: PP-CP2 Parity Error Detected\n");
+			if (err.s.dteperr)
+				printk("DFA: DTE Parity Error Detected\n");
+
+			if (err.s.dtedbe)
+				printk("ECC: DFA DTE 29b Double Bit Error Detected\n");
+			if (err.s.dtesbe && ECC_REPORT_SINGLE_BIT_ERRORS)
+				printk("ECC: DFA DTE 29b Single Bit Error Corrected\n");
+			if (err.s.dtedbe ||
+			    (err.s.dtesbe && ECC_REPORT_SINGLE_BIT_ERRORS))
+				printk("ECC:\tFailing syndrome %u\n",
+				       err.s.dtesyn);
+
+			if (err.s.cp2dbe)
+				printk("ECC: DFA PP-CP2 Double Bit Error Detected\n");
+			if (err.s.cp2sbe && ECC_REPORT_SINGLE_BIT_ERRORS)
+				printk("ECC: DFA PP-CP2 Single Bit Error Corrected\n");
+			if (err.s.cp2dbe ||
+			    (err.s.cp2sbe && ECC_REPORT_SINGLE_BIT_ERRORS))
+				printk("ECC:\tFailing syndrome %u\n",
+				       err.s.cp2syn);
+			result = IRQ_HANDLED;
+		}
+	}
+
+	if (rsl_blocks.s.fpa) {	/* 5 - FPA_INT_SUM */
+		cvmx_fpa_int_sum_t err;
+
+		err.u64 = cvmx_read_csr(CVMX_FPA_INT_SUM);
+		cvmx_write_csr(CVMX_FPA_INT_SUM, err.u64);
+		if (err.u64) {
+			/* Don't report disabled errors */
+			err.u64 &= cvmx_read_csr(CVMX_FPA_INT_ENB);
+			if (err.s.q7_perr)
+				printk("FPA: Queue 7 pointer read from the stack in the L2C does not have the FPA owner ship bit set.\n");
+			if (err.s.q7_coff)
+				printk("FPA: Queue 7 stack end tag is present and the count available is greater than than pointers present in the FPA.\n");
+			if (err.s.q7_und)
+				printk("FPA: Queue 7 page count available goes negative.\n");
+			if (err.s.q6_perr)
+				printk("FPA: Queue 6 pointer read from the stack in the L2C does not have the FPA owner ship bit set.\n");
+			if (err.s.q6_coff)
+				printk("FPA: Queue 6 stack end tag is present and the count available is greater than than pointers present in the FPA.\n");
+			if (err.s.q6_und)
+				printk("FPA: Queue 6 page count available goes negative.\n");
+			if (err.s.q5_perr)
+				printk("FPA: Queue 5 pointer read from the stack in the L2C does not have the FPA owner ship bit set.\n");
+			if (err.s.q5_coff)
+				printk("FPA: Queue 5 stack end tag is present and the count available is greater than than pointers present in the FPA.\n");
+			if (err.s.q5_und)
+				printk("FPA: Queue 5 page count available goes negative.\n");
+			if (err.s.q4_perr)
+				printk("FPA: Queue 4 pointer read from the stack in the L2C does not have the FPA owner ship bit set.\n");
+			if (err.s.q4_coff)
+				printk("FPA: Queue 4 stack end tag is present and the count available is greater than than pointers present in the FPA.\n");
+			if (err.s.q4_und)
+				printk("FPA: Queue 4 page count available goes negative.\n");
+			if (err.s.q3_perr)
+				printk("FPA: Queue 3 pointer read from the stack in the L2C does not have the FPA owner ship bit set.\n");
+			if (err.s.q3_coff)
+				printk("FPA: Queue 3 stack end tag is present and the count available is greater than than pointers present in the FPA.\n");
+			if (err.s.q3_und)
+				printk("FPA: Queue 3 page count available goes negative.\n");
+			if (err.s.q2_perr)
+				printk("FPA: Queue 2 pointer read from the stack in the L2C does not have the FPA owner ship bit set.\n");
+			if (err.s.q2_coff)
+				printk("FPA: Queue 2 stack end tag is present and the count available is greater than than pointers present in the FPA.\n");
+			if (err.s.q2_und)
+				printk("FPA: Queue 2 page count available goes negative.\n");
+			if (err.s.q1_perr)
+				printk("FPA: Queue 1 pointer read from the stack in the L2C does not have the FPA owner ship bit set.\n");
+			if (err.s.q1_coff)
+				printk("FPA: Queue 1 stack end tag is present and the count available is greater than than pointers present in the FPA.\n");
+			if (err.s.q1_und)
+				printk("FPA: Queue 1 page count available goes negative.\n");
+			if (err.s.q0_perr)
+				printk("FPA: Queue 0 pointer read from the stack in the L2C does not have the FPA owner ship bit set.\n");
+			if (err.s.q0_coff)
+				printk("FPA: Queue 0 stack end tag is present and the count available is greater than than pointers present in the FPA.\n");
+			if (err.s.q0_und)
+				printk("FPA: Queue 0 page count available goes negative.\n");
+			if (err.s.fed1_dbe)
+				printk("ECC: FPA Double Bit Error detected in FPF1.\n");
+			if (err.s.fed1_sbe && ECC_REPORT_SINGLE_BIT_ERRORS)
+				printk("ECC: FPA Single Bit Error detected in FPF1.\n");
+			if (err.s.fed0_dbe)
+				printk("ECC: FPA Double Bit Error detected in FPF0.\n");
+			if (err.s.fed0_sbe && ECC_REPORT_SINGLE_BIT_ERRORS)
+				printk("ECC: FPA Single Bit Error detected in FPF0.\n");
+			result = IRQ_HANDLED;
+		}
+	}
+
+	if (rsl_blocks.s.key) {	/* 4 - KEY_INT_SUM */
+		cvmx_key_int_sum_t err;
+
+		err.u64 = cvmx_read_csr(CVMX_KEY_INT_SUM);
+		cvmx_write_csr(CVMX_KEY_INT_SUM, err.u64);
+		if (err.u64) {
+			/* Don't report disabled errors */
+			err.u64 &= cvmx_read_csr(CVMX_KEY_INT_ENB);
+			if (err.s.ked1_dbe)
+				printk("ECC: KED1 double-bit error.\n");
+			if (err.s.ked1_sbe && ECC_REPORT_SINGLE_BIT_ERRORS)
+				printk("ECC: KED1 single-bit error.\n");
+			if (err.s.ked0_dbe)
+				printk("ECC: KED0 double-bit error.\n");
+			if (err.s.ked0_sbe && ECC_REPORT_SINGLE_BIT_ERRORS)
+				printk("ECC: KED0 single-bit error.\n");
+			result = IRQ_HANDLED;
+		}
+	}
+
+	if (rsl_blocks.s.npei) {	/* 3 - NPI_INT_SUM */
+		if (!octeon_has_feature(OCTEON_FEATURE_PCIE)) {
+			cvmx_pci_int_sum2_t err;
+
+			err.u64 = cvmx_read_csr(CVMX_NPI_PCI_INT_SUM2);
+			cvmx_write_csr(CVMX_NPI_PCI_INT_SUM2, err.u64);
+			if (err.u64) {
+				/* Don't report disabled errors */
+				err.u64 &= cvmx_read_csr(CVMX_NPI_PCI_INT_ENB2);
+				if (err.s.ill_rd)
+					printk("PCI ERROR[ILL_RD]: A read to a disabled area of BAR1 or BAR2, when the mem area is disabled.\n");
+				if (err.s.ill_wr)
+					printk("PCI ERROR[ILL_WR]: A write to a disabled area of BAR1 or BAR2, when the mem area is disabled.\n");
+				if (err.s.win_wr)
+					printk("PCI ERROR[WIN_WR]: A write to the disabled window write-data register took place.\n");
+				if (err.s.dma1_fi)
+					printk("PCI ERROR[DMA1_FI]: A DMA operation operation finished that was required to set the FORCE-INT bit for counter 1.\n");
+				if (err.s.dma0_fi)
+					printk("PCI ERROR[DMA0_FI]: A DMA operation operation finished that was required to set the FORCE-INT bit for counter 0.\n");
+				if (err.s.dtime1)
+					printk("PCI ERROR[DTIME1]: When the value in the PCI_DMA_CNT1 register is not 0 the DMA_CNT1\n" "                  timer counts. When the DMA1_CNT timer has a value greater than the\n" "                  PCI_DMA_TIME1 register this bit is set. The timer is reset when the bit\n" "                  is written with a one.\n");
+				if (err.s.dtime0)
+					printk("PCI ERROR[DTIME0]: When the value in the PCI_DMA_CNT0 register is not 0 the DMA_CNT0\n" "                  timer counts. When the DMA0_CNT timer has a value greater than the\n" "                  PCI_DMA_TIME0 register this bit is set. The timer is reset when the bit\n" "                  is written with a one.\n");
+				if (err.s.dcnt1)
+					printk("PCI ERROR[DCNT1]: This bit indicates that PCI_DMA_CNT1 value is greater than the value in\n" "                  the PCI_DMA_INT_LEV1 register.\n");
+				if (err.s.dcnt0)
+					printk("PCI ERROR[DCNT0]: This bit indicates that PCI_DMA_CNT0 value is greater than the value in\n" "                  the PCI_DMA_INT_LEV0 register.\n");
+				if (err.s.ptime3)
+					printk("PCI ERROR[PTIME3]: When the value in the PCI_PKTS_SENT3 register is not 0 the Sent3\n" "                  timer counts. When the Sent3 timer has a value greater than the\n" "                  PCI_PKTS_SENT_TIME3 register this bit is set. The timer is reset when\n" "                  the bit is written with a 1.\n");
+				if (err.s.ptime2)
+					printk("PCI ERROR[PTIME2]: When the value in the PCI_PKTS_SENT(2) register is not 0 the Sent2 timer\n" "                  counts. When the Sent2 timer has a value greater than the\n" "                  PCI_PKTS_SENT2 register this bit is set. The timer is reset when the bit\n" "                  is written with a 1.\n");
+				if (err.s.ptime1)
+					printk("PCI ERROR[PTIME1]: When the value in the PCI_PKTS_SENT(1) register is not 0 the Sent1 timer\n" "                  counts. When the Sent1 timer has a value greater than the\n" "                  PCI_PKTS_SENT1 register this bit is set. The timer is reset when the bit\n" "                  is written with a 1.\n");
+				if (err.s.ptime0)
+					printk("PCI ERROR[PTIME0]: When the value in the PCI_PKTS_SENT0 register is not 0 the Sent0 timer\n" "                  counts. When the Sent0 timer has a value greater than the\n" "                  PCI_PKTS_SENT0 register this bit is set. The timer is reset when the bit\n" "                  is written with a 1.\n");
+				if (err.s.pcnt3)
+					printk("PCI ERROR[PCNT3]: This bit indicates that PCI_PKTS_SENT3 value is greater than the value in\n" "                  the PCI_PKTS_SENT_INT_LEV3 register.\n");
+				if (err.s.pcnt2)
+					printk("PCI ERROR[PCNT2]: This bit indicates that PCI_PKTS_SENT2 value is greater than the value in\n" "                  the PCI_PKTS_SENT_INT_LEV2 register.\n");
+				if (err.s.pcnt1)
+					printk("PCI ERROR[PCNT1]: This bit indicates that PCI_PKTS_SENT1 value is greater than the value in\n" "                  the PCI_PKTS_SENT_INT_LEV1 register.\n");
+				if (err.s.pcnt0)
+					printk("PCI ERROR[PCNT0]: This bit indicates that PCI_PKTS_SENT0 value is greater than the value in\n" "                  the PCI_PKTS_SENT_INT_LEV0 register.\n");
+				if (err.s.rsl_int)
+					printk("PCI ERROR[RSL_INT]: This bit is set when the RSL Chain has generated an interrupt.\n");
+				if (err.s.ill_rrd)
+					printk("PCI ERROR[ILL_RRD]: A read to the disabled PCI registers took place.\n");
+				if (err.s.ill_rwr)
+					printk("PCI ERROR[ILL_RWR]: A write to the disabled PCI registers took place.\n");
+				if (err.s.dperr)
+					printk("PCI ERROR[DPERR]: Data parity error detected by PCX core\n");
+				if (err.s.aperr)
+					printk("PCI ERROR[APERR]: Address parity error detected by PCX core\n");
+				if (err.s.serr)
+					printk("PCI ERROR[SERR]: SERR# detected by PCX core\n");
+				if (err.s.tsr_abt)
+					printk("PCI ERROR[TSR_ABT]: Target split-read abort detected\n");
+				if (err.s.msc_msg)
+					printk("PCI ERROR[MSC_MSG]: Master split completion message detected\n");
+				if (err.s.msi_mabt)
+					printk("PCI ERROR[MSI_MABT]: PCI MSI master abort.\n");
+				if (err.s.msi_tabt)
+					printk("PCI ERROR[MSI_TABT]: PCI MSI target abort.\n");
+				if (err.s.msi_per)
+					printk("PCI ERROR[MSI_PER]: PCI MSI parity error.\n");
+				if (err.s.mr_tto)
+					printk("PCI ERROR[MR_TTO]: PCI master retry time-out on read.\n");
+				if (err.s.mr_abt)
+					printk("PCI ERROR[MR_ABT]: PCI master abort on read.\n");
+				if (err.s.tr_abt)
+					printk("PCI ERROR[TR_ABT]: PCI target abort on read.\n");
+				if (err.s.mr_wtto)
+					printk("PCI ERROR[MR_WTTO]: PCI master retry time-out on write.\n");
+				if (err.s.mr_wabt)
+					printk("PCI ERROR[MR_WABT]: PCI master abort detected on write.\n");
+				if (err.s.tr_wabt)
+					printk("PCI ERROR[TR_WABT]: PCI target abort detected on write.\n");
+				result = IRQ_HANDLED;
+			}
+		} else {
+			printk("FIXME: NPEI_INT_SUM needs decode\n");
+		}
+	}
+
+	if (rsl_blocks.s.gmx1) {	/* 2 - GMX1_RX*_INT_REG &
+					   GMX1_TX_INT_REG */
+		/* Currently not checked. These should be checked by the
+		   ethernet driver */
+	}
+
+	if (rsl_blocks.s.gmx0) {	/* 1 - GMX0_RX*_INT_REG &
+					   GMX0_TX_INT_REG */
+		/* Currently not checked. These should be checked by the
+		   ethernet driver */
+	}
+
+	if (rsl_blocks.s.mio) {	/* 0 - MIO_BOOT_ERR */
+		cvmx_mio_boot_err_t err;
+
+		err.u64 = cvmx_read_csr(CVMX_MIO_BOOT_ERR);
+		cvmx_write_csr(CVMX_MIO_BOOT_ERR, err.u64);
+		if (err.u64) {
+			/* Don't report disabled errors */
+			err.u64 &= cvmx_read_csr(CVMX_MIO_BOOT_INT);
+			if (err.s.wait_err)
+				printk("BOOT BUS: Wait mode error\n");
+			if (err.s.adr_err)
+				printk("BOOT BUS: Address decode error\n");
+			result = IRQ_HANDLED;
+		}
+	}
+
+	return result;
+}
+
+
+/**
+ * Return a string representing the system type
+ *
+ * @return
+ */
+const char *get_system_type(void)
+{
+	return octeon_board_type_string();
+}
+
+
+/**
+ * Early entry point for arch setup
+ */
+void prom_init(void)
+{
+	const int coreid = cvmx_get_core_num();
+	int i;
+	int argc;
+	struct uart_port octeon_port;
+	int octeon_uart;
+	extern void pci_console_init(const char *arg);
+
+	octeon_hal_init();
+	octeon_check_cpu_bist();
+#ifdef CONFIG_CAVIUM_OCTEON_2ND_KERNEL
+	octeon_uart = 1;
+#else
+	octeon_uart = octeon_get_boot_uart();
+#endif
+
+	/* Disable All CIU Interrupts. The ones we need will be enabled later.
+	   Read the SUM register so we know the write completed. */
+	cvmx_write_csr(CVMX_CIU_INTX_EN0((coreid * 2)), 0);
+	cvmx_write_csr(CVMX_CIU_INTX_EN0((coreid * 2 + 1)), 0);
+	cvmx_write_csr(CVMX_CIU_INTX_EN1((coreid * 2)), 0);
+	cvmx_write_csr(CVMX_CIU_INTX_EN1((coreid * 2 + 1)), 0);
+	cvmx_read_csr(CVMX_CIU_INTX_SUM0((coreid * 2)));
+
+#ifdef CONFIG_SMP
+	octeon_write_lcd("LinuxSMP");
+#else
+	octeon_write_lcd("Linux");
+#endif
+
+#if !defined(CONFIG_KGDB) || !defined(CONFIG_CAVIUM_GDB)
+	/* When debugging the linux kernel, force the cores to enter the debug
+	   exception handler to break in.  */
+	if (octeon_get_boot_debug_flag()) {
+		cvmx_write_csr(CVMX_CIU_DINT, 1 << cvmx_get_core_num());
+		cvmx_read_csr(CVMX_CIU_DINT);
+	}
+#endif
+
+	/* Default to 64MB in the simulator to speed things up */
+	if (octeon_is_simulation())
+		MAX_MEMORY = 64ull << 20;
+
+	arcs_cmdline[0] = 0;
+	argc = octeon_get_boot_num_arguments();
+	for (i = 0; i < argc; i++) {
+		const char *arg = octeon_get_boot_argument(i);
+		if ((strncmp(arg, "MEM=", 4) == 0) ||
+		    (strncmp(arg, "mem=", 4) == 0)) {
+			sscanf(arg + 4, "%llu", &MAX_MEMORY);
+			MAX_MEMORY <<= 20;
+			if (MAX_MEMORY == 0)
+				MAX_MEMORY = 32ull << 30;
+		} else if (strcmp(arg, "ecc_verbose") == 0) {
+			ECC_REPORT_SINGLE_BIT_ERRORS = 1;
+			printk("Reporting of single bit ECC errors is turned on\n");
+		} else if (strlen(arcs_cmdline) + strlen(arg) + 1 <
+			   sizeof(arcs_cmdline) - 1) {
+			strcat(arcs_cmdline, " ");
+			strcat(arcs_cmdline, arg);
+		}
+	}
+
+	if (strstr(arcs_cmdline, "console=pci"))
+		pci_console_init(strstr(arcs_cmdline, "console=pci") + 8);
+
+	if (strstr(arcs_cmdline, "console=") == NULL) {
+#ifdef CONFIG_GDB_CONSOLE
+		strcat(arcs_cmdline, " console=gdb");
+#else
+#ifdef CONFIG_CAVIUM_OCTEON_2ND_KERNEL
+		strcat(arcs_cmdline, " console=ttyS0,115200");
+#else
+		if (octeon_uart == 1)
+			strcat(arcs_cmdline, " console=ttyS1,115200");
+		else
+			strcat(arcs_cmdline, " console=ttyS0,115200");
+#endif
+#endif
+	}
+
+	if (octeon_is_simulation()) {
+		/* The simulator uses a mtdram device pre filled with the
+		   filesystem. Also specify the calibration delay to avoid
+		   calculating it every time */
+		strcat(arcs_cmdline,
+		       " rw root=1f00 lpj=60176 slram=root,0x40000000,+1073741824");
+	}
+
+	/* you should these macros defined in include/asm/bootinfo.h */
+	mips_machgroup = MACH_GROUP_CAVIUM;
+	mips_machtype = MACH_CAVIUM_OCTEON;
+
+	mips_hpt_frequency = octeon_get_clock_rate();
+	clocksource_mips.read = octeon_hpt_read;
+	mips_timer_ack = octeon_timer_ack;
+	CYCLES_PER_JIFFY = ((mips_hpt_frequency + HZ / 2) / HZ);
+
+	_machine_restart = octeon_restart;
+	_machine_halt = octeon_halt;
+
+	memset(&octeon_port, 0, sizeof(octeon_port));
+	octeon_port.flags = ASYNC_SKIP_TEST | UPF_SHARE_IRQ;
+	octeon_port.iotype = UPIO_MEM;
+	octeon_port.regshift = 3;	/* I/O addresses are every 8 bytes */
+	octeon_port.uartclk = mips_hpt_frequency;	/* Clock rate of the
+							   chip */
+	octeon_port.fifosize = 64;
+	octeon_port.mapbase = 0x0001180000000800ull + (1024 * octeon_uart);
+	octeon_port.membase = cvmx_phys_to_ptr(octeon_port.mapbase);
+#ifdef CONFIG_CAVIUM_OCTEON_2ND_KERNEL
+	octeon_port.line = 0;
+#else
+	octeon_port.line = octeon_uart;
+#endif
+	if (!OCTEON_IS_MODEL(OCTEON_CN38XX_PASS2))	/* Only CN38XXp{1,2}
+							   has errata with uart
+							   interrupt */
+		octeon_port.irq = 42 + octeon_uart;
+	early_serial_setup(&octeon_port);
+
+	octeon_user_io_init();
+
+#ifdef CONFIG_KGDB
+	{
+		extern void putDebugChar(char ch);
+		const char *s = "\r\nConnect GDB to this port\r\n";
+		while (*s)
+			putDebugChar(*s++);
+	}
+#endif
+}
+
+
+
+void __init plat_mem_setup(void)
+{
+	uint64_t mem_alloc_size;
+	uint64_t total;
+
+	/* The Mips memory init uses the first memory location for some memory
+	   vectors. When SPARSEMEM is in use, it doesn't verify that the size
+	   is big enough for the final vectors. Making the smallest chuck 4MB
+	   seems to be enough to consistantly work. This needs to be debugged
+	   more */
+	mem_alloc_size = 4 << 20;
+	total = 0;
+	if (mem_alloc_size > MAX_MEMORY)
+		mem_alloc_size = MAX_MEMORY;
+
+	/* When allocating memory, we want incrementing addresses from
+	   bootmem_alloc so the code in add_memory_region can merge regions
+	   next to each other */
+	cvmx_bootmem_lock();
+	while ((boot_mem_map.nr_map < BOOT_MEM_MAP_MAX) && (total < MAX_MEMORY)) {
+#if defined(CONFIG_64BIT) || defined(CONFIG_64BIT_PHYS_ADDR)
+		int64_t memory =
+			cvmx_bootmem_phy_alloc(mem_alloc_size, 0, 0, 0x100000,
+					       CVMX_BOOTMEM_FLAG_NO_LOCKING);
+#elif defined(CONFIG_HIGHMEM)
+		int64_t memory =
+			cvmx_bootmem_phy_alloc(mem_alloc_size, 0, 1ull << 31,
+					       0x100000,
+					       CVMX_BOOTMEM_FLAG_NO_LOCKING);
+#else
+		int64_t memory =
+			cvmx_bootmem_phy_alloc(mem_alloc_size, 0, 512 << 20,
+					       0x100000,
+					       CVMX_BOOTMEM_FLAG_NO_LOCKING);
+#endif
+		if (memory >= 0) {
+			/* This function automatically merges address regions
+			   next to each other if they are received in
+			   incrementing order */
+			add_memory_region(memory, mem_alloc_size, BOOT_MEM_RAM);
+			total += mem_alloc_size;
+		} else
+			break;
+	}
+	cvmx_bootmem_unlock();
+
+	if (total == 0)
+		panic("Unable to allocate memory from cvmx_bootmem_phy_alloc\n");
+}
+
+
+void prom_free_prom_memory(void)
+{
+	/* Add an interrupt handler for general failures. */
+	request_irq(OCTEON_IRQ_RML, octeon_rlm_interrupt, SA_SHIRQ, "RML Error",
+		    octeon_rlm_interrupt);
+
+	/* Enable interrupt on IOB port SOP and EOP errors */
+	{
+		cvmx_iob_int_enb_t csr;
+		csr.u64 = cvmx_read_csr(CVMX_IOB_INT_ENB);
+		if (!OCTEON_IS_MODEL(OCTEON_CN3XXX)) {
+			/* These two don't exist for chips before CN58XX */
+			csr.s.p_dat = 1;
+			csr.s.np_dat = 1;
+		}
+		csr.s.p_eop = 1;
+		csr.s.p_sop = 1;
+		csr.s.np_eop = 1;
+		csr.s.np_sop = 1;
+		cvmx_write_csr(CVMX_IOB_INT_ENB, csr.u64);
+	}
+
+	/* Enable ASX interrupts on errors */
+	if (OCTEON_IS_MODEL(OCTEON_CN3XXX) || OCTEON_IS_MODEL(OCTEON_CN58XX) ||
+	    OCTEON_IS_MODEL(OCTEON_CN50XX)) {
+		int mask = 0xf;
+		cvmx_asxx_int_en_t csr;
+		if (OCTEON_IS_MODEL(OCTEON_CN31XX) ||
+		    OCTEON_IS_MODEL(OCTEON_CN30XX) ||
+		    OCTEON_IS_MODEL(OCTEON_CN50XX)) {
+			mask = 0x7;
+		} else {
+			csr.u64 = cvmx_read_csr(CVMX_ASXX_INT_EN(1));
+			csr.s.txpsh = mask;
+			csr.s.txpop = mask;
+			csr.s.ovrflw = mask;
+			cvmx_write_csr(CVMX_ASXX_INT_EN(1), csr.u64);
+		}
+
+		csr.u64 = cvmx_read_csr(CVMX_ASXX_INT_EN(0));
+		csr.s.txpsh = mask;
+		csr.s.txpop = mask;
+		csr.s.ovrflw = mask;
+		cvmx_write_csr(CVMX_ASXX_INT_EN(0), csr.u64);
+	}
+
+	/* Enable PIP interrupts on errors */
+	{
+		cvmx_pip_int_en_t csr;
+		csr.u64 = cvmx_read_csr(CVMX_PIP_INT_EN);
+		csr.s.beperr = 1;
+		csr.s.feperr = 1;
+		csr.s.todoovr = 1;
+		csr.s.skprunt = 1;
+		csr.s.badtag = 1;
+		csr.s.prtnxa = 1;
+		csr.s.bckprs = 0;
+		csr.s.crcerr = 0;
+		csr.s.pktdrp = 0;
+		cvmx_write_csr(CVMX_PIP_INT_EN, csr.u64);
+	}
+
+	/* Enable ECC Interrupts for double bit errors from main memory */
+	if (OCTEON_IS_MODEL(OCTEON_CN3XXX) || OCTEON_IS_MODEL(OCTEON_CN58XX) ||
+	    OCTEON_IS_MODEL(OCTEON_CN50XX)) {
+		cvmx_lmc_mem_cfg0_t csr;
+		csr.u64 = cvmx_read_csr(CVMX_LMC_MEM_CFG0);
+		csr.s.intr_ded_ena = 1;
+		csr.s.intr_sec_ena = ECC_REPORT_SINGLE_BIT_ERRORS;
+		cvmx_write_csr(CVMX_LMC_MEM_CFG0, csr.u64);
+	} else {
+		cvmx_lmc_mem_cfg0_t csr;
+		cvmx_l2c_cfg_t l2c_cfg;
+		l2c_cfg.u64 = cvmx_read_csr(CVMX_L2C_CFG);
+		if (l2c_cfg.s.dpres0) {
+			csr.u64 = cvmx_read_csr(CVMX_LMCX_MEM_CFG0(0));
+			csr.s.intr_ded_ena = 1;
+			csr.s.intr_sec_ena = ECC_REPORT_SINGLE_BIT_ERRORS;
+			cvmx_write_csr(CVMX_LMCX_MEM_CFG0(0), csr.u64);
+		}
+		if (l2c_cfg.s.dpres1) {
+			csr.u64 = cvmx_read_csr(CVMX_LMCX_MEM_CFG0(1));
+			csr.s.intr_ded_ena = 1;
+			csr.s.intr_sec_ena = ECC_REPORT_SINGLE_BIT_ERRORS;
+			cvmx_write_csr(CVMX_LMCX_MEM_CFG0(1), csr.u64);
+		}
+	}
+
+	/* Enable ECC Interrupts for double bit errors from L2C Tags */
+	{
+		cvmx_l2t_err_t csr;
+		csr.u64 = cvmx_read_csr(CVMX_L2T_ERR);
+		csr.s.lck_intena2 = 1;
+		csr.s.lck_intena = 1;
+		csr.s.ded_intena = 1;
+		csr.s.sec_intena = ECC_REPORT_SINGLE_BIT_ERRORS;
+		csr.s.ecc_ena = 1;
+		cvmx_write_csr(CVMX_L2T_ERR, csr.u64);
+	}
+
+	/* Enable ECC Interrupts for double bit errors from L2D Errors */
+	{
+		cvmx_l2d_err_t csr;
+		csr.u64 = cvmx_read_csr(CVMX_L2D_ERR);
+		csr.s.ded_intena = 1;
+		csr.s.sec_intena = ECC_REPORT_SINGLE_BIT_ERRORS;
+		csr.s.ecc_ena = 1;
+		cvmx_write_csr(CVMX_L2D_ERR, csr.u64);
+	}
+
+	/* Enable ECC Interrupts for double bit errors from the POW */
+	{
+		cvmx_pow_ecc_err_t csr;
+		csr.u64 = cvmx_read_csr(CVMX_POW_ECC_ERR);
+		if (!OCTEON_IS_MODEL(OCTEON_CN38XX_PASS2) &&
+		    !OCTEON_IS_MODEL(OCTEON_CN31XX)) {
+			/* These doesn't exist for chips CN31XX and CN38XXp2 */
+			csr.s.iop_ie = 0x1fff;
+		}
+		csr.s.rpe_ie = 1;
+		csr.s.dbe_ie = 1;
+		csr.s.sbe_ie = ECC_REPORT_SINGLE_BIT_ERRORS;
+		cvmx_write_csr(CVMX_POW_ECC_ERR, csr.u64);
+	}
+
+	/* Enable Timer interrupt on errors */
+	{
+		cvmx_tim_reg_int_mask_t csr;
+		csr.u64 = cvmx_read_csr(CVMX_TIM_REG_INT_MASK);
+		csr.s.mask = 0xffff;
+		cvmx_write_csr(CVMX_TIM_REG_INT_MASK, csr.u64);
+	}
+
+	/* Enable PKO interrupt on errors */
+	{
+		cvmx_pko_reg_int_mask_t csr;
+		csr.u64 = cvmx_read_csr(CVMX_PKO_REG_INT_MASK);
+		if (!OCTEON_IS_MODEL(OCTEON_CN3XXX)) {
+			/* These doesn't exist for chips before CN58XX */
+			csr.s.currzero = 1;
+		}
+		csr.s.doorbell = 1;
+		csr.s.parity = 1;
+		cvmx_write_csr(CVMX_PKO_REG_INT_MASK, csr.u64);
+	}
+
+	/* Enable interrupts on IPD errors */
+	{
+		cvmx_ipd_int_enb_t csr;
+		csr.u64 = cvmx_read_csr(CVMX_IPD_INT_ENB);
+		if (!OCTEON_IS_MODEL(OCTEON_CN3XXX) &&
+		    !OCTEON_IS_MODEL(OCTEON_CN58XX) &&
+		    !OCTEON_IS_MODEL(OCTEON_CN50XX)) {
+			/* These doesn't exist for chips before CN56XX */
+			csr.s.pq_sub = 1;
+			csr.s.pq_add = 1;
+		}
+		if (!OCTEON_IS_MODEL(OCTEON_CN38XX_PASS2) &&
+		    !OCTEON_IS_MODEL(OCTEON_CN31XX) &&
+		    !OCTEON_IS_MODEL(OCTEON_CN30XX) &&
+		    !OCTEON_IS_MODEL(OCTEON_CN50XX)) {
+			/* These doesn't exist for chips CN31XX and CN38XXp2 */
+			csr.s.bc_ovr = 1;
+			csr.s.d_coll = 1;
+			csr.s.c_coll = 1;
+			csr.s.cc_ovr = 1;
+			csr.s.dc_ovr = 1;
+		}
+		csr.s.bp_sub = 1;
+		csr.s.prc_par3 = 1;
+		csr.s.prc_par2 = 1;
+		csr.s.prc_par1 = 1;
+		csr.s.prc_par0 = 1;
+		cvmx_write_csr(CVMX_IPD_INT_ENB, csr.u64);
+	}
+
+	/* Enable zip interrupt on errors */
+	if (octeon_has_feature(OCTEON_FEATURE_ZIP)) {
+		cvmx_zip_int_mask_t csr;
+		csr.u64 = cvmx_read_csr(CVMX_ZIP_INT_MASK);
+		csr.s.doorbell = 1;
+		cvmx_write_csr(CVMX_ZIP_INT_MASK, csr.u64);
+	}
+
+	/* Enable DFA interrupts on errors. CN30XX, CN56XX don't have a DFA
+	   block. For others we have to read the fuse in CVMCTL to know if is
+	   there. We disable DFA_ERROR[CP2PARENA] since they fire even when you
+	   aren't using the parity features */
+	if (!OCTEON_IS_MODEL(OCTEON_CN30XX) &&
+	    !OCTEON_IS_MODEL(OCTEON_CN56XX) &&
+	    !OCTEON_IS_MODEL(OCTEON_CN50XX) &&
+	    ((__read_64bit_c0_register($9, 7) & (1 << 28)) == 0)) {
+		cvmx_dfa_err_t csr;
+		csr.u64 = cvmx_read_csr(CVMX_DFA_ERR);
+		csr.s.dblina = 1;
+		csr.s.cp2pina = 1;
+		csr.s.cp2parena = 0;
+		csr.s.dtepina = 1;
+		csr.s.dteparena = 1;
+		csr.s.dtedbina = 1;
+		csr.s.dtesbina = ECC_REPORT_SINGLE_BIT_ERRORS;
+		csr.s.dteeccena = 1;
+		csr.s.cp2dbina = 1;
+		csr.s.cp2sbina = ECC_REPORT_SINGLE_BIT_ERRORS;
+		csr.s.cp2eccena = 1;
+		cvmx_write_csr(CVMX_DFA_ERR, csr.u64);
+	}
+
+	/* Enable FPA interrupt on errors */
+	{
+		/* The Queue X stack end tag check is disabled due to Octeon
+		   Pass 2 errata FPA-100. This error condition can be set
+		   erroneously. */
+		cvmx_fpa_int_enb_t csr;
+		csr.u64 = cvmx_read_csr(CVMX_FPA_INT_ENB);
+		csr.s.q7_perr = 1;
+		csr.s.q7_coff = 0;
+		csr.s.q7_und = 1;
+		csr.s.q6_perr = 1;
+		csr.s.q6_coff = 0;
+		csr.s.q6_und = 1;
+		csr.s.q5_perr = 1;
+		csr.s.q5_coff = 0;
+		csr.s.q5_und = 1;
+		csr.s.q4_perr = 1;
+		csr.s.q4_coff = 0;
+		csr.s.q4_und = 1;
+		csr.s.q3_perr = 1;
+		csr.s.q3_coff = 0;
+		csr.s.q3_und = 1;
+		csr.s.q2_perr = 1;
+		csr.s.q2_coff = 0;
+		csr.s.q2_und = 1;
+		csr.s.q1_perr = 1;
+		csr.s.q1_coff = 0;
+		csr.s.q1_und = 1;
+		csr.s.q0_perr = 1;
+		csr.s.q0_coff = 0;
+		csr.s.q0_und = 1;
+		csr.s.fed1_dbe = 1;
+		csr.s.fed1_sbe = ECC_REPORT_SINGLE_BIT_ERRORS;
+		csr.s.fed0_dbe = 1;
+		csr.s.fed0_sbe = ECC_REPORT_SINGLE_BIT_ERRORS;
+		cvmx_write_csr(CVMX_FPA_INT_ENB, csr.u64);
+	}
+
+	/* Enable Key memory interupts on errors */
+	if (octeon_has_feature(OCTEON_FEATURE_KEY_MEMORY)) {
+		cvmx_key_int_enb_t csr;
+		csr.u64 = cvmx_read_csr(CVMX_KEY_INT_ENB);
+		csr.s.ked1_dbe = 1;
+		csr.s.ked1_sbe = ECC_REPORT_SINGLE_BIT_ERRORS;
+		csr.s.ked0_dbe = 1;
+		csr.s.ked0_sbe = ECC_REPORT_SINGLE_BIT_ERRORS;
+		cvmx_write_csr(CVMX_KEY_INT_ENB, csr.u64);
+	}
+
+	/* Enable reporting PCI bus errors */
+	if (octeon_is_pci_host()) {
+		cvmx_pci_int_enb2_t csr;
+		cvmx_npi_int_enb_t enb;
+		csr.u64 = cvmx_read_csr(CVMX_NPI_PCI_INT_ENB2);
+		csr.s.ill_rd = 1;
+		csr.s.ill_wr = 1;
+		csr.s.win_wr = 1;
+		csr.s.dma1_fi = 1;
+		csr.s.dma0_fi = 1;
+		csr.s.rdtime1 = 1;
+		csr.s.rdtime0 = 1;
+		csr.s.rdcnt1 = 1;
+		csr.s.rdcnt0 = 1;
+		csr.s.rptime3 = 1;
+		csr.s.rptime2 = 1;
+		csr.s.rptime1 = 1;
+		csr.s.rptime0 = 1;
+		csr.s.rpcnt3 = 1;
+		csr.s.rpcnt2 = 1;
+		csr.s.rpcnt1 = 1;
+		csr.s.rpcnt0 = 1;
+		csr.s.rrsl_int = 1;
+		csr.s.ill_rrd = 1;
+		csr.s.ill_rwr = 1;
+		csr.s.rdperr = 1;
+		csr.s.raperr = 1;
+		csr.s.rserr = 1;
+		csr.s.rtsr_abt = 1;
+		csr.s.rmsc_msg = 0;
+		csr.s.rmsi_mabt = 1;
+		csr.s.rmsi_tabt = 1;
+		csr.s.rmsi_per = 1;
+		csr.s.rmr_tto = 1;
+		csr.s.rmr_abt = 0;
+		csr.s.rtr_abt = 1;
+		csr.s.rmr_wtto = 1;
+		csr.s.rmr_wabt = 1;
+		csr.s.rtr_wabt = 1;
+		cvmx_write_csr(CVMX_NPI_PCI_INT_ENB2, csr.u64);
+
+		enb.u64 = cvmx_read_csr(CVMX_NPI_INT_ENB);
+		enb.s.pci_rsl = 1;
+		cvmx_write_csr(CVMX_NPI_INT_ENB, enb.u64);
+	}
+
+	/* Enable Bootbus interupts on errors */
+	{
+		cvmx_mio_boot_int_t csr;
+		csr.u64 = cvmx_read_csr(CVMX_MIO_BOOT_INT);
+		csr.s.adr_int = 1;
+		csr.s.wait_int = 1;
+		cvmx_write_csr(CVMX_MIO_BOOT_INT, csr.u64);
+	}
+
+	/* This call is here so that it is performed after any TLB
+	   initializations. It needs to be after these in case the
+	   CONFIG_CAVIUM_RESERVE32_USE_WIRED_TLB option is set */
+	octeon_hal_setup_reserved32();
+}
diff --git a/arch/mips/cavium-octeon/simulator.c b/arch/mips/cavium-octeon/simulator.c
new file mode 100644
index 0000000..719dd01
--- /dev/null
+++ b/arch/mips/cavium-octeon/simulator.c
@@ -0,0 +1,28 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2004-2007 Cavium Networks
+ */
+
+
+/**
+ * Octeon Simulator magic printf interface. Generates printf
+ * style output on the simulator console. Format arguments
+ * must all be %ll?. Nothing other than 64bit numbers can
+ * be displayed.
+ *
+ * @param fmt    Format string
+ */
+void octeon_simprintf(const char *fmt, ...)
+{
+	asm volatile ("\
+        sync\n\
+        add $25, $0, 6\n\
+        dli $15,0x8000000feffe0000\n\
+        dadd $24, $31, $0\n\
+        jalr $15\n\
+        dadd $31, $24, $0\n\
+        "::);
+}
diff --git a/arch/mips/cavium-octeon/smp.c b/arch/mips/cavium-octeon/smp.c
new file mode 100644
index 0000000..32a911f
--- /dev/null
+++ b/arch/mips/cavium-octeon/smp.c
@@ -0,0 +1,200 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2004-2007 Cavium Networks
+ */
+#include <linux/init.h>
+#include <linux/delay.h>
+#include <linux/smp.h>
+#include <linux/kernel_stat.h>
+#include <linux/sched.h>
+#include <linux/module.h>
+#include <asm/mmu_context.h>
+#include <asm/time.h>
+#include <asm/system.h>
+
+#include "hal.h"
+
+extern void octeon_user_io_init(void);
+
+volatile unsigned long octeon_processor_boot = 0xff;
+volatile unsigned long octeon_processor_cycle;
+volatile unsigned long octeon_processor_sp;
+volatile unsigned long octeon_processor_gp;
+
+
+/**
+ * Cause the function described by call_data to be executed on the passed
+ * cpu.  When the function has finished, increment the finished field of
+ * call_data.
+ *
+ * @param cpu
+ * @param action
+ */
+void core_send_ipi(int cpu, unsigned int action)
+{
+	int coreid = cpu_logical_map(cpu);
+	// printk("SMP: Mailbox send cpu=%d, coreid=%d, action=%u\n", cpu,
+	// coreid, action);
+	cvmx_write_csr(CVMX_CIU_MBOX_SETX(coreid), action);
+}
+
+
+/**
+ * Detect available CPUs, populate phys_cpu_present_map
+ */
+void plat_smp_setup(void)
+{
+	const int coreid = cvmx_get_core_num();
+	int cpus;
+	int id;
+
+	int core_mask = octeon_get_boot_coremask();
+
+	cpus_clear(phys_cpu_present_map);
+	__cpu_number_map[coreid] = 0;
+	__cpu_logical_map[0] = coreid;
+	cpu_set(0, phys_cpu_present_map);
+
+	cpus = 1;
+	for (id = 0; id < 16; id++) {
+		if ((id != coreid) && (core_mask & (1 << id))) {
+			cpu_set(cpus, phys_cpu_present_map);
+			__cpu_number_map[id] = cpus;
+			__cpu_logical_map[cpus] = id;
+			cpus++;
+		}
+	}
+}
+
+
+/**
+ * Firmware CPU startup hook
+ *
+ * @param cpu
+ * @param idle
+ */
+void prom_boot_secondary(int cpu, struct task_struct *idle)
+{
+	int count;
+
+	printk("SMP: Booting CPU%02d (CoreId %2d)...", cpu,
+	       cpu_logical_map(cpu));
+
+	octeon_processor_sp = __KSTK_TOS(idle);
+	octeon_processor_gp = (unsigned long) idle->thread_info;
+	__sync();		/* Use sync so all ops are done. This makes the
+				   cycle counter propagate in a more bounded
+				   amount of time */
+	octeon_processor_cycle = get_cycles();
+	octeon_processor_boot = cpu_logical_map(cpu);
+	mb();
+
+	count = 10000;
+	while (octeon_processor_sp && count) {
+		/* Waiting for processor to get the SP and GP */
+		udelay(1);
+		count--;
+	}
+	if (count == 0)
+		printk("Timeout\n");
+}
+
+
+/**
+ * After we've done initial boot, this function is called to allow the
+ * board code to clean up state, if needed
+ */
+void prom_init_secondary(void)
+{
+	const int coreid = cvmx_get_core_num();
+	cvmx_ciu_intx0_t interrupt_enable;
+
+	octeon_check_cpu_bist();
+
+	// printk("SMP: CPU%d (CoreId %lu) started\n", cpu, coreid);
+
+	/* Enable Mailbox interrupts to this core. These are the only
+	   interrupts allowed on line 3 */
+	cvmx_write_csr(CVMX_CIU_MBOX_CLRX(coreid), 0xffffffff);
+	interrupt_enable.u64 = 0;
+	interrupt_enable.s.mbox = 0x3;
+	cvmx_write_csr(CVMX_CIU_INTX_EN0((coreid * 2)), 0);
+	cvmx_write_csr(CVMX_CIU_INTX_EN0((coreid * 2 + 1)),
+		       interrupt_enable.u64);
+	cvmx_write_csr(CVMX_CIU_INTX_EN1((coreid * 2)), 0);
+	cvmx_write_csr(CVMX_CIU_INTX_EN1((coreid * 2 + 1)), 0);
+	set_c0_status(0x8c01);	/* Enable core interrupt processing for 2,3 and
+				   7 */
+}
+
+
+/**
+ * Callout to firmware before smp_init
+ *
+ * @param max_cpus
+ */
+void plat_prepare_cpus(unsigned int max_cpus)
+{
+	extern void plat_irq_dispatch(void);
+	const int coreid = cvmx_get_core_num();
+	cvmx_ciu_intx0_t interrupt_enable;
+
+	/* This irq register is just a placeholder. For speed, the low level
+	   interrupt handler calls mailbox_interrupt directly. This just lets
+	   the normal interrupt handling stuff know */
+	request_irq(3, (irq_handler_t) plat_irq_dispatch, SA_SHIRQ, "IPC",
+		    plat_irq_dispatch);
+
+	/* Enable Mailbox interrupts to this core. These are the only
+	   interrupts allowed on line 3 */
+	cvmx_write_csr(CVMX_CIU_MBOX_CLRX(coreid), 0xffffffff);
+	interrupt_enable.u64 = 0;
+	interrupt_enable.s.mbox = 0x3;
+	cvmx_write_csr(CVMX_CIU_INTX_EN0((coreid * 2 + 1)),
+		       interrupt_enable.u64);
+}
+
+
+/**
+ * Last chance for the board code to finish SMP initialization before
+ * the CPU is "online".
+ */
+void prom_smp_finish(void)
+{
+#ifdef CONFIG_CAVIUM_GDB
+	unsigned long tmp;
+	/* Pulse MCD0 signal on Ctrl-C to stop all the cores. Also set the MCD0
+	   to be not masked by this core so we know the signal is received by
+	   someone */
+	asm volatile ("dmfc0 %0, $22\n"
+		      "ori   %0, %0, 0x9100\n" "dmtc0 %0, $22\n":"=r" (tmp));
+#endif
+
+#ifdef CONFIG_CAVIUM_OCTEON_USER_MEM
+	octeon_user_io_init();
+#endif
+
+	/* to generate the first CPU timer interrupt */
+	write_c0_compare(read_c0_count() + mips_hpt_frequency / HZ);
+}
+
+
+/**
+ * Hook for after all CPUs are online
+ */
+void prom_cpus_done(void)
+{
+#ifdef CONFIG_CAVIUM_GDB
+	unsigned long tmp;
+	/* Pulse MCD0 signal on Ctrl-C to stop all the cores. Also set the MCD0
+	   to be not masked by this core so we know the signal is received by
+	   someone */
+	asm volatile ("dmfc0 %0, $22\n"
+		      "ori   %0, %0, 0x9100\n" "dmtc0 %0, $22\n":"=r" (tmp));
+#endif
+}
+
+EXPORT_SYMBOL(__cpu_logical_map);
diff --git a/arch/mips/cavium-octeon/userio.c b/arch/mips/cavium-octeon/userio.c
new file mode 100644
index 0000000..972dde6
--- /dev/null
+++ b/arch/mips/cavium-octeon/userio.c
@@ -0,0 +1,162 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2004-2007 Cavium Networks
+ */
+#include <linux/init.h>
+#include <linux/delay.h>
+#include <linux/irq.h>
+#include <linux/interrupt.h>
+#include <linux/serial.h>
+#include <linux/types.h>
+#include <linux/string.h>	/* for memset */
+#include <linux/console.h>
+#include <linux/serial.h>
+#include <linux/tty.h>
+#include <asm/time.h>
+#include <linux/serial_core.h>
+
+#include <asm/reboot.h>
+#include <asm/io.h>
+#include <asm/time.h>
+#include <asm/processor.h>
+#include <asm/reboot.h>
+#include <asm/system.h>
+#include <asm/irq_cpu.h>
+#include <asm/mipsregs.h>
+#include <asm/gdb-stub.h>
+#include <asm/bootinfo.h>
+
+#include "hal.h"
+
+
+/**
+ *
+ * @return
+ */
+void octeon_user_io_init(void)
+{
+	octeon_cvmemctl_t cvmmemctl;
+	cvmx_iob_fau_timeout_t fau_timeout;
+	cvmx_pow_nw_tim_t nm_tim;
+	uint64_t cvmctl;
+
+	/* Get the current settings for CP0_CVMMEMCTL_REG */
+	cvmmemctl.u64 = __read_64bit_c0_register($11, 7);
+
+	cvmmemctl.s.dismarkwblongto = 1;	/* R/W If set, marked
+						   write-buffer entries time
+						   out the same as as other
+						   entries; if clear, marked
+						   write-buffer entries use the
+						   maximum timeout. */
+	cvmmemctl.s.dismrgclrwbto = 0;	/* R/W If set, a merged store does not
+					   clear the write-buffer entry timeout
+					   state. */
+	cvmmemctl.s.iobdmascrmsb = 0;	/* R/W Two bits that are the MSBs of
+					   the resultant CVMSEG LM word
+					   location for an IOBDMA. The other 8
+					   bits come from the SCRADDR field of
+					   the IOBDMA. */
+	cvmmemctl.s.syncwsmarked = 0;	/* R/W If set, SYNCWS and SYNCS only
+					   order marked stores; if clear,
+					   SYNCWS and SYNCS only order unmarked
+					   stores. SYNCWSMARKED has no effect
+					   when DISSYNCWS is set. */
+	cvmmemctl.s.dissyncws = 0;	/* R/W If set, SYNCWS acts as SYNCW and
+					   SYNCS acts as SYNC. */
+	if (OCTEON_IS_MODEL(OCTEON_CN38XX_PASS2))
+		cvmmemctl.s.diswbfst = 1;	/* R/W If set, no stall happens
+						   on write buffer full. */
+	else
+		cvmmemctl.s.diswbfst = 0;	/* R/W If set, no stall happens
+						   on write buffer full. */
+	cvmmemctl.s.xkmemenas = 0;	/* R/W If set (and SX set),
+					   supervisor-level loads/stores can
+					   use XKPHYS addresses with VA<48>==0 */
+#ifdef CONFIG_CAVIUM_OCTEON_USER_MEM
+	cvmmemctl.s.xkmemenau = 1;	/* R/W If set (and UX set), user-level
+					   loads/stores can use XKPHYS
+					   addresses with VA<48>==0 */
+#else
+	cvmmemctl.s.xkmemenau = 0;
+#endif
+	cvmmemctl.s.xkioenas = 0;	/* R/W If set (and SX set),
+					   supervisor-level loads/stores can
+					   use XKPHYS addresses with VA<48>==1 */
+#ifdef CONFIG_CAVIUM_OCTEON_USER_IO
+	cvmmemctl.s.xkioenau = 1;	/* R/W If set (and UX set), user-level
+					   loads/stores can use XKPHYS
+					   addresses with VA<48>==1 */
+#else
+	cvmmemctl.s.xkioenau = 0;
+#endif
+	cvmmemctl.s.allsyncw = 0;	/* R/W If set, all stores act as SYNCW
+					   (NOMERGE must be set when this is
+					   set) RW, reset to 0. */
+	cvmmemctl.s.nomerge = 0;	/* R/W If set, no stores merge, and all
+					   stores reach the coherent bus in
+					   order. */
+	cvmmemctl.s.didtto = 0;	/* R/W Selects the bit in the counter used for
+				   DID time-outs 0 = 231, 1 = 230, 2 = 229, 3 =
+				   214. Actual time-out is between 1 and 2
+				   this interval. For example, with DIDTTO=3,
+				   expiration interval is between 16K and 32K. */
+	cvmmemctl.s.csrckalwys = 0;	/* R/W If set, the (mem) CSR clock
+					   never turns off. */
+	cvmmemctl.s.mclkalwys = 0;	/* R/W If set, mclk never turns off. */
+	cvmmemctl.s.wbfltime = 0;	/* R/W Selects the bit in the counter
+					   used for write buffer flush
+					   time-outs (WBFLT+11) is the bit
+					   position in an internal counter used
+					   to determine expiration. The write
+					   buffer expires between 1 and 2
+					   this interval. For example, with
+					   WBFLT = 0, a write buffer expires
+					   between 2K and 4K cycles after the
+					   write buffer entry is allocated. */
+	cvmmemctl.s.istrnol2 = 0;	/* R/W If set, do not put Istream in
+					   the L2 cache. */
+	cvmmemctl.s.wbthresh = 10;	/* R/W The write buffer threshold. */
+#if CONFIG_CAVIUM_OCTEON_CVMSEG_SIZE > 0
+	cvmmemctl.s.cvmsegenak = 1;	/* R/W If set, CVMSEG is available for
+					   loads/stores in kernel/debug mode. */
+#else
+	cvmmemctl.s.cvmsegenak = 0;
+#endif
+	cvmmemctl.s.cvmsegenas = 0;	/* R/W If set, CVMSEG is available for
+					   loads/stores in supervisor mode. */
+	cvmmemctl.s.cvmsegenau = 0;	/* R/W If set, CVMSEG is available for
+					   loads/stores in user mode. */
+	cvmmemctl.s.lmemsz = CONFIG_CAVIUM_OCTEON_CVMSEG_SIZE;
+	/* R/W Size of local memory in cache blocks, 54 (6912 bytes) is max
+	   legal value. */
+
+	if (smp_processor_id() == 0)
+		printk("CVMSEG size: %d cache lines (%d bytes)\n",
+		       CONFIG_CAVIUM_OCTEON_CVMSEG_SIZE,
+		       CONFIG_CAVIUM_OCTEON_CVMSEG_SIZE * 128);
+
+	__write_64bit_c0_register($11, 7, cvmmemctl.u64);
+
+	/* Move the performance counter interrupts to IRQ 6 */
+	cvmctl = __read_64bit_c0_register($9, 7);
+	cvmctl &= ~(7 << 7);
+	cvmctl |= 6 << 7;
+	__write_64bit_c0_register($9, 7, cvmctl);
+
+	/* Set a default for the hardware timeouts */
+	fau_timeout.u64 = 0;
+	fau_timeout.s.tout_val = 0xfff;
+	fau_timeout.s.tout_enb = 0;	/* Disable tagwait FAU timeout */
+	cvmx_write_csr(CVMX_IOB_FAU_TIMEOUT, fau_timeout.u64);
+
+	nm_tim.u64 = 0;
+	nm_tim.s.nw_tim = 3;	/* 4096 cycles */
+	cvmx_write_csr(CVMX_POW_NW_TIM, nm_tim.u64);
+
+	write_c0_cacheerr(0);
+	write_c0_derraddr1(0);
+}
diff --git a/arch/mips/cavium-octeon/watchdog.c b/arch/mips/cavium-octeon/watchdog.c
new file mode 100644
index 0000000..3120ef6
--- /dev/null
+++ b/arch/mips/cavium-octeon/watchdog.c
@@ -0,0 +1,136 @@
+/*
+ *   Octeon Watchdog driver
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2007 Cavium Networks
+ */
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/ip.h>
+#include <linux/string.h>
+#include <linux/delay.h>
+#include "hal.h"
+
+static struct timer_list watchdog_poke_timers[NR_CPUS];
+
+int timeout = 0;		/* A zero value is replaced with the max
+				   timeout */
+module_param(timeout, int, 0444);
+MODULE_PARM_DESC(timeout,
+		 " Watchdog timeout in milliseconds. First timeout causes a masked interrupt, \n"
+		 "\t\tthe second causes an NMI, and the third causes a soft reset.");
+
+/**
+ * Poke the watchdog when a timer fires
+ *
+ * @param unused
+ */
+static void watchdog_poke(unsigned long unused)
+{
+	/* We're alive, poke the watchdog */
+	// printk("Poking watchdog on core %d\n", cvmx_get_core_num());
+	cvmx_write_csr(CVMX_CIU_PP_POKEX(cvmx_get_core_num()), 1);
+	mod_timer(watchdog_poke_timers + smp_processor_id(), jiffies + HZ/10);
+}
+
+
+/**
+ * Module/ driver initialization.
+ *
+ * @return Zero on success
+ */
+static int __init watchdog_init(void)
+{
+	cvmx_ciu_wdogx_t ciu_wdog;
+	uint64_t threshold;
+	int cpu;
+	int i;
+	extern void octeon_watchdog_nmi(void);
+
+	/* Watchdog time expiration length = The 16 bits of LEN represent the
+	   most significant bits of a 24 bit decrementer that decrements every
+	   256 cycles. */
+	if (timeout)
+		threshold = timeout * octeon_get_clock_rate() / 1000 >> (8 + 8);
+	else {
+		threshold = 65535;
+		timeout =
+			(65535ull << (8 + 8)) * 1000 / octeon_get_clock_rate();
+	}
+	if ((threshold < 10) || (threshold >= 65536)) {
+		printk("Illegal watchdog timeout. Timeout must be between %u and %u.\n", (int) ((10ull << (8 + 8)) * 1000 / octeon_get_clock_rate()), (int) ((65535ull << (8 + 8)) * 1000 / octeon_get_clock_rate()));
+		return -1;
+	}
+
+	/* Install the NMI handler */
+	for (i = 0; i < 16; i++) {
+		uint64_t *ptr = (uint64_t *) octeon_watchdog_nmi;
+		cvmx_write_csr(CVMX_MIO_BOOT_LOC_ADR, i * 8);
+		cvmx_write_csr(CVMX_MIO_BOOT_LOC_DAT, ptr[i]);
+	}
+	cvmx_write_csr(CVMX_MIO_BOOT_LOC_CFGX(0), 0x81fc0000);
+
+	ciu_wdog.u64 = 0;
+	ciu_wdog.s.len = threshold;
+	ciu_wdog.s.mode = 3;	/* 3 = Interrupt + NMI + Soft-Reset */
+
+	preempt_disable();
+	for (cpu = 0; cpu < NR_CPUS; cpu++) {
+		if (cpu_online(cpu)) {
+#ifdef CONFIG_SMP
+			int core = cpu_logical_map(cpu);
+#else
+			int core = cvmx_get_core_num();
+#endif
+			/* Poke the watchdog to clear out its state */
+			cvmx_write_csr(CVMX_CIU_PP_POKEX(core), 1);
+
+			setup_timer(watchdog_poke_timers + cpu, watchdog_poke,
+				    0);
+			add_timer_on(watchdog_poke_timers + cpu, cpu);
+
+			/* Finally enable the watchdog now that all handlers
+			   are installed */
+			cvmx_write_csr(CVMX_CIU_WDOGX(core), ciu_wdog.u64);
+		}
+	}
+	preempt_enable();
+
+	printk("Octeon watchdog driver loaded with a timeout of %d ms.\n",
+	       timeout);
+	return 0;
+}
+
+
+/**
+ * Module / driver shutdown
+ */
+static void __exit watchdog_cleanup(void)
+{
+	int cpu;
+	preempt_disable();
+	for (cpu = 0; cpu < NR_CPUS; cpu++) {
+		if (cpu_online(cpu)) {
+#ifdef CONFIG_SMP
+			int core = cpu_logical_map(cpu);
+#else
+			int core = cvmx_get_core_num();
+#endif
+			/* Disable the watchdog */
+			cvmx_write_csr(CVMX_CIU_WDOGX(core), 0);
+			del_timer(watchdog_poke_timers + cpu);
+		}
+	}
+	preempt_enable();
+}
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Cavium Networks <support@caviumnetworks.com>");
+MODULE_DESCRIPTION("Cavium Networks Octeon Watchdog driver.");
+module_init(watchdog_init);
+module_exit(watchdog_cleanup);
diff --git a/arch/mips/cavium-octeon/watchdog_nmi.S b/arch/mips/cavium-octeon/watchdog_nmi.S
new file mode 100644
index 0000000..fe0be83
--- /dev/null
+++ b/arch/mips/cavium-octeon/watchdog_nmi.S
@@ -0,0 +1,32 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2007 Cavium Networks
+ */
+#include <asm/asm.h>
+#include <asm/mipsregs.h>
+#include <asm/addrspace.h>
+#include <asm/regdef.h>
+#include <asm/stackframe.h>
+
+	.text
+	.set 	noreorder
+        NESTED(octeon_watchdog_nmi, PT_SIZE, sp)
+	sync
+	dli	k0, 0x8001180000000840	// Uart 0 TX register
+	PTR_LA	k1, octeon_watchdog_nmi_message
+octeon_watchdog_nmi_print:
+	lb	t0, 0(k1)
+	sd	t0, 0(k0)
+	bnez	t0, octeon_watchdog_nmi_print
+	 daddu	k1, 1
+	sync
+octeon_watchdog_spin:
+	b	octeon_watchdog_spin
+	 nop
+octeon_watchdog_nmi_message:
+	.asciz "\r\n*** NMI Watchdog interrupt. Chip soft reset ***\r\n"
+        END(octeon_watchdog_nmi)
+
diff --git a/arch/mips/kernel/irq-octeon.c b/arch/mips/kernel/irq-octeon.c
new file mode 100644
index 0000000..8202b7d
--- /dev/null
+++ b/arch/mips/kernel/irq-octeon.c
@@ -0,0 +1,377 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2004-2007 Cavium Networks
+ */
+#include <linux/init.h>
+#include <linux/interrupt.h>
+#include <linux/kernel.h>
+#include <linux/pci.h>
+
+#include <asm/irq_cpu.h>
+#include <asm/mipsregs.h>
+#include <asm/system.h>
+
+#include "../cavium-octeon/hal.h"
+
+DEFINE_RWLOCK(octeon_irq_ciu_rwlock);
+DEFINE_SPINLOCK(octeon_irq_msi_lock);
+
+static void octeon_irq_core_ack(unsigned int irq)
+{
+	/* We don't need to disable IRQs to make these atomic since they are
+	   already disabled earlier in the low level interrupt code */
+	clear_c0_status(0x100 << irq);
+}
+
+static void octeon_irq_core_eoi(unsigned int irq)
+{
+	irq_desc_t *desc = irq_desc + irq;
+	/* If an IRQ is being processed while we are disabling it the handler
+	   will attempt to unmask the interrupt after it has been disabled */
+	if (desc->status & IRQ_DISABLED)
+		return;
+	/* We don't need to disable IRQs to make these atomic since they are
+	   already disabled earlier in the low level interrupt code */
+	clear_c0_cause(0x100 << irq);
+	set_c0_status(0x100 << irq);
+}
+
+static void octeon_irq_core_enable(unsigned int irq)
+{
+	/* We need to disable interrupts to make sure our updates are atomic */
+	unsigned long flags;
+	local_irq_save(flags);
+	clear_c0_cause(0x100 << irq);
+	set_c0_status(0x100 << irq);
+	local_irq_restore(flags);
+}
+
+static void octeon_irq_core_disable_local(unsigned int irq)
+{
+	/* We need to disable interrupts to make sure our updates are atomic */
+	unsigned long flags;
+	local_irq_save(flags);
+	clear_c0_status(0x100 << irq);
+	local_irq_restore(flags);
+}
+
+static void octeon_irq_core_disable(unsigned int irq)
+{
+#ifdef CONFIG_SMP
+	on_each_cpu((void (*)(void *)) octeon_irq_core_disable_local,
+		    (void *) (long) irq, 0, 1);
+#else
+	octeon_irq_core_disable_local(irq);
+#endif
+}
+
+struct irq_chip octeon_irq_chip_core = {
+	.name = "Core",
+	.enable = octeon_irq_core_enable,
+	.disable = octeon_irq_core_disable,
+	.ack = octeon_irq_core_ack,
+	.eoi = octeon_irq_core_eoi,
+};
+
+
+static void octeon_irq_ciu_ack(unsigned int irq)
+{
+	/* In order to avoid any locking accessing the CIU, we acknowledge CIU
+	   interrupts by disabling all of them. This way we can use a per core
+	   register and avoid any out of core locking requirements. This has
+	   the side affect that CIU interrupts can't be processed recursively */
+	/* We don't need to disable IRQs to make these atomic since they are
+	   already disabled earlier in the low level interrupt code */
+	clear_c0_status(0x100 << 2);
+}
+
+static void octeon_irq_ciu_eoi(unsigned int irq)
+{
+	/* Enable all CIU interrupts again */
+	/* We don't need to disable IRQs to make these atomic since they are
+	   already disabled earlier in the low level interrupt code */
+	set_c0_status(0x100 << 2);
+}
+
+static void octeon_irq_ciu_enable(unsigned int irq)
+{
+	int coreid = cvmx_get_core_num();
+	if (unlikely(irq == OCTEON_IRQ_WATCHDOG)) {
+		/* The watchdog is special in that the normal EN0 bits can't
+		   turn it on. We need to use the EN1 bits */
+		cvmx_write_csr(CVMX_CIU_INTX_EN1(coreid * 2), 1 << coreid);
+	} else {
+		unsigned long flags;
+		uint64_t en0;
+		int bit = (irq - 8) & 0x3f;	/* Bit 0-63 of EN0 */
+
+		/* A read lock is used here to make sure only one core is ever
+		   updating the CIU enable bits at a time. During an enable the
+		   cores don't interfere with each other. During a disable the
+		   write lock stops any enables that might cause a problem */
+		read_lock_irqsave(&octeon_irq_ciu_rwlock, flags);
+		en0 = cvmx_read_csr(CVMX_CIU_INTX_EN0(coreid * 2));
+		en0 |= 1ull << bit;
+		cvmx_write_csr(CVMX_CIU_INTX_EN0(coreid * 2), en0);
+		cvmx_read_csr(CVMX_CIU_INTX_EN0(coreid * 2));
+		read_unlock_irqrestore(&octeon_irq_ciu_rwlock, flags);
+	}
+}
+
+static void octeon_irq_ciu_disable(unsigned int irq)
+{
+	if (unlikely(irq == OCTEON_IRQ_WATCHDOG)) {
+		/* The watchdog is special in that the normal EN0 bits can't
+		   shut it off. We need to use the EN1 bits */
+#ifdef CONFIG_SMP
+		int cpu;
+		for (cpu = 0; cpu < NR_CPUS; cpu++) {
+			if (cpu_present(cpu)) {
+				cvmx_write_csr(CVMX_CIU_INTX_EN1
+					       (cpu_logical_map(cpu) * 2), 0);
+			}
+		}
+#else
+		cvmx_write_csr(CVMX_CIU_INTX_EN1(cvmx_get_core_num() * 2), 0);
+#endif
+	} else {
+		int bit = (irq - 8) & 0x3f;	/* Bit 0-63 of EN0 */
+		unsigned long flags;
+		uint64_t en0;
+#ifdef CONFIG_SMP
+		int cpu;
+		write_lock_irqsave(&octeon_irq_ciu_rwlock, flags);
+		for (cpu = 0; cpu < NR_CPUS; cpu++) {
+			if (cpu_present(cpu)) {
+				int coreid = cpu_logical_map(cpu);
+				en0 = cvmx_read_csr(CVMX_CIU_INTX_EN0
+						    (coreid * 2));
+				en0 &= ~(1ull << bit);
+				cvmx_write_csr(CVMX_CIU_INTX_EN0(coreid * 2),
+					       en0);
+			}
+		}
+		/* We need to do a read after the last update to make sure all
+		   of them are done */
+		cvmx_read_csr(CVMX_CIU_INTX_EN0(cvmx_get_core_num() * 2));
+		write_unlock_irqrestore(&octeon_irq_ciu_rwlock, flags);
+#else
+		int coreid = cvmx_get_core_num();
+		local_irq_save(flags);
+		en0 = cvmx_read_csr(CVMX_CIU_INTX_EN0(coreid * 2));
+		en0 &= ~(1ull << bit);
+		cvmx_write_csr(CVMX_CIU_INTX_EN0(coreid * 2), en0);
+		cvmx_read_csr(CVMX_CIU_INTX_EN0(coreid * 2));
+		local_irq_restore(flags);
+#endif
+	}
+}
+
+#ifdef CONFIG_SMP
+static void octeon_irq_ciu_set_affinity(unsigned int irq, cpumask_t dest)
+{
+	int cpu;
+	unsigned long flags;
+	irq_desc_t *desc = irq_desc + irq;
+	int bit = (irq - 8) & 0x3f;	/* Bit 0-63 of EN0 */
+
+	/* Watchdog affinity can't be changed */
+	if (unlikely(irq == OCTEON_IRQ_WATCHDOG))
+		return;
+
+	spin_lock_irqsave(&desc->lock, flags);
+	write_lock(&octeon_irq_ciu_rwlock);
+	for (cpu = 0; cpu < NR_CPUS; cpu++) {
+		if (cpu_present(cpu)) {
+			int coreid = cpu_logical_map(cpu);
+			uint64_t en0 =
+				cvmx_read_csr(CVMX_CIU_INTX_EN0(coreid * 2));
+			if (cpu_isset(cpu, dest))
+				en0 |= 1ull << bit;
+			else
+				en0 &= ~(1ull << bit);
+			cvmx_write_csr(CVMX_CIU_INTX_EN0(coreid * 2), en0);
+		}
+	}
+	/* We need to do a read after the last update to make sure all of them
+	   are done */
+	cvmx_read_csr(CVMX_CIU_INTX_EN0(cvmx_get_core_num() * 2));
+	write_unlock(&octeon_irq_ciu_rwlock);
+	spin_unlock_irqrestore(&desc->lock, flags);
+}
+#endif
+
+struct irq_chip octeon_irq_chip_ciu = {
+	.name = "CIU",
+	.enable = octeon_irq_ciu_enable,
+	.disable = octeon_irq_ciu_disable,
+	.ack = octeon_irq_ciu_ack,
+	.eoi = octeon_irq_ciu_eoi,
+#ifdef CONFIG_SMP
+	.set_affinity = octeon_irq_ciu_set_affinity,
+#endif
+};
+
+
+static void octeon_irq_i8289_master_unmask(unsigned int irq)
+{
+	unsigned long flags;
+	local_irq_save(flags);
+	outb(inb(0x21) & ~(1 << (irq - OCTEON_IRQ_I8259M0)), 0x21);
+	local_irq_restore(flags);
+}
+
+static void octeon_irq_i8289_master_mask(unsigned int irq)
+{
+	unsigned long flags;
+	local_irq_save(flags);
+	outb(inb(0x21) | (1 << (irq - OCTEON_IRQ_I8259M0)), 0x21);
+	local_irq_restore(flags);
+}
+
+struct irq_chip octeon_irq_chip_i8259_master = {
+	.name = "i8259M",
+	.mask = octeon_irq_i8289_master_mask,
+	.mask_ack = octeon_irq_i8289_master_mask,
+	.unmask = octeon_irq_i8289_master_unmask,
+	.eoi = octeon_irq_i8289_master_unmask,
+};
+
+
+static void octeon_irq_i8289_slave_unmask(unsigned int irq)
+{
+	outb(inb(0xa1) & ~(1 << (irq - OCTEON_IRQ_I8259S0)), 0xa1);
+}
+
+static void octeon_irq_i8289_slave_mask(unsigned int irq)
+{
+	outb(inb(0xa1) | (1 << (irq - OCTEON_IRQ_I8259S0)), 0xa1);
+}
+
+struct irq_chip octeon_irq_chip_i8259_slave = {
+	.name = "i8259S",
+	.mask = octeon_irq_i8289_slave_mask,
+	.mask_ack = octeon_irq_i8289_slave_mask,
+	.unmask = octeon_irq_i8289_slave_unmask,
+	.eoi = octeon_irq_i8289_slave_unmask,
+};
+
+#ifdef CONFIG_PCI_MSI
+
+static void octeon_irq_msi_ack(unsigned int irq)
+{
+	if (!octeon_has_feature(OCTEON_FEATURE_PCIE)) {
+		/* These chips have PCI */
+		cvmx_write_csr(CVMX_NPI_NPI_MSI_RCV,
+			       1ull << (irq - OCTEON_IRQ_MSI_BIT0));
+	} else {
+		/* These chips have PCIe. Thankfully the ACK doesn't need any
+		   locking */
+		cvmx_write_csr(CVMX_PEXP_NPEI_MSI_RCV0,
+			       1ull << (irq - OCTEON_IRQ_MSI_BIT0));
+	}
+}
+
+static void octeon_irq_msi_eoi(unsigned int irq)
+{
+	/* Nothing needed */
+}
+
+static void octeon_irq_msi_enable(unsigned int irq)
+{
+	if (!octeon_has_feature(OCTEON_FEATURE_PCIE)) {
+		/* Octeon PCI doesn't have the ability to mask/unmask MSI
+		   interrupts individually. Instead of masking/unmasking them
+		   in groups of 16, we simple assume MSI devices are well
+		   behaved. MSI interrupts are always enable and the ACK is
+		   assumed to be enough */
+	} else {
+		/* These chips have PCIe. Note that we only support the first
+		   64 MSI interrupts. Unfortunately all the MSI enables are in
+		   the same register. We use MSI0's lock to control access to
+		   them all. */
+		uint64_t en;
+		unsigned long flags;
+		spin_lock_irqsave(&octeon_irq_msi_lock, flags);
+		en = cvmx_read_csr(CVMX_PEXP_NPEI_MSI_ENB0);
+		en |= 1ull << (irq - OCTEON_IRQ_MSI_BIT0);
+		cvmx_write_csr(CVMX_PEXP_NPEI_MSI_ENB0, en);
+		cvmx_read_csr(CVMX_PEXP_NPEI_MSI_ENB0);
+		spin_unlock_irqrestore(&octeon_irq_msi_lock, flags);
+	}
+}
+
+static void octeon_irq_msi_disable(unsigned int irq)
+{
+	if (!octeon_has_feature(OCTEON_FEATURE_PCIE)) {
+		/* See comment in enable */
+	} else {
+		/* These chips have PCIe. Note that we only support the first
+		   64 MSI interrupts. Unfortunately all the MSI enables are in
+		   the same register. We use MSI0's lock to control access to
+		   them all. */
+		uint64_t en;
+		unsigned long flags;
+		spin_lock_irqsave(&octeon_irq_msi_lock, flags);
+		en = cvmx_read_csr(CVMX_PEXP_NPEI_MSI_ENB0);
+		en &= ~(1ull << (irq - OCTEON_IRQ_MSI_BIT0));
+		cvmx_write_csr(CVMX_PEXP_NPEI_MSI_ENB0, en);
+		cvmx_read_csr(CVMX_PEXP_NPEI_MSI_ENB0);
+		spin_unlock_irqrestore(&octeon_irq_msi_lock, flags);
+	}
+}
+
+struct irq_chip octeon_irq_chip_msi = {
+	.name = "MSI",
+	.enable = octeon_irq_msi_enable,
+	.disable = octeon_irq_msi_disable,
+	.ack = octeon_irq_msi_ack,
+	.eoi = octeon_irq_msi_eoi,
+};
+#endif
+
+void __init arch_init_irq(void)
+{
+	int irq;
+
+	/* Interrupt 0-7 map to the Mips internal interrupts. Cascaded off of
+	   interrupt 2, the CIU uses irq numbers 8-71. 72-79 are not used, they
+	   exist only to align the i8259 interrupts on a multiple of 16. This is
+	   so the Via southbridges don't get confused when they and off the lower
+	   bits for internal use. 80-87 is the master 8259, 88-95 is the slave. */
+
+	if (NR_IRQS < OCTEON_IRQ_LAST)
+		printk("octeon_irq_init: NR_IRQS is set too low\n");
+
+	for (irq = OCTEON_IRQ_SW0; irq <= OCTEON_IRQ_TIMER; irq++) {
+		set_irq_chip_and_handler(irq, &octeon_irq_chip_core,
+					 handle_percpu_irq);
+	}
+
+	for (irq = OCTEON_IRQ_WORKQ0; irq <= OCTEON_IRQ_BOOTDMA; irq++) {
+		set_irq_chip_and_handler(irq, &octeon_irq_chip_ciu,
+					 handle_percpu_irq);
+	}
+
+	for (irq = OCTEON_IRQ_I8259M0; irq <= OCTEON_IRQ_I8259M7; irq++) {
+		set_irq_chip_and_handler(irq, &octeon_irq_chip_i8259_master,
+					 handle_level_irq);
+	}
+
+	for (irq = OCTEON_IRQ_I8259S0; irq <= OCTEON_IRQ_I8259S7; irq++) {
+		set_irq_chip_and_handler(irq, &octeon_irq_chip_i8259_slave,
+					 handle_level_irq);
+	}
+
+#ifdef CONFIG_PCI_MSI
+	for (irq = OCTEON_IRQ_MSI_BIT0; irq <= OCTEON_IRQ_MSI_BIT63; irq++) {
+		set_irq_chip_and_handler(irq, &octeon_irq_chip_msi,
+					 handle_percpu_irq);
+	}
+#endif
+
+	set_c0_status(0x100 << 2);
+}
diff --git a/arch/mips/kernel/octeon_switch.S b/arch/mips/kernel/octeon_switch.S
new file mode 100644
index 0000000..a3fe6ca
--- /dev/null
+++ b/arch/mips/kernel/octeon_switch.S
@@ -0,0 +1,511 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 1994, 1995, 1996, 1998, 1999, 2002, 2003 Ralf Baechle
+ * Copyright (C) 1996 David S. Miller (dm@engr.sgi.com)
+ * Copyright (C) 1994, 1995, 1996, by Andreas Busse
+ * Copyright (C) 1999 Silicon Graphics, Inc.
+ * Copyright (C) 2000 MIPS Technologies, Inc.
+ *    written by Carsten Langgaard, carstenl@mips.com
+ */
+#include <asm/asm.h>
+#include <asm/cachectl.h>
+#include <asm/fpregdef.h>
+#include <asm/mipsregs.h>
+#include <asm/asm-offsets.h>
+#include <asm/page.h>
+#include <asm/pgtable-bits.h>
+#include <asm/regdef.h>
+#include <asm/stackframe.h>
+#include <asm/thread_info.h>
+
+#include <asm/asmmacro.h>
+
+/*
+ * Offset to the current process status flags, the first 32 bytes of the
+ * stack are not used.
+ */
+#define ST_OFF (_THREAD_SIZE - 32 - PT_SIZE + PT_STATUS)
+
+/*
+ * task_struct *resume(task_struct *prev, task_struct *next,
+ *                     struct thread_info *next_ti)
+ */
+	.align	7
+	LEAF(resume)
+	.set arch=octeon
+#ifndef CONFIG_CPU_HAS_LLSC
+	sw	zero, ll_bit
+#endif
+	mfc0	t1, CP0_STATUS
+	LONG_S	t1, THREAD_STATUS(a0)
+	cpu_save_nonscratch a0
+	LONG_S	ra, THREAD_REG31(a0)
+
+	/* check if we need to save COP2 registers */
+	PTR_L	t2, TASK_THREAD_INFO(a0)
+	LONG_L	t0, ST_OFF(t2)
+	bbit0	t0, 30, 1f
+
+	/* Disable COP2 in the stored process state */
+	li	t1, ST0_CU2
+	xor	t0, t1
+	LONG_S	t0, ST_OFF(t2)
+
+	/* Enable COP2 so we can save it */
+	mfc0	t0, CP0_STATUS
+	or	t0, t1
+	mtc0	t0, CP0_STATUS
+
+	/* Save COP2 */
+	daddu	a0, THREAD_CP2
+	jal octeon_cop2_save
+	dsubu	a0, THREAD_CP2
+
+	/* Disable COP2 now that we are done */
+	mfc0	t0, CP0_STATUS
+	li	t1, ST0_CU2
+	xor	t0, t1
+	mtc0	t0, CP0_STATUS
+
+1:
+#if CONFIG_CAVIUM_OCTEON_CVMSEG_SIZE > 0
+	/* Check if we need to store CVMSEG state */
+	mfc0	t0, $11,7 	/* CvmMemCtl */
+	bbit0	t0, 6, 3f	/* Is user access enabled? */
+
+	/* Store the CVMSEG state */
+	andi	t0, 0x3f	/* Extract the size of CVMSEG */
+	sll	t0, 7-LONGLOG-1	/* Multiply * (cache line size/sizeof(long)/2) */
+	li	t1, -32768 	/* Base address of CVMSEG */
+	LONG_ADDI t2, a0, THREAD_CVMSEG	/* Where to store CVMSEG to */
+	synciobdma
+2:
+	.set noreorder
+	LONG_L	t8, 0(t1)	/* Load from CVMSEG */
+	subu	t0, 1		/* Decrement loop var */
+	LONG_L	t9, LONGSIZE(t1)/* Load from CVMSEG */
+	LONG_ADDU t1, LONGSIZE*2 /* Increment loc in CVMSEG */
+	LONG_S	t8, 0(t2)	/* Store CVMSEG to thread storage */
+	LONG_ADDU t2, LONGSIZE*2 /* Increment loc in thread storage */
+	bnez	t0, 2b		/* Loop until we've copied it all */
+	 LONG_S	t9, -LONGSIZE(t2)/* Store CVMSEG to thread storage */
+	.set reorder
+
+	/* Disable access to CVMSEG */
+	mfc0	t0, $11,7 	/* CvmMemCtl */
+	xori	t0, t0, 0x40	/* Bit 6 is CVMSEG user enable */
+	mtc0	t0, $11,7 	/* CvmMemCtl */
+#endif
+3:
+	/*
+	 * The order of restoring the registers takes care of the race
+	 * updating $28, $29 and kernelsp without disabling ints.
+	 */
+	move	$28, a2
+	cpu_restore_nonscratch a1
+
+#if (_THREAD_SIZE - 32) < 0x8000
+	PTR_ADDIU	t0, $28, _THREAD_SIZE - 32
+#else
+	PTR_LI		t0, _THREAD_SIZE - 32
+	PTR_ADDU	t0, $28
+#endif
+	set_saved_sp	t0, t1, t2
+
+#ifdef CONFIG_FAST_ACCESS_TO_THREAD_POINTER
+	/* We need to put the thread pointer in CVMMEM immediately. The
+		kernel will use this value during TLB exceptions even
+		though userspace hasn't accessed CVMMEM */
+	LONG_L	t1, TI_TP_VALUE($28)
+	LONG_S	t1, FAST_ACCESS_THREAD_OFFSET($0)
+#endif
+
+	mfc0	t1, CP0_STATUS		/* Do we really need this? */
+	li	a3, 0xff01
+	and	t1, a3
+	LONG_L	a2, THREAD_STATUS(a1)
+	nor	a3, $0, a3
+	and	a2, a3
+	or	a2, t1
+	mtc0	a2, CP0_STATUS
+	move	v0, a0
+	jr	ra
+	END(resume)
+
+/*
+ * void octeon_cop2_save(struct octeon_cop2_state *a0)
+ */
+	.align	7
+	LEAF(octeon_cop2_save)
+
+	dmfc0	t9, $9,7	/* CvmCtl register. */
+
+        /* Save the COP2 CRC state */
+	dmfc2	t0, 0x0201
+	dmfc2	t1, 0x0202
+	dmfc2	t2, 0x0200
+	sd	t0, OCTEON_CP2_CRC_IV(a0)
+	sd	t1, OCTEON_CP2_CRC_LENGTH(a0)
+	sd	t2, OCTEON_CP2_CRC_POLY(a0)
+
+	bbit1	t9, 28, 1f	/* Skip next instructions if CvmCtl[NODFA_CP2] set */
+
+	/* Save the LLM state */
+	dmfc2	t0, 0x0402
+	dmfc2	t1, 0x040A
+	sd	t0, OCTEON_CP2_LLM_DAT(a0)
+	sd	t1, OCTEON_CP2_LLM_DAT+8(a0)
+
+1:      bbit1	t9, 26, 3f	/* done if CvmCtl[NOCRYPTO] set */
+
+	/* Save the COP2 crypto state */
+        /* this part is mostly common to both pass 1 and later revisions */
+	dmfc2 	t0, 0x0084
+	dmfc2 	t1, 0x0080
+	dmfc2 	t2, 0x0081
+	dmfc2 	t3, 0x0082
+	sd	t0, OCTEON_CP2_3DES_IV(a0)
+	dmfc2 	t0, 0x0088
+	sd	t1, OCTEON_CP2_3DES_KEY(a0)
+	dmfc2 	t1, 0x0111                      /* only necessary for pass 1 */
+	sd	t2, OCTEON_CP2_3DES_KEY+8(a0)
+	dmfc2 	t2, 0x0102
+	sd	t3, OCTEON_CP2_3DES_KEY+16(a0)
+	dmfc2 	t3, 0x0103
+	sd	t0, OCTEON_CP2_3DES_RESULT(a0)
+	dmfc2 	t0, 0x0104
+	sd	t1, OCTEON_CP2_AES_INP0(a0)     /* only necessary for pass 1 */
+	dmfc2 	t1, 0x0105
+	sd	t2, OCTEON_CP2_AES_IV(a0)
+	dmfc2	t2, 0x0106
+	sd	t3, OCTEON_CP2_AES_IV+8(a0)
+	dmfc2 	t3, 0x0107
+	sd	t0, OCTEON_CP2_AES_KEY(a0)
+	dmfc2	t0, 0x0110
+	sd	t1, OCTEON_CP2_AES_KEY+8(a0)
+	dmfc2	t1, 0x0100
+	sd	t2, OCTEON_CP2_AES_KEY+16(a0)
+	dmfc2	t2, 0x0101
+	sd	t3, OCTEON_CP2_AES_KEY+24(a0)
+	mfc0	t3, $15,0 	/* Get the processor ID register */
+	sd	t0, OCTEON_CP2_AES_KEYLEN(a0)
+	li	t0, 0x000d0000	/* This is the processor ID of Octeon Pass1 */
+	sd	t1, OCTEON_CP2_AES_RESULT(a0)
+	sd	t2, OCTEON_CP2_AES_RESULT+8(a0)
+
+	beq	t3, t0, 2f	/* Skip to the Pass1 version of the remainder of the COP2 state */
+
+        /* the non-pass1 state when !CvmCtl[NOCRYPTO] */
+	dmfc2	t1, 0x0240
+	dmfc2	t2, 0x0241
+	dmfc2	t3, 0x0242
+	dmfc2	t0, 0x0243
+	sd	t1, OCTEON_CP2_HSH_DATW(a0)
+	dmfc2	t1, 0x0244
+	sd	t2, OCTEON_CP2_HSH_DATW+8(a0)
+	dmfc2	t2, 0x0245
+	sd	t3, OCTEON_CP2_HSH_DATW+16(a0)
+	dmfc2	t3, 0x0246
+	sd	t0, OCTEON_CP2_HSH_DATW+24(a0)
+	dmfc2	t0, 0x0247
+	sd	t1, OCTEON_CP2_HSH_DATW+32(a0)
+	dmfc2	t1, 0x0248
+	sd	t2, OCTEON_CP2_HSH_DATW+40(a0)
+	dmfc2	t2, 0x0249
+	sd	t3, OCTEON_CP2_HSH_DATW+48(a0)
+	dmfc2	t3, 0x024A
+	sd	t0, OCTEON_CP2_HSH_DATW+56(a0)
+	dmfc2	t0, 0x024B
+	sd	t1, OCTEON_CP2_HSH_DATW+64(a0)
+	dmfc2	t1, 0x024C
+	sd	t2, OCTEON_CP2_HSH_DATW+72(a0)
+	dmfc2	t2, 0x024D
+	sd	t3, OCTEON_CP2_HSH_DATW+80(a0)
+	dmfc2 	t3, 0x024E
+	sd	t0, OCTEON_CP2_HSH_DATW+88(a0)
+	dmfc2	t0, 0x0250
+	sd	t1, OCTEON_CP2_HSH_DATW+96(a0)
+	dmfc2	t1, 0x0251
+	sd	t2, OCTEON_CP2_HSH_DATW+104(a0)
+	dmfc2	t2, 0x0252
+	sd	t3, OCTEON_CP2_HSH_DATW+112(a0)
+	dmfc2	t3, 0x0253
+	sd	t0, OCTEON_CP2_HSH_IVW(a0)
+	dmfc2	t0, 0x0254
+	sd	t1, OCTEON_CP2_HSH_IVW+8(a0)
+	dmfc2	t1, 0x0255
+	sd	t2, OCTEON_CP2_HSH_IVW+16(a0)
+	dmfc2	t2, 0x0256
+	sd	t3, OCTEON_CP2_HSH_IVW+24(a0)
+	dmfc2	t3, 0x0257
+	sd	t0, OCTEON_CP2_HSH_IVW+32(a0)
+	dmfc2 	t0, 0x0258
+	sd	t1, OCTEON_CP2_HSH_IVW+40(a0)
+	dmfc2 	t1, 0x0259
+	sd	t2, OCTEON_CP2_HSH_IVW+48(a0)
+	dmfc2	t2, 0x025E
+	sd	t3, OCTEON_CP2_HSH_IVW+56(a0)
+	dmfc2	t3, 0x025A
+	sd	t0, OCTEON_CP2_GFM_MULT(a0)
+	dmfc2	t0, 0x025B
+	sd	t1, OCTEON_CP2_GFM_MULT+8(a0)
+	sd	t2, OCTEON_CP2_GFM_POLY(a0)
+	sd	t3, OCTEON_CP2_GFM_RESULT(a0)
+	sd	t0, OCTEON_CP2_GFM_RESULT+8(a0)
+	jr	ra
+
+2:      /* pass 1 special stuff when !CvmCtl[NOCRYPTO] */
+	dmfc2	t3, 0x0040
+	dmfc2	t0, 0x0041
+	dmfc2	t1, 0x0042
+	dmfc2	t2, 0x0043
+	sd	t3, OCTEON_CP2_HSH_DATW(a0)
+	dmfc2	t3, 0x0044
+	sd	t0, OCTEON_CP2_HSH_DATW+8(a0)
+	dmfc2	t0, 0x0045
+	sd	t1, OCTEON_CP2_HSH_DATW+16(a0)
+	dmfc2	t1, 0x0046
+	sd	t2, OCTEON_CP2_HSH_DATW+24(a0)
+	dmfc2	t2, 0x0048
+	sd	t3, OCTEON_CP2_HSH_DATW+32(a0)
+	dmfc2	t3, 0x0049
+	sd	t0, OCTEON_CP2_HSH_DATW+40(a0)
+	dmfc2	t0, 0x004A
+	sd	t1, OCTEON_CP2_HSH_DATW+48(a0)
+	sd	t2, OCTEON_CP2_HSH_IVW(a0)
+	sd	t3, OCTEON_CP2_HSH_IVW+8(a0)
+	sd	t0, OCTEON_CP2_HSH_IVW+16(a0)
+
+3:      /* pass 1 or CvmCtl[NOCRYPTO] set */
+	jr	ra
+	END(octeon_cop2_save)
+
+/*
+ * void octeon_cop2_restore(struct octeon_cop2_state *a0)
+ */
+	.align	7
+	.set push
+	.set noreorder
+	LEAF(octeon_cop2_restore)
+        /* First cache line was prefetched before the call */
+        pref    4,  128(a0)
+	dmfc0	t9, $9,7	/* CvmCtl register. */
+
+        pref    4,  256(a0)
+	ld	t0, OCTEON_CP2_CRC_IV(a0)
+        pref    4,  384(a0)
+	ld	t1, OCTEON_CP2_CRC_LENGTH(a0)
+	ld	t2, OCTEON_CP2_CRC_POLY(a0)
+
+	/* Restore the COP2 CRC state */
+	dmtc2	t0, 0x0201
+	dmtc2 	t1, 0x1202
+	bbit1	t9, 28, 2f	/* Skip LLM if CvmCtl[NODFA_CP2] is set */
+	 dmtc2	t2, 0x4200
+
+	/* Restore the LLM state */
+	ld	t0, OCTEON_CP2_LLM_DAT(a0)
+	ld	t1, OCTEON_CP2_LLM_DAT+8(a0)
+	dmtc2	t0, 0x0402
+	dmtc2	t1, 0x040A
+
+2:
+	bbit1	t9, 26, done_restore	/* done if CvmCtl[NOCRYPTO] set */
+	 nop
+
+	/* Restore the COP2 crypto state common to pass 1 and pass 2 */
+	ld	t0, OCTEON_CP2_3DES_IV(a0)
+	ld	t1, OCTEON_CP2_3DES_KEY(a0)
+	ld	t2, OCTEON_CP2_3DES_KEY+8(a0)
+	dmtc2 	t0, 0x0084
+	ld	t0, OCTEON_CP2_3DES_KEY+16(a0)
+	dmtc2 	t1, 0x0080
+	ld	t1, OCTEON_CP2_3DES_RESULT(a0)
+	dmtc2 	t2, 0x0081
+	ld	t2, OCTEON_CP2_AES_INP0(a0)       /* only really needed for pass 1 */
+	dmtc2	t0, 0x0082
+	ld	t0, OCTEON_CP2_AES_IV(a0)
+	dmtc2 	t1, 0x0098
+	ld	t1, OCTEON_CP2_AES_IV+8(a0)
+	dmtc2 	t2, 0x010A                        /* only really needed for pass 1 */
+	ld	t2, OCTEON_CP2_AES_KEY(a0)
+	dmtc2 	t0, 0x0102
+	ld	t0, OCTEON_CP2_AES_KEY+8(a0)
+	dmtc2	t1, 0x0103
+	ld	t1, OCTEON_CP2_AES_KEY+16(a0)
+	dmtc2	t2, 0x0104
+	ld	t2, OCTEON_CP2_AES_KEY+24(a0)
+	dmtc2	t0, 0x0105
+	ld	t0, OCTEON_CP2_AES_KEYLEN(a0)
+	dmtc2	t1, 0x0106
+	ld	t1, OCTEON_CP2_AES_RESULT(a0)
+	dmtc2	t2, 0x0107
+	ld	t2, OCTEON_CP2_AES_RESULT+8(a0)
+	mfc0	t3, $15,0 	/* Get the processor ID register */
+	dmtc2	t0, 0x0110
+	li	t0, 0x000d0000	/* This is the processor ID of Octeon Pass1 */
+	dmtc2	t1, 0x0100
+	bne	t0, t3, 3f	/* Skip the next stuff for non-pass1 */
+	 dmtc2	t2, 0x0101
+
+        /* this code is specific for pass 1 */
+	ld	t0, OCTEON_CP2_HSH_DATW(a0)
+	ld	t1, OCTEON_CP2_HSH_DATW+8(a0)
+	ld	t2, OCTEON_CP2_HSH_DATW+16(a0)
+	dmtc2	t0, 0x0040
+	ld	t0, OCTEON_CP2_HSH_DATW+24(a0)
+	dmtc2	t1, 0x0041
+	ld	t1, OCTEON_CP2_HSH_DATW+32(a0)
+	dmtc2	t2, 0x0042
+	ld	t2, OCTEON_CP2_HSH_DATW+40(a0)
+	dmtc2	t0, 0x0043
+	ld	t0, OCTEON_CP2_HSH_DATW+48(a0)
+	dmtc2	t1, 0x0044
+	ld	t1, OCTEON_CP2_HSH_IVW(a0)
+	dmtc2	t2, 0x0045
+	ld	t2, OCTEON_CP2_HSH_IVW+8(a0)
+	dmtc2	t0, 0x0046
+	ld	t0, OCTEON_CP2_HSH_IVW+16(a0)
+	dmtc2	t1, 0x0048
+	dmtc2	t2, 0x0049
+        b done_restore   /* unconditional branch */
+	 dmtc2	t0, 0x004A
+
+3:      /* this is post-pass1 code */
+	ld	t2, OCTEON_CP2_HSH_DATW(a0)
+	ld	t0, OCTEON_CP2_HSH_DATW+8(a0)
+	ld	t1, OCTEON_CP2_HSH_DATW+16(a0)
+	dmtc2	t2, 0x0240
+	ld	t2, OCTEON_CP2_HSH_DATW+24(a0)
+	dmtc2	t0, 0x0241
+	ld	t0, OCTEON_CP2_HSH_DATW+32(a0)
+	dmtc2	t1, 0x0242
+	ld	t1, OCTEON_CP2_HSH_DATW+40(a0)
+	dmtc2	t2, 0x0243
+	ld	t2, OCTEON_CP2_HSH_DATW+48(a0)
+	dmtc2	t0, 0x0244
+	ld	t0, OCTEON_CP2_HSH_DATW+56(a0)
+	dmtc2	t1, 0x0245
+	ld	t1, OCTEON_CP2_HSH_DATW+64(a0)
+	dmtc2	t2, 0x0246
+	ld	t2, OCTEON_CP2_HSH_DATW+72(a0)
+	dmtc2	t0, 0x0247
+	ld	t0, OCTEON_CP2_HSH_DATW+80(a0)
+	dmtc2	t1, 0x0248
+	ld	t1, OCTEON_CP2_HSH_DATW+88(a0)
+	dmtc2	t2, 0x0249
+	ld	t2, OCTEON_CP2_HSH_DATW+96(a0)
+	dmtc2	t0, 0x024A
+	ld	t0, OCTEON_CP2_HSH_DATW+104(a0)
+	dmtc2	t1, 0x024B
+	ld	t1, OCTEON_CP2_HSH_DATW+112(a0)
+	dmtc2	t2, 0x024C
+	ld	t2, OCTEON_CP2_HSH_IVW(a0)
+	dmtc2	t0, 0x024D
+	ld	t0, OCTEON_CP2_HSH_IVW+8(a0)
+	dmtc2	t1, 0x024E
+	ld	t1, OCTEON_CP2_HSH_IVW+16(a0)
+	dmtc2	t2, 0x0250
+	ld	t2, OCTEON_CP2_HSH_IVW+24(a0)
+	dmtc2	t0, 0x0251
+	ld	t0, OCTEON_CP2_HSH_IVW+32(a0)
+	dmtc2	t1, 0x0252
+	ld	t1, OCTEON_CP2_HSH_IVW+40(a0)
+	dmtc2	t2, 0x0253
+	ld	t2, OCTEON_CP2_HSH_IVW+48(a0)
+	dmtc2	t0, 0x0254
+	ld	t0, OCTEON_CP2_HSH_IVW+56(a0)
+	dmtc2	t1, 0x0255
+	ld	t1, OCTEON_CP2_GFM_MULT(a0)
+	dmtc2	t2, 0x0256
+	ld	t2, OCTEON_CP2_GFM_MULT+8(a0)
+	dmtc2	t0, 0x0257
+	ld	t0, OCTEON_CP2_GFM_POLY(a0)
+	dmtc2	t1, 0x0258
+	ld	t1, OCTEON_CP2_GFM_RESULT(a0)
+	dmtc2	t2, 0x0259
+	ld	t2, OCTEON_CP2_GFM_RESULT+8(a0)
+	dmtc2	t0, 0x025E
+	dmtc2	t1, 0x025A
+	dmtc2	t2, 0x025B
+
+done_restore:
+	jr	ra
+	 nop
+	END(octeon_cop2_restore)
+	.set pop
+
+/*
+ * void octeon_mult_save()
+ * sp is assumed to point to a struct pt_regs
+ *
+ * NOTE: This is called in SAVE_SOME in stackframe.h. It can only
+ *       safely modify k0 and k1.
+ */
+	.align	7
+	.set push
+	.set noreorder
+	LEAF(octeon_mult_save)
+	dmfc0	k0, $9,7	/* CvmCtl register. */
+	bbit1	k0, 27, 1f	/* Skip CvmCtl[NOMUL] */
+	 nop
+
+	/* Save the multiplier state */
+	v3mulu	k0, $0, $0
+	v3mulu	k1, $0, $0
+	sd	k0, PT_MTP(sp)        /* PT_MTP    has P0 */
+	v3mulu	k0, $0, $0
+	sd	k1, PT_MTP+8(sp)      /* PT_MTP+8  has P1 */
+	ori	k1, $0, 1
+	v3mulu	k1, k1, $0
+	sd	k0, PT_MTP+16(sp)     /* PT_MTP+16 has P2 */
+	v3mulu	k0, $0, $0
+	sd	k1, PT_MPL(sp)        /* PT_MPL    has MPL0 */
+	v3mulu	k1, $0, $0
+	sd	k0, PT_MPL+8(sp)      /* PT_MPL+8  has MPL1 */
+	jr	ra
+	 sd	k1, PT_MPL+16(sp)     /* PT_MPL+16 has MPL2 */
+
+1:	/* Resume here if CvmCtl[NOMUL] */
+	jr	ra
+	END(octeon_mult_save)
+	.set pop
+
+/*
+ * void octeon_mult_restore()
+ * sp is assumed to point to a struct pt_regs
+ *
+ * NOTE: This is called in RESTORE_SOME in stackframe.h.
+ */
+	.align	7
+	.set push
+	.set noreorder
+	LEAF(octeon_mult_restore)
+	dmfc0	k1, $9,7		/* CvmCtl register. */
+	ld	v0, PT_MPL(sp)        	/* MPL0 */
+	ld	v1, PT_MPL+8(sp)      	/* MPL1 */
+	ld	k0, PT_MPL+16(sp)     	/* MPL2 */
+	bbit1	k1, 27, 1f		/* Skip CvmCtl[NOMUL] */
+	 nop				/* Normally falls through, so no time wasted here */
+
+	/* Restore the multiplier state */
+	ld	k1, PT_MTP+16(sp)     	/* P2 */
+	MTM0	v0			/* MPL0 */
+	ld	v0, PT_MTP+8(sp)	/* P1 */
+	MTM1	v1			/* MPL1 */
+	ld	v1, PT_MTP(sp)   	/* P0 */
+	MTM2	k0			/* MPL2 */
+	MTP2	k1			/* P2 */
+	MTP1	v0			/* P1 */
+	jr	ra
+	 MTP0	v1			/* P0 */
+
+1:	/* Resume here if CvmCtl[NOMUL] */
+	jr	ra
+	 nop
+	END(octeon_mult_restore)
+	.set pop
+
diff --git a/arch/mips/lib/octeon-memcpy.S b/arch/mips/lib/octeon-memcpy.S
new file mode 100644
index 0000000..dc25e9e
--- /dev/null
+++ b/arch/mips/lib/octeon-memcpy.S
@@ -0,0 +1,628 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Unified implementation of memcpy, memmove and the __copy_user backend.
+ *
+ * Copyright (C) 1998, 99, 2000, 01, 2002 Ralf Baechle (ralf@gnu.org)
+ * Copyright (C) 1999, 2000, 01, 2002 Silicon Graphics, Inc.
+ * Copyright (C) 2002 Broadcom, Inc.
+ *   memcpy/copy_user author: Mark Vandevoorde
+ *
+ * Mnemonic names for arguments to memcpy/__copy_user
+ */
+
+/*
+ * Hack to resolve longstanding prefetch issue
+ *
+ * Prefetching may be fatal on some systems if we're prefetching beyond the
+ * end of memory on some systems.  It's also a seriously bad idea on non
+ * dma-coherent systems.
+ */
+#ifndef CONFIG_CPU_CAVIUM_OCTEON
+#if !defined(CONFIG_DMA_COHERENT) || !defined(CONFIG_DMA_IP27)
+#undef CONFIG_CPU_HAS_PREFETCH
+#endif
+#ifdef CONFIG_MIPS_MALTA
+#undef CONFIG_CPU_HAS_PREFETCH
+#endif
+#endif
+
+#include <asm/asm.h>
+#include <asm/asm-offsets.h>
+#include <asm/regdef.h>
+
+#define dst a0
+#define src a1
+#define len a2
+
+/*
+ * Spec
+ *
+ * memcpy copies len bytes from src to dst and sets v0 to dst.
+ * It assumes that
+ *   - src and dst don't overlap
+ *   - src is readable
+ *   - dst is writable
+ * memcpy uses the standard calling convention
+ *
+ * __copy_user copies up to len bytes from src to dst and sets a2 (len) to
+ * the number of uncopied bytes due to an exception caused by a read or write.
+ * __copy_user assumes that src and dst don't overlap, and that the call is
+ * implementing one of the following:
+ *   copy_to_user
+ *     - src is readable  (no exceptions when reading src)
+ *   copy_from_user
+ *     - dst is writable  (no exceptions when writing dst)
+ * __copy_user uses a non-standard calling convention; see
+ * include/asm-mips/uaccess.h
+ *
+ * When an exception happens on a load, the handler must
+ # ensure that all of the destination buffer is overwritten to prevent
+ * leaking information to user mode programs.
+ */
+
+/*
+ * Implementation
+ */
+
+/*
+ * The exception handler for loads requires that:
+ *  1- AT contain the address of the byte just past the end of the source
+ *     of the copy,
+ *  2- src_entry <= src < AT, and
+ *  3- (dst - src) == (dst_entry - src_entry),
+ * The _entry suffix denotes values when __copy_user was called.
+ *
+ * (1) is set up up by uaccess.h and maintained by not writing AT in copy_user
+ * (2) is met by incrementing src by the number of bytes copied
+ * (3) is met by not doing loads between a pair of increments of dst and src
+ *
+ * The exception handlers for stores adjust len (if necessary) and return.
+ * These handlers do not need to overwrite any data.
+ *
+ * For __rmemcpy and memmove an exception is always a kernel bug, therefore
+ * they're not protected.
+ */
+
+#define EXC(inst_reg,addr,handler)		\
+9:	inst_reg, addr;				\
+	.section __ex_table,"a";		\
+	PTR	9b, handler;			\
+	.previous
+
+/*
+ * Only on the 64-bit kernel we can made use of 64-bit registers.
+ */
+#ifdef CONFIG_64BIT
+#define USE_DOUBLE
+#endif
+
+#ifdef USE_DOUBLE
+
+#define LOAD   ld
+#define LOADL  ldl
+#define LOADR  ldr
+#define STOREL sdl
+#define STORER sdr
+#define STORE  sd
+#define ADD    daddu
+#define SUB    dsubu
+#define SRL    dsrl
+#define SRA    dsra
+#define SLL    dsll
+#define SLLV   dsllv
+#define SRLV   dsrlv
+#define NBYTES 8
+#define LOG_NBYTES 3
+
+/*
+ * As we are sharing code base with the mips32 tree (which use the o32 ABI
+ * register definitions). We need to redefine the register definitions from
+ * the n64 ABI register naming to the o32 ABI register naming.
+ */
+#undef t0
+#undef t1
+#undef t2
+#undef t3
+#define t0	$8
+#define t1	$9
+#define t2	$10
+#define t3	$11
+#define t4	$12
+#define t5	$13
+#define t6	$14
+#define t7	$15
+
+#else
+
+#define LOAD   lw
+#define LOADL  lwl
+#define LOADR  lwr
+#define STOREL swl
+#define STORER swr
+#define STORE  sw
+#define ADD    addu
+#define SUB    subu
+#define SRL    srl
+#define SLL    sll
+#define SRA    sra
+#define SLLV   sllv
+#define SRLV   srlv
+#define NBYTES 4
+#define LOG_NBYTES 2
+
+#endif /* USE_DOUBLE */
+
+#ifdef CONFIG_CPU_LITTLE_ENDIAN
+#define LDFIRST LOADR
+#define LDREST  LOADL
+#define STFIRST STORER
+#define STREST  STOREL
+#define SHIFT_DISCARD SLLV
+#else
+#define LDFIRST LOADL
+#define LDREST  LOADR
+#define STFIRST STOREL
+#define STREST  STORER
+#define SHIFT_DISCARD SRLV
+#endif
+
+#define FIRST(unit) ((unit)*NBYTES)
+#define REST(unit)  (FIRST(unit)+NBYTES-1)
+#define UNIT(unit)  FIRST(unit)
+
+#define ADDRMASK (NBYTES-1)
+
+	.text
+	.set	noreorder
+	.set	noat
+
+/*
+ * A combined memcpy/__copy_user
+ * __copy_user sets len to 0 for success; else to an upper bound of
+ * the number of uncopied bytes.
+ * memcpy sets v0 to dst.
+ */
+	.align	7
+LEAF(memcpy)					/* a0=dst a1=src a2=len */
+	move	v0, dst				/* return value */
+#ifdef CONFIG_CPU_CAVIUM_OCTEON
+	.align	3	/* Align the next branch on 8 bytes */
+#endif
+__memcpy:
+FEXPORT(__copy_user)
+	/*
+	 * Note: dst & src may be unaligned, len may be 0
+	 * Temps
+	 */
+#define rem t8
+
+	/*
+	 * The "issue break"s below are very approximate.
+	 * Issue delays for dcache fills will perturb the schedule, as will
+	 * load queue full replay traps, etc.
+	 *
+	 * If len < NBYTES use byte operations.
+	 */
+	PREF(	0, 0(src) )
+#ifndef CONFIG_CPU_CAVIUM_OCTEON
+	PREF(	1, 0(dst) )
+#endif
+	sltu	t2, len, NBYTES
+	and	t1, dst, ADDRMASK
+#ifndef CONFIG_CPU_CAVIUM_OCTEON
+	PREF(	0, 1*32(src) )
+	PREF(	1, 1*32(dst) )
+#endif
+	bnez	t2, copy_bytes_checklen
+	 and	t0, src, ADDRMASK
+#ifdef CONFIG_CPU_CAVIUM_OCTEON
+	PREF(	0, 128(src) )
+#else
+	PREF(	0, 2*32(src) )
+	PREF(	1, 2*32(dst) )
+#endif
+	bnez	t1, dst_unaligned
+	 nop
+	bnez	t0, src_unaligned_dst_aligned
+	/*
+	 * use delay slot for fall-through
+	 * src and dst are aligned; need to compute rem
+	 */
+both_aligned:
+	 SRL	t0, len, LOG_NBYTES+3    # +3 for 8 units/iter
+	beqz	t0, cleanup_both_aligned # len < 8*NBYTES
+	 and	rem, len, (8*NBYTES-1)	 # rem = len % (8*NBYTES)
+#ifdef CONFIG_CPU_CAVIUM_OCTEON
+	PREF(	0, 256(src) )
+#else
+	PREF(	0, 3*32(src) )
+	PREF(	1, 3*32(dst) )
+#endif
+	.align	4
+1:
+EXC(	LOAD	t0, UNIT(0)(src),	l_exc)
+EXC(	LOAD	t1, UNIT(1)(src),	l_exc_copy)
+EXC(	LOAD	t2, UNIT(2)(src),	l_exc_copy)
+EXC(	LOAD	t3, UNIT(3)(src),	l_exc_copy)
+	SUB	len, len, 8*NBYTES
+EXC(	LOAD	t4, UNIT(4)(src),	l_exc_copy)
+EXC(	LOAD	t7, UNIT(5)(src),	l_exc_copy)
+EXC(	STORE	t0, UNIT(0)(dst),	s_exc_p8u)
+EXC(	STORE	t1, UNIT(1)(dst),	s_exc_p7u)
+EXC(	LOAD	t0, UNIT(6)(src),	l_exc_copy)
+	ADD	dst, dst, 8*NBYTES
+EXC(	LOAD	t1, UNIT(7)(src),	l_exc_copy)
+	ADD	src, src, 8*NBYTES
+EXC(	STORE	t2, UNIT(-6)(dst),	s_exc_p6u)
+EXC(	STORE	t3, UNIT(-5)(dst),	s_exc_p5u)
+EXC(	STORE	t4, UNIT(-4)(dst),	s_exc_p4u)
+EXC(	STORE	t7, UNIT(-3)(dst),	s_exc_p3u)
+EXC(	STORE	t0, UNIT(-2)(dst),	s_exc_p2u)
+#ifdef CONFIG_CPU_CAVIUM_OCTEON
+	beq	len, rem, cleanup_both_aligned
+EXC(	 STORE	t1, UNIT(-1)(dst),	s_exc_p1u)
+EXC(	LOAD	t0, UNIT(0)(src),	l_exc)
+EXC(	LOAD	t1, UNIT(1)(src),	l_exc_copy)
+EXC(	LOAD	t2, UNIT(2)(src),	l_exc_copy)
+EXC(	LOAD	t3, UNIT(3)(src),	l_exc_copy)
+	SUB	len, len, 8*NBYTES
+EXC(	LOAD	t4, UNIT(4)(src),	l_exc_copy)
+EXC(	LOAD	t7, UNIT(5)(src),	l_exc_copy)
+EXC(	STORE	t0, UNIT(0)(dst),	s_exc_p8u)
+EXC(	STORE	t1, UNIT(1)(dst),	s_exc_p7u)
+EXC(	LOAD	t0, UNIT(6)(src),	l_exc_copy)
+	ADD	dst, dst, 8*NBYTES
+EXC(	LOAD	t1, UNIT(7)(src),	l_exc_copy)
+	ADD	src, src, 8*NBYTES
+EXC(	STORE	t2, UNIT(-6)(dst),	s_exc_p6u)
+EXC(	STORE	t3, UNIT(-5)(dst),	s_exc_p5u)
+EXC(	STORE	t4, UNIT(-4)(dst),	s_exc_p4u)
+EXC(	STORE	t7, UNIT(-3)(dst),	s_exc_p3u)
+EXC(	STORE	t0, UNIT(-2)(dst),	s_exc_p2u)
+EXC(	STORE	t1, UNIT(-1)(dst),	s_exc_p1u)
+	bne	len, rem, 1b
+	 PREF(	0, 256(src) )
+#else
+EXC(	STORE	t1, UNIT(-1)(dst),	s_exc_p1u)
+	PREF(	0, 8*32(src) )
+	PREF(	1, 8*32(dst) )
+	bne	len, rem, 1b
+	 nop
+#endif
+
+	/*
+	 * len == rem == the number of bytes left to copy < 8*NBYTES
+	 */
+cleanup_both_aligned:
+	beqz	len, done
+	 sltu	t0, len, 4*NBYTES
+	bnez	t0, less_than_4units
+	 and	rem, len, (NBYTES-1)	# rem = len % NBYTES
+	/*
+	 * len >= 4*NBYTES
+	 */
+EXC(	LOAD	t0, UNIT(0)(src),	l_exc)
+EXC(	LOAD	t1, UNIT(1)(src),	l_exc_copy)
+EXC(	LOAD	t2, UNIT(2)(src),	l_exc_copy)
+EXC(	LOAD	t3, UNIT(3)(src),	l_exc_copy)
+	SUB	len, len, 4*NBYTES
+	ADD	src, src, 4*NBYTES
+EXC(	STORE	t0, UNIT(0)(dst),	s_exc_p4u)
+EXC(	STORE	t1, UNIT(1)(dst),	s_exc_p3u)
+EXC(	STORE	t2, UNIT(2)(dst),	s_exc_p2u)
+EXC(	STORE	t3, UNIT(3)(dst),	s_exc_p1u)
+	beqz	len, done
+	 ADD	dst, dst, 4*NBYTES
+less_than_4units:
+	/*
+	 * rem = len % NBYTES
+	 */
+	beq	rem, len, copy_bytes
+	 nop
+1:
+EXC(	LOAD	t0, 0(src),		l_exc)
+	ADD	src, src, NBYTES
+	SUB	len, len, NBYTES
+EXC(	STORE	t0, 0(dst),		s_exc_p1u)
+	bne	rem, len, 1b
+	 ADD	dst, dst, NBYTES
+
+	/*
+	 * src and dst are aligned, need to copy rem bytes (rem < NBYTES)
+	 * A loop would do only a byte at a time with possible branch
+	 * mispredicts.  Can't do an explicit LOAD dst,mask,or,STORE
+	 * because can't assume read-access to dst.  Instead, use
+	 * STREST dst, which doesn't require read access to dst.
+	 *
+	 * This code should perform better than a simple loop on modern,
+	 * wide-issue mips processors because the code has fewer branches and
+	 * more instruction-level parallelism.
+	 */
+#define bits t2
+	beqz	len, done
+	 ADD	t1, dst, len	# t1 is just past last byte of dst
+	li	bits, 8*NBYTES
+	SLL	rem, len, 3	# rem = number of bits to keep
+EXC(	LOAD	t0, 0(src),		l_exc)
+	SUB	bits, bits, rem	# bits = number of bits to discard
+	SHIFT_DISCARD t0, t0, bits
+EXC(	STREST	t0, -1(t1),		s_exc)
+	jr	ra
+	 move	len, zero
+dst_unaligned:
+	/*
+	 * dst is unaligned
+	 * t0 = src & ADDRMASK
+	 * t1 = dst & ADDRMASK; T1 > 0
+	 * len >= NBYTES
+	 *
+	 * Copy enough bytes to align dst
+	 * Set match = (src and dst have same alignment)
+	 */
+#define match rem
+EXC(	LDFIRST	t3, FIRST(0)(src),	l_exc)
+	ADD	t2, zero, NBYTES
+EXC(	LDREST	t3, REST(0)(src),	l_exc_copy)
+	SUB	t2, t2, t1	# t2 = number of bytes copied
+	xor	match, t0, t1
+EXC(	STFIRST t3, FIRST(0)(dst),	s_exc)
+	beq	len, t2, done
+	 SUB	len, len, t2
+	ADD	dst, dst, t2
+	beqz	match, both_aligned
+	 ADD	src, src, t2
+
+src_unaligned_dst_aligned:
+	SRL	t0, len, LOG_NBYTES+2    # +2 for 4 units/iter
+#ifndef CONFIG_CPU_CAVIUM_OCTEON
+	PREF(	0, 3*32(src) )
+#endif
+	beqz	t0, cleanup_src_unaligned
+	 and	rem, len, (4*NBYTES-1)   # rem = len % 4*NBYTES
+#ifndef CONFIG_CPU_CAVIUM_OCTEON
+	PREF(	1, 3*32(dst) )
+#endif
+1:
+#ifdef CONFIG_CPU_CAVIUM_OCTEON
+	PREF(	0, 256(src) )
+#endif
+/*
+ * Avoid consecutive LD*'s to the same register since some mips
+ * implementations can't issue them in the same cycle.
+ * It's OK to load FIRST(N+1) before REST(N) because the two addresses
+ * are to the same unit (unless src is aligned, but it's not).
+ */
+EXC(	LDFIRST	t0, FIRST(0)(src),	l_exc)
+EXC(	LDFIRST	t1, FIRST(1)(src),	l_exc_copy)
+	SUB     len, len, 4*NBYTES
+EXC(	LDREST	t0, REST(0)(src),	l_exc_copy)
+EXC(	LDREST	t1, REST(1)(src),	l_exc_copy)
+EXC(	LDFIRST	t2, FIRST(2)(src),	l_exc_copy)
+EXC(	LDFIRST	t3, FIRST(3)(src),	l_exc_copy)
+EXC(	LDREST	t2, REST(2)(src),	l_exc_copy)
+EXC(	LDREST	t3, REST(3)(src),	l_exc_copy)
+#ifndef CONFIG_CPU_CAVIUM_OCTEON
+	PREF(	0, 9*32(src) )		# 0 is PREF_LOAD  (not streamed)
+#endif
+	ADD	src, src, 4*NBYTES
+#ifdef CONFIG_CPU_SB1
+	nop				# improves slotting
+#endif
+EXC(	STORE	t0, UNIT(0)(dst),	s_exc_p4u)
+EXC(	STORE	t1, UNIT(1)(dst),	s_exc_p3u)
+EXC(	STORE	t2, UNIT(2)(dst),	s_exc_p2u)
+EXC(	STORE	t3, UNIT(3)(dst),	s_exc_p1u)
+
+#ifdef CONFIG_CPU_CAVIUM_OCTEON
+	beq	len, rem, cleanup_src_unaligned
+	 ADD	dst, dst, 4*NBYTES
+EXC(	LDFIRST	t0, FIRST(0)(src),	l_exc)
+EXC(	LDFIRST	t1, FIRST(1)(src),	l_exc_copy)
+	SUB     len, len, 4*NBYTES
+EXC(	LDREST	t0, REST(0)(src),	l_exc_copy)
+EXC(	LDREST	t1, REST(1)(src),	l_exc_copy)
+EXC(	LDFIRST	t2, FIRST(2)(src),	l_exc_copy)
+EXC(	LDFIRST	t3, FIRST(3)(src),	l_exc_copy)
+EXC(	LDREST	t2, REST(2)(src),	l_exc_copy)
+EXC(	LDREST	t3, REST(3)(src),	l_exc_copy)
+	ADD	src, src, 4*NBYTES
+EXC(	STORE	t0, UNIT(0)(dst),	s_exc_p4u)
+EXC(	STORE	t1, UNIT(1)(dst),	s_exc_p3u)
+EXC(	STORE	t2, UNIT(2)(dst),	s_exc_p2u)
+EXC(	STORE	t3, UNIT(3)(dst),	s_exc_p1u)
+	beq	len, rem, cleanup_src_unaligned
+	 ADD	dst, dst, 4*NBYTES
+EXC(	LDFIRST	t0, FIRST(0)(src),	l_exc)
+EXC(	LDFIRST	t1, FIRST(1)(src),	l_exc_copy)
+	SUB     len, len, 4*NBYTES
+EXC(	LDREST	t0, REST(0)(src),	l_exc_copy)
+EXC(	LDREST	t1, REST(1)(src),	l_exc_copy)
+EXC(	LDFIRST	t2, FIRST(2)(src),	l_exc_copy)
+EXC(	LDFIRST	t3, FIRST(3)(src),	l_exc_copy)
+EXC(	LDREST	t2, REST(2)(src),	l_exc_copy)
+EXC(	LDREST	t3, REST(3)(src),	l_exc_copy)
+	ADD	src, src, 4*NBYTES
+EXC(	STORE	t0, UNIT(0)(dst),	s_exc_p4u)
+EXC(	STORE	t1, UNIT(1)(dst),	s_exc_p3u)
+EXC(	STORE	t2, UNIT(2)(dst),	s_exc_p2u)
+EXC(	STORE	t3, UNIT(3)(dst),	s_exc_p1u)
+	beq	len, rem, cleanup_src_unaligned
+	 ADD	dst, dst, 4*NBYTES
+EXC(	LDFIRST	t0, FIRST(0)(src),	l_exc)
+EXC(	LDFIRST	t1, FIRST(1)(src),	l_exc_copy)
+	SUB     len, len, 4*NBYTES
+EXC(	LDREST	t0, REST(0)(src),	l_exc_copy)
+EXC(	LDREST	t1, REST(1)(src),	l_exc_copy)
+EXC(	LDFIRST	t2, FIRST(2)(src),	l_exc_copy)
+EXC(	LDFIRST	t3, FIRST(3)(src),	l_exc_copy)
+EXC(	LDREST	t2, REST(2)(src),	l_exc_copy)
+EXC(	LDREST	t3, REST(3)(src),	l_exc_copy)
+	ADD	src, src, 4*NBYTES
+EXC(	STORE	t0, UNIT(0)(dst),	s_exc_p4u)
+EXC(	STORE	t1, UNIT(1)(dst),	s_exc_p3u)
+EXC(	STORE	t2, UNIT(2)(dst),	s_exc_p2u)
+EXC(	STORE	t3, UNIT(3)(dst),	s_exc_p1u)
+	bne	len, rem, 1b
+	 ADD	dst, dst, 4*NBYTES
+#else
+	PREF(	1, 9*32(dst) )     	# 1 is PREF_STORE (not streamed)
+	bne	len, rem, 1b
+	 ADD	dst, dst, 4*NBYTES
+#endif
+
+cleanup_src_unaligned:
+	beqz	len, done
+	 and	rem, len, NBYTES-1  # rem = len % NBYTES
+	beq	rem, len, copy_bytes
+	 nop
+1:
+EXC(	LDFIRST t0, FIRST(0)(src),	l_exc)
+EXC(	LDREST	t0, REST(0)(src),	l_exc_copy)
+	ADD	src, src, NBYTES
+	SUB	len, len, NBYTES
+EXC(	STORE	t0, 0(dst),		s_exc_p1u)
+	bne	len, rem, 1b
+	 ADD	dst, dst, NBYTES
+
+copy_bytes_checklen:
+	beqz	len, done
+	 nop
+copy_bytes:
+	/* 0 < len < NBYTES  */
+#define COPY_BYTE(N)			\
+EXC(	lb	t0, N(src), l_exc);	\
+	SUB	len, len, 1;		\
+	beqz	len, done;		\
+EXC(	 sb	t0, N(dst), s_exc_p1)
+
+	COPY_BYTE(0)
+	COPY_BYTE(1)
+#ifdef USE_DOUBLE
+	COPY_BYTE(2)
+	COPY_BYTE(3)
+	COPY_BYTE(4)
+	COPY_BYTE(5)
+#endif
+EXC(	lb	t0, NBYTES-2(src), l_exc)
+	SUB	len, len, 1
+	jr	ra
+EXC(	 sb	t0, NBYTES-2(dst), s_exc_p1)
+done:
+	jr	ra
+	 nop
+	END(memcpy)
+
+l_exc_copy:
+	/*
+	 * Copy bytes from src until faulting load address (or until a
+	 * lb faults)
+	 *
+	 * When reached by a faulting LDFIRST/LDREST, THREAD_BUADDR($28)
+	 * may be more than a byte beyond the last address.
+	 * Hence, the lb below may get an exception.
+	 *
+	 * Assumes src < THREAD_BUADDR($28)
+	 */
+	LOAD	t0, TI_TASK($28)
+	 nop
+	LOAD	t0, THREAD_BUADDR(t0)
+1:
+EXC(	lb	t1, 0(src),	l_exc)
+	ADD	src, src, 1
+	sb	t1, 0(dst)	# can't fault -- we're copy_from_user
+	bne	src, t0, 1b
+	 ADD	dst, dst, 1
+l_exc:
+	LOAD	t0, TI_TASK($28)
+	 nop
+	LOAD	t0, THREAD_BUADDR(t0)	# t0 is just past last good address
+	 nop
+	SUB	len, AT, t0		# len number of uncopied bytes
+	/*
+	 * Here's where we rely on src and dst being incremented in tandem,
+	 *   See (3) above.
+	 * dst += (fault addr - src) to put dst at first byte to clear
+	 */
+	ADD	dst, t0			# compute start address in a1
+	SUB	dst, src
+	/*
+	 * Clear len bytes starting at dst.  Can't call __bzero because it
+	 * might modify len.  An inefficient loop for these rare times...
+	 */
+	beqz	len, done
+	 SUB	src, len, 1
+1:	sb	zero, 0(dst)
+	ADD	dst, dst, 1
+	bnez	src, 1b
+	 SUB	src, src, 1
+	jr	ra
+	 nop
+
+
+#define SEXC(n)				\
+s_exc_p ## n ## u:			\
+	jr	ra;			\
+	 ADD	len, len, n*NBYTES
+
+SEXC(8)
+SEXC(7)
+SEXC(6)
+SEXC(5)
+SEXC(4)
+SEXC(3)
+SEXC(2)
+SEXC(1)
+
+s_exc_p1:
+	jr	ra
+	 ADD	len, len, 1
+s_exc:
+	jr	ra
+	 nop
+
+	.align	5
+LEAF(memmove)
+	ADD	t0, a0, a2
+	ADD	t1, a1, a2
+	sltu	t0, a1, t0			# dst + len <= src -> memcpy
+	sltu	t1, a0, t1			# dst >= src + len -> memcpy
+	and	t0, t1
+	beqz	t0, __memcpy
+	 move	v0, a0				/* return value */
+	beqz	a2, r_out
+	END(memmove)
+
+	/* fall through to __rmemcpy */
+LEAF(__rmemcpy)					/* a0=dst a1=src a2=len */
+	 sltu	t0, a1, a0
+	beqz	t0, r_end_bytes_up		# src >= dst
+	 nop
+	ADD	a0, a2				# dst = dst + len
+	ADD	a1, a2				# src = src + len
+
+r_end_bytes:
+	lb	t0, -1(a1)
+	SUB	a2, a2, 0x1
+	sb	t0, -1(a0)
+	SUB	a1, a1, 0x1
+	bnez	a2, r_end_bytes
+	 SUB	a0, a0, 0x1
+
+r_out:
+	jr	ra
+	 move	a2, zero
+
+r_end_bytes_up:
+	lb	t0, (a1)
+	SUB	a2, a2, 0x1
+	sb	t0, (a0)
+	ADD	a1, a1, 0x1
+	bnez	a2, r_end_bytes_up
+	 ADD	a0, a0, 0x1
+
+	jr	ra
+	 move	a2, zero
+	END(__rmemcpy)
diff --git a/arch/mips/mm/c-octeon.c b/arch/mips/mm/c-octeon.c
new file mode 100644
index 0000000..d373ea6
--- /dev/null
+++ b/arch/mips/mm/c-octeon.c
@@ -0,0 +1,293 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2005-2007 Cavium Networks
+ */
+#include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/mm.h>
+#include <linux/bitops.h>
+
+#include <asm/bcache.h>
+#include <asm/bootinfo.h>
+#include <asm/cacheops.h>
+#include <asm/cpu.h>
+#include <asm/cpu-features.h>
+#include <asm/io.h>
+#include <asm/page.h>
+#include <asm/pgtable.h>
+#include <asm/r4kcache.h>
+#include <asm/system.h>
+#include <asm/mmu_context.h>
+#include <asm/war.h>
+#include "../cavium-octeon/hal.h"
+
+unsigned long long cache_err_dcache[NR_CPUS];
+
+/**
+ * Octeon automatically flushes the dcache on tlb changes, so
+ * from Linux's viewpoint it acts much like a physically
+ * tagged cache. No flushing is needed
+ *
+ * @param addr
+ */
+static void octeon_flush_data_cache_page(unsigned long addr)
+{
+    /* Nothing to do */
+}
+
+
+/**
+ * Flush caches as necessary for all cores affected by a
+ * vma. If no vma is supplied, all cores are flushed.
+ *
+ * @param vma    VMA to flush or NULL to flush all icaches.
+ */
+static void octeon_flush_icache_all_cores(struct vm_area_struct *vma)
+{
+#ifdef CONFIG_SMP
+    int i;
+    int cpu;
+#endif
+    preempt_disable();
+#ifdef CONFIG_SMP
+    cpu = smp_processor_id();
+#endif
+    mb();
+
+    /* If we have a vma structure, we only need to worry about cores it
+        has been used on */
+    if (vma)
+    {
+#ifdef CONFIG_SMP
+        for (i = 0; i < NR_CPUS; i++)
+            if (cpu_isset(i, vma->vm_mm->cpu_vm_mask) && i != cpu)
+                core_send_ipi(i, SMP_ICACHE_FLUSH);
+#endif
+        asm volatile ("synci 0($0)\n");
+    }
+    else
+    {
+        /* No extra info available. Flush the icache on all cores that
+            are online */
+#ifdef CONFIG_SMP
+        for (i = 0; i < NR_CPUS; i++)
+            if (cpu_online(i) && i != cpu)
+                core_send_ipi(i, SMP_ICACHE_FLUSH);
+#endif
+        asm volatile ("synci 0($0)\n");
+    }
+    preempt_enable();
+}
+
+
+/**
+ * Called to flush the icache on all cores
+ */
+static void octeon_flush_icache_all(void)
+{
+	octeon_flush_icache_all_cores(NULL);
+}
+
+
+/**
+ * Called to flush all memory associated with a memory
+ * context.
+ *
+ * @param mm     Memory context to flush
+ */
+static void octeon_flush_cache_mm(struct mm_struct *mm)
+{
+    /* According to the R4K version of this file, CPUs without
+        dcache aliases don't need to do anything here */
+}
+
+
+/**
+ * Flush a range of kernel addresses out of the icache
+ *
+ * @param start
+ * @param end
+ */
+static void octeon_flush_icache_range(unsigned long start, unsigned long end)
+{
+	octeon_flush_icache_all_cores(NULL);
+}
+
+
+/**
+ * Flush the icache for a trampoline. These are used for interrupt
+ * and exception hooking.
+ *
+ * @param addr   Address to flush
+ */
+static void octeon_flush_cache_sigtramp(unsigned long addr)
+{
+    /* Only flush trampolines on the current core */
+    mb();
+    asm volatile ("synci 0(%0)\n":: "r" (addr));
+}
+
+
+/**
+ * Flush a range out of a vma
+ *
+ * @param vma    VMA to flush
+ * @param start
+ * @param end
+ */
+static void octeon_flush_cache_range(struct vm_area_struct *vma, unsigned long start, unsigned long end)
+{
+    if (vma->vm_flags & (VM_EXEC|VM_EXECUTABLE))
+        octeon_flush_icache_all_cores(vma);
+}
+
+
+/**
+ * Flush a specific page of a vma
+ *
+ * @param vma    VMA to flush page for
+ * @param page   Page to flush
+ * @param pfn
+ */
+static void octeon_flush_cache_page(struct vm_area_struct *vma, unsigned long page, unsigned long pfn)
+{
+    if (vma->vm_flags & (VM_EXEC|VM_EXECUTABLE))
+        octeon_flush_icache_all_cores(vma);
+}
+
+
+/**
+ * Probe Octeon's caches
+ *
+ * @return
+ */
+static void __init probe_octeon(void)
+{
+    unsigned long icache_size;
+    unsigned long dcache_size;
+    unsigned int config1;
+    struct cpuinfo_mips *c = &current_cpu_data;
+
+    switch (c->cputype)
+    {
+        case CPU_CAVIUM_OCTEON:
+            config1 = read_c0_config1();
+            c->icache.linesz = 2 << ((config1 >> 19) & 7);
+            c->icache.sets = 64 << ((config1 >> 22) & 7);
+            c->icache.ways = 1 + ((config1 >> 16) & 7);
+            c->icache.flags |= MIPS_CACHE_VTAG;
+            icache_size = c->icache.sets * c->icache.ways * c->icache.linesz;
+            c->icache.waybit = ffs(icache_size / c->icache.ways) - 1;
+            c->dcache.linesz = 128;
+            if (OCTEON_IS_MODEL(OCTEON_CN3XXX))
+                c->dcache.sets = 1; /* CN3XXX has one Dcache set */
+            else
+                c->dcache.sets = 2; /* CN5XXX has two Dcache sets */
+            c->dcache.ways = 64;
+            dcache_size = c->dcache.sets * c->dcache.ways * c->dcache.linesz;
+            c->dcache.waybit = ffs(dcache_size / c->dcache.ways) - 1;
+            c->options |= MIPS_CPU_PREFETCH;
+            break;
+
+        default:
+            panic("Unsupported Cavium Networks CPU type\n");
+            break;
+    }
+
+    /* compute a couple of other cache variables */
+    c->icache.waysize = icache_size / c->icache.ways;
+    c->dcache.waysize = dcache_size / c->dcache.ways;
+
+    c->icache.sets = icache_size / (c->icache.linesz * c->icache.ways);
+    c->dcache.sets = dcache_size / (c->dcache.linesz * c->dcache.ways);
+
+    if (smp_processor_id() == 0)
+    {
+        printk("Primary instruction cache %ldkB, %s, %d way, %d sets, linesize %d bytes.\n",
+               icache_size >> 10,
+               cpu_has_vtag_icache ? "virtually tagged" : "physically tagged",
+               c->icache.ways, c->icache.sets, c->icache.linesz);
+
+        printk("Primary data cache %ldkB, %d-way, %d sets, linesize %d bytes.\n",
+               dcache_size >> 10, c->dcache.ways, c->dcache.sets, c->dcache.linesz);
+    }
+}
+
+
+/**
+ * Setup the Octeon cache flush routines
+ *
+ * @return
+ */
+void __init octeon_cache_init(void)
+{
+    extern unsigned long ebase;
+    extern char except_vec2_octeon;
+
+    memcpy((void *)(ebase + 0x100), &except_vec2_octeon, 0x80);
+    octeon_flush_cache_sigtramp(ebase + 0x100);
+
+    probe_octeon();
+
+    shm_align_mask = PAGE_SIZE - 1;
+
+    flush_cache_all         = octeon_flush_icache_all;
+    __flush_cache_all       = octeon_flush_icache_all;
+    flush_cache_mm          = octeon_flush_cache_mm;
+    flush_cache_page        = octeon_flush_cache_page;
+    flush_cache_range       = octeon_flush_cache_range;
+    flush_cache_sigtramp    = octeon_flush_cache_sigtramp;
+    flush_icache_all        = octeon_flush_icache_all;
+    flush_data_cache_page   = octeon_flush_data_cache_page;
+    flush_icache_range      = octeon_flush_icache_range;
+}
+
+/**
+ * Handle a cache error exception
+ */
+
+static void  cache_parity_error_octeon(int non_recoverable)
+{
+    unsigned long coreid = cvmx_get_core_num();
+    uint64_t icache_err = read_c0_cacheerr();
+
+    printk("Cache error exception:\n");
+    printk("cp0_errorepc == %lx\n", read_c0_errorepc());
+    if (icache_err & 1)
+    {
+        printk("CacheErr (Icache) == %llx\n", (unsigned long long)icache_err);
+        write_c0_cacheerr(0);
+    }
+    if (cache_err_dcache[coreid] & 1)
+    {
+        printk("CacheErr (Dcache) == %llx\n", (unsigned long long)cache_err_dcache[coreid]);
+        cache_err_dcache[coreid] = 0;
+    }
+
+
+    if (non_recoverable)
+        panic("Can't handle cache error: nested exception");
+}
+
+/**
+ * Called when the the exception is not recoverable
+ */
+
+asmlinkage void cache_parity_error_octeon_recoverable(void)
+{
+    cache_parity_error_octeon(0);
+}
+
+/**
+ * Called when the the exception is recoverable
+ */
+
+asmlinkage void cache_parity_error_octeon_non_recoverable(void)
+{
+    cache_parity_error_octeon(1);
+}
+
diff --git a/arch/mips/mm/cex-oct.S b/arch/mips/mm/cex-oct.S
new file mode 100644
index 0000000..9122415
--- /dev/null
+++ b/arch/mips/mm/cex-oct.S
@@ -0,0 +1,73 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2006 Cavium Networks
+ * Cache error handler
+ */
+
+#include <asm/asm.h>
+#include <asm/regdef.h>
+#include <asm/mipsregs.h>
+#include <asm/stackframe.h>
+
+/*
+ * Handle cache error. Indicate to the second level handler whether
+ * the exception is recoverable.
+ */
+	LEAF(except_vec2_octeon)
+
+	.set    push
+	.set	mips64r2
+	.set	noreorder
+	.set	noat
+
+
+	/* due to an errata we need to read the COP0 CacheErr (Dcache) before any
+	 * cache/DRAM access
+	 */
+
+	rdhwr   k0, $0        /* get core_id */
+	PTR_LA  k1, cache_err_dcache
+	sll     k0, k0, 3
+	PTR_ADDU k1, k0, k1    /* k1 = &cache_err_dcache[core_id] */
+
+	dmfc0   k0, CP0_CACHEERR, 1
+	sd      k0, (k1)
+	dmtc0   $0, CP0_CACHEERR, 1
+
+        /* check whether this is a nested exception */
+	mfc0    k1, CP0_STATUS
+	andi    k1, k1, ST0_EXL
+	beqz    k1, 1f
+	nop
+	j	cache_parity_error_octeon_non_recoverable
+	nop
+
+	/* exception is recoverable */
+1:	j	handle_cache_err
+	nop
+
+	.set    pop
+	END(except_vec2_octeon)
+
+/*
+ * We need to jump to handle_cache_err so that the previous handler can fit
+ * within 0x80 bytes. We also move from 0xFFFFFFFFAXXXXXXX space (uncached) to the
+ * 0xFFFFFFFF8XXXXXXX space (cached).
+ */
+	LEAF(handle_cache_err)
+	.set    push
+        .set    noreorder
+        .set    noat
+
+	SAVE_ALL
+	KMODE
+	jal     cache_parity_error_octeon_recoverable
+	nop
+	j       ret_from_exception
+	nop
+
+	.set pop
+	END(handle_cache_err)
diff --git a/arch/mips/mm/pg-octeon.c b/arch/mips/mm/pg-octeon.c
new file mode 100644
index 0000000..f11093c
--- /dev/null
+++ b/arch/mips/mm/pg-octeon.c
@@ -0,0 +1,113 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2005-2007 Cavium Networks
+ */
+#include <linux/module.h>
+
+
+void clear_page(void *page)
+{
+	void *end = page + PAGE_SIZE;
+
+    asm volatile (
+        "   .set	push		\n"
+        "   .set	mips64		\n"
+        "   .set	noreorder	\n"
+        "1: sd      $0, 0(%0)   \n" /* Write zeros to the cache line */
+        "   sd      $0, 8(%0)   \n"
+        "   sd      $0, 16(%0)  \n"
+        "   sd      $0, 24(%0)  \n"
+        "   sd      $0, 32(%0)  \n"
+        "   sd      $0, 40(%0)  \n"
+        "   sd      $0, 48(%0)  \n"
+        "   sd      $0, 56(%0)  \n"
+        "   sd      $0, 64(%0)  \n"
+        "   sd      $0, 72(%0)  \n"
+        "   sd      $0, 80(%0)  \n"
+        "   sd      $0, 88(%0)  \n"
+        "   sd      $0, 96(%0)  \n"
+        "   sd      $0, 104(%0) \n"
+#ifdef CONFIG_64BIT
+        "   daddu   %0, 128     \n" /* Increment to the next address. Will be dual issued */
+#else
+        "   addu    %0, 128     \n"
+#endif
+        "   sd      $0, -16(%0) \n"
+        "   blt     %0, %1, 1b  \n" /* Loop until we've completed the page */
+        "    sd     $0, -8(%0)  \n"
+        "   .set	pop		\n"
+        : "+r" (page)
+        : "r" (end)
+        : "memory"
+    );
+}
+
+
+void copy_page(void *to, void *from)
+{
+#ifdef _ABIO32
+    memcpy(to, from, PAGE_SIZE);
+#else
+	void *end = to + PAGE_SIZE;
+
+    asm volatile (
+        "   .set	push		\n"
+        "   .set	mips64		\n"
+        "   .set	noreorder	\n"
+        "   pref    0,  0(%1)   \n" /* Prefetch the first cache line of "from" */
+        "   pref    0,  128(%1) \n" /* Prefetch the next "from" cache line for the next iteration */
+        "1: pref    0,  256(%1) \n" /* Prefetch for two loops ahead */
+        "   ld      $12, 0(%1)  \n" /* Copy 32 bytes at a time */
+        "   ld      $13, 8(%1)  \n"
+        "   ld      $14, 16(%1) \n"
+        "   ld      $15, 24(%1) \n"
+        "   daddu   %1, 32      \n" /* Dual issued */
+        "   sd      $12, 0(%0)  \n"
+        "   sd      $13, 8(%0)  \n"
+        "   sd      $14, 16(%0) \n"
+        "   sd      $15, 24(%0) \n"
+        "   daddu   %0, 32      \n" /* Dual issued */
+        "   ld      $12, 0(%1)  \n" /* Copy 32 bytes at a time */
+        "   ld      $13, 8(%1)  \n"
+        "   ld      $14, 16(%1) \n"
+        "   ld      $15, 24(%1) \n"
+        "   daddu   %1, 32      \n" /* Dual issued */
+        "   sd      $12, 0(%0)  \n"
+        "   sd      $13, 8(%0)  \n"
+        "   sd      $14, 16(%0) \n"
+        "   sd      $15, 24(%0) \n"
+        "   daddu   %0, 32      \n" /* Dual issued */
+        "   ld      $12, 0(%1)  \n" /* Copy 32 bytes at a time */
+        "   ld      $13, 8(%1)  \n"
+        "   ld      $14, 16(%1) \n"
+        "   ld      $15, 24(%1) \n"
+        "   daddu   %1, 32      \n" /* Dual issued */
+        "   sd      $12, 0(%0)  \n"
+        "   sd      $13, 8(%0)  \n"
+        "   sd      $14, 16(%0) \n"
+        "   sd      $15, 24(%0) \n"
+        "   daddu   %0, 32      \n" /* Dual issued */
+        "   ld      $12, 0(%1)  \n" /* Copy 32 bytes at a time */
+        "   ld      $13, 8(%1)  \n"
+        "   ld      $14, 16(%1) \n"
+        "   ld      $15, 24(%1) \n"
+        "   daddu   %1, 32      \n" /* Dual issued */
+        "   sd      $12, 0(%0)  \n"
+        "   sd      $13, 8(%0)  \n"
+        "   daddu   %0, 32      \n" /* Dual issued */
+        "   sd      $14, -16(%0)\n"
+        "   blt     %0, %2, 1b  \n" /* Loop until we've completed the page */
+        "    sd     $15, -8(%0) \n"
+        "   .set	pop		\n"
+        : "+r" (to), "+r" (from)
+        : "r" (end)
+        : "$12", "$13", "$14", "$15", "memory"
+    );
+#endif
+}
+
+EXPORT_SYMBOL(clear_page);
+EXPORT_SYMBOL(copy_page);
diff --git a/include/asm-mips/mach-cavium-octeon/cpu-feature-overrides.h b/include/asm-mips/mach-cavium-octeon/cpu-feature-overrides.h
new file mode 100644
index 0000000..e109f13
--- /dev/null
+++ b/include/asm-mips/mach-cavium-octeon/cpu-feature-overrides.h
@@ -0,0 +1,68 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2004 Cavium Networks
+ */
+#ifndef __ASM_MACH_CAVIUM_OCTEON_CPU_FEATURE_OVERRIDES_H
+#define __ASM_MACH_CAVIUM_OCTEON_CPU_FEATURE_OVERRIDES_H
+
+#include <linux/types.h>
+#include <asm/mipsregs.h>
+
+/*
+ * Cavium Octeons are MIPS64v2 processors
+ */
+#define cpu_dcache_line_size()	128
+#define cpu_icache_line_size()	128
+
+#ifdef CONFIG_SMP
+#define cpu_has_llsc		1
+#else
+/* Disable LL/SC on non SMP systems. It is faster to disable interrupts for
+   atomic access than a LL/SC */
+#define cpu_has_llsc		0
+#endif
+#define cpu_has_prefetch  	1
+#define cpu_has_dc_aliases      0
+#define cpu_has_fpu             0
+#define cpu_has_64bits          1
+#define cpu_has_octeon_cache    1
+#define cpu_has_saa             octeon_has_saa()
+#define cpu_has_mips64r2        1
+#define cpu_has_counter         1
+/* We actually use two syncw instructions in a row when we need a write memory
+   barrier. This is because the CN3XXX series of Octeons have errata Core-401.
+   This can cause a single syncw to not enforce ordering under very rare
+   conditions. Even if it is rare, better safe than sorry */
+#define OCTEON_SYNCW_STR ".set push\n.set arch=octeon\nsyncw\nsyncw\n.set pop\n"
+
+#define ARCH_HAS_READ_CURRENT_TIMER 1
+#define ARCH_HAS_IRQ_PER_CPU 1
+#define ARCH_HAS_SPINLOCK_PREFETCH 1
+#define spin_lock_prefetch(x) __builtin_prefetch(x)
+#define PREFETCH_STRIDE 128
+
+static inline unsigned long read_current_timer(unsigned long *result)
+{
+	asm volatile ("rdhwr %0,$31\n"
+#ifndef CONFIG_64BIT
+		      "sll %0, 0\n"
+#endif
+		      :"=r" (*result));
+	return *result;
+}
+
+static inline int octeon_has_saa(void)
+{
+#ifdef CONFIG_CPU_CAVIUM_OCTEON
+	int id;
+	asm volatile ("mfc0 %0, $15,0":"=r" (id));
+	return (id >= 0x000d0300);
+#else
+	return 0;
+#endif
+}
+
+#endif
diff --git a/include/asm-mips/mach-cavium-octeon/irq.h b/include/asm-mips/mach-cavium-octeon/irq.h
new file mode 100644
index 0000000..f2d3606
--- /dev/null
+++ b/include/asm-mips/mach-cavium-octeon/irq.h
@@ -0,0 +1,178 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2004-2007 Cavium Networks
+ */
+#ifndef __OCTEON_IRQ_H__
+#define __OCTEON_IRQ_H__
+
+#define NR_IRQS OCTEON_IRQ_LAST
+
+/* 0 - 7 represent the 8 MIPS standard interrupt sources */
+#define OCTEON_IRQ_SW0          0
+#define OCTEON_IRQ_SW1          1
+#define OCTEON_IRQ_CIU0         2
+#define OCTEON_IRQ_CIU1         3
+#define OCTEON_IRQ_CIU4         4
+#define OCTEON_IRQ_5            5
+#define OCTEON_IRQ_PERF         6
+#define OCTEON_IRQ_TIMER        7
+/* 8 - 71 represent the sources in CIU_INTX_EN0 */
+#define OCTEON_IRQ_WORKQ0       8
+#define OCTEON_IRQ_WORKQ1       9
+#define OCTEON_IRQ_WORKQ2       10
+#define OCTEON_IRQ_WORKQ3       11
+#define OCTEON_IRQ_WORKQ4       12
+#define OCTEON_IRQ_WORKQ5       13
+#define OCTEON_IRQ_WORKQ6       14
+#define OCTEON_IRQ_WORKQ7       15
+#define OCTEON_IRQ_WORKQ8       16
+#define OCTEON_IRQ_WORKQ9       17
+#define OCTEON_IRQ_WORKQ10      18
+#define OCTEON_IRQ_WORKQ11      19
+#define OCTEON_IRQ_WORKQ12      20
+#define OCTEON_IRQ_WORKQ13      21
+#define OCTEON_IRQ_WORKQ14      22
+#define OCTEON_IRQ_WORKQ15      23
+#define OCTEON_IRQ_GPIO0        24
+#define OCTEON_IRQ_GPIO1        25
+#define OCTEON_IRQ_GPIO2        26
+#define OCTEON_IRQ_GPIO3        27
+#define OCTEON_IRQ_GPIO4        28
+#define OCTEON_IRQ_GPIO5        29
+#define OCTEON_IRQ_GPIO6        30
+#define OCTEON_IRQ_GPIO7        31
+#define OCTEON_IRQ_GPIO8        32
+#define OCTEON_IRQ_GPIO9        33
+#define OCTEON_IRQ_GPIO10       34
+#define OCTEON_IRQ_GPIO11       35
+#define OCTEON_IRQ_GPIO12       36
+#define OCTEON_IRQ_GPIO13       37
+#define OCTEON_IRQ_GPIO14       38
+#define OCTEON_IRQ_GPIO15       39
+#define OCTEON_IRQ_MBOX0        40
+#define OCTEON_IRQ_MBOX1        41
+#define OCTEON_IRQ_UART0        42
+#define OCTEON_IRQ_UART1        43
+#define OCTEON_IRQ_PCI_INT0     44
+#define OCTEON_IRQ_PCI_INT1     45
+#define OCTEON_IRQ_PCI_INT2     46
+#define OCTEON_IRQ_PCI_INT3     47
+#define OCTEON_IRQ_PCI_MSI0     48
+#define OCTEON_IRQ_PCI_MSI1     49
+#define OCTEON_IRQ_PCI_MSI2     50
+#define OCTEON_IRQ_PCI_MSI3     51
+#define OCTEON_IRQ_WATCHDOG     52
+#define OCTEON_IRQ_TWSI         53
+#define OCTEON_IRQ_RML          54
+#define OCTEON_IRQ_TRACE        55
+#define OCTEON_IRQ_GMX_DRP0     56
+#define OCTEON_IRQ_GMX_DRP1     57
+#define OCTEON_IRQ_IPD_DRP      58
+#define OCTEON_IRQ_KEY_ZERO     59
+#define OCTEON_IRQ_TIMER0       60
+#define OCTEON_IRQ_TIMER1       61
+#define OCTEON_IRQ_TIMER2       62
+#define OCTEON_IRQ_TIMER3       63
+#define OCTEON_IRQ_USB          64
+#define OCTEON_IRQ_RESERVED57   65
+#define OCTEON_IRQ_RESERVED58   66
+#define OCTEON_IRQ_TWSI2        67
+#define OCTEON_IRQ_POWIQ        68
+#define OCTEON_IRQ_IPDPPTHR     69
+#define OCTEON_IRQ_MII          70
+#define OCTEON_IRQ_BOOTDMA      71
+/* 72 - 79 are unused */
+/* 80 - 87 represent the i8259 master */
+#define OCTEON_IRQ_I8259M0      80
+#define OCTEON_IRQ_I8259M1      81
+#define OCTEON_IRQ_I8259M2      82
+#define OCTEON_IRQ_I8259M3      83
+#define OCTEON_IRQ_I8259M4      84
+#define OCTEON_IRQ_I8259M5      85
+#define OCTEON_IRQ_I8259M6      86
+#define OCTEON_IRQ_I8259M7      87
+/* 88 - 95 represent the i8259 slave */
+#define OCTEON_IRQ_I8259S0      88
+#define OCTEON_IRQ_I8259S1      89
+#define OCTEON_IRQ_I8259S2      90
+#define OCTEON_IRQ_I8259S3      91
+#define OCTEON_IRQ_I8259S4      92
+#define OCTEON_IRQ_I8259S5      93
+#define OCTEON_IRQ_I8259S6      94
+#define OCTEON_IRQ_I8259S7      95
+/* 96 - 127 are unused */
+#ifdef CONFIG_PCI_MSI
+/* 128 - 191 represent the MSI interrupts 0-63 */
+#define OCTEON_IRQ_MSI_BIT0     128
+#define OCTEON_IRQ_MSI_BIT1     129
+#define OCTEON_IRQ_MSI_BIT2     130
+#define OCTEON_IRQ_MSI_BIT3     131
+#define OCTEON_IRQ_MSI_BIT4     132
+#define OCTEON_IRQ_MSI_BIT5     133
+#define OCTEON_IRQ_MSI_BIT6     134
+#define OCTEON_IRQ_MSI_BIT7     135
+#define OCTEON_IRQ_MSI_BIT8     136
+#define OCTEON_IRQ_MSI_BIT9     137
+#define OCTEON_IRQ_MSI_BIT10    138
+#define OCTEON_IRQ_MSI_BIT11    139
+#define OCTEON_IRQ_MSI_BIT12    140
+#define OCTEON_IRQ_MSI_BIT13    141
+#define OCTEON_IRQ_MSI_BIT14    142
+#define OCTEON_IRQ_MSI_BIT15    143
+#define OCTEON_IRQ_MSI_BIT16    144
+#define OCTEON_IRQ_MSI_BIT17    145
+#define OCTEON_IRQ_MSI_BIT18    146
+#define OCTEON_IRQ_MSI_BIT19    147
+#define OCTEON_IRQ_MSI_BIT20    148
+#define OCTEON_IRQ_MSI_BIT21    149
+#define OCTEON_IRQ_MSI_BIT22    150
+#define OCTEON_IRQ_MSI_BIT23    151
+#define OCTEON_IRQ_MSI_BIT24    152
+#define OCTEON_IRQ_MSI_BIT25    153
+#define OCTEON_IRQ_MSI_BIT26    154
+#define OCTEON_IRQ_MSI_BIT27    155
+#define OCTEON_IRQ_MSI_BIT28    156
+#define OCTEON_IRQ_MSI_BIT29    157
+#define OCTEON_IRQ_MSI_BIT30    158
+#define OCTEON_IRQ_MSI_BIT31    159
+#define OCTEON_IRQ_MSI_BIT32    160
+#define OCTEON_IRQ_MSI_BIT33    161
+#define OCTEON_IRQ_MSI_BIT34    162
+#define OCTEON_IRQ_MSI_BIT35    163
+#define OCTEON_IRQ_MSI_BIT36    164
+#define OCTEON_IRQ_MSI_BIT37    165
+#define OCTEON_IRQ_MSI_BIT38    166
+#define OCTEON_IRQ_MSI_BIT39    167
+#define OCTEON_IRQ_MSI_BIT40    168
+#define OCTEON_IRQ_MSI_BIT41    169
+#define OCTEON_IRQ_MSI_BIT42    170
+#define OCTEON_IRQ_MSI_BIT43    171
+#define OCTEON_IRQ_MSI_BIT44    172
+#define OCTEON_IRQ_MSI_BIT45    173
+#define OCTEON_IRQ_MSI_BIT46    174
+#define OCTEON_IRQ_MSI_BIT47    175
+#define OCTEON_IRQ_MSI_BIT48    176
+#define OCTEON_IRQ_MSI_BIT49    177
+#define OCTEON_IRQ_MSI_BIT50    178
+#define OCTEON_IRQ_MSI_BIT51    179
+#define OCTEON_IRQ_MSI_BIT52    180
+#define OCTEON_IRQ_MSI_BIT53    181
+#define OCTEON_IRQ_MSI_BIT54    182
+#define OCTEON_IRQ_MSI_BIT55    183
+#define OCTEON_IRQ_MSI_BIT56    184
+#define OCTEON_IRQ_MSI_BIT57    185
+#define OCTEON_IRQ_MSI_BIT58    186
+#define OCTEON_IRQ_MSI_BIT59    187
+#define OCTEON_IRQ_MSI_BIT60    188
+#define OCTEON_IRQ_MSI_BIT61    189
+#define OCTEON_IRQ_MSI_BIT62    190
+#define OCTEON_IRQ_MSI_BIT63    191
+#define OCTEON_IRQ_LAST         192
+#else
+#define OCTEON_IRQ_LAST         96
+#endif
+
+#endif
diff --git a/include/asm-mips/mach-cavium-octeon/kernel-entry-init.h b/include/asm-mips/mach-cavium-octeon/kernel-entry-init.h
new file mode 100644
index 0000000..16a8a2a
--- /dev/null
+++ b/include/asm-mips/mach-cavium-octeon/kernel-entry-init.h
@@ -0,0 +1,116 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2005 Cavium Networks, Inc
+ */
+#ifndef __ASM_MACH_GENERIC_KERNEL_ENTRY_H
+#define __ASM_MACH_GENERIC_KERNEL_ENTRY_H
+
+
+#define CP0_CYCLE_COUNTER $9,6
+#define CP0_CVMCTL_REG $9,7
+#define CP0_CVMMEMCTL_REG $11,7
+#define CP0_PRID_REG $15,0
+#define CP0_PRID_OCTEON_PASS1 0x000d0000
+#define CP0_PRID_OCTEON_CN30XX 0x000d0200
+
+.macro  kernel_entry_setup
+    # Registers set by bootloader:
+    # (only 32 bits set by bootloader, all addresses are physical addresses, and need
+    #   to have the appropriate memory region set by the kernel
+    # a0 = argc
+    # a1 = argv (kseg0 compat addr )
+    # a2 = 1 if init core, zero otherwise
+    # a3 = address of boot descriptor block
+    .set push
+    .set arch=octeon
+    dmfc0   v0, CP0_CVMMEMCTL_REG       # Read the cavium mem control register
+    dins    v0, $0, 0, 6                # Clear the lower 6 bits, the CVMSEG size
+    ori     v0, CONFIG_CAVIUM_OCTEON_CVMSEG_SIZE
+    dmtc0   v0, CP0_CVMMEMCTL_REG       # Write the cavium mem control register
+    dmfc0   v0, CP0_CVMCTL_REG          # Read the cavium control register
+#ifdef CONFIG_CAVIUM_OCTEON_HW_FIX_UNALIGNED
+    or  v0, v0, 0x5001                  # Disable unaligned load/store support but leave HW fixup enabled
+    xor v0, v0, 0x1001
+#else
+    or  v0, v0, 0x5001                  # Disable unaligned load/store and HW fixup support
+    xor v0, v0, 0x5001
+#endif
+    mfc0 v1, CP0_PRID_REG               # Read the processor ID register
+    or  v0, v0, 0x2000                  # Disable instruction prefetching (Octeon Pass1 errata)
+    beq v1, CP0_PRID_OCTEON_PASS1,skip  # Skip reenable of prefetching for Octeon Pass1
+     nop
+    xor v0, v0, 0x2000                  # Reenable instruction prefetching, not on Pass1
+    srl v1, 8                           # Strip off pass number off of processor id
+    sll v1, 8
+    bne v1, CP0_PRID_OCTEON_CN30XX,skip # CN30XX needs some extra stuff turned off for better performance
+     nop
+    or  v0, v0, 0x400                   # CN30XX Use random Icache replacement
+    or  v0, v0, 0x2000                  # CN30XX Disable instruction prefetching
+skip:
+    dmtc0   v0, CP0_CVMCTL_REG          # Write the cavium control register
+    sync
+    cache   9, 0($0)                    # Flush dcache after config change
+
+    PTR_LA  t2, octeon_boot_desc_ptr    # Store the boot descriptor pointer
+    LONG_S  a3, (t2)
+
+    rdhwr   v0, $0                      # Get my core id
+    bne     a2, zero, octeon_main_processor # Jump the master to kernel_entry
+    nop
+
+#ifdef CONFIG_SMP
+
+    #
+    # All cores other than the master need to wait here for SMP bootstrap
+    # to begin
+    #
+    PTR_LA  t0, octeon_processor_boot   # This is the variable where the next core to boot os stored
+octeon_spin_wait_boot:
+    LONG_L  t1, (t0)                    # Get the core id of the next to be booted
+    bne t1, v0, octeon_spin_wait_boot   # Keep looping if it isn't me
+    nop
+    PTR_LA  t0, octeon_processor_cycle  # Synchronize the cycle counters
+    LONG_L  t0, (t0)
+    LONG_ADDU t0, 122                   # Aproximately how many cycles we will be off
+    MTC0    t0, CP0_CYCLE_COUNTER
+    PTR_LA  t0, octeon_processor_gp     # Get my GP from the global variable
+    LONG_L  gp, (t0)
+    PTR_LA  t0, octeon_processor_sp     # Get my SP from the global variable
+    LONG_L  sp, (t0)
+    LONG_S  zero, (t0)                  # Set the SP global variable to zero so the master knows we've started
+#ifdef __OCTEON__
+    syncw
+    syncw
+#else
+    sync
+#endif
+    b   smp_bootstrap                   # Jump to the normal Linux SMP entry point
+    nop
+
+#else /* CONFIG_SMP */
+
+    #
+    # Someone tried to boot SMP with a non SMP kernel. All extra cores
+    # will halt here.
+    #
+octeon_wait_forever:
+    wait
+    b   octeon_wait_forever
+    nop
+
+#endif /* CONFIG_SMP */
+octeon_main_processor:
+    .set pop
+.endm
+
+/*
+ * Do SMP slave processor setup necessary before we can savely execute C code.
+ */
+    .macro  smp_slave_setup
+    .endm
+
+
+#endif /* __ASM_MACH_GENERIC_KERNEL_ENTRY_H */
diff --git a/include/asm-mips/mach-cavium-octeon/mc146818rtc.h b/include/asm-mips/mach-cavium-octeon/mc146818rtc.h
new file mode 100644
index 0000000..f49ece1
--- /dev/null
+++ b/include/asm-mips/mach-cavium-octeon/mc146818rtc.h
@@ -0,0 +1,14 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2004 Cavium Networks
+ */
+#ifndef __ASM_MACH_CAVIUM_OCTEON_MC146818RTC_H__
+#define __ASM_MACH_CAVIUM_OCTEON_MC146818RTC_H__
+
+#define RTC_IRQ 88
+#include <asm/mach-generic/mc146818rtc.h>
+
+#endif
diff --git a/include/asm-mips/mach-cavium-octeon/octeon-hal-read-write.h b/include/asm-mips/mach-cavium-octeon/octeon-hal-read-write.h
new file mode 100644
index 0000000..8c1dfc8
--- /dev/null
+++ b/include/asm-mips/mach-cavium-octeon/octeon-hal-read-write.h
@@ -0,0 +1,38 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2004-2007 Cavium Networks
+ */
+#ifndef __CAVIUM_OCTEON_HAL_READ_WRITE_H__
+#define __CAVIUM_OCTEON_HAL_READ_WRITE_H__
+
+#include "cvmx.h"
+
+
+/**
+ * Write a 32bit value to the Octeon NPI register space
+ *
+ * @param address Address to write to
+ * @param val     Value to write
+ */
+static inline void octeon_npi_write32(uint64_t address, uint32_t val)
+{
+	cvmx_write64_uint32(address ^ 4, val);
+	cvmx_read64_uint32(address ^ 4);
+}
+
+
+/**
+ * Read a 32bit value from the Octeon NPI register space
+ *
+ * @param address Address to read
+ * @return The result
+ */
+static inline uint32_t octeon_npi_read32(uint64_t address)
+{
+	return cvmx_read64_uint32(address ^ 4);
+}
+
+#endif
diff --git a/include/asm-mips/mach-cavium-octeon/param.h b/include/asm-mips/mach-cavium-octeon/param.h
new file mode 100644
index 0000000..e944948
--- /dev/null
+++ b/include/asm-mips/mach-cavium-octeon/param.h
@@ -0,0 +1,13 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2004 Cavium Networks
+ */
+#ifndef __ASM_MACH_CAVIUM_OCTEON_PARAM_OVERRIDES_H
+#define __ASM_MACH_CAVIUM_OCTEON_PARAM_OVERRIDES_H
+
+#define RTC_IRQ 88
+
+#endif
diff --git a/include/asm-mips/mach-cavium-octeon/perf_counters.h b/include/asm-mips/mach-cavium-octeon/perf_counters.h
new file mode 100644
index 0000000..f52a511
--- /dev/null
+++ b/include/asm-mips/mach-cavium-octeon/perf_counters.h
@@ -0,0 +1,24 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2004 Cavium Networks
+ */
+
+/**
+ * The IOCTL numbers supported on /proc/octeon_perf
+ */
+#define PROC_PERF_IOCTL_SETUP_COUNTER0      _IOW(0x81, 0, int)
+#define PROC_PERF_IOCTL_SETUP_COUNTER1      _IOW(0x81, 1, int)
+#define PROC_PERF_IOCTL_SETUP_L2COUNTER0    _IOW(0x81, 2, int)
+#define PROC_PERF_IOCTL_SETUP_L2COUNTER1    _IOW(0x81, 3, int)
+#define PROC_PERF_IOCTL_SETUP_L2COUNTER2    _IOW(0x81, 4, int)
+#define PROC_PERF_IOCTL_SETUP_L2COUNTER3    _IOW(0x81, 5, int)
+#define PROC_PERF_IOCTL_READ_COUNTER0       _IOR(0x81, 6, long long)
+#define PROC_PERF_IOCTL_READ_COUNTER1       _IOR(0x81, 7, long long)
+#define PROC_PERF_IOCTL_READ_L2COUNTER0     _IOR(0x81, 8, long long)
+#define PROC_PERF_IOCTL_READ_L2COUNTER1     _IOR(0x81, 9, long long)
+#define PROC_PERF_IOCTL_READ_L2COUNTER2     _IOR(0x81, 10, long long)
+#define PROC_PERF_IOCTL_READ_L2COUNTER3     _IOR(0x81, 11, long long)
+
-- 
1.5.5.1

