From 948ff959f977a5eabcf00ad514c352f982201d00 Mon Sep 17 00:00:00 2001
From: Greg Moffatt <greg.moffatt@windriver.com>
Date: Fri, 10 Sep 2010 09:32:15 -0700
Subject: [PATCH] RMI/kexec: add kexec support for RPM XLS-based boards

Main feature needed is the specific SMP support. Secondary
cores/threads spin on tight loops in the bootloader memory:
kexec support has to mimic this when doing the handoff from
the first kernel to the second. Unfortunately, this has the
side-effect of preventing from booting a second kernel with
more CPUs than the first one. The opposite works though, as
a crash dump kernel should really be booted with only one CPU
to use an amount of memory as small as possible.

Also included is code to reset devices upon booting.  For kexec to
work properly, the message rings for each CPU must be shut off,
the pending interrupts cleared and ACK'ed, the message ring timers
shut-off and the GMACs reset.

These changes are derived from the kexec support for the XLR732-based
boards completed in the previous release.

Signed-off-by: Greg Moffatt <greg.moffatt@windriver.com>
---
 arch/mips/Makefile                    |    8 +-
 arch/mips/kernel/relocate_kernel.S    |  197 ++++++++++++++++++---
 arch/mips/rmi/phoenix/Makefile        |    1 +
 arch/mips/rmi/phoenix/irq.c           |   33 ++++-
 arch/mips/rmi/phoenix/on_chip.c       |   62 +++++++
 arch/mips/rmi/phoenix/phoenix_kexec.c |  304 +++++++++++++++++++++++++++++++++
 arch/mips/rmi/ptr/config_net.c        |   15 ++
 arch/mips/rmi/ptr/setup.c             |   51 ++++++-
 drivers/net/phoenix_mac.c             |   18 ++
 9 files changed, 655 insertions(+), 34 deletions(-)
 create mode 100644 arch/mips/rmi/phoenix/phoenix_kexec.c

diff --git a/arch/mips/Makefile b/arch/mips/Makefile
index d901ed7..8a368be 100644
--- a/arch/mips/Makefile
+++ b/arch/mips/Makefile
@@ -661,10 +661,6 @@ else
 load-$(CONFIG_CPU_CAVIUM_OCTEON) 	+= 0xffffffff81100000
 endif
 
-ifdef CONFIG_PHYSICAL_START
-load-y				= $(CONFIG_PHYSICAL_START)
-endif
-
 # RMI Phoenix SOC
 core-$(CONFIG_RMI_PHOENIX)      += arch/mips/rmi/phoenix/ 
 
@@ -678,6 +674,10 @@ load-$(CONFIG_RMI_PTR)		+= $(CONFIG_RMI_PHOENIX_LOAD_ADDRESS)
 cflags-y			+= -I$(srctree)/arch/mips/include/asm/mach-generic
 drivers-$(CONFIG_PCI)		+= arch/mips/pci/
 
+ifdef CONFIG_PHYSICAL_START
+load-y				= $(CONFIG_PHYSICAL_START)
+endif
+
 #
 # Automatically detect the build format. By default we choose
 # the elf format according to the load address.
diff --git a/arch/mips/kernel/relocate_kernel.S b/arch/mips/kernel/relocate_kernel.S
index 4324671..e388a20 100644
--- a/arch/mips/kernel/relocate_kernel.S
+++ b/arch/mips/kernel/relocate_kernel.S
@@ -14,12 +14,25 @@
 #include <asm/stackframe.h>
 #include <asm/addrspace.h>
 
+/* These are taken from linux/kexec.h and should always be in sync */
+#ifndef IND_DESTINATION
+#define IND_DESTINATION  0x1
+#endif
+#ifndef IND_INDIRECTION
+#define IND_INDIRECTION  0x2
+#endif
+#ifndef IND_DONE
+#define IND_DONE         0x4
+#endif
+#ifndef IND_SOURCE
+#define IND_SOURCE       0x8
+#endif
 
 LEAF(relocate_new_kernel)
-     PTR_L a0,    arg0
-     PTR_L a1,    arg1
-     PTR_L a2,    arg2
-     PTR_L a3,    arg3
+	PTR_L		a0, arg0
+	PTR_L		a1, arg1
+	PTR_L		a2, arg2
+	PTR_L		a3, arg3
 
 	PTR_LA		s0, kexec_indirection_page
 	PTR_L		s1, kexec_start_address
@@ -28,44 +41,37 @@ process_entry:
 	PTR_L		s2, (s0)
 	PTR_ADD		s0, s0, SZREG
 
-	/* destination page */
-	and		s3, s2, 0x1
+	/* destination page, store it in s4 */
+	and		s3, s2, IND_DESTINATION
 	beq		s3, zero, 1f
 	nop
-	and		s4, s2, ~0x1	/* store destination addr in s4 */
-
-	nop
+	and		s4, s2, ~IND_DESTINATION
 	b		process_entry
 	nop
 
 1:
 	/* indirection page, update s0  */
-	and		s3, s2, 0x2
+	and		s3, s2, IND_INDIRECTION
 	beq		s3, zero, 1f
 	nop
-	and		s0, s2, ~0x2
-
-	nop
-
+	and		s0, s2, ~IND_INDIRECTION
 	b		process_entry
 	nop
 
 1:
-	/* done page */
-	and		s3, s2, 0x4
+	/* done page, stop */
+	and		s3, s2, IND_DONE
 	beq		s3, zero, 1f
 	nop
-
-	nop
 	b		done
 	nop
 
 1:
-	/* source page */
-	and		s3, s2, 0x8
+	/* source page?: copy; if not, skip it (we somehow got crap) */
+	and		s3, s2, IND_SOURCE
 	beq		s3, zero, process_entry
 	nop
-	and		s2, s2, ~0x8
+	and		s2, s2, ~IND_SOURCE
 	li		s6, (1 << PAGE_SHIFT) / SZREG
 
 copy_word:
@@ -81,7 +87,6 @@ copy_word:
 	nop
 
 1:
-	nop
 	b 		process_entry
 	nop
 
@@ -92,7 +97,7 @@ done:
         of kexec_flag.  */
 
 	.align	3
-	bal	1f
+	bal	1f	/* MUST be first instruction in done: */
 	nop
 1:
 	.align	3
@@ -100,8 +105,8 @@ done:
 	move		t1, ra;
 	PTR_LA		t2, done
 	PTR_LA		t0, kexec_flag
-	PTR_SUB		t0, t0, t2;
-	PTR_ADD		t0, t1, t0;
+	PTR_SUBU	t0, t0, t2;
+	PTR_ADDU	t0, t1, t0;
 	LONG_S		zero, (t0)
 #endif
 
@@ -111,13 +116,45 @@ done:
 #ifdef CONFIG_CPU_CAVIUM_OCTEON
 	cache	0, 0($0)
 #endif
+#ifdef CONFIG_RMI_PHOENIX
+	bal     kexec_phoenix_flush_cache
+	nop
+#endif
 
 	/* jump to kexec_start_address */
 	j		s1
 	nop
+END(relocate_new_kernel)
+
+#ifdef CONFIG_RMI_PHOENIX
+LEAF(kexec_phoenix_flush_cache)
+	/* From RMI's boot1/cache.S, but implemented with only 2 regs. */
+	/* FIXME: Obtain size and num cache lines from CP0 config1 reg.
+	 *        (Try to do the following with 2 regs in that case!)
+	 */
+	li		t9,0
+1:
+	li		t8,0x80000000
+	sll		t9,t9,5
+	add		t8,t8,t9
+	sra		t9,t9,5
+	cache		0,0(t8)
+	cache		1,0(t8)
+	addiu		t9,1
+	slti		t8,t9,1024
+	bnez		t8,1b
+	nop
+	nop
+	nop
+	nop
+	nop
+        nop
+        nop
 	nop
+	jr		ra
 	nop
-	END(relocate_new_kernel)
+END(kexec_phoenix_flush_cache)
+#endif /* CONFIG_RMI_PHOENIX */
 
 #ifdef CONFIG_SMP
 /*
@@ -161,8 +198,114 @@ wait:
 	j        s1
 	nop
 	END(kexec_smp_wait)
-#endif
 
+#ifdef CONFIG_RMI_PHOENIX
+/* OK, RMI is interesting. The secondary CPUs do NOT start at the kernel entry,
+ * but rather are signalled to jump to an address in the second kernel, which
+ * is part of the RMI-specific init sequence called from within setup_arch().
+ *
+ * The boot CPU calls a routine installed by the boot loader in boot loader
+ * memory. We have to mimic this by installing a routine in the control page.
+ * This routine will simply set a variable to 1, signalling the secondary CPUs
+ * that they can get out of their spin. The routine also installs a function
+ * pointer that tells the secondary CPUs to jump to a routine (namely
+ * prom_pre_boot_secondary_cpus()) in the second kernel.
+ *
+ * When computing addresses of relocated symbols here, we need the functions
+ * to be double-word aligned, since the pointers are 64-bit, to avoid taking
+ * non-aligned exceptions (the exception handlers are invalid at this point).
+ * The computations are done relative to the start of the functions so that
+ * the start point is aligned. We cannot perform computation based on return
+ * addresses and forward symbols since the assembler might add NOPs after
+ * branches and thus throw the computation off, possibly giving a 4-byte
+ * aligned address instead of an 8-byte aligned one.
+ */
+
+LEAF(kexec_rmi_secondary_cpu_spin)
+	.align	3
+	bal	1f      /* MUST be first instruction */
+	nop
+1:
+	.align	3
+	PTR_ADDIU	ra,ra,-8 /* -8 *if* bal is first instruction */
+	PTR_LA		t0,kexec_rmi_secondary_cpu_spin
+	PTR_LA		v0,kexec_secondary_cpu_spin_var
+	PTR_SUBU	t1,v0,t0 /* t1 <= offset to relocated spin_var */
+	PTR_ADDU	v0,ra,t1 /* v0 now contains the relocated variable addr */
+2:
+	LONG_L		t1,(v0)
+	beq		t1,zero,2b
+	nop
+
+	/* out of the spin loop, kexec_secondary_cpu_next_kernel_entry_point
+	 * now contains the address where to jump to in the second kernel
+	 *
+	 * t0 still contains address of non-relocated
+	 * kexec_rmi_secondary_cpu_spin()
+	 */
+	PTR_LA		v0,kexec_secondary_cpu_next_kernel_entry_point
+	PTR_SUBU	t1,v0,t0 /* t1 <= offset to relocated entry_pt */
+	PTR_ADDU	v0,ra,t1 /* v0 now contains the relocated variable addr */
+	PTR_L		t1,(v0)
+	bal		kexec_phoenix_flush_cache
+	nop
+	jr		t1
+	nop
+END(kexec_rmi_secondary_cpu_spin)
+
+LEAF(kexec_rmi_boot_cpu_wakeup_secondary_cpus)
+	/* a0 contains the address the secondary CPUs will jump to in the
+	 * second kernel
+	 */
+	.align	3
+	move		t3,ra   /* save ra from the second kernel */
+
+	bal		1f      /* MUST be second instruction */
+	nop
+1:
+	.align	3
+	PTR_ADDIU	ra,ra,-12 /* -12 *if* bal is second instruction */
+	PTR_LA		t0,kexec_rmi_boot_cpu_wakeup_secondary_cpus
+	PTR_LA		v0,kexec_secondary_cpu_next_kernel_entry_point
+	PTR_SUBU	t1,v0,t0 /* t1 <= offset to relocated _wakeup_ */
+	PTR_ADDU	v0,ra,t1 /* v0 now contains the relocated var addr */
+	PTR_S		a0,(v0) /* store addr in 2nd kernel where to jump */
+
+	sync
+	nop
+	nop
+	nop
+	nop
+	nop
+
+	/* t0 still contains address of non-relocated
+	 * kexec_rmi_secondary_cpu_spin()
+	 */
+	PTR_LA		v0,kexec_secondary_cpu_spin_var
+	PTR_SUBU	t1,v0,t0 /* t1 <= offset to relocated spin_var */
+	PTR_ADDU	v0,ra,t1 /* v0 now contains the relocated variable addr */
+	dli		t1,0x1
+	PTR_S		t1,(v0)
+
+	jr		t3
+	nop
+END(kexec_rmi_boot_cpu_wakeup_secondary_cpus)
+
+	.align	3
+kexec_secondary_cpu_spin_var:
+EXPORT(kexec_secondary_cpu_spin_var)
+	LONG	0x0
+	.size	kexec_secondary_cpu_spin_var, PTRSIZE
+
+	.align	3
+kexec_secondary_cpu_next_kernel_entry_point:
+EXPORT(kexec_secondary_cpu_next_kernel_entry_point)
+	PTR	0x0
+	.size	kexec_secondary_cpu_next_kernel_entry_point, PTRSIZE
+
+#endif /* CONFIG_RMI_PHOENIX */
+
+#endif /* CONFIG_SMP */
 
 #ifdef __mips64
        /* all PTR's must be aligned to 8 byte in 64-bit mode */
diff --git a/arch/mips/rmi/phoenix/Makefile b/arch/mips/rmi/phoenix/Makefile
index a2528c2..75cf4c1 100644
--- a/arch/mips/rmi/phoenix/Makefile
+++ b/arch/mips/rmi/phoenix/Makefile
@@ -31,6 +31,7 @@ obj-y := irq.o cpu.o time.o on_chip.o platform.o
 obj-y += msgring.o msgring_xls.o msgring_shared.o
 obj-$(CONFIG_SMP) += smp.o
 obj-$(CONFIG_PHOENIX_IP_OVER_PCI) += dma.o
+obj-$(CONFIG_KEXEC) += phoenix_kexec.o
 
 EXTRA_AFLAGS := $(CFLAGS)
 
diff --git a/arch/mips/rmi/phoenix/irq.c b/arch/mips/rmi/phoenix/irq.c
index e81f9bb..07743bb 100644
--- a/arch/mips/rmi/phoenix/irq.c
+++ b/arch/mips/rmi/phoenix/irq.c
@@ -83,9 +83,12 @@ static unsigned int pic_startup(unsigned int irq)
 
 	spin_lock_irqsave(&phnx_pic_lock, flags);
 
-	if (pic_tmask[irq - PIC_IRQ_BASE].set == 0)
+	if (pic_tmask[irq - PIC_IRQ_BASE].set == 0) {
 		phoenix_write_reg(mmio, PIC_IRT_0_BASE + irq - PIC_IRQ_BASE,
 						  thread_mask);
+		/* ACK all IRQs in case they're pending */
+		phoenix_write_reg(mmio, PIC_INT_ACK, (1 << irq));
+	}
 
 	pic_tmask[irq - PIC_IRQ_BASE].valid = 1;
 	/* 
@@ -499,8 +502,17 @@ void do_phnx_IRQ(unsigned int irq, struct pt_regs *regs)
 		do_IRQ(irq);
 }
 
+extern void on_chip_shutoff_msgring(void);
+
 void __cpuinit rmi_smp_irq_init(void)
 {
+	/* shutoff message ring on current CPU */
+	on_chip_shutoff_msgring();
+
+	/* clear all pending interrupts (don't touch timer and soft,
+	 * they're special) */
+	write_64bit_cp0_eirr(0xffffffffffffff7c);
+
 	/* set interrupt mask for non-zero cpus */
 	write_64bit_cp0_eimr(phnx_irq_mask | (1 << IRQ_TIMER));
 
@@ -680,3 +692,22 @@ void pic_setup_threadmask(unsigned int irt, uint32_t mask)
 	phoenix_write_reg(mmio, PIC_IRT_0_BASE + irt, mask);
 	return;
 }
+
+void pic_init(void)
+{
+	int i;
+	phoenix_reg_t *mmio = phoenix_io_mmio(PHOENIX_IO_PIC_OFFSET);
+	uint32_t thread_mask = (1 << hard_smp_processor_id());
+	int level;
+
+	for (i=0; i<PIC_NUM_IRTS; i++) {
+		level = PIC_IRQ_IS_EDGE_TRIGGERED(i);
+
+		phoenix_write_reg(mmio, PIC_IRT_0_BASE + i, thread_mask);
+
+		phoenix_write_reg(mmio, PIC_INT_ACK, (1 << i));
+
+		phoenix_write_reg(mmio, PIC_IRT_1_BASE + i,
+			(level<<30)|(1<<6)|(PIC_IRQ_BASE + i));
+	}
+}
diff --git a/arch/mips/rmi/phoenix/on_chip.c b/arch/mips/rmi/phoenix/on_chip.c
index 329911d..9437164 100644
--- a/arch/mips/rmi/phoenix/on_chip.c
+++ b/arch/mips/rmi/phoenix/on_chip.c
@@ -763,6 +763,66 @@ static void rmi_usb_init(void)
 	}
 }
 
+/* Timer 0 is used for timeouts on the fast messaging network (FMN):
+ * if coming in here from a kexec call, it might still throw interrupts
+ * to CPUs not ready to handle them
+ */
+void on_chip_shutoff_timer_0(void)
+{
+	phoenix_reg_t *mmio = phoenix_io_mmio(PHOENIX_IO_PIC_OFFSET);
+	__u32 control_reg;
+
+	control_reg = pic_read_control();
+	control_reg &= ~(1<<8);
+	pic_write_control(control_reg);
+	phoenix_write_reg(mmio, PIC_INT_ACK, (1 << (PIC_TIMER_0_IRQ - PIC_IRQ_BASE)));
+}
+
+/* need COP2 to be accessible */
+void on_chip_msgring_drain_msgs(void)
+{
+	unsigned int bucket_empty_bm;
+	int bucket;
+	int size, code, stid;
+	struct msgrng_msg msg;
+
+	/* loop is just to be safe, but should only hit once in theory */
+	for(;;) {
+	        bucket_empty_bm = (msgrng_read_status() >> 24) & 0xff;
+
+	        /* all buckets empty, break*/
+	        if (bucket_empty_bm == 0xff) break;
+
+	        /* loop and drop messages */
+	        for (bucket = 0; bucket < 8; bucket++) {
+	                if ((bucket_empty_bm & (1 << bucket)) ||
+	                        !((1 << bucket) & 0xff))
+	                        continue;
+
+	                (void)message_receive(bucket, &size, &code, &stid, &msg);
+	        }
+	}
+}
+
+/* Message ring of the fast messaging network (FMN):
+ * if coming in here from a kexec call, it might still throw interrupts
+ * to CPUs not ready to handle them
+ */
+void on_chip_shutoff_msgring(void)
+{
+       unsigned long flags = 0;
+       msgrng_enable(flags);
+
+       /* shut everything off the hard way */
+       msgrng_write_config(0);
+       /* drain queues */
+       on_chip_msgring_drain_msgs();
+
+       msgrng_disable(flags);
+}
+
+extern void pic_init(void);
+
 void on_chip_init(void)
 {
 	int i = 0, j = 0;
@@ -776,6 +836,8 @@ void on_chip_init(void)
 
 	phnx_msgring_config();
 
+	pic_init();
+
 	phoenix_msgring_cpu_init();
 
 	for (i = 0; i < NR_CPUS; i++)
diff --git a/arch/mips/rmi/phoenix/phoenix_kexec.c b/arch/mips/rmi/phoenix/phoenix_kexec.c
new file mode 100644
index 0000000..b18c768
--- /dev/null
+++ b/arch/mips/rmi/phoenix/phoenix_kexec.c
@@ -0,0 +1,304 @@
+/*
+ * phoenix_kexec.c, kexec support for RMI XLR732-based boards
+ *
+ * Copyright (c) 2009 Wind River Systems, Inc.
+ *
+ * Author:     Benjamin Walsh <benjamin.walsh@windriver.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+ * See the GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ */
+
+#include <linux/kexec.h>
+#include <asm/kexec.h>
+#include <asm/rmi/sim.h>
+#include <asm/bootinfo.h>
+#include <asm/uaccess.h>
+#include <linux/mm.h>
+#include <asm/page.h>
+#include <asm/rmi/mips-exts.h>
+#include <linux/delay.h>
+#include <linux/ctype.h>
+
+#define WAIT_TIME_FOR_OTHER_CPUS_IN_MSECS 10000
+
+extern struct psb_info prom_info_copy;
+extern struct boot_mem_map boot_physaddr_info;
+extern struct boot_mem_map psb_mem_map_copy;
+extern struct boot_mem_map avail_mem_map_copy;
+extern struct boot_mem_map *avail_mem_map_copy_ptr;
+
+static int phoenix_kexec_prepare(struct kimage*);
+static void phoenix_kexec_shutdown(void);
+static void phoenix_smp_handle_restart(unsigned long reloc);
+static void phoenix_crash_shutdown(struct pt_regs *regs);
+static void phoenix_dont_do_cache_flush(void);
+extern void phnx_local_flush_tlb_all(void);
+
+static int rebooting_cpu = -1;
+static cpumask_t cpus_in_reboot = CPU_MASK_NONE;
+
+#ifdef CONFIG_SMP
+void rmi_patch_bootloader_wakeup_fn(ulong reboot_code_buffer);
+void (*relocated_kexec_rmi_secondary_cpu_spin)(void);
+atomic_t kexec_relocate_kernel_ready = ATOMIC_INIT(0);
+extern void kexec_rmi_secondary_cpu_spin(void);
+#endif
+
+extern const unsigned char relocate_new_kernel[];
+extern const size_t relocate_new_kernel_size;
+extern void kexec_rmi_boot_cpu_wakeup_secondary_cpus(void *callback);
+
+extern int  (*_machine_kexec_prepare)(struct kimage *);
+extern void (*_machine_kexec_shutdown)(void);
+extern void (*_machine_crash_shutdown)(struct pt_regs *regs);
+extern void (*_machine_cache_flush)(void);
+extern void (*_machine_smp_handle_restart)(unsigned long reloc);
+
+void phnx_local_flush_tlb_all(void)
+{
+        unsigned long old_ctx;
+        int entry;
+        int tlbsize;
+        unsigned int config1;
+
+        config1 = read_c0_config1();
+        tlbsize = ((config1 >> 25) & 0x3f) + 1;
+
+        /* Save old context and create impossible VPN2 value */
+        old_ctx = (read_c0_entryhi() & 0xff);
+        write_c0_entrylo0(0);
+        write_c0_entrylo1(0);
+        for (entry = 0; entry < tlbsize; entry++) {
+                write_c0_entryhi(((unsigned long) CKSEG0 + (PAGE_SIZE << 1) * entry));
+                write_c0_index(entry);
+                tlb_write_indexed();
+        }
+        write_c0_entryhi(old_ctx);
+}
+
+void phoenix_kexec_init(void)
+{
+	_machine_kexec_prepare      = phoenix_kexec_prepare;
+	_machine_kexec_shutdown     = phoenix_kexec_shutdown;
+	_machine_crash_shutdown     = phoenix_crash_shutdown;
+	_machine_cache_flush	    = phoenix_dont_do_cache_flush;
+	_machine_smp_handle_restart = phoenix_smp_handle_restart;
+}
+
+static void phoenix_dont_do_cache_flush(void)
+{
+	/* don't do anything */
+}
+
+static void shutdown_secondary_cpus(void *crash)
+{
+	struct pt_regs *regs = NULL;
+	int cpu;
+
+	if((unsigned long)crash) {
+		regs = task_pt_regs(current);
+	}
+
+	local_irq_disable();
+	cpu = smp_processor_id();
+
+	if (!cpu_online(cpu)) {
+		return;
+	}
+
+	if (!cpu_isset(cpu, cpus_in_reboot)) {
+		if(regs) {
+			crash_save_cpu(regs, cpu);
+		}
+		cpu_set(cpu, cpus_in_reboot);
+	}
+
+	while(!atomic_read(&kexec_relocate_kernel_ready)) {
+		cpu_relax();
+	}
+	phnx_local_flush_tlb_all();
+	relocated_kexec_rmi_secondary_cpu_spin();
+	/* NOTREACHED */
+}
+
+static void prepare_cpus(const unsigned long crash)
+{
+	smp_call_function(shutdown_secondary_cpus, (void*)crash, 0);
+	smp_wmb();
+	local_irq_disable();
+}
+
+static void wait_for_cpus(void)
+{
+	unsigned int msecs;
+	unsigned int ncpus = num_online_cpus();
+	unsigned int rebooting_cpus;
+
+	rebooting_cpu = smp_processor_id();
+	cpu_set(rebooting_cpu, cpus_in_reboot);
+
+	msecs = WAIT_TIME_FOR_OTHER_CPUS_IN_MSECS;
+	while((rebooting_cpus = cpus_weight(cpus_in_reboot)) < ncpus) {
+		if(--msecs <= 0) {
+			break;
+		}
+		cpu_relax();
+		mdelay(1);
+	}
+}
+
+static int phoenix_kexec_prepare(struct kimage* kimage)
+{
+	struct page *boot_info;
+	struct psb_info *next_kernel_psb_info;
+	void *copy_va;
+	struct kexec_segment *s;
+	char buffer[32], *c;
+	int len, ii, pos, new_argc;
+	int32_t *new_argv;
+	int slurping_spaces;
+
+	/* we have to copy the boot information in a temporary page that is
+	 * available to the second kernel after the first kernel's data/bss
+	 * has been overwritten */
+	boot_info = alloc_pages(GFP_KERNEL, 0);
+	if(!boot_info) {
+		printk(KERN_EMERG "Cannot allocate page for boot info\n");
+		return -ENOMEM;
+	}
+	boot_info->mapping = NULL;
+	set_page_private(boot_info, 0);
+	SetPageReserved(boot_info);
+	copy_va = (void*)page_to_pfn(boot_info);
+	copy_va = (void *)((ulong)copy_va << PAGE_SHIFT);
+	copy_va = (void *)phys_to_virt((ulong)copy_va);
+
+	kexec_args[3] = (unsigned long)copy_va;
+	next_kernel_psb_info = (void *)kexec_args[3];
+
+	memcpy(copy_va, (void*)&prom_info_copy, sizeof(prom_info_copy));
+	copy_va = PTR_ALIGN(((void*)copy_va)+sizeof(prom_info_copy),sizeof(void*));
+
+	memcpy(copy_va, (void*)&boot_physaddr_info, sizeof(boot_physaddr_info));
+	next_kernel_psb_info->psb_physaddr_map = (uint64_t)copy_va;
+	copy_va = PTR_ALIGN(((void*)copy_va)+sizeof(boot_physaddr_info),sizeof(void*));
+
+	memcpy(copy_va, (void*)&psb_mem_map_copy, sizeof(psb_mem_map_copy));
+	next_kernel_psb_info->psb_mem_map = (uint64_t)copy_va;
+	copy_va = PTR_ALIGN(((void*)copy_va)+sizeof(psb_mem_map_copy),sizeof(void*));
+
+	if(avail_mem_map_copy_ptr) {
+		memcpy(copy_va, (void*)&avail_mem_map_copy,
+			sizeof(avail_mem_map_copy));
+		next_kernel_psb_info->avail_mem_map = (uint64_t)copy_va;
+		copy_va = PTR_ALIGN(((void*)copy_va)+sizeof(avail_mem_map_copy),
+					sizeof(void*));
+	} else {
+		next_kernel_psb_info->avail_mem_map = (uint64_t)NULL;
+	}
+	for(ii = 0; ii < kimage->nr_segments; ii++) {
+		s = &kimage->segment[ii];
+		len = min(32, (int)s->bufsz);
+		copy_from_user((void*)buffer, s->buf, len);
+		buffer[len] = 0; /* just in case... */
+		if(strncmp(buffer, "kexec ", 6) == 0) {
+			break;
+		}
+	}
+	if(ii == kimage->nr_segments) {
+		return -1;
+	}
+
+	new_argv = (void*)copy_va;
+	new_argc = 1;
+	new_argv[0] = s->mem|0x80000000;	/* program name: 'kexec' */
+	slurping_spaces = 0; /* 'kexec' string at the beginning, always */
+	for(pos = 0; pos < s->bufsz; pos += len) {
+		len = min(32, (int)s->bufsz - pos);
+		copy_from_user((void*)buffer, s->buf+pos, len);
+		c = buffer;
+		while(c < (buffer + len)) {
+			int buf_offset = (int)(c - buffer);
+			if(slurping_spaces) {
+				if(!isspace(*c)) {
+					new_argv[new_argc] =
+						(int)(s->mem + pos +
+							buf_offset);
+					new_argv[new_argc] |= 0x80000000;
+					++new_argc;
+					slurping_spaces = 0;
+				}
+			} else {
+				if(isspace(*c)) {
+					copy_to_user(s->buf+pos+buf_offset,
+							"", 1);
+					slurping_spaces = 1;
+				}
+			}
+			++c;
+		}
+	}
+	kexec_args[0] = new_argc;
+	kexec_args[1] = (unsigned long)new_argv;
+	kexec_args[2] = 0;	/* no envp */
+
+	return 0;
+}
+
+static void phoenix_kexec_shutdown(void)
+{
+	prepare_cpus(0);
+	wait_for_cpus();
+	phnx_local_flush_tlb_all();
+}
+
+static void phoenix_crash_shutdown(struct pt_regs *regs)
+{
+	crash_save_cpu(regs, smp_processor_id());
+	prepare_cpus(1);
+	wait_for_cpus();
+	phnx_local_flush_tlb_all();
+}
+
+#ifdef CONFIG_SMP
+void rmi_patch_bootloader_wakeup_fn(unsigned long reloc)
+{
+	void (*relocated_wakeup_callback)(void *);
+
+	relocated_wakeup_callback = (void *)(reloc +
+		((ulong)kexec_rmi_boot_cpu_wakeup_secondary_cpus -
+			(ulong)relocate_new_kernel));
+
+	((struct psb_info *)kexec_args[3])->wakeup =
+		(uint64_t)relocated_wakeup_callback;
+}
+
+static void phoenix_smp_handle_restart(unsigned long reloc)
+{
+	/* All secondary cpus now may jump to kexec_wait cycle */
+	relocated_kexec_smp_wait =
+		(void *)(reloc + (kexec_smp_wait - relocate_new_kernel));
+	rmi_patch_bootloader_wakeup_fn(reloc);
+	relocated_kexec_rmi_secondary_cpu_spin =
+		(void *)(reloc + ((ulong)kexec_rmi_secondary_cpu_spin -
+				(ulong)relocate_new_kernel));
+	smp_wmb();
+	atomic_set(&kexec_relocate_kernel_ready, 1);
+}
+#else
+static void phoenix_smp_handle_restart(unsigned long reloc)
+{
+}
+#endif /* CONFIG_SMP */
+
diff --git a/arch/mips/rmi/ptr/config_net.c b/arch/mips/rmi/ptr/config_net.c
index 27cf3b2..3984a3f 100644
--- a/arch/mips/rmi/ptr/config_net.c
+++ b/arch/mips/rmi/ptr/config_net.c
@@ -367,6 +367,11 @@ int phnx_get_phy_info(int instance, int mode, unsigned long *mii_addr,
 	return phy_addr;
 }
 
+extern void rmi_reset_gmacs(void);
+extern unsigned int reset_devices;
+extern void on_chip_shutoff_msgring(void);
+extern void on_chip_shutoff_timer_0(void);
+
 void config_net_init(void)
 {
 
@@ -374,6 +379,16 @@ void config_net_init(void)
 	int i, mode, gmac_pblock = 0;
 	int num_desc = MAX_NUM_DESC;
 
+        if (reset_devices) {
+        	/* Upon a reboot from kexec, some devices can be left in a
+		 * state different than from coming up from the boot loader.
+		 * Put them in a known state.
+         	 */
+        	rmi_reset_gmacs();
+        	on_chip_shutoff_timer_0();
+        	on_chip_shutoff_msgring();
+        }
+
 	init_gmac_ports();
 	for (i = 0; i < PHOENIX_MAX_GMACS; i++) {
 		/* general config for gmac */
diff --git a/arch/mips/rmi/ptr/setup.c b/arch/mips/rmi/ptr/setup.c
index 7fb2742..0423dfb 100644
--- a/arch/mips/rmi/ptr/setup.c
+++ b/arch/mips/rmi/ptr/setup.c
@@ -1048,6 +1048,18 @@ void prom_update_exclude_region(void)
 }
 
 struct boot_mem_map prom_map;
+#ifdef CONFIG_KEXEC
+/* In the case of kexec, these are needed to be passed to the next kernel.
+ * The bootloader's memory gets overwritten, so we have to keep another copy
+ * in the kernel's memory so that they can get passed to the second kernel.
+ * Since the second kernel's data/bss segments will most probably be stomping
+ * on the first kernel's, these will be copied to an intermediate page
+ * obtained at runtime when preparing the second kernel.
+ */
+struct boot_mem_map psb_mem_map_copy;
+struct boot_mem_map avail_mem_map_copy;
+struct boot_mem_map *avail_mem_map_copy_ptr = NULL;
+#endif
 int use_default_phymem = 0;
 
 void read_prom_memory(void)
@@ -1074,6 +1086,16 @@ void read_prom_memory(void)
 		goto use_default;
 	memcpy(&prom_map, map, sizeof(struct boot_mem_map));
 
+#ifdef CONFIG_KEXEC
+       /* keep what's needed for a possible kexec call */
+       memcpy(&psb_mem_map_copy, (void*)prom_info->psb_mem_map,
+                       sizeof(struct boot_mem_map));
+       if(prom_info->avail_mem_map) {
+               memcpy(&avail_mem_map_copy, map, sizeof(struct boot_mem_map));
+               avail_mem_map_copy_ptr = &avail_mem_map_copy;
+       }
+#endif
+
 	return;
 
       use_default:
@@ -1228,6 +1250,10 @@ void __init rmi_nmi_setup(void)
 
 
 extern struct plat_smp_ops rmi_smp_ops;
+#ifdef CONFIG_KEXEC
+extern void phoenix_kexec_init(void);
+#endif
+
 /*
  * prom_init is called just after the cpu type is determined, from setup_arch()
  */
@@ -1247,9 +1273,21 @@ void __init prom_init(void)
 	void (*wakeup) (void *, void *, __u32);
 	__u32 wakeup_mask;
 
-	temp = (int) fw_arg1;
+       /* Normally, when booting from the bootloader, the addresses passed are
+        * in the 32-bit kseg0 segment (ie. 0xffffffff8xxxxxxx). However, when
+        * coming in from a kexec call, they are actually 64-bit living in the
+        * xkphys 0xa800000000000000 segment. Find out where we're from.
+        */
+       if(((fw_arg1 >> 56) & 0xff) == 0xa8) {
+               temp = (long)fw_arg1;
+       } else {
+               temp = (int)fw_arg1;
+       }
 	argv = (char **) temp;
 
+       /* envp doesn't seem to be used... kexec always passes in a NULL pointer
+        * so we don't have to look for a pointer in the xkphys segment
+        */
 	temp = (int) fw_arg2;
 	envp = (char **) temp;
 
@@ -1266,7 +1304,12 @@ void __init prom_init(void)
 	phoenix_psb_shm = 0;
 
 	prom_info = &prom_info_copy;
-	temp = (int) fw_arg3;
+	/* see comment above for fw_arg1 */
+	if(((fw_arg3 >> 56) & 0xff) == 0xa8) {
+		temp = (long)fw_arg3;
+	} else {
+		temp = (int)fw_arg3;
+	}
 	prom_info = &prom_info_copy;
 	t_prom_info = (struct psb_info *) temp;
 	memcpy((void *) prom_info, (void *) t_prom_info,
@@ -1422,6 +1465,10 @@ void __init prom_init(void)
 	prom_reconfigure_thr_resources();
 
 	register_smp_ops(&rmi_smp_ops);
+
+#ifdef CONFIG_KEXEC
+	phoenix_kexec_init();
+#endif
 }
 
 void prom_free_prom_memory(void)
diff --git a/drivers/net/phoenix_mac.c b/drivers/net/phoenix_mac.c
index 3dfde1d..5234dcb 100644
--- a/drivers/net/phoenix_mac.c
+++ b/drivers/net/phoenix_mac.c
@@ -4257,6 +4257,24 @@ void rmi_reset_gmac(phoenix_reg_t * mmio)
 	phoenix_write_reg(mmio, R_RX_CONTROL, val);
 }
 
+void rmi_reset_gmacs(void)
+{
+	int i;
+	struct driver_data *priv;
+
+	if (xlr_loader_support && xlr_loader_sharedcore)
+		return;
+
+	if (xlr_hybrid_rmios_ipsec() || xlr_hybrid_rmios_tcpip_stack())
+		return;
+
+	for (i = 0; i < PHOENIX_MAX_MACS; i++) {
+		priv = netdev_priv(dev_mac_type[TYPE_GMAC][i]);
+		if (priv)
+			rmi_reset_gmac(priv->mmio);
+	}
+}
+
 void rmi_reset_xaui(phoenix_reg_t * mmio)
 {
 	volatile uint32_t val;
-- 
1.6.5.2

