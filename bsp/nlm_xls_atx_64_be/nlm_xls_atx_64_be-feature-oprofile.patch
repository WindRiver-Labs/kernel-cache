From 1ec8819487a75505f59a87ef772c79db6da95be3 Mon Sep 17 00:00:00 2001
From: Jack Tan <jack.tan@windriver.com>
Date: Fri, 21 May 2010 13:07:28 +0800
Subject: [PATCH 38/38] nlm_xls_atx_64_be: feature oprofile

oprofile support

This patch is from RMI SDK 1.7.0

Signed-off-by: Jack Tan <jack.tan@windriver.com>
---
 arch/mips/oprofile/Makefile           |    1 +
 arch/mips/oprofile/common.c           |    3 +
 arch/mips/oprofile/op_model_phoenix.c |  357 +++++++++++++++++++++++++++++++++
 arch/mips/rmi/phoenix/irq.c           |   15 ++
 arch/mips/rmi/phoenix/smp.c           |   10 +
 arch/mips/rmi/phoenix/time.c          |   39 ++++
 6 files changed, 425 insertions(+), 0 deletions(-)
 create mode 100644 arch/mips/oprofile/op_model_phoenix.c

diff --git a/arch/mips/oprofile/Makefile b/arch/mips/oprofile/Makefile
index 3e9b950..ac1227f 100644
--- a/arch/mips/oprofile/Makefile
+++ b/arch/mips/oprofile/Makefile
@@ -17,3 +17,4 @@ oprofile-$(CONFIG_CPU_SB1)		+= op_model_mipsxx.o
 oprofile-$(CONFIG_CPU_VR5500)		+= op_model_vr5500.o
 oprofile-$(CONFIG_CPU_RM9000)		+= op_model_rm9000.o
 oprofile-$(CONFIG_CPU_LOONGSON2)	+= op_model_loongson2.o
+oprofile-$(CONFIG_RMI_PHOENIX)		+= op_model_phoenix.o
diff --git a/arch/mips/oprofile/common.c b/arch/mips/oprofile/common.c
index b1f1b24..fb1839f 100644
--- a/arch/mips/oprofile/common.c
+++ b/arch/mips/oprofile/common.c
@@ -20,6 +20,7 @@ extern struct op_mips_model op_model_mipsxx_ops __weak;
 extern struct op_mips_model op_model_rm9000_ops __weak;
 extern struct op_mips_model op_model_vr5500_ops __weak;
 extern struct op_mips_model op_model_loongson2_ops __weak;
+extern struct op_mips_model op_model_phoenix_ops __weak;
 
 static struct op_mips_model *model;
 
@@ -104,6 +105,8 @@ int __init oprofile_arch_init(struct oprofile_operations *ops)
 	case CPU_R5500:
 		lmodel = &op_model_vr5500_ops;
 		break;
+	case CPU_PHOENIX:
+		lmodel = &op_model_phoenix_ops;
 	};
 
 
diff --git a/arch/mips/oprofile/op_model_phoenix.c b/arch/mips/oprofile/op_model_phoenix.c
new file mode 100644
index 0000000..571ac40
--- /dev/null
+++ b/arch/mips/oprofile/op_model_phoenix.c
@@ -0,0 +1,357 @@
+/*********************************************************************
+
+  Copyright 2003-2006 Raza Microelectronics, Inc. (RMI). All rights
+  reserved.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+  1. Redistributions of source code must retain the above copyright
+  notice, this list of conditions and the following disclaimer.
+  2. Redistributions in binary form must reproduce the above copyright
+  notice, this list of conditions and the following disclaimer in
+  the documentation and/or other materials provided with the
+  distribution.
+
+  THIS SOFTWARE IS PROVIDED BY Raza Microelectronics, Inc. ``AS IS'' AND
+  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL RMI OR CONTRIBUTORS BE LIABLE
+  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+  INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+  CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+  THE POSSIBILITY OF SUCH DAMAGE.
+
+  *****************************#RMI_2#**********************************/
+
+#include <linux/init.h>
+#include <linux/oprofile.h>
+#include <linux/interrupt.h>
+#include <linux/smp.h>
+#include <asm/mipsregs.h>
+
+#include "op_impl.h"
+
+#define PHOENIX_PMC_EVENT_MASK			0x3f
+#define PHOENIX_PMC_EVENT(event) 		\
+				((event & PHOENIX_PMC_EVENT_MASK) << 5)
+#define PHOENIX_PMC_DOM_EXL       		(1U << 0)
+#define PHOENIX_PMC_DOM_KERNEL			(1U << 1)
+#define PHOENIX_PMC_DOM_USR			(1U << 3)
+#define PHOENIX_PMC_ENABLE_INT          (1U << 4)
+#define PHOENIX_PMC_THREAD_ID_MASK		0x03
+#define PHOENIX_PMC_THREAD_ID(tid)      	\
+				((tid & PHOENIX_PMC_THREAD_ID_MASK) << 11)
+#define PHOENIX_PMC_COUNT_ALL_THREADS	(1U << 13)
+
+#define XLR_MAX_PERF_COUNTERS 2
+#define XLR_MAX_CPU_CORES 4
+#define XLR_MAX_CPUS 16
+
+struct op_mips_model op_model_phoenix_ops;
+
+static struct phoenix_register_config
+{
+	unsigned int control[XLR_MAX_PERF_COUNTERS];
+	unsigned int reset_counter[XLR_MAX_PERF_COUNTERS];
+} reg;
+
+/* Compute all of the registers in preparation for enabling profiling. */
+volatile int phnx_perf_core_setup[XLR_MAX_CPU_CORES];
+spinlock_t phnx_perf_lock = SPIN_LOCK_UNLOCKED;
+int g_stop_pmc[XLR_MAX_CPUS];
+
+/* 
+ * Per core Performance counter overflow mask. This 
+ * mask is set to 0xF by the perf counter "owner" 
+ * when a performance event counter overflows. This 
+ * is used by other threads in the core to call 
+ * oprofile_add_sample
+ */
+volatile int phnx_pc_of_mask1[XLR_MAX_CPUS];
+volatile int phnx_pc_of_mask2[XLR_MAX_CPUS];
+
+/* Check if this thread is the owner for PerfCounters in this core */
+int phnx_pmc_owner(void)
+{
+	int cpu_id;
+	unsigned long flags;
+
+	/* Allow only one thread in each core to set perfcounter events */
+	spin_lock_irqsave(&phnx_perf_lock, flags);
+	cpu_id = phoenix_cpu_id();
+	if (phnx_perf_core_setup[cpu_id] == hard_smp_processor_id()) {
+		spin_unlock_irqrestore(&phnx_perf_lock, flags);
+		return 1;
+	}
+	spin_unlock_irqrestore(&phnx_perf_lock, flags);
+	return 0;
+}
+
+/* To be called only from perf interrup handler */
+int phnx_pmc_owner_nolock(void)
+{
+	int cpu_id, h_id;
+	cpu_id = phoenix_cpu_id();
+	h_id = hard_smp_processor_id();
+
+	if (phnx_perf_core_setup[cpu_id] == h_id) {
+		return 1;
+	}
+	return 0;
+}
+
+/* 
+ * Check if perfcounters is already owned 
+ * by some other thread in this core 
+*/
+static int phnx_pmc_owned(void)
+{
+	int cpu_id;
+	unsigned long flags;
+	int thr_id;
+
+	/* Allow only thread0 in each core to set perfcounter events */
+	spin_lock_irqsave(&phnx_perf_lock, flags);
+	thr_id = phoenix_thr_id();
+	if (thr_id) {
+		spin_unlock_irqrestore(&phnx_perf_lock, flags);
+		return 1;
+	}
+	cpu_id = phoenix_cpu_id();
+	if (phnx_perf_core_setup[cpu_id] == -1) {
+		phnx_perf_core_setup[cpu_id] = hard_smp_processor_id();
+		spin_unlock_irqrestore(&phnx_perf_lock, flags);
+		return 0;
+	}
+	if (phnx_perf_core_setup[cpu_id] == hard_smp_processor_id()) {
+		spin_unlock_irqrestore(&phnx_perf_lock, flags);
+		return 0;
+	}
+	spin_unlock_irqrestore(&phnx_perf_lock, flags);
+	return 1;
+}
+
+static void phoenix_reg_setup(struct op_counter_config *ctr)
+{
+	unsigned int counters = op_model_phoenix_ops.num_counters;
+	int i;
+	unsigned long flags;
+
+	/* Compute the performance counter control word. */
+	local_irq_save(flags);
+	for (i = 0; i < counters; i++) {
+		reg.control[i] = 0;
+		reg.reset_counter[i] = 0;
+
+		if (!ctr[i].enabled)
+			continue;
+
+		reg.control[i] = PHOENIX_PMC_EVENT(ctr[i].event) |
+			PHOENIX_PMC_ENABLE_INT | PHOENIX_PMC_COUNT_ALL_THREADS;
+		if (ctr[i].kernel)
+			reg.control[i] |= PHOENIX_PMC_DOM_KERNEL;
+		if (ctr[i].user)
+			reg.control[i] |= PHOENIX_PMC_DOM_USR;
+		if (ctr[i].exl)
+			reg.control[i] |= PHOENIX_PMC_DOM_EXL;
+
+		reg.reset_counter[i] = 0x80000000 - ctr[i].count;
+	}
+	wmb();
+	local_irq_restore(flags);
+}
+
+/* Program all of the registers in preparation for enabling profiling. */
+static void phoenix_cpu_setup(void *args)
+{
+	unsigned long flags;
+	/* 
+	 * Check if some other thread has already taken 
+	 * the ownership of setting perf counters. If not,
+	 * set the ownership
+	*/
+	if (phnx_pmc_owned())
+		return;
+
+	local_irq_save(flags);
+	__write_32bit_c0_register($25, 1, reg.reset_counter[0]);
+	__write_32bit_c0_register($25, 3, reg.reset_counter[1]);
+	local_irq_restore(flags);
+}
+
+static void phoenix_cpu_start(void *args)
+{
+	int core_start = phoenix_cpu_id() * 4;
+	int i;
+	unsigned long flags;
+
+	if (phnx_pmc_owned()) {
+		return;
+	}
+	/* Start all counters on current CPU */
+	local_irq_save(flags);
+
+	__write_32bit_c0_register($25, 0, reg.control[0]);
+	__write_32bit_c0_register($25, 2, reg.control[1]);
+
+	for (i = 0; i < 4; i++)
+		g_stop_pmc[core_start + i] = 0;
+	wmb();
+	local_irq_restore(flags);
+}
+
+static void phoenix_cpu_stop(void *args)
+{
+	int cpu_id = phoenix_cpu_id();
+	int core_start = cpu_id * 4;
+	int i;
+	unsigned long flags;
+
+	local_irq_save(flags);
+	g_stop_pmc[hard_smp_processor_id()] = 1;
+	local_irq_restore(flags);
+
+	if (phnx_pmc_owned())
+		return;
+	/* Stop all counters on current CPU */
+	local_irq_save(flags);
+	__write_32bit_c0_register($25, 0, 0);
+	__write_32bit_c0_register($25, 2, 0);
+	for (i = 0; i < 4; i++) {
+		phnx_pc_of_mask1[core_start + i] = 0;
+		phnx_pc_of_mask2[core_start + i] = 0;
+	}
+	local_irq_restore(flags);
+}
+
+/* 
+ * This handler is called from count compare timer 
+ * interrupt as the perf counter overflow interrupt 
+ * shares the same count compare IRQ.
+*/
+void phoenix_oprofile_int_handler(int irq, void *dev_id, struct pt_regs *regs)
+{
+	uint32_t counter1, counter2;
+	uint32_t control1, control2;
+	int i;
+	int cpu_id = phoenix_cpu_id() * 4;	/* 0, 4, 8 ... 28 */
+	int h_id = hard_smp_processor_id();	/* 0, 1, 2, 3, 4, .....31 */
+	int ret, lcpu;
+	int sample1_taken = 0;
+	int sample2_taken = 0;
+
+	if (g_stop_pmc[h_id])
+		return;
+
+	if (((ret = phnx_pmc_owner_nolock()) == 0)) {
+		/* if any counter overflow occured on this core.... */
+		if (phnx_pc_of_mask1[h_id]) {
+			oprofile_add_sample(regs, 0);
+		}
+		if (phnx_pc_of_mask2[h_id]) {
+			oprofile_add_sample(regs, 1);
+		}
+		return;
+	}
+
+	control1 = __read_32bit_c0_register($25, 0);
+	control2 = __read_32bit_c0_register($25, 2);
+
+	counter1 = __read_32bit_c0_register($25, 1);
+	counter2 = __read_32bit_c0_register($25, 3);
+
+	if (((int) counter1) < 0) {
+		__write_32bit_c0_register($25, 0, 0);
+		oprofile_add_sample(regs, 0);
+		counter1 = reg.reset_counter[0];
+		sample1_taken = 1;
+
+		for (i = 0; i < 4; i++)
+			phnx_pc_of_mask1[cpu_id + i] = 1;
+		wmb();
+		for (i = 1; i < 4; i++) {
+			lcpu = cpu_number_map(cpu_id + i);
+			if (lcpu && cpu_isset(lcpu, cpu_online_map)) {
+				core_send_ipi(lcpu, SMP_OPROFILE_IPI);
+			}
+		}
+	}
+
+	if (((int) counter2) < 0) {
+		__write_32bit_c0_register($25, 2, 0);
+		oprofile_add_sample(regs, 1);
+		counter2 = reg.reset_counter[1];
+		sample2_taken = 1;
+
+		for (i = 0; i < 4; i++)
+			phnx_pc_of_mask2[cpu_id + i] = 1;
+		wmb();
+		for (i = 1; i < 4; i++) {
+			lcpu = cpu_number_map(cpu_id + i);
+			if (lcpu && cpu_isset(lcpu, cpu_online_map)) {
+				core_send_ipi(lcpu, SMP_OPROFILE_IPI);
+			}
+		}
+	}
+
+	if (sample1_taken) {
+		__write_32bit_c0_register($25, 1, counter1);
+		__write_32bit_c0_register($25, 0, reg.control[0]);
+	}
+
+	if (sample2_taken) {
+		__write_32bit_c0_register($25, 3, counter2);
+		__write_32bit_c0_register($25, 2, reg.control[1]);
+	}
+
+	return;
+}
+
+static void phnx_reset_perf_counters(void)
+{
+	__write_32bit_c0_register($25, 0, 0);
+	__write_32bit_c0_register($25, 1, 0);
+
+	__write_32bit_c0_register($25, 2, 0);
+	__write_32bit_c0_register($25, 3, 0);
+}
+
+static int __init phoenix_init(void)
+{
+	int i;
+
+	for (i = 0; i < XLR_MAX_CPU_CORES; i++)
+		phnx_perf_core_setup[i] = -1;
+
+	for (i = 0; i < XLR_MAX_CPUS; i++)
+		g_stop_pmc[i] = 1;
+	phnx_reset_perf_counters();
+
+	return 0;
+}
+
+static void phoenix_exit(void)
+{
+	phnx_reset_perf_counters();
+	return;
+}
+
+/*
+ * The following is assigned to the variable 
+ * 'lmodel' in oprofile_arch_init()
+ */
+struct op_mips_model op_model_phoenix_ops = {
+	.reg_setup = phoenix_reg_setup,
+	.cpu_setup = phoenix_cpu_setup,
+	.init = phoenix_init,
+	.exit = phoenix_exit,
+	.cpu_start = phoenix_cpu_start,
+	.cpu_stop = phoenix_cpu_stop,
+	.cpu_type = "mips/phoenix",
+	.num_counters = XLR_MAX_PERF_COUNTERS,
+};
diff --git a/arch/mips/rmi/phoenix/irq.c b/arch/mips/rmi/phoenix/irq.c
index 38505f8..e81f9bb 100644
--- a/arch/mips/rmi/phoenix/irq.c
+++ b/arch/mips/rmi/phoenix/irq.c
@@ -449,6 +449,10 @@ void __init init_phoenix_irqs(void)
 	for (i = PIC_IRT_FIRST_IRQ; i <= PIC_IRT_LAST_IRQ(); i++)
 		phnx_irq_mask |= (1ULL << i);
 
+#ifdef CONFIG_OPROFILE
+	phnx_irq_mask |= (1ULL << IRQ_IPI_OPROFILE);
+#endif
+
 #ifdef CONFIG_KGDB
 	phnx_irq_mask |= (1ULL << IRQ_IPI_SMP_KGDB);
 #endif
@@ -459,6 +463,11 @@ void __init init_phoenix_irqs(void)
 extern irqreturn_t xlr_kgdb_ipi_handler(int irq, struct pt_regs *regs);
 #endif
 
+#ifdef CONFIG_OPROFILE
+extern void phoenix_oprofile_int_handler(int irq, void *dev_id,
+										 struct pt_regs *regs);
+#endif
+
 void do_phnx_IRQ(unsigned int irq, struct pt_regs *regs)
 {
 #ifdef CONFIG_SMP
@@ -480,6 +489,12 @@ void do_phnx_IRQ(unsigned int irq, struct pt_regs *regs)
 
 	}
 #endif
+#ifdef CONFIG_OPROFILE
+	else if (irq == IRQ_IPI_OPROFILE) {
+		if (phoenix_thr_id() != 0)
+			phoenix_oprofile_int_handler(irq, NULL, regs);
+	}
+#endif
 	else
 		do_IRQ(irq);
 }
diff --git a/arch/mips/rmi/phoenix/smp.c b/arch/mips/rmi/phoenix/smp.c
index baf5b7c..7c7e5cd 100644
--- a/arch/mips/rmi/phoenix/smp.c
+++ b/arch/mips/rmi/phoenix/smp.c
@@ -90,6 +90,16 @@ void core_send_ipi(int logical_cpu, unsigned int action)
 		printk("Sending KGDB IPI 0x%08x to tid %d pid %d\n", ipi, tid, pid);
 #endif
 	}
+
+#ifdef CONFIG_OPROFILE
+	else if(action & SMP_OPROFILE_IPI) {
+		ipi = (tid << 16) | (pid << 20) | IRQ_IPI_OPROFILE;
+#ifdef IPI_PRINTK_DEBUG
+		printk("Sending OPROFILE IPI 0x%08x to tid %d pid %d\n", ipi, tid, pid);
+#endif
+ 	}
+#endif
+
 #ifdef CONFIG_PHOENIX_IP_FLOW_AFFINITY
 	else if (action & SMP_NETRX_IPI)
 	{
diff --git a/arch/mips/rmi/phoenix/time.c b/arch/mips/rmi/phoenix/time.c
index 52404d4..057471a 100644
--- a/arch/mips/rmi/phoenix/time.c
+++ b/arch/mips/rmi/phoenix/time.c
@@ -51,6 +51,10 @@ static unsigned long phoenix_timer_epc[NR_CPUS] ____cacheline_aligned;
 static unsigned long phoenix_timer_cpu[NR_CPUS] ____cacheline_aligned;
 static int free_running_timer_id;
 
+#if defined(CONFIG_PERFCTR) && defined(CONFIG_OPROFILE)
+#error "Cannot enable both VPERF and OProfile at the same time"
+#endif
+
 #ifndef RMI_MAX_PIC_TIMERS
 #define RMI_MAX_PIC_TIMERS 8
 #endif
@@ -63,6 +67,11 @@ extern void phoenix_user_mac_update_time(void);
 extern void phoenix_user_mac_update_ktime(void);
 #endif
  
+#ifdef CONFIG_OPROFILE
+extern void phoenix_oprofile_int_handler(int irq, void * dev_id,
+											struct pt_regs *regs);
+#endif
+
 extern struct irq_chip phnx_rsvd_pic;
 extern struct irqaction phnx_rsvd_action;
 
@@ -84,6 +93,12 @@ void phoenix_timer_interrupt(struct pt_regs *regs, int irq)
 #endif
 	unsigned int    count;
 
+#ifdef CONFIG_OPROFILE
+	int cntr0, cntr1;
+	uint32_t ctrl0, ctrl1;
+	int perfctr_overflow = 0;
+#endif
+
 #ifdef CONFIG_RMI_WATCHDOG
 	/* ack the watchdog */
 	phoenix_write_reg(mmio, 0x0c, 1<<cpu_logical_map(cpu));
@@ -95,6 +110,21 @@ void phoenix_timer_interrupt(struct pt_regs *regs, int irq)
 	}
 
 	count = read_c0_count();
+#ifdef CONFIG_OPROFILE
+	ctrl0 = __read_32bit_c0_register($25, 0);
+	ctrl1 = __read_32bit_c0_register($25, 2);
+	cntr0 = __read_32bit_c0_register($25, 1);
+	cntr1 = __read_32bit_c0_register($25, 3);
+
+	/* 
+	 * if interrupts are enabled for perf events, check if any counter has
+	 * overflowed. Then we know for sure that this is a perf event
+	*/
+	if((ctrl0 & 0x10) || (ctrl1 & 0x10))
+		if((cntr0 < 0) || (cntr1 < 0))
+			perfctr_overflow = 1;
+	if(perfctr_overflow == 0)
+#endif
 	{
 		do_IRQ(irq);
 
@@ -109,6 +139,15 @@ void phoenix_timer_interrupt(struct pt_regs *regs, int irq)
 		save_epc(&phoenix_timer_epc[cpu]);
 		phoenix_timer_cpu[cpu] += irq;
 	}
+
+#ifdef CONFIG_OPROFILE
+	if (perfctr_overflow) {
+		if(phoenix_thr_id() == 0) {
+			phoenix_oprofile_int_handler (irq, NULL, regs);
+		}
+	}
+#endif
+
 }
 
 /* can't do floating in the kernel, so use 64 as an approximation */
-- 
1.6.5.2

