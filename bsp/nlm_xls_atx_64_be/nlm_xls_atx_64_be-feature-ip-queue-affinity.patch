From 3016288854e47f0e03f67eb6c2040319bdcb21a9 Mon Sep 17 00:00:00 2001
From: Jack Tan <jack.tan@windriver.com>
Date: Fri, 21 May 2010 13:07:28 +0800
Subject: [PATCH 37/38] nlm_xls_atx_64_be: feature ip queue affinity

Experimental feature extending IP Queues by allowing multiple user space
processes to receive IP packets from the kernel. Client processes should come
with CPU affinity set to single logical CPU and will get packets which are
recieved and processed by network stack on that logical CPU.

Example:

	Let's Process_1 has CPU affinity set to x
	Let's Process_2 has CPU affinity set to y

	Packet1 --> Interrupt on CPU x --> IP Queues --> Process_1
	Packet1 --> Interrupt on CPU y --> IP Queues --> Process_2

This feature could be useful for packet processing architectures requiring user
space handling of multiple IP flows.

This patch is from RMI SDK 1.7.0

Signed-off-by: Jack Tan <jack.tan@windriver.com>
---
 arch/mips/rmi/Kconfig         |   24 ++++++++++
 drivers/net/phoenix_mac.c     |    9 +++-
 net/ipv4/netfilter/ip_queue.c |   96 +++++++++++++++++++++++++++++++++++++++++
 3 files changed, 127 insertions(+), 2 deletions(-)

diff --git a/arch/mips/rmi/Kconfig b/arch/mips/rmi/Kconfig
index c9d9d1c..55ef265 100644
--- a/arch/mips/rmi/Kconfig
+++ b/arch/mips/rmi/Kconfig
@@ -85,6 +85,30 @@ config PHOENIX_IP_FLOW_AFFINITY
 
 	  If in doubt, say N.
 
+config PHOENIX_IP_QUEUE_AFFINITY
+	bool "Enable multiprocess support for IP Queues"
+	depends on RMI_PHOENIX && IP_NF_QUEUE
+	default n
+	help
+		Experimental feature extending IP Queues by allowing multiple user space 
+		processes to receive IP packets from the kernel. Client processes should come 
+		with CPU affinity set to single logical CPU and will get packets which are 
+		recieved and processed by network stack on that logical CPU.
+
+		Example:
+
+			Let's Process_1 has CPU affinity set to x
+			Let's Process_2 has CPU affinity set to y
+
+			Packet1 --> Interrupt on CPU x --> IP Queues --> Process_1
+			Packet1 --> Interrupt on CPU y --> IP Queues --> Process_2
+
+			This feature could be useful for packet processing architectures requiring user 
+			space handling of multiple IP flows.
+
+		If in doubt, say N.
+
+
 config RMI_PHOENIX_LOAD_ADDRESS
 	hex "RMI Linux kernel start address"
 	depends on RMI_PHOENIX
diff --git a/drivers/net/phoenix_mac.c b/drivers/net/phoenix_mac.c
index bf6b90d..3dfde1d 100644
--- a/drivers/net/phoenix_mac.c
+++ b/drivers/net/phoenix_mac.c
@@ -2490,8 +2490,6 @@ void rmi_phnx_rmios_msgring_handler(int bucket, int size, int code,
 	netif_rx(skb);
 }
 
-
-
 /* This function is called from an interrupt handler */
 void rmi_phnx_mac_msgring_handler(int bucket, int size, int code,
 				  int stid, struct msgrng_msg *msg,
@@ -2655,6 +2653,13 @@ do { \
 		phnx_set_counter(NETIF_RX_CYCLES,
 				 (read_c0_count() - msgrng_msg_cycles));
 
+#ifdef CONFIG_PHOENIX_IP_QUEUE_AFFINITY
+		/* 
+		 * We pass bucket number in the last field of skb->cb[] structure 
+		 * it might be later picked up by multiprocess ip_queue
+        */
+		skb->cb[sizeof(skb->cb) - 1] = bucket;
+#endif
 
 #ifdef CONFIG_PHOENIX_IP_FLOW_AFFINITY
 		skb_transfer(bucket, skb);
diff --git a/net/ipv4/netfilter/ip_queue.c b/net/ipv4/netfilter/ip_queue.c
index e278704..de8b929 100644
--- a/net/ipv4/netfilter/ip_queue.c
+++ b/net/ipv4/netfilter/ip_queue.c
@@ -38,12 +38,20 @@
 #define NET_IPQ_QMAX 2088
 #define NET_IPQ_QMAX_NAME "ip_queue_maxlen"
 
+#undef  RMI_IPQ_DEBUG
+
 typedef int (*ipq_cmpfn)(struct nf_queue_entry *, unsigned long);
 
 static unsigned char copy_mode __read_mostly = IPQ_COPY_NONE;
 static unsigned int queue_maxlen __read_mostly = IPQ_QMAX_DEFAULT;
 static DEFINE_RWLOCK(queue_lock);
+
+#ifdef CONFIG_PHOENIX_IP_QUEUE_AFFINITY
+static int peer_pid[NR_CPUS];
+#else
 static int peer_pid __read_mostly;
+#endif /* CONFIG_PHOENIX_IP_QUEUE_AFFINITY */
+
 static unsigned int copy_range __read_mostly;
 static unsigned int queue_total;
 static unsigned int queue_dropped = 0;
@@ -90,10 +98,30 @@ static void __ipq_flush(ipq_cmpfn cmpfn, unsigned long data);
 static inline void
 __ipq_reset(void)
 {
+#ifdef CONFIG_PHOENIX_IP_QUEUE_AFFINITY
+	int i;
+	int total = 0;
+#ifdef RMI_IPQ_DEBUG	
+	printk(KERN_ALERT "%s: Removing PID %d from core %d\n",
+			__FUNCTION__, peer_pid[smp_processor_id()], smp_processor_id());
+#endif
+	peer_pid[smp_processor_id()] = 0;
+	
+	for (i = 0; i < NR_CPUS; i++) {
+		total |= peer_pid[i];
+	}
+	
+	if (total == 0) {
+		net_disable_timestamp();
+		__ipq_set_mode(IPQ_COPY_NONE, 0);
+		__ipq_flush(NF_DROP, 0);
+	}
+#else
 	peer_pid = 0;
 	net_disable_timestamp();
 	__ipq_set_mode(IPQ_COPY_NONE, 0);
 	__ipq_flush(NULL, 0);
+#endif /* CONFIG_PHOENIX_IP_QUEUE_AFFINITY */
 }
 
 static struct nf_queue_entry *
@@ -235,6 +263,10 @@ ipq_enqueue_packet(struct nf_queue_entry *entry, unsigned int queuenum)
 {
 	int status = -EINVAL;
 	struct sk_buff *nskb;
+#ifdef CONFIG_PHOENIX_IP_QUEUE_AFFINITY
+	int dst_cpu;
+	struct sk_buff *skb = entry->skb;
+#endif /* CONFIG_PHOENIX_IP_QUEUE_AFFINITY */
 
 	if (copy_mode == IPQ_COPY_NONE)
 		return -EAGAIN;
@@ -245,7 +277,12 @@ ipq_enqueue_packet(struct nf_queue_entry *entry, unsigned int queuenum)
 
 	write_lock_bh(&queue_lock);
 
+#ifdef CONFIG_PHOENIX_IP_QUEUE_AFFINITY
+	dst_cpu = smp_processor_id() + (skb->cb[sizeof(skb->cb) - 1] & 3);
+	if (!peer_pid[dst_cpu])
+#else
 	if (!peer_pid)
+#endif /* CONFIG_PHOENIX_IP_QUEUE_AFFINITY */
 		goto err_out_free_nskb;
 
 	if (queue_total >= queue_maxlen) {
@@ -259,7 +296,17 @@ ipq_enqueue_packet(struct nf_queue_entry *entry, unsigned int queuenum)
 	}
 
 	/* netlink_unicast will either free the nskb or attach it to a socket */
+
+#ifdef CONFIG_PHOENIX_IP_QUEUE_AFFINITY
+#ifdef RMI_IPQ_DEBUG
+	printk(KERN_ALERT "%s: Sending packet from bucket: %d\n",
+			__FUNCTION__, skb->cb[sizeof(skb->cb) - 1]);
+#endif
+	status = netlink_unicast(ipqnl, nskb, peer_pid[dst_cpu], MSG_DONTWAIT);
+#else
 	status = netlink_unicast(ipqnl, nskb, peer_pid, MSG_DONTWAIT);
+#endif  /* CONFIG_PHOENIX_IP_QUEUE_AFFINITY */
+
 	if (status < 0) {
 		queue_user_dropped++;
 		goto err_out_unlock;
@@ -443,6 +490,25 @@ __ipq_rcv_skb(struct sk_buff *skb)
 
 	write_lock_bh(&queue_lock);
 
+#ifdef CONFIG_PHOENIX_IP_QUEUE_AFFINITY
+#ifdef RMI_IPQ_DEBUG
+	if (current->pid != pid)
+	{
+		printk(KERN_ALERT "%s: PID mismatch: current: %d, PID: %d, core %d\n",
+				__FUNCTION__, current->pid, pid, smp_processor_id());
+	}
+#endif /* RMI_IPQ_DEBUG */
+
+	if (!peer_pid[smp_processor_id()])
+	{
+		net_enable_timestamp();
+		peer_pid[smp_processor_id()] = pid;
+#ifdef  RMI_IPQ_DEBUG
+		printk(KERN_ALERT "%s: Setting PID %d for core %d\n",
+				__FUNCTION__, pid, smp_processor_id());
+#endif
+	}
+#else
 	if (peer_pid) {
 		if (peer_pid != pid) {
 			write_unlock_bh(&queue_lock);
@@ -452,6 +518,7 @@ __ipq_rcv_skb(struct sk_buff *skb)
 		net_enable_timestamp();
 		peer_pid = pid;
 	}
+#endif  /* CONFIG_PHOENIX_IP_QUEUE_AFFINITY */
 
 	write_unlock_bh(&queue_lock);
 
@@ -500,7 +567,11 @@ ipq_rcv_nl_event(struct notifier_block *this,
 
 	if (event == NETLINK_URELEASE && n->protocol == NETLINK_FIREWALL) {
 		write_lock_bh(&queue_lock);
+#ifdef CONFIG_PHOENIX_IP_QUEUE_AFFINITY
+		if ((net_eq(n->net, &init_net)) && (n->pid == peer_pid[smp_processor_id()]))
+#else
 		if ((net_eq(n->net, &init_net)) && (n->pid == peer_pid))
+#endif
 			__ipq_reset();
 		write_unlock_bh(&queue_lock);
 	}
@@ -529,8 +600,32 @@ static ctl_table ipq_table[] = {
 #ifdef CONFIG_PROC_FS
 static int ip_queue_show(struct seq_file *m, void *v)
 {
+#ifdef CONFIG_PHOENIX_IP_QUEUE_AFFINITY
+	int i;
+#endif /* CONFIG_PHOENIX_IP_QUEUE_AFFINITY */
+
 	read_lock_bh(&queue_lock);
 
+#ifdef CONFIG_PHOENIX_IP_QUEUE_AFFINITY
+	seq_printf(m, "Peer PIDs: \n");
+	for (i = 0; i < NR_CPUS; i++)
+		seq_printf(m, "%d", peer_pid[i]);
+
+	seq_printf(m, "\n");
+	seq_printf(m,
+		      "Copy mode         : %hu\n"
+		      "Copy range        : %u\n"
+		      "Queue length      : %u\n"
+		      "Queue max. length : %u\n"
+		      "Queue dropped     : %u\n"
+		      "Netlink dropped   : %u\n",
+		      copy_mode,
+		      copy_range,
+		      queue_total,
+		      queue_maxlen,
+		      queue_dropped,
+		      queue_user_dropped);
+#else
 	seq_printf(m,
 		      "Peer PID          : %d\n"
 		      "Copy mode         : %hu\n"
@@ -546,6 +641,7 @@ static int ip_queue_show(struct seq_file *m, void *v)
 		      queue_maxlen,
 		      queue_dropped,
 		      queue_user_dropped);
+#endif  /* CONFIG_PHOENIX_IP_QUEUE_AFFINITY */
 
 	read_unlock_bh(&queue_lock);
 	return 0;
-- 
1.6.5.2

