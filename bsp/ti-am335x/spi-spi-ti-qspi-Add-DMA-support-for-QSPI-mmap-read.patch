From b645dc3b929d7e94a21db454c9c81c97c14f9e01 Mon Sep 17 00:00:00 2001
From: Vignesh R <vigneshr@ti.com>
Date: Fri, 16 Oct 2015 09:51:12 +0530
Subject: [PATCH 619/800] spi: spi-ti-qspi: Add DMA support for QSPI mmap read

Since QSPI support mmap-read, mem-to-mem DMA can be used to increase
throughput and decrease CPU load. But, the flash filesystems like JFFS2
and UBIFS don't provide kmalloc'd buffers, hence are not DMA safe. So,
use 128KB, pre-allocated bounce buffer. The data from flash is first
read into bounce buffer using mem-to-mem DMA copy, this data is then
copied to destination buffer using memcpy(). Even with bounce buffer,
the throughput is far better than existing mmap-mode read.

With this patch the raw-read throughput is ~14MB/s on DRA74 EVM. And CPU
load is <20%.

Signed-off-by: Vignesh R <vigneshr@ti.com>
Signed-off-by: Sekhar Nori <nsekhar@ti.com>
[zou: Original patch taken from PROCESSOR-SDK-LINUX-AM335X 02_00_01_07]
Signed-off-by: zou cao <cao.zou@windriver.com>
---
 drivers/spi/spi-ti-qspi.c |  112 +++++++++++++++++++++++++++++++++++++++++++-
 1 files changed, 109 insertions(+), 3 deletions(-)

diff --git a/drivers/spi/spi-ti-qspi.c b/drivers/spi/spi-ti-qspi.c
index af5d23c..cb59d1e 100644
--- a/drivers/spi/spi-ti-qspi.c
+++ b/drivers/spi/spi-ti-qspi.c
@@ -21,7 +21,8 @@
 #include <linux/delay.h>
 #include <linux/dma-mapping.h>
 #include <linux/dmaengine.h>
-#include <linux/omap-dma.h>
+#include <linux/omap-dmaengine.h>
+#include <linux/edma.h>
 #include <linux/platform_device.h>
 #include <linux/err.h>
 #include <linux/clk.h>
@@ -39,6 +40,8 @@ struct ti_qspi_regs {
 };
 
 struct ti_qspi {
+	struct completion	transfer_complete;
+
 	/* list synchronization */
 	struct mutex            list_lock;
 
@@ -48,9 +51,13 @@ struct ti_qspi {
 	void __iomem            *mmap_base;
 	struct clk		*fclk;
 	struct device           *dev;
-
 	struct ti_qspi_regs     ctx_reg;
 
+	dma_addr_t		mmap_phys_base;
+	dma_addr_t		rx_bb_dma_addr;
+	void			*rx_bb_addr;
+	struct dma_chan		*rx_chan;
+
 	u32 spi_max_frequency;
 	u32 cmd;
 	u32 dc;
@@ -116,6 +123,8 @@ struct ti_qspi {
 #define QSPI_SETUP_ADDR_SHIFT		8
 #define QSPI_SETUP_DUMMY_SHIFT		10
 
+#define QSPI_DMA_BUFFER_SIZE		131072U
+
 static inline unsigned long ti_qspi_read(struct ti_qspi *qspi,
 		unsigned long reg)
 {
@@ -373,6 +382,59 @@ static int qspi_transfer_msg(struct ti_qspi *qspi, struct spi_transfer *t)
 	return 0;
 }
 
+static void qspi_dma_callback(void *param)
+{
+	struct ti_qspi *qspi = param;
+
+	complete(&qspi->transfer_complete);
+}
+
+static int qspi_dma_transfer(struct ti_qspi *qspi, dma_addr_t dma_dst,
+			     dma_addr_t dma_src, size_t len)
+{
+	struct dma_chan *chan = qspi->rx_chan;
+	struct dma_device *dma_dev = chan->device;
+	dma_cookie_t cookie;
+	enum dma_ctrl_flags flags = DMA_CTRL_ACK | DMA_PREP_INTERRUPT;
+	struct dma_async_tx_descriptor *tx = NULL;
+	int ret;
+
+	tx = dma_dev->device_prep_dma_memcpy(chan, dma_dst, dma_src,
+					     len, flags);
+	if (!tx) {
+		dev_err(qspi->dev, "device_prep_dma_memcpy error\n");
+		ret = -EIO;
+		goto err;
+	}
+
+	tx->callback = qspi_dma_callback;
+	tx->callback_param = qspi;
+	cookie = tx->tx_submit(tx);
+
+	ret = dma_submit_error(cookie);
+	if (ret) {
+		dev_err(qspi->dev, "dma_submit_error %d\n", cookie);
+		goto err;
+	}
+
+	dma_async_issue_pending(chan);
+
+	ret = wait_for_completion_timeout(&qspi->transfer_complete,
+					  msecs_to_jiffies(len));
+	if (ret <= 0) {
+		dmaengine_terminate_all(chan);
+		dev_err(qspi->dev, "DMA wait_for_completion_timeout\n");
+		if (!ret)
+			ret = -ETIMEDOUT;
+		goto err;
+	}
+
+	ret = 0;
+
+err:
+	return ret;
+}
+
 static void ti_qspi_enable_memory_map(struct spi_device *spi)
 {
 	struct ti_qspi  *qspi = spi_master_get_devdata(spi->master);
@@ -470,7 +532,27 @@ static int ti_qspi_mmap_read(struct spi_master *master,
 		status = -EBUSY;
 		goto err;
 	}
-	memcpy((void *)to, qspi->mmap_base + from, len);
+	if (qspi->rx_chan) {
+		/*
+		 * Use bounce buffer as FS like jffs2, ubifs do not
+		 * provide kmalloc'd buffers.
+		 */
+		size_t readsize = len;
+		dma_addr_t dma_src = qspi->mmap_phys_base + from;
+
+		while (readsize != 0) {
+			size_t xfer_len = min(QSPI_DMA_BUFFER_SIZE, readsize);
+
+			qspi_dma_transfer(qspi, qspi->rx_bb_dma_addr,
+					  dma_src, xfer_len);
+			memcpy((void *)to, qspi->rx_bb_addr, xfer_len);
+			readsize -= xfer_len;
+			dma_src += xfer_len;
+			to += xfer_len;
+		}
+	} else {
+		memcpy_fromio((void *)to, qspi->mmap_base + from, len);
+	}
 
 err:
 	ti_qspi_disable_memory_map(spi);
@@ -562,6 +644,7 @@ static int ti_qspi_probe(struct platform_device *pdev)
 	struct spi_master *master;
 	struct resource         *r, *res_ctrl, *res_mmap;
 	struct device_node *np = pdev->dev.of_node;
+	dma_cap_mask_t mask;
 	u32 max_freq;
 	int ret = 0, num_cs, irq;
 
@@ -640,6 +723,7 @@ static int ti_qspi_probe(struct platform_device *pdev)
 	}
 
 	if (res_mmap) {
+		qspi->mmap_phys_base = (dma_addr_t)res_mmap->start;
 		qspi->mmap_base = devm_ioremap_resource(&pdev->dev, res_mmap);
 		if (IS_ERR(qspi->mmap_base)) {
 			ret = PTR_ERR(qspi->mmap_base);
@@ -664,10 +748,29 @@ static int ti_qspi_probe(struct platform_device *pdev)
 	if (ret)
 		goto free_master;
 
+	dma_cap_zero(mask);
+	dma_cap_set(DMA_MEMCPY, mask);
+
+	qspi->rx_chan = dma_request_channel(mask, NULL, NULL);
+	if (!qspi->rx_chan) {
+		dev_err(qspi->dev,
+			"No Rx DMA available, using PIO mode\n");
+		ret = 0;
+		goto no_dma;
+	}
+	qspi->rx_bb_addr = dma_alloc_coherent(qspi->dev,
+					      QSPI_DMA_BUFFER_SIZE,
+					      &qspi->rx_bb_dma_addr,
+					      GFP_KERNEL | GFP_DMA);
+	if (!qspi->rx_bb_addr)
+		dev_err(qspi->dev,
+			"dma_alloc_coherent falied, using PIO mode\n");
+	init_completion(&qspi->transfer_complete);
 	return 0;
 
 free_master:
 	spi_master_put(master);
+no_dma:
 	return ret;
 }
 
@@ -682,6 +785,9 @@ static int ti_qspi_remove(struct platform_device *pdev)
 		return ret;
 	}
 
+	dma_free_coherent(qspi->dev, QSPI_DMA_BUFFER_SIZE,
+			  qspi->rx_bb_addr, qspi->rx_bb_dma_addr);
+
 	pm_runtime_put(qspi->dev);
 	pm_runtime_disable(&pdev->dev);
 
-- 
1.7.5.4

