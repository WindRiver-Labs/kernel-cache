From f03cf40200364588a89892ac4f50ba7df6860e8f Mon Sep 17 00:00:00 2001
From: auto commit <unknown@unknown>
Date: Fri, 24 Oct 2008 12:22:49 -0700
Subject: [PATCH] cavium: import executive from SDK

Import of Simple Executive from SDK 1.8.0 build 270.

Signed-off-by: Tomaso Paoletti <tpaoletti@caviumnetworks.com>
---
 arch/mips/cavium-octeon/executive/README.txt       |   43 +
 arch/mips/cavium-octeon/executive/cvmip.h          |  215 +
 arch/mips/cavium-octeon/executive/cvmx-abi.h       |   99 +
 .../cavium-octeon/executive/cvmx-app-init-linux.c  |  437 +++
 arch/mips/cavium-octeon/executive/cvmx-app-init.c  |  670 ++++
 arch/mips/cavium-octeon/executive/cvmx-app-init.h  |  273 ++
 arch/mips/cavium-octeon/executive/cvmx-asm.h       |  500 +++
 arch/mips/cavium-octeon/executive/cvmx-asx.h       |   74 +
 arch/mips/cavium-octeon/executive/cvmx-atomic.h    |  674 ++++
 arch/mips/cavium-octeon/executive/cvmx-bootmem.c   |  932 +++++
 arch/mips/cavium-octeon/executive/cvmx-bootmem.h   |  437 +++
 arch/mips/cavium-octeon/executive/cvmx-ciu.h       |   73 +
 arch/mips/cavium-octeon/executive/cvmx-cmd-queue.c |  319 ++
 arch/mips/cavium-octeon/executive/cvmx-cmd-queue.h |  619 +++
 .../cavium-octeon/executive/cvmx-cn3010-evb-hs5.c  |  222 ++
 .../cavium-octeon/executive/cvmx-cn3010-evb-hs5.h  |   77 +
 .../cavium-octeon/executive/cvmx-compactflash.c    |  244 ++
 .../cavium-octeon/executive/cvmx-compactflash.h    |   74 +
 arch/mips/cavium-octeon/executive/cvmx-core.c      |  155 +
 arch/mips/cavium-octeon/executive/cvmx-core.h      |  174 +
 arch/mips/cavium-octeon/executive/cvmx-coremask.c  |  140 +
 arch/mips/cavium-octeon/executive/cvmx-coremask.h  |  169 +
 arch/mips/cavium-octeon/executive/cvmx-cvmmem.h    |   81 +
 arch/mips/cavium-octeon/executive/cvmx-dfa.c       |  128 +
 arch/mips/cavium-octeon/executive/cvmx-dfa.h       |  808 ++++
 .../mips/cavium-octeon/executive/cvmx-dma-engine.c |  391 ++
 .../mips/cavium-octeon/executive/cvmx-dma-engine.h |  340 ++
 arch/mips/cavium-octeon/executive/cvmx-ebt3000.c   |  120 +
 arch/mips/cavium-octeon/executive/cvmx-ebt3000.h   |   75 +
 arch/mips/cavium-octeon/executive/cvmx-fau.h       |  644 +++
 arch/mips/cavium-octeon/executive/cvmx-flash.c     |  680 ++++
 arch/mips/cavium-octeon/executive/cvmx-flash.h     |  142 +
 arch/mips/cavium-octeon/executive/cvmx-fpa.c       |  219 ++
 arch/mips/cavium-octeon/executive/cvmx-fpa.h       |  308 ++
 arch/mips/cavium-octeon/executive/cvmx-gmx.h       |  102 +
 arch/mips/cavium-octeon/executive/cvmx-gpio.h      |  130 +
 .../cavium-octeon/executive/cvmx-helper-board.c    |  633 +++
 .../cavium-octeon/executive/cvmx-helper-board.h    |  174 +
 .../executive/cvmx-helper-check-defines.h          |  110 +
 .../cavium-octeon/executive/cvmx-helper-errata.c   |  320 ++
 .../cavium-octeon/executive/cvmx-helper-errata.h   |  104 +
 .../mips/cavium-octeon/executive/cvmx-helper-fpa.c |  254 ++
 .../mips/cavium-octeon/executive/cvmx-helper-fpa.h |   89 +
 .../cavium-octeon/executive/cvmx-helper-loop.c     |  119 +
 .../cavium-octeon/executive/cvmx-helper-loop.h     |   98 +
 .../mips/cavium-octeon/executive/cvmx-helper-npi.c |  111 +
 .../mips/cavium-octeon/executive/cvmx-helper-npi.h |   88 +
 .../cavium-octeon/executive/cvmx-helper-rgmii.c    |  429 ++
 .../cavium-octeon/executive/cvmx-helper-rgmii.h    |  121 +
 .../cavium-octeon/executive/cvmx-helper-sgmii.c    |  487 +++
 .../cavium-octeon/executive/cvmx-helper-sgmii.h    |  115 +
 .../mips/cavium-octeon/executive/cvmx-helper-spi.c |  227 ++
 .../mips/cavium-octeon/executive/cvmx-helper-spi.h |  115 +
 .../cavium-octeon/executive/cvmx-helper-util.c     |  424 ++
 .../cavium-octeon/executive/cvmx-helper-util.h     |  220 ++
 .../cavium-octeon/executive/cvmx-helper-xaui.c     |  300 ++
 .../cavium-octeon/executive/cvmx-helper-xaui.h     |  116 +
 arch/mips/cavium-octeon/executive/cvmx-helper.c    |  768 ++++
 arch/mips/cavium-octeon/executive/cvmx-helper.h    |  251 ++
 arch/mips/cavium-octeon/executive/cvmx-higig.h     |  305 ++
 .../executive/cvmx-interrupt-decodes.c             | 3424 ++++++++++++++++
 .../executive/cvmx-interrupt-handler.S             |  189 +
 .../cavium-octeon/executive/cvmx-interrupt-rsl.c   |  702 ++++
 arch/mips/cavium-octeon/executive/cvmx-interrupt.c |  519 +++
 arch/mips/cavium-octeon/executive/cvmx-interrupt.h |  304 ++
 arch/mips/cavium-octeon/executive/cvmx-iob.h       |   74 +
 arch/mips/cavium-octeon/executive/cvmx-ipd.h       |  183 +
 arch/mips/cavium-octeon/executive/cvmx-key.h       |  121 +
 arch/mips/cavium-octeon/executive/cvmx-l2c.c       |  762 ++++
 arch/mips/cavium-octeon/executive/cvmx-l2c.h       |  372 ++
 arch/mips/cavium-octeon/executive/cvmx-llm.c       |  975 +++++
 arch/mips/cavium-octeon/executive/cvmx-llm.h       |  409 ++
 arch/mips/cavium-octeon/executive/cvmx-lmc.h       |   74 +
 arch/mips/cavium-octeon/executive/cvmx-log-arc.S   |  184 +
 arch/mips/cavium-octeon/executive/cvmx-log.c       |  544 +++
 arch/mips/cavium-octeon/executive/cvmx-log.h       |  220 ++
 arch/mips/cavium-octeon/executive/cvmx-malloc.h    |  227 ++
 .../executive/cvmx-malloc/README-malloc            |   12 +
 .../cavium-octeon/executive/cvmx-malloc/arena.c    |  293 ++
 .../cavium-octeon/executive/cvmx-malloc/malloc.c   | 4106 ++++++++++++++++++++
 .../cavium-octeon/executive/cvmx-malloc/malloc.h   |  213 +
 .../cavium-octeon/executive/cvmx-malloc/thread-m.h |   73 +
 arch/mips/cavium-octeon/executive/cvmx-mdio.h      |  353 ++
 arch/mips/cavium-octeon/executive/cvmx-mgmt-port.c |  665 ++++
 arch/mips/cavium-octeon/executive/cvmx-mgmt-port.h |  183 +
 arch/mips/cavium-octeon/executive/cvmx-mio.h       |   74 +
 arch/mips/cavium-octeon/executive/cvmx-npi.h       |   73 +
 arch/mips/cavium-octeon/executive/cvmx-packet.h    |   92 +
 arch/mips/cavium-octeon/executive/cvmx-pci.h       |   76 +
 arch/mips/cavium-octeon/executive/cvmx-pcie.c      | 1028 +++++
 arch/mips/cavium-octeon/executive/cvmx-pcie.h      |  301 ++
 arch/mips/cavium-octeon/executive/cvmx-pip.h       |  483 +++
 arch/mips/cavium-octeon/executive/cvmx-pko.c       |  497 +++
 arch/mips/cavium-octeon/executive/cvmx-pko.h       |  755 ++++
 arch/mips/cavium-octeon/executive/cvmx-platform.h  |  187 +
 arch/mips/cavium-octeon/executive/cvmx-pow.c       |  488 +++
 arch/mips/cavium-octeon/executive/cvmx-pow.h       | 1750 +++++++++
 arch/mips/cavium-octeon/executive/cvmx-raid.c      |  140 +
 arch/mips/cavium-octeon/executive/cvmx-raid.h      |  210 +
 .../cavium-octeon/executive/cvmx-resources.config  |  198 +
 arch/mips/cavium-octeon/executive/cvmx-rng.h       |  170 +
 arch/mips/cavium-octeon/executive/cvmx-rtc.h       |  168 +
 arch/mips/cavium-octeon/executive/cvmx-rwlock.h    |  177 +
 arch/mips/cavium-octeon/executive/cvmx-scratch.h   |  169 +
 .../executive/cvmx-shared-linux-n32.ld             |  287 ++
 .../executive/cvmx-shared-linux-o32.ld             |  285 ++
 .../cavium-octeon/executive/cvmx-shared-linux.ld   |  286 ++
 arch/mips/cavium-octeon/executive/cvmx-spi.c       |  580 +++
 arch/mips/cavium-octeon/executive/cvmx-spi.h       |  273 ++
 arch/mips/cavium-octeon/executive/cvmx-spi4000.c   |  603 +++
 arch/mips/cavium-octeon/executive/cvmx-spinlock.h  |  438 +++
 arch/mips/cavium-octeon/executive/cvmx-swap.h      |  149 +
 arch/mips/cavium-octeon/executive/cvmx-sysinfo.c   |  215 +
 arch/mips/cavium-octeon/executive/cvmx-sysinfo.h   |  174 +
 arch/mips/cavium-octeon/executive/cvmx-thunder.c   |  336 ++
 arch/mips/cavium-octeon/executive/cvmx-thunder.h   |  156 +
 arch/mips/cavium-octeon/executive/cvmx-tim.c       |  265 ++
 arch/mips/cavium-octeon/executive/cvmx-tim.h       |  339 ++
 arch/mips/cavium-octeon/executive/cvmx-tra.c       |  330 ++
 arch/mips/cavium-octeon/executive/cvmx-tra.h       |  419 ++
 arch/mips/cavium-octeon/executive/cvmx-twsi.c      |  157 +
 arch/mips/cavium-octeon/executive/cvmx-twsi.h      |  163 +
 arch/mips/cavium-octeon/executive/cvmx-uart.h      |   81 +
 arch/mips/cavium-octeon/executive/cvmx-usb.c       | 3481 +++++++++++++++++
 arch/mips/cavium-octeon/executive/cvmx-usb.h       | 1133 ++++++
 arch/mips/cavium-octeon/executive/cvmx-version.h   |   12 +
 arch/mips/cavium-octeon/executive/cvmx-warn.c      |   87 +
 arch/mips/cavium-octeon/executive/cvmx-warn.h      |   72 +
 arch/mips/cavium-octeon/executive/cvmx-wqe.h       |  321 ++
 arch/mips/cavium-octeon/executive/cvmx-zip.c       |  135 +
 arch/mips/cavium-octeon/executive/cvmx-zip.h       |  255 ++
 arch/mips/cavium-octeon/executive/cvmx-zone.c      |  177 +
 arch/mips/cavium-octeon/executive/cvmx.h           | 1134 ++++++
 arch/mips/cavium-octeon/executive/cvmx.mk          |  151 +
 .../executive/executive-config.h.template          |  188 +
 arch/mips/cavium-octeon/executive/octeon-feature.h |  137 +
 arch/mips/cavium-octeon/executive/octeon-model.c   |  395 ++
 arch/mips/cavium-octeon/executive/octeon-model.h   |  254 ++
 .../cavium-octeon/executive/octeon-pci-console.c   |  488 +++
 .../cavium-octeon/executive/octeon-pci-console.h   |  146 +
 140 files changed, 53085 insertions(+), 0 deletions(-)
 create mode 100644 arch/mips/cavium-octeon/executive/README.txt
 create mode 100644 arch/mips/cavium-octeon/executive/cvmip.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-abi.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-app-init-linux.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-app-init.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-app-init.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-asm.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-asx.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-atomic.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-bootmem.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-bootmem.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-ciu.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-cmd-queue.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-cmd-queue.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-cn3010-evb-hs5.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-cn3010-evb-hs5.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-compactflash.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-compactflash.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-core.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-core.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-coremask.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-coremask.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-cvmmem.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-dfa.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-dfa.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-dma-engine.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-dma-engine.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-ebt3000.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-ebt3000.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-fau.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-flash.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-flash.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-fpa.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-fpa.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-gmx.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-gpio.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-helper-board.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-helper-board.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-helper-check-defines.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-helper-errata.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-helper-errata.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-helper-fpa.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-helper-fpa.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-helper-loop.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-helper-loop.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-helper-npi.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-helper-npi.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-helper-rgmii.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-helper-rgmii.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-helper-sgmii.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-helper-sgmii.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-helper-spi.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-helper-spi.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-helper-util.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-helper-util.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-helper-xaui.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-helper-xaui.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-helper.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-helper.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-higig.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-interrupt-decodes.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-interrupt-handler.S
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-interrupt-rsl.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-interrupt.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-interrupt.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-iob.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-ipd.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-key.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-l2c.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-l2c.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-llm.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-llm.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-lmc.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-log-arc.S
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-log.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-log.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-malloc.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-malloc/README-malloc
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-malloc/arena.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-malloc/malloc.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-malloc/malloc.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-malloc/thread-m.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-mdio.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-mgmt-port.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-mgmt-port.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-mio.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-npi.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-packet.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-pci.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-pcie.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-pcie.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-pip.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-pko.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-pko.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-platform.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-pow.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-pow.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-raid.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-raid.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-resources.config
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-rng.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-rtc.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-rwlock.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-scratch.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-shared-linux-n32.ld
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-shared-linux-o32.ld
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-shared-linux.ld
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-spi.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-spi.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-spi4000.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-spinlock.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-swap.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-sysinfo.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-sysinfo.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-thunder.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-thunder.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-tim.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-tim.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-tra.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-tra.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-twsi.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-twsi.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-uart.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-usb.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-usb.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-version.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-warn.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-warn.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-wqe.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-zip.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-zip.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-zone.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx.h
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx.mk
 create mode 100644 arch/mips/cavium-octeon/executive/executive-config.h.template
 create mode 100644 arch/mips/cavium-octeon/executive/octeon-feature.h
 create mode 100644 arch/mips/cavium-octeon/executive/octeon-model.c
 create mode 100644 arch/mips/cavium-octeon/executive/octeon-model.h
 create mode 100644 arch/mips/cavium-octeon/executive/octeon-pci-console.c
 create mode 100644 arch/mips/cavium-octeon/executive/octeon-pci-console.h

diff --git a/arch/mips/cavium-octeon/executive/README.txt b/arch/mips/cavium-octeon/executive/README.txt
new file mode 100644
index 0000000..553c46d
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/README.txt
@@ -0,0 +1,43 @@
+Readme for the Octeon Executive Library
+
+
+The Octeon Executive Library provides runtime support and hardware 
+abstraction for the Octeon processor.  The executive is composed of the 
+libcvmx.a library as well as header files that provide  
+functionality with inline functions.
+
+
+Usage:
+
+The libcvmx.a library is built for every application as part of the
+application build. (Please refer to the 'related pages' section of the 
+HTML documentation for more information on the build system.)  
+Applications using the executive should include the header files from
+$OCTEON_ROOT/target/include and link against the library that is built in 
+the local obj directory. Each file using the executive 
+should include the following two header files in order:
+
+#include "cvmx-config.h"
+#include "cvmx.h"
+
+The cvmx-config.h file contains configuration information for the 
+executive and is generated by the cvmx-config script from an 
+'executive-config.h' file. A sample version of this file is provided 
+in the executive directory as 'executive-config.h.template'.  
+
+Copy this file to 'executive-config.h' into the 'config' subdirectory 
+of the application directory and customize as required by the application. 
+Applications that don't use any simple executive functionality can omit 
+the cvmx-config.h header file. Please refer to the examples for a 
+demonstration of where to put the executive-config.h file and for an
+example of generated cvmx-config.h.
+
+For file specific information please see the documentation within the 
+source files or the HTML documentation provided in docs/html/index.html.
+The HTML documentation is automatically generated by Doxygen from the 
+source files.
+
+
+
+==========================================================================
+Please see the release notes for version specific information.
diff --git a/arch/mips/cavium-octeon/executive/cvmip.h b/arch/mips/cavium-octeon/executive/cvmip.h
new file mode 100644
index 0000000..432e534
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmip.h
@@ -0,0 +1,215 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Cavium Networks Internet Protocol (IP)
+ *
+ * Definitions for the Internet Protocol (IP) support.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ *
+ */
+
+#ifndef __CVMIP_H__
+#define __CVMIP_H__
+
+
+/*
+ * IP protocol values (1 byte)
+ *
+ */
+#define  CVMIP_PROTO_ICMP  1    /* Internet Control Message Protocol */
+#define  CVMIP_PROTO_TCP   6    /* Transmission Control Protocol */
+#define  CVMIP_PROTO_UDP  17    /* User Datagram Protocol */
+#define  CVMIP_PROTO_ESP  50    /* Encapsulated Security Payload */
+#define  CVMIP_PROTO_AH   51    /* Authentication Header */
+
+
+/**
+ * network packet header definitions
+ * (originally from octane_hw.h)
+ *
+ */
+
+/**
+ * UDP Packet header
+ */
+typedef struct {
+   union {
+      int32_t           s32     ;
+      uint32_t          u32     ;
+      struct {
+         uint16_t        src_prt ;
+         uint16_t        dst_prt ;
+      } s;
+   } prts;
+   uint16_t            len     ;
+   uint16_t            chksum  ;
+} cvmip_udp_hdr_t;
+
+/**
+ * TCP Packet header
+ */
+typedef struct {
+   uint16_t            src_prt ;
+   uint16_t            dst_prt ;
+   uint32_t            seq     ;
+   uint32_t            ack_seq ;
+   uint32_t            hlen    :4;
+   uint32_t            rsvd    :6;
+   uint32_t            urg     :1;
+   uint32_t            ack     :1;
+   uint32_t            psh     :1;
+   uint32_t            rst     :1;
+   uint32_t            syn     :1;
+   uint32_t            fin     :1;
+   uint16_t            win_sz  ;
+   uint16_t            chksum  ;
+   uint16_t            urg_ptr ;
+   uint32_t            junk    ;
+} cvmip_tcp_hdr_t;
+
+/**
+ * L4 Packet header
+ */
+typedef union {
+   cvmip_udp_hdr_t udphdr;
+   cvmip_tcp_hdr_t tcphdr;
+   struct {
+      union {
+         int32_t           s32    ;
+         uint32_t          u32    ;
+         struct {
+            uint16_t        src_prt;
+            uint16_t        dst_prt;
+         } s;
+      } prts;
+      uint16_t            len     ;
+      uint16_t            chksum  ;
+      char              dat[48] ; // 48 for IPv6 with no extension hdrs, 64 for IPv4 without options
+   } udp;
+   struct {
+      uint16_t            src_prt ;
+      uint16_t            dst_prt ;
+      uint32_t            seq     ;
+      uint32_t            ack_seq ;
+      uint32_t            hlen    :4;
+      uint32_t            rsvd    :6;
+      uint32_t            urg     :1;
+      uint32_t            ack     :1;
+      uint32_t            psh     :1;
+      uint32_t            rst     :1;
+      uint32_t            syn     :1;
+      uint32_t            fin     :1;
+      uint16_t            win_sz  ;
+      uint16_t            chksum  ;
+      uint16_t            urg_ptr ;
+      char              dat[36] ; // 36 for IPv6 with no extension hdrs, 52 for IPv6 without options
+   } tcp;
+} cvmip_l4_info_t;
+
+/**
+ * Special struct to add a pad to IPv4 header
+ */
+typedef struct {
+   uint32_t            pad;
+
+   uint32_t            version : 4;
+   uint32_t            hl      : 4;
+   uint8_t             tos     ;
+   uint16_t            len     ;
+
+   uint16_t            id      ;
+   uint32_t            mbz     : 1;
+   uint32_t            df      : 1;
+   uint32_t            mf      : 1;
+   uint32_t            off     :13;
+
+   uint8_t             ttl     ;
+   uint8_t             protocol;
+   uint16_t            chksum  ;
+
+   union {
+      uint64_t          u64;
+      struct {
+         uint32_t        src;
+         uint32_t        dst;
+      } s;
+   } src_dst;
+} cvmip_ipv4_hdr_t;
+
+/**
+ * IPv6 Packet header
+ */
+typedef struct {
+
+   uint32_t            version : 4;
+   uint32_t            v6class : 8;
+   uint32_t            flow    :20;
+
+   uint16_t            len     ;    // includes extension headers plus payload (add 40 to be equiv to v4 len field)
+   uint8_t             next_hdr;    // equivalent to the v4 protocol field
+   uint8_t             hop_lim ;    // equivalent to the v4 TTL field
+
+   union {
+      uint64_t          u64[4];
+      struct {
+         uint64_t        src[2];
+         uint64_t        dst[2];
+      } s;
+   } src_dst;
+
+} cvmip_ipv6_hdr_t;
+
+
+#endif /* __CVMIP_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-abi.h b/arch/mips/cavium-octeon/executive/cvmx-abi.h
new file mode 100644
index 0000000..d833570
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-abi.h
@@ -0,0 +1,99 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * This file defines macros for use in determining the current calling ABI.
+ *
+ * <hr>$Revision: 32636 $<hr>
+*/
+
+#ifndef __CVMX_ABI_H__
+#define __CVMX_ABI_H__
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+/* Check for N32 ABI, defined for 32-bit Simple Exec applications
+   and Linux N32 ABI.*/
+#if (defined _ABIN32 && _MIPS_SIM == _ABIN32)
+#define CVMX_ABI_N32
+/* Check for N64 ABI, defined for 64-bit Linux toolchain. */
+#elif (defined _ABI64 && _MIPS_SIM == _ABI64)
+#define CVMX_ABI_N64
+/* Check for O32 ABI, defined for Linux 032 ABI, not supported yet. */
+#elif (defined _ABIO32 && _MIPS_SIM == _ABIO32)
+#define CVMX_ABI_O32
+/* Check for EABI ABI, defined for 64-bit Simple Exec applications. */
+#else
+#define CVMX_ABI_EABI
+#endif
+
+#ifndef __BYTE_ORDER
+    #if defined(__BIG_ENDIAN) && !defined(__LITTLE_ENDIAN)
+        #define __BYTE_ORDER __BIG_ENDIAN
+    #elif !defined(__BIG_ENDIAN) && defined(__LITTLE_ENDIAN)
+        #define __BYTE_ORDER __LITTLE_ENDIAN
+    #elif !defined(__BIG_ENDIAN) && !defined(__LITTLE_ENDIAN)
+        #define __BIG_ENDIAN 4321
+        #define __BYTE_ORDER __BIG_ENDIAN
+    #else
+        #error Unable to determine Endian mode
+    #endif
+#endif
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif /* __CVMX_ABI_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-app-init-linux.c b/arch/mips/cavium-octeon/executive/cvmx-app-init-linux.c
new file mode 100644
index 0000000..4c82db0
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-app-init-linux.c
@@ -0,0 +1,437 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+/**
+ * @file
+ * Simple executive application initialization for Linux user space. This
+ * file should be used instead of cvmx-app-init.c for running simple executive
+ * applications under Linux in userspace. The following are some of the key
+ * points to remember when writing applications to run both under the
+ * standalone simple executive and userspace under Linux.
+ *
+ * -# Application main must be called "appmain" under Linux. Use and ifdef
+ *      based on __linux__ to determine the proper name.
+ * -# Be careful to use cvmx_ptr_to_phys() and cvmx_phys_to_ptr. The simple
+ *      executive 1-1 TLB mappings allow you to be sloppy and interchange
+ *      hardware addresses with virtual address. This isn't true under Linux.
+ * -# If you're talking directly to hardware, be careful. The normal Linux
+ *      protections are circumvented. If you do something bad, Linux won't
+ *      save you.
+ * -# Most hardware can only be initialized once. Unless you're very careful,
+ *      this also means you Linux application can only run once.
+ *
+ * <hr>$Revision: 34488 $<hr>
+ *
+ */
+#define _GNU_SOURCE
+#include <stdint.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <stdarg.h>
+#include <string.h>
+#include <unistd.h>
+#include <errno.h>
+#include <fcntl.h>
+#include <sys/mman.h>
+#include <signal.h>
+#include <sys/statfs.h>
+#include <sys/wait.h>
+#include <sched.h>
+#include <octeon-app-init.h>
+
+#include "cvmx-config.h"
+#include "cvmx.h"
+#include "cvmx-atomic.h"
+#include "cvmx-sysinfo.h"
+#include "cvmx-coremask.h"
+#include "cvmx-spinlock.h"
+#include "cvmx-bootmem.h"
+
+int octeon_model_version_check(uint32_t chip_id);
+
+#define OCTEON_ECLOCK_MULT_INPUT_X16    ((int)(33.4*16))
+
+/* Applications using the simple executive libraries under Linux userspace must
+    rename their "main" function to match the prototype below. This allows the
+    simple executive to perform needed memory initialization and process
+    creation before the application runs. */
+extern int appmain(int argc, const char *argv[]);
+
+/* These two external addresses provide the beginning and end markers for the
+    CVMX_SHARED section. These are defined by the cvmx-shared.ld linker script.
+    If they aren't defined, you probably forgot to link using this script. */
+extern void __cvmx_shared_start;
+extern void __cvmx_shared_end;
+extern uint64_t linux_mem32_min;
+extern uint64_t linux_mem32_max;
+extern uint64_t linux_mem32_wired;
+extern uint64_t linux_mem32_offset;
+
+/**
+ * This function performs some default initialization of the Octeon executive.  It initializes
+ * the cvmx_bootmem memory allocator with the list of physical memory shared by the bootloader.
+ * This function should be called on all cores that will use the bootmem allocator.
+ * Applications which require a different configuration can replace this function with a suitable application
+ * specific one.
+ *
+ * @return 0 on success
+ *         -1 on failure
+ */
+int cvmx_user_app_init(void)
+{
+    return 0;
+}
+
+
+/**
+ * Simulator magic is not supported in user mode under Linux.
+ * This version of simprintf simply calls the underlying C
+ * library printf for output. It also makes sure that two
+ * calls to simprintf provide atomic output.
+ *
+ * @param fmt    Format string in the same format as printf.
+ */
+void simprintf(const char *fmt, ...)
+{
+    CVMX_SHARED static cvmx_spinlock_t simprintf_lock = CVMX_SPINLOCK_UNLOCKED_INITIALIZER;
+    va_list ap;
+
+    cvmx_spinlock_lock(&simprintf_lock);
+    printf("SIMPRINTF(%d): ", (int)cvmx_get_core_num());
+    va_start(ap, fmt);
+    vprintf(fmt, ap);
+    va_end(ap);
+    cvmx_spinlock_unlock(&simprintf_lock);
+}
+
+
+/**
+ * Setup the CVMX_SHARED data section to be shared across
+ * all processors running this application. A memory mapped
+ * region is allocated using shm_open and mmap. The current
+ * contents of the CVMX_SHARED section are copied into the
+ * region. Then the new region is remapped to replace the
+ * existing CVMX_SHARED data.
+ *
+ * This function will display a message and abort the
+ * application under any error conditions. The Linux tmpfs
+ * filesystem must be mounted under /dev/shm.
+ */
+static void setup_cvmx_shared(void)
+{
+    const char *SHM_NAME = "cvmx_shared";
+    unsigned long shared_size = &__cvmx_shared_end - &__cvmx_shared_start;
+    int fd;
+
+    /* If there isn't and shared data we can skip all this */
+    if (shared_size)
+    {
+        char shm_name[30];
+        printf("CVMX_SHARED: %p-%p\n", &__cvmx_shared_start, &__cvmx_shared_end);
+
+#ifdef __UCLIBC__
+	const char *defaultdir = "/dev/shm/";
+	struct statfs f;
+	int pid;
+	/* The canonical place is /dev/shm. */
+	if (statfs (defaultdir, &f) == 0)
+	{
+	    pid = getpid();
+	    sprintf (shm_name, "%s%s-%d", defaultdir, SHM_NAME, pid);
+	}
+	else
+	{
+	    perror("/dev/shm is not mounted");
+	    exit(-1);
+	}
+
+	/* shm_open(), shm_unlink() are not implemented in uClibc. Do the
+	   same thing using open() and close() system calls.  */
+	fd = open (shm_name, O_RDWR | O_CREAT | O_TRUNC, 0);
+
+	if (fd < 0)
+	{
+	    perror("Failed to open CVMX_SHARED(shm_name)");
+	    exit(errno);
+	}
+
+	unlink (shm_name);
+#else
+	sprintf(shm_name, "%s-%d", SHM_NAME, getpid());
+        /* Open a new shared memory region for use as CVMX_SHARED */
+        fd = shm_open(shm_name, O_RDWR | O_CREAT | O_TRUNC, 0);
+        if (fd <0)
+        {
+            perror("Failed to setup CVMX_SHARED(shm_open)");
+            exit(errno);
+        }
+
+        /* We don't want the file on the filesystem. Immediately unlink it so
+            another application can create its own shared region */
+        shm_unlink(SHM_NAME);
+#endif
+
+        /* Resize the region to match the size of CVMX_SHARED */
+        ftruncate(fd, shared_size);
+
+        /* Map the region into some random location temporarily so we can
+            copy the shared data to it */
+        void *ptr = mmap(NULL, shared_size, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);
+        if (ptr == NULL)
+        {
+            perror("Failed to setup CVMX_SHARED(mmap copy)");
+            exit(errno);
+        }
+
+        /* Copy CVMX_SHARED to the new shared region so we don't lose
+            initializers */
+        memcpy(ptr, &__cvmx_shared_start, shared_size);
+        munmap(ptr, shared_size);
+
+        /* Remap the shared region to replace the old CVMX_SHARED region */
+        ptr = mmap(&__cvmx_shared_start, shared_size, PROT_READ | PROT_WRITE, MAP_SHARED | MAP_FIXED, fd, 0);
+        if (ptr == NULL)
+        {
+            perror("Failed to setup CVMX_SHARED(mmap final)");
+            exit(errno);
+        }
+
+        /* Once mappings are setup, the file handle isn't needed anymore */
+        close(fd);
+    }
+}
+
+
+/**
+ * Shutdown and free the shared CVMX_SHARED region setup by
+ * setup_cvmx_shared.
+ */
+static void shutdown_cvmx_shared(void)
+{
+    unsigned long shared_size = &__cvmx_shared_end - &__cvmx_shared_start;
+    if (shared_size)
+        munmap(&__cvmx_shared_start, shared_size);
+}
+
+
+/**
+ * Setup access to the CONFIG_CAVIUM_RESERVE32 memory section
+ * created by the kernel. This memory is used for shared
+ * hardware buffers with 32 bit userspace applications.
+ */
+static void setup_reserve32(void)
+{
+    if (linux_mem32_min && linux_mem32_max)
+    {
+        int region_size = linux_mem32_max - linux_mem32_min + 1;
+        int mmap_flags = MAP_SHARED;
+        void *linux_mem32_base_ptr = NULL;
+
+        /* Although not strictly necessary, we are going to mmap() the wired
+            TLB region so it is in the process page tables. These pages will
+            never fault in, but they will allow GDB to access the wired
+            region. We need the mappings to exactly match the wired TLB
+            entry. */
+        if (linux_mem32_wired)
+        {
+            mmap_flags |= MAP_FIXED;
+            linux_mem32_base_ptr = CASTPTR(void, (1ull<<31) - region_size);
+        }
+
+        int fd = open("/dev/mem", O_RDWR);
+        if (fd < 0)
+        {
+            perror("ERROR opening /dev/mem");
+            exit(-1);
+        }
+
+        linux_mem32_base_ptr = mmap64(linux_mem32_base_ptr,
+                                          region_size,
+                                          PROT_READ | PROT_WRITE,
+                                          mmap_flags,
+                                          fd,
+                                          linux_mem32_min);
+        close(fd);
+
+        if (MAP_FAILED == linux_mem32_base_ptr)
+        {
+            perror("Error mapping reserve32");
+            exit(-1);
+        }
+
+        linux_mem32_offset = CAST64(linux_mem32_base_ptr) - linux_mem32_min;
+    }
+}
+
+
+/**
+ * Main entrypoint of the application. Here we setup shared
+ * memory and fork processes for each cpu. This simulates the
+ * normal simple executive environment of one process per
+ * cpu core.
+ *
+ * @param argc   Number of command line arguments
+ * @param argv   The command line arguments
+ * @return Return value for the process
+ */
+int main(int argc, const char *argv[])
+{
+    CVMX_SHARED static cvmx_spinlock_t mask_lock = CVMX_SPINLOCK_UNLOCKED_INITIALIZER;
+    CVMX_SHARED static int32_t pending_fork;
+    unsigned long cpumask;
+    unsigned long cpu;
+    int lastcpu = 0;
+
+    cvmx_sysinfo_linux_userspace_initialize();
+
+    if (sizeof(void*) == 4)
+    {
+        if (linux_mem32_min)
+            setup_reserve32();
+        else
+        {
+            printf("\nFailed to access 32bit shared memory region. Most likely the Kernel\n"
+                   "has not been configured for 32bit shared memory access. Check the\n"
+                   "kernel configuration.\n"
+                   "Aborting...\n\n");
+            exit(-1);
+        }
+    }
+
+    setup_cvmx_shared();
+    cvmx_bootmem_init(cvmx_sysinfo_get()->phy_mem_desc_ptr);
+
+    /* Check to make sure the Chip version matches the configured version */
+    octeon_model_version_check(cvmx_get_proc_id());
+
+    /* Get the list of logical cpus we should run on */
+    if (sched_getaffinity(0, sizeof(cpumask), (cpu_set_t*)&cpumask))
+    {
+        perror("sched_getaffinity failed");
+        exit(errno);
+    }
+
+    cvmx_sysinfo_t *system_info = cvmx_sysinfo_get();
+
+    cvmx_atomic_set32(&pending_fork, 1);
+    for (cpu=0; cpu<16; cpu++)
+    {
+        if (cpumask & (1<<cpu))
+        {
+            /* Turn off the bit for this CPU number. We've counted him */
+            cpumask ^= (1<<cpu);
+            /* If this is the last CPU to run on, use this process instead of forking another one */
+            if (cpumask == 0)
+            {
+                lastcpu = 1;
+                break;
+            }
+            /* Increment the number of CPUs running this app */
+            cvmx_atomic_add32(&pending_fork, 1);
+            /* Flush all IO streams before the fork. Otherwise any buffered
+                data in the C library will be duplicated. This results in
+                duplicate output from a single print */
+            fflush(NULL);
+            /* Fork a process for the new CPU */
+            int pid = fork();
+            if (pid == 0)
+            {
+                break;
+            }
+            else if (pid == -1)
+            {
+                perror("Fork failed");
+                exit(errno);
+            }
+        }
+    }
+
+    /* Set affinity to lock me to the correct CPU */
+    cpumask = (1<<cpu);
+    if (sched_setaffinity(0, sizeof(cpumask), (cpu_set_t*)&cpumask))
+    {
+        perror("sched_setaffinity failed");
+        exit(errno);
+    }
+
+    cvmx_spinlock_lock(&mask_lock);
+    system_info->core_mask |= 1<<cvmx_get_core_num();
+    cvmx_atomic_add32(&pending_fork, -1);
+    if (cvmx_atomic_get32(&pending_fork) == 0)
+        cvmx_dprintf("Active coremask = 0x%x\n", system_info->core_mask);
+    if (lastcpu)
+        system_info->init_core = cvmx_get_core_num();
+    cvmx_spinlock_unlock(&mask_lock);
+
+    /* Spinning waiting for forks to complete */
+    while (cvmx_atomic_get32(&pending_fork)) {}
+
+    cvmx_coremask_barrier_sync(system_info->core_mask);
+
+    int result = appmain(argc, argv);
+
+    /* Wait for all forks to complete. This needs to be the core that started
+        all of the forks. It may not be the lowest numbered core! */
+    if (cvmx_get_core_num() == system_info->init_core)
+    {
+        int num_waits;
+        CVMX_POP(num_waits, system_info->core_mask);
+        num_waits--;
+        while (num_waits--)
+        {
+            if (wait(NULL) == -1)
+                perror("CVMX: Wait for forked child failed\n");
+        }
+    }
+
+    shutdown_cvmx_shared();
+
+    return result;
+}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-app-init.c b/arch/mips/cavium-octeon/executive/cvmx-app-init.c
new file mode 100644
index 0000000..a41cfca
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-app-init.c
@@ -0,0 +1,670 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+#include <stdio.h>
+#include <stdint.h>
+#include <stdlib.h>
+#include <string.h>
+#include "cvmx-config.h"
+#include "cvmx.h"
+#include "cvmx-spinlock.h"
+#include <octeon-app-init.h>
+#include "cvmx-sysinfo.h"
+#include "cvmx-bootmem.h"
+#include "cvmx-uart.h"
+#include "cvmx-ciu.h"
+#include "cvmx-coremask.h"
+#include "cvmx-core.h"
+#include "cvmx-interrupt.h"
+#include "cvmx-ebt3000.h"
+#include "../../bootloader/u-boot/include/octeon_mem_map.h"
+/**
+ * @file
+ *
+ * Main entry point for all simple executive based programs.
+ */
+
+
+extern void cvmx_interrupt_initialize(void);
+
+
+
+/**
+ * Main entry point for all simple executive based programs.
+ * This is the first C function called. It completes
+ * initialization, calls main, and performs C level cleanup.
+ *
+ * @param app_desc_addr
+ *               Address of the application description structure passed
+ *               brom the boot loader.
+ */
+EXTERN_ASM void __cvmx_app_init(uint64_t app_desc_addr);
+
+
+/**
+ * Set up sysinfo structure from boot descriptor versions 3-5
+ *
+ * @param app_desc_ptr
+ *               pointer to boot descriptor block
+ *
+ * @param sys_info_ptr
+ *               pointer to sysinfo structure to fill in
+ */
+static void process_boot_desc_ver_5(octeon_boot_descriptor_t *app_desc_ptr, cvmx_sysinfo_t *sys_info_ptr)
+{
+
+    /* copy application information for simple exec use */
+    /* Populate the sys_info structure from the boot descriptor block created by the bootloader.
+    ** The boot descriptor block is put in the top of the heap, so it will be overwritten when the
+    ** heap is fully used.  Information that is to be used must be copied before that.
+    ** Applications should only use the sys_info structure, not the boot descriptor
+    */
+    sys_info_ptr->core_mask = app_desc_ptr->core_mask;
+    sys_info_ptr->heap_base = app_desc_ptr->heap_base;
+    sys_info_ptr->heap_size = app_desc_ptr->heap_size;
+    sys_info_ptr->stack_top = app_desc_ptr->stack_top;
+    sys_info_ptr->stack_size = app_desc_ptr->stack_size;
+    sys_info_ptr->init_core = cvmx_get_core_num();
+    sys_info_ptr->phy_mem_desc_ptr = CASTPTR(void, CVMX_ADD_SEG32(CVMX_MIPS32_SPACE_KSEG0, app_desc_ptr->phy_mem_desc_addr));
+    sys_info_ptr->exception_base_addr = app_desc_ptr->exception_base_addr;
+    if (app_desc_ptr->dram_size > 16*1024*1024)
+        sys_info_ptr->system_dram_size = (uint64_t)app_desc_ptr->dram_size;  /* older bootloaders incorrectly gave this in bytes, so don't convert */
+    else
+        sys_info_ptr->system_dram_size = (uint64_t)app_desc_ptr->dram_size * 1024 * 1024;  /* convert from Megabytes to bytes */
+
+
+    if (app_desc_ptr->desc_version > 3)
+    {
+        sys_info_ptr->cpu_clock_hz  = app_desc_ptr->eclock_hz;
+        sys_info_ptr->dram_data_rate_hz  = app_desc_ptr->dclock_hz * 2;
+    }
+    else
+    {
+        printf("Warning: CPU speed not passed from bootloader, defaulting to 333 Mhz.\n");
+        printf("Warning: If this default is incorrect, LLM and other functionality may be broken.\n");
+        sys_info_ptr->cpu_clock_hz  = 333*1000000;
+    }
+
+    if (app_desc_ptr->desc_version > 4)
+    {
+        sys_info_ptr->board_type = app_desc_ptr->board_type;
+        sys_info_ptr->board_rev_major = app_desc_ptr->board_rev_major;
+        sys_info_ptr->board_rev_minor = app_desc_ptr->board_rev_minor;
+        memcpy(sys_info_ptr->mac_addr_base, app_desc_ptr->mac_addr_base, 6);
+        sys_info_ptr->mac_addr_count = app_desc_ptr->mac_addr_count;
+        memcpy(sys_info_ptr->board_serial_number, app_desc_ptr->board_serial_number, OCTEON_SERIAL_LEN);
+    }
+
+
+    sys_info_ptr->bootloader_config_flags = 0;
+    if (app_desc_ptr->flags & OCTEON_BL_FLAG_DEBUG)
+        sys_info_ptr->bootloader_config_flags |= CVMX_BOOTINFO_CFG_FLAG_DEBUG;
+#if OCTEON_APP_INIT_H_VERSION >= 3
+    if (app_desc_ptr->flags & OCTEON_BL_FLAG_BREAK)
+        sys_info_ptr->bootloader_config_flags |= CVMX_BOOTINFO_CFG_FLAG_BREAK;
+#endif
+    if (app_desc_ptr->flags & OCTEON_BL_FLAG_NO_MAGIC)
+        sys_info_ptr->bootloader_config_flags |= CVMX_BOOTINFO_CFG_FLAG_NO_MAGIC;
+}
+
+/**
+ * Set up sysinfo structure from boot descriptor versions 6 and higher.
+ * In these versions, the interesting data in not in the boot info structure
+ * defined by the toolchain, but is in the cvmx_bootinfo structure defined in
+ * the simple exec.
+ *
+ * @param app_desc_ptr
+ *               pointer to boot descriptor block
+ *
+ * @param sys_info_ptr
+ *               pointer to sysinfo structure to fill in
+ */
+static void process_boot_desc_ver_6(octeon_boot_descriptor_t *app_desc_ptr, cvmx_sysinfo_t *sys_info_ptr)
+{
+    cvmx_bootinfo_t *cvmx_bootinfo_ptr = CASTPTR(cvmx_bootinfo_t, app_desc_ptr->cvmx_desc_vaddr);
+
+    /* copy application information for simple exec use */
+    /* Populate the sys_info structure from the boot descriptor block created by the bootloader.
+    ** The boot descriptor block is put in the top of the heap, so it will be overwritten when the
+    ** heap is fully used.  Information that is to be used must be copied before that.
+    ** Applications should only use the sys_info structure, not the boot descriptor
+    */
+    if (cvmx_bootinfo_ptr->major_version == 1)
+    {
+        sys_info_ptr->core_mask = cvmx_bootinfo_ptr->core_mask;
+        sys_info_ptr->heap_base = cvmx_bootinfo_ptr->heap_base;
+        sys_info_ptr->heap_size = cvmx_bootinfo_ptr->heap_end - cvmx_bootinfo_ptr->heap_base;
+        sys_info_ptr->stack_top = cvmx_bootinfo_ptr->stack_top;
+        sys_info_ptr->stack_size = cvmx_bootinfo_ptr->stack_size;
+        sys_info_ptr->init_core = cvmx_get_core_num();
+        sys_info_ptr->phy_mem_desc_ptr = CASTPTR(void, CVMX_ADD_SEG32(CVMX_MIPS32_SPACE_KSEG0, app_desc_ptr->phy_mem_desc_addr));
+        sys_info_ptr->exception_base_addr = cvmx_bootinfo_ptr->exception_base_addr;
+        sys_info_ptr->cpu_clock_hz  = cvmx_bootinfo_ptr->eclock_hz;
+        sys_info_ptr->dram_data_rate_hz  = cvmx_bootinfo_ptr->dclock_hz * 2;
+
+        sys_info_ptr->board_type = cvmx_bootinfo_ptr->board_type;
+        sys_info_ptr->board_rev_major = cvmx_bootinfo_ptr->board_rev_major;
+        sys_info_ptr->board_rev_minor = cvmx_bootinfo_ptr->board_rev_minor;
+        memcpy(sys_info_ptr->mac_addr_base, cvmx_bootinfo_ptr->mac_addr_base, 6);
+        sys_info_ptr->mac_addr_count = cvmx_bootinfo_ptr->mac_addr_count;
+        memcpy(sys_info_ptr->board_serial_number, cvmx_bootinfo_ptr->board_serial_number, CVMX_BOOTINFO_OCTEON_SERIAL_LEN);
+        sys_info_ptr->console_uart_num = 0;
+        if (cvmx_bootinfo_ptr->flags & OCTEON_BL_FLAG_CONSOLE_UART1)
+            sys_info_ptr->console_uart_num = 1;
+
+        if (app_desc_ptr->dram_size > 16*1024*1024)
+            sys_info_ptr->system_dram_size = (uint64_t)app_desc_ptr->dram_size;  /* older bootloaders incorrectly gave this in bytes, so don't convert */
+        else
+            sys_info_ptr->system_dram_size = (uint64_t)app_desc_ptr->dram_size * 1024 * 1024;  /* convert from Megabytes to bytes */
+        if (cvmx_bootinfo_ptr->minor_version >= 1)
+        {
+            sys_info_ptr->compact_flash_common_base_addr = cvmx_bootinfo_ptr->compact_flash_common_base_addr;
+            sys_info_ptr->compact_flash_attribute_base_addr = cvmx_bootinfo_ptr->compact_flash_attribute_base_addr;
+            sys_info_ptr->led_display_base_addr = cvmx_bootinfo_ptr->led_display_base_addr;
+        }
+        else if (sys_info_ptr->board_type == CVMX_BOARD_TYPE_EBT3000 ||
+                 sys_info_ptr->board_type == CVMX_BOARD_TYPE_EBT5800)
+        {
+            /* Default these variables so that users of structure can be the same no
+            ** matter what version fo boot info block the bootloader passes */
+            sys_info_ptr->compact_flash_common_base_addr = 0x1d000000 + 0x800;
+            sys_info_ptr->compact_flash_attribute_base_addr = 0x1d010000;
+            if (sys_info_ptr->board_rev_major == 1)
+                sys_info_ptr->led_display_base_addr = 0x1d020000;
+            else
+                sys_info_ptr->led_display_base_addr = 0x1d020000 + 0xf8;
+        }
+        else
+        {
+            sys_info_ptr->compact_flash_common_base_addr = 0;
+            sys_info_ptr->compact_flash_attribute_base_addr = 0;
+            sys_info_ptr->led_display_base_addr = 0;
+        }
+
+        if (cvmx_bootinfo_ptr->minor_version >= 2)
+        {
+            sys_info_ptr->dfa_ref_clock_hz = cvmx_bootinfo_ptr->dfa_ref_clock_hz;
+            sys_info_ptr->bootloader_config_flags = cvmx_bootinfo_ptr->config_flags;
+        }
+        else
+        {
+            sys_info_ptr->dfa_ref_clock_hz = 0;
+            sys_info_ptr->bootloader_config_flags = 0;
+            if (app_desc_ptr->flags & OCTEON_BL_FLAG_DEBUG)
+                sys_info_ptr->bootloader_config_flags |= CVMX_BOOTINFO_CFG_FLAG_DEBUG;
+            if (app_desc_ptr->flags & OCTEON_BL_FLAG_NO_MAGIC)
+                sys_info_ptr->bootloader_config_flags |= CVMX_BOOTINFO_CFG_FLAG_NO_MAGIC;
+        }
+
+    }
+    else
+    {
+        printf("ERROR: Incompatible CVMX descriptor passed by bootloader: %d.%d\n",
+               (int)cvmx_bootinfo_ptr->major_version, (int)cvmx_bootinfo_ptr->minor_version);
+        while (1);
+    }
+}
+
+
+/**
+ * Interrupt handler for debugger Control-C interrupts.
+ *
+ * @param irq_number IRQ interrupt number
+ * @param registers  CPU registers at the time of the interrupt
+ * @param user_arg   Unused user argument
+ */
+static void process_debug_interrupt(int irq_number, uint64_t registers[32], void *user_arg)
+{
+    int uart = irq_number - CVMX_IRQ_UART0;
+    cvmx_uart_lsr_t lsrval;
+
+    /* Check for a Control-C interrupt from the debugger. This loop will eat
+        all input received on the uart */
+    lsrval.u64 = cvmx_read_csr(CVMX_MIO_UARTX_LSR(uart));
+    while (lsrval.s.dr)
+    {
+        int c = cvmx_read_csr(CVMX_MIO_UARTX_RBR(uart));
+        if (c == '\003')
+        {
+            register uint64_t tmp;
+            fflush(stderr);
+            fflush(stdout);
+            /* Pulse MCD0 signal on Ctrl-C to stop all the cores. Also
+                set the MCD0 to be not masked by this core so we know
+                the signal is received by someone */
+            asm volatile (
+                "dmfc0 %0, $22\n"
+                "ori   %0, %0, 0x1110\n"
+                "dmtc0 %0, $22\n"
+                : "=r" (tmp));
+        }
+        lsrval.u64 = cvmx_read_csr(CVMX_MIO_UARTX_LSR(uart));
+    }
+}
+
+/**
+ * Interrupt handler for calling exit on Control-C interrupts.
+ *
+ * @param irq_number IRQ interrupt number
+ * @param registers  CPU registers at the time of the interrupt
+ * @param user_arg   Unused user argument
+ */
+static void process_break_interrupt(int irq_number, uint64_t registers[32], void *user_arg)
+{
+    /* Exclude new functionality when building with older toolchains */
+#if OCTEON_APP_INIT_H_VERSION >= 3
+    int uart = irq_number - CVMX_IRQ_UART0;
+    cvmx_uart_lsr_t lsrval;
+
+    /* Check for a Control-C interrupt from the console. This loop will eat
+        all input received on the uart */
+    lsrval.u64 = cvmx_read_csr(CVMX_MIO_UARTX_LSR(uart));
+    while (lsrval.s.dr)
+    {
+        int c = cvmx_read_csr(CVMX_MIO_UARTX_RBR(uart));
+        if (c == '\003')
+        {
+            register uint64_t tmp;
+
+	    /* Wait for an another Control-C if right now we have no
+	       access to the console.  After this point we hold the
+	       lock and use a different lock to synchronize between
+	       the memfile dumps from different cores.  As a
+	       consequence regular printfs *don't* work after this
+	       point! */
+	    if (__octeon_uart_trylock () == 1)
+		return;
+
+            /* Pulse MCD0 signal on Ctrl-C to stop all the cores. Also
+	       set the MCD0 to be not masked by this core so we know
+	       the signal is received by someone */
+            asm volatile (
+                "dmfc0 %0, $22\n"
+                "ori   %0, %0, 0x1110\n"
+                "dmtc0 %0, $22\n"
+                : "=r" (tmp));
+        }
+        lsrval.u64 = cvmx_read_csr(CVMX_MIO_UARTX_LSR(uart));
+    }
+#endif
+}
+
+/* Add string signature to applications so that we can easily tell what
+** Octeon revision they were compiled for. Don't make static to avoid unused
+** variable warning. */
+#define xstr(s) str(s)
+#define str(s) #s
+
+int octeon_model_version_check(uint32_t chip_id);
+
+#define OMS xstr(OCTEON_MODEL)
+char octeon_rev_signature[] =
+#ifdef USE_RUNTIME_MODEL_CHECKS
+    "Compiled for runtime Octeon model checking";
+#else
+    "Compiled for Octeon processor id: "OMS;
+#endif
+
+void __cvmx_app_init(uint64_t app_desc_addr)
+{
+    /* App descriptor used by bootloader */
+    octeon_boot_descriptor_t *app_desc_ptr = CASTPTR(octeon_boot_descriptor_t, app_desc_addr);
+
+    /* app info structure used by the simple exec */
+    cvmx_sysinfo_t *sys_info_ptr = cvmx_sysinfo_get();
+
+    if (cvmx_coremask_first_core(app_desc_ptr->core_mask))
+    {
+        /* do once per application setup  */
+        if (app_desc_ptr->desc_version < 3)
+        {
+            printf("Obsolete bootloader, can't run application\n");
+            while (1)
+                ;
+        }
+        else if (app_desc_ptr->desc_version >=3 && app_desc_ptr->desc_version <= 5)
+        {
+            /* Handle versions 3-5 here */
+            process_boot_desc_ver_5(app_desc_ptr,sys_info_ptr);
+        }
+        else
+        {
+            /* Handle all newer versions here.... */
+            if (app_desc_ptr->desc_version > 6)
+            {
+                printf("Warning: newer boot descripter version than expected\n");
+            }
+            process_boot_desc_ver_6(app_desc_ptr,sys_info_ptr);
+
+        }
+    }
+    cvmx_coremask_barrier_sync(app_desc_ptr->core_mask);
+
+    /* All cores need to enable MCD0 signals if the debugger flag is set */
+    if (sys_info_ptr->bootloader_config_flags & CVMX_BOOTINFO_CFG_FLAG_DEBUG)
+    {
+        /* Set all cores to stop on MCD0 signals */
+        uint64_t tmp;
+        asm volatile(
+            "dmfc0 %0, $22, 0\n"
+            "or %0, %0, 0x1100\n"
+            "dmtc0 %0, $22, 0\n" : "=r" (tmp));
+    }
+
+    cvmx_interrupt_initialize();
+
+    if (cvmx_coremask_first_core(sys_info_ptr->core_mask))
+    {
+        /* Check to make sure the Chip version matches the configured version */
+        uint32_t chip_id = cvmx_get_proc_id();
+	int debugflag = sys_info_ptr->bootloader_config_flags & CVMX_BOOTINFO_CFG_FLAG_DEBUG;
+	int breakflag = sys_info_ptr->bootloader_config_flags & CVMX_BOOTINFO_CFG_FLAG_BREAK;
+	int uart;
+
+        /* Intialize the bootmem allocator with the descriptor that was provided by
+        ** the bootloader
+        ** IMPORTANT:  All printfs must happen after this since PCI console uses named
+        ** blocks.
+        */
+        cvmx_bootmem_init(sys_info_ptr->phy_mem_desc_ptr);
+
+        /* Make sure we can properly run on this chip */
+        octeon_model_version_check(chip_id);
+
+        /* If the debugger flag is set, setup the uart Control-C interrupt
+            handler */
+        if (debugflag)
+        {
+            /* Default to the second uart port */
+            uart = 1;
+
+            /* Search through the arguments for a debug=X */
+            unsigned int i;
+            for (i=0; i<app_desc_ptr->argc; i++)
+            {
+                const char *argv = CASTPTR(const char, CVMX_ADD_SEG32(CVMX_MIPS32_SPACE_KSEG0, app_desc_ptr->argv[i]));
+                if (strncmp(argv, "debug=", 6) == 0)
+                {
+                    /* Use the supplied uart as an override */
+                    uart = atoi(argv+6);
+                    break;
+                }
+            }
+	    cvmx_interrupt_register(CVMX_IRQ_UART0+uart, process_debug_interrupt, NULL);
+	}
+	else if (breakflag)
+	{
+            unsigned int i;
+	    int32_t *trampoline = CASTPTR(int32_t, CVMX_ADD_SEG32(CVMX_MIPS32_SPACE_KSEG0, BOOTLOADER_DEBUG_TRAMPOLINE));
+	    /* Default to the first uart port. */
+	    uart = 0;
+
+            /* Search through the arguments for a break=X */
+            for (i = 0; i < app_desc_ptr->argc; i++)
+            {
+                const char *argv = CASTPTR(const char, CVMX_ADD_SEG32(CVMX_MIPS32_SPACE_KSEG0, app_desc_ptr->argv[i]));
+                if (strncmp(argv, "break=", 6) == 0)
+                {
+                    /* Use the supplied uart as an override */
+                    uart = atoi(argv+6);
+                    break;
+                }
+            }
+
+	    /* We just want to call exit from all cores. */
+	    *trampoline = (int32_t) &exit;
+	    cvmx_interrupt_register(CVMX_IRQ_UART0 + uart, process_break_interrupt, NULL);
+	}
+	if (debugflag || breakflag)
+	{
+            /* Enable uart interrupts for debugger Control-C processing */
+            cvmx_uart_ier_t ier;
+            ier.u64 = cvmx_read_csr(CVMX_MIO_UARTX_IER(uart));
+            ier.s.erbfi = 1;
+            cvmx_write_csr(CVMX_MIO_UARTX_IER(uart), ier.u64);
+
+            cvmx_interrupt_unmask_irq(CVMX_IRQ_UART0+uart);
+        }
+    }
+
+    /* Clear BEV now that we have installed exception handlers. */
+    uint64_t tmp;
+    asm volatile (
+               "   .set push                  \n"
+               "   .set mips64                  \n"
+               "   .set noreorder               \n"
+               "   .set noat               \n"
+               "   mfc0 %[tmp], $12, 0          \n"
+               "   li   $at, 1 << 22            \n"
+               "   not  $at, $at                \n"
+               "   and  %[tmp], $at             \n"
+               "   mtc0 %[tmp], $12, 0          \n"
+               "   .set pop                  \n"
+                  : [tmp] "=&r" (tmp) : );
+
+    /* Set all cores to stop on MCD0 signals */
+    asm volatile(
+        "dmfc0 %0, $22, 0\n"
+        "or %0, %0, 0x1100\n"
+        "dmtc0 %0, $22, 0\n" : "=r" (tmp));
+
+    CVMX_SYNC;
+    /* Synchronise all cores at this point */
+    cvmx_coremask_barrier_sync(app_desc_ptr->core_mask);
+
+}
+
+int cvmx_user_app_init(void)
+{
+    uint64_t bist_val;
+    uint64_t mask;
+    int bist_errors = 0;
+    uint64_t tmp;
+    uint64_t base_addr;
+
+
+    /* Put message on LED display */
+    if (cvmx_sysinfo_get()->board_type != CVMX_BOARD_TYPE_SIM)
+      ebt3000_str_write("CVMX    ");
+
+    /* Check BIST results for COP0 registers, some values only meaningful in pass 2 */
+    CVMX_MF_CACHE_ERR(bist_val);
+    mask = (1ULL<<32) | (1ULL<<33) | (1ULL<<34) | (1ULL<<35) | (1ULL<<36);
+    bist_val &= mask;
+    if (bist_val)
+    {
+        printf("BIST FAILURE: COP0_CACHE_ERR: 0x%llx\n", (unsigned long long)bist_val);
+        bist_errors++;
+    }
+    /* Clear parity error bits */
+    CVMX_MF_CACHE_ERR(bist_val);
+    bist_val &= ~0x7ull;
+    CVMX_MT_CACHE_ERR(bist_val);
+
+
+    mask = 0xfc00000000000000ull;
+    CVMX_MF_CVM_MEM_CTL(bist_val);
+    bist_val &=  mask;
+    if (bist_val)
+    {
+        printf("BIST FAILURE: COP0_CVM_MEM_CTL: 0x%llx\n", (unsigned long long)bist_val);
+        bist_errors++;
+    }
+
+    /* Clear DCACHE parity error bit */
+    bist_val = 0;
+    CVMX_MF_DCACHE_ERR(bist_val);
+
+    mask =     0x18ull;
+    bist_val = cvmx_read_csr(CVMX_L2D_ERR);
+    if (bist_val & mask)
+    {
+        printf("ERROR: ECC error detected in L2 Data, L2D_ERR: 0x%llx\n", (unsigned long long)bist_val);
+        cvmx_write_csr(CVMX_L2D_ERR, bist_val); /* Clear error bits if set */
+    }
+    bist_val = cvmx_read_csr(CVMX_L2T_ERR);
+    if (bist_val & mask)
+    {
+        printf("ERROR: ECC error detected in L2 Tags, L2T_ERR: 0x%llx\n", (unsigned long long)bist_val);
+        cvmx_write_csr(CVMX_L2T_ERR, bist_val); /* Clear error bits if set */
+    }
+
+
+    /* Set up 4 cache lines of local memory, make available from Kernel space */
+    CVMX_MF_CVM_MEM_CTL(tmp);
+    tmp &= ~0x1ffull;
+    tmp |= 0x104ull;
+    CVMX_MT_CVM_MEM_CTL(tmp);
+
+
+#if CVMX_USE_1_TO_1_TLB_MAPPINGS
+
+    /* Check to see if the bootloader is indicating that the application is outside
+    ** of the 0x10000000 0x20000000 range, in which case we can't use 1-1 mappings */
+    if (cvmx_sysinfo_get()->bootloader_config_flags & CVMX_BOOTINFO_CFG_FLAG_OVERSIZE_TLB_MAPPING)
+    {
+        printf("ERROR: 1-1 TLB mappings configured and oversize application loaded.\n");
+        printf("ERROR: Either 1-1 TLB mappings must be disabled or application size reduced.\n");
+        while (1)
+            ;
+    }
+
+
+    /* Create 1-1 Mappings for all DRAM up to 8 gigs, excluding the low 1 Megabyte.  This area
+    ** is reserved for the bootloader and exception vectors.  By not mapping this area, NULL pointer
+    ** dereferences will be caught with TLB exceptions.  Exception handlers should be written
+    ** using XKPHYS or KSEG0 addresses. */
+#if CVMX_NULL_POINTER_PROTECT
+    /* Exclude low 1 MByte from mapping to detect NULL pointer accesses.
+    ** The only down side of this is it uses more TLB mappings */
+    cvmx_core_add_fixed_tlb_mapping_bits(0x0, 0x0, 0x100000  | TLB_DIRTY | TLB_VALID | TLB_GLOBAL, CVMX_TLB_PAGEMASK_1M);
+    cvmx_core_add_fixed_tlb_mapping(0x200000, 0x200000, 0x300000, CVMX_TLB_PAGEMASK_1M);
+    cvmx_core_add_fixed_tlb_mapping(0x400000, 0x400000, 0x500000, CVMX_TLB_PAGEMASK_1M);
+    cvmx_core_add_fixed_tlb_mapping(0x600000, 0x600000, 0x700000, CVMX_TLB_PAGEMASK_1M);
+
+    cvmx_core_add_fixed_tlb_mapping(0x800000,  0x800000,  0xC00000, CVMX_TLB_PAGEMASK_4M);
+    cvmx_core_add_fixed_tlb_mapping(0x1000000, 0x1000000, 0x1400000, CVMX_TLB_PAGEMASK_4M);
+    cvmx_core_add_fixed_tlb_mapping(0x1800000, 0x1800000, 0x1c00000, CVMX_TLB_PAGEMASK_4M);
+
+    cvmx_core_add_fixed_tlb_mapping(0x2000000, 0x2000000, 0x3000000, CVMX_TLB_PAGEMASK_16M);
+    cvmx_core_add_fixed_tlb_mapping(0x4000000, 0x4000000, 0x5000000, CVMX_TLB_PAGEMASK_16M);
+    cvmx_core_add_fixed_tlb_mapping(0x6000000, 0x6000000, 0x7000000, CVMX_TLB_PAGEMASK_16M);
+#else
+    /* Map entire low 128 Megs, including 0x0 */
+    cvmx_core_add_fixed_tlb_mapping(0x0, 0x0, 0x4000000ULL, CVMX_TLB_PAGEMASK_64M);
+#endif
+    cvmx_core_add_fixed_tlb_mapping(0x8000000ULL, 0x8000000ULL, 0xc000000ULL, CVMX_TLB_PAGEMASK_64M);
+
+    /* Create 1-1 mapping for next 256 megs
+    ** bottom page is not valid */
+    cvmx_core_add_fixed_tlb_mapping_bits(0x400000000ULL, 0, 0x410000000ULL  | TLB_DIRTY | TLB_VALID | TLB_GLOBAL, CVMX_TLB_PAGEMASK_256M);
+
+    /* Map from 0.5 up to the installed memory size in 512 MByte chunks.  If this loop runs out of memory,
+    ** the NULL pointer detection can be disabled to free up more TLB entries. */
+    if (cvmx_sysinfo_get()->system_dram_size > 0x20000000ULL)
+    {
+        for (base_addr = 0x20000000ULL; base_addr <= (cvmx_sysinfo_get()->system_dram_size - 0x20000000ULL); base_addr += 0x20000000ULL)
+        {
+            if (0 > cvmx_core_add_fixed_tlb_mapping(base_addr,  base_addr,  base_addr + 0x10000000ULL, CVMX_TLB_PAGEMASK_256M))
+            {
+                printf("ERROR adding 1-1 TLB mapping for address 0x%llx\n", (unsigned long long)base_addr);
+                while (1);    /* Hang here, as expected memory mappings aren't set up if this fails */
+            }
+        }
+    }
+
+
+#endif
+
+
+    cvmx_sysinfo_t *sys_info_ptr = cvmx_sysinfo_get();
+    cvmx_bootmem_init(sys_info_ptr->phy_mem_desc_ptr);
+
+    return(0);
+}
+
+void __cvmx_app_exit(void)
+{
+    if (cvmx_sysinfo_get()->board_type != CVMX_BOARD_TYPE_SIM)
+    {
+       uint64_t val;
+       uint64_t mask, expected;
+       int bist_errors = 0;
+
+       mask =     0x1ull;
+       expected = 0x0ull;
+       CVMX_MF_DCACHE_ERR(val);
+       val = (val & mask) ^ expected;
+       if (val)
+       {
+           printf("DCACHE Parity error: 0x%llx\n", (unsigned long long)val);
+           bist_errors++;
+       }
+
+       mask =     0x18ull;
+       expected = 0x0ull;
+       val = cvmx_read_csr(CVMX_L2D_ERR);
+       val = (val & mask) ^ expected;
+       if (val)
+       {
+           printf("L2 Parity error: 0x%llx\n", (unsigned long long)val);
+           bist_errors++;
+       }
+
+
+       while (1)
+           ;
+
+    }
+}
+
+
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-app-init.h b/arch/mips/cavium-octeon/executive/cvmx-app-init.h
new file mode 100644
index 0000000..449ea78
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-app-init.h
@@ -0,0 +1,273 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+/**
+ * @file
+ * Header file for simple executive application initialization.  This defines
+ * part of the ABI between the bootloader and the application.
+ * <hr>$Revision: 35349 $<hr>
+ *
+ */
+
+#ifndef __CVMX_APP_INIT_H__
+#define __CVMX_APP_INIT_H__
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+
+/* Current major and minor versions of the CVMX bootinfo block that is passed
+** from the bootloader to the application.  This is versioned so that applications
+** can properly handle multiple bootloader versions. */
+#define CVMX_BOOTINFO_MAJ_VER 1
+#define CVMX_BOOTINFO_MIN_VER 2
+
+
+#if (CVMX_BOOTINFO_MAJ_VER == 1)
+#define CVMX_BOOTINFO_OCTEON_SERIAL_LEN 20
+/* This structure is populated by the bootloader.  For binary
+** compatibility the only changes that should be made are
+** adding members to the end of the structure, and the minor
+** version should be incremented at that time.
+** If an incompatible change is made, the major version
+** must be incremented, and the minor version should be reset
+** to 0.
+*/
+typedef struct
+{
+    uint32_t major_version;
+    uint32_t minor_version;
+
+    uint64_t stack_top;
+    uint64_t heap_base;
+    uint64_t heap_end;
+    uint64_t desc_vaddr;
+
+    uint32_t exception_base_addr;
+    uint32_t stack_size;
+    uint32_t flags;
+    uint32_t core_mask;
+    uint32_t dram_size;  /**< DRAM size in megabytes */
+    uint32_t phy_mem_desc_addr;  /**< physical address of free memory descriptor block*/
+    uint32_t debugger_flags_base_addr;  /**< used to pass flags from app to debugger */
+    uint32_t eclock_hz;  /**< CPU clock speed, in hz */
+    uint32_t dclock_hz;  /**< DRAM clock speed, in hz */
+    uint32_t reserved0;
+    uint16_t board_type;
+    uint8_t board_rev_major;
+    uint8_t board_rev_minor;
+    uint16_t reserved1;
+    uint8_t reserved2;
+    uint8_t reserved3;
+    char board_serial_number[CVMX_BOOTINFO_OCTEON_SERIAL_LEN];
+    uint8_t mac_addr_base[6];
+    uint8_t mac_addr_count;
+#if (CVMX_BOOTINFO_MIN_VER >= 1)
+    /* Several boards support compact flash on the Octeon boot bus.  The CF
+    ** memory spaces may be mapped to different addresses on different boards.
+    ** These are the physical addresses, so care must be taken to use the correct
+    ** XKPHYS/KSEG0 addressing depending on the application's ABI.
+    ** These values will be 0 if CF is not present */
+    uint64_t compact_flash_common_base_addr;
+    uint64_t compact_flash_attribute_base_addr;
+    /* Base address of the LED display (as on EBT3000 board)
+    ** This will be 0 if LED display not present. */
+    uint64_t led_display_base_addr;
+#endif
+#if (CVMX_BOOTINFO_MIN_VER >= 2)
+    uint32_t dfa_ref_clock_hz;  /**< DFA reference clock in hz (if applicable)*/
+    uint32_t config_flags;  /**< flags indicating various configuration options.  These flags supercede
+                            ** the 'flags' variable and should be used instead if available */
+#endif
+
+
+} cvmx_bootinfo_t;
+
+#define CVMX_BOOTINFO_CFG_FLAG_PCI_HOST			(1ull << 0)
+#define CVMX_BOOTINFO_CFG_FLAG_PCI_TARGET		(1ull << 1)
+#define CVMX_BOOTINFO_CFG_FLAG_DEBUG			(1ull << 2)
+#define CVMX_BOOTINFO_CFG_FLAG_NO_MAGIC			(1ull << 3)
+/* This flag is set if the TLB mappings are not contained in the
+** 0x10000000 - 0x20000000 boot bus region. */
+#define CVMX_BOOTINFO_CFG_FLAG_OVERSIZE_TLB_MAPPING     (1ull << 4)
+#define CVMX_BOOTINFO_CFG_FLAG_BREAK			(1ull << 5)
+
+#endif /*   (CVMX_BOOTINFO_MAJ_VER == 1) */
+
+
+/* Type defines for board and chip types */
+enum cvmx_board_types_enum {
+    CVMX_BOARD_TYPE_NULL           =  0,
+    CVMX_BOARD_TYPE_SIM            =  1,
+    CVMX_BOARD_TYPE_EBT3000        =  2,
+    CVMX_BOARD_TYPE_KODAMA         =  3,
+    CVMX_BOARD_TYPE_NIAGARA        =  4,
+    CVMX_BOARD_TYPE_NAC38          =  5,	/* formerly NAO38 */
+    CVMX_BOARD_TYPE_THUNDER        =  6,
+    CVMX_BOARD_TYPE_TRANTOR        =  7,
+    CVMX_BOARD_TYPE_EBH3000        =  8,
+    CVMX_BOARD_TYPE_EBH3100        =  9,
+    CVMX_BOARD_TYPE_HIKARI         = 10,
+    CVMX_BOARD_TYPE_CN3010_EVB_HS5 = 11,
+    CVMX_BOARD_TYPE_CN3005_EVB_HS5 = 12,
+    CVMX_BOARD_TYPE_KBP            = 13,
+    CVMX_BOARD_TYPE_CN3020_EVB_HS5 = 14,  /* Deprecated, CVMX_BOARD_TYPE_CN3010_EVB_HS5 supports the CN3020 */
+    CVMX_BOARD_TYPE_EBT5800        = 15,
+    CVMX_BOARD_TYPE_NICPRO2        = 16,
+    CVMX_BOARD_TYPE_EBH5600        = 17,
+    CVMX_BOARD_TYPE_EBH5601        = 18,
+    CVMX_BOARD_TYPE_EBH5200        = 19,
+    CVMX_BOARD_TYPE_BBGW_REF       = 20,
+    CVMX_BOARD_TYPE_NIC_XLE_4G     = 21,
+    CVMX_BOARD_TYPE_EBT5600        = 22,
+    CVMX_BOARD_TYPE_EBH5201        = 23,
+    CVMX_BOARD_TYPE_MAX,
+
+    /* The range from CVMX_BOARD_TYPE_MAX to CVMX_BOARD_TYPE_CUST_DEFINED_MIN is reserved
+    ** for future SDK use. */
+
+    /* Set aside a range for customer boards.  These numbers are managed
+    ** by Cavium.
+    */
+    CVMX_BOARD_TYPE_CUST_DEFINED_MIN = 10000,
+    CVMX_BOARD_TYPE_CUST_WSX16       = 10001,
+    CVMX_BOARD_TYPE_CUST_NS0216      = 10002,
+    CVMX_BOARD_TYPE_CUST_NB5         = 10003,
+    CVMX_BOARD_TYPE_CUST_WMR500      = 10004,
+    CVMX_BOARD_TYPE_CUST_DEFINED_MAX = 20000,
+
+    /* Set aside a range for customer private use.  The SDK won't
+    ** use any numbers in this range. */
+    CVMX_BOARD_TYPE_CUST_PRIVATE_MIN = 20001,
+    CVMX_BOARD_TYPE_CUST_PRIVATE_MAX = 30000,
+
+    /* The remaining range is reserved for future use. */
+};
+enum cvmx_chip_types_enum {
+    CVMX_CHIP_TYPE_NULL = 0,
+    CVMX_CHIP_SIM_TYPE_DEPRECATED = 1,
+    CVMX_CHIP_TYPE_OCTEON_SAMPLE = 2,
+    CVMX_CHIP_TYPE_MAX,
+};
+
+/* Compatability alias for NAC38 name change, planned to be removed from SDK 1.7 */
+#define CVMX_BOARD_TYPE_NAO38	CVMX_BOARD_TYPE_NAC38
+
+/* Functions to return string based on type */
+#define ENUM_BRD_TYPE_CASE(x)   case x: return(#x + 16);   /* Skip CVMX_BOARD_TYPE_ */
+static inline const char *cvmx_board_type_to_string(enum cvmx_board_types_enum type)
+{
+    switch (type)
+    {
+        ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_NULL)
+        ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_SIM)
+        ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_EBT3000)
+        ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_KODAMA)
+        ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_NIAGARA)
+        ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_NAC38)
+        ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_THUNDER)
+        ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_TRANTOR)
+        ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_EBH3000)
+        ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_EBH3100)
+        ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_HIKARI)
+        ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_CN3010_EVB_HS5)
+        ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_CN3005_EVB_HS5)
+        ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_KBP)
+        ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_CN3020_EVB_HS5)
+        ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_EBT5800)
+        ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_NICPRO2)
+        ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_EBH5600)
+        ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_EBH5601)
+        ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_EBH5200)
+        ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_BBGW_REF)
+        ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_NIC_XLE_4G)
+        ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_EBT5600)
+        ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_EBH5201)
+        ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_MAX)
+
+        /* Customer boards listed here */
+        ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_CUST_DEFINED_MIN)
+        ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_CUST_WSX16)
+        ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_CUST_NS0216)
+        ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_CUST_NB5)
+        ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_CUST_WMR500)
+        ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_CUST_DEFINED_MAX)
+
+        /* Customer private range */
+        ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_CUST_PRIVATE_MIN)
+        ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_CUST_PRIVATE_MAX)
+    }
+    return "Unsupported Board";
+}
+
+#define ENUM_CHIP_TYPE_CASE(x)   case x: return(#x + 15);   /* Skip CVMX_CHIP_TYPE */
+static inline const char *cvmx_chip_type_to_string(enum cvmx_chip_types_enum type)
+{
+    switch (type)
+    {
+        ENUM_CHIP_TYPE_CASE(CVMX_CHIP_TYPE_NULL)
+        ENUM_CHIP_TYPE_CASE(CVMX_CHIP_SIM_TYPE_DEPRECATED)
+        ENUM_CHIP_TYPE_CASE(CVMX_CHIP_TYPE_OCTEON_SAMPLE)
+        ENUM_CHIP_TYPE_CASE(CVMX_CHIP_TYPE_MAX)
+    }
+    return "Unsupported Chip";
+}
+
+
+
+
+
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif /* __CVMX_APP_INIT_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-asm.h b/arch/mips/cavium-octeon/executive/cvmx-asm.h
new file mode 100644
index 0000000..2a46e73
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-asm.h
@@ -0,0 +1,500 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * This is file defines ASM primitives for the executive.
+
+ * <hr>$Revision: 32636 $<hr>
+ *
+ *
+ */
+#ifndef __CVMX_ASM_H__
+#define __CVMX_ASM_H__
+
+#include "octeon-model.h"
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+/* other useful stuff */
+#define CVMX_BREAK asm volatile ("break")
+#define CVMX_SYNC asm volatile ("sync" : : :"memory")
+/* String version of SYNCW macro for using in inline asm constructs */
+#define CVMX_SYNCW_STR "syncw\nsyncw\n"
+#ifdef __OCTEON__
+    #define CVMX_SYNCIO asm volatile ("nop")   /* Deprecated, will be removed in future release */
+    #define CVMX_SYNCIOBDMA asm volatile ("synciobdma" : : :"memory")
+    #define CVMX_SYNCIOALL asm volatile ("nop")   /* Deprecated, will be removed in future release */
+    /* We actually use two syncw instructions in a row when we need a write
+        memory barrier. This is because the CN3XXX series of Octeons have
+        errata Core-401. This can cause a single syncw to not enforce
+        ordering under very rare conditions. Even if it is rare, better safe
+        than sorry */
+    #define CVMX_SYNCW asm volatile ("syncw\nsyncw\n" : : :"memory")
+#if defined(VXWORKS) || defined(__linux__)
+    /* Define new sync instructions to be normal SYNC instructions for
+        operating systems that use threads */
+    #define CVMX_SYNCWS CVMX_SYNCW
+    #define CVMX_SYNCS  CVMX_SYNC
+    #define CVMX_SYNCWS_STR CVMX_SYNCW_STR
+#else
+    #if OCTEON_IS_COMMON_BINARY()
+        #if defined(CVMX_BUILD_FOR_TOOLCHAIN)
+	    /* While building simple exec toolchain, always use syncw to
+	       support all Octeon models. */
+	    #define CVMX_SYNCWS CVMX_SYNCW
+	    #define CVMX_SYNCS  CVMX_SYNC
+            #define CVMX_SYNCWS_STR CVMX_SYNCW_STR
+	#else
+            /* Again, just like syncw, we may need two syncws instructions in a row due
+                errata Core-401 */
+            #define CVMX_SYNCWS asm volatile ("syncws\nsyncws\n" : : :"memory")
+            #define CVMX_SYNCS asm volatile ("syncs" : : :"memory")
+            #define CVMX_SYNCWS_STR "syncws\nsyncws\n"
+	#endif
+    #else /* OCTEON_IS_COMMON_BINARY() */
+        #if OCTEON_MODEL == OCTEON_CN38XX_PASS1
+            /* Define new sync instructions to be normal SYNC instructions for
+                OCTEON_CN38XX_PASS1. It doesn't support these */
+            #define CVMX_SYNCWS CVMX_SYNCW
+            #define CVMX_SYNCS  CVMX_SYNC
+            #define CVMX_SYNCWS_STR CVMX_SYNCW_STR
+        #else
+            /* Again, just like syncw, we may need two syncws instructions in a row due
+                errata Core-401 */
+            #define CVMX_SYNCWS asm volatile ("syncws\nsyncws\n" : : :"memory")
+            #define CVMX_SYNCS asm volatile ("syncs" : : :"memory")
+            #define CVMX_SYNCWS_STR "syncws\nsyncws\n"
+        #endif
+    #endif /* OCTEON_IS_COMMON_BINARY() */
+#endif
+#else
+    /* Not using a Cavium compiler, always use the slower sync so the assembler stays happy */
+    #define CVMX_SYNCIO asm volatile ("nop")   /* Deprecated, will be removed in future release */
+    #define CVMX_SYNCIOBDMA asm volatile ("sync" : : :"memory")
+    #define CVMX_SYNCIOALL asm volatile ("nop")   /* Deprecated, will be removed in future release */
+    #define CVMX_SYNCW asm volatile ("sync" : : :"memory")
+    #define CVMX_SYNCWS CVMX_SYNCW
+    #define CVMX_SYNCS  CVMX_SYNC
+    #define CVMX_SYNCWS_STR CVMX_SYNCW_STR
+#endif
+#define CVMX_SYNCI(address, offset) asm volatile ("synci " CVMX_TMP_STR(offset) "(%[rbase])" : : [rbase] "d" (address) )
+#define CVMX_PREFETCH0(address) CVMX_PREFETCH(address, 0)
+#define CVMX_PREFETCH128(address) CVMX_PREFETCH(address, 128)
+// a normal prefetch
+#define CVMX_PREFETCH(address, offset) CVMX_PREFETCH_PREF0(address, offset)
+// normal prefetches that use the pref instruction
+#define CVMX_PREFETCH_PREF0(address, offset) asm volatile ("pref 0, " CVMX_TMP_STR(offset) "(%[rbase])" : : [rbase] "d" (address) )
+#define CVMX_PREFETCH_PREF1(address, offset) asm volatile ("pref 1, " CVMX_TMP_STR(offset) "(%[rbase])" : : [rbase] "d" (address) )
+#define CVMX_PREFETCH_PREF6(address, offset) asm volatile ("pref 6, " CVMX_TMP_STR(offset) "(%[rbase])" : : [rbase] "d" (address) )
+#define CVMX_PREFETCH_PREF7(address, offset) asm volatile ("pref 7, " CVMX_TMP_STR(offset) "(%[rbase])" : : [rbase] "d" (address) )
+// prefetch into L1, do not put the block in the L2
+#define CVMX_PREFETCH_NOTL2(address, offset) asm volatile ("pref 4, " CVMX_TMP_STR(offset) "(%[rbase])" : : [rbase] "d" (address) )
+#define CVMX_PREFETCH_NOTL22(address, offset) asm volatile ("pref 5, " CVMX_TMP_STR(offset) "(%[rbase])" : : [rbase] "d" (address) )
+// prefetch into L2, do not put the block in the L1
+#define CVMX_PREFETCH_L2(address, offset) asm volatile ("pref 28, " CVMX_TMP_STR(offset) "(%[rbase])" : : [rbase] "d" (address) )
+// CVMX_PREPARE_FOR_STORE makes each byte of the block unpredictable (actually old value or zero) until
+// that byte is stored to (by this or another processor. Note that the value of each byte is not only
+// unpredictable, but may also change again - up until the point when one of the cores stores to the
+// byte.
+#define CVMX_PREPARE_FOR_STORE(address, offset) asm volatile ("pref 30, " CVMX_TMP_STR(offset) "(%[rbase])" : : [rbase] "d" (address) )
+// This is a command headed to the L2 controller to tell it to clear its dirty bit for a
+// block. Basically, SW is telling HW that the current version of the block will not be
+// used.
+#define CVMX_DONT_WRITE_BACK(address, offset) asm volatile ("pref 29, " CVMX_TMP_STR(offset) "(%[rbase])" : : [rbase] "d" (address) )
+
+#define CVMX_ICACHE_INVALIDATE  { CVMX_SYNC; asm volatile ("synci 0($0)" : : ); }    // flush stores, invalidate entire icache
+#define CVMX_ICACHE_INVALIDATE2 { CVMX_SYNC; asm volatile ("cache 0, 0($0)" : : ); } // flush stores, invalidate entire icache
+#define CVMX_DCACHE_INVALIDATE  { CVMX_SYNC; asm volatile ("cache 9, 0($0)" : : ); } // complete prefetches, invalidate entire dcache
+
+/* new instruction to make RC4 run faster */
+#define CVMX_BADDU(result, input1, input2) asm ("baddu %[rd],%[rs],%[rt]" : [rd] "=d" (result) : [rs] "d" (input1) , [rt] "d" (input2))
+
+// misc v2 stuff
+#define CVMX_ROTR(result, input1, shiftconst) asm ("rotr %[rd],%[rs]," CVMX_TMP_STR(shiftconst) : [rd] "=d" (result) : [rs] "d" (input1))
+#define CVMX_ROTRV(result, input1, input2) asm ("rotrv %[rd],%[rt],%[rs]" : [rd] "=d" (result) : [rt] "d" (input1) , [rs] "d" (input2))
+#define CVMX_DROTR(result, input1, shiftconst) asm ("drotr %[rd],%[rs]," CVMX_TMP_STR(shiftconst) : [rd] "=d" (result) : [rs] "d" (input1))
+#define CVMX_DROTRV(result, input1, input2) asm ("drotrv %[rd],%[rt],%[rs]" : [rd] "=d" (result) : [rt] "d" (input1) , [rs] "d" (input2))
+#define CVMX_SEB(result, input1) asm ("seb %[rd],%[rt]" : [rd] "=d" (result) : [rt] "d" (input1))
+#define CVMX_SEH(result, input1) asm ("seh %[rd],%[rt]" : [rd] "=d" (result) : [rt] "d" (input1))
+#define CVMX_DSBH(result, input1) asm ("dsbh %[rd],%[rt]" : [rd] "=d" (result) : [rt] "d" (input1))
+#define CVMX_DSHD(result, input1) asm ("dshd %[rd],%[rt]" : [rd] "=d" (result) : [rt] "d" (input1))
+#define CVMX_WSBH(result, input1) asm ("wsbh %[rd],%[rt]" : [rd] "=d" (result) : [rt] "d" (input1))
+
+// Endian swap
+#define CVMX_ES64(result, input) \
+        do {\
+        CVMX_DSBH(result, input); \
+        CVMX_DSHD(result, result); \
+        } while (0)
+#define CVMX_ES32(result, input) \
+        do {\
+        CVMX_WSBH(result, input); \
+        CVMX_ROTR(result, result, 16); \
+        } while (0)
+
+
+/* extract and insert - NOTE that pos and len variables must be constants! */
+/* the P variants take len rather than lenm1 */
+/* the M1 variants take lenm1 rather than len */
+#define CVMX_EXTS(result,input,pos,lenm1) asm ("exts %[rt],%[rs]," CVMX_TMP_STR(pos) "," CVMX_TMP_STR(lenm1) : [rt] "=d" (result) : [rs] "d" (input))
+#define CVMX_EXTSP(result,input,pos,len) CVMX_EXTS(result,input,pos,(len)-1)
+
+#define CVMX_DEXT(result,input,pos,len) asm ("dext %[rt],%[rs]," CVMX_TMP_STR(pos) "," CVMX_TMP_STR(len) : [rt] "=d" (result) : [rs] "d" (input))
+#define CVMX_DEXTM1(result,input,pos,lenm1) CVMX_DEXT(result,input,pos,(lenm1)+1)
+
+#define CVMX_EXT(result,input,pos,len) asm ("ext %[rt],%[rs]," CVMX_TMP_STR(pos) "," CVMX_TMP_STR(len) : [rt] "=d" (result) : [rs] "d" (input))
+#define CVMX_EXTM1(result,input,pos,lenm1) CVMX_EXT(result,input,pos,(lenm1)+1)
+
+// removed
+// #define CVMX_EXTU(result,input,pos,lenm1) asm ("extu %[rt],%[rs]," CVMX_TMP_STR(pos) "," CVMX_TMP_STR(lenm1) : [rt] "=d" (result) : [rs] "d" (input))
+// #define CVMX_EXTUP(result,input,pos,len) CVMX_EXTU(result,input,pos,(len)-1)
+
+#define CVMX_CINS(result,input,pos,lenm1) asm ("cins %[rt],%[rs]," CVMX_TMP_STR(pos) "," CVMX_TMP_STR(lenm1) : [rt] "=d" (result) : [rs] "d" (input))
+#define CVMX_CINSP(result,input,pos,len) CVMX_CINS(result,input,pos,(len)-1)
+
+#define CVMX_DINS(result,input,pos,len) asm ("dins %[rt],%[rs]," CVMX_TMP_STR(pos) "," CVMX_TMP_STR(len): [rt] "=d" (result): [rs] "d" (input), "[rt]" (result))
+#define CVMX_DINSM1(result,input,pos,lenm1) CVMX_DINS(result,input,pos,(lenm1)+1)
+#define CVMX_DINSC(result,pos,len) asm ("dins %[rt],$0," CVMX_TMP_STR(pos) "," CVMX_TMP_STR(len): [rt] "=d" (result): "[rt]" (result))
+#define CVMX_DINSCM1(result,pos,lenm1) CVMX_DINSC(result,pos,(lenm1)+1)
+
+#define CVMX_INS(result,input,pos,len) asm ("ins %[rt],%[rs]," CVMX_TMP_STR(pos) "," CVMX_TMP_STR(len): [rt] "=d" (result): [rs] "d" (input), "[rt]" (result))
+#define CVMX_INSM1(result,input,pos,lenm1) CVMX_INS(result,input,pos,(lenm1)+1)
+#define CVMX_INSC(result,pos,len) asm ("ins %[rt],$0," CVMX_TMP_STR(pos) "," CVMX_TMP_STR(len): [rt] "=d" (result): "[rt]" (result))
+#define CVMX_INSCM1(result,pos,lenm1) CVMX_INSC(result,pos,(lenm1)+1)
+
+// removed
+// #define CVMX_INS0(result,input,pos,lenm1) asm("ins0 %[rt],%[rs]," CVMX_TMP_STR(pos) "," CVMX_TMP_STR(lenm1): [rt] "=d" (result): [rs] "d" (input), "[rt]" (result))
+// #define CVMX_INS0P(result,input,pos,len) CVMX_INS0(result,input,pos,(len)-1)
+// #define CVMX_INS0C(result,pos,lenm1) asm ("ins0 %[rt],$0," CVMX_TMP_STR(pos) "," CVMX_TMP_STR(lenm1) : [rt] "=d" (result) : "[rt]" (result))
+// #define CVMX_INS0CP(result,pos,len) CVMX_INS0C(result,pos,(len)-1)
+
+#define CVMX_CLZ(result, input) asm ("clz %[rd],%[rs]" : [rd] "=d" (result) : [rs] "d" (input))
+#define CVMX_DCLZ(result, input) asm ("dclz %[rd],%[rs]" : [rd] "=d" (result) : [rs] "d" (input))
+#define CVMX_CLO(result, input) asm ("clo %[rd],%[rs]" : [rd] "=d" (result) : [rs] "d" (input))
+#define CVMX_DCLO(result, input) asm ("dclo %[rd],%[rs]" : [rd] "=d" (result) : [rs] "d" (input))
+#define CVMX_POP(result, input) asm ("pop %[rd],%[rs]" : [rd] "=d" (result) : [rs] "d" (input))
+#define CVMX_DPOP(result, input) asm ("dpop %[rd],%[rs]" : [rd] "=d" (result) : [rs] "d" (input))
+
+// some new cop0-like stuff
+#define CVMX_RDHWR(result, regstr) asm volatile ("rdhwr %[rt],$" CVMX_TMP_STR(regstr) : [rt] "=d" (result))
+#define CVMX_RDHWRNV(result, regstr) asm ("rdhwr %[rt],$" CVMX_TMP_STR(regstr) : [rt] "=d" (result))
+#define CVMX_DI(result) asm volatile ("di %[rt]" : [rt] "=d" (result))
+#define CVMX_DI_NULL asm volatile ("di")
+#define CVMX_EI(result) asm volatile ("ei %[rt]" : [rt] "=d" (result))
+#define CVMX_EI_NULL asm volatile ("ei")
+#define CVMX_EHB asm volatile ("ehb")
+
+/* mul stuff */
+#define CVMX_MTM0(m) asm volatile ("mtm0 %[rs]" : : [rs] "d" (m))
+#define CVMX_MTM1(m) asm volatile ("mtm1 %[rs]" : : [rs] "d" (m))
+#define CVMX_MTM2(m) asm volatile ("mtm2 %[rs]" : : [rs] "d" (m))
+#define CVMX_MTP0(p) asm volatile ("mtp0 %[rs]" : : [rs] "d" (p))
+#define CVMX_MTP1(p) asm volatile ("mtp1 %[rs]" : : [rs] "d" (p))
+#define CVMX_MTP2(p) asm volatile ("mtp2 %[rs]" : : [rs] "d" (p))
+#define CVMX_VMULU(dest,mpcand,accum) asm volatile ("vmulu %[rd],%[rs],%[rt]" : [rd] "=d" (dest) : [rs] "d" (mpcand), [rt] "d" (accum))
+#define CVMX_VMM0(dest,mpcand,accum) asm volatile ("vmm0 %[rd],%[rs],%[rt]" : [rd] "=d" (dest) : [rs] "d" (mpcand), [rt] "d" (accum))
+#define CVMX_V3MULU(dest,mpcand,accum) asm volatile ("v3mulu %[rd],%[rs],%[rt]" : [rd] "=d" (dest) : [rs] "d" (mpcand), [rt] "d" (accum))
+
+/* branch stuff */
+// these are hard to make work because the compiler does not realize that the
+// instruction is a branch so may optimize away the label
+// the labels to these next two macros must not include a ":" at the end
+#define CVMX_BBIT1(var, pos, label) asm volatile ("bbit1 %[rs]," CVMX_TMP_STR(pos) "," CVMX_TMP_STR(label) : : [rs] "d" (var))
+#define CVMX_BBIT0(var, pos, label) asm volatile ("bbit0 %[rs]," CVMX_TMP_STR(pos) "," CVMX_TMP_STR(label) : : [rs] "d" (var))
+// the label to this macro must include a ":" at the end
+#define CVMX_ASM_LABEL(label) label \
+                             asm volatile (CVMX_TMP_STR(label) : : )
+
+//
+// Low-latency memory stuff
+//
+// set can be 0-1
+#define CVMX_MT_LLM_READ_ADDR(set,val)    asm volatile ("dmtc2 %[rt],0x0400+(8*(" CVMX_TMP_STR(set) "))" : : [rt] "d" (val))
+#define CVMX_MT_LLM_WRITE_ADDR_INTERNAL(set,val)   asm volatile ("dmtc2 %[rt],0x0401+(8*(" CVMX_TMP_STR(set) "))" : : [rt] "d" (val))
+#define CVMX_MT_LLM_READ64_ADDR(set,val)  asm volatile ("dmtc2 %[rt],0x0404+(8*(" CVMX_TMP_STR(set) "))" : : [rt] "d" (val))
+#define CVMX_MT_LLM_WRITE64_ADDR_INTERNAL(set,val) asm volatile ("dmtc2 %[rt],0x0405+(8*(" CVMX_TMP_STR(set) "))" : : [rt] "d" (val))
+#define CVMX_MT_LLM_DATA(set,val)         asm volatile ("dmtc2 %[rt],0x0402+(8*(" CVMX_TMP_STR(set) "))" : : [rt] "d" (val))
+#define CVMX_MF_LLM_DATA(set,val)         asm volatile ("dmfc2 %[rt],0x0402+(8*(" CVMX_TMP_STR(set) "))" : [rt] "=d" (val) : )
+
+
+// load linked, store conditional
+#define CVMX_LL(dest, address, offset) asm volatile ("ll %[rt], " CVMX_TMP_STR(offset) "(%[rbase])" : [rt] "=d" (dest) : [rbase] "d" (address) )
+#define CVMX_LLD(dest, address, offset) asm volatile ("lld %[rt], " CVMX_TMP_STR(offset) "(%[rbase])" : [rt] "=d" (dest) : [rbase] "d" (address) )
+#define CVMX_SC(srcdest, address, offset) asm volatile ("sc %[rt], " CVMX_TMP_STR(offset) "(%[rbase])" : [rt] "=d" (srcdest) : [rbase] "d" (address), "[rt]" (srcdest) )
+#define CVMX_SCD(srcdest, address, offset) asm volatile ("scd %[rt], " CVMX_TMP_STR(offset) "(%[rbase])" : [rt] "=d" (srcdest) : [rbase] "d" (address), "[rt]" (srcdest) )
+
+// load/store word left/right
+#define CVMX_LWR(srcdest, address, offset) asm volatile ("lwr %[rt], " CVMX_TMP_STR(offset) "(%[rbase])" : [rt] "=d" (srcdest) : [rbase] "d" (address), "[rt]" (srcdest) )
+#define CVMX_LWL(srcdest, address, offset) asm volatile ("lwl %[rt], " CVMX_TMP_STR(offset) "(%[rbase])" : [rt] "=d" (srcdest) : [rbase] "d" (address), "[rt]" (srcdest) )
+#define CVMX_LDR(srcdest, address, offset) asm volatile ("ldr %[rt], " CVMX_TMP_STR(offset) "(%[rbase])" : [rt] "=d" (srcdest) : [rbase] "d" (address), "[rt]" (srcdest) )
+#define CVMX_LDL(srcdest, address, offset) asm volatile ("ldl %[rt], " CVMX_TMP_STR(offset) "(%[rbase])" : [rt] "=d" (srcdest) : [rbase] "d" (address), "[rt]" (srcdest) )
+
+#define CVMX_SWR(src, address, offset) asm volatile ("swr %[rt], " CVMX_TMP_STR(offset) "(%[rbase])" : : [rbase] "d" (address), [rt] "d" (src) )
+#define CVMX_SWL(src, address, offset) asm volatile ("swl %[rt], " CVMX_TMP_STR(offset) "(%[rbase])" : : [rbase] "d" (address), [rt] "d" (src) )
+#define CVMX_SDR(src, address, offset) asm volatile ("sdr %[rt], " CVMX_TMP_STR(offset) "(%[rbase])" : : [rbase] "d" (address), [rt] "d" (src) )
+#define CVMX_SDL(src, address, offset) asm volatile ("sdl %[rt], " CVMX_TMP_STR(offset) "(%[rbase])" : : [rbase] "d" (address), [rt] "d" (src) )
+
+
+
+//
+// Useful crypto ASM's
+//
+
+// CRC
+
+#define CVMX_MT_CRC_POLYNOMIAL(val)         asm volatile ("dmtc2 %[rt],0x4200" : : [rt] "d" (val))
+#define CVMX_MT_CRC_IV(val)                 asm volatile ("dmtc2 %[rt],0x0201" : : [rt] "d" (val))
+#define CVMX_MT_CRC_LEN(val)                asm volatile ("dmtc2 %[rt],0x1202" : : [rt] "d" (val))
+#define CVMX_MT_CRC_BYTE(val)               asm volatile ("dmtc2 %[rt],0x0204" : : [rt] "d" (val))
+#define CVMX_MT_CRC_HALF(val)               asm volatile ("dmtc2 %[rt],0x0205" : : [rt] "d" (val))
+#define CVMX_MT_CRC_WORD(val)               asm volatile ("dmtc2 %[rt],0x0206" : : [rt] "d" (val))
+#define CVMX_MT_CRC_DWORD(val)              asm volatile ("dmtc2 %[rt],0x1207" : : [rt] "d" (val))
+#define CVMX_MT_CRC_VAR(val)                asm volatile ("dmtc2 %[rt],0x1208" : : [rt] "d" (val))
+#define CVMX_MT_CRC_POLYNOMIAL_REFLECT(val) asm volatile ("dmtc2 %[rt],0x4210" : : [rt] "d" (val))
+#define CVMX_MT_CRC_IV_REFLECT(val)         asm volatile ("dmtc2 %[rt],0x0211" : : [rt] "d" (val))
+#define CVMX_MT_CRC_BYTE_REFLECT(val)       asm volatile ("dmtc2 %[rt],0x0214" : : [rt] "d" (val))
+#define CVMX_MT_CRC_HALF_REFLECT(val)       asm volatile ("dmtc2 %[rt],0x0215" : : [rt] "d" (val))
+#define CVMX_MT_CRC_WORD_REFLECT(val)       asm volatile ("dmtc2 %[rt],0x0216" : : [rt] "d" (val))
+#define CVMX_MT_CRC_DWORD_REFLECT(val)      asm volatile ("dmtc2 %[rt],0x1217" : : [rt] "d" (val))
+#define CVMX_MT_CRC_VAR_REFLECT(val)        asm volatile ("dmtc2 %[rt],0x1218" : : [rt] "d" (val))
+
+#define CVMX_MF_CRC_POLYNOMIAL(val)         asm volatile ("dmfc2 %[rt],0x0200" : [rt] "=d" (val) : )
+#define CVMX_MF_CRC_IV(val)                 asm volatile ("dmfc2 %[rt],0x0201" : [rt] "=d" (val) : )
+#define CVMX_MF_CRC_IV_REFLECT(val)         asm volatile ("dmfc2 %[rt],0x0203" : [rt] "=d" (val) : )
+#define CVMX_MF_CRC_LEN(val)                asm volatile ("dmfc2 %[rt],0x0202" : [rt] "=d" (val) : )
+
+// MD5 and SHA-1
+
+// pos can be 0-6
+#define CVMX_MT_HSH_DAT(val,pos)    asm volatile ("dmtc2 %[rt],0x0040+" CVMX_TMP_STR(pos) :                 : [rt] "d" (val))
+#define CVMX_MT_HSH_DATZ(pos)       asm volatile ("dmtc2    $0,0x0040+" CVMX_TMP_STR(pos) :                 :               )
+// pos can be 0-14
+#define CVMX_MT_HSH_DATW(val,pos)   asm volatile ("dmtc2 %[rt],0x0240+" CVMX_TMP_STR(pos) :                 : [rt] "d" (val))
+#define CVMX_MT_HSH_DATWZ(pos)      asm volatile ("dmtc2    $0,0x0240+" CVMX_TMP_STR(pos) :                 :               )
+#define CVMX_MT_HSH_STARTMD5(val)   asm volatile ("dmtc2 %[rt],0x4047"                   :                 : [rt] "d" (val))
+#define CVMX_MT_HSH_STARTSHA(val)   asm volatile ("dmtc2 %[rt],0x4057"                   :                 : [rt] "d" (val))
+#define CVMX_MT_HSH_STARTSHA256(val)   asm volatile ("dmtc2 %[rt],0x404f"                   :                 : [rt] "d" (val))
+#define CVMX_MT_HSH_STARTSHA512(val)   asm volatile ("dmtc2 %[rt],0x424f"                   :                 : [rt] "d" (val))
+// pos can be 0-3
+#define CVMX_MT_HSH_IV(val,pos)     asm volatile ("dmtc2 %[rt],0x0048+" CVMX_TMP_STR(pos) :                 : [rt] "d" (val))
+// pos can be 0-7
+#define CVMX_MT_HSH_IVW(val,pos)     asm volatile ("dmtc2 %[rt],0x0250+" CVMX_TMP_STR(pos) :                 : [rt] "d" (val))
+
+// pos can be 0-6
+#define CVMX_MF_HSH_DAT(val,pos)    asm volatile ("dmfc2 %[rt],0x0040+" CVMX_TMP_STR(pos) : [rt] "=d" (val) :               )
+// pos can be 0-14
+#define CVMX_MF_HSH_DATW(val,pos)   asm volatile ("dmfc2 %[rt],0x0240+" CVMX_TMP_STR(pos) : [rt] "=d" (val) :               )
+// pos can be 0-3
+#define CVMX_MF_HSH_IV(val,pos)     asm volatile ("dmfc2 %[rt],0x0048+" CVMX_TMP_STR(pos) : [rt] "=d" (val) :               )
+// pos can be 0-7
+#define CVMX_MF_HSH_IVW(val,pos)     asm volatile ("dmfc2 %[rt],0x0250+" CVMX_TMP_STR(pos) : [rt] "=d" (val) :               )
+
+// 3DES
+
+// pos can be 0-2
+#define CVMX_MT_3DES_KEY(val,pos)   asm volatile ("dmtc2 %[rt],0x0080+" CVMX_TMP_STR(pos) :                 : [rt] "d" (val))
+#define CVMX_MT_3DES_IV(val)        asm volatile ("dmtc2 %[rt],0x0084"                   :                 : [rt] "d" (val))
+#define CVMX_MT_3DES_ENC_CBC(val)   asm volatile ("dmtc2 %[rt],0x4088"                   :                 : [rt] "d" (val))
+#define CVMX_MT_3DES_ENC(val)       asm volatile ("dmtc2 %[rt],0x408a"                   :                 : [rt] "d" (val))
+#define CVMX_MT_3DES_DEC_CBC(val)   asm volatile ("dmtc2 %[rt],0x408c"                   :                 : [rt] "d" (val))
+#define CVMX_MT_3DES_DEC(val)       asm volatile ("dmtc2 %[rt],0x408e"                   :                 : [rt] "d" (val))
+#define CVMX_MT_3DES_RESULT(val)    asm volatile ("dmtc2 %[rt],0x0098"                   :                 : [rt] "d" (val))
+
+// pos can be 0-2
+#define CVMX_MF_3DES_KEY(val,pos)   asm volatile ("dmfc2 %[rt],0x0080+" CVMX_TMP_STR(pos) : [rt] "=d" (val) :               )
+#define CVMX_MF_3DES_IV(val)        asm volatile ("dmfc2 %[rt],0x0084"                   : [rt] "=d" (val) :               )
+#define CVMX_MF_3DES_RESULT(val)    asm volatile ("dmfc2 %[rt],0x0088"                   : [rt] "=d" (val) :               )
+
+// KASUMI
+
+// pos can be 0-1
+#define CVMX_MT_KAS_KEY(val,pos)    CVMX_MT_3DES_KEY(val,pos)
+#define CVMX_MT_KAS_ENC_CBC(val)    asm volatile ("dmtc2 %[rt],0x4089"                   :                 : [rt] "d" (val))
+#define CVMX_MT_KAS_ENC(val)        asm volatile ("dmtc2 %[rt],0x408b"                   :                 : [rt] "d" (val))
+#define CVMX_MT_KAS_RESULT(val)     CVMX_MT_3DES_RESULT(val)
+
+// pos can be 0-1
+#define CVMX_MF_KAS_KEY(val,pos)    CVMX_MF_3DES_KEY(val,pos)
+#define CVMX_MF_KAS_RESULT(val)     CVMX_MF_3DES_RESULT(val)
+
+// AES
+
+#define CVMX_MT_AES_ENC_CBC0(val)   asm volatile ("dmtc2 %[rt],0x0108"                   :                 : [rt] "d" (val))
+#define CVMX_MT_AES_ENC_CBC1(val)   asm volatile ("dmtc2 %[rt],0x3109"                   :                 : [rt] "d" (val))
+#define CVMX_MT_AES_ENC0(val)       asm volatile ("dmtc2 %[rt],0x010a"                   :                 : [rt] "d" (val))
+#define CVMX_MT_AES_ENC1(val)       asm volatile ("dmtc2 %[rt],0x310b"                   :                 : [rt] "d" (val))
+#define CVMX_MT_AES_DEC_CBC0(val)   asm volatile ("dmtc2 %[rt],0x010c"                   :                 : [rt] "d" (val))
+#define CVMX_MT_AES_DEC_CBC1(val)   asm volatile ("dmtc2 %[rt],0x310d"                   :                 : [rt] "d" (val))
+#define CVMX_MT_AES_DEC0(val)       asm volatile ("dmtc2 %[rt],0x010e"                   :                 : [rt] "d" (val))
+#define CVMX_MT_AES_DEC1(val)       asm volatile ("dmtc2 %[rt],0x310f"                   :                 : [rt] "d" (val))
+// pos can be 0-3
+#define CVMX_MT_AES_KEY(val,pos)    asm volatile ("dmtc2 %[rt],0x0104+" CVMX_TMP_STR(pos) :                 : [rt] "d" (val))
+// pos can be 0-1
+#define CVMX_MT_AES_IV(val,pos)     asm volatile ("dmtc2 %[rt],0x0102+" CVMX_TMP_STR(pos) :                 : [rt] "d" (val))
+#define CVMX_MT_AES_KEYLENGTH(val)  asm volatile ("dmtc2 %[rt],0x0110"                   :                 : [rt] "d" (val)) // write the keylen
+// pos can be 0-1
+#define CVMX_MT_AES_RESULT(val,pos) asm volatile ("dmtc2 %[rt],0x0100+" CVMX_TMP_STR(pos) :                 : [rt] "d" (val))
+
+// pos can be 0-1
+#define CVMX_MF_AES_RESULT(val,pos) asm volatile ("dmfc2 %[rt],0x0100+" CVMX_TMP_STR(pos) : [rt] "=d" (val) :               )
+// pos can be 0-1
+#define CVMX_MF_AES_IV(val,pos)     asm volatile ("dmfc2 %[rt],0x0102+" CVMX_TMP_STR(pos) : [rt] "=d" (val) :               )
+// pos can be 0-3
+#define CVMX_MF_AES_KEY(val,pos)    asm volatile ("dmfc2 %[rt],0x0104+" CVMX_TMP_STR(pos) : [rt] "=d" (val) :               )
+#define CVMX_MF_AES_KEYLENGTH(val)  asm volatile ("dmfc2 %[rt],0x0110"                   : [rt] "=d" (val) :               ) // read the keylen
+#define CVMX_MF_AES_DAT0(val)       asm volatile ("dmfc2 %[rt],0x0111"                   : [rt] "=d" (val) :               ) // first piece of input data
+/* GFM COP2 macros */
+/* index can be 0 or 1 */
+#define CVMX_MF_GFM_MUL(val, index)     asm volatile ("dmfc2 %[rt],0x0258+" CVMX_TMP_STR(index) : [rt] "=d" (val) :               )
+#define CVMX_MF_GFM_POLY(val)           asm volatile ("dmfc2 %[rt],0x025e"                      : [rt] "=d" (val) :               )
+#define CVMX_MF_GFM_RESINP(val, index)  asm volatile ("dmfc2 %[rt],0x025a+" CVMX_TMP_STR(index) : [rt] "=d" (val) :               )
+
+#define CVMX_MT_GFM_MUL(val, index)     asm volatile ("dmtc2 %[rt],0x0258+" CVMX_TMP_STR(index) :                 : [rt] "d" (val))
+#define CVMX_MT_GFM_POLY(val)           asm volatile ("dmtc2 %[rt],0x025e"                      :                 : [rt] "d" (val))
+#define CVMX_MT_GFM_RESINP(val, index)  asm volatile ("dmtc2 %[rt],0x025a+" CVMX_TMP_STR(index) :                 : [rt] "d" (val))
+#define CVMX_MT_GFM_XOR0(val)           asm volatile ("dmtc2 %[rt],0x025c"                      :                 : [rt] "d" (val))
+#define CVMX_MT_GFM_XORMUL1(val)        asm volatile ("dmtc2 %[rt],0x425d"                      :                 : [rt] "d" (val))
+
+
+/* check_ordering stuff */
+#if 0
+#define CVMX_MF_CHORD(dest)         asm volatile ("dmfc2 %[rt],0x400" : [rt] "=d" (dest) : )
+#else
+#define CVMX_MF_CHORD(dest)         CVMX_RDHWR(dest, 30)
+#endif
+
+#if 0
+#define CVMX_MF_CYCLE(dest)         asm volatile ("dmfc0 %[rt],$9,6" : [rt] "=d" (dest) : ) // Use (64-bit) CvmCount register rather than Count
+#else
+#define CVMX_MF_CYCLE(dest)         CVMX_RDHWR(dest, 31) /* reads the current (64-bit) CvmCount value */
+#endif
+
+#define CVMX_MT_CYCLE(src)         asm volatile ("dmtc0 %[rt],$9,6" :: [rt] "d" (src))
+
+#define CVMX_MF_CACHE_ERR(val)            asm volatile ("dmfc0 %[rt],$27,0" :  [rt] "=d" (val):)
+#define CVMX_MF_DCACHE_ERR(val)           asm volatile ("dmfc0 %[rt],$27,1" :  [rt] "=d" (val):)
+#define CVMX_MF_CVM_MEM_CTL(val)          asm volatile ("dmfc0 %[rt],$11,7" :  [rt] "=d" (val):)
+#define CVMX_MF_CVM_CTL(val)              asm volatile ("dmfc0 %[rt],$9,7"  :  [rt] "=d" (val):)
+#define CVMX_MT_CACHE_ERR(val)            asm volatile ("dmtc0 %[rt],$27,0" : : [rt] "d" (val))
+#define CVMX_MT_DCACHE_ERR(val)           asm volatile ("dmtc0 %[rt],$27,1" : : [rt] "d" (val))
+#define CVMX_MT_CVM_MEM_CTL(val)          asm volatile ("dmtc0 %[rt],$11,7" : : [rt] "d" (val))
+#define CVMX_MT_CVM_CTL(val)              asm volatile ("dmtc0 %[rt],$9,7"  : : [rt] "d" (val))
+
+/* Macros for TLB */
+#define CVMX_TLBWI                       asm volatile ("tlbwi" : : )
+#define CVMX_TLBWR                       asm volatile ("tlbwr" : : )
+#define CVMX_TLBR                        asm volatile ("tlbr" : : )
+#define CVMX_MT_ENTRY_HIGH(val)          asm volatile ("dmtc0 %[rt],$10,0" : : [rt] "d" (val))
+#define CVMX_MT_ENTRY_LO_0(val)          asm volatile ("dmtc0 %[rt],$2,0" : : [rt] "d" (val))
+#define CVMX_MT_ENTRY_LO_1(val)          asm volatile ("dmtc0 %[rt],$3,0" : : [rt] "d" (val))
+#define CVMX_MT_PAGEMASK(val)            asm volatile ("mtc0 %[rt],$5,0" : : [rt] "d" (val))
+#define CVMX_MT_PAGEGRAIN(val)           asm volatile ("mtc0 %[rt],$5,1" : : [rt] "d" (val))
+#define CVMX_MT_TLB_INDEX(val)           asm volatile ("mtc0 %[rt],$0,0" : : [rt] "d" (val))
+#define CVMX_MT_TLB_CONTEXT(val)         asm volatile ("dmtc0 %[rt],$4,0" : : [rt] "d" (val))
+#define CVMX_MT_TLB_WIRED(val)           asm volatile ("mtc0 %[rt],$6,0" : : [rt] "d" (val))
+#define CVMX_MT_TLB_RANDOM(val)          asm volatile ("mtc0 %[rt],$1,0" : : [rt] "d" (val))
+#define CVMX_MF_ENTRY_LO_0(val)          asm volatile ("dmfc0 %[rt],$2,0" :  [rt] "=d" (val):)
+#define CVMX_MF_ENTRY_LO_1(val)          asm volatile ("dmfc0 %[rt],$3,0" :  [rt] "=d" (val):)
+#define CVMX_MF_ENTRY_HIGH(val)          asm volatile ("dmfc0 %[rt],$10,0" :  [rt] "=d" (val):)
+#define CVMX_MF_PAGEMASK(val)            asm volatile ("mfc0 %[rt],$5,0" :  [rt] "=d" (val):)
+#define CVMX_MF_PAGEGRAIN(val)           asm volatile ("mfc0 %[rt],$5,1" :  [rt] "=d" (val):)
+#define CVMX_MF_TLB_WIRED(val)           asm volatile ("mfc0 %[rt],$6,0" :  [rt] "=d" (val):)
+#define CVMX_MF_TLB_RANDOM(val)          asm volatile ("mfc0 %[rt],$1,0" :  [rt] "=d" (val):)
+#define TLB_DIRTY   (0x1ULL<<2)
+#define TLB_VALID   (0x1ULL<<1)
+#define TLB_GLOBAL  (0x1ULL<<0)
+
+
+
+/* assembler macros to guarantee byte loads/stores are used */
+/* for an unaligned 16-bit access (these use AT register) */
+/* we need the hidden argument (__a) so that GCC gets the dependencies right */
+#define CVMX_LOADUNA_INT16(result, address, offset) \
+	{ char *__a = (char *)(address); \
+	  asm ("ulh %[rdest], " CVMX_TMP_STR(offset) "(%[rbase])" : [rdest] "=d" (result) : [rbase] "d" (__a), "m"(__a[offset]), "m"(__a[offset + 1])); }
+#define CVMX_LOADUNA_UINT16(result, address, offset) \
+	{ char *__a = (char *)(address); \
+	  asm ("ulhu %[rdest], " CVMX_TMP_STR(offset) "(%[rbase])" : [rdest] "=d" (result) : [rbase] "d" (__a), "m"(__a[offset + 0]), "m"(__a[offset + 1])); }
+#define CVMX_STOREUNA_INT16(data, address, offset) \
+	{ char *__a = (char *)(address); \
+	  asm ("ush %[rsrc], " CVMX_TMP_STR(offset) "(%[rbase])" : "=m"(__a[offset + 0]), "=m"(__a[offset + 1]): [rsrc] "d" (data), [rbase] "d" (__a)); }
+
+#define CVMX_LOADUNA_INT32(result, address, offset) \
+	{ char *__a = (char *)(address); \
+	  asm ("ulw %[rdest], " CVMX_TMP_STR(offset) "(%[rbase])" : [rdest] "=d" (result) : \
+	       [rbase] "d" (__a), "m"(__a[offset + 0]), "m"(__a[offset + 1]), "m"(__a[offset + 2]), "m"(__a[offset + 3])); }
+#define CVMX_STOREUNA_INT32(data, address, offset) \
+	{ char *__a = (char *)(address); \
+	  asm ("usw %[rsrc], " CVMX_TMP_STR(offset) "(%[rbase])" : \
+	       "=m"(__a[offset + 0]), "=m"(__a[offset + 1]), "=m"(__a[offset + 2]), "=m"(__a[offset + 3]) : \
+	       [rsrc] "d" (data), [rbase] "d" (__a)); }
+
+#define CVMX_LOADUNA_INT64(result, address, offset) \
+	{ char *__a = (char *)(address); \
+	  asm ("uld %[rdest], " CVMX_TMP_STR(offset) "(%[rbase])" : [rdest] "=d" (result) : \
+	       [rbase] "d" (__a), "m"(__a[offset + 0]), "m"(__a[offset + 1]), "m"(__a[offset + 2]), "m"(__a[offset + 3]), \
+	       "m"(__a[offset + 4]), "m"(__a[offset + 5]), "m"(__a[offset + 6]), "m"(__a[offset + 7])); }
+#define CVMX_STOREUNA_INT64(data, address, offset) \
+	{ char *__a = (char *)(address); \
+	  asm ("usd %[rsrc], " CVMX_TMP_STR(offset) "(%[rbase])" : \
+	       "=m"(__a[offset + 0]), "=m"(__a[offset + 1]), "=m"(__a[offset + 2]), "=m"(__a[offset + 3]), \
+	       "=m"(__a[offset + 4]), "=m"(__a[offset + 5]), "=m"(__a[offset + 6]), "=m"(__a[offset + 7]) : \
+	       [rsrc] "d" (data), [rbase] "d" (__a)); }
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif /* __CVMX_ASM_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-asx.h b/arch/mips/cavium-octeon/executive/cvmx-asx.h
new file mode 100644
index 0000000..4bf1128
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-asx.h
@@ -0,0 +1,74 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Interface to the ASX hardware.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+
+#ifndef __CVMX_ASX_H__
+#define __CVMX_ASX_H__
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+/* CSR typedefs have been moved to cvmx-csr-*.h */
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-atomic.h b/arch/mips/cavium-octeon/executive/cvmx-atomic.h
new file mode 100644
index 0000000..0e97a05
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-atomic.h
@@ -0,0 +1,674 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * This file provides atomic operations
+ *
+ * <hr>$Revision: 32636 $<hr>
+ *
+ *
+ */
+
+
+#ifndef __CVMX_ATOMIC_H__
+#define __CVMX_ATOMIC_H__
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+
+/**
+ * Atomically adds a signed value to a 32 bit (aligned) memory location.
+ *
+ * This version does not perform 'sync' operations to enforce memory
+ * operations.  This should only be used when there are no memory operation
+ * ordering constraints.  (This should NOT be used for reference counting -
+ * use the standard version instead.)
+ *
+ * @param ptr    address in memory to add incr to
+ * @param incr   amount to increment memory location by (signed)
+ */
+static inline void cvmx_atomic_add32_nosync(int32_t *ptr, int32_t incr)
+{
+    if (OCTEON_IS_MODEL(OCTEON_CN3XXX))
+    {
+	uint32_t tmp;
+
+        __asm__ __volatile__(
+        ".set noreorder         \n"
+        "1: ll   %[tmp], %[val] \n"
+        "   addu %[tmp], %[inc] \n"
+        "   sc   %[tmp], %[val] \n"
+        "   beqz %[tmp], 1b     \n"
+        "   nop                 \n"
+        ".set reorder           \n"
+        : [val] "+m" (*ptr), [tmp] "=&r" (tmp)
+        : [inc] "r" (incr)
+        : "memory");
+    }
+    else
+    {
+        __asm__ __volatile__(
+        "   saa %[inc], (%[base]) \n"
+        : "+m" (*ptr)
+        : [inc] "r" (incr), [base] "r" (ptr)
+        : "memory");
+    }
+}
+
+/**
+ * Atomically adds a signed value to a 32 bit (aligned) memory location.
+ *
+ * Memory access ordering is enforced before/after the atomic operation,
+ * so no additional 'sync' instructions are required.
+ *
+ *
+ * @param ptr    address in memory to add incr to
+ * @param incr   amount to increment memory location by (signed)
+ */
+static inline void cvmx_atomic_add32(int32_t *ptr, int32_t incr)
+{
+    CVMX_SYNCWS;
+    cvmx_atomic_add32_nosync(ptr, incr);
+    CVMX_SYNCWS;
+}
+
+/**
+ * Atomically sets a 32 bit (aligned) memory location to a value
+ *
+ * @param ptr    address of memory to set
+ * @param value  value to set memory location to.
+ */
+static inline void cvmx_atomic_set32(int32_t *ptr, int32_t value)
+{
+    CVMX_SYNCWS;
+    *ptr = value;
+    CVMX_SYNCWS;
+}
+
+/**
+ * Returns the current value of a 32 bit (aligned) memory
+ * location.
+ *
+ * @param ptr    Address of memory to get
+ * @return Value of the memory
+ */
+static inline int32_t cvmx_atomic_get32(int32_t *ptr)
+{
+    return *(volatile int32_t *)ptr;
+}
+
+/**
+ * Atomically adds a signed value to a 64 bit (aligned) memory location.
+ *
+ * This version does not perform 'sync' operations to enforce memory
+ * operations.  This should only be used when there are no memory operation
+ * ordering constraints.  (This should NOT be used for reference counting -
+ * use the standard version instead.)
+ *
+ * @param ptr    address in memory to add incr to
+ * @param incr   amount to increment memory location by (signed)
+ */
+static inline void cvmx_atomic_add64_nosync(int64_t *ptr, int64_t incr)
+{
+    if (OCTEON_IS_MODEL(OCTEON_CN3XXX))
+    {
+	uint64_t tmp;
+        __asm__ __volatile__(
+        ".set noreorder         \n"
+        "1: lld  %[tmp], %[val] \n"
+        "   daddu %[tmp], %[inc] \n"
+        "   scd  %[tmp], %[val] \n"
+        "   beqz %[tmp], 1b     \n"
+        "   nop                 \n"
+        ".set reorder           \n"
+        : [val] "+m" (*ptr), [tmp] "=&r" (tmp)
+        : [inc] "r" (incr)
+        : "memory");
+    }
+    else
+    {
+        __asm__ __volatile__(
+        "   saad %[inc], (%[base])  \n"
+        : "+m" (*ptr)
+        : [inc] "r" (incr), [base] "r" (ptr)
+        : "memory");
+    }
+}
+
+/**
+ * Atomically adds a signed value to a 64 bit (aligned) memory location.
+ *
+ * Memory access ordering is enforced before/after the atomic operation,
+ * so no additional 'sync' instructions are required.
+ *
+ *
+ * @param ptr    address in memory to add incr to
+ * @param incr   amount to increment memory location by (signed)
+ */
+static inline void cvmx_atomic_add64(int64_t *ptr, int64_t incr)
+{
+    CVMX_SYNCWS;
+    cvmx_atomic_add64_nosync(ptr, incr);
+    CVMX_SYNCWS;
+}
+
+/**
+ * Atomically sets a 64 bit (aligned) memory location to a value
+ *
+ * @param ptr    address of memory to set
+ * @param value  value to set memory location to.
+ */
+static inline void cvmx_atomic_set64(int64_t *ptr, int64_t value)
+{
+    CVMX_SYNCWS;
+    *ptr = value;
+    CVMX_SYNCWS;
+}
+
+/**
+ * Returns the current value of a 64 bit (aligned) memory
+ * location.
+ *
+ * @param ptr    Address of memory to get
+ * @return Value of the memory
+ */
+static inline int64_t cvmx_atomic_get64(int64_t *ptr)
+{
+    return *(volatile int64_t *)ptr;
+}
+
+/**
+ * Atomically compares the old value with the value at ptr, and if they match,
+ * stores new_val to ptr.
+ * If *ptr and old don't match, function returns failure immediately.
+ * If *ptr and old match, function spins until *ptr updated to new atomically, or
+ *  until *ptr and old no longer match
+ *
+ * Does no memory synchronization.
+ *
+ * @return 1 on success (match and store)
+ *         0 on no match
+ */
+static inline uint32_t cvmx_atomic_compare_and_store32_nosync(uint32_t *ptr, uint32_t old_val, uint32_t new_val)
+{
+    uint32_t tmp, ret;
+
+    __asm__ __volatile__(
+    ".set noreorder         \n"
+    "1: ll   %[tmp], %[val] \n"
+    "   li   %[ret], 0     \n"
+    "   bne  %[tmp], %[old], 2f \n"
+    "   move %[tmp], %[new_val] \n"
+    "   sc   %[tmp], %[val] \n"
+    "   beqz %[tmp], 1b     \n"
+    "   li   %[ret], 1      \n"
+    "2: nop               \n"
+    ".set reorder           \n"
+    : [val] "+m" (*ptr), [tmp] "=&r" (tmp), [ret] "=&r" (ret)
+    : [old] "r" (old_val), [new_val] "r" (new_val)
+    : "memory");
+
+    return(ret);
+
+}
+
+/**
+ * Atomically compares the old value with the value at ptr, and if they match,
+ * stores new_val to ptr.
+ * If *ptr and old don't match, function returns failure immediately.
+ * If *ptr and old match, function spins until *ptr updated to new atomically, or
+ *  until *ptr and old no longer match
+ *
+ * Does memory synchronization that is required to use this as a locking primitive.
+ *
+ * @return 1 on success (match and store)
+ *         0 on no match
+ */
+static inline uint32_t cvmx_atomic_compare_and_store32(uint32_t *ptr, uint32_t old_val, uint32_t new_val)
+{
+    uint32_t ret;
+    CVMX_SYNCWS;
+    ret = cvmx_atomic_compare_and_store32_nosync(ptr, old_val, new_val);
+    CVMX_SYNCWS;
+    return ret;
+
+
+}
+
+/**
+ * Atomically compares the old value with the value at ptr, and if they match,
+ * stores new_val to ptr.
+ * If *ptr and old don't match, function returns failure immediately.
+ * If *ptr and old match, function spins until *ptr updated to new atomically, or
+ *  until *ptr and old no longer match
+ *
+ * Does no memory synchronization.
+ *
+ * @return 1 on success (match and store)
+ *         0 on no match
+ */
+static inline uint64_t cvmx_atomic_compare_and_store64_nosync(uint64_t *ptr, uint64_t old_val, uint64_t new_val)
+{
+    uint64_t tmp, ret;
+
+    __asm__ __volatile__(
+    ".set noreorder         \n"
+    "1: lld  %[tmp], %[val] \n"
+    "   li   %[ret], 0     \n"
+    "   bne  %[tmp], %[old], 2f \n"
+    "   move %[tmp], %[new_val] \n"
+    "   scd  %[tmp], %[val] \n"
+    "   beqz %[tmp], 1b     \n"
+    "   li   %[ret], 1      \n"
+    "2: nop               \n"
+    ".set reorder           \n"
+    : [val] "+m" (*ptr), [tmp] "=&r" (tmp), [ret] "=&r" (ret)
+    : [old] "r" (old_val), [new_val] "r" (new_val)
+    : "memory");
+
+    return(ret);
+
+}
+
+/**
+ * Atomically compares the old value with the value at ptr, and if they match,
+ * stores new_val to ptr.
+ * If *ptr and old don't match, function returns failure immediately.
+ * If *ptr and old match, function spins until *ptr updated to new atomically, or
+ *  until *ptr and old no longer match
+ *
+ * Does memory synchronization that is required to use this as a locking primitive.
+ *
+ * @return 1 on success (match and store)
+ *         0 on no match
+ */
+static inline uint64_t cvmx_atomic_compare_and_store64(uint64_t *ptr, uint64_t old_val, uint64_t new_val)
+{
+    uint64_t ret;
+    CVMX_SYNCWS;
+    ret = cvmx_atomic_compare_and_store64_nosync(ptr, old_val, new_val);
+    CVMX_SYNCWS;
+    return ret;
+}
+
+/**
+ * Atomically adds a signed value to a 64 bit (aligned) memory location,
+ * and returns previous value.
+ *
+ * This version does not perform 'sync' operations to enforce memory
+ * operations.  This should only be used when there are no memory operation
+ * ordering constraints.  (This should NOT be used for reference counting -
+ * use the standard version instead.)
+ *
+ * @param ptr    address in memory to add incr to
+ * @param incr   amount to increment memory location by (signed)
+ *
+ * @return Value of memory location before increment
+ */
+static inline int64_t cvmx_atomic_fetch_and_add64_nosync(int64_t *ptr, int64_t incr)
+{
+    uint64_t tmp, ret;
+
+    __asm__ __volatile__(
+    ".set noreorder          \n"
+    "1: lld   %[tmp], %[val] \n"
+    "   move  %[ret], %[tmp] \n"
+    "   daddu %[tmp], %[inc] \n"
+    "   scd   %[tmp], %[val] \n"
+    "   beqz  %[tmp], 1b     \n"
+    "   nop                  \n"
+    ".set reorder            \n"
+    : [val] "+m" (*ptr), [tmp] "=&r" (tmp), [ret] "=&r" (ret)
+    : [inc] "r" (incr)
+    : "memory");
+
+    return (ret);
+}
+
+/**
+ * Atomically adds a signed value to a 64 bit (aligned) memory location,
+ * and returns previous value.
+ *
+ * Memory access ordering is enforced before/after the atomic operation,
+ * so no additional 'sync' instructions are required.
+ *
+ * @param ptr    address in memory to add incr to
+ * @param incr   amount to increment memory location by (signed)
+ *
+ * @return Value of memory location before increment
+ */
+static inline int64_t cvmx_atomic_fetch_and_add64(int64_t *ptr, int64_t incr)
+{
+    uint64_t ret;
+    CVMX_SYNCWS;
+    ret = cvmx_atomic_fetch_and_add64_nosync(ptr, incr);
+    CVMX_SYNCWS;
+    return ret;
+}
+
+/**
+ * Atomically adds a signed value to a 32 bit (aligned) memory location,
+ * and returns previous value.
+ *
+ * This version does not perform 'sync' operations to enforce memory
+ * operations.  This should only be used when there are no memory operation
+ * ordering constraints.  (This should NOT be used for reference counting -
+ * use the standard version instead.)
+ *
+ * @param ptr    address in memory to add incr to
+ * @param incr   amount to increment memory location by (signed)
+ *
+ * @return Value of memory location before increment
+ */
+static inline int32_t cvmx_atomic_fetch_and_add32_nosync(int32_t *ptr, int32_t incr)
+{
+    uint32_t tmp, ret;
+
+    __asm__ __volatile__(
+    ".set noreorder         \n"
+    "1: ll   %[tmp], %[val] \n"
+    "   move %[ret], %[tmp] \n"
+    "   addu %[tmp], %[inc] \n"
+    "   sc   %[tmp], %[val] \n"
+    "   beqz %[tmp], 1b     \n"
+    "   nop                 \n"
+    ".set reorder           \n"
+    : [val] "+m" (*ptr), [tmp] "=&r" (tmp), [ret] "=&r" (ret)
+    : [inc] "r" (incr)
+    : "memory");
+
+    return (ret);
+}
+
+/**
+ * Atomically adds a signed value to a 32 bit (aligned) memory location,
+ * and returns previous value.
+ *
+ * Memory access ordering is enforced before/after the atomic operation,
+ * so no additional 'sync' instructions are required.
+ *
+ * @param ptr    address in memory to add incr to
+ * @param incr   amount to increment memory location by (signed)
+ *
+ * @return Value of memory location before increment
+ */
+static inline int32_t cvmx_atomic_fetch_and_add32(int32_t *ptr, int32_t incr)
+{
+    uint32_t ret;
+    CVMX_SYNCWS;
+    ret = cvmx_atomic_fetch_and_add32_nosync(ptr, incr);
+    CVMX_SYNCWS;
+    return ret;
+}
+
+/**
+ * Atomically set bits in a 64 bit (aligned) memory location,
+ * and returns previous value.
+ *
+ * This version does not perform 'sync' operations to enforce memory
+ * operations.  This should only be used when there are no memory operation
+ * ordering constraints.
+ *
+ * @param ptr    address in memory
+ * @param mask   mask of bits to set
+ *
+ * @return Value of memory location before setting bits
+ */
+static inline uint64_t cvmx_atomic_fetch_and_bset64_nosync(uint64_t *ptr, uint64_t mask)
+{
+    uint64_t tmp, ret;
+
+    __asm__ __volatile__(
+    ".set noreorder         \n"
+    "1: lld  %[tmp], %[val] \n"
+    "   move %[ret], %[tmp] \n"
+    "   or   %[tmp], %[msk] \n"
+    "   scd  %[tmp], %[val] \n"
+    "   beqz %[tmp], 1b     \n"
+    "   nop                 \n"
+    ".set reorder           \n"
+    : [val] "+m" (*ptr), [tmp] "=&r" (tmp), [ret] "=&r" (ret)
+    : [msk] "r" (mask)
+    : "memory");
+
+    return (ret);
+}
+
+/**
+ * Atomically set bits in a 32 bit (aligned) memory location,
+ * and returns previous value.
+ *
+ * This version does not perform 'sync' operations to enforce memory
+ * operations.  This should only be used when there are no memory operation
+ * ordering constraints.
+ *
+ * @param ptr    address in memory
+ * @param mask   mask of bits to set
+ *
+ * @return Value of memory location before setting bits
+ */
+static inline uint32_t cvmx_atomic_fetch_and_bset32_nosync(uint32_t *ptr, uint32_t mask)
+{
+    uint32_t tmp, ret;
+
+    __asm__ __volatile__(
+    ".set noreorder         \n"
+    "1: ll   %[tmp], %[val] \n"
+    "   move %[ret], %[tmp] \n"
+    "   or   %[tmp], %[msk] \n"
+    "   sc   %[tmp], %[val] \n"
+    "   beqz %[tmp], 1b     \n"
+    "   nop                 \n"
+    ".set reorder           \n"
+    : [val] "+m" (*ptr), [tmp] "=&r" (tmp), [ret] "=&r" (ret)
+    : [msk] "r" (mask)
+    : "memory");
+
+    return (ret);
+}
+
+/**
+ * Atomically clear bits in a 64 bit (aligned) memory location,
+ * and returns previous value.
+ *
+ * This version does not perform 'sync' operations to enforce memory
+ * operations.  This should only be used when there are no memory operation
+ * ordering constraints.
+ *
+ * @param ptr    address in memory
+ * @param mask   mask of bits to clear
+ *
+ * @return Value of memory location before clearing bits
+ */
+static inline uint64_t cvmx_atomic_fetch_and_bclr64_nosync(uint64_t *ptr, uint64_t mask)
+{
+    uint64_t tmp, ret;
+
+    __asm__ __volatile__(
+    ".set noreorder         \n"
+    "   nor  %[msk], 0      \n"
+    "1: lld  %[tmp], %[val] \n"
+    "   move %[ret], %[tmp] \n"
+    "   and  %[tmp], %[msk] \n"
+    "   scd  %[tmp], %[val] \n"
+    "   beqz %[tmp], 1b     \n"
+    "   nop                 \n"
+    ".set reorder           \n"
+    : [val] "+m" (*ptr), [tmp] "=&r" (tmp), [ret] "=&r" (ret)
+    : [msk] "r" (mask)
+    : "memory");
+
+    return (ret);
+}
+
+/**
+ * Atomically clear bits in a 32 bit (aligned) memory location,
+ * and returns previous value.
+ *
+ * This version does not perform 'sync' operations to enforce memory
+ * operations.  This should only be used when there are no memory operation
+ * ordering constraints.
+ *
+ * @param ptr    address in memory
+ * @param mask   mask of bits to clear
+ *
+ * @return Value of memory location before clearing bits
+ */
+static inline uint32_t cvmx_atomic_fetch_and_bclr32_nosync(uint32_t *ptr, uint32_t mask)
+{
+    uint32_t tmp, ret;
+
+    __asm__ __volatile__(
+    ".set noreorder         \n"
+    "   nor  %[msk], 0      \n"
+    "1: ll   %[tmp], %[val] \n"
+    "   move %[ret], %[tmp] \n"
+    "   and  %[tmp], %[msk] \n"
+    "   sc   %[tmp], %[val] \n"
+    "   beqz %[tmp], 1b     \n"
+    "   nop                 \n"
+    ".set reorder           \n"
+    : [val] "+m" (*ptr), [tmp] "=&r" (tmp), [ret] "=&r" (ret)
+    : [msk] "r" (mask)
+    : "memory");
+
+    return (ret);
+}
+
+/**
+ * Atomically swaps value in 64 bit (aligned) memory location,
+ * and returns previous value.
+ *
+ * This version does not perform 'sync' operations to enforce memory
+ * operations.  This should only be used when there are no memory operation
+ * ordering constraints.
+ *
+ * @param ptr       address in memory
+ * @param new_val   new value to write
+ *
+ * @return Value of memory location before swap operation
+ */
+static inline uint64_t cvmx_atomic_swap64_nosync(uint64_t *ptr, uint64_t new_val)
+{
+    uint64_t tmp, ret;
+
+    __asm__ __volatile__(
+    ".set noreorder         \n"
+    "1: lld  %[ret], %[val] \n"
+    "   move %[tmp], %[new_val] \n"
+    "   scd  %[tmp], %[val] \n"
+    "   beqz %[tmp],  1b    \n"
+    "   nop                 \n"
+    ".set reorder           \n"
+    : [val] "+m" (*ptr), [tmp] "=&r" (tmp), [ret] "=&r" (ret)
+    : [new_val] "r"  (new_val)
+    : "memory");
+
+    return (ret);
+}
+
+/**
+ * Atomically swaps value in 32 bit (aligned) memory location,
+ * and returns previous value.
+ *
+ * This version does not perform 'sync' operations to enforce memory
+ * operations.  This should only be used when there are no memory operation
+ * ordering constraints.
+ *
+ * @param ptr       address in memory
+ * @param new_val   new value to write
+ *
+ * @return Value of memory location before swap operation
+ */
+static inline uint32_t cvmx_atomic_swap32_nosync(uint32_t *ptr, uint32_t new_val)
+{
+    uint32_t tmp, ret;
+
+    __asm__ __volatile__(
+    ".set noreorder         \n"
+    "1: ll   %[ret], %[val] \n"
+    "   move %[tmp], %[new_val] \n"
+    "   sc   %[tmp], %[val] \n"
+    "   beqz %[tmp],  1b    \n"
+    "   nop                 \n"
+    ".set reorder           \n"
+    : [val] "+m" (*ptr), [tmp] "=&r" (tmp), [ret] "=&r" (ret)
+    : [new_val] "r"  (new_val)
+    : "memory");
+
+    return (ret);
+}
+
+/**
+ * This atomic operation is now named cvmx_atomic_compare_and_store32_nosync
+ * and the (deprecated) macro is provided for backward compatibility.
+ * @deprecated
+ */
+#define cvmx_atomic_compare_and_store_nosync32 cvmx_atomic_compare_and_store32_nosync
+
+/**
+ * This atomic operation is now named cvmx_atomic_compare_and_store64_nosync
+ * and the (deprecated) macro is provided for backward compatibility.
+ * @deprecated
+ */
+#define cvmx_atomic_compare_and_store_nosync64 cvmx_atomic_compare_and_store64_nosync
+
+
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif /* __CVMX_ATOMIC_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-bootmem.c b/arch/mips/cavium-octeon/executive/cvmx-bootmem.c
new file mode 100644
index 0000000..e27564d
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-bootmem.c
@@ -0,0 +1,932 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+/**
+ * @file
+ * Simple allocate only memory allocator.  Used to allocate memory at application
+ * start time.
+ *
+ * <hr>$Revision: 34615 $<hr>
+ *
+ */
+
+#include "cvmx.h"
+#include "cvmx-spinlock.h"
+#include "cvmx-bootmem.h"
+
+
+//#define DEBUG
+
+
+#undef	MAX
+#define MAX(a, b)  (((a) > (b)) ? (a) : (b))
+
+#undef	MIN
+#define MIN(a, b)  (((a) < (b)) ? (a) : (b))
+
+#define ALIGN_ADDR_UP(addr, align)     (((addr) + (~(align))) & (align))
+
+static cvmx_bootmem_desc_t *cvmx_bootmem_desc = NULL;
+
+/* See header file for descriptions of functions */
+
+/* Wrapper functions are provided for reading/writing the size and next block
+** values as these may not be directly addressible (in 32 bit applications, for instance.)
+*/
+/* Offsets of data elements in bootmem list, must match cvmx_bootmem_block_header_t */
+#define NEXT_OFFSET 0
+#define SIZE_OFFSET 8
+static void cvmx_bootmem_phy_set_size(uint64_t addr, uint64_t size)
+{
+    cvmx_write64_uint64((addr + SIZE_OFFSET) | (1ull << 63), size);
+}
+static void cvmx_bootmem_phy_set_next(uint64_t addr, uint64_t next)
+{
+    cvmx_write64_uint64((addr + NEXT_OFFSET) | (1ull << 63), next);
+}
+static uint64_t cvmx_bootmem_phy_get_size(uint64_t addr)
+{
+    return(cvmx_read64_uint64((addr + SIZE_OFFSET) | (1ull << 63)));
+}
+static uint64_t cvmx_bootmem_phy_get_next(uint64_t addr)
+{
+    return(cvmx_read64_uint64((addr + NEXT_OFFSET) | (1ull << 63)));
+}
+
+void *cvmx_bootmem_alloc_range(uint64_t size, uint64_t alignment, uint64_t min_addr, uint64_t max_addr)
+{
+    int64_t address;
+    address = cvmx_bootmem_phy_alloc(size, min_addr, max_addr, alignment, 0);
+
+    if (address > 0)
+        return cvmx_phys_to_ptr(address);
+    else
+        return NULL;
+}
+
+void *cvmx_bootmem_alloc_address(uint64_t size, uint64_t address, uint64_t alignment)
+{
+    return cvmx_bootmem_alloc_range(size, alignment, address, address + size);
+}
+
+
+void *cvmx_bootmem_alloc(uint64_t size, uint64_t alignment)
+{
+    return cvmx_bootmem_alloc_range(size, alignment, 0, 0);
+}
+
+void *cvmx_bootmem_alloc_named_range(uint64_t size, uint64_t min_addr, uint64_t max_addr, uint64_t align, char *name)
+{
+    int64_t addr;
+
+    addr = cvmx_bootmem_phy_named_block_alloc(size, min_addr, max_addr, align, name, 0);
+    if (addr >= 0)
+        return cvmx_phys_to_ptr(addr);
+    else
+        return NULL;
+
+}
+void *cvmx_bootmem_alloc_named_address(uint64_t size, uint64_t address, char *name)
+{
+    return(cvmx_bootmem_alloc_named_range(size, address, address + size, 0, name));
+}
+void *cvmx_bootmem_alloc_named(uint64_t size, uint64_t alignment, char *name)
+{
+    return(cvmx_bootmem_alloc_named_range(size, 0, 0, alignment, name));
+}
+
+int cvmx_bootmem_free_named(char *name)
+{
+    return(cvmx_bootmem_phy_named_block_free(name, 0));
+}
+
+cvmx_bootmem_named_block_desc_t * cvmx_bootmem_find_named_block(char *name)
+{
+    return(cvmx_bootmem_phy_named_block_find(name, 0));
+}
+
+void cvmx_bootmem_print_named(void)
+{
+    cvmx_bootmem_phy_named_block_print();
+}
+
+#if defined(__linux__) && defined(CVMX_ABI_N32)
+cvmx_bootmem_named_block_desc_t *linux32_named_block_array_ptr;
+#endif
+
+int cvmx_bootmem_init(void *mem_desc_ptr)
+{
+    /* Verify that the size of cvmx_spinlock_t meets our assumptions */
+    if (sizeof(cvmx_spinlock_t) != 4)
+    {
+        cvmx_dprintf("ERROR: Unexpected size of cvmx_spinlock_t\n");
+        return(-1);
+    }
+
+    /* Here we set the global pointer to the bootmem descriptor block.  This pointer will
+    ** be used directly, so we will set it up to be directly usable by the application.
+    ** It is set up as follows for the various runtime/ABI combinations:
+    ** Linux 64 bit: Set XKPHYS bit
+    ** Linux 32 bit: use mmap to create mapping, use virtual address
+    ** CVMX 64 bit:  use physical address directly
+    ** CVMX 32 bit:  use physical address directly
+    ** Note that the CVMX environment assumes the use of 1-1 TLB mappings so that the physical addresses
+    ** can be used directly
+    */
+    if (!cvmx_bootmem_desc)
+    {
+#if defined(CVMX_BUILD_FOR_LINUX_USER) && defined(CVMX_ABI_N32)
+        void *base_ptr;
+        /* For 32 bit, we need to use mmap to create a mapping for the bootmem descriptor */
+        int dm_fd = open("/dev/mem", O_RDWR);
+        if (dm_fd < 0)
+        {
+            cvmx_dprintf("ERROR opening /dev/mem for boot descriptor mapping\n");
+            return(-1);
+        }
+
+        base_ptr = mmap(NULL,
+                        sizeof(cvmx_bootmem_desc_t) + sysconf(_SC_PAGESIZE),
+                        PROT_READ | PROT_WRITE,
+                        MAP_SHARED,
+                        dm_fd,
+                        ((off_t)mem_desc_ptr) & ~(sysconf(_SC_PAGESIZE) - 1));
+
+        if (MAP_FAILED == base_ptr)
+        {
+            cvmx_dprintf("Error mapping bootmem descriptor!\n");
+            close(dm_fd);
+            return(-1);
+        }
+
+        /* Adjust pointer to point to bootmem_descriptor, rather than start of page it is in */
+        cvmx_bootmem_desc =  (cvmx_bootmem_desc_t*)((char*)base_ptr + (((off_t)mem_desc_ptr) & (sysconf(_SC_PAGESIZE) - 1)));
+
+        /* Also setup mapping for named memory block desc. while we are at it.  Here we must keep another
+        ** pointer around, as the value in the bootmem descriptor is shared with other applications. */
+        base_ptr = mmap(NULL,
+                        sizeof(cvmx_bootmem_named_block_desc_t) * cvmx_bootmem_desc->named_block_num_blocks + sysconf(_SC_PAGESIZE),
+                        PROT_READ | PROT_WRITE,
+                        MAP_SHARED,
+                        dm_fd,
+                        ((off_t)cvmx_bootmem_desc->named_block_array_addr) & ~(sysconf(_SC_PAGESIZE) - 1));
+
+        close(dm_fd);
+
+        if (MAP_FAILED == base_ptr)
+        {
+            cvmx_dprintf("Error mapping named block descriptor!\n");
+            return(-1);
+        }
+
+        /* Adjust pointer to point to named block array, rather than start of page it is in */
+        linux32_named_block_array_ptr = (cvmx_bootmem_named_block_desc_t*)((char*)base_ptr + (((off_t)cvmx_bootmem_desc->named_block_array_addr) & (sysconf(_SC_PAGESIZE) - 1)));
+
+#elif (defined(CVMX_BUILD_FOR_LINUX_KERNEL) || defined(CVMX_BUILD_FOR_LINUX_USER)) && defined(CVMX_ABI_64)
+        /* Set XKPHYS bit */
+        cvmx_bootmem_desc = cvmx_phys_to_ptr(CAST64(mem_desc_ptr));
+#else
+        cvmx_bootmem_desc = (cvmx_bootmem_desc_t*)mem_desc_ptr;
+#endif
+    }
+
+
+    return(0);
+}
+
+
+uint64_t cvmx_bootmem_available_mem(uint64_t min_block_size)
+{
+    return(cvmx_bootmem_phy_available_mem(min_block_size));
+}
+
+
+
+
+
+/*********************************************************************
+** The cvmx_bootmem_phy* functions below return 64 bit physical addresses,
+** and expose more features that the cvmx_bootmem_functions above.  These are
+** required for full memory space access in 32 bit applications, as well as for
+** using some advance features.
+** Most applications should not need to use these.
+**
+**/
+
+
+int64_t cvmx_bootmem_phy_alloc(uint64_t req_size, uint64_t address_min, uint64_t address_max, uint64_t alignment, uint32_t flags)
+{
+
+    uint64_t head_addr;
+    uint64_t ent_addr;
+    uint64_t prev_addr = 0;  /* points to previous list entry, NULL current entry is head of list */
+    uint64_t new_ent_addr = 0;
+    uint64_t desired_min_addr;
+    uint64_t alignment_mask = ~(alignment - 1);
+
+#ifdef DEBUG
+    cvmx_dprintf("cvmx_bootmem_phy_alloc: req_size: 0x%llx, min_addr: 0x%llx, max_addr: 0x%llx, align: 0x%llx\n",
+           (unsigned long long)req_size, (unsigned long long)address_min, (unsigned long long)address_max, (unsigned long long)alignment);
+#endif
+
+    if (cvmx_bootmem_desc->major_version > 3)
+    {
+        cvmx_dprintf("ERROR: Incompatible bootmem descriptor version: %d.%d at addr: %p\n",
+               (int)cvmx_bootmem_desc->major_version, (int)cvmx_bootmem_desc->minor_version, cvmx_bootmem_desc);
+        goto error_out;
+    }
+
+    /* Do a variety of checks to validate the arguments.  The allocator code will later assume
+    ** that these checks have been made.  We validate that the requested constraints are not
+    ** self-contradictory before we look through the list of available memory
+    */
+
+    /* 0 is not a valid req_size for this allocator */
+    if (!req_size)
+        goto error_out;
+
+    /* Round req_size up to mult of minimum alignment bytes */
+    req_size = (req_size + (CVMX_BOOTMEM_ALIGNMENT_SIZE - 1)) & ~(CVMX_BOOTMEM_ALIGNMENT_SIZE - 1);
+
+    /* Convert !0 address_min and 0 address_max to special case of range that specifies an exact
+    ** memory block to allocate.  Do this before other checks and adjustments so that this tranformation will be validated */
+    if (address_min && !address_max)
+        address_max = address_min + req_size;
+    else if (!address_min && !address_max)
+        address_max = ~0ull;   /* If no limits given, use max limits */
+
+
+#if defined(__linux__) && defined(CVMX_ABI_N32) && !defined(CONFIG_OCTEON_U_BOOT)
+    {
+        extern uint64_t linux_mem32_min;
+        extern uint64_t linux_mem32_max;
+        cvmx_dprintf("min: 0x%llx, max: 0x%llx\n", linux_mem32_min, linux_mem32_max);
+        /* For 32 bit Linux apps, we need to restrict the allocations to the range
+        ** of memory configured for access from userspace.  Also, we need to add mappings
+        ** for the data structures that we access.*/
+
+        /* Reject specific location requests that are not fully within bounds */
+        if (req_size == address_max - address_min && ((address_min > linux_mem32_max || address_min < linux_mem32_min)))
+            goto error_out;
+
+        /* Narrow range requests to be bounded by the 32 bit limits.  octeon_phy_mem_block_alloc()
+        ** will reject inconsistent req_size/range requests, so we don't repeat those checks here.
+        ** If max unspecified, set to 32 bit maximum. */
+        address_min = MIN(MAX(address_min, linux_mem32_min), linux_mem32_max);
+        if (!address_max)
+            address_max = linux_mem32_max;
+        else
+            address_max = MAX(MIN(address_max, linux_mem32_max), linux_mem32_min);
+    }
+#endif
+
+
+    /* Enforce minimum alignment (this also keeps the minimum free block
+    ** req_size the same as the alignment req_size */
+    if (alignment < CVMX_BOOTMEM_ALIGNMENT_SIZE)
+    {
+        alignment = CVMX_BOOTMEM_ALIGNMENT_SIZE;
+    }
+    alignment_mask = ~(alignment - 1);
+
+    /* Adjust address minimum based on requested alignment (round up to meet alignment).  Do this here so we can
+    ** reject impossible requests up front. (NOP for address_min == 0) */
+    if (alignment)
+        address_min = (address_min + (alignment - 1)) & ~(alignment - 1);
+
+
+    /* Reject inconsistent args.  We have adjusted these, so this may fail due to our internal changes
+    ** even if this check would pass for the values the user supplied. */
+    if (req_size > address_max - address_min)
+        goto error_out;
+
+    /* Walk through the list entries - first fit found is returned */
+
+    if (!(flags & CVMX_BOOTMEM_FLAG_NO_LOCKING))
+        cvmx_spinlock_lock((cvmx_spinlock_t *)&(cvmx_bootmem_desc->lock));
+    head_addr = cvmx_bootmem_desc->head_addr;
+    ent_addr = head_addr;
+    while (ent_addr)
+    {
+        uint64_t usable_base, usable_max;
+        uint64_t ent_size = cvmx_bootmem_phy_get_size(ent_addr);
+
+        if (cvmx_bootmem_phy_get_next(ent_addr) && ent_addr > cvmx_bootmem_phy_get_next(ent_addr))
+        {
+            cvmx_dprintf("Internal bootmem_alloc() error: ent: 0x%llx, next: 0x%llx\n",
+                   (unsigned long long)ent_addr, (unsigned long long)cvmx_bootmem_phy_get_next(ent_addr));
+            goto error_out;
+        }
+
+        /* Determine if this is an entry that can satisify the request */
+        /* Check to make sure entry is large enough to satisfy request */
+        usable_base = ALIGN_ADDR_UP(MAX(address_min, ent_addr), alignment_mask);
+        usable_max = MIN(address_max, ent_addr + ent_size);
+        /* We should be able to allocate block at address usable_base */
+
+        desired_min_addr = usable_base;
+
+        /* Determine if request can be satisfied from the current entry */
+        if ((((ent_addr + ent_size) > usable_base && ent_addr < address_max))
+            && req_size <= usable_max - usable_base)
+        {
+            /* We have found an entry that has room to satisfy the request, so allocate it from this entry */
+
+            /* If end CVMX_BOOTMEM_FLAG_END_ALLOC set, then allocate from the end of this block
+            ** rather than the beginning */
+            if (flags & CVMX_BOOTMEM_FLAG_END_ALLOC)
+            {
+                desired_min_addr = usable_max - req_size;
+                /* Align desired address down to required alignment */
+                desired_min_addr &= alignment_mask;
+            }
+
+            /* Match at start of entry */
+            if (desired_min_addr == ent_addr)
+            {
+                if (req_size < ent_size)
+                {
+                    /* big enough to create a new block from top portion of block */
+                    new_ent_addr = ent_addr + req_size;
+                    cvmx_bootmem_phy_set_next(new_ent_addr, cvmx_bootmem_phy_get_next(ent_addr));
+                    cvmx_bootmem_phy_set_size(new_ent_addr, ent_size - req_size);
+
+                    /* Adjust next pointer as following code uses this */
+                    cvmx_bootmem_phy_set_next(ent_addr, new_ent_addr);
+                }
+
+                /* adjust prev ptr or head to remove this entry from list */
+                if (prev_addr)
+                {
+                    cvmx_bootmem_phy_set_next(prev_addr, cvmx_bootmem_phy_get_next(ent_addr));
+                }
+                else
+                {
+                    /* head of list being returned, so update head ptr */
+                    cvmx_bootmem_desc->head_addr = cvmx_bootmem_phy_get_next(ent_addr);
+                }
+                if (!(flags & CVMX_BOOTMEM_FLAG_NO_LOCKING))
+                    cvmx_spinlock_unlock((cvmx_spinlock_t *)&(cvmx_bootmem_desc->lock));
+                return(desired_min_addr);
+            }
+
+
+            /* block returned doesn't start at beginning of entry, so we know
+            ** that we will be splitting a block off the front of this one.  Create a new block
+            ** from the beginning, add to list, and go to top of loop again.
+            **
+            ** create new block from high portion of block, so that top block
+            ** starts at desired addr
+            **/
+            new_ent_addr = desired_min_addr;
+            cvmx_bootmem_phy_set_next(new_ent_addr, cvmx_bootmem_phy_get_next(ent_addr));
+            cvmx_bootmem_phy_set_size(new_ent_addr, cvmx_bootmem_phy_get_size(ent_addr) - (desired_min_addr - ent_addr));
+            cvmx_bootmem_phy_set_size(ent_addr, desired_min_addr - ent_addr);
+            cvmx_bootmem_phy_set_next(ent_addr, new_ent_addr);
+            /* Loop again to handle actual alloc from new block */
+        }
+
+        prev_addr = ent_addr;
+        ent_addr = cvmx_bootmem_phy_get_next(ent_addr);
+    }
+error_out:
+    /* We didn't find anything, so return error */
+    if (!(flags & CVMX_BOOTMEM_FLAG_NO_LOCKING))
+        cvmx_spinlock_unlock((cvmx_spinlock_t *)&(cvmx_bootmem_desc->lock));
+    return(-1);
+}
+
+
+
+int __cvmx_bootmem_phy_free(uint64_t phy_addr, uint64_t size, uint32_t flags)
+{
+    uint64_t cur_addr;
+    uint64_t prev_addr = 0;  /* zero is invalid */
+    int retval = 0;
+
+#ifdef DEBUG
+    cvmx_dprintf("__cvmx_bootmem_phy_free addr: 0x%llx, size: 0x%llx\n", (unsigned long long)phy_addr, (unsigned long long)size);
+#endif
+    if (cvmx_bootmem_desc->major_version > 3)
+    {
+        cvmx_dprintf("ERROR: Incompatible bootmem descriptor version: %d.%d at addr: %p\n",
+               (int)cvmx_bootmem_desc->major_version, (int)cvmx_bootmem_desc->minor_version, cvmx_bootmem_desc);
+        return(0);
+    }
+
+    /* 0 is not a valid size for this allocator */
+    if (!size)
+        return(0);
+
+
+    if (!(flags & CVMX_BOOTMEM_FLAG_NO_LOCKING))
+        cvmx_spinlock_lock((cvmx_spinlock_t *)&(cvmx_bootmem_desc->lock));
+    cur_addr = cvmx_bootmem_desc->head_addr;
+    if (cur_addr == 0 || phy_addr < cur_addr)
+    {
+        /* add at front of list - special case with changing head ptr */
+        if (cur_addr && phy_addr + size > cur_addr)
+            goto bootmem_free_done; /* error, overlapping section */
+        else if (phy_addr + size == cur_addr)
+        {
+            /* Add to front of existing first block */
+            cvmx_bootmem_phy_set_next(phy_addr, cvmx_bootmem_phy_get_next(cur_addr));
+            cvmx_bootmem_phy_set_size(phy_addr, cvmx_bootmem_phy_get_size(cur_addr) + size);
+            cvmx_bootmem_desc->head_addr = phy_addr;
+
+        }
+        else
+        {
+            /* New block before first block */
+            cvmx_bootmem_phy_set_next(phy_addr, cur_addr);  /* OK if cur_addr is 0 */
+            cvmx_bootmem_phy_set_size(phy_addr, size);
+            cvmx_bootmem_desc->head_addr = phy_addr;
+        }
+        retval = 1;
+        goto bootmem_free_done;
+    }
+
+    /* Find place in list to add block */
+    while (cur_addr && phy_addr > cur_addr)
+    {
+        prev_addr = cur_addr;
+        cur_addr = cvmx_bootmem_phy_get_next(cur_addr);
+    }
+
+    if (!cur_addr)
+    {
+        /* We have reached the end of the list, add on to end, checking
+        ** to see if we need to combine with last block
+        **/
+        if (prev_addr +  cvmx_bootmem_phy_get_size(prev_addr) == phy_addr)
+        {
+            cvmx_bootmem_phy_set_size(prev_addr, cvmx_bootmem_phy_get_size(prev_addr) + size);
+        }
+        else
+        {
+            cvmx_bootmem_phy_set_next(prev_addr, phy_addr);
+            cvmx_bootmem_phy_set_size(phy_addr, size);
+            cvmx_bootmem_phy_set_next(phy_addr, 0);
+        }
+        retval = 1;
+        goto bootmem_free_done;
+    }
+    else
+    {
+        /* insert between prev and cur nodes, checking for merge with either/both */
+
+        if (prev_addr +  cvmx_bootmem_phy_get_size(prev_addr) == phy_addr)
+        {
+            /* Merge with previous */
+            cvmx_bootmem_phy_set_size(prev_addr, cvmx_bootmem_phy_get_size(prev_addr) + size);
+            if (phy_addr + size == cur_addr)
+            {
+                /* Also merge with current */
+                cvmx_bootmem_phy_set_size(prev_addr, cvmx_bootmem_phy_get_size(cur_addr) + cvmx_bootmem_phy_get_size(prev_addr));
+                cvmx_bootmem_phy_set_next(prev_addr, cvmx_bootmem_phy_get_next(cur_addr));
+            }
+            retval = 1;
+            goto bootmem_free_done;
+        }
+        else if (phy_addr + size == cur_addr)
+        {
+            /* Merge with current */
+            cvmx_bootmem_phy_set_size(phy_addr, cvmx_bootmem_phy_get_size(cur_addr) + size);
+            cvmx_bootmem_phy_set_next(phy_addr, cvmx_bootmem_phy_get_next(cur_addr));
+            cvmx_bootmem_phy_set_next(prev_addr, phy_addr);
+            retval = 1;
+            goto bootmem_free_done;
+        }
+
+        /* It is a standalone block, add in between prev and cur */
+        cvmx_bootmem_phy_set_size(phy_addr, size);
+        cvmx_bootmem_phy_set_next(phy_addr, cur_addr);
+        cvmx_bootmem_phy_set_next(prev_addr, phy_addr);
+
+
+    }
+    retval = 1;
+
+bootmem_free_done:
+    if (!(flags & CVMX_BOOTMEM_FLAG_NO_LOCKING))
+        cvmx_spinlock_unlock((cvmx_spinlock_t *)&(cvmx_bootmem_desc->lock));
+    return(retval);
+
+}
+
+
+
+void cvmx_bootmem_phy_list_print(void)
+{
+    uint64_t addr;
+
+    addr = cvmx_bootmem_desc->head_addr;
+    cvmx_dprintf("\n\n\nPrinting bootmem block list, descriptor: %p,  head is 0x%llx\n",
+           cvmx_bootmem_desc, (unsigned long long)addr);
+    cvmx_dprintf("Descriptor version: %d.%d\n", (int)cvmx_bootmem_desc->major_version, (int)cvmx_bootmem_desc->minor_version);
+    if (cvmx_bootmem_desc->major_version > 3)
+    {
+        cvmx_dprintf("Warning: Bootmem descriptor version is newer than expected\n");
+    }
+    if (!addr)
+    {
+        cvmx_dprintf("mem list is empty!\n");
+    }
+    while (addr)
+    {
+        cvmx_dprintf("Block address: 0x%08qx, size: 0x%08qx, next: 0x%08qx\n",
+               (unsigned long long)addr,
+               (unsigned long long)cvmx_bootmem_phy_get_size(addr),
+               (unsigned long long)cvmx_bootmem_phy_get_next(addr));
+        addr = cvmx_bootmem_phy_get_next(addr);
+    }
+    cvmx_dprintf("\n\n");
+
+}
+
+
+uint64_t cvmx_bootmem_phy_available_mem(uint64_t min_block_size)
+{
+    uint64_t addr;
+
+    uint64_t available_mem = 0;
+
+    cvmx_spinlock_lock((cvmx_spinlock_t *)&(cvmx_bootmem_desc->lock));
+    addr = cvmx_bootmem_desc->head_addr;
+    while (addr)
+    {
+        if (cvmx_bootmem_phy_get_size(addr) >= min_block_size)
+            available_mem += cvmx_bootmem_phy_get_size(addr);
+        addr = cvmx_bootmem_phy_get_next(addr);
+    }
+    cvmx_spinlock_unlock((cvmx_spinlock_t *)&(cvmx_bootmem_desc->lock));
+    return(available_mem);
+
+}
+
+
+
+cvmx_bootmem_named_block_desc_t * cvmx_bootmem_phy_named_block_find(char *name, uint32_t flags)
+{
+    unsigned int i;
+    cvmx_bootmem_named_block_desc_t *named_block_array_ptr;
+
+
+#ifdef DEBUG
+    cvmx_dprintf("cvmx_bootmem_phy_named_block_find: %s\n", name);
+#endif
+    /* Lock the structure to make sure that it is not being changed while we are
+    ** examining it.
+    */
+    if (!(flags & CVMX_BOOTMEM_FLAG_NO_LOCKING))
+        cvmx_spinlock_lock((cvmx_spinlock_t *)&(cvmx_bootmem_desc->lock));
+
+#if defined(__linux__) && !defined(CONFIG_OCTEON_U_BOOT)
+#ifdef CVMX_ABI_N32
+    /* Need to use mmapped named block pointer in 32 bit linux apps */
+extern cvmx_bootmem_named_block_desc_t *linux32_named_block_array_ptr;
+    named_block_array_ptr = linux32_named_block_array_ptr;
+#else
+    /* Use XKPHYS for 64 bit linux */
+    named_block_array_ptr = (cvmx_bootmem_named_block_desc_t *)cvmx_phys_to_ptr(cvmx_bootmem_desc->named_block_array_addr);
+#endif
+#else
+    /* Simple executive case. (and u-boot)
+    ** This could be in the low 1 meg of memory that is not 1-1 mapped, so we need use XKPHYS/KSEG0 addressing for it */
+    named_block_array_ptr = CASTPTR(cvmx_bootmem_named_block_desc_t, CVMX_ADD_SEG32(CVMX_MIPS32_SPACE_KSEG0,cvmx_bootmem_desc->named_block_array_addr));
+#endif
+
+#ifdef DEBUG
+    cvmx_dprintf("cvmx_bootmem_phy_named_block_find: named_block_array_ptr: %p\n", named_block_array_ptr);
+#endif
+    if (cvmx_bootmem_desc->major_version == 3)
+    {
+        for (i = 0; i < cvmx_bootmem_desc->named_block_num_blocks; i++)
+        {
+            if ((name && named_block_array_ptr[i].size && !strncmp(name, named_block_array_ptr[i].name, cvmx_bootmem_desc->named_block_name_len - 1))
+                || (!name && !named_block_array_ptr[i].size))
+            {
+                if (!(flags & CVMX_BOOTMEM_FLAG_NO_LOCKING))
+                    cvmx_spinlock_unlock((cvmx_spinlock_t *)&(cvmx_bootmem_desc->lock));
+
+                return(&(named_block_array_ptr[i]));
+            }
+        }
+    }
+    else
+    {
+        cvmx_dprintf("ERROR: Incompatible bootmem descriptor version: %d.%d at addr: %p\n",
+               (int)cvmx_bootmem_desc->major_version, (int)cvmx_bootmem_desc->minor_version, cvmx_bootmem_desc);
+    }
+    if (!(flags & CVMX_BOOTMEM_FLAG_NO_LOCKING))
+        cvmx_spinlock_unlock((cvmx_spinlock_t *)&(cvmx_bootmem_desc->lock));
+
+    return(NULL);
+}
+
+int cvmx_bootmem_phy_named_block_free(char *name, uint32_t flags)
+{
+    cvmx_bootmem_named_block_desc_t *named_block_ptr;
+
+    if (cvmx_bootmem_desc->major_version != 3)
+    {
+        cvmx_dprintf("ERROR: Incompatible bootmem descriptor version: %d.%d at addr: %p\n",
+               (int)cvmx_bootmem_desc->major_version, (int)cvmx_bootmem_desc->minor_version, cvmx_bootmem_desc);
+        return(0);
+    }
+#ifdef DEBUG
+    cvmx_dprintf("cvmx_bootmem_phy_named_block_free: %s\n", name);
+#endif
+
+    /* Take lock here, as name lookup/block free/name free need to be atomic */
+    cvmx_spinlock_lock((cvmx_spinlock_t *)&(cvmx_bootmem_desc->lock));
+
+    named_block_ptr = cvmx_bootmem_phy_named_block_find(name, CVMX_BOOTMEM_FLAG_NO_LOCKING);
+    if (named_block_ptr)
+    {
+#ifdef DEBUG
+        cvmx_dprintf("cvmx_bootmem_phy_named_block_free: %s, base: 0x%llx, size: 0x%llx\n", name, (unsigned long long)named_block_ptr->base_addr, (unsigned long long)named_block_ptr->size);
+#endif
+        __cvmx_bootmem_phy_free(named_block_ptr->base_addr, named_block_ptr->size, CVMX_BOOTMEM_FLAG_NO_LOCKING);
+        named_block_ptr->size = 0;
+        /* Set size to zero to indicate block not used. */
+    }
+
+    cvmx_spinlock_unlock((cvmx_spinlock_t *)&(cvmx_bootmem_desc->lock));
+
+    return(!!named_block_ptr);  /* 0 on failure, 1 on success */
+}
+
+
+
+
+
+int64_t cvmx_bootmem_phy_named_block_alloc(uint64_t size, uint64_t min_addr, uint64_t max_addr, uint64_t alignment, char *name, uint32_t flags)
+{
+    int64_t addr_allocated;
+    cvmx_bootmem_named_block_desc_t *named_block_desc_ptr;
+
+#ifdef DEBUG
+    cvmx_dprintf("cvmx_bootmem_phy_named_block_alloc: size: 0x%llx, min: 0x%llx, max: 0x%llx, align: 0x%llx, name: %s\n",
+                 (unsigned long long)size,
+                 (unsigned long long)min_addr,
+                 (unsigned long long)max_addr,
+                 (unsigned long long)alignment,
+                 name);
+#endif
+    if (cvmx_bootmem_desc->major_version != 3)
+    {
+        cvmx_dprintf("ERROR: Incompatible bootmem descriptor version: %d.%d at addr: %p\n",
+               (int)cvmx_bootmem_desc->major_version, (int)cvmx_bootmem_desc->minor_version, cvmx_bootmem_desc);
+        return(-1);
+    }
+
+
+    /* Take lock here, as name lookup/block alloc/name add need to be atomic */
+
+    if (!(flags & CVMX_BOOTMEM_FLAG_NO_LOCKING))
+        cvmx_spinlock_lock((cvmx_spinlock_t *)&(cvmx_bootmem_desc->lock));
+
+    /* Get pointer to first available named block descriptor */
+    named_block_desc_ptr = cvmx_bootmem_phy_named_block_find(NULL, flags | CVMX_BOOTMEM_FLAG_NO_LOCKING);
+
+    /* Check to see if name already in use, return error if name
+    ** not available or no more room for blocks.
+    */
+    if (cvmx_bootmem_phy_named_block_find(name, flags | CVMX_BOOTMEM_FLAG_NO_LOCKING) || !named_block_desc_ptr)
+    {
+        if (!(flags & CVMX_BOOTMEM_FLAG_NO_LOCKING))
+            cvmx_spinlock_unlock((cvmx_spinlock_t *)&(cvmx_bootmem_desc->lock));
+        return(-1);
+    }
+
+
+    /* Round size up to mult of minimum alignment bytes
+    ** We need the actual size allocated to allow for blocks to be coallesced
+    ** when they are freed.  The alloc routine does the same rounding up
+    ** on all allocations. */
+    size = (size + (CVMX_BOOTMEM_ALIGNMENT_SIZE - 1)) & ~(CVMX_BOOTMEM_ALIGNMENT_SIZE - 1);
+
+    addr_allocated = cvmx_bootmem_phy_alloc(size, min_addr, max_addr, alignment, flags | CVMX_BOOTMEM_FLAG_NO_LOCKING);
+    if (addr_allocated >= 0)
+    {
+        named_block_desc_ptr->base_addr = addr_allocated;
+        named_block_desc_ptr->size = size;
+        strncpy(named_block_desc_ptr->name, name, cvmx_bootmem_desc->named_block_name_len);
+        named_block_desc_ptr->name[cvmx_bootmem_desc->named_block_name_len - 1] = 0;
+    }
+
+    if (!(flags & CVMX_BOOTMEM_FLAG_NO_LOCKING))
+        cvmx_spinlock_unlock((cvmx_spinlock_t *)&(cvmx_bootmem_desc->lock));
+
+    return(addr_allocated);
+}
+
+
+
+
+void cvmx_bootmem_phy_named_block_print(void)
+{
+    unsigned int i;
+    int printed = 0;
+
+#if defined(__linux__) && !defined(CONFIG_OCTEON_U_BOOT)
+#ifdef CVMX_ABI_N32
+    /* Need to use mmapped named block pointer in 32 bit linux apps */
+extern cvmx_bootmem_named_block_desc_t *linux32_named_block_array_ptr;
+    cvmx_bootmem_named_block_desc_t *named_block_array_ptr = linux32_named_block_array_ptr;
+#else
+    /* Use XKPHYS for 64 bit linux */
+    cvmx_bootmem_named_block_desc_t *named_block_array_ptr = (cvmx_bootmem_named_block_desc_t *)cvmx_phys_to_ptr(cvmx_bootmem_desc->named_block_array_addr);
+#endif
+#else
+    /* Simple executive case. (and u-boot)
+    ** This could be in the low 1 meg of memory that is not 1-1 mapped, so we need use XKPHYS/KSEG0 addressing for it */
+    cvmx_bootmem_named_block_desc_t *named_block_array_ptr = CASTPTR(cvmx_bootmem_named_block_desc_t, CVMX_ADD_SEG32(CVMX_MIPS32_SPACE_KSEG0,cvmx_bootmem_desc->named_block_array_addr));
+#endif
+#ifdef DEBUG
+    cvmx_dprintf("cvmx_bootmem_phy_named_block_print, desc addr: %p\n", cvmx_bootmem_desc);
+#endif
+    if (cvmx_bootmem_desc->major_version != 3)
+    {
+        cvmx_dprintf("ERROR: Incompatible bootmem descriptor version: %d.%d at addr: %p\n",
+               (int)cvmx_bootmem_desc->major_version, (int)cvmx_bootmem_desc->minor_version, cvmx_bootmem_desc);
+        return;
+    }
+    cvmx_dprintf("List of currently allocated named bootmem blocks:\n");
+    for (i = 0; i < cvmx_bootmem_desc->named_block_num_blocks; i++)
+    {
+        if (named_block_array_ptr[i].size)
+        {
+            printed++;
+            cvmx_dprintf("Name: %s, address: 0x%08qx, size: 0x%08qx, index: %d\n",
+                   named_block_array_ptr[i].name,
+                   (unsigned long long)named_block_array_ptr[i].base_addr,
+                   (unsigned long long)named_block_array_ptr[i].size,
+                   i);
+
+        }
+    }
+    if (!printed)
+    {
+        cvmx_dprintf("No named bootmem blocks exist.\n");
+    }
+
+}
+
+
+/* Real physical addresses of memory regions */
+#define OCTEON_DDR0_BASE    (0x0ULL)
+#define OCTEON_DDR0_SIZE    (0x010000000ULL)
+#define OCTEON_DDR1_BASE    (0x410000000ULL)
+#define OCTEON_DDR1_SIZE    (0x010000000ULL)
+#define OCTEON_DDR2_BASE    (0x020000000ULL)
+#define OCTEON_DDR2_SIZE    (0x3e0000000ULL)
+#define OCTEON_MAX_PHY_MEM_SIZE (16*1024*1024*1024ULL)
+int64_t cvmx_bootmem_phy_mem_list_init(uint64_t mem_size, uint32_t low_reserved_bytes, cvmx_bootmem_desc_t *desc_buffer)
+{
+    uint64_t cur_block_addr;
+    int64_t addr;
+
+#ifdef DEBUG
+    cvmx_dprintf("cvmx_bootmem_phy_mem_list_init (arg desc ptr: %p, cvmx_bootmem_desc: %p)\n", desc_buffer, cvmx_bootmem_desc);
+#endif
+
+    /* Descriptor buffer needs to be in 32 bit addressable space to be compatible with
+    ** 32 bit applications */
+    if (!desc_buffer)
+    {
+        cvmx_dprintf("ERROR: no memory for cvmx_bootmem descriptor provided\n");
+        return 0;
+    }
+
+    if (mem_size > OCTEON_MAX_PHY_MEM_SIZE)
+    {
+        mem_size = OCTEON_MAX_PHY_MEM_SIZE;
+        cvmx_dprintf("ERROR: requested memory size too large, truncating to maximum size\n");
+    }
+
+    if (cvmx_bootmem_desc)
+        return 1;
+
+    /* Initialize cvmx pointer to descriptor */
+    cvmx_bootmem_init(desc_buffer);
+
+    /* Set up global pointer to start of list, exclude low 64k for exception vectors, space for global descriptor */
+    memset(cvmx_bootmem_desc, 0x0, sizeof(cvmx_bootmem_desc_t));
+    /* Set version of bootmem descriptor */
+    cvmx_bootmem_desc->major_version = CVMX_BOOTMEM_DESC_MAJ_VER;
+    cvmx_bootmem_desc->minor_version = CVMX_BOOTMEM_DESC_MIN_VER;
+
+    cur_block_addr = cvmx_bootmem_desc->head_addr = (OCTEON_DDR0_BASE + low_reserved_bytes);
+
+    cvmx_bootmem_desc->head_addr = 0;
+
+    if (mem_size <= OCTEON_DDR0_SIZE)
+    {
+        __cvmx_bootmem_phy_free(cur_block_addr, mem_size - low_reserved_bytes, 0);
+        goto frees_done;
+    }
+
+    __cvmx_bootmem_phy_free(cur_block_addr, OCTEON_DDR0_SIZE - low_reserved_bytes, 0);
+
+    mem_size -= OCTEON_DDR0_SIZE;
+
+    /* Add DDR2 block next if present */
+    if (mem_size > OCTEON_DDR1_SIZE)
+    {
+        __cvmx_bootmem_phy_free(OCTEON_DDR1_BASE, OCTEON_DDR1_SIZE, 0);
+        __cvmx_bootmem_phy_free(OCTEON_DDR2_BASE, mem_size - OCTEON_DDR1_SIZE, 0);
+    }
+    else
+    {
+        __cvmx_bootmem_phy_free(OCTEON_DDR1_BASE, mem_size, 0);
+
+    }
+frees_done:
+
+    /* Initialize the named block structure */
+    cvmx_bootmem_desc->named_block_name_len = CVMX_BOOTMEM_NAME_LEN;
+    cvmx_bootmem_desc->named_block_num_blocks = CVMX_BOOTMEM_NUM_NAMED_BLOCKS;
+    cvmx_bootmem_desc->named_block_array_addr = 0;
+
+    /* Allocate this near the top of the low 256 MBytes of memory */
+    addr = cvmx_bootmem_phy_alloc(CVMX_BOOTMEM_NUM_NAMED_BLOCKS * sizeof(cvmx_bootmem_named_block_desc_t),0, 0x10000000, 0 ,CVMX_BOOTMEM_FLAG_END_ALLOC);
+    if (addr >= 0)
+        cvmx_bootmem_desc->named_block_array_addr = addr;
+
+#ifdef DEBUG
+    cvmx_dprintf("cvmx_bootmem_phy_mem_list_init: named_block_array_addr: 0x%llx)\n", (unsigned long long)cvmx_bootmem_desc->named_block_array_addr);
+#endif
+    if (!cvmx_bootmem_desc->named_block_array_addr)
+    {
+        cvmx_dprintf("FATAL ERROR: unable to allocate memory for bootmem descriptor!\n");
+        return(0);
+    }
+    memset((void *)(unsigned long)cvmx_bootmem_desc->named_block_array_addr, 0x0, CVMX_BOOTMEM_NUM_NAMED_BLOCKS * sizeof(cvmx_bootmem_named_block_desc_t));
+
+    return(1);
+}
+
+
+void cvmx_bootmem_lock(void)
+{
+    cvmx_spinlock_lock((cvmx_spinlock_t *)&(cvmx_bootmem_desc->lock));
+}
+
+void cvmx_bootmem_unlock(void)
+{
+    cvmx_spinlock_unlock((cvmx_spinlock_t *)&(cvmx_bootmem_desc->lock));
+}
+
+void *__cvmx_bootmem_internal_get_desc_ptr(void)
+{
+    return(cvmx_bootmem_desc);
+}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-bootmem.h b/arch/mips/cavium-octeon/executive/cvmx-bootmem.h
new file mode 100644
index 0000000..e33e89a
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-bootmem.h
@@ -0,0 +1,437 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+/**
+ * @file
+ * Simple allocate only memory allocator.  Used to allocate memory at application
+ * start time.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ *
+ */
+
+#ifndef __CVMX_BOOTMEM_H__
+#define __CVMX_BOOTMEM_H__
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#define CVMX_BOOTMEM_NAME_LEN 128   /* Must be multiple of 8, changing breaks ABI */
+#define CVMX_BOOTMEM_NUM_NAMED_BLOCKS 64  /* Can change without breaking ABI */
+#define CVMX_BOOTMEM_ALIGNMENT_SIZE     (16ull)  /* minimum alignment of bootmem alloced blocks */
+
+/* Flags for cvmx_bootmem_phy_mem* functions */
+#define CVMX_BOOTMEM_FLAG_END_ALLOC    (1 << 0)     /* Allocate from end of block instead of beginning */
+#define CVMX_BOOTMEM_FLAG_NO_LOCKING   (1 << 1)     /* Don't do any locking. */
+
+
+/* First bytes of each free physical block of memory contain this structure,
+ * which is used to maintain the free memory list.  Since the bootloader is
+ * only 32 bits, there is a union providing 64 and 32 bit versions.  The
+ * application init code converts addresses to 64 bit addresses before the
+ * application starts.
+ */
+typedef struct
+{
+    /* Note: these are referenced from assembly routines in the bootloader, so this structure
+    ** should not be changed without changing those routines as well. */
+    uint64_t next_block_addr;
+    uint64_t size;
+
+} cvmx_bootmem_block_header_t;
+
+
+/* Structure for named memory blocks
+** Number of descriptors
+** available can be changed without affecting compatiblity,
+** but name length changes require a bump in the bootmem
+** descriptor version
+** Note: This structure must be naturally 64 bit aligned, as a single
+** memory image will be used by both 32 and 64 bit programs.
+*/
+typedef struct
+{
+    uint64_t base_addr;     /**< Base address of named block */
+    uint64_t size;          /**< Size actually allocated for named block (may differ from requested) */
+    char name[CVMX_BOOTMEM_NAME_LEN];   /**< name of named block */
+} cvmx_bootmem_named_block_desc_t;
+
+
+
+/* Current descriptor versions */
+#define CVMX_BOOTMEM_DESC_MAJ_VER   3   /* CVMX bootmem descriptor major version */
+#define CVMX_BOOTMEM_DESC_MIN_VER   0   /* CVMX bootmem descriptor minor version */
+
+/* First three members of cvmx_bootmem_desc_t are left in original
+** positions for backwards compatibility.
+*/
+typedef struct
+{
+    uint32_t    lock;       /**< spinlock to control access to list */
+    uint32_t    flags;      /**< flags for indicating various conditions */
+    uint64_t    head_addr;
+
+    uint32_t    major_version;  /**< incremented changed when incompatible changes made */
+    uint32_t    minor_version;  /**< incremented changed when compatible changes made, reset to zero when major incremented */
+    uint64_t    app_data_addr;
+    uint64_t    app_data_size;
+
+    uint32_t    named_block_num_blocks;  /**< number of elements in named blocks array */
+    uint32_t    named_block_name_len;    /**< length of name array in bootmem blocks */
+    uint64_t    named_block_array_addr;  /**< address of named memory block descriptors */
+
+} cvmx_bootmem_desc_t;
+
+
+/**
+ * Initialize the boot alloc memory structures. This is
+ * normally called inside of cvmx_user_app_init()
+ *
+ * @param mem_desc_ptr	Address of the free memory list
+ * @return
+ */
+extern int cvmx_bootmem_init(void *mem_desc_ptr);
+
+
+/**
+ * Allocate a block of memory from the free list that was passed
+ * to the application by the bootloader.
+ * This is an allocate-only algorithm, so freeing memory is not possible.
+ *
+ * @param size      Size in bytes of block to allocate
+ * @param alignment Alignment required - must be power of 2
+ *
+ * @return pointer to block of memory, NULL on error
+ */
+extern void *cvmx_bootmem_alloc(uint64_t size, uint64_t alignment);
+
+/**
+ * Allocate a block of memory from the free list that was
+ * passed to the application by the bootloader at a specific
+ * address. This is an allocate-only algorithm, so
+ * freeing memory is not possible. Allocation will fail if
+ * memory cannot be allocated at the specified address.
+ *
+ * @param size      Size in bytes of block to allocate
+ * @param address   Physical address to allocate memory at.  If this memory is not
+ *                  available, the allocation fails.
+ * @param alignment Alignment required - must be power of 2
+ * @return pointer to block of memory, NULL on error
+ */
+extern void *cvmx_bootmem_alloc_address(uint64_t size, uint64_t address, uint64_t alignment);
+
+
+
+/**
+ * Allocate a block of memory from the free list that was
+ * passed to the application by the bootloader within a specified
+ * address range. This is an allocate-only algorithm, so
+ * freeing memory is not possible. Allocation will fail if
+ * memory cannot be allocated in the requested range.
+ *
+ * @param size      Size in bytes of block to allocate
+ * @param min_addr  defines the minimum address of the range
+ * @param max_addr  defines the maximum address of the range
+ * @param alignment Alignment required - must be power of 2
+ * @return pointer to block of memory, NULL on error
+ */
+extern void *cvmx_bootmem_alloc_range(uint64_t size, uint64_t alignment, uint64_t min_addr, uint64_t max_addr);
+
+
+/**
+ * Allocate a block of memory from the free list that was passed
+ * to the application by the bootloader, and assign it a name in the
+ * global named block table.  (part of the cvmx_bootmem_descriptor_t structure)
+ * Named blocks can later be freed.
+ *
+ * @param size      Size in bytes of block to allocate
+ * @param alignment Alignment required - must be power of 2
+ * @param name      name of block - must be less than CVMX_BOOTMEM_NAME_LEN bytes
+ *
+ * @return pointer to block of memory, NULL on error
+ */
+extern void *cvmx_bootmem_alloc_named(uint64_t size, uint64_t alignment, char *name);
+
+
+
+/**
+ * Allocate a block of memory from the free list that was passed
+ * to the application by the bootloader, and assign it a name in the
+ * global named block table.  (part of the cvmx_bootmem_descriptor_t structure)
+ * Named blocks can later be freed.
+ *
+ * @param size      Size in bytes of block to allocate
+ * @param address   Physical address to allocate memory at.  If this memory is not
+ *                  available, the allocation fails.
+ * @param name      name of block - must be less than CVMX_BOOTMEM_NAME_LEN bytes
+ *
+ * @return pointer to block of memory, NULL on error
+ */
+extern void *cvmx_bootmem_alloc_named_address(uint64_t size, uint64_t address, char *name);
+
+
+
+/**
+ * Allocate a block of memory from a specific range of the free list that was passed
+ * to the application by the bootloader, and assign it a name in the
+ * global named block table.  (part of the cvmx_bootmem_descriptor_t structure)
+ * Named blocks can later be freed.
+ * If request cannot be satisfied within the address range specified, NULL is returned
+ *
+ * @param size      Size in bytes of block to allocate
+ * @param min_addr  minimum address of range
+ * @param max_addr  maximum address of range
+ * @param align  Alignment of memory to be allocated. (must be a power of 2)
+ * @param name      name of block - must be less than CVMX_BOOTMEM_NAME_LEN bytes
+ *
+ * @return pointer to block of memory, NULL on error
+ */
+extern void *cvmx_bootmem_alloc_named_range(uint64_t size, uint64_t min_addr, uint64_t max_addr, uint64_t align, char *name);
+
+/**
+ * Frees a previously allocated named bootmem block.
+ *
+ * @param name   name of block to free
+ *
+ * @return 0 on failure,
+ *         !0 on success
+ */
+extern int cvmx_bootmem_free_named(char *name);
+
+
+/**
+ * Finds a named bootmem block by name.
+ *
+ * @param name   name of block to free
+ *
+ * @return pointer to named block descriptor on success
+ *         0 on failure
+ */
+cvmx_bootmem_named_block_desc_t * cvmx_bootmem_find_named_block(char *name);
+
+
+
+/**
+ * Returns the size of available memory in bytes, only
+ * counting blocks that are at least as big as the minimum block
+ * size.
+ *
+ * @param min_block_size
+ *               Minimum block size to count in total.
+ *
+ * @return Number of bytes available for allocation that meet the block size requirement
+ */
+uint64_t cvmx_bootmem_available_mem(uint64_t min_block_size);
+
+
+
+/**
+ * Prints out the list of named blocks that have been allocated
+ * along with their addresses and sizes.
+ * This is primarily used for debugging purposes
+ */
+void cvmx_bootmem_print_named(void);
+
+
+/**
+ * Allocates a block of physical memory from the free list, at (optional) requested address and alignment.
+ *
+ * @param req_size  size of region to allocate.  All requests are rounded up to be a multiple CVMX_BOOTMEM_ALIGNMENT_SIZE bytes size
+ * @param address_min
+ *                  Minimum address that block can occupy.
+ * @param address_max
+ *                  Specifies the maximum address_min (inclusive) that the allocation can use.
+ * @param alignment Requested alignment of the block.  If this alignment cannot be met, the allocation fails.
+ *                  This must be a power of 2.
+ *                  (Note: Alignment of CVMX_BOOTMEM_ALIGNMENT_SIZE bytes is required, and internally enforced.  Requested alignments of
+ *                  less than CVMX_BOOTMEM_ALIGNMENT_SIZE are set to CVMX_BOOTMEM_ALIGNMENT_SIZE.)
+ * @param flags     Flags to control options for the allocation.
+ *
+ * @return physical address of block allocated, or -1 on failure
+ */
+int64_t cvmx_bootmem_phy_alloc(uint64_t req_size, uint64_t address_min, uint64_t address_max, uint64_t alignment, uint32_t flags);
+
+
+
+/**
+ * Allocates a named block of physical memory from the free list, at (optional) requested address and alignment.
+ *
+ * @param size      size of region to allocate.  All requests are rounded up to be a multiple CVMX_BOOTMEM_ALIGNMENT_SIZE bytes size
+ * @param min_addr
+ *                  Minimum address that block can occupy.
+ * @param max_addr
+ *                  Specifies the maximum address_min (inclusive) that the allocation can use.
+ * @param alignment Requested alignment of the block.  If this alignment cannot be met, the allocation fails.
+ *                  This must be a power of 2.
+ *                  (Note: Alignment of CVMX_BOOTMEM_ALIGNMENT_SIZE bytes is required, and internally enforced.  Requested alignments of
+ *                  less than CVMX_BOOTMEM_ALIGNMENT_SIZE are set to CVMX_BOOTMEM_ALIGNMENT_SIZE.)
+ * @param name      name to assign to named block
+ * @param flags     Flags to control options for the allocation.
+ *
+ * @return physical address of block allocated, or -1 on failure
+ */
+int64_t cvmx_bootmem_phy_named_block_alloc(uint64_t size, uint64_t min_addr, uint64_t max_addr, uint64_t alignment, char *name, uint32_t flags);
+
+
+/**
+ * Finds a named memory block by name.
+ * Also used for finding an unused entry in the named block table.
+ *
+ * @param name   Name of memory block to find.
+ *               If NULL pointer given, then finds unused descriptor, if available.
+ * @param flags     Flags to control options for the allocation.
+ *
+ * @return Pointer to memory block descriptor, NULL if not found.
+ *         If NULL returned when name parameter is NULL, then no memory
+ *         block descriptors are available.
+ */
+cvmx_bootmem_named_block_desc_t * cvmx_bootmem_phy_named_block_find(char *name, uint32_t flags);
+
+
+/**
+ * Returns the size of available memory in bytes, only
+ * counting blocks that are at least as big as the minimum block
+ * size.
+ *
+ * @param min_block_size
+ *               Minimum block size to count in total.
+ *
+ * @return Number of bytes available for allocation that meet the block size requirement
+ */
+uint64_t cvmx_bootmem_phy_available_mem(uint64_t min_block_size);
+
+/**
+ * Frees a named block.
+ *
+ * @param name   name of block to free
+ * @param flags  flags for passing options
+ *
+ * @return 0 on failure
+ *         1 on success
+ */
+int cvmx_bootmem_phy_named_block_free(char *name, uint32_t flags);
+
+/**
+ * Frees a block to the bootmem allocator list.  This must
+ * be used with care, as the size provided must match the size
+ * of the block that was allocated, or the list will become
+ * corrupted.
+ *
+ * IMPORTANT:  This is only intended to be used as part of named block
+ * frees and initial population of the free memory list.
+ *                                                      *
+ *
+ * @param phy_addr physical address of block
+ * @param size     size of block in bytes.
+ * @param flags    flags for passing options
+ *
+ * @return 1 on success,
+ *         0 on failure
+ */
+int __cvmx_bootmem_phy_free(uint64_t phy_addr, uint64_t size, uint32_t flags);
+
+
+/**
+ * Prints the list of currently allocated named blocks
+ *
+ */
+void cvmx_bootmem_phy_named_block_print(void);
+
+
+/**
+ * Prints the list of available memory.
+ *
+ */
+void cvmx_bootmem_phy_list_print(void);
+
+
+
+/**
+ * This function initializes the free memory list used by cvmx_bootmem.
+ * This must be called before any allocations can be done.
+ *
+ * @param mem_size Total memory available, in bytes
+ * @param low_reserved_bytes
+ *                 Number of bytes to reserve (leave out of free list) at address 0x0.
+ * @param desc_buffer
+ *                 Buffer for the bootmem descriptor.  This must be a 32 bit addressable
+ *                 address.
+ *
+ * @return 1 on success
+ *         0 on failure
+ */
+int64_t cvmx_bootmem_phy_mem_list_init(uint64_t mem_size, uint32_t low_reserved_bytes, cvmx_bootmem_desc_t *desc_buffer);
+
+/**
+ * Locks the bootmem allocator.  This is useful in certain situations
+ * where multiple allocations must be made without being interrupted.
+ * This should be used with the CVMX_BOOTMEM_FLAG_NO_LOCKING flag.
+ *
+ */
+void cvmx_bootmem_lock(void);
+
+/**
+ * Unlocks the bootmem allocator.  This is useful in certain situations
+ * where multiple allocations must be made without being interrupted.
+ * This should be used with the CVMX_BOOTMEM_FLAG_NO_LOCKING flag.
+ *
+ */
+void cvmx_bootmem_unlock(void);
+
+/**
+ * Internal use function to get the current descriptor pointer */
+void *__cvmx_bootmem_internal_get_desc_ptr(void);
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif /*   __CVMX_BOOTMEM_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-ciu.h b/arch/mips/cavium-octeon/executive/cvmx-ciu.h
new file mode 100644
index 0000000..2e3c3df
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-ciu.h
@@ -0,0 +1,73 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Interface to the hardware Interrupt Unit.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+
+#ifndef __CVMX_CIU_H__
+#define __CVMX_CIU_H__
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+/* CSR typedefs have been moved to cvmx-csr-*.h */
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif
diff --git a/arch/mips/cavium-octeon/executive/cvmx-cmd-queue.c b/arch/mips/cavium-octeon/executive/cvmx-cmd-queue.c
new file mode 100644
index 0000000..3dafa21
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-cmd-queue.c
@@ -0,0 +1,319 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Support functions for managing command queues used for
+ * various hardware blocks.
+ *
+ * <hr>$Revision: 34264 $<hr>
+ */
+#include "cvmx-config.h"
+#include "cvmx.h"
+#include "cvmx-fpa.h"
+#include "cvmx-cmd-queue.h"
+#include "cvmx-bootmem.h"
+
+/**
+ * This application uses this pointer to access the global queue
+ * state. It points to a bootmem named block.
+ */
+CVMX_SHARED __cvmx_cmd_queue_all_state_t *__cvmx_cmd_queue_state_ptr = NULL;
+
+
+/**
+ * @INTERNAL
+ * Initialize the Global queue state pointer.
+ *
+ * @return CVMX_CMD_QUEUE_SUCCESS or a failure code
+ */
+static cvmx_cmd_queue_result_t __cvmx_cmd_queue_init_state_ptr(void)
+{
+    char *alloc_name = "cvmx_cmd_queues";
+#if defined(CONFIG_CAVIUM_RESERVE32) && CONFIG_CAVIUM_RESERVE32
+    extern uint64_t octeon_reserve32_memory;
+#endif
+
+    if (cvmx_likely(__cvmx_cmd_queue_state_ptr))
+        return CVMX_CMD_QUEUE_SUCCESS;
+
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+#if CONFIG_CAVIUM_RESERVE32
+    if (octeon_reserve32_memory)
+        __cvmx_cmd_queue_state_ptr = cvmx_bootmem_alloc_named_range(sizeof(*__cvmx_cmd_queue_state_ptr),
+                                                              octeon_reserve32_memory,
+                                                              octeon_reserve32_memory + (CONFIG_CAVIUM_RESERVE32<<20) - 1,
+                                                              128, alloc_name);
+    else
+#endif
+        __cvmx_cmd_queue_state_ptr = cvmx_bootmem_alloc_named(sizeof(*__cvmx_cmd_queue_state_ptr), 128, alloc_name);
+#else
+    __cvmx_cmd_queue_state_ptr = cvmx_bootmem_alloc_named(sizeof(*__cvmx_cmd_queue_state_ptr), 128, alloc_name);
+#endif
+    if (__cvmx_cmd_queue_state_ptr)
+        memset(__cvmx_cmd_queue_state_ptr, 0, sizeof(*__cvmx_cmd_queue_state_ptr));
+    else
+    {
+        cvmx_bootmem_named_block_desc_t *block_desc = cvmx_bootmem_find_named_block(alloc_name);
+        if (block_desc)
+            __cvmx_cmd_queue_state_ptr = cvmx_phys_to_ptr(block_desc->base_addr);
+        else
+        {
+            cvmx_dprintf("ERROR: cvmx_cmd_queue_initialize: Unable to get named block %s.\n", alloc_name);
+            return CVMX_CMD_QUEUE_NO_MEMORY;
+        }
+    }
+    return CVMX_CMD_QUEUE_SUCCESS;
+}
+
+
+/**
+ * Initialize a command queue for use. The initial FPA buffer is
+ * allocated and the hardware unit is configured to point to the
+ * new command queue.
+ *
+ * @param queue_id  Hardware command queue to initialize.
+ * @param max_depth Maximum outstanding commands that can be queued.
+ * @param fpa_pool  FPA pool the command queues should come from.
+ * @param pool_size Size of each buffer in the FPA pool (bytes)
+ *
+ * @return CVMX_CMD_QUEUE_SUCCESS or a failure code
+ */
+cvmx_cmd_queue_result_t cvmx_cmd_queue_initialize(cvmx_cmd_queue_id_t queue_id, int max_depth, int fpa_pool, int pool_size)
+{
+    __cvmx_cmd_queue_state_t *qstate;
+    cvmx_cmd_queue_result_t result = __cvmx_cmd_queue_init_state_ptr();
+    if (result != CVMX_CMD_QUEUE_SUCCESS)
+        return result;
+
+    qstate = __cvmx_cmd_queue_get_state(queue_id);
+    if (qstate == NULL)
+        return CVMX_CMD_QUEUE_INVALID_PARAM;
+
+    /* We artificially limit max_depth to 1<<20 words. It is an arbitrary limit */
+    if (CVMX_CMD_QUEUE_ENABLE_MAX_DEPTH)
+    {
+        if ((max_depth < 0) || (max_depth > 1<<20))
+            return CVMX_CMD_QUEUE_INVALID_PARAM;
+    }
+    else if (max_depth != 0)
+        return CVMX_CMD_QUEUE_INVALID_PARAM;
+
+    if ((fpa_pool < 0) || (fpa_pool > 7))
+        return CVMX_CMD_QUEUE_INVALID_PARAM;
+    if ((pool_size < 128) || (pool_size > 65536))
+        return CVMX_CMD_QUEUE_INVALID_PARAM;
+
+    /* See if someone else has already initialized the queue */
+    if (qstate->base_ptr_div128)
+    {
+        if (max_depth != (int)qstate->max_depth)
+        {
+            cvmx_dprintf("ERROR: cvmx_cmd_queue_initialize: Queue already initalized with different max_depth (%d).\n", (int)qstate->max_depth);
+            return CVMX_CMD_QUEUE_INVALID_PARAM;
+        }
+        if (fpa_pool != qstate->fpa_pool)
+        {
+            cvmx_dprintf("ERROR: cvmx_cmd_queue_initialize: Queue already initalized with different FPA pool (%u).\n", qstate->fpa_pool);
+            return CVMX_CMD_QUEUE_INVALID_PARAM;
+        }
+        if ((pool_size>>3)-1 != qstate->pool_size_m1)
+        {
+            cvmx_dprintf("ERROR: cvmx_cmd_queue_initialize: Queue already initalized with different FPA pool size (%u).\n", (qstate->pool_size_m1+1)<<3);
+            return CVMX_CMD_QUEUE_INVALID_PARAM;
+        }
+        CVMX_SYNCWS;
+        return CVMX_CMD_QUEUE_ALREADY_SETUP;
+    }
+    else
+    {
+        cvmx_fpa_ctl_status_t status;
+        void *buffer;
+
+        status.u64 = cvmx_read_csr(CVMX_FPA_CTL_STATUS);
+        if (!status.s.enb)
+        {
+            cvmx_dprintf("ERROR: cvmx_cmd_queue_initialize: FPA is not enabled.\n");
+            return CVMX_CMD_QUEUE_NO_MEMORY;
+        }
+        buffer = cvmx_fpa_alloc(fpa_pool);
+        if (buffer == NULL)
+        {
+            cvmx_dprintf("ERROR: cvmx_cmd_queue_initialize: Unable to allocate initial buffer.\n");
+            return CVMX_CMD_QUEUE_NO_MEMORY;
+        }
+
+        memset(qstate, 0, sizeof(*qstate));
+        qstate->max_depth = max_depth;
+        qstate->fpa_pool = fpa_pool;
+        qstate->pool_size_m1 = (pool_size>>3)-1;
+        qstate->base_ptr_div128 = cvmx_ptr_to_phys(buffer) / 128;
+        /* We zeroed the now serving field so we need to also zero the ticket */
+        __cvmx_cmd_queue_state_ptr->ticket[__cvmx_cmd_queue_get_index(queue_id)] = 0;
+        CVMX_SYNCWS;
+        return CVMX_CMD_QUEUE_SUCCESS;
+    }
+}
+
+
+/**
+ * Shutdown a queue a free it's command buffers to the FPA. The
+ * hardware connected to the queue must be stopped before this
+ * function is called.
+ *
+ * @param queue_id Queue to shutdown
+ *
+ * @return CVMX_CMD_QUEUE_SUCCESS or a failure code
+ */
+cvmx_cmd_queue_result_t cvmx_cmd_queue_shutdown(cvmx_cmd_queue_id_t queue_id)
+{
+    __cvmx_cmd_queue_state_t *qptr = __cvmx_cmd_queue_get_state(queue_id);
+    if (qptr == NULL)
+    {
+        cvmx_dprintf("ERROR: cvmx_cmd_queue_shutdown: Unable to get queue information.\n");
+        return CVMX_CMD_QUEUE_INVALID_PARAM;
+    }
+
+    if (cvmx_cmd_queue_length(queue_id) > 0)
+    {
+        cvmx_dprintf("ERROR: cvmx_cmd_queue_shutdown: Queue still has data in it.\n");
+        return CVMX_CMD_QUEUE_FULL;
+    }
+
+    __cvmx_cmd_queue_lock(queue_id, qptr);
+    if (qptr->base_ptr_div128)
+    {
+        cvmx_fpa_free(cvmx_phys_to_ptr((uint64_t)qptr->base_ptr_div128<<7), qptr->fpa_pool, 0);
+        qptr->base_ptr_div128 = 0;
+    }
+    __cvmx_cmd_queue_unlock(qptr);
+
+    return CVMX_CMD_QUEUE_SUCCESS;
+}
+
+
+/**
+ * Return the number of command words pending in the queue. This
+ * function may be relatively slow for some hardware units.
+ *
+ * @param queue_id Hardware command queue to query
+ *
+ * @return Number of outstanding commands
+ */
+int cvmx_cmd_queue_length(cvmx_cmd_queue_id_t queue_id)
+{
+    if (CVMX_ENABLE_PARAMETER_CHECKING)
+    {
+        if (__cvmx_cmd_queue_get_state(queue_id) == NULL)
+            return CVMX_CMD_QUEUE_INVALID_PARAM;
+    }
+
+    /* The cast is here so gcc with check that all values in the
+        cvmx_cmd_queue_id_t enumeration are here */
+    switch ((cvmx_cmd_queue_id_t)(queue_id & 0xff0000))
+    {
+#if CVMX_PKO_USE_FAU_FOR_OUTPUT_QUEUES == 0
+        case CVMX_CMD_QUEUE_PKO_BASE:
+            /* FIXME: Need atomic lock on CVMX_PKO_REG_READ_IDX. Right now we
+                are normally called with the queue lock, so that is a SLIGHT
+                amount of protection */
+            cvmx_write_csr(CVMX_PKO_REG_READ_IDX, queue_id & 0xffff);
+            if (OCTEON_IS_MODEL(OCTEON_CN3XXX))
+            {
+                cvmx_pko_mem_debug9_t debug9;
+                debug9.u64 = cvmx_read_csr(CVMX_PKO_MEM_DEBUG9);
+                return debug9.cn38xx.doorbell;
+            }
+            else
+            {
+                cvmx_pko_mem_debug8_t debug8;
+                debug8.u64 = cvmx_read_csr(CVMX_PKO_MEM_DEBUG8);
+                return debug8.cn58xx.doorbell;
+            }
+#endif
+        case CVMX_CMD_QUEUE_ZIP:
+        case CVMX_CMD_QUEUE_DFA:
+        case CVMX_CMD_QUEUE_RAID:
+            // FIXME: Implement other lengths
+            return 0;
+        case CVMX_CMD_QUEUE_DMA_BASE:
+            {
+                cvmx_npei_dmax_counts_t dmax_counts;
+                dmax_counts.u64 = cvmx_read_csr(CVMX_PEXP_NPEI_DMAX_COUNTS(queue_id & 0x7));
+                return dmax_counts.s.dbell;
+            }
+        case CVMX_CMD_QUEUE_END:
+            return CVMX_CMD_QUEUE_INVALID_PARAM;
+    }
+    return CVMX_CMD_QUEUE_INVALID_PARAM;
+}
+
+
+/**
+ * Return the command buffer to be written to. The purpose of this
+ * function is to allow CVMX routine access t othe low level buffer
+ * for initial hardware setup. User applications should not call this
+ * function directly.
+ *
+ * @param queue_id Command queue to query
+ *
+ * @return Command buffer or NULL on failure
+ */
+void *cvmx_cmd_queue_buffer(cvmx_cmd_queue_id_t queue_id)
+{
+    __cvmx_cmd_queue_state_t *qptr = __cvmx_cmd_queue_get_state(queue_id);
+    if (qptr && qptr->base_ptr_div128)
+        return cvmx_phys_to_ptr((uint64_t)qptr->base_ptr_div128<<7);
+    else
+        return NULL;
+}
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-cmd-queue.h b/arch/mips/cavium-octeon/executive/cvmx-cmd-queue.h
new file mode 100644
index 0000000..62f7c12
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-cmd-queue.h
@@ -0,0 +1,619 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Support functions for managing command queues used for
+ * various hardware blocks.
+ *
+ * The common command queue infrastructure abstracts out the
+ * software necessary for adding to Octeon's chained queue
+ * structures. These structures are used for commands to the
+ * PKO, ZIP, DFA, RAID, and DMA engine blocks. Although each
+ * hardware unit takes commands and CSRs of different types,
+ * they all use basic linked command buffers to store the
+ * pending request. In general, users of the CVMX API don't
+ * call cvmx-cmd-queue functions directly. Instead the hardware
+ * unit specific wrapper should be used. The wrappers perform
+ * unit specific validation and CSR writes to submit the
+ * commands.
+ *
+ * Even though most software will never directly interact with
+ * cvmx-cmd-queue, knowledge of its internal working can help
+ * in diagnosing performance problems and help with debugging.
+ *
+ * Command queue pointers are stored in a global named block
+ * called "cvmx_cmd_queues". Except for the PKO queues, each
+ * hardware queue is stored in its own cache line to reduce SMP
+ * contention on spin locks. The PKO queues are stored such that
+ * every 16th queue is next to each other in memory. This scheme
+ * allows for queues being in separate cache lines when there
+ * are low number of queues per port. With 16 queues per port,
+ * the first queue for each port is in the same cache area. The
+ * second queues for each port are in another area, etc. This
+ * allows software to implement very efficient lockless PKO with
+ * 16 queues per port using a minimum of cache lines per core.
+ * All queues for a given core will be isolated in the same
+ * cache area.
+ *
+ * In addition to the memory pointer layout, cvmx-cmd-queue
+ * provides an optimized fair ll/sc locking mechanism for the
+ * queues. The lock uses a "ticket / now serving" model to
+ * maintain fair order on contended locks. In addition, it uses
+ * predicted locking time to limit cache contention. When a core
+ * know it must wait in line for a lock, it spins on the
+ * internal cycle counter to completely eliminate any causes of
+ * bus traffic.
+ *
+ * <hr> $Revision: 35733 $ <hr>
+ */
+
+#ifndef __CVMX_CMD_QUEUE_H__
+#define __CVMX_CMD_QUEUE_H__
+
+#include "executive-config.h"
+#include "cvmx-config.h"
+#include "cvmx-fpa.h"
+
+#ifndef CVMX_PKO_USE_FAU_FOR_OUTPUT_QUEUES
+//#warning CVMX_PKO_USE_FAU_FOR_OUTPUT_QUEUES isn't defined, setting it to 0
+#define CVMX_PKO_USE_FAU_FOR_OUTPUT_QUEUES 0
+#endif
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+/**
+ * By default we disable the max depth support. Most programs
+ * don't use it and it slows down the command queue processing
+ * significantly.
+ */
+#ifndef CVMX_CMD_QUEUE_ENABLE_MAX_DEPTH
+#define CVMX_CMD_QUEUE_ENABLE_MAX_DEPTH 0
+#endif
+
+/**
+ * Enumeration representing all hardware blocks that use command
+ * queues. Each hardware block has up to 65536 sub identifiers for
+ * multiple command queues. Not all chips support all hardware
+ * units.
+ */
+typedef enum
+{
+#if CVMX_PKO_USE_FAU_FOR_OUTPUT_QUEUES == 0
+    CVMX_CMD_QUEUE_PKO_BASE = 0x00000,
+#define CVMX_CMD_QUEUE_PKO(queue) ((cvmx_cmd_queue_id_t)(CVMX_CMD_QUEUE_PKO_BASE + (0xffff&(queue))))
+#endif
+    CVMX_CMD_QUEUE_ZIP      = 0x10000,
+    CVMX_CMD_QUEUE_DFA      = 0x20000,
+    CVMX_CMD_QUEUE_RAID     = 0x30000,
+    CVMX_CMD_QUEUE_DMA_BASE = 0x40000,
+#define CVMX_CMD_QUEUE_DMA(queue) ((cvmx_cmd_queue_id_t)(CVMX_CMD_QUEUE_DMA_BASE + (0xffff&(queue))))
+    CVMX_CMD_QUEUE_END      = 0x50000,
+} cvmx_cmd_queue_id_t;
+
+/**
+ * Command write operations can fail if the comamnd queue needs
+ * a new buffer and the associated FPA pool is empty. It can also
+ * fail if the number of queued command words reaches the maximum
+ * set at initialization.
+ */
+typedef enum
+{
+    CVMX_CMD_QUEUE_SUCCESS = 0,
+    CVMX_CMD_QUEUE_NO_MEMORY = -1,
+    CVMX_CMD_QUEUE_FULL = -2,
+    CVMX_CMD_QUEUE_INVALID_PARAM = -3,
+    CVMX_CMD_QUEUE_ALREADY_SETUP = -4,
+} cvmx_cmd_queue_result_t;
+
+typedef struct
+{
+    uint8_t  now_serving;           /**< You have lock when this is your ticket */
+    uint64_t unused1        : 24;
+    uint32_t max_depth;             /**< Maximum outstanding command words */
+    uint64_t fpa_pool       : 3;    /**< FPA pool buffers come from */
+    uint64_t base_ptr_div128: 29;   /**< Top of command buffer pointer shifted 7 */
+    uint64_t unused2        : 6;
+    uint64_t pool_size_m1   : 13;   /**< FPA buffer size in 64bit words minus 1 */
+    uint64_t index          : 13;   /**< Number of comamnds already used in buffer */
+} __cvmx_cmd_queue_state_t;
+
+/**
+ * This structure contains the global state of all comamnd queues.
+ * It is stored in a bootmem named block and shared by all
+ * applications running on Octeon. Tickets are stored in a differnet
+ * cahce line that queue information to reduce the contention on the
+ * ll/sc used to get a ticket. If this is not the case, the update
+ * of queue state causes the ll/sc to fail quite often.
+ */
+typedef struct
+{
+    uint64_t                 ticket[(CVMX_CMD_QUEUE_END>>16) * 256];
+    __cvmx_cmd_queue_state_t state[(CVMX_CMD_QUEUE_END>>16) * 256];
+} __cvmx_cmd_queue_all_state_t;
+
+/**
+ * Initialize a command queue for use. The initial FPA buffer is
+ * allocated and the hardware unit is configured to point to the
+ * new command queue.
+ *
+ * @param queue_id  Hardware command queue to initialize.
+ * @param max_depth Maximum outstanding commands that can be queued.
+ * @param fpa_pool  FPA pool the command queues should come from.
+ * @param pool_size Size of each buffer in the FPA pool (bytes)
+ *
+ * @return CVMX_CMD_QUEUE_SUCCESS or a failure code
+ */
+cvmx_cmd_queue_result_t cvmx_cmd_queue_initialize(cvmx_cmd_queue_id_t queue_id, int max_depth, int fpa_pool, int pool_size);
+
+/**
+ * Shutdown a queue a free it's command buffers to the FPA. The
+ * hardware connected to the queue must be stopped before this
+ * function is called.
+ *
+ * @param queue_id Queue to shutdown
+ *
+ * @return CVMX_CMD_QUEUE_SUCCESS or a failure code
+ */
+cvmx_cmd_queue_result_t cvmx_cmd_queue_shutdown(cvmx_cmd_queue_id_t queue_id);
+
+/**
+ * Return the number of command words pending in the queue. This
+ * function may be relatively slow for some hardware units.
+ *
+ * @param queue_id Hardware command queue to query
+ *
+ * @return Number of outstanding commands
+ */
+int cvmx_cmd_queue_length(cvmx_cmd_queue_id_t queue_id);
+
+/**
+ * Return the command buffer to be written to. The purpose of this
+ * function is to allow CVMX routine access t othe low level buffer
+ * for initial hardware setup. User applications should not call this
+ * function directly.
+ *
+ * @param queue_id Command queue to query
+ *
+ * @return Command buffer or NULL on failure
+ */
+void *cvmx_cmd_queue_buffer(cvmx_cmd_queue_id_t queue_id);
+
+/**
+ * @INTERNAL
+ * Get the index into the state arrays for the supplied queue id.
+ *
+ * @param queue_id Queue ID to get an index for
+ *
+ * @return Index into the state arrays
+ */
+static inline int __cvmx_cmd_queue_get_index(cvmx_cmd_queue_id_t queue_id)
+{
+    /* Warning: This code currently only works with devices that have 256 queues
+        or less. Devices with more than 16 queues are layed out in memory to allow
+        cores quick access to every 16th queue. This reduces cache thrashing
+        when you are running 16 queues per port to support lockless operation */
+    int unit = queue_id>>16;
+    int q = (queue_id >> 4) & 0xf;
+    int core = queue_id & 0xf;
+    return unit*256 + core*16 + q;
+}
+
+
+/**
+ * @INTERNAL
+ * Lock the supplied queue so nobody else is updating it at the same
+ * time as us.
+ *
+ * @param queue_id Queue ID to lock
+ * @param qptr     Pointer to the queue's global state
+ */
+static inline void __cvmx_cmd_queue_lock(cvmx_cmd_queue_id_t queue_id, __cvmx_cmd_queue_state_t *qptr)
+{
+    extern CVMX_SHARED __cvmx_cmd_queue_all_state_t *__cvmx_cmd_queue_state_ptr;
+    int tmp;
+    int my_ticket;
+    CVMX_PREFETCH(qptr, 0);
+    asm volatile (
+        ".set push\n"
+        ".set noreorder\n"
+        "1:\n"
+        "ll     %[my_ticket], %[ticket_ptr]\n"
+        "li     %[ticket], 1\n"
+        "baddu  %[ticket], %[my_ticket]\n"
+        "sc     %[ticket], %[ticket_ptr]\n"
+        "beqz   %[ticket], 1b\n"
+        " nop\n"
+        "lbu    %[ticket], %[now_serving]\n"
+        "2:\n"
+        "beq    %[ticket], %[my_ticket], 4f\n"
+        " subu   %[ticket], %[my_ticket], %[ticket]\n"
+        "subu  %[ticket], 1\n"
+        "cins   %[ticket], %[ticket], 5, 7\n"
+        "3:\n"
+        "bnez   %[ticket], 3b\n"
+        " subu  %[ticket], 1\n"
+        "b      2b\n"
+        " lbu   %[ticket], %[now_serving]\n"
+        "4:\n"
+        ".set pop\n"
+        : [ticket_ptr] "=m" (__cvmx_cmd_queue_state_ptr->ticket[__cvmx_cmd_queue_get_index(queue_id)]),
+          [now_serving] "=m" (qptr->now_serving),
+          [ticket] "=r" (tmp),
+          [my_ticket] "=r" (my_ticket)
+    );
+}
+
+
+/**
+ * @INTERNAL
+ * Unlock the queue, flushing all writes.
+ *
+ * @param qptr   Queue to unlock
+ */
+static inline void __cvmx_cmd_queue_unlock(__cvmx_cmd_queue_state_t *qptr)
+{
+    qptr->now_serving++;
+    CVMX_SYNCWS;
+}
+
+
+/**
+ * @INTERNAL
+ * Get the queue state structure for the given queue id
+ *
+ * @param queue_id Queue id to get
+ *
+ * @return Queue structure or NULL on failure
+ */
+static inline __cvmx_cmd_queue_state_t *__cvmx_cmd_queue_get_state(cvmx_cmd_queue_id_t queue_id)
+{
+    extern CVMX_SHARED __cvmx_cmd_queue_all_state_t *__cvmx_cmd_queue_state_ptr;
+    if (CVMX_ENABLE_PARAMETER_CHECKING)
+    {
+        if (cvmx_unlikely(queue_id >= CVMX_CMD_QUEUE_END))
+            return NULL;
+        if (cvmx_unlikely((queue_id & 0xffff) >= 256))
+            return NULL;
+    }
+    return &__cvmx_cmd_queue_state_ptr->state[__cvmx_cmd_queue_get_index(queue_id)];
+}
+
+
+/**
+ * Write an arbitrary number of command words to a command queue.
+ * This is a generic function; the fixed number of comamnd word
+ * functions yield higher performance.
+ *
+ * @param queue_id  Hardware command queue to write to
+ * @param use_locking
+ *                  Use internal locking to ensure exclusive access for queue
+ *                  updates. If you don't use this locking you must ensure
+ *                  exclusivity some other way. Locking is strongly recommended.
+ * @param cmd_count Number of command words to write
+ * @param cmds      Array of comamnds to write
+ *
+ * @return CVMX_CMD_QUEUE_SUCCESS or a failure code
+ */
+static inline cvmx_cmd_queue_result_t cvmx_cmd_queue_write(cvmx_cmd_queue_id_t queue_id, int use_locking, int cmd_count, uint64_t *cmds)
+{
+    __cvmx_cmd_queue_state_t *qptr = __cvmx_cmd_queue_get_state(queue_id);
+
+    if (CVMX_ENABLE_PARAMETER_CHECKING)
+    {
+        if (cvmx_unlikely(qptr == NULL))
+            return CVMX_CMD_QUEUE_INVALID_PARAM;
+        if (cvmx_unlikely((cmd_count < 1) || (cmd_count > 32)))
+            return CVMX_CMD_QUEUE_INVALID_PARAM;
+        if (cvmx_unlikely(cmds == NULL))
+            return CVMX_CMD_QUEUE_INVALID_PARAM;
+    }
+
+    /* Make sure nobody else is updating the same queue */
+    if (cvmx_likely(use_locking))
+        __cvmx_cmd_queue_lock(queue_id, qptr);
+
+    /* If a max queue length was specified then make sure we don't
+        exceed it. If any part of the command would be below the limit
+        we allow it */
+    if (CVMX_CMD_QUEUE_ENABLE_MAX_DEPTH && cvmx_unlikely(qptr->max_depth))
+    {
+        if (cvmx_unlikely(cvmx_cmd_queue_length(queue_id) > (int)qptr->max_depth))
+        {
+            if (cvmx_likely(use_locking))
+                __cvmx_cmd_queue_unlock(qptr);
+            return CVMX_CMD_QUEUE_FULL;
+        }
+    }
+
+    /* Normally there is plenty of room in the current buffer for the command */
+    if (cvmx_likely(qptr->index + cmd_count < qptr->pool_size_m1))
+    {
+        uint64_t *ptr = (uint64_t *)cvmx_phys_to_ptr((uint64_t)qptr->base_ptr_div128<<7);
+        ptr += qptr->index;
+        qptr->index += cmd_count;
+        while (cmd_count--)
+            *ptr++ = *cmds++;
+    }
+    else
+    {
+        uint64_t *ptr;
+        int count;
+        /* We need a new comamnd buffer. Fail if there isn't one available */
+        uint64_t *new_buffer = (uint64_t *)cvmx_fpa_alloc(qptr->fpa_pool);
+        if (cvmx_unlikely(new_buffer == NULL))
+        {
+            if (cvmx_likely(use_locking))
+                __cvmx_cmd_queue_unlock(qptr);
+            return CVMX_CMD_QUEUE_NO_MEMORY;
+        }
+        ptr = (uint64_t *)cvmx_phys_to_ptr((uint64_t)qptr->base_ptr_div128<<7);
+        /* Figure out how many command words will fit in this buffer. One
+            location will be needed for the next buffer pointer */
+        count = qptr->pool_size_m1 - qptr->index;
+        ptr += qptr->index;
+        cmd_count-=count;
+        while (count--)
+            *ptr++ = *cmds++;
+        *ptr = cvmx_ptr_to_phys(new_buffer);
+        /* The current buffer is full and has a link to the next buffer. Time
+            to write the rest of the commands into the new buffer */
+        qptr->base_ptr_div128 = *ptr >> 7;
+        qptr->index = cmd_count;
+        ptr = new_buffer;
+        while (cmd_count--)
+            *ptr++ = *cmds++;
+    }
+
+    /* All updates are complete. Release the lock and return */
+    if (cvmx_likely(use_locking))
+        __cvmx_cmd_queue_unlock(qptr);
+    return CVMX_CMD_QUEUE_SUCCESS;
+}
+
+
+/**
+ * Simple function to write two command words to a command
+ * queue.
+ *
+ * @param queue_id Hardware command queue to write to
+ * @param use_locking
+ *                 Use internal locking to ensure exclusive access for queue
+ *                 updates. If you don't use this locking you must ensure
+ *                 exclusivity some other way. Locking is strongly recommended.
+ * @param cmd1     Command
+ * @param cmd2     Command
+ *
+ * @return CVMX_CMD_QUEUE_SUCCESS or a failure code
+ */
+static inline cvmx_cmd_queue_result_t cvmx_cmd_queue_write2(cvmx_cmd_queue_id_t queue_id, int use_locking, uint64_t cmd1, uint64_t cmd2)
+{
+    __cvmx_cmd_queue_state_t *qptr = __cvmx_cmd_queue_get_state(queue_id);
+
+    if (CVMX_ENABLE_PARAMETER_CHECKING)
+    {
+        if (cvmx_unlikely(qptr == NULL))
+            return CVMX_CMD_QUEUE_INVALID_PARAM;
+    }
+
+    /* Make sure nobody else is updating the same queue */
+    if (cvmx_likely(use_locking))
+        __cvmx_cmd_queue_lock(queue_id, qptr);
+
+    /* If a max queue length was specified then make sure we don't
+        exceed it. If any part of the command would be below the limit
+        we allow it */
+    if (CVMX_CMD_QUEUE_ENABLE_MAX_DEPTH && cvmx_unlikely(qptr->max_depth))
+    {
+        if (cvmx_unlikely(cvmx_cmd_queue_length(queue_id) > (int)qptr->max_depth))
+        {
+            if (cvmx_likely(use_locking))
+                __cvmx_cmd_queue_unlock(qptr);
+            return CVMX_CMD_QUEUE_FULL;
+        }
+    }
+
+    /* Normally there is plenty of room in the current buffer for the command */
+    if (cvmx_likely(qptr->index + 2 < qptr->pool_size_m1))
+    {
+        uint64_t *ptr = (uint64_t *)cvmx_phys_to_ptr((uint64_t)qptr->base_ptr_div128<<7);
+        ptr += qptr->index;
+        qptr->index += 2;
+        ptr[0] = cmd1;
+        ptr[1] = cmd2;
+    }
+    else
+    {
+        uint64_t *ptr;
+        /* Figure out how many command words will fit in this buffer. One
+            location will be needed for the next buffer pointer */
+        int count = qptr->pool_size_m1 - qptr->index;
+        /* We need a new comamnd buffer. Fail if there isn't one available */
+        uint64_t *new_buffer = (uint64_t *)cvmx_fpa_alloc(qptr->fpa_pool);
+        if (cvmx_unlikely(new_buffer == NULL))
+        {
+            if (cvmx_likely(use_locking))
+                __cvmx_cmd_queue_unlock(qptr);
+            return CVMX_CMD_QUEUE_NO_MEMORY;
+        }
+        count--;
+        ptr = (uint64_t *)cvmx_phys_to_ptr((uint64_t)qptr->base_ptr_div128<<7);
+        ptr += qptr->index;
+        *ptr++ = cmd1;
+        if (cvmx_likely(count))
+            *ptr++ = cmd2;
+        *ptr = cvmx_ptr_to_phys(new_buffer);
+        /* The current buffer is full and has a link to the next buffer. Time
+            to write the rest of the commands into the new buffer */
+        qptr->base_ptr_div128 = *ptr >> 7;
+        qptr->index = 0;
+        if (cvmx_unlikely(count == 0))
+        {
+            qptr->index = 1;
+            new_buffer[0] = cmd2;
+        }
+    }
+
+    /* All updates are complete. Release the lock and return */
+    if (cvmx_likely(use_locking))
+        __cvmx_cmd_queue_unlock(qptr);
+    return CVMX_CMD_QUEUE_SUCCESS;
+}
+
+
+/**
+ * Simple function to write three command words to a command
+ * queue.
+ *
+ * @param queue_id Hardware command queue to write to
+ * @param use_locking
+ *                 Use internal locking to ensure exclusive access for queue
+ *                 updates. If you don't use this locking you must ensure
+ *                 exclusivity some other way. Locking is strongly recommended.
+ * @param cmd1     Command
+ * @param cmd2     Command
+ * @param cmd3     Command
+ *
+ * @return CVMX_CMD_QUEUE_SUCCESS or a failure code
+ */
+static inline cvmx_cmd_queue_result_t cvmx_cmd_queue_write3(cvmx_cmd_queue_id_t queue_id, int use_locking, uint64_t cmd1, uint64_t cmd2, uint64_t cmd3)
+{
+    __cvmx_cmd_queue_state_t *qptr = __cvmx_cmd_queue_get_state(queue_id);
+
+    if (CVMX_ENABLE_PARAMETER_CHECKING)
+    {
+        if (cvmx_unlikely(qptr == NULL))
+            return CVMX_CMD_QUEUE_INVALID_PARAM;
+    }
+
+    /* Make sure nobody else is updating the same queue */
+    if (cvmx_likely(use_locking))
+        __cvmx_cmd_queue_lock(queue_id, qptr);
+
+    /* If a max queue length was specified then make sure we don't
+        exceed it. If any part of the command would be below the limit
+        we allow it */
+    if (CVMX_CMD_QUEUE_ENABLE_MAX_DEPTH && cvmx_unlikely(qptr->max_depth))
+    {
+        if (cvmx_unlikely(cvmx_cmd_queue_length(queue_id) > (int)qptr->max_depth))
+        {
+            if (cvmx_likely(use_locking))
+                __cvmx_cmd_queue_unlock(qptr);
+            return CVMX_CMD_QUEUE_FULL;
+        }
+    }
+
+    /* Normally there is plenty of room in the current buffer for the command */
+    if (cvmx_likely(qptr->index + 3 < qptr->pool_size_m1))
+    {
+        uint64_t *ptr = (uint64_t *)cvmx_phys_to_ptr((uint64_t)qptr->base_ptr_div128<<7);
+        ptr += qptr->index;
+        qptr->index += 3;
+        ptr[0] = cmd1;
+        ptr[1] = cmd2;
+        ptr[2] = cmd3;
+    }
+    else
+    {
+        uint64_t *ptr;
+        /* Figure out how many command words will fit in this buffer. One
+            location will be needed for the next buffer pointer */
+        int count = qptr->pool_size_m1 - qptr->index;
+        /* We need a new comamnd buffer. Fail if there isn't one available */
+        uint64_t *new_buffer = (uint64_t *)cvmx_fpa_alloc(qptr->fpa_pool);
+        if (cvmx_unlikely(new_buffer == NULL))
+        {
+            if (cvmx_likely(use_locking))
+                __cvmx_cmd_queue_unlock(qptr);
+            return CVMX_CMD_QUEUE_NO_MEMORY;
+        }
+        count--;
+        ptr = (uint64_t *)cvmx_phys_to_ptr((uint64_t)qptr->base_ptr_div128<<7);
+        ptr += qptr->index;
+        *ptr++ = cmd1;
+        if (count)
+        {
+            *ptr++ = cmd2;
+            if (count > 1)
+                *ptr++ = cmd3;
+        }
+        *ptr = cvmx_ptr_to_phys(new_buffer);
+        /* The current buffer is full and has a link to the next buffer. Time
+            to write the rest of the commands into the new buffer */
+        qptr->base_ptr_div128 = *ptr >> 7;
+        qptr->index = 0;
+        ptr = new_buffer;
+        if (count == 0)
+        {
+            *ptr++ = cmd2;
+            qptr->index++;
+        }
+        if (count < 2)
+        {
+            *ptr++ = cmd2;
+            qptr->index++;
+        }
+    }
+
+    /* All updates are complete. Release the lock and return */
+    if (cvmx_likely(use_locking))
+        __cvmx_cmd_queue_unlock(qptr);
+    return CVMX_CMD_QUEUE_SUCCESS;
+}
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif /* __CVMX_CMD_QUEUE_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-cn3010-evb-hs5.c b/arch/mips/cavium-octeon/executive/cvmx-cn3010-evb-hs5.c
new file mode 100644
index 0000000..c7660ac
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-cn3010-evb-hs5.c
@@ -0,0 +1,222 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Interface to the EBH-30xx specific devices
+ *
+ * <hr>$Revision: 32636 $<hr>
+ *
+ */
+
+#include <time.h>
+#include "cvmx-config.h"
+#include "cvmx.h"
+#include "cvmx-sysinfo.h"
+#include "cvmx-cn3010-evb-hs5.h"
+#include "cvmx-twsi.h"
+
+
+static inline uint8_t bin2bcd(uint8_t bin)
+{
+    return (bin / 10) << 4 | (bin % 10);
+}
+
+static inline uint8_t bcd2bin(uint8_t bcd)
+{
+    return (bcd >> 4) * 10 + (bcd & 0xf);
+}
+
+#define TM_CHECK(_expr, _msg) \
+        do { \
+            if (_expr) { \
+                cvmx_dprintf("Warning: RTC has invalid %s field\n", (_msg)); \
+                rc = -1; \
+            } \
+        } while(0);
+
+static int validate_tm_struct(struct tm * tms)
+{
+    int rc = 0;
+
+    if (!tms)
+	return -1;
+
+    TM_CHECK(tms->tm_sec < 0  || tms->tm_sec > 60,  "second"); /* + Leap sec */
+    TM_CHECK(tms->tm_min < 0  || tms->tm_min > 59,  "minute");
+    TM_CHECK(tms->tm_hour < 0 || tms->tm_hour > 23, "hour");
+    TM_CHECK(tms->tm_mday < 1 || tms->tm_mday > 31, "day");
+    TM_CHECK(tms->tm_wday < 0 || tms->tm_wday > 6,  "day of week");
+    TM_CHECK(tms->tm_mon < 0  || tms->tm_mon > 11,  "month");
+    TM_CHECK(tms->tm_year < 0 || tms->tm_year > 200,"year");
+
+    return rc;
+}
+
+/*
+ * Board-specifc RTC read
+ * Time is expressed in seconds from epoch (Jan 1 1970 at 00:00:00 UTC)
+ * and converted internally to calendar format.
+ */
+uint32_t cvmx_rtc_ds1337_read(void)
+{
+    int       i, retry;
+    uint32_t  time;
+    uint8_t   reg[8];
+    uint8_t   sec;
+    struct tm tms;
+
+
+    memset(&reg, 0, sizeof(reg));
+    memset(&tms, 0, sizeof(struct tm));
+
+    for(retry=0; retry<2; retry++)
+    {
+	/* Lockless read: detects the infrequent roll-over and retries */
+	reg[0] = cvmx_twsi_read8(CVMX_RTC_DS1337_ADDR, 0x0);
+	for(i=1; i<7; i++)
+	    reg[i] = cvmx_twsi_read8_cur_addr(CVMX_RTC_DS1337_ADDR);
+
+	sec = cvmx_twsi_read8(CVMX_RTC_DS1337_ADDR, 0x0);
+	if ((sec & 0xf) == (reg[0] & 0xf))
+	    break; /* Time did not roll-over, value is correct */
+    }
+
+    tms.tm_sec  = bcd2bin(reg[0] & 0x7f);
+    tms.tm_min  = bcd2bin(reg[1] & 0x7f);
+    tms.tm_hour = bcd2bin(reg[2] & 0x3f);
+    if ((reg[2] & 0x40) && (reg[2] & 0x20))   /* AM/PM format and is PM time */
+    {
+	tms.tm_hour = (tms.tm_hour + 12) % 24;
+    }
+    tms.tm_wday = (reg[3] & 0x7) - 1;         /* Day of week field is 0..6 */
+    tms.tm_mday = bcd2bin(reg[4] & 0x3f);
+    tms.tm_mon  = bcd2bin(reg[5] & 0x1f) - 1; /* Month field is 0..11 */
+    tms.tm_year = ((reg[5] & 0x80) ? 100 : 0) + bcd2bin(reg[6]);
+
+
+    if (validate_tm_struct(&tms))
+	cvmx_dprintf("Warning: RTC calendar is not configured properly\n");
+
+    time = mktime(&tms);
+
+    return time;
+}
+
+/*
+ * Board-specific RTC write
+ * Time returned is in seconds from epoch (Jan 1 1970 at 00:00:00 UTC)
+ */
+int cvmx_rtc_ds1337_write(uint32_t time)
+{
+    int       i, rc, retry;
+    struct tm tms;
+    uint8_t   reg[8];
+    uint8_t   sec;
+    time_t    time_from_epoch = time;
+
+
+    localtime_r(&time_from_epoch, &tms);
+
+    if (validate_tm_struct(&tms))
+    {
+	cvmx_dprintf("Error: RTC was passed wrong calendar values, write failed\n");
+	goto tm_invalid;
+    }
+
+    reg[0] = bin2bcd(tms.tm_sec);
+    reg[1] = bin2bcd(tms.tm_min);
+    reg[2] = bin2bcd(tms.tm_hour);      /* Force 0..23 format even if using AM/PM */
+    reg[3] = bin2bcd(tms.tm_wday + 1);
+    reg[4] = bin2bcd(tms.tm_mday);
+    reg[5] = bin2bcd(tms.tm_mon + 1);
+    if (tms.tm_year >= 100)             /* Set century bit*/
+    {
+	reg[5] |= 0x80;
+    }
+    reg[6] = bin2bcd(tms.tm_year % 100);
+
+    /* Lockless write: detects the infrequent roll-over and retries */
+    for(retry=0; retry<2; retry++)
+    {
+	rc = 0;
+	for(i=0; i<7; i++)
+	{
+	    rc |= cvmx_twsi_write8(CVMX_RTC_DS1337_ADDR, i, reg[i]);
+	}
+
+	sec = cvmx_twsi_read8(CVMX_RTC_DS1337_ADDR, 0x0);
+	if ((sec & 0xf) == (reg[0] & 0xf))
+	    break; /* Time did not roll-over, value is correct */
+    }
+
+    return (rc ? -1 : 0);
+
+ tm_invalid:
+    return -1;
+}
+
+#ifdef CVMX_RTC_DEBUG
+
+void cvmx_rtc_ds1337_dump_state(void)
+{
+    int i = 0;
+
+    printf("RTC:\n");
+    printf("%d : %02X ", i, cvmx_twsi_read8(CVMX_RTC_DS1337_ADDR, 0x0));
+    for(i=1; i<16; i++) {
+	printf("%02X ", cvmx_twsi_read8_cur_addr(CVMX_RTC_DS1337_ADDR));
+    }
+    printf("\n");
+}
+
+#endif /* CVMX_RTC_DEBUG */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-cn3010-evb-hs5.h b/arch/mips/cavium-octeon/executive/cvmx-cn3010-evb-hs5.h
new file mode 100644
index 0000000..285c260
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-cn3010-evb-hs5.h
@@ -0,0 +1,77 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+#ifndef __CVMX_CN3010_EVB_HS5_H__
+#define __CVMX_CN3010_EVB_HS5_H__
+
+/**
+ * @file
+ *
+ * Interface to the EBH-30xx specific devices
+ *
+ * <hr>$Revision: 32636 $<hr>
+ *
+ */
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#define CVMX_RTC_DS1337_ADDR   (0x68)
+
+uint32_t cvmx_rtc_ds1337_read(void);
+int      cvmx_rtc_ds1337_write(uint32_t time);
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif  /* __CVMX_CN3010_EVB_HS5_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-compactflash.c b/arch/mips/cavium-octeon/executive/cvmx-compactflash.c
new file mode 100644
index 0000000..0d66eb0
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-compactflash.c
@@ -0,0 +1,244 @@
+/***********************license start***************
+ * Copyright (c) 2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+#include "cvmx.h"
+#include "cvmx-sysinfo.h"
+#include "cvmx-compactflash.h"
+
+
+#ifndef MAX
+#define	MAX(a,b) (((a)>(b))?(a):(b))
+#endif
+#define FLASH_RoundUP(_Dividend, _Divisor) (((_Dividend)+(_Divisor))/(_Divisor))
+/**
+ * Convert nanosecond based time to setting used in the
+ * boot bus timing register, based on timing multiple
+ * 
+ * 
+ */
+static uint32_t ns_to_tim_reg(int tim_mult, uint32_t nsecs)
+{
+	uint32_t val;
+
+	/* Compute # of eclock periods to get desired duration in nanoseconds */
+	val = FLASH_RoundUP(nsecs * (cvmx_sysinfo_get()->cpu_clock_hz/1000000), 1000);
+	
+	/* Factor in timing multiple, if not 1 */
+	if (tim_mult != 1)
+		val = FLASH_RoundUP(val, tim_mult);
+	
+	return (val);
+}
+
+uint64_t cvmx_compactflash_generate_dma_tim(int tim_mult, uint16_t *ident_data, int *mwdma_mode_ptr)
+{
+
+	cvmx_mio_boot_dma_timx_t dma_tim;
+	int oe_a;
+	int oe_n;
+	int dma_acks;
+	int dma_ackh;
+	int dma_arq;
+	int pause;
+	int To,Tkr,Td;
+	int mwdma_mode = -1;
+        uint16_t word53_field_valid;
+        uint16_t word63_mwdma;
+        uint16_t word163_adv_timing_info;
+
+        if (!ident_data)
+            return 0;
+
+        word53_field_valid = ident_data[53];
+        word63_mwdma = ident_data[63]; 
+        word163_adv_timing_info = ident_data[163];
+
+	dma_tim.u64 = 0;
+
+	/* Check for basic MWDMA modes */
+	if (word53_field_valid & 0x2)
+	{
+		if (word63_mwdma & 0x4)
+			mwdma_mode = 2;
+		else if (word63_mwdma & 0x2)
+			mwdma_mode = 1;
+		else if (word63_mwdma & 0x1)
+			mwdma_mode = 0;
+	}
+
+	/* Check for advanced MWDMA modes */
+	switch ((word163_adv_timing_info >> 3) & 0x7)
+	{
+		case 1:
+			mwdma_mode = 3;
+			break;
+		case 2:
+			mwdma_mode = 4;
+			break;
+		default:
+			break;
+
+	}
+	/* DMA is not supported by this card */
+	if (mwdma_mode < 0)
+            return 0;
+
+	/* Now set up the DMA timing */
+	switch (tim_mult)
+	{
+		case 1:
+		    dma_tim.s.tim_mult = 1;
+		    break;
+		case 2:
+		    dma_tim.s.tim_mult = 2;
+		    break;
+		case 4:
+		    dma_tim.s.tim_mult = 0;
+		    break;
+		case 8:
+		    dma_tim.s.tim_mult = 3;
+		    break;
+		default:
+		    cvmx_dprintf("ERROR: invalid boot bus dma tim_mult setting\n");
+		    break;
+	}
+
+
+	switch (mwdma_mode)
+	{
+		case 4:
+			To = 80;
+			Td = 55;
+			Tkr = 20;
+			
+			oe_a = Td + 20;  // Td (Seem to need more margin here....
+			oe_n = MAX(To - oe_a, Tkr);  // Tkr from cf spec, lengthened to meet To
+			
+			// oe_n + oe_h must be >= To (cycle time)
+			dma_acks = 0; //Ti
+			dma_ackh = 5; // Tj
+			
+			dma_arq = 8;  // not spec'ed, value in eclocks, not affected by tim_mult
+			pause = 25 - dma_arq * 1000/(cvmx_sysinfo_get()->cpu_clock_hz/1000000); // Tz
+			break;
+		case 3:
+			To = 100;
+			Td = 65;
+			Tkr = 20;
+			
+			oe_a = Td + 20;  // Td (Seem to need more margin here....
+			oe_n = MAX(To - oe_a, Tkr);  // Tkr from cf spec, lengthened to meet To
+			
+			// oe_n + oe_h must be >= To (cycle time)
+			dma_acks = 0; //Ti
+			dma_ackh = 5; // Tj
+			
+			dma_arq = 8;  // not spec'ed, value in eclocks, not affected by tim_mult
+			pause = 25 - dma_arq * 1000/(cvmx_sysinfo_get()->cpu_clock_hz/1000000); // Tz
+			break;
+		case 2:
+			// +20 works
+			// +10 works
+			// + 10 + 0 fails
+			// n=40, a=80 works
+			To = 120;
+			Td = 70;
+			Tkr = 25;
+
+                        // oe_a 0 fudge doesn't work; 10 seems to
+			oe_a = Td + 20 + 10;  // Td (Seem to need more margin here....
+			oe_n = MAX(To - oe_a, Tkr) + 10;  // Tkr from cf spec, lengthened to meet To
+                        // oe_n 0 fudge fails;;; 10 boots
+
+                        // 20 ns fudge needed on dma_acks
+			// oe_n + oe_h must be >= To (cycle time)
+			dma_acks = 0 + 20; //Ti
+			dma_ackh = 5; // Tj
+			
+			dma_arq = 8;  // not spec'ed, value in eclocks, not affected by tim_mult
+			pause = 25 - dma_arq * 1000/(cvmx_sysinfo_get()->cpu_clock_hz/1000000); // Tz
+                        // no fudge needed on pause
+			
+			break;
+		case 1:
+		case 0:
+		default:
+			cvmx_dprintf("ERROR: Unsupported DMA mode: %d\n", mwdma_mode);
+			return(-1);
+			break;
+	}
+
+        if (mwdma_mode_ptr)
+            *mwdma_mode_ptr = mwdma_mode;
+	
+	dma_tim.s.dmack_pi = 1;
+	
+	dma_tim.s.oe_n = ns_to_tim_reg(tim_mult, oe_n);
+	dma_tim.s.oe_a = ns_to_tim_reg(tim_mult, oe_a);
+	
+	dma_tim.s.dmack_s = ns_to_tim_reg(tim_mult, dma_acks);
+	dma_tim.s.dmack_h = ns_to_tim_reg(tim_mult, dma_ackh); 
+	
+	dma_tim.s.dmarq = dma_arq;
+	dma_tim.s.pause = ns_to_tim_reg(tim_mult, pause);
+	
+	dma_tim.s.rd_dly = 0; /* Sample right on edge */
+	
+	/*  writes only */
+	dma_tim.s.we_n = ns_to_tim_reg(tim_mult, oe_n);
+	dma_tim.s.we_a = ns_to_tim_reg(tim_mult, oe_a);
+	
+#if 0
+	cvmx_dprintf("ns to ticks (mult %d) of %d is: %d\n", TIM_MULT, 60, ns_to_tim_reg(60));
+	cvmx_dprintf("oe_n: %d, oe_a: %d, dmack_s: %d, dmack_h: %d, dmarq: %d, pause: %d\n",
+	   dma_tim.s.oe_n, dma_tim.s.oe_a, dma_tim.s.dmack_s, dma_tim.s.dmack_h, dma_tim.s.dmarq, dma_tim.s.pause);
+#endif
+
+	return(dma_tim.u64);
+
+
+}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-compactflash.h b/arch/mips/cavium-octeon/executive/cvmx-compactflash.h
new file mode 100644
index 0000000..20e26f2
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-compactflash.h
@@ -0,0 +1,74 @@
+/***********************license start***************
+ * Copyright (c) 2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+#ifndef __CVMX_COMPACTFLASH_H__
+#define __CVMX_COMPACTFLASH_H__
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+
+/**
+ * This function takes values from the compact flash device
+ * identify response, and returns the appropriate value to write
+ * into the boot bus DMA timing register.
+ *
+ * @param tim_mult   Eclock timing multiple to use
+ * @param ident_data Data returned by the 'identify' command.  This is used to
+ *                   determine the DMA modes supported by the card, if any.
+ * @param mwdma_mode_ptr
+ *                   Optional pointer to return MWDMA mode in
+ *
+ * @return 64 bit value to write to DMA timing register
+ */
+extern uint64_t cvmx_compactflash_generate_dma_tim(int tim_mult, uint16_t *ident_data, int *mwdma_mode_ptr);
+
+#ifdef	__cplusplus
+}
+#endif
+#endif   /* __CVMX_COMPACTFLASH_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-core.c b/arch/mips/cavium-octeon/executive/cvmx-core.c
new file mode 100644
index 0000000..ea91038
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-core.c
@@ -0,0 +1,155 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Module to support operations on core such as TLB config, etc.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ *
+ */
+
+#include "cvmx-config.h"
+#include "cvmx.h"
+#include "cvmx-core.h"
+
+
+/**
+ * Adds a wired TLB entry, and returns the index of the entry added.
+ * Parameters are written to TLB registers without further processing.
+ *
+ * @param hi     HI register value
+ * @param lo0    lo0 register value
+ * @param lo1    lo1 register value
+ * @param page_mask   pagemask register value
+ *
+ * @return Success: TLB index used (0-31) or (0-63) for OCTEON Plus
+ *         Failure: -1
+ */
+int cvmx_core_add_wired_tlb_entry(uint64_t hi, uint64_t lo0, uint64_t lo1, cvmx_tlb_pagemask_t page_mask)
+{
+    uint32_t index;
+    uint32_t index_limit = 31;
+
+    if (!OCTEON_IS_MODEL(OCTEON_CN3XXX))
+    {
+        index_limit=63;
+    }
+
+    CVMX_MF_TLB_WIRED(index);
+    if (index >= index_limit)
+    {
+        return(-1);
+    }
+    CVMX_MT_ENTRY_HIGH(hi);
+    CVMX_MT_ENTRY_LO_0(lo0);
+    CVMX_MT_ENTRY_LO_1(lo1);
+    CVMX_MT_PAGEMASK(page_mask);
+    CVMX_MT_TLB_INDEX(index);
+    CVMX_MT_TLB_WIRED(index + 1);
+    CVMX_EHB;
+    CVMX_TLBWI;
+    CVMX_EHB;
+    return(index);
+}
+
+
+
+/**
+ * Adds a fixed (wired) TLB mapping.  Returns TLB index used or -1 on error.
+ * This is a wrapper around cvmx_core_add_wired_tlb_entry()
+ *
+ * @param vaddr      Virtual address to map
+ * @param page0_addr page 0 physical address, with low 3 bits representing the DIRTY, VALID, and GLOBAL bits
+ * @param page1_addr page1 physical address, with low 3 bits representing the DIRTY, VALID, and GLOBAL bits
+ * @param page_mask  page mask.
+ *
+ * @return Success: TLB index used (0-31)
+ *         Failure: -1
+ */
+int cvmx_core_add_fixed_tlb_mapping_bits(uint64_t vaddr, uint64_t page0_addr, uint64_t page1_addr, cvmx_tlb_pagemask_t page_mask)
+{
+
+    if ((vaddr & (page_mask | 0x7fff))
+        || ((page0_addr & ~0x7ULL) & ((page_mask | 0x7fff) >> 1))
+        || ((page1_addr & ~0x7ULL) & ((page_mask | 0x7fff) >> 1)))
+    {
+        cvmx_dprintf("Error adding tlb mapping: invalid address alignment at vaddr: 0x%llx\n", (unsigned long long)vaddr);
+        return(-1);
+    }
+
+
+    return(cvmx_core_add_wired_tlb_entry(vaddr,
+                                         (page0_addr >> 6) | (page0_addr & 0x7),
+                                         (page1_addr >> 6) | (page1_addr & 0x7),
+                                         page_mask));
+
+}
+/**
+ * Adds a fixed (wired) TLB mapping.  Returns TLB index used or -1 on error.
+ * Assumes both pages are valid.  Use cvmx_core_add_fixed_tlb_mapping_bits for more control.
+ * This is a wrapper around cvmx_core_add_wired_tlb_entry()
+ *
+ * @param vaddr      Virtual address to map
+ * @param page0_addr page 0 physical address
+ * @param page1_addr page1 physical address
+ * @param page_mask  page mask.
+ *
+ * @return Success: TLB index used (0-31)
+ *         Failure: -1
+ */
+int cvmx_core_add_fixed_tlb_mapping(uint64_t vaddr, uint64_t page0_addr, uint64_t page1_addr, cvmx_tlb_pagemask_t page_mask)
+{
+
+    return(cvmx_core_add_fixed_tlb_mapping_bits(vaddr, page0_addr | TLB_DIRTY | TLB_VALID | TLB_GLOBAL, page1_addr | TLB_DIRTY | TLB_VALID | TLB_GLOBAL, page_mask));
+
+}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-core.h b/arch/mips/cavium-octeon/executive/cvmx-core.h
new file mode 100644
index 0000000..942dd40
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-core.h
@@ -0,0 +1,174 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Module to support operations on core such as TLB config, etc.
+ *
+ * <hr>$Revision: 35609 $<hr>
+ *
+ */
+
+
+#ifndef __CVMX_CORE_H__
+#define __CVMX_CORE_H__
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+/**
+ * The types of performance counters supported per cpu
+ */
+typedef enum
+{
+    CVMX_CORE_PERF_NONE      = 0,    /**< Turn off the performance counter */
+    CVMX_CORE_PERF_CLK       = 1,    /**< Conditionally clocked cycles (as opposed to count/cvm_count which count even with no clocks) */
+    CVMX_CORE_PERF_ISSUE     = 2,    /**< Instructions issued but not retired */
+    CVMX_CORE_PERF_RET       = 3,    /**< Instructions retired */
+    CVMX_CORE_PERF_NISSUE    = 4,    /**< Cycles no issue */
+    CVMX_CORE_PERF_SISSUE    = 5,    /**< Cycles single issue */
+    CVMX_CORE_PERF_DISSUE    = 6,    /**< Cycles dual issue */
+    CVMX_CORE_PERF_IFI       = 7,    /**< Cycle ifetch issued (but not necessarily commit to pp_mem) */
+    CVMX_CORE_PERF_BR        = 8,    /**< Branches retired */
+    CVMX_CORE_PERF_BRMIS     = 9,    /**< Branch mispredicts */
+    CVMX_CORE_PERF_J         = 10,   /**< Jumps retired */
+    CVMX_CORE_PERF_JMIS      = 11,   /**< Jumps mispredicted */
+    CVMX_CORE_PERF_REPLAY    = 12,   /**< Mem Replays */
+    CVMX_CORE_PERF_IUNA      = 13,   /**< Cycles idle due to unaligned_replays */
+    CVMX_CORE_PERF_TRAP      = 14,   /**< trap_6a signal */
+    CVMX_CORE_PERF_UULOAD    = 16,   /**< Unexpected unaligned loads (REPUN=1) */
+    CVMX_CORE_PERF_UUSTORE   = 17,   /**< Unexpected unaligned store (REPUN=1) */
+    CVMX_CORE_PERF_ULOAD     = 18,   /**< Unaligned loads (REPUN=1 or USEUN=1) */
+    CVMX_CORE_PERF_USTORE    = 19,   /**< Unaligned store (REPUN=1 or USEUN=1) */
+    CVMX_CORE_PERF_EC        = 20,   /**< Exec clocks(must set CvmCtl[DISCE] for accurate timing) */
+    CVMX_CORE_PERF_MC        = 21,   /**< Mul clocks(must set CvmCtl[DISCE] for accurate timing) */
+    CVMX_CORE_PERF_CC        = 22,   /**< Crypto clocks(must set CvmCtl[DISCE] for accurate timing) */
+    CVMX_CORE_PERF_CSRC      = 23,   /**< Issue_csr clocks(must set CvmCtl[DISCE] for accurate timing) */
+    CVMX_CORE_PERF_CFETCH    = 24,   /**< Icache committed fetches (demand+prefetch) */
+    CVMX_CORE_PERF_CPREF     = 25,   /**< Icache committed prefetches */
+    CVMX_CORE_PERF_ICA       = 26,   /**< Icache aliases */
+    CVMX_CORE_PERF_II        = 27,   /**< Icache invalidates */
+    CVMX_CORE_PERF_IP        = 28,   /**< Icache parity error */
+    CVMX_CORE_PERF_CIMISS    = 29,   /**< Cycles idle due to imiss (must set CvmCtl[DISCE] for accurate timing) */
+    CVMX_CORE_PERF_WBUF      = 32,   /**< Number of write buffer entries created */
+    CVMX_CORE_PERF_WDAT      = 33,   /**< Number of write buffer data cycles used (may need to set CvmCtl[DISCE] for accurate counts) */
+    CVMX_CORE_PERF_WBUFLD    = 34,   /**< Number of write buffer entries forced out by loads */
+    CVMX_CORE_PERF_WBUFFL    = 35,   /**< Number of cycles that there was no available write buffer entry (may need to set CvmCtl[DISCE] and CvmMemCtl[MCLK] for accurate counts) */
+    CVMX_CORE_PERF_WBUFTR    = 36,   /**< Number of stores that found no available write buffer entries */
+    CVMX_CORE_PERF_BADD      = 37,   /**< Number of address bus cycles used (may need to set CvmCtl[DISCE] for accurate counts) */
+    CVMX_CORE_PERF_BADDL2    = 38,   /**< Number of address bus cycles not reflected (i.e. destined for L2) (may need to set CvmCtl[DISCE] for accurate counts) */
+    CVMX_CORE_PERF_BFILL     = 39,   /**< Number of fill bus cycles used (may need to set CvmCtl[DISCE] for accurate counts) */
+    CVMX_CORE_PERF_DDIDS     = 40,   /**< Number of Dstream DIDs created */
+    CVMX_CORE_PERF_IDIDS     = 41,   /**< Number of Istream DIDs created */
+    CVMX_CORE_PERF_DIDNA     = 42,   /**< Number of cycles that no DIDs were available (may need to set CvmCtl[DISCE] and CvmMemCtl[MCLK] for accurate counts) */
+    CVMX_CORE_PERF_LDS       = 43,   /**< Number of load issues */
+    CVMX_CORE_PERF_LMLDS     = 44,   /**< Number of local memory load */
+    CVMX_CORE_PERF_IOLDS     = 45,   /**< Number of I/O load issues */
+    CVMX_CORE_PERF_DMLDS     = 46,   /**< Number of loads that were not prefetches and missed in the cache */
+    CVMX_CORE_PERF_STS       = 48,   /**< Number of store issues */
+    CVMX_CORE_PERF_LMSTS     = 49,   /**< Number of local memory store issues */
+    CVMX_CORE_PERF_IOSTS     = 50,   /**< Number of I/O store issues */
+    CVMX_CORE_PERF_IOBDMA    = 51,   /**< Number of IOBDMAs */
+    CVMX_CORE_PERF_DTLB      = 53,   /**< Number of dstream TLB refill, invalid, or modified exceptions */
+    CVMX_CORE_PERF_DTLBAD    = 54,   /**< Number of dstream TLB address errors */
+    CVMX_CORE_PERF_ITLB      = 55,   /**< Number of istream TLB refill, invalid, or address error exceptions */
+    CVMX_CORE_PERF_SYNC      = 56,   /**< Number of SYNC stall cycles (may need to set CvmCtl[DISCE] for accurate counts) */
+    CVMX_CORE_PERF_SYNCIOB   = 57,   /**< Number of SYNCIOBDMA stall cycles (may need to set CvmCtl[DISCE] for accurate counts) */
+    CVMX_CORE_PERF_SYNCW     = 58,   /**< Number of SYNCWs */
+    CVMX_CORE_PERF_MAX               /**< This not a counter, just a marker for the highest number */
+} cvmx_core_perf_t;
+
+/**
+ * Bit description of the COP0 counter control register
+ */
+typedef union
+{
+    uint32_t u32;
+    struct
+    {
+        uint32_t m              : 1; /**< Set to 1 for sel 0 and 0 for sel 2, indicating there are two performance counters */
+        uint32_t w              : 1; /**< Set to 1 indicating coutners are 64 bit */
+        uint32_t reserved_11_29 :19;
+        cvmx_core_perf_t event  : 6; /**< Selects the event to be counted by the corresponding Counter Register */
+        uint32_t ie             : 1; /**< Count in interrupt context */
+        uint32_t u              : 1; /**< Count in user mode */
+        uint32_t s              : 1; /**< Count in supervisor mode */
+        uint32_t k              : 1; /**< Count in kernel mode */
+        uint32_t ex             : 1; /**< Count in exception context */
+    } s;
+} cvmx_core_perf_control_t;
+
+typedef enum {
+    CVMX_TLB_PAGEMASK_4K   = 0x3     << 11,
+    CVMX_TLB_PAGEMASK_16K  = 0xF     << 11,
+    CVMX_TLB_PAGEMASK_64K  = 0x3F    << 11,
+    CVMX_TLB_PAGEMASK_256K = 0xFF    << 11,
+    CVMX_TLB_PAGEMASK_1M   = 0x3FF   << 11,
+    CVMX_TLB_PAGEMASK_4M   = 0xFFF   << 11,
+    CVMX_TLB_PAGEMASK_16M  = 0x3FFF  << 11,
+    CVMX_TLB_PAGEMASK_64M  = 0xFFFF  << 11,
+    CVMX_TLB_PAGEMASK_256M = 0x3FFFF << 11,
+} cvmx_tlb_pagemask_t;
+
+
+int cvmx_core_add_wired_tlb_entry(uint64_t hi, uint64_t lo0, uint64_t lo1, cvmx_tlb_pagemask_t page_mask);
+
+
+int cvmx_core_add_fixed_tlb_mapping(uint64_t vaddr, uint64_t page0_addr, uint64_t page1_addr, cvmx_tlb_pagemask_t page_mask);
+int cvmx_core_add_fixed_tlb_mapping_bits(uint64_t vaddr, uint64_t page0_addr, uint64_t page1_addr, cvmx_tlb_pagemask_t page_mask);
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif /* __CVMX_CORE_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-coremask.c b/arch/mips/cavium-octeon/executive/cvmx-coremask.c
new file mode 100644
index 0000000..1f8d2c9
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-coremask.c
@@ -0,0 +1,140 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Module to support operations on bitmap of cores. Coremask can be used to
+ * select a specific core, a group of cores, or all available cores, for
+ * initialization and differentiation of roles within a single shared binary
+ * executable image.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ *
+ */
+
+#include "cvmx-config.h"
+#include "cvmx.h"
+#include "cvmx-spinlock.h"
+#include "cvmx-coremask.h"
+
+
+#define  CVMX_COREMASK_MAX_SYNCS  20  /* maximum number of coremasks for barrier sync */
+
+/**
+ * This structure defines the private state maintained by coremask module.
+ *
+ */
+CVMX_SHARED static struct {
+
+    cvmx_spinlock_t            lock;       /**< mutex spinlock */
+
+    struct {
+
+        unsigned int           coremask;   /**< coremask specified for barrier */
+        unsigned int           checkin;    /**< bitmask of cores checking in */
+        volatile unsigned int  exit;       /**< variable to poll for exit condition */
+
+    } s[CVMX_COREMASK_MAX_SYNCS];
+
+} state = {
+
+    { CVMX_SPINLOCK_UNLOCKED_VAL },
+
+    { { 0, 0, 0 } },
+};
+
+
+/**
+ * Wait (stall) until all cores in the given coremask has reached this point
+ * in the program execution before proceeding.
+ *
+ * @param  coremask  the group of cores performing the barrier sync
+ *
+ */
+void cvmx_coremask_barrier_sync(unsigned int coremask)
+{
+    int i;
+    unsigned int target;
+
+    assert(coremask != 0);
+
+    cvmx_spinlock_lock(&state.lock);
+
+    for (i = 0; i < CVMX_COREMASK_MAX_SYNCS; i++) {
+
+        if (state.s[i].coremask == 0) {
+            /* end of existing coremask list, create new entry, fall-thru */
+            state.s[i].coremask = coremask;
+        }
+
+        if (state.s[i].coremask == coremask) {
+
+            target = state.s[i].exit + 1;  /* wrap-around at 32b */
+
+            state.s[i].checkin |= cvmx_coremask_core(cvmx_get_core_num());
+            if (state.s[i].checkin == coremask) {
+                state.s[i].checkin = 0;
+                state.s[i].exit = target;  /* signal exit condition */
+            }
+            cvmx_spinlock_unlock(&state.lock);
+
+            while (state.s[i].exit != target)
+                ;
+
+            return;
+        }
+    }
+
+    /* error condition - coremask array overflowed */
+    cvmx_spinlock_unlock(&state.lock);
+    assert(0);
+}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-coremask.h b/arch/mips/cavium-octeon/executive/cvmx-coremask.h
new file mode 100644
index 0000000..0290833
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-coremask.h
@@ -0,0 +1,169 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Module to support operations on bitmap of cores. Coremask can be used to
+ * select a specific core, a group of cores, or all available cores, for
+ * initialization and differentiation of roles within a single shared binary
+ * executable image.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ *
+ */
+
+
+#ifndef __CVMX_COREMASK_H__
+#define __CVMX_COREMASK_H__
+
+#include "cvmx-asm.h"
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+/*
+ * coremask is simply unsigned int (32 bits).
+ *
+ * NOTE: supports up to 32 cores maximum.
+ *
+ * union of coremasks is simply bitwise-or.
+ * intersection of coremasks is simply bitwise-and.
+ *
+ */
+
+#define  CVMX_COREMASK_MAX  0xFFFFFFFFu    /* maximum supported mask */
+
+
+/**
+ * Compute coremask for a specific core.
+ *
+ * @param  core_id  The core ID
+ *
+ * @return  coremask for a specific core
+ *
+ */
+static inline unsigned int cvmx_coremask_core(unsigned int core_id)
+{
+    return (1u << core_id);
+}
+
+/**
+ * Compute coremask for num_cores cores starting with core 0.
+ *
+ * @param  num_cores  number of cores
+ *
+ * @return  coremask for num_cores cores
+ *
+ */
+static inline unsigned int cvmx_coremask_numcores(unsigned int num_cores)
+{
+    return (CVMX_COREMASK_MAX >> (32 - num_cores));
+}
+
+/**
+ * Compute coremask for a range of cores from core low to core high.
+ *
+ * @param  low   first core in the range
+ * @param  high  last core in the range
+ *
+ * @return  coremask for the range of cores
+ *
+ */
+static inline unsigned int cvmx_coremask_range(unsigned int low, unsigned int high)
+{
+    return ((CVMX_COREMASK_MAX >> (31 - high + low)) << low);
+}
+
+
+/**
+ * Test to see if current core is a member of coremask.
+ *
+ * @param  coremask  the coremask to test against
+ *
+ * @return  1 if current core is a member of coremask, 0 otherwise
+ *
+ */
+static inline int cvmx_coremask_is_member(unsigned int coremask)
+{
+    return ((cvmx_coremask_core(cvmx_get_core_num()) & coremask) != 0);
+}
+
+/**
+ * Test to see if current core is first core in coremask.
+ *
+ * @param  coremask  the coremask to test against
+ *
+ * @return  1 if current core is first core in the coremask, 0 otherwise
+ *
+ */
+static inline int cvmx_coremask_first_core(unsigned int coremask)
+{
+    return cvmx_coremask_is_member(coremask)
+        && ((cvmx_get_core_num() == 0) ||
+            ((cvmx_coremask_numcores(cvmx_get_core_num()) & coremask) == 0));
+}
+
+/**
+ * Wait (stall) until all cores in the given coremask has reached this point
+ * in the program execution before proceeding.
+ *
+ * @param  coremask  the group of cores performing the barrier sync
+ *
+ */
+extern void cvmx_coremask_barrier_sync(unsigned int coremask);
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif /* __CVMX_COREMASK_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-cvmmem.h b/arch/mips/cavium-octeon/executive/cvmx-cvmmem.h
new file mode 100644
index 0000000..948a62d
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-cvmmem.h
@@ -0,0 +1,81 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Interfaces and definitions for processor local memory
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+
+#ifndef __CVMX_CVMMEM_H__
+#define __CVMX_CVMMEM_H__
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+
+
+
+
+
+
+
+
+
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif // __CVMX_CVMMEM_H__
diff --git a/arch/mips/cavium-octeon/executive/cvmx-dfa.c b/arch/mips/cavium-octeon/executive/cvmx-dfa.c
new file mode 100644
index 0000000..ba551fe
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-dfa.c
@@ -0,0 +1,128 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Support library for the hardware DFA engine.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+#include "executive-config.h"
+#ifdef CVMX_ENABLE_DFA_FUNCTIONS
+
+#include "cvmx-config.h"
+#include "cvmx.h"
+#include "cvmx-fau.h"
+#include "cvmx-dfa.h"
+
+
+
+/**
+ * Initialize the DFA hardware before use
+ */
+int cvmx_dfa_initialize(void)
+{
+    cvmx_dfa_difctl_t control;
+    void *initial_base_address;
+    cvmx_dfa_state_t initial_state;
+    if (!cvmx_octeon_dfa_present())
+    {
+        cvmx_dprintf("ERROR: attempting to initialize DFA when no DFA hardware present\n.");
+        return -1;
+    }
+
+    control.u64 = 0;
+    control.s.dwbcnt = CVMX_FPA_DFA_POOL_SIZE / 128;
+    control.s.pool = CVMX_FPA_DFA_POOL;
+    control.s.size = (CVMX_FPA_DFA_POOL_SIZE - 8) / sizeof(cvmx_dfa_command_t);
+    CVMX_SYNCWS;
+    cvmx_write_csr(CVMX_DFA_DIFCTL, control.u64);
+
+    initial_base_address = cvmx_fpa_alloc(CVMX_FPA_DFA_POOL);
+
+    initial_state.u64 = 0;
+    initial_state.s.base_address_div16 = (CAST64(initial_base_address))/16;
+    cvmx_fau_atomic_write64(CVMX_FAU_DFA_STATE, initial_state.u64);
+
+    CVMX_SYNCWS;
+    cvmx_write_csr(CVMX_DFA_DIFRDPTR, cvmx_ptr_to_phys(initial_base_address));
+
+    return 0;
+}
+
+
+/**
+ * Shutdown and cleanup resources used by the DFA
+ */
+void cvmx_dfa_shutdown(void)
+{
+    void *final_base_address;
+    cvmx_dfa_state_t final_state;
+
+    CVMX_SYNCWS;
+
+    final_state.u64 = cvmx_fau_fetch_and_add64(CVMX_FAU_DFA_STATE, 0);
+
+    // make sure the carry is clear
+    final_base_address = CASTPTR(void, (final_state.s2.base_address_div32 * 32ull));
+
+    if (final_base_address)
+    {
+        cvmx_fpa_free(final_base_address, CVMX_FPA_DFA_POOL, 0);
+    }
+
+    CVMX_SYNCWS;
+    final_state.u64 = 0;
+    cvmx_fau_atomic_write64(CVMX_FAU_DFA_STATE, final_state.u64);
+}
+
+#endif
diff --git a/arch/mips/cavium-octeon/executive/cvmx-dfa.h b/arch/mips/cavium-octeon/executive/cvmx-dfa.h
new file mode 100644
index 0000000..e4e44b3
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-dfa.h
@@ -0,0 +1,808 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Interface to the hardware DFA engine.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+
+#ifndef __CVMX_DFA_H__
+#define __CVMX_DFA_H__
+#include "cvmx-llm.h"
+#include "cvmx-wqe.h"
+#include "cvmx-fpa.h"
+
+#include "executive-config.h"
+#ifdef CVMX_ENABLE_DFA_FUNCTIONS
+#include "cvmx-config.h"
+#endif
+
+#define ENABLE_DEPRECATED   /* Set to enable the old 18/36 bit names */
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+
+/* Maximum nodes available in a small encoding */
+#define CVMX_DFA_NODESM_MAX_NODES       ((OCTEON_IS_MODEL(OCTEON_CN31XX)) ? 0x8000 : 0x20000)
+#define CVMX_DFA_NODESM_SIZE            512     /* Size of each node for small encoding */
+#define CVMX_DFA_NODELG_SIZE            1024    /* Size of each node for large encoding */
+#define CVMX_DFA_NODESM_LAST_TERMINAL  (CVMX_DFA_NODESM_MAX_NODES-1)
+
+#ifdef ENABLE_DEPRECATED
+/* These defines are for compatability with old code. They are deprecated */
+#define CVMX_DFA_NODE18_SIZE            CVMX_DFA_NODESM_SIZE
+#define CVMX_DFA_NODE36_SIZE            CVMX_DFA_NODELG_SIZE
+#define CVMX_DFA_NODE18_MAX_NODES       CVMX_DFA_NODESM_MAX_NODES
+#define CVMX_DFA_NODE18_LAST_TERMINAL   CVMX_DFA_NODESM_LAST_TERMINAL
+#endif
+
+/**
+ * Which type of memory encoding is this graph using. Make sure you setup
+ * the LLM to match.
+ */
+typedef enum
+{
+    CVMX_DFA_GRAPH_TYPE_SM              = 0,
+    CVMX_DFA_GRAPH_TYPE_LG              = 1,
+#ifdef ENABLE_DEPRECATED
+    CVMX_DFA_GRAPH_TYPE_18b             = 0,    /* Deprecated */
+    CVMX_DFA_GRAPH_TYPE_36b             = 1     /* Deprecated */
+#endif
+} cvmx_dfa_graph_type_t;
+
+/**
+ * The possible node types.
+ */
+typedef enum
+{
+    CVMX_DFA_NODE_TYPE_NORMAL           = 0,    /**< Node is a branch */
+    CVMX_DFA_NODE_TYPE_MARKED           = 1,    /**< Node is marked special */
+    CVMX_DFA_NODE_TYPE_TERMINAL         = 2     /**< Node is a terminal leaf */
+} cvmx_dfa_node_type_t;
+
+/**
+ * The possible reasons the DFA stopped processing.
+ */
+typedef enum
+{
+    CVMX_DFA_STOP_REASON_DATA_GONE      = 0,    /**< DFA ran out of data */
+    CVMX_DFA_STOP_REASON_PARITY_ERROR   = 1,    /**< DFA encountered a memory error */
+    CVMX_DFA_STOP_REASON_FULL           = 2,    /**< DFA is full */
+    CVMX_DFA_STOP_REASON_TERMINAL       = 3     /**< DFA hit a terminal */
+} cvmx_dfa_stop_reason_t;
+
+/**
+ * This format describes the DFA pointers in small mode
+ */
+typedef union
+{
+    uint64_t u64;
+    struct
+    {
+        uint64_t                mbz         :32;/**< Must be zero */
+        uint64_t                p1          : 1;/**< Set if next_node1 is odd parity */
+        uint64_t                next_node1  :15;/**< Next node if an odd character match */
+        uint64_t                p0          : 1;/**< Set if next_node0 is odd parity */
+        uint64_t                next_node0  :15;/**< Next node if an even character match */
+    } w32;
+    struct
+    {
+        uint64_t                mbz         :28;/**< Must be zero */
+        uint64_t                p1          : 1;/**< Set if next_node1 is odd parity */
+        uint64_t                next_node1  :17;/**< Next node if an odd character match */
+        uint64_t                p0          : 1;/**< Set if next_node0 is odd parity */
+        uint64_t                next_node0  :17;/**< Next node if an even character match */
+    } w36;
+    struct /**< @ this structure only applies starting in CN58XX and if DFA_CFG[NRPL_ENA] == 1 and IWORD0[NREPLEN] == 1.  */
+    {
+        uint64_t                mbz         :28;/**< Must be zero */
+        uint64_t                p1          : 1;/**< Set if next_node1 is odd parity */
+        uint64_t                per_node_repl1  : 1;/**< enable for extra replicaiton for next node (CN58XX) */
+        uint64_t                next_node_repl1 : 2;/**< extra replicaiton for next node (CN58XX) (if per_node_repl1 is set) */
+        uint64_t                next_node1  :14;/**< Next node if an odd character match - IWORD3[Msize], if per_node_repl1==1. */
+        uint64_t                p0          : 1;/**< Set if next_node0 is odd parity */
+        uint64_t                per_node_repl0  : 1;/**< enable for extra replicaiton for next node (CN58XX) */
+        uint64_t                next_node_repl0 : 2;/**< extra replicaiton for next node (CN58XX) (if per_node_repl0 is set) */
+        uint64_t                next_node0  :14;/**< Next node if an odd character match - IWORD3[Msize], if per_node_repl0==1. */
+    } w36nrepl_en; /**< use when next_node_repl[01] is 1. */
+    struct /**< this structure only applies starting in CN58XX and if DFA_CFG[NRPL_ENA] == 1 and IWORD0[NREPLEN] == 1.  */
+    {
+        uint64_t                mbz         :28;/**< Must be zero */
+        uint64_t                p1          : 1;/**< Set if next_node1 is odd parity */
+        uint64_t                per_node_repl1  : 1;/**< enable for extra replicaiton for next node (CN58XX) */
+        uint64_t                next_node1  :16;/**< Next node if an odd character match, if per_node_repl1==0. */
+        uint64_t                p0          : 1;/**< Set if next_node0 is odd parity */
+        uint64_t                per_node_repl0  : 1;/**< enable for extra replicaiton for next node (CN58XX) */
+        uint64_t                next_node0  :16;/**< Next node if an odd character match, if per_node_repl0==0. */
+    } w36nrepl_dis; /**< use when next_node_repl[01] is 0. */
+#if defined(ENABLE_DEPRECATED) && !OCTEON_IS_COMMON_BINARY()
+#if OCTEON_IS_MODEL(OCTEON_CN31XX)
+    struct /**< @deprecated unnamed reference to members */
+    {
+        uint64_t                mbz         :32;/**< Must be zero */
+        uint64_t                p1          : 1;/**< Set if next_node1 is odd parity */
+        uint64_t                next_node1  :15;/**< Next node if an odd character match */
+        uint64_t                p0          : 1;/**< Set if next_node0 is odd parity */
+        uint64_t                next_node0  :15;/**< Next node if an even character match */
+    };
+#elif OCTEON_IS_MODEL(OCTEON_CN38XX)
+    struct /**< @deprecated unnamed reference to members */
+    {
+        uint64_t                mbz         :28;/**< Must be zero */
+        uint64_t                p1          : 1;/**< Set if next_node1 is odd parity */
+        uint64_t                next_node1  :17;/**< Next node if an odd character match */
+        uint64_t                p0          : 1;/**< Set if next_node0 is odd parity */
+        uint64_t                next_node0  :17;/**< Next node if an even character match */
+    };
+#else
+    /* Other chips don't support the deprecated unnamed unions */
+#endif
+#endif
+} cvmx_dfa_node_next_sm_t;
+
+/**
+ * This format describes the DFA pointers in large mode
+ */
+typedef union
+{
+    uint64_t u64;
+    struct
+    {
+        uint64_t                mbz         :32;/**< Must be zero */
+        uint64_t                ecc         : 7;/**< ECC checksum on the rest of the bits */
+        cvmx_dfa_node_type_t    type        : 2;/**< Node type */
+        uint64_t                mbz2        : 3;/**< Must be zero */
+        uint64_t                next_node   :20;/**< Next node */
+    } w32;
+    struct
+    {
+        uint64_t                mbz         :28;/**< Must be zero */
+        uint64_t                ecc         : 7;/**< ECC checksum on the rest of the bits */
+        cvmx_dfa_node_type_t    type        : 2;/**< Node type */
+        uint64_t                extra_bits     : 5;/**< bits copied to report (PASS3/CN58XX), Must be zero previously */
+        uint64_t                next_node_repl : 2;/**< extra replicaiton for next node (PASS3/CN58XX), Must be zero previously */
+        uint64_t                next_node   :20;/**< Next node ID,  Note, combine with next_node_repl to use as start_node
+                                                     for continuation, as in cvmx_dfa_node_next_lgb_t. */
+    } w36;
+#if defined(ENABLE_DEPRECATED) && !OCTEON_IS_COMMON_BINARY()
+#if OCTEON_IS_MODEL(OCTEON_CN31XX)
+    struct /**< @deprecated unnamed reference to members */
+    {
+        uint64_t                mbz         :32;/**< Must be zero */
+        uint64_t                ecc         : 7;/**< ECC checksum on the rest of the bits */
+        cvmx_dfa_node_type_t    type        : 2;/**< Node type */
+        uint64_t                mbz2        : 3;/**< Must be zero */
+        uint64_t                next_node   :20;/**< Next node */
+    };
+#elif OCTEON_IS_MODEL(OCTEON_CN38XX)
+    struct /**< @deprecated unnamed reference to members */
+    {
+        uint64_t                mbz         :28;/**< Must be zero */
+        uint64_t                ecc         : 7;/**< ECC checksum on the rest of the bits */
+        cvmx_dfa_node_type_t    type        : 2;/**< Node type */
+        uint64_t                extra_bits     : 5;/**< bits copied to report (PASS3/CN58XX), Must be zero previously */
+        uint64_t                next_node_repl : 2;/**< extra replicaiton for next node (PASS3/CN58XX), Must be zero previously */
+        uint64_t                next_node   :20;/**< Next node ID,  Note, combine with next_node_repl to use as start_node
+                                                     for continuation, as in cvmx_dfa_node_next_lgb_t. */
+    };
+#else
+    /* Other chips don't support the deprecated unnamed unions */
+#endif
+#endif
+} cvmx_dfa_node_next_lg_t;
+
+/**
+ * This format describes the DFA pointers in large mode, another way
+ */
+typedef union
+{
+    uint64_t u64;
+    struct
+    {
+        uint64_t                mbz         :32;/**< Must be zero */
+        uint64_t                ecc         : 7;/**< ECC checksum on the rest of the bits */
+        uint64_t  		type_terminal : 1;/**< Node type */
+        uint64_t	        type_marked   : 1;/**< Node type */
+        uint64_t                mbz2        : 3;/**< Must be zero */
+        uint64_t                next_node   :20;/**< Next node */
+    } w32;
+    struct
+    {
+        uint64_t                mbz         :28;/**< Must be zero */
+        uint64_t                ecc         : 7;/**< ECC checksum on the rest of the bits */
+        uint64_t                type_terminal : 1;/**< Node type */
+        uint64_t                type_marked   : 1;/**< Node type */
+        uint64_t                extra_bits     : 5;/**< bits copied to report (PASS3/CN58XX), Must be zero previously */
+        uint64_t                next_node_id_and_repl   :22;/**< Next node ID (and repl for PASS3/CN58XX or repl=0 if not),
+                                                                 use this as start node for continuation. */
+    } w36;
+#if defined(ENABLE_DEPRECATED) && !OCTEON_IS_COMMON_BINARY()
+#if OCTEON_IS_MODEL(OCTEON_CN31XX)
+    struct /**< @deprecated unnamed reference to members */
+    {
+        uint64_t                mbz         :32;/**< Must be zero */
+        uint64_t                ecc         : 7;/**< ECC checksum on the rest of the bits */
+        uint64_t  		type_terminal : 1;/**< Node type */
+        uint64_t	        type_marked   : 1;/**< Node type */
+        uint64_t                mbz2        : 3;/**< Must be zero */
+        uint64_t                next_node   :20;/**< Next node */
+    };
+#elif OCTEON_IS_MODEL(OCTEON_CN38XX)
+    struct /**< @deprecated unnamed reference to members */
+    {
+        uint64_t                mbz         :28;/**< Must be zero */
+        uint64_t                ecc         : 7;/**< ECC checksum on the rest of the bits */
+        uint64_t                type_terminal : 1;/**< Node type */
+        uint64_t                type_marked   : 1;/**< Node type */
+        uint64_t                extra_bits     : 5;/**< bits copied to report (PASS3/CN58XX), Must be zero previously */
+        uint64_t                next_node_id_and_repl   :22;/**< Next node ID (and repl for PASS3/CN58XX or repl=0 if not),
+                                                                 use this as start node for continuation. */
+    };
+#else
+    /* Other chips don't support the deprecated unnamed unions */
+#endif
+#endif
+} cvmx_dfa_node_next_lgb_t;
+
+/**
+ * This format describes the DFA pointers in large mode
+ */
+typedef union
+{
+    uint64_t u64;
+    struct
+    {
+        uint64_t                mbz         :27;/**< Must be zero */
+        uint64_t                x0          : 1;/**< XOR of the rest of the bits */
+        uint64_t                reserved    : 4;/**< Must be zero */
+        uint64_t                data        :32;/**< LLM Data */
+    } w32;
+    struct
+    {
+        uint64_t                mbz         :27;/**< Must be zero */
+        uint64_t                x0          : 1;/**< XOR of the rest of the bits */
+        uint64_t                data        :36;/**< LLM Data */
+    } w36;
+#if defined(ENABLE_DEPRECATED) && !OCTEON_IS_COMMON_BINARY()
+#if OCTEON_IS_MODEL(OCTEON_CN31XX)
+    struct /**< @deprecated unnamed reference to members */
+    {
+        uint64_t                mbz         :27;/**< Must be zero */
+        uint64_t                x0          : 1;/**< XOR of the rest of the bits */
+        uint64_t                reserved    : 4;/**< Must be zero */
+        uint64_t                data        :32;/**< LLM Data */
+    };
+#elif OCTEON_IS_MODEL(OCTEON_CN38XX)
+    struct /**< @deprecated unnamed reference to members */
+    {
+        uint64_t                mbz         :27;/**< Must be zero */
+        uint64_t                x0          : 1;/**< XOR of the rest of the bits */
+        uint64_t                data        :36;/**< LLM Data */
+    };
+#else
+    /* Other chips don't support the deprecated unnamed unions */
+#endif
+#endif
+} cvmx_dfa_node_next_read_t;
+
+/**
+ * This structure defines the data format in the low-latency memory
+ */
+typedef union
+{
+    uint64_t u64;
+    cvmx_dfa_node_next_sm_t     sm;     /**< This format describes the DFA pointers in small mode */
+    cvmx_dfa_node_next_lg_t     lg;     /**< This format describes the DFA pointers in large mode */
+    cvmx_dfa_node_next_lgb_t    lgb;    /**< This format describes the DFA pointers in large mode, another way */
+    cvmx_dfa_node_next_read_t   read;   /**< This format describes the DFA pointers in large mode */
+#ifdef ENABLE_DEPRECATED
+    cvmx_dfa_node_next_sm_t     s18;    /**< Deprecated */
+    cvmx_dfa_node_next_lg_t     s36;    /**< Deprecated */
+    cvmx_dfa_node_next_lgb_t    s36b;   /**< Deprecated */
+#endif
+} cvmx_dfa_node_next_t;
+
+/**
+ * These structures define a DFA instruction
+ */
+typedef union
+{
+    uint64_t u64[4];
+    uint32_t u32;
+    struct
+    {
+        // WORD 0
+        uint64_t gxor                   : 8;   /**< Graph XOR value (PASS3/CN58XX), Must be zero for other chips
+                                                     or if DFA_CFG[GXOR_ENA] == 0.  */
+        uint64_t nxoren                 : 1;   /**< Node XOR enable (PASS3/CN58XX), Must be zero for other chips
+                                                     or if DFA_CFG[NXOR_ENA] == 0.  */
+        uint64_t nreplen                : 1;   /**< Node Replication mode enable (PASS3/CN58XX), Must be zero for other chips
+                                                     or if DFA_CFG[NRPL_ENA] == 0 or IWORD0[Ty] == 0.  */
+#if 0
+        uint64_t snrepl                 : 2;   /**< Start_Node Replication (PASS3/CN58XX), Must be zero for other chips
+                                                     or if DFA_CFG[NRPL_ENA] == 0 or IWORD0[Ty] == 0 or IWORD0[NREPLEN] == 0.  */
+        uint64_t start_node_id          : 20;   /**< Node to start the walk from */
+#else
+        uint64_t start_node             : 22;   /**< Node to start the walk from, includes ID and snrepl, see notes above. */
+#endif
+
+        uint64_t unused02               :  2;   /**< Must be zero */
+        cvmx_llm_replication_t replication : 2; /**< Type of memory replication to use */
+        uint64_t unused03               :  3;   /**< Must be zero */
+        cvmx_dfa_graph_type_t type      :  1;   /**< Type of graph */
+        uint64_t unused04               :  4;   /**< Must be zero */
+        uint64_t base                   : 20;   /**< All tables start on 1KB boundary */
+
+        // WORD 1
+        uint64_t input_length           : 16;   /**< In bytes, # pointers in gather case */
+        uint64_t use_gather             :  1;   /**< Set to use gather */
+        uint64_t no_L2_alloc            :  1;   /**< Set to disable loading of the L2 cache by the DFA */
+        uint64_t full_block_write       :  1;   /**< If set, HW can write entire cache blocks @ result_ptr */
+        uint64_t little_endian          :  1;   /**< Affects only packet data, not instruction, gather list, or result */
+        uint64_t unused1                :  8;   /**< Must be zero */
+        uint64_t data_ptr               : 36;   /**< Either directly points to data or the gather list. If gather list,
+                                                    data_ptr<2:0> must be zero (i.e. 8B aligned) */
+        // WORD 2
+        uint64_t max_results            : 16;   /**< in 64-bit quantities, mbz for store */
+        uint64_t unused2                : 12;   /**< Must be zero */
+        uint64_t result_ptr             : 36;   /**< must be 128 byte aligned */
+
+        // WORD 3
+        uint64_t tsize                  :  8;   /**< tsize*256 is the number of terminal nodes for GRAPH_TYPE_SM */
+        uint64_t msize                  : 16;   /**< msize is the number of marked nodes for GRAPH_TYPE_SM */
+        uint64_t unused3                :  4;   /**< Must be zero */
+        uint64_t wq_ptr                 : 36;   /**< 0 for no work queue entry creation */
+    } s;
+} cvmx_dfa_command_t;
+
+/**
+ * Format of the first result word written by the hardware.
+ */
+typedef union
+{
+    uint64_t u64;
+    struct
+    {
+        cvmx_dfa_stop_reason_t  reas        : 2;/**< Reason the DFA stopped */
+        uint64_t                mbz         :44;/**< Zero */
+        uint64_t                last_marked : 1;/**< Set if the last entry written is marked */
+        uint64_t                done        : 1;/**< Set to 1 when the DFA completes */
+        uint64_t                num_entries :16;/**< Number of result words written */
+    } s;
+} cvmx_dfa_result0_t;
+
+/**
+ * Format of the second result word and subsequent result words written by the hardware.
+ */
+typedef union
+{
+    uint64_t u64;
+    struct
+    {
+        uint64_t byte_offset    : 16;   /**< Number of bytes consumed */
+        uint64_t extra_bits_high:  4;   /**< If PASS3 or CN58XX and DFA_CFG[NRPL_ENA] == 1 and IWORD0[Ty] == 1,
+                                             then set to <27:24> of the last next-node pointer. Else set to 0x0.  */
+        uint64_t prev_node      : 20;   /**< Index of the previous node */
+        uint64_t extra_bits_low :  2;   /**< If PASS3 or CN58XX and DFA_CFG[NRPL_ENA] == 1 and IWORD0[Ty] == 1,
+                                             then set to <23:22> of the last next-node pointer. Else set to 0x0.  */
+        uint64_t next_node_repl :  2;   /**< If PASS3 or CN58XX and DFA_CFG[NRPL_ENA] == 1 and IWORD0[Ty] == 1, then set
+                                             to next_node_repl (<21:20>) of the last next-node pointer. Else set to 0x0.  */
+        uint64_t current_node   : 20;   /**< Index of the current node */
+    } s;
+    struct
+    {
+        uint64_t byte_offset    : 16;   /**< Number of bytes consumed */
+        uint64_t extra_bits_high:  4;   /**< If PASS3 or CN58XX and DFA_CFG[NRPL_ENA] == 1 and IWORD0[Ty] == 1,
+                                             then set to <27:24> of the last next-node pointer. Else set to 0x0.  */
+        uint64_t prev_node      : 20;   /**< Index of the previous node */
+        uint64_t extra_bits_low :  2;   /**< If PASS3 or CN58XX and DFA_CFG[NRPL_ENA] == 1 and IWORD0[Ty] == 1,
+                                             then set to <23:22> of the last next-node pointer. Else set to 0x0.  */
+        uint64_t curr_id_and_repl:22;   /**< Use ths as start_node for continuation. */
+    } s2;
+} cvmx_dfa_result1_t;
+
+/**
+ * Abstract DFA graph
+ */
+typedef struct
+{
+    cvmx_llm_replication_t      replication;        /**< Level of memory replication to use. Must match the LLM setup */
+    cvmx_dfa_graph_type_t       type;               /**< Type of graph */
+    uint64_t                    base_address;       /**< LLM start address of the graph */
+    union {
+        struct {
+            uint64_t            gxor         : 8;   /**< Graph XOR value (PASS3/CN58XX), Must be zero for other chips
+                                                          or if DFA_CFG[GXOR_ENA] == 0.  */
+            uint64_t            nxoren       : 1;   /**< Node XOR enable (PASS3/CN58XX), Must be zero for other chips
+                                                          or if DFA_CFG[NXOR_ENA] == 0.  */
+            uint64_t            nreplen      : 1;   /**< Node Replication mode enable (PASS3/CN58XX), Must be zero for other chips
+                                                          or if DFA_CFG[NRPL_ENA] == 0 or IWORD0[Ty] == 0.  */
+            uint64_t            snrepl       : 2;   /**< Start_Node Replication (PASS3/CN58XX), Must be zero for other chips
+                                                          or if DFA_CFG[NRPL_ENA] == 0 or IWORD0[Ty] == 0 or IWORD0[NREPLEN] == 0.*/
+            uint64_t            start_node_id : 20; /**< Start node index for the root of the graph */
+        };
+        uint32_t                start_node;         /**< Start node index for the root of the graph, incl. snrepl (PASS3/CN58XX)
+                                                           NOTE: for backwards compatibility this name includes the the
+                                                                 gxor, nxoren, nreplen, and snrepl fields which will all be
+                                                                 zero in applicaitons existing before the introduction of these
+                                                                 fields, so that existing applicaiton do not need to change. */
+    };
+    int                         num_terminal_nodes; /**< Number of terminal nodes in the graph. Only needed for small graphs. */
+    int                         num_marked_nodes;   /**< Number of marked nodes in the graph. Only needed for small graphs. */
+} cvmx_dfa_graph_t;
+
+/**
+ * DFA internal global state -- stored in 8 bytes of FAU
+ */
+typedef union
+{
+    uint64_t u64;
+    struct {
+#define CVMX_DFA_STATE_TICKET_BIT_POS 16
+#if __BYTE_ORDER == __BIG_ENDIAN
+	// NOTE:  must clear LSB of base_address_div16 due to ticket overflow
+	uint32_t		base_address_div16;  /**< Current DFA instruction queue chunck base address/16 (clear LSB). */
+	uint8_t			ticket_loops;	     /**< bits [15:8] of total number of tickets requested. */
+	uint8_t			ticket;		     /**< bits [7:0] of total number of tickets requested (current ticket held). */
+	// NOTE: index and now_serving are written together
+	uint8_t			now_serving;	     /**< current ticket being served (or ready to be served). */
+	uint8_t			index;		     /**< index into current chunk: (base_address_div16*16)[index] = next entry. */
+#else	// NOTE: little endian mode probably won't work
+	uint8_t			index;
+	uint8_t			now_serving;
+	uint8_t			ticket;
+	uint8_t			ticket_loops;
+	uint32_t		base_address_div16;
+#endif
+    } s;
+    struct {	// a bitfield version of the same thing to extract base address while clearing carry.
+#if __BYTE_ORDER == __BIG_ENDIAN
+	uint64_t		base_address_div32	: 31;	/**< Current DFA instruction queue chunck base address/32. */
+	uint64_t		carry			: 1;	/**< Carry out from total_tickets. */
+	uint64_t		total_tickets		: 16;	/**< Total tickets. */
+	uint64_t		now_serving		: 8 ;	/**< current ticket being served (or ready to be served). */
+	uint64_t		index			: 8 ;   /**< index into current chunk. */
+#else	// NOTE: little endian mode probably won't work
+	uint64_t		index			: 8 ;
+	uint64_t		now_serving		: 8 ;
+	uint64_t		total_tickets		: 16;
+	uint64_t		carry			: 1;
+	uint64_t		base_address_div32	: 31;
+#endif
+    } s2;
+} cvmx_dfa_state_t;
+
+/* CSR typedefs have been moved to cvmx-csr-*.h */
+
+/**
+ * Write a small node edge to LLM.
+ *
+ * @param graph  Graph to modify
+ * @param source_node
+ *               Source node for this edge
+ * @param match_index
+ *               Index into the node edge table. This is the match character/2.
+ * @param destination_node0
+ *               Destination if the character matches (match_index*2).
+ * @param destination_node1
+ *               Destination if the character matches (match_index*2+1).
+ */
+static inline void cvmx_dfa_write_edge_sm(const cvmx_dfa_graph_t *graph,
+                                         uint64_t source_node, uint64_t match_index,
+                                         uint64_t destination_node0, uint64_t destination_node1)
+{
+    cvmx_llm_address_t address;
+    cvmx_dfa_node_next_t    next_ptr;
+
+    address.u64 = graph->base_address + source_node * CVMX_DFA_NODESM_SIZE + match_index * 4;
+
+    next_ptr.u64 = 0;
+    if (OCTEON_IS_MODEL(OCTEON_CN31XX))
+    {
+        next_ptr.sm.w32.next_node0 = destination_node0;
+        next_ptr.sm.w32.p0 = cvmx_llm_parity(destination_node0);
+
+        next_ptr.sm.w32.next_node1 = destination_node1;
+        next_ptr.sm.w32.p1 = cvmx_llm_parity(destination_node1);
+    }
+    else
+    {
+        next_ptr.sm.w36.next_node0 = destination_node0;
+        next_ptr.sm.w36.p0 = cvmx_llm_parity(destination_node0);
+
+        next_ptr.sm.w36.next_node1 = destination_node1;
+        next_ptr.sm.w36.p1 = cvmx_llm_parity(destination_node1);
+    }
+
+    cvmx_llm_write36(address, next_ptr.u64, 0);
+}
+#ifdef ENABLE_DEPRECATED
+#define cvmx_dfa_write_edge18 cvmx_dfa_write_edge_sm
+#endif
+
+
+/**
+ * Write a large node edge to LLM.
+ *
+ * @param graph  Graph to modify
+ * @param source_node
+ *               Source node for this edge
+ * @param match  Character to match before taking this edge.
+ * @param destination_node
+ *               Destination node of the edge.
+ * @param destination_type
+ *               Node type at the end of this edge.
+ */
+static inline void cvmx_dfa_write_node_lg(const cvmx_dfa_graph_t *graph,
+                                         uint64_t source_node, unsigned char match,
+                                         uint64_t destination_node, cvmx_dfa_node_type_t destination_type)
+{
+    cvmx_llm_address_t      address;
+    cvmx_dfa_node_next_t    next_ptr;
+
+    address.u64 = graph->base_address + source_node * CVMX_DFA_NODELG_SIZE + (uint64_t)match * 4;
+
+    next_ptr.u64 = 0;
+    if (OCTEON_IS_MODEL(OCTEON_CN31XX))
+    {
+        next_ptr.lg.w32.type = destination_type;
+        next_ptr.lg.w32.next_node = destination_node;
+        next_ptr.lg.w32.ecc = cvmx_llm_ecc(next_ptr.u64);
+    }
+    else
+    {
+        next_ptr.lg.w36.type = destination_type;
+        next_ptr.lg.w36.next_node = destination_node;
+        next_ptr.lg.w36.ecc = cvmx_llm_ecc(next_ptr.u64);
+    }
+
+    cvmx_llm_write36(address, next_ptr.u64, 0);
+}
+#ifdef ENABLE_DEPRECATED
+#define cvmx_dfa_write_node36 cvmx_dfa_write_node_lg
+#endif
+
+/**
+ * Ring the DFA doorbell telling it that new commands are
+ * available.
+ *
+ * @param num_commands
+ *               Number of new commands
+ */
+static inline void cvmx_dfa_write_doorbell(uint64_t num_commands)
+{
+    CVMX_SYNCWS;
+    cvmx_write_csr(CVMX_DFA_DBELL, num_commands);
+}
+
+/**
+ * @INTERNAL
+ * Write a new command to the DFA. Calls to this function
+ * are internally synchronized across all processors, and
+ * the doorbell is rung during this function.
+ *
+ * @param command Command to write
+ */
+
+#ifdef CVMX_ENABLE_DFA_FUNCTIONS
+static inline void __cvmx_dfa_write_command(cvmx_dfa_command_t *command)
+{
+    cvmx_dfa_state_t cvmx_dfa_state;
+    uint64_t my_ticket;	// needs to wrap to 8 bits
+    uint64_t index;
+    cvmx_dfa_command_t *head;
+
+    CVMX_PREFETCH0(command);
+    // take a ticket.
+    cvmx_dfa_state.u64 = cvmx_fau_fetch_and_add64(CVMX_FAU_DFA_STATE, 1ull<<CVMX_DFA_STATE_TICKET_BIT_POS);
+    my_ticket = cvmx_dfa_state.s.ticket;
+
+    // see if it is our turn
+    while (my_ticket != cvmx_dfa_state.s.now_serving) {
+	int delta = my_ticket - cvmx_dfa_state.s.now_serving;
+	if (delta < 0) delta += 256;
+	cvmx_wait(10*delta);	// reduce polling load on system
+	cvmx_dfa_state.u64 = cvmx_fau_fetch_and_add64(CVMX_FAU_DFA_STATE, 0);		// poll for my_ticket==now_serving
+    }
+
+    // compute index and instruction queue head pointer
+    index = cvmx_dfa_state.s.index;
+
+    // NOTE: the DFA only supports 36-bit addressing
+    head = &((CASTPTR(cvmx_dfa_command_t, (cvmx_dfa_state.s2.base_address_div32 * 32ull))[index]));
+    head = (cvmx_dfa_command_t*)cvmx_phys_to_ptr(CAST64(head));	// NOTE: since we are not storing bit 63 of address, we must set it now
+
+    // copy the command to the instruction queue
+    *head++ = *command;
+
+    // check if a new chunk is needed
+    if (cvmx_unlikely((++index >= ((CVMX_FPA_DFA_POOL_SIZE-8)/sizeof(cvmx_dfa_command_t))))) {
+        uint64_t *new_base = (uint64_t*)cvmx_fpa_alloc(CVMX_FPA_DFA_POOL);	// could make this async
+        if (new_base) {
+	    // put the link into the instruction queue's "Next Chunk Buffer Ptr"
+            *(uint64_t *)head = cvmx_ptr_to_phys(new_base);
+	    // update our state (note 32-bit write to not disturb other fields)
+            cvmx_fau_atomic_write32((cvmx_fau_reg_32_t)(CVMX_FAU_DFA_STATE + (CAST64(&cvmx_dfa_state.s.base_address_div16)-CAST64(&cvmx_dfa_state))),
+		    (CAST64(new_base))/16);
+        }
+        else {
+            cvmx_dprintf("__cvmx_dfa_write_command: Out of memory. Expect crashes.\n");
+        }
+	index=0;
+    }
+
+    cvmx_dfa_write_doorbell(1);
+
+    // update index and now_serving in the DFA state FAU location (NOTE: this write16 updates to 8-bit values.)
+    // NOTE: my_ticket+1 carry out is lost due to write16 and index has already been wrapped to fit in uint8.
+    cvmx_fau_atomic_write16((cvmx_fau_reg_16_t)(CVMX_FAU_DFA_STATE+(CAST64(&cvmx_dfa_state.s.now_serving) - CAST64(&cvmx_dfa_state))),
+	    ((my_ticket+1)<<8) | index);
+}
+
+
+/**
+ * Submit work to the DFA units for processing
+ *
+ * @param graph   Graph to process
+ * @param start_node
+ *                The node to start (or continue) walking from
+ *                includes. start_node_id and snrepl (PASS3/CN58XX), but gxor,
+ *                nxoren, and nreplen are taken from the graph structure
+ * @param input   The input to match against
+ * @param input_length
+ *                The length of the input in bytes
+ * @param use_gather
+ *		  The input and input_length are of a gather list
+ * @param is_little_endian
+ *                Set to 1 if the input is in little endian format and must
+ *                be swapped before compare.
+ * @param result  Location the DFA should put the results in. This must be
+ *                an area sized in multiples of a cache line.
+ * @param max_results
+ *                The maximum number of 64-bit result1 words after result0.
+ *                That is, "size of the result area in 64-bit words" - 1.
+ *                max_results must be at least 1.
+ * @param work    Work queue entry to submit when DFA completes. Can be NULL.
+ */
+static inline void cvmx_dfa_submit(const cvmx_dfa_graph_t *graph, int start_node,
+                                  void *input, int input_length, int use_gather, int is_little_endian,
+                                  cvmx_dfa_result0_t *result, int max_results, cvmx_wqe_t *work)
+{
+    cvmx_dfa_command_t command;
+
+    /* Make sure the result's first 64bit word is zero so we can tell when the
+        DFA is done. */
+    result->u64 = 0;
+
+    // WORD 0
+    command.u64[0] = 0;
+    command.s.gxor          = graph->gxor;      // (PASS3/CN58XX)
+    command.s.nxoren        = graph->nxoren;    // (PASS3/CN58XX)
+    command.s.nreplen       = graph->nreplen;   // (PASS3/CN58XX)
+    command.s.start_node    = start_node;       // includes snrepl (PASS3/CN58XX)
+    command.s.replication   = graph->replication;
+    command.s.type          = graph->type;
+    command.s.base          = graph->base_address>>10;
+
+    // WORD 1
+    command.u64[1] = 0;
+    command.s.input_length  = input_length;
+    command.s.use_gather   = use_gather;
+    command.s.no_L2_alloc   = 0;
+    command.s.full_block_write = 1;
+    command.s.little_endian = is_little_endian;
+    command.s.data_ptr      = cvmx_ptr_to_phys(input);
+
+    // WORD 2
+    command.u64[2] = 0;
+    command.s.max_results   = max_results;
+    command.s.result_ptr    = cvmx_ptr_to_phys(result);
+
+    // WORD 3
+    command.u64[3] = 0;
+    if (graph->type == CVMX_DFA_GRAPH_TYPE_SM)
+    {
+        command.s.tsize     = (graph->num_terminal_nodes + 255) / 256;
+        command.s.msize     = graph->num_marked_nodes;
+    }
+    command.s.wq_ptr        = cvmx_ptr_to_phys(work);
+
+    __cvmx_dfa_write_command(&command);	// NOTE: this does synchronization and rings doorbell
+}
+#endif
+
+/**
+ * DFA gather list element
+ */
+typedef struct {
+    uint64_t length         : 16;   /**< length of piece of data at addr */
+    uint64_t reserved       : 12;   /**< reserved, set to 0 */
+    uint64_t addr           : 36;   /**< pointer to piece of data */
+} cvmx_dfa_gather_entry_t;
+
+
+/**
+ * Check if a DFA has completed processing
+ *
+ * @param result_ptr Result area the DFA is using
+ * @return Non zero if the DFA is done
+ */
+static inline uint64_t cvmx_dfa_is_done(cvmx_dfa_result0_t *result_ptr)
+{
+    /* DFA sets the first result 64bit word to non zero when it's done */
+    return ((volatile cvmx_dfa_result0_t *)result_ptr)->s.done;
+}
+
+
+#ifdef CVMX_ENABLE_DFA_FUNCTIONS
+/**
+ * Initialize the DFA hardware before use
+ * Returns 0 on success, -1 on failure
+ */
+int cvmx_dfa_initialize(void);
+
+
+/**
+ * Shutdown and cleanup resources used by the DFA
+ */
+void cvmx_dfa_shutdown(void);
+#endif
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif /* __CVMX_DFA_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-dma-engine.c b/arch/mips/cavium-octeon/executive/cvmx-dma-engine.c
new file mode 100644
index 0000000..d2a3f77
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-dma-engine.c
@@ -0,0 +1,391 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Interface to the PCIe DMA engines. These are only avialable
+ * on chips with PCIe.
+ *
+ * <hr>$Revision: 35733 $<hr>
+ */
+#include "executive-config.h"
+#include "cvmx-config.h"
+#include "cvmx.h"
+#include "cvmx-cmd-queue.h"
+#include "cvmx-dma-engine.h"
+
+#ifdef CVMX_ENABLE_PKO_FUNCTIONS
+
+/**
+ * Return the number of DMA engimes supported by this chip
+ *
+ * @return Number of DMA engines
+ */
+int cvmx_dma_engine_get_num(void)
+{
+    if (octeon_has_feature(OCTEON_FEATURE_PCIE))
+    {
+        if (OCTEON_IS_MODEL(OCTEON_CN52XX_PASS1))
+            return 4;
+        else
+            return 5;
+    }
+    else
+        return 0;
+}
+
+/**
+ * Initialize the DMA engines for use
+ *
+ * @return Zero on success, negative on failure
+ */
+int cvmx_dma_engine_initialize(void)
+{
+    cvmx_npei_dma_control_t dma_control;
+    cvmx_npei_dmax_ibuff_saddr_t dmax_ibuff_saddr;
+    int engine;
+
+    for (engine=0; engine < cvmx_dma_engine_get_num(); engine++)
+    {
+        cvmx_cmd_queue_result_t result;
+        result = cvmx_cmd_queue_initialize(CVMX_CMD_QUEUE_DMA(engine),
+                                           0, CVMX_FPA_OUTPUT_BUFFER_POOL,
+                                           CVMX_FPA_OUTPUT_BUFFER_POOL_SIZE);
+        if (result != CVMX_CMD_QUEUE_SUCCESS)
+            return -1;
+        dmax_ibuff_saddr.u64 = 0;
+        dmax_ibuff_saddr.s.saddr = cvmx_ptr_to_phys(cvmx_cmd_queue_buffer(CVMX_CMD_QUEUE_DMA(engine))) >> 7;
+        cvmx_write_csr(CVMX_PEXP_NPEI_DMAX_IBUFF_SADDR(engine), dmax_ibuff_saddr.u64);
+    }
+
+    dma_control.u64 = 0;
+    if (cvmx_dma_engine_get_num() >= 5)
+        dma_control.s.dma4_enb = 1;
+    dma_control.s.dma3_enb = 1;
+    dma_control.s.dma2_enb = 1;
+    dma_control.s.dma1_enb = 1;
+    dma_control.s.dma0_enb = 1;
+    //dma_control.s.dwb_denb = 1;
+    //dma_control.s.dwb_ichk = CVMX_FPA_OUTPUT_BUFFER_POOL_SIZE/128;
+    dma_control.s.fpa_que = CVMX_FPA_OUTPUT_BUFFER_POOL;
+    dma_control.s.csize = CVMX_FPA_OUTPUT_BUFFER_POOL_SIZE/8;
+    cvmx_write_csr(CVMX_PEXP_NPEI_DMA_CONTROL, dma_control.u64);
+
+    return 0;
+}
+
+
+/**
+ * Shutdown all DMA engines. The engeines must be idle when this
+ * function is called.
+ *
+ * @return Zero on success, negative on failure
+ */
+int cvmx_dma_engine_shutdown(void)
+{
+    cvmx_npei_dma_control_t dma_control;
+    int engine;
+
+    for (engine=0; engine < cvmx_dma_engine_get_num(); engine++)
+    {
+        if (cvmx_cmd_queue_length(CVMX_CMD_QUEUE_DMA(engine)))
+        {
+            cvmx_dprintf("ERROR: cvmx_dma_engine_shutdown: Engine not idle.\n");
+            return -1;
+        }
+    }
+
+    dma_control.u64 = cvmx_read_csr(CVMX_PEXP_NPEI_DMA_CONTROL);
+    if (cvmx_dma_engine_get_num() >= 5)
+        dma_control.s.dma4_enb = 0;
+    dma_control.s.dma3_enb = 0;
+    dma_control.s.dma2_enb = 0;
+    dma_control.s.dma1_enb = 0;
+    dma_control.s.dma0_enb = 0;
+    cvmx_write_csr(CVMX_PEXP_NPEI_DMA_CONTROL, dma_control.u64);
+    /* Make sure the disable completes */
+    cvmx_read_csr(CVMX_PEXP_NPEI_DMA_CONTROL);
+
+    for (engine=0; engine < cvmx_dma_engine_get_num(); engine++)
+    {
+        cvmx_cmd_queue_shutdown(CVMX_CMD_QUEUE_DMA(engine));
+        cvmx_write_csr(CVMX_PEXP_NPEI_DMAX_IBUFF_SADDR(engine), 0);
+    }
+
+    return 0;
+}
+
+
+/**
+ * Submit a series of DMA comamnd to the DMA engines.
+ *
+ * @param engine  Engine to submit to (0-4)
+ * @param header  Command header
+ * @param num_buffers
+ *                The number of data pointers
+ * @param buffers Comamnd data pointers
+ *
+ * @return Zero on success, negative on failure
+ */
+int cvmx_dma_engine_submit(int engine, cvmx_dma_engine_header_t header, int num_buffers, cvmx_dma_engine_buffer_t buffers[])
+{
+    cvmx_cmd_queue_result_t result;
+    int cmd_count = 1;
+    uint64_t cmds[num_buffers + 1];
+
+    /* Check for Errata PCIe-604 */
+    if ((header.s.nfst > 11) || (header.s.nlst > 11) || (header.s.nfst + header.s.nlst > 15))
+    {
+        cvmx_dprintf("DMA engine submit too large\n");
+        return -1;
+    }
+
+    cmds[0] = header.u64;
+    while (num_buffers--)
+    {
+        cmds[cmd_count++] = buffers->u64;
+        buffers++;
+    }
+
+    result = cvmx_cmd_queue_write(CVMX_CMD_QUEUE_DMA(engine), 1, cmd_count, cmds);
+    /* DMA doorbells are 32bit writes in little endian space. This means we need to xor the address with 4 */
+    if (result == CVMX_CMD_QUEUE_SUCCESS)
+        cvmx_write64_uint32(CVMX_PEXP_NPEI_DMAX_DBELL(engine)^4, cmd_count);
+    return result;
+}
+
+
+/**
+ * @INTERNAL
+ * Function used by cvmx_dma_engine_transfer() to build the
+ * internal address list.
+ *
+ * @param buffers Location to store the list
+ * @param address Address to build list for
+ * @param size    Length of the memory pointed to by address
+ *
+ * @return Number of internal pointer chunks created
+ */
+static inline int __cvmx_dma_engine_build_internal_pointers(cvmx_dma_engine_buffer_t *buffers, uint64_t address, int size)
+{
+    int segments = 0;
+    while (size)
+    {
+        /* Each internal chunk can contain a maximum of 8191 bytes */
+        int chunk = size;
+        if (chunk > 8191)
+            chunk = 8191;
+        buffers[segments].u64 = 0;
+        buffers[segments].internal.size = chunk;
+        buffers[segments].internal.addr = address;
+        address += chunk;
+        size -= chunk;
+        segments++;
+    }
+    return segments;
+}
+
+
+/**
+ * @INTERNAL
+ * Function used by cvmx_dma_engine_transfer() to build the
+ * PCIe address list.
+ * @param buffers Location to store the list
+ * @param address Address to build list for
+ * @param size    Length of the memory pointed to by address
+ *
+ * @return Number of PCIe address chunks created. The number of words used
+ *         will be segments + (segments-1)/4 + 1.
+ */
+static inline int __cvmx_dma_engine_build_external_pointers(cvmx_dma_engine_buffer_t *buffers, uint64_t address, int size)
+{
+    int segments = 0;
+    while (size)
+    {
+        /* Each block of 4 PCIe pointers uses one dword for lengths followed by
+            up to 4 addresses. This then repeats if more data is needed */
+        buffers[0].u64 = 0;
+        if (size <= 65535)
+        {
+            /* Only one more segment needed */
+            buffers[0].pcie_length.len0 = size;
+            buffers[1].u64 = address;
+            segments++;
+            break;
+        }
+        else if (size <= 65535 * 2)
+        {
+            /* Two more segments needed */
+            buffers[0].pcie_length.len0 = 65535;
+            buffers[0].pcie_length.len1 = size - 65535;
+            buffers[1].u64 = address;
+            address += 65535;
+            buffers[2].u64 = address;
+            segments+=2;
+            break;
+        }
+        else if (size <= 65535 * 3)
+        {
+            /* Three more segments needed */
+            buffers[0].pcie_length.len0 = 65535;
+            buffers[0].pcie_length.len1 = 65535;
+            buffers[0].pcie_length.len2 = size - 65535 * 2;
+            buffers[1].u64 = address;
+            address += 65535;
+            buffers[2].u64 = address;
+            address += 65535;
+            buffers[3].u64 = address;
+            segments+=3;
+            break;
+        }
+        else if (size <= 65535 * 4)
+        {
+            /* Four more segments needed */
+            buffers[0].pcie_length.len0 = 65535;
+            buffers[0].pcie_length.len1 = 65535;
+            buffers[0].pcie_length.len2 = 65535;
+            buffers[0].pcie_length.len3 = size - 65535 * 3;
+            buffers[1].u64 = address;
+            address += 65535;
+            buffers[2].u64 = address;
+            address += 65535;
+            buffers[3].u64 = address;
+            address += 65535;
+            buffers[4].u64 = address;
+            segments+=4;
+            break;
+        }
+        else
+        {
+            /* Five or more segments are needed */
+            buffers[0].pcie_length.len0 = 65535;
+            buffers[0].pcie_length.len1 = 65535;
+            buffers[0].pcie_length.len2 = 65535;
+            buffers[0].pcie_length.len3 = 65535;
+            buffers[1].u64 = address;
+            address += 65535;
+            buffers[2].u64 = address;
+            address += 65535;
+            buffers[3].u64 = address;
+            address += 65535;
+            buffers[4].u64 = address;
+            address += 65535;
+            size -= 65535*4;
+            buffers += 5;
+            segments+=4;
+        }
+    }
+    return segments;
+}
+
+
+/**
+ * Build the first and last pointers based on a DMA engine header
+ * and submit them to the engine. The purpose of this function is
+ * to simplify the building of DMA engine commands by automatically
+ * converting a simple address and size into the apropriate internal
+ * or PCIe address list. This function does not support gather lists,
+ * so you will need to build your own lists in that case.
+ *
+ * @param engine Engine to submit to (0-4)
+ * @param header DMA Command header. Note that the nfst and nlst fields do not
+ *               need to be filled in. All other fields must be set properly.
+ * @param first_address
+ *               Address to use for the first pointers. In the case of INTERNAL,
+ *               INBOUND, and OUTBOUND this is an Octeon memory address. In the
+ *               case of EXTERNAL, this is the source PCIe address.
+ * @param last_address
+ *               Address to use for the last pointers. In the case of EXTERNAL,
+ *               INBOUND, and OUTBOUND this is a PCIe address. In the
+ *               case of INTERNAL, this is the Octeon memory destination address.
+ * @param size   Size of the transfer to perform.
+ *
+ * @return Zero on success, negative on failure
+ */
+int cvmx_dma_engine_transfer(int engine, cvmx_dma_engine_header_t header,
+                             uint64_t first_address, uint64_t last_address,
+                             int size)
+{
+    cvmx_dma_engine_buffer_t buffers[32];
+    int words = 0;
+
+    switch (header.s.type)
+    {
+        case CVMX_DMA_ENGINE_TRANSFER_INTERNAL:
+            header.s.nfst = __cvmx_dma_engine_build_internal_pointers(buffers, first_address, size);
+            words += header.s.nfst;
+            header.s.nlst = __cvmx_dma_engine_build_internal_pointers(buffers + words, last_address, size);
+            words += header.s.nlst;
+            break;
+        case CVMX_DMA_ENGINE_TRANSFER_INBOUND:
+            header.s.nfst = __cvmx_dma_engine_build_internal_pointers(buffers, first_address, size);
+            words += header.s.nfst;
+            header.s.nlst = __cvmx_dma_engine_build_external_pointers(buffers + words, last_address, size);
+            words +=  header.s.nlst + ((header.s.nlst-1) >> 2) + 1;
+            break;
+        case CVMX_DMA_ENGINE_TRANSFER_OUTBOUND:
+            header.s.nfst = __cvmx_dma_engine_build_internal_pointers(buffers, first_address, size);
+            words += header.s.nfst;
+            header.s.nlst = __cvmx_dma_engine_build_external_pointers(buffers + words, last_address, size);
+            words +=  header.s.nlst + ((header.s.nlst-1) >> 2) + 1;
+            break;
+        case CVMX_DMA_ENGINE_TRANSFER_EXTERNAL:
+            header.s.nfst = __cvmx_dma_engine_build_external_pointers(buffers, first_address, size);
+            words +=  header.s.nfst + ((header.s.nfst-1) >> 2) + 1;
+            header.s.nlst = __cvmx_dma_engine_build_external_pointers(buffers + words, last_address, size);
+            words +=  header.s.nlst + ((header.s.nlst-1) >> 2) + 1;
+            break;
+    }
+    return cvmx_dma_engine_submit(engine, header, words, buffers);
+}
+
+#endif
diff --git a/arch/mips/cavium-octeon/executive/cvmx-dma-engine.h b/arch/mips/cavium-octeon/executive/cvmx-dma-engine.h
new file mode 100644
index 0000000..19ab564
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-dma-engine.h
@@ -0,0 +1,340 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Interface to the PCIe DMA engines. These are only avialable
+ * on chips with PCIe.
+ *
+ * <hr>$Revision: 35733 $<hr>
+ */
+
+#ifndef __CVMX_DMA_ENGINES_H__
+#define __CVMX_DMA_ENGINES_H__
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+typedef enum
+{
+    CVMX_DMA_ENGINE_TRANSFER_OUTBOUND = 0,  /**< OUTBOUND (read from L2/DRAM, write into PCIe memory space) */
+    CVMX_DMA_ENGINE_TRANSFER_INBOUND = 1,   /**< INBOUND (read from PCIe memory space, write into L2/DRAM) */
+    CVMX_DMA_ENGINE_TRANSFER_INTERNAL = 2,  /**< INTERNAL-ONLY (read from L2/DRAM, write into L2/DRAM) */
+    CVMX_DMA_ENGINE_TRANSFER_EXTERNAL = 3,  /**< EXTERNAL-ONLY (read from PCIe memory space, write into PCIe memory space) */
+} cvmx_dma_engine_transfer_t;
+
+typedef union
+{
+    uint64_t u64;
+    struct
+    {
+        uint64_t    reserved_60_63 :   4;   /**< Must be zero */
+        uint64_t    fport           :  2;   /**< First port. FPort indicates the physical PCIe port used for the
+                                                PCIe memory space pointers in the FIRST POINTERS block in the
+                                                EXTERNAL-ONLY case. Must be zero in the OUTBOUND, INBOUND and
+                                                INTERNAL-ONLY cases. */
+        uint64_t    lport           :  2;   /**< Last port. LPort indicates the physical PCIe port used for the
+                                                PCIe memory space pointers in the LAST POINTERS block in the
+                                                OUTBOUND, INBOUND, and EXTERNAL-ONLY cases. Must be zero in the
+                                                INTERNAL-ONLY case. */
+        cvmx_dma_engine_transfer_t type :  2; /**< Type  A given PCI DMA transfer is either OUTBOUND (read from L2/DRAM,
+                                                write into PCIe memory space), INBOUND (read from PCIe memory space, write
+                                                into L2/DRAM), INTERNAL-ONLY (read from L2/DRAM, write into L2/DRAM), or
+                                                EXTERNAL-ONLY (read from PCIe memory space, write into PCIe memory space). */
+        uint64_t    wqp             :  1;   /**< Work-queue pointer. When WQP = 1, PTR (if non-zero) is a pointer to a
+                                                work-queue entry that is submitted by the hardware after completing the DMA;
+                                                when WQP = 0, PTR (if non-zero) is a pointer to a byte in local memory that
+                                                is written to 0 by the hardware after completing the DMA. */
+        uint64_t    c               :  1;   /**< C  Counter. 1 = use counter 1, 0 = use counter 0.
+                                                The C bit selects between the two counters (NPEI_DMA_CNTS[DMA0,DMA1])
+                                                that can optionally be updated after an OUTBOUND or EXTERNAL-ONLY
+                                                transfer, and also selects between the two forced-interrupt bits
+                                                (NPEI_INT_SUMn[DMA0_FI, DMA1_FI]) that can optionally be set after an
+                                                OUTBOUND or EXTERNAL-ONLY transfer. C must be zero for INBOUND or
+                                                INTERNAL-ONLY transfers. */
+        uint64_t    ca              :  1;   /**< CA  Counter add.
+                                                When CA = 1, the hardware updates the selected counter after it completes the
+                                                PCI DMA OUTBOUND or EXTERNAL-ONLY Instruction.
+                                                    - If C = 0, PCIE_DMA_CNT0 is updated
+                                                    - If C = 1, PCIE_DMA_CNT1 is updated.
+                                                Note that this update may indirectly cause
+                                                NPEI_INT_SUM[DCNT0,DCNT1,DTIME0,DTIME1] to become set (depending
+                                                on the NPEI_DMA*_INT_LEVEL settings), so may cause interrupts to occur on a
+                                                remote PCI host.
+                                                    - If NPEI_DMA_CONTROL[O_ADD1] = 1, the counter is updated by 1.
+                                                    - If NPEI_DMA_CONTROL[O_ADD1] = 0, the counter is updated by the total
+                                                    bytes in the transfer.
+                                                When CA = 0, the hardware does not update any counters.
+                                                For an INBOUND or INTERNAL-ONLY PCI DMA transfer, CA must never be
+                                                set, and the hardware never adds to the counters. */
+        uint64_t    fi              :  1;   /**< FI  Force interrupt.
+                                                When FI is set for an OUTBOUND or EXTERNAL-ONLY transfer, the hardware
+                                                sets a forced interrupt bit after it completes the PCI DMA Instruction. If C = 0,
+                                                NPEI_INT_SUMn[DMA0_FI] is set, else NPEI_INT_SUMn[DMA1_FI] is set. For
+                                                an INBOUND or INTERNAL-ONLY PCI DMA operation, FI must never be set,
+                                                and the hardware never generates interrupts. */
+        uint64_t    ii              :  1;   /**< II Ignore the I bit (i.e. the I bit of the PCI DMA instruction local pointer).
+                                                For OUTBOUND transfers when II = 1, ignore the I bit and the FL bit in the
+                                                DMA HDR alone determines whether the hardware frees any/all of the local
+                                                buffers in the FIRST POINTERS area:
+                                                    - when FL = 1, the hardware frees the local buffer when II=1.
+                                                    - when FL = 0, the hardware does not free the local buffer when II=1.
+                                                For OUTBOUND transfers when II = 0, the I bit in the local pointer selects
+                                                whether local buffers are freed on a pointer-by-pointer basis:
+                                                    - when (FL  I) is true, the hardware frees the local buffer when II=0.
+                                                For INBOUND, INTERNAL-ONLY, and EXTERNAL-ONLY PCI DMA transfers,
+                                                II must never be set, and local buffers are never freed. */
+        uint64_t    fl              :  1;   /**< FL  Free local buffer.
+                                                When FL = 1, for an OUTBOUND operation, it indicates that the local buffers in
+                                                the FIRST BUFFERS area should be freed.
+                                                If II = 1, the FL bit alone indicates whether the local buffer should be freed:
+                                                    - when FL = 1, the hardware frees the local buffer when II=1.
+                                                    - when FL = 0, the hardware does not free the local buffer when II=1.
+                                                If II = 0, the I bit in the local pointer (refer to Section 9.5.2) determines whether
+                                                the local buffer is freed:
+                                                    - when (FL  I) is true, the hardware frees the local buffer when II=0.
+                                                For an INBOUND, INTERNAL-ONLY, or EXTERNAL-ONLY PCI DMA transfer,
+                                                FL must never be set, and local buffers are never freed. */
+        uint64_t    nlst            :  4;   /**< NLST  Number Last pointers.
+                                                The number of pointers in the LAST POINTERS area.
+                                                In the INBOUND, OUTBOUND, and EXTERNAL-ONLY cases, the LAST
+                                                POINTERS area contains PCI components, and the number of 64-bit words
+                                                required in the LAST POINTERS area is:
+                                                    - HDR.NLST + ((HDR.NLST + 3)/4) where the division removes the fraction.
+                                                In the INTERNAL-ONLY case, the LAST POINTERS area contains local
+                                                pointers, and the number of 64-bit words required in the LAST POINTERS area is:
+                                                    - HDR.NLST
+                                                Note that the sum of the number of 64-bit words in the LAST POINTERS and
+                                                FIRST POINTERS area must never exceed 31. */
+        uint64_t    nfst            :  4;   /**< NFST  Number First pointers.
+                                                The number of pointers in the FIRST POINTERS area.
+                                                In the INBOUND, OUTBOUND, and INTERNAL-ONLY cases, the FIRST
+                                                POINTERS area contains local pointers, and the number of 64-bit words required
+                                                in the FIRST POINTERS area is:
+                                                    - HDR.NFST
+                                                In the EXTERNAL-ONLY case, the FIRST POINTERS area contains PCI
+                                                components, and the number of 64-bit words required in the FIRST POINTERS
+                                                area is:
+                                                    - HDR.NFST + ((HDR.NFST + 3)/4) where the division removes the fraction. */
+        uint64_t    addr            : 40;   /**< PTR  Pointer, either a work-queue-entry pointer (when WQP = 1) or a local
+                                                memory pointer (WQP = 0).
+                                                When WQP = 1 and PTR  0x0, the hardware inserts the work-queue entry
+                                                indicated by PTR into a POW input queue after the PCI DMA operation is
+                                                complete. (Section 5.4 describes the work queue entry requirements in this
+                                                case.) When WQP = 1, PTR<2:0> must be 0x0.
+                                                When WQP = 0 and PTR  0x0, the hardware writes the single byte in local
+                                                memory indicated by PTR to 0x0 after the PCI DMA operation is complete.
+                                                NPEI_DMA_CONTROL[B0_LEND] selects the endian-ness of PTR in this
+                                                case.
+                                                When PTR = 0x0, the hardware performs no operation after the PCI DMA
+                                                operation is complete. */
+    } s;
+} cvmx_dma_engine_header_t;
+
+typedef union
+{
+    uint64_t u64;
+    struct
+    {
+        uint64_t    i               :  1;   /**< I  Invert free.
+                                                This bit gives the software the ability to free buffers independently for an
+                                                OUTBOUND PCI DMA transfer. I is not used by the hardware when II is set. I
+                                                must not be set, and buffers are never freed, for INBOUND, INTERNAL-ONLY,
+                                                and EXTERNAL-ONLY PCI DMA transfers. */
+        uint64_t    back            :  4;   /**< Back  Backup amount.
+                                                Allows the start of a buffer that is to be freed during an OUTBOUND transfer to
+                                                be different from the ptr value. Back specifies the amount to subtract from the
+                                                pointer to reach the start when freeing a buffer.
+                                                The address that is the start of the buffer being freed is:
+                                                    - Buffer start address = ((ptr >> 7) - Back) << 7.
+                                                Back is only used by the hardware when the buffer corresponding to ptr is freed.
+                                                Back must be 0x0, and buffers are never freed, for INBOUND, INTERNAL-ONLY,
+                                                and EXTERNAL-ONLY PCI DMA transfers. */
+        uint64_t    pool            :  3;   /**< Pool  Free pool.
+                                                Specifies which pool (of the eight hardware-managed FPA free pools) receives the
+                                                buffer associated with ptr when freed during an OUTBOUND transfer.
+                                                Pool is only used when the buffer corresponding to ptr is freed. Pool must be 0x0,
+                                                and buffers are never freed, for INBOUND, INTERNAL-ONLY, and EXTERNAL-ONLY
+                                                PCI DMA transfers. */
+        uint64_t    f               :  1;   /**< F  Full-block writes are allowed.
+                                                When set, the hardware is permitted to write all the bytes in the cache blocks
+                                                covered by ptr, ptr + Size - 1. This can improve memory system performance
+                                                when the write misses in the L2 cache.
+                                                F can only be set for local pointers that can be written to:
+                                                    - The local pointers in the FIRST POINTERS area that are write pointers for
+                                                    INBOUND transfers.
+                                                    - The local pointers in the LAST POINTERS area that are always write
+                                                    pointers (when present for INTERNAL-ONLY transfers).
+                                                F must not be set for local pointers that are not written to:
+                                                    - The local pointers in the FIRST POINTERS area for OUTBOUND and
+                                                    INTERNAL-ONLY transfers. */
+        uint64_t    a               :  1;   /**< A  Allocate L2.
+                                                This is a hint to the hardware that the cache blocks should be allocated in the L2
+                                                cache (if they were not already). */
+        uint64_t    l               :  1;   /**< L  Little-endian.
+                                                When L is set, the data at ptr is in little-endian format rather than big-endian. */
+        uint64_t    size            : 13;   /**< Size  Size in bytes of the contiguous space specified by ptr. A Size value of 0 is
+                                                illegal. Note that the sum of the sizes in the FIRST POINTERS area must always
+                                                exactly equal the sum of the sizes/lengths in the LAST POINTERS area:
+                                                    - In the OUTBOUND and INBOUND cases, the HDR.NFST size fields in the
+                                                    local pointers in the FIRST POINTERS area must exactly equal the lengths
+                                                    of the HDR.NLST fragments in the PCI components in the LAST POINTERS
+                                                    area.
+                                                    - In the INTERNAL-ONLY case, the HDR.NFST size fields in the local
+                                                    pointers in the FIRST POINTERS area must equal the HDR.NLST size
+                                                    fields in the local pointers in the LAST POINTERS area. */
+        uint64_t    reserved_36_39  :  4;   /**< Must be zero */
+        uint64_t    addr            : 36;   /**< L2/DRAM byte pointer. Points to where the packet data starts.
+                                                Ptr can be any byte alignment. Note that ptr is interpreted as a big-endian byte
+                                                pointer when L is clear, a little-endian byte pointer when L is set. */
+    } internal;
+    struct
+    {
+        uint64_t    len0            : 16;   /**< Length of PCIe memory for address 0 */
+        uint64_t    len1            : 16;   /**< Length of PCIe memory for address 1 */
+        uint64_t    len2            : 16;   /**< Length of PCIe memory for address 2 */
+        uint64_t    len3            : 16;   /**< Length of PCIe memory for address 3 */
+    } pcie_length;
+} cvmx_dma_engine_buffer_t;
+
+/**
+ * Initialize the DMA engines for use
+ *
+ * @return Zero on success, negative on failure
+ */
+int cvmx_dma_engine_initialize(void);
+
+/**
+ * Shutdown all DMA engines. The engeines must be idle when this
+ * function is called.
+ *
+ * @return Zero on success, negative on failure
+ */
+int cvmx_dma_engine_shutdown(void);
+
+/**
+ * Return the number of DMA engimes supported by this chip
+ *
+ * @return Number of DMA engines
+ */
+int cvmx_dma_engine_get_num(void);
+
+/**
+ * Submit a series of DMA comamnd to the DMA engines.
+ *
+ * @param engine  Engine to submit to (0-4)
+ * @param header  Command header
+ * @param num_buffers
+ *                The number of data pointers
+ * @param buffers Comamnd data pointers
+ *
+ * @return Zero on success, negative on failure
+ */
+int cvmx_dma_engine_submit(int engine, cvmx_dma_engine_header_t header, int num_buffers, cvmx_dma_engine_buffer_t buffers[]);
+
+/**
+ * Build the first and last pointers based on a DMA engine header
+ * and submit them to the engine. The purpose of this function is
+ * to simplify the building of DMA engine commands by automatically
+ * converting a simple address and size into the apropriate internal
+ * or PCIe address list. This function does not support gather lists,
+ * so you will need to build your own lists in that case.
+ *
+ * @param engine Engine to submit to (0-4)
+ * @param header DMA Command header. Note that the nfst and nlst fields do not
+ *               need to be filled in. All other fields must be set properly.
+ * @param first_address
+ *               Address to use for the first pointers. In the case of INTERNAL,
+ *               INBOUND, and OUTBOUND this is an Octeon memory address. In the
+ *               case of EXTERNAL, this is the source PCIe address.
+ * @param last_address
+ *               Address to use for the last pointers. In the case of EXTERNAL,
+ *               INBOUND, and OUTBOUND this is a PCIe address. In the
+ *               case of INTERNAL, this is the Octeon memory destination address.
+ * @param size   Size of the transfer to perform.
+ *
+ * @return Zero on success, negative on failure
+ */
+int cvmx_dma_engine_transfer(int engine, cvmx_dma_engine_header_t header,
+                             uint64_t first_address, uint64_t last_address,
+                             int size);
+
+/**
+ * Simplified interface to the DMA engines to emulate memcpy()
+ *
+ * @param engine Engine to submit to (0-4)
+ * @param dest   Pointer to the destination memory. cvmx_ptr_to_phys() will be
+ *               used to turn this into a physical address. It cannot be a local
+ *               or CVMX_SHARED block.
+ * @param source Pointer to the source memory.
+ *               cvmx_ptr_to_phys() will be used to turn this
+ *               into a physical address. It cannot be a local
+ *               or CVMX_SHARED block.
+ * @param length Number of bytes to copy
+ *
+ * @return Zero on success, negative on failure
+ */
+static inline int cvmx_dma_engine_memcpy(int engine, void *dest, void *source, int length)
+{
+    cvmx_dma_engine_header_t header;
+    header.u64 = 0;
+    header.s.type = CVMX_DMA_ENGINE_TRANSFER_INTERNAL;
+    return cvmx_dma_engine_transfer(engine, header, cvmx_ptr_to_phys(source),
+                                    cvmx_ptr_to_phys(dest), length);
+}
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif // __CVMX_CMD_QUEUE_H__
diff --git a/arch/mips/cavium-octeon/executive/cvmx-ebt3000.c b/arch/mips/cavium-octeon/executive/cvmx-ebt3000.c
new file mode 100644
index 0000000..69eb4e6
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-ebt3000.c
@@ -0,0 +1,120 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Interface to the EBT3000 specific devices
+ *
+ * <hr>$Revision: 32636 $<hr>
+ *
+ */
+
+#include "cvmx-config.h"
+#include "cvmx.h"
+#include "cvmx-sysinfo.h"
+
+
+void ebt3000_char_write(int char_position, char val)
+{
+    /* Note: phys_to_ptr won't work here, as we are most likely going to access the boot bus. */
+    void *led_base = CASTPTR(void, CVMX_ADD_SEG32(CVMX_MIPS32_SPACE_KSEG0, cvmx_sysinfo_get()->led_display_base_addr));
+    if (!led_base)
+        return;
+    if (cvmx_sysinfo_get()->board_type == CVMX_BOARD_TYPE_EBT3000 && cvmx_sysinfo_get()->board_rev_major == 1)
+    {
+        /* Rev 1 board */
+        char *ptr = (char *)(led_base + 4);
+        char_position &= 0x3;  /* only 4 chars */
+        ptr[3 - char_position] = val;
+    }
+    else
+    {
+        /* rev 2 or later board */
+        char *ptr = (char *)(led_base);
+        char_position &= 0x7;  /* only 8 chars */
+        ptr[char_position] = val;
+    }
+}
+
+void ebt3000_str_write(const char *str)
+{
+    /* Note: phys_to_ptr won't work here, as we are most likely going to access the boot bus. */
+    void *led_base;
+    if (!cvmx_sysinfo_get()->led_display_base_addr)
+        return;
+    led_base = CASTPTR(void, CVMX_ADD_SEG32(CVMX_MIPS32_SPACE_KSEG0, cvmx_sysinfo_get()->led_display_base_addr));
+    if (cvmx_sysinfo_get()->board_type == CVMX_BOARD_TYPE_EBT3000 && cvmx_sysinfo_get()->board_rev_major == 1)
+    {
+        char *ptr = (char *)(led_base + 4);
+        int i;
+        for (i=0; i<4; i++)
+        {
+            if (*str)
+                ptr[3 - i] = *str++;
+            else
+                ptr[3 - i] = ' ';
+        }
+    }
+    else
+    {
+        /* rev 2 board */
+        char *ptr = (char *)(led_base);
+        int i;
+        for (i=0; i<8; i++)
+        {
+            if (*str)
+                ptr[i] = *str++;
+            else
+                ptr[i] = ' ';
+        }
+    }
+}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-ebt3000.h b/arch/mips/cavium-octeon/executive/cvmx-ebt3000.h
new file mode 100644
index 0000000..7df6af4
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-ebt3000.h
@@ -0,0 +1,75 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+#ifndef __CVMX_EBT3000_H__
+#define __CVMX_EBT3000_H__
+
+/**
+ * @file
+ *
+ * Interface to the EBT3000 specific devices
+ *
+ * <hr>$Revision: 32636 $<hr>
+ *
+ */
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+void ebt3000_str_write(const char *str);
+void ebt3000_char_write(int char_position, char val);
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif  /* __CVMX_EBT3000_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-fau.h b/arch/mips/cavium-octeon/executive/cvmx-fau.h
new file mode 100644
index 0000000..4f2e28f
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-fau.h
@@ -0,0 +1,644 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Interface to the hardware Fetch and Add Unit.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+
+#ifndef __CVMX_FAU_H__
+#define __CVMX_FAU_H__
+
+#ifndef CVMX_DONT_INCLUDE_CONFIG
+#include "cvmx-config.h"
+#else
+typedef int cvmx_fau_reg_64_t;
+typedef int cvmx_fau_reg_32_t;
+typedef int cvmx_fau_reg_16_t;
+typedef int cvmx_fau_reg_8_t;
+#endif
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+/*
+ * Octeon Fetch and Add Unit (FAU)
+ */
+
+#define CVMX_FAU_LOAD_IO_ADDRESS    cvmx_build_io_address(0x1e, 0)
+#define CVMX_FAU_BITS_SCRADDR       63,56
+#define CVMX_FAU_BITS_LEN           55,48
+#define CVMX_FAU_BITS_INEVAL        35,14
+#define CVMX_FAU_BITS_TAGWAIT       13,13
+#define CVMX_FAU_BITS_NOADD         13,13
+#define CVMX_FAU_BITS_SIZE          12,11
+#define CVMX_FAU_BITS_REGISTER      10,0
+
+
+typedef enum {
+   CVMX_FAU_OP_SIZE_8  = 0,
+   CVMX_FAU_OP_SIZE_16 = 1,
+   CVMX_FAU_OP_SIZE_32 = 2,
+   CVMX_FAU_OP_SIZE_64 = 3
+} cvmx_fau_op_size_t;
+
+/**
+ * Tagwait return definition. If a timeout occurs, the error
+ * bit will be set. Otherwise the value of the register before
+ * the update will be returned.
+ */
+typedef struct
+{
+    uint64_t    error   : 1;
+    int64_t     value   : 63;
+} cvmx_fau_tagwait64_t;
+
+/**
+ * Tagwait return definition. If a timeout occurs, the error
+ * bit will be set. Otherwise the value of the register before
+ * the update will be returned.
+ */
+typedef struct
+{
+    uint64_t    error   : 1;
+    int32_t     value   : 31;
+} cvmx_fau_tagwait32_t;
+
+/**
+ * Tagwait return definition. If a timeout occurs, the error
+ * bit will be set. Otherwise the value of the register before
+ * the update will be returned.
+ */
+typedef struct
+{
+    uint64_t    error   : 1;
+    int16_t     value   : 15;
+} cvmx_fau_tagwait16_t;
+
+/**
+ * Tagwait return definition. If a timeout occurs, the error
+ * bit will be set. Otherwise the value of the register before
+ * the update will be returned.
+ */
+typedef struct
+{
+    uint64_t    error   : 1;
+    int8_t     value    : 7;
+} cvmx_fau_tagwait8_t;
+
+/**
+ * Asynchronous tagwait return definition. If a timeout occurs,
+ * the error bit will be set. Otherwise the value of the
+ * register before the update will be returned.
+ */
+typedef union {
+   uint64_t        u64;
+   struct {
+      uint64_t     invalid: 1;
+      uint64_t     data   :63; // unpredictable if invalid is set
+   } s;
+} cvmx_fau_async_tagwait_result_t;
+
+
+/**
+ * @INTERNAL
+ * Builds a store I/O address for writing to the FAU
+ *
+ * @param noadd  0 = Store value is atomically added to the current value
+ *               1 = Store value is atomically written over the current value
+ * @param reg    FAU atomic register to access. 0 <= reg < 2048.
+ *               - Step by 2 for 16 bit access.
+ *               - Step by 4 for 32 bit access.
+ *               - Step by 8 for 64 bit access.
+ * @return Address to store for atomic update
+ */
+static inline uint64_t __cvmx_fau_store_address(uint64_t noadd, uint64_t reg)
+{
+    return (CVMX_ADD_IO_SEG(CVMX_FAU_LOAD_IO_ADDRESS) |
+            cvmx_build_bits(CVMX_FAU_BITS_NOADD, noadd) |
+            cvmx_build_bits(CVMX_FAU_BITS_REGISTER, reg));
+}
+
+
+/**
+ * @INTERNAL
+ * Builds a I/O address for accessing the FAU
+ *
+ * @param tagwait Should the atomic add wait for the current tag switch
+ *                operation to complete.
+ *                - 0 = Don't wait
+ *                - 1 = Wait for tag switch to complete
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ *                - Step by 2 for 16 bit access.
+ *                - Step by 4 for 32 bit access.
+ *                - Step by 8 for 64 bit access.
+ * @param value   Signed value to add.
+ *                Note: When performing 32 and 64 bit access, only the low
+ *                22 bits are available.
+ * @return Address to read from for atomic update
+ */
+static inline uint64_t __cvmx_fau_atomic_address(uint64_t tagwait, uint64_t reg, int64_t value)
+{
+    return (CVMX_ADD_IO_SEG(CVMX_FAU_LOAD_IO_ADDRESS) |
+            cvmx_build_bits(CVMX_FAU_BITS_INEVAL, value) |
+            cvmx_build_bits(CVMX_FAU_BITS_TAGWAIT, tagwait) |
+            cvmx_build_bits(CVMX_FAU_BITS_REGISTER, reg));
+}
+
+
+/**
+ * Perform an atomic 64 bit add
+ *
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ *                - Step by 8 for 64 bit access.
+ * @param value   Signed value to add.
+ *                Note: Only the low 22 bits are available.
+ * @return Value of the register before the update
+ */
+static inline int64_t cvmx_fau_fetch_and_add64(cvmx_fau_reg_64_t reg, int64_t value)
+{
+    return cvmx_read64_int64(__cvmx_fau_atomic_address(0, reg, value));
+}
+
+
+/**
+ * Perform an atomic 32 bit add
+ *
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ *                - Step by 4 for 32 bit access.
+ * @param value   Signed value to add.
+ *                Note: Only the low 22 bits are available.
+ * @return Value of the register before the update
+ */
+static inline int32_t cvmx_fau_fetch_and_add32(cvmx_fau_reg_32_t reg, int32_t value)
+{
+    return cvmx_read64_int32(__cvmx_fau_atomic_address(0, reg, value));
+}
+
+
+/**
+ * Perform an atomic 16 bit add
+ *
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ *                - Step by 2 for 16 bit access.
+ * @param value   Signed value to add.
+ * @return Value of the register before the update
+ */
+static inline int16_t cvmx_fau_fetch_and_add16(cvmx_fau_reg_16_t reg, int16_t value)
+{
+    return cvmx_read64_int16(__cvmx_fau_atomic_address(0, reg, value));
+}
+
+
+/**
+ * Perform an atomic 8 bit add
+ *
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ * @param value   Signed value to add.
+ * @return Value of the register before the update
+ */
+static inline int8_t cvmx_fau_fetch_and_add8(cvmx_fau_reg_8_t reg, int8_t value)
+{
+    return cvmx_read64_int8(__cvmx_fau_atomic_address(0, reg, value));
+}
+
+
+/**
+ * Perform an atomic 64 bit add after the current tag switch
+ * completes
+ *
+ * @param reg    FAU atomic register to access. 0 <= reg < 2048.
+ *               - Step by 8 for 64 bit access.
+ * @param value  Signed value to add.
+ *               Note: Only the low 22 bits are available.
+ * @return If a timeout occurs, the error bit will be set. Otherwise
+ *         the value of the register before the update will be
+ *         returned
+ */
+static inline cvmx_fau_tagwait64_t cvmx_fau_tagwait_fetch_and_add64(cvmx_fau_reg_64_t reg, int64_t value)
+{
+    union
+    {
+        uint64_t i64;
+        cvmx_fau_tagwait64_t t;
+    } result;
+    result.i64 = cvmx_read64_int64(__cvmx_fau_atomic_address(1, reg, value));
+    return result.t;
+}
+
+
+/**
+ * Perform an atomic 32 bit add after the current tag switch
+ * completes
+ *
+ * @param reg    FAU atomic register to access. 0 <= reg < 2048.
+ *               - Step by 4 for 32 bit access.
+ * @param value  Signed value to add.
+ *               Note: Only the low 22 bits are available.
+ * @return If a timeout occurs, the error bit will be set. Otherwise
+ *         the value of the register before the update will be
+ *         returned
+ */
+static inline cvmx_fau_tagwait32_t cvmx_fau_tagwait_fetch_and_add32(cvmx_fau_reg_32_t reg, int32_t value)
+{
+    union
+    {
+        uint64_t i32;
+        cvmx_fau_tagwait32_t t;
+    } result;
+    result.i32 = cvmx_read64_int32(__cvmx_fau_atomic_address(1, reg, value));
+    return result.t;
+}
+
+
+/**
+ * Perform an atomic 16 bit add after the current tag switch
+ * completes
+ *
+ * @param reg    FAU atomic register to access. 0 <= reg < 2048.
+ *               - Step by 2 for 16 bit access.
+ * @param value  Signed value to add.
+ * @return If a timeout occurs, the error bit will be set. Otherwise
+ *         the value of the register before the update will be
+ *         returned
+ */
+static inline cvmx_fau_tagwait16_t cvmx_fau_tagwait_fetch_and_add16(cvmx_fau_reg_16_t reg, int16_t value)
+{
+    union
+    {
+        uint64_t i16;
+        cvmx_fau_tagwait16_t t;
+    } result;
+    result.i16 = cvmx_read64_int16(__cvmx_fau_atomic_address(1, reg, value));
+    return result.t;
+}
+
+
+/**
+ * Perform an atomic 8 bit add after the current tag switch
+ * completes
+ *
+ * @param reg    FAU atomic register to access. 0 <= reg < 2048.
+ * @param value  Signed value to add.
+ * @return If a timeout occurs, the error bit will be set. Otherwise
+ *         the value of the register before the update will be
+ *         returned
+ */
+static inline cvmx_fau_tagwait8_t cvmx_fau_tagwait_fetch_and_add8(cvmx_fau_reg_8_t reg, int8_t value)
+{
+    union
+    {
+        uint64_t i8;
+        cvmx_fau_tagwait8_t t;
+    } result;
+    result.i8 = cvmx_read64_int8(__cvmx_fau_atomic_address(1, reg, value));
+    return result.t;
+}
+
+
+/**
+ * @INTERNAL
+ * Builds I/O data for async operations
+ *
+ * @param scraddr Scratch pad byte addres to write to.  Must be 8 byte aligned
+ * @param value   Signed value to add.
+ *                Note: When performing 32 and 64 bit access, only the low
+ *                22 bits are available.
+ * @param tagwait Should the atomic add wait for the current tag switch
+ *                operation to complete.
+ *                - 0 = Don't wait
+ *                - 1 = Wait for tag switch to complete
+ * @param size    The size of the operation:
+ *                - CVMX_FAU_OP_SIZE_8  (0) = 8 bits
+ *                - CVMX_FAU_OP_SIZE_16 (1) = 16 bits
+ *                - CVMX_FAU_OP_SIZE_32 (2) = 32 bits
+ *                - CVMX_FAU_OP_SIZE_64 (3) = 64 bits
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ *                - Step by 2 for 16 bit access.
+ *                - Step by 4 for 32 bit access.
+ *                - Step by 8 for 64 bit access.
+ * @return Data to write using cvmx_send_single
+ */
+static inline uint64_t __cvmx_fau_iobdma_data(uint64_t scraddr, int64_t value, uint64_t tagwait,
+                                          cvmx_fau_op_size_t size, uint64_t reg)
+{
+    return (CVMX_FAU_LOAD_IO_ADDRESS |
+                      cvmx_build_bits(CVMX_FAU_BITS_SCRADDR, scraddr>>3) |
+                      cvmx_build_bits(CVMX_FAU_BITS_LEN, 1) |
+                      cvmx_build_bits(CVMX_FAU_BITS_INEVAL, value) |
+                      cvmx_build_bits(CVMX_FAU_BITS_TAGWAIT, tagwait) |
+                      cvmx_build_bits(CVMX_FAU_BITS_SIZE, size) |
+                      cvmx_build_bits(CVMX_FAU_BITS_REGISTER, reg));
+}
+
+
+/**
+ * Perform an async atomic 64 bit add. The old value is
+ * placed in the scratch memory at byte address scraddr.
+ *
+ * @param scraddr Scratch memory byte address to put response in.
+ *                Must be 8 byte aligned.
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ *                - Step by 8 for 64 bit access.
+ * @param value   Signed value to add.
+ *                Note: Only the low 22 bits are available.
+ * @return Placed in the scratch pad register
+ */
+static inline void cvmx_fau_async_fetch_and_add64(uint64_t scraddr, cvmx_fau_reg_64_t reg, int64_t value)
+{
+    cvmx_send_single(__cvmx_fau_iobdma_data(scraddr, value, 0, CVMX_FAU_OP_SIZE_64, reg));
+}
+
+
+/**
+ * Perform an async atomic 32 bit add. The old value is
+ * placed in the scratch memory at byte address scraddr.
+ *
+ * @param scraddr Scratch memory byte address to put response in.
+ *                Must be 8 byte aligned.
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ *                - Step by 4 for 32 bit access.
+ * @param value   Signed value to add.
+ *                Note: Only the low 22 bits are available.
+ * @return Placed in the scratch pad register
+ */
+static inline void cvmx_fau_async_fetch_and_add32(uint64_t scraddr, cvmx_fau_reg_32_t reg, int32_t value)
+{
+    cvmx_send_single(__cvmx_fau_iobdma_data(scraddr, value, 0, CVMX_FAU_OP_SIZE_32, reg));
+}
+
+
+/**
+ * Perform an async atomic 16 bit add. The old value is
+ * placed in the scratch memory at byte address scraddr.
+ *
+ * @param scraddr Scratch memory byte address to put response in.
+ *                Must be 8 byte aligned.
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ *                - Step by 2 for 16 bit access.
+ * @param value   Signed value to add.
+ * @return Placed in the scratch pad register
+ */
+static inline void cvmx_fau_async_fetch_and_add16(uint64_t scraddr, cvmx_fau_reg_16_t reg, int16_t value)
+{
+    cvmx_send_single(__cvmx_fau_iobdma_data(scraddr, value, 0, CVMX_FAU_OP_SIZE_16, reg));
+}
+
+
+/**
+ * Perform an async atomic 8 bit add. The old value is
+ * placed in the scratch memory at byte address scraddr.
+ *
+ * @param scraddr Scratch memory byte address to put response in.
+ *                Must be 8 byte aligned.
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ * @param value   Signed value to add.
+ * @return Placed in the scratch pad register
+ */
+static inline void cvmx_fau_async_fetch_and_add8(uint64_t scraddr, cvmx_fau_reg_8_t reg, int8_t value)
+{
+    cvmx_send_single(__cvmx_fau_iobdma_data(scraddr, value, 0, CVMX_FAU_OP_SIZE_8, reg));
+}
+
+
+/**
+ * Perform an async atomic 64 bit add after the current tag
+ * switch completes.
+ *
+ * @param scraddr Scratch memory byte address to put response in.
+ *                Must be 8 byte aligned.
+ *                If a timeout occurs, the error bit (63) will be set. Otherwise
+ *                the value of the register before the update will be
+ *                returned
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ *                - Step by 8 for 64 bit access.
+ * @param value   Signed value to add.
+ *                Note: Only the low 22 bits are available.
+ * @return Placed in the scratch pad register
+ */
+static inline void cvmx_fau_async_tagwait_fetch_and_add64(uint64_t scraddr, cvmx_fau_reg_64_t reg, int64_t value)
+{
+    cvmx_send_single(__cvmx_fau_iobdma_data(scraddr, value, 1, CVMX_FAU_OP_SIZE_64, reg));
+}
+
+
+/**
+ * Perform an async atomic 32 bit add after the current tag
+ * switch completes.
+ *
+ * @param scraddr Scratch memory byte address to put response in.
+ *                Must be 8 byte aligned.
+ *                If a timeout occurs, the error bit (63) will be set. Otherwise
+ *                the value of the register before the update will be
+ *                returned
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ *                - Step by 4 for 32 bit access.
+ * @param value   Signed value to add.
+ *                Note: Only the low 22 bits are available.
+ * @return Placed in the scratch pad register
+ */
+static inline void cvmx_fau_async_tagwait_fetch_and_add32(uint64_t scraddr, cvmx_fau_reg_32_t reg, int32_t value)
+{
+    cvmx_send_single(__cvmx_fau_iobdma_data(scraddr, value, 1, CVMX_FAU_OP_SIZE_32, reg));
+}
+
+
+/**
+ * Perform an async atomic 16 bit add after the current tag
+ * switch completes.
+ *
+ * @param scraddr Scratch memory byte address to put response in.
+ *                Must be 8 byte aligned.
+ *                If a timeout occurs, the error bit (63) will be set. Otherwise
+ *                the value of the register before the update will be
+ *                returned
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ *                - Step by 2 for 16 bit access.
+ * @param value   Signed value to add.
+ * @return Placed in the scratch pad register
+ */
+static inline void cvmx_fau_async_tagwait_fetch_and_add16(uint64_t scraddr, cvmx_fau_reg_16_t reg, int16_t value)
+{
+    cvmx_send_single(__cvmx_fau_iobdma_data(scraddr, value, 1, CVMX_FAU_OP_SIZE_16, reg));
+}
+
+
+/**
+ * Perform an async atomic 8 bit add after the current tag
+ * switch completes.
+ *
+ * @param scraddr Scratch memory byte address to put response in.
+ *                Must be 8 byte aligned.
+ *                If a timeout occurs, the error bit (63) will be set. Otherwise
+ *                the value of the register before the update will be
+ *                returned
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ * @param value   Signed value to add.
+ * @return Placed in the scratch pad register
+ */
+static inline void cvmx_fau_async_tagwait_fetch_and_add8(uint64_t scraddr, cvmx_fau_reg_8_t reg, int8_t value)
+{
+    cvmx_send_single(__cvmx_fau_iobdma_data(scraddr, value, 1, CVMX_FAU_OP_SIZE_8, reg));
+}
+
+
+
+
+/**
+ * Perform an atomic 64 bit add
+ *
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ *                - Step by 8 for 64 bit access.
+ * @param value   Signed value to add.
+ */
+static inline void cvmx_fau_atomic_add64(cvmx_fau_reg_64_t reg, int64_t value)
+{
+    cvmx_write64_int64(__cvmx_fau_store_address(0, reg), value);
+}
+
+
+/**
+ * Perform an atomic 32 bit add
+ *
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ *                - Step by 4 for 32 bit access.
+ * @param value   Signed value to add.
+ */
+static inline void cvmx_fau_atomic_add32(cvmx_fau_reg_32_t reg, int32_t value)
+{
+    cvmx_write64_int32(__cvmx_fau_store_address(0, reg), value);
+}
+
+
+/**
+ * Perform an atomic 16 bit add
+ *
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ *                - Step by 2 for 16 bit access.
+ * @param value   Signed value to add.
+ */
+static inline void cvmx_fau_atomic_add16(cvmx_fau_reg_16_t reg, int16_t value)
+{
+    cvmx_write64_int16(__cvmx_fau_store_address(0, reg), value);
+}
+
+
+/**
+ * Perform an atomic 8 bit add
+ *
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ * @param value   Signed value to add.
+ */
+static inline void cvmx_fau_atomic_add8(cvmx_fau_reg_8_t reg, int8_t value)
+{
+    cvmx_write64_int8(__cvmx_fau_store_address(0, reg), value);
+}
+
+
+/**
+ * Perform an atomic 64 bit write
+ *
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ *                - Step by 8 for 64 bit access.
+ * @param value   Signed value to write.
+ */
+static inline void cvmx_fau_atomic_write64(cvmx_fau_reg_64_t reg, int64_t value)
+{
+    cvmx_write64_int64(__cvmx_fau_store_address(1, reg), value);
+}
+
+
+/**
+ * Perform an atomic 32 bit write
+ *
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ *                - Step by 4 for 32 bit access.
+ * @param value   Signed value to write.
+ */
+static inline void cvmx_fau_atomic_write32(cvmx_fau_reg_32_t reg, int32_t value)
+{
+    cvmx_write64_int32(__cvmx_fau_store_address(1, reg), value);
+}
+
+
+/**
+ * Perform an atomic 16 bit write
+ *
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ *                - Step by 2 for 16 bit access.
+ * @param value   Signed value to write.
+ */
+static inline void cvmx_fau_atomic_write16(cvmx_fau_reg_16_t reg, int16_t value)
+{
+    cvmx_write64_int16(__cvmx_fau_store_address(1, reg), value);
+}
+
+
+/**
+ * Perform an atomic 8 bit write
+ *
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ * @param value   Signed value to write.
+ */
+static inline void cvmx_fau_atomic_write8(cvmx_fau_reg_8_t reg, int8_t value)
+{
+    cvmx_write64_int8(__cvmx_fau_store_address(1, reg), value);
+}
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif  /* __CVMX_FAU_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-flash.c b/arch/mips/cavium-octeon/executive/cvmx-flash.c
new file mode 100644
index 0000000..5ede03a
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-flash.c
@@ -0,0 +1,680 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * This file provides bootbus flash operations
+ *
+ * <hr>$Revision: 32636 $<hr>
+ *
+ *
+ */
+
+#include "cvmx-config.h"
+#include "cvmx.h"
+#include "cvmx-sysinfo.h"
+#include "cvmx-spinlock.h"
+#include "cvmx-flash.h"
+
+#define MAX_NUM_FLASH_CHIPS 8   /* Maximum number of flash chips */
+#define MAX_NUM_REGIONS     8   /* Maximum number of block regions per chip */
+#define DEBUG 1
+
+#define CFI_CMDSET_NONE             0
+#define CFI_CMDSET_INTEL_EXTENDED   1
+#define CFI_CMDSET_AMD_STANDARD     2
+#define CFI_CMDSET_INTEL_STANDARD   3
+#define CFI_CMDSET_AMD_EXTENDED     4
+#define CFI_CMDSET_MITSU_STANDARD   256
+#define CFI_CMDSET_MITSU_EXTENDED   257
+#define CFI_CMDSET_SST              258
+
+typedef struct
+{
+    void *              base_ptr;       /**< Memory pointer to start of flash */
+    int                 is_16bit;       /**< Chip is 16bits wide in 8bit mode */
+    uint16_t            vendor;         /**< Vendor ID of Chip */
+    int                 size;           /**< Size of the chip in bytes */
+    uint64_t            erase_timeout;  /**< Erase timeout in cycles */
+    uint64_t            write_timeout;  /**< Write timeout in cycles */
+    int                 num_regions;    /**< Number of block regions */
+    cvmx_flash_region_t region[MAX_NUM_REGIONS];
+} cvmx_flash_t;
+
+static CVMX_SHARED cvmx_flash_t flash_info[MAX_NUM_FLASH_CHIPS];
+static CVMX_SHARED cvmx_spinlock_t flash_lock = CVMX_SPINLOCK_UNLOCKED_INITIALIZER;
+
+
+/**
+ * @INTERNAL
+ * Read a byte from flash
+ *
+ * @param chip_id Chip to read from
+ * @param offset  Offset into the chip
+ * @return Value read
+ */
+static uint8_t __cvmx_flash_read8(int chip_id, int offset)
+{
+    return *(volatile uint8_t *)(flash_info[chip_id].base_ptr + offset);
+}
+
+
+/**
+ * @INTERNAL
+ * Read a byte from flash (for commands)
+ *
+ * @param chip_id Chip to read from
+ * @param offset  Offset into the chip
+ * @return Value read
+ */
+static uint8_t __cvmx_flash_read_cmd(int chip_id, int offset)
+{
+    if (flash_info[chip_id].is_16bit)
+        offset<<=1;
+    return __cvmx_flash_read8(chip_id, offset);
+}
+
+
+/**
+ * @INTERNAL
+ * Read 16bits from flash (for commands)
+ *
+ * @param chip_id Chip to read from
+ * @param offset  Offset into the chip
+ * @return Value read
+ */
+static uint16_t __cvmx_flash_read_cmd16(int chip_id, int offset)
+{
+    uint16_t v = __cvmx_flash_read_cmd(chip_id, offset);
+    v |= __cvmx_flash_read_cmd(chip_id, offset + 1)<<8;
+    return v;
+}
+
+
+/**
+ * @INTERNAL
+ * Write a byte to flash
+ *
+ * @param chip_id Chip to write to
+ * @param offset  Offset into the chip
+ * @param data    Value to write
+ */
+static void __cvmx_flash_write8(int chip_id, int offset, uint8_t data)
+{
+    volatile uint8_t *flash_ptr = (volatile uint8_t *)flash_info[chip_id].base_ptr;
+    flash_ptr[offset] = data;
+}
+
+
+/**
+ * @INTERNAL
+ * Write a byte to flash (for commands)
+ *
+ * @param chip_id Chip to write to
+ * @param offset  Offset into the chip
+ * @param data    Value to write
+ */
+static void __cvmx_flash_write_cmd(int chip_id, int offset, uint8_t data)
+{
+    volatile uint8_t *flash_ptr = (volatile uint8_t *)flash_info[chip_id].base_ptr;
+    flash_ptr[offset<<flash_info[chip_id].is_16bit] = data;
+}
+
+
+/**
+ * @INTERNAL
+ * Query a address and see if a CFI flash chip is there.
+ *
+ * @param chip_id  Chip ID data to fill in if the chip is there
+ * @param base_ptr Memory pointer to the start address to query
+ * @return Zero on success, Negative on failure
+ */
+static int __cvmx_flash_queury_cfi(int chip_id, void *base_ptr)
+{
+    int region;
+    cvmx_flash_t *flash = flash_info + chip_id;
+
+    /* Set the minimum needed for the read and write primitives to work */
+    flash->base_ptr = base_ptr;
+    flash->is_16bit = 1;   /* FIXME: Currently assumes the chip is 16bits */
+
+    /* Put flash in CFI query mode */
+    __cvmx_flash_write_cmd(chip_id, 0x00, 0xf0); /* Reset the flash chip */
+    __cvmx_flash_write_cmd(chip_id, 0x55, 0x98);
+
+    /* Make sure we get the QRY response we should */
+    if ((__cvmx_flash_read_cmd(chip_id, 0x10) != 'Q') ||
+        (__cvmx_flash_read_cmd(chip_id, 0x11) != 'R') ||
+        (__cvmx_flash_read_cmd(chip_id, 0x12) != 'Y'))
+    {
+        flash->base_ptr = NULL;
+        return -1;
+    }
+
+    /* Read the 16bit vendor ID */
+    flash->vendor = __cvmx_flash_read_cmd16(chip_id, 0x13);
+
+    /* Read the write timeout. The timeout is microseconds(us) is 2^0x1f
+        typically. The worst case is this value time 2^0x23 */
+    flash->write_timeout = 1ull << (__cvmx_flash_read_cmd(chip_id, 0x1f) +
+                                    __cvmx_flash_read_cmd(chip_id, 0x23));
+
+    /* Read the erase timeout. The timeout is milliseconds(ms) is 2^0x21
+        typically. The worst case is this value time 2^0x25 */
+    flash->erase_timeout = 1ull << (__cvmx_flash_read_cmd(chip_id, 0x21) +
+                                    __cvmx_flash_read_cmd(chip_id, 0x25));
+
+    /* Get the flash size. This is 2^0x27 */
+    flash->size = 1<<__cvmx_flash_read_cmd(chip_id, 0x27);
+
+    /* Get the number of different sized block regions from 0x2c */
+    flash->num_regions = __cvmx_flash_read_cmd(chip_id, 0x2c);
+
+    int start_offset = 0;
+    /* Loop through all regions get information about each */
+    for (region=0; region<flash->num_regions; region++)
+    {
+        cvmx_flash_region_t *rgn_ptr = flash->region + region;
+        rgn_ptr->start_offset = start_offset;
+
+        /* The number of blocks in each region is a 16 bit little endian
+            endian field. It is encoded at 0x2d + region*4 as (blocks-1) */
+        uint16_t blocks = __cvmx_flash_read_cmd16(chip_id, 0x2d + region*4);
+        rgn_ptr->num_blocks =  1u + blocks;
+
+        /* The size of each block is a 16 bit little endian endian field. It
+            is encoded at 0x2d + region*4 + 2 as (size/256). Zero is a special
+            case representing 128 */
+        uint16_t size = __cvmx_flash_read_cmd16(chip_id, 0x2d + region*4 + 2);
+        if (size == 0)
+            rgn_ptr->block_size = 128;
+        else
+            rgn_ptr->block_size = 256u * size;
+
+        start_offset += rgn_ptr->block_size * rgn_ptr->num_blocks;
+    }
+
+    /* Take the chip out of CFI query mode */
+    switch (flash_info[chip_id].vendor)
+    {
+        case CFI_CMDSET_AMD_STANDARD:
+            __cvmx_flash_write_cmd(chip_id, 0x00, 0xf0);
+        case CFI_CMDSET_INTEL_STANDARD:
+        case CFI_CMDSET_INTEL_EXTENDED:
+            __cvmx_flash_write_cmd(chip_id, 0x00, 0xff);
+            break;
+    }
+
+    /* Convert the timeouts to cycles */
+    flash->write_timeout *= cvmx_sysinfo_get()->cpu_clock_hz / 1000000;
+    flash->erase_timeout *= cvmx_sysinfo_get()->cpu_clock_hz / 1000;
+
+#if DEBUG
+    /* Print the information about the chip */
+    cvmx_dprintf("cvmx-flash: Base pointer:  %p\n"
+           "            Vendor:        0x%04x\n"
+           "            Size:          %d bytes\n"
+           "            Num regions:   %d\n"
+           "            Erase timeout: %llu cycles\n"
+           "            Write timeout: %llu cycles\n",
+           flash->base_ptr,
+           (unsigned int)flash->vendor,
+           flash->size,
+           flash->num_regions,
+           (unsigned long long)flash->erase_timeout,
+           (unsigned long long)flash->write_timeout);
+
+    for (region=0; region<flash->num_regions; region++)
+    {
+        cvmx_dprintf("            Region %d: offset 0x%x, %d blocks, %d bytes/block\n",
+               region,
+               flash->region[region].start_offset,
+               flash->region[region].num_blocks,
+               flash->region[region].block_size);
+    }
+#endif
+
+    return 0;
+}
+
+
+/**
+ * Initialize the flash access library
+ */
+void cvmx_flash_initialize(void)
+{
+    int boot_region;
+    int chip_id = 0;
+
+    memset(flash_info, 0, sizeof(flash_info));
+
+    /* Loop through each boot bus chip select region */
+    for (boot_region=0; boot_region<MAX_NUM_FLASH_CHIPS; boot_region++)
+    {
+        cvmx_mio_boot_reg_cfgx_t region_cfg;
+        region_cfg.u64 = cvmx_read_csr(CVMX_MIO_BOOT_REG_CFG0 + boot_region*8);
+        /* Only try chip select regions that are enabled. This assumes the
+            bootloader already setup the flash */
+        if (region_cfg.s.en)
+        {
+            /* Convert the hardware address to a pointer. Note that the bootbus,
+                unlike memory, isn't 1:1 mapped in the simple exec */
+            void *base_ptr = cvmx_phys_to_ptr((region_cfg.s.base<<16) | 0xffffffff80000000ull);
+            if (__cvmx_flash_queury_cfi(chip_id, base_ptr) == 0)
+            {
+                /* Valid CFI flash chip found */
+                chip_id++;
+            }
+        }
+    }
+
+    if (chip_id == 0)
+        cvmx_dprintf("cvmx-flash: No CFI chips found\n");
+}
+
+
+/**
+ * Return a pointer to the flash chip
+ *
+ * @param chip_id Chip ID to return
+ * @return NULL if the chip doesn't exist
+ */
+void *cvmx_flash_get_base(int chip_id)
+{
+    return flash_info[chip_id].base_ptr;
+}
+
+
+/**
+ * Return the number of erasable regions on the chip
+ *
+ * @param chip_id Chip to return info for
+ * @return Number of regions
+ */
+int cvmx_flash_get_num_regions(int chip_id)
+{
+    return flash_info[chip_id].num_regions;
+}
+
+
+/**
+ * Return information about a flash chips region
+ *
+ * @param chip_id Chip to get info for
+ * @param region  Region to get info for
+ * @return Region information
+ */
+const cvmx_flash_region_t *cvmx_flash_get_region_info(int chip_id, int region)
+{
+    return flash_info[chip_id].region + region;
+}
+
+
+/**
+ * Erase a block on the flash chip
+ *
+ * @param chip_id Chip to erase a block on
+ * @param region  Region to erase a block in
+ * @param block   Block number to erase
+ * @return Zero on success. Negative on failure
+ */
+int cvmx_flash_erase_block(int chip_id, int region, int block)
+{
+    cvmx_spinlock_lock(&flash_lock);
+#if DEBUG
+    cvmx_dprintf("cvmx-flash: Erasing chip %d, region %d, block %d\n",
+           chip_id, region, block);
+#endif
+
+    int offset = flash_info[chip_id].region[region].start_offset +
+                block * flash_info[chip_id].region[region].block_size;
+
+    switch (flash_info[chip_id].vendor)
+    {
+        case CFI_CMDSET_AMD_STANDARD:
+        {
+            /* Send the erase sector command sequence */
+            __cvmx_flash_write_cmd(chip_id, 0x00, 0xf0); /* Reset the flash chip */
+            __cvmx_flash_write_cmd(chip_id, 0x555, 0xaa);
+            __cvmx_flash_write_cmd(chip_id, 0x2aa, 0x55);
+            __cvmx_flash_write_cmd(chip_id, 0x555, 0x80);
+            __cvmx_flash_write_cmd(chip_id, 0x555, 0xaa);
+            __cvmx_flash_write_cmd(chip_id, 0x2aa, 0x55);
+            __cvmx_flash_write8(chip_id, offset, 0x30);
+
+            /* Loop checking status */
+            uint8_t status = __cvmx_flash_read8(chip_id, offset);
+            uint64_t start_cycle = cvmx_get_cycle();
+            while (1)
+            {
+                /* Read the status and xor it with the old status so we can
+                    find toggling bits */
+                uint8_t old_status = status;
+                status = __cvmx_flash_read8(chip_id, offset);
+                uint8_t toggle = status ^ old_status;
+
+                /* Check if the erase in progress bit is toggling */
+                if (toggle & (1<<6))
+                {
+                    /* Check hardware timeout */
+                    if (status & (1<<5))
+                    {
+                        /* Chip has signalled a timeout. Reread the status */
+                        old_status = __cvmx_flash_read8(chip_id, offset);
+                        status = __cvmx_flash_read8(chip_id, offset);
+                        toggle = status ^ old_status;
+
+                        /* Check if the erase in progress bit is toggling */
+                        if (toggle & (1<<6))
+                        {
+                            cvmx_dprintf("cvmx-flash: Hardware timeout erasing block\n");
+                            cvmx_spinlock_unlock(&flash_lock);
+                            return -1;
+                        }
+                        else
+                            break;  /* Not toggling, erase complete */
+                    }
+                }
+                else
+                    break;  /* Not toggling, erase complete */
+
+                if (cvmx_get_cycle() > start_cycle + flash_info[chip_id].erase_timeout)
+                {
+                    cvmx_dprintf("cvmx-flash: Timeout erasing block\n");
+                    cvmx_spinlock_unlock(&flash_lock);
+                    return -1;
+                }
+            }
+
+            __cvmx_flash_write_cmd(chip_id, 0x00, 0xf0); /* Reset the flash chip */
+            cvmx_spinlock_unlock(&flash_lock);
+            return 0;
+        }
+        case CFI_CMDSET_INTEL_STANDARD:
+        case CFI_CMDSET_INTEL_EXTENDED:
+        {
+            /* Send the erase sector command sequence */
+            __cvmx_flash_write_cmd(chip_id, 0x00, 0xff); /* Reset the flash chip */
+            __cvmx_flash_write8(chip_id, offset, 0x20);
+            __cvmx_flash_write8(chip_id, offset, 0xd0);
+
+            /* Loop checking status */
+            uint8_t status = __cvmx_flash_read8(chip_id, offset);
+            uint64_t start_cycle = cvmx_get_cycle();
+            while ((status & 0x80) == 0)
+            {
+                if (cvmx_get_cycle() > start_cycle + flash_info[chip_id].erase_timeout)
+                {
+                    cvmx_dprintf("cvmx-flash: Timeout erasing block\n");
+                    cvmx_spinlock_unlock(&flash_lock);
+                    return -1;
+                }
+                status = __cvmx_flash_read8(chip_id, offset);
+            }
+
+            /* Check the final status */
+            if (status & 0x7f)
+            {
+                cvmx_dprintf("cvmx-flash: Hardware failure erasing block\n");
+                cvmx_spinlock_unlock(&flash_lock);
+                return -1;
+            }
+
+            __cvmx_flash_write_cmd(chip_id, 0x00, 0xff); /* Reset the flash chip */
+            cvmx_spinlock_unlock(&flash_lock);
+            return 0;
+        }
+    }
+
+    cvmx_dprintf("cvmx-flash: Unsupported flash vendor\n");
+    cvmx_spinlock_unlock(&flash_lock);
+    return -1;
+}
+
+
+/**
+ * Write a block on the flash chip
+ *
+ * @param chip_id Chip to write a block on
+ * @param region  Region to write a block in
+ * @param block   Block number to write
+ * @param data    Data to write
+ * @return Zero on success. Negative on failure
+ */
+int cvmx_flash_write_block(int chip_id, int region, int block, const void *data)
+{
+    cvmx_spinlock_lock(&flash_lock);
+#if DEBUG
+    cvmx_dprintf("cvmx-flash: Writing chip %d, region %d, block %d\n",
+           chip_id, region, block);
+#endif
+    int offset = flash_info[chip_id].region[region].start_offset +
+                block * flash_info[chip_id].region[region].block_size;
+    int len = flash_info[chip_id].region[region].block_size;
+    const uint8_t *ptr = (const uint8_t *)data;
+
+    switch (flash_info[chip_id].vendor)
+    {
+        case CFI_CMDSET_AMD_STANDARD:
+        {
+            /* Loop through one byte at a time */
+            while (len--)
+            {
+                /* Send the program sequence */
+                __cvmx_flash_write_cmd(chip_id, 0x00, 0xf0); /* Reset the flash chip */
+                __cvmx_flash_write_cmd(chip_id, 0x555, 0xaa);
+                __cvmx_flash_write_cmd(chip_id, 0x2aa, 0x55);
+                __cvmx_flash_write_cmd(chip_id, 0x555, 0xa0);
+                __cvmx_flash_write8(chip_id, offset, *ptr);
+
+                /* Loop polling for status */
+                uint64_t start_cycle = cvmx_get_cycle();
+                while (1)
+                {
+                    uint8_t status = __cvmx_flash_read8(chip_id, offset);
+                    if (((status ^ *ptr) & (1<<7)) == 0)
+                        break;  /* Data matches, this byte is done */
+                    else if (status & (1<<5))
+                    {
+                        /* Hardware timeout, recheck status */
+                        status = __cvmx_flash_read8(chip_id, offset);
+                        if (((status ^ *ptr) & (1<<7)) == 0)
+                            break;  /* Data matches, this byte is done */
+                        else
+                        {
+                            cvmx_dprintf("cvmx-flash: Hardware write timeout\n");
+                            cvmx_spinlock_unlock(&flash_lock);
+                            return -1;
+                        }
+                    }
+
+                    if (cvmx_get_cycle() > start_cycle + flash_info[chip_id].write_timeout)
+                    {
+                        cvmx_dprintf("cvmx-flash: Timeout writing block\n");
+                        cvmx_spinlock_unlock(&flash_lock);
+                        return -1;
+                    }
+                }
+
+                /* Increment to the next byte */
+                ptr++;
+                offset++;
+            }
+
+            __cvmx_flash_write_cmd(chip_id, 0x00, 0xf0); /* Reset the flash chip */
+            cvmx_spinlock_unlock(&flash_lock);
+            return 0;
+        }
+        case CFI_CMDSET_INTEL_STANDARD:
+        case CFI_CMDSET_INTEL_EXTENDED:
+        {
+cvmx_dprintf("%s:%d len=%d\n", __FUNCTION__, __LINE__, len);
+            /* Loop through one byte at a time */
+            while (len--)
+            {
+                /* Send the program sequence */
+                __cvmx_flash_write_cmd(chip_id, 0x00, 0xff); /* Reset the flash chip */
+                __cvmx_flash_write8(chip_id, offset, 0x40);
+                __cvmx_flash_write8(chip_id, offset, *ptr);
+
+                /* Loop polling for status */
+                uint8_t status = __cvmx_flash_read8(chip_id, offset);
+                uint64_t start_cycle = cvmx_get_cycle();
+                while ((status & 0x80) == 0)
+                {
+                    if (cvmx_get_cycle() > start_cycle + flash_info[chip_id].write_timeout)
+                    {
+                        cvmx_dprintf("cvmx-flash: Timeout writing block\n");
+                        cvmx_spinlock_unlock(&flash_lock);
+                        return -1;
+                    }
+                    status = __cvmx_flash_read8(chip_id, offset);
+                }
+
+                /* Check the final status */
+                if (status & 0x7f)
+                {
+                    cvmx_dprintf("cvmx-flash: Hardware failure erasing block\n");
+                    cvmx_spinlock_unlock(&flash_lock);
+                    return -1;
+                }
+
+                /* Increment to the next byte */
+                ptr++;
+                offset++;
+            }
+cvmx_dprintf("%s:%d\n", __FUNCTION__, __LINE__);
+
+            __cvmx_flash_write_cmd(chip_id, 0x00, 0xff); /* Reset the flash chip */
+            cvmx_spinlock_unlock(&flash_lock);
+            return 0;
+        }
+    }
+
+    cvmx_dprintf("cvmx-flash: Unsupported flash vendor\n");
+    cvmx_spinlock_unlock(&flash_lock);
+    return -1;
+}
+
+
+/**
+ * Erase and write data to a flash
+ *
+ * @param address Memory address to write to
+ * @param data    Data to write
+ * @param len     Length of the data
+ * @return Zero on success. Negative on failure
+ */
+int cvmx_flash_write(void *address, const void *data, int len)
+{
+    int chip_id;
+
+    /* Find which chip controls this address. Don't allow the write to span
+        multiple chips */
+    for (chip_id=0; chip_id<MAX_NUM_FLASH_CHIPS; chip_id++)
+    {
+        if ((flash_info[chip_id].base_ptr <= address) &&
+            (flash_info[chip_id].base_ptr + flash_info[chip_id].size >= address + len))
+            break;
+    }
+
+    if (chip_id == MAX_NUM_FLASH_CHIPS)
+    {
+        cvmx_dprintf("cvmx-flash: Unable to find chip that contains address %p\n", address);
+        return -1;
+    }
+
+    cvmx_flash_t *flash = flash_info + chip_id;
+
+    /* Determine which block region we need to start writing to */
+    void *region_base = flash->base_ptr;
+    int region = 0;
+    while (region_base + flash->region[region].num_blocks * flash->region[region].block_size <= address)
+    {
+        region++;
+        region_base = flash->base_ptr + flash->region[region].start_offset;
+    }
+
+    /* Determine which block in the region to start at */
+    int block = (address - region_base) / flash->region[region].block_size;
+
+    /* Require all writes to start on block boundries */
+    if (address != region_base + block*flash->region[region].block_size)
+    {
+        cvmx_dprintf("cvmx-flash: Write address not aligned on a block boundry\n");
+        return -1;
+    }
+
+    /* Loop until we're out of data */
+    while (len > 0)
+    {
+        /* Erase the current block */
+        if (cvmx_flash_erase_block(chip_id, region, block))
+            return -1;
+        /* Write the new data */
+        if (cvmx_flash_write_block(chip_id, region, block, data))
+            return -1;
+
+        /* Increment to the next block */
+        data += flash->region[region].block_size;
+        len -= flash->region[region].block_size;
+        block++;
+        if (block >= flash->region[region].num_blocks)
+        {
+            block = 0;
+            region++;
+        }
+    }
+
+    return 0;
+}
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-flash.h b/arch/mips/cavium-octeon/executive/cvmx-flash.h
new file mode 100644
index 0000000..8501b3c
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-flash.h
@@ -0,0 +1,142 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * This file provides bootbus flash operations
+ *
+ * <hr>$Revision: 32636 $<hr>
+ *
+ *
+ */
+
+
+#ifndef __CVMX_FLASH_H__
+#define __CVMX_FLASH_H__
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+typedef struct
+{
+    int start_offset;
+    int block_size;
+    int num_blocks;
+} cvmx_flash_region_t;
+
+/**
+ * Initialize the flash access library
+ */
+void cvmx_flash_initialize(void);
+
+/**
+ * Return a pointer to the flash chip
+ *
+ * @param chip_id Chip ID to return
+ * @return NULL if the chip doesn't exist
+ */
+void *cvmx_flash_get_base(int chip_id);
+
+/**
+ * Return the number of erasable regions on the chip
+ *
+ * @param chip_id Chip to return info for
+ * @return Number of regions
+ */
+int cvmx_flash_get_num_regions(int chip_id);
+
+/**
+ * Return information about a flash chips region
+ *
+ * @param chip_id Chip to get info for
+ * @param region  Region to get info for
+ * @return Region information
+ */
+const cvmx_flash_region_t *cvmx_flash_get_region_info(int chip_id, int region);
+
+/**
+ * Erase a block on the flash chip
+ *
+ * @param chip_id Chip to erase a block on
+ * @param region  Region to erase a block in
+ * @param block   Block number to erase
+ * @return Zero on success. Negative on failure
+ */
+int cvmx_flash_erase_block(int chip_id, int region, int block);
+
+/**
+ * Write a block on the flash chip
+ *
+ * @param chip_id Chip to write a block on
+ * @param region  Region to write a block in
+ * @param block   Block number to write
+ * @param data    Data to write
+ * @return Zero on success. Negative on failure
+ */
+int cvmx_flash_write_block(int chip_id, int region, int block, const void *data);
+
+/**
+ * Erase and write data to a flash
+ *
+ * @param address Memory address to write to
+ * @param data    Data to write
+ * @param len     Length of the data
+ * @return Zero on success. Negative on failure
+ */
+int cvmx_flash_write(void *address, const void *data, int len);
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif /* __CVMX_FLASH_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-fpa.c b/arch/mips/cavium-octeon/executive/cvmx-fpa.c
new file mode 100644
index 0000000..99ed0e1
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-fpa.c
@@ -0,0 +1,219 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Support library for the hardware Free Pool Allocator.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ *
+ */
+
+#include "cvmx-config.h"
+#include "cvmx.h"
+#include "cvmx-fpa.h"
+
+/**
+ * Current state of all the pools. Use access functions
+ * instead of using it directly.
+ */
+CVMX_SHARED cvmx_fpa_pool_info_t cvmx_fpa_pool_info[CVMX_FPA_NUM_POOLS];
+
+
+/**
+ * Setup a FPA pool to control a new block of memory. The
+ * buffer pointer must be a physical address.
+ *
+ * @param pool       Pool to initialize
+ *                   0 <= pool < 8
+ * @param name       Constant character string to name this pool.
+ *                   String is not copied.
+ * @param buffer     Pointer to the block of memory to use. This must be
+ *                   accessable by all processors and external hardware.
+ * @param block_size Size for each block controlled by the FPA
+ * @param num_blocks Number of blocks
+ *
+ * @return 0 on Success,
+ *         -1 on failure
+ */
+int cvmx_fpa_setup_pool(uint64_t pool, const char *name, void *buffer,
+                         uint64_t block_size, uint64_t num_blocks)
+{
+    char *ptr;
+    if (!buffer)
+    {
+        cvmx_dprintf("ERROR: cvmx_fpa_setup_pool: NULL buffer pointer!\n");
+        return(-1);
+    }
+    if (pool >= CVMX_FPA_NUM_POOLS)
+    {
+        cvmx_dprintf("ERROR: cvmx_fpa_setup_pool: Illegal pool!\n");
+        return(-1);
+    }
+
+    if (block_size < CVMX_FPA_MIN_BLOCK_SIZE)
+    {
+        cvmx_dprintf("ERROR: cvmx_fpa_setup_pool: Block size too small.\n");
+        return(-1);
+    }
+
+    if (((unsigned long)buffer & (CVMX_FPA_ALIGNMENT-1)) != 0)
+    {
+        cvmx_dprintf("ERROR: cvmx_fpa_setup_pool: Buffer not aligned properly.\n");
+        return(-1);
+    }
+
+    cvmx_fpa_pool_info[pool].name = name;
+    cvmx_fpa_pool_info[pool].size = block_size;
+    cvmx_fpa_pool_info[pool].starting_element_count = num_blocks;
+    cvmx_fpa_pool_info[pool].base = buffer;
+
+    ptr = (char*)buffer;
+    while (num_blocks--)
+    {
+        cvmx_fpa_free(ptr, pool, 0);
+        ptr += block_size;
+    }
+    return(0);
+}
+
+
+/**
+ * Shutdown a Memory pool and validate that it had all of
+ * the buffers originally placed in it.
+ *
+ * @param pool   Pool to shutdown
+ * @return Zero on success
+ *         - Positive is count of missing buffers
+ *         - Negative is too many buffers or corrupted pointers
+ */
+uint64_t cvmx_fpa_shutdown_pool(uint64_t pool)
+{
+    uint64_t errors = 0;
+    uint64_t count  = 0;
+    uint64_t base   = cvmx_ptr_to_phys(cvmx_fpa_pool_info[pool].base);
+    uint64_t finish = base + cvmx_fpa_pool_info[pool].size * cvmx_fpa_pool_info[pool].starting_element_count;
+    void *ptr;
+    uint64_t address;
+    uint64_t lost;
+
+    count = 0;
+    do
+    {
+        ptr = cvmx_fpa_alloc(pool);
+        if (ptr)
+            address = cvmx_ptr_to_phys(ptr);
+        else
+            address = 0;
+        if (address)
+        {
+            if ((address >= base) && (address < finish) &&
+                (((address - base) % cvmx_fpa_pool_info[pool].size) == 0))
+            {
+                count++;
+            }
+            else
+            {
+                cvmx_dprintf("ERROR: cvmx_fpa_shutdown_pool: Illegal address 0x%llx in pool %s(%d)\n",
+                       (unsigned long long)address, cvmx_fpa_pool_info[pool].name, (int)pool);
+                errors++;
+            }
+        }
+    } while (address);
+
+    lost = cvmx_fpa_pool_info[pool].starting_element_count - count;
+    if (lost)
+    {
+        if ((pool == 0) && (lost <= 96))
+        {
+            /* Don't report up to 96 lost packet buffers. These are prefeteched
+                by IPD(64) and PIP(32) */
+            lost = 0;
+        }
+        else if ((pool == 1) && (lost <= 64))
+        {
+            /* Don't report up to 64 lost work queue buffers. These are prefeteched
+                by IPD(64) */
+            lost = 0;
+        }
+        else
+        {
+            cvmx_dprintf("ERROR: cvmx_fpa_shutdown_pool: Missing %llu buffers in pool %s(%d)\n",
+                   (unsigned long long)lost, cvmx_fpa_pool_info[pool].name, (int)pool);
+        }
+    }
+
+    if (errors)
+    {
+        cvmx_dprintf("ERROR: cvmx_fpa_shutdown_pool: Pool %s(%d) started at 0x%llx, ended at 0x%llx, with a step of 0x%llx\n",
+               cvmx_fpa_pool_info[pool].name, (int)pool, (unsigned long long)base, (unsigned long long)finish, (unsigned long long)cvmx_fpa_pool_info[pool].size);
+        return -errors;
+    }
+    else
+        return lost;
+}
+
+uint64_t cvmx_fpa_get_block_size(uint64_t pool)
+{
+    switch (pool)
+    {
+        case 0: return(CVMX_FPA_POOL_0_SIZE);
+        case 1: return(CVMX_FPA_POOL_1_SIZE);
+        case 2: return(CVMX_FPA_POOL_2_SIZE);
+        case 3: return(CVMX_FPA_POOL_3_SIZE);
+        case 4: return(CVMX_FPA_POOL_4_SIZE);
+        case 5: return(CVMX_FPA_POOL_5_SIZE);
+        case 6: return(CVMX_FPA_POOL_6_SIZE);
+        case 7: return(CVMX_FPA_POOL_7_SIZE);
+        default: return(0);
+    }
+}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-fpa.h b/arch/mips/cavium-octeon/executive/cvmx-fpa.h
new file mode 100644
index 0000000..a34d78d
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-fpa.h
@@ -0,0 +1,308 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Interface to the hardware Free Pool Allocator.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ *
+ */
+
+#ifndef __CVMX_FPA_H__
+#define __CVMX_FPA_H__
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#define CVMX_FPA_NUM_POOLS      8
+#define CVMX_FPA_MIN_BLOCK_SIZE 128
+#define CVMX_FPA_ALIGNMENT      128
+
+/**
+ * Structure describing the data format used for stores to the FPA.
+ */
+typedef union
+{
+    uint64_t        u64;
+    struct {
+        uint64_t    scraddr : 8;    /**< the (64-bit word) location in scratchpad to write to (if len != 0) */
+        uint64_t    len     : 8;    /**< the number of words in the response (0 => no response) */
+        uint64_t    did     : 8;    /**< the ID of the device on the non-coherent bus */
+        uint64_t    addr    :40;    /**< the address that will appear in the first tick on the NCB bus */
+    } s;
+} cvmx_fpa_iobdma_data_t;
+
+/**
+ * Structure describing the current state of a FPA pool.
+ */
+typedef struct
+{
+    const char *name;                   /**< Name it was created under */
+    uint64_t    size;                   /**< Size of each block */
+    void *      base;                   /**< The base memory address of whole block */
+    uint64_t    starting_element_count; /**< The number of elements in the pool at creation */
+} cvmx_fpa_pool_info_t;
+
+/**
+ * Current state of all the pools. Use access functions
+ * instead of using it directly.
+ */
+extern cvmx_fpa_pool_info_t cvmx_fpa_pool_info[CVMX_FPA_NUM_POOLS];
+
+/* CSR typedefs have been moved to cvmx-csr-*.h */
+
+/**
+ * Return the name of the pool
+ *
+ * @param pool   Pool to get the name of
+ * @return The name
+ */
+static inline const char *cvmx_fpa_get_name(uint64_t pool)
+{
+    return cvmx_fpa_pool_info[pool].name;
+}
+
+/**
+ * Return the base of the pool
+ *
+ * @param pool   Pool to get the base of
+ * @return The base
+ */
+static inline void *cvmx_fpa_get_base(uint64_t pool)
+{
+    return cvmx_fpa_pool_info[pool].base;
+}
+
+/**
+ * Check if a pointer belongs to an FPA pool. Return non-zero
+ * if the supplied pointer is inside the memory controlled by
+ * an FPA pool.
+ *
+ * @param pool   Pool to check
+ * @param ptr    Pointer to check
+ * @return Non-zero if pointer is in the pool. Zero if not
+ */
+static inline int cvmx_fpa_is_member(uint64_t pool, void *ptr)
+{
+    return ((ptr >= cvmx_fpa_pool_info[pool].base) &&
+            ((char*)ptr < ((char*)(cvmx_fpa_pool_info[pool].base)) + cvmx_fpa_pool_info[pool].size * cvmx_fpa_pool_info[pool].starting_element_count));
+}
+
+
+
+/**
+ * Enable the FPA for use. Must be performed after any CSR
+ * configuration but before any other FPA functions.
+ */
+static inline void cvmx_fpa_enable(void)
+{
+    cvmx_fpa_ctl_status_t status;
+
+    status.u64 = cvmx_read_csr(CVMX_FPA_CTL_STATUS);
+    if (status.s.enb)
+    {
+        cvmx_dprintf("Warning: Enabling FPA when FPA already enabled.\n");
+    }
+
+    /* Do runtime check as we allow pass1 compiled code to run on pass2 chips */
+    if (cvmx_octeon_is_pass1())
+    {
+        cvmx_fpa_fpf_marks_t marks;
+        int i;
+        for (i=1; i<8; i++)
+        {
+            marks.u64 = cvmx_read_csr(CVMX_FPA_FPF1_MARKS + (i-1)*8ull);
+            marks.s.fpf_wr = 0xe0;
+            cvmx_write_csr(CVMX_FPA_FPF1_MARKS + (i-1)*8ull, marks.u64);
+        }
+
+        /* Enforce a 10 cycle delay between config and enable */
+        cvmx_wait(10);
+    }
+
+    status.u64 = 0; /* FIXME: CVMX_FPA_CTL_STATUS read is unmodelled */
+    status.s.enb = 1;
+    cvmx_write_csr(CVMX_FPA_CTL_STATUS, status.u64);
+}
+
+
+/**
+ * Get a new block from the FPA
+ *
+ * @param pool   Pool to get the block from
+ * @return Pointer to the block or NULL on failure
+ */
+static inline void *cvmx_fpa_alloc(uint64_t pool)
+{
+    uint64_t address = cvmx_read_csr(CVMX_ADDR_DID(CVMX_FULL_DID(CVMX_OCT_DID_FPA,pool)));
+    if (address)
+        return cvmx_phys_to_ptr(address);
+    else
+        return NULL;
+}
+
+
+/**
+ * Asynchronously get a new block from the FPA
+ *
+ * @param scr_addr Local scratch address to put response in.  This is a byte address,
+ *                  but must be 8 byte aligned.
+ * @param pool      Pool to get the block from
+ */
+static inline void cvmx_fpa_async_alloc(uint64_t scr_addr, uint64_t pool)
+{
+   cvmx_fpa_iobdma_data_t data;
+
+   /* Hardware only uses 64 bit alligned locations, so convert from byte address
+   ** to 64-bit index
+   */
+   data.s.scraddr = scr_addr >> 3;
+   data.s.len = 1;
+   data.s.did = CVMX_FULL_DID(CVMX_OCT_DID_FPA,pool);
+   data.s.addr = 0;
+   cvmx_send_single(data.u64);
+}
+
+
+/**
+ * Free a block allocated with a FPA pool.
+ * Does NOT provide memory ordering in cases where the memory block was modified by the core.
+ *
+ * @param ptr    Block to free
+ * @param pool   Pool to put it in
+ * @param num_cache_lines
+ *               Cache lines to invalidate
+ */
+static inline void cvmx_fpa_free_nosync(void *ptr, uint64_t pool, uint64_t num_cache_lines)
+{
+    cvmx_addr_t newptr;
+    newptr.u64 = cvmx_ptr_to_phys(ptr);
+    newptr.sfilldidspace.didspace = CVMX_ADDR_DIDSPACE(CVMX_FULL_DID(CVMX_OCT_DID_FPA,pool));
+    asm volatile ("" : : : "memory");  /* Prevent GCC from reordering around free */
+    /* value written is number of cache lines not written back */
+    cvmx_write_io(newptr.u64, num_cache_lines);
+}
+
+/**
+ * Free a block allocated with a FPA pool.  Provides required memory
+ * ordering in cases where memory block was modified by core.
+ *
+ * @param ptr    Block to free
+ * @param pool   Pool to put it in
+ * @param num_cache_lines
+ *               Cache lines to invalidate
+ */
+static inline void cvmx_fpa_free(void *ptr, uint64_t pool, uint64_t num_cache_lines)
+{
+    cvmx_addr_t newptr;
+    newptr.u64 = cvmx_ptr_to_phys(ptr);
+    newptr.sfilldidspace.didspace = CVMX_ADDR_DIDSPACE(CVMX_FULL_DID(CVMX_OCT_DID_FPA,pool));
+    /* Make sure that any previous writes to memory go out before we free this buffer.
+    ** This also serves as a barrier to prevent GCC from reordering operations to after
+    ** the free. */
+    CVMX_SYNCWS;
+    /* value written is number of cache lines not written back */
+    cvmx_write_io(newptr.u64, num_cache_lines);
+}
+
+
+/**
+ * Setup a FPA pool to control a new block of memory.
+ * This can only be called once per pool. Make sure proper
+ * locking enforces this.
+ *
+ * @param pool       Pool to initialize
+ *                   0 <= pool < 8
+ * @param name       Constant character string to name this pool.
+ *                   String is not copied.
+ * @param buffer     Pointer to the block of memory to use. This must be
+ *                   accessable by all processors and external hardware.
+ * @param block_size Size for each block controlled by the FPA
+ * @param num_blocks Number of blocks
+ *
+ * @return 0 on Success,
+ *         -1 on failure
+ */
+extern int cvmx_fpa_setup_pool(uint64_t pool, const char *name, void *buffer,
+                                uint64_t block_size, uint64_t num_blocks);
+
+
+/**
+ * Shutdown a Memory pool and validate that it had all of
+ * the buffers originally placed in it. This should only be
+ * called by one processor after all hardware has finished
+ * using the pool.
+ *
+ * @param pool   Pool to shutdown
+ * @return Zero on success
+ *         - Positive is count of missing buffers
+ *         - Negative is too many buffers or corrupted pointers
+ */
+extern uint64_t cvmx_fpa_shutdown_pool(uint64_t pool);
+
+
+/**
+ * Get the size of blocks controlled by the pool
+ * This is resolved to a constant at compile time.
+ *
+ * @param pool   Pool to access
+ * @return Size of the block in bytes
+ */
+uint64_t cvmx_fpa_get_block_size(uint64_t pool);
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif //  __CVM_FPA_H__
diff --git a/arch/mips/cavium-octeon/executive/cvmx-gmx.h b/arch/mips/cavium-octeon/executive/cvmx-gmx.h
new file mode 100644
index 0000000..d420fd1
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-gmx.h
@@ -0,0 +1,102 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Interface to the GMX hardware.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+
+#ifndef __CVMX_GMX_H__
+#define __CVMX_GMX_H__
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+/* CSR typedefs have been moved to cvmx-csr-*.h */
+
+/**
+ * Disables the sending of flow control (pause) frames on the specified
+ * RGMII port(s).
+ *
+ * @param interface Which interface (0 or 1)
+ * @param port_mask Mask (4bits) of which ports on the interface to disable
+ *                  backpressure on.
+ *                  1 => disable backpressure
+ *                  0 => enable backpressure
+ *
+ * @return 0 on success
+ *         -1 on error
+ */
+
+static inline int cvmx_gmx_set_backpressure_override(uint32_t interface, uint32_t port_mask)
+{
+    cvmx_gmxx_tx_ovr_bp_t gmxx_tx_ovr_bp;
+    /* Check for valid arguments */
+    if (port_mask & ~0xf || interface & ~0x1)
+        return(-1);
+    gmxx_tx_ovr_bp.u64 = 0;
+    gmxx_tx_ovr_bp.s.en = port_mask; /* Per port Enable back pressure override */
+    gmxx_tx_ovr_bp.s.ign_full = port_mask; /* Ignore the RX FIFO full when computing BP */
+    cvmx_write_csr(CVMX_GMXX_TX_OVR_BP(interface), gmxx_tx_ovr_bp.u64);
+    return(0);
+
+}
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-gpio.h b/arch/mips/cavium-octeon/executive/cvmx-gpio.h
new file mode 100644
index 0000000..176e1bd
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-gpio.h
@@ -0,0 +1,130 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * General Purpose IO interface.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+
+#ifndef __CVMX_GPIO_H__
+#define __CVMX_GPIO_H__
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+/* CSR typedefs have been moved to cvmx-csr-*.h */
+
+/**
+ * Clear the interrupt rising edge detector for the supplied
+ * pins in the mask. Chips which have more than 16 GPIO pins
+ * can't use them for interrupts.
+ *
+ * @param clear_mask Mask of pins to clear
+ */
+static inline void cvmx_gpio_interrupt_clear(uint16_t clear_mask)
+{
+    cvmx_gpio_int_clr_t gpio_int_clr;
+    gpio_int_clr.u64 = 0;
+    gpio_int_clr.s.type = clear_mask;
+    cvmx_write_csr(CVMX_GPIO_INT_CLR, gpio_int_clr.u64);
+}
+
+
+/**
+ * GPIO Read Data
+ *
+ * @return Status of the GPIO pins
+ */
+static inline uint32_t cvmx_gpio_read(void)
+{
+    cvmx_gpio_rx_dat_t gpio_rx_dat;
+    gpio_rx_dat.u64 = cvmx_read_csr(CVMX_GPIO_RX_DAT);
+    return gpio_rx_dat.s.dat;
+}
+
+
+/**
+ * GPIO Clear pin
+ *
+ * @param clear_mask Bit mask to indicate which bits to drive to '0'.
+ */
+static inline void cvmx_gpio_clear(uint32_t clear_mask)
+{
+    cvmx_gpio_tx_clr_t gpio_tx_clr;
+    gpio_tx_clr.u64 = 0;
+    gpio_tx_clr.s.clr = clear_mask;
+    cvmx_write_csr(CVMX_GPIO_TX_CLR, gpio_tx_clr.u64);
+}
+
+
+/**
+ * GPIO Set pin
+ *
+ * @param set_mask Bit mask to indicate which bits to drive to '1'.
+ */
+static inline void cvmx_gpio_set(uint32_t set_mask)
+{
+    cvmx_gpio_tx_set_t gpio_tx_set;
+    gpio_tx_set.u64 = 0;
+    gpio_tx_set.s.set = set_mask;
+    cvmx_write_csr(CVMX_GPIO_TX_SET, gpio_tx_set.u64);
+}
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-board.c b/arch/mips/cavium-octeon/executive/cvmx-helper-board.c
new file mode 100644
index 0000000..f096f3c
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-board.c
@@ -0,0 +1,633 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Helper functions to abstract board specific data about
+ * network ports from the rest of the cvmx-helper files.
+ *
+ * <hr>$Revision: 35733 $<hr>
+ */
+#include "cvmx.h"
+#include "cvmx-app-init.h"
+#include "cvmx-mdio.h"
+#include "cvmx-sysinfo.h"
+#include "cvmx-helper.h"
+#include "cvmx-helper-util.h"
+#include "cvmx-helper-board.h"
+
+/**
+ * cvmx_override_board_link_get(int ipd_port) is a function
+ * pointer. It is meant to allow customization of the process of
+ * talking to a PHY to determine link speed. It is called every
+ * time a PHY must be polled for link status. Users should set
+ * this pointer to a function before calling any cvmx-helper
+ * operations.
+ */
+CVMX_SHARED cvmx_helper_link_info_t (*cvmx_override_board_link_get)(int ipd_port) = NULL;
+
+/**
+ * Return the MII PHY address associated with the given IPD
+ * port. A result of -1 means there isn't a MII capable PHY
+ * connected to this port. On chips supporting multiple MII
+ * busses the bus number is encoded in bits <15:8>.
+ *
+ * This function must be modified for every new Octeon board.
+ * Internally it uses switch statements based on the cvmx_sysinfo
+ * data to determine board types and revisions. It replies on the
+ * fact that every Octeon board receives a unique board type
+ * enumeration from the bootloader.
+ *
+ * @param ipd_port Octeon IPD port to get the MII address for.
+ *
+ * @return MII PHY address and bus number or -1.
+ */
+int cvmx_helper_board_get_mii_address(int ipd_port)
+{
+    switch (cvmx_sysinfo_get()->board_type)
+    {
+        case CVMX_BOARD_TYPE_SIM:
+            /* Simulator doesn't have MII */
+            return -1;
+        case CVMX_BOARD_TYPE_EBT3000:
+        case CVMX_BOARD_TYPE_EBT5800:
+        case CVMX_BOARD_TYPE_THUNDER:
+        case CVMX_BOARD_TYPE_NICPRO2:
+            /* Interface 0 is SPI4, interface 1 is RGMII */
+            if ((ipd_port >= 16) && (ipd_port < 20))
+                return ipd_port - 16;
+            else
+                return -1;
+        case CVMX_BOARD_TYPE_KODAMA:
+        case CVMX_BOARD_TYPE_EBH3100:
+        case CVMX_BOARD_TYPE_HIKARI:
+        case CVMX_BOARD_TYPE_CN3010_EVB_HS5:
+        case CVMX_BOARD_TYPE_CN3005_EVB_HS5:
+        case CVMX_BOARD_TYPE_CN3020_EVB_HS5:
+            /* Port 0 is WAN connected to a PHY, Port 1 is GMII connected to a
+                switch */
+            if (ipd_port == 0)
+                return 4;
+            else if (ipd_port == 1)
+                return 9;
+            else
+                return -1;
+        case CVMX_BOARD_TYPE_NAC38:
+            /* Board has 8 RGMII ports PHYs are 0-7 */
+            if ((ipd_port >= 0) && (ipd_port < 4))
+                return ipd_port;
+            else if ((ipd_port >= 16) && (ipd_port < 20))
+                return ipd_port - 16 + 4;
+            else
+                return -1;
+        case CVMX_BOARD_TYPE_EBH3000:
+            /* Board has dual SPI4 and no PHYs */
+            return -1;
+        case CVMX_BOARD_TYPE_EBH5200:
+        case CVMX_BOARD_TYPE_EBH5201:
+            /* Board has 4 SGMII ports. The PHYs start right after the MII
+                ports MII0 = 0, MII1 = 1, SGMII = 2-5 */
+            if ((ipd_port >= 0) && (ipd_port < 4))
+                return ipd_port+2;
+            else
+                return -1;
+        case CVMX_BOARD_TYPE_EBH5600:
+        case CVMX_BOARD_TYPE_EBH5601:
+            /* Board has 8 SGMII ports. 4 connect out, two connect to a switch,
+                and 2 loop to each other */
+            if ((ipd_port >= 0) && (ipd_port < 4))
+                return ipd_port+1;
+            else
+                return -1;
+        case CVMX_BOARD_TYPE_CUST_NB5:
+            if (ipd_port == 2)
+                return 4;
+            else
+                return -1;
+        case CVMX_BOARD_TYPE_NIC_XLE_4G:
+            /* Board has 4 SGMII ports. connected QLM3(interface 1) */
+            if ((ipd_port >= 16) && (ipd_port < 20))
+                return ipd_port - 16 + 1;
+            else
+                return -1;
+        case CVMX_BOARD_TYPE_BBGW_REF:
+            return -1;  /* No PHYs are connected to Octeon, everything is through switch */
+    }
+
+    /* Some unknown board. Somebody forgot to update this function... */
+    cvmx_dprintf("cvmx_helper_board_get_mii_address: Unknown board type %d\n",
+                 cvmx_sysinfo_get()->board_type);
+    return -1;
+}
+
+
+/**
+ * @INTERNAL
+ * This function is the board specific method of determining an
+ * ethernet ports link speed. Most Octeon boards have Marvell PHYs
+ * and are handled by the fall through case. This function must be
+ * updated for boards that don't have the normal Marvell PHYs.
+ *
+ * This function must be modified for every new Octeon board.
+ * Internally it uses switch statements based on the cvmx_sysinfo
+ * data to determine board types and revisions. It relies on the
+ * fact that every Octeon board receives a unique board type
+ * enumeration from the bootloader.
+ *
+ * @param ipd_port IPD input port associated with the port we want to get link
+ *                 status for.
+ *
+ * @return The ports link status. If the link isn't fully resolved, this must
+ *         return zero.
+ */
+cvmx_helper_link_info_t __cvmx_helper_board_link_get(int ipd_port)
+{
+    cvmx_helper_link_info_t result;
+    int phy_addr;
+    int is_broadcom_phy = 0;
+
+    /* Give the user a chance to override the processing of this function */
+    if (cvmx_override_board_link_get)
+        return cvmx_override_board_link_get(ipd_port);
+
+    /* Unless we fix it later, all links are defaulted to down */
+    result.u64 = 0;
+
+    /* This switch statement should handle all ports that either don't use
+        Marvell PHYS, or don't support in-band status */
+    switch (cvmx_sysinfo_get()->board_type)
+    {
+        case CVMX_BOARD_TYPE_SIM:
+            /* The simulator gives you a simulated 1Gbps full duplex link */
+            result.s.link_up = 1;
+            result.s.full_duplex = 1;
+            result.s.speed = 1000;
+            return result;
+        case CVMX_BOARD_TYPE_EBH3100:
+        case CVMX_BOARD_TYPE_CN3010_EVB_HS5:
+        case CVMX_BOARD_TYPE_CN3005_EVB_HS5:
+        case CVMX_BOARD_TYPE_CN3020_EVB_HS5:
+            /* Port 1 on these boards is always Gigabit */
+            if (ipd_port == 1)
+            {
+                result.s.link_up = 1;
+                result.s.full_duplex = 1;
+                result.s.speed = 1000;
+                return result;
+            }
+            /* Fall through to the generic code below */
+            break;
+        case CVMX_BOARD_TYPE_CUST_NB5:
+            /* Port 1 on these boards is always Gigabit */
+            if (ipd_port == 1)
+            {
+                result.s.link_up = 1;
+                result.s.full_duplex = 1;
+                result.s.speed = 1000;
+                return result;
+            }
+            else /* The other port uses a broadcom PHY */
+                is_broadcom_phy = 1;
+            break;
+        case CVMX_BOARD_TYPE_BBGW_REF:
+            /* Port 1 on these boards is always Gigabit */
+            if (ipd_port == 2)
+            {   
+                /* Port 2 is not hooked up */
+                result.u64 = 0;
+                return result;
+            }
+            else
+            {
+                /* Ports 0 and 1 connect to the switch */
+                result.s.link_up = 1;
+                result.s.full_duplex = 1;
+                result.s.speed = 1000;
+                return result;
+            }
+            break;
+    }
+
+    phy_addr = cvmx_helper_board_get_mii_address(ipd_port);
+    if (phy_addr != -1)
+    {
+        if (is_broadcom_phy)
+        {
+            /* Below we are going to read SMI/MDIO register 0x19 which works
+                on Broadcom parts */
+            int phy_status = cvmx_mdio_read(phy_addr >> 8, phy_addr & 0xff, 0x19);
+            switch ((phy_status>>8) & 0x7)
+            {
+                case 0:
+                    result.u64 = 0;
+                    break;
+                case 1:
+                    result.s.link_up = 1;
+                    result.s.full_duplex = 0;
+                    result.s.speed = 10;
+                    break;
+                case 2:
+                    result.s.link_up = 1;
+                    result.s.full_duplex = 1;
+                    result.s.speed = 10;
+                    break;
+                case 3:
+                    result.s.link_up = 1;
+                    result.s.full_duplex = 0;
+                    result.s.speed = 100;
+                    break;
+                case 4:
+                    result.s.link_up = 1;
+                    result.s.full_duplex = 1;
+                    result.s.speed = 100;
+                    break;
+                case 5:
+                    result.s.link_up = 1;
+                    result.s.full_duplex = 1;
+                    result.s.speed = 100;
+                    break;
+                case 6:
+                    result.s.link_up = 1;
+                    result.s.full_duplex = 0;
+                    result.s.speed = 1000;
+                    break;
+                case 7:
+                    result.s.link_up = 1;
+                    result.s.full_duplex = 1;
+                    result.s.speed = 1000;
+                    break;
+            }
+        }
+        else
+        {
+            /* This code assumes we are using a Marvell Gigabit PHY. All the
+                speed information can be read from register 17 in one go. Somebody
+                using a different PHY will need to handle it above in the board
+                specific area */
+            int phy_status = cvmx_mdio_read(phy_addr >> 8, phy_addr & 0xff, 17);
+
+            /* If the resolve bit 11 isn't set, see if autoneg is turned off
+                (bit 12, reg 0). The resolve bit doesn't get set properly when
+                autoneg is off, so force it */
+            if ((phy_status & (1<<11)) == 0)
+            {
+                int auto_status = cvmx_mdio_read(phy_addr >> 8, phy_addr & 0xff, 0);
+                if ((auto_status & (1<<12)) == 0)
+                    phy_status |= 1<<11;
+            }
+
+            /* Only return a link if the PHY has finished auto negotiation
+                and set the resolved bit (bit 11) */
+            if (phy_status & (1<<11))
+            {
+                result.s.link_up = 1;
+                result.s.full_duplex = ((phy_status>>13)&1);
+                switch ((phy_status>>14)&3)
+                {
+                    case 0: /* 10 Mbps */
+                        result.s.speed = 10;
+                        break;
+                    case 1: /* 100 Mbps */
+                        result.s.speed = 100;
+                        break;
+                    case 2: /* 1 Gbps */
+                        result.s.speed = 1000;
+                        break;
+                    case 3: /* Illegal */
+                        result.u64 = 0;
+                        break;
+                }
+            }
+        }
+    }
+    else if (OCTEON_IS_MODEL(OCTEON_CN3XXX) || OCTEON_IS_MODEL(OCTEON_CN58XX) || OCTEON_IS_MODEL(OCTEON_CN50XX))
+    {
+        /* We don't have a PHY address, so attempt to use in-band status. It is
+            really important that boards not supporting in-band status never get
+            here. Reading broken in-band status tends to do bad things */
+        cvmx_gmxx_rxx_rx_inbnd_t inband_status;
+        int interface = cvmx_helper_get_interface_num(ipd_port);
+        int index = cvmx_helper_get_interface_index_num(ipd_port);
+        inband_status.u64 = cvmx_read_csr(CVMX_GMXX_RXX_RX_INBND(index, interface));
+
+        result.s.link_up = inband_status.s.status;
+        result.s.full_duplex = inband_status.s.duplex;
+        switch (inband_status.s.speed)
+        {
+            case 0: /* 10 Mbps */
+                result.s.speed = 10;
+                break;
+            case 1: /* 100 Mbps */
+                result.s.speed = 100;
+                break;
+            case 2: /* 1 Gbps */
+                result.s.speed = 1000;
+                break;
+            case 3: /* Illegal */
+                result.u64 = 0;
+                break;
+        }
+    }
+    else
+    {
+        /* We don't have a PHY address and we don't have in-band status. There
+            is no way to determine the link speed. Return down assuming this
+            port isn't wired */
+        result.u64 = 0;
+    }
+
+    /* If link is down, return all fields as zero. */
+    if (!result.s.link_up)
+        result.u64 = 0;
+
+    return result;
+}
+
+
+/**
+ * This function as a board specific method of changing the PHY
+ * speed, duplex, and auto-negotiation. This programs the PHY and
+ * not Octeon. This can be used to force Octeon's links to
+ * specific settings.
+ *
+ * @param phy_addr  The address of the PHY to program
+ * @param enable_autoneg
+ *                  Non zero if you want to enable auto-negotiation.
+ * @param link_info Link speed to program. If the speed is zero and auto-negotiation
+ *                  is enabled, all possible negotiation speeds are advertised.
+ *
+ * @return Zero on success, negative on failure
+ */
+int cvmx_helper_board_link_set_phy(int phy_addr, int enable_autoneg,
+                                   cvmx_helper_link_info_t link_info)
+{
+    /* If speed isn't set and autoneg is on advertise all supported modes */
+    if (enable_autoneg && (link_info.s.speed == 0))
+    {
+        cvmx_mdio_phy_reg_control_t reg_control;
+        cvmx_mdio_phy_reg_status_t reg_status;
+        cvmx_mdio_phy_reg_autoneg_adver_t reg_autoneg_adver;
+        cvmx_mdio_phy_reg_extended_status_t reg_extended_status;
+        cvmx_mdio_phy_reg_control_1000_t reg_control_1000;
+
+        reg_status.u16 = cvmx_mdio_read(phy_addr >> 8, phy_addr & 0xff, CVMX_MDIO_PHY_REG_STATUS);
+        reg_autoneg_adver.u16 = cvmx_mdio_read(phy_addr >> 8, phy_addr & 0xff, CVMX_MDIO_PHY_REG_AUTONEG_ADVER);
+        reg_autoneg_adver.s.advert_100base_t4 = reg_status.s.capable_100base_t4;
+        reg_autoneg_adver.s.advert_10base_tx_full = reg_status.s.capable_10_full;
+        reg_autoneg_adver.s.advert_10base_tx_half = reg_status.s.capable_10_half;
+        reg_autoneg_adver.s.advert_100base_tx_full = reg_status.s.capable_100base_x_full;
+        reg_autoneg_adver.s.advert_100base_tx_half = reg_status.s.capable_100base_x_half;
+        cvmx_mdio_write(phy_addr >> 8, phy_addr & 0xff, CVMX_MDIO_PHY_REG_AUTONEG_ADVER, reg_autoneg_adver.u16);
+        if (reg_status.s.capable_extended_status)
+        {
+            reg_extended_status.u16 = cvmx_mdio_read(phy_addr >> 8, phy_addr & 0xff, CVMX_MDIO_PHY_REG_EXTENDED_STATUS);
+            reg_control_1000.u16 = cvmx_mdio_read(phy_addr >> 8, phy_addr & 0xff, CVMX_MDIO_PHY_REG_CONTROL_1000);
+            reg_control_1000.s.advert_1000base_t_full = reg_extended_status.s.capable_1000base_t_full;
+            reg_control_1000.s.advert_1000base_t_half = reg_extended_status.s.capable_1000base_t_half;
+            cvmx_mdio_write(phy_addr >> 8, phy_addr & 0xff, CVMX_MDIO_PHY_REG_CONTROL_1000, reg_control_1000.u16);
+        }
+        reg_control.u16 = cvmx_mdio_read(phy_addr >> 8, phy_addr & 0xff, CVMX_MDIO_PHY_REG_CONTROL);
+        reg_control.s.autoneg_enable = 1;
+        reg_control.s.restart_autoneg = 1;
+        cvmx_mdio_write(phy_addr >> 8, phy_addr & 0xff, CVMX_MDIO_PHY_REG_CONTROL, reg_control.u16);
+    }
+    else if (enable_autoneg)
+    {
+        cvmx_mdio_phy_reg_control_t reg_control;
+        cvmx_mdio_phy_reg_status_t reg_status;
+        cvmx_mdio_phy_reg_autoneg_adver_t reg_autoneg_adver;
+        cvmx_mdio_phy_reg_extended_status_t reg_extended_status;
+        cvmx_mdio_phy_reg_control_1000_t reg_control_1000;
+
+        reg_status.u16 = cvmx_mdio_read(phy_addr >> 8, phy_addr & 0xff, CVMX_MDIO_PHY_REG_STATUS);
+        reg_autoneg_adver.u16 = cvmx_mdio_read(phy_addr >> 8, phy_addr & 0xff, CVMX_MDIO_PHY_REG_AUTONEG_ADVER);
+        reg_autoneg_adver.s.advert_100base_t4 = 0;
+        reg_autoneg_adver.s.advert_10base_tx_full = 0;
+        reg_autoneg_adver.s.advert_10base_tx_half = 0;
+        reg_autoneg_adver.s.advert_100base_tx_full = 0;
+        reg_autoneg_adver.s.advert_100base_tx_half = 0;
+        if (reg_status.s.capable_extended_status)
+        {
+            reg_extended_status.u16 = cvmx_mdio_read(phy_addr >> 8, phy_addr & 0xff, CVMX_MDIO_PHY_REG_EXTENDED_STATUS);
+            reg_control_1000.u16 = cvmx_mdio_read(phy_addr >> 8, phy_addr & 0xff, CVMX_MDIO_PHY_REG_CONTROL_1000);
+            reg_control_1000.s.advert_1000base_t_full = 0;
+            reg_control_1000.s.advert_1000base_t_half = 0;
+        }
+        switch (link_info.s.speed)
+        {
+            case 10:
+                reg_autoneg_adver.s.advert_10base_tx_full = link_info.s.full_duplex;
+                reg_autoneg_adver.s.advert_10base_tx_half = !link_info.s.full_duplex;
+                break;
+            case 100:
+                reg_autoneg_adver.s.advert_100base_tx_full = link_info.s.full_duplex;
+                reg_autoneg_adver.s.advert_100base_tx_half = !link_info.s.full_duplex;
+                break;
+            case 1000:
+                reg_control_1000.s.advert_1000base_t_full = link_info.s.full_duplex;
+                reg_control_1000.s.advert_1000base_t_half = !link_info.s.full_duplex;
+                break;
+        }
+        cvmx_mdio_write(phy_addr >> 8, phy_addr & 0xff, CVMX_MDIO_PHY_REG_AUTONEG_ADVER, reg_autoneg_adver.u16);
+        if (reg_status.s.capable_extended_status)
+            cvmx_mdio_write(phy_addr >> 8, phy_addr & 0xff, CVMX_MDIO_PHY_REG_CONTROL_1000, reg_control_1000.u16);
+        reg_control.u16 = cvmx_mdio_read(phy_addr >> 8, phy_addr & 0xff, CVMX_MDIO_PHY_REG_CONTROL);
+        reg_control.s.autoneg_enable = 1;
+        reg_control.s.restart_autoneg = 1;
+        cvmx_mdio_write(phy_addr >> 8, phy_addr & 0xff, CVMX_MDIO_PHY_REG_CONTROL, reg_control.u16);
+    }
+    else
+    {
+        cvmx_mdio_phy_reg_control_t reg_control;
+        reg_control.u16 = cvmx_mdio_read(phy_addr >> 8, phy_addr & 0xff, CVMX_MDIO_PHY_REG_CONTROL);
+        reg_control.s.autoneg_enable = 0;
+        reg_control.s.restart_autoneg = 1;
+        reg_control.s.duplex = link_info.s.full_duplex;
+        if (link_info.s.speed == 1000)
+        {
+            reg_control.s.speed_msb = 1;
+            reg_control.s.speed_lsb = 0;
+        }
+        else if (link_info.s.speed == 100)
+        {
+            reg_control.s.speed_msb = 0;
+            reg_control.s.speed_lsb = 1;
+        }
+        else if (link_info.s.speed == 10)
+        {
+            reg_control.s.speed_msb = 0;
+            reg_control.s.speed_lsb = 0;
+        }
+        cvmx_mdio_write(phy_addr >> 8, phy_addr & 0xff, CVMX_MDIO_PHY_REG_CONTROL, reg_control.u16);
+    }
+    return 0;
+}
+
+
+/**
+ * @INTERNAL
+ * This function is called by cvmx_helper_interface_probe() after it
+ * determines the number of ports Octeon can support on a specific
+ * interface. This function is the per board location to override
+ * this value. It is called with the number of ports Octeon might
+ * support and should return the number of actual ports on the
+ * board.
+ *
+ * This function must be modifed for every new Octeon board.
+ * Internally it uses switch statements based on the cvmx_sysinfo
+ * data to determine board types and revisions. It relys on the
+ * fact that every Octeon board receives a unique board type
+ * enumeration from the bootloader.
+ *
+ * @param interface Interface to probe
+ * @param supported_ports
+ *                  Number of ports Octeon supports.
+ *
+ * @return Number of ports the actual board supports. Many times this will
+ *         simple be "support_ports".
+ */
+int __cvmx_helper_board_interface_probe(int interface, int supported_ports)
+{
+    switch (cvmx_sysinfo_get()->board_type)
+    {
+        case CVMX_BOARD_TYPE_CN3005_EVB_HS5:
+            if (interface == 0)
+                return 2;
+        case CVMX_BOARD_TYPE_BBGW_REF:
+            if (interface == 0)
+                return 2;
+        case CVMX_BOARD_TYPE_NIC_XLE_4G:
+            if (interface == 0)
+                return 0;
+
+    }
+#ifdef CVMX_BUILD_FOR_UBOOT
+    if (CVMX_HELPER_INTERFACE_MODE_SPI == cvmx_helper_interface_get_mode(interface) && getenv("disable_spi"))
+        return 0;
+#endif
+    return supported_ports;
+}
+
+
+/**
+ * @INTERNAL
+ * Enable packet input/output from the hardware. This function is
+ * called after by cvmx_helper_packet_hardware_enable() to
+ * perform board specific initialization. For most boards
+ * nothing is needed.
+ *
+ * @param interface Interface to enable
+ *
+ * @return Zero on success, negative on failure
+ */
+int __cvmx_helper_board_hardware_enable(int interface)
+{
+    if (cvmx_sysinfo_get()->board_type == CVMX_BOARD_TYPE_TRANTOR)
+    {
+        if (interface < 2)
+        {
+            int index;
+            for (index = 0; index<4; index++)
+            {
+                cvmx_write_csr(CVMX_ASXX_TX_CLK_SETX(index, interface), 0);
+                cvmx_write_csr(CVMX_ASXX_RX_CLK_SETX(index, interface), 16);
+            }
+        }
+    }
+    else if (cvmx_sysinfo_get()->board_type == CVMX_BOARD_TYPE_CN3005_EVB_HS5)
+    {
+        if (interface == 0)
+        {
+            /* Different config for switch port */
+            cvmx_write_csr(CVMX_ASXX_TX_CLK_SETX(1, interface), 0);
+            cvmx_write_csr(CVMX_ASXX_RX_CLK_SETX(1, interface), 0);
+            /* Boards with gigabit WAN ports need a different setting that is
+                compatible with 100 Mbit settings */
+            cvmx_write_csr(CVMX_ASXX_TX_CLK_SETX(0, interface), 0xc);
+            cvmx_write_csr(CVMX_ASXX_RX_CLK_SETX(0, interface), 0xc);
+        }
+    }
+    else if (cvmx_sysinfo_get()->board_type == CVMX_BOARD_TYPE_CN3010_EVB_HS5)
+    {
+        /* Broadcom PHYs require differnet ASX clocks. Unfortunately
+            many customer don't define a new board Id and simply
+            mangle the CN3010_EVB_HS5 */
+        if (interface == 0)
+        {
+            /* Some customers boards use a hacked up bootloader that identifies them as
+            ** CN3010_EVB_HS5 evaluation boards.  This leads to all kinds of configuration
+            ** problems.  Detect one case, and print warning, while trying to do the right thing.
+            */
+            int phy_addr = cvmx_helper_board_get_mii_address(0);
+            if (phy_addr != -1)
+            {
+                int phy_identifier = cvmx_mdio_read(phy_addr >> 8, phy_addr & 0xff, 0x2);
+                /* Is it a Broadcom PHY? */
+                if (phy_identifier == 0x0143)
+                {
+                    cvmx_dprintf("\n");
+                    cvmx_dprintf("ERROR:\n");
+                    cvmx_dprintf("ERROR: Board type is CVMX_BOARD_TYPE_CN3010_EVB_HS5, but Broadcom PHY found.\n");
+                    cvmx_dprintf("ERROR: The board type is mis-configured, and software malfunctions are likely.\n");
+                    cvmx_dprintf("ERROR: All boards require a unique board type to identify them.\n");
+                    cvmx_dprintf("ERROR:\n");
+                    cvmx_dprintf("\n");
+                    cvmx_wait(1000000000);
+                    cvmx_write_csr(CVMX_ASXX_RX_CLK_SETX(0, interface), 5);
+                    cvmx_write_csr(CVMX_ASXX_TX_CLK_SETX(0, interface), 5);
+                }
+            }
+        }
+    }
+    return 0;
+}
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-board.h b/arch/mips/cavium-octeon/executive/cvmx-helper-board.h
new file mode 100644
index 0000000..e1204c0
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-board.h
@@ -0,0 +1,174 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Helper functions to abstract board specific data about
+ * network ports from the rest of the cvmx-helper files.
+ *
+ * <hr>$Revision: 35310 $<hr>
+ */
+#ifndef __CVMX_HELPER_BOARD_H__
+#define __CVMX_HELPER_BOARD_H__
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+/**
+ * cvmx_override_board_link_get(int ipd_port) is a function
+ * pointer. It is meant to allow customization of the process of
+ * talking to a PHY to determine link speed. It is called every
+ * time a PHY must be polled for link status. Users should set
+ * this pointer to a function before calling any cvmx-helper
+ * operations.
+ */
+extern cvmx_helper_link_info_t (*cvmx_override_board_link_get)(int ipd_port);
+
+/**
+ * Return the MII PHY address associated with the given IPD
+ * port. A result of -1 means there isn't a MII capable PHY
+ * connected to this port. On chips supporting multiple MII
+ * busses the bus number is encoded in bits <15:8>.
+ *
+ * This function must be modifed for every new Octeon board.
+ * Internally it uses switch statements based on the cvmx_sysinfo
+ * data to determine board types and revisions. It relys on the
+ * fact that every Octeon board receives a unique board type
+ * enumeration from the bootloader.
+ *
+ * @param ipd_port Octeon IPD port to get the MII address for.
+ *
+ * @return MII PHY address and bus number or -1.
+ */
+extern int cvmx_helper_board_get_mii_address(int ipd_port);
+
+/**
+ * This function as a board specific method of changing the PHY
+ * speed, duplex, and autonegotiation. This programs the PHY and
+ * not Octeon. This can be used to force Octeon's links to
+ * specific settings.
+ *
+ * @param phy_addr  The address of the PHY to program
+ * @param enable_autoneg
+ *                  Non zero if you want to enable autonegotiaion.
+ * @param link_info Link speed to program. If the speed is zero and autonegotiation
+ *                  is enabled, all possible negotiation speeds are advertised.
+ *
+ * @return Zero on success, negative on failure
+ */
+int cvmx_helper_board_link_set_phy(int phy_addr, int enable_autoneg,
+                                   cvmx_helper_link_info_t link_info);
+
+/**
+ * @INTERNAL
+ * This function is the board specific method of determining an
+ * ethernet ports link speed. Most Octeon boards have Marvell PHYs
+ * and are handled by the fall through case. This function must be
+ * updated for boards that don't have the normal Marvell PHYs.
+ *
+ * This function must be modifed for every new Octeon board.
+ * Internally it uses switch statements based on the cvmx_sysinfo
+ * data to determine board types and revisions. It relys on the
+ * fact that every Octeon board receives a unique board type
+ * enumeration from the bootloader.
+ *
+ * @param ipd_port IPD input port associated with the port we want to get link
+ *                 status for.
+ *
+ * @return The ports link status. If the link isn't fully resolved, this must
+ *         return zero.
+ */
+extern cvmx_helper_link_info_t __cvmx_helper_board_link_get(int ipd_port);
+
+/**
+ * @INTERNAL
+ * This function is called by cvmx_helper_interface_probe() after it
+ * determines the number of ports Octeon can support on a specific
+ * interface. This function is the per board location to override
+ * this value. It is called with the number of ports Octeon might
+ * support and should return the number of actual ports on the
+ * board.
+ *
+ * This function must be modifed for every new Octeon board.
+ * Internally it uses switch statements based on the cvmx_sysinfo
+ * data to determine board types and revisions. It relys on the
+ * fact that every Octeon board receives a unique board type
+ * enumeration from the bootloader.
+ *
+ * @param interface Interface to probe
+ * @param supported_ports
+ *                  Number of ports Octeon supports.
+ *
+ * @return Number of ports the actual board supports. Many times this will
+ *         simple be "support_ports".
+ */
+extern int __cvmx_helper_board_interface_probe(int interface, int supported_ports);
+
+/**
+ * @INTERNAL
+ * Enable packet input/output from the hardware. This function is
+ * called after by cvmx_helper_packet_hardware_enable() to
+ * perform board specific initialization. For most boards
+ * nothing is needed.
+ *
+ * @param interface Interface to enable
+ *
+ * @return Zero on success, negative on failure
+ */
+extern int __cvmx_helper_board_hardware_enable(int interface);
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif  /* __CVMX_HELPER_BOARD_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-check-defines.h b/arch/mips/cavium-octeon/executive/cvmx-helper-check-defines.h
new file mode 100644
index 0000000..9f4d63d
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-check-defines.h
@@ -0,0 +1,110 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Validate defines required by cvmx-helper. This header file
+ * validates a number of defines required for cvmx-helper to
+ * function properly. It either supplies a default or fails
+ * compile if a define is incorrect.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+#ifndef __CVMX_HELPER_CHECK_DEFINES_H__
+#define __CVMX_HELPER_CHECK_DEFINES_H__
+
+/* CVMX_HELPER_FIRST_MBUFF_SKIP is the number of bytes to reserve before
+    the beginning of the packet. Override in executive-config.h */
+#ifndef CVMX_HELPER_FIRST_MBUFF_SKIP
+#define CVMX_HELPER_FIRST_MBUFF_SKIP 184
+#warning WARNING: default CVMX_HELPER_FIRST_MBUFF_SKIP used.  Defaults deprecated, please set in executive-config.h
+#endif
+
+/* CVMX_HELPER_NOT_FIRST_MBUFF_SKIP is the number of bytes to reserve in each
+    chained packet element. Override in executive-config.h */
+#ifndef CVMX_HELPER_NOT_FIRST_MBUFF_SKIP
+#define CVMX_HELPER_NOT_FIRST_MBUFF_SKIP 0
+#warning WARNING: default CVMX_HELPER_NOT_FIRST_MBUFF_SKIP used.  Defaults deprecated, please set in executive-config.h
+#endif
+
+/* CVMX_HELPER_ENABLE_BACK_PRESSURE controls whether back pressure is enabled
+    for all input ports. Override in executive-config.h */
+#ifndef CVMX_HELPER_ENABLE_BACK_PRESSURE
+#define CVMX_HELPER_ENABLE_BACK_PRESSURE 1
+#warning WARNING: default CVMX_HELPER_ENABLE_BACK_PRESSURE used.  Defaults deprecated, please set in executive-config.h
+#endif
+
+/* CVMX_HELPER_ENABLE_IPD controls if the IPD is enabled in the helper
+    function. Once it is enabled the hardware starts accepting packets. You
+    might want to skip the IPD enable if configuration changes are need
+    from the default helper setup. Override in executive-config.h */
+#ifndef CVMX_HELPER_ENABLE_IPD
+#define CVMX_HELPER_ENABLE_IPD 1
+#warning WARNING: default CVMX_HELPER_ENABLE_IPD used.  Defaults deprecated, please set in executive-config.h
+#endif
+
+/* Set default (defaults are deprecated) input tag type */
+#ifndef  CVMX_HELPER_INPUT_TAG_TYPE
+#define  CVMX_HELPER_INPUT_TAG_TYPE CVMX_POW_TAG_TYPE_ORDERED
+#warning WARNING: default CVMX_HELPER_INPUT_TAG_TYPE used.  Defaults deprecated, please set in executive-config.h
+#endif
+
+#ifndef CVMX_HELPER_INPUT_PORT_SKIP_MODE
+#define CVMX_HELPER_INPUT_PORT_SKIP_MODE	CVMX_PIP_PORT_CFG_MODE_SKIPL2
+#warning WARNING: default CVMX_HELPER_INPUT_PORT_SKIP_MODE used.  Defaults deprecated, please set in executive-config.h
+#endif
+
+#if defined(CVMX_ENABLE_HELPER_FUNCTIONS) && !defined(CVMX_HELPER_INPUT_TAG_INPUT_PORT)
+#error CVMX_HELPER_INPUT_TAG_* values for determining tag hash inputs must be defined in executive-config.h
+#endif
+
+#endif
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-errata.c b/arch/mips/cavium-octeon/executive/cvmx-helper-errata.c
new file mode 100644
index 0000000..48ae59b
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-errata.c
@@ -0,0 +1,320 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Fixes and workaround for Octeon chip errata. This file
+ * contains functions called by cvmx-helper to workaround known
+ * chip errata. For the most part, code doesn't need to call
+ * these functions directly.
+ *
+ * <hr>$Revision: 34451 $<hr>
+ */
+#include "executive-config.h"
+#include "cvmx-config.h"
+
+#ifdef CVMX_ENABLE_PKO_FUNCTIONS
+
+#include "cvmx.h"
+#include "cvmx-fpa.h"
+#include "cvmx-pip.h"
+#include "cvmx-pko.h"
+#include "cvmx-ipd.h"
+#include "cvmx-asx.h"
+#include "cvmx-gmx.h"
+#include "cvmx-spi.h"
+#include "cvmx-pow.h"
+#include "cvmx-sysinfo.h"
+#include "cvmx-helper.h"
+
+
+/**
+ * @INTERNAL
+ * Function to adjust internal IPD pointer alignments
+ *
+ * @return 0 on success
+ *         !0 on failure
+ */
+int __cvmx_helper_errata_fix_ipd_ptr_alignment(void)
+{
+#define FIX_IPD_FIRST_BUFF_PAYLOAD_BYTES     (CVMX_FPA_PACKET_POOL_SIZE-8-CVMX_HELPER_FIRST_MBUFF_SKIP)
+#define FIX_IPD_NON_FIRST_BUFF_PAYLOAD_BYTES (CVMX_FPA_PACKET_POOL_SIZE-8-CVMX_HELPER_NOT_FIRST_MBUFF_SKIP)
+#define FIX_IPD_OUTPORT 0
+#define INTERFACE(port) (port >> 4) /* Ports 0-15 are interface 0, 16-31 are interface 1 */
+#define INDEX(port) (port & 0xf)
+    uint64_t *p64;
+    cvmx_pko_command_word0_t    pko_command;
+    cvmx_buf_ptr_t              g_buffer, pkt_buffer;
+    cvmx_wqe_t *work;
+    int size, num_segs = 0, wqe_pcnt, pkt_pcnt;
+    cvmx_gmxx_prtx_cfg_t gmx_cfg;
+    int retry_cnt;
+    int retry_loop_cnt;
+    int mtu;
+    int i;
+    cvmx_helper_link_info_t link_info;
+
+    /* Save values for restore at end */
+    uint64_t prtx_cfg = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(INDEX(FIX_IPD_OUTPORT), INTERFACE(FIX_IPD_OUTPORT)));
+    uint64_t tx_ptr_en = cvmx_read_csr(CVMX_ASXX_TX_PRT_EN(INTERFACE(FIX_IPD_OUTPORT)));
+    uint64_t rx_ptr_en = cvmx_read_csr(CVMX_ASXX_RX_PRT_EN(INTERFACE(FIX_IPD_OUTPORT)));
+    uint64_t rxx_jabber = cvmx_read_csr(CVMX_GMXX_RXX_JABBER(INDEX(FIX_IPD_OUTPORT), INTERFACE(FIX_IPD_OUTPORT)));
+    uint64_t frame_max = cvmx_read_csr(CVMX_GMXX_RXX_FRM_MAX(INDEX(FIX_IPD_OUTPORT), INTERFACE(FIX_IPD_OUTPORT)));
+
+    /* Configure port to gig FDX as required for loopback mode */
+    cvmx_helper_rgmii_internal_loopback(FIX_IPD_OUTPORT);
+
+    /* Disable reception on all ports so if traffic is present it will not interfere. */
+    cvmx_write_csr(CVMX_ASXX_RX_PRT_EN(INTERFACE(FIX_IPD_OUTPORT)), 0);
+
+    cvmx_wait(100000000ull);
+
+    for (retry_loop_cnt = 0;retry_loop_cnt < 10;retry_loop_cnt++)
+    {
+        retry_cnt = 100000;
+        wqe_pcnt = cvmx_read_csr(CVMX_IPD_PTR_COUNT);
+        pkt_pcnt = (wqe_pcnt >> 7) & 0x7f;
+        wqe_pcnt &= 0x7f;
+
+        num_segs = (2 + pkt_pcnt - wqe_pcnt) & 3;
+
+        if (num_segs == 0)
+            goto fix_ipd_exit;
+
+        num_segs += 1;
+
+        size = FIX_IPD_FIRST_BUFF_PAYLOAD_BYTES + ((num_segs-1)*FIX_IPD_NON_FIRST_BUFF_PAYLOAD_BYTES) -
+            (FIX_IPD_NON_FIRST_BUFF_PAYLOAD_BYTES / 2);
+
+        cvmx_write_csr(CVMX_ASXX_PRT_LOOP(INTERFACE(FIX_IPD_OUTPORT)), 1 << INDEX(FIX_IPD_OUTPORT));
+        CVMX_SYNC;
+
+        g_buffer.u64 = 0;
+        g_buffer.s.addr = cvmx_ptr_to_phys(cvmx_fpa_alloc(CVMX_FPA_WQE_POOL));
+        if (g_buffer.s.addr == 0) {
+            cvmx_dprintf("WARNING: FIX_IPD_PTR_ALIGNMENT buffer allocation failure.\n");
+            goto fix_ipd_exit;
+        }
+
+        g_buffer.s.pool = CVMX_FPA_WQE_POOL;
+        g_buffer.s.size = num_segs;
+
+        pkt_buffer.u64 = 0;
+        pkt_buffer.s.addr = cvmx_ptr_to_phys(cvmx_fpa_alloc(CVMX_FPA_PACKET_POOL));
+        if (pkt_buffer.s.addr == 0) {
+            cvmx_dprintf("WARNING: FIX_IPD_PTR_ALIGNMENT buffer allocation failure.\n");
+            goto fix_ipd_exit;
+        }
+        pkt_buffer.s.i = 1;
+        pkt_buffer.s.pool = CVMX_FPA_PACKET_POOL;
+        pkt_buffer.s.size = FIX_IPD_FIRST_BUFF_PAYLOAD_BYTES;
+
+        p64 = (uint64_t*) cvmx_phys_to_ptr(pkt_buffer.s.addr);
+        p64[0] = 0xffffffffffff0000ull;
+        p64[1] = 0x08004510ull;
+        p64[2] = ((uint64_t)(size-14) << 48) | 0x5ae740004000ull;
+        p64[3] = 0x3a5fc0a81073c0a8ull;
+
+        for (i=0;i<num_segs;i++)
+        {
+            if (i>0)
+                pkt_buffer.s.size = FIX_IPD_NON_FIRST_BUFF_PAYLOAD_BYTES;
+
+            if (i==(num_segs-1))
+                pkt_buffer.s.i = 0;
+
+            *(uint64_t*)cvmx_phys_to_ptr(g_buffer.s.addr + 8*i) = pkt_buffer.u64;
+        }
+
+        /* Build the PKO command */
+        pko_command.u64 = 0;
+        pko_command.s.segs = num_segs;
+        pko_command.s.total_bytes = size;
+        pko_command.s.dontfree = 0;
+        pko_command.s.gather = 1;
+
+        gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(INDEX(FIX_IPD_OUTPORT), INTERFACE(FIX_IPD_OUTPORT)));
+        gmx_cfg.s.en = 1;
+        cvmx_write_csr(CVMX_GMXX_PRTX_CFG(INDEX(FIX_IPD_OUTPORT), INTERFACE(FIX_IPD_OUTPORT)), gmx_cfg.u64);
+        cvmx_write_csr(CVMX_ASXX_TX_PRT_EN(INTERFACE(FIX_IPD_OUTPORT)), 1 << INDEX(FIX_IPD_OUTPORT));
+        cvmx_write_csr(CVMX_ASXX_RX_PRT_EN(INTERFACE(FIX_IPD_OUTPORT)), 1 << INDEX(FIX_IPD_OUTPORT));
+
+        mtu = cvmx_read_csr(CVMX_GMXX_RXX_JABBER(INDEX(FIX_IPD_OUTPORT), INTERFACE(FIX_IPD_OUTPORT)));
+        cvmx_write_csr(CVMX_GMXX_RXX_JABBER(INDEX(FIX_IPD_OUTPORT), INTERFACE(FIX_IPD_OUTPORT)), 65392-14-4);
+        cvmx_write_csr(CVMX_GMXX_RXX_FRM_MAX(INDEX(FIX_IPD_OUTPORT), INTERFACE(FIX_IPD_OUTPORT)), 65392-14-4);
+
+#if CVMX_PKO_USE_FAU_FOR_OUTPUT_QUEUES
+        cvmx_pko_send_packet_prepare(FIX_IPD_OUTPORT, cvmx_pko_get_base_queue(FIX_IPD_OUTPORT), CVMX_PKO_LOCK_NONE);
+        cvmx_pko_send_packet_finish(FIX_IPD_OUTPORT, cvmx_pko_get_base_queue(FIX_IPD_OUTPORT), pko_command, g_buffer, CVMX_PKO_LOCK_NONE);
+#else
+        cvmx_pko_send_packet_prepare(FIX_IPD_OUTPORT, cvmx_pko_get_base_queue(FIX_IPD_OUTPORT), CVMX_PKO_LOCK_CMD_QUEUE);
+        cvmx_pko_send_packet_finish(FIX_IPD_OUTPORT, cvmx_pko_get_base_queue(FIX_IPD_OUTPORT), pko_command, g_buffer, CVMX_PKO_LOCK_CMD_QUEUE);
+#endif
+        CVMX_SYNC;
+
+        do {
+            work = cvmx_pow_work_request_sync(CVMX_POW_WAIT);
+            retry_cnt--;
+        } while ((work == NULL) && (retry_cnt > 0));
+
+        if (!retry_cnt)
+            cvmx_dprintf("WARNING: FIX_IPD_PTR_ALIGNMENT get_work() timeout occured.\n");
+
+
+        /* Free packet */
+        if (work)
+            cvmx_helper_free_packet_data(work);
+    }
+
+fix_ipd_exit:
+
+    /* Return CSR configs to saved values */
+    cvmx_write_csr(CVMX_GMXX_PRTX_CFG(INDEX(FIX_IPD_OUTPORT), INTERFACE(FIX_IPD_OUTPORT)), prtx_cfg);
+    cvmx_write_csr(CVMX_ASXX_TX_PRT_EN(INTERFACE(FIX_IPD_OUTPORT)), tx_ptr_en);
+    cvmx_write_csr(CVMX_ASXX_RX_PRT_EN(INTERFACE(FIX_IPD_OUTPORT)), rx_ptr_en);
+    cvmx_write_csr(CVMX_GMXX_RXX_JABBER(INDEX(FIX_IPD_OUTPORT), INTERFACE(FIX_IPD_OUTPORT)), rxx_jabber);
+    cvmx_write_csr(CVMX_GMXX_RXX_FRM_MAX(INDEX(FIX_IPD_OUTPORT), INTERFACE(FIX_IPD_OUTPORT)), frame_max);
+    cvmx_write_csr(CVMX_ASXX_PRT_LOOP(INTERFACE(FIX_IPD_OUTPORT)), 0);
+    link_info.u64 = 0;  /* Set link to down so autonegotiation will set it up again */
+    cvmx_helper_link_set(FIX_IPD_OUTPORT, link_info);
+
+    /* Bring the link back up as autonegotiation is not done in user applications. */
+    cvmx_helper_link_autoconf(FIX_IPD_OUTPORT);
+
+    CVMX_SYNC;
+    if (num_segs)
+        cvmx_dprintf("WARNING: FIX_IPD_PTR_ALIGNMENT failed.\n");
+
+    return(!!num_segs);
+
+}
+
+
+/**
+ * @INTERNAL
+ * Workaround ASX setup errata with CN38XX pass1
+ *
+ * @param interface Interface to setup
+ * @param port      Port to setup (0..3)
+ * @param cpu_clock_hz
+ *                  Chip frequency in Hertz
+ *
+ * @return Zero on success, negative on failure
+ */
+int __cvmx_helper_errata_asx_pass1(int interface, int port, int cpu_clock_hz)
+{
+    /* Set hi water mark as per errata GMX-4 */
+    if (cpu_clock_hz >= 325000000 && cpu_clock_hz < 375000000)
+        cvmx_write_csr(CVMX_ASXX_TX_HI_WATERX(port, interface), 12);
+    else if (cpu_clock_hz >= 375000000 && cpu_clock_hz < 437000000)
+        cvmx_write_csr(CVMX_ASXX_TX_HI_WATERX(port, interface), 11);
+    else if (cpu_clock_hz >= 437000000 && cpu_clock_hz < 550000000)
+        cvmx_write_csr(CVMX_ASXX_TX_HI_WATERX(port, interface), 10);
+    else if (cpu_clock_hz >= 550000000 && cpu_clock_hz < 687000000)
+        cvmx_write_csr(CVMX_ASXX_TX_HI_WATERX(port, interface), 9);
+    else
+        cvmx_dprintf("Illegal clock frequency (%d). CVMX_ASXX_TX_HI_WATERX not set\n", cpu_clock_hz);
+    return 0;
+}
+
+
+/**
+ * This function needs to be called on all Octeon chips with
+ * errata PKI-100.
+ *
+ * The Size field is 8 too large in WQE and next pointers
+ *
+ *  The Size field generated by IPD is 8 larger than it should
+ *  be. The Size field is <55:40> of both:
+ *      - WORD3 in the work queue entry, and
+ *      - the next buffer pointer (which precedes the packet data
+ *        in each buffer).
+ *
+ * @param work   Work queue entry to fix
+ * @return Zero on success. Negative on failure
+ */
+int cvmx_helper_fix_ipd_packet_chain(cvmx_wqe_t *work)
+{
+    uint64_t number_buffers = work->word2.s.bufs;
+
+    /* We only need to do this if the work has buffers */
+    if (number_buffers)
+    {
+        cvmx_buf_ptr_t buffer_ptr = work->packet_ptr;
+        /* Check for errata PKI-100 */
+        if ( (buffer_ptr.s.pool == 0) && (((uint64_t)buffer_ptr.s.size +
+                 ((uint64_t)buffer_ptr.s.back << 7) + ((uint64_t)buffer_ptr.s.addr & 0x7F))
+                 != (CVMX_FPA_PACKET_POOL_SIZE+8))) {
+            /* fix is not needed */
+            return 0;
+        }
+        /* Decrement the work packet pointer */
+        buffer_ptr.s.size -= 8;
+        work->packet_ptr = buffer_ptr;
+
+        /* Now loop through decrementing the size for each additional buffer */
+        while (--number_buffers)
+        {
+            /* Chain pointers are 8 bytes before the data */
+            cvmx_buf_ptr_t *ptr = (cvmx_buf_ptr_t*)cvmx_phys_to_ptr(buffer_ptr.s.addr - 8);
+            buffer_ptr = *ptr;
+            buffer_ptr.s.size -= 8;
+            *ptr = buffer_ptr;
+        }
+    }
+    /* Make sure that these write go out before other operations such as FPA frees */
+    CVMX_SYNCWS;
+    return 0;
+}
+
+#endif /* CVMX_ENABLE_PKO_FUNCTIONS */
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-errata.h b/arch/mips/cavium-octeon/executive/cvmx-helper-errata.h
new file mode 100644
index 0000000..659fd24
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-errata.h
@@ -0,0 +1,104 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Fixes and workaround for Octeon chip errata. This file
+ * contains functions called by cvmx-helper to workaround known
+ * chip errata. For the most part, code doesn't need to call
+ * these functions directly.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+#ifndef __CVMX_HELPER_ERRATA_H__
+#define __CVMX_HELPER_ERRATA_H__
+
+/**
+ * @INTERNAL
+ * Function to adjust internal IPD pointer alignments
+ *
+ * @return 0 on success
+ *         !0 on failure
+ */
+extern int __cvmx_helper_errata_fix_ipd_ptr_alignment(void);
+
+/**
+ * @INTERNAL
+ * Workaround ASX setup errata with CN38XX pass1
+ *
+ * @param interface Interface to setup
+ * @param port      Port to setup (0..3)
+ * @param cpu_clock_hz
+ *                  Chip frequency in Hertz
+ *
+ * @return Zero on success, negative on failure
+ */
+extern int __cvmx_helper_errata_asx_pass1(int interface, int port, int cpu_clock_hz);
+
+/**
+ * This function needs to be called on all Octeon chips with
+ * errata PKI-100.
+ *
+ * The Size field is 8 too large in WQE and next pointers
+ *
+ *  The Size field generated by IPD is 8 larger than it should
+ *  be. The Size field is <55:40> of both:
+ *      - WORD3 in the work queue entry, and
+ *      - the next buffer pointer (which precedes the packet data
+ *        in each buffer).
+ *
+ * @param work   Work queue entry to fix
+ * @return Zero on success. Negative on failure
+ */
+extern int cvmx_helper_fix_ipd_packet_chain(cvmx_wqe_t *work);
+
+#endif
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-fpa.c b/arch/mips/cavium-octeon/executive/cvmx-helper-fpa.c
new file mode 100644
index 0000000..eceb210
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-fpa.c
@@ -0,0 +1,254 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Helper functions for FPA setup.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+#include "executive-config.h"
+#include "cvmx-config.h"
+#include "cvmx.h"
+#include "cvmx-bootmem.h"
+#include "cvmx-fpa.h"
+#include "cvmx-helper-fpa.h"
+
+/**
+ * @INTERNAL
+ * Allocate memory for and initialize a single FPA pool.
+ *
+ * @param pool    Pool to initialize
+ * @param buffer_size  Size of buffers to allocate in bytes
+ * @param buffers Number of buffers to put in the pool. Zero is allowed
+ * @param name    String name of the pool for debugging purposes
+ * @return Zero on success, non-zero on failure
+ */
+static int __cvmx_helper_initialize_fpa_pool(int pool, uint64_t buffer_size,
+                                           uint64_t buffers, const char *name)
+{
+    uint64_t current_num;
+    void *memory;
+    uint64_t align = CVMX_CACHE_LINE_SIZE;
+
+    /* Align the allocation so that power of 2 size buffers are naturally aligned */
+    while (align < buffer_size)
+        align = align << 1;
+
+    if (buffers == 0)
+        return 0;
+
+    current_num = cvmx_read_csr(CVMX_FPA_QUEX_AVAILABLE(pool));
+    if (current_num)
+    {
+        cvmx_dprintf("Fpa pool %d(%s) already has %llu buffers. Skipping setup.\n",
+                     pool, name, (unsigned long long)current_num);
+        return 0;
+    }
+
+    memory = cvmx_bootmem_alloc(buffer_size * buffers, align);
+    if (memory == NULL)
+    {
+        cvmx_dprintf("Out of memory initializing fpa pool %d(%s).\n", pool, name);
+        return -1;
+    }
+    cvmx_fpa_setup_pool(pool, name, memory, buffer_size, buffers);
+    return 0;
+}
+
+
+/**
+ * @INTERNAL
+ * Allocate memory and initialize the FPA pools using memory
+ * from cvmx-bootmem. Specifying zero for the number of
+ * buffers will cause that FPA pool to not be setup. This is
+ * useful if you aren't using some of the hardware and want
+ * to save memory. Use cvmx_helper_initialize_fpa instead of
+ * this function directly.
+ *
+ * @param pip_pool Should always be CVMX_FPA_PACKET_POOL
+ * @param pip_size Should always be CVMX_FPA_PACKET_POOL_SIZE
+ * @param pip_buffers
+ *                 Number of packet buffers.
+ * @param wqe_pool Should always be CVMX_FPA_WQE_POOL
+ * @param wqe_size Should always be CVMX_FPA_WQE_POOL_SIZE
+ * @param wqe_entries
+ *                 Number of work queue entries
+ * @param pko_pool Should always be CVMX_FPA_OUTPUT_BUFFER_POOL
+ * @param pko_size Should always be CVMX_FPA_OUTPUT_BUFFER_POOL_SIZE
+ * @param pko_buffers
+ *                 PKO Command buffers. You should at minimum have two per
+ *                 each PKO queue.
+ * @param tim_pool Should always be CVMX_FPA_TIMER_POOL
+ * @param tim_size Should always be CVMX_FPA_TIMER_POOL_SIZE
+ * @param tim_buffers
+ *                 TIM ring buffer command queues. At least two per timer bucket
+ *                 is recommened.
+ * @param dfa_pool Should always be CVMX_FPA_DFA_POOL
+ * @param dfa_size Should always be CVMX_FPA_DFA_POOL_SIZE
+ * @param dfa_buffers
+ *                 DFA command buffer. A relatively small (32 for example)
+ *                 number should work.
+ * @return Zero on success, non-zero if out of memory
+ */
+static int __cvmx_helper_initialize_fpa(int pip_pool, int pip_size, int pip_buffers,
+                                        int wqe_pool, int wqe_size, int wqe_entries,
+                                        int pko_pool, int pko_size, int pko_buffers,
+                                        int tim_pool, int tim_size, int tim_buffers,
+                                        int dfa_pool, int dfa_size, int dfa_buffers)
+{
+    int status;
+
+    cvmx_fpa_enable();
+
+    if ((pip_buffers > 0) && (pip_buffers <= 64))
+        cvmx_dprintf("Warning: %d packet buffers may not be enough for hardware"
+                     " prefetch. 65 or more is recommended.\n", pip_buffers);
+
+    if (pip_pool >= 0)
+    {
+        status = __cvmx_helper_initialize_fpa_pool(pip_pool, pip_size, pip_buffers,
+                                                 "Packet Buffers");
+        if (status)
+            return status;
+    }
+
+    if (wqe_pool >= 0)
+    {
+        status = __cvmx_helper_initialize_fpa_pool(wqe_pool, wqe_size, wqe_entries,
+                                                 "Work Queue Entries");
+        if (status)
+            return status;
+    }
+
+    if (pko_pool >= 0)
+    {
+        status = __cvmx_helper_initialize_fpa_pool(pko_pool, pko_size, pko_buffers,
+                                                 "PKO Command Buffers");
+        if (status)
+            return status;
+    }
+
+    if (tim_pool >= 0)
+    {
+        status = __cvmx_helper_initialize_fpa_pool(tim_pool, tim_size, tim_buffers,
+                                                 "TIM Command Buffers");
+        if (status)
+            return status;
+    }
+
+    if (dfa_pool >= 0)
+    {
+        status = __cvmx_helper_initialize_fpa_pool(dfa_pool, dfa_size, dfa_buffers,
+                                                 "DFA Command Buffers");
+        if (status)
+            return status;
+    }
+
+    return 0;
+}
+
+
+/**
+ * Allocate memory and initialize the FPA pools using memory
+ * from cvmx-bootmem. Sizes of each element in the pools is
+ * controlled by the cvmx-config.h header file. Specifying
+ * zero for any parameter will cause that FPA pool to not be
+ * setup. This is useful if you aren't using some of the
+ * hardware and want to save memory.
+ *
+ * @param packet_buffers
+ *               Number of packet buffers to allocate
+ * @param work_queue_entries
+ *               Number of work queue entries
+ * @param pko_buffers
+ *               PKO Command buffers. You should at minimum have two per
+ *               each PKO queue.
+ * @param tim_buffers
+ *               TIM ring buffer command queues. At least two per timer bucket
+ *               is recommened.
+ * @param dfa_buffers
+ *               DFA command buffer. A relatively small (32 for example)
+ *               number should work.
+ * @return Zero on success, non-zero if out of memory
+ */
+int cvmx_helper_initialize_fpa(int packet_buffers, int work_queue_entries,
+                               int pko_buffers, int tim_buffers, int dfa_buffers)
+{
+#ifndef CVMX_FPA_PACKET_POOL
+#define CVMX_FPA_PACKET_POOL -1
+#define CVMX_FPA_PACKET_POOL_SIZE 0
+#endif
+#ifndef CVMX_FPA_WQE_POOL
+#define CVMX_FPA_WQE_POOL -1
+#define CVMX_FPA_WQE_POOL_SIZE 0
+#endif
+#ifndef CVMX_FPA_OUTPUT_BUFFER_POOL
+#define CVMX_FPA_OUTPUT_BUFFER_POOL -1
+#define CVMX_FPA_OUTPUT_BUFFER_POOL_SIZE 0
+#endif
+#ifndef CVMX_FPA_TIMER_POOL
+#define CVMX_FPA_TIMER_POOL -1
+#define CVMX_FPA_TIMER_POOL_SIZE 0
+#endif
+#ifndef CVMX_FPA_DFA_POOL
+#define CVMX_FPA_DFA_POOL -1
+#define CVMX_FPA_DFA_POOL_SIZE 0
+#endif
+    return __cvmx_helper_initialize_fpa(
+        CVMX_FPA_PACKET_POOL,        CVMX_FPA_PACKET_POOL_SIZE,          packet_buffers,
+        CVMX_FPA_WQE_POOL,           CVMX_FPA_WQE_POOL_SIZE,             work_queue_entries,
+        CVMX_FPA_OUTPUT_BUFFER_POOL, CVMX_FPA_OUTPUT_BUFFER_POOL_SIZE,   pko_buffers,
+        CVMX_FPA_TIMER_POOL,         CVMX_FPA_TIMER_POOL_SIZE,           tim_buffers,
+        CVMX_FPA_DFA_POOL,           CVMX_FPA_DFA_POOL_SIZE,             dfa_buffers);
+}
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-fpa.h b/arch/mips/cavium-octeon/executive/cvmx-helper-fpa.h
new file mode 100644
index 0000000..7aa9829
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-fpa.h
@@ -0,0 +1,89 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Helper functions for FPA setup.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+#ifndef __CVMX_HELPER_H_FPA__
+#define __CVMX_HELPER_H_FPA__
+
+/**
+ * Allocate memory and initialize the FPA pools using memory
+ * from cvmx-bootmem. Sizes of each element in the pools is
+ * controlled by the cvmx-config.h header file. Specifying
+ * zero for any parameter will cause that FPA pool to not be
+ * setup. This is useful if you aren't using some of the
+ * hardware and want to save memory.
+ *
+ * @param packet_buffers
+ *               Number of packet buffers to allocate
+ * @param work_queue_entries
+ *               Number of work queue entries
+ * @param pko_buffers
+ *               PKO Command buffers. You should at minimum have two per
+ *               each PKO queue.
+ * @param tim_buffers
+ *               TIM ring buffer command queues. At least two per timer bucket
+ *               is recommened.
+ * @param dfa_buffers
+ *               DFA command buffer. A relatively small (32 for example)
+ *               number should work.
+ * @return Zero on success, non-zero if out of memory
+ */
+extern int cvmx_helper_initialize_fpa(int packet_buffers, int work_queue_entries,
+                                      int pko_buffers, int tim_buffers,
+                                      int dfa_buffers);
+
+#endif  /* __CVMX_HELPER_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-loop.c b/arch/mips/cavium-octeon/executive/cvmx-helper-loop.c
new file mode 100644
index 0000000..34688ca
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-loop.c
@@ -0,0 +1,119 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Functions for LOOP initialization, configuration,
+ * and monitoring.
+ *
+ * <hr>$Revision: 35609 $<hr>
+ */
+#include "executive-config.h"
+#include "cvmx-config.h"
+#ifdef CVMX_ENABLE_PKO_FUNCTIONS
+
+#include "cvmx.h"
+#include "cvmx-helper.h"
+
+
+/**
+ * @INTERNAL
+ * Probe a LOOP interface and determine the number of ports
+ * connected to it. The LOOP interface should still be down
+ * after this call.
+ *
+ * @param interface Interface to probe
+ *
+ * @return Number of ports on the interface. Zero to disable.
+ */
+int __cvmx_helper_loop_probe(int interface)
+{
+    return 4;
+}
+
+
+/**
+ * @INTERNAL
+ * Bringup and enable a LOOP interface. After this call packet
+ * I/O should be fully functional. This is called with IPD
+ * enabled but PKO disabled.
+ *
+ * @param interface Interface to bring up
+ *
+ * @return Zero on success, negative on failure
+ */
+int __cvmx_helper_loop_enable(int interface)
+{
+    /* Do nothing. */
+    return 0;
+}
+
+/**
+ * @INTERNAL
+ * Disable FCS stripping for LOOP interface.
+ * @param interface Interface number to disable FCS stripping on
+ *
+ * @return Zero on success, negative on failure
+ */
+int __cvmx_helper_loop_disable_fcs_stripping(int interface)
+{
+    cvmx_ipd_sub_port_fcs_t ipd_sub_port_fcs;
+
+    /* Disable FCS stripping for loopback ports */
+    ipd_sub_port_fcs.u64 = cvmx_read_csr(CVMX_IPD_SUB_PORT_FCS);
+    ipd_sub_port_fcs.s.port_bit2 = 0;
+    cvmx_write_csr(CVMX_IPD_SUB_PORT_FCS, ipd_sub_port_fcs.u64);
+    return 0;
+}
+
+#endif /* CVMX_ENABLE_PKO_FUNCTIONS */
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-loop.h b/arch/mips/cavium-octeon/executive/cvmx-helper-loop.h
new file mode 100644
index 0000000..39f7bfc
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-loop.h
@@ -0,0 +1,98 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Functions for LOOP initialization, configuration,
+ * and monitoring.
+ *
+ * <hr>$Revision: 35352 $<hr>
+ */
+#ifndef __CVMX_HELPER_LOOP_H__
+#define __CVMX_HELPER_LOOP_H__
+
+/**
+ * @INTERNAL
+ * Probe a LOOP interface and determine the number of ports
+ * connected to it. The LOOP interface should still be down after
+ * this call.
+ *
+ * @param interface Interface to probe
+ *
+ * @return Number of ports on the interface. Zero to disable.
+ */
+extern int __cvmx_helper_loop_probe(int interface);
+
+/**
+ * @INTERNAL
+ * Bringup and enable a LOOP interface. After this call packet
+ * I/O should be fully functional. This is called with IPD
+ * enabled but PKO disabled.
+ *
+ * @param interface Interface to bring up
+ *
+ * @return Zero on success, negative on failure
+ */
+extern int __cvmx_helper_loop_enable(int interface);
+
+/**
+ * @INTERNAL
+ * Disable FCS stripping for LOOP interface. 
+ *
+ * @param interface Interface to bring up
+ *
+ * @return Zero on success, negative on failure
+ */
+extern int __cvmx_helper_loop_disable_fcs_stripping(int interface);
+
+#endif
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-npi.c b/arch/mips/cavium-octeon/executive/cvmx-helper-npi.c
new file mode 100644
index 0000000..a23db78
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-npi.c
@@ -0,0 +1,111 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Functions for NPI initialization, configuration,
+ * and monitoring.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+#include "executive-config.h"
+#include "cvmx-config.h"
+#ifdef CVMX_ENABLE_PKO_FUNCTIONS
+
+#include "cvmx.h"
+#include "cvmx-helper.h"
+
+
+/**
+ * @INTERNAL
+ * Probe a NPI interface and determine the number of ports
+ * connected to it. The NPI interface should still be down
+ * after this call.
+ *
+ * @param interface Interface to probe
+ *
+ * @return Number of ports on the interface. Zero to disable.
+ */
+int __cvmx_helper_npi_probe(int interface)
+{
+#if 0 // Currently the NPI packet interfaces aren't setup by the helper
+    if (OCTEON_IS_MODEL(OCTEON_CN38XX) || OCTEON_IS_MODEL(OCTEON_CN58XX))
+        return 4;
+    else if (OCTEON_IS_MODEL(OCTEON_CN31XX))
+        return 2;
+    else if (OCTEON_IS_MODEL(OCTEON_CN30XX))
+        return 1;
+    else if (OCTEON_IS_MODEL(OCTEON_CN56XX))
+        return 4;
+#endif
+    return 0;
+}
+
+
+/**
+ * @INTERNAL
+ * Bringup and enable a NPI interface. After this call packet
+ * I/O should be fully functional. This is called with IPD
+ * enabled but PKO disabled.
+ *
+ * @param interface Interface to bring up
+ *
+ * @return Zero on success, negative on failure
+ */
+int __cvmx_helper_npi_enable(int interface)
+{
+    /* Not implemented */
+    return -1;
+}
+
+#endif /* CVMX_ENABLE_PKO_FUNCTIONS */
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-npi.h b/arch/mips/cavium-octeon/executive/cvmx-helper-npi.h
new file mode 100644
index 0000000..1906451
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-npi.h
@@ -0,0 +1,88 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Functions for NPI initialization, configuration,
+ * and monitoring.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+#ifndef __CVMX_HELPER_NPI_H__
+#define __CVMX_HELPER_NPI_H__
+
+/**
+ * @INTERNAL
+ * Probe a NPI interface and determine the number of ports
+ * connected to it. The NPI interface should still be down after
+ * this call.
+ *
+ * @param interface Interface to probe
+ *
+ * @return Number of ports on the interface. Zero to disable.
+ */
+extern int __cvmx_helper_npi_probe(int interface);
+
+/**
+ * @INTERNAL
+ * Bringup and enable a NPI interface. After this call packet
+ * I/O should be fully functional. This is called with IPD
+ * enabled but PKO disabled.
+ *
+ * @param interface Interface to bring up
+ *
+ * @return Zero on success, negative on failure
+ */
+extern int __cvmx_helper_npi_enable(int interface);
+
+#endif
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-rgmii.c b/arch/mips/cavium-octeon/executive/cvmx-helper-rgmii.c
new file mode 100644
index 0000000..d147f0a
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-rgmii.c
@@ -0,0 +1,429 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Functions for RGMII/GMII/MII initialization, configuration,
+ * and monitoring.
+ *
+ * <hr>$Revision: 35609 $<hr>
+ */
+#include "executive-config.h"
+#include "cvmx-config.h"
+#ifdef CVMX_ENABLE_PKO_FUNCTIONS
+
+#include "cvmx.h"
+#include "cvmx-sysinfo.h"
+#include "cvmx-mdio.h"
+#include "cvmx-pko.h"
+#include "cvmx-helper.h"
+#include "cvmx-helper-board.h"
+
+/**
+ * @INTERNAL
+ * Probe RGMII ports and determine the number present
+ *
+ * @param interface Interface to probe
+ *
+ * @return Number of RGMII/GMII/MII ports (0-4).
+ */
+int __cvmx_helper_rgmii_probe(int interface)
+{
+    int num_ports = 0;
+    cvmx_gmxx_inf_mode_t mode;
+    mode.u64 = cvmx_read_csr(CVMX_GMXX_INF_MODE(interface));
+
+    if (mode.s.type)
+    {
+        if (OCTEON_IS_MODEL(OCTEON_CN38XX) || OCTEON_IS_MODEL(OCTEON_CN58XX))
+        {
+            cvmx_dprintf("ERROR: RGMII initialize called in SPI interface\n");
+        }
+        else if (OCTEON_IS_MODEL(OCTEON_CN31XX) || OCTEON_IS_MODEL(OCTEON_CN30XX) || OCTEON_IS_MODEL(OCTEON_CN50XX))
+        {
+            /* On these chips "type" says we're in GMII/MII mode. This
+                limits us to 2 ports */
+            num_ports = 2;
+        }
+        else
+        {
+            cvmx_dprintf("ERROR: Unsupported Octeon model in %s\n", __FUNCTION__);
+        }
+    }
+    else
+    {
+        if (OCTEON_IS_MODEL(OCTEON_CN38XX) || OCTEON_IS_MODEL(OCTEON_CN58XX))
+        {
+            num_ports = 4;
+        }
+        else if (OCTEON_IS_MODEL(OCTEON_CN31XX) || OCTEON_IS_MODEL(OCTEON_CN30XX) || OCTEON_IS_MODEL(OCTEON_CN50XX))
+        {
+            num_ports = 3;
+        }
+        else
+        {
+            cvmx_dprintf("ERROR: Unsupported Octeon model in %s\n", __FUNCTION__);
+        }
+    }
+    return num_ports;
+}
+
+
+/**
+ * Put an RGMII interface in loopback mode. Internal packets sent
+ * out will be received back again on the same port. Externally
+ * received packets will echo back out.
+ *
+ * @param port   IPD port number to loop.
+ */
+void cvmx_helper_rgmii_internal_loopback(int port)
+{
+    int interface = (port >> 4) & 1;
+    int index = port & 0xf;
+    uint64_t tmp;
+
+    cvmx_gmxx_prtx_cfg_t gmx_cfg;
+    gmx_cfg.u64 = 0;
+    gmx_cfg.s.duplex = 1;
+    gmx_cfg.s.slottime = 1;
+    gmx_cfg.s.speed = 1;
+    cvmx_write_csr(CVMX_GMXX_TXX_CLK(index, interface), 1);
+    cvmx_write_csr(CVMX_GMXX_TXX_SLOT(index, interface), 0x200);
+    cvmx_write_csr(CVMX_GMXX_TXX_BURST(index, interface), 0x2000);
+    cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmx_cfg.u64);
+    tmp = cvmx_read_csr(CVMX_ASXX_PRT_LOOP(interface));
+    cvmx_write_csr(CVMX_ASXX_PRT_LOOP(interface), (1 << index) | tmp);
+    tmp = cvmx_read_csr(CVMX_ASXX_TX_PRT_EN(interface));
+    cvmx_write_csr(CVMX_ASXX_TX_PRT_EN(interface), (1 << index) | tmp);
+    tmp = cvmx_read_csr(CVMX_ASXX_RX_PRT_EN(interface));
+    cvmx_write_csr(CVMX_ASXX_RX_PRT_EN(interface), (1 << index) | tmp);
+    gmx_cfg.s.en = 1;
+    cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmx_cfg.u64);
+}
+
+
+/**
+ * @INTERNAL
+ * Configure all of the ASX, GMX, and PKO regsiters required
+ * to get RGMII to function on the supplied interface.
+ *
+ * @param interface PKO Interface to configure (0 or 1)
+ *
+ * @return Zero on success
+ */
+int __cvmx_helper_rgmii_enable(int interface)
+{
+    int num_ports = cvmx_helper_ports_on_interface(interface);
+    int port;
+    cvmx_sysinfo_t *sys_info_ptr = cvmx_sysinfo_get();
+    cvmx_gmxx_inf_mode_t mode;
+    cvmx_asxx_tx_prt_en_t asx_tx;
+    cvmx_asxx_rx_prt_en_t asx_rx;
+
+    mode.u64 = cvmx_read_csr(CVMX_GMXX_INF_MODE(interface));
+
+    if (mode.s.en == 0)
+        return -1;
+    if ((OCTEON_IS_MODEL(OCTEON_CN38XX) || OCTEON_IS_MODEL(OCTEON_CN58XX)) && mode.s.type == 1)   /* Ignore SPI interfaces */
+        return -1;
+
+    /* Configure the ASX registers needed to use the RGMII ports */
+    asx_tx.u64 = 0;
+    asx_tx.s.prt_en = cvmx_build_mask(num_ports);
+    cvmx_write_csr(CVMX_ASXX_TX_PRT_EN(interface), asx_tx.u64);
+
+    asx_rx.u64 = 0;
+    asx_rx.s.prt_en = cvmx_build_mask(num_ports);
+    cvmx_write_csr(CVMX_ASXX_RX_PRT_EN(interface), asx_rx.u64);
+
+    /* Configure the GMX registers needed to use the RGMII ports */
+    for (port=0; port<num_ports; port++)
+    {
+        cvmx_write_csr(CVMX_GMXX_TXX_THRESH(port, interface), 32);
+
+        if (cvmx_octeon_is_pass1())
+            __cvmx_helper_errata_asx_pass1(interface, port, sys_info_ptr->cpu_clock_hz);
+        else
+        {
+            /* Configure more flexible RGMII preamble checking. Pass 1 doesn't
+                support this feature. */
+            cvmx_gmxx_rxx_frm_ctl_t frm_ctl;
+            frm_ctl.u64 = cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(port, interface));
+            frm_ctl.s.pre_free = 1;  /* New field, so must be compile time */
+            cvmx_write_csr(CVMX_GMXX_RXX_FRM_CTL(port, interface), frm_ctl.u64);
+        }
+
+        /* Each pause frame transmitted will ask for about 10M bit times
+            before resume.  If buffer space comes available before that time
+            has expired, an XON pause frame (0 time) will be transmitted to
+            restart the flow. */
+        cvmx_write_csr(CVMX_GMXX_TXX_PAUSE_PKT_TIME(port, interface), 20000);
+        cvmx_write_csr(CVMX_GMXX_TXX_PAUSE_PKT_INTERVAL(port, interface), 19000);
+
+        if (OCTEON_IS_MODEL(OCTEON_CN50XX))
+        {
+            cvmx_write_csr(CVMX_ASXX_TX_CLK_SETX(port, interface), 16);
+            cvmx_write_csr(CVMX_ASXX_RX_CLK_SETX(port, interface), 16);
+        }
+        else
+        {
+            cvmx_write_csr(CVMX_ASXX_TX_CLK_SETX(port, interface), 24);
+            cvmx_write_csr(CVMX_ASXX_RX_CLK_SETX(port, interface), 24);
+        }
+    }
+
+    __cvmx_helper_setup_gmx(interface, num_ports);
+
+    /* enable the ports now */
+    for (port=0; port<num_ports; port++)
+    {
+        cvmx_gmxx_prtx_cfg_t gmx_cfg;
+        cvmx_helper_link_autoconf(cvmx_helper_get_ipd_port(interface, port));
+        gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(port, interface));
+        gmx_cfg.s.en = 1;
+        cvmx_write_csr(CVMX_GMXX_PRTX_CFG(port, interface), gmx_cfg.u64);
+    }
+
+    return 0;
+}
+
+
+/**
+ * @INTERNAL
+ * Return the link state of an IPD/PKO port as returned by
+ * auto negotiation. The result of this function may not match
+ * Octeon's link config if auto negotiation has changed since
+ * the last call to cvmx_helper_link_set().
+ *
+ * @param ipd_port IPD/PKO port to query
+ *
+ * @return Link state
+ */
+cvmx_helper_link_info_t __cvmx_helper_rgmii_link_get(int ipd_port)
+{
+    return __cvmx_helper_board_link_get(ipd_port);
+}
+
+
+/**
+ * @INTERNAL
+ * Configure an IPD/PKO port for the specified link state. This
+ * function does not influence auto negotiation at the PHY level.
+ * The passed link state must always match the link state returned
+ * by cvmx_helper_link_get(). It is normally best to use
+ * cvmx_helper_link_autoconf() instead.
+ *
+ * @param ipd_port  IPD/PKO port to configure
+ * @param link_info The new link state
+ *
+ * @return Zero on success, negative on failure
+ */
+int __cvmx_helper_rgmii_link_set(int ipd_port, cvmx_helper_link_info_t link_info)
+{
+    int result = 0;
+    int interface = cvmx_helper_get_interface_num(ipd_port);
+    int index = cvmx_helper_get_interface_index_num(ipd_port);
+    cvmx_gmxx_prtx_cfg_t original_gmx_cfg;
+    cvmx_gmxx_prtx_cfg_t new_gmx_cfg;
+    cvmx_pko_mem_queue_qos_t pko_mem_queue_qos;
+    cvmx_pko_mem_queue_qos_t pko_mem_queue_qos_save[16];
+    cvmx_gmxx_tx_ovr_bp_t gmx_tx_ovr_bp;
+    cvmx_gmxx_tx_ovr_bp_t gmx_tx_ovr_bp_save;
+    int i;
+
+    /* Ignore speed sets in the simulator */
+    if (cvmx_sysinfo_get()->board_type == CVMX_BOARD_TYPE_SIM)
+        return 0;
+
+    /* Read the current settings so we know the current enable state */
+    original_gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
+    new_gmx_cfg = original_gmx_cfg;
+
+    /* Disable the lowest level RX */
+    cvmx_write_csr(CVMX_ASXX_RX_PRT_EN(interface),
+                   cvmx_read_csr(CVMX_ASXX_RX_PRT_EN(interface)) & ~(1<<index));
+
+    /* Disable all queues so that TX should become idle */
+    for (i=0; i<cvmx_pko_get_num_queues(ipd_port); i++)
+    {
+        int queue = cvmx_pko_get_base_queue(ipd_port) + i;
+        cvmx_write_csr(CVMX_PKO_REG_READ_IDX, queue);
+        pko_mem_queue_qos.u64 = cvmx_read_csr(CVMX_PKO_MEM_QUEUE_QOS);
+        pko_mem_queue_qos.s.pid = ipd_port;
+        pko_mem_queue_qos.s.qid = queue;
+        pko_mem_queue_qos_save[i] = pko_mem_queue_qos;
+        pko_mem_queue_qos.s.qos_mask = 0;
+        cvmx_write_csr(CVMX_PKO_MEM_QUEUE_QOS, pko_mem_queue_qos.u64);
+    }
+
+    /* Disable backpressure */
+    gmx_tx_ovr_bp.u64 = cvmx_read_csr(CVMX_GMXX_TX_OVR_BP(interface));
+    gmx_tx_ovr_bp_save = gmx_tx_ovr_bp;
+    gmx_tx_ovr_bp.s.bp &= ~(1<<index);
+    gmx_tx_ovr_bp.s.en |= 1<<index;
+    cvmx_write_csr(CVMX_GMXX_TX_OVR_BP(interface), gmx_tx_ovr_bp.u64);
+    cvmx_read_csr(CVMX_GMXX_TX_OVR_BP(interface));
+
+    /* Poll the GMX state machine waiting for it to become idle. Preferably we
+        should only change speed when it is idle. If it doesn't become idle we
+        will still do the speed change, but there is a slight chance that GMX
+        will lockup */
+    cvmx_write_csr(CVMX_NPI_DBG_SELECT, interface*0x800 + index*0x100 + 0x880);
+    CVMX_WAIT_FOR_FIELD64(CVMX_DBG_DATA, cvmx_dbg_data_t, data&7, ==, 0, 10000);
+    CVMX_WAIT_FOR_FIELD64(CVMX_DBG_DATA, cvmx_dbg_data_t, data&0xf, ==, 0, 10000);
+
+    /* Disable the port before we make any changes */
+    new_gmx_cfg.s.en = 0;
+    cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), new_gmx_cfg.u64);
+    cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
+
+    /* Set full/half duplex */
+    if (cvmx_octeon_is_pass1())
+        new_gmx_cfg.s.duplex = 1;   /* Half duplex is broken for 38XX Pass 1 */
+    else if (!link_info.s.link_up)
+        new_gmx_cfg.s.duplex = 1;   /* Force full duplex on down links */
+    else
+        new_gmx_cfg.s.duplex = link_info.s.full_duplex;
+
+    /* Set the link speed. Anything unknown is set to 1Gbps */
+    if (link_info.s.speed == 10)
+    {
+        new_gmx_cfg.s.slottime = 0;
+        new_gmx_cfg.s.speed = 0;
+    }
+    else if (link_info.s.speed == 100)
+    {
+        new_gmx_cfg.s.slottime = 0;
+        new_gmx_cfg.s.speed = 0;
+    }
+    else
+    {
+        new_gmx_cfg.s.slottime = 1;
+        new_gmx_cfg.s.speed = 1;
+    }
+
+    /* Adjust the clocks */
+    if (link_info.s.speed == 10)
+    {
+        cvmx_write_csr(CVMX_GMXX_TXX_CLK(index, interface), 50);
+        cvmx_write_csr(CVMX_GMXX_TXX_SLOT(index, interface), 0x40);
+        cvmx_write_csr(CVMX_GMXX_TXX_BURST(index, interface), 0);
+    }
+    else if (link_info.s.speed == 100)
+    {
+        cvmx_write_csr(CVMX_GMXX_TXX_CLK(index, interface), 5);
+        cvmx_write_csr(CVMX_GMXX_TXX_SLOT(index, interface), 0x40);
+        cvmx_write_csr(CVMX_GMXX_TXX_BURST(index, interface), 0);
+    }
+    else
+    {
+        cvmx_write_csr(CVMX_GMXX_TXX_CLK(index, interface), 1);
+        cvmx_write_csr(CVMX_GMXX_TXX_SLOT(index, interface), 0x200);
+        cvmx_write_csr(CVMX_GMXX_TXX_BURST(index, interface), 0x2000);
+    }
+
+    if (OCTEON_IS_MODEL(OCTEON_CN30XX) || OCTEON_IS_MODEL(OCTEON_CN50XX))
+    {
+        if ((link_info.s.speed == 10) || (link_info.s.speed == 100))
+        {
+            cvmx_gmxx_inf_mode_t mode;
+            mode.u64 = cvmx_read_csr(CVMX_GMXX_INF_MODE(interface));
+
+            /*
+            ** Port  .en  .type  .p0mii  Configuration
+            ** ----  ---  -----  ------  -----------------------------------------
+            **  X      0     X      X    All links are disabled.
+            **  0      1     X      0    Port 0 is RGMII
+            **  0      1     X      1    Port 0 is MII
+            **  1      1     0      X    Ports 1 and 2 are configured as RGMII ports.
+            **  1      1     1      X    Port 1: GMII/MII; Port 2: disabled. GMII or
+            **                           MII port is selected by GMX_PRT1_CFG[SPEED].
+            */
+
+            /* In MII mode, CLK_CNT = 1. */
+            if (((index == 0) && (mode.s.p0mii == 1)) || ((index != 0) && (mode.s.type == 1)))
+            {
+                cvmx_write_csr(CVMX_GMXX_TXX_CLK(index, interface), 1);
+            }
+        }
+    }
+
+    /* Do a read to make sure all setup stuff is complete */
+    cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
+
+    /* Save the new GMX setting without enabling the port */
+    cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), new_gmx_cfg.u64);
+
+    /* Enable the lowest level RX */
+    cvmx_write_csr(CVMX_ASXX_RX_PRT_EN(interface),
+                   cvmx_read_csr(CVMX_ASXX_RX_PRT_EN(interface)) | (1<<index));
+
+    /* Re-enable the TX path */
+    for (i=0; i<cvmx_pko_get_num_queues(ipd_port); i++)
+    {
+        int queue = cvmx_pko_get_base_queue(ipd_port) + i;
+        cvmx_write_csr(CVMX_PKO_REG_READ_IDX, queue);
+        cvmx_write_csr(CVMX_PKO_MEM_QUEUE_QOS, pko_mem_queue_qos_save[i].u64);
+    }
+
+    /* Restore backpressure */
+    cvmx_write_csr(CVMX_GMXX_TX_OVR_BP(interface), gmx_tx_ovr_bp_save.u64);
+
+    /* Restore the GMX enable state. Port config is complete */
+    new_gmx_cfg.s.en = original_gmx_cfg.s.en;
+    cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), new_gmx_cfg.u64);
+
+    return result;
+}
+
+#endif /* CVMX_ENABLE_PKO_FUNCTIONS */
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-rgmii.h b/arch/mips/cavium-octeon/executive/cvmx-helper-rgmii.h
new file mode 100644
index 0000000..c78b706
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-rgmii.h
@@ -0,0 +1,121 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Functions for RGMII/GMII/MII initialization, configuration,
+ * and monitoring.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+#ifndef __CVMX_HELPER_RGMII_H__
+#define __CVMX_HELPER_RGMII_H__
+
+/**
+ * @INTERNAL
+ * Probe RGMII ports and determine the number present
+ *
+ * @param interface Interface to probe
+ *
+ * @return Number of RGMII/GMII/MII ports (0-4).
+ */
+extern int __cvmx_helper_rgmii_probe(int interface);
+
+/**
+ * Put an RGMII interface in loopback mode. Internal packets sent
+ * out will be received back again on the same port. Externally
+ * received packets will echo back out.
+ *
+ * @param port   IPD port number to loop.
+ */
+extern void cvmx_helper_rgmii_internal_loopback(int port);
+
+/**
+ * @INTERNAL
+ * Configure all of the ASX, GMX, and PKO regsiters required
+ * to get RGMII to function on the supplied interface.
+ *
+ * @param interface PKO Interface to configure (0 or 1)
+ *
+ * @return Zero on success
+ */
+extern int __cvmx_helper_rgmii_enable(int interface);
+
+/**
+ * @INTERNAL
+ * Return the link state of an IPD/PKO port as returned by
+ * auto negotiation. The result of this function may not match
+ * Octeon's link config if auto negotiation has changed since
+ * the last call to cvmx_helper_link_set().
+ *
+ * @param ipd_port IPD/PKO port to query
+ *
+ * @return Link state
+ */
+extern cvmx_helper_link_info_t __cvmx_helper_rgmii_link_get(int ipd_port);
+
+/**
+ * @INTERNAL
+ * Configure an IPD/PKO port for the specified link state. This
+ * function does not influence auto negotiation at the PHY level.
+ * The passed link state must always match the link state returned
+ * by cvmx_helper_link_get(). It is normally best to use
+ * cvmx_helper_link_autoconf() instead.
+ *
+ * @param ipd_port  IPD/PKO port to configure
+ * @param link_info The new link state
+ *
+ * @return Zero on success, negative on failure
+ */
+extern int __cvmx_helper_rgmii_link_set(int ipd_port, cvmx_helper_link_info_t link_info);
+
+#endif
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-sgmii.c b/arch/mips/cavium-octeon/executive/cvmx-helper-sgmii.c
new file mode 100644
index 0000000..5c1f76f
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-sgmii.c
@@ -0,0 +1,487 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Functions for SGMII initialization, configuration,
+ * and monitoring.
+ *
+ * <hr>$Revision: 34722 $<hr>
+ */
+#include "executive-config.h"
+#include "cvmx-config.h"
+#ifdef CVMX_ENABLE_PKO_FUNCTIONS
+
+#include "cvmx.h"
+#include "cvmx-sysinfo.h"
+#include "cvmx-mdio.h"
+#include "cvmx-helper.h"
+#include "cvmx-helper-board.h"
+
+
+/**
+ * @INTERNAL
+ * Perform initialization required only once for an SGMII port.
+ *
+ * @param interface Interface to init
+ * @param index     Index of prot on the interface
+ *
+ * @return Zero on success, negative on failure
+ */
+static int __cvmx_helper_sgmii_hardware_init_one_time(int interface, int index)
+{
+    const uint64_t clock_mhz = cvmx_sysinfo_get()->cpu_clock_hz / 1000000;
+    cvmx_gmxx_inf_mode_t mode;
+    cvmx_pcsx_linkx_timer_count_reg_t pcsx_linkx_timer_count_reg;
+    cvmx_gmxx_prtx_cfg_t gmxx_prtx_cfg;
+
+    /* Disable GMX */
+    gmxx_prtx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
+    gmxx_prtx_cfg.s.en = 0;
+    cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmxx_prtx_cfg.u64);
+
+    /* Write PCS*_LINK*_TIMER_COUNT_REG[COUNT] with the appropriate
+        value. 1000BASE-X specifies a 10ms interval. SGMII specifies a 1.6ms
+        interval. */
+    mode.u64 = cvmx_read_csr(CVMX_GMXX_INF_MODE(interface));
+    pcsx_linkx_timer_count_reg.u64 = cvmx_read_csr(CVMX_PCSX_LINKX_TIMER_COUNT_REG(index, interface));
+    if (mode.cn56xx.type)
+    {
+        /* 1000BASE-X */
+        pcsx_linkx_timer_count_reg.s.count = (10000ull * clock_mhz) >> 10;
+    }
+    else
+    {
+        /* SGMII */
+        pcsx_linkx_timer_count_reg.s.count = (1600ull * clock_mhz) >> 10;
+    }
+    cvmx_write_csr(CVMX_PCSX_LINKX_TIMER_COUNT_REG(index, interface), pcsx_linkx_timer_count_reg.u64);
+
+    /* Write the advertisement register to be used as the
+        tx_Config_Reg<D15:D0> of the autonegotiation.
+        In 1000BASE-X mode, tx_Config_Reg<D15:D0> is PCS*_AN*_ADV_REG.
+        In SGMII PHY mode, tx_Config_Reg<D15:D0> is PCS*_SGM*_AN_ADV_REG.
+        In SGMII MAC mode, tx_Config_Reg<D15:D0> is the fixed value 0x4001, so
+        this step can be skipped. */
+    if (mode.cn56xx.type)
+    {
+        /* 1000BASE-X */
+        cvmx_pcsx_anx_adv_reg_t pcsx_anx_adv_reg;
+        pcsx_anx_adv_reg.u64 = cvmx_read_csr(CVMX_PCSX_ANX_ADV_REG(index, interface));
+        pcsx_anx_adv_reg.s.rem_flt = 0;
+        pcsx_anx_adv_reg.s.pause = 3;
+        pcsx_anx_adv_reg.s.hfd = 1;
+        pcsx_anx_adv_reg.s.fd = 1;
+        cvmx_write_csr(CVMX_PCSX_ANX_ADV_REG(index, interface), pcsx_anx_adv_reg.u64);
+    }
+    else
+    {
+        cvmx_pcsx_miscx_ctl_reg_t pcsx_miscx_ctl_reg;
+        pcsx_miscx_ctl_reg.u64 = cvmx_read_csr(CVMX_PCSX_MISCX_CTL_REG(index, interface));
+        if (pcsx_miscx_ctl_reg.s.mac_phy)
+        {
+            /* PHY Mode */
+            cvmx_pcsx_sgmx_an_adv_reg_t pcsx_sgmx_an_adv_reg;
+            pcsx_sgmx_an_adv_reg.u64 = cvmx_read_csr(CVMX_PCSX_SGMX_AN_ADV_REG(index, interface));
+            pcsx_sgmx_an_adv_reg.s.link = 1;
+            pcsx_sgmx_an_adv_reg.s.dup = 1;
+            pcsx_sgmx_an_adv_reg.s.speed= 2;
+            cvmx_write_csr(CVMX_PCSX_SGMX_AN_ADV_REG(index, interface), pcsx_sgmx_an_adv_reg.u64);
+        }
+        else
+        {
+            /* MAC Mode - Nothing to do */
+        }
+    }
+    return 0;
+}
+
+
+/**
+ * @INTERNAL
+ * Initialize the SERTES link for the first time or after a loss
+ * of link.
+ *
+ * @param interface Interface to init
+ * @param index     Index of prot on the interface
+ *
+ * @return Zero on success, negative on failure
+ */
+static int __cvmx_helper_sgmii_hardware_init_link(int interface, int index)
+{
+    cvmx_pcsx_mrx_control_reg_t control_reg;
+
+    /* Take PCS through a reset sequence.
+        PCS*_MR*_CONTROL_REG[PWR_DN] should be cleared to zero.
+        Write PCS*_MR*_CONTROL_REG[RESET]=1 (while not changing the value of
+            the other PCS*_MR*_CONTROL_REG bits).
+        Read PCS*_MR*_CONTROL_REG[RESET] until it changes value to zero. */
+    control_reg.u64 = cvmx_read_csr(CVMX_PCSX_MRX_CONTROL_REG(index, interface));
+    if (cvmx_sysinfo_get()->board_type != CVMX_BOARD_TYPE_SIM)
+    {
+        control_reg.s.reset = 1;
+        cvmx_write_csr(CVMX_PCSX_MRX_CONTROL_REG(index, interface), control_reg.u64);
+        if (CVMX_WAIT_FOR_FIELD64(CVMX_PCSX_MRX_CONTROL_REG(index, interface), cvmx_pcsx_mrx_control_reg_t, reset, ==, 0, 10000))
+        {
+            cvmx_dprintf("SGMII%d: Timeout waiting for port %d to finish reset\n", interface, index);
+            return -1;
+        }
+    }
+
+    /* Write PCS*_MR*_CONTROL_REG[RST_AN]=1 to ensure a fresh sgmii negotiation starts. */
+    control_reg.s.rst_an = 1;
+    control_reg.s.an_en = 1;
+    control_reg.s.pwr_dn = 0;
+    cvmx_write_csr(CVMX_PCSX_MRX_CONTROL_REG(index, interface), control_reg.u64);
+
+    /* Wait for PCS*_MR*_STATUS_REG[AN_CPT] to be set, indicating that
+        sgmii autonegotiation is complete. In MAC mode this isn't an ethernet
+        link, but a link between Octeon and the PHY */
+    if ((cvmx_sysinfo_get()->board_type != CVMX_BOARD_TYPE_SIM) &&
+        CVMX_WAIT_FOR_FIELD64(CVMX_PCSX_MRX_STATUS_REG(index, interface), cvmx_pcsx_mrx_status_reg_t, an_cpt, ==, 1, 10000))
+    {
+        //cvmx_dprintf("SGMII%d: Port %d link timeout\n", interface, index);
+        return -1;
+    }
+    return 0;
+}
+
+
+/**
+ * @INTERNAL
+ * Configure an SGMII link to the specified speed after the SERTES
+ * link is up.
+ *
+ * @param interface Interface to init
+ * @param index     Index of prot on the interface
+ * @param link_info Link state to configure
+ *
+ * @return Zero on success, negative on failure
+ */
+static int __cvmx_helper_sgmii_hardware_init_link_speed(int interface, int index, cvmx_helper_link_info_t link_info)
+{
+    int is_enabled;
+    cvmx_gmxx_prtx_cfg_t gmxx_prtx_cfg;
+    cvmx_pcsx_miscx_ctl_reg_t pcsx_miscx_ctl_reg;
+
+    /* Disable GMX before we make any changes. Remember the enable state */
+    gmxx_prtx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
+    is_enabled = gmxx_prtx_cfg.s.en;
+    gmxx_prtx_cfg.s.en = 0;
+    cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmxx_prtx_cfg.u64);
+
+    /* Wait for GMX to be idle */
+    if (CVMX_WAIT_FOR_FIELD64(CVMX_GMXX_PRTX_CFG(index, interface), cvmx_gmxx_prtx_cfg_t, rx_idle, ==, 1, 10000) ||
+        CVMX_WAIT_FOR_FIELD64(CVMX_GMXX_PRTX_CFG(index, interface), cvmx_gmxx_prtx_cfg_t, tx_idle, ==, 1, 10000))
+    {
+        cvmx_dprintf("SGMII%d: Timeout waiting for port %d to be idle\n", interface, index);
+        return -1;
+    }
+
+    /* Read GMX CFG again to make sure the disable completed */
+    gmxx_prtx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
+
+    /* Get the misc control for PCS. We will need to set the duplication amount */
+    pcsx_miscx_ctl_reg.u64 = cvmx_read_csr(CVMX_PCSX_MISCX_CTL_REG(index, interface));
+
+    /* Use GMXENO to force the link down if the status we get says it should be down */
+    pcsx_miscx_ctl_reg.s.gmxeno = !link_info.s.link_up;
+
+    /* Only change the duplex setting if the link is up */
+    if (link_info.s.link_up)
+        gmxx_prtx_cfg.s.duplex = link_info.s.full_duplex;
+
+    /* Do speed based setting for GMX */
+    switch (link_info.s.speed)
+    {
+        case 10:
+            gmxx_prtx_cfg.s.speed = 0;
+            gmxx_prtx_cfg.s.speed_msb = 1;
+            gmxx_prtx_cfg.s.slottime = 0;
+            pcsx_miscx_ctl_reg.s.samp_pt = 25; /* Setting from GMX-603 */
+            cvmx_write_csr(CVMX_GMXX_TXX_SLOT(index, interface), 64);
+            cvmx_write_csr(CVMX_GMXX_TXX_BURST(index, interface), 0);
+            break;
+        case 100:
+            gmxx_prtx_cfg.s.speed = 0;
+            gmxx_prtx_cfg.s.speed_msb = 0;
+            gmxx_prtx_cfg.s.slottime = 0;
+            pcsx_miscx_ctl_reg.s.samp_pt = 0x5;
+            cvmx_write_csr(CVMX_GMXX_TXX_SLOT(index, interface), 64);
+            cvmx_write_csr(CVMX_GMXX_TXX_BURST(index, interface), 0);
+            break;
+        case 1000:
+            gmxx_prtx_cfg.s.speed = 1;
+            gmxx_prtx_cfg.s.speed_msb = 0;
+            gmxx_prtx_cfg.s.slottime = 1;
+            pcsx_miscx_ctl_reg.s.samp_pt = 1;
+            cvmx_write_csr(CVMX_GMXX_TXX_SLOT(index, interface), 512);
+            cvmx_write_csr(CVMX_GMXX_TXX_BURST(index, interface), 8192);
+            break;
+        default:
+            break;
+    }
+
+    /* Write the new misc control for PCS */
+    cvmx_write_csr(CVMX_PCSX_MISCX_CTL_REG(index, interface), pcsx_miscx_ctl_reg.u64);
+
+    /* Write the new GMX settings with the port still disabled */
+    cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmxx_prtx_cfg.u64);
+
+    /* Read GMX CFG again to make sure the config completed */
+    gmxx_prtx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
+
+    /* Restore the enabled / disabled state */
+    gmxx_prtx_cfg.s.en = is_enabled;
+    cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmxx_prtx_cfg.u64);
+
+    return 0;
+}
+
+
+/**
+ * @INTERNAL
+ * Bring up the SGMII interface to be ready for packet I/O but
+ * leave I/O disabled using the GMX override. This function
+ * follows the bringup documented in 10.6.3 of the manual.
+ *
+ * @param interface Interface to bringup
+ * @param num_ports Number of ports on the interface
+ *
+ * @return Zero on success, negative on failure
+ */
+static int __cvmx_helper_sgmii_hardware_init(int interface, int num_ports)
+{
+    int index;
+    cvmx_gmxx_inf_mode_t mode;
+
+    __cvmx_helper_setup_gmx(interface, num_ports);
+
+    /* Enable the interface. Set GMX0/1_INF_MODE[EN]=1 if not already set. */
+    mode.u64 = cvmx_read_csr(CVMX_GMXX_INF_MODE(interface));
+    if (!mode.s.en)
+    {
+        mode.s.en = 1;
+        cvmx_write_csr(CVMX_GMXX_INF_MODE(interface), mode.u64);
+    }
+
+    for (index=0; index<num_ports; index++)
+    {
+        int ipd_port = cvmx_helper_get_ipd_port(interface, index);
+        __cvmx_helper_sgmii_hardware_init_one_time(interface, index);
+        __cvmx_helper_sgmii_link_set(ipd_port, __cvmx_helper_sgmii_link_get(ipd_port));
+
+    }
+
+    return 0;
+}
+
+
+/**
+ * @INTERNAL
+ * Probe a SGMII interface and determine the number of ports
+ * connected to it. The SGMII interface should still be down after
+ * this call.
+ *
+ * @param interface Interface to probe
+ *
+ * @return Number of ports on the interface. Zero to disable.
+ */
+int __cvmx_helper_sgmii_probe(int interface)
+{
+    return 4;
+}
+
+
+/**
+ * @INTERNAL
+ * Bringup and enable a SGMII interface. After this call packet
+ * I/O should be fully functional. This is called with IPD
+ * enabled but PKO disabled.
+ *
+ * @param interface Interface to bring up
+ *
+ * @return Zero on success, negative on failure
+ */
+int __cvmx_helper_sgmii_enable(int interface)
+{
+    int num_ports = cvmx_helper_ports_on_interface(interface);
+    int index;
+
+    __cvmx_helper_sgmii_hardware_init(interface, num_ports);
+
+    for (index=0; index<num_ports; index++)
+    {
+        cvmx_gmxx_prtx_cfg_t gmxx_prtx_cfg;
+        gmxx_prtx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
+        gmxx_prtx_cfg.s.en = 1;
+        cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmxx_prtx_cfg.u64);
+    }
+    return 0;
+}
+
+
+/**
+ * @INTERNAL
+ * Return the link state of an IPD/PKO port as returned by
+ * auto negotiation. The result of this function may not match
+ * Octeon's link config if auto negotiation has changed since
+ * the last call to cvmx_helper_link_set().
+ *
+ * @param ipd_port IPD/PKO port to query
+ *
+ * @return Link state
+ */
+cvmx_helper_link_info_t __cvmx_helper_sgmii_link_get(int ipd_port)
+{
+    cvmx_helper_link_info_t result;
+    cvmx_gmxx_inf_mode_t mode;
+    int interface = cvmx_helper_get_interface_num(ipd_port);
+    int index = cvmx_helper_get_interface_index_num(ipd_port);
+
+    result.u64 = 0;
+
+    if (cvmx_sysinfo_get()->board_type == CVMX_BOARD_TYPE_SIM)
+    {
+        /* The simulator gives you a simulated 1Gbps full duplex link */
+        result.s.link_up = 1;
+        result.s.full_duplex = 1;
+        result.s.speed = 1000;
+        return result;
+    }
+
+    mode.u64 = cvmx_read_csr(CVMX_GMXX_INF_MODE(interface));
+    if (mode.cn56xx.type)
+    {
+        /* 1000BASE-X */
+        // FIXME
+    }
+    else
+    {
+        cvmx_pcsx_miscx_ctl_reg_t pcsx_miscx_ctl_reg;
+        pcsx_miscx_ctl_reg.u64 = cvmx_read_csr(CVMX_PCSX_MISCX_CTL_REG(index, interface));
+        if (pcsx_miscx_ctl_reg.s.mac_phy)
+        {
+            /* PHY Mode */
+            cvmx_pcsx_mrx_status_reg_t pcsx_mrx_status_reg;
+            cvmx_pcsx_anx_results_reg_t pcsx_anx_results_reg;
+
+            /* Don't bother continuing if the SERTES low level link is down */
+            pcsx_mrx_status_reg.u64 = cvmx_read_csr(CVMX_PCSX_MRX_STATUS_REG(index, interface));
+            if (pcsx_mrx_status_reg.s.lnk_st == 0)
+            {
+                if (__cvmx_helper_sgmii_hardware_init_link(interface, index) != 0)
+                    return result;
+            }
+
+            /* Read the autoneg results */
+            pcsx_anx_results_reg.u64 = cvmx_read_csr(CVMX_PCSX_ANX_RESULTS_REG(index, interface));
+            if (pcsx_anx_results_reg.s.an_cpt)
+            {
+                /* Auto negotiation is complete. Set status accordingly */
+                result.s.full_duplex = pcsx_anx_results_reg.s.dup;
+                result.s.link_up = pcsx_anx_results_reg.s.link_ok;
+                switch (pcsx_anx_results_reg.s.spd)
+                {
+                    case 0:
+                        result.s.speed = 10;
+                        break;
+                    case 1:
+                        result.s.speed = 100;
+                        break;
+                    case 2:
+                        result.s.speed = 1000;
+                        break;
+                    default:
+                        result.s.speed = 0;
+                        result.s.link_up = 0;
+                        break;
+                }
+            }
+            else
+            {
+                /* Auto negotiation isn't complete. Return link down */
+                result.s.speed = 0;
+                result.s.link_up = 0;
+            }
+        }
+        else /* MAC Mode */
+        {
+            result = __cvmx_helper_board_link_get(ipd_port);
+        }
+    }
+    return result;
+}
+
+
+/**
+ * @INTERNAL
+ * Configure an IPD/PKO port for the specified link state. This
+ * function does not influence auto negotiation at the PHY level.
+ * The passed link state must always match the link state returned
+ * by cvmx_helper_link_get(). It is normally best to use
+ * cvmx_helper_link_autoconf() instead.
+ *
+ * @param ipd_port  IPD/PKO port to configure
+ * @param link_info The new link state
+ *
+ * @return Zero on success, negative on failure
+ */
+int __cvmx_helper_sgmii_link_set(int ipd_port, cvmx_helper_link_info_t link_info)
+{
+    int interface = cvmx_helper_get_interface_num(ipd_port);
+    int index = cvmx_helper_get_interface_index_num(ipd_port);
+    __cvmx_helper_sgmii_hardware_init_link(interface, index);
+    return __cvmx_helper_sgmii_hardware_init_link_speed(interface, index, link_info);
+}
+
+#endif /* CVMX_ENABLE_PKO_FUNCTIONS */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-sgmii.h b/arch/mips/cavium-octeon/executive/cvmx-helper-sgmii.h
new file mode 100644
index 0000000..14c3682
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-sgmii.h
@@ -0,0 +1,115 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Functions for SGMII initialization, configuration,
+ * and monitoring.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+#ifndef __CVMX_HELPER_SGMII_H__
+#define __CVMX_HELPER_SGMII_H__
+
+/**
+ * @INTERNAL
+ * Probe a SGMII interface and determine the number of ports
+ * connected to it. The SGMII interface should still be down after
+ * this call.
+ *
+ * @param interface Interface to probe
+ *
+ * @return Number of ports on the interface. Zero to disable.
+ */
+extern int __cvmx_helper_sgmii_probe(int interface);
+
+/**
+ * @INTERNAL
+ * Bringup and enable a SGMII interface. After this call packet
+ * I/O should be fully functional. This is called with IPD
+ * enabled but PKO disabled.
+ *
+ * @param interface Interface to bring up
+ *
+ * @return Zero on success, negative on failure
+ */
+extern int __cvmx_helper_sgmii_enable(int interface);
+
+/**
+ * @INTERNAL
+ * Return the link state of an IPD/PKO port as returned by
+ * auto negotiation. The result of this function may not match
+ * Octeon's link config if auto negotiation has changed since
+ * the last call to cvmx_helper_link_set().
+ *
+ * @param ipd_port IPD/PKO port to query
+ *
+ * @return Link state
+ */
+extern cvmx_helper_link_info_t __cvmx_helper_sgmii_link_get(int ipd_port);
+
+/**
+ * @INTERNAL
+ * Configure an IPD/PKO port for the specified link state. This
+ * function does not influence auto negotiation at the PHY level.
+ * The passed link state must always match the link state returned
+ * by cvmx_helper_link_get(). It is normally best to use
+ * cvmx_helper_link_autoconf() instead.
+ *
+ * @param ipd_port  IPD/PKO port to configure
+ * @param link_info The new link state
+ *
+ * @return Zero on success, negative on failure
+ */
+extern int __cvmx_helper_sgmii_link_set(int ipd_port, cvmx_helper_link_info_t link_info);
+
+#endif
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-spi.c b/arch/mips/cavium-octeon/executive/cvmx-helper-spi.c
new file mode 100644
index 0000000..1ec40dd
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-spi.c
@@ -0,0 +1,227 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Functions for SPI initialization, configuration,
+ * and monitoring.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+#include "executive-config.h"
+#include "cvmx-config.h"
+#ifdef CVMX_ENABLE_PKO_FUNCTIONS
+
+#include "cvmx.h"
+#include "cvmx-spi.h"
+#include "cvmx-sysinfo.h"
+#include "cvmx-helper.h"
+
+/* CVMX_HELPER_SPI_TIMEOUT is used to determine how long the SPI initialization
+    routines wait for SPI training. You can override the value using
+    executive-config.h if necessary */
+#ifndef CVMX_HELPER_SPI_TIMEOUT
+#define CVMX_HELPER_SPI_TIMEOUT 10
+#endif
+
+
+/**
+ * @INTERNAL
+ * Probe a SPI interface and determine the number of ports
+ * connected to it. The SPI interface should still be down after
+ * this call.
+ *
+ * @param interface Interface to probe
+ *
+ * @return Number of ports on the interface. Zero to disable.
+ */
+int __cvmx_helper_spi_probe(int interface)
+{
+    int num_ports = 0;
+
+    if ((cvmx_sysinfo_get()->board_type != CVMX_BOARD_TYPE_SIM) &&
+        cvmx_spi4000_is_present(interface))
+    {
+        num_ports = 10;
+    }
+    else
+    {
+        cvmx_pko_reg_crc_enable_t enable;
+        num_ports = 16;
+        /* Unlike the SPI4000, most SPI devices don't automatically
+            put on the L2 CRC. For everything except for the SPI4000
+            have PKO append the L2 CRC to the packet */
+        enable.u64 = cvmx_read_csr(CVMX_PKO_REG_CRC_ENABLE);
+        enable.s.enable |= 0xffff << (interface*16);
+        cvmx_write_csr(CVMX_PKO_REG_CRC_ENABLE, enable.u64);
+    }
+    __cvmx_helper_setup_gmx(interface, num_ports);
+    return num_ports;
+}
+
+
+/**
+ * @INTERNAL
+ * Bringup and enable a SPI interface. After this call packet I/O
+ * should be fully functional. This is called with IPD enabled but
+ * PKO disabled.
+ *
+ * @param interface Interface to bring up
+ *
+ * @return Zero on success, negative on failure
+ */
+int __cvmx_helper_spi_enable(int interface)
+{
+    /* Normally the ethernet L2 CRC is checked and stripped in the GMX block.
+        When you are using SPI, this isn' the case and IPD needs to check
+        the L2 CRC */
+    int num_ports = cvmx_helper_ports_on_interface(interface);
+    int ipd_port;
+    for (ipd_port=interface*16; ipd_port<interface*16+num_ports; ipd_port++)
+    {
+        cvmx_pip_port_cfg_t port_config;
+        port_config.u64 = cvmx_read_csr(CVMX_PIP_PRT_CFGX(ipd_port));
+        port_config.s.crc_en = 1;
+        cvmx_write_csr(CVMX_PIP_PRT_CFGX(ipd_port), port_config.u64);
+    }
+
+    if (cvmx_sysinfo_get()->board_type != CVMX_BOARD_TYPE_SIM)
+    {
+        cvmx_spi_start_interface(interface, CVMX_SPI_MODE_DUPLEX, CVMX_HELPER_SPI_TIMEOUT, num_ports);
+        if (cvmx_spi4000_is_present(interface))
+            cvmx_spi4000_initialize(interface);
+    }
+    return 0;
+}
+
+/**
+ * @INTERNAL
+ * Return the link state of an IPD/PKO port as returned by
+ * auto negotiation. The result of this function may not match
+ * Octeon's link config if auto negotiation has changed since
+ * the last call to cvmx_helper_link_set().
+ *
+ * @param ipd_port IPD/PKO port to query
+ *
+ * @return Link state
+ */
+cvmx_helper_link_info_t __cvmx_helper_spi_link_get(int ipd_port)
+{
+    cvmx_helper_link_info_t result;
+    int interface = cvmx_helper_get_interface_num(ipd_port);
+    int index = cvmx_helper_get_interface_index_num(ipd_port);
+    result.u64 = 0;
+
+    if (cvmx_sysinfo_get()->board_type == CVMX_BOARD_TYPE_SIM)
+    {
+        /* The simulator gives you a simulated full duplex link */
+        result.s.link_up = 1;
+        result.s.full_duplex = 1;
+        result.s.speed = 10000;
+    }
+    else if (cvmx_spi4000_is_present(interface))
+    {
+        cvmx_gmxx_rxx_rx_inbnd_t inband = cvmx_spi4000_check_speed(interface, index);
+        result.s.link_up = inband.s.status;
+        result.s.full_duplex = inband.s.duplex;
+        switch (inband.s.speed)
+        {
+            case 0: /* 10 Mbps */
+                result.s.speed = 10;
+                break;
+            case 1: /* 100 Mbps */
+                result.s.speed = 100;
+                break;
+            case 2: /* 1 Gbps */
+                result.s.speed = 1000;
+                break;
+            case 3: /* Illegal */
+                result.s.speed = 0;
+                result.s.link_up = 0;
+                break;
+        }
+    }
+    else
+    {
+        /* For generic SPI we can't determine the link, just return some
+            sane results */
+        result.s.link_up = 1;
+        result.s.full_duplex = 1;
+        result.s.speed = 10000;
+    }
+    return result;
+}
+
+
+/**
+ * @INTERNAL
+ * Configure an IPD/PKO port for the specified link state. This
+ * function does not influence auto negotiation at the PHY level.
+ * The passed link state must always match the link state returned
+ * by cvmx_helper_link_get(). It is normally best to use
+ * cvmx_helper_link_autoconf() instead.
+ *
+ * @param ipd_port  IPD/PKO port to configure
+ * @param link_info The new link state
+ *
+ * @return Zero on success, negative on failure
+ */
+int __cvmx_helper_spi_link_set(int ipd_port, cvmx_helper_link_info_t link_info)
+{
+    /* Nothing to do. If we have a SPI4000 then the setup was already performed
+        by cvmx_spi4000_check_speed(). If not then there isn't any link
+        info */
+    return 0;
+}
+
+#endif /* CVMX_ENABLE_PKO_FUNCTIONS */
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-spi.h b/arch/mips/cavium-octeon/executive/cvmx-helper-spi.h
new file mode 100644
index 0000000..17bf3ff
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-spi.h
@@ -0,0 +1,115 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Functions for SPI initialization, configuration,
+ * and monitoring.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+#ifndef __CVMX_HELPER_SPI_H__
+#define __CVMX_HELPER_SPI_H__
+
+/**
+ * @INTERNAL
+ * Probe a SPI interface and determine the number of ports
+ * connected to it. The SPI interface should still be down after
+ * this call.
+ *
+ * @param interface Interface to probe
+ *
+ * @return Number of ports on the interface. Zero to disable.
+ */
+extern int __cvmx_helper_spi_probe(int interface);
+
+/**
+ * @INTERNAL
+ * Bringup and enable a SPI interface. After this call packet I/O
+ * should be fully functional. This is called with IPD enabled but
+ * PKO disabled.
+ *
+ * @param interface Interface to bring up
+ *
+ * @return Zero on success, negative on failure
+ */
+extern int __cvmx_helper_spi_enable(int interface);
+
+/**
+ * @INTERNAL
+ * Return the link state of an IPD/PKO port as returned by
+ * auto negotiation. The result of this function may not match
+ * Octeon's link config if auto negotiation has changed since
+ * the last call to cvmx_helper_link_set().
+ *
+ * @param ipd_port IPD/PKO port to query
+ *
+ * @return Link state
+ */
+extern cvmx_helper_link_info_t __cvmx_helper_spi_link_get(int ipd_port);
+
+/**
+ * @INTERNAL
+ * Configure an IPD/PKO port for the specified link state. This
+ * function does not influence auto negotiation at the PHY level.
+ * The passed link state must always match the link state returned
+ * by cvmx_helper_link_get(). It is normally best to use
+ * cvmx_helper_link_autoconf() instead.
+ *
+ * @param ipd_port  IPD/PKO port to configure
+ * @param link_info The new link state
+ *
+ * @return Zero on success, negative on failure
+ */
+extern int __cvmx_helper_spi_link_set(int ipd_port, cvmx_helper_link_info_t link_info);
+
+#endif
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-util.c b/arch/mips/cavium-octeon/executive/cvmx-helper-util.c
new file mode 100644
index 0000000..d866346
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-util.c
@@ -0,0 +1,424 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Small helper utilities.
+ *
+ * <hr>$Revision: 34041 $<hr>
+ */
+#include "executive-config.h"
+#include "cvmx-config.h"
+#include "cvmx.h"
+#include "cvmx-bootmem.h"
+#include "cvmx-fpa.h"
+#include "cvmx-pip.h"
+#include "cvmx-pko.h"
+#include "cvmx-ipd.h"
+#include "cvmx-asx.h"
+#include "cvmx-gmx.h"
+#include "cvmx-spi.h"
+#include "cvmx-sysinfo.h"
+#include "cvmx-helper.h"
+#include "cvmx-helper-util.h"
+#include "cvmx-version.h"
+
+#ifdef CVMX_ENABLE_HELPER_FUNCTIONS
+
+/**
+ * Get the version of the CVMX libraries.
+ *
+ * @return Version string. Note this buffer is allocated statically
+ *         and will be shared by all callers.
+ */
+const char *cvmx_helper_get_version(void)
+{
+    return OCTEON_SDK_VERSION_STRING;
+}
+
+
+/**
+ * Convert a interface mode into a human readable string
+ *
+ * @param mode   Mode to convert
+ *
+ * @return String
+ */
+const char *cvmx_helper_interface_mode_to_string(cvmx_helper_interface_mode_t mode)
+{
+    switch (mode)
+    {
+        case CVMX_HELPER_INTERFACE_MODE_DISABLED:   return "DISABLED";
+        case CVMX_HELPER_INTERFACE_MODE_RGMII:      return "RGMII";
+        case CVMX_HELPER_INTERFACE_MODE_GMII:       return "GMII";
+        case CVMX_HELPER_INTERFACE_MODE_SPI:        return "SPI";
+        case CVMX_HELPER_INTERFACE_MODE_PCIE:       return "PCIE";
+        case CVMX_HELPER_INTERFACE_MODE_XAUI:       return "XAUI";
+        case CVMX_HELPER_INTERFACE_MODE_SGMII:      return "SGMII";
+        case CVMX_HELPER_INTERFACE_MODE_PICMG:      return "PICMG";
+        case CVMX_HELPER_INTERFACE_MODE_NPI:        return "NPI";
+        case CVMX_HELPER_INTERFACE_MODE_LOOP:       return "LOOP";
+    }
+    return "UNKNOWN";
+}
+
+
+/**
+ * Debug routine to dump the packet structure to the console
+ *
+ * @param work   Work queue entry containing the packet to dump
+ * @return
+ */
+int cvmx_helper_dump_packet(cvmx_wqe_t *work)
+{
+    uint64_t        count;
+    uint64_t        remaining_bytes;
+    cvmx_buf_ptr_t  buffer_ptr;
+    uint64_t        start_of_buffer;
+    uint8_t *       data_address;
+    uint8_t *       end_of_data;
+
+    cvmx_dprintf("Packet Length:   %u\n", work->len);
+    cvmx_dprintf("    Input Port:  %u\n", work->ipprt);
+    cvmx_dprintf("    QoS:         %u\n", work->qos);
+    cvmx_dprintf("    Buffers:     %u\n", work->word2.s.bufs);
+
+    if (work->word2.s.bufs == 0)
+    {
+        cvmx_ipd_wqe_fpa_queue_t wqe_pool;
+        wqe_pool.u64 = cvmx_read_csr(CVMX_IPD_WQE_FPA_QUEUE);
+        buffer_ptr.u64 = 0;
+        buffer_ptr.s.pool = wqe_pool.s.wqe_pool;
+        buffer_ptr.s.size = 128;
+        buffer_ptr.s.addr = cvmx_ptr_to_phys(work->packet_data);
+        if (cvmx_likely(!work->word2.s.not_IP))
+        {
+            if (work->word2.s.is_v6)
+                buffer_ptr.s.addr += 2;
+            else
+                buffer_ptr.s.addr += 6;
+        }
+    }
+    else
+        buffer_ptr = work->packet_ptr;
+    remaining_bytes = work->len;
+
+    while (remaining_bytes)
+    {
+        start_of_buffer = ((buffer_ptr.s.addr >> 7) - buffer_ptr.s.back) << 7;
+        cvmx_dprintf("    Buffer Start:%llx\n", (unsigned long long)start_of_buffer);
+        cvmx_dprintf("    Buffer I   : %u\n", buffer_ptr.s.i);
+        cvmx_dprintf("    Buffer Back: %u\n", buffer_ptr.s.back);
+        cvmx_dprintf("    Buffer Pool: %u\n", buffer_ptr.s.pool);
+        cvmx_dprintf("    Buffer Data: %llx\n", (unsigned long long)buffer_ptr.s.addr);
+        cvmx_dprintf("    Buffer Size: %u\n", buffer_ptr.s.size);
+
+        cvmx_dprintf("\t\t");
+        data_address = (uint8_t *)cvmx_phys_to_ptr(buffer_ptr.s.addr);
+        end_of_data = data_address + buffer_ptr.s.size;
+        count = 0;
+        while (data_address < end_of_data)
+        {
+            if (remaining_bytes == 0)
+                break;
+            else
+                remaining_bytes--;
+            cvmx_dprintf("%02x", (unsigned int)*data_address);
+            data_address++;
+            if (remaining_bytes && (count == 7))
+            {
+                cvmx_dprintf("\n\t\t");
+                count = 0;
+            }
+            else
+                count++;
+        }
+        cvmx_dprintf("\n");
+
+        if (remaining_bytes)
+            buffer_ptr = *(cvmx_buf_ptr_t*)cvmx_phys_to_ptr(buffer_ptr.s.addr - 8);
+    }
+    return 0;
+}
+
+
+/**
+ * Setup Random Early Drop on a specific input queue
+ *
+ * @param queue  Input queue to setup RED on (0-7)
+ * @param pass_thresh
+ *               Packets will begin slowly dropping when there are less than
+ *               this many packet buffers free in FPA 0.
+ * @param drop_thresh
+ *               All incomming packets will be dropped when there are less
+ *               than this many free packet buffers in FPA 0.
+ * @return Zero on success. Negative on failure
+ */
+int cvmx_helper_setup_red_queue(int queue, int pass_thresh, int drop_thresh)
+{
+    cvmx_ipd_qos_red_marks_t red_marks;
+    cvmx_ipd_red_quex_param_t red_param;
+
+    /* Set RED to begin dropping packets when there are pass_thresh buffers
+        left. It will linearly drop more packets until reaching drop_thresh
+        buffers */
+    red_marks.u64 = 0;
+    red_marks.s.drop = drop_thresh;
+    red_marks.s.pass = pass_thresh;
+    cvmx_write_csr(CVMX_IPD_QOSX_RED_MARKS(queue), red_marks.u64);
+
+    /* Use the actual queue 0 counter, not the average */
+    red_param.u64 = 0;
+    red_param.s.prb_con = (255ul<<24) / (red_marks.s.pass - red_marks.s.drop);
+    red_param.s.avg_con = 1;
+    red_param.s.new_con = 255;
+    red_param.s.use_pcnt = 1;
+    cvmx_write_csr(CVMX_IPD_RED_QUEX_PARAM(queue), red_param.u64);
+    return 0;
+}
+
+
+/**
+ * Setup Random Early Drop to automatically begin dropping packets.
+ *
+ * @param pass_thresh
+ *               Packets will begin slowly dropping when there are less than
+ *               this many packet buffers free in FPA 0.
+ * @param drop_thresh
+ *               All incomming packets will be dropped when there are less
+ *               than this many free packet buffers in FPA 0.
+ * @return Zero on success. Negative on failure
+ */
+int cvmx_helper_setup_red(int pass_thresh, int drop_thresh)
+{
+    cvmx_ipd_portx_bp_page_cnt_t page_cnt;
+    cvmx_ipd_bp_prt_red_end_t ipd_bp_prt_red_end;
+    cvmx_ipd_red_port_enable_t red_port_enable;
+    int queue;
+    int interface;
+    int port;
+
+    /* Disable backpressure based on queued buffers. It needs SW support */
+    page_cnt.u64 = 0;
+    page_cnt.s.bp_enb = 0;
+    page_cnt.s.page_cnt = 100;
+    for (interface=0; interface<2; interface++)
+    {
+        for (port=cvmx_helper_get_first_ipd_port(interface); port<cvmx_helper_get_last_ipd_port(interface); port++)
+            cvmx_write_csr(CVMX_IPD_PORTX_BP_PAGE_CNT(port), page_cnt.u64);
+    }
+
+    for (queue=0; queue<8; queue++)
+        cvmx_helper_setup_red_queue(queue, pass_thresh, drop_thresh);
+
+    /* Shutoff the dropping based on the per port page count. SW isn't
+        decrementing it right now */
+    ipd_bp_prt_red_end.u64 = 0;
+    ipd_bp_prt_red_end.s.prt_enb = 0;
+    cvmx_write_csr(CVMX_IPD_BP_PRT_RED_END, ipd_bp_prt_red_end.u64);
+
+    red_port_enable.u64 = 0;
+    red_port_enable.s.prt_enb = 0xfffffffffull;
+    red_port_enable.s.avg_dly = 10000;
+    red_port_enable.s.prb_dly = 10000;
+    cvmx_write_csr(CVMX_IPD_RED_PORT_ENABLE, red_port_enable.u64);
+
+    return 0;
+}
+
+
+/**
+ * @INTERNAL
+ * Setup the common GMX settings that determine the number of
+ * ports. These setting apply to almost all configurations of all
+ * chips.
+ *
+ * @param interface Interface to configure
+ * @param num_ports Number of ports on the interface
+ *
+ * @return Zero on success, negative on failure
+ */
+int __cvmx_helper_setup_gmx(int interface, int num_ports)
+{
+    cvmx_gmxx_tx_prts_t gmx_tx_prts;
+    cvmx_gmxx_rx_prts_t gmx_rx_prts;
+    cvmx_pko_reg_gmx_port_mode_t pko_mode;
+
+    /* Tell GMX the number of TX ports on this interface */
+    gmx_tx_prts.u64 = cvmx_read_csr(CVMX_GMXX_TX_PRTS(interface));
+    gmx_tx_prts.s.prts = num_ports;
+    cvmx_write_csr(CVMX_GMXX_TX_PRTS(interface), gmx_tx_prts.u64);
+
+    /* Tell GMX the number of RX ports on this interface.  This only
+    ** applies to *GMII and XAUI ports */
+    if (cvmx_helper_interface_get_mode(interface) == CVMX_HELPER_INTERFACE_MODE_RGMII
+        || cvmx_helper_interface_get_mode(interface) == CVMX_HELPER_INTERFACE_MODE_SGMII
+        || cvmx_helper_interface_get_mode(interface) == CVMX_HELPER_INTERFACE_MODE_GMII
+        || cvmx_helper_interface_get_mode(interface) == CVMX_HELPER_INTERFACE_MODE_XAUI)
+    {
+        if (num_ports > 4)
+        {
+            cvmx_dprintf("__cvmx_helper_setup_gmx: Illegal num_ports\n");
+            return(-1);
+        }
+
+        gmx_rx_prts.u64 = cvmx_read_csr(CVMX_GMXX_RX_PRTS(interface));
+        gmx_rx_prts.s.prts = num_ports;
+        cvmx_write_csr(CVMX_GMXX_RX_PRTS(interface), gmx_rx_prts.u64);
+    }
+
+    /* Skip setting CVMX_PKO_REG_GMX_PORT_MODE on 30XX and 31XX */
+    if (OCTEON_IS_MODEL(OCTEON_CN30XX) || OCTEON_IS_MODEL(OCTEON_CN31XX) || OCTEON_IS_MODEL(OCTEON_CN50XX))
+        return 0;
+
+    /* Tell PKO the number of ports on this interface */
+    pko_mode.u64 = cvmx_read_csr(CVMX_PKO_REG_GMX_PORT_MODE);
+    if (interface == 0)
+    {
+        if (num_ports == 1)
+            pko_mode.s.mode0 = 4;
+        else if (num_ports == 2)
+            pko_mode.s.mode0 = 3;
+        else if (num_ports <= 4)
+            pko_mode.s.mode0 = 2;
+        else if (num_ports <= 8)
+            pko_mode.s.mode0 = 1;
+        else
+            pko_mode.s.mode0 = 0;
+    }
+    else
+    {
+        if (num_ports == 1)
+            pko_mode.s.mode1 = 4;
+        else if (num_ports == 2)
+            pko_mode.s.mode1 = 3;
+        else if (num_ports <= 4)
+            pko_mode.s.mode1 = 2;
+        else if (num_ports <= 8)
+            pko_mode.s.mode1 = 1;
+        else
+            pko_mode.s.mode1 = 0;
+    }
+    cvmx_write_csr(CVMX_PKO_REG_GMX_PORT_MODE, pko_mode.u64);
+    return 0;
+}
+
+
+/**
+ * Returns the IPD/PKO port number for a port on teh given
+ * interface.
+ *
+ * @param interface Interface to use
+ * @param port      Port on the interface
+ *
+ * @return IPD/PKO port number
+ */
+int cvmx_helper_get_ipd_port(int interface, int port)
+{
+    switch (interface)
+    {
+        case 0: return port;
+        case 1: return port + 16;
+        case 2: return port + 32;
+        case 3: return port + 36;
+    }
+    return -1;
+}
+
+
+/**
+ * Returns the interface number for an IPD/PKO port number.
+ *
+ * @param ipd_port IPD/PKO port number
+ *
+ * @return Interface number
+ */
+int cvmx_helper_get_interface_num(int ipd_port)
+{
+    if (ipd_port < 16)
+        return 0;
+    else if (ipd_port < 32)
+        return 1;
+    else if (ipd_port < 36)
+        return 2;
+    else if (ipd_port < 40)
+        return 3;
+    else
+        cvmx_dprintf("cvmx_helper_get_interface_num: Illegal IPD port number\n");
+
+    return -1;
+}
+
+
+/**
+ * Returns the interface index number for an IPD/PKO port
+ * number.
+ *
+ * @param ipd_port IPD/PKO port number
+ *
+ * @return Interface index number
+ */
+int cvmx_helper_get_interface_index_num(int ipd_port)
+{
+    if (ipd_port < 32)
+        return ipd_port & 15;
+    else if (ipd_port < 36)
+        return ipd_port & 3;
+    else if (ipd_port < 40)
+        return ipd_port & 3;
+    else
+        cvmx_dprintf("cvmx_helper_get_interface_index_num: Illegal IPD port number\n");
+
+    return -1;
+}
+
+#endif /* CVMX_ENABLE_HELPER_FUNCTIONS */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-util.h b/arch/mips/cavium-octeon/executive/cvmx-helper-util.h
new file mode 100644
index 0000000..e846189
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-util.h
@@ -0,0 +1,220 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Small helper utilities.
+ *
+ * <hr>$Revision: 33191 $<hr>
+ */
+
+#ifndef __CVMX_HELPER_UTIL_H__
+#define __CVMX_HELPER_UTIL_H__
+
+
+/**
+ * Convert a interface mode into a human readable string
+ *
+ * @param mode   Mode to convert
+ *
+ * @return String
+ */
+extern const char *cvmx_helper_interface_mode_to_string(cvmx_helper_interface_mode_t mode);
+
+/**
+ * Debug routine to dump the packet structure to the console
+ *
+ * @param work   Work queue entry containing the packet to dump
+ * @return
+ */
+extern int cvmx_helper_dump_packet(cvmx_wqe_t *work);
+
+/**
+ * Setup Random Early Drop on a specific input queue
+ *
+ * @param queue  Input queue to setup RED on (0-7)
+ * @param pass_thresh
+ *               Packets will begin slowly dropping when there are less than
+ *               this many packet buffers free in FPA 0.
+ * @param drop_thresh
+ *               All incomming packets will be dropped when there are less
+ *               than this many free packet buffers in FPA 0.
+ * @return Zero on success. Negative on failure
+ */
+extern int cvmx_helper_setup_red_queue(int queue, int pass_thresh, int drop_thresh);
+
+/**
+ * Setup Random Early Drop to automatically begin dropping packets.
+ *
+ * @param pass_thresh
+ *               Packets will begin slowly dropping when there are less than
+ *               this many packet buffers free in FPA 0.
+ * @param drop_thresh
+ *               All incomming packets will be dropped when there are less
+ *               than this many free packet buffers in FPA 0.
+ * @return Zero on success. Negative on failure
+ */
+extern int cvmx_helper_setup_red(int pass_thresh, int drop_thresh);
+
+
+/**
+ * Get the version of the CVMX libraries.
+ *
+ * @return Version string. Note this buffer is allocated statically
+ *         and will be shared by all callers.
+ */
+extern const char *cvmx_helper_get_version(void);
+
+
+/**
+ * @INTERNAL
+ * Setup the common GMX settings that determine the number of
+ * ports. These setting apply to almost all configurations of all
+ * chips.
+ *
+ * @param interface Interface to configure
+ * @param num_ports Number of ports on the interface
+ *
+ * @return Zero on success, negative on failure
+ */
+extern int __cvmx_helper_setup_gmx(int interface, int num_ports);
+
+/**
+ * Returns the IPD/PKO port number for a port on the given
+ * interface.
+ *
+ * @param interface Interface to use
+ * @param port      Port on the interface
+ *
+ * @return IPD/PKO port number
+ */
+extern int cvmx_helper_get_ipd_port(int interface, int port);
+
+
+/**
+ * Returns the IPD/PKO port number for the first port on the given
+ * interface.
+ *
+ * @param interface Interface to use
+ *
+ * @return IPD/PKO port number
+ */
+static inline int cvmx_helper_get_first_ipd_port(int interface)
+{
+    return (cvmx_helper_get_ipd_port (interface, 0));
+}
+
+/**
+ * Returns the IPD/PKO port number for the last port on the given
+ * interface.
+ *
+ * @param interface Interface to use
+ *
+ * @return IPD/PKO port number
+ */
+static inline int cvmx_helper_get_last_ipd_port (int interface)
+{
+    extern int cvmx_helper_ports_on_interface (int interface);
+
+    return (cvmx_helper_get_first_ipd_port (interface) +
+  	    cvmx_helper_ports_on_interface (interface) - 1);
+}
+
+/**
+ * Returns the interface number for an IPD/PKO port number.
+ *
+ * @param ipd_port IPD/PKO port number
+ *
+ * @return Interface number
+ */
+extern int cvmx_helper_get_interface_num(int ipd_port);
+
+
+/**
+ * Returns the interface index number for an IPD/PKO port
+ * number.
+ *
+ * @param ipd_port IPD/PKO port number
+ *
+ * @return Interface index number
+ */
+extern int cvmx_helper_get_interface_index_num(int ipd_port);
+
+
+/**
+ * Free the packet buffers contained in a work queue entry.
+ * The work queue entry is not freed.
+ *
+ * @param work   Work queue entry with packet to free
+ */
+static inline void cvmx_helper_free_packet_data(cvmx_wqe_t *work)
+{
+    uint64_t        number_buffers;
+    cvmx_buf_ptr_t  buffer_ptr;
+    cvmx_buf_ptr_t  next_buffer_ptr;
+    uint64_t        start_of_buffer;
+
+    buffer_ptr = work->packet_ptr;
+    number_buffers = work->word2.s.bufs;
+
+    while (number_buffers--)
+    {
+        /* Remember the back pointer is in cache lines, not 64bit words */
+        start_of_buffer = ((buffer_ptr.s.addr >> 7) - buffer_ptr.s.back) << 7;
+        /* Read pointer to next buffer before we free the current buffer. */
+        next_buffer_ptr = *(cvmx_buf_ptr_t*)cvmx_phys_to_ptr(buffer_ptr.s.addr - 8);
+        cvmx_fpa_free(cvmx_phys_to_ptr(start_of_buffer), buffer_ptr.s.pool, 0);
+        buffer_ptr = next_buffer_ptr;
+    }
+}
+
+#endif  /* __CVMX_HELPER_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-xaui.c b/arch/mips/cavium-octeon/executive/cvmx-helper-xaui.c
new file mode 100644
index 0000000..585f2c1
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-xaui.c
@@ -0,0 +1,300 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Functions for XAUI initialization, configuration,
+ * and monitoring.
+ *
+ * <hr>$Revision: 34750 $<hr>
+ */
+#include "executive-config.h"
+#include "cvmx-config.h"
+#ifdef CVMX_ENABLE_PKO_FUNCTIONS
+
+#include "cvmx.h"
+#include "cvmx-helper.h"
+
+
+/**
+ * @INTERNAL
+ * Probe a XAUI interface and determine the number of ports
+ * connected to it. The XAUI interface should still be down
+ * after this call.
+ *
+ * @param interface Interface to probe
+ *
+ * @return Number of ports on the interface. Zero to disable.
+ */
+int __cvmx_helper_xaui_probe(int interface)
+{
+    int i;
+    cvmx_gmxx_hg2_control_t gmx_hg2_control;
+
+    __cvmx_helper_setup_gmx(interface, 1);
+
+    /* Setup PKO to support 16 ports for HiGig2 virtual ports. We're pointing
+        all of the PKO packet ports for this interface to the XAUI. This allows
+        us to use HiGig2 backpressure per port */
+    for (i=0; i<16; i++)
+    {
+        cvmx_pko_mem_port_ptrs_t pko_mem_port_ptrs;
+        pko_mem_port_ptrs.u64 = 0;
+        /* We set each PKO port to have equal priority in a round robin
+            fashion */
+        pko_mem_port_ptrs.s.static_p = 0;
+        pko_mem_port_ptrs.s.qos_mask = 0xff;
+        /* All PKO ports map to the same XAUI hardware port */
+        pko_mem_port_ptrs.s.eid = interface*4;
+        pko_mem_port_ptrs.s.pid = interface*16 + i;
+        cvmx_write_csr(CVMX_PKO_MEM_PORT_PTRS, pko_mem_port_ptrs.u64);
+    }
+
+    /* If HiGig2 is enabled return 16 ports, otherwise return 1 port */
+    gmx_hg2_control.u64 = cvmx_read_csr(CVMX_GMXX_HG2_CONTROL(interface));
+    if (gmx_hg2_control.s.hg2tx_en)
+        return 16;
+    else
+        return 1;
+}
+
+
+/**
+ * @INTERNAL
+ * Bringup and enable a XAUI interface. After this call packet
+ * I/O should be fully functional. This is called with IPD
+ * enabled but PKO disabled.
+ *
+ * @param interface Interface to bring up
+ *
+ * @return Zero on success, negative on failure
+ */
+int __cvmx_helper_xaui_enable(int interface)
+{
+    cvmx_gmxx_inf_mode_t          mode;
+    cvmx_gmxx_prtx_cfg_t          gmx_cfg;
+    cvmx_pcsxx_control1_reg_t     xauiCtl;
+    cvmx_pcsxx_misc_ctl_reg_t     xauiMiscCtl;
+    cvmx_gmxx_tx_xaui_ctl_t       gmxXauiTxCtl;
+    cvmx_gmxx_rxx_int_en_t        gmx_rx_int_en;
+    cvmx_gmxx_tx_int_en_t         gmx_tx_int_en;
+    cvmx_pcsxx_int_en_reg_t       pcsx_int_en_reg;
+
+    /* (1) Enable the interface. */
+    mode.u64 = cvmx_read_csr(CVMX_GMXX_INF_MODE(interface));
+    mode.s.en = 1;
+    cvmx_write_csr(CVMX_GMXX_INF_MODE(interface), mode.u64);
+
+    /* (2) Disable GMX. */
+    xauiMiscCtl.u64 = cvmx_read_csr(CVMX_PCSXX_MISC_CTL_REG(interface));
+    xauiMiscCtl.s.gmxeno = 1;
+    cvmx_write_csr (CVMX_PCSXX_MISC_CTL_REG(interface), xauiMiscCtl.u64);
+
+    /* (3) Disable GMX and PCSX interrupts. */
+    gmx_rx_int_en.u64 = cvmx_read_csr(CVMX_GMXX_RXX_INT_EN(0,interface));
+    cvmx_write_csr(CVMX_GMXX_RXX_INT_EN(0,interface), 0x0);
+    gmx_tx_int_en.u64 = cvmx_read_csr(CVMX_GMXX_TX_INT_EN(interface));
+    cvmx_write_csr(CVMX_GMXX_TX_INT_EN(interface), 0x0);
+    pcsx_int_en_reg.u64 = cvmx_read_csr(CVMX_PCSXX_INT_EN_REG(interface));
+    cvmx_write_csr(CVMX_PCSXX_INT_EN_REG(interface), 0x0);
+
+    /* (4) Bring up the PCSX and GMX reconciliation layer. */
+    /* (4)a Set polarity and lane swapping. */
+    /* (4)b */
+    gmxXauiTxCtl.u64 = cvmx_read_csr (CVMX_GMXX_TX_XAUI_CTL(interface));
+    gmxXauiTxCtl.s.dic_en = 0;
+    gmxXauiTxCtl.s.uni_en = 0;
+    cvmx_write_csr (CVMX_GMXX_TX_XAUI_CTL(interface), gmxXauiTxCtl.u64);
+
+    /* (4)c Aply reset sequence */
+    xauiCtl.u64 = cvmx_read_csr (CVMX_PCSXX_CONTROL1_REG(interface));
+    xauiCtl.s.lo_pwr = 0;
+    xauiCtl.s.reset  = 1;
+    cvmx_write_csr (CVMX_PCSXX_CONTROL1_REG(interface), xauiCtl.u64);
+
+    /* Wait for PCS to come out of reset */
+    if (CVMX_WAIT_FOR_FIELD64(CVMX_PCSXX_CONTROL1_REG(interface), cvmx_pcsxx_control1_reg_t, reset, ==, 0, 10000))
+        return -1;
+    /* Wait for PCS to be aligned */
+    if (CVMX_WAIT_FOR_FIELD64(CVMX_PCSXX_10GBX_STATUS_REG(interface), cvmx_pcsxx_10gbx_status_reg_t, alignd, ==, 1, 10000))
+        return -1;
+    /* Wait for RX to be ready */
+    if (CVMX_WAIT_FOR_FIELD64(CVMX_GMXX_RX_XAUI_CTL(interface), cvmx_gmxx_rx_xaui_ctl_t, status, ==, 0, 10000))
+        return -1;
+
+    /* (6) Configure GMX */
+    gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(0, interface));
+    gmx_cfg.s.en = 0;
+    cvmx_write_csr(CVMX_GMXX_PRTX_CFG(0, interface), gmx_cfg.u64);
+
+    /* Wait for GMX RX to be idle */
+    if (CVMX_WAIT_FOR_FIELD64(CVMX_GMXX_PRTX_CFG(0, interface), cvmx_gmxx_prtx_cfg_t, rx_idle, ==, 1, 10000))
+        return -1;
+    /* Wait for GMX TX to be idle */
+    if (CVMX_WAIT_FOR_FIELD64(CVMX_GMXX_PRTX_CFG(0, interface), cvmx_gmxx_prtx_cfg_t, tx_idle, ==, 1, 10000))
+        return -1;
+
+    /* GMX configure */
+    gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(0, interface));
+    gmx_cfg.s.speed = 1;
+    gmx_cfg.s.speed_msb = 0;
+    gmx_cfg.s.slottime = 1;
+    cvmx_write_csr(CVMX_GMXX_TX_PRTS(interface), 1);
+    cvmx_write_csr(CVMX_GMXX_TXX_SLOT(0, interface), 512);
+    cvmx_write_csr(CVMX_GMXX_TXX_BURST(0, interface), 8192);
+    cvmx_write_csr(CVMX_GMXX_PRTX_CFG(0, interface), gmx_cfg.u64);
+
+    /* (7) Clear out any error state */
+    cvmx_write_csr(CVMX_GMXX_RXX_INT_REG(0,interface), cvmx_read_csr(CVMX_GMXX_RXX_INT_REG(0,interface)));
+    cvmx_write_csr(CVMX_GMXX_TX_INT_REG(interface), cvmx_read_csr(CVMX_GMXX_TX_INT_REG(interface)));
+    cvmx_write_csr(CVMX_PCSXX_INT_REG(interface), cvmx_read_csr(CVMX_PCSXX_INT_REG(interface)));
+
+    /* Wait for receive link */
+    if (CVMX_WAIT_FOR_FIELD64(CVMX_PCSXX_STATUS1_REG(interface), cvmx_pcsxx_status1_reg_t, rcv_lnk, ==, 1, 10000))
+        return -1;
+    if (CVMX_WAIT_FOR_FIELD64(CVMX_PCSXX_STATUS2_REG(interface), cvmx_pcsxx_status2_reg_t, xmtflt, ==, 0, 10000))
+        return -1;
+    if (CVMX_WAIT_FOR_FIELD64(CVMX_PCSXX_STATUS2_REG(interface), cvmx_pcsxx_status2_reg_t, rcvflt, ==, 0, 10000))
+        return -1;
+
+    cvmx_write_csr(CVMX_GMXX_RXX_INT_EN(0,interface), gmx_rx_int_en.u64);
+    cvmx_write_csr(CVMX_GMXX_TX_INT_EN(interface), gmx_tx_int_en.u64);
+    cvmx_write_csr(CVMX_PCSXX_INT_EN_REG(interface), pcsx_int_en_reg.u64);
+
+    cvmx_helper_link_autoconf(cvmx_helper_get_ipd_port(interface, 0));
+
+    /* (8) Enable packet reception */
+    xauiMiscCtl.s.gmxeno = 0;
+    cvmx_write_csr (CVMX_PCSXX_MISC_CTL_REG(interface), xauiMiscCtl.u64);
+
+    gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(0, interface));
+    gmx_cfg.s.en = 1;
+    cvmx_write_csr(CVMX_GMXX_PRTX_CFG(0, interface), gmx_cfg.u64);
+    return 0;
+}
+
+/**
+ * @INTERNAL
+ * Return the link state of an IPD/PKO port as returned by
+ * auto negotiation. The result of this function may not match
+ * Octeon's link config if auto negotiation has changed since
+ * the last call to cvmx_helper_link_set().
+ *
+ * @param ipd_port IPD/PKO port to query
+ *
+ * @return Link state
+ */
+cvmx_helper_link_info_t __cvmx_helper_xaui_link_get(int ipd_port)
+{
+    int interface = cvmx_helper_get_interface_num(ipd_port);
+    cvmx_gmxx_tx_xaui_ctl_t gmxx_tx_xaui_ctl;
+    cvmx_gmxx_rx_xaui_ctl_t gmxx_rx_xaui_ctl;
+    cvmx_helper_link_info_t result;
+
+    gmxx_tx_xaui_ctl.u64 = cvmx_read_csr(CVMX_GMXX_TX_XAUI_CTL(interface));
+    gmxx_rx_xaui_ctl.u64 = cvmx_read_csr(CVMX_GMXX_RX_XAUI_CTL(interface));
+    result.u64 = 0;
+
+    /* Only return a link if both RX and TX are happy */
+    if ((gmxx_tx_xaui_ctl.s.ls == 0) && (gmxx_rx_xaui_ctl.s.status == 0))
+    {
+        result.s.link_up = 1;
+        result.s.full_duplex = 1;
+        result.s.speed = 10000;
+    }
+    else
+    {
+        /* Disable GMX and PCSX interrupts. */
+        cvmx_write_csr (CVMX_GMXX_RXX_INT_EN(0,interface), 0x0);
+        cvmx_write_csr (CVMX_GMXX_TX_INT_EN(interface), 0x0);
+        cvmx_write_csr (CVMX_PCSXX_INT_EN_REG(interface), 0x0);
+    }
+    return result;
+}
+
+
+/**
+ * @INTERNAL
+ * Configure an IPD/PKO port for the specified link state. This
+ * function does not influence auto negotiation at the PHY level.
+ * The passed link state must always match the link state returned
+ * by cvmx_helper_link_get(). It is normally best to use
+ * cvmx_helper_link_autoconf() instead.
+ *
+ * @param ipd_port  IPD/PKO port to configure
+ * @param link_info The new link state
+ *
+ * @return Zero on success, negative on failure
+ */
+int __cvmx_helper_xaui_link_set(int ipd_port, cvmx_helper_link_info_t link_info)
+{
+    int interface = cvmx_helper_get_interface_num(ipd_port);
+    cvmx_gmxx_tx_xaui_ctl_t gmxx_tx_xaui_ctl;
+    cvmx_gmxx_rx_xaui_ctl_t gmxx_rx_xaui_ctl;
+
+    gmxx_tx_xaui_ctl.u64 = cvmx_read_csr(CVMX_GMXX_TX_XAUI_CTL(interface));
+    gmxx_rx_xaui_ctl.u64 = cvmx_read_csr(CVMX_GMXX_RX_XAUI_CTL(interface));
+
+    /* If the link shouldn't be up, then just return */
+    if (!link_info.s.link_up)
+        return 0;
+
+    /* Do nothing if both RX and TX are happy */
+    if ((gmxx_tx_xaui_ctl.s.ls == 0) && (gmxx_rx_xaui_ctl.s.status == 0))
+        return 0;
+
+    /* Bring the link up */
+    return __cvmx_helper_xaui_enable(interface);
+}
+
+#endif /* CVMX_ENABLE_PKO_FUNCTIONS */
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-xaui.h b/arch/mips/cavium-octeon/executive/cvmx-helper-xaui.h
new file mode 100644
index 0000000..4b1ac9d
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-xaui.h
@@ -0,0 +1,116 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Functions for XAUI initialization, configuration,
+ * and monitoring.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+#ifndef __CVMX_HELPER_XAUI_H__
+#define __CVMX_HELPER_XAUI_H__
+
+/**
+ * @INTERNAL
+ * Probe a XAUI interface and determine the number of ports
+ * connected to it. The XAUI interface should still be down
+ * after this call.
+ *
+ * @param interface Interface to probe
+ *
+ * @return Number of ports on the interface. Zero to disable.
+ */
+extern int __cvmx_helper_xaui_probe(int interface);
+
+/**
+ * @INTERNAL
+ * Bringup and enable a XAUI interface. After this call packet
+ * I/O should be fully functional. This is called with IPD
+ * enabled but PKO disabled.
+ *
+ * @param interface Interface to bring up
+ *
+ * @return Zero on success, negative on failure
+ */
+extern int __cvmx_helper_xaui_enable(int interface);
+
+/**
+ * @INTERNAL
+ * Return the link state of an IPD/PKO port as returned by
+ * auto negotiation. The result of this function may not match
+ * Octeon's link config if auto negotiation has changed since
+ * the last call to cvmx_helper_link_set().
+ *
+ * @param ipd_port IPD/PKO port to query
+ *
+ * @return Link state
+ */
+extern cvmx_helper_link_info_t __cvmx_helper_xaui_link_get(int ipd_port);
+
+/**
+ * @INTERNAL
+ * Configure an IPD/PKO port for the specified link state. This
+ * function does not influence auto negotiation at the PHY level.
+ * The passed link state must always match the link state returned
+ * by cvmx_helper_link_get(). It is normally best to use
+ * cvmx_helper_link_autoconf() instead.
+ *
+ * @param ipd_port  IPD/PKO port to configure
+ * @param link_info The new link state
+ *
+ * @return Zero on success, negative on failure
+ */
+extern int __cvmx_helper_xaui_link_set(int ipd_port, cvmx_helper_link_info_t link_info);
+
+#endif
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper.c b/arch/mips/cavium-octeon/executive/cvmx-helper.c
new file mode 100644
index 0000000..db28f7d
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper.c
@@ -0,0 +1,768 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Helper functions for common, but complicated tasks.
+ *
+ * <hr>$Revision: 35352 $<hr>
+ */
+#include "executive-config.h"
+#include "cvmx-config.h"
+#include "cvmx.h"
+#include "cvmx-bootmem.h"
+#include "cvmx-fpa.h"
+#include "cvmx-pip.h"
+#include "cvmx-pko.h"
+#include "cvmx-ipd.h"
+#include "cvmx-asx.h"
+#include "cvmx-gmx.h"
+#include "cvmx-spi.h"
+#include "cvmx-sysinfo.h"
+#include "cvmx-helper.h"
+#include "cvmx-version.h"
+#include "cvmx-helper-check-defines.h"
+#include "cvmx-helper-board.h"
+
+#ifdef CVMX_ENABLE_PKO_FUNCTIONS
+
+/**
+ * cvmx_override_pko_queue_priority(int ipd_port, uint64_t
+ * priorities[16]) is a function pointer. It is meant to allow
+ * customization of the PKO queue priorities based on the port
+ * number. Users should set this pointer to a function before
+ * calling any cvmx-helper operations.
+ */
+CVMX_SHARED void (*cvmx_override_pko_queue_priority)(int pko_port, uint64_t priorities[16]) = NULL;
+
+/**
+ * cvmx_override_ipd_port_setup(int ipd_port) is a function
+ * pointer. It is meant to allow customization of the IPD port
+ * setup before packet input/output comes online. It is called
+ * after cvmx-helper does the default IPD configuration, but
+ * before IPD is enabled. Users should set this pointer to a
+ * function before calling any cvmx-helper operations.
+ */
+CVMX_SHARED void (*cvmx_override_ipd_port_setup)(int ipd_port) = NULL;
+
+/* Port count per interface */
+static CVMX_SHARED int interface_port_count[4] = {0,0,0,0};
+/* Port last configured link info index by IPD/PKO port */
+static CVMX_SHARED cvmx_helper_link_info_t port_link_info[CVMX_PIP_NUM_INPUT_PORTS];
+
+
+/**
+ * Return the number of interfaces the chip has. Each interface
+ * may have multiple ports. Most chips support two interfaces,
+ * but the CNX0XX and CNX1XX are exceptions. These only support
+ * one interface.
+ *
+ * @return Number of interfaces on chip
+ */
+int cvmx_helper_get_number_of_interfaces(void)
+{
+    if (OCTEON_IS_MODEL(OCTEON_CN56XX) || OCTEON_IS_MODEL(OCTEON_CN52XX))
+        return 4;
+    else
+        return 3;
+}
+
+
+/**
+ * Return the number of ports on an interface. Depending on the
+ * chip and configuration, this can be 1-16. A value of 0
+ * specifies that the interface doesn't exist or isn't usable.
+ *
+ * @param interface Interface to get the port count for
+ *
+ * @return Number of ports on interface. Can be Zero.
+ */
+int cvmx_helper_ports_on_interface(int interface)
+{
+    return interface_port_count[interface];
+}
+
+
+/**
+ * Get the operating mode of an interface. Depending on the Octeon
+ * chip and configuration, this function returns an enumeration
+ * of the type of packet I/O supported by an interface.
+ *
+ * @param interface Interface to probe
+ *
+ * @return Mode of the interface. Unknown or unsupported interfaces return
+ *         DISABLED.
+ */
+cvmx_helper_interface_mode_t cvmx_helper_interface_get_mode(int interface)
+{
+    cvmx_gmxx_inf_mode_t mode;
+    if (interface == 2)
+        return CVMX_HELPER_INTERFACE_MODE_NPI;
+
+    if (interface == 3)
+    {
+        if (OCTEON_IS_MODEL(OCTEON_CN56XX) || OCTEON_IS_MODEL(OCTEON_CN52XX))
+            return CVMX_HELPER_INTERFACE_MODE_LOOP;
+        else
+            return CVMX_HELPER_INTERFACE_MODE_DISABLED;
+    }
+
+    if (interface == 0 && cvmx_sysinfo_get()->board_type == CVMX_BOARD_TYPE_CN3005_EVB_HS5 && cvmx_sysinfo_get()->board_rev_major == 1)
+    {
+        /* Lie about interface type of CN3005 board.  This board has a switch on port 1 like
+        ** the other evaluation boards, but it is connected over RGMII instead of GMII.  Report
+        ** GMII mode so that the speed is forced to 1 Gbit full duplex.  Other than some initial configuration
+        ** (which does not use the output of this function) there is no difference in setup between GMII and RGMII modes.
+        */
+        return CVMX_HELPER_INTERFACE_MODE_GMII;
+    }
+
+    /* Interface 1 is always disabled on CN31XX and CN30XX */
+    if ((interface == 1) && (OCTEON_IS_MODEL(OCTEON_CN31XX) || OCTEON_IS_MODEL(OCTEON_CN30XX) || OCTEON_IS_MODEL(OCTEON_CN50XX) || OCTEON_IS_MODEL(OCTEON_CN52XX)))
+        return CVMX_HELPER_INTERFACE_MODE_DISABLED;
+
+    mode.u64 = cvmx_read_csr(CVMX_GMXX_INF_MODE(interface));
+
+    if (OCTEON_IS_MODEL(OCTEON_CN56XX) || OCTEON_IS_MODEL(OCTEON_CN52XX))
+    {
+        switch(mode.cn56xx.mode)
+        {
+            case 0: return CVMX_HELPER_INTERFACE_MODE_DISABLED;
+            case 1: return CVMX_HELPER_INTERFACE_MODE_XAUI;
+            case 2: return CVMX_HELPER_INTERFACE_MODE_SGMII;
+            case 3: return CVMX_HELPER_INTERFACE_MODE_PICMG;
+            default:return CVMX_HELPER_INTERFACE_MODE_DISABLED;
+        }
+    }
+    else
+    {
+        if (!mode.s.en)
+            return CVMX_HELPER_INTERFACE_MODE_DISABLED;
+
+        if (mode.s.type)
+        {
+            if (OCTEON_IS_MODEL(OCTEON_CN38XX) || OCTEON_IS_MODEL(OCTEON_CN58XX))
+                return CVMX_HELPER_INTERFACE_MODE_SPI;
+            else
+                return CVMX_HELPER_INTERFACE_MODE_GMII;
+        }
+        else
+            return CVMX_HELPER_INTERFACE_MODE_RGMII;
+    }
+}
+
+
+/**
+ * @INTERNAL
+ * Configure the IPD/PIP tagging and QoS options for a specific
+ * port. This function determines the POW work queue entry
+ * contents for a port. The setup performed here is controlled by
+ * the defines in executive-config.h.
+ *
+ * @param ipd_port Port to configure. This follows the IPD numbering, not the
+ *                 per interface numbering
+ *
+ * @return Zero on success, negative on failure
+ */
+static int __cvmx_helper_port_setup_ipd(int ipd_port)
+{
+    cvmx_pip_port_cfg_t port_config;
+    cvmx_pip_port_tag_cfg_t tag_config;
+
+    port_config.u64 = cvmx_read_csr(CVMX_PIP_PRT_CFGX(ipd_port));
+    tag_config.u64 = cvmx_read_csr(CVMX_PIP_PRT_TAGX(ipd_port));
+
+    /* Have each port go to a different POW queue */
+    port_config.s.qos = ipd_port & 0x7;
+
+    /* Process the headers and place the IP header in the work queue */
+    port_config.s.mode = CVMX_HELPER_INPUT_PORT_SKIP_MODE;
+
+    tag_config.s.ip6_src_flag  = CVMX_HELPER_INPUT_TAG_IPV6_SRC_IP;
+    tag_config.s.ip6_dst_flag  = CVMX_HELPER_INPUT_TAG_IPV6_DST_IP;
+    tag_config.s.ip6_sprt_flag = CVMX_HELPER_INPUT_TAG_IPV6_SRC_PORT;
+    tag_config.s.ip6_dprt_flag = CVMX_HELPER_INPUT_TAG_IPV6_DST_PORT;
+    tag_config.s.ip6_nxth_flag = CVMX_HELPER_INPUT_TAG_IPV6_NEXT_HEADER;
+    tag_config.s.ip4_src_flag  = CVMX_HELPER_INPUT_TAG_IPV4_SRC_IP;
+    tag_config.s.ip4_dst_flag  = CVMX_HELPER_INPUT_TAG_IPV4_DST_IP;
+    tag_config.s.ip4_sprt_flag = CVMX_HELPER_INPUT_TAG_IPV4_SRC_PORT;
+    tag_config.s.ip4_dprt_flag = CVMX_HELPER_INPUT_TAG_IPV4_DST_PORT;
+    tag_config.s.ip4_pctl_flag = CVMX_HELPER_INPUT_TAG_IPV4_PROTOCOL;
+    tag_config.s.inc_prt_flag  = CVMX_HELPER_INPUT_TAG_INPUT_PORT;
+    tag_config.s.tcp6_tag_type = CVMX_HELPER_INPUT_TAG_TYPE;
+    tag_config.s.tcp4_tag_type = CVMX_HELPER_INPUT_TAG_TYPE;
+    tag_config.s.ip6_tag_type = CVMX_HELPER_INPUT_TAG_TYPE;
+    tag_config.s.ip4_tag_type = CVMX_HELPER_INPUT_TAG_TYPE;
+    tag_config.s.non_tag_type = CVMX_HELPER_INPUT_TAG_TYPE;
+    /* Put all packets in group 0. Other groups can be used by the app */
+    tag_config.s.grp = 0;
+
+    cvmx_pip_config_port(ipd_port, port_config, tag_config);
+
+    /* Give the user a chance to override our setting for each port */
+    if (cvmx_override_ipd_port_setup)
+        cvmx_override_ipd_port_setup(ipd_port);
+
+    return 0;
+}
+
+
+/**
+ * This function probes an interface to determine the actual
+ * number of hardware ports connected to it. It doesn't setup the
+ * ports or enable them. The main goal here is to set the global
+ * interface_port_count[interface] correctly. Hardware setup of the
+ * ports will be performed later.
+ *
+ * @param interface Interface to probe
+ *
+ * @return Zero on success, negative on failure
+ */
+int cvmx_helper_interface_probe(int interface)
+{
+    /* At this stage in the game we don't want packets to be moving yet.
+        The following probe calls should perform hardware setup
+        needed to determine port counts. Receive must still be disabled */
+    switch (cvmx_helper_interface_get_mode(interface))
+    {
+        /* These types don't support ports to IPD/PKO */
+        case CVMX_HELPER_INTERFACE_MODE_DISABLED:
+        case CVMX_HELPER_INTERFACE_MODE_PCIE:
+            interface_port_count[interface] = 0;
+            break;
+        /* XAUI is a single high speed port */
+        case CVMX_HELPER_INTERFACE_MODE_XAUI:
+            interface_port_count[interface] = __cvmx_helper_xaui_probe(interface);
+            break;
+        /* RGMII/GMII/MII are all treated about the same. Most functions
+            refer to these ports as RGMII */
+        case CVMX_HELPER_INTERFACE_MODE_RGMII:
+        case CVMX_HELPER_INTERFACE_MODE_GMII:
+            interface_port_count[interface] = __cvmx_helper_rgmii_probe(interface);
+            break;
+        /* SPI4 can have 1-16 ports depending on the device at the other end */
+        case CVMX_HELPER_INTERFACE_MODE_SPI:
+            interface_port_count[interface] = __cvmx_helper_spi_probe(interface);
+            break;
+        /* SGMII can have 1-4 ports depending on how many are hooked up */
+        case CVMX_HELPER_INTERFACE_MODE_SGMII:
+        case CVMX_HELPER_INTERFACE_MODE_PICMG:
+            interface_port_count[interface] = __cvmx_helper_sgmii_probe(interface);
+            break;
+        /* PCI target Network Packet Interface */
+        case CVMX_HELPER_INTERFACE_MODE_NPI:
+            interface_port_count[interface] = __cvmx_helper_npi_probe(interface);
+            break;
+        /* Special loopback only ports. These are not the same as other ports
+            in loopback mode */
+        case CVMX_HELPER_INTERFACE_MODE_LOOP:
+            interface_port_count[interface] = __cvmx_helper_loop_probe(interface);
+            break;
+    }
+
+    interface_port_count[interface] = __cvmx_helper_board_interface_probe(interface, interface_port_count[interface]);
+
+    /* Make sure all global variables propagate to other cores */
+    CVMX_SYNCWS;
+
+    return 0;
+}
+
+
+/**
+ * @INTERNAL
+ * Setup the IPD/PIP for the ports on an interface. Packet
+ * classification and tagging are set for every port on the
+ * interface. The number of ports on the interface must already
+ * have been probed.
+ *
+ * @param interface Interface to setup IPD/PIP for
+ *
+ * @return Zero on success, negative on failure
+ */
+static int __cvmx_helper_interface_setup_ipd(int interface)
+{
+    int ipd_port = cvmx_helper_get_ipd_port(interface, 0);
+    int num_ports = interface_port_count[interface];
+    /* Disable FCS stripping for LOOP interface */
+    if (cvmx_helper_interface_get_mode(interface) == CVMX_HELPER_INTERFACE_MODE_LOOP)
+	__cvmx_helper_loop_disable_fcs_stripping(interface);
+
+    while (num_ports--)
+    {
+        __cvmx_helper_port_setup_ipd(ipd_port);
+        ipd_port++;
+    }
+    return 0;
+}
+
+
+/**
+ * @INTERNAL
+ * Setup global setting for IPD/PIP not related to a specific
+ * interface or port. This must be called before IPD is enabled.
+ *
+ * @return Zero on success, negative on failure.
+ */
+static int __cvmx_helper_global_setup_ipd(void)
+{
+    /* Setup the global packet input options */
+    cvmx_ipd_config(CVMX_FPA_PACKET_POOL_SIZE/8,
+                    CVMX_HELPER_FIRST_MBUFF_SKIP/8,
+                    CVMX_HELPER_NOT_FIRST_MBUFF_SKIP/8,
+                    (CVMX_HELPER_FIRST_MBUFF_SKIP+8) / 128, /* The +8 is to account for the next ptr */
+                    (CVMX_HELPER_NOT_FIRST_MBUFF_SKIP+8) / 128, /* The +8 is to account for the next ptr */
+                    CVMX_FPA_WQE_POOL,
+                    CVMX_IPD_OPC_MODE_STT,
+                    CVMX_HELPER_ENABLE_BACK_PRESSURE);
+    return 0;
+}
+
+
+/**
+ * @INTERNAL
+ * Setup the PKO for the ports on an interface. The number of
+ * queues per port and the priority of each PKO output queue
+ * is set here. PKO must be disabled when this function is called.
+ *
+ * @param interface Interface to setup PKO for
+ *
+ * @return Zero on success, negative on failure
+ */
+static int __cvmx_helper_interface_setup_pko(int interface)
+{
+    /* Each packet output queue has an associated priority. The higher the
+        priority, the more often it can send a packet. A priority of 8 means
+        it can send in all 8 rounds of contention. We're going to make each
+        queue one less than the last.
+        The vector of priorities has been extended to support CN5xxx CPUs,
+        where up to 16 queues can be associated to a port.
+        To keep backward compatibility we don't change the initial 8
+        priorities and replicate them in the second half.
+        With per-core PKO queues (PKO lockless operation) all queues have
+        the same priority. */
+#ifdef CVMX_PKO_LOCKLESS_OPERATION
+    uint64_t priorities[16] = {8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8};
+#else
+    uint64_t priorities[16] = {8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1};
+#endif
+
+    /* Setup the IPD/PIP and PKO for the ports discovered above. Here packet
+        classification, tagging and output priorities are set */
+    int ipd_port = cvmx_helper_get_ipd_port(interface, 0);
+    int num_ports = interface_port_count[interface];
+    while (num_ports--)
+    {
+        /* Give the user a chance to override the per queue priorities */
+        if (cvmx_override_pko_queue_priority)
+            cvmx_override_pko_queue_priority(ipd_port, priorities);
+
+        cvmx_pko_config_port(ipd_port, cvmx_pko_get_base_queue_per_core(ipd_port, 0),
+                             cvmx_pko_get_num_queues(ipd_port), priorities);
+        ipd_port++;
+    }
+    return 0;
+}
+
+
+/**
+ * @INTERNAL
+ * Setup global setting for PKO not related to a specific
+ * interface or port. This must be called before PKO is enabled.
+ *
+ * @return Zero on success, negative on failure.
+ */
+static int __cvmx_helper_global_setup_pko(void)
+{
+    /* Disable tagwait FAU timeout. This needs to be done before anyone might
+        start packet output using tags */
+    cvmx_iob_fau_timeout_t fau_to;
+    fau_to.u64 = 0;
+    fau_to.s.tout_val = 0xfff;
+    fau_to.s.tout_enb = 0;
+    cvmx_write_csr(CVMX_IOB_FAU_TIMEOUT, fau_to.u64);
+    return 0;
+}
+
+
+/**
+ * @INTERNAL
+ * Setup global backpressure setting.
+ *
+ * @return Zero on success, negative on failure
+ */
+static int __cvmx_helper_global_setup_backpressure(void)
+{
+#if CVMX_HELPER_DISABLE_RGMII_BACKPRESSURE
+    /* Disable backpressure if configured to do so */
+    /* Disable backpressure (pause frame) generation */
+    int num_interfaces = cvmx_helper_get_number_of_interfaces();
+    int interface;
+    for (interface=0; interface<num_interfaces; interface++)
+    {
+        switch (cvmx_helper_interface_get_mode(interface))
+        {
+            case CVMX_HELPER_INTERFACE_MODE_DISABLED:
+            case CVMX_HELPER_INTERFACE_MODE_PCIE:
+            case CVMX_HELPER_INTERFACE_MODE_NPI:
+            case CVMX_HELPER_INTERFACE_MODE_LOOP:
+            case CVMX_HELPER_INTERFACE_MODE_XAUI:
+                break;
+            case CVMX_HELPER_INTERFACE_MODE_RGMII:
+            case CVMX_HELPER_INTERFACE_MODE_GMII:
+            case CVMX_HELPER_INTERFACE_MODE_SPI:
+            case CVMX_HELPER_INTERFACE_MODE_SGMII:
+            case CVMX_HELPER_INTERFACE_MODE_PICMG:
+                cvmx_gmx_set_backpressure_override(interface, 0xf);
+                break;
+        }
+    }
+    //cvmx_dprintf("Disabling backpressure\n");
+#endif
+
+    return 0;
+}
+
+
+/**
+ * @INTERNAL
+ * Enable packet input/output from the hardware. This function is
+ * called after all internal setup is complete and IPD is enabled.
+ * After this function completes, packets will be accepted from the
+ * hardware ports. PKO should still be disabled to make sure packets
+ * aren't sent out partially setup hardware.
+ *
+ * @param interface Interface to enable
+ *
+ * @return Zero on success, negative on failure
+ */
+static int __cvmx_helper_packet_hardware_enable(int interface)
+{
+    int result = 0;
+    switch (cvmx_helper_interface_get_mode(interface))
+    {
+        /* These types don't support ports to IPD/PKO */
+        case CVMX_HELPER_INTERFACE_MODE_DISABLED:
+        case CVMX_HELPER_INTERFACE_MODE_PCIE:
+            /* Nothing to do */
+            break;
+        /* XAUI is a single high speed port */
+        case CVMX_HELPER_INTERFACE_MODE_XAUI:
+            result = __cvmx_helper_xaui_enable(interface);
+            break;
+        /* RGMII/GMII/MII are all treated about the same. Most functions
+            refer to these ports as RGMII */
+        case CVMX_HELPER_INTERFACE_MODE_RGMII:
+        case CVMX_HELPER_INTERFACE_MODE_GMII:
+            result = __cvmx_helper_rgmii_enable(interface);
+            break;
+        /* SPI4 can have 1-16 ports depending on the device at the other end */
+        case CVMX_HELPER_INTERFACE_MODE_SPI:
+            result = __cvmx_helper_spi_enable(interface);
+            break;
+        /* SGMII can have 1-4 ports depending on how many are hooked up */
+        case CVMX_HELPER_INTERFACE_MODE_SGMII:
+        case CVMX_HELPER_INTERFACE_MODE_PICMG:
+            result = __cvmx_helper_sgmii_enable(interface);
+            break;
+        /* PCI target Network Packet Interface */
+        case CVMX_HELPER_INTERFACE_MODE_NPI:
+            result = __cvmx_helper_npi_enable(interface);
+            break;
+        /* Special loopback only ports. These are not the same as other ports
+            in loopback mode */
+        case CVMX_HELPER_INTERFACE_MODE_LOOP:
+            result = __cvmx_helper_loop_enable(interface);
+            break;
+    }
+    result |= __cvmx_helper_board_hardware_enable(interface);
+    return result;
+}
+
+
+/**
+ * Called after all internal packet IO paths are setup. This
+ * function enables IPD/PIP and begins packet input and output.
+ *
+ * @return Zero on success, negative on failure
+ */
+int cvmx_helper_ipd_and_packet_input_enable(void)
+{
+    int num_interfaces;
+    int interface;
+
+    /* Enable IPD */
+    cvmx_ipd_enable();
+
+    /* Time to enable hardware ports packet input and output. Note that at this
+        point IPD/PIP must be fully functional and PKO must be disabled */
+    num_interfaces = cvmx_helper_get_number_of_interfaces();
+    for (interface=0; interface<num_interfaces; interface++)
+    {
+        if (cvmx_helper_ports_on_interface(interface) > 0)
+        {
+            //cvmx_dprintf("Enabling packet I/O on interface %d\n", interface);
+            __cvmx_helper_packet_hardware_enable(interface);
+        }
+    }
+
+    /* Finally enable PKO now that the entire path is up and running */
+    cvmx_pko_enable();
+
+    if ((OCTEON_IS_MODEL(OCTEON_CN31XX_PASS1) || OCTEON_IS_MODEL(OCTEON_CN30XX_PASS1)) &&
+        (cvmx_sysinfo_get()->board_type != CVMX_BOARD_TYPE_SIM))
+        __cvmx_helper_errata_fix_ipd_ptr_alignment();
+    return 0;
+}
+
+
+/**
+ * Initialize the PIP, IPD, and PKO hardware to support
+ * simple priority based queues for the ethernet ports. Each
+ * port is configured with a number of priority queues based
+ * on CVMX_PKO_QUEUES_PER_PORT_* where each queue is lower
+ * priority than the previous.
+ *
+ * @return Zero on success, non-zero on failure
+ */
+int cvmx_helper_initialize_packet_io_global(void)
+{
+    int result = 0;
+    int interface;
+    const int num_interfaces = cvmx_helper_get_number_of_interfaces();
+
+    cvmx_pko_initialize_global();
+    for (interface=0; interface<num_interfaces; interface++)
+    {
+        result |= cvmx_helper_interface_probe(interface);
+        if (cvmx_helper_ports_on_interface(interface) > 0)
+            cvmx_dprintf("Interface %d has %d ports (%s)\n",
+                     interface, cvmx_helper_ports_on_interface(interface),
+                     cvmx_helper_interface_mode_to_string(cvmx_helper_interface_get_mode(interface)));
+        result |= __cvmx_helper_interface_setup_ipd(interface);
+        result |= __cvmx_helper_interface_setup_pko(interface);
+    }
+
+    result |= __cvmx_helper_global_setup_ipd();
+    result |= __cvmx_helper_global_setup_pko();
+
+    /* Enable any flow control and backpressure */
+    result |= __cvmx_helper_global_setup_backpressure();
+
+#if CVMX_HELPER_ENABLE_IPD
+    result |= cvmx_helper_ipd_and_packet_input_enable();
+#endif
+    return result;
+}
+
+
+/**
+ * Does core local initialization for packet io
+ *
+ * @return Zero on success, non-zero on failure
+ */
+int cvmx_helper_initialize_packet_io_local(void)
+{
+    return cvmx_pko_initialize_local();
+}
+
+
+/**
+ * Auto configure an IPD/PKO port link state and speed. This
+ * function basically does the equivalent of:
+ * cvmx_helper_link_set(ipd_port, cvmx_helper_link_get(ipd_port));
+ *
+ * @param ipd_port IPD/PKO port to auto configure
+ *
+ * @return Link state after configure
+ */
+cvmx_helper_link_info_t cvmx_helper_link_autoconf(int ipd_port)
+{
+    cvmx_helper_link_info_t link_info;
+    int interface = cvmx_helper_get_interface_num(ipd_port);
+    int index = cvmx_helper_get_interface_index_num(ipd_port);
+
+    if (index >= cvmx_helper_ports_on_interface(interface))
+    {
+        link_info.u64 = 0;
+        return link_info;
+    }
+
+    link_info = cvmx_helper_link_get(ipd_port);
+    if (link_info.u64 ==  port_link_info[ipd_port].u64)
+        return link_info;
+
+    /* If we fail to set the link speed, port_link_info will not change */
+    cvmx_helper_link_set(ipd_port, link_info);
+
+    /* port_link_info should be the current value, which will be different
+        than expect if cvmx_helper_link_set() failed */
+    return port_link_info[ipd_port];
+}
+
+
+/**
+ * Return the link state of an IPD/PKO port as returned by
+ * auto negotiation. The result of this function may not match
+ * Octeon's link config if auto negotiation has changed since
+ * the last call to cvmx_helper_link_set().
+ *
+ * @param ipd_port IPD/PKO port to query
+ *
+ * @return Link state
+ */
+cvmx_helper_link_info_t cvmx_helper_link_get(int ipd_port)
+{
+    cvmx_helper_link_info_t result;
+    int interface = cvmx_helper_get_interface_num(ipd_port);
+    int index = cvmx_helper_get_interface_index_num(ipd_port);
+
+    /* The default result will be a down link unless the code below
+        changes it */
+    result.u64 = 0;
+
+    if (index >= cvmx_helper_ports_on_interface(interface))
+        return result;
+
+    switch (cvmx_helper_interface_get_mode(interface))
+    {
+        case CVMX_HELPER_INTERFACE_MODE_DISABLED:
+        case CVMX_HELPER_INTERFACE_MODE_PCIE:
+            /* Network links are not supported */
+            break;
+        case CVMX_HELPER_INTERFACE_MODE_XAUI:
+            result = __cvmx_helper_xaui_link_get(ipd_port);
+            break;
+        case CVMX_HELPER_INTERFACE_MODE_GMII:
+            if (index == 0)
+                result = __cvmx_helper_rgmii_link_get(ipd_port);
+            else
+            {
+                result.s.full_duplex = 1;
+                result.s.link_up = 1;
+                result.s.speed = 1000;
+            }
+            break;
+        case CVMX_HELPER_INTERFACE_MODE_RGMII:
+            result = __cvmx_helper_rgmii_link_get(ipd_port);
+            break;
+        case CVMX_HELPER_INTERFACE_MODE_SPI:
+            result = __cvmx_helper_spi_link_get(ipd_port);
+            break;
+        case CVMX_HELPER_INTERFACE_MODE_SGMII:
+        case CVMX_HELPER_INTERFACE_MODE_PICMG:
+            result = __cvmx_helper_sgmii_link_get(ipd_port);
+            break;
+        case CVMX_HELPER_INTERFACE_MODE_NPI:
+        case CVMX_HELPER_INTERFACE_MODE_LOOP:
+            /* Network links are not supported */
+            break;
+    }
+    return result;
+}
+
+
+/**
+ * Configure an IPD/PKO port for the specified link state. This
+ * function does not influence auto negotiation at the PHY level.
+ * The passed link state must always match the link state returned
+ * by cvmx_helper_link_get(). It is normally best to use
+ * cvmx_helper_link_autoconf() instead.
+ *
+ * @param ipd_port  IPD/PKO port to configure
+ * @param link_info The new link state
+ *
+ * @return Zero on success, negative on failure
+ */
+int cvmx_helper_link_set(int ipd_port, cvmx_helper_link_info_t link_info)
+{
+    int result = -1;
+    int interface = cvmx_helper_get_interface_num(ipd_port);
+    int index = cvmx_helper_get_interface_index_num(ipd_port);
+
+    if (index >= cvmx_helper_ports_on_interface(interface))
+        return -1;
+
+    switch (cvmx_helper_interface_get_mode(interface))
+    {
+        case CVMX_HELPER_INTERFACE_MODE_DISABLED:
+        case CVMX_HELPER_INTERFACE_MODE_PCIE:
+            break;
+        case CVMX_HELPER_INTERFACE_MODE_XAUI:
+            result = __cvmx_helper_xaui_link_set(ipd_port, link_info);
+            break;
+        /* RGMII/GMII/MII are all treated about the same. Most functions
+            refer to these ports as RGMII */
+        case CVMX_HELPER_INTERFACE_MODE_RGMII:
+        case CVMX_HELPER_INTERFACE_MODE_GMII:
+            result = __cvmx_helper_rgmii_link_set(ipd_port, link_info);
+            break;
+        case CVMX_HELPER_INTERFACE_MODE_SPI:
+            result = __cvmx_helper_spi_link_set(ipd_port, link_info);
+            break;
+        case CVMX_HELPER_INTERFACE_MODE_SGMII:
+        case CVMX_HELPER_INTERFACE_MODE_PICMG:
+            result = __cvmx_helper_sgmii_link_set(ipd_port, link_info);
+            break;
+        case CVMX_HELPER_INTERFACE_MODE_NPI:
+        case CVMX_HELPER_INTERFACE_MODE_LOOP:
+            break;
+    }
+    /* Set the port_link_info here so that the link status is updated
+       no matter how cvmx_helper_link_set is called. We don't change
+       the value if link_set failed */
+    if (result == 0)
+        port_link_info[ipd_port].u64 = link_info.u64;
+    return result;
+}
+
+#endif /* CVMX_ENABLE_PKO_FUNCTIONS */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper.h b/arch/mips/cavium-octeon/executive/cvmx-helper.h
new file mode 100644
index 0000000..cf32ec4
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper.h
@@ -0,0 +1,251 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Helper functions for common, but complicated tasks.
+ *
+ * <hr>$Revision: 33034 $<hr>
+ */
+
+#ifndef __CVMX_HELPER_H__
+#define __CVMX_HELPER_H__
+
+#include "executive-config.h"
+#include "cvmx-config.h"
+#include "cvmx-fpa.h"
+#include "cvmx-wqe.h"
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+typedef enum
+{
+    CVMX_HELPER_INTERFACE_MODE_DISABLED,
+    CVMX_HELPER_INTERFACE_MODE_RGMII,
+    CVMX_HELPER_INTERFACE_MODE_GMII,
+    CVMX_HELPER_INTERFACE_MODE_SPI,
+    CVMX_HELPER_INTERFACE_MODE_PCIE,
+    CVMX_HELPER_INTERFACE_MODE_XAUI,
+    CVMX_HELPER_INTERFACE_MODE_SGMII,
+    CVMX_HELPER_INTERFACE_MODE_PICMG,
+    CVMX_HELPER_INTERFACE_MODE_NPI,
+    CVMX_HELPER_INTERFACE_MODE_LOOP,
+} cvmx_helper_interface_mode_t;
+
+typedef union
+{
+    uint64_t u64;
+    struct
+    {
+        uint64_t    reserved_20_63  : 44;
+        uint64_t    link_up         : 1;    /**< Is the physical link up? */
+        uint64_t    full_duplex     : 1;    /**< 1 if the link is full duplex */
+        uint64_t    speed           : 18;   /**< Speed of the link in Mbps */
+    } s;
+} cvmx_helper_link_info_t;
+
+#include "cvmx-helper-fpa.h"
+
+#ifdef CVMX_ENABLE_PKO_FUNCTIONS
+
+#include "cvmx-helper-errata.h"
+#include "cvmx-helper-loop.h"
+#include "cvmx-helper-npi.h"
+#include "cvmx-helper-rgmii.h"
+#include "cvmx-helper-sgmii.h"
+#include "cvmx-helper-spi.h"
+#include "cvmx-helper-util.h"
+#include "cvmx-helper-xaui.h"
+
+/**
+ * cvmx_override_pko_queue_priority(int ipd_port, uint64_t
+ * priorities[16]) is a function pointer. It is meant to allow
+ * customization of the PKO queue priorities based on the port
+ * number. Users should set this pointer to a function before
+ * calling any cvmx-helper operations.
+ */
+extern void (*cvmx_override_pko_queue_priority)(int pko_port, uint64_t priorities[16]);
+
+/**
+ * cvmx_override_ipd_port_setup(int ipd_port) is a function
+ * pointer. It is meant to allow customization of the IPD port
+ * setup before packet input/output comes online. It is called
+ * after cvmx-helper does the default IPD configuration, but
+ * before IPD is enabled. Users should set this pointer to a
+ * function before calling any cvmx-helper operations.
+ */
+extern void (*cvmx_override_ipd_port_setup)(int ipd_port);
+
+/**
+ * This function enables the IPD and also enables the packet interfaces.
+ * The packet interfaces (RGMII and SPI) must be enabled after the
+ * IPD.  This should be called by the user program after any additional
+ * IPD configuration changes are made if CVMX_HELPER_ENABLE_IPD
+ * is not set in the executive-config.h file.
+ *
+ * @return 0 on success
+ *         -1 on failure
+ */
+extern int cvmx_helper_ipd_and_packet_input_enable(void);
+
+/**
+ * Initialize the PIP, IPD, and PKO hardware to support
+ * simple priority based queues for the ethernet ports. Each
+ * port is configured with a number of priority queues based
+ * on CVMX_PKO_QUEUES_PER_PORT_* where each queue is lower
+ * priority than the previous.
+ *
+ * @return Zero on success, non-zero on failure
+ */
+extern int cvmx_helper_initialize_packet_io_global(void);
+
+/**
+ * Does core local initialization for packet io
+ *
+ * @return Zero on success, non-zero on failure
+ */
+extern int cvmx_helper_initialize_packet_io_local(void);
+
+/**
+ * Returns the number of ports on the given interface.
+ * The interface must be initialized before the port count
+ * can be returned.
+ *
+ * @param interface Which interface to return port count for.
+ *
+ * @return Port count for interface
+ *         -1 for uninitialized interface
+ */
+extern int cvmx_helper_ports_on_interface(int interface);
+
+/**
+ * Return the number of interfaces the chip has. Each interface
+ * may have multiple ports. Most chips support two interfaces,
+ * but the CNX0XX and CNX1XX are exceptions. These only support
+ * one interface.
+ *
+ * @return Number of interfaces on chip
+ */
+extern int cvmx_helper_get_number_of_interfaces(void);
+
+/**
+ * Get the operating mode of an interface. Depending on the Octeon
+ * chip and configuration, this function returns an enumeration
+ * of the type of packet I/O supported by an interface.
+ *
+ * @param interface Interface to probe
+ *
+ * @return Mode of the interface. Unknown or unsupported interfaces return
+ *         DISABLED.
+ */
+extern cvmx_helper_interface_mode_t cvmx_helper_interface_get_mode(int interface);
+
+/**
+ * Auto configure an IPD/PKO port link state and speed. This
+ * function basically does the equivalent of:
+ * cvmx_helper_link_set(ipd_port, cvmx_helper_link_get(ipd_port));
+ *
+ * @param ipd_port IPD/PKO port to auto configure
+ *
+ * @return Link state after configure
+ */
+extern cvmx_helper_link_info_t cvmx_helper_link_autoconf(int ipd_port);
+
+/**
+ * Return the link state of an IPD/PKO port as returned by
+ * auto negotiation. The result of this function may not match
+ * Octeon's link config if auto negotiation has changed since
+ * the last call to cvmx_helper_link_set().
+ *
+ * @param ipd_port IPD/PKO port to query
+ *
+ * @return Link state
+ */
+extern cvmx_helper_link_info_t cvmx_helper_link_get(int ipd_port);
+
+/**
+ * Configure an IPD/PKO port for the specified link state. This
+ * function does not influence auto negotiation at the PHY level.
+ * The passed link state must always match the link state returned
+ * by cvmx_helper_link_get(). It is normally best to use
+ * cvmx_helper_link_autoconf() instead.
+ *
+ * @param ipd_port  IPD/PKO port to configure
+ * @param link_info The new link state
+ *
+ * @return Zero on success, negative on failure
+ */
+extern int cvmx_helper_link_set(int ipd_port, cvmx_helper_link_info_t link_info);
+
+
+
+/**
+ * This function probes an interface to determine the actual
+ * number of hardware ports connected to it. It doesn't setup the
+ * ports or enable them. The main goal here is to set the global
+ * interface_port_count[interface] correctly. Hardware setup of the
+ * ports will be performed later.
+ *
+ * @param interface Interface to probe
+ *
+ * @return Zero on success, negative on failure
+ */
+extern int cvmx_helper_interface_probe(int interface);
+#endif /* CVMX_ENABLE_PKO_FUNCTIONS */
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif  /* __CVMX_HELPER_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-higig.h b/arch/mips/cavium-octeon/executive/cvmx-higig.h
new file mode 100644
index 0000000..39ea167
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-higig.h
@@ -0,0 +1,305 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Functions and typedefs for using Octeon in HiGig/HiGig+/HiGig2 mode over
+ * XAUI.
+ *
+ * <hr>$Revision: 35609 $<hr>
+ */
+
+#ifndef __CVMX_HIGIG_H__
+#define __CVMX_HIGIG_H__
+#include "cvmx-wqe.h"
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+typedef struct
+{
+    union
+    {
+        uint32_t u32;
+        struct
+        {
+            uint32_t start          : 8; /**< 8-bits of Preamble indicating start of frame */
+            uint32_t dst_modid_6    : 1; /**< This field is valid only if the HGI field is a b'10' and it represents Bit 6 of
+                                            DST_MODID (bits 4:0 are in Byte 7 and bit 5 is in Byte 9). ). For HGI field
+                                            value of b'01' this field should be b'1'. For all other values of HGI it is don't
+                                            care. */
+            uint32_t src_modid_6    : 1; /**< This field is valid only if the HGI field is a b'10' and it represents Bit 6 of
+                                            SRC_MODID (bits 4:0 are in Byte 4 and bit 5 is in Byte 9). For HGI field
+                                            value of b'01' this field should be b'0'. For all other values of HGI it is don't
+                                            care. */
+            uint32_t hdr_ext_len    : 3; /**< This field is valid only if the HGI field is a b'10' and it indicates the extension
+                                            to the standard 12-bytes of XGS HiGig header. Each unit represents 4
+                                            bytes, giving a total of 16 additional extension bytes. Value of b'101', b'110'
+                                            and b'111' are reserved. For HGI field value of b'01' this field should be
+                                            b'01'. For all other values of HGI it is don't care. */
+            uint32_t cng_high       : 1; /**< Congestion Bit High flag */
+            uint32_t hgi            : 2; /**< HiGig interface format indicator
+                                            00 = Reserved
+                                            01 = Pure preamble - IEEE standard framing of 10GE
+                                            10 = XGS header - framing based on XGS family definition In this
+                                                format, the default length of the header is 12 bytes and additional
+                                                bytes are indicated by the HDR_EXT_LEN field
+                                            11 = Reserved */
+            uint32_t vid_high       : 8; /**< 8-bits of the VLAN tag information */
+            uint32_t vid_low        : 8; /**< 8 bits LSB of the VLAN tag information */
+        } s;
+    } dw0;
+    union
+    {
+        uint32_t u32;
+        struct
+        {
+            uint32_t opcode         : 3; /**< XGS HiGig op-code, indicating the type of packet
+                                            000 =     Control frames used for CPU to CPU communications
+                                            001 =     Unicast packet with destination resolved; The packet can be
+                                                      either Layer 2 unicast packet or L3 unicast packet that was
+                                                      routed in the ingress chip.
+                                            010 =     Broadcast or unknown Unicast packet or unknown multicast,
+                                                      destined to all members of the VLAN
+                                            011 =     L2 Multicast packet, destined to all ports of the group indicated
+                                                      in the L2MC_INDEX which is overlayed on DST_PORT/DST_MODID fields
+                                            100 =     IP Multicast packet, destined to all ports of the group indicated
+                                                      in the IPMC_INDEX which is overlayed on DST_PORT/DST_MODID fields
+                                            101 =     Reserved
+                                            110 =     Reserved
+                                            111 =     Reserved */
+            uint32_t src_modid_low  : 5; /**< Bits 4:0 of Module ID of the source module on which the packet ingress (bit
+                                            5 is in Byte 9 and bit 6 Is in Byte 1) */
+            uint32_t src_port_tgid  : 6; /**< If the MSB of this field is set, then it indicates the LAG the packet ingressed
+                                            on, else it represents the physical port the packet ingressed on. */
+            uint32_t pfm            : 2; /**< Three Port Filtering Modes (0, 1, 2) used in handling registed/unregistered
+                                            multicast (unknown L2 multicast and IPMC) packets. This field is used
+                                            when OPCODE is 011 or 100 Semantics of PFM bits are as follows;
+                                            For registered L2 multicast packets:
+                                                PFM= 0  Flood to VLAN
+                                                PFM= 1 or 2  Send to group members in the L2MC table
+                                            For unregistered L2 multicast packets:
+                                                PFM= 0 or 1  Flood to VLAN
+                                                PFM= 2  Drop the packet */
+            uint32_t priority       : 3; /**< This is the internal priority of the packet. This internal priority will go through
+                                            COS_SEL mapping registers to map to the actual MMU queues. */
+            uint32_t dst_port       : 5; /**< Port number of destination port on which the packet needs to egress. */
+            uint32_t dst_modid_low  : 5; /**< Bits [4-: 0] of Module ID of the destination port on which the packet needs to egress. */
+            uint32_t cng_low        : 1; /**< Semantics of CNG_HIGH and CNG_LOW are as follows: The following
+                                            encodings are to make it backward compatible:
+                                            {CNG_HIGH, CNG_LOW] - COLOR
+                                            [0, 0]  Packet is green
+                                            [0, 1]  Packet is red
+                                            [1, 1]  Packet is yellow
+                                            [1, 0]  Undefined */
+            uint32_t header_type    : 2; /**< Indicates the format of the next 4 bytes of the XGS HiGig header
+                                            00 = Overlay 1 (default)
+                                            01 = Overlay 2 (Classification Tag)
+                                            10 = Reserved
+                                            11 = Reserved */
+        } s;
+    } dw1;
+    union
+    {
+        uint32_t u32;
+        struct
+        {
+            uint32_t mirror         : 1; /**< Mirror: XGS3 mode: a mirror copy packet. XGS1/2 mode: Indicates that the
+                                            packet was switched and only needs to be mirrored. */
+            uint32_t mirror_done    : 1; /**< Mirroring Done: XGS1/2 mode: Indicates that the packet was mirrored and
+                                            may still need to be switched. */
+            uint32_t mirror_only    : 1; /**< Mirror Only: XGS 1/2 mode: Indicates that the packet was switched and only
+                                            needs to be mirrored. */
+            uint32_t ingress_tagged : 1; /**< Ingress Tagged: Indicates whether the packet was tagged when it originally
+                                            ingressed the system. */
+            uint32_t dst_tgid       : 3; /**< Destination Trunk Group ID: Trunk group ID of the destination port. The
+                                            DO_NOT_LEARN bit is overlaid on the second bit of this field. */
+            uint32_t dst_t          : 1; /**< Destination Trunk: Indicates that the destination port is a member of a trunk
+                                            group. */
+            uint32_t vc_label_16_19 : 4; /**< VC Label: Bits 19:16 of VC label: HiGig+ added field */
+            uint32_t label_present  : 1; /**< Label Present: Indicates that header contains a 20-bit VC label: HiGig+
+                                            added field. */
+            uint32_t l3             : 1; /**< L3: Indicates that the packet is L3 switched */
+            uint32_t dst_modid_5    : 1; /**< Destination Module ID: Bit 5 of Dst_ModID (bits 4:0 are in byte 7 and bit 6
+                                            is in byte 1) */
+            uint32_t src_modid_5    : 1; /**< Source Module ID: Bit 5 of Src_ModID (bits 4:0 are in byte 4 and bit 6 is in
+                                            byte 1) */
+            uint32_t vc_label_0_15  : 16;/**< VC Label: Bits 15:0 of VC label: HiGig+ added field */
+        } o1;
+        struct
+        {
+            uint32_t classification : 16; /**< Classification tag information from the HiGig device FFP */
+            uint32_t reserved_0_15  : 16;
+
+        } o2;
+    } dw2;
+} cvmx_higig_header_t;
+
+
+/**
+ * Initialize the HiGig aspects of a XAUI interface. This function
+ * should be called before the cvmx-helper generic init.
+ *
+ * @param interface Interface to initialize HiGig on (0-1)
+ * @param enable_higig2
+ *                  Non zero to enable HiGig2 support. Zero to support HiGig
+ *                  and HiGig+.
+ *
+ * @return Zero on success, negative on failure
+ */
+static inline int cvmx_higig_initialize(int interface, int enable_higig2)
+{
+    cvmx_pip_prt_cfgx_t pip_prt_cfg;
+    cvmx_gmxx_rxx_udd_skp_t gmx_rx_udd_skp;
+    cvmx_gmxx_txx_min_pkt_t gmx_tx_min_pkt;
+    cvmx_gmxx_txx_append_t gmx_tx_append;
+    cvmx_gmxx_tx_ifg_t gmx_tx_ifg;
+    cvmx_gmxx_tx_ovr_bp_t gmx_tx_ovr_bp;
+    cvmx_gmxx_rxx_frm_ctl_t gmx_rx_frm_ctl;
+    cvmx_gmxx_tx_xaui_ctl_t gmx_tx_xaui_ctl;
+    int i;
+    int header_size = (enable_higig2) ? 16 : 12;
+
+    /* Setup PIP to handle HiGig */
+    pip_prt_cfg.u64 = cvmx_read_csr(CVMX_PIP_PRT_CFGX(interface*16));
+    pip_prt_cfg.s.dsa_en = 0;
+    pip_prt_cfg.s.higig_en = 1;
+    pip_prt_cfg.s.hg_qos = 1;
+    pip_prt_cfg.s.skip = header_size;
+    cvmx_write_csr(CVMX_PIP_PRT_CFGX(interface*16), pip_prt_cfg.u64);
+
+    /* Setup some sample QoS defaults. These can be changed later */
+    for (i=0; i<64; i++)
+    {
+        cvmx_pip_hg_pri_qos_t pip_hg_pri_qos;
+        pip_hg_pri_qos.u64 = 0;
+        pip_hg_pri_qos.s.pri = i;
+        pip_hg_pri_qos.s.qos = i&7;
+        cvmx_write_csr(CVMX_PIP_HG_PRI_QOS, pip_hg_pri_qos.u64);
+    }
+
+    /* Setup GMX RX to treat the HiGig header as user data to ignore */
+    gmx_rx_udd_skp.u64 = cvmx_read_csr(CVMX_GMXX_RXX_UDD_SKP(0, interface));
+    gmx_rx_udd_skp.s.len = header_size;
+    gmx_rx_udd_skp.s.fcssel = 0;
+    cvmx_write_csr(CVMX_GMXX_RXX_UDD_SKP(0, interface), gmx_rx_udd_skp.u64);
+
+    /* Disable GMX preamble checking */
+    gmx_rx_frm_ctl.u64 = cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(0, interface));
+    gmx_rx_frm_ctl.s.pre_chk = 0;
+    cvmx_write_csr(CVMX_GMXX_RXX_FRM_CTL(0, interface), gmx_rx_frm_ctl.u64);
+
+    /* Setup GMX TX to pad properly min sized packets */
+    gmx_tx_min_pkt.u64 = cvmx_read_csr(CVMX_GMXX_TXX_MIN_PKT(0, interface));
+    gmx_tx_min_pkt.s.min_size = 59 + header_size;
+    cvmx_write_csr(CVMX_GMXX_TXX_MIN_PKT(0, interface), gmx_tx_min_pkt.u64);
+
+    /* Setup GMX TX to not add a preamble */
+    gmx_tx_append.u64 = cvmx_read_csr(CVMX_GMXX_TXX_APPEND(0, interface));
+    gmx_tx_append.s.preamble = 0;
+    cvmx_write_csr(CVMX_GMXX_TXX_APPEND(0, interface), gmx_tx_append.u64);
+
+    /* Reduce the inter frame gap to 8 bytes */
+    gmx_tx_ifg.u64 = cvmx_read_csr(CVMX_GMXX_TX_IFG(interface));
+    gmx_tx_ifg.s.ifg1 = 4;
+    gmx_tx_ifg.s.ifg2 = 4;
+    cvmx_write_csr(CVMX_GMXX_TX_IFG(interface), gmx_tx_ifg.u64);
+
+    /* Disable GMX backpressure */
+    gmx_tx_ovr_bp.u64 = cvmx_read_csr(CVMX_GMXX_TX_OVR_BP(interface));
+    gmx_tx_ovr_bp.s.bp = 0;
+    gmx_tx_ovr_bp.s.en = 0xf;
+    gmx_tx_ovr_bp.s.ign_full = 0xf;
+    cvmx_write_csr(CVMX_GMXX_TX_OVR_BP(interface), gmx_tx_ovr_bp.u64);
+
+    if (enable_higig2)
+    {
+        /* Enable HiGig2 support and forwarding of virtual port backpressure
+            to PKO */
+        cvmx_gmxx_hg2_control_t gmx_hg2_control;
+        gmx_hg2_control.u64 = cvmx_read_csr(CVMX_GMXX_HG2_CONTROL(interface));
+        gmx_hg2_control.s.hg2rx_en = 1;
+        gmx_hg2_control.s.hg2tx_en = 1;
+        gmx_hg2_control.s.logl_en = 0xffff;
+        gmx_hg2_control.s.phys_en = 1;
+        cvmx_write_csr(CVMX_GMXX_HG2_CONTROL(interface), gmx_hg2_control.u64);
+    }
+
+    /* Enable HiGig */
+    gmx_tx_xaui_ctl.u64 = cvmx_read_csr(CVMX_GMXX_TX_XAUI_CTL(interface));
+    gmx_tx_xaui_ctl.s.hg_en = 1;
+    cvmx_write_csr(CVMX_GMXX_TX_XAUI_CTL(interface), gmx_tx_xaui_ctl.u64);
+
+    return 0;
+}
+
+
+/**
+ * Given a WQE pointer received from IPD and POW, return a
+ * pointer to the HiGig header.
+ *
+ * @param wqe    Work Queue Entry from IPD/POW.
+ *
+ * @return Pointer or NULL of failure
+ */
+static inline cvmx_higig_header_t *cvmx_higig_get_header(cvmx_wqe_t *wqe)
+{
+    // FIXME
+    return NULL;
+}
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif //  __CVMX_HIGIG_H__
diff --git a/arch/mips/cavium-octeon/executive/cvmx-interrupt-decodes.c b/arch/mips/cavium-octeon/executive/cvmx-interrupt-decodes.c
new file mode 100644
index 0000000..7dc6c17
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-interrupt-decodes.c
@@ -0,0 +1,3424 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * Automatically generated functions useful for enabling
+ * and decoding RSL_INT_BLOCKS interrupts.
+ *
+ * This file is auto generated. Do not edit.
+ *
+ * <hr>$Revision$<hr>
+ *
+ */
+
+#include "cvmx.h"
+#include "cvmx-pcie.h"
+
+#ifndef PRINT_ERROR
+#define PRINT_ERROR(format, ...) cvmx_dprintf("ERROR " format, ##__VA_ARGS__)
+#endif
+
+void __cvmx_interrupt_pci_int_enb2_enable(void);
+void __cvmx_interrupt_pci_int_sum2_decode(void);
+void __cvmx_interrupt_pescx_dbg_info_en_enable(int index);
+void __cvmx_interrupt_pescx_dbg_info_decode(int index);
+
+/**
+ * __cvmx_interrupt_agl_gmx_rxx_int_en_enable enables all interrupt bits in cvmx_agl_gmx_rxx_int_en_t
+ */
+void __cvmx_interrupt_agl_gmx_rxx_int_en_enable(int index)
+{
+    cvmx_agl_gmx_rxx_int_en_t agl_gmx_rx_int_en;
+    cvmx_write_csr(CVMX_AGL_GMX_RXX_INT_REG(index), cvmx_read_csr(CVMX_AGL_GMX_RXX_INT_REG(index)));
+    agl_gmx_rx_int_en.u64 = 0;
+    if (OCTEON_IS_MODEL(OCTEON_CN56XX))
+    {
+        // Skipping agl_gmx_rx_int_en.s.reserved_20_63
+        agl_gmx_rx_int_en.s.pause_drp = 1;
+        // Skipping agl_gmx_rx_int_en.s.reserved_16_18
+        agl_gmx_rx_int_en.s.ifgerr = 1;
+        //agl_gmx_rx_int_en.s.coldet = 1; // Collsion detect
+        //agl_gmx_rx_int_en.s.falerr = 1; // False carrier error or extend error after slottime
+        //agl_gmx_rx_int_en.s.rsverr = 1; // RGMII reserved opcodes
+        //agl_gmx_rx_int_en.s.pcterr = 1; // Bad Preamble / Protocol
+        agl_gmx_rx_int_en.s.ovrerr = 1;
+        // Skipping agl_gmx_rx_int_en.s.reserved_9_9
+        agl_gmx_rx_int_en.s.skperr = 1;
+        agl_gmx_rx_int_en.s.rcverr = 1;
+        agl_gmx_rx_int_en.s.lenerr = 1;
+        agl_gmx_rx_int_en.s.alnerr = 1;
+        agl_gmx_rx_int_en.s.fcserr = 1;
+        agl_gmx_rx_int_en.s.jabber = 1;
+        agl_gmx_rx_int_en.s.maxerr = 1;
+        // Skipping agl_gmx_rx_int_en.s.reserved_1_1
+        agl_gmx_rx_int_en.s.minerr = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN52XX))
+    {
+        // Skipping agl_gmx_rx_int_en.s.reserved_20_63
+        agl_gmx_rx_int_en.s.pause_drp = 1;
+        // Skipping agl_gmx_rx_int_en.s.reserved_16_18
+        agl_gmx_rx_int_en.s.ifgerr = 1;
+        //agl_gmx_rx_int_en.s.coldet = 1; // Collsion detect
+        //agl_gmx_rx_int_en.s.falerr = 1; // False carrier error or extend error after slottime
+        //agl_gmx_rx_int_en.s.rsverr = 1; // RGMII reserved opcodes
+        //agl_gmx_rx_int_en.s.pcterr = 1; // Bad Preamble / Protocol
+        agl_gmx_rx_int_en.s.ovrerr = 1;
+        // Skipping agl_gmx_rx_int_en.s.reserved_9_9
+        agl_gmx_rx_int_en.s.skperr = 1;
+        agl_gmx_rx_int_en.s.rcverr = 1;
+        agl_gmx_rx_int_en.s.lenerr = 1;
+        agl_gmx_rx_int_en.s.alnerr = 1;
+        agl_gmx_rx_int_en.s.fcserr = 1;
+        agl_gmx_rx_int_en.s.jabber = 1;
+        agl_gmx_rx_int_en.s.maxerr = 1;
+        // Skipping agl_gmx_rx_int_en.s.reserved_1_1
+        agl_gmx_rx_int_en.s.minerr = 1;
+    }
+    cvmx_write_csr(CVMX_AGL_GMX_RXX_INT_EN(index), agl_gmx_rx_int_en.u64);
+}
+
+
+/**
+ * __cvmx_interrupt_agl_gmx_rxx_int_reg_decode decodes all interrupt bits in cvmx_agl_gmx_rxx_int_reg_t
+ */
+void __cvmx_interrupt_agl_gmx_rxx_int_reg_decode(int index)
+{
+    cvmx_agl_gmx_rxx_int_reg_t agl_gmx_rx_int_reg;
+    agl_gmx_rx_int_reg.u64 = cvmx_read_csr(CVMX_AGL_GMX_RXX_INT_REG(index));
+    cvmx_write_csr(CVMX_AGL_GMX_RXX_INT_REG(index), agl_gmx_rx_int_reg.u64);
+    agl_gmx_rx_int_reg.u64 &= cvmx_read_csr(CVMX_AGL_GMX_RXX_INT_EN(index));
+    // Skipping agl_gmx_rx_int_reg.s.reserved_20_63
+    if (agl_gmx_rx_int_reg.s.pause_drp)
+        PRINT_ERROR("AGL_GMX_RX%d_INT_REG[PAUSE_DRP]: Pause packet was dropped due to full GMX RX FIFO\n", index);
+    // Skipping agl_gmx_rx_int_reg.s.reserved_16_18
+    if (agl_gmx_rx_int_reg.s.ifgerr)
+        PRINT_ERROR("AGL_GMX_RX%d_INT_REG[IFGERR]: Interframe Gap Violation\n"
+                    "    Does not necessarily indicate a failure\n", index);
+    if (agl_gmx_rx_int_reg.s.coldet)
+        PRINT_ERROR("AGL_GMX_RX%d_INT_REG[COLDET]: Collision Detection\n", index);
+    if (agl_gmx_rx_int_reg.s.falerr)
+        PRINT_ERROR("AGL_GMX_RX%d_INT_REG[FALERR]: False carrier error or extend error after slottime\n", index);
+    if (agl_gmx_rx_int_reg.s.rsverr)
+        PRINT_ERROR("AGL_GMX_RX%d_INT_REG[RSVERR]: MII reserved opcodes\n", index);
+    if (agl_gmx_rx_int_reg.s.pcterr)
+        PRINT_ERROR("AGL_GMX_RX%d_INT_REG[PCTERR]: Bad Preamble / Protocol\n", index);
+    if (agl_gmx_rx_int_reg.s.ovrerr)
+        PRINT_ERROR("AGL_GMX_RX%d_INT_REG[OVRERR]: Internal Data Aggregation Overflow\n"
+                    "    This interrupt should never assert\n", index);
+    // Skipping agl_gmx_rx_int_reg.s.reserved_9_9
+    if (agl_gmx_rx_int_reg.s.skperr)
+        PRINT_ERROR("AGL_GMX_RX%d_INT_REG[SKPERR]: Skipper error\n", index);
+    if (agl_gmx_rx_int_reg.s.rcverr)
+        PRINT_ERROR("AGL_GMX_RX%d_INT_REG[RCVERR]: Frame was received with MII Data reception error\n", index);
+    if (agl_gmx_rx_int_reg.s.lenerr)
+        PRINT_ERROR("AGL_GMX_RX%d_INT_REG[LENERR]: Frame was received with length error\n", index);
+    if (agl_gmx_rx_int_reg.s.alnerr)
+        PRINT_ERROR("AGL_GMX_RX%d_INT_REG[ALNERR]: Frame was received with an alignment error\n", index);
+    if (agl_gmx_rx_int_reg.s.fcserr)
+        PRINT_ERROR("AGL_GMX_RX%d_INT_REG[FCSERR]: Frame was received with FCS/CRC error\n", index);
+    if (agl_gmx_rx_int_reg.s.jabber)
+        PRINT_ERROR("AGL_GMX_RX%d_INT_REG[JABBER]: Frame was received with length > sys_length\n", index);
+    if (agl_gmx_rx_int_reg.s.maxerr)
+        PRINT_ERROR("AGL_GMX_RX%d_INT_REG[MAXERR]: Frame was received with length > max_length\n", index);
+    // Skipping agl_gmx_rx_int_reg.s.reserved_1_1
+    if (agl_gmx_rx_int_reg.s.minerr)
+        PRINT_ERROR("AGL_GMX_RX%d_INT_REG[MINERR]: Frame was received with length < min_length\n", index);
+}
+
+
+/**
+ * __cvmx_interrupt_fpa_int_enb_enable enables all interrupt bits in cvmx_fpa_int_enb_t
+ */
+void __cvmx_interrupt_fpa_int_enb_enable(void)
+{
+    cvmx_fpa_int_enb_t fpa_int_enb;
+    cvmx_write_csr(CVMX_FPA_INT_SUM, cvmx_read_csr(CVMX_FPA_INT_SUM));
+    fpa_int_enb.u64 = 0;
+    if (OCTEON_IS_MODEL(OCTEON_CN56XX))
+    {
+        // Skipping fpa_int_enb.s.reserved_28_63
+        fpa_int_enb.s.q7_perr = 1;
+        fpa_int_enb.s.q7_coff = 1;
+        fpa_int_enb.s.q7_und = 1;
+        fpa_int_enb.s.q6_perr = 1;
+        fpa_int_enb.s.q6_coff = 1;
+        fpa_int_enb.s.q6_und = 1;
+        fpa_int_enb.s.q5_perr = 1;
+        fpa_int_enb.s.q5_coff = 1;
+        fpa_int_enb.s.q5_und = 1;
+        fpa_int_enb.s.q4_perr = 1;
+        fpa_int_enb.s.q4_coff = 1;
+        fpa_int_enb.s.q4_und = 1;
+        fpa_int_enb.s.q3_perr = 1;
+        fpa_int_enb.s.q3_coff = 1;
+        fpa_int_enb.s.q3_und = 1;
+        fpa_int_enb.s.q2_perr = 1;
+        fpa_int_enb.s.q2_coff = 1;
+        fpa_int_enb.s.q2_und = 1;
+        fpa_int_enb.s.q1_perr = 1;
+        fpa_int_enb.s.q1_coff = 1;
+        fpa_int_enb.s.q1_und = 1;
+        fpa_int_enb.s.q0_perr = 1;
+        fpa_int_enb.s.q0_coff = 1;
+        fpa_int_enb.s.q0_und = 1;
+        fpa_int_enb.s.fed1_dbe = 1;
+        fpa_int_enb.s.fed1_sbe = 1;
+        fpa_int_enb.s.fed0_dbe = 1;
+        fpa_int_enb.s.fed0_sbe = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN30XX))
+    {
+        // Skipping fpa_int_enb.s.reserved_28_63
+        fpa_int_enb.s.q7_perr = 1;
+        fpa_int_enb.s.q7_coff = 1;
+        fpa_int_enb.s.q7_und = 1;
+        fpa_int_enb.s.q6_perr = 1;
+        fpa_int_enb.s.q6_coff = 1;
+        fpa_int_enb.s.q6_und = 1;
+        fpa_int_enb.s.q5_perr = 1;
+        fpa_int_enb.s.q5_coff = 1;
+        fpa_int_enb.s.q5_und = 1;
+        fpa_int_enb.s.q4_perr = 1;
+        fpa_int_enb.s.q4_coff = 1;
+        fpa_int_enb.s.q4_und = 1;
+        fpa_int_enb.s.q3_perr = 1;
+        fpa_int_enb.s.q3_coff = 1;
+        fpa_int_enb.s.q3_und = 1;
+        fpa_int_enb.s.q2_perr = 1;
+        fpa_int_enb.s.q2_coff = 1;
+        fpa_int_enb.s.q2_und = 1;
+        fpa_int_enb.s.q1_perr = 1;
+        fpa_int_enb.s.q1_coff = 1;
+        fpa_int_enb.s.q1_und = 1;
+        fpa_int_enb.s.q0_perr = 1;
+        fpa_int_enb.s.q0_coff = 1;
+        fpa_int_enb.s.q0_und = 1;
+        fpa_int_enb.s.fed1_dbe = 1;
+        fpa_int_enb.s.fed1_sbe = 1;
+        fpa_int_enb.s.fed0_dbe = 1;
+        fpa_int_enb.s.fed0_sbe = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN50XX))
+    {
+        // Skipping fpa_int_enb.s.reserved_28_63
+        fpa_int_enb.s.q7_perr = 1;
+        fpa_int_enb.s.q7_coff = 1;
+        fpa_int_enb.s.q7_und = 1;
+        fpa_int_enb.s.q6_perr = 1;
+        fpa_int_enb.s.q6_coff = 1;
+        fpa_int_enb.s.q6_und = 1;
+        fpa_int_enb.s.q5_perr = 1;
+        fpa_int_enb.s.q5_coff = 1;
+        fpa_int_enb.s.q5_und = 1;
+        fpa_int_enb.s.q4_perr = 1;
+        fpa_int_enb.s.q4_coff = 1;
+        fpa_int_enb.s.q4_und = 1;
+        fpa_int_enb.s.q3_perr = 1;
+        fpa_int_enb.s.q3_coff = 1;
+        fpa_int_enb.s.q3_und = 1;
+        fpa_int_enb.s.q2_perr = 1;
+        fpa_int_enb.s.q2_coff = 1;
+        fpa_int_enb.s.q2_und = 1;
+        fpa_int_enb.s.q1_perr = 1;
+        fpa_int_enb.s.q1_coff = 1;
+        fpa_int_enb.s.q1_und = 1;
+        fpa_int_enb.s.q0_perr = 1;
+        fpa_int_enb.s.q0_coff = 1;
+        fpa_int_enb.s.q0_und = 1;
+        fpa_int_enb.s.fed1_dbe = 1;
+        fpa_int_enb.s.fed1_sbe = 1;
+        fpa_int_enb.s.fed0_dbe = 1;
+        fpa_int_enb.s.fed0_sbe = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN38XX))
+    {
+        // Skipping fpa_int_enb.s.reserved_28_63
+        fpa_int_enb.s.q7_perr = 1;
+        fpa_int_enb.s.q7_coff = 1;
+        fpa_int_enb.s.q7_und = 1;
+        fpa_int_enb.s.q6_perr = 1;
+        fpa_int_enb.s.q6_coff = 1;
+        fpa_int_enb.s.q6_und = 1;
+        fpa_int_enb.s.q5_perr = 1;
+        fpa_int_enb.s.q5_coff = 1;
+        fpa_int_enb.s.q5_und = 1;
+        fpa_int_enb.s.q4_perr = 1;
+        fpa_int_enb.s.q4_coff = 1;
+        fpa_int_enb.s.q4_und = 1;
+        fpa_int_enb.s.q3_perr = 1;
+        fpa_int_enb.s.q3_coff = 1;
+        fpa_int_enb.s.q3_und = 1;
+        fpa_int_enb.s.q2_perr = 1;
+        fpa_int_enb.s.q2_coff = 1;
+        fpa_int_enb.s.q2_und = 1;
+        fpa_int_enb.s.q1_perr = 1;
+        fpa_int_enb.s.q1_coff = 1;
+        fpa_int_enb.s.q1_und = 1;
+        fpa_int_enb.s.q0_perr = 1;
+        fpa_int_enb.s.q0_coff = 1;
+        fpa_int_enb.s.q0_und = 1;
+        fpa_int_enb.s.fed1_dbe = 1;
+        fpa_int_enb.s.fed1_sbe = 1;
+        fpa_int_enb.s.fed0_dbe = 1;
+        fpa_int_enb.s.fed0_sbe = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN31XX))
+    {
+        // Skipping fpa_int_enb.s.reserved_28_63
+        fpa_int_enb.s.q7_perr = 1;
+        fpa_int_enb.s.q7_coff = 1;
+        fpa_int_enb.s.q7_und = 1;
+        fpa_int_enb.s.q6_perr = 1;
+        fpa_int_enb.s.q6_coff = 1;
+        fpa_int_enb.s.q6_und = 1;
+        fpa_int_enb.s.q5_perr = 1;
+        fpa_int_enb.s.q5_coff = 1;
+        fpa_int_enb.s.q5_und = 1;
+        fpa_int_enb.s.q4_perr = 1;
+        fpa_int_enb.s.q4_coff = 1;
+        fpa_int_enb.s.q4_und = 1;
+        fpa_int_enb.s.q3_perr = 1;
+        fpa_int_enb.s.q3_coff = 1;
+        fpa_int_enb.s.q3_und = 1;
+        fpa_int_enb.s.q2_perr = 1;
+        fpa_int_enb.s.q2_coff = 1;
+        fpa_int_enb.s.q2_und = 1;
+        fpa_int_enb.s.q1_perr = 1;
+        fpa_int_enb.s.q1_coff = 1;
+        fpa_int_enb.s.q1_und = 1;
+        fpa_int_enb.s.q0_perr = 1;
+        fpa_int_enb.s.q0_coff = 1;
+        fpa_int_enb.s.q0_und = 1;
+        fpa_int_enb.s.fed1_dbe = 1;
+        fpa_int_enb.s.fed1_sbe = 1;
+        fpa_int_enb.s.fed0_dbe = 1;
+        fpa_int_enb.s.fed0_sbe = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN58XX))
+    {
+        // Skipping fpa_int_enb.s.reserved_28_63
+        fpa_int_enb.s.q7_perr = 1;
+        fpa_int_enb.s.q7_coff = 1;
+        fpa_int_enb.s.q7_und = 1;
+        fpa_int_enb.s.q6_perr = 1;
+        fpa_int_enb.s.q6_coff = 1;
+        fpa_int_enb.s.q6_und = 1;
+        fpa_int_enb.s.q5_perr = 1;
+        fpa_int_enb.s.q5_coff = 1;
+        fpa_int_enb.s.q5_und = 1;
+        fpa_int_enb.s.q4_perr = 1;
+        fpa_int_enb.s.q4_coff = 1;
+        fpa_int_enb.s.q4_und = 1;
+        fpa_int_enb.s.q3_perr = 1;
+        fpa_int_enb.s.q3_coff = 1;
+        fpa_int_enb.s.q3_und = 1;
+        fpa_int_enb.s.q2_perr = 1;
+        fpa_int_enb.s.q2_coff = 1;
+        fpa_int_enb.s.q2_und = 1;
+        fpa_int_enb.s.q1_perr = 1;
+        fpa_int_enb.s.q1_coff = 1;
+        fpa_int_enb.s.q1_und = 1;
+        fpa_int_enb.s.q0_perr = 1;
+        fpa_int_enb.s.q0_coff = 1;
+        fpa_int_enb.s.q0_und = 1;
+        fpa_int_enb.s.fed1_dbe = 1;
+        fpa_int_enb.s.fed1_sbe = 1;
+        fpa_int_enb.s.fed0_dbe = 1;
+        fpa_int_enb.s.fed0_sbe = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN52XX))
+    {
+        // Skipping fpa_int_enb.s.reserved_28_63
+        fpa_int_enb.s.q7_perr = 1;
+        fpa_int_enb.s.q7_coff = 1;
+        fpa_int_enb.s.q7_und = 1;
+        fpa_int_enb.s.q6_perr = 1;
+        fpa_int_enb.s.q6_coff = 1;
+        fpa_int_enb.s.q6_und = 1;
+        fpa_int_enb.s.q5_perr = 1;
+        fpa_int_enb.s.q5_coff = 1;
+        fpa_int_enb.s.q5_und = 1;
+        fpa_int_enb.s.q4_perr = 1;
+        fpa_int_enb.s.q4_coff = 1;
+        fpa_int_enb.s.q4_und = 1;
+        fpa_int_enb.s.q3_perr = 1;
+        fpa_int_enb.s.q3_coff = 1;
+        fpa_int_enb.s.q3_und = 1;
+        fpa_int_enb.s.q2_perr = 1;
+        fpa_int_enb.s.q2_coff = 1;
+        fpa_int_enb.s.q2_und = 1;
+        fpa_int_enb.s.q1_perr = 1;
+        fpa_int_enb.s.q1_coff = 1;
+        fpa_int_enb.s.q1_und = 1;
+        fpa_int_enb.s.q0_perr = 1;
+        fpa_int_enb.s.q0_coff = 1;
+        fpa_int_enb.s.q0_und = 1;
+        fpa_int_enb.s.fed1_dbe = 1;
+        fpa_int_enb.s.fed1_sbe = 1;
+        fpa_int_enb.s.fed0_dbe = 1;
+        fpa_int_enb.s.fed0_sbe = 1;
+    }
+    cvmx_write_csr(CVMX_FPA_INT_ENB, fpa_int_enb.u64);
+}
+
+
+/**
+ * __cvmx_interrupt_fpa_int_sum_decode decodes all interrupt bits in cvmx_fpa_int_sum_t
+ */
+void __cvmx_interrupt_fpa_int_sum_decode(void)
+{
+    cvmx_fpa_int_sum_t fpa_int_sum;
+    fpa_int_sum.u64 = cvmx_read_csr(CVMX_FPA_INT_SUM);
+    cvmx_write_csr(CVMX_FPA_INT_SUM, fpa_int_sum.u64);
+    fpa_int_sum.u64 &= cvmx_read_csr(CVMX_FPA_INT_ENB);
+    // Skipping fpa_int_sum.s.reserved_28_63
+    if (fpa_int_sum.s.q7_perr)
+        PRINT_ERROR("FPA_INT_SUM[Q7_PERR]: Set when a Queue0 pointer read from the stack in\n"
+                    "    the L2C does not have the FPA owner ship bit set.\n");
+    if (fpa_int_sum.s.q7_coff)
+        PRINT_ERROR("FPA_INT_SUM[Q7_COFF]: Set when a Queue0 stack end tag is present and\n"
+                    "    the count available is greater than than pointers\n"
+                    "    present in the FPA.\n");
+    if (fpa_int_sum.s.q7_und)
+        PRINT_ERROR("FPA_INT_SUM[Q7_UND]: Set when a Queue0 page count available goes\n"
+                    "    negative.\n");
+    if (fpa_int_sum.s.q6_perr)
+        PRINT_ERROR("FPA_INT_SUM[Q6_PERR]: Set when a Queue0 pointer read from the stack in\n"
+                    "    the L2C does not have the FPA owner ship bit set.\n");
+    if (fpa_int_sum.s.q6_coff)
+        PRINT_ERROR("FPA_INT_SUM[Q6_COFF]: Set when a Queue0 stack end tag is present and\n"
+                    "    the count available is greater than than pointers\n"
+                    "    present in the FPA.\n");
+    if (fpa_int_sum.s.q6_und)
+        PRINT_ERROR("FPA_INT_SUM[Q6_UND]: Set when a Queue0 page count available goes\n"
+                    "    negative.\n");
+    if (fpa_int_sum.s.q5_perr)
+        PRINT_ERROR("FPA_INT_SUM[Q5_PERR]: Set when a Queue0 pointer read from the stack in\n"
+                    "    the L2C does not have the FPA owner ship bit set.\n");
+    if (fpa_int_sum.s.q5_coff)
+        PRINT_ERROR("FPA_INT_SUM[Q5_COFF]: Set when a Queue0 stack end tag is present and\n"
+                    "    the count available is greater than than pointers\n"
+                    "    present in the FPA.\n");
+    if (fpa_int_sum.s.q5_und)
+        PRINT_ERROR("FPA_INT_SUM[Q5_UND]: Set when a Queue0 page count available goes\n"
+                    "    negative.\n");
+    if (fpa_int_sum.s.q4_perr)
+        PRINT_ERROR("FPA_INT_SUM[Q4_PERR]: Set when a Queue0 pointer read from the stack in\n"
+                    "    the L2C does not have the FPA owner ship bit set.\n");
+    if (fpa_int_sum.s.q4_coff)
+        PRINT_ERROR("FPA_INT_SUM[Q4_COFF]: Set when a Queue0 stack end tag is present and\n"
+                    "    the count available is greater than than pointers\n"
+                    "    present in the FPA.\n");
+    if (fpa_int_sum.s.q4_und)
+        PRINT_ERROR("FPA_INT_SUM[Q4_UND]: Set when a Queue0 page count available goes\n"
+                    "    negative.\n");
+    if (fpa_int_sum.s.q3_perr)
+        PRINT_ERROR("FPA_INT_SUM[Q3_PERR]: Set when a Queue0 pointer read from the stack in\n"
+                    "    the L2C does not have the FPA owner ship bit set.\n");
+    if (fpa_int_sum.s.q3_coff)
+        PRINT_ERROR("FPA_INT_SUM[Q3_COFF]: Set when a Queue0 stack end tag is present and\n"
+                    "    the count available is greater than than pointers\n"
+                    "    present in the FPA.\n");
+    if (fpa_int_sum.s.q3_und)
+        PRINT_ERROR("FPA_INT_SUM[Q3_UND]: Set when a Queue0 page count available goes\n"
+                    "    negative.\n");
+    if (fpa_int_sum.s.q2_perr)
+        PRINT_ERROR("FPA_INT_SUM[Q2_PERR]: Set when a Queue0 pointer read from the stack in\n"
+                    "    the L2C does not have the FPA owner ship bit set.\n");
+    if (fpa_int_sum.s.q2_coff)
+        PRINT_ERROR("FPA_INT_SUM[Q2_COFF]: Set when a Queue0 stack end tag is present and\n"
+                    "    the count available is greater than than pointers\n"
+                    "    present in the FPA.\n");
+    if (fpa_int_sum.s.q2_und)
+        PRINT_ERROR("FPA_INT_SUM[Q2_UND]: Set when a Queue0 page count available goes\n"
+                    "    negative.\n");
+    if (fpa_int_sum.s.q1_perr)
+        PRINT_ERROR("FPA_INT_SUM[Q1_PERR]: Set when a Queue0 pointer read from the stack in\n"
+                    "    the L2C does not have the FPA owner ship bit set.\n");
+    if (fpa_int_sum.s.q1_coff)
+        PRINT_ERROR("FPA_INT_SUM[Q1_COFF]: Set when a Queue0 stack end tag is present and\n"
+                    "    the count available is greater than pointers\n"
+                    "    present in the FPA.\n");
+    if (fpa_int_sum.s.q1_und)
+        PRINT_ERROR("FPA_INT_SUM[Q1_UND]: Set when a Queue0 page count available goes\n"
+                    "    negative.\n");
+    if (fpa_int_sum.s.q0_perr)
+        PRINT_ERROR("FPA_INT_SUM[Q0_PERR]: Set when a Queue0 pointer read from the stack in\n"
+                    "    the L2C does not have the FPA owner ship bit set.\n");
+    if (fpa_int_sum.s.q0_coff)
+        PRINT_ERROR("FPA_INT_SUM[Q0_COFF]: Set when a Queue0 stack end tag is present and\n"
+                    "    the count available is greater than pointers\n"
+                    "    present in the FPA.\n");
+    if (fpa_int_sum.s.q0_und)
+        PRINT_ERROR("FPA_INT_SUM[Q0_UND]: Set when a Queue0 page count available goes\n"
+                    "    negative.\n");
+    if (fpa_int_sum.s.fed1_dbe)
+        PRINT_ERROR("FPA_INT_SUM[FED1_DBE]: Set when a Double Bit Error is detected in FPF1.\n");
+    if (fpa_int_sum.s.fed1_sbe)
+        PRINT_ERROR("FPA_INT_SUM[FED1_SBE]: Set when a Single Bit Error is detected in FPF1.\n");
+    if (fpa_int_sum.s.fed0_dbe)
+        PRINT_ERROR("FPA_INT_SUM[FED0_DBE]: Set when a Double Bit Error is detected in FPF0.\n");
+    if (fpa_int_sum.s.fed0_sbe)
+        PRINT_ERROR("FPA_INT_SUM[FED0_SBE]: Set when a Single Bit Error is detected in FPF0.\n");
+}
+
+
+/**
+ * __cvmx_interrupt_gmxx_rxx_int_en_enable enables all interrupt bits in cvmx_gmxx_rxx_int_en_t
+ */
+void __cvmx_interrupt_gmxx_rxx_int_en_enable(int index, int block)
+{
+    cvmx_gmxx_rxx_int_en_t gmx_rx_int_en;
+    cvmx_write_csr(CVMX_GMXX_RXX_INT_REG(index, block), cvmx_read_csr(CVMX_GMXX_RXX_INT_REG(index, block)));
+    gmx_rx_int_en.u64 = 0;
+    if (OCTEON_IS_MODEL(OCTEON_CN56XX))
+    {
+        // Skipping gmx_rx_int_en.s.reserved_29_63
+        gmx_rx_int_en.s.hg2cc = 1;
+        gmx_rx_int_en.s.hg2fld = 1;
+        gmx_rx_int_en.s.undat = 1;
+        gmx_rx_int_en.s.uneop = 1;
+        gmx_rx_int_en.s.unsop = 1;
+        gmx_rx_int_en.s.bad_term = 1;
+        gmx_rx_int_en.s.bad_seq = 1;
+        gmx_rx_int_en.s.rem_fault = 1;
+        gmx_rx_int_en.s.loc_fault = 1;
+        gmx_rx_int_en.s.pause_drp = 1;
+        // Skipping gmx_rx_int_en.s.reserved_16_18
+        //gmx_rx_int_en.s.ifgerr = 1;
+        //gmx_rx_int_en.s.coldet = 1; // Collsion detect
+        //gmx_rx_int_en.s.falerr = 1; // False carrier error or extend error after slottime
+        //gmx_rx_int_en.s.rsverr = 1; // RGMII reserved opcodes
+        //gmx_rx_int_en.s.pcterr = 1; // Bad Preamble / Protocol
+        gmx_rx_int_en.s.ovrerr = 1;
+        // Skipping gmx_rx_int_en.s.reserved_9_9
+        gmx_rx_int_en.s.skperr = 1;
+        gmx_rx_int_en.s.rcverr = 1;
+        // Skipping gmx_rx_int_en.s.reserved_5_6
+        //gmx_rx_int_en.s.fcserr = 1; // FCS errors are handled when we get work
+        gmx_rx_int_en.s.jabber = 1;
+        // Skipping gmx_rx_int_en.s.reserved_2_2
+        gmx_rx_int_en.s.carext = 1;
+        // Skipping gmx_rx_int_en.s.reserved_0_0
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN30XX))
+    {
+        // Skipping gmx_rx_int_en.s.reserved_19_63
+        //gmx_rx_int_en.s.phy_dupx = 1;
+        //gmx_rx_int_en.s.phy_spd = 1;
+        //gmx_rx_int_en.s.phy_link = 1;
+        //gmx_rx_int_en.s.ifgerr = 1;
+        //gmx_rx_int_en.s.coldet = 1; // Collsion detect
+        //gmx_rx_int_en.s.falerr = 1; // False carrier error or extend error after slottime
+        //gmx_rx_int_en.s.rsverr = 1; // RGMII reserved opcodes
+        //gmx_rx_int_en.s.pcterr = 1; // Bad Preamble / Protocol
+        gmx_rx_int_en.s.ovrerr = 1;
+        gmx_rx_int_en.s.niberr = 1;
+        gmx_rx_int_en.s.skperr = 1;
+        gmx_rx_int_en.s.rcverr = 1;
+        //gmx_rx_int_en.s.lenerr = 1; // Length errors are handled when we get work
+        gmx_rx_int_en.s.alnerr = 1;
+        //gmx_rx_int_en.s.fcserr = 1; // FCS errors are handled when we get work
+        gmx_rx_int_en.s.jabber = 1;
+        gmx_rx_int_en.s.maxerr = 1;
+        gmx_rx_int_en.s.carext = 1;
+        gmx_rx_int_en.s.minerr = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN50XX))
+    {
+        // Skipping gmx_rx_int_en.s.reserved_20_63
+        gmx_rx_int_en.s.pause_drp = 1;
+        //gmx_rx_int_en.s.phy_dupx = 1;
+        //gmx_rx_int_en.s.phy_spd = 1;
+        //gmx_rx_int_en.s.phy_link = 1;
+        //gmx_rx_int_en.s.ifgerr = 1;
+        //gmx_rx_int_en.s.coldet = 1; // Collsion detect
+        //gmx_rx_int_en.s.falerr = 1; // False carrier error or extend error after slottime
+        //gmx_rx_int_en.s.rsverr = 1; // RGMII reserved opcodes
+        //gmx_rx_int_en.s.pcterr = 1; // Bad Preamble / Protocol
+        gmx_rx_int_en.s.ovrerr = 1;
+        gmx_rx_int_en.s.niberr = 1;
+        gmx_rx_int_en.s.skperr = 1;
+        gmx_rx_int_en.s.rcverr = 1;
+        // Skipping gmx_rx_int_en.s.reserved_6_6
+        gmx_rx_int_en.s.alnerr = 1;
+        //gmx_rx_int_en.s.fcserr = 1; // FCS errors are handled when we get work
+        gmx_rx_int_en.s.jabber = 1;
+        // Skipping gmx_rx_int_en.s.reserved_2_2
+        gmx_rx_int_en.s.carext = 1;
+        // Skipping gmx_rx_int_en.s.reserved_0_0
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN38XX))
+    {
+        // Skipping gmx_rx_int_en.s.reserved_19_63
+        //gmx_rx_int_en.s.phy_dupx = 1;
+        //gmx_rx_int_en.s.phy_spd = 1;
+        //gmx_rx_int_en.s.phy_link = 1;
+        //gmx_rx_int_en.s.ifgerr = 1;
+        //gmx_rx_int_en.s.coldet = 1; // Collsion detect
+        //gmx_rx_int_en.s.falerr = 1; // False carrier error or extend error after slottime
+        //gmx_rx_int_en.s.rsverr = 1; // RGMII reserved opcodes
+        //gmx_rx_int_en.s.pcterr = 1; // Bad Preamble / Protocol
+        gmx_rx_int_en.s.ovrerr = 1;
+        gmx_rx_int_en.s.niberr = 1;
+        gmx_rx_int_en.s.skperr = 1;
+        gmx_rx_int_en.s.rcverr = 1;
+        //gmx_rx_int_en.s.lenerr = 1; // Length errors are handled when we get work
+        gmx_rx_int_en.s.alnerr = 1;
+        //gmx_rx_int_en.s.fcserr = 1; // FCS errors are handled when we get work
+        gmx_rx_int_en.s.jabber = 1;
+        gmx_rx_int_en.s.maxerr = 1;
+        gmx_rx_int_en.s.carext = 1;
+        gmx_rx_int_en.s.minerr = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN31XX))
+    {
+        // Skipping gmx_rx_int_en.s.reserved_19_63
+        //gmx_rx_int_en.s.phy_dupx = 1;
+        //gmx_rx_int_en.s.phy_spd = 1;
+        //gmx_rx_int_en.s.phy_link = 1;
+        //gmx_rx_int_en.s.ifgerr = 1;
+        //gmx_rx_int_en.s.coldet = 1; // Collsion detect
+        //gmx_rx_int_en.s.falerr = 1; // False carrier error or extend error after slottime
+        //gmx_rx_int_en.s.rsverr = 1; // RGMII reserved opcodes
+        //gmx_rx_int_en.s.pcterr = 1; // Bad Preamble / Protocol
+        gmx_rx_int_en.s.ovrerr = 1;
+        gmx_rx_int_en.s.niberr = 1;
+        gmx_rx_int_en.s.skperr = 1;
+        gmx_rx_int_en.s.rcverr = 1;
+        //gmx_rx_int_en.s.lenerr = 1; // Length errors are handled when we get work
+        gmx_rx_int_en.s.alnerr = 1;
+        //gmx_rx_int_en.s.fcserr = 1; // FCS errors are handled when we get work
+        gmx_rx_int_en.s.jabber = 1;
+        gmx_rx_int_en.s.maxerr = 1;
+        gmx_rx_int_en.s.carext = 1;
+        gmx_rx_int_en.s.minerr = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN58XX))
+    {
+        // Skipping gmx_rx_int_en.s.reserved_20_63
+        gmx_rx_int_en.s.pause_drp = 1;
+        //gmx_rx_int_en.s.phy_dupx = 1;
+        //gmx_rx_int_en.s.phy_spd = 1;
+        //gmx_rx_int_en.s.phy_link = 1;
+        //gmx_rx_int_en.s.ifgerr = 1;
+        //gmx_rx_int_en.s.coldet = 1; // Collsion detect
+        //gmx_rx_int_en.s.falerr = 1; // False carrier error or extend error after slottime
+        //gmx_rx_int_en.s.rsverr = 1; // RGMII reserved opcodes
+        //gmx_rx_int_en.s.pcterr = 1; // Bad Preamble / Protocol
+        gmx_rx_int_en.s.ovrerr = 1;
+        gmx_rx_int_en.s.niberr = 1;
+        gmx_rx_int_en.s.skperr = 1;
+        gmx_rx_int_en.s.rcverr = 1;
+        //gmx_rx_int_en.s.lenerr = 1; // Length errors are handled when we get work
+        gmx_rx_int_en.s.alnerr = 1;
+        //gmx_rx_int_en.s.fcserr = 1; // FCS errors are handled when we get work
+        gmx_rx_int_en.s.jabber = 1;
+        gmx_rx_int_en.s.maxerr = 1;
+        gmx_rx_int_en.s.carext = 1;
+        gmx_rx_int_en.s.minerr = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN52XX))
+    {
+        // Skipping gmx_rx_int_en.s.reserved_29_63
+        gmx_rx_int_en.s.hg2cc = 1;
+        gmx_rx_int_en.s.hg2fld = 1;
+        gmx_rx_int_en.s.undat = 1;
+        gmx_rx_int_en.s.uneop = 1;
+        gmx_rx_int_en.s.unsop = 1;
+        gmx_rx_int_en.s.bad_term = 1;
+        gmx_rx_int_en.s.bad_seq = 1;
+        gmx_rx_int_en.s.rem_fault = 1;
+        gmx_rx_int_en.s.loc_fault = 1;
+        gmx_rx_int_en.s.pause_drp = 1;
+        // Skipping gmx_rx_int_en.s.reserved_16_18
+        //gmx_rx_int_en.s.ifgerr = 1;
+        //gmx_rx_int_en.s.coldet = 1; // Collsion detect
+        //gmx_rx_int_en.s.falerr = 1; // False carrier error or extend error after slottime
+        //gmx_rx_int_en.s.rsverr = 1; // RGMII reserved opcodes
+        //gmx_rx_int_en.s.pcterr = 1; // Bad Preamble / Protocol
+        gmx_rx_int_en.s.ovrerr = 1;
+        // Skipping gmx_rx_int_en.s.reserved_9_9
+        gmx_rx_int_en.s.skperr = 1;
+        gmx_rx_int_en.s.rcverr = 1;
+        // Skipping gmx_rx_int_en.s.reserved_5_6
+        //gmx_rx_int_en.s.fcserr = 1; // FCS errors are handled when we get work
+        gmx_rx_int_en.s.jabber = 1;
+        // Skipping gmx_rx_int_en.s.reserved_2_2
+        gmx_rx_int_en.s.carext = 1;
+        // Skipping gmx_rx_int_en.s.reserved_0_0
+    }
+    cvmx_write_csr(CVMX_GMXX_RXX_INT_EN(index, block), gmx_rx_int_en.u64);
+}
+
+
+/**
+ * __cvmx_interrupt_gmxx_rxx_int_reg_decode decodes all interrupt bits in cvmx_gmxx_rxx_int_reg_t
+ */
+void __cvmx_interrupt_gmxx_rxx_int_reg_decode(int index, int block)
+{
+    cvmx_gmxx_rxx_int_reg_t gmx_rx_int_reg;
+    gmx_rx_int_reg.u64 = cvmx_read_csr(CVMX_GMXX_RXX_INT_REG(index, block));
+    /* Don't clear inband status bits so someone else can use them */
+    gmx_rx_int_reg.s.phy_dupx = 0;
+    gmx_rx_int_reg.s.phy_spd = 0;
+    gmx_rx_int_reg.s.phy_link = 0;
+    cvmx_write_csr(CVMX_GMXX_RXX_INT_REG(index, block), gmx_rx_int_reg.u64);
+    gmx_rx_int_reg.u64 &= cvmx_read_csr(CVMX_GMXX_RXX_INT_EN(index, block));
+    // Skipping gmx_rx_int_reg.s.reserved_29_63
+    if (gmx_rx_int_reg.s.hg2cc)
+        PRINT_ERROR("GMX%d_RX%d_INT_REG[HG2CC]: HiGig2 received message CRC or Control char  error\n"
+                    "    Set when either CRC8 error detected or when\n"
+                    "    a Control Character is found in the message\n"
+                    "    bytes after the K.SOM\n"
+                    "    NOTE: HG2CC has higher priority than HG2FLD\n"
+                    "          i.e. a HiGig2 message that results in HG2CC\n"
+                    "          getting set, will never set HG2FLD.\n", block, index);
+    if (gmx_rx_int_reg.s.hg2fld)
+        PRINT_ERROR("GMX%d_RX%d_INT_REG[HG2FLD]: HiGig2 received message field error, as below\n"
+                    "    1) MSG_TYPE field not 6'b00_0000\n"
+                    "       i.e. it is not a FLOW CONTROL message, which\n"
+                    "       is the only defined type for HiGig2\n"
+                    "    2) FWD_TYPE field not 2'b00 i.e. Link Level msg\n"
+                    "       which is the only defined type for HiGig2\n"
+                    "    3) FC_OBJECT field is neither 4'b0000 for\n"
+                    "       Physical Link nor 4'b0010 for Logical Link.\n"
+                    "       Those are the only two defined types in HiGig2\n", block, index);
+    if (gmx_rx_int_reg.s.undat)
+        PRINT_ERROR("GMX%d_RX%d_INT_REG[UNDAT]: Unexpected Data\n"
+                    "    (XAUI Mode only)\n", block, index);
+    if (gmx_rx_int_reg.s.uneop)
+        PRINT_ERROR("GMX%d_RX%d_INT_REG[UNEOP]: Unexpected EOP\n"
+                    "    (XAUI Mode only)\n", block, index);
+    if (gmx_rx_int_reg.s.unsop)
+        PRINT_ERROR("GMX%d_RX%d_INT_REG[UNSOP]: Unexpected SOP\n"
+                    "    (XAUI Mode only)\n", block, index);
+    if (gmx_rx_int_reg.s.bad_term)
+        PRINT_ERROR("GMX%d_RX%d_INT_REG[BAD_TERM]: Frame is terminated by control character other\n"
+                    "    than /T/.  The error propagation control\n"
+                    "    character /E/ will be included as part of the\n"
+                    "    frame and does not cause a frame termination.\n"
+                    "    (XAUI Mode only)\n", block, index);
+    if (gmx_rx_int_reg.s.bad_seq)
+        PRINT_ERROR("GMX%d_RX%d_INT_REG[BAD_SEQ]: Reserved Sequence Deteted\n"
+                    "    (XAUI Mode only)\n", block, index);
+    if (gmx_rx_int_reg.s.rem_fault)
+        PRINT_ERROR("GMX%d_RX%d_INT_REG[REM_FAULT]: Remote Fault Sequence Deteted\n"
+                    "    (XAUI Mode only)\n", block, index);
+    if (gmx_rx_int_reg.s.loc_fault)
+        PRINT_ERROR("GMX%d_RX%d_INT_REG[LOC_FAULT]: Local Fault Sequence Deteted\n"
+                    "    (XAUI Mode only)\n", block, index);
+    if (gmx_rx_int_reg.s.pause_drp)
+        PRINT_ERROR("GMX%d_RX%d_INT_REG[PAUSE_DRP]: Pause packet was dropped due to full GMX RX FIFO\n", block, index);
+#if 0
+    if (gmx_rx_int_reg.s.phy_dupx)
+        PRINT_ERROR("GMX%d_RX%d_INT_REG[PHY_DUPX]: Change in the RMGII inbound LinkDuplex\n", block, index);
+    if (gmx_rx_int_reg.s.phy_spd)
+        PRINT_ERROR("GMX%d_RX%d_INT_REG[PHY_SPD]: Change in the RMGII inbound LinkSpeed\n", block, index);
+    if (gmx_rx_int_reg.s.phy_link)
+        PRINT_ERROR("GMX%d_RX%d_INT_REG[PHY_LINK]: Change in the RMGII inbound LinkStatus\n", block, index);
+#endif
+    if (gmx_rx_int_reg.s.ifgerr)
+        PRINT_ERROR("GMX%d_RX%d_INT_REG[IFGERR]: Interframe Gap Violation\n"
+                    "    Does not necessarily indicate a failure\n", block, index);
+    if (gmx_rx_int_reg.s.coldet)
+        PRINT_ERROR("GMX%d_RX%d_INT_REG[COLDET]: Collision Detection\n", block, index);
+    if (gmx_rx_int_reg.s.falerr)
+        PRINT_ERROR("GMX%d_RX%d_INT_REG[FALERR]: False carrier error or extend error after slottime\n", block, index);
+    if (gmx_rx_int_reg.s.rsverr)
+        PRINT_ERROR("GMX%d_RX%d_INT_REG[RSVERR]: RGMII reserved opcodes\n", block, index);
+    if (gmx_rx_int_reg.s.pcterr)
+        PRINT_ERROR("GMX%d_RX%d_INT_REG[PCTERR]: Bad Preamble / Protocol\n", block, index);
+    if (gmx_rx_int_reg.s.ovrerr)
+        PRINT_ERROR("GMX%d_RX%d_INT_REG[OVRERR]: Internal Data Aggregation Overflow\n"
+                    "    This interrupt should never assert\n", block, index);
+    if (gmx_rx_int_reg.s.niberr)
+        PRINT_ERROR("GMX%d_RX%d_INT_REG[NIBERR]: Nibble error (hi_nibble != lo_nibble)\n", block, index);
+    if (gmx_rx_int_reg.s.skperr)
+        PRINT_ERROR("GMX%d_RX%d_INT_REG[SKPERR]: Skipper error\n", block, index);
+    if (gmx_rx_int_reg.s.rcverr)
+        PRINT_ERROR("GMX%d_RX%d_INT_REG[RCVERR]: Frame was received with RMGII Data reception error\n", block, index);
+    if (gmx_rx_int_reg.s.lenerr)
+        PRINT_ERROR("GMX%d_RX%d_INT_REG[LENERR]: Frame was received with length error\n", block, index);
+    if (gmx_rx_int_reg.s.alnerr)
+        PRINT_ERROR("GMX%d_RX%d_INT_REG[ALNERR]: Frame was received with an alignment error\n", block, index);
+    if (gmx_rx_int_reg.s.fcserr)
+        PRINT_ERROR("GMX%d_RX%d_INT_REG[FCSERR]: Frame was received with FCS/CRC error\n", block, index);
+    if (gmx_rx_int_reg.s.jabber)
+        PRINT_ERROR("GMX%d_RX%d_INT_REG[JABBER]: Frame was received with length > sys_length\n", block, index);
+    if (gmx_rx_int_reg.s.maxerr)
+        PRINT_ERROR("GMX%d_RX%d_INT_REG[MAXERR]: Frame was received with length > max_length\n", block, index);
+    if (gmx_rx_int_reg.s.carext)
+        PRINT_ERROR("GMX%d_RX%d_INT_REG[CAREXT]: RGMII carrier extend error\n", block, index);
+    if (gmx_rx_int_reg.s.minerr)
+        PRINT_ERROR("GMX%d_RX%d_INT_REG[MINERR]: Frame was received with length < min_length\n", block, index);
+}
+
+
+/**
+ * __cvmx_interrupt_iob_int_enb_enable enables all interrupt bits in cvmx_iob_int_enb_t
+ */
+void __cvmx_interrupt_iob_int_enb_enable(void)
+{
+    cvmx_iob_int_enb_t iob_int_enb;
+    cvmx_write_csr(CVMX_IOB_INT_SUM, cvmx_read_csr(CVMX_IOB_INT_SUM));
+    iob_int_enb.u64 = 0;
+    if (OCTEON_IS_MODEL(OCTEON_CN56XX))
+    {
+        // Skipping iob_int_enb.s.reserved_6_63
+        iob_int_enb.s.p_dat = 1;
+        iob_int_enb.s.np_dat = 1;
+        iob_int_enb.s.p_eop = 1;
+        iob_int_enb.s.p_sop = 1;
+        iob_int_enb.s.np_eop = 1;
+        iob_int_enb.s.np_sop = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN30XX))
+    {
+        // Skipping iob_int_enb.s.reserved_4_63
+        iob_int_enb.s.p_eop = 1;
+        iob_int_enb.s.p_sop = 1;
+        iob_int_enb.s.np_eop = 1;
+        iob_int_enb.s.np_sop = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN50XX))
+    {
+        // Skipping iob_int_enb.s.reserved_6_63
+        iob_int_enb.s.p_dat = 1;
+        iob_int_enb.s.np_dat = 1;
+        iob_int_enb.s.p_eop = 1;
+        iob_int_enb.s.p_sop = 1;
+        iob_int_enb.s.np_eop = 1;
+        iob_int_enb.s.np_sop = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN38XX))
+    {
+        // Skipping iob_int_enb.s.reserved_4_63
+        iob_int_enb.s.p_eop = 1;
+        iob_int_enb.s.p_sop = 1;
+        iob_int_enb.s.np_eop = 1;
+        iob_int_enb.s.np_sop = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN31XX))
+    {
+        // Skipping iob_int_enb.s.reserved_4_63
+        iob_int_enb.s.p_eop = 1;
+        iob_int_enb.s.p_sop = 1;
+        iob_int_enb.s.np_eop = 1;
+        iob_int_enb.s.np_sop = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN58XX))
+    {
+        // Skipping iob_int_enb.s.reserved_6_63
+        iob_int_enb.s.p_dat = 1;
+        iob_int_enb.s.np_dat = 1;
+        iob_int_enb.s.p_eop = 1;
+        iob_int_enb.s.p_sop = 1;
+        iob_int_enb.s.np_eop = 1;
+        iob_int_enb.s.np_sop = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN52XX))
+    {
+        // Skipping iob_int_enb.s.reserved_6_63
+        iob_int_enb.s.p_dat = 1;
+        iob_int_enb.s.np_dat = 1;
+        iob_int_enb.s.p_eop = 1;
+        iob_int_enb.s.p_sop = 1;
+        iob_int_enb.s.np_eop = 1;
+        iob_int_enb.s.np_sop = 1;
+    }
+    cvmx_write_csr(CVMX_IOB_INT_ENB, iob_int_enb.u64);
+}
+
+
+/**
+ * __cvmx_interrupt_iob_int_sum_decode decodes all interrupt bits in cvmx_iob_int_sum_t
+ */
+void __cvmx_interrupt_iob_int_sum_decode(void)
+{
+    cvmx_iob_int_sum_t iob_int_sum;
+    iob_int_sum.u64 = cvmx_read_csr(CVMX_IOB_INT_SUM);
+    cvmx_write_csr(CVMX_IOB_INT_SUM, iob_int_sum.u64);
+    iob_int_sum.u64 &= cvmx_read_csr(CVMX_IOB_INT_ENB);
+    // Skipping iob_int_sum.s.reserved_6_63
+    if (iob_int_sum.s.p_dat)
+        PRINT_ERROR("IOB_INT_SUM[P_DAT]: Set when a data arrives before a SOP for the same\n"
+                    "    port for a passthrough packet.\n"
+                    "    The first detected error associated with bits [5:0]\n"
+                    "    of this register will only be set here. A new bit\n"
+                    "    can be set when the previous reported bit is cleared.\n");
+    if (iob_int_sum.s.np_dat)
+        PRINT_ERROR("IOB_INT_SUM[NP_DAT]: Set when a data arrives before a SOP for the same\n"
+                    "    port for a non-passthrough packet.\n"
+                    "    The first detected error associated with bits [5:0]\n"
+                    "    of this register will only be set here. A new bit\n"
+                    "    can be set when the previous reported bit is cleared.\n");
+    if (iob_int_sum.s.p_eop)
+        PRINT_ERROR("IOB_INT_SUM[P_EOP]: Set when a EOP is followed by an EOP for the same\n"
+                    "    port for a passthrough packet.\n"
+                    "    The first detected error associated with bits [5:0]\n"
+                    "    of this register will only be set here. A new bit\n"
+                    "    can be set when the previous reported bit is cleared.\n");
+    if (iob_int_sum.s.p_sop)
+        PRINT_ERROR("IOB_INT_SUM[P_SOP]: Set when a SOP is followed by an SOP for the same\n"
+                    "    port for a passthrough packet.\n"
+                    "    The first detected error associated with bits [5:0]\n"
+                    "    of this register will only be set here. A new bit\n"
+                    "    can be set when the previous reported bit is cleared.\n");
+    if (iob_int_sum.s.np_eop)
+        PRINT_ERROR("IOB_INT_SUM[NP_EOP]: Set when a EOP is followed by an EOP for the same\n"
+                    "    port for a non-passthrough packet.\n"
+                    "    The first detected error associated with bits [5:0]\n"
+                    "    of this register will only be set here. A new bit\n"
+                    "    can be set when the previous reported bit is cleared.\n");
+    if (iob_int_sum.s.np_sop)
+        PRINT_ERROR("IOB_INT_SUM[NP_SOP]: Set when a SOP is followed by an SOP for the same\n"
+                    "    port for a non-passthrough packet.\n"
+                    "    The first detected error associated with bits [5:0]\n"
+                    "    of this register will only be set here. A new bit\n"
+                    "    can be set when the previous reported bit is cleared.\n");
+}
+
+
+/**
+ * __cvmx_interrupt_ipd_int_enb_enable enables all interrupt bits in cvmx_ipd_int_enb_t
+ */
+void __cvmx_interrupt_ipd_int_enb_enable(void)
+{
+    cvmx_ipd_int_enb_t ipd_int_enb;
+    cvmx_write_csr(CVMX_IPD_INT_SUM, cvmx_read_csr(CVMX_IPD_INT_SUM));
+    ipd_int_enb.u64 = 0;
+    if (OCTEON_IS_MODEL(OCTEON_CN56XX))
+    {
+        // Skipping ipd_int_enb.s.reserved_12_63
+        //ipd_int_enb.s.pq_sub = 1; // Disable per port backpressure overflow checking since it happens when not in use
+        //ipd_int_enb.s.pq_add = 1; // Disable per port backpressure overflow checking since it happens when not in use
+        ipd_int_enb.s.bc_ovr = 1;
+        ipd_int_enb.s.d_coll = 1;
+        ipd_int_enb.s.c_coll = 1;
+        ipd_int_enb.s.cc_ovr = 1;
+        ipd_int_enb.s.dc_ovr = 1;
+        ipd_int_enb.s.bp_sub = 1;
+        ipd_int_enb.s.prc_par3 = 1;
+        ipd_int_enb.s.prc_par2 = 1;
+        ipd_int_enb.s.prc_par1 = 1;
+        ipd_int_enb.s.prc_par0 = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN30XX))
+    {
+        // Skipping ipd_int_enb.s.reserved_5_63
+        ipd_int_enb.s.bp_sub = 1;
+        ipd_int_enb.s.prc_par3 = 1;
+        ipd_int_enb.s.prc_par2 = 1;
+        ipd_int_enb.s.prc_par1 = 1;
+        ipd_int_enb.s.prc_par0 = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN50XX))
+    {
+        // Skipping ipd_int_enb.s.reserved_10_63
+        ipd_int_enb.s.bc_ovr = 1;
+        ipd_int_enb.s.d_coll = 1;
+        ipd_int_enb.s.c_coll = 1;
+        ipd_int_enb.s.cc_ovr = 1;
+        ipd_int_enb.s.dc_ovr = 1;
+        ipd_int_enb.s.bp_sub = 1;
+        ipd_int_enb.s.prc_par3 = 1;
+        ipd_int_enb.s.prc_par2 = 1;
+        ipd_int_enb.s.prc_par1 = 1;
+        ipd_int_enb.s.prc_par0 = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN38XX))
+    {
+        // Skipping ipd_int_enb.s.reserved_10_63
+        if (!OCTEON_IS_MODEL(OCTEON_CN38XX_PASS2))
+        {
+            ipd_int_enb.s.bc_ovr = 1;
+            ipd_int_enb.s.d_coll = 1;
+            ipd_int_enb.s.c_coll = 1;
+            ipd_int_enb.s.cc_ovr = 1;
+            ipd_int_enb.s.dc_ovr = 1;
+        }
+        ipd_int_enb.s.bp_sub = 1;
+        ipd_int_enb.s.prc_par3 = 1;
+        ipd_int_enb.s.prc_par2 = 1;
+        ipd_int_enb.s.prc_par1 = 1;
+        ipd_int_enb.s.prc_par0 = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN31XX))
+    {
+        // Skipping ipd_int_enb.s.reserved_5_63
+        ipd_int_enb.s.bp_sub = 1;
+        ipd_int_enb.s.prc_par3 = 1;
+        ipd_int_enb.s.prc_par2 = 1;
+        ipd_int_enb.s.prc_par1 = 1;
+        ipd_int_enb.s.prc_par0 = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN58XX))
+    {
+        // Skipping ipd_int_enb.s.reserved_10_63
+        ipd_int_enb.s.bc_ovr = 1;
+        ipd_int_enb.s.d_coll = 1;
+        ipd_int_enb.s.c_coll = 1;
+        ipd_int_enb.s.cc_ovr = 1;
+        ipd_int_enb.s.dc_ovr = 1;
+        ipd_int_enb.s.bp_sub = 1;
+        ipd_int_enb.s.prc_par3 = 1;
+        ipd_int_enb.s.prc_par2 = 1;
+        ipd_int_enb.s.prc_par1 = 1;
+        ipd_int_enb.s.prc_par0 = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN52XX))
+    {
+        // Skipping ipd_int_enb.s.reserved_12_63
+        //ipd_int_enb.s.pq_sub = 1; // Disable per port backpressure overflow checking since it happens when not in use
+        //ipd_int_enb.s.pq_add = 1; // Disable per port backpressure overflow checking since it happens when not in use
+        ipd_int_enb.s.bc_ovr = 1;
+        ipd_int_enb.s.d_coll = 1;
+        ipd_int_enb.s.c_coll = 1;
+        ipd_int_enb.s.cc_ovr = 1;
+        ipd_int_enb.s.dc_ovr = 1;
+        ipd_int_enb.s.bp_sub = 1;
+        ipd_int_enb.s.prc_par3 = 1;
+        ipd_int_enb.s.prc_par2 = 1;
+        ipd_int_enb.s.prc_par1 = 1;
+        ipd_int_enb.s.prc_par0 = 1;
+    }
+    cvmx_write_csr(CVMX_IPD_INT_ENB, ipd_int_enb.u64);
+}
+
+
+/**
+ * __cvmx_interrupt_ipd_int_sum_decode decodes all interrupt bits in cvmx_ipd_int_sum_t
+ */
+void __cvmx_interrupt_ipd_int_sum_decode(void)
+{
+    cvmx_ipd_int_sum_t ipd_int_sum;
+    ipd_int_sum.u64 = cvmx_read_csr(CVMX_IPD_INT_SUM);
+    cvmx_write_csr(CVMX_IPD_INT_SUM, ipd_int_sum.u64);
+    ipd_int_sum.u64 &= cvmx_read_csr(CVMX_IPD_INT_ENB);
+    // Skipping ipd_int_sum.s.reserved_12_63
+    if (ipd_int_sum.s.pq_sub)
+        PRINT_ERROR("IPD_INT_SUM[PQ_SUB]: Set when a port-qos does an sub to the count\n"
+                    "    that causes the counter to wrap.\n");
+    if (ipd_int_sum.s.pq_add)
+        PRINT_ERROR("IPD_INT_SUM[PQ_ADD]: Set when a port-qos does an add to the count\n"
+                    "    that causes the counter to wrap.\n");
+    if (ipd_int_sum.s.bc_ovr)
+        PRINT_ERROR("IPD_INT_SUM[BC_OVR]: Set when the byte-count to send to IOB overflows.\n"
+                    "    This is a PASS-3 Field.\n");
+    if (ipd_int_sum.s.d_coll)
+        PRINT_ERROR("IPD_INT_SUM[D_COLL]: Set when the packet/WQE data to be sent to IOB\n"
+                    "    collides.\n"
+                    "    This is a PASS-3 Field.\n");
+    if (ipd_int_sum.s.c_coll)
+        PRINT_ERROR("IPD_INT_SUM[C_COLL]: Set when the packet/WQE commands to be sent to IOB\n"
+                    "    collides.\n"
+                    "    This is a PASS-3 Field.\n");
+    if (ipd_int_sum.s.cc_ovr)
+        PRINT_ERROR("IPD_INT_SUM[CC_OVR]: Set when the command credits to the IOB overflow.\n"
+                    "    This is a PASS-3 Field.\n");
+    if (ipd_int_sum.s.dc_ovr)
+        PRINT_ERROR("IPD_INT_SUM[DC_OVR]: Set when the data credits to the IOB overflow.\n"
+                    "    This is a PASS-3 Field.\n");
+    if (ipd_int_sum.s.bp_sub)
+        PRINT_ERROR("IPD_INT_SUM[BP_SUB]: Set when a backpressure subtract is done with a\n"
+                    "    supplied illegal value.\n");
+    if (ipd_int_sum.s.prc_par3)
+        PRINT_ERROR("IPD_INT_SUM[PRC_PAR3]: Set when a parity error is dected for bits\n"
+                    "    [127:96] of the PBM memory.\n");
+    if (ipd_int_sum.s.prc_par2)
+        PRINT_ERROR("IPD_INT_SUM[PRC_PAR2]: Set when a parity error is dected for bits\n"
+                    "    [95:64] of the PBM memory.\n");
+    if (ipd_int_sum.s.prc_par1)
+        PRINT_ERROR("IPD_INT_SUM[PRC_PAR1]: Set when a parity error is dected for bits\n"
+                    "    [63:32] of the PBM memory.\n");
+    if (ipd_int_sum.s.prc_par0)
+        PRINT_ERROR("IPD_INT_SUM[PRC_PAR0]: Set when a parity error is dected for bits\n"
+                    "    [31:0] of the PBM memory.\n");
+}
+
+
+/**
+ * __cvmx_interrupt_key_int_enb_enable enables all interrupt bits in cvmx_key_int_enb_t
+ */
+void __cvmx_interrupt_key_int_enb_enable(void)
+{
+    cvmx_key_int_enb_t key_int_enb;
+    cvmx_write_csr(CVMX_KEY_INT_SUM, cvmx_read_csr(CVMX_KEY_INT_SUM));
+    key_int_enb.u64 = 0;
+    if (OCTEON_IS_MODEL(OCTEON_CN56XX))
+    {
+        // Skipping key_int_enb.s.reserved_4_63
+        key_int_enb.s.ked1_dbe = 1;
+        key_int_enb.s.ked1_sbe = 1;
+        key_int_enb.s.ked0_dbe = 1;
+        key_int_enb.s.ked0_sbe = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN38XX))
+    {
+        // Skipping key_int_enb.s.reserved_4_63
+        key_int_enb.s.ked1_dbe = 1;
+        key_int_enb.s.ked1_sbe = 1;
+        key_int_enb.s.ked0_dbe = 1;
+        key_int_enb.s.ked0_sbe = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN58XX))
+    {
+        // Skipping key_int_enb.s.reserved_4_63
+        key_int_enb.s.ked1_dbe = 1;
+        key_int_enb.s.ked1_sbe = 1;
+        key_int_enb.s.ked0_dbe = 1;
+        key_int_enb.s.ked0_sbe = 1;
+    }
+    cvmx_write_csr(CVMX_KEY_INT_ENB, key_int_enb.u64);
+}
+
+
+/**
+ * __cvmx_interrupt_key_int_sum_decode decodes all interrupt bits in cvmx_key_int_sum_t
+ */
+void __cvmx_interrupt_key_int_sum_decode(void)
+{
+    cvmx_key_int_sum_t key_int_sum;
+    key_int_sum.u64 = cvmx_read_csr(CVMX_KEY_INT_SUM);
+    cvmx_write_csr(CVMX_KEY_INT_SUM, key_int_sum.u64);
+    key_int_sum.u64 &= cvmx_read_csr(CVMX_KEY_INT_ENB);
+    // Skipping key_int_sum.s.reserved_4_63
+    if (key_int_sum.s.ked1_dbe)
+        PRINT_ERROR("KEY_INT_SUM[KED1_DBE]: Error bit\n");
+    if (key_int_sum.s.ked1_sbe)
+        PRINT_ERROR("KEY_INT_SUM[KED1_SBE]: Error bit\n");
+    if (key_int_sum.s.ked0_dbe)
+        PRINT_ERROR("KEY_INT_SUM[KED0_DBE]: Error bit\n");
+    if (key_int_sum.s.ked0_sbe)
+        PRINT_ERROR("KEY_INT_SUM[KED0_SBE]: Error bit\n");
+}
+
+
+/**
+ * __cvmx_interrupt_mio_boot_int_enable enables all interrupt bits in cvmx_mio_boot_int_t
+ */
+void __cvmx_interrupt_mio_boot_int_enable(void)
+{
+    cvmx_mio_boot_int_t mio_boot_int;
+    cvmx_write_csr(CVMX_MIO_BOOT_ERR, cvmx_read_csr(CVMX_MIO_BOOT_ERR));
+    mio_boot_int.u64 = 0;
+    if (OCTEON_IS_MODEL(OCTEON_CN56XX))
+    {
+        // Skipping mio_boot_int.s.reserved_2_63
+        mio_boot_int.s.wait_int = 1;
+        mio_boot_int.s.adr_int = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN30XX))
+    {
+        // Skipping mio_boot_int.s.reserved_2_63
+        mio_boot_int.s.wait_int = 1;
+        mio_boot_int.s.adr_int = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN50XX))
+    {
+        // Skipping mio_boot_int.s.reserved_2_63
+        mio_boot_int.s.wait_int = 1;
+        mio_boot_int.s.adr_int = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN38XX))
+    {
+        // Skipping mio_boot_int.s.reserved_2_63
+        mio_boot_int.s.wait_int = 1;
+        mio_boot_int.s.adr_int = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN31XX))
+    {
+        // Skipping mio_boot_int.s.reserved_2_63
+        mio_boot_int.s.wait_int = 1;
+        mio_boot_int.s.adr_int = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN58XX))
+    {
+        // Skipping mio_boot_int.s.reserved_2_63
+        mio_boot_int.s.wait_int = 1;
+        mio_boot_int.s.adr_int = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN52XX))
+    {
+        // Skipping mio_boot_int.s.reserved_2_63
+        mio_boot_int.s.wait_int = 1;
+        mio_boot_int.s.adr_int = 1;
+    }
+    cvmx_write_csr(CVMX_MIO_BOOT_INT, mio_boot_int.u64);
+}
+
+
+/**
+ * __cvmx_interrupt_mio_boot_err_decode decodes all interrupt bits in cvmx_mio_boot_err_t
+ */
+void __cvmx_interrupt_mio_boot_err_decode(void)
+{
+    cvmx_mio_boot_err_t mio_boot_err;
+    mio_boot_err.u64 = cvmx_read_csr(CVMX_MIO_BOOT_ERR);
+    cvmx_write_csr(CVMX_MIO_BOOT_ERR, mio_boot_err.u64);
+    mio_boot_err.u64 &= cvmx_read_csr(CVMX_MIO_BOOT_INT);
+    // Skipping mio_boot_err.s.reserved_2_63
+    if (mio_boot_err.s.wait_err)
+        PRINT_ERROR("MIO_BOOT_ERR[WAIT_ERR]: Wait mode error\n");
+    if (mio_boot_err.s.adr_err)
+        PRINT_ERROR("MIO_BOOT_ERR[ADR_ERR]: Address decode error\n");
+}
+
+
+/**
+ * __cvmx_interrupt_npei_int_sum_decode decodes all interrupt bits in cvmx_npei_int_sum_t
+ */
+void __cvmx_interrupt_npei_int_sum_decode(void)
+{
+    cvmx_npei_int_sum_t npei_int_sum;
+    npei_int_sum.u64 = cvmx_read_csr(CVMX_PEXP_NPEI_INT_SUM);
+    cvmx_write_csr(CVMX_PEXP_NPEI_INT_SUM, npei_int_sum.u64);
+    npei_int_sum.u64 &= cvmx_read_csr(CVMX_PEXP_NPEI_INT_ENB2);
+    if (npei_int_sum.s.mio_inta)
+        PRINT_ERROR("NPEI_INT_SUM[MIO_INTA]: Interrupt from MIO.\n");
+    // Skipping npei_int_sum.s.reserved_62_62
+    if (npei_int_sum.s.int_a)
+        PRINT_ERROR("NPEI_INT_SUM[INT_A]: Set when a bit in the NPEI_INT_A_SUM register and\n"
+                    "    the cooresponding bit in the NPEI_INT_A_ENB\n"
+                    "    register is set.\n");
+    if (npei_int_sum.s.c1_ldwn)
+    {
+        PRINT_ERROR("NPEI_INT_SUM[C1_LDWN]: Reset request due to link1 down status.\n");
+        /* Attempt to automatically bring the link back up */
+        cvmx_pcie_rc_shutdown(1);
+        cvmx_pcie_rc_initialize(1);
+        cvmx_write_csr(CVMX_PEXP_NPEI_INT_SUM, cvmx_read_csr(CVMX_PEXP_NPEI_INT_SUM));
+    }
+    if (npei_int_sum.s.c0_ldwn)
+    {
+        PRINT_ERROR("NPEI_INT_SUM[C0_LDWN]: Reset request due to link0 down status.\n");
+        /* Attempt to automatically bring the link back up */
+        cvmx_pcie_rc_shutdown(0);
+        cvmx_pcie_rc_initialize(0);
+        cvmx_write_csr(CVMX_PEXP_NPEI_INT_SUM, cvmx_read_csr(CVMX_PEXP_NPEI_INT_SUM));
+    }
+    if (npei_int_sum.s.c1_exc)
+    {
+#if 0
+        PRINT_ERROR("NPEI_INT_SUM[C1_EXC]: Set when the PESC1_DBG_INFO register has a bit\n"
+                    "    set and its cooresponding PESC1_DBG_INFO_EN bit\n"
+                    "    is set.\n");
+#endif
+        __cvmx_interrupt_pescx_dbg_info_decode(1);
+    }
+    if (npei_int_sum.s.c0_exc)
+    {
+#if 0
+        PRINT_ERROR("NPEI_INT_SUM[C0_EXC]: Set when the PESC0_DBG_INFO register has a bit\n"
+                    "    set and its cooresponding PESC0_DBG_INFO_EN bit\n"
+                    "    is set.\n");
+#endif
+        __cvmx_interrupt_pescx_dbg_info_decode(0);
+    }
+    if (npei_int_sum.s.c1_up_wf)
+        PRINT_ERROR("NPEI_INT_SUM[C1_UP_WF]: Received Unsupported P-TLP for filtered window\n"
+                    "    register. Core1.\n");
+    if (npei_int_sum.s.c0_up_wf)
+        PRINT_ERROR("NPEI_INT_SUM[C0_UP_WF]: Received Unsupported P-TLP for filtered window\n"
+                    "    register. Core0.\n");
+    if (npei_int_sum.s.c1_un_wf)
+        PRINT_ERROR("NPEI_INT_SUM[C1_UN_WF]: Received Unsupported N-TLP for filtered window\n"
+                    "    register. Core1.\n");
+    if (npei_int_sum.s.c0_un_wf)
+        PRINT_ERROR("NPEI_INT_SUM[C0_UN_WF]: Received Unsupported N-TLP for filtered window\n"
+                    "    register. Core0.\n");
+    if (npei_int_sum.s.c1_un_bx)
+        PRINT_ERROR("NPEI_INT_SUM[C1_UN_BX]: Received Unsupported N-TLP for unknown Bar.\n"
+                    "    Core 1.\n");
+    if (npei_int_sum.s.c1_un_wi)
+        PRINT_ERROR("NPEI_INT_SUM[C1_UN_WI]: Received Unsupported N-TLP for Window Register.\n"
+                    "    Core 1.\n");
+    if (npei_int_sum.s.c1_un_b2)
+        PRINT_ERROR("NPEI_INT_SUM[C1_UN_B2]: Received Unsupported N-TLP for Bar2.\n"
+                    "    Core 1.\n");
+    if (npei_int_sum.s.c1_un_b1)
+        PRINT_ERROR("NPEI_INT_SUM[C1_UN_B1]: Received Unsupported N-TLP for Bar1.\n"
+                    "    Core 1.\n");
+    if (npei_int_sum.s.c1_un_b0)
+        PRINT_ERROR("NPEI_INT_SUM[C1_UN_B0]: Received Unsupported N-TLP for Bar0.\n"
+                    "    Core 1.\n");
+    if (npei_int_sum.s.c1_up_bx)
+        PRINT_ERROR("NPEI_INT_SUM[C1_UP_BX]: Received Unsupported P-TLP for unknown Bar.\n"
+                    "    Core 1.\n");
+    if (npei_int_sum.s.c1_up_wi)
+        PRINT_ERROR("NPEI_INT_SUM[C1_UP_WI]: Received Unsupported P-TLP for Window Register.\n"
+                    "    Core 1.\n");
+    if (npei_int_sum.s.c1_up_b2)
+        PRINT_ERROR("NPEI_INT_SUM[C1_UP_B2]: Received Unsupported P-TLP for Bar2.\n"
+                    "    Core 1.\n");
+    if (npei_int_sum.s.c1_up_b1)
+        PRINT_ERROR("NPEI_INT_SUM[C1_UP_B1]: Received Unsupported P-TLP for Bar1.\n"
+                    "    Core 1.\n");
+    if (npei_int_sum.s.c1_up_b0)
+        PRINT_ERROR("NPEI_INT_SUM[C1_UP_B0]: Received Unsupported P-TLP for Bar0.\n"
+                    "    Core 1.\n");
+    if (npei_int_sum.s.c0_un_bx)
+        PRINT_ERROR("NPEI_INT_SUM[C0_UN_BX]: Received Unsupported N-TLP for unknown Bar.\n"
+                    "    Core 0.\n");
+    if (npei_int_sum.s.c0_un_wi)
+        PRINT_ERROR("NPEI_INT_SUM[C0_UN_WI]: Received Unsupported N-TLP for Window Register.\n"
+                    "    Core 0.\n");
+    if (npei_int_sum.s.c0_un_b2)
+        PRINT_ERROR("NPEI_INT_SUM[C0_UN_B2]: Received Unsupported N-TLP for Bar2.\n"
+                    "    Core 0.\n");
+    if (npei_int_sum.s.c0_un_b1)
+        PRINT_ERROR("NPEI_INT_SUM[C0_UN_B1]: Received Unsupported N-TLP for Bar1.\n"
+                    "    Core 0.\n");
+    if (npei_int_sum.s.c0_un_b0)
+        PRINT_ERROR("NPEI_INT_SUM[C0_UN_B0]: Received Unsupported N-TLP for Bar0.\n"
+                    "    Core 0.\n");
+    if (npei_int_sum.s.c0_up_bx)
+        PRINT_ERROR("NPEI_INT_SUM[C0_UP_BX]: Received Unsupported P-TLP for unknown Bar.\n"
+                    "    Core 0.\n");
+    if (npei_int_sum.s.c0_up_wi)
+        PRINT_ERROR("NPEI_INT_SUM[C0_UP_WI]: Received Unsupported P-TLP for Window Register.\n"
+                    "    Core 0.\n");
+    if (npei_int_sum.s.c0_up_b2)
+        PRINT_ERROR("NPEI_INT_SUM[C0_UP_B2]: Received Unsupported P-TLP for Bar2.\n"
+                    "    Core 0.\n");
+    if (npei_int_sum.s.c0_up_b1)
+        PRINT_ERROR("NPEI_INT_SUM[C0_UP_B1]: Received Unsupported P-TLP for Bar1.\n"
+                    "    Core 0.\n");
+    if (npei_int_sum.s.c0_up_b0)
+        PRINT_ERROR("NPEI_INT_SUM[C0_UP_B0]: Received Unsupported P-TLP for Bar0.\n"
+                    "    Core 0.\n");
+    if (npei_int_sum.s.c1_hpint)
+        PRINT_ERROR("NPEI_INT_SUM[C1_HPINT]: Hot-Plug Interrupt.\n"
+                    "    Pcie Core 1 (hp_int).\n"
+                    "    This interrupt will only be generated when\n"
+                    "    PCIERC0_CFG034[DLLS_C] is generated. Hot plug is\n"
+                    "    not supported.\n");
+    if (npei_int_sum.s.c1_pmei)
+        PRINT_ERROR("NPEI_INT_SUM[C1_PMEI]: PME Interrupt.\n"
+                    "    Pcie Core 1. (cfg_pme_int)\n");
+    if (npei_int_sum.s.c1_wake)
+        PRINT_ERROR("NPEI_INT_SUM[C1_WAKE]: Wake up from Power Management Unit.\n"
+                    "    Pcie Core 1. (wake_n)\n"
+                    "    Octeon will never generate this interrupt.\n");
+    if (npei_int_sum.s.crs1_dr)
+        PRINT_ERROR("NPEI_INT_SUM[CRS1_DR]: Had a CRS when Retries were disabled.\n");
+    if (npei_int_sum.s.c1_se)
+        PRINT_ERROR("NPEI_INT_SUM[C1_SE]: System Error, RC Mode Only.\n"
+                    "    Pcie Core 1. (cfg_sys_err_rc)\n");
+    if (npei_int_sum.s.crs1_er)
+        PRINT_ERROR("NPEI_INT_SUM[CRS1_ER]: Had a CRS Timeout when Retries were enabled.\n");
+    if (npei_int_sum.s.c1_aeri)
+        PRINT_ERROR("NPEI_INT_SUM[C1_AERI]: Advanced Error Reporting Interrupt, RC Mode Only.\n"
+                    "    Pcie Core 1.\n");
+    if (npei_int_sum.s.c0_hpint)
+        PRINT_ERROR("NPEI_INT_SUM[C0_HPINT]: Hot-Plug Interrupt.\n"
+                    "    Pcie Core 0 (hp_int).\n"
+                    "    This interrupt will only be generated when\n"
+                    "    PCIERC0_CFG034[DLLS_C] is generated. Hot plug is\n"
+                    "    not supported.\n");
+    if (npei_int_sum.s.c0_pmei)
+        PRINT_ERROR("NPEI_INT_SUM[C0_PMEI]: PME Interrupt.\n"
+                    "    Pcie Core 0. (cfg_pme_int)\n");
+    if (npei_int_sum.s.c0_wake)
+        PRINT_ERROR("NPEI_INT_SUM[C0_WAKE]: Wake up from Power Management Unit.\n"
+                    "    Pcie Core 0. (wake_n)\n"
+                    "    Octeon will never generate this interrupt.\n");
+    if (npei_int_sum.s.crs0_dr)
+        PRINT_ERROR("NPEI_INT_SUM[CRS0_DR]: Had a CRS when Retries were disabled.\n");
+    if (npei_int_sum.s.c0_se)
+        PRINT_ERROR("NPEI_INT_SUM[C0_SE]: System Error, RC Mode Only.\n"
+                    "    Pcie Core 0. (cfg_sys_err_rc)\n");
+    if (npei_int_sum.s.crs0_er)
+        PRINT_ERROR("NPEI_INT_SUM[CRS0_ER]: Had a CRS Timeout when Retries were enabled.\n");
+    if (npei_int_sum.s.c0_aeri)
+        PRINT_ERROR("NPEI_INT_SUM[C0_AERI]: Advanced Error Reporting Interrupt, RC Mode Only.\n"
+                    "    Pcie Core 0 (cfg_aer_rc_err_int).\n");
+    if (npei_int_sum.s.ptime)
+        PRINT_ERROR("NPEI_INT_SUM[PTIME]: Packet Timer has an interrupt. Which ports can\n"
+                    "    be found in NPEI_PKT_TIME_INT.\n");
+    if (npei_int_sum.s.pcnt)
+        PRINT_ERROR("NPEI_INT_SUM[PCNT]: Packet Counter has an interrupt. Which ports can\n"
+                    "    be found in NPEI_PKT_CNT_INT.\n");
+    if (npei_int_sum.s.pidbof)
+        PRINT_ERROR("NPEI_INT_SUM[PIDBOF]: Packet Instruction Doorbell count overflowed. Which\n"
+                    "    doorbell can be found in NPEI_INT_INFO[PIDBOF]\n");
+    if (npei_int_sum.s.psldbof)
+        PRINT_ERROR("NPEI_INT_SUM[PSLDBOF]: Packet Scatterlist Doorbell count overflowed. Which\n"
+                    "    doorbell can be found in NPEI_INT_INFO[PSLDBOF]\n");
+    if (npei_int_sum.s.dtime1)
+        PRINT_ERROR("NPEI_INT_SUM[DTIME1]: Whenever NPEI_DMA_CNTS[DMA1] is not 0, the\n"
+                    "    DMA_CNT1 timer increments every core clock. When\n"
+                    "    DMA_CNT1 timer exceeds NPEI_DMA1_INT_LEVEL[TIME],\n"
+                    "    this bit is set. Writing a '1' to this bit also\n"
+                    "    clears the DMA_CNT1 timer.\n");
+    if (npei_int_sum.s.dtime0)
+        PRINT_ERROR("NPEI_INT_SUM[DTIME0]: Whenever NPEI_DMA_CNTS[DMA0] is not 0, the\n"
+                    "    DMA_CNT0 timer increments every core clock. When\n"
+                    "    DMA_CNT0 timer exceeds NPEI_DMA0_INT_LEVEL[TIME],\n"
+                    "    this bit is set. Writing a '1' to this bit also\n"
+                    "    clears the DMA_CNT0 timer.\n");
+    if (npei_int_sum.s.dcnt1)
+        PRINT_ERROR("NPEI_INT_SUM[DCNT1]: This bit indicates that NPEI_DMA_CNTS[DMA1] was/is\n"
+                    "    greater than NPEI_DMA1_INT_LEVEL[CNT].\n");
+    if (npei_int_sum.s.dcnt0)
+        PRINT_ERROR("NPEI_INT_SUM[DCNT0]: This bit indicates that NPEI_DMA_CNTS[DMA0] was/is\n"
+                    "    greater than NPEI_DMA0_INT_LEVEL[CNT].\n");
+    if (npei_int_sum.s.dma1fi)
+        PRINT_ERROR("NPEI_INT_SUM[DMA1FI]: DMA0 set Forced Interrupt.\n");
+    if (npei_int_sum.s.dma0fi)
+        PRINT_ERROR("NPEI_INT_SUM[DMA0FI]: DMA0 set Forced Interrupt.\n");
+    if (npei_int_sum.s.dma4dbo)
+        PRINT_ERROR("NPEI_INT_SUM[DMA4DBO]: DMA4 doorbell overflow.\n"
+                    "    Bit[32] of the doorbell count was set.\n");
+    if (npei_int_sum.s.dma3dbo)
+        PRINT_ERROR("NPEI_INT_SUM[DMA3DBO]: DMA3 doorbell overflow.\n"
+                    "    Bit[32] of the doorbell count was set.\n");
+    if (npei_int_sum.s.dma2dbo)
+        PRINT_ERROR("NPEI_INT_SUM[DMA2DBO]: DMA2 doorbell overflow.\n"
+                    "    Bit[32] of the doorbell count was set.\n");
+    if (npei_int_sum.s.dma1dbo)
+        PRINT_ERROR("NPEI_INT_SUM[DMA1DBO]: DMA1 doorbell overflow.\n"
+                    "    Bit[32] of the doorbell count was set.\n");
+    if (npei_int_sum.s.dma0dbo)
+        PRINT_ERROR("NPEI_INT_SUM[DMA0DBO]: DMA0 doorbell overflow.\n"
+                    "    Bit[32] of the doorbell count was set.\n");
+    if (npei_int_sum.s.iob2big)
+        PRINT_ERROR("NPEI_INT_SUM[IOB2BIG]: A requested IOBDMA is to large.\n");
+    if (npei_int_sum.s.bar0_to)
+        PRINT_ERROR("NPEI_INT_SUM[BAR0_TO]: BAR0 R/W to a NCB device did not receive\n"
+                    "    read-data/commit in 0xffff core clocks.\n");
+    if (npei_int_sum.s.rml_wto)
+        PRINT_ERROR("NPEI_INT_SUM[RML_WTO]: RML write did not get commit in 0xffff core clocks.\n");
+    if (npei_int_sum.s.rml_rto)
+        PRINT_ERROR("NPEI_INT_SUM[RML_RTO]: RML read did not return data in 0xffff core clocks.\n");
+}
+
+
+/**
+ * __cvmx_interrupt_npei_int_enb2_enable enables all interrupt bits in cvmx_npei_int_enb2_t
+ */
+void __cvmx_interrupt_npei_int_enb2_enable(void)
+{
+    cvmx_npei_int_enb2_t npei_int_enb2;
+    /* Reset NPEI_INT_SUM, as NPEI_INT_SUM2 is a read-only copy of NPEI_INT_SUM. */
+    cvmx_write_csr(CVMX_PEXP_NPEI_INT_SUM, cvmx_read_csr(CVMX_PEXP_NPEI_INT_SUM));
+    npei_int_enb2.u64 = 0;
+    if (OCTEON_IS_MODEL(OCTEON_CN56XX))
+    {
+        // Skipping npei_int_enb2.s.reserved_62_63
+        npei_int_enb2.s.int_a = 1;
+        npei_int_enb2.s.c1_ldwn = 1;
+        npei_int_enb2.s.c0_ldwn = 1;
+        npei_int_enb2.s.c1_exc = 1;
+        npei_int_enb2.s.c0_exc = 1;
+        npei_int_enb2.s.c1_up_wf = 1;
+        npei_int_enb2.s.c0_up_wf = 1;
+        npei_int_enb2.s.c1_un_wf = 1;
+        npei_int_enb2.s.c0_un_wf = 1;
+        npei_int_enb2.s.c1_un_bx = 1;
+        npei_int_enb2.s.c1_un_wi = 1;
+        npei_int_enb2.s.c1_un_b2 = 1;
+        npei_int_enb2.s.c1_un_b1 = 1;
+        npei_int_enb2.s.c1_un_b0 = 1;
+        npei_int_enb2.s.c1_up_bx = 1;
+        npei_int_enb2.s.c1_up_wi = 1;
+        npei_int_enb2.s.c1_up_b2 = 1;
+        npei_int_enb2.s.c1_up_b1 = 1;
+        npei_int_enb2.s.c1_up_b0 = 1;
+        npei_int_enb2.s.c0_un_bx = 1;
+        npei_int_enb2.s.c0_un_wi = 1;
+        npei_int_enb2.s.c0_un_b2 = 1;
+        npei_int_enb2.s.c0_un_b1 = 1;
+        npei_int_enb2.s.c0_un_b0 = 1;
+        npei_int_enb2.s.c0_up_bx = 1;
+        npei_int_enb2.s.c0_up_wi = 1;
+        npei_int_enb2.s.c0_up_b2 = 1;
+        npei_int_enb2.s.c0_up_b1 = 1;
+        npei_int_enb2.s.c0_up_b0 = 1;
+        npei_int_enb2.s.c1_hpint = 1;
+        npei_int_enb2.s.c1_pmei = 1;
+        npei_int_enb2.s.c1_wake = 1;
+        npei_int_enb2.s.crs1_dr = 1;
+        npei_int_enb2.s.c1_se = 1;
+        npei_int_enb2.s.crs1_er = 1;
+        npei_int_enb2.s.c1_aeri = 1;
+        npei_int_enb2.s.c0_hpint = 1;
+        npei_int_enb2.s.c0_pmei = 1;
+        npei_int_enb2.s.c0_wake = 1;
+        npei_int_enb2.s.crs0_dr = 1;
+        npei_int_enb2.s.c0_se = 1;
+        npei_int_enb2.s.crs0_er = 1;
+        npei_int_enb2.s.c0_aeri = 1;
+        npei_int_enb2.s.ptime = 1;
+        npei_int_enb2.s.pcnt = 1;
+        npei_int_enb2.s.pidbof = 1;
+        npei_int_enb2.s.psldbof = 1;
+        npei_int_enb2.s.dtime1 = 1;
+        npei_int_enb2.s.dtime0 = 1;
+        npei_int_enb2.s.dcnt1 = 1;
+        npei_int_enb2.s.dcnt0 = 1;
+        npei_int_enb2.s.dma1fi = 1;
+        npei_int_enb2.s.dma0fi = 1;
+        npei_int_enb2.s.dma4dbo = 1;
+        npei_int_enb2.s.dma3dbo = 1;
+        npei_int_enb2.s.dma2dbo = 1;
+        npei_int_enb2.s.dma1dbo = 1;
+        npei_int_enb2.s.dma0dbo = 1;
+        npei_int_enb2.s.iob2big = 1;
+        npei_int_enb2.s.bar0_to = 1;
+        npei_int_enb2.s.rml_wto = 1;
+        npei_int_enb2.s.rml_rto = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN52XX))
+    {
+        // Skipping npei_int_enb2.s.reserved_62_63
+        npei_int_enb2.s.int_a = 1;
+        npei_int_enb2.s.c1_ldwn = 1;
+        npei_int_enb2.s.c0_ldwn = 1;
+        npei_int_enb2.s.c1_exc = 1;
+        npei_int_enb2.s.c0_exc = 1;
+        npei_int_enb2.s.c1_up_wf = 1;
+        npei_int_enb2.s.c0_up_wf = 1;
+        npei_int_enb2.s.c1_un_wf = 1;
+        npei_int_enb2.s.c0_un_wf = 1;
+        npei_int_enb2.s.c1_un_bx = 1;
+        npei_int_enb2.s.c1_un_wi = 1;
+        npei_int_enb2.s.c1_un_b2 = 1;
+        npei_int_enb2.s.c1_un_b1 = 1;
+        npei_int_enb2.s.c1_un_b0 = 1;
+        npei_int_enb2.s.c1_up_bx = 1;
+        npei_int_enb2.s.c1_up_wi = 1;
+        npei_int_enb2.s.c1_up_b2 = 1;
+        npei_int_enb2.s.c1_up_b1 = 1;
+        npei_int_enb2.s.c1_up_b0 = 1;
+        npei_int_enb2.s.c0_un_bx = 1;
+        npei_int_enb2.s.c0_un_wi = 1;
+        npei_int_enb2.s.c0_un_b2 = 1;
+        npei_int_enb2.s.c0_un_b1 = 1;
+        npei_int_enb2.s.c0_un_b0 = 1;
+        npei_int_enb2.s.c0_up_bx = 1;
+        npei_int_enb2.s.c0_up_wi = 1;
+        npei_int_enb2.s.c0_up_b2 = 1;
+        npei_int_enb2.s.c0_up_b1 = 1;
+        npei_int_enb2.s.c0_up_b0 = 1;
+        npei_int_enb2.s.c1_hpint = 1;
+        npei_int_enb2.s.c1_pmei = 1;
+        npei_int_enb2.s.c1_wake = 1;
+        npei_int_enb2.s.crs1_dr = 1;
+        npei_int_enb2.s.c1_se = 1;
+        npei_int_enb2.s.crs1_er = 1;
+        npei_int_enb2.s.c1_aeri = 1;
+        npei_int_enb2.s.c0_hpint = 1;
+        npei_int_enb2.s.c0_pmei = 1;
+        npei_int_enb2.s.c0_wake = 1;
+        npei_int_enb2.s.crs0_dr = 1;
+        npei_int_enb2.s.c0_se = 1;
+        npei_int_enb2.s.crs0_er = 1;
+        npei_int_enb2.s.c0_aeri = 1;
+        npei_int_enb2.s.ptime = 1;
+        npei_int_enb2.s.pcnt = 1;
+        npei_int_enb2.s.pidbof = 1;
+        npei_int_enb2.s.psldbof = 1;
+        npei_int_enb2.s.dtime1 = 1;
+        npei_int_enb2.s.dtime0 = 1;
+        npei_int_enb2.s.dcnt1 = 1;
+        npei_int_enb2.s.dcnt0 = 1;
+        npei_int_enb2.s.dma1fi = 1;
+        npei_int_enb2.s.dma0fi = 1;
+        // Skipping npei_int_enb2.s.reserved_8_8
+        npei_int_enb2.s.dma3dbo = 1;
+        npei_int_enb2.s.dma2dbo = 1;
+        npei_int_enb2.s.dma1dbo = 1;
+        npei_int_enb2.s.dma0dbo = 1;
+        npei_int_enb2.s.iob2big = 1;
+        npei_int_enb2.s.bar0_to = 1;
+        npei_int_enb2.s.rml_wto = 1;
+        npei_int_enb2.s.rml_rto = 1;
+    }
+    cvmx_write_csr(CVMX_PEXP_NPEI_INT_ENB2, npei_int_enb2.u64);
+    __cvmx_interrupt_pescx_dbg_info_en_enable(0);
+
+    /* Skip the 2nd port on CN52XX if port 0 is in 4 lane mode */
+    if (OCTEON_IS_MODEL(OCTEON_CN52XX))
+    {
+        cvmx_npei_dbg_data_t npei_dbg_data;
+        npei_dbg_data.u64 = cvmx_read_csr(CVMX_PEXP_NPEI_DBG_DATA);
+        if (npei_dbg_data.cn52xx.qlm0_link_width)
+            return;
+    }
+    __cvmx_interrupt_pescx_dbg_info_en_enable(1);
+}
+
+
+/**
+ * __cvmx_interrupt_npi_int_enb_enable enables all interrupt bits in cvmx_npi_int_enb_t
+ */
+void __cvmx_interrupt_npi_int_enb_enable(void)
+{
+    cvmx_npi_int_enb_t npi_int_enb;
+    cvmx_write_csr(CVMX_NPI_INT_SUM, cvmx_read_csr(CVMX_NPI_INT_SUM));
+    npi_int_enb.u64 = 0;
+    if (OCTEON_IS_MODEL(OCTEON_CN30XX))
+    {
+        // Skipping npi_int_enb.s.reserved_62_63
+        npi_int_enb.s.q1_a_f = 1;
+        npi_int_enb.s.q1_s_e = 1;
+        npi_int_enb.s.pdf_p_f = 1;
+        npi_int_enb.s.pdf_p_e = 1;
+        npi_int_enb.s.pcf_p_f = 1;
+        npi_int_enb.s.pcf_p_e = 1;
+        npi_int_enb.s.rdx_s_e = 1;
+        npi_int_enb.s.rwx_s_e = 1;
+        npi_int_enb.s.pnc_a_f = 1;
+        npi_int_enb.s.pnc_s_e = 1;
+        npi_int_enb.s.com_a_f = 1;
+        npi_int_enb.s.com_s_e = 1;
+        npi_int_enb.s.q3_a_f = 1;
+        npi_int_enb.s.q3_s_e = 1;
+        npi_int_enb.s.q2_a_f = 1;
+        npi_int_enb.s.q2_s_e = 1;
+        npi_int_enb.s.pcr_a_f = 1;
+        npi_int_enb.s.pcr_s_e = 1;
+        npi_int_enb.s.fcr_a_f = 1;
+        npi_int_enb.s.fcr_s_e = 1;
+        npi_int_enb.s.iobdma = 1;
+        npi_int_enb.s.p_dperr = 1;
+        npi_int_enb.s.win_rto = 1;
+        // Skipping npi_int_enb.s.reserved_36_38
+        npi_int_enb.s.i0_pperr = 1;
+        // Skipping npi_int_enb.s.reserved_32_34
+        npi_int_enb.s.p0_ptout = 1;
+        // Skipping npi_int_enb.s.reserved_28_30
+        npi_int_enb.s.p0_pperr = 1;
+        // Skipping npi_int_enb.s.reserved_24_26
+        npi_int_enb.s.g0_rtout = 1;
+        // Skipping npi_int_enb.s.reserved_20_22
+        npi_int_enb.s.p0_perr = 1;
+        // Skipping npi_int_enb.s.reserved_16_18
+        npi_int_enb.s.p0_rtout = 1;
+        // Skipping npi_int_enb.s.reserved_12_14
+        npi_int_enb.s.i0_overf = 1;
+        // Skipping npi_int_enb.s.reserved_8_10
+        npi_int_enb.s.i0_rtout = 1;
+        // Skipping npi_int_enb.s.reserved_4_6
+        npi_int_enb.s.po0_2sml = 1;
+        npi_int_enb.s.pci_rsl = 1;
+        npi_int_enb.s.rml_wto = 1;
+        npi_int_enb.s.rml_rto = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN50XX))
+    {
+        // Skipping npi_int_enb.s.reserved_62_63
+        npi_int_enb.s.q1_a_f = 1;
+        npi_int_enb.s.q1_s_e = 1;
+        npi_int_enb.s.pdf_p_f = 1;
+        npi_int_enb.s.pdf_p_e = 1;
+        npi_int_enb.s.pcf_p_f = 1;
+        npi_int_enb.s.pcf_p_e = 1;
+        npi_int_enb.s.rdx_s_e = 1;
+        npi_int_enb.s.rwx_s_e = 1;
+        npi_int_enb.s.pnc_a_f = 1;
+        npi_int_enb.s.pnc_s_e = 1;
+        npi_int_enb.s.com_a_f = 1;
+        npi_int_enb.s.com_s_e = 1;
+        npi_int_enb.s.q3_a_f = 1;
+        npi_int_enb.s.q3_s_e = 1;
+        npi_int_enb.s.q2_a_f = 1;
+        npi_int_enb.s.q2_s_e = 1;
+        npi_int_enb.s.pcr_a_f = 1;
+        npi_int_enb.s.pcr_s_e = 1;
+        npi_int_enb.s.fcr_a_f = 1;
+        npi_int_enb.s.fcr_s_e = 1;
+        npi_int_enb.s.iobdma = 1;
+        npi_int_enb.s.p_dperr = 1;
+        npi_int_enb.s.win_rto = 1;
+        // Skipping npi_int_enb.s.reserved_37_38
+        npi_int_enb.s.i1_pperr = 1;
+        npi_int_enb.s.i0_pperr = 1;
+        // Skipping npi_int_enb.s.reserved_33_34
+        npi_int_enb.s.p1_ptout = 1;
+        npi_int_enb.s.p0_ptout = 1;
+        // Skipping npi_int_enb.s.reserved_29_30
+        npi_int_enb.s.p1_pperr = 1;
+        npi_int_enb.s.p0_pperr = 1;
+        // Skipping npi_int_enb.s.reserved_25_26
+        npi_int_enb.s.g1_rtout = 1;
+        npi_int_enb.s.g0_rtout = 1;
+        // Skipping npi_int_enb.s.reserved_21_22
+        npi_int_enb.s.p1_perr = 1;
+        npi_int_enb.s.p0_perr = 1;
+        // Skipping npi_int_enb.s.reserved_17_18
+        npi_int_enb.s.p1_rtout = 1;
+        npi_int_enb.s.p0_rtout = 1;
+        // Skipping npi_int_enb.s.reserved_13_14
+        npi_int_enb.s.i1_overf = 1;
+        npi_int_enb.s.i0_overf = 1;
+        // Skipping npi_int_enb.s.reserved_9_10
+        npi_int_enb.s.i1_rtout = 1;
+        npi_int_enb.s.i0_rtout = 1;
+        // Skipping npi_int_enb.s.reserved_5_6
+        npi_int_enb.s.po1_2sml = 1;
+        npi_int_enb.s.po0_2sml = 1;
+        npi_int_enb.s.pci_rsl = 1;
+        npi_int_enb.s.rml_wto = 1;
+        npi_int_enb.s.rml_rto = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN38XX))
+    {
+        // Skipping npi_int_enb.s.reserved_62_63
+        npi_int_enb.s.q1_a_f = 1;
+        npi_int_enb.s.q1_s_e = 1;
+        npi_int_enb.s.pdf_p_f = 1;
+        npi_int_enb.s.pdf_p_e = 1;
+        npi_int_enb.s.pcf_p_f = 1;
+        npi_int_enb.s.pcf_p_e = 1;
+        npi_int_enb.s.rdx_s_e = 1;
+        npi_int_enb.s.rwx_s_e = 1;
+        npi_int_enb.s.pnc_a_f = 1;
+        npi_int_enb.s.pnc_s_e = 1;
+        npi_int_enb.s.com_a_f = 1;
+        npi_int_enb.s.com_s_e = 1;
+        npi_int_enb.s.q3_a_f = 1;
+        npi_int_enb.s.q3_s_e = 1;
+        npi_int_enb.s.q2_a_f = 1;
+        npi_int_enb.s.q2_s_e = 1;
+        npi_int_enb.s.pcr_a_f = 1;
+        npi_int_enb.s.pcr_s_e = 1;
+        npi_int_enb.s.fcr_a_f = 1;
+        npi_int_enb.s.fcr_s_e = 1;
+        npi_int_enb.s.iobdma = 1;
+        npi_int_enb.s.p_dperr = 1;
+        npi_int_enb.s.win_rto = 1;
+        npi_int_enb.s.i3_pperr = 1;
+        npi_int_enb.s.i2_pperr = 1;
+        npi_int_enb.s.i1_pperr = 1;
+        npi_int_enb.s.i0_pperr = 1;
+        npi_int_enb.s.p3_ptout = 1;
+        npi_int_enb.s.p2_ptout = 1;
+        npi_int_enb.s.p1_ptout = 1;
+        npi_int_enb.s.p0_ptout = 1;
+        npi_int_enb.s.p3_pperr = 1;
+        npi_int_enb.s.p2_pperr = 1;
+        npi_int_enb.s.p1_pperr = 1;
+        npi_int_enb.s.p0_pperr = 1;
+        npi_int_enb.s.g3_rtout = 1;
+        npi_int_enb.s.g2_rtout = 1;
+        npi_int_enb.s.g1_rtout = 1;
+        npi_int_enb.s.g0_rtout = 1;
+        npi_int_enb.s.p3_perr = 1;
+        npi_int_enb.s.p2_perr = 1;
+        npi_int_enb.s.p1_perr = 1;
+        npi_int_enb.s.p0_perr = 1;
+        npi_int_enb.s.p3_rtout = 1;
+        npi_int_enb.s.p2_rtout = 1;
+        npi_int_enb.s.p1_rtout = 1;
+        npi_int_enb.s.p0_rtout = 1;
+        npi_int_enb.s.i3_overf = 1;
+        npi_int_enb.s.i2_overf = 1;
+        npi_int_enb.s.i1_overf = 1;
+        npi_int_enb.s.i0_overf = 1;
+        npi_int_enb.s.i3_rtout = 1;
+        npi_int_enb.s.i2_rtout = 1;
+        npi_int_enb.s.i1_rtout = 1;
+        npi_int_enb.s.i0_rtout = 1;
+        npi_int_enb.s.po3_2sml = 1;
+        npi_int_enb.s.po2_2sml = 1;
+        npi_int_enb.s.po1_2sml = 1;
+        npi_int_enb.s.po0_2sml = 1;
+        npi_int_enb.s.pci_rsl = 1;
+        npi_int_enb.s.rml_wto = 1;
+        npi_int_enb.s.rml_rto = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN31XX))
+    {
+        // Skipping npi_int_enb.s.reserved_62_63
+        npi_int_enb.s.q1_a_f = 1;
+        npi_int_enb.s.q1_s_e = 1;
+        npi_int_enb.s.pdf_p_f = 1;
+        npi_int_enb.s.pdf_p_e = 1;
+        npi_int_enb.s.pcf_p_f = 1;
+        npi_int_enb.s.pcf_p_e = 1;
+        npi_int_enb.s.rdx_s_e = 1;
+        npi_int_enb.s.rwx_s_e = 1;
+        npi_int_enb.s.pnc_a_f = 1;
+        npi_int_enb.s.pnc_s_e = 1;
+        npi_int_enb.s.com_a_f = 1;
+        npi_int_enb.s.com_s_e = 1;
+        npi_int_enb.s.q3_a_f = 1;
+        npi_int_enb.s.q3_s_e = 1;
+        npi_int_enb.s.q2_a_f = 1;
+        npi_int_enb.s.q2_s_e = 1;
+        npi_int_enb.s.pcr_a_f = 1;
+        npi_int_enb.s.pcr_s_e = 1;
+        npi_int_enb.s.fcr_a_f = 1;
+        npi_int_enb.s.fcr_s_e = 1;
+        npi_int_enb.s.iobdma = 1;
+        npi_int_enb.s.p_dperr = 1;
+        npi_int_enb.s.win_rto = 1;
+        // Skipping npi_int_enb.s.reserved_37_38
+        npi_int_enb.s.i1_pperr = 1;
+        npi_int_enb.s.i0_pperr = 1;
+        // Skipping npi_int_enb.s.reserved_33_34
+        npi_int_enb.s.p1_ptout = 1;
+        npi_int_enb.s.p0_ptout = 1;
+        // Skipping npi_int_enb.s.reserved_29_30
+        npi_int_enb.s.p1_pperr = 1;
+        npi_int_enb.s.p0_pperr = 1;
+        // Skipping npi_int_enb.s.reserved_25_26
+        npi_int_enb.s.g1_rtout = 1;
+        npi_int_enb.s.g0_rtout = 1;
+        // Skipping npi_int_enb.s.reserved_21_22
+        npi_int_enb.s.p1_perr = 1;
+        npi_int_enb.s.p0_perr = 1;
+        // Skipping npi_int_enb.s.reserved_17_18
+        npi_int_enb.s.p1_rtout = 1;
+        npi_int_enb.s.p0_rtout = 1;
+        // Skipping npi_int_enb.s.reserved_13_14
+        npi_int_enb.s.i1_overf = 1;
+        npi_int_enb.s.i0_overf = 1;
+        // Skipping npi_int_enb.s.reserved_9_10
+        npi_int_enb.s.i1_rtout = 1;
+        npi_int_enb.s.i0_rtout = 1;
+        // Skipping npi_int_enb.s.reserved_5_6
+        npi_int_enb.s.po1_2sml = 1;
+        npi_int_enb.s.po0_2sml = 1;
+        npi_int_enb.s.pci_rsl = 1;
+        npi_int_enb.s.rml_wto = 1;
+        npi_int_enb.s.rml_rto = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN58XX))
+    {
+        // Skipping npi_int_enb.s.reserved_62_63
+        npi_int_enb.s.q1_a_f = 1;
+        npi_int_enb.s.q1_s_e = 1;
+        npi_int_enb.s.pdf_p_f = 1;
+        npi_int_enb.s.pdf_p_e = 1;
+        npi_int_enb.s.pcf_p_f = 1;
+        npi_int_enb.s.pcf_p_e = 1;
+        npi_int_enb.s.rdx_s_e = 1;
+        npi_int_enb.s.rwx_s_e = 1;
+        npi_int_enb.s.pnc_a_f = 1;
+        npi_int_enb.s.pnc_s_e = 1;
+        npi_int_enb.s.com_a_f = 1;
+        npi_int_enb.s.com_s_e = 1;
+        npi_int_enb.s.q3_a_f = 1;
+        npi_int_enb.s.q3_s_e = 1;
+        npi_int_enb.s.q2_a_f = 1;
+        npi_int_enb.s.q2_s_e = 1;
+        npi_int_enb.s.pcr_a_f = 1;
+        npi_int_enb.s.pcr_s_e = 1;
+        npi_int_enb.s.fcr_a_f = 1;
+        npi_int_enb.s.fcr_s_e = 1;
+        npi_int_enb.s.iobdma = 1;
+        npi_int_enb.s.p_dperr = 1;
+        npi_int_enb.s.win_rto = 1;
+        npi_int_enb.s.i3_pperr = 1;
+        npi_int_enb.s.i2_pperr = 1;
+        npi_int_enb.s.i1_pperr = 1;
+        npi_int_enb.s.i0_pperr = 1;
+        npi_int_enb.s.p3_ptout = 1;
+        npi_int_enb.s.p2_ptout = 1;
+        npi_int_enb.s.p1_ptout = 1;
+        npi_int_enb.s.p0_ptout = 1;
+        npi_int_enb.s.p3_pperr = 1;
+        npi_int_enb.s.p2_pperr = 1;
+        npi_int_enb.s.p1_pperr = 1;
+        npi_int_enb.s.p0_pperr = 1;
+        npi_int_enb.s.g3_rtout = 1;
+        npi_int_enb.s.g2_rtout = 1;
+        npi_int_enb.s.g1_rtout = 1;
+        npi_int_enb.s.g0_rtout = 1;
+        npi_int_enb.s.p3_perr = 1;
+        npi_int_enb.s.p2_perr = 1;
+        npi_int_enb.s.p1_perr = 1;
+        npi_int_enb.s.p0_perr = 1;
+        npi_int_enb.s.p3_rtout = 1;
+        npi_int_enb.s.p2_rtout = 1;
+        npi_int_enb.s.p1_rtout = 1;
+        npi_int_enb.s.p0_rtout = 1;
+        npi_int_enb.s.i3_overf = 1;
+        npi_int_enb.s.i2_overf = 1;
+        npi_int_enb.s.i1_overf = 1;
+        npi_int_enb.s.i0_overf = 1;
+        npi_int_enb.s.i3_rtout = 1;
+        npi_int_enb.s.i2_rtout = 1;
+        npi_int_enb.s.i1_rtout = 1;
+        npi_int_enb.s.i0_rtout = 1;
+        npi_int_enb.s.po3_2sml = 1;
+        npi_int_enb.s.po2_2sml = 1;
+        npi_int_enb.s.po1_2sml = 1;
+        npi_int_enb.s.po0_2sml = 1;
+        npi_int_enb.s.pci_rsl = 1;
+        npi_int_enb.s.rml_wto = 1;
+        npi_int_enb.s.rml_rto = 1;
+    }
+    cvmx_write_csr(CVMX_NPI_INT_ENB, npi_int_enb.u64);
+    __cvmx_interrupt_pci_int_enb2_enable();
+}
+
+
+/**
+ * __cvmx_interrupt_npi_int_sum_decode decodes all interrupt bits in cvmx_npi_int_sum_t
+ */
+void __cvmx_interrupt_npi_int_sum_decode(void)
+{
+    cvmx_npi_int_sum_t npi_int_sum;
+    npi_int_sum.u64 = cvmx_read_csr(CVMX_NPI_INT_SUM);
+    cvmx_write_csr(CVMX_NPI_INT_SUM, npi_int_sum.u64);
+    npi_int_sum.u64 &= cvmx_read_csr(CVMX_NPI_INT_ENB);
+    // Skipping npi_int_sum.s.reserved_62_63
+    if (npi_int_sum.s.q1_a_f)
+        PRINT_ERROR("NPI_INT_SUM[Q1_A_F]: Attempted to add when Queue-1 FIFO is full.\n"
+                    "    PASS3 Field.\n");
+    if (npi_int_sum.s.q1_s_e)
+        PRINT_ERROR("NPI_INT_SUM[Q1_S_E]: Attempted to subtract when Queue-1 FIFO is empty.\n"
+                    "    PASS3 Field.\n");
+    if (npi_int_sum.s.pdf_p_f)
+        PRINT_ERROR("NPI_INT_SUM[PDF_P_F]: Attempted to push a full PCN-DATA-FIFO.\n"
+                    "    PASS3 Field.\n");
+    if (npi_int_sum.s.pdf_p_e)
+        PRINT_ERROR("NPI_INT_SUM[PDF_P_E]: Attempted to pop an empty PCN-DATA-FIFO.\n"
+                    "    PASS3 Field.\n");
+    if (npi_int_sum.s.pcf_p_f)
+        PRINT_ERROR("NPI_INT_SUM[PCF_P_F]: Attempted to push a full PCN-CNT-FIFO.\n"
+                    "    PASS3 Field.\n");
+    if (npi_int_sum.s.pcf_p_e)
+        PRINT_ERROR("NPI_INT_SUM[PCF_P_E]: Attempted to pop an empty PCN-CNT-FIFO.\n"
+                    "    PASS3 Field.\n");
+    if (npi_int_sum.s.rdx_s_e)
+        PRINT_ERROR("NPI_INT_SUM[RDX_S_E]: Attempted to subtract when DPI-XFR-Wait count is 0.\n"
+                    "    PASS3 Field.\n");
+    if (npi_int_sum.s.rwx_s_e)
+        PRINT_ERROR("NPI_INT_SUM[RWX_S_E]: Attempted to subtract when RDN-XFR-Wait count is 0.\n"
+                    "    PASS3 Field.\n");
+    if (npi_int_sum.s.pnc_a_f)
+        PRINT_ERROR("NPI_INT_SUM[PNC_A_F]: Attempted to add when PNI-NPI Credits are max.\n"
+                    "    PASS3 Field.\n");
+    if (npi_int_sum.s.pnc_s_e)
+        PRINT_ERROR("NPI_INT_SUM[PNC_S_E]: Attempted to subtract when PNI-NPI Credits are 0.\n"
+                    "    PASS3 Field.\n");
+    if (npi_int_sum.s.com_a_f)
+        PRINT_ERROR("NPI_INT_SUM[COM_A_F]: Attempted to add when PCN-Commit Counter is max.\n"
+                    "    PASS3 Field.\n");
+    if (npi_int_sum.s.com_s_e)
+        PRINT_ERROR("NPI_INT_SUM[COM_S_E]: Attempted to subtract when PCN-Commit Counter is 0.\n"
+                    "    PASS3 Field.\n");
+    if (npi_int_sum.s.q3_a_f)
+        PRINT_ERROR("NPI_INT_SUM[Q3_A_F]: Attempted to add when Queue-3 FIFO is full.\n"
+                    "    PASS3 Field.\n");
+    if (npi_int_sum.s.q3_s_e)
+        PRINT_ERROR("NPI_INT_SUM[Q3_S_E]: Attempted to subtract when Queue-3 FIFO is empty.\n"
+                    "    PASS3 Field.\n");
+    if (npi_int_sum.s.q2_a_f)
+        PRINT_ERROR("NPI_INT_SUM[Q2_A_F]: Attempted to add when Queue-2 FIFO is full.\n"
+                    "    PASS3 Field.\n");
+    if (npi_int_sum.s.q2_s_e)
+        PRINT_ERROR("NPI_INT_SUM[Q2_S_E]: Attempted to subtract when Queue-2 FIFO is empty.\n"
+                    "    PASS3 Field.\n");
+    if (npi_int_sum.s.pcr_a_f)
+        PRINT_ERROR("NPI_INT_SUM[PCR_A_F]: Attempted to add when POW Credits is full.\n"
+                    "    PASS3 Field.\n");
+    if (npi_int_sum.s.pcr_s_e)
+        PRINT_ERROR("NPI_INT_SUM[PCR_S_E]: Attempted to subtract when POW Credits is empty.\n"
+                    "    PASS3 Field.\n");
+    if (npi_int_sum.s.fcr_a_f)
+        PRINT_ERROR("NPI_INT_SUM[FCR_A_F]: Attempted to add when FPA Credits is full.\n"
+                    "    PASS3 Field.\n");
+    if (npi_int_sum.s.fcr_s_e)
+        PRINT_ERROR("NPI_INT_SUM[FCR_S_E]: Attempted to subtract when FPA Credits is empty.\n"
+                    "    PASS3 Field.\n");
+    if (npi_int_sum.s.iobdma)
+        PRINT_ERROR("NPI_INT_SUM[IOBDMA]: Requested IOBDMA read size exceeded 128 words.\n");
+    if (npi_int_sum.s.p_dperr)
+        PRINT_ERROR("NPI_INT_SUM[P_DPERR]: If a parity error occured on data written to L2C\n"
+                    "    from the PCI this bit may be set.\n");
+    if (npi_int_sum.s.win_rto)
+        PRINT_ERROR("NPI_INT_SUM[WIN_RTO]: Windowed Load Timed Out.\n");
+    if (npi_int_sum.s.i3_pperr)
+        PRINT_ERROR("NPI_INT_SUM[I3_PPERR]: If a parity error occured on the port's instruction\n"
+                    "    this bit may be set.\n");
+    if (npi_int_sum.s.i2_pperr)
+        PRINT_ERROR("NPI_INT_SUM[I2_PPERR]: If a parity error occured on the port's instruction\n"
+                    "    this bit may be set.\n");
+    if (npi_int_sum.s.i1_pperr)
+        PRINT_ERROR("NPI_INT_SUM[I1_PPERR]: If a parity error occured on the port's instruction\n"
+                    "    this bit may be set.\n");
+    if (npi_int_sum.s.i0_pperr)
+        PRINT_ERROR("NPI_INT_SUM[I0_PPERR]: If a parity error occured on the port's instruction\n"
+                    "    this bit may be set.\n");
+    if (npi_int_sum.s.p3_ptout)
+        PRINT_ERROR("NPI_INT_SUM[P3_PTOUT]: Port-3 output had a read timeout on a DATA/INFO\n"
+                    "    pair.\n");
+    if (npi_int_sum.s.p2_ptout)
+        PRINT_ERROR("NPI_INT_SUM[P2_PTOUT]: Port-2 output had a read timeout on a DATA/INFO\n"
+                    "    pair.\n");
+    if (npi_int_sum.s.p1_ptout)
+        PRINT_ERROR("NPI_INT_SUM[P1_PTOUT]: Port-1 output had a read timeout on a DATA/INFO\n"
+                    "    pair.\n");
+    if (npi_int_sum.s.p0_ptout)
+        PRINT_ERROR("NPI_INT_SUM[P0_PTOUT]: Port-0 output had a read timeout on a DATA/INFO\n"
+                    "    pair.\n");
+    if (npi_int_sum.s.p3_pperr)
+        PRINT_ERROR("NPI_INT_SUM[P3_PPERR]: If a parity error occured on the port DATA/INFO\n"
+                    "    pointer-pair, this bit may be set.\n");
+    if (npi_int_sum.s.p2_pperr)
+        PRINT_ERROR("NPI_INT_SUM[P2_PPERR]: If a parity error occured on the port DATA/INFO\n"
+                    "    pointer-pair, this bit may be set.\n");
+    if (npi_int_sum.s.p1_pperr)
+        PRINT_ERROR("NPI_INT_SUM[P1_PPERR]: If a parity error occured on the port DATA/INFO\n"
+                    "    pointer-pair, this bit may be set.\n");
+    if (npi_int_sum.s.p0_pperr)
+        PRINT_ERROR("NPI_INT_SUM[P0_PPERR]: If a parity error occured on the port DATA/INFO\n"
+                    "    pointer-pair, this bit may be set.\n");
+    if (npi_int_sum.s.g3_rtout)
+        PRINT_ERROR("NPI_INT_SUM[G3_RTOUT]: Port-3 had a read timeout while attempting to\n"
+                    "    read a gather list.\n");
+    if (npi_int_sum.s.g2_rtout)
+        PRINT_ERROR("NPI_INT_SUM[G2_RTOUT]: Port-2 had a read timeout while attempting to\n"
+                    "    read a gather list.\n");
+    if (npi_int_sum.s.g1_rtout)
+        PRINT_ERROR("NPI_INT_SUM[G1_RTOUT]: Port-1 had a read timeout while attempting to\n"
+                    "    read a gather list.\n");
+    if (npi_int_sum.s.g0_rtout)
+        PRINT_ERROR("NPI_INT_SUM[G0_RTOUT]: Port-0 had a read timeout while attempting to\n"
+                    "    read a gather list.\n");
+    if (npi_int_sum.s.p3_perr)
+        PRINT_ERROR("NPI_INT_SUM[P3_PERR]: If a parity error occured on the port's packet\n"
+                    "    data this bit may be set.\n");
+    if (npi_int_sum.s.p2_perr)
+        PRINT_ERROR("NPI_INT_SUM[P2_PERR]: If a parity error occured on the port's packet\n"
+                    "    data this bit may be set.\n");
+    if (npi_int_sum.s.p1_perr)
+        PRINT_ERROR("NPI_INT_SUM[P1_PERR]: If a parity error occured on the port's packet\n"
+                    "    data this bit may be set.\n");
+    if (npi_int_sum.s.p0_perr)
+        PRINT_ERROR("NPI_INT_SUM[P0_PERR]: If a parity error occured on the port's packet\n"
+                    "    data this bit may be set.\n");
+    if (npi_int_sum.s.p3_rtout)
+        PRINT_ERROR("NPI_INT_SUM[P3_RTOUT]: Port-3 had a read timeout while attempting to\n"
+                    "    read packet data.\n");
+    if (npi_int_sum.s.p2_rtout)
+        PRINT_ERROR("NPI_INT_SUM[P2_RTOUT]: Port-2 had a read timeout while attempting to\n"
+                    "    read packet data.\n");
+    if (npi_int_sum.s.p1_rtout)
+        PRINT_ERROR("NPI_INT_SUM[P1_RTOUT]: Port-1 had a read timeout while attempting to\n"
+                    "    read packet data.\n");
+    if (npi_int_sum.s.p0_rtout)
+        PRINT_ERROR("NPI_INT_SUM[P0_RTOUT]: Port-0 had a read timeout while attempting to\n"
+                    "    read packet data.\n");
+    if (npi_int_sum.s.i3_overf)
+        PRINT_ERROR("NPI_INT_SUM[I3_OVERF]: Port-3 had a doorbell overflow. Bit[31] of the\n"
+                    "    doorbell count was set.\n");
+    if (npi_int_sum.s.i2_overf)
+        PRINT_ERROR("NPI_INT_SUM[I2_OVERF]: Port-2 had a doorbell overflow. Bit[31] of the\n"
+                    "    doorbell count was set.\n");
+    if (npi_int_sum.s.i1_overf)
+        PRINT_ERROR("NPI_INT_SUM[I1_OVERF]: Port-1 had a doorbell overflow. Bit[31] of the\n"
+                    "    doorbell count was set.\n");
+    if (npi_int_sum.s.i0_overf)
+        PRINT_ERROR("NPI_INT_SUM[I0_OVERF]: Port-0 had a doorbell overflow. Bit[31] of the\n"
+                    "    doorbell count was set.\n");
+    if (npi_int_sum.s.i3_rtout)
+        PRINT_ERROR("NPI_INT_SUM[I3_RTOUT]: Port-3 had a read timeout while attempting to\n"
+                    "    read instructions.\n");
+    if (npi_int_sum.s.i2_rtout)
+        PRINT_ERROR("NPI_INT_SUM[I2_RTOUT]: Port-2 had a read timeout while attempting to\n"
+                    "    read instructions.\n");
+    if (npi_int_sum.s.i1_rtout)
+        PRINT_ERROR("NPI_INT_SUM[I1_RTOUT]: Port-1 had a read timeout while attempting to\n"
+                    "    read instructions.\n");
+    if (npi_int_sum.s.i0_rtout)
+        PRINT_ERROR("NPI_INT_SUM[I0_RTOUT]: Port-0 had a read timeout while attempting to\n"
+                    "    read instructions.\n");
+    if (npi_int_sum.s.po3_2sml)
+        PRINT_ERROR("NPI_INT_SUM[PO3_2SML]: The packet being sent out on Port3 is smaller\n"
+                    "    than the NPI_BUFF_SIZE_OUTPUT3[ISIZE] field.\n");
+    if (npi_int_sum.s.po2_2sml)
+        PRINT_ERROR("NPI_INT_SUM[PO2_2SML]: The packet being sent out on Port2 is smaller\n"
+                    "    than the NPI_BUFF_SIZE_OUTPUT2[ISIZE] field.\n");
+    if (npi_int_sum.s.po1_2sml)
+        PRINT_ERROR("NPI_INT_SUM[PO1_2SML]: The packet being sent out on Port1 is smaller\n"
+                    "    than the NPI_BUFF_SIZE_OUTPUT1[ISIZE] field.\n");
+    if (npi_int_sum.s.po0_2sml)
+        PRINT_ERROR("NPI_INT_SUM[PO0_2SML]: The packet being sent out on Port0 is smaller\n"
+                    "    than the NPI_BUFF_SIZE_OUTPUT0[ISIZE] field.\n");
+    if (npi_int_sum.s.pci_rsl)
+    {
+#if 0
+        PRINT_ERROR("NPI_INT_SUM[PCI_RSL]: This '1' when a bit in PCI_INT_SUM2 is SET and the\n"
+                    "    corresponding bit in the PCI_INT_ENB2 is SET.\n");
+#endif
+        __cvmx_interrupt_pci_int_sum2_decode();
+    }
+    if (npi_int_sum.s.rml_wto)
+        PRINT_ERROR("NPI_INT_SUM[RML_WTO]: Set '1' when the RML does not receive a commit\n"
+                    "    back from a RSL after sending a write command to\n"
+                    "    a RSL.\n");
+    if (npi_int_sum.s.rml_rto)
+        PRINT_ERROR("NPI_INT_SUM[RML_RTO]: Set '1' when the RML does not receive read data\n"
+                    "    back from a RSL after sending a read command to\n"
+                    "    a RSL.\n");
+}
+
+
+/**
+ * __cvmx_interrupt_pci_int_enb2_enable enables all interrupt bits in cvmx_pci_int_enb2_t
+ */
+void __cvmx_interrupt_pci_int_enb2_enable(void)
+{
+    cvmx_pci_int_enb2_t pci_int_enb2;
+    cvmx_write_csr(CVMX_NPI_PCI_INT_SUM2, cvmx_read_csr(CVMX_NPI_PCI_INT_SUM2));
+    pci_int_enb2.u64 = 0;
+    if (OCTEON_IS_MODEL(OCTEON_CN30XX))
+    {
+        // Skipping pci_int_enb2.s.reserved_34_63
+        pci_int_enb2.s.ill_rd = 1;
+        pci_int_enb2.s.ill_wr = 1;
+        pci_int_enb2.s.win_wr = 1;
+        // pci_int_enb2.s.dma1_fi = 1; // Not an error condition
+        // pci_int_enb2.s.dma0_fi = 1; // Not an error condition
+        // pci_int_enb2.s.rdtime1 = 1; // Not an error condition
+        // pci_int_enb2.s.rdtime0 = 1; // Not an error condition
+        // pci_int_enb2.s.rdcnt1 = 1; // Not an error condition
+        // pci_int_enb2.s.rdcnt0 = 1; // Not an error condition
+        // Skipping pci_int_enb2.s.reserved_22_24
+        // pci_int_enb2.s.rptime0 = 1; // Not an error condition
+        // Skipping pci_int_enb2.s.reserved_18_20
+        // pci_int_enb2.s.rpcnt0 = 1; // Not an error condition
+        // pci_int_enb2.s.rrsl_int = 1; // Not an error condition
+        pci_int_enb2.s.ill_rrd = 1;
+        pci_int_enb2.s.ill_rwr = 1;
+        pci_int_enb2.s.rdperr = 1;
+        pci_int_enb2.s.raperr = 1;
+        pci_int_enb2.s.rserr = 1;
+        pci_int_enb2.s.rtsr_abt = 1;
+        pci_int_enb2.s.rmsc_msg = 1;
+        pci_int_enb2.s.rmsi_mabt = 1;
+        pci_int_enb2.s.rmsi_tabt = 1;
+        pci_int_enb2.s.rmsi_per = 1;
+        pci_int_enb2.s.rmr_tto = 1;
+        pci_int_enb2.s.rmr_abt = 1;
+        pci_int_enb2.s.rtr_abt = 1;
+        pci_int_enb2.s.rmr_wtto = 1;
+        pci_int_enb2.s.rmr_wabt = 1;
+        pci_int_enb2.s.rtr_wabt = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN50XX))
+    {
+        // Skipping pci_int_enb2.s.reserved_34_63
+        pci_int_enb2.s.ill_rd = 1;
+        pci_int_enb2.s.ill_wr = 1;
+        pci_int_enb2.s.win_wr = 1;
+        // pci_int_enb2.s.dma1_fi = 1; // Not an error condition
+        // pci_int_enb2.s.dma0_fi = 1; // Not an error condition
+        // pci_int_enb2.s.rdtime1 = 1; // Not an error condition
+        // pci_int_enb2.s.rdtime0 = 1; // Not an error condition
+        // pci_int_enb2.s.rdcnt1 = 1; // Not an error condition
+        // pci_int_enb2.s.rdcnt0 = 1; // Not an error condition
+        // Skipping pci_int_enb2.s.reserved_23_24
+        // pci_int_enb2.s.rptime1 = 1; // Not an error condition
+        // pci_int_enb2.s.rptime0 = 1; // Not an error condition
+        // Skipping pci_int_enb2.s.reserved_19_20
+        // pci_int_enb2.s.rpcnt1 = 1; // Not an error condition
+        // pci_int_enb2.s.rpcnt0 = 1; // Not an error condition
+        // pci_int_enb2.s.rrsl_int = 1; // Not an error condition
+        pci_int_enb2.s.ill_rrd = 1;
+        pci_int_enb2.s.ill_rwr = 1;
+        pci_int_enb2.s.rdperr = 1;
+        pci_int_enb2.s.raperr = 1;
+        pci_int_enb2.s.rserr = 1;
+        pci_int_enb2.s.rtsr_abt = 1;
+        pci_int_enb2.s.rmsc_msg = 1;
+        pci_int_enb2.s.rmsi_mabt = 1;
+        pci_int_enb2.s.rmsi_tabt = 1;
+        pci_int_enb2.s.rmsi_per = 1;
+        pci_int_enb2.s.rmr_tto = 1;
+        pci_int_enb2.s.rmr_abt = 1;
+        pci_int_enb2.s.rtr_abt = 1;
+        pci_int_enb2.s.rmr_wtto = 1;
+        pci_int_enb2.s.rmr_wabt = 1;
+        pci_int_enb2.s.rtr_wabt = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN38XX))
+    {
+        // Skipping pci_int_enb2.s.reserved_34_63
+        pci_int_enb2.s.ill_rd = 1;
+        pci_int_enb2.s.ill_wr = 1;
+        pci_int_enb2.s.win_wr = 1;
+        // pci_int_enb2.s.dma1_fi = 1; // Not an error condition
+        // pci_int_enb2.s.dma0_fi = 1; // Not an error condition
+        // pci_int_enb2.s.rdtime1 = 1; // Not an error condition
+        // pci_int_enb2.s.rdtime0 = 1; // Not an error condition
+        // pci_int_enb2.s.rdcnt1 = 1; // Not an error condition
+        // pci_int_enb2.s.rdcnt0 = 1; // Not an error condition
+        // pci_int_enb2.s.rptime3 = 1; // Not an error condition
+        // pci_int_enb2.s.rptime2 = 1; // Not an error condition
+        // pci_int_enb2.s.rptime1 = 1; // Not an error condition
+        // pci_int_enb2.s.rptime0 = 1; // Not an error condition
+        // pci_int_enb2.s.rpcnt3 = 1; // Not an error condition
+        // pci_int_enb2.s.rpcnt2 = 1; // Not an error condition
+        // pci_int_enb2.s.rpcnt1 = 1; // Not an error condition
+        // pci_int_enb2.s.rpcnt0 = 1; // Not an error condition
+        // pci_int_enb2.s.rrsl_int = 1; // Not an error condition
+        pci_int_enb2.s.ill_rrd = 1;
+        pci_int_enb2.s.ill_rwr = 1;
+        pci_int_enb2.s.rdperr = 1;
+        pci_int_enb2.s.raperr = 1;
+        pci_int_enb2.s.rserr = 1;
+        pci_int_enb2.s.rtsr_abt = 1;
+        pci_int_enb2.s.rmsc_msg = 1;
+        pci_int_enb2.s.rmsi_mabt = 1;
+        pci_int_enb2.s.rmsi_tabt = 1;
+        pci_int_enb2.s.rmsi_per = 1;
+        pci_int_enb2.s.rmr_tto = 1;
+        pci_int_enb2.s.rmr_abt = 1;
+        pci_int_enb2.s.rtr_abt = 1;
+        pci_int_enb2.s.rmr_wtto = 1;
+        pci_int_enb2.s.rmr_wabt = 1;
+        pci_int_enb2.s.rtr_wabt = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN31XX))
+    {
+        // Skipping pci_int_enb2.s.reserved_34_63
+        pci_int_enb2.s.ill_rd = 1;
+        pci_int_enb2.s.ill_wr = 1;
+        pci_int_enb2.s.win_wr = 1;
+        // pci_int_enb2.s.dma1_fi = 1; // Not an error condition
+        // pci_int_enb2.s.dma0_fi = 1; // Not an error condition
+        // pci_int_enb2.s.rdtime1 = 1; // Not an error condition
+        // pci_int_enb2.s.rdtime0 = 1; // Not an error condition
+        // pci_int_enb2.s.rdcnt1 = 1; // Not an error condition
+        // pci_int_enb2.s.rdcnt0 = 1; // Not an error condition
+        // Skipping pci_int_enb2.s.reserved_23_24
+        // pci_int_enb2.s.rptime1 = 1; // Not an error condition
+        // pci_int_enb2.s.rptime0 = 1; // Not an error condition
+        // Skipping pci_int_enb2.s.reserved_19_20
+        // pci_int_enb2.s.rpcnt1 = 1; // Not an error condition
+        // pci_int_enb2.s.rpcnt0 = 1; // Not an error condition
+        // pci_int_enb2.s.rrsl_int = 1; // Not an error condition
+        pci_int_enb2.s.ill_rrd = 1;
+        pci_int_enb2.s.ill_rwr = 1;
+        pci_int_enb2.s.rdperr = 1;
+        pci_int_enb2.s.raperr = 1;
+        pci_int_enb2.s.rserr = 1;
+        pci_int_enb2.s.rtsr_abt = 1;
+        pci_int_enb2.s.rmsc_msg = 1;
+        pci_int_enb2.s.rmsi_mabt = 1;
+        pci_int_enb2.s.rmsi_tabt = 1;
+        pci_int_enb2.s.rmsi_per = 1;
+        pci_int_enb2.s.rmr_tto = 1;
+        pci_int_enb2.s.rmr_abt = 1;
+        pci_int_enb2.s.rtr_abt = 1;
+        pci_int_enb2.s.rmr_wtto = 1;
+        pci_int_enb2.s.rmr_wabt = 1;
+        pci_int_enb2.s.rtr_wabt = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN58XX))
+    {
+        // Skipping pci_int_enb2.s.reserved_34_63
+        pci_int_enb2.s.ill_rd = 1;
+        pci_int_enb2.s.ill_wr = 1;
+        pci_int_enb2.s.win_wr = 1;
+        // pci_int_enb2.s.dma1_fi = 1; // Not an error condition
+        // pci_int_enb2.s.dma0_fi = 1; // Not an error condition
+        // pci_int_enb2.s.rdtime1 = 1; // Not an error condition
+        // pci_int_enb2.s.rdtime0 = 1; // Not an error condition
+        // pci_int_enb2.s.rdcnt1 = 1; // Not an error condition
+        // pci_int_enb2.s.rdcnt0 = 1; // Not an error condition
+        // pci_int_enb2.s.rptime3 = 1; // Not an error condition
+        // pci_int_enb2.s.rptime2 = 1; // Not an error condition
+        // pci_int_enb2.s.rptime1 = 1; // Not an error condition
+        // pci_int_enb2.s.rptime0 = 1; // Not an error condition
+        // pci_int_enb2.s.rpcnt3 = 1; // Not an error condition
+        // pci_int_enb2.s.rpcnt2 = 1; // Not an error condition
+        // pci_int_enb2.s.rpcnt1 = 1; // Not an error condition
+        // pci_int_enb2.s.rpcnt0 = 1; // Not an error condition
+        // pci_int_enb2.s.rrsl_int = 1; // Not an error condition
+        pci_int_enb2.s.ill_rrd = 1;
+        pci_int_enb2.s.ill_rwr = 1;
+        pci_int_enb2.s.rdperr = 1;
+        pci_int_enb2.s.raperr = 1;
+        pci_int_enb2.s.rserr = 1;
+        pci_int_enb2.s.rtsr_abt = 1;
+        pci_int_enb2.s.rmsc_msg = 1;
+        pci_int_enb2.s.rmsi_mabt = 1;
+        pci_int_enb2.s.rmsi_tabt = 1;
+        pci_int_enb2.s.rmsi_per = 1;
+        pci_int_enb2.s.rmr_tto = 1;
+        pci_int_enb2.s.rmr_abt = 1;
+        pci_int_enb2.s.rtr_abt = 1;
+        pci_int_enb2.s.rmr_wtto = 1;
+        pci_int_enb2.s.rmr_wabt = 1;
+        pci_int_enb2.s.rtr_wabt = 1;
+    }
+    cvmx_write_csr(CVMX_NPI_PCI_INT_ENB2, pci_int_enb2.u64);
+}
+
+
+/**
+ * __cvmx_interrupt_pci_int_sum2_decode decodes all interrupt bits in cvmx_pci_int_sum2_t
+ */
+void __cvmx_interrupt_pci_int_sum2_decode(void)
+{
+    cvmx_pci_int_sum2_t pci_int_sum2;
+    pci_int_sum2.u64 = cvmx_read_csr(CVMX_NPI_PCI_INT_SUM2);
+    cvmx_write_csr(CVMX_NPI_PCI_INT_SUM2, pci_int_sum2.u64);
+    pci_int_sum2.u64 &= cvmx_read_csr(CVMX_NPI_PCI_INT_ENB2);
+    // Skipping pci_int_sum2.s.reserved_34_63
+    if (pci_int_sum2.s.ill_rd)
+        PRINT_ERROR("PCI_INT_SUM2[ILL_RD]: A read to a disabled area of bar1 or bar2,\n"
+                    "    when the mem area is disabled.\n");
+    if (pci_int_sum2.s.ill_wr)
+        PRINT_ERROR("PCI_INT_SUM2[ILL_WR]: A write to a disabled area of bar1 or bar2,\n"
+                    "    when the mem area is disabled.\n");
+    if (pci_int_sum2.s.win_wr)
+        PRINT_ERROR("PCI_INT_SUM2[WIN_WR]: A write to the disabled Window Write Data or\n"
+                    "    Read-Address Register took place.\n");
+    if (pci_int_sum2.s.dma1_fi)
+        PRINT_ERROR("PCI_INT_SUM2[DMA1_FI]: A DMA operation operation finished that was\n"
+                    "    required to set the FORCE-INT bit for counter 1.\n");
+    if (pci_int_sum2.s.dma0_fi)
+        PRINT_ERROR("PCI_INT_SUM2[DMA0_FI]: A DMA operation operation finished that was\n"
+                    "    required to set the FORCE-INT bit for counter 0.\n");
+    if (pci_int_sum2.s.dtime1)
+        PRINT_ERROR("PCI_INT_SUM2[DTIME1]: When the value in the PCI_DMA_CNT1\n"
+                    "    register is not 0 the DMA_CNT1 timer counts.\n"
+                    "    When the DMA1_CNT timer has a value greater\n"
+                    "    than the PCI_DMA_TIME1 register this\n"
+                    "    bit is set. The timer is reset when bit is\n"
+                    "    written with a one.\n");
+    if (pci_int_sum2.s.dtime0)
+        PRINT_ERROR("PCI_INT_SUM2[DTIME0]: When the value in the PCI_DMA_CNT0\n"
+                    "    register is not 0 the DMA_CNT0 timer counts.\n"
+                    "    When the DMA0_CNT timer has a value greater\n"
+                    "    than the PCI_DMA_TIME0 register this\n"
+                    "    bit is set. The timer is reset when bit is\n"
+                    "    written with a one.\n");
+    if (pci_int_sum2.s.dcnt1)
+        PRINT_ERROR("PCI_INT_SUM2[DCNT1]: This bit indicates that PCI_DMA_CNT1\n"
+                    "    value is greater than the value\n"
+                    "    in the PCI_DMA_INT_LEV1 register.\n");
+    if (pci_int_sum2.s.dcnt0)
+        PRINT_ERROR("PCI_INT_SUM2[DCNT0]: This bit indicates that PCI_DMA_CNT0\n"
+                    "    value is greater than the value\n"
+                    "    in the PCI_DMA_INT_LEV0 register.\n");
+    if (pci_int_sum2.s.ptime3)
+        PRINT_ERROR("PCI_INT_SUM2[PTIME3]: When the value in the PCI_PKTS_SENT3\n"
+                    "    register is not 0 the Sent-3 timer counts.\n"
+                    "    When the Sent-3 timer has a value greater\n"
+                    "    than the PCI_PKTS_SENT_TIME3 register this\n"
+                    "    bit is set. The timer is reset when bit is\n"
+                    "    written with a one.\n");
+    if (pci_int_sum2.s.ptime2)
+        PRINT_ERROR("PCI_INT_SUM2[PTIME2]: When the value in the PCI_PKTS_SENT2\n"
+                    "    register is not 0 the Sent-2 timer counts.\n"
+                    "    When the Sent-2 timer has a value greater\n"
+                    "    than the PCI_PKTS_SENT_TIME2 register this\n"
+                    "    bit is set. The timer is reset when bit is\n"
+                    "    written with a one.\n");
+    if (pci_int_sum2.s.ptime1)
+        PRINT_ERROR("PCI_INT_SUM2[PTIME1]: When the value in the PCI_PKTS_SENT1\n"
+                    "    register is not 0 the Sent-1 timer counts.\n"
+                    "    When the Sent-1 timer has a value greater\n"
+                    "    than the PCI_PKTS_SENT_TIME1 register this\n"
+                    "    bit is set. The timer is reset when bit is\n"
+                    "    written with a one.\n");
+    if (pci_int_sum2.s.ptime0)
+        PRINT_ERROR("PCI_INT_SUM2[PTIME0]: When the value in the PCI_PKTS_SENT0\n"
+                    "    register is not 0 the Sent-0 timer counts.\n"
+                    "    When the Sent-0 timer has a value greater\n"
+                    "    than the PCI_PKTS_SENT_TIME0 register this\n"
+                    "    bit is set. The timer is reset when bit is\n"
+                    "    written with a one.\n");
+    if (pci_int_sum2.s.pcnt3)
+        PRINT_ERROR("PCI_INT_SUM2[PCNT3]: This bit indicates that PCI_PKTS_SENT3\n"
+                    "    value is greater than the value\n"
+                    "    in the PCI_PKTS_SENT_INT_LEV3 register.\n");
+    if (pci_int_sum2.s.pcnt2)
+        PRINT_ERROR("PCI_INT_SUM2[PCNT2]: This bit indicates that PCI_PKTS_SENT2\n"
+                    "    value is greater than the value\n"
+                    "    in the PCI_PKTS_SENT_INT_LEV2 register.\n");
+    if (pci_int_sum2.s.pcnt1)
+        PRINT_ERROR("PCI_INT_SUM2[PCNT1]: This bit indicates that PCI_PKTS_SENT1\n"
+                    "    value is greater than the value\n"
+                    "    in the PCI_PKTS_SENT_INT_LEV1 register.\n");
+    if (pci_int_sum2.s.pcnt0)
+        PRINT_ERROR("PCI_INT_SUM2[PCNT0]: This bit indicates that PCI_PKTS_SENT0\n"
+                    "    value is greater than the value\n"
+                    "    in the PCI_PKTS_SENT_INT_LEV0 register.\n");
+    if (pci_int_sum2.s.rsl_int)
+        PRINT_ERROR("PCI_INT_SUM2[RSL_INT]: This bit is set when the RSL Chain has\n"
+                    "    generated an interrupt.\n");
+    if (pci_int_sum2.s.ill_rrd)
+        PRINT_ERROR("PCI_INT_SUM2[ILL_RRD]: A read  to the disabled PCI registers took place.\n");
+    if (pci_int_sum2.s.ill_rwr)
+        PRINT_ERROR("PCI_INT_SUM2[ILL_RWR]: A write to the disabled PCI registers took place.\n");
+    if (pci_int_sum2.s.dperr)
+        PRINT_ERROR("PCI_INT_SUM2[DPERR]: Data Parity Error detected by PCX Core\n");
+    if (pci_int_sum2.s.aperr)
+        PRINT_ERROR("PCI_INT_SUM2[APERR]: Address Parity Error detected by PCX Core\n");
+    if (pci_int_sum2.s.serr)
+        PRINT_ERROR("PCI_INT_SUM2[SERR]: SERR# detected by PCX Core\n");
+    if (pci_int_sum2.s.tsr_abt)
+        PRINT_ERROR("PCI_INT_SUM2[TSR_ABT]: Target Split-Read Abort Detected\n");
+    if (pci_int_sum2.s.msc_msg)
+        PRINT_ERROR("PCI_INT_SUM2[MSC_MSG]: Master Split Completion Message Detected\n");
+    if (pci_int_sum2.s.msi_mabt)
+        PRINT_ERROR("PCI_INT_SUM2[MSI_MABT]: PCI MSI Master Abort.\n");
+    if (pci_int_sum2.s.msi_tabt)
+        PRINT_ERROR("PCI_INT_SUM2[MSI_TABT]: PCI MSI Target Abort.\n");
+    if (pci_int_sum2.s.msi_per)
+        PRINT_ERROR("PCI_INT_SUM2[MSI_PER]: PCI MSI Parity Error.\n");
+    if (pci_int_sum2.s.mr_tto)
+        PRINT_ERROR("PCI_INT_SUM2[MR_TTO]: PCI Master Retry Timeout On Read.\n");
+    if (pci_int_sum2.s.mr_abt)
+        PRINT_ERROR("PCI_INT_SUM2[MR_ABT]: PCI Master Abort On Read.\n");
+    if (pci_int_sum2.s.tr_abt)
+        PRINT_ERROR("PCI_INT_SUM2[TR_ABT]: PCI Target Abort On Read.\n");
+    if (pci_int_sum2.s.mr_wtto)
+        PRINT_ERROR("PCI_INT_SUM2[MR_WTTO]: PCI Master Retry Timeout on write.\n");
+    if (pci_int_sum2.s.mr_wabt)
+        PRINT_ERROR("PCI_INT_SUM2[MR_WABT]: PCI Master Abort detected on write.\n");
+    if (pci_int_sum2.s.tr_wabt)
+        PRINT_ERROR("PCI_INT_SUM2[TR_WABT]: PCI Target Abort detected on write.\n");
+}
+
+
+/**
+ * __cvmx_interrupt_pcsx_intx_en_reg_enable enables all interrupt bits in cvmx_pcsx_intx_en_reg_t
+ */
+void __cvmx_interrupt_pcsx_intx_en_reg_enable(int index, int block)
+{
+    cvmx_pcsx_intx_en_reg_t pcs_int_en_reg;
+    cvmx_write_csr(CVMX_PCSX_INTX_REG(index, block), cvmx_read_csr(CVMX_PCSX_INTX_REG(index, block)));
+    pcs_int_en_reg.u64 = 0;
+    if (OCTEON_IS_MODEL(OCTEON_CN56XX))
+    {
+        // Skipping pcs_int_en_reg.s.reserved_12_63
+        //pcs_int_en_reg.s.dup = 1; // This happens during normal operation
+        pcs_int_en_reg.s.sync_bad_en = 1;
+        pcs_int_en_reg.s.an_bad_en = 1;
+        pcs_int_en_reg.s.rxlock_en = 1;
+        pcs_int_en_reg.s.rxbad_en = 1;
+        //pcs_int_en_reg.s.rxerr_en = 1; // This happens during normal operation
+        pcs_int_en_reg.s.txbad_en = 1;
+        pcs_int_en_reg.s.txfifo_en = 1;
+        pcs_int_en_reg.s.txfifu_en = 1;
+        pcs_int_en_reg.s.an_err_en = 1;
+        //pcs_int_en_reg.s.xmit_en = 1; // This happens during normal operation
+        //pcs_int_en_reg.s.lnkspd_en = 1; // This happens during normal operation
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN52XX))
+    {
+        // Skipping pcs_int_en_reg.s.reserved_12_63
+        //pcs_int_en_reg.s.dup = 1; // This happens during normal operation
+        pcs_int_en_reg.s.sync_bad_en = 1;
+        pcs_int_en_reg.s.an_bad_en = 1;
+        pcs_int_en_reg.s.rxlock_en = 1;
+        pcs_int_en_reg.s.rxbad_en = 1;
+        //pcs_int_en_reg.s.rxerr_en = 1; // This happens during normal operation
+        pcs_int_en_reg.s.txbad_en = 1;
+        pcs_int_en_reg.s.txfifo_en = 1;
+        pcs_int_en_reg.s.txfifu_en = 1;
+        pcs_int_en_reg.s.an_err_en = 1;
+        //pcs_int_en_reg.s.xmit_en = 1; // This happens during normal operation
+        //pcs_int_en_reg.s.lnkspd_en = 1; // This happens during normal operation
+    }
+    cvmx_write_csr(CVMX_PCSX_INTX_EN_REG(index, block), pcs_int_en_reg.u64);
+}
+
+
+/**
+ * __cvmx_interrupt_pcsx_intx_reg_decode decodes all interrupt bits in cvmx_pcsx_intx_reg_t
+ */
+void __cvmx_interrupt_pcsx_intx_reg_decode(int index, int block)
+{
+    cvmx_pcsx_intx_reg_t pcs_int_reg;
+    pcs_int_reg.u64 = cvmx_read_csr(CVMX_PCSX_INTX_REG(index, block));
+    cvmx_write_csr(CVMX_PCSX_INTX_REG(index, block), pcs_int_reg.u64);
+    pcs_int_reg.u64 &= cvmx_read_csr(CVMX_PCSX_INTX_EN_REG(index, block));
+    // Skipping pcs_int_reg.s.reserved_12_63
+    if (pcs_int_reg.s.dup)
+        PRINT_ERROR("PCS%d_INT%d_REG[DUP]: Set whenever Duplex mode changes on the link\n", block, index);
+    if (pcs_int_reg.s.sync_bad)
+        PRINT_ERROR("PCS%d_INT%d_REG[SYNC_BAD]: Set by HW whenever rx sync st machine reaches a bad\n"
+                    "    state. Should never be set during normal operation\n", block, index);
+    if (pcs_int_reg.s.an_bad)
+        PRINT_ERROR("PCS%d_INT%d_REG[AN_BAD]: Set by HW whenever AN st machine reaches a bad\n"
+                    "    state. Should never be set during normal operation\n", block, index);
+    if (pcs_int_reg.s.rxlock)
+        PRINT_ERROR("PCS%d_INT%d_REG[RXLOCK]: Set by HW whenever code group Sync or bit lock\n"
+                    "    failure occurs\n"
+                    "    Cannot fire in loopback1 mode\n", block, index);
+    if (pcs_int_reg.s.rxbad)
+        PRINT_ERROR("PCS%d_INT%d_REG[RXBAD]: Set by HW whenever rx st machine reaches a  bad\n"
+                    "    state. Should never be set during normal operation\n", block, index);
+    if (pcs_int_reg.s.rxerr)
+        PRINT_ERROR("PCS%d_INT%d_REG[RXERR]: Set whenever RX receives a code group error in\n"
+                    "    10 bit to 8 bit decode logic\n"
+                    "    Cannot fire in loopback1 mode\n", block, index);
+    if (pcs_int_reg.s.txbad)
+        PRINT_ERROR("PCS%d_INT%d_REG[TXBAD]: Set by HW whenever tx st machine reaches a bad\n"
+                    "    state. Should never be set during normal operation\n", block, index);
+    if (pcs_int_reg.s.txfifo)
+        PRINT_ERROR("PCS%d_INT%d_REG[TXFIFO]: Set whenever HW detects a TX fifo overflow\n"
+                    "    condition\n", block, index);
+    if (pcs_int_reg.s.txfifu)
+        PRINT_ERROR("PCS%d_INT%d_REG[TXFIFU]: Set whenever HW detects a TX fifo underflowflow\n"
+                    "    condition\n", block, index);
+    if (pcs_int_reg.s.an_err)
+        PRINT_ERROR("PCS%d_INT%d_REG[AN_ERR]: AN Error, AN resolution function failed\n", block, index);
+    if (pcs_int_reg.s.xmit)
+        PRINT_ERROR("PCS%d_INT%d_REG[XMIT]: Set whenever HW detects a change in the XMIT\n"
+                    "    variable. XMIT variable states are IDLE, CONFIG and\n"
+                    "    DATA\n", block, index);
+    if (pcs_int_reg.s.lnkspd)
+        PRINT_ERROR("PCS%d_INT%d_REG[LNKSPD]: Set by HW whenever Link Speed has changed\n", block, index);
+}
+
+
+/**
+ * __cvmx_interrupt_pescx_dbg_info_en_enable enables all interrupt bits in cvmx_pescx_dbg_info_en_t
+ */
+void __cvmx_interrupt_pescx_dbg_info_en_enable(int index)
+{
+    cvmx_pescx_dbg_info_en_t pesc_dbg_info_en;
+    cvmx_write_csr(CVMX_PESCX_DBG_INFO(index), cvmx_read_csr(CVMX_PESCX_DBG_INFO(index)));
+    pesc_dbg_info_en.u64 = 0;
+    if (OCTEON_IS_MODEL(OCTEON_CN56XX))
+    {
+        // Skipping pesc_dbg_info_en.s.reserved_31_63
+        pesc_dbg_info_en.s.ecrc_e = 1;
+        pesc_dbg_info_en.s.rawwpp = 1;
+        pesc_dbg_info_en.s.racpp = 1;
+        pesc_dbg_info_en.s.ramtlp = 1;
+        pesc_dbg_info_en.s.rarwdns = 1;
+        pesc_dbg_info_en.s.caar = 1;
+        pesc_dbg_info_en.s.racca = 1;
+        pesc_dbg_info_en.s.racur = 1;
+        pesc_dbg_info_en.s.rauc = 1;
+        pesc_dbg_info_en.s.rqo = 1;
+        pesc_dbg_info_en.s.fcuv = 1;
+        pesc_dbg_info_en.s.rpe = 1;
+        pesc_dbg_info_en.s.fcpvwt = 1;
+        pesc_dbg_info_en.s.dpeoosd = 1;
+        pesc_dbg_info_en.s.rtwdle = 1;
+        pesc_dbg_info_en.s.rdwdle = 1;
+        pesc_dbg_info_en.s.mre = 1;
+        pesc_dbg_info_en.s.rte = 1;
+        pesc_dbg_info_en.s.acto = 1;
+        pesc_dbg_info_en.s.rvdm = 1;
+        pesc_dbg_info_en.s.rumep = 1;
+        pesc_dbg_info_en.s.rptamrc = 1;
+        pesc_dbg_info_en.s.rpmerc = 1;
+        pesc_dbg_info_en.s.rfemrc = 1;
+        pesc_dbg_info_en.s.rnfemrc = 1;
+        pesc_dbg_info_en.s.rcemrc = 1;
+        pesc_dbg_info_en.s.rpoison = 1;
+        pesc_dbg_info_en.s.recrce = 1;
+        pesc_dbg_info_en.s.rtlplle = 1;
+#if 0
+    /* RTLPMAL is disabled since it will be generated under normal conditions,
+        like devices causing legacy PCI interrupts */
+        pesc_dbg_info_en.s.rtlpmal = 1;
+#endif
+        pesc_dbg_info_en.s.spoison = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN52XX))
+    {
+        // Skipping pesc_dbg_info_en.s.reserved_31_63
+        pesc_dbg_info_en.s.ecrc_e = 1;
+        pesc_dbg_info_en.s.rawwpp = 1;
+        pesc_dbg_info_en.s.racpp = 1;
+        pesc_dbg_info_en.s.ramtlp = 1;
+        pesc_dbg_info_en.s.rarwdns = 1;
+        pesc_dbg_info_en.s.caar = 1;
+        pesc_dbg_info_en.s.racca = 1;
+        pesc_dbg_info_en.s.racur = 1;
+        pesc_dbg_info_en.s.rauc = 1;
+        pesc_dbg_info_en.s.rqo = 1;
+        pesc_dbg_info_en.s.fcuv = 1;
+        pesc_dbg_info_en.s.rpe = 1;
+        pesc_dbg_info_en.s.fcpvwt = 1;
+        pesc_dbg_info_en.s.dpeoosd = 1;
+        pesc_dbg_info_en.s.rtwdle = 1;
+        pesc_dbg_info_en.s.rdwdle = 1;
+        pesc_dbg_info_en.s.mre = 1;
+        pesc_dbg_info_en.s.rte = 1;
+        pesc_dbg_info_en.s.acto = 1;
+        pesc_dbg_info_en.s.rvdm = 1;
+        pesc_dbg_info_en.s.rumep = 1;
+        pesc_dbg_info_en.s.rptamrc = 1;
+        pesc_dbg_info_en.s.rpmerc = 1;
+        pesc_dbg_info_en.s.rfemrc = 1;
+        pesc_dbg_info_en.s.rnfemrc = 1;
+        pesc_dbg_info_en.s.rcemrc = 1;
+        pesc_dbg_info_en.s.rpoison = 1;
+        pesc_dbg_info_en.s.recrce = 1;
+        pesc_dbg_info_en.s.rtlplle = 1;
+#if 0
+    /* RTLPMAL is disabled since it will be generated under normal conditions,
+        like devices causing legacy PCI interrupts */
+        pesc_dbg_info_en.s.rtlpmal = 1;
+#endif
+        pesc_dbg_info_en.s.spoison = 1;
+    }
+    cvmx_write_csr(CVMX_PESCX_DBG_INFO_EN(index), pesc_dbg_info_en.u64);
+}
+
+
+/**
+ * __cvmx_interrupt_pescx_dbg_info_decode decodes all interrupt bits in cvmx_pescx_dbg_info_t
+ */
+void __cvmx_interrupt_pescx_dbg_info_decode(int index)
+{
+    cvmx_pescx_dbg_info_t pesc_dbg_info;
+    pesc_dbg_info.u64 = cvmx_read_csr(CVMX_PESCX_DBG_INFO(index));
+    cvmx_write_csr(CVMX_PESCX_DBG_INFO(index), pesc_dbg_info.u64);
+    pesc_dbg_info.u64 &= cvmx_read_csr(CVMX_PESCX_DBG_INFO_EN(index));
+    // Skipping pesc_dbg_info.s.reserved_31_63
+    if (pesc_dbg_info.s.ecrc_e)
+        PRINT_ERROR("PESC%d_DBG_INFO[ECRC_E]: Received a ECRC error.\n"
+                    "    radm_ecrc_err\n", index);
+    if (pesc_dbg_info.s.rawwpp)
+        PRINT_ERROR("PESC%d_DBG_INFO[RAWWPP]: Received a write with poisoned payload\n"
+                    "    radm_rcvd_wreq_poisoned\n", index);
+    if (pesc_dbg_info.s.racpp)
+        PRINT_ERROR("PESC%d_DBG_INFO[RACPP]: Received a completion with poisoned payload\n"
+                    "    radm_rcvd_cpl_poisoned\n", index);
+    if (pesc_dbg_info.s.ramtlp)
+        PRINT_ERROR("PESC%d_DBG_INFO[RAMTLP]: Received a malformed TLP\n"
+                    "    radm_mlf_tlp_err\n", index);
+    if (pesc_dbg_info.s.rarwdns)
+        PRINT_ERROR("PESC%d_DBG_INFO[RARWDNS]: Recieved a request which device does not support\n"
+                    "    radm_rcvd_ur_req\n", index);
+    if (pesc_dbg_info.s.caar)
+        PRINT_ERROR("PESC%d_DBG_INFO[CAAR]: Completer aborted a request\n"
+                    "    radm_rcvd_ca_req\n"
+                    "    This bit will never be set because Octeon does\n"
+                    "    not generate Completer Aborts.\n", index);
+    if (pesc_dbg_info.s.racca)
+        PRINT_ERROR("PESC%d_DBG_INFO[RACCA]: Received a completion with CA status\n"
+                    "    radm_rcvd_cpl_ca\n", index);
+    if (pesc_dbg_info.s.racur)
+        PRINT_ERROR("PESC%d_DBG_INFO[RACUR]: Received a completion with UR status\n"
+                    "    radm_rcvd_cpl_ur\n", index);
+    if (pesc_dbg_info.s.rauc)
+        PRINT_ERROR("PESC%d_DBG_INFO[RAUC]: Received an unexpected completion\n"
+                    "    radm_unexp_cpl_err\n", index);
+    if (pesc_dbg_info.s.rqo)
+        PRINT_ERROR("PESC%d_DBG_INFO[RQO]: Receive queue overflow. Normally happens only when\n"
+                    "    flow control advertisements are ignored\n"
+                    "    radm_qoverflow\n", index);
+    if (pesc_dbg_info.s.fcuv)
+        PRINT_ERROR("PESC%d_DBG_INFO[FCUV]: Flow Control Update Violation (opt. checks)\n"
+                    "    int_xadm_fc_prot_err\n", index);
+    if (pesc_dbg_info.s.rpe)
+        PRINT_ERROR("PESC%d_DBG_INFO[RPE]: Received Phy Error\n"
+                    "    rmlh_rcvd_err\n", index);
+    if (pesc_dbg_info.s.fcpvwt)
+        PRINT_ERROR("PESC%d_DBG_INFO[FCPVWT]: Flow Control Protocol Violation (Watchdog Timer)\n"
+                    "    rtlh_fc_prot_err\n", index);
+    if (pesc_dbg_info.s.dpeoosd)
+        PRINT_ERROR("PESC%d_DBG_INFO[DPEOOSD]: DLLP protocol error (out of sequence DLLP)\n"
+                    "    rdlh_prot_err\n", index);
+    if (pesc_dbg_info.s.rtwdle)
+        PRINT_ERROR("PESC%d_DBG_INFO[RTWDLE]: Received TLP with DataLink Layer Error\n"
+                    "    rdlh_bad_tlp_err\n", index);
+    if (pesc_dbg_info.s.rdwdle)
+        PRINT_ERROR("PESC%d_DBG_INFO[RDWDLE]: Received DLLP with DataLink Layer Error\n"
+                    "    rdlh_bad_dllp_err\n", index);
+    if (pesc_dbg_info.s.mre)
+        PRINT_ERROR("PESC%d_DBG_INFO[MRE]: Max Retries Exceeded\n"
+                    "    xdlh_replay_num_rlover_err\n", index);
+    if (pesc_dbg_info.s.rte)
+        PRINT_ERROR("PESC%d_DBG_INFO[RTE]: Replay Timer Expired\n"
+                    "    xdlh_replay_timeout_err\n"
+                    "    This bit is set when the REPLAY_TIMER expires in\n"
+                    "    the PCIE core. The probability of this bit being\n"
+                    "    set will increase with the traffic load.\n", index);
+    if (pesc_dbg_info.s.acto)
+        PRINT_ERROR("PESC%d_DBG_INFO[ACTO]: A Completion Timeout Occured\n"
+                    "    pedc_radm_cpl_timeout\n", index);
+    if (pesc_dbg_info.s.rvdm)
+        PRINT_ERROR("PESC%d_DBG_INFO[RVDM]: Received Vendor-Defined Message\n"
+                    "    pedc_radm_vendor_msg\n", index);
+    if (pesc_dbg_info.s.rumep)
+        PRINT_ERROR("PESC%d_DBG_INFO[RUMEP]: Received Unlock Message (EP Mode Only)\n"
+                    "    pedc_radm_msg_unlock\n", index);
+    if (pesc_dbg_info.s.rptamrc)
+        PRINT_ERROR("PESC%d_DBG_INFO[RPTAMRC]: Received PME Turnoff Acknowledge Message\n"
+                    "    (RC Mode only)\n"
+                    "    pedc_radm_pm_to_ack\n", index);
+    if (pesc_dbg_info.s.rpmerc)
+        PRINT_ERROR("PESC%d_DBG_INFO[RPMERC]: Received PME Message (RC Mode only)\n"
+                    "    pedc_radm_pm_pme\n", index);
+    if (pesc_dbg_info.s.rfemrc)
+        PRINT_ERROR("PESC%d_DBG_INFO[RFEMRC]: Received Fatal Error Message (RC Mode only)\n"
+                    "    pedc_radm_fatal_err\n"
+                    "    Bit set when a message with ERR_FATAL is set.\n", index);
+    if (pesc_dbg_info.s.rnfemrc)
+        PRINT_ERROR("PESC%d_DBG_INFO[RNFEMRC]: Received Non-Fatal Error Message (RC Mode only)\n"
+                    "    pedc_radm_nonfatal_err\n", index);
+    if (pesc_dbg_info.s.rcemrc)
+        PRINT_ERROR("PESC%d_DBG_INFO[RCEMRC]: Received Correctable Error Message (RC Mode only)\n"
+                    "    pedc_radm_correctable_err\n", index);
+    if (pesc_dbg_info.s.rpoison)
+        PRINT_ERROR("PESC%d_DBG_INFO[RPOISON]: Received Poisoned TLP\n"
+                    "    pedc__radm_trgt1_poisoned & pedc__radm_trgt1_hv\n", index);
+    if (pesc_dbg_info.s.recrce)
+        PRINT_ERROR("PESC%d_DBG_INFO[RECRCE]: Received ECRC Error\n"
+                    "    pedc_radm_trgt1_ecrc_err & pedc__radm_trgt1_eot\n", index);
+    if (pesc_dbg_info.s.rtlplle)
+        PRINT_ERROR("PESC%d_DBG_INFO[RTLPLLE]: Received TLP has link layer error\n"
+                    "    pedc_radm_trgt1_dllp_abort & pedc__radm_trgt1_eot\n", index);
+    if (pesc_dbg_info.s.rtlpmal)
+        PRINT_ERROR("PESC%d_DBG_INFO[RTLPMAL]: Received TLP is malformed or a message.\n"
+                    "    pedc_radm_trgt1_tlp_abort & pedc__radm_trgt1_eot\n"
+                    "    If the core receives a MSG (or Vendor Message)\n"
+                    "    this bit will be set.\n", index);
+    if (pesc_dbg_info.s.spoison)
+        PRINT_ERROR("PESC%d_DBG_INFO[SPOISON]: Poisoned TLP sent\n"
+                    "    peai__client0_tlp_ep & peai__client0_tlp_hv\n", index);
+}
+
+
+/**
+ * __cvmx_interrupt_pip_int_en_enable enables all interrupt bits in cvmx_pip_int_en_t
+ */
+void __cvmx_interrupt_pip_int_en_enable(void)
+{
+    cvmx_pip_int_en_t pip_int_en;
+    cvmx_write_csr(CVMX_PIP_INT_REG, cvmx_read_csr(CVMX_PIP_INT_REG));
+    pip_int_en.u64 = 0;
+    if (OCTEON_IS_MODEL(OCTEON_CN56XX))
+    {
+        // Skipping pip_int_en.s.reserved_13_63
+        pip_int_en.s.punyerr = 1;
+        //pip_int_en.s.lenerr = 1; // Signalled in packet WQE
+        //pip_int_en.s.maxerr = 1; // Signalled in packet WQE
+        //pip_int_en.s.minerr = 1; // Signalled in packet WQE
+        pip_int_en.s.beperr = 1;
+        pip_int_en.s.feperr = 1;
+        pip_int_en.s.todoovr = 1;
+        pip_int_en.s.skprunt = 1;
+        pip_int_en.s.badtag = 1;
+        pip_int_en.s.prtnxa = 1;
+        //pip_int_en.s.bckprs = 1; // Don't care
+        //pip_int_en.s.crcerr = 1; // Signalled in packet WQE
+        //pip_int_en.s.pktdrp = 1; // Don't care
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN30XX))
+    {
+        // Skipping pip_int_en.s.reserved_9_63
+        pip_int_en.s.beperr = 1;
+        pip_int_en.s.feperr = 1;
+        pip_int_en.s.todoovr = 1;
+        pip_int_en.s.skprunt = 1;
+        pip_int_en.s.badtag = 1;
+        pip_int_en.s.prtnxa = 1;
+        //pip_int_en.s.bckprs = 1; // Don't care
+        //pip_int_en.s.crcerr = 1; // Signalled in packet WQE
+        //pip_int_en.s.pktdrp = 1; // Don't care
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN50XX))
+    {
+        // Skipping pip_int_en.s.reserved_12_63
+        //pip_int_en.s.lenerr = 1; // Signalled in packet WQE
+        //pip_int_en.s.maxerr = 1; // Signalled in packet WQE
+        //pip_int_en.s.minerr = 1; // Signalled in packet WQE
+        pip_int_en.s.beperr = 1;
+        pip_int_en.s.feperr = 1;
+        pip_int_en.s.todoovr = 1;
+        pip_int_en.s.skprunt = 1;
+        pip_int_en.s.badtag = 1;
+        pip_int_en.s.prtnxa = 1;
+        //pip_int_en.s.bckprs = 1; // Don't care
+        // Skipping pip_int_en.s.reserved_1_1
+        //pip_int_en.s.pktdrp = 1; // Don't care
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN38XX))
+    {
+        // Skipping pip_int_en.s.reserved_9_63
+        pip_int_en.s.beperr = 1;
+        pip_int_en.s.feperr = 1;
+        pip_int_en.s.todoovr = 1;
+        pip_int_en.s.skprunt = 1;
+        pip_int_en.s.badtag = 1;
+        pip_int_en.s.prtnxa = 1;
+        //pip_int_en.s.bckprs = 1; // Don't care
+        //pip_int_en.s.crcerr = 1; // Signalled in packet WQE
+        //pip_int_en.s.pktdrp = 1; // Don't care
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN31XX))
+    {
+        // Skipping pip_int_en.s.reserved_9_63
+        pip_int_en.s.beperr = 1;
+        pip_int_en.s.feperr = 1;
+        pip_int_en.s.todoovr = 1;
+        pip_int_en.s.skprunt = 1;
+        pip_int_en.s.badtag = 1;
+        pip_int_en.s.prtnxa = 1;
+        //pip_int_en.s.bckprs = 1; // Don't care
+        //pip_int_en.s.crcerr = 1; // Signalled in packet WQE
+        //pip_int_en.s.pktdrp = 1; // Don't care
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN58XX))
+    {
+        // Skipping pip_int_en.s.reserved_13_63
+        pip_int_en.s.punyerr = 1;
+        // Skipping pip_int_en.s.reserved_9_11
+        pip_int_en.s.beperr = 1;
+        pip_int_en.s.feperr = 1;
+        pip_int_en.s.todoovr = 1;
+        pip_int_en.s.skprunt = 1;
+        pip_int_en.s.badtag = 1;
+        pip_int_en.s.prtnxa = 1;
+        //pip_int_en.s.bckprs = 1; // Don't care
+        //pip_int_en.s.crcerr = 1; // Signalled in packet WQE
+        //pip_int_en.s.pktdrp = 1; // Don't care
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN52XX))
+    {
+        // Skipping pip_int_en.s.reserved_13_63
+        pip_int_en.s.punyerr = 1;
+        //pip_int_en.s.lenerr = 1; // Signalled in packet WQE
+        //pip_int_en.s.maxerr = 1; // Signalled in packet WQE
+        //pip_int_en.s.minerr = 1; // Signalled in packet WQE
+        pip_int_en.s.beperr = 1;
+        pip_int_en.s.feperr = 1;
+        pip_int_en.s.todoovr = 1;
+        pip_int_en.s.skprunt = 1;
+        pip_int_en.s.badtag = 1;
+        pip_int_en.s.prtnxa = 1;
+        //pip_int_en.s.bckprs = 1; // Don't care
+        // Skipping pip_int_en.s.reserved_1_1
+        //pip_int_en.s.pktdrp = 1; // Don't care
+    }
+    cvmx_write_csr(CVMX_PIP_INT_EN, pip_int_en.u64);
+}
+
+
+/**
+ * __cvmx_interrupt_pip_int_reg_decode decodes all interrupt bits in cvmx_pip_int_reg_t
+ */
+void __cvmx_interrupt_pip_int_reg_decode(void)
+{
+    cvmx_pip_int_reg_t pip_int_reg;
+    pip_int_reg.u64 = cvmx_read_csr(CVMX_PIP_INT_REG);
+    cvmx_write_csr(CVMX_PIP_INT_REG, pip_int_reg.u64);
+    pip_int_reg.u64 &= cvmx_read_csr(CVMX_PIP_INT_EN);
+    // Skipping pip_int_reg.s.reserved_13_63
+    if (pip_int_reg.s.punyerr)
+        PRINT_ERROR("PIP_INT_REG[PUNYERR]: Frame was received with length <=4B when CRC\n"
+                    "    stripping in IPD is enable\n");
+    if (pip_int_reg.s.lenerr)
+        PRINT_ERROR("PIP_INT_REG[LENERR]: Frame was received with length error\n");
+    if (pip_int_reg.s.maxerr)
+        PRINT_ERROR("PIP_INT_REG[MAXERR]: Frame was received with length > max_length\n");
+    if (pip_int_reg.s.minerr)
+        PRINT_ERROR("PIP_INT_REG[MINERR]: Frame was received with length < min_length\n");
+    if (pip_int_reg.s.beperr)
+        PRINT_ERROR("PIP_INT_REG[BEPERR]: Parity Error in back end memory\n");
+    if (pip_int_reg.s.feperr)
+        PRINT_ERROR("PIP_INT_REG[FEPERR]: Parity Error in front end memory\n");
+    if (pip_int_reg.s.todoovr)
+        PRINT_ERROR("PIP_INT_REG[TODOOVR]: Todo list overflow (see PIP_BCK_PRS[HIWATER])\n");
+    if (pip_int_reg.s.skprunt)
+        PRINT_ERROR("PIP_INT_REG[SKPRUNT]: Packet was engulfed by skipper\n");
+    if (pip_int_reg.s.badtag)
+        PRINT_ERROR("PIP_INT_REG[BADTAG]: A bad tag was sent from IPD\n");
+    if (pip_int_reg.s.prtnxa)
+        PRINT_ERROR("PIP_INT_REG[PRTNXA]: Non-existent port\n");
+    if (pip_int_reg.s.bckprs)
+        PRINT_ERROR("PIP_INT_REG[BCKPRS]: PIP asserted backpressure\n");
+    if (pip_int_reg.s.crcerr)
+        PRINT_ERROR("PIP_INT_REG[CRCERR]: PIP calculated bad CRC\n");
+    if (pip_int_reg.s.pktdrp)
+        PRINT_ERROR("PIP_INT_REG[PKTDRP]: Packet Dropped due to QOS\n");
+}
+
+
+/**
+ * __cvmx_interrupt_pko_reg_int_mask_enable enables all interrupt bits in cvmx_pko_reg_int_mask_t
+ */
+void __cvmx_interrupt_pko_reg_int_mask_enable(void)
+{
+    cvmx_pko_reg_int_mask_t pko_reg_int_mask;
+    cvmx_write_csr(CVMX_PKO_REG_ERROR, cvmx_read_csr(CVMX_PKO_REG_ERROR));
+    pko_reg_int_mask.u64 = 0;
+    if (OCTEON_IS_MODEL(OCTEON_CN56XX))
+    {
+        // Skipping pko_reg_int_mask.s.reserved_3_63
+        pko_reg_int_mask.s.currzero = 1;
+        pko_reg_int_mask.s.doorbell = 1;
+        pko_reg_int_mask.s.parity = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN30XX))
+    {
+        // Skipping pko_reg_int_mask.s.reserved_2_63
+        pko_reg_int_mask.s.doorbell = 1;
+        pko_reg_int_mask.s.parity = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN50XX))
+    {
+        // Skipping pko_reg_int_mask.s.reserved_3_63
+        pko_reg_int_mask.s.currzero = 1;
+        pko_reg_int_mask.s.doorbell = 1;
+        pko_reg_int_mask.s.parity = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN38XX))
+    {
+        // Skipping pko_reg_int_mask.s.reserved_2_63
+        pko_reg_int_mask.s.doorbell = 1;
+        pko_reg_int_mask.s.parity = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN31XX))
+    {
+        // Skipping pko_reg_int_mask.s.reserved_2_63
+        pko_reg_int_mask.s.doorbell = 1;
+        pko_reg_int_mask.s.parity = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN58XX))
+    {
+        // Skipping pko_reg_int_mask.s.reserved_3_63
+        pko_reg_int_mask.s.currzero = 1;
+        pko_reg_int_mask.s.doorbell = 1;
+        pko_reg_int_mask.s.parity = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN52XX))
+    {
+        // Skipping pko_reg_int_mask.s.reserved_3_63
+        pko_reg_int_mask.s.currzero = 1;
+        pko_reg_int_mask.s.doorbell = 1;
+        pko_reg_int_mask.s.parity = 1;
+    }
+    cvmx_write_csr(CVMX_PKO_REG_INT_MASK, pko_reg_int_mask.u64);
+}
+
+
+/**
+ * __cvmx_interrupt_pko_reg_error_decode decodes all interrupt bits in cvmx_pko_reg_error_t
+ */
+void __cvmx_interrupt_pko_reg_error_decode(void)
+{
+    cvmx_pko_reg_error_t pko_reg_error;
+    pko_reg_error.u64 = cvmx_read_csr(CVMX_PKO_REG_ERROR);
+    cvmx_write_csr(CVMX_PKO_REG_ERROR, pko_reg_error.u64);
+    pko_reg_error.u64 &= cvmx_read_csr(CVMX_PKO_REG_INT_MASK);
+    // Skipping pko_reg_error.s.reserved_3_63
+    if (pko_reg_error.s.currzero)
+        PRINT_ERROR("PKO_REG_ERROR[CURRZERO]: A packet data pointer has size=0\n");
+    if (pko_reg_error.s.doorbell)
+        PRINT_ERROR("PKO_REG_ERROR[DOORBELL]: A doorbell count has overflowed\n");
+    if (pko_reg_error.s.parity)
+        PRINT_ERROR("PKO_REG_ERROR[PARITY]: Read parity error at port data buffer\n");
+}
+
+
+/**
+ * __cvmx_interrupt_rad_reg_int_mask_enable enables all interrupt bits in cvmx_rad_reg_int_mask_t
+ */
+void __cvmx_interrupt_rad_reg_int_mask_enable(void)
+{
+    cvmx_rad_reg_int_mask_t rad_reg_int_mask;
+    cvmx_write_csr(CVMX_RAD_REG_ERROR, cvmx_read_csr(CVMX_RAD_REG_ERROR));
+    rad_reg_int_mask.u64 = 0;
+    if (OCTEON_IS_MODEL(OCTEON_CN56XX))
+    {
+        // Skipping rad_reg_int_mask.s.reserved_1_63
+        rad_reg_int_mask.s.doorbell = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN52XX))
+    {
+        // Skipping rad_reg_int_mask.s.reserved_1_63
+        rad_reg_int_mask.s.doorbell = 1;
+    }
+    cvmx_write_csr(CVMX_RAD_REG_INT_MASK, rad_reg_int_mask.u64);
+}
+
+
+/**
+ * __cvmx_interrupt_rad_reg_error_decode decodes all interrupt bits in cvmx_rad_reg_error_t
+ */
+void __cvmx_interrupt_rad_reg_error_decode(void)
+{
+    cvmx_rad_reg_error_t rad_reg_error;
+    rad_reg_error.u64 = cvmx_read_csr(CVMX_RAD_REG_ERROR);
+    cvmx_write_csr(CVMX_RAD_REG_ERROR, rad_reg_error.u64);
+    rad_reg_error.u64 &= cvmx_read_csr(CVMX_RAD_REG_INT_MASK);
+    // Skipping rad_reg_error.s.reserved_1_63
+    if (rad_reg_error.s.doorbell)
+        PRINT_ERROR("RAD_REG_ERROR[DOORBELL]: A doorbell count has overflowed\n");
+}
+
+
+/**
+ * __cvmx_interrupt_spxx_int_msk_enable enables all interrupt bits in cvmx_spxx_int_msk_t
+ */
+void __cvmx_interrupt_spxx_int_msk_enable(int index)
+{
+    cvmx_spxx_int_msk_t spx_int_msk;
+    cvmx_write_csr(CVMX_SPXX_INT_REG(index), cvmx_read_csr(CVMX_SPXX_INT_REG(index)));
+    spx_int_msk.u64 = 0;
+    if (OCTEON_IS_MODEL(OCTEON_CN38XX))
+    {
+        // Skipping spx_int_msk.s.reserved_12_63
+        spx_int_msk.s.calerr = 1;
+        spx_int_msk.s.syncerr = 1;
+        spx_int_msk.s.diperr = 1;
+        spx_int_msk.s.tpaovr = 1;
+        spx_int_msk.s.rsverr = 1;
+        spx_int_msk.s.drwnng = 1;
+        spx_int_msk.s.clserr = 1;
+        spx_int_msk.s.spiovr = 1;
+        // Skipping spx_int_msk.s.reserved_2_3
+        spx_int_msk.s.abnorm = 1;
+        spx_int_msk.s.prtnxa = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN58XX))
+    {
+        // Skipping spx_int_msk.s.reserved_12_63
+        spx_int_msk.s.calerr = 1;
+        spx_int_msk.s.syncerr = 1;
+        spx_int_msk.s.diperr = 1;
+        spx_int_msk.s.tpaovr = 1;
+        spx_int_msk.s.rsverr = 1;
+        spx_int_msk.s.drwnng = 1;
+        spx_int_msk.s.clserr = 1;
+        spx_int_msk.s.spiovr = 1;
+        // Skipping spx_int_msk.s.reserved_2_3
+        spx_int_msk.s.abnorm = 1;
+        spx_int_msk.s.prtnxa = 1;
+    }
+    cvmx_write_csr(CVMX_SPXX_INT_MSK(index), spx_int_msk.u64);
+}
+
+
+/**
+ * __cvmx_interrupt_spxx_int_reg_decode decodes all interrupt bits in cvmx_spxx_int_reg_t
+ */
+void __cvmx_interrupt_spxx_int_reg_decode(int index)
+{
+    cvmx_spxx_int_reg_t spx_int_reg;
+    spx_int_reg.u64 = cvmx_read_csr(CVMX_SPXX_INT_REG(index));
+    cvmx_write_csr(CVMX_SPXX_INT_REG(index), spx_int_reg.u64);
+    spx_int_reg.u64 &= cvmx_read_csr(CVMX_SPXX_INT_MSK(index));
+    // Skipping spx_int_reg.s.reserved_32_63
+    if (spx_int_reg.s.spf)
+        PRINT_ERROR("SPX%d_INT_REG[SPF]: Spi interface down\n", index);
+    // Skipping spx_int_reg.s.reserved_12_30
+    if (spx_int_reg.s.calerr)
+        PRINT_ERROR("SPX%d_INT_REG[CALERR]: Spi4 Calendar table parity error\n", index);
+    if (spx_int_reg.s.syncerr)
+        PRINT_ERROR("SPX%d_INT_REG[SYNCERR]: Consecutive Spi4 DIP4 errors have exceeded\n"
+                    "    SPX_ERR_CTL[ERRCNT]\n", index);
+    if (spx_int_reg.s.diperr)
+        PRINT_ERROR("SPX%d_INT_REG[DIPERR]: Spi4 DIP4 error\n", index);
+    if (spx_int_reg.s.tpaovr)
+        PRINT_ERROR("SPX%d_INT_REG[TPAOVR]: Selected port has hit TPA overflow\n", index);
+    if (spx_int_reg.s.rsverr)
+        PRINT_ERROR("SPX%d_INT_REG[RSVERR]: Spi4 reserved control word detected\n", index);
+    if (spx_int_reg.s.drwnng)
+        PRINT_ERROR("SPX%d_INT_REG[DRWNNG]: Spi4 receive FIFO drowning/overflow\n", index);
+    if (spx_int_reg.s.clserr)
+        PRINT_ERROR("SPX%d_INT_REG[CLSERR]: Spi4 packet closed on non-16B alignment without EOP\n", index);
+    if (spx_int_reg.s.spiovr)
+        PRINT_ERROR("SPX%d_INT_REG[SPIOVR]: Spi async FIFO overflow\n", index);
+    // Skipping spx_int_reg.s.reserved_2_3
+    if (spx_int_reg.s.abnorm)
+        PRINT_ERROR("SPX%d_INT_REG[ABNORM]: Abnormal packet termination (ERR bit)\n", index);
+    if (spx_int_reg.s.prtnxa)
+        PRINT_ERROR("SPX%d_INT_REG[PRTNXA]: Port out of range\n", index);
+}
+
+
+/**
+ * __cvmx_interrupt_usbnx_int_enb_enable enables all interrupt bits in cvmx_usbnx_int_enb_t
+ */
+void __cvmx_interrupt_usbnx_int_enb_enable(int index)
+{
+    cvmx_usbnx_int_enb_t usbn_int_enb;
+    cvmx_write_csr(CVMX_USBNX_INT_SUM(index), cvmx_read_csr(CVMX_USBNX_INT_SUM(index)));
+    usbn_int_enb.u64 = 0;
+    if (OCTEON_IS_MODEL(OCTEON_CN30XX))
+    {
+        // Skipping usbn_int_enb.s.reserved_38_63
+        usbn_int_enb.s.nd4o_dpf = 1;
+        usbn_int_enb.s.nd4o_dpe = 1;
+        usbn_int_enb.s.nd4o_rpf = 1;
+        usbn_int_enb.s.nd4o_rpe = 1;
+        usbn_int_enb.s.ltl_f_pf = 1;
+        usbn_int_enb.s.ltl_f_pe = 1;
+        usbn_int_enb.s.u2n_c_pe = 1;
+        usbn_int_enb.s.u2n_c_pf = 1;
+        usbn_int_enb.s.u2n_d_pf = 1;
+        usbn_int_enb.s.u2n_d_pe = 1;
+        usbn_int_enb.s.n2u_pe = 1;
+        usbn_int_enb.s.n2u_pf = 1;
+        usbn_int_enb.s.uod_pf = 1;
+        usbn_int_enb.s.uod_pe = 1;
+        usbn_int_enb.s.rq_q3_e = 1;
+        usbn_int_enb.s.rq_q3_f = 1;
+        usbn_int_enb.s.rq_q2_e = 1;
+        usbn_int_enb.s.rq_q2_f = 1;
+        usbn_int_enb.s.rg_fi_f = 1;
+        usbn_int_enb.s.rg_fi_e = 1;
+        usbn_int_enb.s.l2_fi_f = 1;
+        usbn_int_enb.s.l2_fi_e = 1;
+        usbn_int_enb.s.l2c_a_f = 1;
+        usbn_int_enb.s.l2c_s_e = 1;
+        usbn_int_enb.s.dcred_f = 1;
+        usbn_int_enb.s.dcred_e = 1;
+        usbn_int_enb.s.lt_pu_f = 1;
+        usbn_int_enb.s.lt_po_e = 1;
+        usbn_int_enb.s.nt_pu_f = 1;
+        usbn_int_enb.s.nt_po_e = 1;
+        usbn_int_enb.s.pt_pu_f = 1;
+        usbn_int_enb.s.pt_po_e = 1;
+        usbn_int_enb.s.lr_pu_f = 1;
+        usbn_int_enb.s.lr_po_e = 1;
+        usbn_int_enb.s.nr_pu_f = 1;
+        usbn_int_enb.s.nr_po_e = 1;
+        usbn_int_enb.s.pr_pu_f = 1;
+        usbn_int_enb.s.pr_po_e = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN50XX))
+    {
+        // Skipping usbn_int_enb.s.reserved_38_63
+        usbn_int_enb.s.nd4o_dpf = 1;
+        usbn_int_enb.s.nd4o_dpe = 1;
+        usbn_int_enb.s.nd4o_rpf = 1;
+        usbn_int_enb.s.nd4o_rpe = 1;
+        usbn_int_enb.s.ltl_f_pf = 1;
+        usbn_int_enb.s.ltl_f_pe = 1;
+        // Skipping usbn_int_enb.s.reserved_26_31
+        usbn_int_enb.s.uod_pf = 1;
+        usbn_int_enb.s.uod_pe = 1;
+        usbn_int_enb.s.rq_q3_e = 1;
+        usbn_int_enb.s.rq_q3_f = 1;
+        usbn_int_enb.s.rq_q2_e = 1;
+        usbn_int_enb.s.rq_q2_f = 1;
+        usbn_int_enb.s.rg_fi_f = 1;
+        usbn_int_enb.s.rg_fi_e = 1;
+        usbn_int_enb.s.l2_fi_f = 1;
+        usbn_int_enb.s.l2_fi_e = 1;
+        usbn_int_enb.s.l2c_a_f = 1;
+        usbn_int_enb.s.l2c_s_e = 1;
+        usbn_int_enb.s.dcred_f = 1;
+        usbn_int_enb.s.dcred_e = 1;
+        usbn_int_enb.s.lt_pu_f = 1;
+        usbn_int_enb.s.lt_po_e = 1;
+        usbn_int_enb.s.nt_pu_f = 1;
+        usbn_int_enb.s.nt_po_e = 1;
+        usbn_int_enb.s.pt_pu_f = 1;
+        usbn_int_enb.s.pt_po_e = 1;
+        usbn_int_enb.s.lr_pu_f = 1;
+        usbn_int_enb.s.lr_po_e = 1;
+        usbn_int_enb.s.nr_pu_f = 1;
+        usbn_int_enb.s.nr_po_e = 1;
+        usbn_int_enb.s.pr_pu_f = 1;
+        usbn_int_enb.s.pr_po_e = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN31XX))
+    {
+        // Skipping usbn_int_enb.s.reserved_38_63
+        usbn_int_enb.s.nd4o_dpf = 1;
+        usbn_int_enb.s.nd4o_dpe = 1;
+        usbn_int_enb.s.nd4o_rpf = 1;
+        usbn_int_enb.s.nd4o_rpe = 1;
+        usbn_int_enb.s.ltl_f_pf = 1;
+        usbn_int_enb.s.ltl_f_pe = 1;
+        usbn_int_enb.s.u2n_c_pe = 1;
+        usbn_int_enb.s.u2n_c_pf = 1;
+        usbn_int_enb.s.u2n_d_pf = 1;
+        usbn_int_enb.s.u2n_d_pe = 1;
+        usbn_int_enb.s.n2u_pe = 1;
+        usbn_int_enb.s.n2u_pf = 1;
+        usbn_int_enb.s.uod_pf = 1;
+        usbn_int_enb.s.uod_pe = 1;
+        usbn_int_enb.s.rq_q3_e = 1;
+        usbn_int_enb.s.rq_q3_f = 1;
+        usbn_int_enb.s.rq_q2_e = 1;
+        usbn_int_enb.s.rq_q2_f = 1;
+        usbn_int_enb.s.rg_fi_f = 1;
+        usbn_int_enb.s.rg_fi_e = 1;
+        usbn_int_enb.s.l2_fi_f = 1;
+        usbn_int_enb.s.l2_fi_e = 1;
+        usbn_int_enb.s.l2c_a_f = 1;
+        usbn_int_enb.s.l2c_s_e = 1;
+        usbn_int_enb.s.dcred_f = 1;
+        usbn_int_enb.s.dcred_e = 1;
+        usbn_int_enb.s.lt_pu_f = 1;
+        usbn_int_enb.s.lt_po_e = 1;
+        usbn_int_enb.s.nt_pu_f = 1;
+        usbn_int_enb.s.nt_po_e = 1;
+        usbn_int_enb.s.pt_pu_f = 1;
+        usbn_int_enb.s.pt_po_e = 1;
+        usbn_int_enb.s.lr_pu_f = 1;
+        usbn_int_enb.s.lr_po_e = 1;
+        usbn_int_enb.s.nr_pu_f = 1;
+        usbn_int_enb.s.nr_po_e = 1;
+        usbn_int_enb.s.pr_pu_f = 1;
+        usbn_int_enb.s.pr_po_e = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN56XX))
+    {
+        // Skipping usbn_int_enb.s.reserved_38_63
+        usbn_int_enb.s.nd4o_dpf = 1;
+        usbn_int_enb.s.nd4o_dpe = 1;
+        usbn_int_enb.s.nd4o_rpf = 1;
+        usbn_int_enb.s.nd4o_rpe = 1;
+        usbn_int_enb.s.ltl_f_pf = 1;
+        usbn_int_enb.s.ltl_f_pe = 1;
+        // Skipping usbn_int_enb.s.reserved_26_31
+        usbn_int_enb.s.uod_pf = 1;
+        usbn_int_enb.s.uod_pe = 1;
+        usbn_int_enb.s.rq_q3_e = 1;
+        usbn_int_enb.s.rq_q3_f = 1;
+        usbn_int_enb.s.rq_q2_e = 1;
+        usbn_int_enb.s.rq_q2_f = 1;
+        usbn_int_enb.s.rg_fi_f = 1;
+        usbn_int_enb.s.rg_fi_e = 1;
+        usbn_int_enb.s.l2_fi_f = 1;
+        usbn_int_enb.s.l2_fi_e = 1;
+        usbn_int_enb.s.l2c_a_f = 1;
+        usbn_int_enb.s.l2c_s_e = 1;
+        usbn_int_enb.s.dcred_f = 1;
+        usbn_int_enb.s.dcred_e = 1;
+        usbn_int_enb.s.lt_pu_f = 1;
+        usbn_int_enb.s.lt_po_e = 1;
+        usbn_int_enb.s.nt_pu_f = 1;
+        usbn_int_enb.s.nt_po_e = 1;
+        usbn_int_enb.s.pt_pu_f = 1;
+        usbn_int_enb.s.pt_po_e = 1;
+        usbn_int_enb.s.lr_pu_f = 1;
+        usbn_int_enb.s.lr_po_e = 1;
+        usbn_int_enb.s.nr_pu_f = 1;
+        usbn_int_enb.s.nr_po_e = 1;
+        usbn_int_enb.s.pr_pu_f = 1;
+        usbn_int_enb.s.pr_po_e = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN52XX))
+    {
+        // Skipping usbn_int_enb.s.reserved_38_63
+        usbn_int_enb.s.nd4o_dpf = 1;
+        usbn_int_enb.s.nd4o_dpe = 1;
+        usbn_int_enb.s.nd4o_rpf = 1;
+        usbn_int_enb.s.nd4o_rpe = 1;
+        usbn_int_enb.s.ltl_f_pf = 1;
+        usbn_int_enb.s.ltl_f_pe = 1;
+        // Skipping usbn_int_enb.s.reserved_26_31
+        usbn_int_enb.s.uod_pf = 1;
+        usbn_int_enb.s.uod_pe = 1;
+        usbn_int_enb.s.rq_q3_e = 1;
+        usbn_int_enb.s.rq_q3_f = 1;
+        usbn_int_enb.s.rq_q2_e = 1;
+        usbn_int_enb.s.rq_q2_f = 1;
+        usbn_int_enb.s.rg_fi_f = 1;
+        usbn_int_enb.s.rg_fi_e = 1;
+        usbn_int_enb.s.l2_fi_f = 1;
+        usbn_int_enb.s.l2_fi_e = 1;
+        usbn_int_enb.s.l2c_a_f = 1;
+        usbn_int_enb.s.l2c_s_e = 1;
+        usbn_int_enb.s.dcred_f = 1;
+        usbn_int_enb.s.dcred_e = 1;
+        usbn_int_enb.s.lt_pu_f = 1;
+        usbn_int_enb.s.lt_po_e = 1;
+        usbn_int_enb.s.nt_pu_f = 1;
+        usbn_int_enb.s.nt_po_e = 1;
+        usbn_int_enb.s.pt_pu_f = 1;
+        usbn_int_enb.s.pt_po_e = 1;
+        usbn_int_enb.s.lr_pu_f = 1;
+        usbn_int_enb.s.lr_po_e = 1;
+        usbn_int_enb.s.nr_pu_f = 1;
+        usbn_int_enb.s.nr_po_e = 1;
+        usbn_int_enb.s.pr_pu_f = 1;
+        usbn_int_enb.s.pr_po_e = 1;
+    }
+    cvmx_write_csr(CVMX_USBNX_INT_ENB(index), usbn_int_enb.u64);
+}
+
+
+/**
+ * __cvmx_interrupt_usbnx_int_sum_decode decodes all interrupt bits in cvmx_usbnx_int_sum_t
+ */
+void __cvmx_interrupt_usbnx_int_sum_decode(int index)
+{
+    cvmx_usbnx_int_sum_t usbn_int_sum;
+    usbn_int_sum.u64 = cvmx_read_csr(CVMX_USBNX_INT_SUM(index));
+    cvmx_write_csr(CVMX_USBNX_INT_SUM(index), usbn_int_sum.u64);
+    usbn_int_sum.u64 &= cvmx_read_csr(CVMX_USBNX_INT_ENB(index));
+    // Skipping usbn_int_sum.s.reserved_38_63
+    if (usbn_int_sum.s.nd4o_dpf)
+        PRINT_ERROR("USBN%d_INT_SUM[ND4O_DPF]: NCB DMA Out Data Fifo Push Full.\n", index);
+    if (usbn_int_sum.s.nd4o_dpe)
+        PRINT_ERROR("USBN%d_INT_SUM[ND4O_DPE]: NCB DMA Out Data Fifo Pop Empty.\n", index);
+    if (usbn_int_sum.s.nd4o_rpf)
+        PRINT_ERROR("USBN%d_INT_SUM[ND4O_RPF]: NCB DMA Out Request Fifo Push Full.\n", index);
+    if (usbn_int_sum.s.nd4o_rpe)
+        PRINT_ERROR("USBN%d_INT_SUM[ND4O_RPE]: NCB DMA Out Request Fifo Pop Empty.\n", index);
+    if (usbn_int_sum.s.ltl_f_pf)
+        PRINT_ERROR("USBN%d_INT_SUM[LTL_F_PF]: L2C Transfer Length Fifo Push Full.\n", index);
+    if (usbn_int_sum.s.ltl_f_pe)
+        PRINT_ERROR("USBN%d_INT_SUM[LTL_F_PE]: L2C Transfer Length Fifo Pop Empty.\n", index);
+    if (usbn_int_sum.s.u2n_c_pe)
+        PRINT_ERROR("USBN%d_INT_SUM[U2N_C_PE]: U2N Control Fifo Pop Empty.\n", index);
+    if (usbn_int_sum.s.u2n_c_pf)
+        PRINT_ERROR("USBN%d_INT_SUM[U2N_C_PF]: U2N Control Fifo Push Full.\n", index);
+    if (usbn_int_sum.s.u2n_d_pf)
+        PRINT_ERROR("USBN%d_INT_SUM[U2N_D_PF]: U2N Data Fifo Push Full.\n", index);
+    if (usbn_int_sum.s.u2n_d_pe)
+        PRINT_ERROR("USBN%d_INT_SUM[U2N_D_PE]: U2N Data Fifo Pop Empty.\n", index);
+    if (usbn_int_sum.s.n2u_pe)
+        PRINT_ERROR("USBN%d_INT_SUM[N2U_PE]: N2U Fifo Pop Empty.\n", index);
+    if (usbn_int_sum.s.n2u_pf)
+        PRINT_ERROR("USBN%d_INT_SUM[N2U_PF]: N2U Fifo Push Full.\n", index);
+    if (usbn_int_sum.s.uod_pf)
+        PRINT_ERROR("USBN%d_INT_SUM[UOD_PF]: UOD Fifo Push Full.\n", index);
+    if (usbn_int_sum.s.uod_pe)
+        PRINT_ERROR("USBN%d_INT_SUM[UOD_PE]: UOD Fifo Pop Empty.\n", index);
+    if (usbn_int_sum.s.rq_q3_e)
+        PRINT_ERROR("USBN%d_INT_SUM[RQ_Q3_E]: Request Queue-3 Fifo Pushed When Full.\n", index);
+    if (usbn_int_sum.s.rq_q3_f)
+        PRINT_ERROR("USBN%d_INT_SUM[RQ_Q3_F]: Request Queue-3 Fifo Pushed When Full.\n", index);
+    if (usbn_int_sum.s.rq_q2_e)
+        PRINT_ERROR("USBN%d_INT_SUM[RQ_Q2_E]: Request Queue-2 Fifo Pushed When Full.\n", index);
+    if (usbn_int_sum.s.rq_q2_f)
+        PRINT_ERROR("USBN%d_INT_SUM[RQ_Q2_F]: Request Queue-2 Fifo Pushed When Full.\n", index);
+    if (usbn_int_sum.s.rg_fi_f)
+        PRINT_ERROR("USBN%d_INT_SUM[RG_FI_F]: Register Request Fifo Pushed When Full.\n", index);
+    if (usbn_int_sum.s.rg_fi_e)
+        PRINT_ERROR("USBN%d_INT_SUM[RG_FI_E]: Register Request Fifo Pushed When Full.\n", index);
+    if (usbn_int_sum.s.lt_fi_f)
+        PRINT_ERROR("USBN%d_INT_SUM[LT_FI_F]: L2C Request Fifo Pushed When Full.\n", index);
+    if (usbn_int_sum.s.lt_fi_e)
+        PRINT_ERROR("USBN%d_INT_SUM[LT_FI_E]: L2C Request Fifo Pushed When Full.\n", index);
+    if (usbn_int_sum.s.l2c_a_f)
+        PRINT_ERROR("USBN%d_INT_SUM[L2C_A_F]: L2C Credit Count Added When Full.\n", index);
+    if (usbn_int_sum.s.l2c_s_e)
+        PRINT_ERROR("USBN%d_INT_SUM[L2C_S_E]: L2C Credit Count Subtracted When Empty.\n", index);
+    if (usbn_int_sum.s.dcred_f)
+        PRINT_ERROR("USBN%d_INT_SUM[DCRED_F]: Data CreditFifo Pushed When Full.\n", index);
+    if (usbn_int_sum.s.dcred_e)
+        PRINT_ERROR("USBN%d_INT_SUM[DCRED_E]: Data Credit Fifo Pushed When Full.\n", index);
+    if (usbn_int_sum.s.lt_pu_f)
+        PRINT_ERROR("USBN%d_INT_SUM[LT_PU_F]: L2C Trasaction Fifo Pushed When Full.\n", index);
+    if (usbn_int_sum.s.lt_po_e)
+        PRINT_ERROR("USBN%d_INT_SUM[LT_PO_E]: L2C Trasaction Fifo Popped When Full.\n", index);
+    if (usbn_int_sum.s.nt_pu_f)
+        PRINT_ERROR("USBN%d_INT_SUM[NT_PU_F]: NPI Trasaction Fifo Pushed When Full.\n", index);
+    if (usbn_int_sum.s.nt_po_e)
+        PRINT_ERROR("USBN%d_INT_SUM[NT_PO_E]: NPI Trasaction Fifo Popped When Full.\n", index);
+    if (usbn_int_sum.s.pt_pu_f)
+        PRINT_ERROR("USBN%d_INT_SUM[PT_PU_F]: PP  Trasaction Fifo Pushed When Full.\n", index);
+    if (usbn_int_sum.s.pt_po_e)
+        PRINT_ERROR("USBN%d_INT_SUM[PT_PO_E]: PP  Trasaction Fifo Popped When Full.\n", index);
+    if (usbn_int_sum.s.lr_pu_f)
+        PRINT_ERROR("USBN%d_INT_SUM[LR_PU_F]: L2C Request Fifo Pushed When Full.\n", index);
+    if (usbn_int_sum.s.lr_po_e)
+        PRINT_ERROR("USBN%d_INT_SUM[LR_PO_E]: L2C Request Fifo Popped When Empty.\n", index);
+    if (usbn_int_sum.s.nr_pu_f)
+        PRINT_ERROR("USBN%d_INT_SUM[NR_PU_F]: NPI Request Fifo Pushed When Full.\n", index);
+    if (usbn_int_sum.s.nr_po_e)
+        PRINT_ERROR("USBN%d_INT_SUM[NR_PO_E]: NPI Request Fifo Popped When Empty.\n", index);
+    if (usbn_int_sum.s.pr_pu_f)
+        PRINT_ERROR("USBN%d_INT_SUM[PR_PU_F]: PP  Request Fifo Pushed When Full.\n", index);
+    if (usbn_int_sum.s.pr_po_e)
+        PRINT_ERROR("USBN%d_INT_SUM[PR_PO_E]: PP  Request Fifo Popped When Empty.\n", index);
+}
+
+
+/**
+ * __cvmx_interrupt_zip_int_mask_enable enables all interrupt bits in cvmx_zip_int_mask_t
+ */
+void __cvmx_interrupt_zip_int_mask_enable(void)
+{
+    cvmx_zip_int_mask_t zip_int_mask;
+    cvmx_write_csr(CVMX_ZIP_ERROR, cvmx_read_csr(CVMX_ZIP_ERROR));
+    zip_int_mask.u64 = 0;
+    if (OCTEON_IS_MODEL(OCTEON_CN56XX))
+    {
+        // Skipping zip_int_mask.s.reserved_1_63
+        zip_int_mask.s.doorbell = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN38XX))
+    {
+        // Skipping zip_int_mask.s.reserved_1_63
+        zip_int_mask.s.doorbell = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN31XX))
+    {
+        // Skipping zip_int_mask.s.reserved_1_63
+        zip_int_mask.s.doorbell = 1;
+    }
+    if (OCTEON_IS_MODEL(OCTEON_CN58XX))
+    {
+        // Skipping zip_int_mask.s.reserved_1_63
+        zip_int_mask.s.doorbell = 1;
+    }
+    cvmx_write_csr(CVMX_ZIP_INT_MASK, zip_int_mask.u64);
+}
+
+
+/**
+ * __cvmx_interrupt_zip_error_decode decodes all interrupt bits in cvmx_zip_error_t
+ */
+void __cvmx_interrupt_zip_error_decode(void)
+{
+    cvmx_zip_error_t zip_error;
+    zip_error.u64 = cvmx_read_csr(CVMX_ZIP_ERROR);
+    cvmx_write_csr(CVMX_ZIP_ERROR, zip_error.u64);
+    zip_error.u64 &= cvmx_read_csr(CVMX_ZIP_INT_MASK);
+    // Skipping zip_error.s.reserved_1_63
+    if (zip_error.s.doorbell)
+        PRINT_ERROR("ZIP_ERROR[DOORBELL]: A doorbell count has overflowed\n");
+}
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-interrupt-handler.S b/arch/mips/cavium-octeon/executive/cvmx-interrupt-handler.S
new file mode 100644
index 0000000..c807507
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-interrupt-handler.S
@@ -0,0 +1,189 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+
+
+
+#include <machine/asm.h>
+#include <machine/regdef.h>
+
+.set noreorder
+.set noat
+
+LEAF(cvmx_interrupt_stage1)
+	dla     k0, cvmx_interrupt_stage2
+	jalr 	k1, k0   // Save our address in k1, so we can tell which
+                         // vector we are coming from.
+	nop
+END(cvmx_interrupt_stage1)
+
+#define STACK_SIZE  (36*8)
+LEAF(cvmx_interrupt_stage2)
+	dsubu	sp, sp, STACK_SIZE
+	sd	zero, 0(sp)	// Just a place holder
+	sd	$1, 8(sp)	// start saving registers
+	sd	$2, 16(sp)
+	sd 	$3, 24(sp)
+	sd 	$4, 32(sp)
+	sd	$5, 40(sp)
+	sd	$6, 48(sp)
+	sd	$7, 56(sp)
+	sd	$8, 64(sp)
+	sd	$9, 72(sp)
+	sd	$10, 80(sp)
+	sd	$11, 88(sp)
+	sd	$12, 96(sp)
+	sd	$13, 104(sp)
+	sd	$14, 112(sp)
+	sd	$15, 120(sp)
+	sd	$16, 128(sp)
+	sd	$17, 136(sp)
+	sd	$18, 144(sp)
+	sd	$19, 152(sp)
+	sd	$20, 160(sp)
+	sd	$21, 168(sp)
+	sd	$22, 176(sp)
+	sd	$23, 184(sp)
+	sd	$24, 192(sp)
+	sd	$25, 200(sp)
+	sd	$26, 208(sp)
+	sd	$27, 216(sp)
+	mfhi	k0		// Reading lo and high takes multiple cycles
+	mflo	k1		// Do it here so it completes by the time we need it
+	sd	$28, 224(sp)
+	daddu	$1, sp, STACK_SIZE // Correct the SP for the space we used
+	sd	$1, 232(sp)
+	sd	$30, 240(sp)
+	sd	$31, 248(sp)	// saved all general purpose registers
+	sd	k0, 256(sp)	// save hi
+	sd	k1, 264(sp)	// save lo
+        /* Save DCACHE error register early, since any non-errored DCACHE accesses will clear
+        ** error bit */
+        dmfc0   k0, $27, 1
+        sd      k0, 272(sp)
+        /* Store DEPC for GCC's frame unwinder. */
+        dmfc0   k0, $14
+        sd      k0, 280(sp)
+
+	dla     k0, cvmx_interrupt_do_irq
+	jal 	k0
+	dadd	a0, sp, 0	// First argument is array of registers
+
+	ld	k0, 256(sp)	// read hi
+	ld	k1, 264(sp)	// read lo
+	mthi	k0		// restore hi
+	mtlo	k1		// restore lo
+
+	ld	$1, 8(sp)	// start restoring registers
+	ld	$2, 16(sp)
+	ld 	$3, 24(sp)
+	ld 	$4, 32(sp)
+	ld	$5, 40(sp)
+	ld	$6, 48(sp)
+	ld	$7, 56(sp)
+	ld	$8, 64(sp)
+	ld	$9, 72(sp)
+	ld	$10, 80(sp)
+	ld	$11, 88(sp)
+	ld	$12, 96(sp)
+	ld	$13, 104(sp)
+	ld	$14, 112(sp)
+	ld	$15, 120(sp)
+	ld	$16, 128(sp)
+	ld	$17, 136(sp)
+	ld	$18, 144(sp)
+	ld	$19, 152(sp)
+	ld	$20, 160(sp)
+	ld	$21, 168(sp)
+	ld	$22, 176(sp)
+	ld	$23, 184(sp)
+	ld	$24, 192(sp)
+	ld	$25, 200(sp)
+	ld	$26, 208(sp)
+	ld	$28, 224(sp)
+	ld	$30, 240(sp)
+	ld	$31, 248(sp)	// restored all general purpose registers
+	ld	$29, 232(sp)	// No need to correct for STACK_SIZE
+	eret
+	nop
+END(cvmx_interrupt_stage2)
+
+// Icache and Dcache exception handler. This code is executed
+// with ERL set so we can't us virtual addresses. We save and restore
+// K0 to a global memory location so we can handle cache errors from exception
+// context. This means that if two cores get a cache exception at the same time
+// the K0 might be corrupted. This entire handler MUST fit in 128 bytes.
+#define K0_STORE_LOCATION	8
+#define DCACHE_ERROR_COUNT	16
+#define ICACHE_ERROR_COUNT	24
+LEAF(cvmx_interrupt_cache_error)
+	.set push
+	.set noreorder
+	sd	k0, K0_STORE_LOCATION($0)	// Store K0 into global loc in case we're in an exception
+	dmfc0	k0, $27, 1			// Get Dcache error status before any loads
+	bbit0	k0, 0, not_dcache_error		// Skip dcache count if no error
+	 dmtc0	k0, $27, 1			// Clear any Dcache errors
+	ld	k0, DCACHE_ERROR_COUNT($0)	// Load the dcache error count
+	daddu	k0, 1				// Increment the dcache error count
+	sd	k0, DCACHE_ERROR_COUNT($0)	// Store the dcache error count
+not_dcache_error:
+	dmfc0	k0, $27, 0			// Get the Icache error status
+	bbit0	k0, 0, not_icache_error		// Skip Icache count if no error
+	 dmtc0	k0, $27, 0			// Clear any Icache errors
+	ld	k0, ICACHE_ERROR_COUNT($0)	// Load the icache error count
+	daddu	k0, 1				// Increment the icache error count
+	sd	k0, ICACHE_ERROR_COUNT($0)	// Store the icache error count
+not_icache_error:
+	ld	k0, K0_STORE_LOCATION($0)	// Restore K0 since we might have been in an exception
+	eret					// Return from the Icache exception
+	.set pop
+END(cvmx_interrupt_cache_error)
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-interrupt-rsl.c b/arch/mips/cavium-octeon/executive/cvmx-interrupt-rsl.c
new file mode 100644
index 0000000..19a87c6
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-interrupt-rsl.c
@@ -0,0 +1,702 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * Utility functions to decode Octeon's RSL_INT_BLOCKS
+ * interrupts into error messages.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+#include "cvmx.h"
+#include "cvmx-l2c.h"
+
+#ifndef PRINT_ERROR
+#define PRINT_ERROR(format, ...) cvmx_dprintf("ERROR " format, ##__VA_ARGS__)
+#endif
+
+/* Change this to a 1 before calling cvmx_interrupt_rsl_enable() to report
+    single bit ecc errors and other correctable errors */
+CVMX_SHARED int __cvmx_interrupt_ecc_report_single_bit_errors = 0;
+
+void __cvmx_interrupt_agl_gmx_rxx_int_en_enable(int index);
+void __cvmx_interrupt_agl_gmx_rxx_int_reg_decode(int index);
+void __cvmx_interrupt_fpa_int_enb_enable(void);
+void __cvmx_interrupt_fpa_int_sum_decode(void);
+void __cvmx_interrupt_gmxx_rxx_int_en_enable(int index, int block);
+void __cvmx_interrupt_gmxx_rxx_int_reg_decode(int index, int block);
+void __cvmx_interrupt_iob_int_enb_enable(void);
+void __cvmx_interrupt_iob_int_sum_decode(void);
+void __cvmx_interrupt_ipd_int_enb_enable(void);
+void __cvmx_interrupt_ipd_int_sum_decode(void);
+void __cvmx_interrupt_key_int_enb_enable(void);
+void __cvmx_interrupt_key_int_sum_decode(void);
+void __cvmx_interrupt_mio_boot_int_enable(void);
+void __cvmx_interrupt_mio_boot_err_decode(void);
+void __cvmx_interrupt_npei_int_sum_decode(void);
+void __cvmx_interrupt_npei_int_enb2_enable(void);
+void __cvmx_interrupt_npi_int_enb_enable(void);
+void __cvmx_interrupt_npi_int_sum_decode(void);
+void __cvmx_interrupt_pcsx_intx_en_reg_enable(int index, int block);
+void __cvmx_interrupt_pcsx_intx_reg_decode(int index, int block);
+void __cvmx_interrupt_pescx_dbg_info_en_enable(int index);
+void __cvmx_interrupt_pescx_dbg_info_decode(int index);
+void __cvmx_interrupt_pip_int_en_enable(void);
+void __cvmx_interrupt_pip_int_reg_decode(void);
+void __cvmx_interrupt_pko_reg_int_mask_enable(void);
+void __cvmx_interrupt_pko_reg_error_decode(void);
+void __cvmx_interrupt_rad_reg_int_mask_enable(void);
+void __cvmx_interrupt_rad_reg_error_decode(void);
+void __cvmx_interrupt_spxx_int_msk_enable(int index);
+void __cvmx_interrupt_spxx_int_reg_decode(int index);
+void __cvmx_interrupt_usbnx_int_enb_enable(int index);
+void __cvmx_interrupt_usbnx_int_sum_decode(int index);
+void __cvmx_interrupt_zip_int_mask_enable(void);
+void __cvmx_interrupt_zip_error_decode(void);
+
+
+/**
+ * Enable ASX error interrupts that exist on CN3XXX, CN50XX, and
+ * CN58XX.
+ *
+ * @param block  Interface to enable 0-1
+ */
+static void __cvmx_interrupt_asxx_enable(int block)
+{
+    int mask;
+    cvmx_asxx_int_en_t csr;
+    /* CN38XX and CN58XX have two interfaces with 4 ports per interface. All
+        other chips have a max of 3 ports on interface 0 */
+    if (OCTEON_IS_MODEL(OCTEON_CN38XX) || OCTEON_IS_MODEL(OCTEON_CN58XX))
+        mask = 0xf; /* Set enables for 4 ports */
+    else
+        mask = 0x7; /* Set enables for 3 ports */
+
+    /* Enable interface interrupts */
+    csr.u64 = cvmx_read_csr(CVMX_ASXX_INT_EN(block));
+    csr.s.txpsh = mask;
+    csr.s.txpop = mask;
+    csr.s.ovrflw = mask;
+    cvmx_write_csr(CVMX_ASXX_INT_EN(block), csr.u64);
+}
+
+
+/**
+ * Decode ASX error interrupts for CN3XXX, CN50XX, and CN58XX
+ *
+ * @param block  Interface to decode 0-1
+ */
+static void __cvmx_interrupt_asxx_decode(int block)
+{
+    cvmx_asxx_int_reg_t err;
+    err.u64 = cvmx_read_csr(CVMX_ASXX_INT_REG(block));
+    cvmx_write_csr(CVMX_ASXX_INT_REG(block), err.u64);
+    if (err.u64)
+    {
+        int port;
+        for (port = 0; port < 4; port++)
+        {
+            if (err.s.ovrflw & (1 << port))
+                PRINT_ERROR("ASX%d_INT_REG[OVRFLW]: RX FIFO overflow on RMGII port %d\n",
+                             block, port + block*16);
+            if (err.s.txpop & (1 << port))
+                PRINT_ERROR("ASX%d_INT_REG[TXPOP]: TX FIFO underflow on RMGII port %d\n",
+                             block, port + block*16);
+            if (err.s.txpsh & (1 << port))
+                PRINT_ERROR("ASX%d_INT_REG[TXPSH]: TX FIFO overflow on RMGII port %d\n",
+                             block, port + block*16);
+        }
+    }
+}
+
+
+/**
+ * Enable DFA errors for CN38XX, CN58XX, and CN31XX
+ */
+static void __cvmx_interrupt_dfa_enable(void)
+{
+    cvmx_dfa_err_t csr;
+    csr.u64 = cvmx_read_csr(CVMX_DFA_ERR);
+    csr.s.dblina = 1;
+    csr.s.cp2pina = 1;
+    csr.s.cp2parena = 0;
+    csr.s.dtepina = 1;
+    csr.s.dteparena = 1;
+    csr.s.dtedbina = 1;
+    csr.s.dtesbina = __cvmx_interrupt_ecc_report_single_bit_errors;
+    csr.s.dteeccena = 1;
+    csr.s.cp2dbina = 1;
+    csr.s.cp2sbina = __cvmx_interrupt_ecc_report_single_bit_errors;
+    csr.s.cp2eccena = 1;
+    cvmx_write_csr(CVMX_DFA_ERR, csr.u64);
+}
+
+
+/**
+ * Decode DFA errors for CN38XX, CN58XX, and CN31XX
+ */
+static void __cvmx_interrupt_dfa_decode(void)
+{
+    cvmx_dfa_err_t err;
+
+    err.u64 = cvmx_read_csr(CVMX_DFA_ERR);
+    cvmx_write_csr(CVMX_DFA_ERR, err.u64);
+    if (err.u64)
+    {
+        if (err.s.dblovf)
+            PRINT_ERROR("DFA_ERR[DBLOVF]: Doorbell Overflow detected\n");
+        if (err.s.cp2perr)
+            PRINT_ERROR("DFA_ERR[CP2PERR]: PP-CP2 Parity Error Detected\n");
+        if (err.s.dteperr)
+            PRINT_ERROR("DFA_ERR[DTEPERR]: DTE Parity Error Detected\n");
+
+        if (err.s.dtedbe)
+            PRINT_ERROR("DFA_ERR[DTEDBE]: DFA DTE 29b Double Bit Error Detected\n");
+        if (err.s.dtesbe && __cvmx_interrupt_ecc_report_single_bit_errors)
+            PRINT_ERROR("DFA_ERR[DTESBE]: DFA DTE 29b Single Bit Error Corrected\n");
+        if (err.s.dtedbe || (err.s.dtesbe && __cvmx_interrupt_ecc_report_single_bit_errors))
+            PRINT_ERROR("DFA_ERR[DTESYN]: Failing syndrome %u\n", err.s.dtesyn);
+
+        if (err.s.cp2dbe)
+            PRINT_ERROR("DFA_ERR[CP2DBE]: DFA PP-CP2 Double Bit Error Detected\n");
+        if (err.s.cp2sbe && __cvmx_interrupt_ecc_report_single_bit_errors)
+            PRINT_ERROR("DFA_ERR[CP2SBE]: DFA PP-CP2 Single Bit Error Corrected\n");
+        if (err.s.cp2dbe || (err.s.cp2sbe && __cvmx_interrupt_ecc_report_single_bit_errors))
+            PRINT_ERROR("DFA_ERR[CP2SYN]: Failing syndrome %u\n", err.s.cp2syn);
+    }
+}
+
+
+/**
+ * Enable L2 error interrupts for all chips
+ */
+static void __cvmx_interrupt_l2_enable(void)
+{
+    cvmx_l2t_err_t csr;
+    cvmx_l2d_err_t csr2;
+
+    /* Enable ECC Interrupts for double bit errors from L2C Tags */
+    csr.u64 = cvmx_read_csr(CVMX_L2T_ERR);
+    csr.s.lck_intena2 = 1;
+    csr.s.lck_intena = 1;
+    csr.s.ded_intena = 1;
+    csr.s.sec_intena = __cvmx_interrupt_ecc_report_single_bit_errors;
+    csr.s.ecc_ena = 1;
+    cvmx_write_csr(CVMX_L2T_ERR, csr.u64);
+
+    /* Enable ECC Interrupts for double bit errors from L2D Errors */
+    csr2.u64 = cvmx_read_csr(CVMX_L2D_ERR);
+    csr2.s.ded_intena = 1;
+    csr2.s.sec_intena = __cvmx_interrupt_ecc_report_single_bit_errors;
+    csr2.s.ecc_ena = 1;
+    cvmx_write_csr(CVMX_L2D_ERR, csr2.u64);
+}
+
+
+/**
+ * Decode L2 error interrupts for all chips
+ */
+static void __cvmx_interrupt_l2_decode(void)
+{
+    cvmx_l2t_err_t terr;
+    cvmx_l2d_err_t derr;
+    uint64_t clr_val;
+
+    terr.u64 = cvmx_read_csr(CVMX_L2T_ERR);
+    cvmx_write_csr(CVMX_L2T_ERR, terr.u64);
+    if (terr.u64)
+    {
+        if (terr.s.ded_err)
+            PRINT_ERROR("L2T_ERR[DED_ERR]: double bit:\tfadr: 0x%x, fset: 0x%x, fsyn: 0x%x\n",
+                         terr.s.fadr, terr.s.fset, terr.s.fsyn);
+        if (terr.s.sec_err && __cvmx_interrupt_ecc_report_single_bit_errors)
+            PRINT_ERROR("L2T_ERR[SEC_ERR]: single bit:\tfadr: 0x%x, fset: 0x%x, fsyn: 0x%x\n",
+                         terr.s.fadr, terr.s.fset, terr.s.fsyn);
+        if (terr.s.ded_err || terr.s.sec_err)
+        {
+            if (!terr.s.fsyn)
+            {
+                /* Syndrome is zero, which means error was in non-hit line,
+                    so flush all associations */
+                int i;
+                int l2_assoc = cvmx_l2c_get_num_assoc();
+
+                for (i = 0; i < l2_assoc; i++)
+                    cvmx_l2c_flush_line(i, terr.s.fadr);
+            }
+            else
+                cvmx_l2c_flush_line(terr.s.fset, terr.s.fadr);
+
+        }
+        if (terr.s.lckerr2)
+            PRINT_ERROR("L2T_ERR[LCKERR2]: HW detected a case where a Rd/Wr Miss from PP#n could not find an available/unlocked set (for replacement).\n");
+        if (terr.s.lckerr)
+            PRINT_ERROR("L2T_ERR[LCKERR]: SW attempted to LOCK DOWN the last available set of the INDEX (which is ignored by HW - but reported to SW).\n");
+    }
+
+    clr_val = derr.u64 = cvmx_read_csr(CVMX_L2D_ERR);
+    if (derr.u64)
+    {
+        cvmx_l2d_fadr_t fadr;
+
+        if (derr.s.ded_err || (derr.s.sec_err && __cvmx_interrupt_ecc_report_single_bit_errors))
+        {
+            const int coreid = (int) cvmx_get_core_num();
+            uint64_t syn0 = cvmx_read_csr(CVMX_L2D_FSYN0);
+            uint64_t syn1 = cvmx_read_csr(CVMX_L2D_FSYN1);
+            fadr.u64 = cvmx_read_csr(CVMX_L2D_FADR);
+            if (derr.s.ded_err)
+                PRINT_ERROR("L2D_ERR[DED_ERR] ECC double (core %d): fadr: 0x%llx, syn0:0x%llx, syn1: 0x%llx\n",
+                             coreid, (unsigned long long) fadr.u64, (unsigned long long) syn0, (unsigned long long) syn1);
+            else
+                PRINT_ERROR("L2D_ERR[SEC_ERR] ECC single (core %d): fadr: 0x%llx, syn0:0x%llx, syn1: 0x%llx\n",
+                             coreid, (unsigned long long) fadr.u64, (unsigned long long) syn0, (unsigned long long) syn1);
+            /* Flush the line that had the error */
+            if (derr.s.ded_err || derr.s.sec_err)
+                cvmx_l2c_flush_line(fadr.s.fset, fadr.s.fadr >> 1);
+        }
+    }
+    cvmx_write_csr(CVMX_L2D_ERR, clr_val);
+}
+
+
+/**
+ * Enable LMC (DDR controller) interrupts for all chips
+ *
+ * @param ddr_controller
+ *               Which controller to enable for 0-1
+ */
+static void __cvmx_interrupt_lmcx_enable(int ddr_controller)
+{
+    cvmx_lmc_mem_cfg0_t csr;
+
+    /* The LMC controllers can be independently enabled/disabled on CN56XX.
+        If a controller is disabled it can't be accessed at all since it
+        isn't clocked */
+    if (OCTEON_IS_MODEL(OCTEON_CN56XX))
+    {
+        cvmx_l2c_cfg_t l2c_cfg;
+        l2c_cfg.u64 = cvmx_read_csr(CVMX_L2C_CFG);
+        if (!l2c_cfg.s.dpres0 && (ddr_controller == 0))
+            return;
+        if (!l2c_cfg.s.dpres1 && (ddr_controller == 1))
+            return;
+    }
+
+    csr.u64 = cvmx_read_csr(CVMX_LMCX_MEM_CFG0(ddr_controller));
+    csr.s.intr_ded_ena = 1;
+    csr.s.intr_sec_ena = __cvmx_interrupt_ecc_report_single_bit_errors;
+    cvmx_write_csr(CVMX_LMCX_MEM_CFG0(ddr_controller), csr.u64);
+}
+
+
+/**
+ * Decode LMC (DDR controller) interrupts for all chips
+ *
+ * @param ddr_controller
+ *               Which controller to decode 0-1
+ */
+static void __cvmx_interrupt_lmcx_decode(int ddr_controller)
+{
+    /* These static counters are used to track ECC error counts */
+    static CVMX_SHARED unsigned long single_bit_errors[2] = {0, 0};
+    static CVMX_SHARED unsigned long double_bit_errors[2] = {0, 0};
+    cvmx_lmcx_mem_cfg0_t mem_cfg0;
+    cvmx_lmc_fadr_t fadr;
+
+    mem_cfg0.u64 =cvmx_read_csr(CVMX_LMCX_MEM_CFG0(ddr_controller));
+    fadr.u64 = cvmx_read_csr(CVMX_LMCX_FADR (ddr_controller));
+    cvmx_write_csr(CVMX_LMCX_MEM_CFG0(ddr_controller),mem_cfg0.u64);
+    if (mem_cfg0.s.sec_err || mem_cfg0.s.ded_err)
+    {
+        int pop_count;
+        CVMX_DPOP(pop_count, mem_cfg0.s.sec_err);
+        single_bit_errors[ddr_controller] += pop_count;
+        CVMX_DPOP(pop_count, mem_cfg0.s.ded_err);
+        double_bit_errors[ddr_controller] += pop_count;
+        if (mem_cfg0.s.ded_err || (mem_cfg0.s.sec_err && __cvmx_interrupt_ecc_report_single_bit_errors))
+        {
+            PRINT_ERROR("DDR%d ECC: %lu Single bit corrections, %lu Double bit errors\n"
+                         "DDR%d ECC:\tFailing dimm:   %u\n"
+                         "DDR%d ECC:\tFailing rank:   %u\n"
+                         "DDR%d ECC:\tFailing bank:   %u\n"
+                         "DDR%d ECC:\tFailing row:    0x%x\n"
+                         "DDR%d ECC:\tFailing column: 0x%x\n",
+                         ddr_controller, single_bit_errors[ddr_controller], double_bit_errors[ddr_controller],
+                         ddr_controller, fadr.s.fdimm,
+                         ddr_controller, fadr.s.fbunk,
+                         ddr_controller, fadr.s.fbank,
+                         ddr_controller, fadr.s.frow,
+                         ddr_controller, fadr.s.fcol);
+        }
+    }
+}
+
+
+/**
+ * Decode GMX error interrupts
+ *
+ * @param block  GMX interface to decode
+ */
+static void __cvmx_interrupt_gmxx_decode(int block)
+{
+    int index;
+    cvmx_gmxx_tx_int_reg_t csr;
+
+    csr.u64 = cvmx_read_csr(CVMX_GMXX_TX_INT_REG(block)) & cvmx_read_csr(CVMX_GMXX_TX_INT_EN(block));
+    cvmx_write_csr(CVMX_GMXX_TX_INT_REG(block), csr.u64);
+
+    for (index=0; index<4; index++)
+    {
+        if (csr.s.late_col & (1<<index))
+            PRINT_ERROR("GMX%d_TX%d_INT_REG[LATE_COL]: TX Late Collision\n", block, index);
+        if (csr.s.xsdef & (1<<index))
+            PRINT_ERROR("GMX%d_TX%d_INT_REG[XSDEF]: TX Excessive deferral\n", block, index);
+        if (csr.s.xscol & (1<<index))
+            PRINT_ERROR("GMX%d_TX%d_INT_REG[XSCOL]: TX Excessive collisions\n", block, index);
+        if (csr.s.undflw & (1<<index))
+            PRINT_ERROR("GMX%d_TX%d_INT_REG[UNDFLW]: TX Underflow\n", block, index);
+    }
+    if (csr.s.ncb_nxa)
+        PRINT_ERROR("GMX%d_TX_INT_REG[NCB_NXA]: Port address out-of-range from NCB Interface\n", block);
+    if (csr.s.pko_nxa)
+        PRINT_ERROR("GMX%d_TX_INT_REG[PKO_NXA]: Port address out-of-range from PKO Interface\n", block);
+
+    __cvmx_interrupt_gmxx_rxx_int_reg_decode(0, block);
+    __cvmx_interrupt_gmxx_rxx_int_reg_decode(1, block);
+    __cvmx_interrupt_gmxx_rxx_int_reg_decode(2, block);
+    if (!(OCTEON_IS_MODEL(OCTEON_CN31XX) || OCTEON_IS_MODEL(OCTEON_CN30XX) || OCTEON_IS_MODEL(OCTEON_CN50XX)))
+        __cvmx_interrupt_gmxx_rxx_int_reg_decode(3, block);
+}
+
+
+/**
+ * Enable POW error interrupts for all chips
+ */
+static void __cvmx_interrupt_pow_enable(void)
+{
+    cvmx_pow_ecc_err_t csr;
+    csr.u64 = cvmx_read_csr(CVMX_POW_ECC_ERR);
+    if (!OCTEON_IS_MODEL(OCTEON_CN38XX_PASS2) && !OCTEON_IS_MODEL(OCTEON_CN31XX))
+    {
+            /* These doesn't exist for chips CN31XX and CN38XXp2 */
+            csr.s.iop_ie = 0x1fff;
+    }
+    csr.s.rpe_ie = 1;
+    csr.s.dbe_ie = 1;
+    csr.s.sbe_ie = __cvmx_interrupt_ecc_report_single_bit_errors;
+    cvmx_write_csr(CVMX_POW_ECC_ERR, csr.u64);
+}
+
+
+/**
+ * Decode POW error interrupts for all chips
+ */
+static void __cvmx_interrupt_pow_decode(void)
+{
+    cvmx_pow_ecc_err_t err;
+
+    err.u64 = cvmx_read_csr(CVMX_POW_ECC_ERR);
+    cvmx_write_csr(CVMX_POW_ECC_ERR, err.u64);
+    if (err.u64)
+    {
+        if (err.s.sbe && __cvmx_interrupt_ecc_report_single_bit_errors)
+            PRINT_ERROR("POW_ECC_ERR[SBE]: POW single bit error\n");
+        if (err.s.dbe)
+            PRINT_ERROR("POW_ECC_ERR[DBE]: POW double bit error\n");
+        if (err.s.dbe || (err.s.sbe && __cvmx_interrupt_ecc_report_single_bit_errors))
+            PRINT_ERROR("POW_ECC_ERR[SYN]: Failing syndrome %u\n", err.s.syn);
+        if (err.s.rpe)
+            PRINT_ERROR("POW_ECC_ERR[RPE]: Remote pointer error\n");
+        if (err.s.iop & (1 << 0))
+            PRINT_ERROR("POW_ECC_ERR[IOP0]: Received SWTAG/SWTAG_FULL/SWTAG_DESCH/DESCH/UPD_WQP from PP in NULL_NULL state\n");
+        if (err.s.iop & (1 << 1))
+            PRINT_ERROR("POW_ECC_ERR[IOP1]: Received SWTAG/SWTAG_DESCH/DESCH/UPD_WQP from PP in NULL state\n");
+        if (err.s.iop & (1 << 2))
+            PRINT_ERROR("POW_ECC_ERR[IOP2]: Received SWTAG/SWTAG_FULL/SWTAG_DESCH/GET_WORK from PP with pending tag switch to ORDERED or ATOMIC\n");
+        if (err.s.iop & (1 << 3))
+            PRINT_ERROR("POW_ECC_ERR[IOP3]: Received SWTAG/SWTAG_FULL/SWTAG_DESCH from PP with tag specified as NULL_NULL\n");
+        if (err.s.iop & (1 << 4))
+            PRINT_ERROR("POW_ECC_ERR[IOP4]: Received SWTAG_FULL/SWTAG_DESCH from PP with tag specified as NULL\n");
+        if (err.s.iop & (1 << 5))
+            PRINT_ERROR("POW_ECC_ERR[IOP5]: Received SWTAG/SWTAG_FULL/SWTAG_DESCH/DESCH/UPD_WQP/GET_WORK/NULL_RD from PP with GET_WORK pending\n");
+        if (err.s.iop & (1 << 6))
+            PRINT_ERROR("POW_ECC_ERR[IOP6]: Received SWTAG/SWTAG_FULL/SWTAG_DESCH/DESCH/UPD_WQP/GET_WORK/NULL_RD from PP with NULL_RD pending\n");
+        if (err.s.iop & (1 << 7))
+            PRINT_ERROR("POW_ECC_ERR[IOP7]: Received CLR_NSCHED from PP with SWTAG_DESCH/DESCH/CLR_NSCHED pending\n");
+        if (err.s.iop & (1 << 8))
+            PRINT_ERROR("POW_ECC_ERR[IOP8]: Received SWTAG/SWTAG_FULL/SWTAG_DESCH/DESCH/UPD_WQP/GET_WORK/NULL_RD from PP with CLR_NSCHED pending\n");
+        if (err.s.iop & (1 << 9))
+            PRINT_ERROR("POW_ECC_ERR[IOP9]: Received illegal opcode\n");
+        if (err.s.iop & (1 << 10))
+            PRINT_ERROR("POW_ECC_ERR[IOP10]: Received ADD_WORK with tag specified as NULL_NULL\n");
+        if (err.s.iop & (1 << 11))
+            PRINT_ERROR("POW_ECC_ERR[IOP11]: Received DBG load from PP with DBG load pending\n");
+        if (err.s.iop & (1 << 12))
+            PRINT_ERROR("POW_ECC_ERR[IOP12]: Received CSR load from PP with CSR load pending\n");
+    }
+}
+
+
+/**
+ * Enable TIM tiemr wheel interrupts for all chips
+ */
+static void __cvmx_interrupt_tim_enable(void)
+{
+    cvmx_tim_reg_int_mask_t csr;
+    csr.u64 = cvmx_read_csr(CVMX_TIM_REG_INT_MASK);
+    csr.s.mask = 0xffff;
+    cvmx_write_csr(CVMX_TIM_REG_INT_MASK, csr.u64);
+}
+
+
+/**
+ * Decode TIM timer wheel interrupts
+ */
+static void __cvmx_interrupt_tim_decode(void)
+{
+    cvmx_tim_reg_error_t err;
+
+    err.u64 = cvmx_read_csr(CVMX_TIM_REG_ERROR);
+    cvmx_write_csr(CVMX_TIM_REG_ERROR, err.u64);
+    if (err.u64)
+    {
+        int i;
+        for (i = 0; i < 16; i++)
+            if (err.s.mask & (1 << i))
+                PRINT_ERROR("TIM_REG_ERROR[MASK]: Timer wheel %d error\n", i);
+    }
+}
+
+
+/**
+ * Utility function to decode Octeon's RSL_INT_BLOCKS interrupts
+ * into error messages.
+ */
+void cvmx_interrupt_rsl_decode(void)
+{
+    uint64_t rsl_int_blocks;
+
+    /* Reading the RSL interrupts is different between PCI and PCIe chips */
+    if (octeon_has_feature(OCTEON_FEATURE_PCIE))
+        rsl_int_blocks = cvmx_read_csr(CVMX_PEXP_NPEI_RSL_INT_BLOCKS);
+    else
+        rsl_int_blocks = cvmx_read_csr(CVMX_NPI_RSL_INT_BLOCKS);
+
+    /* Not all chips support all error interrupts. This code assumes
+        that unsupported interrupts always are zero */
+
+    /* Bits 63-31 are unused on all chips */
+    if (rsl_int_blocks & (1ull<<30)) __cvmx_interrupt_iob_int_sum_decode();
+    if (rsl_int_blocks & (1ull<<29)) __cvmx_interrupt_lmcx_decode(1);
+    if (rsl_int_blocks & (1ull<<28))
+    {
+        __cvmx_interrupt_agl_gmx_rxx_int_reg_decode(0);
+        if (OCTEON_IS_MODEL(OCTEON_CN52XX))
+            __cvmx_interrupt_agl_gmx_rxx_int_reg_decode(1);
+    }
+    /* Bit 27-24 are unused on all chips */
+    if (rsl_int_blocks & (1ull<<23))
+    {
+        if (octeon_has_feature(OCTEON_FEATURE_PCIE))
+        {
+            __cvmx_interrupt_pcsx_intx_reg_decode(0, 1);
+            __cvmx_interrupt_pcsx_intx_reg_decode(1, 1);
+            __cvmx_interrupt_pcsx_intx_reg_decode(2, 1);
+            __cvmx_interrupt_pcsx_intx_reg_decode(3, 1);
+        }
+        else
+            __cvmx_interrupt_asxx_decode(1);
+    }
+    if (rsl_int_blocks & (1ull<<22))
+    {
+        if (octeon_has_feature(OCTEON_FEATURE_PCIE))
+        {
+            __cvmx_interrupt_pcsx_intx_reg_decode(0, 0);
+            __cvmx_interrupt_pcsx_intx_reg_decode(1, 0);
+            __cvmx_interrupt_pcsx_intx_reg_decode(2, 0);
+            __cvmx_interrupt_pcsx_intx_reg_decode(3, 0);
+        }
+        else
+            __cvmx_interrupt_asxx_decode(0);
+    }
+    /* Bit 21 is unsed on all chips */
+    if (rsl_int_blocks & (1ull<<20)) __cvmx_interrupt_pip_int_reg_decode();
+    if (rsl_int_blocks & (1ull<<19)) __cvmx_interrupt_spxx_int_reg_decode(1);
+    if (rsl_int_blocks & (1ull<<18)) __cvmx_interrupt_spxx_int_reg_decode(0);
+    if (rsl_int_blocks & (1ull<<17)) __cvmx_interrupt_lmcx_decode(0);
+    if (rsl_int_blocks & (1ull<<16)) __cvmx_interrupt_l2_decode();
+    if (rsl_int_blocks & (1ull<<15)) __cvmx_interrupt_usbnx_int_sum_decode(1);
+    if (rsl_int_blocks & (1ull<<14)) __cvmx_interrupt_rad_reg_error_decode();
+    if (rsl_int_blocks & (1ull<<13)) __cvmx_interrupt_usbnx_int_sum_decode(0);
+    if (rsl_int_blocks & (1ull<<12)) __cvmx_interrupt_pow_decode();
+    if (rsl_int_blocks & (1ull<<11)) __cvmx_interrupt_tim_decode();
+    if (rsl_int_blocks & (1ull<<10)) __cvmx_interrupt_pko_reg_error_decode();
+    if (rsl_int_blocks & (1ull<< 9)) __cvmx_interrupt_ipd_int_sum_decode();
+    /* Bit 8 is unused on all chips */
+    if (rsl_int_blocks & (1ull<< 7)) __cvmx_interrupt_zip_error_decode();
+    if (rsl_int_blocks & (1ull<< 6)) __cvmx_interrupt_dfa_decode();
+    if (rsl_int_blocks & (1ull<< 5)) __cvmx_interrupt_fpa_int_sum_decode();
+    if (rsl_int_blocks & (1ull<< 4)) __cvmx_interrupt_key_int_sum_decode();
+    if (rsl_int_blocks & (1ull<< 3))
+    {
+        if (octeon_has_feature(OCTEON_FEATURE_PCIE))
+            __cvmx_interrupt_npei_int_sum_decode();
+        else
+            __cvmx_interrupt_npi_int_sum_decode();
+    }
+    if (rsl_int_blocks & (1ull<< 2)) __cvmx_interrupt_gmxx_decode(1);
+    if (rsl_int_blocks & (1ull<< 1)) __cvmx_interrupt_gmxx_decode(0);
+    if (rsl_int_blocks & (1ull<< 0)) __cvmx_interrupt_mio_boot_err_decode();
+}
+
+
+/**
+ * Utility function to enable all RSL error interupts
+ */
+void cvmx_interrupt_rsl_enable(void)
+{
+    /* Bits 63-31 are unused on all chips */
+    __cvmx_interrupt_iob_int_enb_enable();
+    if (OCTEON_IS_MODEL(OCTEON_CN56XX))
+        __cvmx_interrupt_lmcx_enable(1);
+    if (octeon_has_feature(OCTEON_FEATURE_MGMT_PORT))
+    {
+        // FIXME __cvmx_interrupt_agl_gmx_rxx_int_en_enable(0);
+        //if (OCTEON_IS_MODEL(OCTEON_CN52XX))
+        //    __cvmx_interrupt_agl_gmx_rxx_int_en_enable(1);
+    }
+    /* Bit 27-24 are unused on all chips */
+    if (OCTEON_IS_MODEL(OCTEON_CN38XX) || OCTEON_IS_MODEL(OCTEON_CN58XX))
+        __cvmx_interrupt_asxx_enable(1);
+    if (OCTEON_IS_MODEL(OCTEON_CN56XX))
+    {
+        __cvmx_interrupt_pcsx_intx_en_reg_enable(0, 1);
+        __cvmx_interrupt_pcsx_intx_en_reg_enable(1, 1);
+        __cvmx_interrupt_pcsx_intx_en_reg_enable(2, 1);
+        __cvmx_interrupt_pcsx_intx_en_reg_enable(3, 1);
+    }
+    if (octeon_has_feature(OCTEON_FEATURE_PCIE))
+    {
+        __cvmx_interrupt_pcsx_intx_en_reg_enable(0, 0);
+        __cvmx_interrupt_pcsx_intx_en_reg_enable(1, 0);
+        __cvmx_interrupt_pcsx_intx_en_reg_enable(2, 0);
+        __cvmx_interrupt_pcsx_intx_en_reg_enable(3, 0);
+    }
+    else
+        __cvmx_interrupt_asxx_enable(0);
+    /* Bit 21 is unsed on all chips */
+    __cvmx_interrupt_pip_int_en_enable();
+    if (OCTEON_IS_MODEL(OCTEON_CN38XX) || OCTEON_IS_MODEL(OCTEON_CN58XX))
+    {
+        __cvmx_interrupt_spxx_int_msk_enable(1);
+        __cvmx_interrupt_spxx_int_msk_enable(0);
+    }
+    __cvmx_interrupt_lmcx_enable(0);
+    __cvmx_interrupt_l2_enable();
+    if (OCTEON_IS_MODEL(OCTEON_CN52XX))
+        __cvmx_interrupt_usbnx_int_enb_enable(1);
+    if (octeon_has_feature(OCTEON_FEATURE_RAID))
+        __cvmx_interrupt_rad_reg_int_mask_enable();
+    if (octeon_has_feature(OCTEON_FEATURE_USB))
+        __cvmx_interrupt_usbnx_int_enb_enable(0);
+    __cvmx_interrupt_pow_enable();
+    __cvmx_interrupt_tim_enable();
+    __cvmx_interrupt_pko_reg_int_mask_enable();
+    __cvmx_interrupt_ipd_int_enb_enable();
+    /* Bit 8 is unused on all chips */
+    if (octeon_has_feature(OCTEON_FEATURE_ZIP))
+        __cvmx_interrupt_zip_int_mask_enable();
+    if (OCTEON_IS_MODEL(OCTEON_CN38XX) || OCTEON_IS_MODEL(OCTEON_CN58XX))
+        __cvmx_interrupt_dfa_enable();
+    __cvmx_interrupt_fpa_int_enb_enable();
+    if (octeon_has_feature(OCTEON_FEATURE_KEY_MEMORY))
+        __cvmx_interrupt_key_int_enb_enable();
+    if (octeon_has_feature(OCTEON_FEATURE_PCIE))
+    {
+        cvmx_ciu_soft_prst_t ciu_soft_prst;
+        ciu_soft_prst.u64 = cvmx_read_csr(CVMX_CIU_SOFT_PRST);
+        if (ciu_soft_prst.s.soft_prst == 0)
+            __cvmx_interrupt_npei_int_enb2_enable();
+    }
+    else if (cvmx_sysinfo_get()->bootloader_config_flags & CVMX_BOOTINFO_CFG_FLAG_PCI_HOST)
+        __cvmx_interrupt_npi_int_enb_enable();
+    if (OCTEON_IS_MODEL(OCTEON_CN38XX) || OCTEON_IS_MODEL(OCTEON_CN58XX))
+    {
+        cvmx_gmxx_tx_int_en_t gmx_tx_int_en;
+        gmx_tx_int_en.u64 = 0;
+        gmx_tx_int_en.s.ncb_nxa = 1;
+        gmx_tx_int_en.s.pko_nxa = 1;
+        gmx_tx_int_en.s.undflw = 0xf;
+        cvmx_write_csr(CVMX_GMXX_TX_INT_EN(1), gmx_tx_int_en.u64);
+        __cvmx_interrupt_gmxx_rxx_int_en_enable(0, 1);
+        __cvmx_interrupt_gmxx_rxx_int_en_enable(1, 1);
+        __cvmx_interrupt_gmxx_rxx_int_en_enable(2, 1);
+        __cvmx_interrupt_gmxx_rxx_int_en_enable(3, 1);
+    }
+    {
+        cvmx_gmxx_tx_int_en_t gmx_tx_int_en;
+        gmx_tx_int_en.u64 = 0;
+	if (OCTEON_IS_MODEL(OCTEON_CN38XX) || OCTEON_IS_MODEL(OCTEON_CN58XX))
+            gmx_tx_int_en.s.ncb_nxa = 1;
+        gmx_tx_int_en.s.pko_nxa = 1;
+        if (!(OCTEON_IS_MODEL(OCTEON_CN31XX) || OCTEON_IS_MODEL(OCTEON_CN30XX) || OCTEON_IS_MODEL(OCTEON_CN50XX)))
+            gmx_tx_int_en.s.undflw = 0xf;
+        else
+            gmx_tx_int_en.s.undflw = 0x3;
+        cvmx_write_csr(CVMX_GMXX_TX_INT_EN(0), gmx_tx_int_en.u64);
+    }
+    __cvmx_interrupt_gmxx_rxx_int_en_enable(0, 0);
+    __cvmx_interrupt_gmxx_rxx_int_en_enable(1, 0);
+    if (!(OCTEON_IS_MODEL(OCTEON_CN31XX) || OCTEON_IS_MODEL(OCTEON_CN30XX) || OCTEON_IS_MODEL(OCTEON_CN50XX)))
+    {
+        __cvmx_interrupt_gmxx_rxx_int_en_enable(2, 0);
+        __cvmx_interrupt_gmxx_rxx_int_en_enable(3, 0);
+    }
+    __cvmx_interrupt_mio_boot_int_enable();
+}
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-interrupt.c b/arch/mips/cavium-octeon/executive/cvmx-interrupt.c
new file mode 100644
index 0000000..0b49c3d
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-interrupt.c
@@ -0,0 +1,519 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Interface to the Mips interrupts.
+ *
+ * <hr>$Revision: 34048 $<hr>
+ */
+#if __GNUC__ >= 4
+/* Backtrace is only available with the new toolchain.  */
+#include <execinfo.h>
+#endif
+#include "cvmx-config.h"
+#include "cvmx.h"
+#include "cvmx-interrupt.h"
+#include "cvmx-sysinfo.h"
+#include "cvmx-uart.h"
+#include "cvmx-pow.h"
+#include "cvmx-ebt3000.h"
+#include "cvmx-coremask.h"
+#include "cvmx-spinlock.h"
+
+EXTERN_ASM void cvmx_interrupt_stage1(void);
+EXTERN_ASM void cvmx_interrupt_cache_error(void);
+
+/**
+ * Internal status the interrupt registration
+ */
+typedef struct
+{
+    cvmx_interrupt_func_t handlers[256];  /**< One function to call per interrupt */
+    void *                data[256];      /**< User data per interrupt */
+    cvmx_interrupt_exception_t exception_handler;
+} cvmx_interrupt_state_t;
+
+/**
+ * Internal state the interrupt registration
+ */
+static CVMX_SHARED cvmx_interrupt_state_t cvmx_interrupt_state;
+static CVMX_SHARED cvmx_spinlock_t cvmx_interrupt_default_lock;
+
+#define COP0_CAUSE      "$13,0"
+#define COP0_STATUS     "$12,0"
+#define COP0_BADVADDR   "$8,0"
+#define COP0_EPC        "$14,0"
+#define READ_COP0(dest, R) asm volatile ("dmfc0 %[rt]," R : [rt] "=r" (dest))
+
+
+/**
+ * @INTERNAL
+ * version of printf that works better in exception context.
+ *
+ * @param format
+ */
+static void safe_printf(const char *format, ...)
+{
+    static char buffer[256];
+    va_list args;
+    va_start(args, format);
+    int count = vsnprintf(buffer, sizeof(buffer), format, args);
+    va_end(args);
+
+    char *ptr = buffer;
+    while (count-- > 0)
+    {
+        cvmx_uart_lsr_t lsrval;
+
+        /* Spin until there is room */
+        do
+        {
+            lsrval.u64 = cvmx_read_csr(CVMX_MIO_UARTX_LSR(0));
+            if (lsrval.s.temt == 0)
+                cvmx_wait(10000);   /* Just to reduce the load on the system */
+        }
+        while (lsrval.s.temt == 0);
+
+        if (*ptr == '\n')
+            cvmx_write_csr(CVMX_MIO_UARTX_THR(0), '\r');
+        cvmx_write_csr(CVMX_MIO_UARTX_THR(0), *ptr++);
+    }
+}
+
+
+/**
+ * @INTERNAL
+ * Dump all useful registers to the console
+ *
+ * @param registers CPU register to dump
+ */
+static void __cvmx_interrupt_dump_registers(uint64_t registers[32])
+{
+    static const char *name[32] = {"r0","at","v0","v1","a0","a1","a2","a3",
+        "t0","t1","t2","t3","t4","t5","t6","t7","s0","s1","s2","s3","s4","s5",
+        "s6","s7", "t8","t9", "k0","k1","gp","sp","s8","ra"};
+    uint64_t reg;
+    for (reg=0; reg<16; reg++)
+    {
+        safe_printf("%3s ($%02ld): 0x%016lx \t %3s ($%02ld): 0x%016lx\n",
+               name[reg], reg, registers[reg], name[reg+16], reg+16, registers[reg+16]);
+    }
+    READ_COP0(reg, COP0_CAUSE);
+    safe_printf("%16s: 0x%016lx\n", "COP0_CAUSE", reg);
+    READ_COP0(reg, COP0_STATUS);
+    safe_printf("%16s: 0x%016lx\n", "COP0_STATUS", reg);
+    READ_COP0(reg, COP0_BADVADDR);
+    safe_printf("%16s: 0x%016lx\n", "COP0_BADVADDR", reg);
+    READ_COP0(reg, COP0_EPC);
+    safe_printf("%16s: 0x%016lx\n", "COP0_EPC", reg);
+}
+
+
+/**
+ * @INTERNAL
+ * Default exception handler. Prints out the exception
+ * cause decode and all relevant registers.
+ *
+ * @param registers Registers at time of the exception
+ */
+static void __cvmx_interrupt_default_exception_handler(uint64_t registers[32])
+{
+    uint64_t trap_print_cause;
+
+    ebt3000_str_write("Trap");
+    cvmx_spinlock_lock(&cvmx_interrupt_default_lock);
+    safe_printf("******************************************************************\n");
+    safe_printf("Core %lu: Unhandled Exception. Cause register decodes to:\n", cvmx_get_core_num());
+    READ_COP0(trap_print_cause, COP0_CAUSE);
+    switch ((trap_print_cause >> 2) & 0x1f)
+    {
+        case 0x0:
+            safe_printf("Interrupt\n");
+            break;
+        case 0x1:
+            safe_printf("TLB Mod\n");
+            break;
+        case 0x2:
+            safe_printf("tlb load/fetch\n");
+            break;
+        case 0x3:
+            safe_printf("tlb store\n");
+            break;
+        case 0x4:
+            safe_printf("address exc, load/fetch\n");
+            break;
+        case 0x5:
+            safe_printf("address exc, store\n");
+            break;
+        case 0x6:
+            safe_printf("bus error, inst. fetch\n");
+            break;
+        case 0x7:
+            safe_printf("bus error, load/store\n");
+            break;
+        case 0x8:
+            safe_printf("syscall\n");
+            break;
+        case 0x9:
+            safe_printf("breakpoint \n");
+            break;
+        case 0xa:
+            safe_printf("reserved instruction\n");
+            break;
+        case 0xb:
+            safe_printf("cop unusable\n");
+            break;
+        case 0xc:
+            safe_printf("arithmetic overflow\n");
+            break;
+        case 0xd:
+            safe_printf("trap\n");
+            break;
+        case 0xf:
+            safe_printf("floating point exc\n");
+            break;
+        case 0x12:
+            safe_printf("cop2 exception\n");
+            break;
+        case 0x16:
+            safe_printf("mdmx unusable\n");
+            break;
+        case 0x17:
+            safe_printf("watch\n");
+            break;
+        case 0x18:
+            safe_printf("machine check\n");
+            break;
+        case 0x1e:
+            safe_printf("cache error\n");
+            break;
+        default:
+            safe_printf("Reserved exception cause.\n");
+            break;
+
+    }
+
+    safe_printf("******************************************************************\n");
+    __cvmx_interrupt_dump_registers(registers);
+    safe_printf("******************************************************************\n");
+
+#if __GNUC__ >= 4 && !defined(OCTEON_DISABLE_BACKTRACE)
+    safe_printf("Backtrace:\n\n");
+    __octeon_print_backtrace_func ((__octeon_backtrace_printf_t)safe_printf);
+    safe_printf("******************************************************************\n");
+#endif
+
+    cvmx_spinlock_unlock(&cvmx_interrupt_default_lock);
+
+    while (1)
+    {
+ 	if (cvmx_sysinfo_get()->board_type == CVMX_BOARD_TYPE_SIM)
+	    CVMX_BREAK;
+        else
+            asm volatile ("wait");
+    }
+}
+
+
+/**
+ * @INTERNAL
+ * Default interrupt handler if the user doesn't register one.
+ *
+ * @param irq_number IRQ that caused this interrupt
+ * @param registers  Register at the time of the interrupt
+ * @param user_arg   Unused optional user data
+ */
+static void __cvmx_interrupt_default(int irq_number, uint64_t registers[32], void *user_arg)
+{
+    safe_printf("cvmx_interrupt_default: Received interrupt %d\n", irq_number);
+    __cvmx_interrupt_dump_registers(registers);
+}
+
+
+/**
+ * @INTERNAL
+ * Handler for interrupt lines 2 and 3. These are directly tied
+ * to the CIU. The handler queres the status of the CIU and
+ * calls the secondary handler for the CIU interrupt that
+ * occurred.
+ *
+ * @param irq_number Interrupt number that fired (2 or 3)
+ * @param registers  Registers at the time of the interrupt
+ * @param user_arg   Unused user argument
+ */
+static void __cvmx_interrupt_ciu(int irq_number, uint64_t registers[32], void *user_arg)
+{
+    int ciu_offset = cvmx_get_core_num() * 2 + irq_number - 2;
+    uint64_t irq_mask = cvmx_read_csr(CVMX_CIU_INTX_SUM0(ciu_offset)) & cvmx_read_csr(CVMX_CIU_INTX_EN0(ciu_offset));
+    int irq = 8;
+
+    /* Handle EN0 sources */
+    while (irq_mask)
+    {
+        if (irq_mask&1)
+        {
+            cvmx_interrupt_state.handlers[irq](irq, registers, cvmx_interrupt_state.data[irq]);
+            return;
+        }
+        irq_mask = irq_mask >> 1;
+        irq++;
+    }
+
+    /* Handle EN1 sources */
+    irq_mask = cvmx_read_csr(CVMX_CIU_INT_SUM1) & cvmx_read_csr(CVMX_CIU_INTX_EN1(ciu_offset));
+    irq = 8 + 64;
+    while (irq_mask)
+    {
+        if (irq_mask&1)
+        {
+            cvmx_interrupt_state.handlers[irq](irq, registers, cvmx_interrupt_state.data[irq]);
+            return;
+        }
+        irq_mask = irq_mask >> 1;
+        irq++;
+    }
+}
+
+
+/**
+ * @INTERNAL
+ * Called for all RML interrupts. This is usually an ECC error
+ *
+ * @param irq_number Interrupt number that we're being called for
+ * @param registers  Registers at the time of the interrupt
+ * @param user_arg   Unused user argument
+ */
+static void __cvmx_interrupt_ecc(int irq_number, uint64_t registers[32], void *user_arg)
+{
+    cvmx_interrupt_rsl_decode();
+}
+
+
+/**
+ * Process an interrupt request
+ *
+ * @param registers Registers at time of interrupt / exception
+ * Registers 0-31 are standard MIPS, others specific to this routine
+ * @return
+ */
+EXTERN_ASM void cvmx_interrupt_do_irq(uint64_t registers[35]);
+void cvmx_interrupt_do_irq(uint64_t registers[35])
+{
+    uint64_t        mask;
+    uint64_t        cause;
+    uint64_t        status;
+    uint64_t        cache_err;
+    int             i;
+    uint32_t exc_vec;
+
+    /* Determine the cause of the interrupt */
+    asm volatile ("dmfc0 %0,$13,0" : "=r" (cause));
+    asm volatile ("dmfc0 %0,$12,0" : "=r" (status));
+
+    /* The assembly stub at each exception vector saves its address in k1 when
+    ** it calls the stage 2 handler.  We use this to compute the exception vector
+    ** that brought us here */
+    exc_vec = (uint32_t)(registers[27] & 0x780);  /* Mask off bits we need to ignore */
+
+    /* Check for cache errors.  The cache errors go to a separate exception vector,
+    ** so we will only check these if we got here from a cache error exception, and
+    ** the ERL (error level) bit is set. */
+    if (exc_vec == 0x100 && (status & 0x4))
+    {
+        i = cvmx_get_core_num();
+        CVMX_MF_CACHE_ERR(cache_err);
+
+        /* Use copy of DCACHE_ERR register that early exception stub read */
+        if (registers[34] & 0x1)
+        {
+            safe_printf("Dcache error detected: core: %d, set: %d, va 6:3: 0x%x\n", i, (cache_err >> 3) & 0x3, (cache_err >> 3) & 0xf);
+            uint64_t dcache_err = 0;
+            CVMX_MT_DCACHE_ERR(dcache_err);
+        }
+        else if (cache_err & 0x1)
+        {
+            safe_printf("Icache error detected: core: %d, set: %d, way : %d\n", i, (cache_err >> 5) & 0x3f, (cache_err >> 7) & 0x3);
+            cache_err = 0;
+            CVMX_MT_CACHE_ERR(cache_err);
+        }
+        else
+            safe_printf("Cache error exception: core %d\n", i);
+    }
+
+    if ((cause & 0x7c) != 0)
+    {
+        cvmx_interrupt_state.exception_handler(registers);
+        return;
+    }
+
+    /* Convert the cause into an active mask */
+    mask = ((cause & status) >> 8) & 0xff;
+    if (mask == 0)
+        return; /* Spurious interrupt */
+
+    for (i=0; i<8; i++)
+    {
+        if (mask & (1<<i))
+        {
+            cvmx_interrupt_state.handlers[i](i, registers, cvmx_interrupt_state.data[i]);
+            return;
+        }
+    }
+
+    /* We should never get here */
+    __cvmx_interrupt_default_exception_handler(registers);
+}
+
+
+/**
+ * Initialize the interrupt routine and copy the low level
+ * stub into the correct interrupt vector. This is called
+ * automatically during application startup.
+ */
+void cvmx_interrupt_initialize(void)
+{
+    void *low_level_loc;
+    cvmx_sysinfo_t *sys_info_ptr = cvmx_sysinfo_get();
+    int i;
+
+    /* Disable all CIU interrupts by default */
+    cvmx_write_csr(CVMX_CIU_INTX_EN0(cvmx_get_core_num()*2), 0);
+    cvmx_write_csr(CVMX_CIU_INTX_EN0(cvmx_get_core_num()*2+1), 0);
+    cvmx_write_csr(CVMX_CIU_INTX_EN1(cvmx_get_core_num()*2), 0);
+    cvmx_write_csr(CVMX_CIU_INTX_EN1(cvmx_get_core_num()*2+1), 0);
+
+    if (cvmx_coremask_first_core(sys_info_ptr->core_mask))
+    {
+        cvmx_interrupt_state.exception_handler = __cvmx_interrupt_default_exception_handler;
+
+        for (i=0; i<256; i++)
+        {
+            cvmx_interrupt_state.handlers[i] = __cvmx_interrupt_default;
+            cvmx_interrupt_state.data[i] = NULL;
+        }
+
+        low_level_loc = CASTPTR(void, CVMX_ADD_SEG32(CVMX_MIPS32_SPACE_KSEG0,sys_info_ptr->exception_base_addr));
+        memcpy(low_level_loc + 0x80, (void*)cvmx_interrupt_stage1, 0x80);
+        memcpy(low_level_loc + 0x100, (void*)cvmx_interrupt_cache_error, 0x80);
+        memcpy(low_level_loc + 0x180, (void*)cvmx_interrupt_stage1, 0x80);
+        memcpy(low_level_loc + 0x200, (void*)cvmx_interrupt_stage1, 0x80);
+        /* Make sure the locations used to count Icache and Dcache exceptions
+            starts out as zero */
+        cvmx_write64_uint64(CVMX_ADD_SEG32(CVMX_MIPS32_SPACE_KSEG0, 8), 0);
+        cvmx_write64_uint64(CVMX_ADD_SEG32(CVMX_MIPS32_SPACE_KSEG0, 16), 0);
+        cvmx_write64_uint64(CVMX_ADD_SEG32(CVMX_MIPS32_SPACE_KSEG0, 24), 0);
+        CVMX_SYNC;
+
+        /* Add an interrupt handlers for chained CIU interrupts */
+        cvmx_interrupt_register(CVMX_IRQ_CIU0, __cvmx_interrupt_ciu, NULL);
+        cvmx_interrupt_register(CVMX_IRQ_CIU1, __cvmx_interrupt_ciu, NULL);
+
+        /* Add an interrupt handler for ECC failures */
+        cvmx_interrupt_register(CVMX_IRQ_RML, __cvmx_interrupt_ecc, NULL);
+
+        cvmx_interrupt_rsl_enable();
+        cvmx_interrupt_unmask_irq(CVMX_IRQ_RML);
+    }
+
+    cvmx_interrupt_unmask_irq(CVMX_IRQ_CIU0);
+    cvmx_interrupt_unmask_irq(CVMX_IRQ_CIU1);
+    CVMX_ICACHE_INVALIDATE;
+
+    /* Enable interrupts for each core (bit0 of COP0 Status) */
+    uint32_t mask;
+    asm volatile (
+        "mfc0   %0,$12,0\n"
+        "ori    %0, %0, 1\n"
+        "mtc0   %0,$12,0\n"
+        : "=r" (mask));
+}
+
+
+/**
+ * Register an interrupt handler for the specified interrupt number.
+ *
+ * @param irq_number Interrupt number to register for (0-135)  See
+ *                   cvmx-interrupt.h for enumeration and description of sources.
+ * @param func       Function to call on interrupt.
+ * @param user_arg   User data to pass to the interrupt handler
+ */
+void cvmx_interrupt_register(cvmx_irq_t irq_number, cvmx_interrupt_func_t func, void *user_arg)
+{
+    cvmx_interrupt_state.handlers[irq_number] = func;
+    cvmx_interrupt_state.data[irq_number] = user_arg;
+    CVMX_SYNCWS;
+}
+
+
+/**
+ * Set the exception handler for all non interrupt sources.
+ *
+ * @param handler New exception handler
+ * @return Old exception handler
+ */
+cvmx_interrupt_exception_t cvmx_interrupt_set_exception(cvmx_interrupt_exception_t handler)
+{
+    cvmx_interrupt_exception_t result = cvmx_interrupt_state.exception_handler;
+    cvmx_interrupt_state.exception_handler = handler;
+    CVMX_SYNCWS;
+    return result;
+}
+
+
+
+
+
+
+
+
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-interrupt.h b/arch/mips/cavium-octeon/executive/cvmx-interrupt.h
new file mode 100644
index 0000000..d998a90
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-interrupt.h
@@ -0,0 +1,304 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Interface to the Mips interrupts.
+ *
+ * <hr>$Revision: 33997 $<hr>
+ */
+#ifndef __CVMX_INTERRUPT_H__
+#define __CVMX_INTERRUPT_H__
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+/**
+ * Enumeration of Interrupt numbers
+ */
+typedef enum
+{
+    /* 0 - 7 represent the 8 MIPS standard interrupt sources */
+    CVMX_IRQ_SW0        = 0,
+    CVMX_IRQ_SW1        = 1,
+    CVMX_IRQ_CIU0       = 2,
+    CVMX_IRQ_CIU1       = 3,
+    CVMX_IRQ_4          = 4,
+    CVMX_IRQ_5          = 5,
+    CVMX_IRQ_6          = 6,
+    CVMX_IRQ_7          = 7,
+
+    /* 8 - 71 represent the sources in CIU_INTX_EN0 */
+    CVMX_IRQ_WORKQ0     = 8,
+    CVMX_IRQ_WORKQ1     = 9,
+    CVMX_IRQ_WORKQ2     = 10,
+    CVMX_IRQ_WORKQ3     = 11,
+    CVMX_IRQ_WORKQ4     = 12,
+    CVMX_IRQ_WORKQ5     = 13,
+    CVMX_IRQ_WORKQ6     = 14,
+    CVMX_IRQ_WORKQ7     = 15,
+    CVMX_IRQ_WORKQ8     = 16,
+    CVMX_IRQ_WORKQ9     = 17,
+    CVMX_IRQ_WORKQ10    = 18,
+    CVMX_IRQ_WORKQ11    = 19,
+    CVMX_IRQ_WORKQ12    = 20,
+    CVMX_IRQ_WORKQ13    = 21,
+    CVMX_IRQ_WORKQ14    = 22,
+    CVMX_IRQ_WORKQ15    = 23,
+    CVMX_IRQ_GPIO0      = 24,
+    CVMX_IRQ_GPIO1      = 25,
+    CVMX_IRQ_GPIO2      = 26,
+    CVMX_IRQ_GPIO3      = 27,
+    CVMX_IRQ_GPIO4      = 28,
+    CVMX_IRQ_GPIO5      = 29,
+    CVMX_IRQ_GPIO6      = 30,
+    CVMX_IRQ_GPIO7      = 31,
+    CVMX_IRQ_GPIO8      = 32,
+    CVMX_IRQ_GPIO9      = 33,
+    CVMX_IRQ_GPIO10     = 34,
+    CVMX_IRQ_GPIO11     = 35,
+    CVMX_IRQ_GPIO12     = 36,
+    CVMX_IRQ_GPIO13     = 37,
+    CVMX_IRQ_GPIO14     = 38,
+    CVMX_IRQ_GPIO15     = 39,
+    CVMX_IRQ_MBOX0      = 40,
+    CVMX_IRQ_MBOX1      = 41,
+    CVMX_IRQ_UART0      = 42,
+    CVMX_IRQ_UART1      = 43,
+    CVMX_IRQ_PCI_INT0   = 44,
+    CVMX_IRQ_PCI_INT1   = 45,
+    CVMX_IRQ_PCI_INT2   = 46,
+    CVMX_IRQ_PCI_INT3   = 47,
+    CVMX_IRQ_PCI_MSI0   = 48,
+    CVMX_IRQ_PCI_MSI1   = 49,
+    CVMX_IRQ_PCI_MSI2   = 50,
+    CVMX_IRQ_PCI_MSI3   = 51,
+    CVMX_IRQ_RESERVED44 = 52,
+    CVMX_IRQ_TWSI       = 53,
+    CVMX_IRQ_RML        = 54,
+    CVMX_IRQ_TRACE      = 55,
+    CVMX_IRQ_GMX_DRP0   = 56,
+    CVMX_IRQ_GMX_DRP1   = 57,
+    CVMX_IRQ_IPD_DRP    = 58,
+    CVMX_IRQ_KEY_ZERO   = 59,
+    CVMX_IRQ_TIMER0     = 60,
+    CVMX_IRQ_TIMER1     = 61,
+    CVMX_IRQ_TIMER2     = 62,
+    CVMX_IRQ_TIMER3     = 63,
+    CVMX_IRQ_USB        = 64,   /* Doesn't apply on CN38XX or CN58XX */
+    CVMX_IRQ_PCM        = 65,
+    CVMX_IRQ_MPI        = 66,
+    CVMX_IRQ_TWSI2      = 67,   /* Added in CN56XX */
+    CVMX_IRQ_POWIQ      = 68,   /* Added in CN56XX */
+    CVMX_IRQ_IPDPPTHR   = 69,   /* Added in CN56XX */
+    CVMX_IRQ_MII        = 70,   /* Added in CN56XX */
+    CVMX_IRQ_BOOTDMA    = 71,   /* Added in CN56XX */
+
+    /* 72 - 135 represent the sources in CIU_INTX_EN1 */
+    CVMX_IRQ_WDOG0 = 72,
+    CVMX_IRQ_WDOG1 = 73,
+    CVMX_IRQ_WDOG2 = 74,
+    CVMX_IRQ_WDOG3 = 75,
+    CVMX_IRQ_WDOG4 = 76,
+    CVMX_IRQ_WDOG5 = 77,
+    CVMX_IRQ_WDOG6 = 78,
+    CVMX_IRQ_WDOG7 = 79,
+    CVMX_IRQ_WDOG8 = 80,
+    CVMX_IRQ_WDOG9 = 81,
+    CVMX_IRQ_WDOG10= 82,
+    CVMX_IRQ_WDOG11= 83,
+    CVMX_IRQ_WDOG12= 84,
+    CVMX_IRQ_WDOG13= 85,
+    CVMX_IRQ_WDOG14= 86,
+    CVMX_IRQ_WDOG15= 87
+    /* numbers 88 - 135 are reserved */
+} cvmx_irq_t;
+
+/**
+ * Function prototype for the exception handler
+ */
+typedef void (*cvmx_interrupt_exception_t)(uint64_t registers[32]);
+
+/**
+ * Function prototype for interrupt handlers
+ */
+typedef void (*cvmx_interrupt_func_t)(int irq_number, uint64_t registers[32], void *user_arg);
+
+/**
+ * Register an interrupt handler for the specified interrupt number.
+ *
+ * @param irq_number Interrupt number to register for (0-135)
+ * @param func       Function to call on interrupt.
+ * @param user_arg   User data to pass to the interrupt handler
+ */
+void cvmx_interrupt_register(cvmx_irq_t irq_number, cvmx_interrupt_func_t func, void *user_arg);
+
+/**
+ * Set the exception handler for all non interrupt sources.
+ *
+ * @param handler New exception handler
+ * @return Old exception handler
+ */
+cvmx_interrupt_exception_t cvmx_interrupt_set_exception(cvmx_interrupt_exception_t handler);
+
+/**
+ * Masks a given interrupt number.
+ * EN0 sources are masked on IP2
+ * EN1 sources are masked on IP3
+ *
+ * @param irq_number interrupt number to mask (0-135)
+ */
+static inline void cvmx_interrupt_mask_irq(int irq_number)
+{
+    if (irq_number<8)
+    {
+        uint32_t mask;
+        asm volatile ("mfc0 %0,$12,0" : "=r" (mask));
+        mask &= ~(1<< (8 + irq_number));
+        asm volatile ("mtc0 %0,$12,0" : : "r" (mask));
+    }
+    else if (irq_number < 8 + 64)
+    {
+        int ciu_bit = (irq_number - 8) & 63;
+        int ciu_offset = cvmx_get_core_num() * 2;
+        uint64_t mask = cvmx_read_csr(CVMX_CIU_INTX_EN0(ciu_offset));
+        mask &= ~(1ull << ciu_bit);
+        cvmx_write_csr(CVMX_CIU_INTX_EN0(ciu_offset), mask);
+    }
+    else
+    {
+        int ciu_bit = (irq_number - 8) & 63;
+        int ciu_offset = cvmx_get_core_num() * 2 + 1;
+        uint64_t mask = cvmx_read_csr(CVMX_CIU_INTX_EN1(ciu_offset));
+        mask &= ~(1ull << ciu_bit);
+        cvmx_write_csr(CVMX_CIU_INTX_EN1(ciu_offset), mask);
+    }
+}
+
+
+/**
+ * Unmasks a given interrupt number
+ * EN0 sources are unmasked on IP2
+ * EN1 sources are unmasked on IP3
+ *
+ * @param irq_number interrupt number to unmask (0-135)
+ */
+static inline void cvmx_interrupt_unmask_irq(int irq_number)
+{
+    if (irq_number<8)
+    {
+        uint32_t mask;
+        asm volatile ("mfc0 %0,$12,0" : "=r" (mask));
+        mask |= (1<< (8 + irq_number));
+        asm volatile ("mtc0 %0,$12,0" : : "r" (mask));
+    }
+    else if (irq_number < 8 + 64)
+    {
+        int ciu_bit = (irq_number - 8) & 63;
+        int ciu_offset = cvmx_get_core_num() * 2;
+        uint64_t mask = cvmx_read_csr(CVMX_CIU_INTX_EN0(ciu_offset));
+        mask |= (1ull << ciu_bit);
+        cvmx_write_csr(CVMX_CIU_INTX_EN0(ciu_offset), mask);
+    }
+    else
+    {
+        int ciu_bit = (irq_number - 8) & 63;
+        int ciu_offset = cvmx_get_core_num() * 2 + 1;
+        uint64_t mask = cvmx_read_csr(CVMX_CIU_INTX_EN1(ciu_offset));
+        mask |= (1ull << ciu_bit);
+        cvmx_write_csr(CVMX_CIU_INTX_EN1(ciu_offset), mask);
+    }
+}
+
+
+/* Disable interrupts by clearing bit 0 of the COP0 status register,
+** and return the previous contents of the status register.
+** Note: this is only used to track interrupt status. */
+static inline uint32_t cvmx_interrupt_disable_save(void)
+{
+    uint32_t flags;
+    asm volatile (
+        "DI   %[flags]\n"
+        : [flags]"=r" (flags));
+    return(flags);
+}
+
+/* Restore the contents of the cop0 status register.  Used with
+** cvmx_interrupt_disable_save to allow recursive interrupt disabling */
+static inline void cvmx_interrupt_restore(uint32_t flags)
+{
+    /* If flags value indicates interrupts should be enabled, then enable them */
+    if (flags & 1)
+    {
+        asm volatile (
+            "EI     \n"
+            ::);
+    }
+}
+
+/**
+ * Utility function to decode Octeon's RSL_INT_BLOCKS interrupts
+ * into error messages.
+ */
+extern void cvmx_interrupt_rsl_decode(void);
+
+/**
+ * Utility function to enable all RSL error interupts
+ */
+extern void cvmx_interrupt_rsl_enable(void);
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif
diff --git a/arch/mips/cavium-octeon/executive/cvmx-iob.h b/arch/mips/cavium-octeon/executive/cvmx-iob.h
new file mode 100644
index 0000000..59d9eb5
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-iob.h
@@ -0,0 +1,74 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * This file contains defines for the IO bridge
+
+ * <hr>$Revision: 32636 $<hr>
+ *
+ *
+ */
+#ifndef __CVMX_IOB_H__
+#define __CVMX_IOB_H__
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+/* CSR typedefs have been moved to cvmx-csr-*.h */
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif  /* __CVMX_IOB_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-ipd.h b/arch/mips/cavium-octeon/executive/cvmx-ipd.h
new file mode 100644
index 0000000..11966ed
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-ipd.h
@@ -0,0 +1,183 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Interface to the hardware Input Packet Data unit.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+
+
+#ifndef __CVMX_IPD_H__
+#define __CVMX_IPD_H__
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#ifndef CVMX_ENABLE_LEN_M8_FIX
+#define CVMX_ENABLE_LEN_M8_FIX 0
+#endif
+
+/* CSR typedefs have been moved to cvmx-csr-*.h */
+
+typedef cvmx_ipd_mbuff_first_skip_t cvmx_ipd_mbuff_not_first_skip_t;
+typedef cvmx_ipd_first_next_ptr_back_t cvmx_ipd_second_next_ptr_back_t;
+
+
+/**
+ * Configure IPD
+ *
+ * @param mbuff_size Packets buffer size in 8 byte words
+ * @param first_mbuff_skip
+ *                   Number of 8 byte words to skip in the first buffer
+ * @param not_first_mbuff_skip
+ *                   Number of 8 byte words to skip in each following buffer
+ * @param first_back Must be same as first_mbuff_skip / 128
+ * @param second_back
+ *                   Must be same as not_first_mbuff_skip / 128
+ * @param wqe_fpa_pool
+ *                   FPA pool to get work entries from
+ * @param cache_mode
+ * @param back_pres_enable_flag
+ *                   Enable or disable port back pressure
+ */
+static inline void cvmx_ipd_config(uint64_t mbuff_size,
+                                   uint64_t first_mbuff_skip,
+                                   uint64_t not_first_mbuff_skip,
+                                   uint64_t first_back,
+                                   uint64_t second_back,
+                                   uint64_t wqe_fpa_pool,
+                                   cvmx_ipd_mode_t cache_mode,
+                                   uint64_t back_pres_enable_flag
+                                  )
+{
+    cvmx_ipd_mbuff_first_skip_t first_skip;
+    cvmx_ipd_mbuff_not_first_skip_t not_first_skip;
+    cvmx_ipd_mbuff_size_t size;
+    cvmx_ipd_first_next_ptr_back_t first_back_struct;
+    cvmx_ipd_second_next_ptr_back_t second_back_struct;
+    cvmx_ipd_wqe_fpa_pool_t wqe_pool;
+    cvmx_ipd_ctl_status_t ipd_ctl_reg;
+
+    first_skip.u64 = 0;
+    first_skip.s.skip_sz = first_mbuff_skip;
+    cvmx_write_csr(CVMX_IPD_1ST_MBUFF_SKIP, first_skip.u64);
+
+    not_first_skip.u64 = 0;
+    not_first_skip.s.skip_sz = not_first_mbuff_skip;
+    cvmx_write_csr(CVMX_IPD_NOT_1ST_MBUFF_SKIP, not_first_skip.u64);
+
+    size.u64 = 0;
+    size.s.mb_size = mbuff_size;
+    cvmx_write_csr(CVMX_IPD_PACKET_MBUFF_SIZE, size.u64);
+
+    first_back_struct.u64 = 0;
+    first_back_struct.s.back = first_back;
+    cvmx_write_csr(CVMX_IPD_1st_NEXT_PTR_BACK, first_back_struct.u64);
+
+    second_back_struct.u64 = 0;
+    second_back_struct.s.back = second_back;
+    cvmx_write_csr(CVMX_IPD_2nd_NEXT_PTR_BACK,second_back_struct.u64);
+
+    wqe_pool.u64 = 0;
+    wqe_pool.s.wqe_pool = wqe_fpa_pool;
+    cvmx_write_csr(CVMX_IPD_WQE_FPA_QUEUE, wqe_pool.u64);
+
+    ipd_ctl_reg.u64 = cvmx_read_csr(CVMX_IPD_CTL_STATUS);
+    ipd_ctl_reg.s.opc_mode = cache_mode;
+    ipd_ctl_reg.s.pbp_en = back_pres_enable_flag;
+    cvmx_write_csr(CVMX_IPD_CTL_STATUS, ipd_ctl_reg.u64);
+
+    /* Note: the example RED code that used to be here has been moved to
+        cvmx_helper_setup_red */
+}
+
+
+/**
+ * Enable IPD
+ */
+static inline void cvmx_ipd_enable(void)
+{
+    cvmx_ipd_ctl_status_t ipd_reg;
+    ipd_reg.u64 = cvmx_read_csr(CVMX_IPD_CTL_STATUS);
+    if (ipd_reg.s.ipd_en)
+    {
+        cvmx_dprintf("Warning: Enabling IPD when IPD already enabled.\n");
+    }
+    ipd_reg.s.ipd_en = TRUE;
+    #if  CVMX_ENABLE_LEN_M8_FIX
+    if(!OCTEON_IS_MODEL(OCTEON_CN38XX_PASS2)) {
+        ipd_reg.s.len_m8 = TRUE;
+    }
+    #endif
+    cvmx_write_csr(CVMX_IPD_CTL_STATUS, ipd_reg.u64);
+}
+
+
+/**
+ * Disable IPD
+ */
+static inline void cvmx_ipd_disable(void)
+{
+    cvmx_ipd_ctl_status_t ipd_reg;
+    ipd_reg.u64 = cvmx_read_csr(CVMX_IPD_CTL_STATUS);
+    ipd_reg.s.ipd_en = FALSE;
+    cvmx_write_csr(CVMX_IPD_CTL_STATUS, ipd_reg.u64);
+}
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif  /*  __CVMX_IPD_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-key.h b/arch/mips/cavium-octeon/executive/cvmx-key.h
new file mode 100644
index 0000000..eb97dbc
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-key.h
@@ -0,0 +1,121 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Interface to the on chip key memory. Key memory is
+ * 8k on chip that is inaccessible from off chip. It can
+ * also be cleared using an external hardware pin.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ *
+ */
+
+#ifndef __CVMX_KEY_H__
+#define __CVMX_KEY_H__
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#define CVMX_KEY_MEM_SIZE 8192  /* Size in bytes */
+
+
+/**
+ * Read from KEY memory
+ *
+ * @param address Address (byte) in key memory to read
+ *                0 <= address < CVMX_KEY_MEM_SIZE
+ * @return Value from key memory
+ */
+static inline uint64_t cvmx_key_read(uint64_t address)
+{
+    cvmx_addr_t ptr;
+
+    ptr.u64 = 0;
+    ptr.sio.mem_region  = CVMX_IO_SEG;
+    ptr.sio.is_io       = 1;
+    ptr.sio.did         = CVMX_OCT_DID_KEY_RW;
+    ptr.sio.offset      = address;
+
+    return cvmx_read_csr(ptr.u64);
+}
+
+
+/**
+ * Write to KEY memory
+ *
+ * @param address Address (byte) in key memory to write
+ *                0 <= address < CVMX_KEY_MEM_SIZE
+ * @param value   Value to write to key memory
+ */
+static inline void cvmx_key_write(uint64_t address, uint64_t value)
+{
+    cvmx_addr_t ptr;
+
+    ptr.u64 = 0;
+    ptr.sio.mem_region  = CVMX_IO_SEG;
+    ptr.sio.is_io       = 1;
+    ptr.sio.did         = CVMX_OCT_DID_KEY_RW;
+    ptr.sio.offset      = address;
+
+    cvmx_write_io(ptr.u64, value);
+}
+
+
+/* CSR typedefs have been moved to cvmx-csr-*.h */
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif /*  __CVMX_KEY_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-l2c.c b/arch/mips/cavium-octeon/executive/cvmx-l2c.c
new file mode 100644
index 0000000..503670d
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-l2c.c
@@ -0,0 +1,762 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Implementation of the Level 2 Cache (L2C) control,
+ * measurement, and debugging facilities.
+ *
+ * <hr>$Revision: 33738 $<hr>
+ *
+ */
+#include "cvmx-config.h"
+#include "cvmx.h"
+#include "cvmx-l2c.h"
+#include "cvmx-spinlock.h"
+#include "cvmx-interrupt.h"
+
+
+/* This spinlock is used internally to ensure that only one core is performing
+** certain L2 operations at a time.
+**
+** NOTE: This only protects calls from within a single application - if multiple applications
+** or operating systems are running, then it is up to the user program to coordinate between them.
+*/
+CVMX_SHARED cvmx_spinlock_t cvmx_l2c_spinlock;
+
+
+static inline int l2_size_half(void)
+{
+    uint64_t val = cvmx_read_csr(CVMX_L2D_FUS3);
+    return !!(val & (1ull << 34));
+}
+int cvmx_l2c_get_core_way_partition(uint32_t core)
+{
+    uint32_t    field;
+
+    /* Validate the core number */
+    if (core >= cvmx_octeon_num_cores())
+        return -1;
+
+    /* Use the lower two bits of the coreNumber to determine the bit offset
+     * of the UMSK[] field in the L2C_SPAR register.
+     */
+    field = (core & 0x3) * 8;
+
+    /* Return the UMSK[] field from the appropriate L2C_SPAR register based
+     * on the coreNumber.
+     */
+
+    switch (core & 0xC)
+    {
+        case 0x0:
+            return((cvmx_read_csr(CVMX_L2C_SPAR0) & (0xFF << field)) >> field);
+        case 0x4:
+            return((cvmx_read_csr(CVMX_L2C_SPAR1) & (0xFF << field)) >> field);
+        case 0x8:
+            return((cvmx_read_csr(CVMX_L2C_SPAR2) & (0xFF << field)) >> field);
+        case 0xC:
+            return((cvmx_read_csr(CVMX_L2C_SPAR3) & (0xFF << field)) >> field);
+    }
+    return(0);
+}
+
+int cvmx_l2c_set_core_way_partition(uint32_t core, uint32_t mask)
+{
+    uint32_t    field;
+    uint32_t    valid_mask;
+
+    valid_mask = (0x1 << cvmx_l2c_get_num_assoc()) - 1;
+
+    mask &= valid_mask;
+
+    /* A UMSK setting which blocks all L2C Ways is an error. */
+    if (mask == valid_mask)
+        return -1;
+
+    /* Validate the core number */
+    if (core >= cvmx_octeon_num_cores())
+        return -1;
+
+    /* Check to make sure current mask & new mask don't block all ways */
+    if (((mask | cvmx_l2c_get_core_way_partition(core)) & valid_mask) == valid_mask)
+        return -1;
+
+
+    /* Use the lower two bits of core to determine the bit offset of the
+     * UMSK[] field in the L2C_SPAR register.
+     */
+    field = (core & 0x3) * 8;
+
+    /* Assign the new mask setting to the UMSK[] field in the appropriate
+     * L2C_SPAR register based on the core_num.
+     *
+     */
+    switch (core & 0xC)
+    {
+        case 0x0:
+            cvmx_write_csr(CVMX_L2C_SPAR0,
+                           (cvmx_read_csr(CVMX_L2C_SPAR0) & ~(0xFF << field)) |
+                           mask << field);
+            break;
+        case 0x4:
+            cvmx_write_csr(CVMX_L2C_SPAR1,
+                           (cvmx_read_csr(CVMX_L2C_SPAR1) & ~(0xFF << field)) |
+                           mask << field);
+            break;
+        case 0x8:
+            cvmx_write_csr(CVMX_L2C_SPAR2,
+                           (cvmx_read_csr(CVMX_L2C_SPAR2) & ~(0xFF << field)) |
+                           mask << field);
+            break;
+        case 0xC:
+            cvmx_write_csr(CVMX_L2C_SPAR3,
+                           (cvmx_read_csr(CVMX_L2C_SPAR3) & ~(0xFF << field)) |
+                           mask << field);
+            break;
+    }
+    return 0;
+}
+
+
+int cvmx_l2c_set_hw_way_partition(uint32_t mask)
+{
+    uint32_t valid_mask;
+
+    valid_mask = 0xff;
+
+    if (OCTEON_IS_MODEL(OCTEON_CN58XX) || OCTEON_IS_MODEL(OCTEON_CN38XX))
+    {
+        if (l2_size_half())
+            valid_mask = 0xf;
+    }
+    else if (l2_size_half())
+        valid_mask = 0x3;
+
+    mask &= valid_mask;
+
+    /* A UMSK setting which blocks all L2C Ways is an error. */
+    if (mask == valid_mask)
+        return -1;
+    /* Check to make sure current mask & new mask don't block all ways */
+    if (((mask | cvmx_l2c_get_hw_way_partition()) & valid_mask) == valid_mask)
+        return -1;
+
+    cvmx_write_csr(CVMX_L2C_SPAR4, (cvmx_read_csr(CVMX_L2C_SPAR4) & ~0xFF) | mask);
+    return 0;
+}
+
+int cvmx_l2c_get_hw_way_partition(void)
+{
+    return(cvmx_read_csr(CVMX_L2C_SPAR4) & (0xFF));
+}
+
+
+void cvmx_l2c_config_perf(uint32_t counter, cvmx_l2c_event_t event,
+                          uint32_t clear_on_read)
+{   cvmx_l2c_pfctl_t pfctl;
+
+    pfctl.u64 = cvmx_read_csr(CVMX_L2C_PFCTL);
+
+    switch (counter)
+    {
+        case 0:
+            pfctl.s.cnt0sel = event;
+            pfctl.s.cnt0ena = 1;
+            if (!cvmx_octeon_is_pass1())
+                pfctl.s.cnt0rdclr = clear_on_read;
+            break;
+        case 1:
+            pfctl.s.cnt1sel = event;
+            pfctl.s.cnt1ena = 1;
+            if (!cvmx_octeon_is_pass1())
+                pfctl.s.cnt1rdclr = clear_on_read;
+            break;
+        case 2:
+            pfctl.s.cnt2sel = event;
+            pfctl.s.cnt2ena = 1;
+            if (!cvmx_octeon_is_pass1())
+                pfctl.s.cnt2rdclr = clear_on_read;
+            break;
+        case 3:
+        default:
+            pfctl.s.cnt3sel = event;
+            pfctl.s.cnt3ena = 1;
+            if (!cvmx_octeon_is_pass1())
+                pfctl.s.cnt3rdclr = clear_on_read;
+            break;
+    }
+
+    cvmx_write_csr(CVMX_L2C_PFCTL, pfctl.u64);
+}
+
+uint64_t cvmx_l2c_read_perf(uint32_t counter)
+{
+    switch (counter)
+    {
+        case 0:
+            return(cvmx_read_csr(CVMX_L2C_PFC0));
+        case 1:
+            return(cvmx_read_csr(CVMX_L2C_PFC1));
+        case 2:
+            return(cvmx_read_csr(CVMX_L2C_PFC2));
+        case 3:
+        default:
+            return(cvmx_read_csr(CVMX_L2C_PFC3));
+    }
+}
+
+
+
+/**
+ * @INTERNAL
+ * Helper function use to fault in cache lines for L2 cache locking
+ *
+ * @param addr   Address of base of memory region to read into L2 cache
+ * @param len    Length (in bytes) of region to fault in
+ */
+static void fault_in(uint64_t addr, int len)
+{
+    volatile char *ptr;
+    volatile char dummy;
+    /* Adjust addr and length so we get all cache lines even for
+    ** small ranges spanning two cache lines */
+    len += addr & CVMX_CACHE_LINE_MASK;
+    addr &= ~CVMX_CACHE_LINE_MASK;
+    ptr = (volatile char *)cvmx_phys_to_ptr(addr);
+    CVMX_DCACHE_INVALIDATE;  /* Invalidate L1 cache to make sure all loads result in data being in L2 */
+    while (len > 0)
+    {
+        dummy += *ptr;
+        len -= CVMX_CACHE_LINE_SIZE;
+        ptr += CVMX_CACHE_LINE_SIZE;
+    }
+}
+
+int cvmx_l2c_lock_line(uint64_t addr)
+{
+    int retval = 0;
+    cvmx_l2c_dbg_t l2cdbg;
+    cvmx_l2c_lckbase_t lckbase;
+    cvmx_l2c_lckoff_t lckoff;
+    cvmx_l2t_err_t l2t_err;
+    l2cdbg.u64 = 0;
+    lckbase.u64 = 0;
+    lckoff.u64 = 0;
+
+    cvmx_spinlock_lock(&cvmx_l2c_spinlock);
+
+    /* Clear l2t error bits if set */
+    l2t_err.u64 = cvmx_read_csr(CVMX_L2T_ERR);
+    l2t_err.s.lckerr = 1;
+    l2t_err.s.lckerr2 = 1;
+    cvmx_write_csr(CVMX_L2T_ERR, l2t_err.u64);
+
+    addr &= ~CVMX_CACHE_LINE_MASK;
+
+    /* Set this core as debug core */
+    l2cdbg.s.ppnum = cvmx_get_core_num();
+    CVMX_SYNC;
+    cvmx_write_csr(CVMX_L2C_DBG, l2cdbg.u64);
+    cvmx_read_csr(CVMX_L2C_DBG);
+
+    lckoff.s.lck_offset = 0; /* Only lock 1 line at a time */
+    cvmx_write_csr(CVMX_L2C_LCKOFF, lckoff.u64);
+    cvmx_read_csr(CVMX_L2C_LCKOFF);
+
+    if (((cvmx_l2c_cfg_t)(cvmx_read_csr(CVMX_L2C_CFG))).s.idxalias)
+    {
+        int alias_shift = CVMX_L2C_IDX_ADDR_SHIFT + 2 * CVMX_L2_SET_BITS - 1;
+        uint64_t addr_tmp = addr ^ (addr & ((1 << alias_shift) - 1)) >> CVMX_L2_SET_BITS;
+        lckbase.s.lck_base = addr_tmp >> 7;
+    }
+    else
+    {
+        lckbase.s.lck_base = addr >> 7;
+    }
+
+    lckbase.s.lck_ena = 1;
+    cvmx_write_csr(CVMX_L2C_LCKBASE, lckbase.u64);
+    cvmx_read_csr(CVMX_L2C_LCKBASE);    // Make sure it gets there
+
+    fault_in(addr, CVMX_CACHE_LINE_SIZE);
+
+    lckbase.s.lck_ena = 0;
+    cvmx_write_csr(CVMX_L2C_LCKBASE, lckbase.u64);
+    cvmx_read_csr(CVMX_L2C_LCKBASE);    // Make sure it gets there
+
+    /* Stop being debug core */
+    cvmx_write_csr(CVMX_L2C_DBG, 0);
+    cvmx_read_csr(CVMX_L2C_DBG);
+
+    l2t_err.u64 = cvmx_read_csr(CVMX_L2T_ERR);
+    if (l2t_err.s.lckerr || l2t_err.s.lckerr2)
+        retval = 1;  /* We were unable to lock the line */
+
+    cvmx_spinlock_unlock(&cvmx_l2c_spinlock);
+
+    return(retval);
+}
+
+
+int cvmx_l2c_lock_mem_region(uint64_t start, uint64_t len)
+{
+    int retval = 0;
+
+    /* Round start/end to cache line boundaries */
+    len += start & CVMX_CACHE_LINE_MASK;
+    start &= ~CVMX_CACHE_LINE_MASK;
+    len = (len + CVMX_CACHE_LINE_MASK) & ~CVMX_CACHE_LINE_MASK;
+
+    while (len)
+    {
+        retval += cvmx_l2c_lock_line(start);
+        start += CVMX_CACHE_LINE_SIZE;
+        len -= CVMX_CACHE_LINE_SIZE;
+    }
+
+    return(retval);
+}
+
+
+void cvmx_l2c_flush(void)
+{
+    uint64_t assoc, set;
+    uint64_t n_assoc, n_set;
+    cvmx_l2c_dbg_t l2cdbg;
+
+    cvmx_spinlock_lock(&cvmx_l2c_spinlock);
+
+    l2cdbg.u64 = 0;
+    if (!OCTEON_IS_MODEL(OCTEON_CN30XX))
+        l2cdbg.s.ppnum = cvmx_get_core_num();
+    l2cdbg.s.finv = 1;
+    n_set = CVMX_L2_SETS;
+    n_assoc = l2_size_half() ? (CVMX_L2_ASSOC/2) : CVMX_L2_ASSOC ;
+    for(set=0; set < n_set; set++)
+    {
+        for(assoc = 0; assoc < n_assoc; assoc++)
+        {
+            l2cdbg.s.set = assoc;
+            /* Enter debug mode, and make sure all other writes complete before we
+            ** enter debug mode */
+            CVMX_SYNCW;
+            cvmx_write_csr(CVMX_L2C_DBG, l2cdbg.u64);
+            cvmx_read_csr(CVMX_L2C_DBG);
+
+            CVMX_PREPARE_FOR_STORE (CVMX_ADD_SEG(CVMX_MIPS_SPACE_XKPHYS, set*CVMX_CACHE_LINE_SIZE), 0);
+            CVMX_SYNCW; /* Push STF out to L2 */
+            /* Exit debug mode */
+            CVMX_SYNC;
+            cvmx_write_csr(CVMX_L2C_DBG, 0);
+            cvmx_read_csr(CVMX_L2C_DBG);
+        }
+    }
+
+    cvmx_spinlock_unlock(&cvmx_l2c_spinlock);
+}
+
+
+int cvmx_l2c_unlock_line(uint64_t address)
+{
+    int assoc;
+    cvmx_l2c_tag_t tag;
+    cvmx_l2c_dbg_t l2cdbg;
+    uint32_t tag_addr;
+
+    uint32_t index = cvmx_l2c_address_to_index(address);
+
+    cvmx_spinlock_lock(&cvmx_l2c_spinlock);
+    /* Compute portion of address that is stored in tag */
+    tag_addr = ((address >> CVMX_L2C_TAG_ADDR_ALIAS_SHIFT) & ((1 << CVMX_L2C_TAG_ADDR_ALIAS_SHIFT) - 1));
+    for(assoc = 0; assoc < CVMX_L2_ASSOC; assoc++)
+    {
+        tag = cvmx_get_l2c_tag(assoc, index);
+
+        if (tag.s.V && (tag.s.addr == tag_addr))
+        {
+            l2cdbg.u64 = 0;
+            l2cdbg.s.ppnum = cvmx_get_core_num();
+            l2cdbg.s.set = assoc;
+            l2cdbg.s.finv = 1;
+
+            CVMX_SYNC;
+            cvmx_write_csr(CVMX_L2C_DBG, l2cdbg.u64); /* Enter debug mode */
+            cvmx_read_csr(CVMX_L2C_DBG);
+
+            CVMX_PREPARE_FOR_STORE (CVMX_ADD_SEG(CVMX_MIPS_SPACE_XKPHYS, address), 0);
+            CVMX_SYNC;
+            /* Exit debug mode */
+            cvmx_write_csr(CVMX_L2C_DBG, 0);
+            cvmx_read_csr(CVMX_L2C_DBG);
+            cvmx_spinlock_unlock(&cvmx_l2c_spinlock);
+            return tag.s.L;
+        }
+    }
+    cvmx_spinlock_unlock(&cvmx_l2c_spinlock);
+    return 0;
+}
+
+int cvmx_l2c_unlock_mem_region(uint64_t start, uint64_t len)
+{
+    int num_unlocked = 0;
+    /* Round start/end to cache line boundaries */
+    len += start & CVMX_CACHE_LINE_MASK;
+    start &= ~CVMX_CACHE_LINE_MASK;
+    len = (len + CVMX_CACHE_LINE_MASK) & ~CVMX_CACHE_LINE_MASK;
+    while (len > 0)
+    {
+        num_unlocked += cvmx_l2c_unlock_line(start);
+        start += CVMX_CACHE_LINE_SIZE;
+        len -= CVMX_CACHE_LINE_SIZE;
+    }
+
+    return num_unlocked;
+}
+
+
+/* Internal l2c tag types.  These are converted to a generic structure
+** that can be used on all chips */
+typedef union
+{
+    uint64_t u64;
+#if __BYTE_ORDER == __BIG_ENDIAN
+    struct cvmx_l2c_tag_cn50xx
+    {
+	uint64_t reserved		: 40;
+	uint64_t V			: 1;	// Line valid
+	uint64_t D			: 1;	// Line dirty
+	uint64_t L			: 1;	// Line locked
+	uint64_t U			: 1;	// Use, LRU eviction
+	uint64_t addr			: 20;	// Phys mem addr (33..14)
+    } cn50xx;
+    struct cvmx_l2c_tag_cn30xx
+    {
+	uint64_t reserved		: 41;
+	uint64_t V			: 1;	// Line valid
+	uint64_t D			: 1;	// Line dirty
+	uint64_t L			: 1;	// Line locked
+	uint64_t U			: 1;	// Use, LRU eviction
+	uint64_t addr			: 19;	// Phys mem addr (33..15)
+    } cn30xx;
+    struct cvmx_l2c_tag_cn31xx
+    {
+	uint64_t reserved		: 42;
+	uint64_t V			: 1;	// Line valid
+	uint64_t D			: 1;	// Line dirty
+	uint64_t L			: 1;	// Line locked
+	uint64_t U			: 1;	// Use, LRU eviction
+	uint64_t addr			: 18;	// Phys mem addr (33..16)
+    } cn31xx;
+    struct cvmx_l2c_tag_cn38xx
+    {
+	uint64_t reserved		: 43;
+	uint64_t V			: 1;	// Line valid
+	uint64_t D			: 1;	// Line dirty
+	uint64_t L			: 1;	// Line locked
+	uint64_t U			: 1;	// Use, LRU eviction
+	uint64_t addr			: 17;	// Phys mem addr (33..17)
+    } cn38xx;
+    struct cvmx_l2c_tag_cn58xx
+    {
+	uint64_t reserved		: 44;
+	uint64_t V			: 1;	// Line valid
+	uint64_t D			: 1;	// Line dirty
+	uint64_t L			: 1;	// Line locked
+	uint64_t U			: 1;	// Use, LRU eviction
+	uint64_t addr			: 16;	// Phys mem addr (33..18)
+    } cn58xx;
+    struct cvmx_l2c_tag_cn58xx   cn56xx; /* 2048 sets */
+    struct cvmx_l2c_tag_cn31xx   cn52xx; /* 512 sets */
+#endif
+} __cvmx_l2c_tag_t;
+
+
+/**
+ * @INTERNAL
+ * Function to read a L2C tag.  This code make the current core
+ * the 'debug core' for the L2.  This code must only be executed by
+ * 1 core at a time.
+ *
+ * @param assoc  Association (way) of the tag to dump
+ * @param index  Index of the cacheline
+ *
+ * @return The Octeon model specific tag structure.  This is translated by a wrapper
+ *         function to a generic form that is easier for applications to use.
+ */
+static __cvmx_l2c_tag_t __read_l2_tag(uint64_t assoc, uint64_t index)
+{
+
+    uint64_t debug_tag_addr = (((1ULL << 63) | (index << 7)) + 96);
+    uint64_t core = cvmx_get_core_num();
+    __cvmx_l2c_tag_t tag_val;
+    uint64_t dbg_addr = CVMX_L2C_DBG;
+    uint32_t flags;
+
+    cvmx_l2c_dbg_t debug_val;
+    debug_val.u64 = 0;
+    /* For low core count parts, the core number is always small enough
+    ** to stay in the correct field and not set any reserved bits */
+    debug_val.s.ppnum = core;
+    debug_val.s.l2t = 1;
+    debug_val.s.set = assoc;
+
+    CVMX_SYNC;  /* Make sure core is quiet (no prefetches, etc.) before entering debug mode */
+    CVMX_DCACHE_INVALIDATE;  /* Flush L1 to make sure debug load misses L1 */
+
+    flags = cvmx_interrupt_disable_save();
+
+    /* The following must be done in assembly as when in debug mode all data loads from
+    ** L2 return special debug data, not normal memory contents.  Also, interrupts must be disabled,
+    ** since if an interrupt occurs while in debug mode the ISR will get debug data from all its memory
+    ** reads instead of the contents of memory */
+
+        asm volatile (
+    "        .set push              \n"
+    "        .set mips64              \n"
+    "        .set noreorder           \n"
+    "        sd    %[dbg_val], 0(%[dbg_addr])  \n"   /* Enter debug mode, wait for store */
+    "        ld    $0, 0(%[dbg_addr]) \n"
+    "        ld    %[tag_val], 0(%[tag_addr]) \n"   /* Read L2C tag data */
+    "        sd    $0, 0(%[dbg_addr])  \n"          /* Exit debug mode, wait for store */
+    "        ld    $0, 0(%[dbg_addr]) \n"
+    "        cache 9, 0($0) \n"             /* Invalidate dcache to discard debug data */
+    "        .set pop             \n"
+    :[tag_val] "=r" (tag_val):  [dbg_addr] "r" (dbg_addr), [dbg_val] "r" (debug_val), [tag_addr] "r" (debug_tag_addr) : "memory");
+
+    cvmx_interrupt_restore(flags);
+
+    return(tag_val);
+
+}
+
+
+cvmx_l2c_tag_t cvmx_l2c_get_tag(uint32_t association, uint32_t index)
+{
+    __cvmx_l2c_tag_t tmp_tag;
+    cvmx_l2c_tag_t tag;
+    tag.u64 = 0;
+
+    if ((int)association >= cvmx_l2c_get_num_assoc())
+    {
+        cvmx_dprintf("ERROR: cvmx_get_l2c_tag association out of range\n");
+        return(tag);
+    }
+    if ((int)index >= cvmx_l2c_get_num_sets())
+    {
+        cvmx_dprintf("ERROR: cvmx_get_l2c_tag index out of range (arg: %d, max: %d\n", index, cvmx_l2c_get_num_sets());
+        return(tag);
+    }
+    /* __read_l2_tag is intended for internal use only */
+    tmp_tag = __read_l2_tag(association, index);
+
+    /* Convert all tag structure types to generic version, as it can represent all models */
+    if (OCTEON_IS_MODEL(OCTEON_CN58XX) || OCTEON_IS_MODEL(OCTEON_CN56XX))
+    {
+        tag.s.V    = tmp_tag.cn58xx.V;
+        tag.s.D    = tmp_tag.cn58xx.D;
+        tag.s.L    = tmp_tag.cn58xx.L;
+        tag.s.U    = tmp_tag.cn58xx.U;
+        tag.s.addr = tmp_tag.cn58xx.addr;
+    }
+    else if (OCTEON_IS_MODEL(OCTEON_CN38XX))
+    {
+        tag.s.V    = tmp_tag.cn38xx.V;
+        tag.s.D    = tmp_tag.cn38xx.D;
+        tag.s.L    = tmp_tag.cn38xx.L;
+        tag.s.U    = tmp_tag.cn38xx.U;
+        tag.s.addr = tmp_tag.cn38xx.addr;
+    }
+    else if (OCTEON_IS_MODEL(OCTEON_CN31XX) || OCTEON_IS_MODEL(OCTEON_CN52XX))
+    {
+        tag.s.V    = tmp_tag.cn31xx.V;
+        tag.s.D    = tmp_tag.cn31xx.D;
+        tag.s.L    = tmp_tag.cn31xx.L;
+        tag.s.U    = tmp_tag.cn31xx.U;
+        tag.s.addr = tmp_tag.cn31xx.addr;
+    }
+    else if (OCTEON_IS_MODEL(OCTEON_CN30XX))
+    {
+        tag.s.V    = tmp_tag.cn30xx.V;
+        tag.s.D    = tmp_tag.cn30xx.D;
+        tag.s.L    = tmp_tag.cn30xx.L;
+        tag.s.U    = tmp_tag.cn30xx.U;
+        tag.s.addr = tmp_tag.cn30xx.addr;
+    }
+    else if (OCTEON_IS_MODEL(OCTEON_CN50XX))
+    {
+        tag.s.V    = tmp_tag.cn50xx.V;
+        tag.s.D    = tmp_tag.cn50xx.D;
+        tag.s.L    = tmp_tag.cn50xx.L;
+        tag.s.U    = tmp_tag.cn50xx.U;
+        tag.s.addr = tmp_tag.cn50xx.addr;
+    }
+    else
+    {
+        cvmx_dprintf("Unsupported OCTEON Model in %s\n", __FUNCTION__);
+    }
+
+    return tag;
+}
+
+uint32_t cvmx_l2c_address_to_index (uint64_t addr)
+{
+    uint64_t idx = addr >> CVMX_L2C_IDX_ADDR_SHIFT;
+    cvmx_l2c_cfg_t l2c_cfg;
+    l2c_cfg.u64 = cvmx_read_csr(CVMX_L2C_CFG);
+
+    if (l2c_cfg.s.idxalias)
+    {
+        idx ^= ((addr & CVMX_L2C_ALIAS_MASK) >> CVMX_L2C_TAG_ADDR_ALIAS_SHIFT);
+    }
+    idx &= CVMX_L2C_IDX_MASK;
+    return(idx);
+}
+
+int cvmx_l2c_get_cache_size_bytes(void)
+{
+    return (cvmx_l2c_get_num_sets() * cvmx_l2c_get_num_assoc() * CVMX_CACHE_LINE_SIZE);
+}
+
+/**
+ * Return log base 2 of the number of sets in the L2 cache
+ * @return
+ */
+int cvmx_l2c_get_set_bits(void)
+{
+    int l2_set_bits;
+    if (OCTEON_IS_MODEL(OCTEON_CN56XX) ||
+        OCTEON_IS_MODEL(OCTEON_CN58XX))
+        l2_set_bits =  11; /* 2048 sets */
+    else if (OCTEON_IS_MODEL(OCTEON_CN38XX))
+        l2_set_bits =  10; /* 1024 sets */
+    else if (OCTEON_IS_MODEL(OCTEON_CN31XX) || OCTEON_IS_MODEL(OCTEON_CN52XX))
+        l2_set_bits =  9; /* 512 sets */
+    else if (OCTEON_IS_MODEL(OCTEON_CN30XX))
+        l2_set_bits =  8; /* 256 sets */
+    else if (OCTEON_IS_MODEL(OCTEON_CN50XX))
+        l2_set_bits =  7; /* 128 sets */
+    else
+    {
+        cvmx_dprintf("Unsupported OCTEON Model in %s\n", __FUNCTION__);
+        l2_set_bits =  11; /* 2048 sets */
+    }
+    return(l2_set_bits);
+
+}
+
+/* Return the number of sets in the L2 Cache */
+int cvmx_l2c_get_num_sets(void)
+{
+    return (1 << cvmx_l2c_get_set_bits());
+}
+
+/* Return the number of associations in the L2 Cache */
+int cvmx_l2c_get_num_assoc(void)
+{
+    int l2_assoc;
+    if (OCTEON_IS_MODEL(OCTEON_CN56XX) ||
+        OCTEON_IS_MODEL(OCTEON_CN52XX) ||
+        OCTEON_IS_MODEL(OCTEON_CN58XX) ||
+        OCTEON_IS_MODEL(OCTEON_CN50XX) ||
+        OCTEON_IS_MODEL(OCTEON_CN38XX))
+        l2_assoc =  8;
+    else if (OCTEON_IS_MODEL(OCTEON_CN31XX) || 
+             OCTEON_IS_MODEL(OCTEON_CN30XX))
+        l2_assoc =  4;
+    else
+    {
+        cvmx_dprintf("Unsupported OCTEON Model in %s\n", __FUNCTION__);
+        l2_assoc =  8;
+    }
+
+    /* Check to see if part of the cache is disabled */
+    if (cvmx_fuse_read(265))
+        l2_assoc = l2_assoc >> 2;
+    else if (cvmx_fuse_read(264))
+        l2_assoc = l2_assoc >> 1;
+
+    return(l2_assoc);
+}
+
+
+
+/**
+ * Flush a line from the L2 cache
+ * This should only be called from one core at a time, as this routine
+ * sets the core to the 'debug' core in order to flush the line.
+ *
+ * @param assoc  Association (or way) to flush
+ * @param index  Index to flush
+ */
+void cvmx_l2c_flush_line(uint32_t assoc, uint32_t index)
+{
+    cvmx_l2c_dbg_t l2cdbg;
+
+    l2cdbg.u64 = 0;
+    l2cdbg.s.ppnum = cvmx_get_core_num();
+    l2cdbg.s.finv = 1;
+
+    l2cdbg.s.set = assoc;
+    /* Enter debug mode, and make sure all other writes complete before we
+    ** enter debug mode */
+    asm volatile ("sync \n"::: "memory");
+    cvmx_write_csr(CVMX_L2C_DBG, l2cdbg.u64);
+    cvmx_read_csr(CVMX_L2C_DBG);
+
+    CVMX_PREPARE_FOR_STORE (((1ULL << 63) + (index)*128), 0);
+    /* Exit debug mode */
+    asm volatile ("sync \n"::: "memory");
+    cvmx_write_csr(CVMX_L2C_DBG, 0);
+    cvmx_read_csr(CVMX_L2C_DBG);
+}
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-l2c.h b/arch/mips/cavium-octeon/executive/cvmx-l2c.h
new file mode 100644
index 0000000..3eff15c
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-l2c.h
@@ -0,0 +1,372 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Interface to the Level 2 Cache (L2C) control, measurement, and debugging
+ * facilities.
+ *
+ * <hr>$Revision: 33738 $<hr>
+ *
+ */
+
+#ifndef __CVMX_L2C_H__
+#define __CVMX_L2C_H__
+
+#define CVMX_L2_ASSOC     cvmx_l2c_get_num_assoc()   /* Deprecated macro, use function */
+#define CVMX_L2_SET_BITS  cvmx_l2c_get_set_bits()    /* Deprecated macro, use function */
+#define CVMX_L2_SETS      cvmx_l2c_get_num_sets()    /* Deprecated macro, use function */
+
+
+#define CVMX_L2C_IDX_ADDR_SHIFT 7  /* based on 128 byte cache line size */
+#define CVMX_L2C_IDX_MASK       (cvmx_l2c_get_num_sets() - 1)
+
+/* Defines for index aliasing computations */
+#define CVMX_L2C_TAG_ADDR_ALIAS_SHIFT (CVMX_L2C_IDX_ADDR_SHIFT + cvmx_l2c_get_set_bits())
+#define CVMX_L2C_ALIAS_MASK (CVMX_L2C_IDX_MASK << CVMX_L2C_TAG_ADDR_ALIAS_SHIFT)
+
+
+  /*------------*/
+  /*  TYPEDEFS  */
+  /*------------*/
+typedef union {        // L2C Tag/Data Store Debug Register
+  uint64_t    u64;
+  struct {
+    uint64_t  reserved: 32,
+	      lfb_enum:  4,
+	      lfb_dmp:   1,
+	      ppnum:     4,
+	      set:       3,
+	      finv:      1,
+	      l2d:       1,
+	      l2t:       1;
+  };
+} cvmx_l2c_dbg;
+
+typedef union
+{
+    uint64_t u64;
+#if __BYTE_ORDER == __BIG_ENDIAN
+    struct
+    {
+	uint64_t reserved		: 28;
+	uint64_t V			: 1;	// Line valid
+	uint64_t D			: 1;	// Line dirty
+	uint64_t L			: 1;	// Line locked
+	uint64_t U			: 1;	// Use, LRU eviction
+	uint64_t addr			: 32;	// Phys mem (not all bits valid)
+    } s;
+#endif
+} cvmx_l2c_tag_t;
+
+
+  /* L2C Performance Counter events. */
+typedef enum
+{
+    CVMX_L2C_EVENT_CYCLES           =  0,
+    CVMX_L2C_EVENT_INSTRUCTION_MISS =  1,
+    CVMX_L2C_EVENT_INSTRUCTION_HIT  =  2,
+    CVMX_L2C_EVENT_DATA_MISS        =  3,
+    CVMX_L2C_EVENT_DATA_HIT         =  4,
+    CVMX_L2C_EVENT_MISS             =  5,
+    CVMX_L2C_EVENT_HIT              =  6,
+    CVMX_L2C_EVENT_VICTIM_HIT       =  7,
+    CVMX_L2C_EVENT_INDEX_CONFLICT   =  8,
+    CVMX_L2C_EVENT_TAG_PROBE        =  9,
+    CVMX_L2C_EVENT_TAG_UPDATE       = 10,
+    CVMX_L2C_EVENT_TAG_COMPLETE     = 11,
+    CVMX_L2C_EVENT_TAG_DIRTY        = 12,
+    CVMX_L2C_EVENT_DATA_STORE_NOP   = 13,
+    CVMX_L2C_EVENT_DATA_STORE_READ  = 14,
+    CVMX_L2C_EVENT_DATA_STORE_WRITE = 15,
+    CVMX_L2C_EVENT_FILL_DATA_VALID  = 16,
+    CVMX_L2C_EVENT_WRITE_REQUEST    = 17,
+    CVMX_L2C_EVENT_READ_REQUEST     = 18,
+    CVMX_L2C_EVENT_WRITE_DATA_VALID = 19,
+    CVMX_L2C_EVENT_XMC_NOP          = 20,
+    CVMX_L2C_EVENT_XMC_LDT          = 21,
+    CVMX_L2C_EVENT_XMC_LDI          = 22,
+    CVMX_L2C_EVENT_XMC_LDD          = 23,
+    CVMX_L2C_EVENT_XMC_STF          = 24,
+    CVMX_L2C_EVENT_XMC_STT          = 25,
+    CVMX_L2C_EVENT_XMC_STP          = 26,
+    CVMX_L2C_EVENT_XMC_STC          = 27,
+    CVMX_L2C_EVENT_XMC_DWB          = 28,
+    CVMX_L2C_EVENT_XMC_PL2          = 29,
+    CVMX_L2C_EVENT_XMC_PSL1         = 30,
+    CVMX_L2C_EVENT_XMC_IOBLD        = 31,
+    CVMX_L2C_EVENT_XMC_IOBST        = 32,
+    CVMX_L2C_EVENT_XMC_IOBDMA       = 33,
+    CVMX_L2C_EVENT_XMC_IOBRSP       = 34,
+    CVMX_L2C_EVENT_XMC_BUS_VALID    = 35,
+    CVMX_L2C_EVENT_XMC_MEM_DATA     = 36,
+    CVMX_L2C_EVENT_XMC_REFL_DATA    = 37,
+    CVMX_L2C_EVENT_XMC_IOBRSP_DATA  = 38,
+    CVMX_L2C_EVENT_RSC_NOP          = 39,
+    CVMX_L2C_EVENT_RSC_STDN         = 40,
+    CVMX_L2C_EVENT_RSC_FILL         = 41,
+    CVMX_L2C_EVENT_RSC_REFL         = 42,
+    CVMX_L2C_EVENT_RSC_STIN         = 43,
+    CVMX_L2C_EVENT_RSC_SCIN         = 44,
+    CVMX_L2C_EVENT_RSC_SCFL         = 45,
+    CVMX_L2C_EVENT_RSC_SCDN         = 46,
+    CVMX_L2C_EVENT_RSC_DATA_VALID   = 47,
+    CVMX_L2C_EVENT_RSC_VALID_FILL   = 48,
+    CVMX_L2C_EVENT_RSC_VALID_STRSP  = 49,
+    CVMX_L2C_EVENT_RSC_VALID_REFL   = 50,
+    CVMX_L2C_EVENT_LRF_REQ          = 51,
+    CVMX_L2C_EVENT_DT_RD_ALLOC      = 52,
+    CVMX_L2C_EVENT_DT_WR_INVAL      = 53
+} cvmx_l2c_event_t;
+
+/**
+ * Configure one of the four L2 Cache performance counters to capture event
+ * occurences.
+ *
+ * @param counter        The counter to configure. Range 0..3.
+ * @param event          The type of L2 Cache event occurrence to count.
+ * @param clear_on_read  When asserted, any read of the performance counter
+ *                       clears the counter.
+ *
+ * @note The routine does not clear the counter.
+ */
+void cvmx_l2c_config_perf(uint32_t         counter,
+                               cvmx_l2c_event_t event,
+                               uint32_t         clear_on_read);
+/**
+ * Read the given L2 Cache performance counter. The counter must be configured
+ * before reading, but this routine does not enforce this requirement.
+ *
+ * @param counter  The counter to configure. Range 0..3.
+ *
+ * @return The current counter value.
+ */
+uint64_t cvmx_l2c_read_perf(uint32_t counter);
+
+/**
+ * Return the L2 Cache way partitioning for a given core.
+ *
+ * @param core  The core processor of interest.
+ *
+ * @return    The mask specifying the partitioning. 0 bits in mask indicates
+ *              the cache 'ways' that a core can evict from.
+ *            -1 on error
+ */
+int cvmx_l2c_get_core_way_partition(uint32_t core);
+
+/**
+ * Partitions the L2 cache for a core
+ *
+ * @param core   The core that the partitioning applies to.
+ * @param mask The partitioning of the ways expressed as a binary mask. A 0 bit allows the core
+ *             to evict cache lines from a way, while a 1 bit blocks the core from evicting any lines
+ *             from that way. There must be at least one allowed way (0 bit) in the mask.
+ *
+ * @note  If any ways are blocked for all cores and the HW blocks, then those ways will never have
+ *        any cache lines evicted from them.  All cores and the hardware blocks are free to read from
+ *        all ways regardless of the partitioning.
+ */
+int cvmx_l2c_set_core_way_partition(uint32_t core, uint32_t mask);
+
+/**
+ * Return the L2 Cache way partitioning for the hw blocks.
+ *
+ * @return    The mask specifying the reserved way. 0 bits in mask indicates
+ *              the cache 'ways' that a core can evict from.
+ *            -1 on error
+ */
+int cvmx_l2c_get_hw_way_partition(void);
+
+/**
+ * Partitions the L2 cache for the hardware blocks.
+ *
+ * @param mask The partitioning of the ways expressed as a binary mask. A 0 bit allows the core
+ *             to evict cache lines from a way, while a 1 bit blocks the core from evicting any lines
+ *             from that way. There must be at least one allowed way (0 bit) in the mask.
+ *
+ * @note  If any ways are blocked for all cores and the HW blocks, then those ways will never have
+ *        any cache lines evicted from them.  All cores and the hardware blocks are free to read from
+ *        all ways regardless of the partitioning.
+ */
+int cvmx_l2c_set_hw_way_partition(uint32_t mask);
+
+
+/**
+ * Locks a line in the L2 cache at the specified physical address
+ *
+ * @param addr   physical address of line to lock
+ *
+ * @return 0 on success,
+ *         1 if line not locked.
+ */
+int cvmx_l2c_lock_line(uint64_t addr);
+
+/**
+ * Locks a specified memory region in the L2 cache.
+ *
+ * Note that if not all lines can be locked, that means that all
+ * but one of the ways (associations) available to the locking
+ * core are locked.  Having only 1 association available for
+ * normal caching may have a significant adverse affect on performance.
+ * Care should be taken to ensure that enough of the L2 cache is left
+ * unlocked to allow for normal caching of DRAM.
+ *
+ * @param start  Physical address of the start of the region to lock
+ * @param len    Length (in bytes) of region to lock
+ *
+ * @return Number of requested lines that where not locked.
+ *         0 on success (all locked)
+ */
+int cvmx_l2c_lock_mem_region(uint64_t start, uint64_t len);
+
+
+/**
+ * Unlock and flush a cache line from the L2 cache.
+ * IMPORTANT: Must only be run by one core at a time due to use
+ * of L2C debug features.
+ * Note that this function will flush a matching but unlocked cache line.
+ * (If address is not in L2, no lines are flushed.)
+ *
+ * @param address Physical address to unlock
+ *
+ * @return 0: line not unlocked
+ *         1: line unlocked
+ */
+int cvmx_l2c_unlock_line(uint64_t address);
+
+/**
+ * Unlocks a region of memory that is locked in the L2 cache
+ *
+ * @param start  start physical address
+ * @param len    length (in bytes) to unlock
+ *
+ * @return Number of locked lines that the call unlocked
+ */
+int cvmx_l2c_unlock_mem_region(uint64_t start, uint64_t len);
+
+
+
+
+/**
+ * Read the L2 controller tag for a given location in L2
+ *
+ * @param association
+ *               Which association to read line from
+ * @param index  Which way to read from.
+ *
+ * @return l2c tag structure for line requested.
+ */
+cvmx_l2c_tag_t cvmx_l2c_get_tag(uint32_t association, uint32_t index);
+
+/* Wrapper around deprecated old function name */
+static inline cvmx_l2c_tag_t cvmx_get_l2c_tag(uint32_t association, uint32_t index)
+{
+    return cvmx_l2c_get_tag(association, index);
+}
+
+
+/**
+ * Returns the cache index for a given physical address
+ *
+ * @param addr   physical address
+ *
+ * @return L2 cache index
+ */
+uint32_t cvmx_l2c_address_to_index (uint64_t addr);
+
+
+/**
+ * Flushes (and unlocks) the entire L2 cache.
+ * IMPORTANT: Must only be run by one core at a time due to use
+ * of L2C debug features.
+ */
+void cvmx_l2c_flush(void);
+
+
+
+/**
+ *
+ * @return Returns the size of the L2 cache in bytes,
+ * -1 on error (unrecognized model)
+ */
+int cvmx_l2c_get_cache_size_bytes(void);
+
+/**
+ * Return the number of sets in the L2 Cache
+ *
+ * @return
+ */
+int cvmx_l2c_get_num_sets(void);
+
+/**
+ * Return log base 2 of the number of sets in the L2 cache
+ * @return
+ */
+int cvmx_l2c_get_set_bits(void);
+/**
+ * Return the number of associations in the L2 Cache
+ *
+ * @return
+ */
+int cvmx_l2c_get_num_assoc(void);
+
+/**
+ * Flush a line from the L2 cache
+ * This should only be called from one core at a time, as this routine
+ * sets the core to the 'debug' core in order to flush the line.
+ *
+ * @param assoc  Association (or way) to flush
+ * @param index  Index to flush
+ */
+void cvmx_l2c_flush_line(uint32_t assoc, uint32_t index);
+
+#endif /* __CVMX_L2C_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-llm.c b/arch/mips/cavium-octeon/executive/cvmx-llm.c
new file mode 100644
index 0000000..05bb2e0
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-llm.c
@@ -0,0 +1,975 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Configuration functions for low latency memory.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+#include "cvmx-config.h"
+#include "cvmx.h"
+#include "cvmx-llm.h"
+#include "cvmx-sysinfo.h"
+#include "cvmx-csr-db.h"
+
+#define	MIN(a,b) (((a)<(b))?(a):(b))
+
+typedef struct
+{
+    uint32_t dfa_memcfg0_base;
+    uint32_t dfa_memcfg1_base;
+    uint32_t mrs_dat_p0bunk0;
+    uint32_t mrs_dat_p0bunk1;
+    uint32_t mrs_dat_p1bunk0;
+    uint32_t mrs_dat_p1bunk1;
+    uint8_t  p0_ena;
+    uint8_t  p1_ena;
+    uint8_t  bunkport;
+} rldram_csr_config_t;
+
+
+
+
+
+int rld_csr_config_generate(llm_descriptor_t *llm_desc_ptr, rldram_csr_config_t *cfg_ptr);
+
+
+void print_rld_cfg(rldram_csr_config_t *cfg_ptr);
+void write_rld_cfg(rldram_csr_config_t *cfg_ptr);
+static void cn31xx_dfa_memory_init(void);
+
+static uint32_t process_address_map_str(uint32_t mrs_dat, char *addr_str);
+
+
+
+#ifndef CVMX_LLM_NUM_PORTS
+#warning WARNING: default CVMX_LLM_NUM_PORTS used.  Defaults deprecated, please set in executive-config.h
+#define CVMX_LLM_NUM_PORTS 1
+#endif
+
+
+#if (CVMX_LLM_NUM_PORTS != 1) && (CVMX_LLM_NUM_PORTS != 2)
+#error "Invalid CVMX_LLM_NUM_PORTS value: must be 1 or 2\n"
+#endif
+
+int cvmx_llm_initialize()
+{
+    if (cvmx_llm_initialize_desc(NULL) < 0)
+        return -1;
+
+    return 0;
+}
+
+
+int cvmx_llm_get_default_descriptor(llm_descriptor_t *llm_desc_ptr)
+{
+    cvmx_sysinfo_t *sys_ptr;
+    sys_ptr = cvmx_sysinfo_get();
+
+    if (!llm_desc_ptr)
+        return -1;
+
+    memset(llm_desc_ptr, 0, sizeof(llm_descriptor_t));
+
+    llm_desc_ptr->cpu_hz = sys_ptr->cpu_clock_hz;
+
+    if (sys_ptr->board_type == CVMX_BOARD_TYPE_EBT3000)
+    { // N3K->RLD0 Address Swizzle
+        strcpy(llm_desc_ptr->addr_rld0_fb_str, "22 21 20 19 18 17 16 15 14 13 12 11 10 09 08 07 06 05 04 03 02 01 00");
+        strcpy(llm_desc_ptr->addr_rld0_bb_str, "22 21 19 20 08 07 06 05 04 03 02 01 00 09 18 17 16 15 14 13 12 11 10");
+        // N3K->RLD1 Address Swizzle
+        strcpy(llm_desc_ptr->addr_rld1_fb_str, "22 21 20 19 18 17 16 15 14 13 12 11 10 09 08 07 06 05 04 03 02 01 00");
+        strcpy(llm_desc_ptr->addr_rld1_bb_str, "22 21 20 00 08 07 06 05 04 13 02 01 03 09 18 17 16 15 14 10 12 11 19");
+        /* NOTE: The ebt3000 has a strange RLDRAM configuration for validation purposes.  It is not recommended to have
+        ** different amounts of memory on different ports as that renders some memory unusable */
+        llm_desc_ptr->rld0_bunks = 2;
+        llm_desc_ptr->rld1_bunks = 2;
+        llm_desc_ptr->rld0_mbytes = 128;          // RLD0: 4x 32Mx9
+        llm_desc_ptr->rld1_mbytes = 64;           // RLD1: 2x 16Mx18
+    }
+    else if (sys_ptr->board_type == CVMX_BOARD_TYPE_EBT5800)
+    {
+        strcpy(llm_desc_ptr->addr_rld0_fb_str, "22 21 20 19 18 17 16 15 14 13 12 11 10 09 08 07 06 05 04 03 02 01 00");
+        strcpy(llm_desc_ptr->addr_rld0_bb_str, "22 21 20 00 08 07 06 05 04 13 02 01 03 09 18 17 16 15 14 10 12 11 19");
+        strcpy(llm_desc_ptr->addr_rld1_fb_str, "22 21 20 19 18 17 16 15 14 13 12 11 10 09 08 07 06 05 04 03 02 01 00");
+        strcpy(llm_desc_ptr->addr_rld1_bb_str, "22 21 20 00 08 07 06 05 04 13 02 01 03 09 18 17 16 15 14 10 12 11 19");
+        llm_desc_ptr->rld0_bunks = 2;
+        llm_desc_ptr->rld1_bunks = 2;
+        llm_desc_ptr->rld0_mbytes = 128;
+        llm_desc_ptr->rld1_mbytes = 128;
+        llm_desc_ptr->max_rld_clock_mhz = 400;  /* CN58XX needs a max clock speed for selecting optimal divisor */
+    }
+    else if (sys_ptr->board_type == CVMX_BOARD_TYPE_EBH3000)
+    {
+        strcpy(llm_desc_ptr->addr_rld0_fb_str, "22 21 20 19 18 17 16 15 14 13 12 11 10 09 08 07 06 05 04 03 02 01 00");
+        strcpy(llm_desc_ptr->addr_rld0_bb_str, "22 21 19 20 08 07 06 05 04 03 02 01 00 09 18 17 16 15 14 13 12 11 10");
+        strcpy(llm_desc_ptr->addr_rld1_fb_str, "22 21 20 19 18 17 16 15 14 13 12 11 10 09 08 07 06 05 04 03 02 01 00");
+        strcpy(llm_desc_ptr->addr_rld1_bb_str, "22 21 19 20 08 07 06 05 04 03 02 01 00 09 18 17 16 15 14 13 12 11 10");
+        llm_desc_ptr->rld0_bunks = 2;
+        llm_desc_ptr->rld1_bunks = 2;
+        llm_desc_ptr->rld0_mbytes = 128;
+        llm_desc_ptr->rld1_mbytes = 128;
+    }
+    else if (sys_ptr->board_type == CVMX_BOARD_TYPE_NAC38)
+    {
+        if (sys_ptr->board_rev_major == 1 && sys_ptr->board_rev_minor == 0)
+        {
+            strcpy(llm_desc_ptr->addr_rld0_fb_str, "22 21 20 00 08 07 06 05 04 13 02 01 03 09 18 17 16 15 14 10 12 11 19");
+            strcpy(llm_desc_ptr->addr_rld0_bb_str, "22 21 20 00 08 07 06 05 04 13 02 01 03 09 18 17 16 15 14 10 12 11 19");
+            strcpy(llm_desc_ptr->addr_rld1_fb_str, "22 21 20 00 08 07 06 05 04 13 02 01 03 09 18 17 16 15 14 10 12 11 19");
+            strcpy(llm_desc_ptr->addr_rld1_bb_str, "22 21 20 00 08 07 06 05 04 13 02 01 03 09 18 17 16 15 14 10 12 11 19");
+            llm_desc_ptr->rld0_bunks = 2;
+            llm_desc_ptr->rld1_bunks = 2;
+            llm_desc_ptr->rld0_mbytes = 128;
+            llm_desc_ptr->rld1_mbytes = 128;
+        }
+        else
+        {   /* Asus new recommendation  */
+            strcpy(llm_desc_ptr->addr_rld0_fb_str, "22 21 09 11 04 06 05 08 15 20 16 18 12 13 00 01 07 02 19 17 10 14 03");
+            strcpy(llm_desc_ptr->addr_rld0_bb_str, "22 21 11 09 00 01 07 02 19 17 10 14 03 13 04 06 05 08 15 20 16 18 12");
+            strcpy(llm_desc_ptr->addr_rld1_fb_str, "22 21 08 13 14 00 04 12 16 11 19 10 07 02 01 05 03 06 17 18 20 09 15");
+            strcpy(llm_desc_ptr->addr_rld1_bb_str, "22 21 13 08 01 05 03 06 17 18 20 09 15 02 14 00 04 12 16 11 19 10 07");
+            llm_desc_ptr->rld0_bunks = 2;
+            llm_desc_ptr->rld1_bunks = 2;
+            llm_desc_ptr->rld0_mbytes = 128;
+            llm_desc_ptr->rld1_mbytes = 128;
+        }
+    }
+    else if (sys_ptr->board_type == CVMX_BOARD_TYPE_THUNDER)
+    {
+
+        if (sys_ptr->board_rev_major >= 4)
+        {
+            strcpy(llm_desc_ptr->addr_rld0_fb_str, "22 21 13 11 01 02 07 19 03 18 10 12 20 06 04 08 17 05 14 16 00 09 15");
+            strcpy(llm_desc_ptr->addr_rld0_bb_str, "22 21 11 13 04 08 17 05 14 16 00 09 15 06 01 02 07 19 03 18 10 12 20");
+            strcpy(llm_desc_ptr->addr_rld1_fb_str, "22 21 02 19 18 17 16 09 14 13 20 11 10 01 08 03 06 15 04 07 05 12 00");
+            strcpy(llm_desc_ptr->addr_rld1_bb_str, "22 21 19 02 08 03 06 15 04 07 05 12 00 01 18 17 16 09 14 13 20 11 10");
+        }
+        else
+        {
+            strcpy(llm_desc_ptr->addr_rld0_fb_str, "22 21 20 19 18 17 16 15 14 13 12 11 10 09 08 07 06 05 04 03 02 01 00");
+            strcpy(llm_desc_ptr->addr_rld0_bb_str, "22 21 20 19 18 17 16 15 14 13 12 11 10 09 08 07 06 05 04 03 02 01 00");
+            strcpy(llm_desc_ptr->addr_rld1_fb_str, "22 21 20 19 18 17 16 15 14 13 12 11 10 09 08 07 06 05 04 03 02 01 00");
+            strcpy(llm_desc_ptr->addr_rld1_bb_str, "22 21 20 19 18 17 16 15 14 13 12 11 10 09 08 07 06 05 04 03 02 01 00");
+        }
+
+        llm_desc_ptr->rld0_bunks = 2;
+        llm_desc_ptr->rld1_bunks = 2;
+        llm_desc_ptr->rld0_mbytes = 128;
+        llm_desc_ptr->rld1_mbytes = 128;
+    }
+    else if (sys_ptr->board_type == CVMX_BOARD_TYPE_NICPRO2)
+    {
+        strcpy(llm_desc_ptr->addr_rld0_fb_str, "22 21 20 19 18 17 16 15 14 13 12 11 10 09 08 07 06 05 04 03 02 01 00");
+        strcpy(llm_desc_ptr->addr_rld0_bb_str, "22 21 19 20 08 07 06 05 04 03 02 01 00 09 18 17 16 15 14 13 12 11 10");
+        strcpy(llm_desc_ptr->addr_rld1_fb_str, "22 21 20 19 18 17 16 15 14 13 12 11 10 09 08 07 06 05 04 03 02 01 00");
+        strcpy(llm_desc_ptr->addr_rld1_bb_str, "22 21 19 20 08 07 06 05 04 03 02 01 00 09 18 17 16 15 14 13 12 11 10");
+        llm_desc_ptr->rld0_bunks = 2;
+        llm_desc_ptr->rld1_bunks = 2;
+        llm_desc_ptr->rld0_mbytes = 256;
+        llm_desc_ptr->rld1_mbytes = 256;
+        llm_desc_ptr->max_rld_clock_mhz = 400;  /* CN58XX needs a max clock speed for selecting optimal divisor */
+    }
+    else if (sys_ptr->board_type == CVMX_BOARD_TYPE_TRANTOR)
+    {
+        strcpy(llm_desc_ptr->addr_rld0_fb_str, "22 21 20 19 18 17 16 15 14 13 12 11 10 09 08 07 06 05 04 03 02 01 00");
+        strcpy(llm_desc_ptr->addr_rld0_bb_str, "22 21 20 19 18 17 16 15 14 13 12 11 10 09 08 07 06 05 04 03 02 01 00");
+        strcpy(llm_desc_ptr->addr_rld1_fb_str, "22 21 20 19 18 17 16 15 14 13 12 11 10 09 08 07 06 05 04 03 02 01 00");
+        strcpy(llm_desc_ptr->addr_rld1_bb_str, "22 21 20 19 18 17 16 15 14 13 12 11 10 09 08 07 06 05 04 03 02 01 00");
+        llm_desc_ptr->rld0_bunks = 2;
+        llm_desc_ptr->rld1_bunks = 2;
+        llm_desc_ptr->rld0_mbytes = 64;
+        llm_desc_ptr->rld1_mbytes = 64;
+    }
+    else if (sys_ptr->board_type == CVMX_BOARD_TYPE_EBH3100)
+    {
+        /* CN31xx DFA memory is DDR based, so it is completely different from the CN38XX DFA memory */
+        llm_desc_ptr->rld0_bunks = 1;
+        llm_desc_ptr->rld0_mbytes = 256;
+    }
+    else if (sys_ptr->board_type == CVMX_BOARD_TYPE_KBP)
+    {
+        strcpy(llm_desc_ptr->addr_rld0_fb_str, "");
+        strcpy(llm_desc_ptr->addr_rld0_bb_str, "");
+        llm_desc_ptr->rld0_bunks = 0;
+        llm_desc_ptr->rld0_mbytes = 0;
+        strcpy(llm_desc_ptr->addr_rld1_fb_str, "22 21 20 19 18 17 16 15 14 13 12 11 10 09 08 07 06 05 04 03 02 01 00");
+        strcpy(llm_desc_ptr->addr_rld1_bb_str, "22 21 20 19 18 17 16 15 14 13 12 11 10 09 08 07 06 05 04 03 02 01 00");
+        llm_desc_ptr->rld1_bunks = 2;
+        llm_desc_ptr->rld1_mbytes = 64;
+    }
+    else
+    {
+        cvmx_dprintf("No default LLM configuration available for board %s (%d)\n", cvmx_board_type_to_string(sys_ptr->board_type),  sys_ptr->board_type);
+        return -1;
+    }
+
+    return(0);
+}
+
+int cvmx_llm_initialize_desc(llm_descriptor_t *llm_desc_ptr)
+{
+    cvmx_sysinfo_t *sys_ptr;
+    sys_ptr = cvmx_sysinfo_get();
+    llm_descriptor_t default_llm_desc;
+
+    memset(&default_llm_desc, 0, sizeof(default_llm_desc));
+    if (sys_ptr->board_type == CVMX_BOARD_TYPE_SIM)
+    {
+        cvmx_dprintf("Skipping llm configuration for simulator.\n");
+        return 0;
+    }
+
+    if (sys_ptr->board_type == CVMX_BOARD_TYPE_EBH3100)
+    {
+        /* CN31xx DFA memory is DDR based, so it is completely different from the CN38XX DFA memory
+        ** config descriptors are not supported yet.*/
+        cvmx_dprintf("Warning: preliminary DFA memory configuration\n");
+        cn31xx_dfa_memory_init();
+        return(256*1024*1024);
+    }
+
+    /* If no descriptor passed, generate default descriptor based on board type.
+    ** Fail if no default available for given board type
+    */
+    if (!llm_desc_ptr)
+    {
+        /* Get default descriptor */
+        if (0 > cvmx_llm_get_default_descriptor(&default_llm_desc))
+            return -1;
+
+        /* Disable second port depending on CVMX config */
+        if (CVMX_LLM_NUM_PORTS == 1)
+	  default_llm_desc.rld0_bunks = 0;        // For single port: Force RLD0(P1) to appear EMPTY
+
+        cvmx_dprintf("Using default LLM configuration for board %s (%d)\n", cvmx_board_type_to_string(sys_ptr->board_type),  sys_ptr->board_type);
+
+        llm_desc_ptr = &default_llm_desc;
+    }
+
+
+
+    rldram_csr_config_t ebt3000_rld_cfg;
+    if (!rld_csr_config_generate(llm_desc_ptr, &ebt3000_rld_cfg))
+    {
+        cvmx_dprintf("Configuring %d llm port(s).\n", !!llm_desc_ptr->rld0_bunks + !!llm_desc_ptr->rld1_bunks);
+        write_rld_cfg(&ebt3000_rld_cfg);
+    }
+    else
+    {
+        cvmx_dprintf("Error creating rldram configuration\n");
+        return(-1);
+    }
+
+    /* Compute how much memory is configured
+    ** Memory is interleaved, so if one port has more than the other some memory is not usable */
+
+    /* If both ports are enabled, handle the case where one port has more than the other.
+    ** This is an unusual and not recommended configuration that exists on the ebt3000 board */
+    if (!!llm_desc_ptr->rld0_bunks && !!llm_desc_ptr->rld1_bunks)
+        llm_desc_ptr->rld0_mbytes = llm_desc_ptr->rld1_mbytes = MIN(llm_desc_ptr->rld0_mbytes, llm_desc_ptr->rld1_mbytes);
+
+    return(((!!llm_desc_ptr->rld0_bunks) * llm_desc_ptr->rld0_mbytes
+          + (!!llm_desc_ptr->rld1_bunks) * llm_desc_ptr->rld1_mbytes) * 1024*1024);
+}
+
+//======================
+// SUPPORT FUNCTIONS:
+//======================
+//======================================================================
+// Extracts srcvec[srcbitpos] and places it in return int (bit[0])
+int bit_extract ( int srcvec,         // source word (to extract)
+                  int srcbitpos       // source bit position
+                )
+{
+    return(((1 << srcbitpos) & srcvec) >> srcbitpos);
+}
+//======================================================================
+// Inserts srcvec[0] into dstvec[dstbitpos] (without affecting other bits)
+int bit_insert ( int srcvec,           // srcvec[0] = bit to be inserted
+                 int dstbitpos,        // Bit position to insert into returned int
+                 int dstvec            // dstvec (destination vector)
+               )
+{
+    return((srcvec << dstbitpos) | dstvec);      // Shift bit to insert into bit position/OR with accumulated number
+}
+//======================================================================
+
+int rld_csr_config_generate(llm_descriptor_t *llm_desc_ptr, rldram_csr_config_t *cfg_ptr)
+{
+    char *addr_rld0_fb_str;
+    char *addr_rld0_bb_str;
+    char *addr_rld1_fb_str;
+    char *addr_rld1_bb_str;
+    int eclk_ps;
+    int mtype = 0;                           // MTYPE (0: RLDRAM/1: FCRAM
+    int trcmin = 20;                         // tRC(min) - from RLDRAM data sheet
+    int trc_cyc;                             // TRC(cyc)
+    int trc_mod;
+    int trl_cyc;                             // TRL(cyc)
+    int twl_cyc;                             // TWL(cyc)
+    int tmrsc_cyc = 6;                       // tMRSC(cyc)  [2-7]
+    int mclk_ps;                             // DFA Memory Clock(in ps) = 2x eclk
+    int rldcfg = 99;                         // RLDRAM-II CFG (1,2,3)
+    int mrs_odt = 0;                         // RLDRAM MRS A[9]=ODT (default)
+    int mrs_impmatch = 0;                    // RLDRAM MRS A[8]=Impedance Matching (default)
+    int mrs_dllrst = 1;                      // RLDRAM MRS A[7]=DLL Reset (default)
+    uint32_t mrs_dat;
+    int mrs_dat_p0bunk0 = 0;                 // MRS Register Data After Address Map (for Port0 Bunk0)
+    int mrs_dat_p0bunk1 = 0;                 // MRS Register Data After Address Map (for Port0 Bunk1)
+    int mrs_dat_p1bunk0 = 0;                 // MRS Register Data After Address Map (for Port1 Bunk0)
+    int mrs_dat_p1bunk1 = 0;                 // MRS Register Data After Address Map (for Port1 Bunk1)
+    int p0_ena = 0;                          // DFA Port#0 Enabled
+    int p1_ena = 0;                          // DFA Port#1 Enabled
+    int memport = 0;                       // Memory(MB) per Port [MAX=512]
+    int membunk;                             // Memory(MB) per Bunk
+    int bunkport = 0;                        // Bunks/Port [1/2]
+    int pbunk = 0;                               // Physical Bunk(or Rank) encoding for address bit
+    int tref_ms = 32;                        // tREF(ms) (RLDRAM-II overall device refresh interval
+    int trefi_ns;                            // tREFI(ns) = tREF(ns)/#rows/bank
+    int rows = 8;                            // #rows/bank (K) typically 8K
+    int ref512int;
+    int ref512mod;
+    int tskw_cyc = 0;
+    int fprch = 1;
+    int bprch = 0;
+    int dfa_memcfg0_base = 0;
+    int dfa_memcfg1_base = 0;
+    int tbl = 1;                             // tBL (1: 2-burst /2: 4-burst)
+    int rw_dly;
+    int wr_dly;
+    int r2r = 1;
+    int sil_lat = 1;
+    int clkdiv = 2;  /* CN38XX is fixed at 2, CN58XX supports 2,3,4 */
+    int clkdiv_enc = 0x0;  /* Encoded clock divisor, only used for CN58XX */
+
+    if (!llm_desc_ptr)
+        return -1;
+
+    /* Setup variables from descriptor */
+
+    addr_rld0_fb_str = llm_desc_ptr->addr_rld0_fb_str;
+    addr_rld0_bb_str = llm_desc_ptr->addr_rld0_bb_str;
+    addr_rld1_fb_str = llm_desc_ptr->addr_rld1_fb_str;
+    addr_rld1_bb_str = llm_desc_ptr->addr_rld1_bb_str;
+
+    p0_ena = !!llm_desc_ptr->rld1_bunks;        // NOTE: P0 == RLD1
+    p1_ena = !!llm_desc_ptr->rld0_bunks;        // NOTE: P1 == RLD0
+
+    // Massage the code, so that if the user had imbalanced memory per-port (or imbalanced bunks/port), we
+    // at least try to configure 'workable' memory.
+    if (p0_ena && p1_ena)  // IF BOTH PORTS Enabled (imbalanced memory), select smaller of BOTH
+    {
+        memport = MIN(llm_desc_ptr->rld0_mbytes, llm_desc_ptr->rld1_mbytes);
+        bunkport = MIN(llm_desc_ptr->rld0_bunks, llm_desc_ptr->rld1_bunks);
+    }
+    else if (p0_ena) // P0=RLD1 Enabled
+    {
+        memport = llm_desc_ptr->rld1_mbytes;
+        bunkport = llm_desc_ptr->rld1_bunks;
+    }
+    else if (p1_ena) // P1=RLD0 Enabled
+    {
+        memport = llm_desc_ptr->rld0_mbytes;
+        bunkport = llm_desc_ptr->rld0_bunks;
+    }
+    else
+        return -1;
+
+    uint32_t eclk_mhz = llm_desc_ptr->cpu_hz/1000000;
+
+
+
+    /* Tweak skew based on cpu clock */
+    if (eclk_mhz <= 367)
+    {
+        tskw_cyc = 0;
+    }
+    else
+    {
+        tskw_cyc = 1;
+    }
+
+    /* Determine clock divider ratio (only required for CN58XX) */
+    if (OCTEON_IS_MODEL(OCTEON_CN58XX))
+    {
+        uint32_t max_llm_clock_mhz = llm_desc_ptr->max_rld_clock_mhz;
+        if (!max_llm_clock_mhz)
+        {
+            max_llm_clock_mhz = 400;  /* Default to 400 MHz */
+            cvmx_dprintf("Warning, using default max_rld_clock_mhz of: %lu MHz\n", (unsigned long)max_llm_clock_mhz);
+        }
+
+        /* Compute the divisor, and round up */
+        clkdiv = eclk_mhz/max_llm_clock_mhz;
+        if (clkdiv * max_llm_clock_mhz < eclk_mhz)
+            clkdiv++;
+
+        if (clkdiv > 4)
+        {
+            cvmx_dprintf("ERROR: CN58XX LLM clock divisor out of range\n");
+            goto TERMINATE;
+        }
+        if (clkdiv < 2)
+            clkdiv = 2;
+
+        cvmx_dprintf("Using llm clock divisor: %d, llm clock is: %lu MHz\n", clkdiv, (unsigned long)eclk_mhz/clkdiv);
+        /* Translate divisor into bit encoding for register */
+        /* 0 -> div 2
+        ** 1 -> reserved
+        ** 2 -> div 3
+        ** 3 -> div 4
+        */
+        if (clkdiv == 2)
+            clkdiv_enc = 0;
+        else
+            clkdiv_enc = clkdiv - 1;
+
+    /* Odd divisor needs sil_lat to be 2 */
+        if (clkdiv == 0x3)
+            sil_lat = 2;
+
+        /* Increment tskw for high clock speeds */
+        if ((unsigned long)eclk_mhz/clkdiv >= 375)
+            tskw_cyc += 1;
+    }
+
+    eclk_ps = (1000000+(eclk_mhz-1)) / eclk_mhz;  // round up if nonzero remainder
+    //=======================================================================
+
+    //=======================================================================
+    // Now, Query User for DFA Memory Type
+    if (mtype != 0)
+    {
+        goto TERMINATE;         // Complete this code for FCRAM usage on N3K-P2
+    }
+    //=======================================================================
+    // Query what the tRC(min) value is from the data sheets
+    //=======================================================================
+    // Now determine the Best CFG based on Memory clock(ps) and tRCmin(ns)
+    mclk_ps = eclk_ps * clkdiv;
+    trc_cyc = ((trcmin * 1000)/mclk_ps);
+    trc_mod = ((trcmin * 1000) % mclk_ps);
+    // If remainder exists, bump up to the next integer multiple
+    if (trc_mod != 0)
+    {
+        trc_cyc = trc_cyc + 1;
+    }
+    // If tRC is now ODD, then bump it to the next EVEN integer (RLDRAM-II does not support odd tRC values at this time).
+    if (trc_cyc & 1)
+    {
+        trc_cyc = trc_cyc + 1;           // Bump it to an even #
+    }
+    // RLDRAM CFG Range Check: If the computed trc_cyc is less than 4, then set it to min CFG1 [tRC=4]
+    if (trc_cyc < 4)
+    {
+        trc_cyc = 4;             // If computed trc_cyc < 4 then clamp to 4
+    }
+    else if (trc_cyc > 8)
+    {    // If the computed trc_cyc > 8, then report an error (because RLDRAM cannot support a tRC>8
+        goto TERMINATE;
+    }
+    // Assuming all is ok(up to here)
+    // At this point the tRC_cyc has been clamped  between 4 and 8 (and is even), So it can only be 4,6,8 which are
+    // the RLDRAM valid CFG range values.
+    trl_cyc = trc_cyc;                 // tRL = tRC (for RLDRAM=II)
+    twl_cyc = trl_cyc + 1;             // tWL = tRL + 1 (for RLDRAM-II)
+    // NOTE: RLDRAM-II (as of 4/25/05) only have 3 supported CFG encodings:
+    if (trc_cyc == 4)
+    {
+        rldcfg = 1;           // CFG #1 (tRL=4/tRC=4/tWL=5)
+    }
+    else if (trc_cyc == 6)
+    {
+        rldcfg = 2;           // CFG #2 (tRL=6/tRC=6/tWL=7)
+    }
+    else if (trc_cyc == 8)
+    {
+        rldcfg = 3;           // CFG #3 (tRL=8/tRC=8/tWL=9)
+    }
+    else
+    {
+        goto TERMINATE;
+    }
+    //=======================================================================
+    mrs_dat = ( (mrs_odt << 9) | (mrs_impmatch << 8) | (mrs_dllrst << 7) | rldcfg );
+    //=======================================================================
+    // If there is only a single bunk, then skip over address mapping queries (which are not required)
+    if (bunkport == 1)
+    {
+        goto CALC_PBUNK;
+    }
+
+    /* Process the address mappings */
+    /* Note that that RLD0 pins corresponds to Port#1, and
+    **                RLD1 pins corresponds to Port#0.
+    */
+    mrs_dat_p1bunk0 = process_address_map_str(mrs_dat, addr_rld0_fb_str);
+    mrs_dat_p1bunk1 = process_address_map_str(mrs_dat, addr_rld0_bb_str);
+    mrs_dat_p0bunk0 = process_address_map_str(mrs_dat, addr_rld1_fb_str);
+    mrs_dat_p0bunk1 = process_address_map_str(mrs_dat, addr_rld1_bb_str);
+
+
+    //=======================================================================
+    CALC_PBUNK:
+    // Determine the PBUNK field (based on Memory/Bunk)
+    // This determines the addr bit used to distinguish when crossing a bunk.
+    // NOTE: For RLDRAM, the bunk bit is extracted from 'a' programmably selected high
+    // order addr bit. [linear address per-bunk]
+    if (bunkport == 2)
+    {
+        membunk = (memport / 2);
+    }
+    else
+    {
+        membunk = memport;
+    }
+    if (membunk == 16)
+    {       // 16MB/bunk MA[19]
+        pbunk = 0;
+    }
+    else if (membunk == 32)
+    {  // 32MB/bunk MA[20]
+        pbunk = 1;
+    }
+    else if (membunk == 64)
+    {  // 64MB/bunk MA[21]
+        pbunk = 2;
+    }
+    else if (membunk == 128)
+    { // 128MB/bunk MA[22]
+        pbunk = 3;
+    }
+    else if (membunk == 256)
+    { // 256MB/bunk MA[23]
+        pbunk = 4;
+    }
+    else if (membunk == 512)
+    { // 512MB/bunk
+        if (cvmx_octeon_is_pass1() == 1)
+        {
+            goto TERMINATE;
+        }
+    }
+    //=======================================================================
+    //=======================================================================
+    //=======================================================================
+    // Now determine N3K REFINT
+    trefi_ns = (tref_ms * 1000 * 1000) / (rows * 1024);
+    ref512int = ((trefi_ns * 1000) / (eclk_ps * 512));
+    ref512mod = ((trefi_ns * 1000) % (eclk_ps * 512));
+    //=======================================================================
+    // Ask about tSKW
+#if 0
+    if (tskw_ps ==  0)
+    {
+        tskw_cyc = 0;
+    }
+    else
+    { // CEILING function
+        tskw_cyc = (tskw_ps / eclk_ps);
+        tskw_mod = (tskw_ps % eclk_ps);
+        if (tskw_mod != 0)
+        {  // If there's a remainder - then bump to next (+1)
+            tskw_cyc = tskw_cyc + 1;
+        }
+    }
+#endif
+    if (tskw_cyc > 3)
+    {
+        goto TERMINATE;
+    }
+
+    tbl = 1;        // BLEN=2 (ALWAYs for RLDRAM)
+    //=======================================================================
+    // RW_DLY = (ROUND_UP{[[(TRL+TBL)*2 + tSKW + BPRCH] + 1] / 2}) - tWL
+    rw_dly = ((((trl_cyc + tbl) * 2 + tskw_cyc + bprch) + 1) / 2);
+    if (rw_dly & 1)
+    { // If it's ODD then round up
+        rw_dly = rw_dly + 1;
+    }
+    rw_dly = rw_dly - twl_cyc +1 ;
+    if (rw_dly < 0)
+    { // range check - is it positive
+        goto TERMINATE;
+    }
+    //=======================================================================
+    // WR_DLY = (ROUND_UP[[(tWL + tBL)*2 - tSKW + FPRCH] / 2]) - tRL
+    wr_dly = (((twl_cyc + tbl) * 2 - tskw_cyc + fprch) / 2);
+    if (wr_dly & 1)
+    { // If it's ODD then round up
+        wr_dly = wr_dly + 1;
+    }
+    wr_dly = wr_dly - trl_cyc + 1;
+    if (wr_dly < 0)
+    { // range check - is it positive
+        goto TERMINATE;
+    }
+
+
+    dfa_memcfg0_base = 0;
+    dfa_memcfg0_base = ( p0_ena |
+                         (p1_ena << 1) |
+                         (mtype << 3) |
+                         (sil_lat << 4) |
+                         (rw_dly << 6) |
+                         (wr_dly << 10) |
+                         (fprch << 14) |
+                         (bprch << 16) |
+                         (0 << 18) |         // BLEN=0(2-burst for RLDRAM)
+                         (pbunk << 19) |
+                         (r2r << 22) |       // R2R=1
+    			 (clkdiv_enc << 28 )
+                       );
+
+
+    dfa_memcfg1_base = 0;
+    dfa_memcfg1_base = ( ref512int |
+                         (tskw_cyc << 4) |
+                         (trl_cyc << 8) |
+                         (twl_cyc << 12) |
+                         (trc_cyc << 16) |
+                         (tmrsc_cyc << 20)
+                       );
+
+
+
+
+    cfg_ptr->dfa_memcfg0_base = dfa_memcfg0_base;
+    cfg_ptr->dfa_memcfg1_base = dfa_memcfg1_base;
+    cfg_ptr->mrs_dat_p0bunk0 =  mrs_dat_p0bunk0;
+    cfg_ptr->mrs_dat_p1bunk0 =  mrs_dat_p1bunk0;
+    cfg_ptr->mrs_dat_p0bunk1 =  mrs_dat_p0bunk1;
+    cfg_ptr->mrs_dat_p1bunk1 =  mrs_dat_p1bunk1;
+    cfg_ptr->p0_ena =           p0_ena;
+    cfg_ptr->p1_ena =           p1_ena;
+    cfg_ptr->bunkport =         bunkport;
+    //=======================================================================
+
+    return(0);
+    TERMINATE:
+    return(-1);
+
+}
+
+
+
+static uint32_t process_address_map_str(uint32_t mrs_dat, char *addr_str)
+{
+    int count = 0;
+    int amap [23];
+    uint32_t new_mrs_dat = 0;
+
+//    cvmx_dprintf("mrs_dat: 0x%x, str: %x\n", mrs_dat, addr_str);
+    char *charptr = strtok(addr_str," ");
+    while ((charptr != NULL) & (count <= 22))
+    {
+        amap[22-count] = atoi(charptr);         // Assign the AMAP Array
+        charptr = strtok(NULL," ");             // Get Next char string (which represents next addr bit mapping)
+        count++;
+    }
+    // Now do the bit swap of MRSDAT (based on address mapping)
+    uint32_t mrsdat_bit;
+    for (count=0;count<=22;count++)
+    {
+        mrsdat_bit = bit_extract(mrs_dat, count);
+        new_mrs_dat = bit_insert(mrsdat_bit, amap[count], new_mrs_dat);
+    }
+
+    return new_mrs_dat;
+}
+
+
+//#define PRINT_LLM_CONFIG
+#ifdef PRINT_LLM_CONFIG
+#define ll_printf printf
+#else
+#define ll_printf(...)
+#define cvmx_csr_db_decode(...)
+#endif
+
+static void cn31xx_dfa_memory_init(void)
+{
+    if (OCTEON_IS_MODEL(OCTEON_CN31XX))
+    {
+        cvmx_dfa_ddr2_cfg_t  dfaCfg;
+        cvmx_dfa_eclkcfg_t   dfaEcklCfg;
+        cvmx_dfa_ddr2_addr_t dfaAddr;
+        cvmx_dfa_ddr2_tmg_t  dfaTmg;
+        cvmx_dfa_ddr2_pll_t  dfaPll;
+        int mem_freq_hz = 533*1000000;
+        int ref_freq_hz = cvmx_sysinfo_get()->dfa_ref_clock_hz;
+        if (!ref_freq_hz)
+            ref_freq_hz = 33*1000000;
+
+        cvmx_dprintf ("Configuring DFA memory for %d MHz operation.\n",mem_freq_hz/1000000);
+
+          /* Turn on the DFA memory port. */
+        dfaCfg.u64 = cvmx_read_csr (CVMX_DFA_DDR2_CFG);
+        dfaCfg.s.prtena = 1;
+        cvmx_write_csr (CVMX_DFA_DDR2_CFG, dfaCfg.u64);
+
+          /* Start the PLL alignment sequence */
+        dfaPll.u64 = 0;
+        dfaPll.s.pll_ratio  = mem_freq_hz/ref_freq_hz         /*400Mhz / 33MHz*/;
+        dfaPll.s.pll_div2   = 1              /*400 - 1 */;
+        dfaPll.s.pll_bypass = 0;
+        cvmx_write_csr (CVMX_DFA_DDR2_PLL, dfaPll.u64);
+
+        dfaPll.s.pll_init = 1;
+        cvmx_write_csr (CVMX_DFA_DDR2_PLL, dfaPll.u64);
+
+        cvmx_wait (RLD_INIT_DELAY); //want 150uS
+        dfaPll.s.qdll_ena = 1;
+        cvmx_write_csr (CVMX_DFA_DDR2_PLL, dfaPll.u64);
+
+        cvmx_wait (RLD_INIT_DELAY); //want 10us
+        dfaEcklCfg.u64 = 0;
+        dfaEcklCfg.s.dfa_frstn = 1;
+        cvmx_write_csr (CVMX_DFA_ECLKCFG, dfaEcklCfg.u64);
+
+          /* Configure the DFA Memory */
+        dfaCfg.s.silo_hc = 1 /*400 - 1 */;
+        dfaCfg.s.silo_qc = 0 /*400 - 0 */;
+        dfaCfg.s.tskw    = 1 /*400 - 1 */;
+        dfaCfg.s.ref_int = 0x820 /*533 - 0x820  400 - 0x618*/;
+        dfaCfg.s.trfc    = 0x1A  /*533 - 0x23   400 - 0x1A*/;
+        dfaCfg.s.fprch   = 0; /* 1 more conservative*/
+        dfaCfg.s.bprch   = 0; /* 1 */
+        cvmx_write_csr (CVMX_DFA_DDR2_CFG, dfaCfg.u64);
+
+        dfaEcklCfg.u64 = cvmx_read_csr (CVMX_DFA_ECLKCFG);
+        dfaEcklCfg.s.maxbnk = 1;
+        cvmx_write_csr (CVMX_DFA_ECLKCFG, dfaEcklCfg.u64);
+
+        dfaAddr.u64 = cvmx_read_csr (CVMX_DFA_DDR2_ADDR);
+        dfaAddr.s.num_cols    = 0x1;
+        dfaAddr.s.num_colrows = 0x2;
+        dfaAddr.s.num_rnks    = 0x1;
+        cvmx_write_csr (CVMX_DFA_DDR2_ADDR, dfaAddr.u64);
+
+        dfaTmg.u64 =  cvmx_read_csr (CVMX_DFA_DDR2_TMG);
+        dfaTmg.s.ddr2t    = 0;
+        dfaTmg.s.tmrd     = 0x2;
+        dfaTmg.s.caslat   = 0x4 /*400 - 0x3, 500 - 0x4*/;
+        dfaTmg.s.pocas    = 0;
+        dfaTmg.s.addlat   = 0;
+        dfaTmg.s.trcd     = 4   /*400 - 3, 500 - 4*/;
+        dfaTmg.s.trrd     = 2;
+        dfaTmg.s.tras     = 0xB /*400 - 8, 500 - 0xB*/;
+        dfaTmg.s.trp      = 4   /*400 - 3, 500 - 4*/;
+        dfaTmg.s.twr      = 4   /*400 - 3, 500 - 4*/;
+        dfaTmg.s.twtr     = 2   /*400 - 2 */;
+        dfaTmg.s.tfaw     = 0xE /*400 - 0xA, 500 - 0xE*/;
+        dfaTmg.s.r2r_slot = 0;
+        dfaTmg.s.dic      = 0;  /*400 - 0 */
+        dfaTmg.s.dqsn_ena = 0;
+        dfaTmg.s.odt_rtt  = 0;
+        cvmx_write_csr (CVMX_DFA_DDR2_TMG, dfaTmg.u64);
+
+          /* Turn on the DDR2 interface and wait a bit for the hardware to setup. */
+        dfaCfg.s.init = 1;
+        cvmx_write_csr (CVMX_DFA_DDR2_CFG, dfaCfg.u64);
+        cvmx_wait(RLD_INIT_DELAY); // want at least 64K cycles
+    }
+}
+
+void write_rld_cfg(rldram_csr_config_t *cfg_ptr)
+{
+    cvmx_dfa_memcfg0_t    memcfg0;
+    cvmx_dfa_memcfg2_t    memcfg2;
+
+    memcfg0.u64 = cfg_ptr->dfa_memcfg0_base;
+
+    if ((OCTEON_IS_MODEL(OCTEON_CN38XX) || OCTEON_IS_MODEL(OCTEON_CN58XX)))
+    {
+        uint32_t dfa_memcfg0;
+
+        if (OCTEON_IS_MODEL (OCTEON_CN58XX)) {
+	      // Set RLDQK90_RST and RDLCK_RST to reset all three DLLs.
+	    memcfg0.s.rldck_rst    = 1;
+	    memcfg0.s.rldqck90_rst = 1;
+            cvmx_write_csr(CVMX_DFA_MEMCFG0, memcfg0.u64);
+            ll_printf("CVMX_DFA_MEMCFG0: 0x%08x  clk/qk90 reset\n", (uint32_t) memcfg0.u64);
+            cvmx_csr_db_decode(cvmx_get_proc_id(), CVMX_DFA_MEMCFG0 & ~(1ull<<63), memcfg0.u64);
+
+	      // Clear RDLCK_RST while asserting RLDQK90_RST to bring RLDCK DLL out of reset.
+	    memcfg0.s.rldck_rst    = 0;
+	    memcfg0.s.rldqck90_rst = 1;
+            cvmx_write_csr(CVMX_DFA_MEMCFG0, memcfg0.u64);
+            cvmx_wait(4000000);  /* Wait  */
+            ll_printf("CVMX_DFA_MEMCFG0: 0x%08x  qk90 reset\n", (uint32_t) memcfg0.u64);
+            cvmx_csr_db_decode(cvmx_get_proc_id(), CVMX_DFA_MEMCFG0 & ~(1ull<<63), memcfg0.u64);
+
+	      // Clear both RDLCK90_RST and RLDQK90_RST to bring the RLDQK90 DLL out of reset.
+	    memcfg0.s.rldck_rst    = 0;
+	    memcfg0.s.rldqck90_rst = 0;
+	    cvmx_write_csr(CVMX_DFA_MEMCFG0, memcfg0.u64);
+            cvmx_wait(4000000);  /* Wait  */
+            ll_printf("CVMX_DFA_MEMCFG0: 0x%08x  DLL out of reset\n", (uint32_t) memcfg0.u64);
+            cvmx_csr_db_decode(cvmx_get_proc_id(), CVMX_DFA_MEMCFG0 & ~(1ull<<63), memcfg0.u64);
+	}
+
+        //=======================================================================
+        // Now print out the sequence of events:
+        cvmx_write_csr(CVMX_DFA_MEMCFG0, cfg_ptr->dfa_memcfg0_base);
+        ll_printf("CVMX_DFA_MEMCFG0: 0x%08x  port enables\n", cfg_ptr->dfa_memcfg0_base);
+        cvmx_csr_db_decode(cvmx_get_proc_id(), CVMX_DFA_MEMCFG0 & ~(1ull<<63), cfg_ptr->dfa_memcfg0_base);
+        cvmx_wait(4000000);  /* Wait  */
+
+        cvmx_write_csr(CVMX_DFA_MEMCFG1, cfg_ptr->dfa_memcfg1_base);
+        ll_printf("CVMX_DFA_MEMCFG1: 0x%08x\n", cfg_ptr->dfa_memcfg1_base);
+        cvmx_csr_db_decode(cvmx_get_proc_id(), CVMX_DFA_MEMCFG1 & ~(1ull<<63), cfg_ptr->dfa_memcfg1_base);
+
+        if (cfg_ptr->p0_ena ==1)
+        {
+            cvmx_write_csr(CVMX_DFA_MEMRLD,  cfg_ptr->mrs_dat_p0bunk0);
+            ll_printf("CVMX_DFA_MEMRLD : 0x%08x  p0_ena memrld\n", cfg_ptr->mrs_dat_p0bunk0);
+            cvmx_csr_db_decode(cvmx_get_proc_id(), CVMX_DFA_MEMRLD & ~(1ull<<63), cfg_ptr->mrs_dat_p0bunk0);
+
+            dfa_memcfg0 = ( cfg_ptr->dfa_memcfg0_base |
+                            (1 << 23) |   // P0_INIT
+                            (1 << 25)     // BUNK_INIT[1:0]=Bunk#0
+                          );
+
+            cvmx_write_csr(CVMX_DFA_MEMCFG0, dfa_memcfg0);
+            ll_printf("CVMX_DFA_MEMCFG0: 0x%08x  p0_init/bunk_init\n", dfa_memcfg0);
+            cvmx_csr_db_decode(cvmx_get_proc_id(), CVMX_DFA_MEMCFG0 & ~(1ull<<63), dfa_memcfg0);
+            cvmx_wait(RLD_INIT_DELAY);
+            ll_printf("Delay.....\n");
+            cvmx_write_csr(CVMX_DFA_MEMCFG0, cfg_ptr->dfa_memcfg0_base);
+            ll_printf("CVMX_DFA_MEMCFG0: 0x%08x  back to base\n", cfg_ptr->dfa_memcfg0_base);
+            cvmx_csr_db_decode(cvmx_get_proc_id(), CVMX_DFA_MEMCFG0 & ~(1ull<<63), cfg_ptr->dfa_memcfg0_base);
+        }
+
+        if (cfg_ptr->p1_ena ==1)
+        {
+            cvmx_write_csr(CVMX_DFA_MEMRLD,  cfg_ptr->mrs_dat_p1bunk0);
+            ll_printf("CVMX_DFA_MEMRLD : 0x%08x  p1_ena memrld\n", cfg_ptr->mrs_dat_p1bunk0);
+            cvmx_csr_db_decode(cvmx_get_proc_id(), CVMX_DFA_MEMRLD & ~(1ull<<63), cfg_ptr->mrs_dat_p1bunk0);
+
+            dfa_memcfg0 = ( cfg_ptr->dfa_memcfg0_base |
+                            (1 << 24) |   // P1_INIT
+                            (1 << 25)     // BUNK_INIT[1:0]=Bunk#0
+                          );
+            cvmx_write_csr(CVMX_DFA_MEMCFG0, dfa_memcfg0);
+            ll_printf("CVMX_DFA_MEMCFG0: 0x%08x  p1_init/bunk_init\n", dfa_memcfg0);
+            cvmx_csr_db_decode(cvmx_get_proc_id(), CVMX_DFA_MEMCFG0 & ~(1ull<<63), dfa_memcfg0);
+            cvmx_wait(RLD_INIT_DELAY);
+            ll_printf("Delay.....\n");
+            cvmx_write_csr(CVMX_DFA_MEMCFG0, cfg_ptr->dfa_memcfg0_base);
+            ll_printf("CVMX_DFA_MEMCFG0: 0x%08x  back to base\n", cfg_ptr->dfa_memcfg0_base);
+            cvmx_csr_db_decode(cvmx_get_proc_id(), CVMX_DFA_MEMCFG0 & ~(1ull<<63), cfg_ptr->dfa_memcfg0_base);
+	}
+
+        // P0 Bunk#1
+        if ((cfg_ptr->p0_ena ==1) && (cfg_ptr->bunkport == 2))
+        {
+            cvmx_write_csr(CVMX_DFA_MEMRLD,  cfg_ptr->mrs_dat_p0bunk1);
+            ll_printf("CVMX_DFA_MEMRLD : 0x%08x  p0_ena memrld\n", cfg_ptr->mrs_dat_p0bunk1);
+            cvmx_csr_db_decode(cvmx_get_proc_id(), CVMX_DFA_MEMRLD & ~(1ull<<63), cfg_ptr->mrs_dat_p0bunk1);
+
+            dfa_memcfg0 = ( cfg_ptr->dfa_memcfg0_base |
+                            (1 << 23) |   // P0_INIT
+                            (2 << 25)     // BUNK_INIT[1:0]=Bunk#1
+                          );
+            cvmx_write_csr(CVMX_DFA_MEMCFG0, dfa_memcfg0);
+            ll_printf("CVMX_DFA_MEMCFG0: 0x%08x  p0_init/bunk_init\n", dfa_memcfg0);
+            cvmx_csr_db_decode(cvmx_get_proc_id(), CVMX_DFA_MEMCFG0 & ~(1ull<<63), dfa_memcfg0);
+            cvmx_wait(RLD_INIT_DELAY);
+            ll_printf("Delay.....\n");
+
+            if (cfg_ptr->p1_ena == 1)
+            { // Re-arm Px_INIT if P1-B1 init is required
+                cvmx_write_csr(CVMX_DFA_MEMCFG0, cfg_ptr->dfa_memcfg0_base);
+                ll_printf("CVMX_DFA_MEMCFG0: 0x%08x  px_init rearm\n", cfg_ptr->dfa_memcfg0_base);
+                cvmx_csr_db_decode(cvmx_get_proc_id(), CVMX_DFA_MEMCFG0 & ~(1ull<<63), cfg_ptr->dfa_memcfg0_base);
+            }
+        }
+
+        if ((cfg_ptr->p1_ena == 1) && (cfg_ptr->bunkport == 2))
+        {
+            cvmx_write_csr(CVMX_DFA_MEMRLD,  cfg_ptr->mrs_dat_p1bunk1);
+            ll_printf("CVMX_DFA_MEMRLD : 0x%08x  p1_ena memrld\n", cfg_ptr->mrs_dat_p1bunk1);
+            cvmx_csr_db_decode(cvmx_get_proc_id(), CVMX_DFA_MEMRLD & ~(1ull<<63), cfg_ptr->mrs_dat_p1bunk1);
+
+            dfa_memcfg0 = ( cfg_ptr->dfa_memcfg0_base |
+                            (1 << 24) |   // P1_INIT
+                            (2 << 25)     // BUNK_INIT[1:0]=10
+                          );
+            cvmx_write_csr(CVMX_DFA_MEMCFG0, dfa_memcfg0);
+            ll_printf("CVMX_DFA_MEMCFG0: 0x%08x  p1_init/bunk_init\n", dfa_memcfg0);
+            cvmx_csr_db_decode(cvmx_get_proc_id(), CVMX_DFA_MEMCFG0 & ~(1ull<<63), dfa_memcfg0);
+        }
+        cvmx_wait(4000000);  // 1/100S, 0.01S, 10mS
+        ll_printf("Delay.....\n");
+
+          /* Enable bunks */
+        dfa_memcfg0 = cfg_ptr->dfa_memcfg0_base |((cfg_ptr->bunkport >= 1) << 25) | ((cfg_ptr->bunkport == 2) << 26);
+        cvmx_write_csr(CVMX_DFA_MEMCFG0, dfa_memcfg0);
+        ll_printf("CVMX_DFA_MEMCFG0: 0x%08x  enable bunks\n", dfa_memcfg0);
+        cvmx_csr_db_decode(cvmx_get_proc_id(), CVMX_DFA_MEMCFG0 & ~(1ull<<63), dfa_memcfg0);
+        cvmx_wait(RLD_INIT_DELAY);
+        ll_printf("Delay.....\n");
+
+          /* Issue a Silo reset by toggling SILRST in memcfg2. */
+        memcfg2.u64 = cvmx_read_csr (CVMX_DFA_MEMCFG2);
+        memcfg2.s.silrst = 1;
+	cvmx_write_csr (CVMX_DFA_MEMCFG2, memcfg2.u64);
+        ll_printf("CVMX_DFA_MEMCFG2: 0x%08x  silo reset start\n", (uint32_t) memcfg2.u64);
+        memcfg2.s.silrst = 0;
+	cvmx_write_csr (CVMX_DFA_MEMCFG2, memcfg2.u64);
+        ll_printf("CVMX_DFA_MEMCFG2: 0x%08x  silo reset done\n", (uint32_t) memcfg2.u64);
+    }
+}
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-llm.h b/arch/mips/cavium-octeon/executive/cvmx-llm.h
new file mode 100644
index 0000000..c7a8be0
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-llm.h
@@ -0,0 +1,409 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * interface to the low latency DRAM
+ *
+ * <hr>$Revision: 32636 $<hr>
+ *
+ */
+
+#ifndef __CVMX_LLM_H__
+#define __CVMX_LLM_H__
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#define ENABLE_DEPRECATED   /* Set to enable the old 18/36 bit names */
+
+typedef enum
+{
+    CVMX_LLM_REPLICATION_NONE = 0,
+    CVMX_LLM_REPLICATION_2X = 1, // on both interfaces, or 2x if only one interface
+    CVMX_LLM_REPLICATION_4X = 2, // both interfaces, 2x, or 4x if only one interface
+    CVMX_LLM_REPLICATION_8X = 3, // both interfaces, 4x,  or 8x if only one interface
+} cvmx_llm_replication_t;
+
+/**
+ * This structure defines the address used to the low-latency memory.
+ * This address format is used for both loads and stores.
+ */
+typedef union
+{
+    uint64_t u64;
+    struct
+    {
+        uint64_t                mbz     :30;
+        cvmx_llm_replication_t  repl    : 2;
+        uint64_t                address :32; // address<1:0> mbz, address<31:30> mbz
+    } s;
+} cvmx_llm_address_t;
+
+/**
+ * This structure defines the data format in the low-latency memory
+ */
+typedef union
+{
+    uint64_t u64;
+
+    /**
+     * this format defines the format returned on a load
+     *   a load returns the 32/36-bits in memory, plus xxor = even_parity(dat<35:0>)
+     *   typically, dat<35> = parity(dat<34:0>), so the xor bit directly indicates parity error
+     *   Note that the data field size is 36 bits on the 36XX/38XX, and 32 bits on the 31XX
+     */
+    struct
+    {
+        uint64_t                       mbz1  :27;
+        uint64_t                       xxor  : 1;
+        uint64_t                       mbz   : 4;
+        uint64_t                       dat   :32;
+    } cn31xx;
+
+    struct
+    {
+        uint64_t                       mbz   :27;
+        uint64_t                       xxor  : 1;
+        uint64_t                       dat   :36;
+    } s;
+
+    /**
+     *  This format defines what should be used if parity is desired.  Hardware returns
+     *  the XOR of all the bits in the 36/32 bit data word, so for parity software must use
+     * one of the data field bits as a parity bit.
+     */
+    struct cn31xx_par_struct
+    {
+        uint64_t                       mbz   :32;
+        uint64_t                       par   : 1;
+        uint64_t                       dat   :31;
+    } cn31xx_par;
+    struct cn38xx_par_struct
+    {
+        uint64_t                       mbz   :28;
+        uint64_t                       par   : 1;
+        uint64_t                       dat   :35;
+    } cn38xx_par;
+#if !OCTEON_IS_COMMON_BINARY()
+#if OCTEON_IS_MODEL(OCTEON_CN31XX)
+    struct cn31xx_par_struct spar;
+#else
+    struct cn38xx_par_struct spar;
+#endif
+#endif
+} cvmx_llm_data_t;
+
+#define CVMX_LLM_NARROW_DATA_WIDTH  ((OCTEON_IS_MODEL(OCTEON_CN31XX)) ? 32 : 36)
+
+/**
+ * Calculate the parity value of a number
+ *
+ * @param value
+ * @return parity value
+ */
+static inline uint64_t cvmx_llm_parity(uint64_t value)
+{
+    uint64_t result;
+    CVMX_DPOP(result, value);
+    return result;
+}
+
+
+/**
+ * Calculate the ECC needed for 36b LLM mode
+ *
+ * @param value
+ * @return  ECC value
+ */
+static inline int cvmx_llm_ecc(uint64_t value)
+{
+    /* FIXME: This needs a re-write */
+    static const uint32_t ecc_code_29[7] = {
+        0x08962595,
+        0x112a4aaa,
+        0x024c934f,
+        0x04711c73,
+        0x0781e07c,
+        0x1801ff80,
+        0x1ffe0000};
+    uint64_t pop0, pop1, pop2, pop3, pop4, pop5, pop6;
+
+    pop0 = ecc_code_29[0];
+    pop1 = ecc_code_29[1];
+    pop2 = ecc_code_29[2];
+    pop0 &= value;
+    pop3 = ecc_code_29[3];
+    CVMX_DPOP(pop0, pop0);
+    pop4 = ecc_code_29[4];
+    pop1 &= value;
+    CVMX_DPOP(pop1, pop1);
+    pop2 &= value;
+    pop5 = ecc_code_29[5];
+    CVMX_DPOP(pop2, pop2);
+    pop6 = ecc_code_29[6];
+    pop3 &= value;
+    CVMX_DPOP(pop3, pop3);
+    pop4 &= value;
+    CVMX_DPOP(pop4, pop4);
+    pop5 &= value;
+    CVMX_DPOP(pop5, pop5);
+    pop6 &= value;
+    CVMX_DPOP(pop6, pop6);
+
+    return((pop6&1)<<6) | ((pop5&1)<<5) | ((pop4&1)<<4) | ((pop3&1)<<3) | ((pop2&1)<<2) | ((pop1&1)<<1) | (pop0&1);
+}
+
+
+#ifdef ENABLE_DEPRECATED
+/* These macros are provided to provide compatibility with code that uses
+** the old names for the llm access functions.  The names were changed
+** when support for the 31XX llm was added, as the widths differ between Octeon Models.
+** The wide/narrow names are preferred, and should be used in all new code */
+#define cvmx_llm_write36 cvmx_llm_write_narrow
+#define cvmx_llm_read36  cvmx_llm_read_narrow
+#define cvmx_llm_write64 cvmx_llm_write_wide
+#define cvmx_llm_read64  cvmx_llm_read_wide
+#endif
+/**
+ * Write to LLM memory - 36 bit
+ *
+ * @param address Address in LLM to write. Consecutive writes increment the
+ *                address by 4. The replication mode is also encoded in this
+ *                address.
+ * @param value   Value to write to LLM. Only the low 36 bits will be used.
+ * @param set     Which of the two coprocessor 2 register sets to use for the
+ *                write. May be used to get two outstanding LLM access at once
+ *                per core. Range: 0-1
+ */
+static inline void cvmx_llm_write_narrow(cvmx_llm_address_t address, uint64_t value, int set)
+{
+    cvmx_llm_data_t data;
+    data.s.mbz = 0;
+
+    if (cvmx_octeon_is_pass1())
+        data.s.dat = ((value & 0x3ffff) << 18) | ((value >> 18) & 0x3ffff);
+    else
+        data.s.dat = value;
+
+    data.s.xxor = 0;
+
+    if (set)
+    {
+        CVMX_MT_LLM_DATA(1, data.u64);
+        CVMX_MT_LLM_WRITE_ADDR_INTERNAL(1, address.u64);
+    }
+    else
+    {
+        CVMX_MT_LLM_DATA(0, data.u64);
+        CVMX_MT_LLM_WRITE_ADDR_INTERNAL(0, address.u64);
+    }
+}
+
+
+/**
+ * Write to LLM memory - 64 bit
+ *
+ * @param address Address in LLM to write. Consecutive writes increment the
+ *                address by 8. The replication mode is also encoded in this
+ *                address.
+ * @param value   Value to write to LLM.
+ * @param set     Which of the two coprocessor 2 register sets to use for the
+ *                write. May be used to get two outstanding LLM access at once
+ *                per core. Range: 0-1
+ */
+static inline void cvmx_llm_write_wide(cvmx_llm_address_t address, uint64_t value, int set)
+{
+    if (cvmx_octeon_is_pass1())
+    {
+        cvmx_llm_write36(address, value & 0xfffffffffull, set);
+        address.s.address+=4;
+        cvmx_llm_write36(address, ((value>>36) & 0xfffffff) | (cvmx_llm_ecc(value) << 28), set);
+    }
+    else
+    {
+        if (set)
+        {
+            CVMX_MT_LLM_DATA(1, value);
+            CVMX_MT_LLM_WRITE64_ADDR_INTERNAL(1, address.u64);
+        }
+        else
+        {
+            CVMX_MT_LLM_DATA(0, value);
+            CVMX_MT_LLM_WRITE64_ADDR_INTERNAL(0, address.u64);
+        }
+    }
+}
+
+
+/**
+ * Read from LLM memory - 36 bit
+ *
+ * @param address Address in LLM to read. Consecutive reads increment the
+ *                address by 4. The replication mode is also encoded in this
+ *                address.
+ * @param set     Which of the two coprocessor 2 register sets to use for the
+ *                write. May be used to get two outstanding LLM access at once
+ *                per core. Range: 0-1
+ * @return The lower 36 bits contain the result of the read
+ */
+static inline cvmx_llm_data_t cvmx_llm_read_narrow(cvmx_llm_address_t address, int set)
+{
+    cvmx_llm_data_t value;
+    if (set)
+    {
+        CVMX_MT_LLM_READ_ADDR(1, address.u64);
+        CVMX_MF_LLM_DATA(1, value.u64);
+    }
+    else
+    {
+        CVMX_MT_LLM_READ_ADDR(0, address.u64);
+        CVMX_MF_LLM_DATA(0, value.u64);
+    }
+    return value;
+}
+
+
+/**
+ * Read from LLM memory - 64 bit
+ *
+ * @param address Address in LLM to read. Consecutive reads increment the
+ *                address by 8. The replication mode is also encoded in this
+ *                address.
+ * @param set     Which of the two coprocessor 2 register sets to use for the
+ *                write. May be used to get two outstanding LLM access at once
+ *                per core. Range: 0-1
+ * @return The result of the read
+ */
+static inline uint64_t cvmx_llm_read_wide(cvmx_llm_address_t address, int set)
+{
+    uint64_t value;
+    if (set)
+    {
+        CVMX_MT_LLM_READ64_ADDR(1, address);
+        CVMX_MF_LLM_DATA(1, value);
+    }
+    else
+    {
+        CVMX_MT_LLM_READ64_ADDR(0, address);
+        CVMX_MF_LLM_DATA(0, value);
+    }
+    return value;
+}
+
+
+#define RLD_INIT_DELAY  (1<<18)
+
+
+
+/* This structure describes the RLDRAM configuration for a board.  This structure
+** must be populated with the correct values and passed to the initialization function.
+*/
+typedef struct
+{
+    uint32_t cpu_hz;            /* CPU frequency in Hz */
+    char addr_rld0_fb_str [100];   /* String describing RLDRAM connections on rld 0 front (0) bunk*/
+    char addr_rld0_bb_str [100];   /* String describing RLDRAM connections on rld 0 back (1) bunk*/
+    char addr_rld1_fb_str [100];   /* String describing RLDRAM connections on rld 1 front (0) bunk*/
+    char addr_rld1_bb_str [100];   /* String describing RLDRAM connections on rld 1 back (1) bunk*/
+    uint8_t rld0_bunks;     /* Number of bunks on rld 0 (0 is disabled) */
+    uint8_t rld1_bunks;     /* Number of bunks on rld 1 (0 is disabled) */
+    uint16_t rld0_mbytes;   /* mbytes on rld 0 */
+    uint16_t rld1_mbytes;   /* mbytes on rld 1 */
+    uint16_t max_rld_clock_mhz;  /* Maximum RLD clock in MHz, only used for CN58XX */
+} llm_descriptor_t;
+
+/**
+ * Initialize LLM memory controller.  This must be done
+ * before the low latency memory can be used.
+ * This is simply a wrapper around cvmx_llm_initialize_desc(),
+ * and is deprecated.
+ *
+ * @return -1 on error
+ *         0 on success
+ */
+int cvmx_llm_initialize(void);
+
+
+/**
+ * Initialize LLM memory controller.  This must be done
+ * before the low latency memory can be used.
+ *
+ * @param llm_desc_ptr
+ *              Pointer to descriptor structure. If NULL
+ *              is passed, a default setting is used if available.
+ *
+ * @return -1 on error
+ *         Size of llm in bytes on success
+ */
+int cvmx_llm_initialize_desc(llm_descriptor_t *llm_desc_ptr);
+
+
+
+/**
+ * Gets the default llm descriptor for the board code is being run on.
+ *
+ * @param llm_desc_ptr
+ *               Pointer to descriptor structure to be filled in.  Contents are only
+ *               valid after successful completion.  Must not be NULL.
+ *
+ * @return -1 on error
+ *         0 on success
+ */
+int cvmx_llm_get_default_descriptor(llm_descriptor_t *llm_desc_ptr);
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif /*  __CVM_LLM_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-lmc.h b/arch/mips/cavium-octeon/executive/cvmx-lmc.h
new file mode 100644
index 0000000..7ecbe5f
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-lmc.h
@@ -0,0 +1,74 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Memory controller interface.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+
+#ifndef __CVMX_ASX_H__
+#define __CVMX_ASX_H__
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+/* CSR typedefs have been moved to cvmx-csr-*.h */
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-log-arc.S b/arch/mips/cavium-octeon/executive/cvmx-log-arc.S
new file mode 100644
index 0000000..760788a
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-log-arc.S
@@ -0,0 +1,184 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+//
+// The function defined here is called for every function as it is executed.
+// These calls are automatically inserted by GCC when the switch "-pg" is
+// used. This allows cvmx-log to add a PC entry as each function is executed.
+// This information, along with the timestamps can give the user a good idea
+// of the performance characteristics of their program. This function normally
+// takes about 22 cycles to execute.
+//
+
+#ifdef __linux__
+#include <asm/asm.h>
+#include <asm/regdef.h>
+#define LA dla
+#else
+#include <machine/asm.h>
+#include <machine/regdef.h>
+#define LA la
+#endif
+
+.set noreorder
+.set noat
+LEAF(_mcount)
+	//
+	// All registers we use must be saved since calls are added by gcc
+	// after register allocation. The at register ($3) will contain the
+	// original ra register before the _mcount call. Also the compiler
+	// automatically performs a "dsubu sp, sp, 16" before we're called.
+	// At the end of this function all registers must have their original
+	// values and the stack pointr must be adjusted by 16. This code is
+	// pretty unreadable since it has been arranged to promote dual issue.
+	//
+#ifdef __linux__
+	dsubu	sp, sp, 32
+#else
+	dsubu	sp, sp, 16
+#endif
+	sd	s3, 24(sp)				// Save register
+	rdhwr	s3, $31					// Read the cycle count
+	sd	s0, 0(sp)				// Save register
+	LA	s0, cvmx_log_buffer_end_ptr		// Load the address of the end of the log buffer
+	sd	s1, 8(sp)				// Save register
+	LA	s1, cvmx_log_buffer_write_ptr		// Load the address of the location in the log buffer
+	sd	s2, 16(sp)				// Save register
+	ld	s0, 0(s0)				// Get value of the current log buffer end location
+	ld	s2, 0(s1)				// Get value of the current log buffer location
+	dsubu	s0, s0, s2				// Subtract the end pointer and the write pointer
+	sltiu	s0, s0, 16				// Check if there are at least 16 bytes
+	bne	s0, $0, call_c_pc			// Call the slow C function if we don't have room in the log
+	li	s0, 0x001				// 11 bit constant that matches the first 11 bits of a CVMX_LOG_TYPE_PC header
+	sd	ra, 8(s2)				// Write the pc to the log
+	dins	s3, s0, 53, 11				// Overwrite the upper cycle count bits with the CVMX_LOG_TYPE_PC header
+	sd	s3, 0(s2)				// Write the log header
+	daddu	s2, s2, 16				// Increment the write location ptr
+	sd	s2, 0(s1)				// Store the write location ptr
+return_c_pc:
+	ld	s0, 0(sp)				// Restore register
+	ld	s1, 8(sp)				// Restore register
+	ld	s2, 16(sp)				// Restore register
+	ld	s3, 24(sp)				// Restore register
+	daddu	sp, sp, 32				// Pop everything off the stack, even the 16 bytes done by gcc
+	jr	ra					// Return to the caller and
+	or	ra, $1, $1				// make sure the ra is back to its original value
+
+call_c_pc:
+	// The registers used by the C code may change based on optimizations. To be
+	// safe, I'll save all registers. We're in the slow path case anyway.
+	dsubu	sp, sp, 216
+	sd	$1, 0(sp)
+	sd	$2, 8(sp)
+	sd	$3, 16(sp)
+	sd 	$4, 24(sp)
+	sd 	$5, 32(sp)
+	sd	$6, 40(sp)
+	sd	$7, 48(sp)
+	sd	$8, 56(sp)
+	sd	$9, 64(sp)
+	sd	$10, 72(sp)
+	sd	$11, 80(sp)
+	sd	$12, 88(sp)
+	sd	$13, 96(sp)
+	sd	$14, 104(sp)
+	sd	$15, 112(sp)
+	// s0, s1, s2, s3 are already saved
+	sd	$20, 120(sp)
+	sd	$21, 128(sp)
+	sd	$22, 136(sp)
+	sd	$23, 144(sp)
+	sd	$24, 152(sp)
+	sd	$25, 160(sp)
+	sd	$26, 168(sp)
+	sd	$27, 176(sp)
+	sd	$28, 184(sp)
+	sd	$29, 192(sp)
+	sd	$30, 200(sp)
+	sd	$31, 208(sp)
+
+	or	a0, ra, ra
+	jal	cvmx_log_pc
+	nop
+
+	ld	$1, 0(sp)
+	ld	$2, 8(sp)
+	ld	$3, 16(sp)
+	ld 	$4, 24(sp)
+	ld 	$5, 32(sp)
+	ld	$6, 40(sp)
+	ld	$7, 48(sp)
+	ld	$8, 56(sp)
+	ld	$9, 64(sp)
+	ld	$10, 72(sp)
+	ld	$11, 80(sp)
+	ld	$12, 88(sp)
+	ld	$13, 96(sp)
+	ld	$14, 104(sp)
+	ld	$15, 112(sp)
+	// s0, s1, s2, s3 will be restored later
+	ld	$20, 120(sp)
+	ld	$21, 128(sp)
+	ld	$22, 136(sp)
+	ld	$23, 144(sp)
+	ld	$24, 152(sp)
+	ld	$25, 160(sp)
+	ld	$26, 168(sp)
+	ld	$27, 176(sp)
+	ld	$28, 184(sp)
+	ld	$29, 192(sp)
+	ld	$30, 200(sp)
+	ld	$31, 208(sp)
+	b	return_c_pc
+	daddu	sp, sp, 216
+
+END(_mcount)
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-log.c b/arch/mips/cavium-octeon/executive/cvmx-log.c
new file mode 100644
index 0000000..e45b9c5
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-log.c
@@ -0,0 +1,544 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * cvmx-log supplies a fast log buffer implementation. Each core writes
+ * log data to a differnet buffer to avoid synchronization overhead. Function
+ * call logging can be turned on with the GCC option "-pg".
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+#include "cvmx.h"
+#include "cvmx-log.h"
+
+#define CVMX_LOG_BUFFER_SIZE (1<<15)
+#define CVMX_LOG_NUM_BUFFERS 4
+
+/**
+ * The possible types of log data that can be stored in the
+ * buffer.
+ */
+typedef enum
+{
+    CVMX_LOG_TYPE_PC = 0,   /**< Log of the program counter location. used for code profiling / tracing */
+    CVMX_LOG_TYPE_PRINTF,   /**< Constant printf format string with two 64bit arguments */
+    CVMX_LOG_TYPE_DATA,     /**< Arbitrary array of dwords. Max size is 31 dwords */
+    CVMX_LOG_TYPE_STRUCTURE,/**< Log a structured data element. Max size is 30 dwords */
+    CVMX_LOG_TYPE_PERF,     /**< Mips performance counters control registers followed by the data */
+} cvmx_log_type_t;
+
+/**
+ * Header definition for each log entry.
+ */
+typedef union
+{
+    uint64_t u64;
+    struct
+    {
+        cvmx_log_type_t     type    : 3; /* Data in the log entry */
+        uint64_t            size    : 8; /* Data size in 64bit words */
+        uint64_t            cycle   :53; /* Low bits of the cycle counter as a timestamp */
+    } s;
+} cvmx_log_header_t;
+
+/**
+ * Circular log buffer. Each processor gets a private one to
+ * write to. Log entries are added at the current write
+ * location, then the write location is incremented. The
+ * buffer may wrap in the middle of a log entry.
+ */
+static uint64_t cvmx_log_buffers[CVMX_LOG_NUM_BUFFERS][CVMX_LOG_BUFFER_SIZE];
+
+/**
+ * Current locations in the log.
+ */
+uint64_t *cvmx_log_buffer_write_ptr             = NULL; /* The next write will occur here */
+uint64_t *cvmx_log_buffer_end_ptr               = NULL; /* Write must move to the next buffer when it equals this */
+uint64_t *cvmx_log_buffer_head_ptr              = NULL; /* Pointer to begin extracting log data from */
+static uint64_t *cvmx_log_buffer_read_ptr       = NULL; /* Location cvmx_display is reading from */
+static uint64_t *cvmx_log_buffer_read_end_ptr   = NULL; /* Location where read will need the next buffer */
+uint64_t cvmx_log_mcd0_on_full                  = 0;    /* If this is set, cvm-log will assert MCD0 when the log
+                                                            is full. This is set by the remote logging utility through
+                                                            the debugger interface. */
+
+
+/**
+ * @INTERNAL
+ * Initialize the log for writing
+ */
+static void __cvmx_log_initialize(void) CVMX_LOG_DISABLE_PC_LOGGING;
+static void __cvmx_log_initialize(void)
+{
+    int buf_num;
+
+    /* Link the buffers together using the last element in each buffer */
+    for (buf_num=0; buf_num<CVMX_LOG_NUM_BUFFERS-1; buf_num++)
+        cvmx_log_buffers[buf_num][CVMX_LOG_BUFFER_SIZE-1] = CAST64(cvmx_log_buffers[buf_num+1]);
+    cvmx_log_buffers[CVMX_LOG_NUM_BUFFERS-1][CVMX_LOG_BUFFER_SIZE-1] = CAST64(NULL);
+
+    cvmx_log_buffer_head_ptr = &cvmx_log_buffers[0][0];
+    cvmx_log_buffer_write_ptr = &cvmx_log_buffers[0][0];
+    cvmx_log_buffer_end_ptr = cvmx_log_buffer_write_ptr + CVMX_LOG_BUFFER_SIZE-1;
+}
+
+
+/**
+ * @INTERNAL
+ * Called when the log is full of data. This function must
+ * make room for more log data before returning.
+ */
+static void __cvmx_log_full_process(void) CVMX_LOG_DISABLE_PC_LOGGING;
+static void __cvmx_log_full_process(void)
+{
+    if (cvmx_log_mcd0_on_full)
+    {
+        register uint64_t tmp;
+        /* Pulse MCD0 signal so a remote utility can extract the data */
+        asm volatile (
+            "dmfc0 %0, $22\n"
+	        "ori   %0, %0, 0x1110\n"
+            "dmtc0 %0, $22\n"
+            "nop\n"
+            "nop\n"
+            "nop\n"
+            "nop\n"
+            "nop\n"
+            "nop\n"
+            : "=r" (tmp));
+    }
+    /* The write ptr may have been modifed by the debugger, check it again */
+    if (!(volatile uint64_t)CAST64(cvmx_log_buffer_write_ptr))
+    {
+        #ifndef __KERNEL__
+            /* Disabled for the Linux kernel since printk is also profiled */
+            cvmx_dprintf("Log is full, reusing first buffer\n");
+        #endif
+        *cvmx_log_buffer_end_ptr = CAST64(cvmx_log_buffer_head_ptr);
+        cvmx_log_buffer_write_ptr = cvmx_log_buffer_head_ptr;
+        cvmx_log_buffer_end_ptr = cvmx_log_buffer_write_ptr + CVMX_LOG_BUFFER_SIZE-1;
+        cvmx_log_buffer_head_ptr = CASTPTR(uint64_t, *cvmx_log_buffer_end_ptr);
+        *cvmx_log_buffer_end_ptr = CAST64(NULL);
+    }
+}
+
+
+/**
+ * @INTERNAL
+ * Simple inline function to build a log header
+ *
+ * @param type   Type of header to build
+ * @param size   Amount of data that follows the header in dwords
+ * @return The header
+ */
+static inline uint64_t __cvmx_log_build_header(cvmx_log_type_t type, uint64_t size) CVMX_LOG_DISABLE_PC_LOGGING;
+static inline uint64_t __cvmx_log_build_header(cvmx_log_type_t type, uint64_t size)
+{
+    cvmx_log_header_t header;
+    header.u64 = 0;
+    header.s.type = type;
+    header.s.size = size;
+    header.s.cycle = cvmx_get_cycle();
+    return header.u64;
+}
+
+
+/**
+ * @INTERNAL
+ * Function to write and increment the position. It rotates
+ * to the next log buffer as necessary.
+ *
+ * @param data   Data to write to the log
+ */
+static inline void __cvmx_log_write(uint64_t data) CVMX_LOG_DISABLE_PC_LOGGING;
+static inline void __cvmx_log_write(uint64_t data)
+{
+    /* Check and see if we need to rotate the log */
+    if (cvmx_likely(cvmx_log_buffer_write_ptr != cvmx_log_buffer_end_ptr))
+    {
+        /* No rotate is necessary, just write the data */
+        *cvmx_log_buffer_write_ptr++ = data;
+    }
+    else
+    {
+        /* Initialize the log if necessary */
+        if (cvmx_unlikely(cvmx_log_buffer_head_ptr == NULL))
+            __cvmx_log_initialize();
+        else
+        {
+            cvmx_log_buffer_write_ptr = CASTPTR(uint64_t, *cvmx_log_buffer_end_ptr);
+            if (cvmx_likely(cvmx_log_buffer_write_ptr))
+            {
+                /* Rotate the log. Might be a good time to send the old buffer
+                    somewhere */
+                cvmx_log_buffer_end_ptr = cvmx_log_buffer_write_ptr + CVMX_LOG_BUFFER_SIZE-1;
+            }
+            else
+                __cvmx_log_full_process();    /* After this function returns, the log must be ready for updates */
+        }
+        *cvmx_log_buffer_write_ptr++ = data;
+    }
+}
+
+
+/**
+ * Log a program counter address to the log. This is caused by
+ * the assembly code function mcount when writing the PC value
+ * is more complicated that the simple case support by it.
+ *
+ * @param pc     Program counter address to log
+ */
+void cvmx_log_pc(uint64_t pc) CVMX_LOG_DISABLE_PC_LOGGING;
+void cvmx_log_pc(uint64_t pc)
+{
+    __cvmx_log_write(__cvmx_log_build_header(CVMX_LOG_TYPE_PC, 1));
+    __cvmx_log_write(pc);
+}
+
+
+/**
+ * Log a constant printf style format string with 0 to 4
+ * arguments. The string must persist until the log is read,
+ * but the parameters are copied into the log.
+ *
+ * @param format  Constant printf style format string.
+ */
+void cvmx_log_printf0(const char *format)
+{
+    __cvmx_log_write(__cvmx_log_build_header(CVMX_LOG_TYPE_PRINTF, 1));
+    __cvmx_log_write(CAST64(format));
+}
+
+
+/**
+ * Log a constant printf style format string with 0 to 4
+ * arguments. The string must persist until the log is read,
+ * but the parameters are copied into the log.
+ *
+ * @param format  Constant printf style format string.
+ * @param number1 64bit argument to the printf format string
+ */
+void cvmx_log_printf1(const char *format, uint64_t number1)
+{
+    __cvmx_log_write(__cvmx_log_build_header(CVMX_LOG_TYPE_PRINTF, 2));
+    __cvmx_log_write(CAST64(format));
+    __cvmx_log_write(number1);
+}
+
+
+/**
+ * Log a constant printf style format string with 0 to 4
+ * arguments. The string must persist until the log is read,
+ * but the parameters are copied into the log.
+ *
+ * @param format  Constant printf style format string.
+ * @param number1 64bit argument to the printf format string
+ * @param number2 64bit argument to the printf format string
+ */
+void cvmx_log_printf2(const char *format, uint64_t number1, uint64_t number2)
+{
+    __cvmx_log_write(__cvmx_log_build_header(CVMX_LOG_TYPE_PRINTF, 3));
+    __cvmx_log_write(CAST64(format));
+    __cvmx_log_write(number1);
+    __cvmx_log_write(number2);
+}
+
+
+/**
+ * Log a constant printf style format string with 0 to 4
+ * arguments. The string must persist until the log is read,
+ * but the parameters are copied into the log.
+ *
+ * @param format  Constant printf style format string.
+ * @param number1 64bit argument to the printf format string
+ * @param number2 64bit argument to the printf format string
+ * @param number3 64bit argument to the printf format string
+ */
+void cvmx_log_printf3(const char *format, uint64_t number1, uint64_t number2, uint64_t number3)
+{
+    __cvmx_log_write(__cvmx_log_build_header(CVMX_LOG_TYPE_PRINTF, 4));
+    __cvmx_log_write(CAST64(format));
+    __cvmx_log_write(number1);
+    __cvmx_log_write(number2);
+    __cvmx_log_write(number3);
+}
+
+
+/**
+ * Log a constant printf style format string with 0 to 4
+ * arguments. The string must persist until the log is read,
+ * but the parameters are copied into the log.
+ *
+ * @param format  Constant printf style format string.
+ * @param number1 64bit argument to the printf format string
+ * @param number2 64bit argument to the printf format string
+ * @param number3 64bit argument to the printf format string
+ * @param number4 64bit argument to the printf format string
+ */
+void cvmx_log_printf4(const char *format, uint64_t number1, uint64_t number2, uint64_t number3, uint64_t number4)
+{
+    __cvmx_log_write(__cvmx_log_build_header(CVMX_LOG_TYPE_PRINTF, 5));
+    __cvmx_log_write(CAST64(format));
+    __cvmx_log_write(number1);
+    __cvmx_log_write(number2);
+    __cvmx_log_write(number3);
+    __cvmx_log_write(number4);
+}
+
+
+/**
+ * Log an arbitrary block of 64bit words. At most 255 64bit
+ * words can be logged. The words are copied into the log.
+ *
+ * @param size_in_dwords
+ *               Number of 64bit dwords to copy into the log.
+ * @param data   Array of 64bit dwords to copy
+ */
+void cvmx_log_data(uint64_t size_in_dwords, const uint64_t *data)
+{
+    if (size_in_dwords > 255)
+        size_in_dwords = 255;
+
+    __cvmx_log_write(__cvmx_log_build_header(CVMX_LOG_TYPE_DATA, size_in_dwords));
+    while (size_in_dwords--)
+        __cvmx_log_write(*data++);
+}
+
+
+/**
+ * Log a structured data object. Post processing will use the
+ * debugging information in the ELF file to determine how to
+ * display the structure. Max of 2032 bytes.
+ *
+ * Example:
+ * cvmx_log_structure("cvmx_wqe_t", work, sizeof(*work));
+ *
+ * @param type   C typedef expressed as a string. This will be used to
+ *               lookup the structure in the debugging infirmation.
+ * @param data   Data to be written to the log.
+ * @param size_in_bytes
+ *               Size if the data in bytes. Normally you'll use the
+ *               sizeof() operator here.
+ */
+void cvmx_log_structure(const char *type, void *data, int size_in_bytes)
+{
+    uint64_t size_in_dwords = (size_in_bytes + 7) >> 3;
+    uint64_t *ptr = (uint64_t*)data;
+
+    if (size_in_dwords > 254)
+        size_in_dwords = 254;
+
+    __cvmx_log_write(__cvmx_log_build_header(CVMX_LOG_TYPE_STRUCTURE, size_in_dwords + 1));
+    __cvmx_log_write(CAST64(type));
+    while (size_in_dwords--)
+        __cvmx_log_write(*ptr++);
+}
+
+
+/**
+ * Setup the mips performance counters
+ *
+ * @param counter1 Event type for counter 1
+ * @param counter2 Event type for counter 2
+ */
+void cvmx_log_perf_setup(cvmx_log_perf_event_t counter1, cvmx_log_perf_event_t counter2)
+{
+    cvmx_log_perf_control_t control;
+
+    control.u32 = 0;
+    control.s.event = counter1;
+    control.s.U = 1;
+    control.s.S = 1;
+    control.s.K = 1;
+    control.s.EX = 1;
+    asm ("mtc0 %0, $25, 0\n" : : "r"(control.u32));
+    control.s.event = counter2;
+    asm ("mtc0 %0, $25, 2\n" : : "r"(control.u32));
+}
+
+
+/**
+ * Log the performance counters
+ */
+void cvmx_log_perf(void)
+{
+    uint64_t control1;
+    uint64_t control2;
+    uint64_t data1;
+    uint64_t data2;
+    asm ("dmfc0 %0, $25, 1\n" : "=r"(data1));
+    asm ("dmfc0 %0, $25, 3\n" : "=r"(data2));
+    asm ("mfc0 %0, $25, 0\n" : "=r"(control1));
+    asm ("mfc0 %0, $25, 2\n" : "=r"(control2));
+    __cvmx_log_write(__cvmx_log_build_header(CVMX_LOG_TYPE_PERF, 3));
+    __cvmx_log_write(((control1 & 0xffffffff) << 32) | (control2 & 0xffffffff));
+    __cvmx_log_write(data1);
+    __cvmx_log_write(data2);
+}
+
+
+/**
+ * @INTERNAL
+ * Read a dword from the log
+ *
+ * @return the dword
+ */
+static uint64_t __cvmx_log_read(void) CVMX_LOG_DISABLE_PC_LOGGING;
+static uint64_t __cvmx_log_read(void)
+{
+    uint64_t data;
+
+    /* Check and see if we need to rotate the log */
+    if (cvmx_likely(cvmx_log_buffer_read_ptr != cvmx_log_buffer_read_end_ptr))
+    {
+        /* No rotate is necessary, just read the data */
+        data = *cvmx_log_buffer_read_ptr++;
+    }
+    else
+    {
+        cvmx_log_buffer_read_ptr = CASTPTR(uint64_t, *cvmx_log_buffer_read_end_ptr);
+        if (cvmx_likely(cvmx_log_buffer_read_ptr))
+        {
+            /* Rotate to the next log buffer */
+            cvmx_log_buffer_read_end_ptr = cvmx_log_buffer_read_ptr + CVMX_LOG_BUFFER_SIZE-1;
+            data = *cvmx_log_buffer_read_ptr++;
+        }
+        else
+        {
+            /* No more log buffers, return 0 */
+            cvmx_log_buffer_read_end_ptr = NULL;
+            data = 0;
+        }
+    }
+
+    return data;
+}
+
+
+/**
+ * Display the current log in a human readable format.
+ */
+void cvmx_log_display(void)
+{
+    unsigned int i;
+    cvmx_log_header_t header;
+
+    cvmx_log_buffer_read_ptr = cvmx_log_buffer_head_ptr;
+    cvmx_log_buffer_read_end_ptr = cvmx_log_buffer_read_ptr + CVMX_LOG_BUFFER_SIZE-1;
+
+    while (cvmx_log_buffer_read_ptr && (cvmx_log_buffer_read_ptr != cvmx_log_buffer_write_ptr))
+    {
+        header.u64 = __cvmx_log_read();
+        if (header.s.cycle == 0)
+            continue;
+        printf("%llu: ", (unsigned long long)header.s.cycle);
+        switch (header.s.type)
+        {
+            case CVMX_LOG_TYPE_PC:
+                if (header.s.size == 1)
+                    printf("pc 0x%016llx\n", (unsigned long long)__cvmx_log_read());
+                else
+                    printf("Illegal size (%d) for log entry: pc\n", header.s.size);
+                break;
+            case CVMX_LOG_TYPE_PRINTF:
+                switch (header.s.size)
+                {
+                    case 1:
+                        printf(CASTPTR(const char, __cvmx_log_read()));
+                        break;
+                    case 2:
+                        printf(CASTPTR(const char, __cvmx_log_read()), __cvmx_log_read());
+                        break;
+                    case 3:
+                        printf(CASTPTR(const char, __cvmx_log_read()), __cvmx_log_read(), __cvmx_log_read());
+                        break;
+                    case 4:
+                        printf(CASTPTR(const char, __cvmx_log_read()), __cvmx_log_read(), __cvmx_log_read(), __cvmx_log_read());
+                        break;
+                    case 5:
+                        printf(CASTPTR(const char, __cvmx_log_read()), __cvmx_log_read(), __cvmx_log_read(), __cvmx_log_read(), __cvmx_log_read());
+                        break;
+                    default:
+                        printf("Illegal size (%d) for log entry: printf\n", header.s.size);
+                        break;
+                }
+                printf("\n");
+                break;
+            case CVMX_LOG_TYPE_DATA:
+                printf("data");
+                for (i=0; i<header.s.size; i++)
+                    printf(" 0x%016llx", (unsigned long long)__cvmx_log_read());
+                printf("\n");
+                break;
+            case CVMX_LOG_TYPE_STRUCTURE:
+                printf("struct %s", CASTPTR(const char, __cvmx_log_read()));
+                for (i=1; i<header.s.size; i++)
+                    printf(" 0x%016llx", (unsigned long long)__cvmx_log_read());
+                printf("\n");
+                break;
+            case CVMX_LOG_TYPE_PERF:
+                if (header.s.size == 3)
+                {
+                    unsigned long long control = __cvmx_log_read();
+                    unsigned long long data1 = __cvmx_log_read();
+                    unsigned long long data2 = __cvmx_log_read();
+                    printf("perf control=0x%016llx data1=0x%016llx data2=0x%016llx\n", control, data1, data2);
+                }
+                else
+                    printf("Illegal size (%d) for log entry: perf\n", header.s.size);
+                break;
+            default:
+                break;
+        }
+    }
+}
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-log.h b/arch/mips/cavium-octeon/executive/cvmx-log.h
new file mode 100644
index 0000000..aabb121
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-log.h
@@ -0,0 +1,220 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+#ifndef __CVMX_LOG_H__
+#define __CVMX_LOG_H__
+
+/**
+ * @file
+ *
+ * cvmx-log supplies a fast log buffer implementation. Each core writes
+ * log data to a differnet buffer to avoid synchronization overhead. Function
+ * call logging can be turned on with the GCC option "-pg".
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+/**
+ * Enumeration of all supported performance counter types
+ */
+typedef enum
+{
+    CVMX_LOG_PERF_CNT_NONE      = 0,    /**< Turn off the performance counter */
+    CVMX_LOG_PERF_CNT_CLK       = 1,    /**< Conditionally clocked cycles (as opposed to count/cvm_count which count even with no clocks) */
+    CVMX_LOG_PERF_CNT_ISSUE     = 2,    /**< Instructions issued but not retired */
+    CVMX_LOG_PERF_CNT_RET       = 3,    /**< Instructions retired */
+    CVMX_LOG_PERF_CNT_NISSUE    = 4,    /**< Cycles no issue */
+    CVMX_LOG_PERF_CNT_SISSUE    = 5,    /**< Cycles single issue */
+    CVMX_LOG_PERF_CNT_DISSUE    = 6,    /**< Cycles dual issue */
+    CVMX_LOG_PERF_CNT_IFI       = 7,    /**< Cycle ifetch issued (but not necessarily commit to pp_mem) */
+    CVMX_LOG_PERF_CNT_BR        = 8,    /**< Branches retired */
+    CVMX_LOG_PERF_CNT_BRMIS     = 9,    /**< Branch mispredicts */
+    CVMX_LOG_PERF_CNT_J         = 10,   /**< Jumps retired */
+    CVMX_LOG_PERF_CNT_JMIS      = 11,   /**< Jumps mispredicted */
+    CVMX_LOG_PERF_CNT_REPLAY    = 12,   /**< Mem Replays */
+    CVMX_LOG_PERF_CNT_IUNA      = 13,   /**< Cycles idle due to unaligned_replays */
+    CVMX_LOG_PERF_CNT_TRAP      = 14,   /**< trap_6a signal */
+    CVMX_LOG_PERF_CNT_UULOAD    = 16,   /**< Unexpected unaligned loads (REPUN=1) */
+    CVMX_LOG_PERF_CNT_UUSTORE   = 17,   /**< Unexpected unaligned store (REPUN=1) */
+    CVMX_LOG_PERF_CNT_ULOAD     = 18,   /**< Unaligned loads (REPUN=1 or USEUN=1) */
+    CVMX_LOG_PERF_CNT_USTORE    = 19,   /**< Unaligned store (REPUN=1 or USEUN=1) */
+    CVMX_LOG_PERF_CNT_EC        = 20,   /**< Exec clocks(must set CvmCtl[DISCE] for accurate timing) */
+    CVMX_LOG_PERF_CNT_MC        = 21,   /**< Mul clocks(must set CvmCtl[DISCE] for accurate timing) */
+    CVMX_LOG_PERF_CNT_CC        = 22,   /**< Crypto clocks(must set CvmCtl[DISCE] for accurate timing) */
+    CVMX_LOG_PERF_CNT_CSRC      = 23,   /**< Issue_csr clocks(must set CvmCtl[DISCE] for accurate timing) */
+    CVMX_LOG_PERF_CNT_CFETCH    = 24,   /**< Icache committed fetches (demand+prefetch) */
+    CVMX_LOG_PERF_CNT_CPREF     = 25,   /**< Icache committed prefetches */
+    CVMX_LOG_PERF_CNT_ICA       = 26,   /**< Icache aliases */
+    CVMX_LOG_PERF_CNT_II        = 27,   /**< Icache invalidates */
+    CVMX_LOG_PERF_CNT_IP        = 28,   /**< Icache parity error */
+    CVMX_LOG_PERF_CNT_CIMISS    = 29,   /**< Cycles idle due to imiss (must set CvmCtl[DISCE] for accurate timing) */
+    CVMX_LOG_PERF_CNT_WBUF      = 32,   /**< Number of write buffer entries created */
+    CVMX_LOG_PERF_CNT_WDAT      = 33,   /**< Number of write buffer data cycles used (may need to set CvmCtl[DISCE] for accurate counts) */
+    CVMX_LOG_PERF_CNT_WBUFLD    = 34,   /**< Number of write buffer entries forced out by loads */
+    CVMX_LOG_PERF_CNT_WBUFFL    = 35,   /**< Number of cycles that there was no available write buffer entry (may need to set CvmCtl[DISCE] and CvmMemCtl[MCLK] for accurate counts) */
+    CVMX_LOG_PERF_CNT_WBUFTR    = 36,   /**< Number of stores that found no available write buffer entries */
+    CVMX_LOG_PERF_CNT_BADD      = 37,   /**< Number of address bus cycles used (may need to set CvmCtl[DISCE] for accurate counts) */
+    CVMX_LOG_PERF_CNT_BADDL2    = 38,   /**< Number of address bus cycles not reflected (i.e. destined for L2) (may need to set CvmCtl[DISCE] for accurate counts) */
+    CVMX_LOG_PERF_CNT_BFILL     = 39,   /**< Number of fill bus cycles used (may need to set CvmCtl[DISCE] for accurate counts) */
+    CVMX_LOG_PERF_CNT_DDIDS     = 40,   /**< Number of Dstream DIDs created */
+    CVMX_LOG_PERF_CNT_IDIDS     = 41,   /**< Number of Istream DIDs created */
+    CVMX_LOG_PERF_CNT_DIDNA     = 42,   /**< Number of cycles that no DIDs were available (may need to set CvmCtl[DISCE] and CvmMemCtl[MCLK] for accurate counts) */
+    CVMX_LOG_PERF_CNT_LDS       = 43,   /**< Number of load issues */
+    CVMX_LOG_PERF_CNT_LMLDS     = 44,   /**< Number of local memory load */
+    CVMX_LOG_PERF_CNT_IOLDS     = 45,   /**< Number of I/O load issues */
+    CVMX_LOG_PERF_CNT_DMLDS     = 46,   /**< Number of loads that were not prefetches and missed in the cache */
+    CVMX_LOG_PERF_CNT_STS       = 48,   /**< Number of store issues */
+    CVMX_LOG_PERF_CNT_LMSTS     = 49,   /**< Number of local memory store issues */
+    CVMX_LOG_PERF_CNT_IOSTS     = 50,   /**< Number of I/O store issues */
+    CVMX_LOG_PERF_CNT_IOBDMA    = 51,   /**< Number of IOBDMAs */
+    CVMX_LOG_PERF_CNT_DTLB      = 53,   /**< Number of dstream TLB refill, invalid, or modified exceptions */
+    CVMX_LOG_PERF_CNT_DTLBAD    = 54,   /**< Number of dstream TLB address errors */
+    CVMX_LOG_PERF_CNT_ITLB      = 55,   /**< Number of istream TLB refill, invalid, or address error exceptions */
+    CVMX_LOG_PERF_CNT_SYNC      = 56,   /**< Number of SYNC stall cycles (may need to set CvmCtl[DISCE] for accurate counts) */
+    CVMX_LOG_PERF_CNT_SYNCIOB   = 57,   /**< Number of SYNCIOBDMA stall cycles (may need to set CvmCtl[DISCE] for accurate counts) */
+    CVMX_LOG_PERF_CNT_SYNCW     = 58,   /**< Number of SYNCWs */
+} cvmx_log_perf_event_t;
+
+/**
+ * Structure of the performance counter control register
+ */
+typedef union
+{
+    uint32_t u32;
+    struct
+    {
+        uint32_t                M       : 1;
+        uint32_t                W       : 1;
+        uint32_t                reserved: 19;
+        cvmx_log_perf_event_t   event   : 6;
+        uint32_t                IE      : 1;
+        uint32_t                U       : 1;
+        uint32_t                S       : 1;
+        uint32_t                K       : 1;
+        uint32_t                EX      : 1;
+    } s;
+} cvmx_log_perf_control_t;
+
+/*
+ * Add CVMX_LOG_DISABLE_PC_LOGGING as an attribute to and function prototype
+ * that you don't want logged when the gcc option "-pg" is supplied. We
+ * use it on the cvmx-log functions since it is pointless to log the
+ * calling of a function than in itself writes to the log.
+ */
+#define CVMX_LOG_DISABLE_PC_LOGGING __attribute__((no_instrument_function))
+
+/**
+ * Log a constant printf style format string with 0 to 4
+ * arguments. The string must persist until the log is read,
+ * but the parameters are copied into the log.
+ *
+ * @param format  Constant printf style format string.
+ * @param numberx 64bit argument to the printf format string
+ */
+void cvmx_log_printf0(const char *format) CVMX_LOG_DISABLE_PC_LOGGING;
+void cvmx_log_printf1(const char *format, uint64_t number1) CVMX_LOG_DISABLE_PC_LOGGING;
+void cvmx_log_printf2(const char *format, uint64_t number1, uint64_t number2) CVMX_LOG_DISABLE_PC_LOGGING;
+void cvmx_log_printf3(const char *format, uint64_t number1, uint64_t number2, uint64_t number3) CVMX_LOG_DISABLE_PC_LOGGING;
+void cvmx_log_printf4(const char *format, uint64_t number1, uint64_t number2, uint64_t number3, uint64_t number4) CVMX_LOG_DISABLE_PC_LOGGING;
+
+/**
+ * Log an arbitrary block of 64bit words. At most 255 64bit
+ * words can be logged. The words are copied into the log.
+ *
+ * @param size_in_dwords
+ *               Number of 64bit dwords to copy into the log.
+ * @param data   Array of 64bit dwords to copy
+ */
+void cvmx_log_data(uint64_t size_in_dwords, const uint64_t *data) CVMX_LOG_DISABLE_PC_LOGGING;
+
+/**
+ * Log a structured data object. Post processing will use the
+ * debugging information in the ELF file to determine how to
+ * display the structure. Max of 2032 bytes.
+ *
+ * Example:
+ * cvmx_log_structure("cvmx_wqe_t", work, sizeof(*work));
+ *
+ * @param type   C typedef expressed as a string. This will be used to
+ *               lookup the structure in the debugging infirmation.
+ * @param data   Data to be written to the log.
+ * @param size_in_bytes
+ *               Size if the data in bytes. Normally you'll use the
+ *               sizeof() operator here.
+ */
+void cvmx_log_structure(const char *type, void *data, int size_in_bytes) CVMX_LOG_DISABLE_PC_LOGGING;
+
+/**
+ * Setup the mips performance counters
+ *
+ * @param counter1 Event type for counter 1
+ * @param counter2 Event type for counter 2
+ */
+void cvmx_log_perf_setup(cvmx_log_perf_event_t counter1, cvmx_log_perf_event_t counter2);
+
+/**
+ * Log the performance counters
+ */
+void cvmx_log_perf(void) CVMX_LOG_DISABLE_PC_LOGGING;
+
+/**
+ * Display the current log in a human readable format.
+ */
+void cvmx_log_display(void);
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif
diff --git a/arch/mips/cavium-octeon/executive/cvmx-malloc.h b/arch/mips/cavium-octeon/executive/cvmx-malloc.h
new file mode 100644
index 0000000..edf72b2
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-malloc.h
@@ -0,0 +1,227 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+/**
+ * @file
+ *
+ * This file provides prototypes for the memory management library functions.
+ * Two different allocators are provided: an arena based allocator that is derived from a
+ * modified version of ptmalloc2 (used in glibc), and a zone allocator for allocating fixed
+ * size memory blocks.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+
+#ifndef __CVMX_MALLOC_H__
+#define __CVMX_MALLOC_H__
+
+#include "cvmx-spinlock.h"
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+
+struct malloc_state; /* forward declaration */
+typedef struct malloc_state *cvmx_arena_list_t;
+
+
+/**
+ * Creates an arena from the memory region specified and adds it
+ * to the supplied arena list.
+ *
+ * @param arena_list Pointer to an arena list to add new arena to.
+ *                   If NULL, new list is created.
+ * @param ptr        pointer to memory region to create arena from
+ *
+ * @param size       Size of memory region available at ptr in bytes.
+ *
+ * @return -1 on Failure
+ *         0 on success
+ */
+int cvmx_add_arena(cvmx_arena_list_t *arena_list, void *ptr, size_t size);
+
+/**
+ * allocate buffer from an arena list
+ *
+ * @param arena_list arena list to allocate buffer from
+ * @param size       size of buffer to allocate (in bytes)
+ *
+ * @return pointer to buffer or NULL if allocation failed
+ */
+void *cvmx_malloc(cvmx_arena_list_t arena_list, size_t size);
+/**
+ * Allocate zero initialized buffer
+ *
+ * @param arena_list arena list to allocate from
+ * @param n          number of elements
+ * @param elem_size  size of elementes
+ *
+ * @return pointer to (n*elem_size) byte zero initialized buffer or NULL
+ *         on allocation failure
+ */
+void *cvmx_calloc(cvmx_arena_list_t arena_list, size_t n, size_t elem_size);
+/**
+ * attempt to increase the size of an already allocated buffer
+ * This function may allocate a new buffer and copy
+ * the data if current buffer can't be extended.
+ *
+ * @param arena_list arena list to allocate from
+ * @param ptr        pointer to buffer to extend
+ * @param size       new buffer size
+ *
+ * @return pointer to expanded buffer (may differ from ptr)
+ *         or NULL on failure
+ */
+void *cvmx_realloc(cvmx_arena_list_t arena_list, void *ptr, size_t size);
+/**
+ * allocate a buffer with a specified alignment
+ *
+ * @param arena_list arena list to allocate from
+ * @param alignment  alignment of buffer.  Must be a power of 2
+ * @param bytes      size of buffer in bytes
+ *
+ * @return pointer to buffer on success
+ *         NULL on failure
+ */
+void *cvmx_memalign(cvmx_arena_list_t arena_list, size_t alignment, size_t bytes);
+/**
+ * free a previously allocated buffer
+ *
+ * @param ptr    pointer of buffer to deallocate
+ */
+void cvmx_free(void *ptr);
+
+
+
+
+#define CVMX_ZONE_OVERHEAD  (64)
+/** Zone allocator definitions
+ *
+ */
+struct cvmx_zone
+{
+	cvmx_spinlock_t lock;
+	char *baseptr;
+	char *name;
+	void *freelist;
+	uint32_t num_elem;
+	uint32_t elem_size;
+	uint32_t align;
+};
+typedef struct cvmx_zone * cvmx_zone_t;
+
+static inline uint32_t cvmx_zone_size(cvmx_zone_t zone)
+{
+    return(zone->elem_size);
+}
+static inline char *cvmx_zone_name(cvmx_zone_t zone)
+{
+    return(zone->name);
+}
+
+
+/**
+ * Creates a memory zone for efficient allocation/deallocation of
+ * fixed size memory blocks from a specified memory region.
+ *
+ * @param name      name of zone.
+ * @param elem_size size of blocks that will be requested from zone
+ * @param num_elem  number of elements to allocate
+ * @param mem_ptr   pointer to memory to allocate zone from
+ * @param mem_size  size of memory region available
+ *                  (must be at least elem_size * num_elem + CVMX_ZONE_OVERHEAD bytes)
+ * @param flags     flags for zone.  Currently unused.
+ *
+ * @return pointer to zone on success or
+ *         NULL on failure
+ */
+cvmx_zone_t cvmx_zone_create_from_addr(char *name, uint32_t elem_size, uint32_t num_elem,
+                             void* mem_ptr, uint64_t mem_size, uint32_t flags);
+/**
+ * Creates a memory zone for efficient allocation/deallocation of
+ * fixed size memory blocks from a previously initialized arena list.
+ *
+ * @param name       name of zone.
+ * @param elem_size  size of blocks that will be requested from zone
+ * @param num_elem   number of elements to allocate
+ * @param align      alignment of buffers (must be power of 2)
+ *                   Elements are allocated contiguously, so the buffer size
+ *                   must be a multiple of the requested alignment for all
+ *                   buffers to have the requested alignment.
+ * @param arena_list arena list to allocate memory from
+ * @param flags      flags for zone.  Currently unused.
+ *
+ * @return pointer to zone on success or
+ *         NULL on failure
+ */
+cvmx_zone_t cvmx_zone_create_from_arena(char *name, uint32_t elem_size, uint32_t num_elem, uint32_t align,
+                             cvmx_arena_list_t arena_list, uint32_t flags);
+/**
+ * Allocate a buffer from a memory zone
+ *
+ * @param zone   zone to allocate buffer from
+ * @param flags  flags (currently unused)
+ *
+ * @return pointer to buffer or NULL on failure
+ */
+void * cvmx_zone_alloc(cvmx_zone_t zone, uint32_t flags);
+/**
+ * Free a previously allocated buffer
+ *
+ * @param zone   zone that buffer was allocated from
+ * @param ptr    pointer to buffer to be freed
+ */
+void cvmx_zone_free(cvmx_zone_t zone, void *ptr);
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif // __CVMX_MALLOC_H__
diff --git a/arch/mips/cavium-octeon/executive/cvmx-malloc/README-malloc b/arch/mips/cavium-octeon/executive/cvmx-malloc/README-malloc
new file mode 100644
index 0000000..922a713
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-malloc/README-malloc
@@ -0,0 +1,12 @@
+Readme for Octeon shared memory malloc
+
+This malloc is based on ptmalloc2, which is the malloc
+implementation of glibc.  Source code and more information
+on this can be found at http://www.malloc.de/en/index.html.
+Please see the individual files for licensing terms.
+
+The main change to the code modifies the way the malloc
+gets memory from the system.  Under Linux/Unix, malloc
+uses the brk or memmap sytem calls to request more memory.
+In this implementation, memory regions must be explicitly
+given to malloc by the application.
diff --git a/arch/mips/cavium-octeon/executive/cvmx-malloc/arena.c b/arch/mips/cavium-octeon/executive/cvmx-malloc/arena.c
new file mode 100644
index 0000000..8e0ce1f
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-malloc/arena.c
@@ -0,0 +1,293 @@
+/*
+Copyright (c) 2001 Wolfram Gloger
+Copyright (c) 2006 Cavium networks
+
+Permission to use, copy, modify, distribute, and sell this software
+and its documentation for any purpose is hereby granted without fee,
+provided that (i) the above copyright notices and this permission
+notice appear in all copies of the software and related documentation,
+and (ii) the name of Wolfram Gloger may not be used in any advertising
+or publicity relating to the software.
+
+THE SOFTWARE IS PROVIDED "AS-IS" AND WITHOUT WARRANTY OF ANY KIND,
+EXPRESS, IMPLIED OR OTHERWISE, INCLUDING WITHOUT LIMITATION, ANY
+WARRANTY OF MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE.
+
+IN NO EVENT SHALL WOLFRAM GLOGER BE LIABLE FOR ANY SPECIAL,
+INCIDENTAL, INDIRECT OR CONSEQUENTIAL DAMAGES OF ANY KIND, OR ANY
+DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS,
+WHETHER OR NOT ADVISED OF THE POSSIBILITY OF DAMAGE, AND ON ANY THEORY
+OF LIABILITY, ARISING OUT OF OR IN CONNECTION WITH THE USE OR
+PERFORMANCE OF THIS SOFTWARE.
+*/
+
+/* $Id: arena.c 30481 2007-12-05 21:46:59Z rfranz $ */
+
+/* Compile-time constants.  */
+
+#define HEAP_MIN_SIZE (4096)   /* Must leave room for struct malloc_state, arena ptrs, etc., totals about 2400 bytes */
+
+#ifndef THREAD_STATS
+#define THREAD_STATS 0
+#endif
+
+/* If THREAD_STATS is non-zero, some statistics on mutex locking are
+   computed.  */
+
+/***************************************************************************/
+
+// made static to avoid conflicts with newlib
+static mstate         _int_new_arena __MALLOC_P ((size_t __ini_size));
+
+/***************************************************************************/
+
+#define top(ar_ptr) ((ar_ptr)->top)
+
+/* A heap is a single contiguous memory region holding (coalesceable)
+   malloc_chunks.    Not used unless compiling with
+   USE_ARENAS. */
+
+typedef struct _heap_info {
+  mstate ar_ptr; /* Arena for this heap. */
+  struct _heap_info *prev; /* Previous heap. */
+  size_t size;   /* Current size in bytes. */
+  size_t pad;    /* Make sure the following data is properly aligned. */
+} heap_info;
+
+/* Thread specific data */
+
+static tsd_key_t arena_key;  // one per PP (thread)
+static CVMX_SHARED mutex_t list_lock;  // shared...
+
+#if THREAD_STATS
+static int stat_n_heaps;
+#define THREAD_STAT(x) x
+#else
+#define THREAD_STAT(x) do ; while(0)
+#endif
+
+/* Mapped memory in non-main arenas (reliable only for NO_THREADS). */
+static unsigned long arena_mem;
+
+/* Already initialized? */
+int CVMX_SHARED cvmx__malloc_initialized = -1;
+
+/**************************************************************************/
+
+#if USE_ARENAS
+
+/* find the heap and corresponding arena for a given ptr */
+
+#define arena_for_chunk(ptr) ((ptr)->arena_ptr)
+#define set_arena_for_chunk(ptr, arena) (ptr)->arena_ptr = (arena)
+
+
+#endif /* USE_ARENAS */
+
+/**************************************************************************/
+
+#ifndef NO_THREADS
+
+/* atfork support.  */
+
+static __malloc_ptr_t (*save_malloc_hook) __MALLOC_P ((size_t __size,
+						       __const __malloc_ptr_t));
+static void           (*save_free_hook) __MALLOC_P ((__malloc_ptr_t __ptr,
+						     __const __malloc_ptr_t));
+static Void_t*        save_arena;
+
+/* Magic value for the thread-specific arena pointer when
+   malloc_atfork() is in use.  */
+
+#define ATFORK_ARENA_PTR ((Void_t*)-1)
+
+/* The following hooks are used while the `atfork' handling mechanism
+   is active. */
+
+static Void_t*
+malloc_atfork(size_t sz, const Void_t *caller)
+{
+return(NULL);
+}
+
+static void
+free_atfork(Void_t* mem, const Void_t *caller)
+{
+  Void_t *vptr = NULL;
+  mstate ar_ptr;
+  mchunkptr p;                          /* chunk corresponding to mem */
+
+  if (mem == 0)                              /* free(0) has no effect */
+    return;
+
+  p = mem2chunk(mem);         /* do not bother to replicate free_check here */
+
+#if HAVE_MMAP
+  if (chunk_is_mmapped(p))                       /* release mmapped memory. */
+  {
+    munmap_chunk(p);
+    return;
+  }
+#endif
+
+  ar_ptr = arena_for_chunk(p);
+  tsd_getspecific(arena_key, vptr);
+  if(vptr != ATFORK_ARENA_PTR)
+    (void)mutex_lock(&ar_ptr->mutex);
+  _int_free(ar_ptr, mem);
+  if(vptr != ATFORK_ARENA_PTR)
+    (void)mutex_unlock(&ar_ptr->mutex);
+}
+
+
+
+#ifdef __linux__
+#error   __linux__defined!
+#endif
+
+#endif /* !defined NO_THREADS */
+
+
+
+/* Initialization routine. */
+#ifdef _LIBC
+#error  _LIBC is defined, and should not be
+#endif /* _LIBC */
+
+static CVMX_SHARED cvmx_spinlock_t malloc_init_spin_lock;
+
+
+
+
+/* Managing heaps and arenas (for concurrent threads) */
+
+#if USE_ARENAS
+
+#if MALLOC_DEBUG > 1
+
+/* Print the complete contents of a single heap to stderr. */
+
+static void
+#if __STD_C
+dump_heap(heap_info *heap)
+#else
+dump_heap(heap) heap_info *heap;
+#endif
+{
+  char *ptr;
+  mchunkptr p;
+
+  fprintf(stderr, "Heap %p, size %10lx:\n", heap, (long)heap->size);
+  ptr = (heap->ar_ptr != (mstate)(heap+1)) ?
+    (char*)(heap + 1) : (char*)(heap + 1) + sizeof(struct malloc_state);
+  p = (mchunkptr)(((unsigned long)ptr + MALLOC_ALIGN_MASK) &
+                  ~MALLOC_ALIGN_MASK);
+  for(;;) {
+    fprintf(stderr, "chunk %p size %10lx", p, (long)p->size);
+    if(p == top(heap->ar_ptr)) {
+      fprintf(stderr, " (top)\n");
+      break;
+    } else if(p->size == (0|PREV_INUSE)) {
+      fprintf(stderr, " (fence)\n");
+      break;
+    }
+    fprintf(stderr, "\n");
+    p = next_chunk(p);
+  }
+}
+
+#endif /* MALLOC_DEBUG > 1 */
+/* Delete a heap. */
+
+
+static mstate cvmx_new_arena(void *addr, size_t size)
+{
+  mstate a;
+  heap_info *h;
+  char *ptr;
+  unsigned long misalign;
+  int page_mask = malloc_getpagesize - 1;
+
+  debug_printf("cvmx_new_arena called, addr: %p, size %ld\n", addr, size);
+  debug_printf("heapinfo size: %ld, mstate size: %d\n", sizeof(heap_info), sizeof(struct malloc_state));
+
+  if (!addr || (size < HEAP_MIN_SIZE))
+  {
+      return(NULL);
+  }
+  /* We must zero out the arena as the malloc code assumes this. */
+  memset(addr, 0, size);
+
+  h = (heap_info *)addr;
+  h->size = size;
+
+  a = h->ar_ptr = (mstate)(h+1);
+  malloc_init_state(a);
+  /*a->next = NULL;*/
+  a->system_mem = a->max_system_mem = h->size;
+  arena_mem += h->size;
+  a->next = a;
+
+  /* Set up the top chunk, with proper alignment. */
+  ptr = (char *)(a + 1);
+  misalign = (unsigned long)chunk2mem(ptr) & MALLOC_ALIGN_MASK;
+  if (misalign > 0)
+    ptr += MALLOC_ALIGNMENT - misalign;
+  top(a) = (mchunkptr)ptr;
+  set_head(top(a), (((char*)h + h->size) - ptr) | PREV_INUSE);
+
+  return a;
+}
+
+
+int cvmx_add_arena(cvmx_arena_list_t *arena_list, void *ptr, size_t size)
+{
+  mstate a;
+
+  /* Enforce required alignement, and adjust size */
+  int misaligned = ((size_t)ptr) & (MALLOC_ALIGNMENT - 1);
+  if (misaligned)
+  {
+      ptr = (char*)ptr + MALLOC_ALIGNMENT - misaligned;
+      size -= MALLOC_ALIGNMENT - misaligned;
+  }
+
+  debug_printf("Adding arena at addr: %p, size %d\n", ptr, size);
+
+  a = cvmx_new_arena(ptr, size);  /* checks ptr and size */
+  if (!a)
+  {
+      return(-1);
+  }
+
+  debug_printf("cmvx_add_arena - arena_list: %p, *arena_list: %p\n", arena_list, *arena_list);
+  debug_printf("cmvx_add_arena - list: %p, new: %p\n", *arena_list, a);
+  mutex_init(&a->mutex);
+  mutex_lock(&a->mutex);
+
+
+  if (*arena_list)
+  {
+      mstate ar_ptr = *arena_list;
+      (void)mutex_lock(&ar_ptr->mutex);
+      a->next = ar_ptr->next;  // lock held on a and ar_ptr
+      ar_ptr->next = a;
+      (void)mutex_unlock(&ar_ptr->mutex);
+  }
+  else
+  {
+      *arena_list = a;
+//      a->next = a;
+  }
+
+  debug_printf("cvmx_add_arena - list: %p, list->next: %p\n", *arena_list, ((mstate)*arena_list)->next);
+
+  // unlock, since it is not going to be used immediately
+  (void)mutex_unlock(&a->mutex);
+
+  return(0);
+}
+
+
+
+#endif /* USE_ARENAS */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-malloc/malloc.c b/arch/mips/cavium-octeon/executive/cvmx-malloc/malloc.c
new file mode 100644
index 0000000..222ad5d
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-malloc/malloc.c
@@ -0,0 +1,4106 @@
+/*
+Copyright (c) 2001 Wolfram Gloger
+Copyright (c) 2006 Cavium networks
+
+Permission to use, copy, modify, distribute, and sell this software
+and its documentation for any purpose is hereby granted without fee,
+provided that (i) the above copyright notices and this permission
+notice appear in all copies of the software and related documentation,
+and (ii) the name of Wolfram Gloger may not be used in any advertising
+or publicity relating to the software.
+
+THE SOFTWARE IS PROVIDED "AS-IS" AND WITHOUT WARRANTY OF ANY KIND,
+EXPRESS, IMPLIED OR OTHERWISE, INCLUDING WITHOUT LIMITATION, ANY
+WARRANTY OF MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE.
+
+IN NO EVENT SHALL WOLFRAM GLOGER BE LIABLE FOR ANY SPECIAL,
+INCIDENTAL, INDIRECT OR CONSEQUENTIAL DAMAGES OF ANY KIND, OR ANY
+DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS,
+WHETHER OR NOT ADVISED OF THE POSSIBILITY OF DAMAGE, AND ON ANY THEORY
+OF LIABILITY, ARISING OUT OF OR IN CONNECTION WITH THE USE OR
+PERFORMANCE OF THIS SOFTWARE.
+*/
+
+/*
+  This is a version (aka ptmalloc2) of malloc/free/realloc written by
+  Doug Lea and adapted to multiple threads/arenas by Wolfram Gloger.
+
+* Version ptmalloc2-20011215
+  $Id: malloc.c 30481 2007-12-05 21:46:59Z rfranz $
+  based on:
+  VERSION 2.7.1pre1 Sat May 12 07:41:21 2001  Doug Lea  (dl at gee)
+
+   Note: There may be an updated version of this malloc obtainable at
+           http://www.malloc.de/malloc/ptmalloc2.tar.gz
+         Check before installing!
+
+* Quickstart
+
+  In order to compile this implementation, a Makefile is provided with
+  the ptmalloc2 distribution, which has pre-defined targets for some
+  popular systems (e.g. "make posix" for Posix threads).  All that is
+  typically required with regard to compiler flags is the selection of
+  the thread package via defining one out of USE_PTHREADS, USE_THR or
+  USE_SPROC.  Check the thread-m.h file for what effects this has.
+  Many/most systems will additionally require USE_TSD_DATA_HACK to be
+  defined, so this is the default for "make posix".
+
+* Why use this malloc?
+
+  This is not the fastest, most space-conserving, most portable, or
+  most tunable malloc ever written. However it is among the fastest
+  while also being among the most space-conserving, portable and tunable.
+  Consistent balance across these factors results in a good general-purpose
+  allocator for malloc-intensive programs.
+
+  The main properties of the algorithms are:
+  * For large (>= 512 bytes) requests, it is a pure best-fit allocator,
+    with ties normally decided via FIFO (i.e. least recently used).
+  * For small (<= 64 bytes by default) requests, it is a caching
+    allocator, that maintains pools of quickly recycled chunks.
+  * In between, and for combinations of large and small requests, it does
+    the best it can trying to meet both goals at once.
+  * For very large requests (>= 128KB by default), it relies on system
+    memory mapping facilities, if supported.
+
+  For a longer but slightly out of date high-level description, see
+     http://gee.cs.oswego.edu/dl/html/malloc.html
+
+  You may already by default be using a C library containing a malloc
+  that is  based on some version of this malloc (for example in
+  linux). You might still want to use the one in this file in order to
+  customize settings or to avoid overheads associated with library
+  versions.
+
+* Contents, described in more detail in "description of public routines" below.
+
+  Standard (ANSI/SVID/...)  functions:
+    malloc(size_t n);
+    calloc(size_t n_elements, size_t element_size);
+    free(Void_t* p);
+    realloc(Void_t* p, size_t n);
+    memalign(size_t alignment, size_t n);
+    valloc(size_t n);
+    mallinfo()
+    mallopt(int parameter_number, int parameter_value)
+
+  Additional functions:
+    independent_calloc(size_t n_elements, size_t size, Void_t* chunks[]);
+    independent_comalloc(size_t n_elements, size_t sizes[], Void_t* chunks[]);
+    pvalloc(size_t n);
+    cfree(Void_t* p);
+    malloc_trim(size_t pad);
+    malloc_usable_size(Void_t* p);
+    malloc_stats();
+
+* Vital statistics:
+
+  Supported pointer representation:       4 or 8 bytes
+  Supported size_t  representation:       4 or 8 bytes
+       Note that size_t is allowed to be 4 bytes even if pointers are 8.
+       You can adjust this by defining INTERNAL_SIZE_T
+
+  Alignment:                              2 * sizeof(size_t) (default)
+       (i.e., 8 byte alignment with 4byte size_t). This suffices for
+       nearly all current machines and C compilers. However, you can
+       define MALLOC_ALIGNMENT to be wider than this if necessary.
+
+  Minimum overhead per allocated chunk:   4 or 8 bytes
+       Each malloced chunk has a hidden word of overhead holding size
+       and status information.
+
+  Minimum allocated size: 4-byte ptrs:  16 bytes    (including 4 overhead)
+                          8-byte ptrs:  24/32 bytes (including, 4/8 overhead)
+
+       When a chunk is freed, 12 (for 4byte ptrs) or 20 (for 8 byte
+       ptrs but 4 byte size) or 24 (for 8/8) additional bytes are
+       needed; 4 (8) for a trailing size field and 8 (16) bytes for
+       free list pointers. Thus, the minimum allocatable size is
+       16/24/32 bytes.
+
+       Even a request for zero bytes (i.e., malloc(0)) returns a
+       pointer to something of the minimum allocatable size.
+
+       The maximum overhead wastage (i.e., number of extra bytes
+       allocated than were requested in malloc) is less than or equal
+       to the minimum size, except for requests >= mmap_threshold that
+       are serviced via mmap(), where the worst case wastage is 2 *
+       sizeof(size_t) bytes plus the remainder from a system page (the
+       minimal mmap unit); typically 4096 or 8192 bytes.
+
+  Maximum allocated size:  4-byte size_t: 2^32 minus about two pages
+                           8-byte size_t: 2^64 minus about two pages
+
+       It is assumed that (possibly signed) size_t values suffice to
+       represent chunk sizes. `Possibly signed' is due to the fact
+       that `size_t' may be defined on a system as either a signed or
+       an unsigned type. The ISO C standard says that it must be
+       unsigned, but a few systems are known not to adhere to this.
+       Additionally, even when size_t is unsigned, sbrk (which is by
+       default used to obtain memory from system) accepts signed
+       arguments, and may not be able to handle size_t-wide arguments
+       with negative sign bit.  Generally, values that would
+       appear as negative after accounting for overhead and alignment
+       are supported only via mmap(), which does not have this
+       limitation.
+
+       Requests for sizes outside the allowed range will perform an optional
+       failure action and then return null. (Requests may also
+       also fail because a system is out of memory.)
+
+  Thread-safety: thread-safe unless NO_THREADS is defined
+
+  Compliance: I believe it is compliant with the 1997 Single Unix Specification
+       (See http://www.opennc.org). Also SVID/XPG, ANSI C, and probably
+       others as well.
+
+* Synopsis of compile-time options:
+
+    People have reported using previous versions of this malloc on all
+    versions of Unix, sometimes by tweaking some of the defines
+    below. It has been tested most extensively on Solaris and
+    Linux. It is also reported to work on WIN32 platforms.
+    People also report using it in stand-alone embedded systems.
+
+    The implementation is in straight, hand-tuned ANSI C.  It is not
+    at all modular. (Sorry!)  It uses a lot of macros.  To be at all
+    usable, this code should be compiled using an optimizing compiler
+    (for example gcc -O3) that can simplify expressions and control
+    paths. (FAQ: some macros import variables as arguments rather than
+    declare locals because people reported that some debuggers
+    otherwise get confused.)
+
+    OPTION                     DEFAULT VALUE
+
+    Compilation Environment options:
+
+    __STD_C                    derived from C compiler defines
+    WIN32                      NOT defined
+    HAVE_MEMCPY                defined
+    USE_MEMCPY                 1 if HAVE_MEMCPY is defined
+    HAVE_MMAP                  defined as 1
+    MMAP_CLEARS                1
+    HAVE_MREMAP                0 unless linux defined
+    USE_ARENAS                 the same as HAVE_MMAP
+    malloc_getpagesize         derived from system #includes, or 4096 if not
+    HAVE_USR_INCLUDE_MALLOC_H  NOT defined
+    LACKS_UNISTD_H             NOT defined unless WIN32
+    LACKS_SYS_PARAM_H          NOT defined unless WIN32
+    LACKS_SYS_MMAN_H           NOT defined unless WIN32
+
+    Changing default word sizes:
+
+    INTERNAL_SIZE_T            size_t
+    MALLOC_ALIGNMENT           2 * sizeof(INTERNAL_SIZE_T)
+
+    Configuration and functionality options:
+
+    USE_DL_PREFIX              NOT defined
+    USE_PUBLIC_MALLOC_WRAPPERS NOT defined
+    USE_MALLOC_LOCK            NOT defined
+    MALLOC_DEBUG               NOT defined
+    REALLOC_ZERO_BYTES_FREES   1
+    MALLOC_FAILURE_ACTION      errno = ENOMEM, if __STD_C defined, else no-op
+    TRIM_FASTBINS              0
+    FIRST_SORTED_BIN_SIZE      512
+
+    Options for customizing MORECORE:
+
+    MORECORE                   sbrk
+    MORECORE_FAILURE           -1
+    MORECORE_CONTIGUOUS        1
+    MORECORE_CANNOT_TRIM       NOT defined
+    MORECORE_CLEARS            1
+    MMAP_AS_MORECORE_SIZE      (1024 * 1024)
+
+    Tuning options that are also dynamically changeable via mallopt:
+
+    DEFAULT_MXFAST             64
+    DEFAULT_TRIM_THRESHOLD     128 * 1024
+    DEFAULT_TOP_PAD            0
+    DEFAULT_MMAP_THRESHOLD     128 * 1024
+    DEFAULT_MMAP_MAX           65536
+
+    There are several other #defined constants and macros that you
+    probably don't want to touch unless you are extending or adapting malloc.  */
+
+/*
+  __STD_C should be nonzero if using ANSI-standard C compiler, a C++
+  compiler, or a C compiler sufficiently close to ANSI to get away
+  with it.
+*/
+
+#include "cvmx-config.h"
+#include "cvmx.h"
+#include "cvmx-spinlock.h"
+#include "cvmx-malloc.h"
+
+
+#ifndef __STD_C
+#if defined(__STDC__) || defined(__cplusplus)
+#define __STD_C     1
+#else
+#define __STD_C     0
+#endif
+#endif /*__STD_C*/
+
+
+/*
+  Void_t* is the pointer type that malloc should say it returns
+*/
+
+#ifndef Void_t
+#if 1
+#define Void_t      void
+#else
+#define Void_t      char
+#endif
+#endif /*Void_t*/
+
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/* define LACKS_UNISTD_H if your system does not have a <unistd.h>. */
+
+/* #define  LACKS_UNISTD_H */
+
+#ifndef LACKS_UNISTD_H
+#include <unistd.h>
+#endif
+
+/* define LACKS_SYS_PARAM_H if your system does not have a <sys/param.h>. */
+
+/* #define  LACKS_SYS_PARAM_H */
+
+
+#include <stdio.h>    /* needed for malloc_stats */
+#include <errno.h>    /* needed for optional MALLOC_FAILURE_ACTION */
+
+
+/*
+  Debugging:
+
+  Because freed chunks may be overwritten with bookkeeping fields, this
+  malloc will often die when freed memory is overwritten by user
+  programs.  This can be very effective (albeit in an annoying way)
+  in helping track down dangling pointers.
+
+  If you compile with -DMALLOC_DEBUG, a number of assertion checks are
+  enabled that will catch more memory errors. You probably won't be
+  able to make much sense of the actual assertion errors, but they
+  should help you locate incorrectly overwritten memory.  The checking
+  is fairly extensive, and will slow down execution
+  noticeably. Calling malloc_stats or mallinfo with MALLOC_DEBUG set
+  will attempt to check every non-mmapped allocated and free chunk in
+  the course of computing the summmaries. (By nature, mmapped regions
+  cannot be checked very much automatically.)
+
+  Setting MALLOC_DEBUG may also be helpful if you are trying to modify
+  this code. The assertions in the check routines spell out in more
+  detail the assumptions and invariants underlying the algorithms.
+
+  Setting MALLOC_DEBUG does NOT provide an automated mechanism for
+  checking that all accesses to malloced memory stay within their
+  bounds. However, there are several add-ons and adaptations of this
+  or other mallocs available that do this.
+*/
+
+#define MALLOC_DEBUG 1
+#if MALLOC_DEBUG
+#include <assert.h>
+#else
+#define assert(x) ((void)0)
+#endif
+
+
+/*
+  INTERNAL_SIZE_T is the word-size used for internal bookkeeping
+  of chunk sizes.
+
+  The default version is the same as size_t.
+
+  While not strictly necessary, it is best to define this as an
+  unsigned type, even if size_t is a signed type. This may avoid some
+  artificial size limitations on some systems.
+
+  On a 64-bit machine, you may be able to reduce malloc overhead by
+  defining INTERNAL_SIZE_T to be a 32 bit `unsigned int' at the
+  expense of not being able to handle more than 2^32 of malloced
+  space. If this limitation is acceptable, you are encouraged to set
+  this unless you are on a platform requiring 16byte alignments. In
+  this case the alignment requirements turn out to negate any
+  potential advantages of decreasing size_t word size.
+
+  Implementors: Beware of the possible combinations of:
+     - INTERNAL_SIZE_T might be signed or unsigned, might be 32 or 64 bits,
+       and might be the same width as int or as long
+     - size_t might have different width and signedness as INTERNAL_SIZE_T
+     - int and long might be 32 or 64 bits, and might be the same width
+  To deal with this, most comparisons and difference computations
+  among INTERNAL_SIZE_Ts should cast them to unsigned long, being
+  aware of the fact that casting an unsigned int to a wider long does
+  not sign-extend. (This also makes checking for negative numbers
+  awkward.) Some of these casts result in harmless compiler warnings
+  on some systems.
+*/
+
+#ifndef INTERNAL_SIZE_T
+#define INTERNAL_SIZE_T size_t
+#endif
+
+/* The corresponding word size */
+#define SIZE_SZ                (sizeof(INTERNAL_SIZE_T))
+
+
+/*
+  MALLOC_ALIGNMENT is the minimum alignment for malloc'ed chunks.
+  It must be a power of two at least 2 * SIZE_SZ, even on machines
+  for which smaller alignments would suffice. It may be defined as
+  larger than this though. Note however that code and data structures
+  are optimized for the case of 8-byte alignment.
+*/
+
+
+#ifndef MALLOC_ALIGNMENT
+#define MALLOC_ALIGNMENT       (2 * SIZE_SZ)
+#endif
+
+/* The corresponding bit mask value */
+#define MALLOC_ALIGN_MASK      (MALLOC_ALIGNMENT - 1)
+
+
+
+/*
+  REALLOC_ZERO_BYTES_FREES should be set if a call to
+  realloc with zero bytes should be the same as a call to free.
+  This is required by the C standard. Otherwise, since this malloc
+  returns a unique pointer for malloc(0), so does realloc(p, 0).
+*/
+
+#ifndef REALLOC_ZERO_BYTES_FREES
+#define REALLOC_ZERO_BYTES_FREES 1
+#endif
+
+/*
+  TRIM_FASTBINS controls whether free() of a very small chunk can
+  immediately lead to trimming. Setting to true (1) can reduce memory
+  footprint, but will almost always slow down programs that use a lot
+  of small chunks.
+
+  Define this only if you are willing to give up some speed to more
+  aggressively reduce system-level memory footprint when releasing
+  memory in programs that use many small chunks.  You can get
+  essentially the same effect by setting MXFAST to 0, but this can
+  lead to even greater slowdowns in programs using many small chunks.
+  TRIM_FASTBINS is an in-between compile-time option, that disables
+  only those chunks bordering topmost memory from being placed in
+  fastbins.
+*/
+
+#ifndef TRIM_FASTBINS
+#define TRIM_FASTBINS  0
+#endif
+
+
+/*
+  USE_DL_PREFIX will prefix all public routines with the string 'dl'.
+  This is necessary when you only want to use this malloc in one part
+  of a program, using your regular system malloc elsewhere.
+*/
+
+#define USE_DL_PREFIX
+
+
+/*
+   Two-phase name translation.
+   All of the actual routines are given mangled names.
+   When wrappers are used, they become the public callable versions.
+   When DL_PREFIX is used, the callable names are prefixed.
+*/
+
+#ifdef USE_DL_PREFIX
+#define public_cALLOc    cvmx_calloc
+#define public_fREe      cvmx_free
+#define public_cFREe     dlcfree
+#define public_mALLOc    cvmx_malloc
+#define public_mEMALIGn  cvmx_memalign
+#define public_rEALLOc   cvmx_realloc
+#define public_vALLOc    dlvalloc
+#define public_pVALLOc   dlpvalloc
+#define public_mALLINFo  dlmallinfo
+#define public_mALLOPt   dlmallopt
+#define public_mTRIm     dlmalloc_trim
+#define public_mSTATs    dlmalloc_stats
+#define public_mUSABLe   dlmalloc_usable_size
+#define public_iCALLOc   dlindependent_calloc
+#define public_iCOMALLOc dlindependent_comalloc
+#define public_gET_STATe dlget_state
+#define public_sET_STATe dlset_state
+#else /* USE_DL_PREFIX */
+#ifdef _LIBC
+#error _LIBC defined and should not be
+/* Special defines for the GNU C library.  */
+#define public_cALLOc    __libc_calloc
+#define public_fREe      __libc_free
+#define public_cFREe     __libc_cfree
+#define public_mALLOc    __libc_malloc
+#define public_mEMALIGn  __libc_memalign
+#define public_rEALLOc   __libc_realloc
+#define public_vALLOc    __libc_valloc
+#define public_pVALLOc   __libc_pvalloc
+#define public_mALLINFo  __libc_mallinfo
+#define public_mALLOPt   __libc_mallopt
+#define public_mTRIm     __malloc_trim
+#define public_mSTATs    __malloc_stats
+#define public_mUSABLe   __malloc_usable_size
+#define public_iCALLOc   __libc_independent_calloc
+#define public_iCOMALLOc __libc_independent_comalloc
+#define public_gET_STATe __malloc_get_state
+#define public_sET_STATe __malloc_set_state
+#define malloc_getpagesize __getpagesize()
+#define open             __open
+#define mmap             __mmap
+#define munmap           __munmap
+#define mremap           __mremap
+#define mprotect         __mprotect
+#define MORECORE         (*__morecore)
+#define MORECORE_FAILURE 0
+
+Void_t * __default_morecore (ptrdiff_t);
+Void_t *(*__morecore)(ptrdiff_t) = __default_morecore;
+
+#else /* !_LIBC */
+#define public_cALLOc    calloc
+#define public_fREe      free
+#define public_cFREe     cfree
+#define public_mALLOc    malloc
+#define public_mEMALIGn  memalign
+#define public_rEALLOc   realloc
+#define public_vALLOc    valloc
+#define public_pVALLOc   pvalloc
+#define public_mALLINFo  mallinfo
+#define public_mALLOPt   mallopt
+#define public_mTRIm     malloc_trim
+#define public_mSTATs    malloc_stats
+#define public_mUSABLe   malloc_usable_size
+#define public_iCALLOc   independent_calloc
+#define public_iCOMALLOc independent_comalloc
+#define public_gET_STATe malloc_get_state
+#define public_sET_STATe malloc_set_state
+#endif /* _LIBC */
+#endif /* USE_DL_PREFIX */
+
+
+/*
+  HAVE_MEMCPY should be defined if you are not otherwise using
+  ANSI STD C, but still have memcpy and memset in your C library
+  and want to use them in calloc and realloc. Otherwise simple
+  macro versions are defined below.
+
+  USE_MEMCPY should be defined as 1 if you actually want to
+  have memset and memcpy called. People report that the macro
+  versions are faster than libc versions on some systems.
+
+  Even if USE_MEMCPY is set to 1, loops to copy/clear small chunks
+  (of <= 36 bytes) are manually unrolled in realloc and calloc.
+*/
+
+#define HAVE_MEMCPY
+
+#ifndef USE_MEMCPY
+#ifdef HAVE_MEMCPY
+#define USE_MEMCPY 1
+#else
+#define USE_MEMCPY 0
+#endif
+#endif
+
+
+#if (__STD_C || defined(HAVE_MEMCPY))
+
+#ifdef WIN32
+/* On Win32 memset and memcpy are already declared in windows.h */
+#else
+#if __STD_C
+void* memset(void*, int, size_t);
+void* memcpy(void*, const void*, size_t);
+#else
+Void_t* memset();
+Void_t* memcpy();
+#endif
+#endif
+#endif
+
+/*
+  MALLOC_FAILURE_ACTION is the action to take before "return 0" when
+  malloc fails to be able to return memory, either because memory is
+  exhausted or because of illegal arguments.
+
+  By default, sets errno if running on STD_C platform, else does nothing.
+*/
+
+#ifndef MALLOC_FAILURE_ACTION
+#if __STD_C
+#define MALLOC_FAILURE_ACTION \
+   errno = ENOMEM;
+
+#else
+#define MALLOC_FAILURE_ACTION
+#endif
+#endif
+
+/*
+  MORECORE-related declarations. By default, rely on sbrk
+*/
+
+
+#ifdef LACKS_UNISTD_H
+#if !defined(__FreeBSD__) && !defined(__OpenBSD__) && !defined(__NetBSD__)
+#if __STD_C
+extern Void_t*     sbrk(ptrdiff_t);
+#else
+extern Void_t*     sbrk();
+#endif
+#endif
+#endif
+
+/*
+  MORECORE is the name of the routine to call to obtain more memory
+  from the system.  See below for general guidance on writing
+  alternative MORECORE functions, as well as a version for WIN32 and a
+  sample version for pre-OSX macos.
+*/
+#undef MORECORE  // not supported
+#ifndef MORECORE
+#define MORECORE notsupported
+#endif
+
+/*
+  MORECORE_FAILURE is the value returned upon failure of MORECORE
+  as well as mmap. Since it cannot be an otherwise valid memory address,
+  and must reflect values of standard sys calls, you probably ought not
+  try to redefine it.
+*/
+
+#ifndef MORECORE_FAILURE
+#define MORECORE_FAILURE (-1)
+#endif
+
+/*
+  If MORECORE_CONTIGUOUS is true, take advantage of fact that
+  consecutive calls to MORECORE with positive arguments always return
+  contiguous increasing addresses.  This is true of unix sbrk.  Even
+  if not defined, when regions happen to be contiguous, malloc will
+  permit allocations spanning regions obtained from different
+  calls. But defining this when applicable enables some stronger
+  consistency checks and space efficiencies.
+*/
+
+#ifndef MORECORE_CONTIGUOUS
+#define MORECORE_CONTIGUOUS 0
+#endif
+
+/*
+  Define MORECORE_CANNOT_TRIM if your version of MORECORE
+  cannot release space back to the system when given negative
+  arguments. This is generally necessary only if you are using
+  a hand-crafted MORECORE function that cannot handle negative arguments.
+*/
+
+#define MORECORE_CANNOT_TRIM 1
+
+/*  MORECORE_CLEARS           (default 1)
+     The degree to which the routine mapped to MORECORE zeroes out
+     memory: never (0), only for newly allocated space (1) or always
+     (2).  The distinction between (1) and (2) is necessary because on
+     some systems, if the application first decrements and then
+     increments the break value, the contents of the reallocated space
+     are unspecified.
+*/
+
+#ifndef MORECORE_CLEARS
+#define MORECORE_CLEARS 0
+#endif
+
+
+/*
+  Define HAVE_MMAP as true to optionally make malloc() use mmap() to
+  allocate very large blocks.  These will be returned to the
+  operating system immediately after a free(). Also, if mmap
+  is available, it is used as a backup strategy in cases where
+  MORECORE fails to provide space from system.
+
+  This malloc is best tuned to work with mmap for large requests.
+  If you do not have mmap, operations involving very large chunks (1MB
+  or so) may be slower than you'd like.
+*/
+
+#undef HAVE_MMAP
+#ifndef HAVE_MMAP
+#define HAVE_MMAP 0
+
+/*
+   Standard unix mmap using /dev/zero clears memory so calloc doesn't
+   need to.
+*/
+
+#ifndef MMAP_CLEARS
+#define MMAP_CLEARS 0
+#endif
+
+#else /* no mmap */
+#ifndef MMAP_CLEARS
+#define MMAP_CLEARS 0
+#endif
+#endif
+
+
+/*
+   MMAP_AS_MORECORE_SIZE is the minimum mmap size argument to use if
+   sbrk fails, and mmap is used as a backup (which is done only if
+   HAVE_MMAP).  The value must be a multiple of page size.  This
+   backup strategy generally applies only when systems have "holes" in
+   address space, so sbrk cannot perform contiguous expansion, but
+   there is still space available on system.  On systems for which
+   this is known to be useful (i.e. most linux kernels), this occurs
+   only when programs allocate huge amounts of memory.  Between this,
+   and the fact that mmap regions tend to be limited, the size should
+   be large, to avoid too many mmap calls and thus avoid running out
+   of kernel resources.
+*/
+
+#ifndef MMAP_AS_MORECORE_SIZE
+#define MMAP_AS_MORECORE_SIZE (1024 * 1024)
+#endif
+
+/*
+  Define HAVE_MREMAP to make realloc() use mremap() to re-allocate
+  large blocks.  This is currently only possible on Linux with
+  kernel versions newer than 1.3.77.
+*/
+#undef linux
+#ifndef HAVE_MREMAP
+#ifdef linux
+#define HAVE_MREMAP 1
+#else
+#define HAVE_MREMAP 0
+#endif
+
+#endif /* HAVE_MMAP */
+
+/* Define USE_ARENAS to enable support for multiple `arenas'.  These
+   are allocated using mmap(), are necessary for threads and
+   occasionally useful to overcome address space limitations affecting
+   sbrk(). */
+
+#ifndef USE_ARENAS
+#define USE_ARENAS 1  // we 'manually' mmap the arenas.....
+#endif
+
+
+/*
+  The system page size. To the extent possible, this malloc manages
+  memory from the system in page-size units.  Note that this value is
+  cached during initialization into a field of malloc_state. So even
+  if malloc_getpagesize is a function, it is only called once.
+
+  The following mechanics for getpagesize were adapted from bsd/gnu
+  getpagesize.h. If none of the system-probes here apply, a value of
+  4096 is used, which should be OK: If they don't apply, then using
+  the actual value probably doesn't impact performance.
+*/
+
+
+#define malloc_getpagesize (4096)
+#ifndef malloc_getpagesize
+
+#ifndef LACKS_UNISTD_H
+#  include <unistd.h>
+#endif
+
+#  ifdef _SC_PAGESIZE         /* some SVR4 systems omit an underscore */
+#    ifndef _SC_PAGE_SIZE
+#      define _SC_PAGE_SIZE _SC_PAGESIZE
+#    endif
+#  endif
+
+#  ifdef _SC_PAGE_SIZE
+#    define malloc_getpagesize sysconf(_SC_PAGE_SIZE)
+#  else
+#    if defined(BSD) || defined(DGUX) || defined(HAVE_GETPAGESIZE)
+       extern size_t getpagesize();
+#      define malloc_getpagesize getpagesize()
+#    else
+#      ifdef WIN32 /* use supplied emulation of getpagesize */
+#        define malloc_getpagesize getpagesize()
+#      else
+#        ifndef LACKS_SYS_PARAM_H
+#          include <sys/param.h>
+#        endif
+#        ifdef EXEC_PAGESIZE
+#          define malloc_getpagesize EXEC_PAGESIZE
+#        else
+#          ifdef NBPG
+#            ifndef CLSIZE
+#              define malloc_getpagesize NBPG
+#            else
+#              define malloc_getpagesize (NBPG * CLSIZE)
+#            endif
+#          else
+#            ifdef NBPC
+#              define malloc_getpagesize NBPC
+#            else
+#              ifdef PAGESIZE
+#                define malloc_getpagesize PAGESIZE
+#              else /* just guess */
+#                define malloc_getpagesize (4096)
+#              endif
+#            endif
+#          endif
+#        endif
+#      endif
+#    endif
+#  endif
+#endif
+
+/*
+  This version of malloc supports the standard SVID/XPG mallinfo
+  routine that returns a struct containing usage properties and
+  statistics. It should work on any SVID/XPG compliant system that has
+  a /usr/include/malloc.h defining struct mallinfo. (If you'd like to
+  install such a thing yourself, cut out the preliminary declarations
+  as described above and below and save them in a malloc.h file. But
+  there's no compelling reason to bother to do this.)
+
+  The main declaration needed is the mallinfo struct that is returned
+  (by-copy) by mallinfo().  The SVID/XPG malloinfo struct contains a
+  bunch of fields that are not even meaningful in this version of
+  malloc.  These fields are are instead filled by mallinfo() with
+  other numbers that might be of interest.
+
+  HAVE_USR_INCLUDE_MALLOC_H should be set if you have a
+  /usr/include/malloc.h file that includes a declaration of struct
+  mallinfo.  If so, it is included; else an SVID2/XPG2 compliant
+  version is declared below.  These must be precisely the same for
+  mallinfo() to work.  The original SVID version of this struct,
+  defined on most systems with mallinfo, declares all fields as
+  ints. But some others define as unsigned long. If your system
+  defines the fields using a type of different width than listed here,
+  you must #include your system version and #define
+  HAVE_USR_INCLUDE_MALLOC_H.
+*/
+
+/* #define HAVE_USR_INCLUDE_MALLOC_H */
+
+#ifdef HAVE_USR_INCLUDE_MALLOC_H
+#include "/usr/include/malloc.h"
+#endif
+
+
+/* ---------- description of public routines ------------ */
+
+/*
+  malloc(size_t n)
+  Returns a pointer to a newly allocated chunk of at least n bytes, or null
+  if no space is available. Additionally, on failure, errno is
+  set to ENOMEM on ANSI C systems.
+
+  If n is zero, malloc returns a minumum-sized chunk. (The minimum
+  size is 16 bytes on most 32bit systems, and 24 or 32 bytes on 64bit
+  systems.)  On most systems, size_t is an unsigned type, so calls
+  with negative arguments are interpreted as requests for huge amounts
+  of space, which will often fail. The maximum supported value of n
+  differs across systems, but is in all cases less than the maximum
+  representable value of a size_t.
+*/
+#if __STD_C
+Void_t*  public_mALLOc(cvmx_arena_list_t arena_list, size_t);
+#else
+Void_t*  public_mALLOc();
+#endif
+
+/*
+  free(Void_t* p)
+  Releases the chunk of memory pointed to by p, that had been previously
+  allocated using malloc or a related routine such as realloc.
+  It has no effect if p is null. It can have arbitrary (i.e., bad!)
+  effects if p has already been freed.
+
+  Unless disabled (using mallopt), freeing very large spaces will
+  when possible, automatically trigger operations that give
+  back unused memory to the system, thus reducing program footprint.
+*/
+#if __STD_C
+void     public_fREe(Void_t*);
+#else
+void     public_fREe();
+#endif
+
+/*
+  calloc(size_t n_elements, size_t element_size);
+  Returns a pointer to n_elements * element_size bytes, with all locations
+  set to zero.
+*/
+#if __STD_C
+Void_t*  public_cALLOc(cvmx_arena_list_t arena_list, size_t, size_t);
+#else
+Void_t*  public_cALLOc();
+#endif
+
+/*
+  realloc(Void_t* p, size_t n)
+  Returns a pointer to a chunk of size n that contains the same data
+  as does chunk p up to the minimum of (n, p's size) bytes, or null
+  if no space is available.
+
+  The returned pointer may or may not be the same as p. The algorithm
+  prefers extending p when possible, otherwise it employs the
+  equivalent of a malloc-copy-free sequence.
+
+  If p is null, realloc is equivalent to malloc.
+
+  If space is not available, realloc returns null, errno is set (if on
+  ANSI) and p is NOT freed.
+
+  if n is for fewer bytes than already held by p, the newly unused
+  space is lopped off and freed if possible.  Unless the #define
+  REALLOC_ZERO_BYTES_FREES is set, realloc with a size argument of
+  zero (re)allocates a minimum-sized chunk.
+
+  Large chunks that were internally obtained via mmap will always
+  be reallocated using malloc-copy-free sequences unless
+  the system supports MREMAP (currently only linux).
+
+  The old unix realloc convention of allowing the last-free'd chunk
+  to be used as an argument to realloc is not supported.
+*/
+#if __STD_C
+Void_t*  public_rEALLOc(cvmx_arena_list_t arena_list, Void_t*, size_t);
+#else
+Void_t*  public_rEALLOc();
+#endif
+
+/*
+  memalign(size_t alignment, size_t n);
+  Returns a pointer to a newly allocated chunk of n bytes, aligned
+  in accord with the alignment argument.
+
+  The alignment argument should be a power of two. If the argument is
+  not a power of two, the nearest greater power is used.
+  8-byte alignment is guaranteed by normal malloc calls, so don't
+  bother calling memalign with an argument of 8 or less.
+
+  Overreliance on memalign is a sure way to fragment space.
+*/
+#if __STD_C
+Void_t*  public_mEMALIGn(cvmx_arena_list_t arena_list, size_t, size_t);
+#else
+Void_t*  public_mEMALIGn();
+#endif
+
+/*
+  valloc(size_t n);
+  Equivalent to memalign(pagesize, n), where pagesize is the page
+  size of the system. If the pagesize is unknown, 4096 is used.
+*/
+#if __STD_C
+Void_t*  public_vALLOc(size_t);
+#else
+Void_t*  public_vALLOc();
+#endif
+
+
+
+/*
+  mallopt(int parameter_number, int parameter_value)
+  Sets tunable parameters The format is to provide a
+  (parameter-number, parameter-value) pair.  mallopt then sets the
+  corresponding parameter to the argument value if it can (i.e., so
+  long as the value is meaningful), and returns 1 if successful else
+  0.  SVID/XPG/ANSI defines four standard param numbers for mallopt,
+  normally defined in malloc.h.  Only one of these (M_MXFAST) is used
+  in this malloc. The others (M_NLBLKS, M_GRAIN, M_KEEP) don't apply,
+  so setting them has no effect. But this malloc also supports four
+  other options in mallopt. See below for details.  Briefly, supported
+  parameters are as follows (listed defaults are for "typical"
+  configurations).
+
+  Symbol            param #   default    allowed param values
+  M_MXFAST          1         64         0-80  (0 disables fastbins)
+  M_TRIM_THRESHOLD -1         128*1024   any   (-1U disables trimming)
+  M_TOP_PAD        -2         0          any
+  M_MMAP_THRESHOLD -3         128*1024   any   (or 0 if no MMAP support)
+  M_MMAP_MAX       -4         65536      any   (0 disables use of mmap)
+*/
+#if __STD_C
+int      public_mALLOPt(int, int);
+#else
+int      public_mALLOPt();
+#endif
+
+
+/*
+  mallinfo()
+  Returns (by copy) a struct containing various summary statistics:
+
+  arena:     current total non-mmapped bytes allocated from system
+  ordblks:   the number of free chunks
+  smblks:    the number of fastbin blocks (i.e., small chunks that
+               have been freed but not use resused or consolidated)
+  hblks:     current number of mmapped regions
+  hblkhd:    total bytes held in mmapped regions
+  usmblks:   the maximum total allocated space. This will be greater
+                than current total if trimming has occurred.
+  fsmblks:   total bytes held in fastbin blocks
+  uordblks:  current total allocated space (normal or mmapped)
+  fordblks:  total free space
+  keepcost:  the maximum number of bytes that could ideally be released
+               back to system via malloc_trim. ("ideally" means that
+               it ignores page restrictions etc.)
+
+  Because these fields are ints, but internal bookkeeping may
+  be kept as longs, the reported values may wrap around zero and
+  thus be inaccurate.
+*/
+#if __STD_C
+struct mallinfo public_mALLINFo(void);
+#else
+struct mallinfo public_mALLINFo();
+#endif
+
+/*
+  independent_calloc(size_t n_elements, size_t element_size, Void_t* chunks[]);
+
+  independent_calloc is similar to calloc, but instead of returning a
+  single cleared space, it returns an array of pointers to n_elements
+  independent elements that can hold contents of size elem_size, each
+  of which starts out cleared, and can be independently freed,
+  realloc'ed etc. The elements are guaranteed to be adjacently
+  allocated (this is not guaranteed to occur with multiple callocs or
+  mallocs), which may also improve cache locality in some
+  applications.
+
+  The "chunks" argument is optional (i.e., may be null, which is
+  probably the most typical usage). If it is null, the returned array
+  is itself dynamically allocated and should also be freed when it is
+  no longer needed. Otherwise, the chunks array must be of at least
+  n_elements in length. It is filled in with the pointers to the
+  chunks.
+
+  In either case, independent_calloc returns this pointer array, or
+  null if the allocation failed.  If n_elements is zero and "chunks"
+  is null, it returns a chunk representing an array with zero elements
+  (which should be freed if not wanted).
+
+  Each element must be individually freed when it is no longer
+  needed. If you'd like to instead be able to free all at once, you
+  should instead use regular calloc and assign pointers into this
+  space to represent elements.  (In this case though, you cannot
+  independently free elements.)
+
+  independent_calloc simplifies and speeds up implementations of many
+  kinds of pools.  It may also be useful when constructing large data
+  structures that initially have a fixed number of fixed-sized nodes,
+  but the number is not known at compile time, and some of the nodes
+  may later need to be freed. For example:
+
+  struct Node { int item; struct Node* next; };
+
+  struct Node* build_list() {
+    struct Node** pool;
+    int n = read_number_of_nodes_needed();
+    if (n <= 0) return 0;
+    pool = (struct Node**)(independent_calloc(n, sizeof(struct Node), 0);
+    if (pool == 0) die();
+    // organize into a linked list...
+    struct Node* first = pool[0];
+    for (i = 0; i < n-1; ++i)
+      pool[i]->next = pool[i+1];
+    free(pool);     // Can now free the array (or not, if it is needed later)
+    return first;
+  }
+*/
+#if __STD_C
+Void_t** public_iCALLOc(size_t, size_t, Void_t**);
+#else
+Void_t** public_iCALLOc();
+#endif
+
+/*
+  independent_comalloc(size_t n_elements, size_t sizes[], Void_t* chunks[]);
+
+  independent_comalloc allocates, all at once, a set of n_elements
+  chunks with sizes indicated in the "sizes" array.    It returns
+  an array of pointers to these elements, each of which can be
+  independently freed, realloc'ed etc. The elements are guaranteed to
+  be adjacently allocated (this is not guaranteed to occur with
+  multiple callocs or mallocs), which may also improve cache locality
+  in some applications.
+
+  The "chunks" argument is optional (i.e., may be null). If it is null
+  the returned array is itself dynamically allocated and should also
+  be freed when it is no longer needed. Otherwise, the chunks array
+  must be of at least n_elements in length. It is filled in with the
+  pointers to the chunks.
+
+  In either case, independent_comalloc returns this pointer array, or
+  null if the allocation failed.  If n_elements is zero and chunks is
+  null, it returns a chunk representing an array with zero elements
+  (which should be freed if not wanted).
+
+  Each element must be individually freed when it is no longer
+  needed. If you'd like to instead be able to free all at once, you
+  should instead use a single regular malloc, and assign pointers at
+  particular offsets in the aggregate space. (In this case though, you
+  cannot independently free elements.)
+
+  independent_comallac differs from independent_calloc in that each
+  element may have a different size, and also that it does not
+  automatically clear elements.
+
+  independent_comalloc can be used to speed up allocation in cases
+  where several structs or objects must always be allocated at the
+  same time.  For example:
+
+  struct Head { ... }
+  struct Foot { ... }
+
+  void send_message(char* msg) {
+    int msglen = strlen(msg);
+    size_t sizes[3] = { sizeof(struct Head), msglen, sizeof(struct Foot) };
+    void* chunks[3];
+    if (independent_comalloc(3, sizes, chunks) == 0)
+      die();
+    struct Head* head = (struct Head*)(chunks[0]);
+    char*        body = (char*)(chunks[1]);
+    struct Foot* foot = (struct Foot*)(chunks[2]);
+    // ...
+  }
+
+  In general though, independent_comalloc is worth using only for
+  larger values of n_elements. For small values, you probably won't
+  detect enough difference from series of malloc calls to bother.
+
+  Overuse of independent_comalloc can increase overall memory usage,
+  since it cannot reuse existing noncontiguous small chunks that
+  might be available for some of the elements.
+*/
+#if __STD_C
+Void_t** public_iCOMALLOc(size_t, size_t*, Void_t**);
+#else
+Void_t** public_iCOMALLOc();
+#endif
+
+
+/*
+  pvalloc(size_t n);
+  Equivalent to valloc(minimum-page-that-holds(n)), that is,
+  round up n to nearest pagesize.
+ */
+#if __STD_C
+Void_t*  public_pVALLOc(size_t);
+#else
+Void_t*  public_pVALLOc();
+#endif
+
+/*
+  cfree(Void_t* p);
+  Equivalent to free(p).
+
+  cfree is needed/defined on some systems that pair it with calloc,
+  for odd historical reasons (such as: cfree is used in example
+  code in the first edition of K&R).
+*/
+#if __STD_C
+void     public_cFREe(Void_t*);
+#else
+void     public_cFREe();
+#endif
+
+/*
+  malloc_trim(size_t pad);
+
+  If possible, gives memory back to the system (via negative
+  arguments to sbrk) if there is unused memory at the `high' end of
+  the malloc pool. You can call this after freeing large blocks of
+  memory to potentially reduce the system-level memory requirements
+  of a program. However, it cannot guarantee to reduce memory. Under
+  some allocation patterns, some large free blocks of memory will be
+  locked between two used chunks, so they cannot be given back to
+  the system.
+
+  The `pad' argument to malloc_trim represents the amount of free
+  trailing space to leave untrimmed. If this argument is zero,
+  only the minimum amount of memory to maintain internal data
+  structures will be left (one page or less). Non-zero arguments
+  can be supplied to maintain enough trailing space to service
+  future expected allocations without having to re-obtain memory
+  from the system.
+
+  Malloc_trim returns 1 if it actually released any memory, else 0.
+  On systems that do not support "negative sbrks", it will always
+  rreturn 0.
+*/
+#if __STD_C
+int      public_mTRIm(size_t);
+#else
+int      public_mTRIm();
+#endif
+
+/*
+  malloc_usable_size(Void_t* p);
+
+  Returns the number of bytes you can actually use in
+  an allocated chunk, which may be more than you requested (although
+  often not) due to alignment and minimum size constraints.
+  You can use this many bytes without worrying about
+  overwriting other allocated objects. This is not a particularly great
+  programming practice. malloc_usable_size can be more useful in
+  debugging and assertions, for example:
+
+  p = malloc(n);
+  assert(malloc_usable_size(p) >= 256);
+
+*/
+#if __STD_C
+size_t   public_mUSABLe(Void_t*);
+#else
+size_t   public_mUSABLe();
+#endif
+
+/*
+  malloc_stats();
+  Prints on stderr the amount of space obtained from the system (both
+  via sbrk and mmap), the maximum amount (which may be more than
+  current if malloc_trim and/or munmap got called), and the current
+  number of bytes allocated via malloc (or realloc, etc) but not yet
+  freed. Note that this is the number of bytes allocated, not the
+  number requested. It will be larger than the number requested
+  because of alignment and bookkeeping overhead. Because it includes
+  alignment wastage as being in use, this figure may be greater than
+  zero even when no user-level chunks are allocated.
+
+  The reported current and maximum system memory can be inaccurate if
+  a program makes other calls to system memory allocation functions
+  (normally sbrk) outside of malloc.
+
+  malloc_stats prints only the most commonly interesting statistics.
+  More information can be obtained by calling mallinfo.
+
+*/
+#if __STD_C
+void     public_mSTATs(void);
+#else
+void     public_mSTATs();
+#endif
+
+/*
+  malloc_get_state(void);
+
+  Returns the state of all malloc variables in an opaque data
+  structure.
+*/
+#if __STD_C
+Void_t*  public_gET_STATe(void);
+#else
+Void_t*  public_gET_STATe();
+#endif
+
+/*
+  malloc_set_state(Void_t* state);
+
+  Restore the state of all malloc variables from data obtained with
+  malloc_get_state().
+*/
+#if __STD_C
+int      public_sET_STATe(Void_t*);
+#else
+int      public_sET_STATe();
+#endif
+
+#ifdef _LIBC
+/*
+  posix_memalign(void **memptr, size_t alignment, size_t size);
+
+  POSIX wrapper like memalign(), checking for validity of size.
+*/
+int      __posix_memalign(void **, size_t, size_t);
+#endif
+
+/* mallopt tuning options */
+
+/*
+  M_MXFAST is the maximum request size used for "fastbins", special bins
+  that hold returned chunks without consolidating their spaces. This
+  enables future requests for chunks of the same size to be handled
+  very quickly, but can increase fragmentation, and thus increase the
+  overall memory footprint of a program.
+
+  This malloc manages fastbins very conservatively yet still
+  efficiently, so fragmentation is rarely a problem for values less
+  than or equal to the default.  The maximum supported value of MXFAST
+  is 80. You wouldn't want it any higher than this anyway.  Fastbins
+  are designed especially for use with many small structs, objects or
+  strings -- the default handles structs/objects/arrays with sizes up
+  to 8 4byte fields, or small strings representing words, tokens,
+  etc. Using fastbins for larger objects normally worsens
+  fragmentation without improving speed.
+
+  M_MXFAST is set in REQUEST size units. It is internally used in
+  chunksize units, which adds padding and alignment.  You can reduce
+  M_MXFAST to 0 to disable all use of fastbins.  This causes the malloc
+  algorithm to be a closer approximation of fifo-best-fit in all cases,
+  not just for larger requests, but will generally cause it to be
+  slower.
+*/
+
+
+/* M_MXFAST is a standard SVID/XPG tuning option, usually listed in malloc.h */
+#ifndef M_MXFAST
+#define M_MXFAST            1
+#endif
+
+#ifndef DEFAULT_MXFAST
+#define DEFAULT_MXFAST     64
+#endif
+
+
+/*
+  M_TRIM_THRESHOLD is the maximum amount of unused top-most memory
+  to keep before releasing via malloc_trim in free().
+
+  Automatic trimming is mainly useful in long-lived programs.
+  Because trimming via sbrk can be slow on some systems, and can
+  sometimes be wasteful (in cases where programs immediately
+  afterward allocate more large chunks) the value should be high
+  enough so that your overall system performance would improve by
+  releasing this much memory.
+
+  The trim threshold and the mmap control parameters (see below)
+  can be traded off with one another. Trimming and mmapping are
+  two different ways of releasing unused memory back to the
+  system. Between these two, it is often possible to keep
+  system-level demands of a long-lived program down to a bare
+  minimum. For example, in one test suite of sessions measuring
+  the XF86 X server on Linux, using a trim threshold of 128K and a
+  mmap threshold of 192K led to near-minimal long term resource
+  consumption.
+
+  If you are using this malloc in a long-lived program, it should
+  pay to experiment with these values.  As a rough guide, you
+  might set to a value close to the average size of a process
+  (program) running on your system.  Releasing this much memory
+  would allow such a process to run in memory.  Generally, it's
+  worth it to tune for trimming rather tham memory mapping when a
+  program undergoes phases where several large chunks are
+  allocated and released in ways that can reuse each other's
+  storage, perhaps mixed with phases where there are no such
+  chunks at all.  And in well-behaved long-lived programs,
+  controlling release of large blocks via trimming versus mapping
+  is usually faster.
+
+  However, in most programs, these parameters serve mainly as
+  protection against the system-level effects of carrying around
+  massive amounts of unneeded memory. Since frequent calls to
+  sbrk, mmap, and munmap otherwise degrade performance, the default
+  parameters are set to relatively high values that serve only as
+  safeguards.
+
+  The trim value It must be greater than page size to have any useful
+  effect.  To disable trimming completely, you can set to
+  (unsigned long)(-1)
+
+  Trim settings interact with fastbin (MXFAST) settings: Unless
+  TRIM_FASTBINS is defined, automatic trimming never takes place upon
+  freeing a chunk with size less than or equal to MXFAST. Trimming is
+  instead delayed until subsequent freeing of larger chunks. However,
+  you can still force an attempted trim by calling malloc_trim.
+
+  Also, trimming is not generally possible in cases where
+  the main arena is obtained via mmap.
+
+  Note that the trick some people use of mallocing a huge space and
+  then freeing it at program startup, in an attempt to reserve system
+  memory, doesn't have the intended effect under automatic trimming,
+  since that memory will immediately be returned to the system.
+*/
+
+#define M_TRIM_THRESHOLD       -1
+
+#ifndef DEFAULT_TRIM_THRESHOLD
+#define DEFAULT_TRIM_THRESHOLD (128 * 1024)
+#endif
+
+/*
+  M_TOP_PAD is the amount of extra `padding' space to allocate or
+  retain whenever sbrk is called. It is used in two ways internally:
+
+  * When sbrk is called to extend the top of the arena to satisfy
+  a new malloc request, this much padding is added to the sbrk
+  request.
+
+  * When malloc_trim is called automatically from free(),
+  it is used as the `pad' argument.
+
+  In both cases, the actual amount of padding is rounded
+  so that the end of the arena is always a system page boundary.
+
+  The main reason for using padding is to avoid calling sbrk so
+  often. Having even a small pad greatly reduces the likelihood
+  that nearly every malloc request during program start-up (or
+  after trimming) will invoke sbrk, which needlessly wastes
+  time.
+
+  Automatic rounding-up to page-size units is normally sufficient
+  to avoid measurable overhead, so the default is 0.  However, in
+  systems where sbrk is relatively slow, it can pay to increase
+  this value, at the expense of carrying around more memory than
+  the program needs.
+*/
+
+#define M_TOP_PAD              -2
+
+#ifndef DEFAULT_TOP_PAD
+#define DEFAULT_TOP_PAD        (0)
+#endif
+
+/*
+  M_MMAP_THRESHOLD is the request size threshold for using mmap()
+  to service a request. Requests of at least this size that cannot
+  be allocated using already-existing space will be serviced via mmap.
+  (If enough normal freed space already exists it is used instead.)
+
+  Using mmap segregates relatively large chunks of memory so that
+  they can be individually obtained and released from the host
+  system. A request serviced through mmap is never reused by any
+  other request (at least not directly; the system may just so
+  happen to remap successive requests to the same locations).
+
+  Segregating space in this way has the benefits that:
+
+   1. Mmapped space can ALWAYS be individually released back
+      to the system, which helps keep the system level memory
+      demands of a long-lived program low.
+   2. Mapped memory can never become `locked' between
+      other chunks, as can happen with normally allocated chunks, which
+      means that even trimming via malloc_trim would not release them.
+   3. On some systems with "holes" in address spaces, mmap can obtain
+      memory that sbrk cannot.
+
+  However, it has the disadvantages that:
+
+   1. The space cannot be reclaimed, consolidated, and then
+      used to service later requests, as happens with normal chunks.
+   2. It can lead to more wastage because of mmap page alignment
+      requirements
+   3. It causes malloc performance to be more dependent on host
+      system memory management support routines which may vary in
+      implementation quality and may impose arbitrary
+      limitations. Generally, servicing a request via normal
+      malloc steps is faster than going through a system's mmap.
+
+  The advantages of mmap nearly always outweigh disadvantages for
+  "large" chunks, but the value of "large" varies across systems.  The
+  default is an empirically derived value that works well in most
+  systems.
+*/
+
+#define M_MMAP_THRESHOLD      -3
+
+#ifndef DEFAULT_MMAP_THRESHOLD
+#define DEFAULT_MMAP_THRESHOLD (128 * 1024)
+#endif
+
+/*
+  M_MMAP_MAX is the maximum number of requests to simultaneously
+  service using mmap. This parameter exists because
+  some systems have a limited number of internal tables for
+  use by mmap, and using more than a few of them may degrade
+  performance.
+
+  The default is set to a value that serves only as a safeguard.
+  Setting to 0 disables use of mmap for servicing large requests.  If
+  HAVE_MMAP is not set, the default value is 0, and attempts to set it
+  to non-zero values in mallopt will fail.
+*/
+
+#define M_MMAP_MAX             -4
+
+#ifndef DEFAULT_MMAP_MAX
+#if HAVE_MMAP
+#define DEFAULT_MMAP_MAX       (65536)
+#else
+#define DEFAULT_MMAP_MAX       (0)
+#endif
+#endif
+
+#ifdef __cplusplus
+};  /* end of extern "C" */
+#endif
+
+#include <cvmx-spinlock.h>
+#include "malloc.h"
+#include "thread-m.h"
+
+#ifdef DEBUG_PRINTS
+#define debug_printf    printf
+#else
+#define debug_printf(format, args...)  
+#endif
+
+#ifndef BOUNDED_N
+#define BOUNDED_N(ptr, sz) (ptr)
+#endif
+#ifndef RETURN_ADDRESS
+#define RETURN_ADDRESS(X_) (NULL)
+#endif
+
+/* On some platforms we can compile internal, not exported functions better.
+   Let the environment provide a macro and define it to be empty if it
+   is not available.  */
+#ifndef internal_function
+# define internal_function
+#endif
+
+/* Forward declarations.  */
+struct malloc_chunk;
+typedef struct malloc_chunk* mchunkptr;
+
+/* Internal routines.  */
+
+#if __STD_C
+
+static Void_t*         _int_malloc(mstate, size_t);
+static void            _int_free(mstate, Void_t*);
+static Void_t*         _int_realloc(mstate, Void_t*, size_t);
+static Void_t*         _int_memalign(mstate, size_t, size_t);
+static Void_t*         _int_valloc(mstate, size_t);
+static Void_t*  _int_pvalloc(mstate, size_t);
+static Void_t*  cALLOc(cvmx_arena_list_t arena_list, size_t, size_t);
+static Void_t** _int_icalloc(mstate, size_t, size_t, Void_t**);
+static Void_t** _int_icomalloc(mstate, size_t, size_t*, Void_t**);
+static int      mTRIm(size_t);
+static size_t   mUSABLe(Void_t*);
+static void     mSTATs(void);
+static int      mALLOPt(int, int);
+static struct mallinfo mALLINFo(mstate);
+
+static Void_t* internal_function mem2mem_check(Void_t *p, size_t sz);
+static int internal_function top_check(void);
+static void internal_function munmap_chunk(mchunkptr p);
+#if HAVE_MREMAP
+static mchunkptr internal_function mremap_chunk(mchunkptr p, size_t new_size);
+#endif
+
+static Void_t*   malloc_check(size_t sz, const Void_t *caller);
+static void      free_check(Void_t* mem, const Void_t *caller);
+static Void_t*   realloc_check(Void_t* oldmem, size_t bytes,
+			       const Void_t *caller);
+static Void_t*   memalign_check(size_t alignment, size_t bytes,
+				const Void_t *caller);
+#ifndef NO_THREADS
+static Void_t*   malloc_starter(size_t sz, const Void_t *caller);
+static void      free_starter(Void_t* mem, const Void_t *caller);
+static Void_t*   malloc_atfork(size_t sz, const Void_t *caller);
+static void      free_atfork(Void_t* mem, const Void_t *caller);
+#endif
+
+#else
+
+Void_t*         _int_malloc();
+void            _int_free();
+Void_t*         _int_realloc();
+Void_t*         _int_memalign();
+Void_t*         _int_valloc();
+Void_t*         _int_pvalloc();
+/*static Void_t*  cALLOc();*/
+static Void_t** _int_icalloc();
+static Void_t** _int_icomalloc();
+static int      mTRIm();
+static size_t   mUSABLe();
+static void     mSTATs();
+static int      mALLOPt();
+static struct mallinfo mALLINFo();
+
+#endif
+
+
+
+
+/* ------------- Optional versions of memcopy ---------------- */
+
+
+#if USE_MEMCPY
+
+/*
+  Note: memcpy is ONLY invoked with non-overlapping regions,
+  so the (usually slower) memmove is not needed.
+*/
+
+#define MALLOC_COPY(dest, src, nbytes)  memcpy(dest, src, nbytes)
+#define MALLOC_ZERO(dest, nbytes)       memset(dest, 0,   nbytes)
+
+#else /* !USE_MEMCPY */
+
+/* Use Duff's device for good zeroing/copying performance. */
+
+#define MALLOC_ZERO(charp, nbytes)                                            \
+do {                                                                          \
+  INTERNAL_SIZE_T* mzp = (INTERNAL_SIZE_T*)(charp);                           \
+  unsigned long mctmp = (nbytes)/sizeof(INTERNAL_SIZE_T);                     \
+  long mcn;                                                                   \
+  if (mctmp < 8) mcn = 0; else { mcn = (mctmp-1)/8; mctmp %= 8; }             \
+  switch (mctmp) {                                                            \
+    case 0: for(;;) { *mzp++ = 0;                                             \
+    case 7:           *mzp++ = 0;                                             \
+    case 6:           *mzp++ = 0;                                             \
+    case 5:           *mzp++ = 0;                                             \
+    case 4:           *mzp++ = 0;                                             \
+    case 3:           *mzp++ = 0;                                             \
+    case 2:           *mzp++ = 0;                                             \
+    case 1:           *mzp++ = 0; if(mcn <= 0) break; mcn--; }                \
+  }                                                                           \
+} while(0)
+
+#define MALLOC_COPY(dest,src,nbytes)                                          \
+do {                                                                          \
+  INTERNAL_SIZE_T* mcsrc = (INTERNAL_SIZE_T*) src;                            \
+  INTERNAL_SIZE_T* mcdst = (INTERNAL_SIZE_T*) dest;                           \
+  unsigned long mctmp = (nbytes)/sizeof(INTERNAL_SIZE_T);                     \
+  long mcn;                                                                   \
+  if (mctmp < 8) mcn = 0; else { mcn = (mctmp-1)/8; mctmp %= 8; }             \
+  switch (mctmp) {                                                            \
+    case 0: for(;;) { *mcdst++ = *mcsrc++;                                    \
+    case 7:           *mcdst++ = *mcsrc++;                                    \
+    case 6:           *mcdst++ = *mcsrc++;                                    \
+    case 5:           *mcdst++ = *mcsrc++;                                    \
+    case 4:           *mcdst++ = *mcsrc++;                                    \
+    case 3:           *mcdst++ = *mcsrc++;                                    \
+    case 2:           *mcdst++ = *mcsrc++;                                    \
+    case 1:           *mcdst++ = *mcsrc++; if(mcn <= 0) break; mcn--; }       \
+  }                                                                           \
+} while(0)
+
+#endif
+
+/* ------------------ MMAP support ------------------  */
+
+
+#if HAVE_MMAP
+
+#include <fcntl.h>
+#ifndef LACKS_SYS_MMAN_H
+#include <sys/mman.h>
+#endif
+
+#if !defined(MAP_ANONYMOUS) && defined(MAP_ANON)
+# define MAP_ANONYMOUS MAP_ANON
+#endif
+#if !defined(MAP_FAILED)
+# define MAP_FAILED ((char*)-1)
+#endif
+
+#ifndef MAP_NORESERVE
+# ifdef MAP_AUTORESRV
+#  define MAP_NORESERVE MAP_AUTORESRV
+# else
+#  define MAP_NORESERVE 0
+# endif
+#endif
+
+/*
+   Nearly all versions of mmap support MAP_ANONYMOUS,
+   so the following is unlikely to be needed, but is
+   supplied just in case.
+*/
+
+#ifndef MAP_ANONYMOUS
+
+static int dev_zero_fd = -1; /* Cached file descriptor for /dev/zero. */
+
+#define MMAP(addr, size, prot, flags) ((dev_zero_fd < 0) ? \
+ (dev_zero_fd = open("/dev/zero", O_RDWR), \
+  mmap((addr), (size), (prot), (flags), dev_zero_fd, 0)) : \
+   mmap((addr), (size), (prot), (flags), dev_zero_fd, 0))
+
+#else
+
+#define MMAP(addr, size, prot, flags) \
+ (mmap((addr), (size), (prot), (flags)|MAP_ANONYMOUS, -1, 0))
+
+#endif
+
+
+#endif /* HAVE_MMAP */
+
+
+/*
+  -----------------------  Chunk representations -----------------------
+*/
+
+
+/*
+  This struct declaration is misleading (but accurate and necessary).
+  It declares a "view" into memory allowing access to necessary
+  fields at known offsets from a given base. See explanation below.
+*/
+struct malloc_chunk {
+
+  INTERNAL_SIZE_T      prev_size;  /* Size of previous chunk (if free).  */
+  INTERNAL_SIZE_T      size;       /* Size in bytes, including overhead. */
+  mstate               arena_ptr;  /* ptr to arena chunk belongs to */
+
+  struct malloc_chunk* fd;         /* double links -- used only if free. */
+  struct malloc_chunk* bk;
+};
+
+
+/*
+   malloc_chunk details:
+
+    (The following includes lightly edited explanations by Colin Plumb.)
+
+    Chunks of memory are maintained using a `boundary tag' method as
+    described in e.g., Knuth or Standish.  (See the paper by Paul
+    Wilson ftp://ftp.cs.utexas.edu/pub/garbage/allocsrv.ps for a
+    survey of such techniques.)  Sizes of free chunks are stored both
+    in the front of each chunk and at the end.  This makes
+    consolidating fragmented chunks into bigger chunks very fast.  The
+    size fields also hold bits representing whether chunks are free or
+    in use.
+
+    An allocated chunk looks like this:
+
+
+    chunk-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
+            |             Size of previous chunk, if allocated            | |
+            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
+            |             Size of chunk, in bytes                         |P|
+      mem-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
+            |             User data starts here...                          .
+            .                                                               .
+            .             (malloc_usable_space() bytes)                     .
+            .                                                               |
+nextchunk-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
+            |             Size of chunk                                     |
+            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
+
+
+    Where "chunk" is the front of the chunk for the purpose of most of
+    the malloc code, but "mem" is the pointer that is returned to the
+    user.  "Nextchunk" is the beginning of the next contiguous chunk.
+
+    Chunks always begin on even word boundries, so the mem portion
+    (which is returned to the user) is also on an even word boundary, and
+    thus at least double-word aligned.
+
+    Free chunks are stored in circular doubly-linked lists, and look like this:
+
+    chunk-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
+            |             Size of previous chunk                            |
+            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
+    `head:' |             Size of chunk, in bytes                         |P|
+      mem-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
+            |             Forward pointer to next chunk in list             |
+            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
+            |             Back pointer to previous chunk in list            |
+            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
+            |             Unused space (may be 0 bytes long)                .
+            .                                                               .
+            .                                                               |
+nextchunk-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
+    `foot:' |             Size of chunk, in bytes                           |
+            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
+
+    The P (PREV_INUSE) bit, stored in the unused low-order bit of the
+    chunk size (which is always a multiple of two words), is an in-use
+    bit for the *previous* chunk.  If that bit is *clear*, then the
+    word before the current chunk size contains the previous chunk
+    size, and can be used to find the front of the previous chunk.
+    The very first chunk allocated always has this bit set,
+    preventing access to non-existent (or non-owned) memory. If
+    prev_inuse is set for any given chunk, then you CANNOT determine
+    the size of the previous chunk, and might even get a memory
+    addressing fault when trying to do so.
+
+    Note that the `foot' of the current chunk is actually represented
+    as the prev_size of the NEXT chunk. This makes it easier to
+    deal with alignments etc but can be very confusing when trying
+    to extend or adapt this code.
+
+    The two exceptions to all this are
+
+     1. The special chunk `top' doesn't bother using the
+        trailing size field since there is no next contiguous chunk
+        that would have to index off it. After initialization, `top'
+        is forced to always exist.  If it would become less than
+        MINSIZE bytes long, it is replenished.
+
+     2. Chunks allocated via mmap, which have the second-lowest-order
+        bit (IS_MMAPPED) set in their size fields.  Because they are
+        allocated one-by-one, each must contain its own trailing size field.
+
+*/
+
+/*
+  ---------- Size and alignment checks and conversions ----------
+*/
+
+/* conversion from malloc headers to user pointers, and back */
+/* Added size for pointer to make room for arena_ptr */
+#define chunk2mem(p)   ((Void_t*)((char*)(p) + 2*SIZE_SZ + sizeof(void *)))
+#define mem2chunk(mem) ((mchunkptr)((char*)(mem) - 2*SIZE_SZ - sizeof(void *)))
+
+/* The smallest possible chunk */
+#define MIN_CHUNK_SIZE        (sizeof(struct malloc_chunk))
+
+/* The smallest size we can malloc is an aligned minimal chunk */
+
+#define MINSIZE  \
+  (unsigned long)(((MIN_CHUNK_SIZE+MALLOC_ALIGN_MASK) & ~MALLOC_ALIGN_MASK))
+
+/* Check if m has acceptable alignment */
+
+#define aligned_OK(m)  (((unsigned long)((m)) & (MALLOC_ALIGN_MASK)) == 0)
+
+
+/*
+   Check if a request is so large that it would wrap around zero when
+   padded and aligned. To simplify some other code, the bound is made
+   low enough so that adding MINSIZE will also not wrap around zero.
+*/
+
+#define REQUEST_OUT_OF_RANGE(req)                                 \
+  ((unsigned long)(req) >=                                        \
+   (unsigned long)(INTERNAL_SIZE_T)(-2 * MINSIZE))
+
+/* pad request bytes into a usable size -- internal version */
+
+
+/* prev_size field of next chunk is overwritten with data
+** when in use.  NOTE - last SIZE_SZ of arena must be left
+** unused for last chunk to use
+*/
+/* Added sizeof(void *) to make room for arena_ptr */
+#define request2size(req)                                         \
+  (((req) + sizeof(void *) + SIZE_SZ + MALLOC_ALIGN_MASK < MINSIZE)  ?             \
+   MINSIZE :                                                      \
+   ((req) + sizeof(void *) + SIZE_SZ + MALLOC_ALIGN_MASK) & ~MALLOC_ALIGN_MASK)
+
+/*  Same, except also perform argument check */
+
+#define checked_request2size(req, sz)                             \
+  if (REQUEST_OUT_OF_RANGE(req)) {                                \
+    MALLOC_FAILURE_ACTION;                                        \
+    return 0;                                                     \
+  }                                                               \
+  (sz) = request2size(req);
+
+/*
+  --------------- Physical chunk operations ---------------
+*/
+
+
+/* size field is or'ed with PREV_INUSE when previous adjacent chunk in use */
+#define PREV_INUSE 0x1
+
+/* extract inuse bit of previous chunk */
+#define prev_inuse(p)       ((p)->size & PREV_INUSE)
+
+
+/* size field is or'ed with IS_MMAPPED if the chunk was obtained with mmap() */
+#define IS_MMAPPED 0x2
+
+/* check for mmap()'ed chunk */
+#define chunk_is_mmapped(p) ((p)->size & IS_MMAPPED)
+
+
+
+/*
+  Bits to mask off when extracting size
+
+  Note: IS_MMAPPED is intentionally not masked off from size field in
+  macros for which mmapped chunks should never be seen. This should
+  cause helpful core dumps to occur if it is tried by accident by
+  people extending or adapting this malloc.
+*/
+#define SIZE_BITS (PREV_INUSE|IS_MMAPPED)
+
+/* Get size, ignoring use bits */
+#define chunksize(p)         ((p)->size & ~(SIZE_BITS))
+
+
+/* Ptr to next physical malloc_chunk. */
+#define next_chunk(p) ((mchunkptr)( ((char*)(p)) + ((p)->size & ~SIZE_BITS) ))
+
+/* Ptr to previous physical malloc_chunk */
+#define prev_chunk(p) ((mchunkptr)( ((char*)(p)) - ((p)->prev_size) ))
+
+/* Treat space at ptr + offset as a chunk */
+#define chunk_at_offset(p, s)  ((mchunkptr)(((char*)(p)) + (s)))
+
+/* extract p's inuse bit */
+#define inuse(p)\
+((((mchunkptr)(((char*)(p))+((p)->size & ~SIZE_BITS)))->size) & PREV_INUSE)
+
+/* set/clear chunk as being inuse without otherwise disturbing */
+#define set_inuse(p)\
+((mchunkptr)(((char*)(p)) + ((p)->size & ~SIZE_BITS)))->size |= PREV_INUSE
+
+#define clear_inuse(p)\
+((mchunkptr)(((char*)(p)) + ((p)->size & ~SIZE_BITS)))->size &= ~(PREV_INUSE)
+
+
+/* check/set/clear inuse bits in known places */
+#define inuse_bit_at_offset(p, s)\
+ (((mchunkptr)(((char*)(p)) + (s)))->size & PREV_INUSE)
+
+#define set_inuse_bit_at_offset(p, s)\
+ (((mchunkptr)(((char*)(p)) + (s)))->size |= PREV_INUSE)
+
+#define clear_inuse_bit_at_offset(p, s)\
+ (((mchunkptr)(((char*)(p)) + (s)))->size &= ~(PREV_INUSE))
+
+
+/* Set size at head, without disturbing its use bit */
+#define set_head_size(p, s)  ((p)->size = (((p)->size & SIZE_BITS) | (s)))
+
+/* Set size/use field */
+#define set_head(p, s)       ((p)->size = (s))
+
+/* Set size at footer (only when chunk is not in use) */
+#define set_foot(p, s)       (((mchunkptr)((char*)(p) + (s)))->prev_size = (s))
+
+
+/*
+  -------------------- Internal data structures --------------------
+
+   All internal state is held in an instance of malloc_state defined
+   below. There are no other static variables, except in two optional
+   cases:
+   * If USE_MALLOC_LOCK is defined, the mALLOC_MUTEx declared above.
+   * If HAVE_MMAP is true, but mmap doesn't support
+     MAP_ANONYMOUS, a dummy file descriptor for mmap.
+
+   Beware of lots of tricks that minimize the total bookkeeping space
+   requirements. The result is a little over 1K bytes (for 4byte
+   pointers and size_t.)
+*/
+
+/*
+  Bins
+
+    An array of bin headers for free chunks. Each bin is doubly
+    linked.  The bins are approximately proportionally (log) spaced.
+    There are a lot of these bins (128). This may look excessive, but
+    works very well in practice.  Most bins hold sizes that are
+    unusual as malloc request sizes, but are more usual for fragments
+    and consolidated sets of chunks, which is what these bins hold, so
+    they can be found quickly.  All procedures maintain the invariant
+    that no consolidated chunk physically borders another one, so each
+    chunk in a list is known to be preceeded and followed by either
+    inuse chunks or the ends of memory.
+
+    Chunks in bins are kept in size order, with ties going to the
+    approximately least recently used chunk. Ordering isn't needed
+    for the small bins, which all contain the same-sized chunks, but
+    facilitates best-fit allocation for larger chunks. These lists
+    are just sequential. Keeping them in order almost never requires
+    enough traversal to warrant using fancier ordered data
+    structures.
+
+    Chunks of the same size are linked with the most
+    recently freed at the front, and allocations are taken from the
+    back.  This results in LRU (FIFO) allocation order, which tends
+    to give each chunk an equal opportunity to be consolidated with
+    adjacent freed chunks, resulting in larger free chunks and less
+    fragmentation.
+
+    To simplify use in double-linked lists, each bin header acts
+    as a malloc_chunk. This avoids special-casing for headers.
+    But to conserve space and improve locality, we allocate
+    only the fd/bk pointers of bins, and then use repositioning tricks
+    to treat these as the fields of a malloc_chunk*.
+*/
+
+typedef struct malloc_chunk* mbinptr;
+
+/* addressing -- note that bin_at(0) does not exist */
+#define bin_at(m, i) ((mbinptr)((char*)&((m)->bins[(i)<<1]) - (SIZE_SZ<<1)))
+
+/* analog of ++bin */
+#define next_bin(b)  ((mbinptr)((char*)(b) + (sizeof(mchunkptr)<<1)))
+
+/* Reminders about list directionality within bins */
+#define first(b)     ((b)->fd)
+#define last(b)      ((b)->bk)
+
+/* Take a chunk off a bin list */
+#define unlink(P, BK, FD) {                                            \
+  FD = P->fd;                                                          \
+  BK = P->bk;                                                          \
+  FD->bk = BK;                                                         \
+  BK->fd = FD;                                                         \
+}
+
+/*
+  Indexing
+
+    Bins for sizes < 512 bytes contain chunks of all the same size, spaced
+    8 bytes apart. Larger bins are approximately logarithmically spaced:
+
+    64 bins of size       8
+    32 bins of size      64
+    16 bins of size     512
+     8 bins of size    4096
+     4 bins of size   32768
+     2 bins of size  262144
+     1 bin  of size what's left
+
+    There is actually a little bit of slop in the numbers in bin_index
+    for the sake of speed. This makes no difference elsewhere.
+
+    The bins top out around 1MB because we expect to service large
+    requests via mmap.
+*/
+
+#define NBINS             128
+#define NSMALLBINS         64
+#define SMALLBIN_WIDTH      8
+#define MIN_LARGE_SIZE    512
+
+#define in_smallbin_range(sz)  \
+  ((unsigned long)(sz) < (unsigned long)MIN_LARGE_SIZE)
+
+#define smallbin_index(sz)     (((unsigned)(sz)) >> 3)
+
+#define largebin_index(sz)                                                   \
+(((((unsigned long)(sz)) >>  6) <= 32)?  56 + (((unsigned long)(sz)) >>  6): \
+ ((((unsigned long)(sz)) >>  9) <= 20)?  91 + (((unsigned long)(sz)) >>  9): \
+ ((((unsigned long)(sz)) >> 12) <= 10)? 110 + (((unsigned long)(sz)) >> 12): \
+ ((((unsigned long)(sz)) >> 15) <=  4)? 119 + (((unsigned long)(sz)) >> 15): \
+ ((((unsigned long)(sz)) >> 18) <=  2)? 124 + (((unsigned long)(sz)) >> 18): \
+                                        126)
+
+#define bin_index(sz) \
+ ((in_smallbin_range(sz)) ? smallbin_index(sz) : largebin_index(sz))
+
+/*
+  FIRST_SORTED_BIN_SIZE is the chunk size corresponding to the
+  first bin that is maintained in sorted order. This must
+  be the smallest size corresponding to a given bin.
+
+  Normally, this should be MIN_LARGE_SIZE. But you can weaken
+  best fit guarantees to sometimes speed up malloc by increasing value.
+  Doing this means that malloc may choose a chunk that is 
+  non-best-fitting by up to the width of the bin.
+
+  Some useful cutoff values:
+      512 - all bins sorted
+     2560 - leaves bins <=     64 bytes wide unsorted  
+    12288 - leaves bins <=    512 bytes wide unsorted
+    65536 - leaves bins <=   4096 bytes wide unsorted
+   262144 - leaves bins <=  32768 bytes wide unsorted
+       -1 - no bins sorted (not recommended!)
+*/
+
+#define FIRST_SORTED_BIN_SIZE MIN_LARGE_SIZE 
+/* #define FIRST_SORTED_BIN_SIZE 65536 */
+
+/*
+  Unsorted chunks
+
+    All remainders from chunk splits, as well as all returned chunks,
+    are first placed in the "unsorted" bin. They are then placed
+    in regular bins after malloc gives them ONE chance to be used before
+    binning. So, basically, the unsorted_chunks list acts as a queue,
+    with chunks being placed on it in free (and malloc_consolidate),
+    and taken off (to be either used or placed in bins) in malloc.
+
+    The NON_MAIN_ARENA flag is never set for unsorted chunks, so it
+    does not have to be taken into account in size comparisons.
+*/
+
+/* The otherwise unindexable 1-bin is used to hold unsorted chunks. */
+#define unsorted_chunks(M)          (bin_at(M, 1))
+
+/*
+  Top
+
+    The top-most available chunk (i.e., the one bordering the end of
+    available memory) is treated specially. It is never included in
+    any bin, is used only if no other chunk is available, and is
+    released back to the system if it is very large (see
+    M_TRIM_THRESHOLD).  Because top initially
+    points to its own bin with initial zero size, thus forcing
+    extension on the first malloc request, we avoid having any special
+    code in malloc to check whether it even exists yet. But we still
+    need to do so when getting memory from system, so we make
+    initial_top treat the bin as a legal but unusable chunk during the
+    interval between initialization and the first call to
+    sYSMALLOc. (This is somewhat delicate, since it relies on
+    the 2 preceding words to be zero during this interval as well.)
+*/
+
+/* Conveniently, the unsorted bin can be used as dummy top on first call */
+#define initial_top(M)              (unsorted_chunks(M))
+
+/*
+  Binmap
+
+    To help compensate for the large number of bins, a one-level index
+    structure is used for bin-by-bin searching.  `binmap' is a
+    bitvector recording whether bins are definitely empty so they can
+    be skipped over during during traversals.  The bits are NOT always
+    cleared as soon as bins are empty, but instead only
+    when they are noticed to be empty during traversal in malloc.
+*/
+
+/* Conservatively use 32 bits per map word, even if on 64bit system */
+#define BINMAPSHIFT      5
+#define BITSPERMAP       (1U << BINMAPSHIFT)
+#define BINMAPSIZE       (NBINS / BITSPERMAP)
+
+#define idx2block(i)     ((i) >> BINMAPSHIFT)
+#define idx2bit(i)       ((1U << ((i) & ((1U << BINMAPSHIFT)-1))))
+
+#define mark_bin(m,i)    ((m)->binmap[idx2block(i)] |=  idx2bit(i))
+#define unmark_bin(m,i)  ((m)->binmap[idx2block(i)] &= ~(idx2bit(i)))
+#define get_binmap(m,i)  ((m)->binmap[idx2block(i)] &   idx2bit(i))
+
+/*
+  Fastbins
+
+    An array of lists holding recently freed small chunks.  Fastbins
+    are not doubly linked.  It is faster to single-link them, and
+    since chunks are never removed from the middles of these lists,
+    double linking is not necessary. Also, unlike regular bins, they
+    are not even processed in FIFO order (they use faster LIFO) since
+    ordering doesn't much matter in the transient contexts in which
+    fastbins are normally used.
+
+    Chunks in fastbins keep their inuse bit set, so they cannot
+    be consolidated with other free chunks. malloc_consolidate
+    releases all chunks in fastbins and consolidates them with
+    other free chunks.
+*/
+
+typedef struct malloc_chunk* mfastbinptr;
+
+/* offset 2 to use otherwise unindexable first 2 bins */
+#define fastbin_index(sz)        ((int)((((unsigned int)(sz)) >> 3) - 2))
+
+/* The maximum fastbin request size we support */
+#define MAX_FAST_SIZE     80
+
+#define NFASTBINS  (fastbin_index(request2size(MAX_FAST_SIZE))+1)
+
+/*
+  FASTBIN_CONSOLIDATION_THRESHOLD is the size of a chunk in free()
+  that triggers automatic consolidation of possibly-surrounding
+  fastbin chunks. This is a heuristic, so the exact value should not
+  matter too much. It is defined at half the default trim threshold as a
+  compromise heuristic to only attempt consolidation if it is likely
+  to lead to trimming. However, it is not dynamically tunable, since
+  consolidation reduces fragmentation surrounding large chunks even
+  if trimming is not used.
+*/
+
+#define FASTBIN_CONSOLIDATION_THRESHOLD  (65536UL)
+
+/*
+  Since the lowest 2 bits in max_fast don't matter in size comparisons,
+  they are used as flags.
+*/
+
+/*
+  FASTCHUNKS_BIT held in max_fast indicates that there are probably
+  some fastbin chunks. It is set true on entering a chunk into any
+  fastbin, and cleared only in malloc_consolidate.
+
+  The truth value is inverted so that have_fastchunks will be true
+  upon startup (since statics are zero-filled), simplifying
+  initialization checks.
+*/
+
+#define FASTCHUNKS_BIT        (1U)
+
+#define have_fastchunks(M)     (((M)->max_fast &  FASTCHUNKS_BIT) == 0)
+#define clear_fastchunks(M)    ((M)->max_fast |=  FASTCHUNKS_BIT)
+#define set_fastchunks(M)      ((M)->max_fast &= ~FASTCHUNKS_BIT)
+
+/*
+  NONCONTIGUOUS_BIT indicates that MORECORE does not return contiguous
+  regions.  Otherwise, contiguity is exploited in merging together,
+  when possible, results from consecutive MORECORE calls.
+
+  The initial value comes from MORECORE_CONTIGUOUS, but is
+  changed dynamically if mmap is ever used as an sbrk substitute.
+*/
+
+#define NONCONTIGUOUS_BIT     (2U)
+
+#define contiguous(M)          (((M)->max_fast &  NONCONTIGUOUS_BIT) == 0)
+#define noncontiguous(M)       (((M)->max_fast &  NONCONTIGUOUS_BIT) != 0)
+#define set_noncontiguous(M)   ((M)->max_fast |=  NONCONTIGUOUS_BIT)
+#define set_contiguous(M)      ((M)->max_fast &= ~NONCONTIGUOUS_BIT)
+
+/*
+   Set value of max_fast.
+   Use impossibly small value if 0.
+   Precondition: there are no existing fastbin chunks.
+   Setting the value clears fastchunk bit but preserves noncontiguous bit.
+*/
+
+#define set_max_fast(M, s) \
+  (M)->max_fast = (((s) == 0)? SMALLBIN_WIDTH: request2size(s)) | \
+  FASTCHUNKS_BIT | \
+  ((M)->max_fast &  NONCONTIGUOUS_BIT)
+
+
+/*
+   ----------- Internal state representation and initialization -----------
+*/
+
+struct malloc_state {
+  /* Serialize access.  */
+  mutex_t mutex;
+
+  /* Statistics for locking.  Only used if THREAD_STATS is defined.  */
+  long stat_lock_direct, stat_lock_loop, stat_lock_wait;
+  long pad0_[1]; /* try to give the mutex its own cacheline */
+
+  /* The maximum chunk size to be eligible for fastbin */
+  INTERNAL_SIZE_T  max_fast;   /* low 2 bits used as flags */
+
+  /* Fastbins */
+  mfastbinptr      fastbins[NFASTBINS];
+
+  /* Base of the topmost chunk -- not otherwise kept in a bin */
+  mchunkptr        top;
+
+  /* The remainder from the most recent split of a small request */
+  mchunkptr        last_remainder;
+
+  /* Normal bins packed as described above */
+  mchunkptr        bins[NBINS * 2];
+
+  /* Bitmap of bins */
+  unsigned int     binmap[BINMAPSIZE];
+
+  /* Linked list */
+  struct malloc_state *next;
+
+  /* Memory allocated from the system in this arena.  */
+  INTERNAL_SIZE_T system_mem;
+  INTERNAL_SIZE_T max_system_mem;
+};
+
+struct malloc_par {
+  /* Tunable parameters */
+  unsigned long    trim_threshold;
+  INTERNAL_SIZE_T  top_pad;
+  INTERNAL_SIZE_T  mmap_threshold;
+
+  /* Memory map support */
+  int              n_mmaps;
+  int              n_mmaps_max;
+  int              max_n_mmaps;
+
+  /* Cache malloc_getpagesize */
+  unsigned int     pagesize;
+
+  /* Statistics */
+  INTERNAL_SIZE_T  mmapped_mem;
+  /*INTERNAL_SIZE_T  sbrked_mem;*/
+  /*INTERNAL_SIZE_T  max_sbrked_mem;*/
+  INTERNAL_SIZE_T  max_mmapped_mem;
+  INTERNAL_SIZE_T  max_total_mem; /* only kept for NO_THREADS */
+
+  /* First address handed out by MORECORE/sbrk.  */
+  char*            sbrk_base;
+};
+
+/* There are several instances of this struct ("arenas") in this
+   malloc.  If you are adapting this malloc in a way that does NOT use
+   a static or mmapped malloc_state, you MUST explicitly zero-fill it
+   before using. This malloc relies on the property that malloc_state
+   is initialized to all zeroes (as is true of C statics).  */
+
+
+
+/*
+  Initialize a malloc_state struct.
+
+  This is called only from within malloc_consolidate, which needs
+  be called in the same contexts anyway.  It is never called directly
+  outside of malloc_consolidate because some optimizing compilers try
+  to inline it at all call points, which turns out not to be an
+  optimization at all. (Inlining it in malloc_consolidate is fine though.)
+*/
+
+#if __STD_C
+static void malloc_init_state(mstate av)
+#else
+static void malloc_init_state(av) mstate av;
+#endif
+{
+  int     i;
+  mbinptr bin;
+
+  /* Establish circular links for normal bins */
+  for (i = 1; i < NBINS; ++i) {
+    bin = bin_at(av,i);
+    bin->fd = bin->bk = bin;
+  }
+
+  set_noncontiguous(av);
+
+  set_max_fast(av, DEFAULT_MXFAST);
+
+  av->top            = initial_top(av);
+}
+
+/*
+   Other internal utilities operating on mstates
+*/
+
+#if __STD_C
+static Void_t*  sYSMALLOc(INTERNAL_SIZE_T, mstate);
+static void     malloc_consolidate(mstate);
+//static Void_t** iALLOc(mstate, size_t, size_t*, int, Void_t**);
+#else
+static Void_t*  sYSMALLOc();
+static void     malloc_consolidate();
+static Void_t** iALLOc();
+#endif
+
+/* ------------------- Support for multiple arenas -------------------- */
+#include "arena.c"
+
+/*
+  Debugging support
+
+  These routines make a number of assertions about the states
+  of data structures that should be true at all times. If any
+  are not true, it's very likely that a user program has somehow
+  trashed memory. (It's also possible that there is a coding error
+  in malloc. In which case, please report it!)
+*/
+
+#if ! MALLOC_DEBUG
+
+#define check_chunk(A,P)
+#define check_free_chunk(A,P)
+#define check_inuse_chunk(A,P)
+#define check_remalloced_chunk(A,P,N)
+#define check_malloced_chunk(A,P,N)
+#define check_malloc_state(A)
+
+#else
+
+#define check_chunk(A,P)              do_check_chunk(A,P)
+#define check_free_chunk(A,P)         do_check_free_chunk(A,P)
+#define check_inuse_chunk(A,P)        do_check_inuse_chunk(A,P)
+#define check_remalloced_chunk(A,P,N) do_check_remalloced_chunk(A,P,N)
+#define check_malloced_chunk(A,P,N)   do_check_malloced_chunk(A,P,N)
+#define check_malloc_state(A)         do_check_malloc_state(A)
+
+/*
+  Properties of all chunks
+*/
+
+#if __STD_C
+static void do_check_chunk(mstate av, mchunkptr p)
+#else
+static void do_check_chunk(av, p) mstate av; mchunkptr p;
+#endif
+{
+  unsigned long sz = chunksize(p);
+  /* min and max possible addresses assuming contiguous allocation */
+  char* max_address = (char*)(av->top) + chunksize(av->top);
+  char* min_address = max_address - av->system_mem;
+
+  if (!chunk_is_mmapped(p)) {
+
+    /* Has legal address ... */
+    if (p != av->top) {
+      if (contiguous(av)) {
+        assert(((char*)p) >= min_address);
+        assert(((char*)p + sz) <= ((char*)(av->top)));
+      }
+    }
+    else {
+      /* top size is always at least MINSIZE */
+      assert((unsigned long)(sz) >= MINSIZE);
+      /* top predecessor always marked inuse */
+      assert(prev_inuse(p));
+    }
+
+  }
+  else {
+#if HAVE_MMAP
+    /* address is outside main heap  */
+    if (contiguous(av) && av->top != initial_top(av)) {
+      assert(((char*)p) < min_address || ((char*)p) > max_address);
+    }
+    /* chunk is page-aligned */
+    assert(((p->prev_size + sz) & (mp_.pagesize-1)) == 0);
+    /* mem is aligned */
+    assert(aligned_OK(chunk2mem(p)));
+#else
+    /* force an appropriate assert violation if debug set */
+    assert(!chunk_is_mmapped(p));
+#endif
+  }
+}
+
+/*
+  Properties of free chunks
+*/
+
+#if __STD_C
+static void do_check_free_chunk(mstate av, mchunkptr p)
+#else
+static void do_check_free_chunk(av, p) mstate av; mchunkptr p;
+#endif
+{
+  INTERNAL_SIZE_T sz = p->size & ~(PREV_INUSE);
+  mchunkptr next = chunk_at_offset(p, sz);
+
+  do_check_chunk(av, p);
+
+  /* Chunk must claim to be free ... */
+  assert(!inuse(p));
+  assert (!chunk_is_mmapped(p));
+
+  /* Unless a special marker, must have OK fields */
+  if ((unsigned long)(sz) >= MINSIZE)
+  {
+    assert((sz & MALLOC_ALIGN_MASK) == 0);
+    assert(aligned_OK(chunk2mem(p)));
+    /* ... matching footer field */
+    assert(next->prev_size == sz);
+    /* ... and is fully consolidated */
+    assert(prev_inuse(p));
+    assert (next == av->top || inuse(next));
+
+    /* ... and has minimally sane links */
+    assert(p->fd->bk == p);
+    assert(p->bk->fd == p);
+  }
+  else /* markers are always of size SIZE_SZ */
+    assert(sz == SIZE_SZ);
+}
+
+/*
+  Properties of inuse chunks
+*/
+
+#if __STD_C
+static void do_check_inuse_chunk(mstate av, mchunkptr p)
+#else
+static void do_check_inuse_chunk(av, p) mstate av; mchunkptr p;
+#endif
+{
+  mchunkptr next;
+
+  do_check_chunk(av, p);
+
+  assert(av == arena_for_chunk(p));
+  if (chunk_is_mmapped(p))
+    return; /* mmapped chunks have no next/prev */
+
+  /* Check whether it claims to be in use ... */
+  assert(inuse(p));
+
+  next = next_chunk(p);
+
+  /* ... and is surrounded by OK chunks.
+    Since more things can be checked with free chunks than inuse ones,
+    if an inuse chunk borders them and debug is on, it's worth doing them.
+  */
+  if (!prev_inuse(p))  {
+    /* Note that we cannot even look at prev unless it is not inuse */
+    mchunkptr prv = prev_chunk(p);
+    assert(next_chunk(prv) == p);
+    do_check_free_chunk(av, prv);
+  }
+
+  if (next == av->top) {
+    assert(prev_inuse(next));
+    assert(chunksize(next) >= MINSIZE);
+  }
+  else if (!inuse(next))
+    do_check_free_chunk(av, next);
+}
+
+/*
+  Properties of chunks recycled from fastbins
+*/
+
+#if __STD_C
+static void do_check_remalloced_chunk(mstate av, mchunkptr p, INTERNAL_SIZE_T s)
+#else
+static void do_check_remalloced_chunk(av, p, s)
+mstate av; mchunkptr p; INTERNAL_SIZE_T s;
+#endif
+{
+  INTERNAL_SIZE_T sz = p->size & ~(PREV_INUSE);
+
+  if (!chunk_is_mmapped(p)) {
+    assert(av == arena_for_chunk(p));
+  }
+
+  do_check_inuse_chunk(av, p);
+
+  /* Legal size ... */
+  assert((sz & MALLOC_ALIGN_MASK) == 0);
+  assert((unsigned long)(sz) >= MINSIZE);
+  /* ... and alignment */
+  assert(aligned_OK(chunk2mem(p)));
+  /* chunk is less than MINSIZE more than request */
+  assert((long)(sz) - (long)(s) >= 0);
+  assert((long)(sz) - (long)(s + MINSIZE) < 0);
+}
+
+/*
+  Properties of nonrecycled chunks at the point they are malloced
+*/
+
+#if __STD_C
+static void do_check_malloced_chunk(mstate av, mchunkptr p, INTERNAL_SIZE_T s)
+#else
+static void do_check_malloced_chunk(av, p, s)
+mstate av; mchunkptr p; INTERNAL_SIZE_T s;
+#endif
+{
+  /* same as recycled case ... */
+  do_check_remalloced_chunk(av, p, s);
+
+  /*
+    ... plus,  must obey implementation invariant that prev_inuse is
+    always true of any allocated chunk; i.e., that each allocated
+    chunk borders either a previously allocated and still in-use
+    chunk, or the base of its memory arena. This is ensured
+    by making all allocations from the the `lowest' part of any found
+    chunk.  This does not necessarily hold however for chunks
+    recycled via fastbins.
+  */
+
+  assert(prev_inuse(p));
+}
+
+
+/*
+  Properties of malloc_state.
+
+  This may be useful for debugging malloc, as well as detecting user
+  programmer errors that somehow write into malloc_state.
+
+  If you are extending or experimenting with this malloc, you can
+  probably figure out how to hack this routine to print out or
+  display chunk addresses, sizes, bins, and other instrumentation.
+*/
+
+static void do_check_malloc_state(mstate av)
+{
+  int i;
+  mchunkptr p;
+  mchunkptr q;
+  mbinptr b;
+  unsigned int binbit;
+  int empty;
+  unsigned int idx;
+  INTERNAL_SIZE_T size;
+  unsigned long total = 0;
+  int max_fast_bin;
+
+  /* internal size_t must be no wider than pointer type */
+  assert(sizeof(INTERNAL_SIZE_T) <= sizeof(char*));
+
+  /* alignment is a power of 2 */
+  assert((MALLOC_ALIGNMENT & (MALLOC_ALIGNMENT-1)) == 0);
+
+  /* cannot run remaining checks until fully initialized */
+  if (av->top == 0 || av->top == initial_top(av))
+    return;
+
+
+  /* properties of fastbins */
+
+  /* max_fast is in allowed range */
+  assert((av->max_fast & ~1) <= request2size(MAX_FAST_SIZE));
+
+  max_fast_bin = fastbin_index(av->max_fast);
+
+  for (i = 0; i < NFASTBINS; ++i) {
+    p = av->fastbins[i];
+
+    /* all bins past max_fast are empty */
+    if (i > max_fast_bin)
+      assert(p == 0);
+
+    while (p != 0) {
+      /* each chunk claims to be inuse */
+      do_check_inuse_chunk(av, p);
+      total += chunksize(p);
+      /* chunk belongs in this bin */
+      assert(fastbin_index(chunksize(p)) == i);
+      p = p->fd;
+    }
+  }
+
+  if (total != 0)
+    assert(have_fastchunks(av));
+  else if (!have_fastchunks(av))
+    assert(total == 0);
+
+  /* check normal bins */
+  for (i = 1; i < NBINS; ++i) {
+    b = bin_at(av,i);
+
+    /* binmap is accurate (except for bin 1 == unsorted_chunks) */
+    if (i >= 2) {
+      binbit = get_binmap(av,i);
+      empty = last(b) == b;
+      if (!binbit)
+        assert(empty);
+      else if (!empty)
+        assert(binbit);
+    }
+
+    for (p = last(b); p != b; p = p->bk) {
+      /* each chunk claims to be free */
+      do_check_free_chunk(av, p);
+      size = chunksize(p);
+      total += size;
+      if (i >= 2) {
+        /* chunk belongs in bin */
+        idx = bin_index(size);
+        assert(idx == (unsigned int)i);
+        /* lists are sorted */
+        if ((unsigned long) size >= (unsigned long)(FIRST_SORTED_BIN_SIZE)) {
+	  assert(p->bk == b ||
+		 (unsigned long)chunksize(p->bk) >=
+		 (unsigned long)chunksize(p));
+	}
+      }
+      /* chunk is followed by a legal chain of inuse chunks */
+      for (q = next_chunk(p);
+           (q != av->top && inuse(q) &&
+             (unsigned long)(chunksize(q)) >= MINSIZE);
+           q = next_chunk(q))
+        do_check_inuse_chunk(av, q);
+    }
+  }
+
+  /* top chunk is OK */
+  check_chunk(av, av->top);
+
+  /* sanity checks for statistics */
+
+
+  assert((unsigned long)(av->system_mem) <=
+         (unsigned long)(av->max_system_mem));
+
+
+}
+#endif
+
+
+
+/* ----------- Routines dealing with system allocation -------------- */
+
+/* No system allocation routines supported */
+
+
+/*------------------------ Public wrappers. --------------------------------*/
+
+
+
+#undef DEBUG_MALLOC
+Void_t*
+public_mALLOc(cvmx_arena_list_t arena_list, size_t bytes)
+{
+  mstate ar_ptr, orig_ar_ptr;
+  Void_t *victim = NULL;
+  static mstate debug_prev_ar;  // debug only!
+#ifdef DEBUG_MALLOC
+  int arena_cnt=0;
+#endif
+  
+  ar_ptr = arena_list;
+
+  if (!ar_ptr)
+  {
+     return(NULL);
+  }
+
+  if (debug_prev_ar != ar_ptr)
+  {
+      debug_printf("New arena: %p\n", ar_ptr);
+#ifdef CVMX_SPINLOCK_DEBUG
+      cvmx_dprintf("lock wait count for arena: %p is %ld\n", ar_ptr, ar_ptr->mutex.wait_cnt);
+#endif
+      debug_prev_ar = ar_ptr;
+  }
+  orig_ar_ptr = ar_ptr;
+
+  // try to get an arena without contention
+  do
+  {
+#ifdef DEBUG_MALLOC
+  arena_cnt++;
+#endif
+      if (!mutex_trylock(&ar_ptr->mutex))
+      {
+          // we locked it
+          victim = _int_malloc(ar_ptr, bytes);
+          (void)mutex_unlock(&ar_ptr->mutex);
+          if(victim)
+          {
+              break;
+          }
+      }
+      ar_ptr = ar_ptr->next;
+  } while (ar_ptr != orig_ar_ptr);
+
+  // we couldn't get the memory without contention, so try all
+  // arenas.  SLOW!
+  if (!victim)
+  {
+      ar_ptr = orig_ar_ptr;
+      do
+      {
+#ifdef DEBUG_MALLOC
+  arena_cnt++;
+#endif
+          mutex_lock(&ar_ptr->mutex);
+          victim = _int_malloc(ar_ptr, bytes);
+          (void)mutex_unlock(&ar_ptr->mutex);
+          if(victim)
+          {
+              break;
+          }
+          ar_ptr = ar_ptr->next;
+      } while (ar_ptr != orig_ar_ptr);
+  }
+
+
+  assert(!victim || chunk_is_mmapped(mem2chunk(victim)) ||
+	 ar_ptr == arena_for_chunk(mem2chunk(victim)));
+
+#ifdef DEBUG_MALLOC
+  if (!victim)
+  {
+     cvmx_dprintf("Malloc failed: size: %ld, arena_cnt: %d\n", bytes, arena_cnt);
+  }
+#endif
+
+  debug_printf("cvmx_malloc(%ld) = %p\n", bytes, victim);
+
+  // remember which arena we last used.....
+  tsd_setspecific(arena_key, (Void_t *)ar_ptr);
+  return victim;
+}
+
+
+
+void
+public_fREe(Void_t* mem)
+{
+  mstate ar_ptr;
+  mchunkptr p;                          /* chunk corresponding to mem */
+
+  debug_printf("cvmx_free(%p)\n", mem);
+
+
+  if (mem == 0)                              /* free(0) has no effect */
+    return;
+
+  p = mem2chunk(mem);
+
+
+  ar_ptr = arena_for_chunk(p);
+  assert(ar_ptr);
+#if THREAD_STATS
+  if(!mutex_trylock(&ar_ptr->mutex))
+    ++(ar_ptr->stat_lock_direct);
+  else {
+    (void)mutex_lock(&ar_ptr->mutex);
+    ++(ar_ptr->stat_lock_wait);
+  }
+#else
+  (void)mutex_lock(&ar_ptr->mutex);
+#endif
+  _int_free(ar_ptr, mem);
+  (void)mutex_unlock(&ar_ptr->mutex);
+}
+
+Void_t*
+public_rEALLOc(cvmx_arena_list_t arena_list, Void_t* oldmem, size_t bytes)
+{
+  mstate ar_ptr;
+  INTERNAL_SIZE_T    nb;      /* padded request size */
+
+  mchunkptr oldp;             /* chunk corresponding to oldmem */
+  INTERNAL_SIZE_T    oldsize; /* its size */
+
+  Void_t* newp;             /* chunk to return */
+
+
+#if REALLOC_ZERO_BYTES_FREES
+  if (bytes == 0 && oldmem != NULL) { public_fREe(oldmem); return 0; }
+#endif
+
+  /* realloc of null is supposed to be same as malloc */
+  if (oldmem == 0) return public_mALLOc(arena_list, bytes);
+
+  oldp    = mem2chunk(oldmem);
+  oldsize = chunksize(oldp);
+
+  checked_request2size(bytes, nb);
+
+
+  ar_ptr = arena_for_chunk(oldp);
+  (void)mutex_lock(&ar_ptr->mutex);
+
+
+  newp = _int_realloc(ar_ptr, oldmem, bytes);
+
+  (void)mutex_unlock(&ar_ptr->mutex);
+  assert(!newp || chunk_is_mmapped(mem2chunk(newp)) ||
+	 ar_ptr == arena_for_chunk(mem2chunk(newp)));
+  return newp;
+}
+
+#undef DEBUG_MEMALIGN
+Void_t*
+public_mEMALIGn(cvmx_arena_list_t arena_list, size_t alignment, size_t bytes)
+{
+  mstate ar_ptr, orig_ar_ptr;
+  Void_t *p = NULL;
+#ifdef DEBUG_MEMALIGN
+  int arena_cnt=0;
+#endif
+
+
+  /* If need less alignment than we give anyway, just relay to malloc */
+  if (alignment <= MALLOC_ALIGNMENT) return public_mALLOc(arena_list, bytes);
+
+  /* Otherwise, ensure that it is at least a minimum chunk size */
+  if (alignment <  MINSIZE) alignment = MINSIZE;
+
+
+  ar_ptr = arena_list;
+
+  if (!ar_ptr)
+  {
+     return(NULL);
+  }
+
+  orig_ar_ptr = ar_ptr;
+
+
+  // try to get an arena without contention
+  do
+  {
+
+#ifdef DEBUG_MEMALIGN
+   arena_cnt++;
+#endif
+      if (!mutex_trylock(&ar_ptr->mutex))
+      {
+          // we locked it
+          p = _int_memalign(ar_ptr, alignment, bytes);
+          (void)mutex_unlock(&ar_ptr->mutex);
+          if(p)
+          {
+              break;
+          }
+      }
+      ar_ptr = ar_ptr->next;
+  } while (ar_ptr != orig_ar_ptr);
+
+
+  // we couldn't get the memory without contention, so try all
+  // arenas.  SLOW!
+  if (!p)
+  {
+#ifdef DEBUG_MEMALIGN
+   arena_cnt++;
+#endif
+      ar_ptr = orig_ar_ptr;
+      do
+      {
+          mutex_lock(&ar_ptr->mutex);
+          p = _int_memalign(ar_ptr, alignment, bytes);
+          (void)mutex_unlock(&ar_ptr->mutex);
+          if(p)
+          {
+              break;
+          }
+          ar_ptr = ar_ptr->next;
+      } while (ar_ptr != orig_ar_ptr);
+  }
+
+
+  if (p)
+  {
+     assert(ar_ptr == arena_for_chunk(mem2chunk(p)));
+  }
+  else
+  {
+#ifdef DEBUG_MEMALIGN
+     cvmx_dprintf("Memalign failed: align: 0x%x, size: %ld, arena_cnt: %ld\n", alignment, bytes, arena_cnt);
+#endif
+  }
+
+  assert(!p || ar_ptr == arena_for_chunk(mem2chunk(p)));
+  return p;
+}
+
+
+
+Void_t*
+public_cALLOc(cvmx_arena_list_t arena_list, size_t n, size_t elem_size)
+{
+  mstate av;
+  mchunkptr oldtop, p;
+  INTERNAL_SIZE_T sz, csz, oldtopsize;
+  Void_t* mem;
+  unsigned long clearsize;
+  unsigned long nclears;
+  INTERNAL_SIZE_T* d;
+
+
+  /* FIXME: check for overflow on multiplication.  */
+  sz = n * elem_size;
+
+  mem = public_mALLOc(arena_list, sz);
+  if (mem)
+  {
+     memset(mem, 0, sz);
+  }
+
+  return mem;
+}
+
+
+#ifndef _LIBC
+
+void
+public_cFREe(Void_t* m)
+{
+  public_fREe(m);
+}
+
+#endif /* _LIBC */
+
+/*
+  ------------------------------ malloc ------------------------------
+*/
+
+static Void_t*
+_int_malloc(mstate av, size_t bytes)
+{
+  INTERNAL_SIZE_T nb;               /* normalized request size */
+  unsigned int    idx;              /* associated bin index */
+  mbinptr         bin;              /* associated bin */
+  mfastbinptr*    fb;               /* associated fastbin */
+
+  mchunkptr       victim;           /* inspected/selected chunk */
+  INTERNAL_SIZE_T size;             /* its size */
+  int             victim_index;     /* its bin index */
+
+  mchunkptr       remainder;        /* remainder from a split */
+  unsigned long   remainder_size;   /* its size */
+
+  unsigned int    block;            /* bit map traverser */
+  unsigned int    bit;              /* bit map traverser */
+  unsigned int    map;              /* current word of binmap */
+
+  mchunkptr       fwd;              /* misc temp for linking */
+  mchunkptr       bck;              /* misc temp for linking */
+
+  /*
+    Convert request size to internal form by adding SIZE_SZ bytes
+    overhead plus possibly more to obtain necessary alignment and/or
+    to obtain a size of at least MINSIZE, the smallest allocatable
+    size. Also, checked_request2size traps (returning 0) request sizes
+    that are so large that they wrap around zero when padded and
+    aligned.
+  */
+
+
+  checked_request2size(bytes, nb);
+
+  /*
+    If the size qualifies as a fastbin, first check corresponding bin.
+    This code is safe to execute even if av is not yet initialized, so we
+    can try it without checking, which saves some time on this fast path.
+  */
+
+  if ((unsigned long)(nb) <= (unsigned long)(av->max_fast)) {
+    fb = &(av->fastbins[(fastbin_index(nb))]);
+    if ( (victim = *fb) != 0) {
+      *fb = victim->fd;
+      check_remalloced_chunk(av, victim, nb);
+      set_arena_for_chunk(victim, av);
+      return chunk2mem(victim);
+    }
+  }
+
+  /*
+    If a small request, check regular bin.  Since these "smallbins"
+    hold one size each, no searching within bins is necessary.
+    (For a large request, we need to wait until unsorted chunks are
+    processed to find best fit. But for small ones, fits are exact
+    anyway, so we can check now, which is faster.)
+  */
+
+  if (in_smallbin_range(nb)) {
+    idx = smallbin_index(nb);
+    bin = bin_at(av,idx);
+
+    if ( (victim = last(bin)) != bin) {
+      if (victim == 0) /* initialization check */
+        malloc_consolidate(av);
+      else {
+        bck = victim->bk;
+        set_inuse_bit_at_offset(victim, nb);
+        bin->bk = bck;
+        bck->fd = bin;
+
+        set_arena_for_chunk(victim, av);
+        check_malloced_chunk(av, victim, nb);
+        return chunk2mem(victim);
+      }
+    }
+  }
+
+  /*
+     If this is a large request, consolidate fastbins before continuing.
+     While it might look excessive to kill all fastbins before
+     even seeing if there is space available, this avoids
+     fragmentation problems normally associated with fastbins.
+     Also, in practice, programs tend to have runs of either small or
+     large requests, but less often mixtures, so consolidation is not
+     invoked all that often in most programs. And the programs that
+     it is called frequently in otherwise tend to fragment.
+  */
+
+  else {
+    idx = largebin_index(nb);
+    if (have_fastchunks(av))
+      malloc_consolidate(av);
+  }
+
+  /*
+    Process recently freed or remaindered chunks, taking one only if
+    it is exact fit, or, if this a small request, the chunk is remainder from
+    the most recent non-exact fit.  Place other traversed chunks in
+    bins.  Note that this step is the only place in any routine where
+    chunks are placed in bins.
+
+    The outer loop here is needed because we might not realize until
+    near the end of malloc that we should have consolidated, so must
+    do so and retry. This happens at most once, and only when we would
+    otherwise need to expand memory to service a "small" request.
+  */
+
+  for(;;) {
+
+    while ( (victim = unsorted_chunks(av)->bk) != unsorted_chunks(av)) {
+      bck = victim->bk;
+      size = chunksize(victim);
+
+      /*
+         If a small request, try to use last remainder if it is the
+         only chunk in unsorted bin.  This helps promote locality for
+         runs of consecutive small requests. This is the only
+         exception to best-fit, and applies only when there is
+         no exact fit for a small chunk.
+      */
+
+      if (in_smallbin_range(nb) &&
+          bck == unsorted_chunks(av) &&
+          victim == av->last_remainder &&
+          (unsigned long)(size) > (unsigned long)(nb + MINSIZE)) {
+
+        /* split and reattach remainder */
+        remainder_size = size - nb;
+        remainder = chunk_at_offset(victim, nb);
+        unsorted_chunks(av)->bk = unsorted_chunks(av)->fd = remainder;
+        av->last_remainder = remainder;
+        remainder->bk = remainder->fd = unsorted_chunks(av);
+
+        set_head(victim, nb | PREV_INUSE);
+        set_head(remainder, remainder_size | PREV_INUSE);
+        set_foot(remainder, remainder_size);
+
+        set_arena_for_chunk(victim, av);
+        check_malloced_chunk(av, victim, nb);
+        return chunk2mem(victim);
+      }
+
+      /* remove from unsorted list */
+      unsorted_chunks(av)->bk = bck;
+      bck->fd = unsorted_chunks(av);
+
+      /* Take now instead of binning if exact fit */
+
+      if (size == nb) {
+        set_inuse_bit_at_offset(victim, size);
+        set_arena_for_chunk(victim, av);
+        check_malloced_chunk(av, victim, nb);
+        return chunk2mem(victim);
+      }
+
+      /* place chunk in bin */
+
+      if (in_smallbin_range(size)) {
+        victim_index = smallbin_index(size);
+        bck = bin_at(av, victim_index);
+        fwd = bck->fd;
+      }
+      else {
+        victim_index = largebin_index(size);
+        bck = bin_at(av, victim_index);
+        fwd = bck->fd;
+
+        if (fwd != bck) {
+          /* if smaller than smallest, place first */
+          if ((unsigned long)(size) < (unsigned long)(bck->bk->size)) {
+            fwd = bck;
+            bck = bck->bk;
+          }
+          else if ((unsigned long)(size) >= 
+                   (unsigned long)(FIRST_SORTED_BIN_SIZE)) {
+
+            /* maintain large bins in sorted order */
+            size |= PREV_INUSE; /* Or with inuse bit to speed comparisons */
+            while ((unsigned long)(size) < (unsigned long)(fwd->size)) {
+              fwd = fwd->fd;
+	    }
+            bck = fwd->bk;
+          }
+        }
+      }
+
+      mark_bin(av, victim_index);
+      victim->bk = bck;
+      victim->fd = fwd;
+      fwd->bk = victim;
+      bck->fd = victim;
+    }
+
+    /*
+      If a large request, scan through the chunks of current bin in
+      sorted order to find smallest that fits.  This is the only step
+      where an unbounded number of chunks might be scanned without doing
+      anything useful with them. However the lists tend to be short.
+    */
+
+    if (!in_smallbin_range(nb)) {
+      bin = bin_at(av, idx);
+
+      for (victim = last(bin); victim != bin; victim = victim->bk) {
+	size = chunksize(victim);
+
+	if ((unsigned long)(size) >= (unsigned long)(nb)) {
+	  remainder_size = size - nb;
+	  unlink(victim, bck, fwd);
+
+	  /* Exhaust */
+	  if (remainder_size < MINSIZE)  {
+	    set_inuse_bit_at_offset(victim, size);
+        set_arena_for_chunk(victim, av);
+	    check_malloced_chunk(av, victim, nb);
+	    return chunk2mem(victim);
+	  }
+	  /* Split */
+	  else {
+	    remainder = chunk_at_offset(victim, nb);
+	    unsorted_chunks(av)->bk = unsorted_chunks(av)->fd = remainder;
+	    remainder->bk = remainder->fd = unsorted_chunks(av);
+	    set_head(victim, nb | PREV_INUSE);
+	    set_head(remainder, remainder_size | PREV_INUSE);
+	    set_foot(remainder, remainder_size);
+        set_arena_for_chunk(victim, av);
+	    check_malloced_chunk(av, victim, nb);
+	    return chunk2mem(victim);
+	  }
+	}
+      }
+    }
+
+    /*
+      Search for a chunk by scanning bins, starting with next largest
+      bin. This search is strictly by best-fit; i.e., the smallest
+      (with ties going to approximately the least recently used) chunk
+      that fits is selected.
+
+      The bitmap avoids needing to check that most blocks are nonempty.
+      The particular case of skipping all bins during warm-up phases
+      when no chunks have been returned yet is faster than it might look.
+    */
+
+    ++idx;
+    bin = bin_at(av,idx);
+    block = idx2block(idx);
+    map = av->binmap[block];
+    bit = idx2bit(idx);
+
+    for (;;) {
+
+      /* Skip rest of block if there are no more set bits in this block.  */
+      if (bit > map || bit == 0) {
+        do {
+          if (++block >= BINMAPSIZE)  /* out of bins */
+            goto use_top;
+        } while ( (map = av->binmap[block]) == 0);
+
+        bin = bin_at(av, (block << BINMAPSHIFT));
+        bit = 1;
+      }
+
+      /* Advance to bin with set bit. There must be one. */
+      while ((bit & map) == 0) {
+        bin = next_bin(bin);
+        bit <<= 1;
+        assert(bit != 0);
+      }
+
+      /* Inspect the bin. It is likely to be non-empty */
+      victim = last(bin);
+
+      /*  If a false alarm (empty bin), clear the bit. */
+      if (victim == bin) {
+        av->binmap[block] = map &= ~bit; /* Write through */
+        bin = next_bin(bin);
+        bit <<= 1;
+      }
+
+      else {
+        size = chunksize(victim);
+
+        /*  We know the first chunk in this bin is big enough to use. */
+        assert((unsigned long)(size) >= (unsigned long)(nb));
+
+        remainder_size = size - nb;
+
+        /* unlink */
+        bck = victim->bk;
+        bin->bk = bck;
+        bck->fd = bin;
+
+        /* Exhaust */
+        if (remainder_size < MINSIZE) {
+          set_inuse_bit_at_offset(victim, size);
+          set_arena_for_chunk(victim, av);
+          check_malloced_chunk(av, victim, nb);
+          return chunk2mem(victim);
+        }
+
+        /* Split */
+        else {
+          remainder = chunk_at_offset(victim, nb);
+
+          unsorted_chunks(av)->bk = unsorted_chunks(av)->fd = remainder;
+          remainder->bk = remainder->fd = unsorted_chunks(av);
+          /* advertise as last remainder */
+          if (in_smallbin_range(nb))
+            av->last_remainder = remainder;
+
+          set_head(victim, nb | PREV_INUSE);
+          set_head(remainder, remainder_size | PREV_INUSE);
+          set_foot(remainder, remainder_size);
+          set_arena_for_chunk(victim, av);
+          check_malloced_chunk(av, victim, nb);
+          return chunk2mem(victim);
+        }
+      }
+    }
+
+  use_top:
+    /*
+      If large enough, split off the chunk bordering the end of memory
+      (held in av->top). Note that this is in accord with the best-fit
+      search rule.  In effect, av->top is treated as larger (and thus
+      less well fitting) than any other available chunk since it can
+      be extended to be as large as necessary (up to system
+      limitations).
+
+      We require that av->top always exists (i.e., has size >=
+      MINSIZE) after initialization, so if it would otherwise be
+      exhuasted by current request, it is replenished. (The main
+      reason for ensuring it exists is that we may need MINSIZE space
+      to put in fenceposts in sysmalloc.)
+    */
+
+    victim = av->top;
+    size = chunksize(victim);
+
+    if ((unsigned long)(size) >= (unsigned long)(nb + MINSIZE)) {
+      remainder_size = size - nb;
+      remainder = chunk_at_offset(victim, nb);
+      av->top = remainder;
+      set_head(victim, nb | PREV_INUSE);
+      set_head(remainder, remainder_size | PREV_INUSE);
+
+      set_arena_for_chunk(victim, av);
+      check_malloced_chunk(av, victim, nb);
+      return chunk2mem(victim);
+    }
+
+    /*
+      If there is space available in fastbins, consolidate and retry,
+      to possibly avoid expanding memory. This can occur only if nb is
+      in smallbin range so we didn't consolidate upon entry.
+    */
+
+    else if (have_fastchunks(av)) {
+      assert(in_smallbin_range(nb));
+      malloc_consolidate(av);
+      idx = smallbin_index(nb); /* restore original bin index */
+    }
+
+    /*
+       Otherwise, relay to handle system-dependent cases
+    */
+    else
+      return(NULL); // sysmalloc not supported
+  }
+}
+
+/*
+  ------------------------------ free ------------------------------
+*/
+
+static void
+_int_free(mstate av, Void_t* mem)
+{
+  mchunkptr       p;           /* chunk corresponding to mem */
+  INTERNAL_SIZE_T size;        /* its size */
+  mfastbinptr*    fb;          /* associated fastbin */
+  mchunkptr       nextchunk;   /* next contiguous chunk */
+  INTERNAL_SIZE_T nextsize;    /* its size */
+  int             nextinuse;   /* true if nextchunk is used */
+  INTERNAL_SIZE_T prevsize;    /* size of previous contiguous chunk */
+  mchunkptr       bck;         /* misc temp for linking */
+  mchunkptr       fwd;         /* misc temp for linking */
+
+
+  /* free(0) has no effect */
+  if (mem != 0) {
+    p = mem2chunk(mem);
+    size = chunksize(p);
+
+    check_inuse_chunk(av, p);
+
+    /*
+      If eligible, place chunk on a fastbin so it can be found
+      and used quickly in malloc.
+    */
+
+    if ((unsigned long)(size) <= (unsigned long)(av->max_fast)
+
+#if TRIM_FASTBINS
+        /*
+           If TRIM_FASTBINS set, don't place chunks
+           bordering top into fastbins
+        */
+        && (chunk_at_offset(p, size) != av->top)
+#endif
+        ) {
+
+      set_fastchunks(av);
+      fb = &(av->fastbins[fastbin_index(size)]);
+      p->fd = *fb;
+      *fb = p;
+    }
+
+    /*
+       Consolidate other non-mmapped chunks as they arrive.
+    */
+
+    else if (!chunk_is_mmapped(p)) {
+      nextchunk = chunk_at_offset(p, size);
+      nextsize = chunksize(nextchunk);
+      assert(nextsize > 0);
+
+      /* consolidate backward */
+      if (!prev_inuse(p)) {
+        prevsize = p->prev_size;
+        size += prevsize;
+        p = chunk_at_offset(p, -((long) prevsize));
+        unlink(p, bck, fwd);
+      }
+
+      if (nextchunk != av->top) {
+        /* get and clear inuse bit */
+        nextinuse = inuse_bit_at_offset(nextchunk, nextsize);
+
+        /* consolidate forward */
+        if (!nextinuse) {
+          unlink(nextchunk, bck, fwd);
+          size += nextsize;
+        } else
+	  clear_inuse_bit_at_offset(nextchunk, 0);
+
+        /*
+          Place the chunk in unsorted chunk list. Chunks are
+          not placed into regular bins until after they have
+          been given one chance to be used in malloc.
+        */
+
+        bck = unsorted_chunks(av);
+        fwd = bck->fd;
+        p->bk = bck;
+        p->fd = fwd;
+        bck->fd = p;
+        fwd->bk = p;
+
+        set_head(p, size | PREV_INUSE);
+        set_foot(p, size);
+
+        check_free_chunk(av, p);
+      }
+
+      /*
+         If the chunk borders the current high end of memory,
+         consolidate into top
+      */
+
+      else {
+        size += nextsize;
+        set_head(p, size | PREV_INUSE);
+        av->top = p;
+        check_chunk(av, p);
+      }
+
+      /*
+        If freeing a large space, consolidate possibly-surrounding
+        chunks. Then, if the total unused topmost memory exceeds trim
+        threshold, ask malloc_trim to reduce top.
+
+        Unless max_fast is 0, we don't know if there are fastbins
+        bordering top, so we cannot tell for sure whether threshold
+        has been reached unless fastbins are consolidated.  But we
+        don't want to consolidate on each free.  As a compromise,
+        consolidation is performed if FASTBIN_CONSOLIDATION_THRESHOLD
+        is reached.
+      */
+
+      if ((unsigned long)(size) >= FASTBIN_CONSOLIDATION_THRESHOLD) {
+        if (have_fastchunks(av))
+          malloc_consolidate(av);
+      }
+    }
+  }
+}
+
+/*
+  ------------------------- malloc_consolidate -------------------------
+
+  malloc_consolidate is a specialized version of free() that tears
+  down chunks held in fastbins.  Free itself cannot be used for this
+  purpose since, among other things, it might place chunks back onto
+  fastbins.  So, instead, we need to use a minor variant of the same
+  code.
+
+  Also, because this routine needs to be called the first time through
+  malloc anyway, it turns out to be the perfect place to trigger
+  initialization code.
+*/
+
+#if __STD_C
+static void malloc_consolidate(mstate av)
+#else
+static void malloc_consolidate(av) mstate av;
+#endif
+{
+  mfastbinptr*    fb;                 /* current fastbin being consolidated */
+  mfastbinptr*    maxfb;              /* last fastbin (for loop control) */
+  mchunkptr       p;                  /* current chunk being consolidated */
+  mchunkptr       nextp;              /* next chunk to consolidate */
+  mchunkptr       unsorted_bin;       /* bin header */
+  mchunkptr       first_unsorted;     /* chunk to link to */
+
+  /* These have same use as in free() */
+  mchunkptr       nextchunk;
+  INTERNAL_SIZE_T size;
+  INTERNAL_SIZE_T nextsize;
+  INTERNAL_SIZE_T prevsize;
+  int             nextinuse;
+  mchunkptr       bck;
+  mchunkptr       fwd;
+
+  /*
+    If max_fast is 0, we know that av hasn't
+    yet been initialized, in which case do so below
+  */
+
+  if (av->max_fast != 0) {
+    clear_fastchunks(av);
+
+    unsorted_bin = unsorted_chunks(av);
+
+    /*
+      Remove each chunk from fast bin and consolidate it, placing it
+      then in unsorted bin. Among other reasons for doing this,
+      placing in unsorted bin avoids needing to calculate actual bins
+      until malloc is sure that chunks aren't immediately going to be
+      reused anyway.
+    */
+
+    maxfb = &(av->fastbins[fastbin_index(av->max_fast)]);
+    fb = &(av->fastbins[0]);
+    do {
+      if ( (p = *fb) != 0) {
+        *fb = 0;
+
+        do {
+          check_inuse_chunk(av, p);
+          nextp = p->fd;
+
+          /* Slightly streamlined version of consolidation code in free() */
+          size = p->size & ~(PREV_INUSE);
+          nextchunk = chunk_at_offset(p, size);
+          nextsize = chunksize(nextchunk);
+
+          if (!prev_inuse(p)) {
+            prevsize = p->prev_size;
+            size += prevsize;
+            p = chunk_at_offset(p, -((long) prevsize));
+            unlink(p, bck, fwd);
+          }
+
+          if (nextchunk != av->top) {
+            nextinuse = inuse_bit_at_offset(nextchunk, nextsize);
+
+            if (!nextinuse) {
+              size += nextsize;
+              unlink(nextchunk, bck, fwd);
+            } else
+	      clear_inuse_bit_at_offset(nextchunk, 0);
+
+            first_unsorted = unsorted_bin->fd;
+            unsorted_bin->fd = p;
+            first_unsorted->bk = p;
+
+            set_head(p, size | PREV_INUSE);
+            p->bk = unsorted_bin;
+            p->fd = first_unsorted;
+            set_foot(p, size);
+          }
+
+          else {
+            size += nextsize;
+            set_head(p, size | PREV_INUSE);
+            av->top = p;
+          }
+
+        } while ( (p = nextp) != 0);
+
+      }
+    } while (fb++ != maxfb);
+  }
+  else {
+    malloc_init_state(av);
+    check_malloc_state(av);
+  }
+}
+
+/*
+  ------------------------------ realloc ------------------------------
+*/
+
+static Void_t*
+_int_realloc(mstate av, Void_t* oldmem, size_t bytes)
+{
+  INTERNAL_SIZE_T  nb;              /* padded request size */
+
+  mchunkptr        oldp;            /* chunk corresponding to oldmem */
+  INTERNAL_SIZE_T  oldsize;         /* its size */
+
+  mchunkptr        newp;            /* chunk to return */
+  INTERNAL_SIZE_T  newsize;         /* its size */
+  Void_t*          newmem;          /* corresponding user mem */
+
+  mchunkptr        next;            /* next contiguous chunk after oldp */
+
+  mchunkptr        remainder;       /* extra space at end of newp */
+  unsigned long    remainder_size;  /* its size */
+
+  mchunkptr        bck;             /* misc temp for linking */
+  mchunkptr        fwd;             /* misc temp for linking */
+
+  unsigned long    copysize;        /* bytes to copy */
+  unsigned int     ncopies;         /* INTERNAL_SIZE_T words to copy */
+  INTERNAL_SIZE_T* s;               /* copy source */
+  INTERNAL_SIZE_T* d;               /* copy destination */
+
+
+#if REALLOC_ZERO_BYTES_FREES
+  if (bytes == 0) {
+    _int_free(av, oldmem);
+    return 0;
+  }
+#endif
+
+  /* realloc of null is supposed to be same as malloc */
+  if (oldmem == 0) return _int_malloc(av, bytes);
+
+  checked_request2size(bytes, nb);
+
+  oldp    = mem2chunk(oldmem);
+  oldsize = chunksize(oldp);
+
+  check_inuse_chunk(av, oldp);
+
+  // force to act like not mmapped
+  if (1) {
+
+    if ((unsigned long)(oldsize) >= (unsigned long)(nb)) {
+      /* already big enough; split below */
+      newp = oldp;
+      newsize = oldsize;
+    }
+
+    else {
+      next = chunk_at_offset(oldp, oldsize);
+
+      /* Try to expand forward into top */
+      if (next == av->top &&
+          (unsigned long)(newsize = oldsize + chunksize(next)) >=
+          (unsigned long)(nb + MINSIZE)) {
+        set_head_size(oldp, nb );
+        av->top = chunk_at_offset(oldp, nb);
+        set_head(av->top, (newsize - nb) | PREV_INUSE);
+    	check_inuse_chunk(av, oldp);
+        set_arena_for_chunk(oldp, av);
+        return chunk2mem(oldp);
+      }
+
+      /* Try to expand forward into next chunk;  split off remainder below */
+      else if (next != av->top &&
+               !inuse(next) &&
+               (unsigned long)(newsize = oldsize + chunksize(next)) >=
+               (unsigned long)(nb)) {
+        newp = oldp;
+        unlink(next, bck, fwd);
+      }
+
+      /* allocate, copy, free */
+      else {
+        newmem = _int_malloc(av, nb - MALLOC_ALIGN_MASK);
+        if (newmem == 0)
+          return 0; /* propagate failure */
+
+        newp = mem2chunk(newmem);
+        newsize = chunksize(newp);
+
+        /*
+          Avoid copy if newp is next chunk after oldp.
+        */
+        if (newp == next) {
+          newsize += oldsize;
+          newp = oldp;
+        }
+        else {
+          /*
+            Unroll copy of <= 36 bytes (72 if 8byte sizes)
+            We know that contents have an odd number of
+            INTERNAL_SIZE_T-sized words; minimally 3.
+          */
+
+          copysize = oldsize - SIZE_SZ;
+          s = (INTERNAL_SIZE_T*)(oldmem);
+          d = (INTERNAL_SIZE_T*)(newmem);
+          ncopies = copysize / sizeof(INTERNAL_SIZE_T);
+          assert(ncopies >= 3);
+
+          if (ncopies > 9)
+            MALLOC_COPY(d, s, copysize);
+
+          else {
+            *(d+0) = *(s+0);
+            *(d+1) = *(s+1);
+            *(d+2) = *(s+2);
+            if (ncopies > 4) {
+              *(d+3) = *(s+3);
+              *(d+4) = *(s+4);
+              if (ncopies > 6) {
+                *(d+5) = *(s+5);
+                *(d+6) = *(s+6);
+                if (ncopies > 8) {
+                  *(d+7) = *(s+7);
+                  *(d+8) = *(s+8);
+                }
+              }
+            }
+          }
+
+          _int_free(av, oldmem);
+          set_arena_for_chunk(newp, av);
+          check_inuse_chunk(av, newp);
+          return chunk2mem(newp);
+        }
+      }
+    }
+
+    /* If possible, free extra space in old or extended chunk */
+
+    assert((unsigned long)(newsize) >= (unsigned long)(nb));
+
+    remainder_size = newsize - nb;
+
+    if (remainder_size < MINSIZE) { /* not enough extra to split off */
+      set_head_size(newp, newsize);
+      set_inuse_bit_at_offset(newp, newsize);
+    }
+    else { /* split remainder */
+      remainder = chunk_at_offset(newp, nb);
+      set_head_size(newp, nb );
+      set_head(remainder, remainder_size | PREV_INUSE );
+      /* Mark remainder as inuse so free() won't complain */
+      set_inuse_bit_at_offset(remainder, remainder_size);
+      set_arena_for_chunk(remainder, av);
+      _int_free(av, chunk2mem(remainder));
+    }
+
+    set_arena_for_chunk(newp, av);
+    check_inuse_chunk(av, newp);
+    return chunk2mem(newp);
+  }
+
+  /*
+    Handle mmap cases
+  */
+
+  else {
+    /* If !HAVE_MMAP, but chunk_is_mmapped, user must have overwritten mem */
+    check_malloc_state(av);
+    MALLOC_FAILURE_ACTION;
+    return 0;
+  }
+}
+
+/*
+  ------------------------------ memalign ------------------------------
+*/
+
+static Void_t*
+_int_memalign(mstate av, size_t alignment, size_t bytes)
+{
+  INTERNAL_SIZE_T nb;             /* padded  request size */
+  char*           m;              /* memory returned by malloc call */
+  mchunkptr       p;              /* corresponding chunk */
+  char*           brk;            /* alignment point within p */
+  mchunkptr       newp;           /* chunk to return */
+  INTERNAL_SIZE_T newsize;        /* its size */
+  INTERNAL_SIZE_T leadsize;       /* leading space before alignment point */
+  mchunkptr       remainder;      /* spare room at end to split off */
+  unsigned long   remainder_size; /* its size */
+  INTERNAL_SIZE_T size;
+
+  /* If need less alignment than we give anyway, just relay to malloc */
+
+  if (alignment <= MALLOC_ALIGNMENT) return _int_malloc(av, bytes);
+
+  /* Otherwise, ensure that it is at least a minimum chunk size */
+
+  if (alignment <  MINSIZE) alignment = MINSIZE;
+
+  /* Make sure alignment is power of 2 (in case MINSIZE is not).  */
+  if ((alignment & (alignment - 1)) != 0) {
+    size_t a = MALLOC_ALIGNMENT * 2;
+    while ((unsigned long)a < (unsigned long)alignment) a <<= 1;
+    alignment = a;
+  }
+
+  checked_request2size(bytes, nb);
+
+  /*
+    Strategy: find a spot within that chunk that meets the alignment
+    request, and then possibly free the leading and trailing space.
+  */
+
+
+  /* Call malloc with worst case padding to hit alignment. */
+
+  m  = (char*)(_int_malloc(av, nb + alignment + MINSIZE));
+
+  if (m == 0) return 0; /* propagate failure */
+
+  p = mem2chunk(m);
+
+  if ((((unsigned long)(m)) % alignment) != 0) { /* misaligned */
+
+    /*
+      Find an aligned spot inside chunk.  Since we need to give back
+      leading space in a chunk of at least MINSIZE, if the first
+      calculation places us at a spot with less than MINSIZE leader,
+      we can move to the next aligned spot -- we've allocated enough
+      total room so that this is always possible.
+    */
+
+    brk = (char*)mem2chunk(((unsigned long)(m + alignment - 1)) &
+                           -((signed long) alignment));
+    if ((unsigned long)(brk - (char*)(p)) < MINSIZE)
+      brk += alignment;
+
+    newp = (mchunkptr)brk;
+    leadsize = brk - (char*)(p);
+    newsize = chunksize(p) - leadsize;
+
+    /* For mmapped chunks, just adjust offset */
+    if (chunk_is_mmapped(p)) {
+      newp->prev_size = p->prev_size + leadsize;
+      set_head(newp, newsize|IS_MMAPPED);
+      set_arena_for_chunk(newp, av);
+      return chunk2mem(newp);
+    }
+
+    /* Otherwise, give back leader, use the rest */
+    set_head(newp, newsize | PREV_INUSE );
+    set_inuse_bit_at_offset(newp, newsize);
+    set_head_size(p, leadsize);
+    set_arena_for_chunk(p, av);
+    _int_free(av, chunk2mem(p));
+    p = newp;
+
+    assert (newsize >= nb &&
+            (((unsigned long)(chunk2mem(p))) % alignment) == 0);
+  }
+
+  /* Also give back spare room at the end */
+  if (!chunk_is_mmapped(p)) {
+    size = chunksize(p);
+    if ((unsigned long)(size) > (unsigned long)(nb + MINSIZE)) {
+      remainder_size = size - nb;
+      remainder = chunk_at_offset(p, nb);
+      set_head(remainder, remainder_size | PREV_INUSE );
+      set_head_size(p, nb);
+      set_arena_for_chunk(remainder, av);
+      _int_free(av, chunk2mem(remainder));
+    }
+  }
+
+  set_arena_for_chunk(p, av);
+  check_inuse_chunk(av, p);
+  return chunk2mem(p);
+}
+
+#if 1
+/*
+  ------------------------------ calloc ------------------------------
+*/
+
+#if __STD_C
+Void_t* cALLOc(cvmx_arena_list_t arena_list, size_t n_elements, size_t elem_size)
+#else
+Void_t* cALLOc(n_elements, elem_size) size_t n_elements; size_t elem_size;
+#endif
+{
+  mchunkptr p;
+  unsigned long clearsize;
+  unsigned long nclears;
+  INTERNAL_SIZE_T* d;
+
+  Void_t* mem = public_mALLOc(arena_list, n_elements * elem_size);
+
+  if (mem != 0) {
+    p = mem2chunk(mem);
+
+    {
+      /*
+        Unroll clear of <= 36 bytes (72 if 8byte sizes)
+        We know that contents have an odd number of
+        INTERNAL_SIZE_T-sized words; minimally 3.
+      */
+
+      d = (INTERNAL_SIZE_T*)mem;
+      clearsize = chunksize(p) - SIZE_SZ;
+      nclears = clearsize / sizeof(INTERNAL_SIZE_T);
+      assert(nclears >= 3);
+
+      if (nclears > 9)
+        MALLOC_ZERO(d, clearsize);
+
+      else {
+        *(d+0) = 0;
+        *(d+1) = 0;
+        *(d+2) = 0;
+        if (nclears > 4) {
+          *(d+3) = 0;
+          *(d+4) = 0;
+          if (nclears > 6) {
+            *(d+5) = 0;
+            *(d+6) = 0;
+            if (nclears > 8) {
+              *(d+7) = 0;
+              *(d+8) = 0;
+            }
+          }
+        }
+      }
+    }
+  }
+  return mem;
+}
+#endif
+
+
+/*
+  ------------------------- malloc_usable_size -------------------------
+*/
+
+#if __STD_C
+size_t mUSABLe(Void_t* mem)
+#else
+size_t mUSABLe(mem) Void_t* mem;
+#endif
+{
+  mchunkptr p;
+  if (mem != 0) {
+    p = mem2chunk(mem);
+    if (chunk_is_mmapped(p))
+      return chunksize(p) - 3*SIZE_SZ; /* updated size for adding arena_ptr */
+    else if (inuse(p))
+      return chunksize(p) - 2*SIZE_SZ; /* updated size for adding arena_ptr */
+  }
+  return 0;
+}
+
+/*
+  ------------------------------ mallinfo ------------------------------
+*/
+
+struct mallinfo mALLINFo(mstate av)
+{
+  struct mallinfo mi;
+  int i;
+  mbinptr b;
+  mchunkptr p;
+  INTERNAL_SIZE_T avail;
+  INTERNAL_SIZE_T fastavail;
+  int nblocks;
+  int nfastblocks;
+
+  /* Ensure initialization */
+  if (av->top == 0)  malloc_consolidate(av);
+
+  check_malloc_state(av);
+
+  /* Account for top */
+  avail = chunksize(av->top);
+  nblocks = 1;  /* top always exists */
+
+  /* traverse fastbins */
+  nfastblocks = 0;
+  fastavail = 0;
+
+  for (i = 0; i < NFASTBINS; ++i) {
+    for (p = av->fastbins[i]; p != 0; p = p->fd) {
+      ++nfastblocks;
+      fastavail += chunksize(p);
+    }
+  }
+
+  avail += fastavail;
+
+  /* traverse regular bins */
+  for (i = 1; i < NBINS; ++i) {
+    b = bin_at(av, i);
+    for (p = last(b); p != b; p = p->bk) {
+      ++nblocks;
+      avail += chunksize(p);
+    }
+  }
+
+  mi.smblks = nfastblocks;
+  mi.ordblks = nblocks;
+  mi.fordblks = avail;
+  mi.uordblks = av->system_mem - avail;
+  mi.arena = av->system_mem;
+  mi.fsmblks = fastavail;
+  mi.keepcost = chunksize(av->top);
+  return mi;
+}
+
+/*
+  ------------------------------ malloc_stats ------------------------------
+*/
+
+void mSTATs()
+{
+}
+
+
+/*
+  ------------------------------ mallopt ------------------------------
+*/
+
+#if 0
+#if __STD_C
+int mALLOPt(int param_number, int value)
+#else
+int mALLOPt(param_number, value) int param_number; int value;
+#endif
+{
+}
+#endif
+
+
+/*
+  -------------------- Alternative MORECORE functions --------------------
+*/
+
+
+/*
+  General Requirements for MORECORE.
+
+  The MORECORE function must have the following properties:
+
+  If MORECORE_CONTIGUOUS is false:
+
+    * MORECORE must allocate in multiples of pagesize. It will
+      only be called with arguments that are multiples of pagesize.
+
+    * MORECORE(0) must return an address that is at least
+      MALLOC_ALIGNMENT aligned. (Page-aligning always suffices.)
+
+  else (i.e. If MORECORE_CONTIGUOUS is true):
+
+    * Consecutive calls to MORECORE with positive arguments
+      return increasing addresses, indicating that space has been
+      contiguously extended.
+
+    * MORECORE need not allocate in multiples of pagesize.
+      Calls to MORECORE need not have args of multiples of pagesize.
+
+    * MORECORE need not page-align.
+
+  In either case:
+
+    * MORECORE may allocate more memory than requested. (Or even less,
+      but this will generally result in a malloc failure.)
+
+    * MORECORE must not allocate memory when given argument zero, but
+      instead return one past the end address of memory from previous
+      nonzero call. This malloc does NOT call MORECORE(0)
+      until at least one call with positive arguments is made, so
+      the initial value returned is not important.
+
+    * Even though consecutive calls to MORECORE need not return contiguous
+      addresses, it must be OK for malloc'ed chunks to span multiple
+      regions in those cases where they do happen to be contiguous.
+
+    * MORECORE need not handle negative arguments -- it may instead
+      just return MORECORE_FAILURE when given negative arguments.
+      Negative arguments are always multiples of pagesize. MORECORE
+      must not misinterpret negative args as large positive unsigned
+      args. You can suppress all such calls from even occurring by defining
+      MORECORE_CANNOT_TRIM,
+
+  There is some variation across systems about the type of the
+  argument to sbrk/MORECORE. If size_t is unsigned, then it cannot
+  actually be size_t, because sbrk supports negative args, so it is
+  normally the signed type of the same width as size_t (sometimes
+  declared as "intptr_t", and sometimes "ptrdiff_t").  It doesn't much
+  matter though. Internally, we use "long" as arguments, which should
+  work across all reasonable possibilities.
+
+  Additionally, if MORECORE ever returns failure for a positive
+  request, and HAVE_MMAP is true, then mmap is used as a noncontiguous
+  system allocator. This is a useful backup strategy for systems with
+  holes in address spaces -- in this case sbrk cannot contiguously
+  expand the heap, but mmap may be able to map noncontiguous space.
+
+  If you'd like mmap to ALWAYS be used, you can define MORECORE to be
+  a function that always returns MORECORE_FAILURE.
+
+  If you are using this malloc with something other than sbrk (or its
+  emulation) to supply memory regions, you probably want to set
+  MORECORE_CONTIGUOUS as false.  As an example, here is a custom
+  allocator kindly contributed for pre-OSX macOS.  It uses virtually
+  but not necessarily physically contiguous non-paged memory (locked
+  in, present and won't get swapped out).  You can use it by
+  uncommenting this section, adding some #includes, and setting up the
+  appropriate defines above:
+
+      #define MORECORE osMoreCore
+      #define MORECORE_CONTIGUOUS 0
+
+  There is also a shutdown routine that should somehow be called for
+  cleanup upon program exit.
+
+  #define MAX_POOL_ENTRIES 100
+  #define MINIMUM_MORECORE_SIZE  (64 * 1024)
+  static int next_os_pool;
+  void *our_os_pools[MAX_POOL_ENTRIES];
+
+  void *osMoreCore(int size)
+  {
+    void *ptr = 0;
+    static void *sbrk_top = 0;
+
+    if (size > 0)
+    {
+      if (size < MINIMUM_MORECORE_SIZE)
+         size = MINIMUM_MORECORE_SIZE;
+      if (CurrentExecutionLevel() == kTaskLevel)
+         ptr = PoolAllocateResident(size + RM_PAGE_SIZE, 0);
+      if (ptr == 0)
+      {
+        return (void *) MORECORE_FAILURE;
+      }
+      // save ptrs so they can be freed during cleanup
+      our_os_pools[next_os_pool] = ptr;
+      next_os_pool++;
+      ptr = (void *) ((((unsigned long) ptr) + RM_PAGE_MASK) & ~RM_PAGE_MASK);
+      sbrk_top = (char *) ptr + size;
+      return ptr;
+    }
+    else if (size < 0)
+    {
+      // we don't currently support shrink behavior
+      return (void *) MORECORE_FAILURE;
+    }
+    else
+    {
+      return sbrk_top;
+    }
+  }
+
+  // cleanup any allocated memory pools
+  // called as last thing before shutting down driver
+
+  void osCleanupMem(void)
+  {
+    void **ptr;
+
+    for (ptr = our_os_pools; ptr < &our_os_pools[MAX_POOL_ENTRIES]; ptr++)
+      if (*ptr)
+      {
+         PoolDeallocate(*ptr);
+         *ptr = 0;
+      }
+  }
+
+*/
+
+
+
+/* ------------------------------------------------------------
+History:
+
+[see ftp://g.oswego.edu/pub/misc/malloc.c for the history of dlmalloc]
+
+*/
diff --git a/arch/mips/cavium-octeon/executive/cvmx-malloc/malloc.h b/arch/mips/cavium-octeon/executive/cvmx-malloc/malloc.h
new file mode 100644
index 0000000..6d6f634
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-malloc/malloc.h
@@ -0,0 +1,213 @@
+/*
+Copyright (c) 2001 Wolfram Gloger
+Copyright (c) 2006 Cavium networks
+
+Permission to use, copy, modify, distribute, and sell this software
+and its documentation for any purpose is hereby granted without fee,
+provided that (i) the above copyright notices and this permission
+notice appear in all copies of the software and related documentation,
+and (ii) the name of Wolfram Gloger may not be used in any advertising
+or publicity relating to the software.
+
+THE SOFTWARE IS PROVIDED "AS-IS" AND WITHOUT WARRANTY OF ANY KIND,
+EXPRESS, IMPLIED OR OTHERWISE, INCLUDING WITHOUT LIMITATION, ANY
+WARRANTY OF MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE.
+
+IN NO EVENT SHALL WOLFRAM GLOGER BE LIABLE FOR ANY SPECIAL,
+INCIDENTAL, INDIRECT OR CONSEQUENTIAL DAMAGES OF ANY KIND, OR ANY
+DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS,
+WHETHER OR NOT ADVISED OF THE POSSIBILITY OF DAMAGE, AND ON ANY THEORY
+OF LIABILITY, ARISING OUT OF OR IN CONNECTION WITH THE USE OR
+PERFORMANCE OF THIS SOFTWARE.
+*/
+
+#ifndef _MALLOC_H
+#define _MALLOC_H 1
+
+#undef _LIBC
+#ifdef _LIBC
+#include <features.h>
+#endif
+
+/*
+  $Id: malloc.h 30481 2007-12-05 21:46:59Z rfranz $
+  `ptmalloc2', a malloc implementation for multiple threads without
+  lock contention, by Wolfram Gloger <wg@malloc.de>.
+
+  VERSION 2.7.0
+
+  This work is mainly derived from malloc-2.7.0 by Doug Lea
+  <dl@cs.oswego.edu>, which is available from:
+
+                 ftp://gee.cs.oswego.edu/pub/misc/malloc.c
+
+  This trimmed-down header file only provides function prototypes and
+  the exported data structures.  For more detailed function
+  descriptions and compile-time options, see the source file
+  `malloc.c'.
+*/
+
+#if 0
+# include <stddef.h>
+# define __malloc_ptr_t  void *
+# undef  size_t
+# define size_t          unsigned long
+# undef  ptrdiff_t
+# define ptrdiff_t       long
+#else
+# undef  Void_t
+# define Void_t       void
+# define __malloc_ptr_t  char *
+#endif
+
+#ifdef _LIBC
+/* Used by GNU libc internals. */
+# define __malloc_size_t size_t
+# define __malloc_ptrdiff_t ptrdiff_t
+#elif !defined __attribute_malloc__
+# define __attribute_malloc__
+#endif
+
+#ifdef __GNUC__
+
+/* GCC can always grok prototypes.  For C++ programs we add throw()
+   to help it optimize the function calls.  But this works only with
+   gcc 2.8.x and egcs.  */
+# if defined __cplusplus && (__GNUC__ >= 3 || __GNUC_MINOR__ >= 8)
+#  define __THROW	throw ()
+# else
+#  define __THROW
+# endif
+# define __MALLOC_P(args)	args __THROW
+/* This macro will be used for functions which might take C++ callback
+   functions.  */
+# define __MALLOC_PMT(args)	args
+
+#else	/* Not GCC.  */
+
+# define __THROW
+
+# if (defined __STDC__ && __STDC__) || defined __cplusplus
+
+#  define __MALLOC_P(args)	args
+#  define __MALLOC_PMT(args)	args
+
+# else	/* Not ANSI C or C++.  */
+
+#  define __MALLOC_P(args)	()	/* No prototypes.  */
+#  define __MALLOC_PMT(args)	()
+
+# endif	/* ANSI C or C++.  */
+
+#endif	/* GCC.  */
+
+#ifndef NULL
+# ifdef __cplusplus
+#  define NULL	0
+# else
+#  define NULL	((__malloc_ptr_t) 0)
+# endif
+#endif
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/* Nonzero if the malloc is already initialized.  */
+#ifdef _LIBC
+/* In the GNU libc we rename the global variable
+   `__malloc_initialized' to `__libc_malloc_initialized'.  */
+# define __malloc_initialized __libc_malloc_initialized
+#endif
+extern int cvmx__malloc_initialized;
+
+
+/* SVID2/XPG mallinfo structure */
+
+struct mallinfo {
+  int arena;    /* non-mmapped space allocated from system */
+  int ordblks;  /* number of free chunks */
+  int smblks;   /* number of fastbin blocks */
+  int hblks;    /* number of mmapped regions */
+  int hblkhd;   /* space in mmapped regions */
+  int usmblks;  /* maximum total allocated space */
+  int fsmblks;  /* space available in freed fastbin blocks */
+  int uordblks; /* total allocated space */
+  int fordblks; /* total free space */
+  int keepcost; /* top-most, releasable (via malloc_trim) space */
+};
+
+/* Returns a copy of the updated current mallinfo. */
+extern struct mallinfo mallinfo __MALLOC_P ((void));
+
+/* SVID2/XPG mallopt options */
+#ifndef M_MXFAST
+# define M_MXFAST  1	/* maximum request size for "fastbins" */
+#endif
+#ifndef M_NLBLKS
+# define M_NLBLKS  2	/* UNUSED in this malloc */
+#endif
+#ifndef M_GRAIN
+# define M_GRAIN   3	/* UNUSED in this malloc */
+#endif
+#ifndef M_KEEP
+# define M_KEEP    4	/* UNUSED in this malloc */
+#endif
+
+/* mallopt options that actually do something */
+#define M_TRIM_THRESHOLD    -1
+#define M_TOP_PAD           -2
+#define M_MMAP_THRESHOLD    -3
+#define M_MMAP_MAX          -4
+#define M_CHECK_ACTION      -5
+
+/* General SVID/XPG interface to tunable parameters. */
+extern int mallopt __MALLOC_P ((int __param, int __val));
+
+/* Release all but __pad bytes of freed top-most memory back to the
+   system. Return 1 if successful, else 0. */
+extern int malloc_trim __MALLOC_P ((size_t __pad));
+
+/* Report the number of usable allocated bytes associated with allocated
+   chunk __ptr. */
+extern size_t malloc_usable_size __MALLOC_P ((__malloc_ptr_t __ptr));
+
+/* Prints brief summary statistics on stderr. */
+extern void malloc_stats __MALLOC_P ((void));
+
+/* Record the state of all malloc variables in an opaque data structure. */
+extern __malloc_ptr_t malloc_get_state __MALLOC_P ((void));
+
+/* Restore the state of all malloc variables from data obtained with
+   malloc_get_state(). */
+extern int malloc_set_state __MALLOC_P ((__malloc_ptr_t __ptr));
+
+/* Called once when malloc is initialized; redefining this variable in
+   the application provides the preferred way to set up the hook
+   pointers. */
+extern void (*cmvx__malloc_initialize_hook) __MALLOC_PMT ((void));
+/* Hooks for debugging and user-defined versions. */
+extern void (*cvmx__free_hook) __MALLOC_PMT ((__malloc_ptr_t __ptr,
+					__const __malloc_ptr_t));
+extern __malloc_ptr_t (*cvmx__malloc_hook) __MALLOC_PMT ((size_t __size,
+						    __const __malloc_ptr_t));
+extern __malloc_ptr_t (*cvmx__realloc_hook) __MALLOC_PMT ((__malloc_ptr_t __ptr,
+						     size_t __size,
+						     __const __malloc_ptr_t));
+extern __malloc_ptr_t (*cvmx__memalign_hook) __MALLOC_PMT ((size_t __alignment,
+						      size_t __size,
+						      __const __malloc_ptr_t));
+extern void (*__after_morecore_hook) __MALLOC_PMT ((void));
+
+/* Activate a standard set of debugging hooks. */
+extern void cvmx__malloc_check_init __MALLOC_P ((void));
+
+/* Internal routines, operating on "arenas".  */
+struct malloc_state;
+typedef struct malloc_state *mstate;
+#ifdef __cplusplus
+}; /* end of extern "C" */
+#endif
+
+
+#endif /* malloc.h */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-malloc/thread-m.h b/arch/mips/cavium-octeon/executive/cvmx-malloc/thread-m.h
new file mode 100644
index 0000000..de9ba6c
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-malloc/thread-m.h
@@ -0,0 +1,73 @@
+/*
+Copyright (c) 2001 Wolfram Gloger
+Copyright (c) 2006 Cavium networks
+
+Permission to use, copy, modify, distribute, and sell this software
+and its documentation for any purpose is hereby granted without fee,
+provided that (i) the above copyright notices and this permission
+notice appear in all copies of the software and related documentation,
+and (ii) the name of Wolfram Gloger may not be used in any advertising
+or publicity relating to the software.
+
+THE SOFTWARE IS PROVIDED "AS-IS" AND WITHOUT WARRANTY OF ANY KIND,
+EXPRESS, IMPLIED OR OTHERWISE, INCLUDING WITHOUT LIMITATION, ANY
+WARRANTY OF MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE.
+
+IN NO EVENT SHALL WOLFRAM GLOGER BE LIABLE FOR ANY SPECIAL,
+INCIDENTAL, INDIRECT OR CONSEQUENTIAL DAMAGES OF ANY KIND, OR ANY
+DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS,
+WHETHER OR NOT ADVISED OF THE POSSIBILITY OF DAMAGE, AND ON ANY THEORY
+OF LIABILITY, ARISING OUT OF OR IN CONNECTION WITH THE USE OR
+PERFORMANCE OF THIS SOFTWARE.
+*/
+
+/* $Id: thread-m.h 30481 2007-12-05 21:46:59Z rfranz $
+   One out of _LIBC, USE_PTHREADS, USE_THR or USE_SPROC should be
+   defined, otherwise the token NO_THREADS and dummy implementations
+   of the macros will be defined.  */
+
+#ifndef _THREAD_M_H
+#define _THREAD_M_H
+
+#undef thread_atfork_static
+
+
+#undef NO_THREADS /* No threads, provide dummy macros */
+
+typedef int thread_id;
+
+/* The mutex functions used to do absolutely nothing, i.e. lock,
+   trylock and unlock would always just return 0.  However, even
+   without any concurrently active threads, a mutex can be used
+   legitimately as an `in use' flag.  To make the code that is
+   protected by a mutex async-signal safe, these macros would have to
+   be based on atomic test-and-set operations, for example. */
+#ifdef __OCTEON__
+typedef cvmx_spinlock_t mutex_t;
+#define MUTEX_INITIALIZER          CMVX_SPINLOCK_UNLOCKED_VAL
+#define mutex_init(m)              cvmx_spinlock_init(m)
+#define mutex_lock(m)              cvmx_spinlock_lock(m)
+#define mutex_trylock(m)           (cvmx_spinlock_trylock(m))
+#define mutex_unlock(m)            cvmx_spinlock_unlock(m)
+#else
+
+typedef int mutex_t;
+
+#define MUTEX_INITIALIZER          0
+#define mutex_init(m)              (*(m) = 0)
+#define mutex_lock(m)              ((*(m) = 1), 0)
+#define mutex_trylock(m)           (*(m) ? 1 : ((*(m) = 1), 0))
+#define mutex_unlock(m)            (*(m) = 0)
+#endif
+
+
+
+typedef void *tsd_key_t;
+#define tsd_key_create(key, destr) do {} while(0)
+#define tsd_setspecific(key, data) ((key) = (data))
+#define tsd_getspecific(key, vptr) (vptr = (key))
+
+#define thread_atfork(prepare, parent, child) do {} while(0)
+
+
+#endif /* !defined(_THREAD_M_H) */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-mdio.h b/arch/mips/cavium-octeon/executive/cvmx-mdio.h
new file mode 100644
index 0000000..411747f
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-mdio.h
@@ -0,0 +1,353 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Interface to the SMI/MDIO hardware.
+ *
+ * <hr>$Revision: 34797 $<hr>
+ */
+
+#ifndef __CVMX_MIO_H__
+#define __CVMX_MIO_H__
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+/**
+ * PHY register 0 from the 802.3 spec
+ */
+#define CVMX_MDIO_PHY_REG_CONTROL 0
+typedef union
+{
+    uint16_t u16;
+    struct
+    {
+        uint16_t reset : 1;
+        uint16_t loopback : 1;
+        uint16_t speed_lsb : 1;
+        uint16_t autoneg_enable : 1;
+        uint16_t power_down : 1;
+        uint16_t isolate : 1;
+        uint16_t restart_autoneg : 1;
+        uint16_t duplex : 1;
+        uint16_t collision_test : 1;
+        uint16_t speed_msb : 1;
+        uint16_t unidirectional_enable : 1;
+        uint16_t reserved_0_4 : 5;
+    } s;
+} cvmx_mdio_phy_reg_control_t;
+
+/**
+ * PHY register 1 from the 802.3 spec
+ */
+#define CVMX_MDIO_PHY_REG_STATUS 1
+typedef union
+{
+    uint16_t u16;
+    struct
+    {
+        uint16_t capable_100base_t4 : 1;
+        uint16_t capable_100base_x_full : 1;
+        uint16_t capable_100base_x_half : 1;
+        uint16_t capable_10_full : 1;
+        uint16_t capable_10_half : 1;
+        uint16_t capable_100base_t2_full : 1;
+        uint16_t capable_100base_t2_half : 1;
+        uint16_t capable_extended_status : 1;
+        uint16_t capable_unidirectional : 1;
+        uint16_t capable_mf_preamble_suppression : 1;
+        uint16_t autoneg_complete : 1;
+        uint16_t remote_fault : 1;
+        uint16_t capable_autoneg : 1;
+        uint16_t link_status : 1;
+        uint16_t jabber_detect : 1;
+        uint16_t capable_extended_registers : 1;
+
+    } s;
+} cvmx_mdio_phy_reg_status_t;
+
+/**
+ * PHY register 2 from the 802.3 spec
+ */
+#define CVMX_MDIO_PHY_REG_ID1 2
+typedef union
+{
+    uint16_t u16;
+    struct
+    {
+        uint16_t oui_bits_3_18;
+    } s;
+} cvmx_mdio_phy_reg_id1_t;
+
+/**
+ * PHY register 3 from the 802.3 spec
+ */
+#define CVMX_MDIO_PHY_REG_ID2 3
+typedef union
+{
+    uint16_t u16;
+    struct
+    {
+        uint16_t oui_bits_19_24 : 6;
+        uint16_t model : 6;
+        uint16_t revision : 4;
+    } s;
+} cvmx_mdio_phy_reg_id2_t;
+
+/**
+ * PHY register 4 from the 802.3 spec
+ */
+#define CVMX_MDIO_PHY_REG_AUTONEG_ADVER 4
+typedef union
+{
+    uint16_t u16;
+    struct
+    {
+        uint16_t next_page : 1;
+        uint16_t reserved_14 : 1;
+        uint16_t remote_fault : 1;
+        uint16_t reserved_12 : 1;
+        uint16_t asymmetric_pause : 1;
+        uint16_t pause : 1;
+        uint16_t advert_100base_t4 : 1;
+        uint16_t advert_100base_tx_full : 1;
+        uint16_t advert_100base_tx_half : 1;
+        uint16_t advert_10base_tx_full : 1;
+        uint16_t advert_10base_tx_half : 1;
+        uint16_t selector : 5;
+    } s;
+} cvmx_mdio_phy_reg_autoneg_adver_t;
+
+/**
+ * PHY register 5 from the 802.3 spec
+ */
+#define CVMX_MDIO_PHY_REG_LINK_PARTNER_ABILITY 5
+typedef union
+{
+    uint16_t u16;
+    struct
+    {
+        uint16_t next_page : 1;
+        uint16_t ack : 1;
+        uint16_t remote_fault : 1;
+        uint16_t reserved_12 : 1;
+        uint16_t asymmetric_pause : 1;
+        uint16_t pause : 1;
+        uint16_t advert_100base_t4 : 1;
+        uint16_t advert_100base_tx_full : 1;
+        uint16_t advert_100base_tx_half : 1;
+        uint16_t advert_10base_tx_full : 1;
+        uint16_t advert_10base_tx_half : 1;
+        uint16_t selector : 5;
+    } s;
+} cvmx_mdio_phy_reg_link_partner_ability_t;
+
+/**
+ * PHY register 6 from the 802.3 spec
+ */
+#define CVMX_MDIO_PHY_REG_AUTONEG_EXPANSION 6
+typedef union
+{
+    uint16_t u16;
+    struct
+    {
+        uint16_t reserved_5_15 : 11;
+        uint16_t parallel_detection_fault : 1;
+        uint16_t link_partner_next_page_capable : 1;
+        uint16_t local_next_page_capable : 1;
+        uint16_t page_received : 1;
+        uint16_t link_partner_autoneg_capable : 1;
+
+    } s;
+} cvmx_mdio_phy_reg_autoneg_expansion_t;
+
+/**
+ * PHY register 9 from the 802.3 spec
+ */
+#define CVMX_MDIO_PHY_REG_CONTROL_1000 9
+typedef union
+{
+    uint16_t u16;
+    struct
+    {
+        uint16_t test_mode : 3;
+        uint16_t manual_master_slave : 1;
+        uint16_t master : 1;
+        uint16_t port_type : 1;
+        uint16_t advert_1000base_t_full : 1;
+        uint16_t advert_1000base_t_half : 1;
+        uint16_t reserved_0_7 : 8;
+    } s;
+} cvmx_mdio_phy_reg_control_1000_t;
+
+/**
+ * PHY register 10 from the 802.3 spec
+ */
+#define CVMX_MDIO_PHY_REG_STATUS_1000 10
+typedef union
+{
+    uint16_t u16;
+    struct
+    {
+        uint16_t master_slave_fault : 1;
+        uint16_t is_master : 1;
+        uint16_t local_receiver_ok : 1;
+        uint16_t remote_receiver_ok : 1;
+        uint16_t remote_capable_1000base_t_full : 1;
+        uint16_t remote_capable_1000base_t_half : 1;
+        uint16_t reserved_8_9 : 2;
+        uint16_t idle_error_count : 8;
+    } s;
+} cvmx_mdio_phy_reg_status_1000_t;
+
+/**
+ * PHY register 15 from the 802.3 spec
+ */
+#define CVMX_MDIO_PHY_REG_EXTENDED_STATUS 15
+typedef union
+{
+    uint16_t u16;
+    struct
+    {
+        uint16_t capable_1000base_x_full : 1;
+        uint16_t capable_1000base_x_half : 1;
+        uint16_t capable_1000base_t_full : 1;
+        uint16_t capable_1000base_t_half : 1;
+        uint16_t reserved_0_11 : 12;
+    } s;
+} cvmx_mdio_phy_reg_extended_status_t;
+
+
+/**
+ * Perform an MII read. This function is used to read PHY
+ * registers controlling auto negotiation.
+ *
+ * @param bus_id   MDIO bus number. Zero on most chips, but some chips (ex CN56XX)
+ *                 support multiple busses.
+ * @param phy_id   The MII phy id
+ * @param location Register location to read
+ *
+ * @return Result from the read or -1 on failure
+ */
+static inline int cvmx_mdio_read(int bus_id, int phy_id, int location)
+{
+    cvmx_smix_cmd_t smi_cmd;
+    cvmx_smix_rd_dat_t smi_rd;
+    int timeout = 1000;
+
+    smi_cmd.u64 = 0;
+    smi_cmd.s.phy_op = 1;
+    smi_cmd.s.phy_adr = phy_id;
+    smi_cmd.s.reg_adr = location;
+    cvmx_write_csr(CVMX_SMIX_CMD(bus_id), smi_cmd.u64);
+
+    do
+    {
+        cvmx_wait(1000);
+        smi_rd.u64 = cvmx_read_csr(CVMX_SMIX_RD_DAT(bus_id));
+    } while (smi_rd.s.pending && timeout--);
+
+    if (smi_rd.s.val)
+        return smi_rd.s.dat;
+    else
+        return -1;
+}
+
+
+/**
+ * Perform an MII write. This function is used to write PHY
+ * registers controlling auto negotiation.
+ *
+ * @param bus_id   MDIO bus number. Zero on most chips, but some chips (ex CN56XX)
+ *                 support multiple busses.
+ * @param phy_id   The MII phy id
+ * @param location Register location to write
+ * @param val      Value to write
+ *
+ * @return -1 on error
+ *         0 on success
+ */
+static inline int cvmx_mdio_write(int bus_id, int phy_id, int location, int val)
+{
+    cvmx_smix_cmd_t smi_cmd;
+    cvmx_smix_wr_dat_t smi_wr;
+    int timeout = 1000;
+
+    smi_wr.u64 = 0;
+    smi_wr.s.dat = val;
+    cvmx_write_csr(CVMX_SMIX_WR_DAT(bus_id), smi_wr.u64);
+
+    smi_cmd.u64 = 0;
+    smi_cmd.s.phy_op = 0;
+    smi_cmd.s.phy_adr = phy_id;
+    smi_cmd.s.reg_adr = location;
+    cvmx_write_csr(CVMX_SMIX_CMD(bus_id), smi_cmd.u64);
+
+    do
+    {
+        cvmx_wait(1000);
+        smi_wr.u64 = cvmx_read_csr(CVMX_SMIX_WR_DAT(bus_id));
+    } while (smi_wr.s.pending && --timeout);
+    if (timeout <= 0)
+        return -1;
+
+    return 0;
+}
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-mgmt-port.c b/arch/mips/cavium-octeon/executive/cvmx-mgmt-port.c
new file mode 100644
index 0000000..a068344
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-mgmt-port.c
@@ -0,0 +1,665 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Support functions for managing the MII management port
+ *
+ * <hr>$Revision: 35733 $<hr>
+ */
+#include "cvmx.h"
+#include "cvmx-bootmem.h"
+#include "cvmx-spinlock.h"
+#include "cvmx-mdio.h"
+#include "cvmx-mgmt-port.h"
+#include "cvmx-sysinfo.h"
+
+#define CVMX_MGMT_PORT_NUM_PORTS        2       /* Right now we only have one mgmt port */
+#define CVMX_MGMT_PORT_NUM_TX_BUFFERS   128     /* Number of TX ring buffer entries and buffers */
+#define CVMX_MGMT_PORT_NUM_RX_BUFFERS   128     /* Number of RX ring buffer entries and buffers */
+#define CVMX_MGMT_PORT_BUFFER_SIZE      1536    /* Size of each TX/RX buffer */
+
+/**
+ * Format of the TX/RX ring buffer entries
+ */
+typedef union
+{
+    uint64_t u64;
+    struct
+    {
+        uint64_t    reserved_62_63  : 2;
+        uint64_t    len             : 14;   /* Length of the buffer/packet in bytes */
+        uint64_t    code            : 8;    /* The RX error code */
+        uint64_t    addr            : 40;   /* Physical address of the buffer */
+    } s;
+} cvmx_mgmt_port_ring_entry_t;
+
+/**
+ * Per port state required for each mgmt port
+ */
+typedef struct
+{
+    cvmx_spinlock_t             lock;           /* Used for exclusive access to this structure */
+    int                         tx_write_index; /* Where the next TX will write in the tx_ring and tx_buffers */
+    int                         rx_read_index;  /* Where the next RX will be in the rx_ring and rx_buffers */
+    int                         phy_id;         /* The SMI/MDIO PHY address */
+    uint64_t                    mac;            /* Our MAC address */
+    cvmx_mgmt_port_ring_entry_t tx_ring[CVMX_MGMT_PORT_NUM_TX_BUFFERS];
+    cvmx_mgmt_port_ring_entry_t rx_ring[CVMX_MGMT_PORT_NUM_RX_BUFFERS];
+    char                        tx_buffers[CVMX_MGMT_PORT_NUM_TX_BUFFERS][CVMX_MGMT_PORT_BUFFER_SIZE];
+    char                        rx_buffers[CVMX_MGMT_PORT_NUM_RX_BUFFERS][CVMX_MGMT_PORT_BUFFER_SIZE];
+} cvmx_mgmt_port_state_t;
+
+/**
+ * Pointers to each mgmt port's state
+ */
+CVMX_SHARED cvmx_mgmt_port_state_t *cvmx_mgmt_port_state_ptr = NULL;
+
+
+/**
+ * Return the number of management ports supported by this chip
+ *
+ * @return Number of ports
+ */
+int __cvmx_mgmt_port_num_ports(void)
+{
+    if (OCTEON_IS_MODEL(OCTEON_CN56XX))
+        return 1;
+    else if (OCTEON_IS_MODEL(OCTEON_CN52XX))
+        return 2;
+    else
+        return 0;
+}
+
+
+/**
+ * Called to initialize a management port for use. Multiple calls
+ * to this function accross applications is safe.
+ *
+ * @param port   Port to initialize
+ *
+ * @return CVMX_MGMT_PORT_SUCCESS or an error code
+ */
+cvmx_mgmt_port_result_t cvmx_mgmt_port_initialize(int port)
+{
+    char *alloc_name = "cvmx_mgmt_port";
+
+    if ((port < 0) || (port >= __cvmx_mgmt_port_num_ports()))
+        return CVMX_MGMT_PORT_INVALID_PARAM;
+
+    cvmx_mgmt_port_state_ptr = cvmx_bootmem_alloc_named(CVMX_MGMT_PORT_NUM_PORTS * sizeof(cvmx_mgmt_port_state_t), 128, alloc_name);
+    if (cvmx_mgmt_port_state_ptr)
+    {
+        memset(cvmx_mgmt_port_state_ptr, 0, CVMX_MGMT_PORT_NUM_PORTS * sizeof(cvmx_mgmt_port_state_t));
+    }
+    else
+    {
+        cvmx_bootmem_named_block_desc_t *block_desc = cvmx_bootmem_find_named_block(alloc_name);
+        if (block_desc)
+            cvmx_mgmt_port_state_ptr = cvmx_phys_to_ptr(block_desc->base_addr);
+        else
+        {
+            cvmx_dprintf("ERROR: cvmx_mgmt_port_initialize: Unable to get named block %s.\n", alloc_name);
+            return CVMX_MGMT_PORT_NO_MEMORY;
+        }
+    }
+
+    if (cvmx_mgmt_port_state_ptr[port].tx_ring[0].u64 == 0)
+    {
+        cvmx_mgmt_port_state_t *state = cvmx_mgmt_port_state_ptr + port;
+        int i;
+        cvmx_mixx_bist_t mix_bist;
+        cvmx_agl_gmx_bist_t agl_gmx_bist;
+        cvmx_mixx_oring1_t oring1;
+        cvmx_mixx_iring1_t iring1;
+        cvmx_mixx_ctl_t mix_ctl;
+
+        /* Make sure BIST passed */
+        mix_bist.u64 = cvmx_read_csr(CVMX_MIXX_BIST(port));
+        if (mix_bist.u64)
+            cvmx_dprintf("WARNING: cvmx_mgmt_port_initialize: Managment port MIX failed BIST (0x%016llx)\n", CAST64(mix_bist.u64));
+
+        agl_gmx_bist.u64 = cvmx_read_csr(CVMX_AGL_GMX_BIST);
+        if (agl_gmx_bist.u64)
+            cvmx_dprintf("WARNING: cvmx_mgmt_port_initialize: Managment port AGL failed BIST (0x%016llx)\n", CAST64(agl_gmx_bist.u64));
+
+        /* Clear all state information */
+        memset(state, 0, sizeof(*state));
+
+        /* Take the control logic out of reset */
+        mix_ctl.u64 = cvmx_read_csr(CVMX_MIXX_CTL(port));
+        mix_ctl.s.reset = 0;
+        cvmx_write_csr(CVMX_MIXX_CTL(port), mix_ctl.u64);
+
+        /* Set the PHY address */
+        if (cvmx_sysinfo_get()->board_type == CVMX_BOARD_TYPE_SIM)
+            state->phy_id = -1;
+        else
+            state->phy_id = port;  /* Will need to be change to match the board */
+
+        /* Create a default MAC address */
+        state->mac = 0x000000dead000000ull;
+        state->mac += 0xffffff & CAST64(state);
+
+        /* Setup the TX ring */
+        for (i=0; i<CVMX_MGMT_PORT_NUM_TX_BUFFERS; i++)
+        {
+            state->tx_ring[i].s.len = CVMX_MGMT_PORT_BUFFER_SIZE;
+            state->tx_ring[i].s.addr = cvmx_ptr_to_phys(state->tx_buffers[i]);
+        }
+
+        /* Tell the HW where the TX ring is */
+        oring1.u64 = 0;
+        oring1.s.obase = cvmx_ptr_to_phys(state->tx_ring)>>3;
+        oring1.s.osize = CVMX_MGMT_PORT_NUM_TX_BUFFERS;
+        CVMX_SYNCWS;
+        cvmx_write_csr(CVMX_MIXX_ORING1(port), oring1.u64);
+
+        /* Setup the RX ring */
+        for (i=0; i<CVMX_MGMT_PORT_NUM_RX_BUFFERS; i++)
+        {
+            state->rx_ring[i].s.len = CVMX_MGMT_PORT_BUFFER_SIZE;
+            state->rx_ring[i].s.addr = cvmx_ptr_to_phys(state->rx_buffers[i]);
+        }
+
+        /* Tell the HW where the RX ring is */
+        iring1.u64 = 0;
+        iring1.s.ibase = cvmx_ptr_to_phys(state->rx_ring)>>3;
+        iring1.s.isize = CVMX_MGMT_PORT_NUM_RX_BUFFERS;
+        CVMX_SYNCWS;
+        cvmx_write_csr(CVMX_MIXX_IRING1(port), iring1.u64);
+        cvmx_write_csr(CVMX_MIXX_IRING2(port), CVMX_MGMT_PORT_NUM_RX_BUFFERS);
+
+        /* Disable the external input/output */
+        cvmx_mgmt_port_disable(port);
+
+        /* Set the MAC address filtering up */
+        cvmx_mgmt_port_set_mac(port, state->mac);
+
+        /* Enable the port HW. Packets are not allowed until cvmx_mgmt_port_enable() is called */
+        mix_ctl.u64 = 0;
+        mix_ctl.s.crc_strip = 1;    /* Strip the ending CRC */
+        mix_ctl.s.en = 1;           /* Enable the port */
+        mix_ctl.s.nbtarb = 0;       /* Arbitration mode */
+        mix_ctl.s.mrq_hwm = 1;      /* MII CB-request FIFO programmable high watermark */
+        cvmx_write_csr(CVMX_MIXX_CTL(port), mix_ctl.u64);
+
+        if (OCTEON_IS_MODEL(OCTEON_CN56XX_PASS1) || OCTEON_IS_MODEL(OCTEON_CN52XX_PASS1))
+        {
+            /* Force compensation values, as they are not determined properly by HW */
+            cvmx_agl_gmx_drv_ctl_t drv_ctl;
+
+            drv_ctl.u64 = cvmx_read_csr(CVMX_AGL_GMX_DRV_CTL);
+            if (port)
+            {
+                drv_ctl.s.byp_en1 = 1;
+                drv_ctl.s.nctl1 = 6;
+                drv_ctl.s.pctl1 = 6;
+            }
+            else
+            {
+                drv_ctl.s.byp_en = 1;
+                drv_ctl.s.nctl = 6;
+                drv_ctl.s.pctl = 6;
+            }
+            cvmx_write_csr(CVMX_AGL_GMX_DRV_CTL, drv_ctl.u64);
+        }
+    }
+    return CVMX_MGMT_PORT_SUCCESS;
+}
+
+
+/**
+ * Shutdown a management port. This currently disables packet IO
+ * but leaves all hardware and buffers. Another application can then
+ * call initialize() without redoing the hardware setup.
+ *
+ * @param port   Management port
+ *
+ * @return CVMX_MGMT_PORT_SUCCESS or an error code
+ */
+cvmx_mgmt_port_result_t cvmx_mgmt_port_shutdown(int port)
+{
+    if ((port < 0) || (port >= __cvmx_mgmt_port_num_ports()))
+        return CVMX_MGMT_PORT_INVALID_PARAM;
+
+    /* Stop packets from comming in */
+    cvmx_mgmt_port_disable(port);
+
+    /* We don't free any memory so the next intialize can reuse the HW setup */
+    return CVMX_MGMT_PORT_SUCCESS;
+}
+
+
+/**
+ * Enable packet IO on a management port
+ *
+ * @param port   Management port
+ *
+ * @return CVMX_MGMT_PORT_SUCCESS or an error code
+ */
+cvmx_mgmt_port_result_t cvmx_mgmt_port_enable(int port)
+{
+    cvmx_mgmt_port_state_t *state;
+    cvmx_agl_gmx_prtx_cfg_t agl_gmx_prtx;
+    cvmx_agl_gmx_inf_mode_t agl_gmx_inf_mode;
+    cvmx_agl_gmx_rxx_frm_ctl_t rxx_frm_ctl;
+
+    if ((port < 0) || (port >= __cvmx_mgmt_port_num_ports()))
+        return CVMX_MGMT_PORT_INVALID_PARAM;
+
+    state = cvmx_mgmt_port_state_ptr + port;
+
+    cvmx_spinlock_lock(&state->lock);
+
+    rxx_frm_ctl.u64 = 0;
+    rxx_frm_ctl.s.pre_align = 1;
+    rxx_frm_ctl.s.pad_len = 1;  /* When set, disables the length check for non-min sized pkts with padding in the client data */
+    rxx_frm_ctl.s.vlan_len = 1; /* When set, disables the length check for VLAN pkts */
+    rxx_frm_ctl.s.pre_free = 1; /* When set, PREAMBLE checking is  less strict */
+    rxx_frm_ctl.s.ctl_smac = 0; /* Control Pause Frames can match station SMAC */
+    rxx_frm_ctl.s.ctl_mcst = 1; /* Control Pause Frames can match globally assign Multicast address */
+    rxx_frm_ctl.s.ctl_bck = 1;  /* Forward pause information to TX block */
+    rxx_frm_ctl.s.ctl_drp = 1;  /* Drop Control Pause Frames */
+    rxx_frm_ctl.s.pre_strp = 1; /* Strip off the preamble */
+    rxx_frm_ctl.s.pre_chk = 1;  /* This port is configured to send PREAMBLE+SFD to begin every frame.  GMX checks that the PREAMBLE is sent correctly */
+    cvmx_write_csr(CVMX_AGL_GMX_RXX_FRM_CTL(port), rxx_frm_ctl.u64);
+
+    /* Don't allow packets that span buffers */
+    cvmx_write_csr(CVMX_AGL_GMX_RXX_FRM_MAX(port), CVMX_MGMT_PORT_BUFFER_SIZE);
+    cvmx_write_csr(CVMX_AGL_GMX_RXX_JABBER(port), CVMX_MGMT_PORT_BUFFER_SIZE);
+
+    /* Enable the AGL block */
+    agl_gmx_inf_mode.u64 = 0;
+    agl_gmx_inf_mode.s.en = 1;
+    cvmx_write_csr(CVMX_AGL_GMX_INF_MODE, agl_gmx_inf_mode.u64);
+
+    /* Configure the port duplex and enables */
+    agl_gmx_prtx.u64 = cvmx_read_csr(CVMX_AGL_GMX_PRTX_CFG(port));
+    agl_gmx_prtx.s.tx_en = 1;
+    agl_gmx_prtx.s.rx_en = 1;
+    if (cvmx_mgmt_port_get_link(port) < 0)
+        agl_gmx_prtx.s.duplex = 0;
+    else
+        agl_gmx_prtx.s.duplex = 1;
+    agl_gmx_prtx.s.en = 1;
+    cvmx_write_csr(CVMX_AGL_GMX_PRTX_CFG(port), agl_gmx_prtx.u64);
+
+    cvmx_spinlock_unlock(&state->lock);
+    return CVMX_MGMT_PORT_SUCCESS;
+}
+
+
+/**
+ * Disable packet IO on a management port
+ *
+ * @param port   Management port
+ *
+ * @return CVMX_MGMT_PORT_SUCCESS or an error code
+ */
+cvmx_mgmt_port_result_t cvmx_mgmt_port_disable(int port)
+{
+    cvmx_mgmt_port_state_t *state;
+    cvmx_agl_gmx_prtx_cfg_t agl_gmx_prtx;
+
+    if ((port < 0) || (port >= __cvmx_mgmt_port_num_ports()))
+        return CVMX_MGMT_PORT_INVALID_PARAM;
+
+    state = cvmx_mgmt_port_state_ptr + port;
+
+    cvmx_spinlock_lock(&state->lock);
+
+    agl_gmx_prtx.u64 = cvmx_read_csr(CVMX_AGL_GMX_PRTX_CFG(port));
+    agl_gmx_prtx.s.en = 0;
+    cvmx_write_csr(CVMX_AGL_GMX_PRTX_CFG(port), agl_gmx_prtx.u64);
+
+    cvmx_spinlock_unlock(&state->lock);
+    return CVMX_MGMT_PORT_SUCCESS;
+}
+
+
+/**
+ * Send a packet out the management port. The packet is copied so
+ * the input buffer isn't used after this call.
+ *
+ * @param port       Management port
+ * @param packet_len Length of the packet to send. It does not include the final CRC
+ * @param buffer     Packet data
+ *
+ * @return CVMX_MGMT_PORT_SUCCESS or an error code
+ */
+cvmx_mgmt_port_result_t cvmx_mgmt_port_send(int port, int packet_len, void *buffer)
+{
+    cvmx_mgmt_port_state_t *state;
+    cvmx_mixx_oring2_t mix_oring2;
+
+    if ((port < 0) || (port >= __cvmx_mgmt_port_num_ports()))
+        return CVMX_MGMT_PORT_INVALID_PARAM;
+
+    /* Max sure the packet size is valid */
+    if ((packet_len < 1) || (packet_len > CVMX_MGMT_PORT_BUFFER_SIZE))
+        return CVMX_MGMT_PORT_INVALID_PARAM;
+
+    if (buffer == NULL)
+        return CVMX_MGMT_PORT_INVALID_PARAM;
+
+    state = cvmx_mgmt_port_state_ptr + port;
+
+    cvmx_spinlock_lock(&state->lock);
+
+    mix_oring2.u64 = cvmx_read_csr(CVMX_MIXX_ORING2(port));
+    if (mix_oring2.s.odbell >= CVMX_MGMT_PORT_NUM_TX_BUFFERS - 1)
+    {
+        /* No room for another packet */
+        cvmx_spinlock_unlock(&state->lock);
+        return CVMX_MGMT_PORT_NO_MEMORY;
+    }
+    else
+    {
+        /* Copy the packet into the output buffer */
+        memcpy(state->tx_buffers[state->tx_write_index], buffer, packet_len);
+        /* Insert the source MAC */
+        memcpy(state->tx_buffers[state->tx_write_index] + 6, ((char*)&state->mac) + 2, 6);
+        /* Update the TX ring buffer entry size */
+        state->tx_ring[state->tx_write_index].s.len = packet_len;
+        /* Increment our TX index */
+        state->tx_write_index = (state->tx_write_index + 1) % CVMX_MGMT_PORT_NUM_TX_BUFFERS;
+        /* Ring the doorbell, send ing the packet */
+        CVMX_SYNCWS;
+        cvmx_write_csr(CVMX_MIXX_ORING2(port), 1);
+        if (cvmx_read_csr(CVMX_MIXX_ORCNT(port)))
+            cvmx_write_csr(CVMX_MIXX_ORCNT(port), cvmx_read_csr(CVMX_MIXX_ORCNT(port)));
+
+        cvmx_spinlock_unlock(&state->lock);
+        return CVMX_MGMT_PORT_SUCCESS;
+    }
+}
+
+
+/**
+ * Receive a packet from the management port.
+ *
+ * @param port       Management port
+ * @param buffer_len Size of the buffer to receive the packet into
+ * @param buffer     Buffer to receive the packet into
+ *
+ * @return The size of the packet, or a negative erorr code on failure. Zero
+ *         means that no packets were available.
+ */
+int cvmx_mgmt_port_receive(int port, int buffer_len, void *buffer)
+{
+    cvmx_mixx_ircnt_t mix_ircnt;
+    cvmx_mgmt_port_state_t *state;
+    int result;
+
+    if ((port < 0) || (port >= __cvmx_mgmt_port_num_ports()))
+        return CVMX_MGMT_PORT_INVALID_PARAM;
+
+    /* Max sure the buffer size is valid */
+    if (buffer_len < 1)
+        return CVMX_MGMT_PORT_INVALID_PARAM;
+
+    if (buffer == NULL)
+        return CVMX_MGMT_PORT_INVALID_PARAM;
+
+    state = cvmx_mgmt_port_state_ptr + port;
+
+    cvmx_spinlock_lock(&state->lock);
+
+    /* Find out how many RX packets are pending */
+    mix_ircnt.u64 = cvmx_read_csr(CVMX_MIXX_IRCNT(port));
+    if (mix_ircnt.s.ircnt)
+    {
+        /* Based on the completion code figure out what happened */
+        switch (state->rx_ring[state->rx_read_index].s.code)
+        {
+            case 15: /* Done - Everything is good */
+                if (buffer_len >= state->rx_ring[state->rx_read_index].s.len)
+                {
+                    /* Some packets seem to be copied at offset 8 in the buffer.  Here we check to see if the first 8 bytes
+                    ** are zero, and if so, skip those bytes.  We clear these bytes so that this check is valid the next time
+                    ** around. */
+                    if (*((uint64_t *)(state->rx_buffers[state->rx_read_index])))
+                        memcpy(buffer, state->rx_buffers[state->rx_read_index], state->rx_ring[state->rx_read_index].s.len);
+                    else
+                        memcpy(buffer, state->rx_buffers[state->rx_read_index] + 8, state->rx_ring[state->rx_read_index].s.len);
+
+                    memset(state->rx_buffers[state->rx_read_index], 0, 8);
+                    result = state->rx_ring[state->rx_read_index].s.len;
+                }
+                else
+                {
+                    /* Not enough room for the packet */
+                    cvmx_dprintf("ERROR: cvmx_mgmt_port_receive: Packet (%d) larger than supplied buffer (%d)\n", state->rx_ring[state->rx_read_index].s.len, buffer_len);
+                    result = CVMX_MGMT_PORT_NO_MEMORY;
+                }
+                break;
+            default:
+                cvmx_dprintf("ERROR: cvmx_mgmt_port_receive: Receive error code %d. Packet dropped\n", state->rx_ring[state->rx_read_index].s.code);
+                result = -state->rx_ring[state->rx_read_index].s.code;
+                break;
+        }
+        /* Clean out the ring buffer entry */
+        state->rx_ring[state->rx_read_index].s.code = 0;
+        state->rx_ring[state->rx_read_index].s.len = CVMX_MGMT_PORT_BUFFER_SIZE;
+        state->rx_read_index = (state->rx_read_index + 1) % CVMX_MGMT_PORT_NUM_RX_BUFFERS;
+        /* Decrement the pending RX count */
+        CVMX_SYNCWS;
+        cvmx_write_csr(CVMX_MIXX_IRCNT(port), 1);
+        /* Increment the number of RX buffers */
+        cvmx_write_csr(CVMX_MIXX_IRING2(port), 1);
+    }
+    else
+    {
+        /* No packets available */
+        result = 0;
+    }
+    cvmx_spinlock_unlock(&state->lock);
+    return result;
+}
+
+
+/**
+ * Get the management port link status:
+ * 100 = 100Mbps, full duplex
+ * 10 = 10Mbps, full duplex
+ * 0 = Link down
+ * -10 = 10Mpbs, half duplex
+ * -100 = 100Mbps, half duplex
+ *
+ * @param port   Management port
+ *
+ * @return
+ */
+int cvmx_mgmt_port_get_link(int port)
+{
+    cvmx_mgmt_port_state_t *state;
+    int phy_status;
+    int duplex;
+
+    if ((port < 0) || (port >= __cvmx_mgmt_port_num_ports()))
+        return CVMX_MGMT_PORT_INVALID_PARAM;
+
+    state = cvmx_mgmt_port_state_ptr + port;
+
+    /* Assume 100Mbps if we don't know the PHY address */
+    if (state->phy_id == -1)
+        return 100;
+
+    /* Read the PHY state */
+    phy_status = cvmx_mdio_read(state->phy_id >> 8, state->phy_id & 0xff, 17);
+
+    /* Only return a link if the PHY has finished auto negotiation
+        and set the resolved bit (bit 11) */
+    if (!(phy_status & (1<<11)))
+        return 0;
+
+    /* Create multiple factor to represent duplex */
+    if ((phy_status>>13)&1)
+        duplex = 1;
+    else
+        duplex = -1;
+
+    /* Speed is encoded on bits 15-14 */
+    switch ((phy_status>>14)&3)
+    {
+        case 0: /* 10 Mbps */
+            return 10 * duplex;
+        case 1: /* 100 Mbps */
+            return 100 * duplex;
+        default:
+            return 0;
+    }
+}
+
+
+/**
+ * Set the MAC address for a management port
+ *
+ * @param port   Management port
+ * @param mac    New MAC address. The lower 6 bytes are used.
+ *
+ * @return CVMX_MGMT_PORT_SUCCESS or an error code
+ */
+cvmx_mgmt_port_result_t cvmx_mgmt_port_set_mac(int port, uint64_t mac)
+{
+    cvmx_mgmt_port_state_t *state;
+    cvmx_agl_gmx_rxx_adr_ctl_t agl_gmx_rxx_adr_ctl;
+
+    if ((port < 0) || (port >= __cvmx_mgmt_port_num_ports()))
+        return CVMX_MGMT_PORT_INVALID_PARAM;
+
+    state = cvmx_mgmt_port_state_ptr + port;
+
+    cvmx_spinlock_lock(&state->lock);
+
+    agl_gmx_rxx_adr_ctl.u64 = 0;
+    agl_gmx_rxx_adr_ctl.s.cam_mode = 1; /* Only accept matching MAC addresses */
+    agl_gmx_rxx_adr_ctl.s.mcst = 0;     /* Drop multicast */
+    agl_gmx_rxx_adr_ctl.s.bcst = 1;     /* Allow broadcast */
+    cvmx_write_csr(CVMX_AGL_GMX_RXX_ADR_CTL(port), agl_gmx_rxx_adr_ctl.u64);
+
+    /* Only using one of the CAMs */
+    cvmx_write_csr(CVMX_AGL_GMX_RXX_ADR_CAM0(port), (mac >> 40) & 0xff);
+    cvmx_write_csr(CVMX_AGL_GMX_RXX_ADR_CAM1(port), (mac >> 32) & 0xff);
+    cvmx_write_csr(CVMX_AGL_GMX_RXX_ADR_CAM2(port), (mac >> 24) & 0xff);
+    cvmx_write_csr(CVMX_AGL_GMX_RXX_ADR_CAM3(port), (mac >> 16) & 0xff);
+    cvmx_write_csr(CVMX_AGL_GMX_RXX_ADR_CAM4(port), (mac >> 8) & 0xff);
+    cvmx_write_csr(CVMX_AGL_GMX_RXX_ADR_CAM5(port), (mac >> 0) & 0xff);
+    cvmx_write_csr(CVMX_AGL_GMX_RXX_ADR_CAM_EN(port), 1);
+    state->mac = mac;
+
+    cvmx_spinlock_unlock(&state->lock);
+    return CVMX_MGMT_PORT_SUCCESS;
+}
+
+
+/**
+ * Get the MAC address for a management port
+ *
+ * @param port   Management port
+ *
+ * @return MAC address
+ */
+uint64_t cvmx_mgmt_port_get_mac(int port)
+{
+    if ((port < 0) || (port >= __cvmx_mgmt_port_num_ports()))
+        return CVMX_MGMT_PORT_INVALID_PARAM;
+
+    return cvmx_mgmt_port_state_ptr[port].mac;
+}
+
+/**
+ * Set the multicast list.
+ *
+ * @param port   Management port
+ * @param flags  Interface flags
+ *
+ * @return
+ */
+void cvmx_mgmt_port_set_multicast_list(int port, int flags)
+{
+    cvmx_mgmt_port_state_t *state;
+    cvmx_agl_gmx_rxx_adr_ctl_t agl_gmx_rxx_adr_ctl;
+
+    if ((port < 0) || (port >= __cvmx_mgmt_port_num_ports()))
+        return;
+
+    state = cvmx_mgmt_port_state_ptr + port;
+
+    cvmx_spinlock_lock(&state->lock);
+
+    agl_gmx_rxx_adr_ctl.u64 = cvmx_read_csr(CVMX_AGL_GMX_RXX_ADR_CTL(port));
+    
+    /* Allow broadcast MAC addresses */
+    if (!agl_gmx_rxx_adr_ctl.s.bcst)
+	agl_gmx_rxx_adr_ctl.s.bcst = 1;
+
+    if ((flags & CVMX_IFF_ALLMULTI) || (flags & CVMX_IFF_PROMISC))
+	agl_gmx_rxx_adr_ctl.s.mcst = 2; /* Force accept multicast packets */
+    else
+	agl_gmx_rxx_adr_ctl.s.mcst = 1; /* Force reject multicast packets */
+
+    if (flags & CVMX_IFF_PROMISC)
+	agl_gmx_rxx_adr_ctl.s.cam_mode = 0; /* Reject matches if promisc. Since CAM is shut off, should accept everything */
+    else
+	agl_gmx_rxx_adr_ctl.s.cam_mode = 1; /* Filter packets based on the CAM */
+
+    cvmx_write_csr(CVMX_AGL_GMX_RXX_ADR_CTL(port), agl_gmx_rxx_adr_ctl.u64);
+
+    if (flags & CVMX_IFF_PROMISC)
+	cvmx_write_csr(CVMX_AGL_GMX_RXX_ADR_CAM_EN(port), 0);
+    else
+	cvmx_write_csr(CVMX_AGL_GMX_RXX_ADR_CAM_EN(port), 1);
+    
+    cvmx_spinlock_unlock(&state->lock);
+}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-mgmt-port.h b/arch/mips/cavium-octeon/executive/cvmx-mgmt-port.h
new file mode 100644
index 0000000..3e9d3d7
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-mgmt-port.h
@@ -0,0 +1,183 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Support functions for managing the MII management port
+ *
+ * <hr>$Revision: 34852 $<hr>
+ */
+
+#ifndef __CVMX_MGMT_PORT_H__
+#define __CVMX_MGMT_PORT_H__
+
+typedef enum
+{
+    CVMX_MGMT_PORT_SUCCESS = 0,
+    CVMX_MGMT_PORT_NO_MEMORY = -1,
+    CVMX_MGMT_PORT_INVALID_PARAM = -2,
+} cvmx_mgmt_port_result_t;
+
+
+/* Enumeration of Net Device interface flags. */
+typedef enum 
+{
+    CVMX_IFF_PROMISC = 0x100, 		/* receive all packets           */
+    CVMX_IFF_ALLMULTI = 0x200, 		/* receive all multicast packets */
+} cvmx_mgmt_port_netdevice_flags_t;
+
+/**
+ * Called to initialize a management port for use. Multiple calls
+ * to this function accross applications is safe.
+ *
+ * @param port   Port to initialize
+ *
+ * @return CVMX_MGMT_PORT_SUCCESS or an error code
+ */
+extern cvmx_mgmt_port_result_t cvmx_mgmt_port_initialize(int port);
+
+/**
+ * Shutdown a management port. This currently disables packet IO
+ * but leaves all hardware and buffers. Another application can then
+ * call initialize() without redoing the hardware setup.
+ *
+ * @param port   Management port
+ *
+ * @return CVMX_MGMT_PORT_SUCCESS or an error code
+ */
+extern cvmx_mgmt_port_result_t cvmx_mgmt_port_shutdown(int port);
+
+/**
+ * Enable packet IO on a management port
+ *
+ * @param port   Management port
+ *
+ * @return CVMX_MGMT_PORT_SUCCESS or an error code
+ */
+extern cvmx_mgmt_port_result_t cvmx_mgmt_port_enable(int port);
+
+/**
+ * Disable packet IO on a management port
+ *
+ * @param port   Management port
+ *
+ * @return CVMX_MGMT_PORT_SUCCESS or an error code
+ */
+extern cvmx_mgmt_port_result_t cvmx_mgmt_port_disable(int port);
+
+/**
+ * Send a packet out the management port. The packet is copied so
+ * the input buffer isn't used after this call.
+ *
+ * @param port       Management port
+ * @param packet_len Length of the packet to send. It does not include the final CRC
+ * @param buffer     Packet data
+ *
+ * @return CVMX_MGMT_PORT_SUCCESS or an error code
+ */
+extern cvmx_mgmt_port_result_t cvmx_mgmt_port_send(int port, int packet_len, void *buffer);
+
+/**
+ * Receive a packet from the management port.
+ *
+ * @param port       Management port
+ * @param buffer_len Size of the buffer to receive the packet into
+ * @param buffer     Buffer to receive the packet into
+ *
+ * @return The size of the packet, or a negative erorr code on failure. Zero
+ *         means that no packets were available.
+ */
+extern int cvmx_mgmt_port_receive(int port, int buffer_len, void *buffer);
+
+/**
+ * Get the management port link status:
+ * 100 = 100Mbps, full duplex
+ * 10 = 10Mbps, full duplex
+ * 0 = Link down
+ * -10 = 10Mpbs, half duplex
+ * -100 = 100Mbps, half duplex
+ *
+ * @param port   Management port
+ *
+ * @return
+ */
+extern int cvmx_mgmt_port_get_link(int port);
+
+/**
+ * Set the MAC address for a management port
+ *
+ * @param port   Management port
+ * @param mac    New MAC address. The lower 6 bytes are used.
+ *
+ * @return CVMX_MGMT_PORT_SUCCESS or an error code
+ */
+extern cvmx_mgmt_port_result_t cvmx_mgmt_port_set_mac(int port, uint64_t mac);
+
+/**
+ * Get the MAC address for a management port
+ *
+ * @param port   Management port
+ *
+ * @return MAC address
+ */
+extern uint64_t cvmx_mgmt_port_get_mac(int port);
+
+/**
+ * Set the multicast list.
+ *
+ * @param port   Management port
+ * @param flags  Interface flags
+ *
+ * @return
+ */
+extern void cvmx_mgmt_port_set_multicast_list(int port, int flags);
+#endif /* __CVMX_MGMT_PORT_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-mio.h b/arch/mips/cavium-octeon/executive/cvmx-mio.h
new file mode 100644
index 0000000..d0af824
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-mio.h
@@ -0,0 +1,74 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Interface to the MIO hardware.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+
+#ifndef __CVMX_MIO_H__
+#define __CVMX_MIO_H__
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+/* CSR typedefs have been moved to cvmx-csr-*.h */
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-npi.h b/arch/mips/cavium-octeon/executive/cvmx-npi.h
new file mode 100644
index 0000000..986e40d
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-npi.h
@@ -0,0 +1,73 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * NPI related structures.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+
+#ifndef __CVMX_NPI_H__
+#define __CVMX_NPI_H__
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+/* CSR typedefs have been moved to cvmx-csr-*.h */
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif  /* __CVMX_NPI_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-packet.h b/arch/mips/cavium-octeon/executive/cvmx-packet.h
new file mode 100644
index 0000000..72f62e9
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-packet.h
@@ -0,0 +1,92 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Packet buffer defines.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ *
+ *
+ */
+
+#ifndef __CVMX_PACKET_H__
+#define __CVMX_PACKET_H__
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+/**
+ * This structure defines a buffer pointer on Octeon
+ */
+typedef union
+{
+    void*           ptr;
+    uint64_t        u64;
+    struct
+    {
+        uint64_t    i    : 1; /**< if set, invert the "free" pick of the overall packet. HW always sets this bit to 0 on inbound packet */
+        uint64_t    back : 4; /**< Indicates the amount to back up to get to the buffer start in cache lines. In most cases
+                                this is less than one complete cache line, so the value is zero */
+        uint64_t    pool : 3; /**< The pool that the buffer came from / goes to */
+        uint64_t    size :16; /**< The size of the segment pointed to by addr (in bytes) */
+        uint64_t    addr :40; /**< Pointer to the first byte of the data, NOT buffer */
+    } s;
+} cvmx_buf_ptr_t;
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif /*  __CVMX_PACKET_H__ */
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pci.h b/arch/mips/cavium-octeon/executive/cvmx-pci.h
new file mode 100644
index 0000000..3e67d01
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-pci.h
@@ -0,0 +1,76 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * PCI related structures.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+
+#ifndef __CVMX_PCI_H__
+#define __CVMX_PCI_H__
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+/* The typedefs and enumerations for Octeon's PCI packet engines have been
+    removed from this file. The definitions in this file were out of date
+    and unused. For current definitions, refer to the Octeon PCI NIC
+    driver. OCTEON-PCI-NIC-*.rpm */
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif  /* __CVMX_PCI_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pcie.c b/arch/mips/cavium-octeon/executive/cvmx-pcie.c
new file mode 100644
index 0000000..e2c5666
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-pcie.c
@@ -0,0 +1,1028 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Interface to PCIe as a host(RC) or target(EP)
+ *
+ * <hr>$Revision: 35733 $<hr>
+ */
+#include "cvmx.h"
+#include "cvmx-csr-db.h"
+#include "cvmx-pcie.h"
+#include "cvmx-sysinfo.h"
+#include "cvmx-swap.h"
+
+
+/**
+ * Return the Core virtual base address for PCIe IO access. IOs are
+ * read/written as an offset from this address.
+ *
+ * @param pcie_port PCIe port the IO is for
+ *
+ * @return 64bit Octeon IO base address for read/write
+ */
+uint64_t cvmx_pcie_get_io_base_address(int pcie_port)
+{
+    cvmx_pcie_address_t pcie_addr;
+    pcie_addr.u64 = 0;
+    pcie_addr.io.upper = 0;
+    pcie_addr.io.io = 1;
+    pcie_addr.io.did = 3;
+    pcie_addr.io.subdid = 2;
+    pcie_addr.io.es = 1;
+    pcie_addr.io.port = pcie_port;
+    return pcie_addr.u64;
+}
+
+
+/**
+ * Size of the IO address region returned at address
+ * cvmx_pcie_get_io_base_address()
+ *
+ * @param pcie_port PCIe port the IO is for
+ *
+ * @return Size of the IO window
+ */
+uint64_t cvmx_pcie_get_io_size(int pcie_port)
+{
+    return 1ull<<32;
+}
+
+
+/**
+ * Return the Core virtual base address for PCIe MEM access. Memory is
+ * read/written as an offset from this address.
+ *
+ * @param pcie_port PCIe port the IO is for
+ *
+ * @return 64bit Octeon IO base address for read/write
+ */
+uint64_t cvmx_pcie_get_mem_base_address(int pcie_port)
+{
+    cvmx_pcie_address_t pcie_addr;
+    pcie_addr.u64 = 0;
+    pcie_addr.mem.upper = 0;
+    pcie_addr.mem.io = 1;
+    pcie_addr.mem.did = 3;
+    pcie_addr.mem.subdid = 3 + pcie_port;
+    return pcie_addr.u64;
+}
+
+
+/**
+ * Size of the Mem address region returned at address
+ * cvmx_pcie_get_mem_base_address()
+ *
+ * @param pcie_port PCIe port the IO is for
+ *
+ * @return Size of the Mem window
+ */
+uint64_t cvmx_pcie_get_mem_size(int pcie_port)
+{
+    return 1ull<<36;
+}
+
+
+/**
+ * @INTERNAL
+ * Initialize the RC config space CSRs
+ *
+ * @param pcie_port PCIe port to initialize
+ */
+static void __cvmx_pcie_rc_initialize_config_space(int pcie_port)
+{
+    /* Max Payload Size (PCIE*_CFG030[MPS]) */
+    /* Max Read Request Size (PCIE*_CFG030[MRRS]) */
+    /* Relaxed-order, no-snoop enables (PCIE*_CFG030[RO_EN,NS_EN] */
+    /* Error Message Enables (PCIE*_CFG030[CE_EN,NFE_EN,FE_EN,UR_EN]) */
+    {
+        cvmx_pciercx_cfg030_t pciercx_cfg030;
+        pciercx_cfg030.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG030(pcie_port));
+        pciercx_cfg030.s.mps = 1; /* Max payload size = 256 bytes */
+        pciercx_cfg030.s.mrrs = 5; /* Max read request size = 4096 bytes */
+        pciercx_cfg030.s.ro_en = 1; /* Enable relaxed ordering. */
+        pciercx_cfg030.s.ns_en = 1; /* Enable no snoop. */
+        pciercx_cfg030.s.ce_en = 1; /* Correctable error reporting enable. */
+        pciercx_cfg030.s.nfe_en = 1; /* Non-fatal error reporting enable. */
+        pciercx_cfg030.s.fe_en = 1; /* Fatal error reporting enable. */
+        pciercx_cfg030.s.ur_en = 1; /* Unsupported request reporting enable. */
+        cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG030(pcie_port), pciercx_cfg030.u32);
+    }
+
+    /* Max Payload Size (NPEI_CTL_STATUS2[MPS]) must match PCIE*_CFG030[MPS] */
+    /* Max Read Request Size (NPEI_CTL_STATUS2[MRRS]) must not exceed PCIE*_CFG030[MRRS] */
+    {
+        cvmx_npei_ctl_status2_t npei_ctl_status2;
+        npei_ctl_status2.u64 = cvmx_read_csr(CVMX_PEXP_NPEI_CTL_STATUS2);
+        npei_ctl_status2.s.mps = 1; /* Max payload size = 256 bytes */
+        npei_ctl_status2.s.mrrs = 5; /* Max read request size = 4096 bytes */
+        cvmx_write_csr(CVMX_PEXP_NPEI_CTL_STATUS2, npei_ctl_status2.u64);
+    }
+
+    /* ECRC Generation (PCIE*_CFG070[GE,CE]) */
+    {
+        cvmx_pciercx_cfg070_t pciercx_cfg070;
+        pciercx_cfg070.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG070(pcie_port));
+        pciercx_cfg070.s.ge = 1; /* ECRC generation enable. */
+        pciercx_cfg070.s.ce = 1; /* ECRC check enable. */
+        cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG070(pcie_port), pciercx_cfg070.u32);
+    }
+
+    /* Access Enables (PCIE*_CFG001[MSAE,ME]) */
+        /* ME and MSAE should always be set. */
+    /* Interrupt Disable (PCIE*_CFG001[I_DIS]) */
+    /* System Error Message Enable (PCIE*_CFG001[SEE]) */
+    {
+        cvmx_pciercx_cfg001_t pciercx_cfg001;
+        pciercx_cfg001.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG001(pcie_port));
+        pciercx_cfg001.s.msae = 1; /* Memory space enable. */
+        pciercx_cfg001.s.me = 1; /* Bus master enable. */
+        pciercx_cfg001.s.i_dis = 1; /* INTx assertion disable. */
+        pciercx_cfg001.s.see = 1; /* SERR# enable */
+        cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG001(pcie_port), pciercx_cfg001.u32);
+    }
+
+
+    /* Advanced Error Recovery Message Enables */
+    /* (PCIE*_CFG066,PCIE*_CFG067,PCIE*_CFG069) */
+    cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG066(pcie_port), 0);
+    /* Use CVMX_PCIERCX_CFG067 hardware default */
+    cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG069(pcie_port), 0);
+
+
+    /* Active State Power Management (PCIE*_CFG032[ASLPC]) */
+    {
+        cvmx_pciercx_cfg032_t pciercx_cfg032;
+        pciercx_cfg032.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG032(pcie_port));
+        pciercx_cfg032.s.aslpc = 0; /* Active state Link PM control. */
+        cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG032(pcie_port), pciercx_cfg032.u32);
+    }
+
+    /* Entrance Latencies (PCIE*_CFG451[L0EL,L1EL]) */
+    // FIXME: Anything needed here?
+
+    /* Link Width Mode (PCIERCn_CFG452[LME]) - Set during cvmx_pcie_rc_initialize_link() */
+    /* Primary Bus Number (PCIERCn_CFG006[PBNUM]) - Hardware default ok */
+
+    /* Memory-mapped I/O BAR (PCIERCn_CFG008) */
+    /* Most applications should disable the memory-mapped I/O BAR by */
+    /* setting PCIERCn_CFG008[ML_ADDR] < PCIERCn_CFG008[MB_ADDR] */
+    {
+        cvmx_pciercx_cfg008_t pciercx_cfg008;
+        pciercx_cfg008.u32 = 0;
+        pciercx_cfg008.s.mb_addr = 0x100;
+        pciercx_cfg008.s.ml_addr = 0;
+        cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG008(pcie_port), pciercx_cfg008.u32);
+    }
+
+    /* Prefetchable BAR (PCIERCn_CFG009,PCIERCn_CFG010,PCIERCn_CFG011) */
+    /* Most applications should disable the prefetchable BAR by setting */
+    /* PCIERCn_CFG011[UMEM_LIMIT],PCIERCn_CFG009[LMEM_LIMIT] < */
+    /* PCIERCn_CFG010[UMEM_BASE],PCIERCn_CFG009[LMEM_BASE] */
+    {
+        cvmx_pciercx_cfg009_t pciercx_cfg009;
+        cvmx_pciercx_cfg010_t pciercx_cfg010;
+        cvmx_pciercx_cfg011_t pciercx_cfg011;
+        pciercx_cfg009.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG009(pcie_port));
+        pciercx_cfg010.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG010(pcie_port));
+        pciercx_cfg011.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG011(pcie_port));
+        pciercx_cfg009.s.lmem_base = 0x100;
+        pciercx_cfg009.s.lmem_limit = 0;
+        pciercx_cfg010.s.umem_base = 0x100;
+        pciercx_cfg011.s.umem_limit = 0;
+        cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG009(pcie_port), pciercx_cfg009.u32);
+        cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG010(pcie_port), pciercx_cfg010.u32);
+        cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG011(pcie_port), pciercx_cfg011.u32);
+    }
+
+    /* System Error Interrupt Enables (PCIERCn_CFG035[SECEE,SEFEE,SENFEE]) */
+    /* PME Interrupt Enables (PCIERCn_CFG035[PMEIE]) */
+    {
+        cvmx_pciercx_cfg035_t pciercx_cfg035;
+        pciercx_cfg035.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG035(pcie_port));
+        pciercx_cfg035.s.secee = 1; /* System error on correctable error enable. */
+        pciercx_cfg035.s.sefee = 1; /* System error on fatal error enable. */
+        pciercx_cfg035.s.senfee = 1; /* System error on non-fatal error enable. */
+        pciercx_cfg035.s.pmeie = 1; /* PME interrupt enable. */
+        cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG035(pcie_port), pciercx_cfg035.u32);
+    }
+
+    /* Advanced Error Recovery Interrupt Enables */
+    /* (PCIERCn_CFG075[CERE,NFERE,FERE]) */
+    {
+        cvmx_pciercx_cfg075_t pciercx_cfg075;
+        pciercx_cfg075.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG075(pcie_port));
+        pciercx_cfg075.s.cere = 1; /* Correctable error reporting enable. */
+        pciercx_cfg075.s.nfere = 1; /* Non-fatal error reporting enable. */
+        pciercx_cfg075.s.fere = 1; /* Fatal error reporting enable. */
+        cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG075(pcie_port), pciercx_cfg075.u32);
+    }
+
+    /* HP Interrupt Enables (PCIERCn_CFG034[HPINT_EN], */
+    /* PCIERCn_CFG034[DLLS_EN,CCINT_EN]) */
+    {
+        cvmx_pciercx_cfg034_t pciercx_cfg034;
+        pciercx_cfg034.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG034(pcie_port));
+        pciercx_cfg034.s.hpint_en = 1; /* Hot-plug interrupt enable. */
+        pciercx_cfg034.s.dlls_en = 1; /* Data Link Layer state changed enable */
+        pciercx_cfg034.s.ccint_en = 1; /* Command completed interrupt enable. */
+        cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG034(pcie_port), pciercx_cfg034.u32);
+    }
+}
+
+static uint32_t shift_in_data(int bits, uint32_t data)
+{
+    cvmx_ciu_qlm_jtgd_t jtgd;
+    jtgd.u64 = 0;
+    jtgd.s.shift = 1;
+    jtgd.s.shft_cnt = bits-1;
+    jtgd.s.shft_reg = data;
+    if (OCTEON_IS_MODEL(OCTEON_CN52XX))
+        jtgd.s.select = 1 << 0 /*qlm*/;
+    cvmx_write_csr(CVMX_CIU_QLM_JTGD, jtgd.u64);
+    do
+    {
+        jtgd.u64 = cvmx_read_csr(CVMX_CIU_QLM_JTGD);
+    } while (jtgd.s.shift);
+    return jtgd.s.shft_reg;
+}
+
+static void __cvmx_pcie_fix_link(void)
+{
+    cvmx_ciu_qlm_jtgc_t jtgc;
+    cvmx_ciu_qlm_jtgd_t jtgd;
+    int clock_div = 0;
+    int divisor = cvmx_sysinfo_get()->cpu_clock_hz / (25 * 1000000);
+    divisor = (divisor-1)>>2;
+    while (divisor)
+    {
+        clock_div++;
+        divisor>>=1;
+    }
+
+    jtgc.u64 = 0;
+    jtgc.s.clk_div = clock_div; /* Clock divider for QLM JTAG operations.  eclk is divided by 2^(CLK_DIV + 2) */
+    jtgc.s.mux_sel = 0;         /* Selects which QLM JTAG shift out is shifted into the QLM JTAG shift register: CIU_QLM_JTGD[SHFT_REG] */
+    jtgc.s.bypass = 0x1;        /* Selects which QLM JTAG shift chains are bypassed by the QLM JTAG data register (CIU_QLM_JTGD) (one bit per QLM) */
+    cvmx_write_csr(CVMX_CIU_QLM_JTGC, jtgc.u64);
+    cvmx_read_csr(CVMX_CIU_QLM_JTGC);
+
+    if (OCTEON_IS_MODEL(OCTEON_CN56XX))
+    {
+        shift_in_data(32, 0x04000000);
+        shift_in_data(32, 0x00000000);
+        shift_in_data(32, 0x81800000);
+        shift_in_data(32, 0x00000000);
+        shift_in_data(32, 0x00000000);
+        shift_in_data(32, 0x00000000);
+        shift_in_data(32, 0x00000000);
+        shift_in_data(32, 0x00000000);
+        shift_in_data(32, 0x00000000);
+        shift_in_data(32, 0x00000040);
+        shift_in_data(32, 0x00000000);
+        shift_in_data(32, 0x00000818);
+        shift_in_data(32, 0x00000000);
+        shift_in_data(32, 0x00000000);
+        shift_in_data(32, 0x00000000);
+        shift_in_data(32, 0x00000000);
+        shift_in_data(32, 0x00000000);
+        shift_in_data(32, 0x00040000);
+        shift_in_data(32, 0x00000000);
+        shift_in_data(32, 0x00818000);
+        shift_in_data(32, 0x00000000);
+        shift_in_data(32, 0x00000000);
+        shift_in_data(32, 0x00014000);
+        shift_in_data(32, 0x00000000);
+        shift_in_data(32, 0x00000000);
+        shift_in_data(32, 0x40000000);
+        shift_in_data(32, 0x00000000);
+        shift_in_data(32, 0x18000000);
+        shift_in_data(32, 0x00000008);
+        shift_in_data(32, 0x00000000);
+        shift_in_data(32, 0x00000000);
+        shift_in_data(32, 0x00000000);
+        shift_in_data(32, 0x00000000);
+        shift_in_data(16, 0x00000000);
+    }
+    else if (OCTEON_IS_MODEL(OCTEON_CN52XX))
+    {
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x2003);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x3000);
+        shift_in_data(16, 0x0200);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0300);
+        shift_in_data(16, 0x0020);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0030);
+        shift_in_data(16, 0x0002);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+        shift_in_data(16, 0x0000);
+    }
+
+    /* Update the new data */
+    jtgd.u64 = 0;
+    jtgd.s.update = 1;
+    if (OCTEON_IS_MODEL(OCTEON_CN52XX))
+        jtgd.s.select = 1 << 0 /*qlm*/;
+    cvmx_write_csr(CVMX_CIU_QLM_JTGD, jtgd.u64);
+    do
+    {
+        jtgd.u64 = cvmx_read_csr(CVMX_CIU_QLM_JTGD);
+    } while (jtgd.s.update);
+}
+
+/**
+ * @INTERNAL
+ * Initialize a host mode PCIe link. This function takes a PCIe
+ * port from reset to a link up state. Software can then begin
+ * configuring the rest of the link.
+ *
+ * @param pcie_port PCIe port to initialize
+ *
+ * @return Zero on success
+ */
+static int __cvmx_pcie_rc_initialize_link(int pcie_port)
+{
+    uint64_t start_cycle;
+    cvmx_pescx_ctl_status_t pescx_ctl_status;
+    cvmx_pciercx_cfg452_t pciercx_cfg452;
+    cvmx_pciercx_cfg032_t pciercx_cfg032;
+    cvmx_pciercx_cfg448_t pciercx_cfg448;
+
+    /* Set the lane width */
+    pciercx_cfg452.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG452(pcie_port));
+    pescx_ctl_status.u64 = cvmx_read_csr(CVMX_PESCX_CTL_STATUS(pcie_port));
+    if (pescx_ctl_status.s.qlm_cfg == 0)
+    {
+        /* We're in 8 lane (56XX) or 4 lane (54XX) mode */
+        pciercx_cfg452.s.lme = 0xf;
+    }
+    else
+    {
+        /* We're in 4 lane (56XX) or 2 lane (52XX) mode */
+        pciercx_cfg452.s.lme = 0x7;
+    }
+    cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG452(pcie_port), pciercx_cfg452.u32);
+
+    /* CN52XX pass 1 has an errata where length mismatches on UR responses can
+        cause bus errors on 64bit memory reads. Turning off length error
+        checking fixes this */
+    if (OCTEON_IS_MODEL(OCTEON_CN52XX_PASS1))
+    {
+        cvmx_pciercx_cfg455_t pciercx_cfg455;
+        pciercx_cfg455.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG455(pcie_port));
+        pciercx_cfg455.s.m_cpl_len_err = 1;
+        cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG455(pcie_port), pciercx_cfg455.u32);
+    }
+
+    /* Lane swap needs to be manually enabled for CN52XX */
+    if (OCTEON_IS_MODEL(OCTEON_CN52XX) && (pcie_port == 1))
+    {
+      pescx_ctl_status.s.lane_swp = 1;
+      cvmx_write_csr(CVMX_PESCX_CTL_STATUS(pcie_port),pescx_ctl_status.u64);
+    }
+
+    /* Bring up the link */
+    pescx_ctl_status.u64 = cvmx_read_csr(CVMX_PESCX_CTL_STATUS(pcie_port));
+    pescx_ctl_status.s.lnk_enb = 1;
+    cvmx_write_csr(CVMX_PESCX_CTL_STATUS(pcie_port), pescx_ctl_status.u64);
+
+    /* CN56XX pass 1: Due to some internal resistors being wrong we need to
+            force Link good on these parts. This does not apply to pass 1.1.
+       CN52XX pass 1: Due to a bug in 2nd order CDR, it needs to be disabled */
+    if (OCTEON_IS_MODEL(OCTEON_CN56XX_PASS1) || OCTEON_IS_MODEL(OCTEON_CN52XX_PASS1))
+        __cvmx_pcie_fix_link();
+
+    /* Wait for the link to come up */
+    cvmx_dprintf("PCIe: Waiting for port %d link\n", pcie_port);
+    start_cycle = cvmx_get_cycle();
+    do
+    {
+        if (cvmx_get_cycle() - start_cycle > 2*cvmx_sysinfo_get()->cpu_clock_hz)
+        {
+            cvmx_dprintf("PCIe: Port %d link timeout\n", pcie_port);
+            return -1;
+        }
+        cvmx_wait(10000);
+        pciercx_cfg032.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG032(pcie_port));
+    } while (pciercx_cfg032.s.dlla == 0);
+
+    /* Display the link status */
+    cvmx_dprintf("PCIe: Port %d link active, %d lanes\n", pcie_port, pciercx_cfg032.s.nlw);
+
+    /* Update the Replay Time Limit. Empirically, some PCIe devices take a
+        little longer to respond than expected under load. As a workaround for
+        this we configure the Replay Time Limit to the value expected for a 512
+        byte MPS instead of our actual 256 byte MPS. The numbers below are
+        directly from the PCIe spec table 3-4 */
+    pciercx_cfg448.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG448(pcie_port));
+    switch (pciercx_cfg032.s.nlw)
+    {
+        case 1: /* 1 lane */
+            pciercx_cfg448.s.rtl = 1677;
+            break;
+        case 2: /* 2 lanes */
+            pciercx_cfg448.s.rtl = 867;
+            break;
+        case 4: /* 4 lanes */
+            pciercx_cfg448.s.rtl = 462;
+            break;
+        case 8: /* 8 lanes */
+            pciercx_cfg448.s.rtl = 258;
+            break;
+    }
+    cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG448(pcie_port), pciercx_cfg448.u32);
+
+    return 0;
+}
+
+
+/**
+ * Initialize a PCIe port for use in host(RC) mode. It doesn't enumerate the bus.
+ *
+ * @param pcie_port PCIe port to initialize
+ *
+ * @return Zero on success
+ */
+int cvmx_pcie_rc_initialize(int pcie_port)
+{
+    int i;
+    cvmx_ciu_soft_prst_t ciu_soft_prst;
+    cvmx_pescx_bist_status_t pescx_bist_status;
+    cvmx_pescx_bist_status2_t pescx_bist_status2;
+    cvmx_npei_ctl_status_t npei_ctl_status;
+    cvmx_npei_mem_access_ctl_t npei_mem_access_ctl;
+    cvmx_npei_mem_access_subidx_t mem_access_subid;
+    cvmx_npei_dbg_data_t npei_dbg_data;
+    cvmx_pescx_ctl_status2_t pescx_ctl_status2;
+
+    /* Make sure we aren't trying to setup a target mode interface in host mode */
+    npei_ctl_status.u64 = cvmx_read_csr(CVMX_PEXP_NPEI_CTL_STATUS);
+    if ((pcie_port==0) && !npei_ctl_status.s.host_mode)
+    {
+        cvmx_dprintf("PCIe: ERROR: cvmx_pcie_rc_initialize() called on port0, but port0 is not in host mode\n");
+        return -1;
+    }
+
+    /* Make sure a CN52XX isn't trying to bring up port 1 when it is disabled */
+    if (OCTEON_IS_MODEL(OCTEON_CN52XX))
+    {
+        npei_dbg_data.u64 = cvmx_read_csr(CVMX_PEXP_NPEI_DBG_DATA);
+        if ((pcie_port==1) && npei_dbg_data.cn52xx.qlm0_link_width)
+        {
+            cvmx_dprintf("PCIe: ERROR: cvmx_pcie_rc_initialize() called on port1, but port1 is disabled\n");
+            return -1;
+        }
+    }
+
+    /* PCIe switch arbitration mode. '0' == fixed priority NPEI, PCIe0, then PCIe1. '1' == round robin. */
+    npei_ctl_status.s.arb = 1;
+    /* Allow up to 0x20 config retries */
+    npei_ctl_status.s.cfg_rtry = 0x20;
+    /* CN52XX pass1 has an errata where P0_NTAGS and P1_NTAGS don't reset */
+    if (OCTEON_IS_MODEL(OCTEON_CN52XX_PASS1))
+    {
+        npei_ctl_status.s.p0_ntags = 0x20;
+        npei_ctl_status.s.p1_ntags = 0x20;
+    }
+    cvmx_write_csr(CVMX_PEXP_NPEI_CTL_STATUS, npei_ctl_status.u64);
+
+    /* Bring the PCIe out of reset */
+    if (cvmx_sysinfo_get()->board_type == CVMX_BOARD_TYPE_EBH5200)
+    {
+        /* The EBH5200 board swapped the PCIe reset lines on the board. As a
+            workaround for this bug, we bring both PCIe ports out of reset at
+            the same time instead of on separate calls. So for port 0, we bring
+            both out of reset and do nothing on port 1 */
+        if (pcie_port == 0)
+        {
+            ciu_soft_prst.u64 = cvmx_read_csr(CVMX_CIU_SOFT_PRST);
+            /* After a chip reset the PCIe will also be in reset. If it isn't,
+                most likely someone is trying to init it again without a proper
+                PCIe reset */
+            if (ciu_soft_prst.s.soft_prst == 0)
+            {
+                cvmx_dprintf("PCIe: ERROR: cvmx_pcie_rc_initialize_link() called but port%d is not in reset\n", pcie_port);
+                return -1;
+            }
+            ciu_soft_prst.u64 = cvmx_read_csr(CVMX_CIU_SOFT_PRST1);
+            ciu_soft_prst.s.soft_prst = 0;
+            cvmx_write_csr(CVMX_CIU_SOFT_PRST1, ciu_soft_prst.u64);
+            ciu_soft_prst.u64 = cvmx_read_csr(CVMX_CIU_SOFT_PRST);
+            ciu_soft_prst.s.soft_prst = 0;
+            cvmx_write_csr(CVMX_CIU_SOFT_PRST, ciu_soft_prst.u64);
+        }
+    }
+    else
+    {
+        /* The normal case: The PCIe ports are completely separate and can be
+            brought out of reset independently */
+        if (pcie_port)
+            ciu_soft_prst.u64 = cvmx_read_csr(CVMX_CIU_SOFT_PRST1);
+        else
+            ciu_soft_prst.u64 = cvmx_read_csr(CVMX_CIU_SOFT_PRST);
+        /* After a chip reset the PCIe will also be in reset. If it isn't,
+            most likely someone is trying to init it again without a proper
+            PCIe reset */
+        if (ciu_soft_prst.s.soft_prst == 0)
+        {
+            cvmx_dprintf("PCIe: ERROR: cvmx_pcie_rc_initialize_link() called but port%d is not in reset\n", pcie_port);
+            return -1;
+        }
+        if (pcie_port)
+        {
+            ciu_soft_prst.u64 = cvmx_read_csr(CVMX_CIU_SOFT_PRST1);
+            ciu_soft_prst.s.soft_prst = 0;
+            cvmx_write_csr(CVMX_CIU_SOFT_PRST1, ciu_soft_prst.u64);
+        }
+        else
+        {
+            ciu_soft_prst.u64 = cvmx_read_csr(CVMX_CIU_SOFT_PRST);
+            ciu_soft_prst.s.soft_prst = 0;
+            cvmx_write_csr(CVMX_CIU_SOFT_PRST, ciu_soft_prst.u64);
+        }
+    }
+
+    /* Wait for PCIe reset to complete. Due to errata PCIE-700, we don't poll
+       PESCX_CTL_STATUS2[PCIERST], but simply wait a fixed number of cycles */
+    cvmx_wait(400000);
+
+    /* Check and make sure PCIe came out of reset. If it doesn't the board
+        probably hasn't wired the clocks up and the interface should be
+        skipped */
+    pescx_ctl_status2.u64 = cvmx_read_csr(CVMX_PESCX_CTL_STATUS2(pcie_port));
+    if (pescx_ctl_status2.s.pcierst)
+    {
+        cvmx_dprintf("PCIe: Port %d appears to not be clocked, skipping.\n", pcie_port);
+        return -1;
+    }
+
+    /* Check BIST2 status. If any bits are set skip this interface */
+    pescx_bist_status2.u64 = cvmx_read_csr(CVMX_PESCX_BIST_STATUS2(pcie_port));
+    if (pescx_bist_status2.u64)
+    {
+        cvmx_dprintf("PCIe: Port %d BIST2 failed. Most likely this port isn't hooked up, skipping.\n", pcie_port);
+        return -1;
+    }
+
+    /* Check BIST status */
+    pescx_bist_status.u64 = cvmx_read_csr(CVMX_PESCX_BIST_STATUS(pcie_port));
+    if (pescx_bist_status.u64)
+        cvmx_dprintf("PCIe: BIST FAILED for port %d (0x%016llx)\n", pcie_port, CAST64(pescx_bist_status.u64));
+
+    /* Initialize the config space CSRs */
+    __cvmx_pcie_rc_initialize_config_space(pcie_port);
+
+    /* Bring the link up */
+    if (__cvmx_pcie_rc_initialize_link(pcie_port))
+    {
+        cvmx_dprintf("PCIe: ERROR: cvmx_pcie_rc_initialize_link() failed\n");
+        return -1;
+    }
+
+    /* Store merge control (NPEI_MEM_ACCESS_CTL[TIMER,MAX_WORD]) */
+    npei_mem_access_ctl.u64 = cvmx_read_csr(CVMX_PEXP_NPEI_MEM_ACCESS_CTL);
+    npei_mem_access_ctl.s.max_word = 0;     /* Allow 16 words to combine */
+    npei_mem_access_ctl.s.timer = 127;      /* Wait up to 127 cycles for more data */
+    cvmx_write_csr(CVMX_PEXP_NPEI_MEM_ACCESS_CTL, npei_mem_access_ctl.u64);
+
+    /* Setup Mem access SubDIDs */
+    mem_access_subid.u64 = 0;
+    mem_access_subid.s.port = pcie_port; /* Port the request is sent to. */
+    mem_access_subid.s.nmerge = 1;  /* Due to an errata on pass 1 chips, no merging is allowed. */
+    mem_access_subid.s.esr = 1;     /* Endian-swap for Reads. */
+    mem_access_subid.s.esw = 1;     /* Endian-swap for Writes. */
+    mem_access_subid.s.nsr = 1;     /* No Snoop for Reads. */
+    mem_access_subid.s.nsw = 1;     /* No Snoop for Writes. */
+    mem_access_subid.s.ror = 0;     /* Disable Relaxed Ordering for Reads. */
+    mem_access_subid.s.row = 0;     /* Disable Relaxed Ordering for Writes. */
+    mem_access_subid.s.ba = 0;      /* PCIe Adddress Bits <63:34>. */
+
+    /* Setup mem access 12-15 for port 0, 16-19 for port 1, supplying 36 bits of address space */
+    for (i=12 + pcie_port*4; i<16 + pcie_port*4; i++)
+    {
+        cvmx_write_csr(CVMX_PEXP_NPEI_MEM_ACCESS_SUBIDX(i), mem_access_subid.u64);
+        mem_access_subid.s.ba += 1; /* Set each SUBID to extend the addressable range */
+    }
+
+    /* Disable the peer to peer forwarding register. This must be setup
+        by the OS after it enumerates the bus and assigns addresses to the
+        PCIe busses */
+    for (i=0; i<4; i++)
+    {
+        cvmx_write_csr(CVMX_PESCX_P2P_BARX_START(i, pcie_port), -1);
+        cvmx_write_csr(CVMX_PESCX_P2P_BARX_END(i, pcie_port), -1);
+    }
+
+    /* Set Octeon's BAR0 to decode 0-16KB. It overlaps with Bar2 */
+    cvmx_write_csr(CVMX_PESCX_P2N_BAR0_START(pcie_port), 0);
+
+    /* Disable Octeon's BAR1. It isn't needed in RC mode since BAR2
+        maps all of memory. BAR2 also maps 256MB-512MB into the 2nd
+        256MB of memory */
+    cvmx_write_csr(CVMX_PESCX_P2N_BAR1_START(pcie_port), -1);
+
+    /* Set Octeon's BAR2 to decode 0-2^39. Bar0 and Bar1 take precedence
+        where they overlap. It also overlaps with the device addresses, so
+        make sure the peer to peer forwarding is set right */
+    cvmx_write_csr(CVMX_PESCX_P2N_BAR2_START(pcie_port), 0);
+
+    /* Setup BAR2 attributes */
+    /* Relaxed Ordering (NPEI_CTL_PORTn[PTLP_RO,CTLP_RO, WAIT_COM]) */
+    /*  PTLP_RO,CTLP_RO should normally be set (except for debug). */
+    /*  WAIT_COM=0 will likely work for all applications. */
+    /* Load completion relaxed ordering (NPEI_CTL_PORTn[WAITL_COM]) */
+    if (pcie_port)
+    {
+        cvmx_npei_ctl_port1_t npei_ctl_port;
+        npei_ctl_port.u64 = cvmx_read_csr(CVMX_PEXP_NPEI_CTL_PORT1);
+        npei_ctl_port.s.bar2_enb = 1;
+        npei_ctl_port.s.bar2_esx = 1;
+        npei_ctl_port.s.bar2_cax = 0;
+        npei_ctl_port.s.ptlp_ro = 1;
+        npei_ctl_port.s.ctlp_ro = 1;
+        npei_ctl_port.s.wait_com = 0;
+        npei_ctl_port.s.waitl_com = 0;
+        cvmx_write_csr(CVMX_PEXP_NPEI_CTL_PORT1, npei_ctl_port.u64);
+    }
+    else
+    {
+        cvmx_npei_ctl_port0_t npei_ctl_port;
+        npei_ctl_port.u64 = cvmx_read_csr(CVMX_PEXP_NPEI_CTL_PORT0);
+        npei_ctl_port.s.bar2_enb = 1;
+        npei_ctl_port.s.bar2_esx = 1;
+        npei_ctl_port.s.bar2_cax = 0;
+        npei_ctl_port.s.ptlp_ro = 1;
+        npei_ctl_port.s.ctlp_ro = 1;
+        npei_ctl_port.s.wait_com = 0;
+        npei_ctl_port.s.waitl_com = 0;
+        cvmx_write_csr(CVMX_PEXP_NPEI_CTL_PORT0, npei_ctl_port.u64);
+    }
+    return 0;
+}
+
+
+/**
+ * Shutdown a PCIe port and put it in reset
+ *
+ * @param pcie_port PCIe port to shutdown
+ *
+ * @return Zero on success
+ */
+int cvmx_pcie_rc_shutdown(int pcie_port)
+{
+    /* Wait for all pending operations to complete */
+    if (CVMX_WAIT_FOR_FIELD64(CVMX_PESCX_CPL_LUT_VALID(pcie_port), cvmx_pescx_cpl_lut_valid_t, tag, ==, 0, 2000))
+        cvmx_dprintf("PCIe: Port %d shutdown timeout\n", pcie_port);
+
+    /* Force reset */
+    if (pcie_port)
+    {
+        cvmx_ciu_soft_prst_t ciu_soft_prst;
+        ciu_soft_prst.u64 = cvmx_read_csr(CVMX_CIU_SOFT_PRST1);
+        ciu_soft_prst.s.soft_prst = 1;
+        cvmx_write_csr(CVMX_CIU_SOFT_PRST1, ciu_soft_prst.u64);
+    }
+    else
+    {
+        cvmx_ciu_soft_prst_t ciu_soft_prst;
+        ciu_soft_prst.u64 = cvmx_read_csr(CVMX_CIU_SOFT_PRST);
+        ciu_soft_prst.s.soft_prst = 1;
+        cvmx_write_csr(CVMX_CIU_SOFT_PRST, ciu_soft_prst.u64);
+    }
+    return 0;
+}
+
+
+/**
+ * @INTERNAL
+ * Build a PCIe config space request address for a device
+ *
+ * @param pcie_port PCIe port to access
+ * @param bus       Sub bus
+ * @param dev       Device ID
+ * @param fn        Device sub function
+ * @param reg       Register to access
+ *
+ * @return 64bit Octeon IO address
+ */
+static inline uint64_t __cvmx_pcie_build_config_addr(int pcie_port, int bus, int dev, int fn, int reg)
+{
+    cvmx_pcie_address_t pcie_addr;
+    pcie_addr.u64 = 0;
+    pcie_addr.config.upper = 2;
+    pcie_addr.config.io = 1;
+    pcie_addr.config.did = 3;
+    pcie_addr.config.subdid = 1;
+    pcie_addr.config.es = 1;
+    pcie_addr.config.port = pcie_port;
+    pcie_addr.config.ty = (bus != 0);
+    pcie_addr.config.bus = bus;
+    pcie_addr.config.dev = dev;
+    pcie_addr.config.func = fn;
+    pcie_addr.config.reg = reg;
+    return pcie_addr.u64;
+}
+
+
+/**
+ * Read 8bits from a Device's config space
+ *
+ * @param pcie_port PCIe port the device is on
+ * @param bus       Sub bus
+ * @param dev       Device ID
+ * @param fn        Device sub function
+ * @param reg       Register to access
+ *
+ * @return Result of the read
+ */
+uint8_t cvmx_pcie_config_read8(int pcie_port, int bus, int dev, int fn, int reg)
+{
+    return cvmx_read64_uint8(__cvmx_pcie_build_config_addr(pcie_port, bus, dev, fn, reg));
+}
+
+
+/**
+ * Read 16bits from a Device's config space
+ *
+ * @param pcie_port PCIe port the device is on
+ * @param bus       Sub bus
+ * @param dev       Device ID
+ * @param fn        Device sub function
+ * @param reg       Register to access
+ *
+ * @return Result of the read
+ */
+uint16_t cvmx_pcie_config_read16(int pcie_port, int bus, int dev, int fn, int reg)
+{
+    return cvmx_le16_to_cpu(cvmx_read64_uint16(__cvmx_pcie_build_config_addr(pcie_port, bus, dev, fn, reg)));
+}
+
+
+/**
+ * Read 32bits from a Device's config space
+ *
+ * @param pcie_port PCIe port the device is on
+ * @param bus       Sub bus
+ * @param dev       Device ID
+ * @param fn        Device sub function
+ * @param reg       Register to access
+ *
+ * @return Result of the read
+ */
+uint32_t cvmx_pcie_config_read32(int pcie_port, int bus, int dev, int fn, int reg)
+{
+    return cvmx_le32_to_cpu(cvmx_read64_uint32(__cvmx_pcie_build_config_addr(pcie_port, bus, dev, fn, reg)));
+}
+
+
+/**
+ * Write 8bits to a Device's config space
+ *
+ * @param pcie_port PCIe port the device is on
+ * @param bus       Sub bus
+ * @param dev       Device ID
+ * @param fn        Device sub function
+ * @param reg       Register to access
+ * @param val       Value to write
+ */
+void cvmx_pcie_config_write8(int pcie_port, int bus, int dev, int fn, int reg, uint8_t val)
+{
+    cvmx_write64_uint8(__cvmx_pcie_build_config_addr(pcie_port, bus, dev, fn, reg), val);
+}
+
+
+/**
+ * Write 16bits to a Device's config space
+ *
+ * @param pcie_port PCIe port the device is on
+ * @param bus       Sub bus
+ * @param dev       Device ID
+ * @param fn        Device sub function
+ * @param reg       Register to access
+ * @param val       Value to write
+ */
+void cvmx_pcie_config_write16(int pcie_port, int bus, int dev, int fn, int reg, uint16_t val)
+{
+    cvmx_write64_uint16(__cvmx_pcie_build_config_addr(pcie_port, bus, dev, fn, reg), cvmx_cpu_to_le16(val));
+}
+
+
+/**
+ * Write 32bits to a Device's config space
+ *
+ * @param pcie_port PCIe port the device is on
+ * @param bus       Sub bus
+ * @param dev       Device ID
+ * @param fn        Device sub function
+ * @param reg       Register to access
+ * @param val       Value to write
+ */
+void cvmx_pcie_config_write32(int pcie_port, int bus, int dev, int fn, int reg, uint32_t val)
+{
+    cvmx_write64_uint32(__cvmx_pcie_build_config_addr(pcie_port, bus, dev, fn, reg), cvmx_cpu_to_le32(val));
+}
+
+
+/**
+ * Read a PCIe config space register indirectly. This is used for
+ * registers of the form PCIEEP_CFG??? and PCIERC?_CFG???.
+ *
+ * @param pcie_port  PCIe port to read from
+ * @param cfg_offset Address to read
+ *
+ * @return Value read
+ */
+uint32_t cvmx_pcie_cfgx_read(int pcie_port, uint32_t cfg_offset)
+{
+    cvmx_pescx_cfg_rd_t pescx_cfg_rd;
+    pescx_cfg_rd.u64 = 0;
+    pescx_cfg_rd.s.addr = cfg_offset;
+    cvmx_write_csr(CVMX_PESCX_CFG_RD(pcie_port), pescx_cfg_rd.u64);
+    pescx_cfg_rd.u64 = cvmx_read_csr(CVMX_PESCX_CFG_RD(pcie_port));
+    return pescx_cfg_rd.s.data;
+}
+
+
+/**
+ * Write a PCIe config space register indirectly. This is used for
+ * registers of the form PCIEEP_CFG??? and PCIERC?_CFG???.
+ *
+ * @param pcie_port  PCIe port to write to
+ * @param cfg_offset Address to write
+ * @param val        Value to write
+ */
+void cvmx_pcie_cfgx_write(int pcie_port, uint32_t cfg_offset, uint32_t val)
+{
+    cvmx_pescx_cfg_wr_t pescx_cfg_wr;
+    pescx_cfg_wr.u64 = 0;
+    pescx_cfg_wr.s.addr = cfg_offset;
+    pescx_cfg_wr.s.data = val;
+    cvmx_write_csr(CVMX_PESCX_CFG_WR(pcie_port), pescx_cfg_wr.u64);
+}
+
+
+/**
+ * Initialize a PCIe port for use in target(EP) mode.
+ *
+ * @return Zero on success
+ */
+int cvmx_pcie_ep_initialize(void)
+{
+    int pcie_port = 0;
+
+    /* Enable bus master and memory */
+    cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIEEP_CFG001, 0x6);
+
+    /* Max Payload Size (PCIE*_CFG030[MPS]) */
+    /* Max Read Request Size (PCIE*_CFG030[MRRS]) */
+    /* Relaxed-order, no-snoop enables (PCIE*_CFG030[RO_EN,NS_EN] */
+    /* Error Message Enables (PCIE*_CFG030[CE_EN,NFE_EN,FE_EN,UR_EN]) */
+    {
+        cvmx_pciercx_cfg030_t pciercx_cfg030;
+        pciercx_cfg030.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG030(pcie_port));
+        pciercx_cfg030.s.mps = 1; /* Max payload size = 256 bytes */
+        pciercx_cfg030.s.mrrs = 5; /* Max read request size = 4096 bytes */
+        pciercx_cfg030.s.ro_en = 1; /* Enable relaxed ordering. */
+        pciercx_cfg030.s.ns_en = 1; /* Enable no snoop. */
+        pciercx_cfg030.s.ce_en = 1; /* Correctable error reporting enable. */
+        pciercx_cfg030.s.nfe_en = 1; /* Non-fatal error reporting enable. */
+        pciercx_cfg030.s.fe_en = 1; /* Fatal error reporting enable. */
+        pciercx_cfg030.s.ur_en = 1; /* Unsupported request reporting enable. */
+        cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG030(pcie_port), pciercx_cfg030.u32);
+    }
+
+    /* Max Payload Size (NPEI_CTL_STATUS2[MPS]) must match PCIE*_CFG030[MPS] */
+    /* Max Read Request Size (NPEI_CTL_STATUS2[MRRS]) must not exceed PCIE*_CFG030[MRRS] */
+    {
+        cvmx_npei_ctl_status2_t npei_ctl_status2;
+        npei_ctl_status2.u64 = cvmx_read_csr(CVMX_PEXP_NPEI_CTL_STATUS2);
+        npei_ctl_status2.s.mps = 1; /* Max payload size = 256 bytes */
+        npei_ctl_status2.s.mrrs = 5; /* Max read request size = 4096 bytes */
+        cvmx_write_csr(CVMX_PEXP_NPEI_CTL_STATUS2, npei_ctl_status2.u64);
+    }
+
+    /* Setup Mem access SubDID 12 to access Host memory */
+    {
+        cvmx_npei_mem_access_subidx_t mem_access_subid;
+        mem_access_subid.u64 = 0;
+        mem_access_subid.s.port = pcie_port; /* Port the request is sent to. */
+        mem_access_subid.s.nmerge = 0;  /* Merging is allowed in this window. */
+        mem_access_subid.s.esr = 0;     /* Endian-swap for Reads. */
+        mem_access_subid.s.esw = 0;     /* Endian-swap for Writes. */
+        mem_access_subid.s.nsr = 1;     /* No Snoop for Reads. */
+        mem_access_subid.s.nsw = 1;     /* No Snoop for Writes. */
+        mem_access_subid.s.ror = 0;     /* Disable Relaxed Ordering for Reads. */
+        mem_access_subid.s.row = 0;     /* Disable Relaxed Ordering for Writes. */
+        mem_access_subid.s.ba = 0;      /* PCIe Adddress Bits <63:34>. */
+        cvmx_write_csr(CVMX_PEXP_NPEI_MEM_ACCESS_SUBIDX(12), mem_access_subid.u64);
+    }
+    return 0;
+}
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pcie.h b/arch/mips/cavium-octeon/executive/cvmx-pcie.h
new file mode 100644
index 0000000..d79959c
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-pcie.h
@@ -0,0 +1,301 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Interface to PCIe as a host(RC) or target(EP)
+ *
+ * <hr>$Revision: 35733 $<hr>
+ */
+
+#ifndef __CVMX_PCIE_H__
+#define __CVMX_PCIE_H__
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+typedef union
+{
+    uint64_t    u64;
+    struct
+    {
+        uint64_t    upper           : 2;    /* Normally 2 for XKPHYS */
+        uint64_t    reserved_49_61  : 13;   /* Must be zero */
+        uint64_t    io              : 1;    /* 1 for IO space access */
+        uint64_t    did             : 5;    /* PCIe DID = 3 */
+        uint64_t    subdid          : 3;    /* PCIe SubDID = 1 */
+        uint64_t    reserved_36_39  : 4;    /* Must be zero */
+        uint64_t    es              : 2;    /* Endian swap = 1 */
+        uint64_t    port            : 2;    /* PCIe port 0,1 */
+        uint64_t    reserved_29_31  : 3;    /* Must be zero */
+        uint64_t    ty              : 1;    /* Selects the type of the configuration request (0 = type 0, 1 = type 1). */
+        uint64_t    bus             : 8;    /* Target bus number sent in the ID in the request. */
+        uint64_t    dev             : 5;    /* Target device number sent in the ID in the request. Note that Dev must be
+                                                zero for type 0 configuration requests. */
+        uint64_t    func            : 3;    /* Target function number sent in the ID in the request. */
+        uint64_t    reg             : 12;   /* Selects a register in the configuration space of the target. */
+    } config;
+    struct
+    {
+        uint64_t    upper           : 2;    /* Normally 2 for XKPHYS */
+        uint64_t    reserved_49_61  : 13;   /* Must be zero */
+        uint64_t    io              : 1;    /* 1 for IO space access */
+        uint64_t    did             : 5;    /* PCIe DID = 3 */
+        uint64_t    subdid          : 3;    /* PCIe SubDID = 2 */
+        uint64_t    reserved_36_39  : 4;    /* Must be zero */
+        uint64_t    es              : 2;    /* Endian swap = 1 */
+        uint64_t    port            : 2;    /* PCIe port 0,1 */
+        uint64_t    address         : 32;   /* PCIe IO address */
+    } io;
+    struct
+    {
+        uint64_t    upper           : 2;    /* Normally 2 for XKPHYS */
+        uint64_t    reserved_49_61  : 13;   /* Must be zero */
+        uint64_t    io              : 1;    /* 1 for IO space access */
+        uint64_t    did             : 5;    /* PCIe DID = 3 */
+        uint64_t    subdid          : 3;    /* PCIe SubDID = 3-6 */
+        uint64_t    reserved_36_39  : 4;    /* Must be zero */
+        uint64_t    address         : 36;   /* PCIe Mem address */
+    } mem;
+} cvmx_pcie_address_t;
+
+
+/**
+ * Return the Core virtual base address for PCIe IO access. IOs are
+ * read/written as an offset from this address.
+ *
+ * @param pcie_port PCIe port the IO is for
+ *
+ * @return 64bit Octeon IO base address for read/write
+ */
+uint64_t cvmx_pcie_get_io_base_address(int pcie_port);
+
+/**
+ * Size of the IO address region returned at address
+ * cvmx_pcie_get_io_base_address()
+ *
+ * @param pcie_port PCIe port the IO is for
+ *
+ * @return Size of the IO window
+ */
+uint64_t cvmx_pcie_get_io_size(int pcie_port);
+
+/**
+ * Return the Core virtual base address for PCIe MEM access. Memory is
+ * read/written as an offset from this address.
+ *
+ * @param pcie_port PCIe port the IO is for
+ *
+ * @return 64bit Octeon IO base address for read/write
+ */
+uint64_t cvmx_pcie_get_mem_base_address(int pcie_port);
+
+/**
+ * Size of the Mem address region returned at address
+ * cvmx_pcie_get_mem_base_address()
+ *
+ * @param pcie_port PCIe port the IO is for
+ *
+ * @return Size of the Mem window
+ */
+uint64_t cvmx_pcie_get_mem_size(int pcie_port);
+
+/**
+ * Initialize a PCIe port for use in host(RC) mode. It doesn't enumerate the bus.
+ *
+ * @param pcie_port PCIe port to initialize
+ *
+ * @return Zero on success
+ */
+int cvmx_pcie_rc_initialize(int pcie_port);
+
+/**
+ * Shutdown a PCIe port and put it in reset
+ *
+ * @param pcie_port PCIe port to shutdown
+ *
+ * @return Zero on success
+ */
+int cvmx_pcie_rc_shutdown(int pcie_port);
+
+/**
+ * Read 8bits from a Device's config space
+ *
+ * @param pcie_port PCIe port the device is on
+ * @param bus       Sub bus
+ * @param dev       Device ID
+ * @param fn        Device sub function
+ * @param reg       Register to access
+ *
+ * @return Result of the read
+ */
+uint8_t cvmx_pcie_config_read8(int pcie_port, int bus, int dev, int fn, int reg);
+
+/**
+ * Read 16bits from a Device's config space
+ *
+ * @param pcie_port PCIe port the device is on
+ * @param bus       Sub bus
+ * @param dev       Device ID
+ * @param fn        Device sub function
+ * @param reg       Register to access
+ *
+ * @return Result of the read
+ */
+uint16_t cvmx_pcie_config_read16(int pcie_port, int bus, int dev, int fn, int reg);
+
+/**
+ * Read 32bits from a Device's config space
+ *
+ * @param pcie_port PCIe port the device is on
+ * @param bus       Sub bus
+ * @param dev       Device ID
+ * @param fn        Device sub function
+ * @param reg       Register to access
+ *
+ * @return Result of the read
+ */
+uint32_t cvmx_pcie_config_read32(int pcie_port, int bus, int dev, int fn, int reg);
+
+/**
+ * Write 8bits to a Device's config space
+ *
+ * @param pcie_port PCIe port the device is on
+ * @param bus       Sub bus
+ * @param dev       Device ID
+ * @param fn        Device sub function
+ * @param reg       Register to access
+ * @param val       Value to write
+ */
+void cvmx_pcie_config_write8(int pcie_port, int bus, int dev, int fn, int reg, uint8_t val);
+
+/**
+ * Write 16bits to a Device's config space
+ *
+ * @param pcie_port PCIe port the device is on
+ * @param bus       Sub bus
+ * @param dev       Device ID
+ * @param fn        Device sub function
+ * @param reg       Register to access
+ * @param val       Value to write
+ */
+void cvmx_pcie_config_write16(int pcie_port, int bus, int dev, int fn, int reg, uint16_t val);
+
+/**
+ * Write 32bits to a Device's config space
+ *
+ * @param pcie_port PCIe port the device is on
+ * @param bus       Sub bus
+ * @param dev       Device ID
+ * @param fn        Device sub function
+ * @param reg       Register to access
+ * @param val       Value to write
+ */
+void cvmx_pcie_config_write32(int pcie_port, int bus, int dev, int fn, int reg, uint32_t val);
+
+/**
+ * Read a PCIe config space register indirectly. This is used for
+ * registers of the form PCIEEP_CFG??? and PCIERC?_CFG???.
+ *
+ * @param pcie_port  PCIe port to read from
+ * @param cfg_offset Address to read
+ *
+ * @return Value read
+ */
+uint32_t cvmx_pcie_cfgx_read(int pcie_port, uint32_t cfg_offset);
+
+/**
+ * Write a PCIe config space register indirectly. This is used for
+ * registers of the form PCIEEP_CFG??? and PCIERC?_CFG???.
+ *
+ * @param pcie_port  PCIe port to write to
+ * @param cfg_offset Address to write
+ * @param val        Value to write
+ */
+void cvmx_pcie_cfgx_write(int pcie_port, uint32_t cfg_offset, uint32_t val);
+
+/**
+ * Write a 32bit value to the Octeon NPEI register space
+ *
+ * @param address Address to write to
+ * @param val     Value to write
+ */
+static inline void cvmx_pcie_npei_write32(uint64_t address, uint32_t val)
+{
+	cvmx_write64_uint32(address ^ 4, val);
+	cvmx_read64_uint32(address ^ 4);
+}
+
+/**
+ * Read a 32bit value from the Octeon NPEI register space
+ *
+ * @param address Address to read
+ * @return The result
+ */
+static inline uint32_t cvmx_pcie_npei_read32(uint64_t address)
+{
+	return cvmx_read64_uint32(address ^ 4);
+}
+
+/**
+ * Initialize a PCIe port for use in target(EP) mode.
+ *
+ * @return Zero on success
+ */
+int cvmx_pcie_ep_initialize(void);
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pip.h b/arch/mips/cavium-octeon/executive/cvmx-pip.h
new file mode 100644
index 0000000..1f5f34b
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-pip.h
@@ -0,0 +1,483 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Interface to the hardware Packet Input Processing unit.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+
+
+#ifndef __CVMX_PIP_H__
+#define __CVMX_PIP_H__
+
+#include "cvmx-wqe.h"
+#include "cvmx-fpa.h"
+#ifndef CVMX_DONT_INCLUDE_CONFIG
+#include "executive-config.h"
+#endif
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#define CVMX_PIP_NUM_INPUT_PORTS                40
+#define CVMX_PIP_NUM_WATCHERS                   4
+
+
+
+
+
+
+
+//
+// Encodes the different error and exception codes
+//
+typedef enum
+{
+    CVMX_PIP_L4_NO_ERR       = 0ull,
+    //        1  = TCP (UDP) packet not long enough to cover TCP (UDP) header
+    CVMX_PIP_L4_MAL_ERR     = 1ull,
+    //        2  = TCP/UDP checksum failure
+    CVMX_PIP_CHK_ERR        = 2ull,
+    //        3  = TCP/UDP length check (TCP/UDP length does not match IP length)
+    CVMX_PIP_L4_LENGTH_ERR  = 3ull,
+    //        4  = illegal TCP/UDP port (either source or dest port is zero)
+    CVMX_PIP_BAD_PRT_ERR    = 4ull,
+    //        8  = TCP flags = FIN only
+    CVMX_PIP_TCP_FLG8_ERR   = 8ull,
+    //        9  = TCP flags = 0
+    CVMX_PIP_TCP_FLG9_ERR   = 9ull,
+    //        10 = TCP flags = FIN+RST+*
+    CVMX_PIP_TCP_FLG10_ERR  = 10ull,
+    //        11 = TCP flags = SYN+URG+*
+    CVMX_PIP_TCP_FLG11_ERR  = 11ull,
+    //        12 = TCP flags = SYN+RST+*
+    CVMX_PIP_TCP_FLG12_ERR  = 12ull,
+    //        13 = TCP flags = SYN+FIN+*
+    CVMX_PIP_TCP_FLG13_ERR  = 13ull
+} cvmx_pip_l4_err_t;
+
+typedef enum
+{
+
+    CVMX_PIP_IP_NO_ERR      = 0ull,
+    //        1 = not IPv4 or IPv6
+    CVMX_PIP_NOT_IP        = 1ull,
+    //        2 = IPv4 header checksum violation
+    CVMX_PIP_IPV4_HDR_CHK  = 2ull,
+    //        3 = malformed (packet not long enough to cover IP hdr)
+    CVMX_PIP_IP_MAL_HDR    = 3ull,
+    //        4 = malformed (packet not long enough to cover len in IP hdr)
+    CVMX_PIP_IP_MAL_PKT    = 4ull,
+    //        5 = TTL / hop count equal zero
+    CVMX_PIP_TTL_HOP       = 5ull,
+    //        6 = IPv4 options / IPv6 early extension headers
+    CVMX_PIP_OPTS          = 6ull
+} cvmx_pip_ip_exc_t;
+
+
+/**
+ * NOTES
+ *       late collision (data received before collision)
+ *            late collisions cannot be detected by the receiver
+ *            they would appear as JAM bits which would appear as bad FCS
+ *            or carrier extend error which is CVMX_PIP_EXTEND_ERR
+ */
+typedef enum
+{
+    /**
+     * No error
+     */
+    CVMX_PIP_RX_NO_ERR      = 0ull,
+
+    CVMX_PIP_PARTIAL_ERR   = 1ull,  // RGM+SPI            1 = partially received packet (buffering/bandwidth not adequate)
+    CVMX_PIP_JABBER_ERR    = 2ull,  // RGM+SPI            2 = receive packet too large and truncated
+    CVMX_PIP_OVER_FCS_ERR  = 3ull,  // RGM                3 = max frame error (pkt len > max frame len) (with FCS error)
+    CVMX_PIP_OVER_ERR      = 4ull,  // RGM+SPI            4 = max frame error (pkt len > max frame len)
+    CVMX_PIP_ALIGN_ERR     = 5ull,  // RGM                5 = nibble error (data not byte multiple - 100M and 10M only)
+    CVMX_PIP_UNDER_FCS_ERR = 6ull,  // RGM                6 = min frame error (pkt len < min frame len) (with FCS error)
+    CVMX_PIP_GMX_FCS_ERR   = 7ull,  // RGM                7 = FCS error
+    CVMX_PIP_UNDER_ERR     = 8ull,  // RGM+SPI            8 = min frame error (pkt len < min frame len)
+    CVMX_PIP_EXTEND_ERR    = 9ull,  // RGM                9 = Frame carrier extend error
+    CVMX_PIP_LENGTH_ERR    = 10ull, // RGM               10 = length mismatch (len did not match len in L2 length/type)
+    CVMX_PIP_DAT_ERR       = 11ull, // RGM               11 = Frame error (some or all data bits marked err)
+    CVMX_PIP_DIP_ERR       = 11ull, //     SPI           11 = DIP4 error
+    CVMX_PIP_SKIP_ERR      = 12ull, // RGM               12 = packet was not large enough to pass the skipper - no inspection could occur
+    CVMX_PIP_NIBBLE_ERR    = 13ull, // RGM               13 = studder error (data not repeated - 100M and 10M only)
+    CVMX_PIP_PIP_FCS       = 16L, // RGM+SPI           16 = FCS error
+    CVMX_PIP_PIP_SKIP_ERR  = 17L, // RGM+SPI+PCI       17 = packet was not large enough to pass the skipper - no inspection could occur
+    CVMX_PIP_PIP_L2_MAL_HDR= 18L  // RGM+SPI+PCI       18 = malformed l2 (packet not long enough to cover L2 hdr)
+    // NOTES
+                                              //       xx = late collision (data received before collision)
+                                              //            late collisions cannot be detected by the receiver
+                                              //            they would appear as JAM bits which would appear as bad FCS
+                                              //            or carrier extend error which is CVMX_PIP_EXTEND_ERR
+
+
+
+} cvmx_pip_rcv_err_t;
+
+/**
+ * This defines the err_code field errors in the work Q entry
+ */
+typedef union
+{
+    cvmx_pip_l4_err_t  l4_err;
+    cvmx_pip_ip_exc_t  ip_exc;
+    cvmx_pip_rcv_err_t rcv_err;
+} cvmx_pip_err_t;
+
+
+/**
+ * Status statistics for a port
+ */
+typedef struct
+{
+    uint32_t    dropped_octets;         /**< Inbound octets marked to be dropped by the IPD */
+    uint32_t    dropped_packets;        /**< Inbound packets marked to be dropped by the IPD */
+    uint32_t    pci_raw_packets;        /**< RAW PCI Packets received by PIP per port */
+    uint32_t    octets;                 /**< Number of octets processed by PIP */
+    uint32_t    packets;                /**< Number of packets processed by PIP */
+    uint32_t    multicast_packets;      /**< Number of indentified L2 multicast packets.
+                                            Does not include broadcast packets.
+                                            Only includes packets whose parse mode is
+                                            SKIP_TO_L2 */
+    uint32_t    broadcast_packets;      /**< Number of indentified L2 broadcast packets.
+                                            Does not include multicast packets.
+                                            Only includes packets whose parse mode is
+                                            SKIP_TO_L2 */
+    uint32_t    len_64_packets;         /**< Number of 64B packets */
+    uint32_t    len_65_127_packets;     /**< Number of 65-127B packets */
+    uint32_t    len_128_255_packets;    /**< Number of 128-255B packets */
+    uint32_t    len_256_511_packets;    /**< Number of 256-511B packets */
+    uint32_t    len_512_1023_packets;   /**< Number of 512-1023B packets */
+    uint32_t    len_1024_1518_packets;  /**< Number of 1024-1518B packets */
+    uint32_t    len_1519_max_packets;   /**< Number of 1519-max packets */
+    uint32_t    fcs_align_err_packets;  /**< Number of packets with FCS or Align opcode errors */
+    uint32_t    runt_packets;           /**< Number of packets with length < min */
+    uint32_t    runt_crc_packets;       /**< Number of packets with length < min and FCS error */
+    uint32_t    oversize_packets;       /**< Number of packets with length > max */
+    uint32_t    oversize_crc_packets;   /**< Number of packets with length > max and FCS error */
+    uint32_t    inb_packets;            /**< Number of packets without GMX/SPX/PCI errors received by PIP */
+    uint64_t    inb_octets;             /**< Total number of octets from all packets received by PIP, including CRC */
+    uint16_t    inb_errors;             /**< Number of packets with GMX/SPX/PCI errors received by PIP */
+} cvmx_pip_port_status_t;
+
+
+/**
+ * Definition of the PIP custom header that can be prepended
+ * to a packet by external hardware.
+ */
+typedef union
+{
+    uint64_t    u64;
+    struct
+    {
+        uint64_t                    rawfull     : 1;    /**< Documented as R - Set if the Packet is RAWFULL. If set,
+                                                            this header must be the full 8 bytes */
+        uint64_t                    reserved0   : 5;    /**< Must be zero */
+        cvmx_pip_port_parse_mode_t  parse_mode  : 2;    /**< PIP parse mode for this packet */
+        uint64_t                    reserved1   : 1;    /**< Must be zero */
+        uint64_t                    skip_len    : 7;    /**< Skip amount, including this header, to the beginning of the packet */
+        uint64_t                    reserved2   : 6;    /**< Must be zero */
+        uint64_t                    qos         : 3;    /**< POW input queue for this packet */
+        uint64_t                    grp         : 4;    /**< POW input group for this packet */
+        uint64_t                    rs          : 1;    /**< Flag to store this packet in the work queue entry, if possible */
+        cvmx_pow_tag_type_t         tag_type    : 2;    /**< POW input tag type */
+        uint64_t                    tag         : 32;   /**< POW input tag */
+    } s;
+} cvmx_pip_pkt_inst_hdr_t;
+
+/* CSR typedefs have been moved to cvmx-csr-*.h */
+
+/**
+ * Configure an ethernet input port
+ *
+ * @param port_num Port number to configure
+ * @param port_cfg Port hardware configuration
+ * @param port_tag_cfg
+ *                 Port POW tagging configuration
+ */
+static inline void cvmx_pip_config_port(uint64_t port_num,
+                                        cvmx_pip_port_cfg_t port_cfg,
+                                        cvmx_pip_port_tag_cfg_t port_tag_cfg)
+{
+    cvmx_write_csr(CVMX_PIP_PRT_CFGX(port_num), port_cfg.u64);
+    cvmx_write_csr(CVMX_PIP_PRT_TAGX(port_num), port_tag_cfg.u64);
+}
+
+
+/**
+ * @deprecated      This function is a thin wrapper around the Pass1 version
+ *                  of the CVMX_PIP_QOS_WATCHX CSR; Pass2 has added a field for
+ *                  setting the group that is incompatible with this function,
+ *                  the preferred upgrade path is to use the CSR directly.
+ *
+ * Configure the global QoS packet watchers. Each watcher is
+ * capable of matching a field in a packet to determine the
+ * QoS queue for scheduling.
+ *
+ * @param watcher    Watcher number to configure (0 - 3).
+ * @param match_type Watcher match type
+ * @param match_value
+ *                   Value the watcher will match against
+ * @param qos        QoS queue for packets matching this watcher
+ */
+static inline void cvmx_pip_config_watcher(uint64_t watcher,
+                                           cvmx_pip_qos_watch_types match_type,
+                                           uint64_t match_value, uint64_t qos)
+{
+    cvmx_pip_port_watcher_cfg_t watcher_config;
+
+    watcher_config.u64 = 0;
+    watcher_config.s.match_type = match_type;
+    watcher_config.s.match_value = match_value;
+    watcher_config.s.qos = qos;
+
+    cvmx_write_csr(CVMX_PIP_QOS_WATCHX(watcher), watcher_config.u64);
+}
+
+
+/**
+ * Configure the VLAN priority to QoS queue mapping.
+ *
+ * @param vlan_priority
+ *               VLAN priority (0-7)
+ * @param qos    QoS queue for packets matching this watcher
+ */
+static inline void cvmx_pip_config_vlan_qos(uint64_t vlan_priority, uint64_t qos)
+{
+    cvmx_pip_qos_vlanx_t pip_qos_vlanx;
+    pip_qos_vlanx.u64 = 0;
+    pip_qos_vlanx.s.qos = qos;
+    cvmx_write_csr(CVMX_PIP_QOS_VLANX(vlan_priority), pip_qos_vlanx.u64);
+}
+
+
+/**
+ * Configure the Diffserv to QoS queue mapping.
+ *
+ * @param diffserv Diffserv field value (0-63)
+ * @param qos      QoS queue for packets matching this watcher
+ */
+static inline void cvmx_pip_config_diffserv_qos(uint64_t diffserv, uint64_t qos)
+{
+    cvmx_pip_qos_diffx_t pip_qos_diffx;
+    pip_qos_diffx.u64 = 0;
+    pip_qos_diffx.s.qos = qos;
+    cvmx_write_csr(CVMX_PIP_QOS_DIFFX(diffserv), pip_qos_diffx.u64);
+}
+
+
+/**
+ * Get the status counters for a port.
+ *
+ * @param port_num Port number to get statistics for.
+ * @param clear    Set to 1 to clear the counters after they are read
+ * @param status   Where to put the results.
+ */
+static inline void cvmx_pip_get_port_status(uint64_t port_num, uint64_t clear, cvmx_pip_port_status_t *status)
+{
+    cvmx_pip_stat_ctl_t pip_stat_ctl;
+    cvmx_pip_stat0_prtx_t stat0;
+    cvmx_pip_stat1_prtx_t stat1;
+    cvmx_pip_stat2_prtx_t stat2;
+    cvmx_pip_stat3_prtx_t stat3;
+    cvmx_pip_stat4_prtx_t stat4;
+    cvmx_pip_stat5_prtx_t stat5;
+    cvmx_pip_stat6_prtx_t stat6;
+    cvmx_pip_stat7_prtx_t stat7;
+    cvmx_pip_stat8_prtx_t stat8;
+    cvmx_pip_stat9_prtx_t stat9;
+    cvmx_pip_stat_inb_pktsx_t pip_stat_inb_pktsx;
+    cvmx_pip_stat_inb_octsx_t pip_stat_inb_octsx;
+    cvmx_pip_stat_inb_errsx_t pip_stat_inb_errsx;
+
+    pip_stat_ctl.u64 = 0;
+    pip_stat_ctl.s.rdclr = clear;
+    cvmx_write_csr(CVMX_PIP_STAT_CTL, pip_stat_ctl.u64);
+
+    stat0.u64 = cvmx_read_csr(CVMX_PIP_STAT0_PRTX(port_num));
+    stat1.u64 = cvmx_read_csr(CVMX_PIP_STAT1_PRTX(port_num));
+    stat2.u64 = cvmx_read_csr(CVMX_PIP_STAT2_PRTX(port_num));
+    stat3.u64 = cvmx_read_csr(CVMX_PIP_STAT3_PRTX(port_num));
+    stat4.u64 = cvmx_read_csr(CVMX_PIP_STAT4_PRTX(port_num));
+    stat5.u64 = cvmx_read_csr(CVMX_PIP_STAT5_PRTX(port_num));
+    stat6.u64 = cvmx_read_csr(CVMX_PIP_STAT6_PRTX(port_num));
+    stat7.u64 = cvmx_read_csr(CVMX_PIP_STAT7_PRTX(port_num));
+    stat8.u64 = cvmx_read_csr(CVMX_PIP_STAT8_PRTX(port_num));
+    stat9.u64 = cvmx_read_csr(CVMX_PIP_STAT9_PRTX(port_num));
+    pip_stat_inb_pktsx.u64 = cvmx_read_csr(CVMX_PIP_STAT_INB_PKTSX(port_num));
+    pip_stat_inb_octsx.u64 = cvmx_read_csr(CVMX_PIP_STAT_INB_OCTSX(port_num));
+    pip_stat_inb_errsx.u64 = cvmx_read_csr(CVMX_PIP_STAT_INB_ERRSX(port_num));
+
+    status->dropped_octets          = stat0.s.drp_octs;
+    status->dropped_packets         = stat0.s.drp_pkts;
+    status->octets                  = stat1.s.octs;
+    status->pci_raw_packets         = stat2.s.raw;
+    status->packets                 = stat2.s.pkts;
+    status->multicast_packets       = stat3.s.mcst;
+    status->broadcast_packets       = stat3.s.bcst;
+    status->len_64_packets          = stat4.s.h64;
+    status->len_65_127_packets      = stat4.s.h65to127;
+    status->len_128_255_packets     = stat5.s.h128to255;
+    status->len_256_511_packets     = stat5.s.h256to511;
+    status->len_512_1023_packets    = stat6.s.h512to1023;
+    status->len_1024_1518_packets   = stat6.s.h1024to1518;
+    status->len_1519_max_packets    = stat7.s.h1519;
+    status->fcs_align_err_packets   = stat7.s.fcs;
+    status->runt_packets            = stat8.s.undersz;
+    status->runt_crc_packets        = stat8.s.frag;
+    status->oversize_packets        = stat9.s.oversz;
+    status->oversize_crc_packets    = stat9.s.jabber;
+    status->inb_packets             = pip_stat_inb_pktsx.s.pkts;
+    status->inb_octets              = pip_stat_inb_octsx.s.octs;
+    status->inb_errors              = pip_stat_inb_errsx.s.errs;
+
+    if (cvmx_octeon_is_pass1())
+    {
+        /* Kludge to fix Octeon Pass 1 errata - Drop counts don't work */
+        if (status->inb_packets > status->packets)
+            status->dropped_packets = status->inb_packets - status->packets;
+        else
+            status->dropped_packets = 0;
+        if (status->inb_octets - status->inb_packets*4 > status->octets)
+            status->dropped_octets = status->inb_octets - status->inb_packets*4 - status->octets;
+        else
+            status->dropped_octets = 0;
+    }
+}
+
+
+/**
+ * Configure the hardware CRC engine
+ *
+ * @param interface Interface to configure (0 or 1)
+ * @param invert_result
+ *                 Invert the result of the CRC
+ * @param reflect  Reflect
+ * @param initialization_vector
+ *                 CRC initialization vector
+ */
+static inline void cvmx_pip_config_crc(uint64_t interface, uint64_t invert_result, uint64_t reflect, uint32_t initialization_vector)
+{
+    if ((OCTEON_IS_MODEL(OCTEON_CN38XX) || OCTEON_IS_MODEL(OCTEON_CN58XX)))
+    {
+        cvmx_pip_crc_ctlx_t config;
+        cvmx_pip_crc_ivx_t pip_crc_ivx;
+
+        config.u64 = 0;
+        config.s.invres = invert_result;
+        config.s.reflect = reflect;
+        cvmx_write_csr(CVMX_PIP_CRC_CTLX(interface), config.u64);
+
+        pip_crc_ivx.u64 = 0;
+        pip_crc_ivx.s.iv = initialization_vector;
+        cvmx_write_csr(CVMX_PIP_CRC_IVX(interface), pip_crc_ivx.u64);
+    }
+}
+
+
+/**
+ * Clear all bits in a tag mask. This should be called on
+ * startup before any calls to cvmx_pip_tag_mask_set. Each bit
+ * set in the final mask represent a byte used in the packet for
+ * tag generation.
+ *
+ * @param mask_index Which tag mask to clear (0..3)
+ */
+static inline void cvmx_pip_tag_mask_clear(uint64_t mask_index)
+{
+    uint64_t index;
+    cvmx_pip_tag_incx_t pip_tag_incx;
+    pip_tag_incx.u64 = 0;
+    pip_tag_incx.s.en = 0;
+    for (index=mask_index*16; index<(mask_index+1)*16; index++)
+        cvmx_write_csr(CVMX_PIP_TAG_INCX(index), pip_tag_incx.u64);
+}
+
+
+/**
+ * Sets a range of bits in the tag mask. The tag mask is used
+ * when the cvmx_pip_port_tag_cfg_t tag_mode is non zero.
+ * There are four separate masks that can be configured.
+ *
+ * @param mask_index Which tag mask to modify (0..3)
+ * @param offset     Offset into the bitmask to set bits at. Use the GCC macro
+ *                   offsetof() to determine the offsets into packet headers.
+ *                   For example, offsetof(ethhdr, protocol) returns the offset
+ *                   of the ethernet protocol field.  The bitmask selects which bytes
+ *                   to include the the tag, with bit offset X selecting byte at offset X
+ *                   from the beginning of the packet data.
+ * @param len        Number of bytes to include. Usually this is the sizeof()
+ *                   the field.
+ */
+static inline void cvmx_pip_tag_mask_set(uint64_t mask_index, uint64_t offset, uint64_t len)
+{
+    while (len--)
+    {
+        cvmx_pip_tag_incx_t pip_tag_incx;
+        uint64_t index = mask_index*16 + offset/8;
+        pip_tag_incx.u64 = cvmx_read_csr(CVMX_PIP_TAG_INCX(index));
+        pip_tag_incx.s.en |= 0x80 >> (offset & 0x7);
+        cvmx_write_csr(CVMX_PIP_TAG_INCX(index), pip_tag_incx.u64);
+        offset++;
+    }
+}
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif  /*  __CVMX_PIP_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pko.c b/arch/mips/cavium-octeon/executive/cvmx-pko.c
new file mode 100644
index 0000000..c6e2266
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-pko.c
@@ -0,0 +1,497 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Support library for the hardware Packet Output unit.
+ *
+ * <hr>$Revision: 34498 $<hr>
+ */
+#include "executive-config.h"
+#include "cvmx-config.h"
+#include "cvmx.h"
+#include "cvmx-pko.h"
+#include "cvmx-sysinfo.h"
+#include "cvmx-helper.h"
+
+/**
+ * Internal state of packet output
+ */
+
+#ifdef CVMX_ENABLE_PKO_FUNCTIONS
+
+/**
+ * Call before any other calls to initialize the packet
+ * output system.  This does chip global config, and should only be
+ * done by one core.
+ */
+
+void cvmx_pko_initialize_global(void)
+{
+    int i;
+    uint64_t priority = 8;
+    cvmx_pko_pool_cfg_t config;
+
+    /* Set the size of the PKO command buffers to an odd number of 64bit
+        words. This allows the normal two word send to stay aligned and never
+        span a comamnd word buffer. */
+    config.u64 = 0;
+    config.s.pool = CVMX_FPA_OUTPUT_BUFFER_POOL;
+    config.s.size = CVMX_FPA_OUTPUT_BUFFER_POOL_SIZE / 8 - 1;
+
+    cvmx_write_csr(CVMX_PKO_REG_CMD_BUF, config.u64);
+
+    for (i=0; i<CVMX_PKO_MAX_OUTPUT_QUEUES; i++)
+        cvmx_pko_config_port(CVMX_PKO_MEM_QUEUE_PTRS_ILLEGAL_PID, i, 1, &priority);
+
+    /* If we aren't using all of the queues optimize PKO's internal memory */
+    if (OCTEON_IS_MODEL(OCTEON_CN38XX) || OCTEON_IS_MODEL(OCTEON_CN58XX) || OCTEON_IS_MODEL(OCTEON_CN56XX))
+    {
+        int num_interfaces = cvmx_helper_get_number_of_interfaces();
+        int last_port = cvmx_helper_get_last_ipd_port(num_interfaces-1);
+        int max_queues = cvmx_pko_get_base_queue(last_port) + cvmx_pko_get_num_queues(last_port);
+        if (OCTEON_IS_MODEL(OCTEON_CN38XX))
+        {
+            if (max_queues <= 32)
+                cvmx_write_csr(CVMX_PKO_REG_QUEUE_MODE, 2);
+            else if (max_queues <= 64)
+                cvmx_write_csr(CVMX_PKO_REG_QUEUE_MODE, 1);
+        }
+        else
+        {
+            if (max_queues <= 64)
+                cvmx_write_csr(CVMX_PKO_REG_QUEUE_MODE, 2);
+            else if (max_queues <= 128)
+                cvmx_write_csr(CVMX_PKO_REG_QUEUE_MODE, 1);
+        }
+    }
+}
+
+/**
+ * This function does per-core initialization required by the PKO routines.
+ * This must be called on all cores that will do packet output, and must
+ * be called after the FPA has been initialized and filled with pages.
+ *
+ * @return 0 on success
+ *         !0 on failure
+ */
+int cvmx_pko_initialize_local(void)
+{
+#if CVMX_PKO_USE_FAU_FOR_OUTPUT_QUEUES
+    /* Preallocate a command buffer into the designated scratch memory location */
+    cvmx_fpa_async_alloc(CVMX_SCR_OQ_BUF_PRE_ALLOC, CVMX_FPA_OUTPUT_BUFFER_POOL);
+    CVMX_SYNCIOBDMA;
+    return(!cvmx_scratch_read64(CVMX_SCR_OQ_BUF_PRE_ALLOC));
+#else
+    /* Nothing to do */
+    return 0;
+#endif
+}
+#endif
+
+/**
+ * Enables the packet output hardware. It must already be
+ * configured.
+ */
+void cvmx_pko_enable(void)
+{
+    cvmx_pko_reg_flags_t flags;
+
+    flags.u64 = cvmx_read_csr(CVMX_PKO_REG_FLAGS);
+    if (flags.s.ena_pko)
+        cvmx_dprintf("Warning: Enabling PKO when PKO already enabled.\n");
+
+    flags.s.ena_dwb = 1;
+    flags.s.ena_pko = 1;
+    flags.s.store_be =1;  /* always enable big endian for 3-word command. Does nothing for 2-word */ 
+    cvmx_write_csr(CVMX_PKO_REG_FLAGS, flags.u64);
+}
+
+
+/**
+ * Disables the packet output. Does not affect any configuration.
+ */
+void cvmx_pko_disable(void)
+{
+    cvmx_pko_reg_flags_t pko_reg_flags;
+    pko_reg_flags.u64 = cvmx_read_csr(CVMX_PKO_REG_FLAGS);
+    pko_reg_flags.s.ena_pko = 0;
+    cvmx_write_csr(CVMX_PKO_REG_FLAGS, pko_reg_flags.u64);
+}
+
+
+#ifdef CVMX_ENABLE_PKO_FUNCTIONS
+/**
+ * @INTERNAL
+ * Reset the packet output.
+ */
+static void __cvmx_pko_reset(void)
+{
+    cvmx_pko_reg_flags_t pko_reg_flags;
+    pko_reg_flags.u64 = cvmx_read_csr(CVMX_PKO_REG_FLAGS);
+    pko_reg_flags.s.reset = 1;
+    cvmx_write_csr(CVMX_PKO_REG_FLAGS, pko_reg_flags.u64);
+}
+
+
+#if CVMX_PKO_USE_FAU_FOR_OUTPUT_QUEUES
+static CVMX_SHARED uint8_t cvmx_pko_queue_map[CVMX_PKO_MAX_OUTPUT_QUEUES_STATIC];  /* Used to track queue usage for shutdown */
+#endif
+/**
+ * Shutdown and free resources required by packet output.
+ */
+void cvmx_pko_shutdown(void)
+{
+    cvmx_pko_queue_cfg_t config;
+    int queue;
+
+    cvmx_pko_disable();
+
+    for (queue=0; queue<CVMX_PKO_MAX_OUTPUT_QUEUES; queue++)
+    {
+        config.u64          = 0;
+        config.s.tail       = 1;
+        config.s.index      = 0;
+        config.s.port       = CVMX_PKO_MEM_QUEUE_PTRS_ILLEGAL_PID;
+        config.s.queue      = queue & 0x7f;
+        config.s.qos_mask   = 0;
+        config.s.buf_ptr    = 0;
+        if (!OCTEON_IS_MODEL(OCTEON_CN3XXX))
+        {
+            cvmx_pko_reg_queue_ptrs1_t config1;
+            config1.u64 = 0;
+            config1.s.qid7 = queue >> 7;
+            cvmx_write_csr(CVMX_PKO_REG_QUEUE_PTRS1, config1.u64);
+        }
+        cvmx_write_csr(CVMX_PKO_MEM_QUEUE_PTRS, config.u64);
+#if CVMX_PKO_USE_FAU_FOR_OUTPUT_QUEUES
+        if(cvmx_pko_queue_map[queue])
+        {
+            uint64_t buf_addr = cvmx_pko_update_queue_index(queue, 0) >> CVMX_PKO_INDEX_BITS;
+            if (buf_addr)
+            {
+                cvmx_fpa_free(cvmx_phys_to_ptr(buf_addr), CVMX_FPA_OUTPUT_BUFFER_POOL, 0);
+            }
+        }
+#else
+        cvmx_cmd_queue_shutdown(CVMX_CMD_QUEUE_PKO(queue));
+#endif
+    }
+    __cvmx_pko_reset();
+}
+
+
+/**
+ * Configure a output port and the associated queues for use.
+ *
+ * @param port       Port to configure.
+ * @param base_queue First queue number to associate with this port.
+ * @param num_queues Number of queues to associate with this port
+ * @param priority   Array of priority levels for each queue. Values are
+ *                   allowed to be 0-8. A value of 8 get 8 times the traffic
+ *                   of a value of 1.  A value of 0 indicates that no rounds
+ *                   will be participated in. These priorities can be changed
+ *                   on the fly while the pko is enabled. A priority of 9
+ *                   indicates that static priority should be used.  If static
+ *                   priority is used all queues with static priority must be
+ *                   contiguous starting at the base_queue, and lower numbered
+ *                   queues have higher priority than higher numbered queues.
+ *                   There must be num_queues elements in the array.
+ */
+cvmx_pko_status_t cvmx_pko_config_port(uint64_t port, uint64_t base_queue, uint64_t num_queues, const uint64_t priority[])
+{
+    cvmx_pko_status_t          result_code;
+    uint64_t                   queue;
+    cvmx_pko_queue_cfg_t       config;
+    cvmx_pko_reg_queue_ptrs1_t config1;
+    int static_priority_base = -1;
+    int static_priority_end = -1;
+
+
+    if ((port >= CVMX_PKO_NUM_OUTPUT_PORTS) && (port != CVMX_PKO_MEM_QUEUE_PTRS_ILLEGAL_PID))
+    {
+        cvmx_dprintf("ERROR: cvmx_pko_config_port: Invalid port %llu\n", (unsigned long long)port);
+        return CVMX_PKO_INVALID_PORT;
+    }
+
+    if (base_queue + num_queues > CVMX_PKO_MAX_OUTPUT_QUEUES)
+    {
+        cvmx_dprintf("ERROR: cvmx_pko_config_port: Invalid queue range %llu\n", (unsigned long long)(base_queue + num_queues));
+        return CVMX_PKO_INVALID_QUEUE;
+    }
+
+    if (port != CVMX_PKO_MEM_QUEUE_PTRS_ILLEGAL_PID)
+    {
+        /* Validate the static queue priority setup and set static_priority_base and static_priority_end
+        ** accordingly. */
+        for (queue = 0; queue < num_queues; queue++)
+        {
+            /* Find first queue of static priority */
+            if (static_priority_base == -1 && priority[queue] == CVMX_PKO_QUEUE_STATIC_PRIORITY)
+                static_priority_base = queue;
+            /* Find last queue of static priority */
+            if (static_priority_base != -1 && static_priority_end == -1 && priority[queue] != CVMX_PKO_QUEUE_STATIC_PRIORITY && queue)
+                static_priority_end = queue - 1;
+            else if (static_priority_base != -1 && static_priority_end == -1 && queue == num_queues - 1)
+                static_priority_end = queue;  /* all queues are static priority */
+            /* Check to make sure all static priority queues are contiguous.  Also catches some cases of
+            ** static priorites not starting at queue 0. */
+            if (static_priority_end != -1 && (int)queue > static_priority_end && priority[queue] == CVMX_PKO_QUEUE_STATIC_PRIORITY)
+            {
+                cvmx_dprintf("ERROR: cvmx_pko_config_port: Static priority queues aren't contiguous or don't start at base queue. q: %d, eq: %d\n", (int)queue, static_priority_end);
+                return CVMX_PKO_INVALID_PRIORITY;
+            }
+        }
+        if (static_priority_base > 0)
+        {
+            cvmx_dprintf("ERROR: cvmx_pko_config_port: Static priority queues don't start at base queue. sq: %d\n", static_priority_base);
+            return CVMX_PKO_INVALID_PRIORITY;
+        }
+#if 0
+        cvmx_dprintf("Port %d: Static priority queue base: %d, end: %d\n", port, static_priority_base, static_priority_end);
+#endif
+    }
+    /* At this point, static_priority_base and static_priority_end are either both -1,
+    ** or are valid start/end queue numbers */
+
+    result_code = CVMX_PKO_SUCCESS;
+
+#ifdef PKO_DEBUG
+    cvmx_dprintf("num queues: %d (%lld,%lld)\n", num_queues, CVMX_PKO_QUEUES_PER_PORT_INTERFACE0, CVMX_PKO_QUEUES_PER_PORT_INTERFACE1);
+#endif
+
+    for (queue = 0; queue < num_queues; queue++)
+    {
+        uint64_t  *buf_ptr = NULL;
+
+        config1.u64         = 0;
+        config1.s.idx3      = queue >> 3;
+        config1.s.qid7      = (base_queue + queue) >> 7;
+
+        config.u64          = 0;
+        config.s.tail       = queue == (num_queues - 1);
+        config.s.index      = queue;
+        config.s.port       = port;
+        config.s.queue      = base_queue + queue;
+
+        if (!cvmx_octeon_is_pass1())
+        {
+            config.s.static_p   = static_priority_base >= 0;
+            config.s.static_q   = (int)queue <= static_priority_end;
+            config.s.s_tail     = (int)queue == static_priority_end;
+        }
+        /* Convert the priority into an enable bit field. Try to space the bits
+            out evenly so the packet don't get grouped up */
+        switch ((int)priority[queue])
+        {
+            case 0: config.s.qos_mask = 0x00; break;
+            case 1: config.s.qos_mask = 0x01; break;
+            case 2: config.s.qos_mask = 0x11; break;
+            case 3: config.s.qos_mask = 0x49; break;
+            case 4: config.s.qos_mask = 0x55; break;
+            case 5: config.s.qos_mask = 0x57; break;
+            case 6: config.s.qos_mask = 0x77; break;
+            case 7: config.s.qos_mask = 0x7f; break;
+            case 8: config.s.qos_mask = 0xff; break;
+            case CVMX_PKO_QUEUE_STATIC_PRIORITY:
+                if (!cvmx_octeon_is_pass1()) /* Pass 1 will fall through to the error case */
+                {
+                    config.s.qos_mask = 0xff;
+                    break;
+                }
+            default:
+                cvmx_dprintf("ERROR: cvmx_pko_config_port: Invalid priority %llu\n", (unsigned long long)priority[queue]);
+                config.s.qos_mask = 0xff;
+                result_code = CVMX_PKO_INVALID_PRIORITY;
+                break;
+        }
+
+        if (port != CVMX_PKO_MEM_QUEUE_PTRS_ILLEGAL_PID)
+        {
+#if CVMX_PKO_USE_FAU_FOR_OUTPUT_QUEUES
+            buf_ptr = (uint64_t*)cvmx_fpa_alloc(CVMX_FPA_OUTPUT_BUFFER_POOL);
+            if (!buf_ptr)
+            {
+                cvmx_dprintf("ERROR: cvmx_pko_config_port: Unable to allocate output buffer.\n");
+                return(CVMX_PKO_NO_MEMORY);
+            }
+
+            /* Set initial command buffer address and index in FAU register for queue */
+            cvmx_pko_set_queue_index(base_queue + queue, cvmx_ptr_to_phys(buf_ptr) << CVMX_PKO_INDEX_BITS);
+            cvmx_pko_queue_map[base_queue + queue] = 1;
+#else
+            cvmx_cmd_queue_result_t cmd_res = cvmx_cmd_queue_initialize(CVMX_CMD_QUEUE_PKO(base_queue + queue),
+                                                                        CVMX_PKO_MAX_QUEUE_DEPTH,
+                                                                        CVMX_FPA_OUTPUT_BUFFER_POOL,
+                                                                        CVMX_FPA_OUTPUT_BUFFER_POOL_SIZE - CVMX_PKO_COMMAND_BUFFER_SIZE_ADJUST*8);
+            if (cmd_res != CVMX_CMD_QUEUE_SUCCESS)
+            {
+                cvmx_dprintf("ERROR: cvmx_pko_config_port: Unable to allocate output buffer.\n");
+                return(CVMX_PKO_NO_MEMORY);
+            }
+
+            buf_ptr = (uint64_t*)cvmx_cmd_queue_buffer(CVMX_CMD_QUEUE_PKO(base_queue + queue));
+#endif
+            config.s.buf_ptr = cvmx_ptr_to_phys(buf_ptr);
+        }
+        else
+            config.s.buf_ptr = 0;
+
+        CVMX_SYNCWS;
+
+        if (!OCTEON_IS_MODEL(OCTEON_CN3XXX))
+        {
+            cvmx_write_csr(CVMX_PKO_REG_QUEUE_PTRS1, config1.u64);
+        }
+        cvmx_write_csr(CVMX_PKO_MEM_QUEUE_PTRS, config.u64);
+    }
+
+    return result_code;
+}
+
+#ifdef PKO_DEBUG
+/**
+ * Show map of ports -> queues for different cores.
+ */
+void cvmx_pko_show_queue_map()
+{
+    int core, port;
+    int pko_output_ports = 36;
+
+    cvmx_dprintf("port");
+    for(port=0; port<pko_output_ports; port++)
+        cvmx_dprintf("%3d ", port);
+    cvmx_dprintf("\n");
+
+    for(core=0; core<CVMX_MAX_CORES; core++)
+    {
+        cvmx_dprintf("\n%2d: ", core);
+        for(port=0; port<pko_output_ports; port++)
+        {
+            cvmx_dprintf("%3d ", cvmx_pko_get_base_queue_per_core(port, core));
+        }
+    }
+    cvmx_dprintf("\n");
+}
+#endif
+
+
+/**
+ * Rate limit a PKO port to a max packets/sec. This function is only
+ * supported on CN57XX, CN56XX, CN55XX, and CN54XX.
+ *
+ * @param port      Port to rate limit
+ * @param packets_s Maximum packet/sec
+ * @param burst     Maximum number of packets to burst in a row before rate
+ *                  limiting cuts in.
+ *
+ * @return Zero on success, negative on failure
+ */
+int cvmx_pko_rate_limit_packets(int port, int packets_s, int burst)
+{
+    cvmx_pko_mem_port_rate0_t pko_mem_port_rate0;
+    cvmx_pko_mem_port_rate1_t pko_mem_port_rate1;
+
+    pko_mem_port_rate0.u64 = 0;
+    pko_mem_port_rate0.s.pid = port;
+    pko_mem_port_rate0.s.rate_pkt = cvmx_sysinfo_get()->cpu_clock_hz / packets_s / 16;
+    /* No cost per word since we are limited by packets/sec, not bits/sec */
+    pko_mem_port_rate0.s.rate_word = 0;
+
+    pko_mem_port_rate1.u64 = 0;
+    pko_mem_port_rate1.s.pid = port;
+    pko_mem_port_rate1.s.rate_lim = ((uint64_t)pko_mem_port_rate0.s.rate_pkt * burst) >> 8;
+
+    cvmx_write_csr(CVMX_PKO_MEM_PORT_RATE0, pko_mem_port_rate0.u64);
+    cvmx_write_csr(CVMX_PKO_MEM_PORT_RATE1, pko_mem_port_rate1.u64);
+    return 0;
+}
+
+
+/**
+ * Rate limit a PKO port to a max bits/sec. This function is only
+ * supported on CN57XX, CN56XX, CN55XX, and CN54XX.
+ *
+ * @param port   Port to rate limit
+ * @param bits_s PKO rate limit in bits/sec
+ * @param burst  Maximum number of bits to burst before rate
+ *               limiting cuts in.
+ *
+ * @return Zero on success, negative on failure
+ */
+int cvmx_pko_rate_limit_bits(int port, uint64_t bits_s, int burst)
+{
+    cvmx_pko_mem_port_rate0_t pko_mem_port_rate0;
+    cvmx_pko_mem_port_rate1_t pko_mem_port_rate1;
+    uint64_t clock_rate = cvmx_sysinfo_get()->cpu_clock_hz;
+    uint64_t tokens_per_bit = clock_rate*16 / bits_s;
+
+    pko_mem_port_rate0.u64 = 0;
+    pko_mem_port_rate0.s.pid = port;
+    /* Each packet has a 12 bytes of interframe gap, an 8 byte preamble, and a
+        4 byte CRC. These are not included in the per word count. Multiply
+        by 8 to covert to bits and divide by 256 for limit granularity */
+    pko_mem_port_rate0.s.rate_pkt = (12 + 8 + 4) * 8 * tokens_per_bit / 256;
+    /* Each 8 byte word has 64bits */
+    pko_mem_port_rate0.s.rate_word = 64 * tokens_per_bit;
+
+    pko_mem_port_rate1.u64 = 0;
+    pko_mem_port_rate1.s.pid = port;
+    pko_mem_port_rate1.s.rate_lim = tokens_per_bit * burst / 256;
+
+    cvmx_write_csr(CVMX_PKO_MEM_PORT_RATE0, pko_mem_port_rate0.u64);
+    cvmx_write_csr(CVMX_PKO_MEM_PORT_RATE1, pko_mem_port_rate1.u64);
+    return 0;
+}
+
+#endif /* CVMX_ENABLE_PKO_FUNCTIONS */
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pko.h b/arch/mips/cavium-octeon/executive/cvmx-pko.h
new file mode 100644
index 0000000..e869586
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-pko.h
@@ -0,0 +1,755 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Interface to the hardware Packet Output unit.
+ *
+ * Starting with SDK 1.7.0, the PKO output functions now support
+ * two types of locking. CVMX_PKO_LOCK_ATOMIC_TAG continues to
+ * function similarly to previous SDKs by using POW atomic tags
+ * to preserve ordering and exclusivity. As a new option, you
+ * can now pass CVMX_PKO_LOCK_CMD_QUEUE which uses a ll/sc
+ * memory based locking instead. This locking has the advantage
+ * of not affecting the tag state but doesn't preserve packet
+ * ordering. CVMX_PKO_LOCK_CMD_QUEUE is appropriate in most
+ * generic code while CVMX_PKO_LOCK_CMD_QUEUE should be used
+ * with hand tuned fast path code.
+ *
+ * Some of other SDK differences visible to the command command
+ * queuing:
+ * - PKO indexes are no longer stored in the FAU. A large
+ *   percentage of the FAU register block used to be tied up
+ *   maintaining PKO queue pointers. These are now stored in a
+ *   global named block.
+ * - The PKO <b>use_locking</b> parameter can now have a global
+ *   effect. Since all application use the same named block,
+ *   queue locking correctly applies across all operating
+ *   systems when using CVMX_PKO_LOCK_CMD_QUEUE.
+ * - PKO 3 word commands are now supported. Use
+ *   cvmx_pko_send_packet_finish3().
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+
+
+#ifndef __CVMX_PKO_H__
+#define __CVMX_PKO_H__
+
+#ifndef CVMX_DONT_INCLUDE_CONFIG
+#include "executive-config.h"
+#ifdef CVMX_ENABLE_PKO_FUNCTIONS
+#include "cvmx-config.h"
+#endif
+#endif
+
+#include "cvmx-cvmmem.h"
+#include "cvmx-fau.h"
+#include "cvmx-fpa.h"
+#include "cvmx-pow.h"
+#include "cvmx-cmd-queue.h"
+
+/* Adjust the command buffer size by 1 word so that in the case of using only
+** two word PKO commands no command words stradle buffers.  The useful values
+** for this are 0 and 1. */
+#define CVMX_PKO_COMMAND_BUFFER_SIZE_ADJUST (1)
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#define CVMX_PKO_MAX_OUTPUT_QUEUES_STATIC 256
+#define CVMX_PKO_MAX_OUTPUT_QUEUES      ((OCTEON_IS_MODEL(OCTEON_CN31XX) || OCTEON_IS_MODEL(OCTEON_CN3010) || OCTEON_IS_MODEL(OCTEON_CN3005) || OCTEON_IS_MODEL(OCTEON_CN50XX)) ? 32 : (OCTEON_IS_MODEL(OCTEON_CN58XX) || OCTEON_IS_MODEL(OCTEON_CN56XX)) ? 256 : 128)
+#define CVMX_PKO_NUM_OUTPUT_PORTS       40
+#define CVMX_PKO_MEM_QUEUE_PTRS_ILLEGAL_PID 63 // use this for queues that are not used
+#define CVMX_PKO_QUEUE_STATIC_PRIORITY  9
+#define CVMX_PKO_ILLEGAL_QUEUE  0xFFFF
+#if CVMX_PKO_USE_FAU_FOR_OUTPUT_QUEUES
+#define CVMX_PKO_INDEX_BITS     12
+#define CVMX_PKO_INDEX_MASK     ((1ull << CVMX_PKO_INDEX_BITS) - 1)
+#else
+#define CVMX_PKO_MAX_QUEUE_DEPTH 0
+#endif
+
+typedef enum
+{
+    CVMX_PKO_SUCCESS,
+    CVMX_PKO_INVALID_PORT,
+    CVMX_PKO_INVALID_QUEUE,
+    CVMX_PKO_INVALID_PRIORITY,
+    CVMX_PKO_NO_MEMORY
+} cvmx_pko_status_t;
+
+/**
+ * This enumeration represents the differnet locking modes supported by PKO.
+ */
+typedef enum
+{
+    CVMX_PKO_LOCK_NONE = 0,         /**< PKO doesn't do any locking. It is the responsibility
+                                        of the application to make sure that no other core is
+                                        accessing the same queue at the smae time */
+    CVMX_PKO_LOCK_ATOMIC_TAG = 1,   /**< PKO performs an atomic tagswitch to insure exclusive
+                                        access to the output queue. This will maintain
+                                        packet ordering on output */
+#if CVMX_PKO_USE_FAU_FOR_OUTPUT_QUEUES == 0
+    CVMX_PKO_LOCK_CMD_QUEUE = 2,    /**< PKO uses the common command queue locks to insure
+                                        exclusive access to the output queue. This is a memory
+                                        based ll/sc. This is the most portable locking
+                                        mechanism */
+#endif
+} cvmx_pko_lock_t;
+
+typedef struct
+{
+    uint32_t    packets;
+    uint64_t    octets;
+  uint64_t doorbell;
+} cvmx_pko_port_status_t;
+
+/**
+ * This structure defines the address to use on a packet enqueue
+ */
+typedef union
+{
+    uint64_t                u64;
+    struct
+    {
+        cvmx_mips_space_t   mem_space   : 2;    /**< Must CVMX_IO_SEG */
+        uint64_t            reserved    :13;    /**< Must be zero */
+        uint64_t            is_io       : 1;    /**< Must be one */
+        uint64_t            did         : 8;    /**< The ID of the device on the non-coherent bus */
+        uint64_t            reserved2   : 4;    /**< Must be zero */
+        uint64_t            reserved3   :18;    /**< Must be zero */
+        uint64_t            port        : 6;    /**< The hardware likes to have the output port in addition to the output queue */
+        uint64_t            queue       : 9;    /**< The output queue to send the packet to (0-127 are legal) */
+        uint64_t            reserved4   : 3;    /**< Must be zero */
+   } s;
+} cvmx_pko_doorbell_address_t;
+
+/**
+ * Structure of the first packet output command word.
+ */
+typedef union
+{
+    uint64_t                u64;
+    struct
+    {
+        cvmx_fau_op_size_t  size1       : 2; /**< The size of the reg1 operation - could be 8, 16, 32, or 64 bits */
+        cvmx_fau_op_size_t  size0       : 2; /**< The size of the reg0 operation - could be 8, 16, 32, or 64 bits */
+        uint64_t            subone1     : 1; /**< If set, subtract 1, if clear, subtract packet size */
+        uint64_t            reg1        :11; /**< The register, subtract will be done if reg1 is non-zero */
+        uint64_t            subone0     : 1; /**< If set, subtract 1, if clear, subtract packet size */
+        uint64_t            reg0        :11; /**< The register, subtract will be done if reg0 is non-zero */
+        uint64_t            le          : 1; /**< When set, interpret segment pointer and segment bytes in little endian order */
+        uint64_t            n2          : 1; /**< When set, packet data not allocated in L2 cache by PKO */
+        uint64_t            wqp         : 1; /**< If set and rsp is set, word3 contains a pointer to a work queue entry */
+        uint64_t            rsp         : 1; /**< If set, the hardware will send a response when done */
+        uint64_t            gather      : 1; /**< If set, the supplied pkt_ptr is really a pointer to a list of pkt_ptr's */
+        uint64_t            ipoffp1     : 7; /**< If ipoffp1 is non zero, (ipoffp1-1) is the number of bytes to IP header,
+                                                and the hardware will calculate and insert the  UDP/TCP checksum */
+        uint64_t            ignore_i    : 1; /**< If set, ignore the I bit (force to zero) from all pointer structures */
+        uint64_t            dontfree    : 1; /**< If clear, the hardware will attempt to free the buffers containing the packet */
+        uint64_t            segs        : 6; /**< The total number of segs in the packet, if gather set, also gather list length */
+        uint64_t            total_bytes :16; /**< Including L2, but no trailing CRC */
+    } s;
+} cvmx_pko_command_word0_t;
+
+/* CSR typedefs have been moved to cvmx-csr-*.h */
+
+/**
+ * Definition of internal state for Packet output processing
+ */
+typedef struct
+{
+    uint64_t *      start_ptr;          /**< ptr to start of buffer, offset kept in FAU reg */
+} cvmx_pko_state_elem_t;
+
+
+#ifdef CVMX_ENABLE_PKO_FUNCTIONS
+/**
+ * Call before any other calls to initialize the packet
+ * output system.
+ */
+extern void cvmx_pko_initialize_global(void);
+extern int cvmx_pko_initialize_local(void);
+
+#endif
+
+
+/**
+ * Enables the packet output hardware. It must already be
+ * configured.
+ */
+extern void cvmx_pko_enable(void);
+
+
+/**
+ * Disables the packet output. Does not affect any configuration.
+ */
+extern void cvmx_pko_disable(void);
+
+
+/**
+ * Shutdown and free resources required by packet output.
+ */
+
+#ifdef CVMX_ENABLE_PKO_FUNCTIONS
+extern void cvmx_pko_shutdown(void);
+#endif
+
+/**
+ * Configure a output port and the associated queues for use.
+ *
+ * @param port       Port to configure.
+ * @param base_queue First queue number to associate with this port.
+ * @param num_queues Number of queues t oassociate with this port
+ * @param priority   Array of priority levels for each queue. Values are
+ *                   allowed to be 1-8. A value of 8 get 8 times the traffic
+ *                   of a value of 1. There must be num_queues elements in the
+ *                   array.
+ */
+extern cvmx_pko_status_t cvmx_pko_config_port(uint64_t port, uint64_t base_queue, uint64_t num_queues, const uint64_t priority[]);
+
+
+/**
+ * Ring the packet output doorbell. This tells the packet
+ * output hardware that "len" command words have been added
+ * to its pending list.  This command includes the required
+ * CVMX_SYNCWS before the doorbell ring.
+ *
+ * @param port   Port the packet is for
+ * @param queue  Queue the packet is for
+ * @param len    Length of the command in 64 bit words
+ */
+static inline void cvmx_pko_doorbell(uint64_t port, uint64_t queue, uint64_t len)
+{
+   cvmx_pko_doorbell_address_t ptr;
+
+   ptr.u64          = 0;
+   ptr.s.mem_space  = CVMX_IO_SEG;
+   ptr.s.did        = CVMX_OCT_DID_PKT_SEND;
+   ptr.s.is_io      = 1;
+   ptr.s.port       = port;
+   ptr.s.queue      = queue;
+   CVMX_SYNCWS;  /* Need to make sure output queue data is in DRAM before doorbell write */
+   cvmx_write_io(ptr.u64, len);
+}
+
+
+#if CVMX_PKO_USE_FAU_FOR_OUTPUT_QUEUES && defined(CVMX_ENABLE_PKO_FUNCTIONS)
+/**
+ * Set queue index to a specific value.
+ * This abstracts the write operation normally performed on FAU registers.
+ *
+ * @param queue  Queue number associated with a port
+ * @param index  Value set, the initial index into the queue.
+ */
+static inline void cvmx_pko_set_queue_index(int queue, uint64_t index)
+{
+#ifdef CVMX_PKO_LOCKLESS_OPERATION
+    /* Remap array index to avoid cross-core memory activity */
+    uint32_t mapped_idx  = (queue & 0xF) << 4 | (queue & 0xF0) >> 4;
+
+    cvmx_pko_queue_index[mapped_idx] = index;
+
+#else
+    cvmx_fau_atomic_write64((cvmx_fau_reg_64_t)(CVMX_FAU_REG_OQ_ADDR_INDEX + 8 * queue), index);
+
+#endif
+}
+
+/**
+ * Increment queue index by a specific value.
+ * This abstracts the fetch-and-add operation normally performed on FAU registers.
+ *
+ * @param queue  Queue number associated with a port
+ * @param added  Signed value added to the queue index.
+ * @return The queue index value before the increment or decrement.
+ */
+static inline uint64_t cvmx_pko_update_queue_index(int queue, int64_t added)
+{
+#ifdef CVMX_PKO_LOCKLESS_OPERATION
+    /* Remap array index to avoid cross-core memory activity */
+    uint32_t mapped_idx  = (queue & 0xF) << 4 | (queue & 0xF0) >> 4;
+    uint64_t prev        = cvmx_pko_queue_index[mapped_idx];
+
+    cvmx_pko_queue_index[mapped_idx] = prev + added;
+    return prev;
+
+#else
+    return cvmx_fau_fetch_and_add64((cvmx_fau_reg_64_t)(CVMX_FAU_REG_OQ_ADDR_INDEX + 8 * queue), added);
+
+#endif
+}
+#endif
+
+
+/**
+ * Prepare to send a packet.  This may initiate a tag switch to
+ * get exclusive access to the output queue structure, and
+ * performs other prep work for the packet send operation.
+ *
+ * cvmx_pko_send_packet_finish() MUST be called after this function is called,
+ * and must be called with the same port/queue/use_locking arguments.
+ *
+ * The use_locking parameter allows the caller to use three
+ * possible locking modes.
+ * - CVMX_PKO_LOCK_NONE
+ *      - PKO doesn't do any locking. It is the responsibility
+ *          of the application to make sure that no other core
+ *          is accessing the same queue at the smae time.
+ * - CVMX_PKO_LOCK_ATOMIC_TAG
+ *      - PKO performs an atomic tagswitch to insure exclusive
+ *          access to the output queue. This will maintain
+ *          packet ordering on output.
+ * - CVMX_PKO_LOCK_CMD_QUEUE
+ *      - PKO uses the common command queue locks to insure
+ *          exclusive access to the output queue. This is a
+ *          memory based ll/sc. This is the most portable
+ *          locking mechanism.
+ *
+ * NOTE: If atomic locking is used, the POW entry CANNOT be
+ * descheduled, as it does not contain a valid WQE pointer.
+ *
+ * @param port   Port to send it on
+ * @param queue  Queue to use
+ * @param use_locking
+ *               CVMX_PKO_LOCK_NONE, CVMX_PKO_LOCK_ATOMIC_TAG, or CVMX_PKO_LOCK_CMD_QUEUE
+ */
+#ifdef CVMX_ENABLE_PKO_FUNCTIONS
+static inline void cvmx_pko_send_packet_prepare(uint64_t port, uint64_t queue, cvmx_pko_lock_t use_locking)
+{
+#if CVMX_PKO_USE_FAU_FOR_OUTPUT_QUEUES && defined(CVMX_PKO_LOCKLESS_OPERATION)
+
+    /* Prefetch queue index array */
+    CVMX_PREFETCH0(cvmx_pko_queue_index + ((queue & 0xF) << 4));
+#else
+    if (use_locking == CVMX_PKO_LOCK_ATOMIC_TAG)
+    {
+        /* Must do a full switch here to handle all cases.  We use a fake WQE pointer, as the POW does
+        ** not access this memory.  The WQE pointer and group are only used if this work is descheduled,
+        ** which is not supported by the cvmx_pko_send_packet_prepare/cvmx_pko_send_packet_finish combination.
+        ** Note that this is a special case in which these fake values can be used - this is not a general technique.
+        */
+        uint32_t tag = CVMX_TAG_SW_BITS_INTERNAL << CVMX_TAG_SW_SHIFT | CVMX_TAG_SUBGROUP_PKO  << CVMX_TAG_SUBGROUP_SHIFT | (CVMX_TAG_SUBGROUP_MASK & queue);
+        cvmx_pow_tag_sw_full((cvmx_wqe_t *)cvmx_phys_to_ptr(0x80), tag, CVMX_POW_TAG_TYPE_ATOMIC, 0);
+#if CVMX_PKO_USE_FAU_FOR_OUTPUT_QUEUES
+        /* start a tagwait FAU operation for command buffer offset */
+        cvmx_fau_async_tagwait_fetch_and_add64(CVMX_SCR_SCRATCH, (cvmx_fau_reg_64_t)(CVMX_FAU_REG_OQ_ADDR_INDEX + 8 * queue), 2);
+    }
+    else
+    {
+        cvmx_fau_async_fetch_and_add64(CVMX_SCR_SCRATCH, (cvmx_fau_reg_64_t)(CVMX_FAU_REG_OQ_ADDR_INDEX + 8 * queue), 2);
+#endif
+    }
+#endif
+}
+
+
+/**
+ * Complete packet output. cvmx_pko_send_packet_prepare() must be called exactly once before this,
+ * and the same parameters must be passed to both cvmx_pko_send_packet_prepare() and
+ * cvmx_pko_send_packet_finish().
+ *
+ * @param port   Port to send it on
+ * @param queue  Queue to use
+ * @param pko_command
+ *               PKO HW command word
+ * @param packet Packet to send
+ * @param use_locking
+ *               CVMX_PKO_LOCK_NONE, CVMX_PKO_LOCK_ATOMIC_TAG, or CVMX_PKO_LOCK_CMD_QUEUE
+ *
+ * @return returns CVMX_PKO_SUCCESS on success, or error code on failure of output
+ */
+static inline cvmx_pko_status_t cvmx_pko_send_packet_finish(uint64_t port, uint64_t queue,
+                                        cvmx_pko_command_word0_t pko_command,
+                                        cvmx_buf_ptr_t packet, cvmx_pko_lock_t use_locking)
+{
+#if CVMX_PKO_USE_FAU_FOR_OUTPUT_QUEUES
+    uint64_t *cur_oq_ptr;
+    uint64_t oq_index;
+    uint64_t hw_newbuf;
+    uint64_t *p_newbuf;
+
+    CVMX_SYNCWS;  /* flush any pending writes before entering critical region, avoid stalling on SYNCW inside critical region */
+
+#ifdef CVMX_PKO_LOCKLESS_OPERATION
+
+    /* Read and increment queue index */
+    oq_index = cvmx_pko_update_queue_index(queue, 2);
+
+#else
+    if (use_locking)
+    {
+        /* Spin while waiting for tag to complete so core can be interrupted. */
+        cvmx_pow_tag_sw_wait();
+    }
+    CVMX_SYNCIOBDMA;  /* wait for index fetch&add and the tag switch response to be in scratch */
+
+    /* Start of critical section.  The switch to atomic has completed.  The critical
+    ** section ends when the main program either switches to another tag or
+    ** gets the next piece of work.
+    */
+
+    /* FAU tagwait timeouts are disabled, and we wait for the tag completion before continuing, so
+    ** we always get a valid FAU response.
+    */
+    oq_index = cvmx_scratch_read64(CVMX_SCR_SCRATCH);
+#endif
+
+    /* Buffer pointer is in upper 64 - CVMX_PKO_INDEX_BITS bits,
+    ** index is in lower CVMX_PKO_INDEX_BITS bits.
+    */
+    cur_oq_ptr = (uint64_t*)cvmx_phys_to_ptr(oq_index >> CVMX_PKO_INDEX_BITS);
+    oq_index &= CVMX_PKO_INDEX_MASK;
+    cur_oq_ptr += oq_index;
+
+    if (cvmx_likely(oq_index < ((CVMX_FPA_OUTPUT_BUFFER_POOL_SIZE/8) - 3 - CVMX_PKO_COMMAND_BUFFER_SIZE_ADJUST)))
+    {
+        cur_oq_ptr[0] = pko_command.u64;
+        cur_oq_ptr[1] = packet.u64;
+
+        cvmx_pko_doorbell(port, queue, 2);
+        return CVMX_PKO_SUCCESS;
+
+    }
+
+    /* We need to allocate new buffer. Allocate new buffer from FPA */
+    hw_newbuf = cvmx_scratch_read64(CVMX_SCR_OQ_BUF_PRE_ALLOC);
+#ifdef PKO_LOCKLESS_OPERATION
+    if (cvmx_unlikely(!hw_newbuf))
+    {
+        /* Async buffer allocation was still pending, force sync and re-read */
+        CVMX_SYNCIOBDMA;
+        hw_newbuf = cvmx_scratch_read64(CVMX_SCR_OQ_BUF_PRE_ALLOC);
+    }
+#endif
+
+    p_newbuf = (uint64_t*)cvmx_phys_to_ptr(hw_newbuf);
+
+    /* Request new oq buffer - SYNCIOBDMA for critical section will ensure completion before use */
+    cvmx_scratch_write64(CVMX_SCR_OQ_BUF_PRE_ALLOC, 0);
+    cvmx_fpa_async_alloc(CVMX_SCR_OQ_BUF_PRE_ALLOC, CVMX_FPA_OUTPUT_BUFFER_POOL);
+
+    if (cvmx_unlikely(!hw_newbuf))
+    {
+        /* We don't have room, so decrement our index, and return error. */
+        cvmx_pko_update_queue_index(queue, -2);
+        return CVMX_PKO_NO_MEMORY;
+    }
+
+    /* we either have 2 or 3 words left */
+    cur_oq_ptr[0] = pko_command.u64;
+
+    /* Since we set the command size word to the pool size minus one, the
+        buffer will always end with 3 words. This is faster than the split
+        case. The "if" statement is still here in case someone begins using
+        the three word command format. At that time this code will work, just
+        be less optimal */
+    if (cvmx_unlikely(oq_index >= (CVMX_FPA_OUTPUT_BUFFER_POOL_SIZE/8) - 3))
+    {
+        /* 2 words left, spills into new buf */
+        p_newbuf[0] = packet.u64;
+        cur_oq_ptr[1] = hw_newbuf;
+        oq_index = 1;
+    }
+    else
+    {
+        /* 3 words left, fills current buffer exactly */
+        cur_oq_ptr[1] = packet.u64;
+        cur_oq_ptr[2] = hw_newbuf;
+        oq_index = 0;
+    }
+
+    /* Set new buffer pointer and index */
+    cvmx_pko_set_queue_index(queue, (hw_newbuf << CVMX_PKO_INDEX_BITS) | oq_index);
+    /* Read it and wait for response to ensure that the index is updated within critical section */
+    cvmx_pko_update_queue_index(queue, 0);
+
+
+    /* Hardware will free the previous buffer when it is finished with it */
+    cvmx_pko_doorbell(port, queue, 2);
+    return CVMX_PKO_SUCCESS;
+#else
+    cvmx_cmd_queue_result_t result;
+    if (use_locking == CVMX_PKO_LOCK_ATOMIC_TAG)
+        cvmx_pow_tag_sw_wait();
+    result = cvmx_cmd_queue_write2(CVMX_CMD_QUEUE_PKO(queue),
+                                   (use_locking == CVMX_PKO_LOCK_CMD_QUEUE),
+                                   pko_command.u64,
+                                   packet.u64);
+    if (cvmx_likely(result == CVMX_CMD_QUEUE_SUCCESS))
+    {
+        cvmx_pko_doorbell(port, queue, 2);
+        return CVMX_PKO_SUCCESS;
+    }
+    else if ((result == CVMX_CMD_QUEUE_NO_MEMORY) || (result == CVMX_CMD_QUEUE_FULL))
+    {
+        return CVMX_PKO_NO_MEMORY;
+    }
+    else
+    {
+        return CVMX_PKO_INVALID_QUEUE;
+    }
+#endif
+}
+
+
+/**
+ * Complete packet output. cvmx_pko_send_packet_prepare() must be called exactly once before this,
+ * and the same parameters must be passed to both cvmx_pko_send_packet_prepare() and
+ * cvmx_pko_send_packet_finish().
+ *
+ * @param port   Port to send it on
+ * @param queue  Queue to use
+ * @param pko_command
+ *               PKO HW command word
+ * @param packet Packet to send
+ * @param addr   Plysical address of a work queue entry or physical address to zero on complete.
+ * @param use_locking
+ *               CVMX_PKO_LOCK_NONE, CVMX_PKO_LOCK_ATOMIC_TAG, or CVMX_PKO_LOCK_CMD_QUEUE
+ *
+ * @return returns CVMX_PKO_SUCCESS on success, or error code on failure of output
+ */
+static inline cvmx_pko_status_t cvmx_pko_send_packet_finish3(uint64_t port, uint64_t queue,
+                                        cvmx_pko_command_word0_t pko_command,
+                                        cvmx_buf_ptr_t packet, uint64_t addr, cvmx_pko_lock_t use_locking)
+{
+#if CVMX_PKO_USE_FAU_FOR_OUTPUT_QUEUES
+    #warning Implement cvmx_pko_send_packet_finish3
+    return CVMX_PKO_NO_MEMORY;
+#else
+    cvmx_cmd_queue_result_t result;
+    if (use_locking == CVMX_PKO_LOCK_ATOMIC_TAG)
+        cvmx_pow_tag_sw_wait();
+    result = cvmx_cmd_queue_write3(CVMX_CMD_QUEUE_PKO(queue),
+                                   (use_locking == CVMX_PKO_LOCK_CMD_QUEUE),
+                                   pko_command.u64,
+                                   packet.u64,
+                                   addr);
+    if (cvmx_likely(result == CVMX_CMD_QUEUE_SUCCESS))
+    {
+        cvmx_pko_doorbell(port, queue, 3);
+        return CVMX_PKO_SUCCESS;
+    }
+    else if ((result == CVMX_CMD_QUEUE_NO_MEMORY) || (result == CVMX_CMD_QUEUE_FULL))
+    {
+        return CVMX_PKO_NO_MEMORY;
+    }
+    else
+    {
+        return CVMX_PKO_INVALID_QUEUE;
+    }
+#endif
+}
+
+/**
+ * Return the pko output queue associated with a port and a specific core.
+ * In normal mode (PKO lockless operation is disabled), the value returned
+ * is the base queue.
+ *
+ * @param port   Port number
+ * @param core   Core to get queue for
+ *
+ * @return Core-specific output queue
+ */
+static inline int cvmx_pko_get_base_queue_per_core(int port, int core)
+{
+#ifndef CVMX_HELPER_PKO_MAX_PORTS_INTERFACE0
+    #define CVMX_HELPER_PKO_MAX_PORTS_INTERFACE0 16
+#endif
+#ifndef CVMX_HELPER_PKO_MAX_PORTS_INTERFACE1
+    #define CVMX_HELPER_PKO_MAX_PORTS_INTERFACE1 16
+#endif
+    if (port < CVMX_PKO_MAX_PORTS_INTERFACE0)
+        return port * CVMX_PKO_QUEUES_PER_PORT_INTERFACE0 + core;
+    else if (port >=16 && port < 16 + CVMX_PKO_MAX_PORTS_INTERFACE1)
+        return CVMX_PKO_MAX_PORTS_INTERFACE0 * CVMX_PKO_QUEUES_PER_PORT_INTERFACE0 +
+	       (port-16) * CVMX_PKO_QUEUES_PER_PORT_INTERFACE1 + core;
+    else if ((port >= 32) && (port < 36))
+        return CVMX_PKO_MAX_PORTS_INTERFACE0 * CVMX_PKO_QUEUES_PER_PORT_INTERFACE0 +
+               CVMX_PKO_MAX_PORTS_INTERFACE1 * CVMX_PKO_QUEUES_PER_PORT_INTERFACE1 +
+               (port-32) * CVMX_PKO_QUEUES_PER_PORT_PCI;
+    else if ((port >= 36) && (port < 40))
+        return CVMX_PKO_MAX_PORTS_INTERFACE0 * CVMX_PKO_QUEUES_PER_PORT_INTERFACE0 +
+               CVMX_PKO_MAX_PORTS_INTERFACE1 * CVMX_PKO_QUEUES_PER_PORT_INTERFACE1 +
+               4 * CVMX_PKO_QUEUES_PER_PORT_PCI +
+               (port-36) * CVMX_PKO_QUEUES_PER_PORT_LOOP;
+    else
+        /* Given the limit on the number of ports we can map to
+         * CVMX_MAX_OUTPUT_QUEUES_STATIC queues (currently 256,
+         * divided among all cores), the remaining unmapped ports
+         * are assigned an illegal queue number */
+        return CVMX_PKO_ILLEGAL_QUEUE;
+}
+
+/**
+ * For a given port number, return the base pko output queue
+ * for the port.
+ *
+ * @param port   Port number
+ * @return Base output queue
+ */
+static inline int cvmx_pko_get_base_queue(int port)
+{
+#ifdef CVMX_PKO_LOCKLESS_OPERATION
+    return cvmx_pko_get_base_queue_per_core(port, cvmx_get_core_num());
+#else
+    return cvmx_pko_get_base_queue_per_core(port, 0);
+#endif
+}
+
+/**
+ * For a given port number, return the number of pko output queues.
+ *
+ * @param port   Port number
+ * @return Number of output queues
+ */
+static inline int cvmx_pko_get_num_queues(int port)
+{
+    if (port < 16)
+        return CVMX_PKO_QUEUES_PER_PORT_INTERFACE0;
+    else if (port<32)
+        return CVMX_PKO_QUEUES_PER_PORT_INTERFACE1;
+    else if (port<36)
+        return CVMX_PKO_QUEUES_PER_PORT_PCI;
+    else if (port<40)
+        return CVMX_PKO_QUEUES_PER_PORT_LOOP;
+    else
+        return 0;
+}
+
+/**
+ * Get the status counters for a port.
+ *
+ * @param port_num Port number to get statistics for.
+ * @param clear    Set to 1 to clear the counters after they are read
+ * @param status   Where to put the results.
+ */
+static inline void cvmx_pko_get_port_status(uint64_t port_num, uint64_t clear, cvmx_pko_port_status_t *status)
+{
+    cvmx_pko_reg_read_idx_t pko_reg_read_idx;
+    cvmx_pko_mem_count0_t pko_mem_count0;
+    cvmx_pko_mem_count1_t pko_mem_count1;
+
+    pko_reg_read_idx.u64 = 0;
+    pko_reg_read_idx.s.index = port_num;
+    cvmx_write_csr(CVMX_PKO_REG_READ_IDX, pko_reg_read_idx.u64);
+
+    pko_mem_count0.u64 = cvmx_read_csr(CVMX_PKO_MEM_COUNT0);
+    status->packets = pko_mem_count0.s.count;
+    if (clear)
+    {
+        pko_mem_count0.s.count = port_num;
+        cvmx_write_csr(CVMX_PKO_MEM_COUNT0, pko_mem_count0.u64);
+    }
+
+    pko_mem_count1.u64 = cvmx_read_csr(CVMX_PKO_MEM_COUNT1);
+    status->octets = pko_mem_count1.s.count;
+    if (clear)
+    {
+        pko_mem_count1.s.count = port_num;
+        cvmx_write_csr(CVMX_PKO_MEM_COUNT1, pko_mem_count1.u64);
+    }
+
+    if (OCTEON_IS_MODEL(OCTEON_CN3XXX))
+    {
+        cvmx_pko_mem_debug9_t debug9;
+        pko_reg_read_idx.s.index = cvmx_pko_get_base_queue(port_num);
+        cvmx_write_csr(CVMX_PKO_REG_READ_IDX, pko_reg_read_idx.u64);
+        debug9.u64 = cvmx_read_csr(CVMX_PKO_MEM_DEBUG9);
+        status->doorbell = debug9.cn38xx.doorbell;
+    }
+    else
+    {
+        cvmx_pko_mem_debug8_t debug8;
+        pko_reg_read_idx.s.index = cvmx_pko_get_base_queue(port_num);
+        cvmx_write_csr(CVMX_PKO_REG_READ_IDX, pko_reg_read_idx.u64);
+        debug8.u64 = cvmx_read_csr(CVMX_PKO_MEM_DEBUG8);
+        status->doorbell = debug8.cn58xx.doorbell;
+    }
+}
+
+
+/**
+ * Rate limit a PKO port to a max packets/sec. This function is only
+ * supported on CN57XX, CN56XX, CN55XX, and CN54XX.
+ *
+ * @param port      Port to rate limit
+ * @param packets_s Maximum packet/sec
+ * @param burst     Maximum number of packets to burst in a row before rate
+ *                  limiting cuts in.
+ *
+ * @return Zero on success, negative on failure
+ */
+extern int cvmx_pko_rate_limit_packets(int port, int packets_s, int burst);
+
+/**
+ * Rate limit a PKO port to a max bits/sec. This function is only
+ * supported on CN57XX, CN56XX, CN55XX, and CN54XX.
+ *
+ * @param port   Port to rate limit
+ * @param bits_s PKO rate limit in bits/sec
+ * @param burst  Maximum number of bits to burst before rate
+ *               limiting cuts in.
+ *
+ * @return Zero on success, negative on failure
+ */
+extern int cvmx_pko_rate_limit_bits(int port, uint64_t bits_s, int burst);
+
+#endif /* CVMX_ENABLE_PKO_FUNCTIONS */
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif   /* __CVMX_PKO_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-platform.h b/arch/mips/cavium-octeon/executive/cvmx-platform.h
new file mode 100644
index 0000000..b253aec
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-platform.h
@@ -0,0 +1,187 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * This file is resposible for including all system dependent
+ * headers for the cvmx-* files.
+ *
+ * <hr>$Revision: 32636 $<hr>
+*/
+
+#ifndef __CVMX_PLATFORM_H__
+#define __CVMX_PLATFORM_H__
+
+#include "cvmx-abi.h"
+
+/* This file defines macros for use in determining the current
+    building environment. It defines a single CVMX_BUILD_FOR_*
+    macro representing the target of the build. The current
+    possibilities are:
+        CVMX_BUILD_FOR_UBOOT
+        CVMX_BUILD_FOR_LINUX_KERNEL
+        CVMX_BUILD_FOR_LINUX_USER
+        CVMX_BUILD_FOR_LINUX_HOST
+        CVMX_BUILD_FOR_VXWORKS
+        CVMX_BUILD_FOR_STANDALONE */
+#if defined(__U_BOOT__)
+    /* We are being used inside of Uboot */
+    #define CVMX_BUILD_FOR_UBOOT
+#elif defined(__linux__)
+    #if !defined(__mips__)
+        /* We are being used under Linux but not on Octeon. Assume
+            we are on a Linux host with an Octeon target over PCI/PCIe */
+        #define CVMX_BUILD_FOR_LINUX_HOST
+    #elif defined(__KERNEL__)
+        /* We are in the Linux kernel on Octeon */
+        #define CVMX_BUILD_FOR_LINUX_KERNEL
+    #else
+        #ifdef CVMX_BUILD_FOR_LINUX_HOST
+            /* This is a manual special case. The host PCI utilities can
+                be configured to run on Octeon. In this case it is impossible
+                to tell the difference between the normal userspace setup
+                and using cvmx_read/write_csr over the PCI bus. The host
+                utilites force this define to fix this */
+        #else
+            /* We are in the Linux userspace on Octeon */
+            #define CVMX_BUILD_FOR_LINUX_USER
+        #endif
+    #endif
+#elif defined(_WRS_KERNEL) || defined(VXWORKS_USER_MAPPINGS)
+    /* We are in VxWorks on Octeon */
+    #define CVMX_BUILD_FOR_VXWORKS
+#elif defined(_OCTEON_TOOLCHAIN_RUNTIME)
+    /* To build the simple exec toolchain runtime (newlib) library. We
+       should only use features available on all Octeon models.  */
+    #define CVMX_BUILD_FOR_TOOLCHAIN
+#else
+    /* We are building a simple exec standalone image for Octeon */
+    #define CVMX_BUILD_FOR_STANDALONE
+#endif
+
+
+
+
+#if defined(CVMX_BUILD_FOR_UBOOT)
+
+    #include <common.h>
+    #include "cvmx-sysinfo.h"
+
+#elif defined(CVMX_BUILD_FOR_LINUX_KERNEL)
+
+    #include <linux/kernel.h>
+    #include <linux/string.h>
+    #include <linux/types.h>
+    #include <stdarg.h>
+
+#elif defined(CVMX_BUILD_FOR_LINUX_USER)
+
+    #include <stddef.h>
+    #include <stdint.h>
+    #include <stdio.h>
+    #include <stdlib.h>
+    #include <stdarg.h>
+    #include <string.h>
+    #include <assert.h>
+    #include <fcntl.h>
+    #include <sys/mman.h>
+    #include <unistd.h>
+
+#elif defined(CVMX_BUILD_FOR_LINUX_HOST)
+
+    #include <stddef.h>
+    #include <stdint.h>
+    #include <stdio.h>
+    #include <stdlib.h>
+    #include <stdarg.h>
+    #include <string.h>
+    #include <assert.h>
+    #include <fcntl.h>
+    #include <sys/mman.h>
+    #include <unistd.h>
+
+#elif defined(CVMX_BUILD_FOR_VXWORKS)
+
+    #include <stdint.h>
+    #include <stdio.h>
+    #include <stdlib.h>
+    #include <stdarg.h>
+    #include <string.h>
+    #include <assert.h>
+
+#elif defined(CVMX_BUILD_FOR_STANDALONE)
+
+    #include <stddef.h>
+    #include <stdint.h>
+    #include <stdio.h>
+    #include <stdlib.h>
+    #include <stdarg.h>
+    #include <string.h>
+    #include <assert.h>
+
+#elif defined(CVMX_BUILD_FOR_TOOLCHAIN)
+
+    #include <stddef.h>
+    #include <stdint.h>
+    #include <stdio.h>
+    #include <stdlib.h>
+    #include <stdarg.h>
+    #include <string.h>
+    #include <assert.h>
+
+#else
+
+    #error Unexpected CVMX_BUILD_FOR_* macro
+
+#endif
+
+#endif /* __CVMX_PLATFORM_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pow.c b/arch/mips/cavium-octeon/executive/cvmx-pow.c
new file mode 100644
index 0000000..0c8d191
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-pow.c
@@ -0,0 +1,488 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Interface to the hardware Packet Order / Work unit.
+ *
+ * <hr>$Revision: 29727 $<hr>
+ */
+
+#include "cvmx.h"
+#include "cvmx-pow.h"
+
+/**
+ * @INTERNAL
+ * This structure stores the internal POW state captured by
+ * cvmx_pow_capture(). It is purposely not exposed to the user
+ * since the format may change without notice.
+ */
+typedef struct
+{
+    cvmx_pow_tag_load_resp_t sstatus[16][8];
+    cvmx_pow_tag_load_resp_t smemload[2048][3];
+    cvmx_pow_tag_load_resp_t sindexload[16][4];
+} __cvmx_pow_dump_t;
+
+typedef enum
+{
+    CVMX_POW_LIST_UNKNOWN=0,
+    CVMX_POW_LIST_FREE=1,
+    CVMX_POW_LIST_INPUT=2,
+    CVMX_POW_LIST_CORE=CVMX_POW_LIST_INPUT+8,
+    CVMX_POW_LIST_DESCHED=CVMX_POW_LIST_CORE+16,
+    CVMX_POW_LIST_NOSCHED=CVMX_POW_LIST_DESCHED+16,
+} __cvmx_pow_list_types_t;
+
+static const char *__cvmx_pow_list_names[] = {
+    "Unknown",
+    "Free List",
+    "Queue 0", "Queue 1", "Queue 2", "Queue 3",
+    "Queue 4", "Queue 5", "Queue 6", "Queue 7",
+    "Core 0", "Core 1", "Core 2", "Core 3",
+    "Core 4", "Core 5", "Core 6", "Core 7",
+    "Core 8", "Core 9", "Core 10", "Core 11",
+    "Core 12", "Core 13", "Core 14", "Core 15",
+    "Desched 0", "Desched 1", "Desched 2", "Desched 3",
+    "Desched 4", "Desched 5", "Desched 6", "Desched 7",
+    "Desched 8", "Desched 9", "Desched 10", "Desched 11",
+    "Desched 12", "Desched 13", "Desched 14", "Desched 15",
+    "Nosched 0", "Nosched 1", "Nosched 2", "Nosched 3",
+    "Nosched 4", "Nosched 5", "Nosched 6", "Nosched 7",
+    "Nosched 8", "Nosched 9", "Nosched 10", "Nosched 11",
+    "Nosched 12", "Nosched 13", "Nosched 14", "Nosched 15"
+};
+
+
+/**
+ * @INTERNAL
+ * Return the number of POW entries supported by this chip
+ *
+ * @return
+ */
+static int __cvmx_pow_get_num_entries(void)
+{
+    if (OCTEON_IS_MODEL(OCTEON_CN30XX))
+        return 64;
+    else if (OCTEON_IS_MODEL(OCTEON_CN31XX) || OCTEON_IS_MODEL(OCTEON_CN50XX))
+        return 256;
+    else
+        return 2048;
+}
+
+
+/**
+ * Store the current POW internal state into the supplied
+ * buffer. It is recommended that you pass a buffer of at least
+ * 128KB. The format of the capture may change based on SDK
+ * version and Octeon chip.
+ *
+ * @param buffer Buffer to store capture into
+ * @param buffer_size
+ *               The size of the supplied buffer
+ *
+ * @return Zero on sucess, negative on failure
+ */
+int cvmx_pow_capture(void *buffer, int buffer_size)
+{
+    __cvmx_pow_dump_t *dump = (__cvmx_pow_dump_t*)buffer;
+    int num_cores;
+    int num_pow_entries = __cvmx_pow_get_num_entries();
+    int core;
+    int index;
+    int bits;
+
+    if (buffer_size < (int)sizeof(__cvmx_pow_dump_t))
+    {
+        cvmx_dprintf("cvmx_pow_capture: Buffer too small\n");
+        return -1;
+    }
+
+    num_cores = cvmx_octeon_num_cores();
+
+    /* Read all core related state */
+    for (core=0; core<num_cores; core++)
+    {
+        cvmx_pow_load_addr_t load_addr;
+        load_addr.u64 = 0;
+        load_addr.sstatus.mem_region = CVMX_IO_SEG;
+        load_addr.sstatus.is_io = 1;
+        load_addr.sstatus.did = CVMX_OCT_DID_TAG_TAG1;
+        load_addr.sstatus.coreid = core;
+        for (bits=0; bits<8; bits++)
+        {
+            load_addr.sstatus.get_rev = (bits & 1) != 0;
+            load_addr.sstatus.get_cur = (bits & 2) != 0;
+            load_addr.sstatus.get_wqp = (bits & 4) != 0;
+            if ((load_addr.sstatus.get_cur == 0) && load_addr.sstatus.get_rev)
+                dump->sstatus[core][bits].u64 = -1;
+            else
+                dump->sstatus[core][bits].u64 = cvmx_read_csr(load_addr.u64);
+        }
+    }
+
+    /* Read all internal POW entries */
+    for (index=0; index<num_pow_entries; index++)
+    {
+        cvmx_pow_load_addr_t load_addr;
+        load_addr.u64 = 0;
+        load_addr.smemload.mem_region = CVMX_IO_SEG;
+        load_addr.smemload.is_io = 1;
+        load_addr.smemload.did = CVMX_OCT_DID_TAG_TAG2;
+        load_addr.smemload.index = index;
+        for (bits=0; bits<3; bits++)
+        {
+            load_addr.smemload.get_des = (bits & 1) != 0;
+            load_addr.smemload.get_wqp = (bits & 2) != 0;
+            dump->smemload[index][bits].u64 = cvmx_read_csr(load_addr.u64);
+        }
+    }
+
+    /* Read all group and queue pointers */
+    for (index=0; index<16; index++)
+    {
+        cvmx_pow_load_addr_t load_addr;
+        load_addr.u64 = 0;
+        load_addr.sindexload.mem_region = CVMX_IO_SEG;
+        load_addr.sindexload.is_io = 1;
+        load_addr.sindexload.did = CVMX_OCT_DID_TAG_TAG3;
+        load_addr.sindexload.qosgrp = index;
+        for (bits=0; bits<4; bits++)
+        {
+            load_addr.sindexload.get_rmt =  (bits & 1) != 0;
+            load_addr.sindexload.get_des_get_tail =  (bits & 2) != 0;
+            /* The first pass only has 8 valid index values */
+            if ((load_addr.sindexload.get_rmt == 0) &&
+                (load_addr.sindexload.get_des_get_tail == 0) &&
+                (index >= 8))
+                dump->sindexload[index][bits].u64 = -1;
+            else
+                dump->sindexload[index][bits].u64 = cvmx_read_csr(load_addr.u64);
+        }
+    }
+    return 0;
+}
+
+
+/**
+ * Function to display a POW internal queue to the user
+ *
+ * @param name       User visible name for the queue
+ * @param name_param Parameter for printf in creating the name
+ * @param valid      Set if the queue contains any elements
+ * @param has_one    Set if the queue contains exactly one element
+ * @param head       The head pointer
+ * @param tail       The tail pointer
+ */
+static void __cvmx_pow_display_list(const char *name, int name_param, int valid, int has_one, uint64_t head, uint64_t tail)
+{
+    printf(name, name_param);
+    printf(": ");
+    if (valid)
+    {
+        if (has_one)
+            printf("One element index=%llu(0x%llx)\n", CAST64(head), CAST64(head));
+        else
+            printf("Multiple elements head=%llu(0x%llx) tail=%llu(0x%llx)\n", CAST64(head), CAST64(head), CAST64(tail), CAST64(tail));
+    }
+    else
+        printf("Empty\n");
+}
+
+
+/**
+ * Mark which list a POW entry is on. Print a warning message if the
+ * entry is already on a list. This happens if the POW changed while
+ * the capture was running.
+ *
+ * @param entry_num  Entry number to mark
+ * @param entry_type List type
+ * @param entry_list Array to store marks
+ *
+ * @return Zero on success, negative if already on a list
+ */
+static int __cvmx_pow_entry_mark_list(int entry_num, __cvmx_pow_list_types_t entry_type, uint8_t entry_list[])
+{
+    if (entry_list[entry_num] == 0)
+    {
+        entry_list[entry_num] = entry_type;
+        return 0;
+    }
+    else
+    {
+        printf("\nWARNING: Entry %d already on list %s, but we tried to add it to %s\n",
+               entry_num, __cvmx_pow_list_names[entry_list[entry_num]], __cvmx_pow_list_names[entry_type]);
+        return -1;
+    }
+}
+
+
+/**
+ * Display a list and mark all elements on the list as belonging to
+ * the list.
+ *
+ * @param entry_type Type of the list to display and mark
+ * @param dump       POW capture data
+ * @param entry_list Array to store marks in
+ * @param valid      Set if the queue contains any elements
+ * @param has_one    Set if the queue contains exactly one element
+ * @param head       The head pointer
+ * @param tail       The tail pointer
+ */
+static void __cvmx_pow_display_list_and_walk(__cvmx_pow_list_types_t entry_type,
+                                             __cvmx_pow_dump_t *dump, uint8_t entry_list[],
+                                             int valid, int has_one, uint64_t head, uint64_t tail)
+{
+    __cvmx_pow_display_list(__cvmx_pow_list_names[entry_type], 0, valid, has_one, head, tail);
+    if (valid)
+    {
+        if (has_one)
+            __cvmx_pow_entry_mark_list(head, entry_type, entry_list);
+        else
+        {
+            while (head != tail)
+            {
+                if (__cvmx_pow_entry_mark_list(head, entry_type, entry_list))
+                    break;
+                head = dump->smemload[head][0].s_smemload0.next_index;
+            }
+            __cvmx_pow_entry_mark_list(tail, entry_type, entry_list);
+        }
+    }
+}
+
+
+/**
+ * Dump a POW capture to the console in a human readable format.
+ *
+ * @param buffer POW capture from cvmx_pow_capture()
+ * @param buffer_size
+ *               Size of the buffer
+ */
+void cvmx_pow_display(void *buffer, int buffer_size)
+{
+    __cvmx_pow_dump_t *dump = (__cvmx_pow_dump_t*)buffer;
+    int num_pow_entries = __cvmx_pow_get_num_entries();
+    int num_cores;
+    int core;
+    int index;
+    uint8_t entry_list[2048];
+
+    if (buffer_size < (int)sizeof(__cvmx_pow_dump_t))
+    {
+        cvmx_dprintf("cvmx_pow_dump: Buffer too small\n");
+        return;
+    }
+
+    memset(entry_list, 0, sizeof(entry_list));
+    num_cores = cvmx_octeon_num_cores();
+
+    printf("POW Display Start\n");
+
+    /* Print the free list info */
+    __cvmx_pow_display_list_and_walk(CVMX_POW_LIST_FREE, dump, entry_list,
+                                     dump->sindexload[0][0].sindexload0.free_val,
+                                     dump->sindexload[0][0].sindexload0.free_one,
+                                     dump->sindexload[0][0].sindexload0.free_head,
+                                     dump->sindexload[0][0].sindexload0.free_tail);
+
+    /* Print the core state */
+    for (core=0; core<num_cores; core++)
+    {
+        const int bit_rev = 1;
+        const int bit_cur = 2;
+        const int bit_wqp = 4;
+        printf("Core %d State:  tag=%s,0x%08x", core,
+               OCT_TAG_TYPE_STRING(dump->sstatus[core][bit_cur].s_sstatus2.tag_type),
+               dump->sstatus[core][bit_cur].s_sstatus2.tag);
+        if (dump->sstatus[core][bit_cur].s_sstatus2.tag_type != CVMX_POW_TAG_TYPE_NULL_NULL)
+        {
+            __cvmx_pow_entry_mark_list(dump->sstatus[core][bit_cur].s_sstatus2.index, CVMX_POW_LIST_CORE + core, entry_list);
+            printf(" grp=%d",                   dump->sstatus[core][bit_cur].s_sstatus2.grp);
+            printf(" wqp=0x%016llx",            CAST64(dump->sstatus[core][bit_cur|bit_wqp].s_sstatus4.wqp));
+            printf(" index=%d",                 dump->sstatus[core][bit_cur].s_sstatus2.index);
+            if (dump->sstatus[core][bit_cur].s_sstatus2.head)
+                printf(" head");
+            else
+                printf(" prev=%d", dump->sstatus[core][bit_cur|bit_rev].s_sstatus3.revlink_index);
+            if (dump->sstatus[core][bit_cur].s_sstatus2.tail)
+                printf(" tail");
+            else
+                printf(" next=%d", dump->sstatus[core][bit_cur].s_sstatus2.link_index);
+        }
+
+        if (dump->sstatus[core][0].s_sstatus0.pend_switch)
+        {
+            printf(" pend_switch=%d",           dump->sstatus[core][0].s_sstatus0.pend_switch);
+            printf(" pend_switch_full=%d",      dump->sstatus[core][0].s_sstatus0.pend_switch_full);
+            printf(" pend_switch_null=%d",      dump->sstatus[core][0].s_sstatus0.pend_switch_null);
+        }
+
+        if (dump->sstatus[core][0].s_sstatus0.pend_desched)
+        {
+            printf(" pend_desched=%d",          dump->sstatus[core][0].s_sstatus0.pend_desched);
+            printf(" pend_desched_switch=%d",   dump->sstatus[core][0].s_sstatus0.pend_desched_switch);
+            printf(" pend_nosched=%d",          dump->sstatus[core][0].s_sstatus0.pend_nosched);
+            if (dump->sstatus[core][0].s_sstatus0.pend_desched_switch)
+                printf(" pend_grp=%d",              dump->sstatus[core][0].s_sstatus0.pend_grp);
+        }
+
+        if (dump->sstatus[core][0].s_sstatus0.pend_new_work)
+        {
+            if (dump->sstatus[core][0].s_sstatus0.pend_new_work_wait)
+                printf(" (Waiting for work)");
+            else
+                printf(" (Getting work)");
+        }
+        if (dump->sstatus[core][0].s_sstatus0.pend_null_rd)
+            printf(" pend_null_rd=%d",          dump->sstatus[core][0].s_sstatus0.pend_null_rd);
+        if (dump->sstatus[core][0].s_sstatus0.pend_nosched_clr)
+        {
+            printf(" pend_nosched_clr=%d",      dump->sstatus[core][0].s_sstatus0.pend_nosched_clr);
+            printf(" pend_index=%d",            dump->sstatus[core][0].s_sstatus0.pend_index);
+        }
+        if (dump->sstatus[core][0].s_sstatus0.pend_switch ||
+            (dump->sstatus[core][0].s_sstatus0.pend_desched &&
+            dump->sstatus[core][0].s_sstatus0.pend_desched_switch))
+        {
+            printf(" pending tag=%s,0x%08x",
+                   OCT_TAG_TYPE_STRING(dump->sstatus[core][0].s_sstatus0.pend_type),
+                   dump->sstatus[core][0].s_sstatus0.pend_tag);
+        }
+        if (dump->sstatus[core][0].s_sstatus0.pend_nosched_clr)
+            printf(" pend_wqp=0x%016llx\n",     CAST64(dump->sstatus[core][bit_wqp].s_sstatus1.pend_wqp));
+        printf("\n");
+    }
+
+    /* Print out the state of the 16 deschedule lists. Each group has two
+        lists. One for entries marked noshed, the other for normal
+        deschedules */
+    for (index=0; index<16; index++)
+    {
+        __cvmx_pow_display_list_and_walk(CVMX_POW_LIST_NOSCHED + index, dump, entry_list,
+                                dump->sindexload[index][2].sindexload1.nosched_val,
+                                dump->sindexload[index][2].sindexload1.nosched_one,
+                                dump->sindexload[index][2].sindexload1.nosched_head,
+                                dump->sindexload[index][2].sindexload1.nosched_tail);
+        __cvmx_pow_display_list_and_walk(CVMX_POW_LIST_DESCHED + index, dump, entry_list,
+                                dump->sindexload[index][2].sindexload1.des_val,
+                                dump->sindexload[index][2].sindexload1.des_one,
+                                dump->sindexload[index][2].sindexload1.des_head,
+                                dump->sindexload[index][2].sindexload1.des_tail);
+    }
+
+    /* Print out the state of the 8 internal input queues */
+    for (index=0; index<8; index++)
+    {
+        __cvmx_pow_display_list_and_walk(CVMX_POW_LIST_INPUT + index, dump, entry_list,
+                                dump->sindexload[index][0].sindexload0.loc_val,
+                                dump->sindexload[index][0].sindexload0.loc_one,
+                                dump->sindexload[index][0].sindexload0.loc_head,
+                                dump->sindexload[index][0].sindexload0.loc_tail);
+    }
+
+    /* Print out the state of the 16 memory queues */
+    for (index=0; index<8; index++)
+    {
+        const char *name;
+        if (dump->sindexload[index][1].sindexload2.rmt_is_head)
+            name = "Queue %da Memory (is head)";
+        else
+            name = "Queue %da Memory";
+        __cvmx_pow_display_list(name, index,
+                                dump->sindexload[index][1].sindexload2.rmt_val,
+                                dump->sindexload[index][1].sindexload2.rmt_one,
+                                dump->sindexload[index][1].sindexload2.rmt_head,
+                                dump->sindexload[index][3].sindexload3.rmt_tail);
+        if (dump->sindexload[index+8][1].sindexload2.rmt_is_head)
+            name = "Queue %db Memory (is head)";
+        else
+            name = "Queue %db Memory";
+        __cvmx_pow_display_list(name, index,
+                                dump->sindexload[index+8][1].sindexload2.rmt_val,
+                                dump->sindexload[index+8][1].sindexload2.rmt_one,
+                                dump->sindexload[index+8][1].sindexload2.rmt_head,
+                                dump->sindexload[index+8][3].sindexload3.rmt_tail);
+    }
+
+    /* Print out each of the internal POW entries. Each entry has a tag, group,
+        wqe, and possibly a next pointer. The next pointer is only valid if this
+        entry isn't make as a tail */
+    for (index=0; index<num_pow_entries; index++)
+    {
+        printf("Entry %d(%-10s): tag=%s,0x%08x grp=%d wqp=0x%016llx", index,
+               __cvmx_pow_list_names[entry_list[index]],
+               OCT_TAG_TYPE_STRING(dump->smemload[index][0].s_smemload0.tag_type),
+               dump->smemload[index][0].s_smemload0.tag,
+               dump->smemload[index][0].s_smemload0.grp,
+               CAST64(dump->smemload[index][2].s_smemload1.wqp));
+        if (dump->smemload[index][0].s_smemload0.tail)
+            printf(" tail");
+        else
+            printf(" next=%d", dump->smemload[index][0].s_smemload0.next_index);
+        if (entry_list[index] >= CVMX_POW_LIST_DESCHED)
+        {
+            printf(" prev=%d", dump->smemload[index][1].s_smemload2.fwd_index);
+            printf(" nosched=%d", dump->smemload[index][1].s_smemload2.nosched);
+            if (dump->smemload[index][1].s_smemload2.pend_switch)
+            {
+                printf(" pending tag=%s,0x%08x",
+                       OCT_TAG_TYPE_STRING(dump->smemload[index][1].s_smemload2.pend_type),
+                       dump->smemload[index][1].s_smemload2.pend_tag);
+            }
+        }
+        printf("\n");
+    }
+
+    printf("POW Display End\n");
+}
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pow.h b/arch/mips/cavium-octeon/executive/cvmx-pow.h
new file mode 100644
index 0000000..86b2a29
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-pow.h
@@ -0,0 +1,1750 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Interface to the hardware Packet Order / Work unit.
+ *
+ * New, starting with SDK 1.7.0, cvmx-pow supports a number of
+ * extended consistency checks. The define
+ * CVMX_ENABLE_POW_CHECKS controls the runtime insertion of POW
+ * internal state checks to find common programming errors. If
+ * CVMX_ENABLE_POW_CHECKS is not defined, checks are by default
+ * enabled. For example, cvmx-pow will check for the following
+ * program errors or POW state inconsistency.
+ * - Requesting a POW operation with an active tag switch in
+ *   progress.
+ * - Waiting for a tag switch to complete for an excessively
+ *   long period. This is normally a sign of an error in locking
+ *   causing deadlock.
+ * - Illegal tag switches from NULL_NULL.
+ * - Illegal tag switches from NULL.
+ * - Illegal deschedule request.
+ * - WQE pointer not matching the one attached to the core by
+ *   the POW.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+
+#ifndef __CVMX_POW_H__
+#define __CVMX_POW_H__
+
+#include "cvmx-scratch.h"
+#include "cvmx-wqe.h"
+#include "cvmx-warn.h"
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+/* Default to having all POW constancy checks turned on */
+#ifndef CVMX_ENABLE_POW_CHECKS
+#define CVMX_ENABLE_POW_CHECKS 1
+#endif
+
+/**
+ * Wait flag values for pow functions.
+ */
+typedef enum
+{
+    CVMX_POW_WAIT = 1,
+    CVMX_POW_NO_WAIT = 0,
+} cvmx_pow_wait_t;
+
+/**
+ *  POW tag operations.  These are used in the data stored to the POW.
+ */
+typedef enum
+{
+    CVMX_POW_TAG_OP_SWTAG = 0L,         /**< switch the tag (only) for this PP
+                                            - the previous tag should be non-NULL in this case
+                                            - tag switch response required
+                                            - fields used: op, type, tag */
+    CVMX_POW_TAG_OP_SWTAG_FULL = 1L,    /**< switch the tag for this PP, with full information
+                                            - this should be used when the previous tag is NULL
+                                            - tag switch response required
+                                            - fields used: address, op, grp, type, tag */
+    CVMX_POW_TAG_OP_SWTAG_DESCH = 2L,   /**< switch the tag (and/or group) for this PP and de-schedule
+                                            - OK to keep the tag the same and only change the group
+                                            - fields used: op, no_sched, grp, type, tag */
+    CVMX_POW_TAG_OP_DESCH = 3L,         /**< just de-schedule
+                                            - fields used: op, no_sched */
+    CVMX_POW_TAG_OP_ADDWQ = 4L,         /**< create an entirely new work queue entry
+                                            - fields used: address, op, qos, grp, type, tag */
+    CVMX_POW_TAG_OP_UPDATE_WQP_GRP = 5L,/**< just update the work queue pointer and grp for this PP
+                                            - fields used: address, op, grp */
+    CVMX_POW_TAG_OP_SET_NSCHED = 6L,    /**< set the no_sched bit on the de-schedule list
+                                            - does nothing if the selected entry is not on the de-schedule list
+                                            - does nothing if the stored work queue pointer does not match the address field
+                                            - fields used: address, index, op
+                                            Before issuing a *_NSCHED operation, SW must guarantee that all
+                                            prior deschedules and set/clr NSCHED operations are complete and all
+                                            prior switches are complete. The hardware provides the opsdone bit
+                                            and swdone bit for SW polling. After issuing a *_NSCHED operation,
+                                            SW must guarantee that the set/clr NSCHED is complete before
+                                            any subsequent operations. */
+    CVMX_POW_TAG_OP_CLR_NSCHED = 7L,    /**< clears the no_sched bit on the de-schedule list
+                                            - does nothing if the selected entry is not on the de-schedule list
+                                            - does nothing if the stored work queue pointer does not match the address field
+                                            - fields used: address, index, op
+                                            Before issuing a *_NSCHED operation, SW must guarantee that all
+                                            prior deschedules and set/clr NSCHED operations are complete and all
+                                            prior switches are complete. The hardware provides the opsdone bit
+                                            and swdone bit for SW polling. After issuing a *_NSCHED operation,
+                                            SW must guarantee that the set/clr NSCHED is complete before
+                                            any subsequent operations. */
+    CVMX_POW_TAG_OP_NOP = 15L           /**< do nothing */
+} cvmx_pow_tag_op_t;
+
+/**
+ * This structure defines the store data on a store to POW
+ */
+typedef union
+{
+    uint64_t u64;
+    struct
+    {
+#if __BYTE_ORDER == __BIG_ENDIAN
+        uint64_t              no_sched  : 1; /**< don't reschedule this entry. no_sched is used for CVMX_POW_TAG_OP_SWTAG_DESCH and CVMX_POW_TAG_OP_DESCH */
+        uint64_t                unused  : 2;
+        uint64_t                 index  :13; /**< contains index of entry for a CVMX_POW_TAG_OP_*_NSCHED */
+        cvmx_pow_tag_op_t          op   : 4; /**< the operation to perform */
+        uint64_t                unused2 : 2;
+        uint64_t                   qos  : 3; /**< the QOS level for the packet. qos is only used for CVMX_POW_TAG_OP_ADDWQ */
+        uint64_t                   grp  : 4; /**< the group that the work queue entry will be scheduled to grp is used for CVMX_POW_TAG_OP_ADDWQ, CVMX_POW_TAG_OP_SWTAG_FULL, CVMX_POW_TAG_OP_SWTAG_DESCH, and CVMX_POW_TAG_OP_UPDATE_WQP_GRP */
+        cvmx_pow_tag_type_t        type : 3; /**< the type of the tag. type is used for everything except CVMX_POW_TAG_OP_DESCH, CVMX_POW_TAG_OP_UPDATE_WQP_GRP, and CVMX_POW_TAG_OP_*_NSCHED */
+        uint64_t                   tag  :32; /**< the actual tag. tag is used for everything except CVMX_POW_TAG_OP_DESCH, CVMX_POW_TAG_OP_UPDATE_WQP_GRP, and CVMX_POW_TAG_OP_*_NSCHED */
+#else
+        uint64_t                   tag  :32;
+        cvmx_pow_tag_type_t        type : 3;
+        uint64_t                   grp  : 4;
+        uint64_t                   qos  : 3;
+        uint64_t                unused2 : 2;
+        cvmx_pow_tag_op_t          op   : 4;
+        uint64_t                 index  :13;
+        uint64_t                unused  : 2;
+        uint64_t              no_sched  : 1;
+#endif
+    } s;
+} cvmx_pow_tag_req_t;
+
+/**
+ * This structure describes the address to load stuff from POW
+ */
+typedef union
+{
+    uint64_t u64;
+
+    /**
+     * Address for new work request loads (did<2:0> == 0)
+     */
+    struct
+    {
+#if __BYTE_ORDER == __BIG_ENDIAN
+        uint64_t    mem_region      : 2;    /**< Mips64 address region. Should be CVMX_IO_SEG */
+        uint64_t    reserved_49_61  : 13;   /**< Must be zero */
+        uint64_t    is_io           : 1;    /**< Must be one */
+        uint64_t    did             : 8;    /**< the ID of POW -- did<2:0> == 0 in this case */
+        uint64_t    reserved_4_39   : 36;   /**< Must be zero */
+        uint64_t    wait            : 1;    /**< If set, don't return load response until work is available */
+        uint64_t    reserved_0_2    : 3;    /**< Must be zero */
+#else
+        uint64_t    reserved_0_2    : 3;
+        uint64_t    wait            : 1;
+        uint64_t    reserved_4_39   : 36;
+        uint64_t    did             : 8;
+        uint64_t    is_io           : 1;
+        uint64_t    reserved_49_61  : 13;
+        uint64_t    mem_region      : 2;
+#endif
+    } swork;
+
+    /**
+     * Address for loads to get POW internal status
+     */
+    struct
+    {
+#if __BYTE_ORDER == __BIG_ENDIAN
+        uint64_t    mem_region      : 2;    /**< Mips64 address region. Should be CVMX_IO_SEG */
+        uint64_t    reserved_49_61  : 13;   /**< Must be zero */
+        uint64_t    is_io           : 1;    /**< Must be one */
+        uint64_t    did             : 8;    /**< the ID of POW -- did<2:0> == 1 in this case */
+        uint64_t    reserved_10_39  : 30;   /**< Must be zero */
+        uint64_t    coreid          : 4;    /**< The core id to get status for */
+        uint64_t    get_rev         : 1;    /**< If set and get_cur is set, return reverse tag-list pointer rather than forward tag-list pointer */
+        uint64_t    get_cur         : 1;    /**< If set, return current status rather than pending status */
+        uint64_t    get_wqp         : 1;    /**< If set, get the work-queue pointer rather than tag/type */
+        uint64_t    reserved_0_2    : 3;    /**< Must be zero */
+#else
+        uint64_t    reserved_0_2    : 3;
+        uint64_t    get_wqp         : 1;
+        uint64_t    get_cur         : 1;
+        uint64_t    get_rev         : 1;
+        uint64_t    coreid          : 4;
+        uint64_t    reserved_10_39  : 30;
+        uint64_t    did             : 8;
+        uint64_t    is_io           : 1;
+        uint64_t    reserved_49_61  : 13;
+        uint64_t    mem_region      : 2;
+#endif
+    } sstatus;
+
+    /**
+     * Address for memory loads to get POW internal state
+     */
+    struct
+    {
+#if __BYTE_ORDER == __BIG_ENDIAN
+        uint64_t    mem_region      : 2;    /**< Mips64 address region. Should be CVMX_IO_SEG */
+        uint64_t    reserved_49_61  : 13;   /**< Must be zero */
+        uint64_t    is_io           : 1;    /**< Must be one */
+        uint64_t    did             : 8;    /**< the ID of POW -- did<2:0> == 2 in this case */
+        uint64_t    reserved_16_39  : 24;   /**< Must be zero */
+        uint64_t    index           : 11;   /**< POW memory index */
+        uint64_t    get_des         : 1;    /**< If set, return deschedule information rather than the standard
+                                                response for work-queue index (invalid if the work-queue entry is not on the
+                                                deschedule list). */
+        uint64_t    get_wqp         : 1;    /**< If set, get the work-queue pointer rather than tag/type (no effect when get_des set). */
+        uint64_t    reserved_0_2    : 3;    /**< Must be zero */
+#else
+        uint64_t    reserved_0_2    : 3;
+        uint64_t    get_wqp         : 1;
+        uint64_t    get_des         : 1;
+        uint64_t    index           : 11;
+        uint64_t    reserved_16_39  : 24;
+        uint64_t    did             : 8;
+        uint64_t    is_io           : 1;
+        uint64_t    reserved_49_61  : 13;
+        uint64_t    mem_region      : 2;
+#endif
+    } smemload;
+
+    /**
+     * Address for index/pointer loads
+     */
+    struct
+    {
+#if __BYTE_ORDER == __BIG_ENDIAN
+        uint64_t    mem_region      : 2;    /**< Mips64 address region. Should be CVMX_IO_SEG */
+        uint64_t    reserved_49_61  : 13;   /**< Must be zero */
+        uint64_t    is_io           : 1;    /**< Must be one */
+        uint64_t    did             : 8;    /**< the ID of POW -- did<2:0> == 3 in this case */
+        uint64_t    reserved_9_39   : 31;   /**< Must be zero */
+        uint64_t    qosgrp          : 4;    /**< when {get_rmt ==0 AND get_des_get_tail == 0}, this field selects one of
+                                                eight POW internal-input queues (0-7), one per QOS level; values 8-15 are
+                                                illegal in this case;
+                                                when {get_rmt ==0 AND get_des_get_tail == 1}, this field selects one of
+                                                16 deschedule lists (per group);
+                                                when get_rmt ==1, this field selects one of 16 memory-input queue lists.
+                                                The two memory-input queue lists associated with each QOS level are:
+                                                - qosgrp = 0, qosgrp = 8:      QOS0
+                                                - qosgrp = 1, qosgrp = 9:      QOS1
+                                                - qosgrp = 2, qosgrp = 10:     QOS2
+                                                - qosgrp = 3, qosgrp = 11:     QOS3
+                                                - qosgrp = 4, qosgrp = 12:     QOS4
+                                                - qosgrp = 5, qosgrp = 13:     QOS5
+                                                - qosgrp = 6, qosgrp = 14:     QOS6
+                                                - qosgrp = 7, qosgrp = 15:     QOS7 */
+        uint64_t    get_des_get_tail: 1;    /**< If set and get_rmt is clear, return deschedule list indexes
+                                                rather than indexes for the specified qos level; if set and get_rmt is set, return
+                                                the tail pointer rather than the head pointer for the specified qos level. */
+        uint64_t    get_rmt         : 1;    /**< If set, return remote pointers rather than the local indexes for the specified qos level. */
+        uint64_t    reserved_0_2    : 3;    /**< Must be zero */
+#else
+        uint64_t    reserved_0_2    : 3;
+        uint64_t    get_rmt         : 1;
+        uint64_t    get_des_get_tail: 1;
+        uint64_t    qosgrp          : 4;
+        uint64_t    reserved_9_39   : 31;
+        uint64_t    did             : 8;
+        uint64_t    is_io           : 1;
+        uint64_t    reserved_49_61  : 13;
+        uint64_t    mem_region      : 2;
+#endif
+    } sindexload;
+
+    /**
+     * address for NULL_RD request (did<2:0> == 4)
+     * when this is read, HW attempts to change the state to NULL if it is NULL_NULL
+     * (the hardware cannot switch from NULL_NULL to NULL if a POW entry is not available -
+     * software may need to recover by finishing another piece of work before a POW
+     * entry can ever become available.)
+     */
+    struct
+    {
+#if __BYTE_ORDER == __BIG_ENDIAN
+        uint64_t    mem_region      : 2;    /**< Mips64 address region. Should be CVMX_IO_SEG */
+        uint64_t    reserved_49_61  : 13;   /**< Must be zero */
+        uint64_t    is_io           : 1;    /**< Must be one */
+        uint64_t    did             : 8;    /**< the ID of POW -- did<2:0> == 4 in this case */
+        uint64_t    reserved_0_39   : 40;   /**< Must be zero */
+#else
+        uint64_t    reserved_0_39   : 40;
+        uint64_t    did             : 8;
+        uint64_t    is_io           : 1;
+        uint64_t    reserved_49_61  : 13;
+        uint64_t    mem_region      : 2;
+#endif
+    } snull_rd;
+} cvmx_pow_load_addr_t;
+
+/**
+ * This structure defines the response to a load/SENDSINGLE to POW (except CSR reads)
+ */
+typedef union
+{
+    uint64_t u64;
+
+    /**
+     * Response to new work request loads
+     */
+    struct
+    {
+#if __BYTE_ORDER == __BIG_ENDIAN
+        uint64_t    no_work          : 1;   /**< Set when no new work queue entry was returned.
+                                                If there was de-scheduled work, the HW will definitely
+                                                return it. When this bit is set, it could mean
+                                                either mean:
+                                                - There was no work, or
+                                                - There was no work that the HW could find. This
+                                                    case can happen, regardless of the wait bit value
+                                                    in the original request, when there is work
+                                                    in the IQ's that is too deep down the list. */
+        uint64_t    reserved_40_62   : 23;  /**< Must be zero */
+        uint64_t    addr             : 40;  /**< 36 in O1 -- the work queue pointer */
+#else
+        uint64_t    addr             : 40;
+        uint64_t    reserved_40_62   : 23;
+        uint64_t    no_work          : 1;
+#endif
+    } s_work;
+
+    /**
+     * Result for a POW Status Load (when get_cur==0 and get_wqp==0)
+     */
+    struct
+    {
+#if __BYTE_ORDER == __BIG_ENDIAN
+        uint64_t    reserved_62_63  : 2;
+        uint64_t    pend_switch     : 1;    /**< Set when there is a pending non-NULL SWTAG or
+                                                SWTAG_FULL, and the POW entry has not left the list for the original tag. */
+        uint64_t    pend_switch_full: 1;    /**< Set when SWTAG_FULL and pend_switch is set. */
+        uint64_t    pend_switch_null: 1;    /**< Set when there is a pending NULL SWTAG, or an implicit switch to NULL. */
+        uint64_t    pend_desched    : 1;    /**< Set when there is a pending DESCHED or SWTAG_DESCHED. */
+        uint64_t    pend_desched_switch: 1; /**< Set when there is a pending SWTAG_DESCHED and pend_desched is set. */
+        uint64_t    pend_nosched    : 1;    /**< Set when nosched is desired and pend_desched is set. */
+        uint64_t    pend_new_work   : 1;    /**< Set when there is a pending GET_WORK. */
+        uint64_t    pend_new_work_wait: 1;  /**< When pend_new_work is set, this bit indicates that the wait bit was set. */
+        uint64_t    pend_null_rd    : 1;    /**< Set when there is a pending NULL_RD. */
+        uint64_t    pend_nosched_clr: 1;    /**< Set when there is a pending CLR_NSCHED. */
+        uint64_t    reserved_51     : 1;
+        uint64_t    pend_index      : 11;   /**< This is the index when pend_nosched_clr is set. */
+        uint64_t    pend_grp        : 4;    /**< This is the new_grp when (pend_desched AND pend_desched_switch) is set. */
+        uint64_t    reserved_34_35  : 2;
+        uint64_t    pend_type       : 2;    /**< This is the tag type when pend_switch or (pend_desched AND pend_desched_switch) are set. */
+        uint64_t    pend_tag        : 32;   /**< - this is the tag when pend_switch or (pend_desched AND pend_desched_switch) are set. */
+#else
+        uint64_t    pend_tag        : 32;
+        uint64_t    pend_type       : 2;
+        uint64_t    reserved_34_35  : 2;
+        uint64_t    pend_grp        : 4;
+        uint64_t    pend_index      : 11;
+        uint64_t    reserved_51     : 1;
+        uint64_t    pend_nosched_clr: 1;
+        uint64_t    pend_null_rd    : 1;
+        uint64_t    pend_new_work_wait: 1;
+        uint64_t    pend_new_work   : 1;
+        uint64_t    pend_nosched    : 1;
+        uint64_t    pend_desched_switch: 1;
+        uint64_t    pend_desched    : 1;
+        uint64_t    pend_switch_null: 1;
+        uint64_t    pend_switch_full: 1;
+        uint64_t    pend_switch     : 1;
+        uint64_t    reserved_62_63  : 2;
+#endif
+    } s_sstatus0;
+
+    /**
+     * Result for a POW Status Load (when get_cur==0 and get_wqp==1)
+     */
+    struct
+    {
+#if __BYTE_ORDER == __BIG_ENDIAN
+        uint64_t    reserved_62_63  : 2;
+        uint64_t    pend_switch     : 1;    /**< Set when there is a pending non-NULL SWTAG or
+                                                SWTAG_FULL, and the POW entry has not left the list for the original tag. */
+        uint64_t    pend_switch_full: 1;    /**< Set when SWTAG_FULL and pend_switch is set. */
+        uint64_t    pend_switch_null: 1;    /**< Set when there is a pending NULL SWTAG, or an implicit switch to NULL. */
+        uint64_t    pend_desched    : 1;    /**< Set when there is a pending DESCHED or SWTAG_DESCHED. */
+        uint64_t    pend_desched_switch: 1; /**< Set when there is a pending SWTAG_DESCHED and pend_desched is set. */
+        uint64_t    pend_nosched    : 1;    /**< Set when nosched is desired and pend_desched is set. */
+        uint64_t    pend_new_work   : 1;    /**< Set when there is a pending GET_WORK. */
+        uint64_t    pend_new_work_wait: 1;  /**< When pend_new_work is set, this bit indicates that the wait bit was set. */
+        uint64_t    pend_null_rd    : 1;    /**< Set when there is a pending NULL_RD. */
+        uint64_t    pend_nosched_clr: 1;    /**< Set when there is a pending CLR_NSCHED. */
+        uint64_t    reserved_51     : 1;
+        uint64_t    pend_index      : 11;   /**< This is the index when pend_nosched_clr is set. */
+        uint64_t    pend_grp        : 4;    /**< This is the new_grp when (pend_desched AND pend_desched_switch) is set. */
+        uint64_t    pend_wqp        : 36;   /**< This is the wqp when pend_nosched_clr is set. */
+#else
+        uint64_t    pend_wqp        : 36;
+        uint64_t    pend_grp        : 4;
+        uint64_t    pend_index      : 11;
+        uint64_t    reserved_51     : 1;
+        uint64_t    pend_nosched_clr: 1;
+        uint64_t    pend_null_rd    : 1;
+        uint64_t    pend_new_work_wait: 1;
+        uint64_t    pend_new_work   : 1;
+        uint64_t    pend_nosched    : 1;
+        uint64_t    pend_desched_switch: 1;
+        uint64_t    pend_desched    : 1;
+        uint64_t    pend_switch_null: 1;
+        uint64_t    pend_switch_full: 1;
+        uint64_t    pend_switch     : 1;
+        uint64_t    reserved_62_63  : 2;
+#endif
+    } s_sstatus1;
+
+    /**
+     * Result for a POW Status Load (when get_cur==1, get_wqp==0, and get_rev==0)
+     */
+    struct
+    {
+#if __BYTE_ORDER == __BIG_ENDIAN
+        uint64_t    reserved_62_63  : 2;
+        uint64_t    link_index      : 11;    /**< Points to the next POW entry in the tag list when tail == 0 (and
+                                                tag_type is not NULL or NULL_NULL). */
+        uint64_t    index           : 11;   /**< The POW entry attached to the core. */
+        uint64_t    grp             : 4;    /**< The group attached to the core (updated when new tag list entered on SWTAG_FULL). */
+        uint64_t    head            : 1;    /**< Set when this POW entry is at the head of its tag list (also set when in
+                                                the NULL or NULL_NULL state). */
+        uint64_t    tail            : 1;    /**< Set when this POW entry is at the tail of its tag list (also set when in the
+                                                NULL or NULL_NULL state). */
+        uint64_t    tag_type        : 2;    /**< The tag type attached to the core (updated when new tag list
+                                                entered on SWTAG, SWTAG_FULL, or SWTAG_DESCHED). */
+        uint64_t    tag             : 32;   /**< The tag attached to the core (updated when new tag list entered on
+                                                SWTAG, SWTAG_FULL, or SWTAG_DESCHED). */
+#else
+        uint64_t    tag             : 32;
+        uint64_t    tag_type        : 2;
+        uint64_t    tail            : 1;
+        uint64_t    head            : 1;
+        uint64_t    grp             : 4;
+        uint64_t    index           : 11;
+        uint64_t    link_index      : 11;
+        uint64_t    reserved_62_63  : 2;
+#endif
+    } s_sstatus2;
+
+    /**
+     * Result for a POW Status Load (when get_cur==1, get_wqp==0, and get_rev==1)
+     */
+    struct
+    {
+#if __BYTE_ORDER == __BIG_ENDIAN
+        uint64_t    reserved_62_63  : 2;
+        uint64_t    revlink_index   : 11;   /**< Points to the prior POW entry in the tag list when head == 0
+                                                (and tag_type is not NULL or NULL_NULL). This field is unpredictable
+                                                when the core's state is NULL or NULL_NULL. */
+        uint64_t    index           : 11;   /**< The POW entry attached to the core. */
+        uint64_t    grp             : 4;    /**< The group attached to the core (updated when new tag list entered on SWTAG_FULL). */
+        uint64_t    head            : 1;    /**< Set when this POW entry is at the head of its tag list (also set when in
+                                                the NULL or NULL_NULL state). */
+        uint64_t    tail            : 1;    /**< Set when this POW entry is at the tail of its tag list (also set when in the
+                                                NULL or NULL_NULL state). */
+        uint64_t    tag_type        : 2;    /**< The tag type attached to the core (updated when new tag list
+                                                entered on SWTAG, SWTAG_FULL, or SWTAG_DESCHED). */
+        uint64_t    tag             : 32;   /**< The tag attached to the core (updated when new tag list entered on
+                                                SWTAG, SWTAG_FULL, or SWTAG_DESCHED). */
+#else
+        uint64_t    tag             : 32;
+        uint64_t    tag_type        : 2;
+        uint64_t    tail            : 1;
+        uint64_t    head            : 1;
+        uint64_t    grp             : 4;
+        uint64_t    index           : 11;
+        uint64_t    revlink_index   : 11;
+        uint64_t    reserved_62_63  : 2;
+#endif
+    } s_sstatus3;
+
+    /**
+     * Result for a POW Status Load (when get_cur==1, get_wqp==1, and get_rev==0)
+     */
+    struct
+    {
+#if __BYTE_ORDER == __BIG_ENDIAN
+        uint64_t    reserved_62_63  : 2;
+        uint64_t    link_index      : 11;    /**< Points to the next POW entry in the tag list when tail == 0 (and
+                                                tag_type is not NULL or NULL_NULL). */
+        uint64_t    index           : 11;   /**< The POW entry attached to the core. */
+        uint64_t    grp             : 4;    /**< The group attached to the core (updated when new tag list entered on SWTAG_FULL). */
+        uint64_t    wqp             : 36;   /**< The wqp attached to the core (updated when new tag list entered on SWTAG_FULL). */
+#else
+        uint64_t    wqp             : 36;
+        uint64_t    grp             : 4;
+        uint64_t    index           : 11;
+        uint64_t    link_index      : 11;
+        uint64_t    reserved_62_63  : 2;
+#endif
+    } s_sstatus4;
+
+    /**
+     * Result for a POW Status Load (when get_cur==1, get_wqp==1, and get_rev==1)
+     */
+    struct
+    {
+#if __BYTE_ORDER == __BIG_ENDIAN
+        uint64_t    reserved_62_63  : 2;
+        uint64_t    revlink_index   : 11;   /**< Points to the prior POW entry in the tag list when head == 0
+                                                (and tag_type is not NULL or NULL_NULL). This field is unpredictable
+                                                when the core's state is NULL or NULL_NULL. */
+        uint64_t    index           : 11;   /**< The POW entry attached to the core. */
+        uint64_t    grp             : 4;    /**< The group attached to the core (updated when new tag list entered on SWTAG_FULL). */
+        uint64_t    wqp             : 36;   /**< The wqp attached to the core (updated when new tag list entered on SWTAG_FULL). */
+#else
+        uint64_t    wqp             : 36;
+        uint64_t    grp             : 4;
+        uint64_t    index           : 11;
+        uint64_t    revlink_index   : 11;
+        uint64_t    reserved_62_63  : 2;
+#endif
+    } s_sstatus5;
+
+    /**
+     * Result For POW Memory Load (get_des == 0 and get_wqp == 0)
+     */
+    struct
+    {
+#if __BYTE_ORDER == __BIG_ENDIAN
+        uint64_t    reserved_51_63  : 13;
+        uint64_t    next_index      : 11;    /**< The next entry in the input, free, descheduled_head list
+                                                (unpredictable if entry is the tail of the list). */
+        uint64_t    grp             : 4;    /**< The group of the POW entry. */
+        uint64_t    reserved_35     : 1;
+        uint64_t    tail            : 1;    /**< Set when this POW entry is at the tail of its tag list (also set when in the
+                                                NULL or NULL_NULL state). */
+        uint64_t    tag_type        : 2;    /**< The tag type of the POW entry. */
+        uint64_t    tag             : 32;   /**< The tag of the POW entry. */
+#else
+        uint64_t    tag             : 32;
+        uint64_t    tag_type        : 2;
+        uint64_t    tail            : 1;
+        uint64_t    reserved_35     : 1;
+        uint64_t    grp             : 4;
+        uint64_t    next_index      : 11;
+        uint64_t    reserved_51_63  : 13;
+#endif
+    } s_smemload0;
+
+    /**
+     * Result For POW Memory Load (get_des == 0 and get_wqp == 1)
+     */
+    struct
+    {
+#if __BYTE_ORDER == __BIG_ENDIAN
+        uint64_t    reserved_51_63  : 13;
+        uint64_t    next_index      : 11;    /**< The next entry in the input, free, descheduled_head list
+                                                (unpredictable if entry is the tail of the list). */
+        uint64_t    grp             : 4;    /**< The group of the POW entry. */
+        uint64_t    wqp             : 36;   /**< The WQP held in the POW entry. */
+#else
+        uint64_t    wqp             : 36;
+        uint64_t    grp             : 4;
+        uint64_t    next_index      : 11;
+        uint64_t    reserved_51_63  : 13;
+#endif
+    } s_smemload1;
+
+    /**
+     * Result For POW Memory Load (get_des == 1)
+     */
+    struct
+    {
+#if __BYTE_ORDER == __BIG_ENDIAN
+        uint64_t    reserved_51_63  : 13;
+        uint64_t    fwd_index       : 11;   /**< The next entry in the tag list connected to the descheduled head. */
+        uint64_t    grp             : 4;    /**< The group of the POW entry. */
+        uint64_t    nosched         : 1;    /**< The nosched bit for the POW entry. */
+        uint64_t    pend_switch     : 1;    /**< There is a pending tag switch */
+        uint64_t    pend_type       : 2;    /**< The next tag type for the new tag list when pend_switch is set. */
+        uint64_t    pend_tag        : 32;   /**< The next tag for the new tag list when pend_switch is set. */
+#else
+        uint64_t    pend_tag        : 32;
+        uint64_t    pend_type       : 2;
+        uint64_t    pend_switch     : 1;
+        uint64_t    nosched         : 1;
+        uint64_t    grp             : 4;
+        uint64_t    fwd_index       : 11;
+        uint64_t    reserved_51_63  : 13;
+#endif
+    } s_smemload2;
+
+    /**
+     * Result For POW Index/Pointer Load (get_rmt == 0/get_des_get_tail == 0)
+     */
+    struct
+    {
+#if __BYTE_ORDER == __BIG_ENDIAN
+        uint64_t    reserved_52_63  : 12;
+        uint64_t    free_val        : 1;    /**< - set when there is one or more POW entries on the free list. */
+        uint64_t    free_one        : 1;    /**< - set when there is exactly one POW entry on the free list. */
+        uint64_t    reserved_49     : 1;
+        uint64_t    free_head       : 11;   /**< - when free_val is set, indicates the first entry on the free list. */
+        uint64_t    reserved_37     : 1;
+        uint64_t    free_tail       : 11;   /**< - when free_val is set, indicates the last entry on the free list. */
+        uint64_t    loc_val         : 1;    /**< - set when there is one or more POW entries on the input Q list selected by qosgrp. */
+        uint64_t    loc_one         : 1;    /**< - set when there is exactly one POW entry on the input Q list selected by qosgrp. */
+        uint64_t    reserved_23     : 1;
+        uint64_t    loc_head        : 11;   /**< - when loc_val is set, indicates the first entry on the input Q list selected by qosgrp. */
+        uint64_t    reserved_11     : 1;
+        uint64_t    loc_tail        : 11;   /**< - when loc_val is set, indicates the last entry on the input Q list selected by qosgrp. */
+#else
+        uint64_t    loc_tail        : 11;
+        uint64_t    reserved_11     : 1;
+        uint64_t    loc_head        : 11;
+        uint64_t    reserved_23     : 1;
+        uint64_t    loc_one         : 1;
+        uint64_t    loc_val         : 1;
+        uint64_t    free_tail       : 11;
+        uint64_t    reserved_37     : 1;
+        uint64_t    free_head       : 11;
+        uint64_t    reserved_49     : 1;
+        uint64_t    free_one        : 1;
+        uint64_t    free_val        : 1;
+        uint64_t    reserved_52_63  : 12;
+#endif
+    } sindexload0;
+
+    /**
+     * Result For POW Index/Pointer Load (get_rmt == 0/get_des_get_tail == 1)
+     */
+    struct
+    {
+#if __BYTE_ORDER == __BIG_ENDIAN
+        uint64_t    reserved_52_63  : 12;
+        uint64_t    nosched_val     : 1;    /**< - set when there is one or more POW entries on the nosched list. */
+        uint64_t    nosched_one     : 1;    /**< - set when there is exactly one POW entry on the nosched list. */
+        uint64_t    reserved_49     : 1;
+        uint64_t    nosched_head    : 11;    /**< - when nosched_val is set, indicates the first entry on the nosched list. */
+        uint64_t    reserved_37     : 1;
+        uint64_t    nosched_tail    : 11;    /**< - when nosched_val is set, indicates the last entry on the nosched list. */
+        uint64_t    des_val         : 1;    /**< - set when there is one or more descheduled heads on the descheduled list selected by qosgrp. */
+        uint64_t    des_one         : 1;    /**< - set when there is exactly one descheduled head on the descheduled list selected by qosgrp. */
+        uint64_t    reserved_23     : 1;
+        uint64_t    des_head        : 11;    /**< - when des_val is set, indicates the first descheduled head on the descheduled list selected by qosgrp. */
+        uint64_t    reserved_11     : 1;
+        uint64_t    des_tail        : 11;    /**< - when des_val is set, indicates the last descheduled head on the descheduled list selected by qosgrp. */
+#else
+        uint64_t    des_tail        : 11;
+        uint64_t    reserved_11     : 1;
+        uint64_t    des_head        : 11;
+        uint64_t    reserved_23     : 1;
+        uint64_t    des_one         : 1;
+        uint64_t    des_val         : 1;
+        uint64_t    nosched_tail    : 11;
+        uint64_t    reserved_37     : 1;
+        uint64_t    nosched_head    : 11;
+        uint64_t    reserved_49     : 1;
+        uint64_t    nosched_one     : 1;
+        uint64_t    nosched_val     : 1;
+        uint64_t    reserved_52_63  : 12;
+#endif
+    } sindexload1;
+
+    /**
+     * Result For POW Index/Pointer Load (get_rmt == 1/get_des_get_tail == 0)
+     */
+    struct
+    {
+#if __BYTE_ORDER == __BIG_ENDIAN
+        uint64_t    reserved_39_63  : 25;
+        uint64_t    rmt_is_head     : 1;    /**< Set when this DRAM list is the current head (i.e. is the next to
+                                                be reloaded when the POW hardware reloads a POW entry from DRAM). The
+                                                POW hardware alternates between the two DRAM lists associated with a QOS
+                                                level when it reloads work from DRAM into the POW unit. */
+        uint64_t    rmt_val         : 1;    /**< Set when the DRAM portion of the input Q list selected by qosgrp
+                                                contains one or more pieces of work. */
+        uint64_t    rmt_one         : 1;    /**< Set when the DRAM portion of the input Q list selected by qosgrp
+                                                contains exactly one piece of work. */
+        uint64_t    rmt_head        : 36;   /**< When rmt_val is set, indicates the first piece of work on the
+                                                DRAM input Q list selected by qosgrp. */
+#else
+        uint64_t    rmt_head        : 36;
+        uint64_t    rmt_one         : 1;
+        uint64_t    rmt_val         : 1;
+        uint64_t    rmt_is_head     : 1;
+        uint64_t    reserved_39_63  : 25;
+#endif
+    } sindexload2;
+
+    /**
+     * Result For POW Index/Pointer Load (get_rmt == 1/get_des_get_tail == 1)
+     */
+    struct
+    {
+#if __BYTE_ORDER == __BIG_ENDIAN
+        uint64_t    reserved_39_63  : 25;
+        uint64_t    rmt_is_head     : 1;    /**< - set when this DRAM list is the current head (i.e. is the next to
+                                                be reloaded when the POW hardware reloads a POW entry from DRAM). The
+                                                POW hardware alternates between the two DRAM lists associated with a QOS
+                                                level when it reloads work from DRAM into the POW unit. */
+        uint64_t    rmt_val         : 1;    /**< - set when the DRAM portion of the input Q list selected by qosgrp
+                                                contains one or more pieces of work. */
+        uint64_t    rmt_one         : 1;    /**< - set when the DRAM portion of the input Q list selected by qosgrp
+                                                contains exactly one piece of work. */
+        uint64_t    rmt_tail        : 36;   /**< - when rmt_val is set, indicates the last piece of work on the DRAM
+                                                input Q list selected by qosgrp. */
+#else
+        uint64_t    rmt_tail        : 36;
+        uint64_t    rmt_one         : 1;
+        uint64_t    rmt_val         : 1;
+        uint64_t    rmt_is_head     : 1;
+        uint64_t    reserved_39_63  : 25;
+#endif
+    } sindexload3;
+
+    /**
+     * Response to NULL_RD request loads
+     */
+    struct
+    {
+#if __BYTE_ORDER == __BIG_ENDIAN
+        uint64_t    unused  : 62;
+        uint64_t    state    : 2;  /**< of type cvmx_pow_tag_type_t. state is one of the following:
+                                        - CVMX_POW_TAG_TYPE_ORDERED
+                                        - CVMX_POW_TAG_TYPE_ATOMIC
+                                        - CVMX_POW_TAG_TYPE_NULL
+                                        - CVMX_POW_TAG_TYPE_NULL_NULL */
+#else
+        uint64_t    state    : 2;
+        uint64_t    unused  : 62;
+#endif
+    } s_null_rd;
+
+} cvmx_pow_tag_load_resp_t;
+
+/**
+ * This structure describes the address used for stores to the POW.
+ *  The store address is meaningful on stores to the POW.  The hardware assumes that an aligned
+ *  64-bit store was used for all these stores.
+ *  Note the assumption that the work queue entry is aligned on an 8-byte
+ *  boundary (since the low-order 3 address bits must be zero).
+ *  Note that not all fields are used by all operations.
+ *
+ *  NOTE: The following is the behavior of the pending switch bit at the PP
+ *       for POW stores (i.e. when did<7:3> == 0xc)
+ *     - did<2:0> == 0      => pending switch bit is set
+ *     - did<2:0> == 1      => no affect on the pending switch bit
+ *     - did<2:0> == 3      => pending switch bit is cleared
+ *     - did<2:0> == 7      => no affect on the pending switch bit
+ *     - did<2:0> == others => must not be used
+ *     - No other loads/stores have an affect on the pending switch bit
+ *     - The switch bus from POW can clear the pending switch bit
+ *
+ *  NOTE: did<2:0> == 2 is used by the HW for a special single-cycle ADDWQ command
+ *  that only contains the pointer). SW must never use did<2:0> == 2.
+ */
+typedef union
+{
+    /**
+     * Unsigned 64 bit integer representation of store address
+     */
+    uint64_t u64;
+
+    struct
+    {
+#if __BYTE_ORDER == __BIG_ENDIAN
+        uint64_t    mem_reg         : 2;    /**< Memory region.  Should be CVMX_IO_SEG in most cases */
+        uint64_t    reserved_49_61  : 13;   /**< Must be zero */
+        uint64_t    is_io           : 1;    /**< Must be one */
+        uint64_t    did             : 8;    /**< Device ID of POW.  Note that different sub-dids are used. */
+        uint64_t    reserved_36_39  : 4;    /**< Must be zero */
+        uint64_t    addr            : 36;   /**< Address field. addr<2:0> must be zero */
+#else
+        uint64_t    addr            : 36;
+        uint64_t    reserved_36_39  : 4;
+        uint64_t    did             : 8;
+        uint64_t    is_io           : 1;
+        uint64_t    reserved_49_61  : 13;
+        uint64_t    mem_reg         : 2;
+#endif
+    } stag;
+} cvmx_pow_tag_store_addr_t;
+
+/**
+ * decode of the store data when an IOBDMA SENDSINGLE is sent to POW
+ */
+typedef union
+{
+    uint64_t u64;
+
+    struct
+    {
+#if __BYTE_ORDER == __BIG_ENDIAN
+        uint64_t    scraddr : 8;    /**< the (64-bit word) location in scratchpad to write to (if len != 0) */
+        uint64_t    len     : 8;    /**< the number of words in the response (0 => no response) */
+        uint64_t    did     : 8;    /**< the ID of the device on the non-coherent bus */
+        uint64_t    unused  :36;
+        uint64_t    wait    : 1;    /**< if set, don't return load response until work is available */
+        uint64_t    unused2 : 3;
+#else
+        uint64_t    unused2 : 3;
+        uint64_t    wait    : 1;
+        uint64_t    unused  :36;
+        uint64_t    did     : 8;
+        uint64_t    len     : 8;
+        uint64_t    scraddr : 8;
+#endif
+    } s;
+
+} cvmx_pow_iobdma_store_t;
+
+
+/* CSR typedefs have been moved to cvmx-csr-*.h */
+
+/**
+ * Get the POW tag for this core. This returns the current
+ * tag type, tag, group, and POW entry index associated with
+ * this core. Index is only valid if the tag type isn't NULL_NULL.
+ * If a tag switch is pending this routine returns the tag before
+ * the tag switch, not after.
+ *
+ * @return Current tag
+ */
+static inline cvmx_pow_tag_req_t cvmx_pow_get_current_tag(void)
+{
+    cvmx_pow_load_addr_t load_addr;
+    cvmx_pow_tag_load_resp_t load_resp;
+    cvmx_pow_tag_req_t result;
+
+    load_addr.u64 = 0;
+    load_addr.sstatus.mem_region = CVMX_IO_SEG;
+    load_addr.sstatus.is_io = 1;
+    load_addr.sstatus.did = CVMX_OCT_DID_TAG_TAG1;
+    load_addr.sstatus.coreid = cvmx_get_core_num();
+    load_addr.sstatus.get_cur = 1;
+    load_resp.u64 = cvmx_read_csr(load_addr.u64);
+    result.u64 = 0;
+    result.s.grp = load_resp.s_sstatus2.grp;
+    result.s.index = load_resp.s_sstatus2.index;
+    result.s.type = (cvmx_pow_tag_type_t)load_resp.s_sstatus2.tag_type;
+    result.s.tag = load_resp.s_sstatus2.tag;
+    return result;
+}
+
+
+/**
+ * Get the POW WQE for this core. This returns the work queue
+ * entry currently associated with this core.
+ *
+ * @return WQE pointer
+ */
+static inline cvmx_wqe_t *cvmx_pow_get_current_wqp(void)
+{
+    cvmx_pow_load_addr_t load_addr;
+    cvmx_pow_tag_load_resp_t load_resp;
+
+    load_addr.u64 = 0;
+    load_addr.sstatus.mem_region = CVMX_IO_SEG;
+    load_addr.sstatus.is_io = 1;
+    load_addr.sstatus.did = CVMX_OCT_DID_TAG_TAG1;
+    load_addr.sstatus.coreid = cvmx_get_core_num();
+    load_addr.sstatus.get_cur = 1;
+    load_addr.sstatus.get_wqp = 1;
+    load_resp.u64 = cvmx_read_csr(load_addr.u64);
+    return (cvmx_wqe_t*)cvmx_phys_to_ptr(load_resp.s_sstatus4.wqp);
+}
+
+
+/**
+ * @INTERNAL
+ * Print a warning if a tag switch is pending for this core
+ *
+ * @param function Function name checking for a pending tag switch
+ */
+static inline void __cvmx_pow_warn_if_pending_switch(const char *function)
+{
+    uint64_t switch_complete;
+    CVMX_MF_CHORD(switch_complete);
+    cvmx_warn_if(!switch_complete, "%s called with tag switch in progress\n", function);
+}
+
+
+/**
+ * Waits for a tag switch to complete by polling the completion bit.
+ * Note that switches to NULL complete immediately and do not need
+ * to be waited for.
+ */
+static inline void cvmx_pow_tag_sw_wait(void)
+{
+    const uint64_t MAX_CYCLES = 1ull<<31;
+    uint64_t switch_complete;
+    uint64_t start_cycle = cvmx_get_cycle();
+    while (1)
+    {
+        CVMX_MF_CHORD(switch_complete);
+        if (cvmx_unlikely(switch_complete))
+            break;
+        if (cvmx_unlikely(cvmx_get_cycle() > start_cycle + MAX_CYCLES))
+        {
+            cvmx_dprintf("WARNING: Tag switch is taking a long time, possible deadlock\n");
+            start_cycle = -MAX_CYCLES-1;
+        }
+    }
+}
+
+
+/**
+ * Synchronous work request.  Requests work from the POW.
+ * This function does NOT wait for previous tag switches to complete,
+ * so the caller must ensure that there is not a pending tag switch.
+ *
+ * @param wait   When set, call stalls until work becomes avaiable, or times out.
+ *               If not set, returns immediately.
+ *
+ * @return Returns the WQE pointer from POW. Returns NULL if no work was available.
+ */
+static inline cvmx_wqe_t * cvmx_pow_work_request_sync_nocheck(cvmx_pow_wait_t wait)
+{
+    cvmx_pow_load_addr_t ptr;
+    cvmx_pow_tag_load_resp_t result;
+
+    if (CVMX_ENABLE_POW_CHECKS)
+        __cvmx_pow_warn_if_pending_switch(__FUNCTION__);
+
+    ptr.u64 = 0;
+    ptr.swork.mem_region = CVMX_IO_SEG;
+    ptr.swork.is_io = 1;
+    ptr.swork.did = CVMX_OCT_DID_TAG_SWTAG;
+    ptr.swork.wait = wait;
+
+    result.u64 = cvmx_read_csr(ptr.u64);
+
+    if (result.s_work.no_work)
+        return NULL;
+    else
+        return (cvmx_wqe_t*)cvmx_phys_to_ptr(result.s_work.addr);
+}
+
+
+/**
+ * Synchronous work request.  Requests work from the POW.
+ * This function waits for any previous tag switch to complete before
+ * requesting the new work.
+ *
+ * @param wait   When set, call stalls until work becomes avaiable, or times out.
+ *               If not set, returns immediately.
+ *
+ * @return Returns the WQE pointer from POW. Returns NULL if no work was available.
+ */
+static inline cvmx_wqe_t * cvmx_pow_work_request_sync(cvmx_pow_wait_t wait)
+{
+    if (CVMX_ENABLE_POW_CHECKS)
+        __cvmx_pow_warn_if_pending_switch(__FUNCTION__);
+
+    /* Must not have a switch pending when requesting work */
+    cvmx_pow_tag_sw_wait();
+    return(cvmx_pow_work_request_sync_nocheck(wait));
+
+}
+
+
+/**
+ * Synchronous null_rd request.  Requests a switch out of NULL_NULL POW state.
+ * This function waits for any previous tag switch to complete before
+ * requesting the null_rd.
+ *
+ * @return Returns the POW state of type cvmx_pow_tag_type_t.
+ */
+static inline cvmx_pow_tag_type_t cvmx_pow_work_request_null_rd(void)
+{
+    cvmx_pow_load_addr_t ptr;
+    cvmx_pow_tag_load_resp_t result;
+
+    if (CVMX_ENABLE_POW_CHECKS)
+        __cvmx_pow_warn_if_pending_switch(__FUNCTION__);
+
+    /* Must not have a switch pending when requesting work */
+    cvmx_pow_tag_sw_wait();
+
+    ptr.u64 = 0;
+    ptr.snull_rd.mem_region = CVMX_IO_SEG;
+    ptr.snull_rd.is_io = 1;
+    ptr.snull_rd.did = CVMX_OCT_DID_TAG_NULL_RD;
+
+    result.u64 = cvmx_read_csr(ptr.u64);
+
+    return (cvmx_pow_tag_type_t)result.s_null_rd.state;
+}
+
+
+/**
+ * Asynchronous work request.  Work is requested from the POW unit, and should later
+ * be checked with function cvmx_pow_work_response_async.
+ * This function does NOT wait for previous tag switches to complete,
+ * so the caller must ensure that there is not a pending tag switch.
+ *
+ * @param scr_addr Scratch memory address that response will be returned to,
+ *                  which is either a valid WQE, or a response with the invalid bit set.
+ *                  Byte address, must be 8 byte aligned.
+ * @param wait      1 to cause response to wait for work to become available (or timeout)
+ *                  0 to cause response to return immediately
+ */
+static inline void cvmx_pow_work_request_async_nocheck(int scr_addr, cvmx_pow_wait_t wait)
+{
+    cvmx_pow_iobdma_store_t data;
+
+    if (CVMX_ENABLE_POW_CHECKS)
+        __cvmx_pow_warn_if_pending_switch(__FUNCTION__);
+
+    /* scr_addr must be 8 byte aligned */
+    data.s.scraddr = scr_addr >> 3;
+    data.s.len = 1;
+    data.s.did = CVMX_OCT_DID_TAG_SWTAG;
+    data.s.wait = wait;
+    cvmx_send_single(data.u64);
+}
+/**
+ * Asynchronous work request.  Work is requested from the POW unit, and should later
+ * be checked with function cvmx_pow_work_response_async.
+ * This function waits for any previous tag switch to complete before
+ * requesting the new work.
+ *
+ * @param scr_addr Scratch memory address that response will be returned to,
+ *                  which is either a valid WQE, or a response with the invalid bit set.
+ *                  Byte address, must be 8 byte aligned.
+ * @param wait      1 to cause response to wait for work to become available (or timeout)
+ *                  0 to cause response to return immediately
+ */
+static inline void cvmx_pow_work_request_async(int scr_addr, cvmx_pow_wait_t wait)
+{
+    if (CVMX_ENABLE_POW_CHECKS)
+        __cvmx_pow_warn_if_pending_switch(__FUNCTION__);
+
+    /* Must not have a switch pending when requesting work */
+    cvmx_pow_tag_sw_wait();
+    cvmx_pow_work_request_async_nocheck(scr_addr, wait);
+}
+
+
+/**
+ * Gets result of asynchronous work request.  Performs a IOBDMA sync
+ * to wait for the response.
+ *
+ * @param scr_addr Scratch memory address to get result from
+ *                  Byte address, must be 8 byte aligned.
+ * @return Returns the WQE from the scratch register, or NULL if no work was available.
+ */
+static inline cvmx_wqe_t * cvmx_pow_work_response_async(int scr_addr)
+{
+    cvmx_pow_tag_load_resp_t result;
+
+    CVMX_SYNCIOBDMA;
+    result.u64 = cvmx_scratch_read64(scr_addr);
+
+    if (result.s_work.no_work)
+        return NULL;
+    else
+        return (cvmx_wqe_t*)cvmx_phys_to_ptr(result.s_work.addr);
+}
+
+
+/**
+ * Checks if a work queue entry pointer returned by a work
+ * request is valid.  It may be invalid due to no work
+ * being available or due to a timeout.
+ *
+ * @param wqe_ptr pointer to a work queue entry returned by the POW
+ *
+ * @return 0 if pointer is valid
+ *         1 if invalid (no work was returned)
+ */
+static inline uint64_t cvmx_pow_work_invalid(cvmx_wqe_t *wqe_ptr)
+{
+    return (wqe_ptr == NULL);
+}
+
+
+
+/**
+ * Starts a tag switch to the provided tag value and tag type.  Completion for
+ * the tag switch must be checked for separately.
+ * This function does NOT update the
+ * work queue entry in dram to match tag value and type, so the application must
+ * keep track of these if they are important to the application.
+ * This tag switch command must not be used for switches to NULL, as the tag
+ * switch pending bit will be set by the switch request, but never cleared by the
+ * hardware.
+ *
+ * NOTE: This should not be used when switching from a NULL tag.  Use
+ * cvmx_pow_tag_sw_full() instead.
+ *
+ * This function does no checks, so the caller must ensure that any previous tag
+ * switch has completed.
+ *
+ * @param tag      new tag value
+ * @param tag_type new tag type (ordered or atomic)
+ */
+static inline void cvmx_pow_tag_sw_nocheck(uint32_t tag, cvmx_pow_tag_type_t tag_type)
+{
+    cvmx_addr_t ptr;
+    cvmx_pow_tag_req_t tag_req;
+
+    if (CVMX_ENABLE_POW_CHECKS)
+    {
+        cvmx_pow_tag_req_t current_tag;
+        __cvmx_pow_warn_if_pending_switch(__FUNCTION__);
+        current_tag = cvmx_pow_get_current_tag();
+        cvmx_warn_if(current_tag.s.type == CVMX_POW_TAG_TYPE_NULL_NULL, "%s called with NULL_NULL tag\n", __FUNCTION__);
+        cvmx_warn_if(current_tag.s.type == CVMX_POW_TAG_TYPE_NULL, "%s called with NULL tag\n", __FUNCTION__);
+        cvmx_warn_if((current_tag.s.type == tag_type) && (current_tag.s.tag == tag), "%s called to perform a tag switch to the same tag\n", __FUNCTION__);
+        cvmx_warn_if(tag_type == CVMX_POW_TAG_TYPE_NULL, "%s called to perform a tag switch to NULL. Use cvmx_pow_tag_sw_null() instead\n", __FUNCTION__);
+    }
+
+    /* Note that WQE in DRAM is not updated here, as the POW does not read from DRAM
+    ** once the WQE is in flight.  See hardware manual for complete details.
+    ** It is the application's responsibility to keep track of the current tag
+    ** value if that is important.
+    */
+
+
+    tag_req.u64 = 0;
+    tag_req.s.op = CVMX_POW_TAG_OP_SWTAG;
+    tag_req.s.tag = tag;
+    tag_req.s.type = tag_type;
+
+    ptr.u64 = 0;
+    ptr.sio.mem_region = CVMX_IO_SEG;
+    ptr.sio.is_io = 1;
+    ptr.sio.did = CVMX_OCT_DID_TAG_SWTAG;
+
+    /* once this store arrives at POW, it will attempt the switch
+       software must wait for the switch to complete separately */
+    cvmx_write_io(ptr.u64, tag_req.u64);
+}
+
+
+/**
+ * Starts a tag switch to the provided tag value and tag type.  Completion for
+ * the tag switch must be checked for separately.
+ * This function does NOT update the
+ * work queue entry in dram to match tag value and type, so the application must
+ * keep track of these if they are important to the application.
+ * This tag switch command must not be used for switches to NULL, as the tag
+ * switch pending bit will be set by the switch request, but never cleared by the
+ * hardware.
+ *
+ * NOTE: This should not be used when switching from a NULL tag.  Use
+ * cvmx_pow_tag_sw_full() instead.
+ *
+ * This function waits for any previous tag switch to complete, and also
+ * displays an error on tag switches to NULL.
+ *
+ * @param tag      new tag value
+ * @param tag_type new tag type (ordered or atomic)
+ */
+static inline void cvmx_pow_tag_sw(uint32_t tag, cvmx_pow_tag_type_t tag_type)
+{
+    if (CVMX_ENABLE_POW_CHECKS)
+        __cvmx_pow_warn_if_pending_switch(__FUNCTION__);
+
+    /* Note that WQE in DRAM is not updated here, as the POW does not read from DRAM
+    ** once the WQE is in flight.  See hardware manual for complete details.
+    ** It is the application's responsibility to keep track of the current tag
+    ** value if that is important.
+    */
+
+    /* Ensure that there is not a pending tag switch, as a tag switch cannot be started
+    ** if a previous switch is still pending.  */
+    cvmx_pow_tag_sw_wait();
+    cvmx_pow_tag_sw_nocheck(tag, tag_type);
+}
+
+
+/**
+ * Starts a tag switch to the provided tag value and tag type.  Completion for
+ * the tag switch must be checked for separately.
+ * This function does NOT update the
+ * work queue entry in dram to match tag value and type, so the application must
+ * keep track of these if they are important to the application.
+ * This tag switch command must not be used for switches to NULL, as the tag
+ * switch pending bit will be set by the switch request, but never cleared by the
+ * hardware.
+ *
+ * This function must be used for tag switches from NULL.
+ *
+ * This function does no checks, so the caller must ensure that any previous tag
+ * switch has completed.
+ *
+ * @param wqp      pointer to work queue entry to submit.  This entry is updated to match the other parameters
+ * @param tag      tag value to be assigned to work queue entry
+ * @param tag_type type of tag
+ * @param group      group value for the work queue entry.
+ */
+static inline void cvmx_pow_tag_sw_full_nocheck(cvmx_wqe_t *wqp, uint32_t tag, cvmx_pow_tag_type_t tag_type, uint64_t group)
+{
+    cvmx_addr_t ptr;
+    cvmx_pow_tag_req_t tag_req;
+
+    if (CVMX_ENABLE_POW_CHECKS)
+    {
+        cvmx_pow_tag_req_t current_tag;
+        __cvmx_pow_warn_if_pending_switch(__FUNCTION__);
+        current_tag = cvmx_pow_get_current_tag();
+        cvmx_warn_if(current_tag.s.type == CVMX_POW_TAG_TYPE_NULL_NULL, "%s called with NULL_NULL tag\n", __FUNCTION__);
+        cvmx_warn_if((current_tag.s.type == tag_type) && (current_tag.s.tag == tag), "%s called to perform a tag switch to the same tag\n", __FUNCTION__);
+        cvmx_warn_if(tag_type == CVMX_POW_TAG_TYPE_NULL, "%s called to perform a tag switch to NULL. Use cvmx_pow_tag_sw_null() instead\n", __FUNCTION__);
+        if (wqp != cvmx_phys_to_ptr(0x80))
+            cvmx_warn_if(wqp != cvmx_pow_get_current_wqp(), "%s passed WQE(%p) doesn't match the address in the POW(%p)\n", __FUNCTION__, wqp, cvmx_pow_get_current_wqp());
+    }
+
+    /* Note that WQE in DRAM is not updated here, as the POW does not read from DRAM
+    ** once the WQE is in flight.  See hardware manual for complete details.
+    ** It is the application's responsibility to keep track of the current tag
+    ** value if that is important.
+    */
+
+    tag_req.u64 = 0;
+    tag_req.s.op = CVMX_POW_TAG_OP_SWTAG_FULL;
+    tag_req.s.tag = tag;
+    tag_req.s.type = tag_type;
+    tag_req.s.grp = group;
+
+    ptr.u64 = 0;
+    ptr.sio.mem_region = CVMX_IO_SEG;
+    ptr.sio.is_io = 1;
+    ptr.sio.did = CVMX_OCT_DID_TAG_SWTAG;
+    ptr.sio.offset = CAST64(wqp);
+
+    /* once this store arrives at POW, it will attempt the switch
+       software must wait for the switch to complete separately */
+    cvmx_write_io(ptr.u64, tag_req.u64);
+}
+
+
+/**
+ * Starts a tag switch to the provided tag value and tag type.  Completion for
+ * the tag switch must be checked for separately.
+ * This function does NOT update the
+ * work queue entry in dram to match tag value and type, so the application must
+ * keep track of these if they are important to the application.
+ * This tag switch command must not be used for switches to NULL, as the tag
+ * switch pending bit will be set by the switch request, but never cleared by the
+ * hardware.
+ *
+ * This function must be used for tag switches from NULL.
+ *
+ * This function waits for any pending tag switches to complete
+ * before requesting the tag switch.
+ *
+ * @param wqp      pointer to work queue entry to submit.  This entry is updated to match the other parameters
+ * @param tag      tag value to be assigned to work queue entry
+ * @param tag_type type of tag
+ * @param group      group value for the work queue entry.
+ */
+static inline void cvmx_pow_tag_sw_full(cvmx_wqe_t *wqp, uint32_t tag, cvmx_pow_tag_type_t tag_type, uint64_t group)
+{
+    if (CVMX_ENABLE_POW_CHECKS)
+        __cvmx_pow_warn_if_pending_switch(__FUNCTION__);
+
+    /* Ensure that there is not a pending tag switch, as a tag switch cannot be started
+    ** if a previous switch is still pending.  */
+    cvmx_pow_tag_sw_wait();
+    cvmx_pow_tag_sw_full_nocheck(wqp, tag, tag_type, group);
+}
+
+
+/**
+ * Switch to a NULL tag, which ends any ordering or
+ * synchronization provided by the POW for the current
+ * work queue entry.  This operation completes immediatly,
+ * so completetion should not be waited for.
+ * This function does NOT wait for previous tag switches to complete,
+ * so the caller must ensure that any previous tag switches have completed.
+ */
+static inline void cvmx_pow_tag_sw_null_nocheck(void)
+{
+    cvmx_addr_t ptr;
+    cvmx_pow_tag_req_t tag_req;
+
+    if (CVMX_ENABLE_POW_CHECKS)
+    {
+        cvmx_pow_tag_req_t current_tag;
+        __cvmx_pow_warn_if_pending_switch(__FUNCTION__);
+        current_tag = cvmx_pow_get_current_tag();
+        cvmx_warn_if(current_tag.s.type == CVMX_POW_TAG_TYPE_NULL_NULL, "%s called with NULL_NULL tag\n", __FUNCTION__);
+        cvmx_warn_if(current_tag.s.type == CVMX_POW_TAG_TYPE_NULL, "%s called when we already have a NULL tag\n", __FUNCTION__);
+    }
+
+    tag_req.u64 = 0;
+    tag_req.s.op = CVMX_POW_TAG_OP_SWTAG;
+    tag_req.s.type = CVMX_POW_TAG_TYPE_NULL;
+
+
+    ptr.u64 = 0;
+    ptr.sio.mem_region = CVMX_IO_SEG;
+    ptr.sio.is_io = 1;
+    ptr.sio.did = CVMX_OCT_DID_TAG_TAG1;
+
+
+    cvmx_write_io(ptr.u64, tag_req.u64);
+
+    /* switch to NULL completes immediately */
+}
+
+/**
+ * Switch to a NULL tag, which ends any ordering or
+ * synchronization provided by the POW for the current
+ * work queue entry.  This operation completes immediatly,
+ * so completetion should not be waited for.
+ * This function waits for any pending tag switches to complete
+ * before requesting the switch to NULL.
+ */
+static inline void cvmx_pow_tag_sw_null(void)
+{
+    if (CVMX_ENABLE_POW_CHECKS)
+        __cvmx_pow_warn_if_pending_switch(__FUNCTION__);
+
+    /* Ensure that there is not a pending tag switch, as a tag switch cannot be started
+    ** if a previous switch is still pending.  */
+    cvmx_pow_tag_sw_wait();
+    cvmx_pow_tag_sw_null_nocheck();
+
+    /* switch to NULL completes immediately */
+}
+
+
+
+/**
+ * Submits work to an input queue.  This function updates the work queue entry in DRAM to match
+ * the arguments given.
+ * Note that the tag provided is for the work queue entry submitted, and is unrelated to the tag that
+ * the core currently holds.
+ *
+ * @param wqp      pointer to work queue entry to submit.  This entry is updated to match the other parameters
+ * @param tag      tag value to be assigned to work queue entry
+ * @param tag_type type of tag
+ * @param qos      Input queue to add to.
+ * @param grp      group value for the work queue entry.
+ */
+static inline void cvmx_pow_work_submit(cvmx_wqe_t *wqp, uint32_t tag, cvmx_pow_tag_type_t tag_type, uint64_t qos, uint64_t grp)
+{
+    cvmx_addr_t ptr;
+    cvmx_pow_tag_req_t tag_req;
+
+    wqp->qos = qos;
+    wqp->tag = tag;
+    wqp->tag_type = tag_type;
+    wqp->grp = grp;
+
+    tag_req.u64 = 0;
+    tag_req.s.op = CVMX_POW_TAG_OP_ADDWQ;
+    tag_req.s.type = tag_type;
+    tag_req.s.tag = tag;
+    tag_req.s.qos = qos;
+    tag_req.s.grp = grp;
+
+
+    ptr.u64 = 0;
+    ptr.sio.mem_region = CVMX_IO_SEG;
+    ptr.sio.is_io = 1;
+    ptr.sio.did = CVMX_OCT_DID_TAG_TAG1;
+    ptr.sio.offset = cvmx_ptr_to_phys(wqp);
+
+    /* SYNC write to memory before the work submit.  This is necessary
+    ** as POW may read values from DRAM at this time */
+    CVMX_SYNCWS;
+    cvmx_write_io(ptr.u64, tag_req.u64);
+}
+
+
+
+/**
+ * This function sets the group mask for a core.  The group mask
+ * indicates which groups each core will accept work from. There are
+ * 16 groups.
+ *
+ * @param core_num   core to apply mask to
+ * @param mask   Group mask. There are 16 groups, so only bits 0-15 are valid,
+ *               representing groups 0-15.
+ *               Each 1 bit in the mask enables the core to accept work from
+ *               the corresponding group.
+ */
+static inline void cvmx_pow_set_group_mask(uint64_t core_num, uint64_t mask)
+{
+    cvmx_pow_pp_grp_mskx_t grp_msk;
+
+    grp_msk.u64 = cvmx_read_csr(CVMX_POW_PP_GRP_MSKX(core_num));
+    grp_msk.s.grp_msk = mask;
+    cvmx_write_csr(CVMX_POW_PP_GRP_MSKX(core_num), grp_msk.u64);
+}
+
+/**
+ * This function sets POW static priorities for a core. Each input queue has
+ * an associated priority value.
+ *
+ * @param core_num   core to apply priorities to
+ * @param priority   Vector of 8 priorities, one per POW Input Queue (0-7).
+ *                   Highest priority is 0 and lowest is 7. A priority value
+ *                   of 0xF instructs POW to skip the Input Queue when
+ *                   scheduling to this specific core.
+ *                   NOTE: priorities should not have gaps in values, meaning
+ *                         {0,1,1,1,1,1,1,1} is a valid configuration while
+ *                         {0,2,2,2,2,2,2,2} is not.
+ */
+static inline void cvmx_pow_set_priority(uint64_t core_num, const uint8_t priority[])
+{
+    /* POW priorities are supported on CN5xxx and later */
+    if (!OCTEON_IS_MODEL(OCTEON_CN3XXX))
+    {
+        cvmx_pow_pp_grp_mskx_t grp_msk;
+
+        grp_msk.u64 = cvmx_read_csr(CVMX_POW_PP_GRP_MSKX(core_num));
+        grp_msk.s.qos0_pri = priority[0];
+        grp_msk.s.qos1_pri = priority[1];
+        grp_msk.s.qos2_pri = priority[2];
+        grp_msk.s.qos3_pri = priority[3];
+        grp_msk.s.qos4_pri = priority[4];
+        grp_msk.s.qos5_pri = priority[5];
+        grp_msk.s.qos6_pri = priority[6];
+        grp_msk.s.qos7_pri = priority[7];
+
+        /* Detect gaps between priorities and flag error */
+        {
+            int i;
+            uint32_t prio_mask = 0;
+
+            for(i=0; i<8; i++)
+               if (priority[i] != 0xF)
+		   prio_mask |= 1<<priority[i];
+
+            if ( prio_mask ^ ((1<<cvmx_pop(prio_mask)) - 1))
+            {
+                cvmx_dprintf("ERROR: POW static priorities should be contiguous (0x%llx)\n", (unsigned long long)prio_mask);
+                return;
+            }
+        }
+
+        cvmx_write_csr(CVMX_POW_PP_GRP_MSKX(core_num), grp_msk.u64);
+    }
+}
+
+/**
+ * Performs a tag switch and then an immediate deschedule. This completes
+ * immediatly, so completion must not be waited for.  This function does NOT
+ * update the wqe in DRAM to match arguments.
+ *
+ * This function does NOT wait for any prior tag switches to complete, so the
+ * calling code must do this.
+ *
+ * Note the following CAVEAT of the Octeon HW behavior when
+ * re-scheduling DE-SCHEDULEd items whose (next) state is
+ * ORDERED:
+ *   - If there are no switches pending at the time that the
+ *     HW executes the de-schedule, the HW will only re-schedule
+ *     the head of the FIFO associated with the given tag. This
+ *     means that in many respects, the HW treats this ORDERED
+ *     tag as an ATOMIC tag. Note that in the SWTAG_DESCH
+ *     case (to an ORDERED tag), the HW will do the switch
+ *     before the deschedule whenever it is possible to do
+ *     the switch immediately, so it may often look like
+ *     this case.
+ *   - If there is a pending switch to ORDERED at the time
+ *     the HW executes the de-schedule, the HW will perform
+ *     the switch at the time it re-schedules, and will be
+ *     able to reschedule any/all of the entries with the
+ *     same tag.
+ * Due to this behavior, the RECOMMENDATION to software is
+ * that they have a (next) state of ATOMIC when they
+ * DE-SCHEDULE. If an ORDERED tag is what was really desired,
+ * SW can choose to immediately switch to an ORDERED tag
+ * after the work (that has an ATOMIC tag) is re-scheduled.
+ * Note that since there are never any tag switches pending
+ * when the HW re-schedules, this switch can be IMMEDIATE upon
+ * the reception of the pointer during the re-schedule.
+ *
+ * @param tag      New tag value
+ * @param tag_type New tag type
+ * @param group    New group value
+ * @param no_sched Control whether this work queue entry will be rescheduled.
+ *                 - 1 : don't schedule this work
+ *                 - 0 : allow this work to be scheduled.
+ */
+static inline void cvmx_pow_tag_sw_desched_nocheck(uint32_t tag, cvmx_pow_tag_type_t tag_type, uint64_t group, uint64_t no_sched)
+{
+    cvmx_addr_t ptr;
+    cvmx_pow_tag_req_t tag_req;
+
+    if (CVMX_ENABLE_POW_CHECKS)
+    {
+        cvmx_pow_tag_req_t current_tag;
+        __cvmx_pow_warn_if_pending_switch(__FUNCTION__);
+        current_tag = cvmx_pow_get_current_tag();
+        cvmx_warn_if(current_tag.s.type == CVMX_POW_TAG_TYPE_NULL_NULL, "%s called with NULL_NULL tag\n", __FUNCTION__);
+        cvmx_warn_if(current_tag.s.type == CVMX_POW_TAG_TYPE_NULL, "%s called with NULL tag. Deschedule not allowed from NULL state\n", __FUNCTION__);
+        cvmx_warn_if((current_tag.s.type != CVMX_POW_TAG_TYPE_ATOMIC) && (tag_type != CVMX_POW_TAG_TYPE_ATOMIC), "%s called where neither the before or after tag is ATOMIC\n", __FUNCTION__);
+    }
+
+    tag_req.u64 = 0;
+    tag_req.s.op = CVMX_POW_TAG_OP_SWTAG_DESCH;
+    tag_req.s.tag = tag;
+    tag_req.s.type = tag_type;
+    tag_req.s.grp = group;
+    tag_req.s.no_sched = no_sched;
+
+    ptr.u64 = 0;
+    ptr.sio.mem_region = CVMX_IO_SEG;
+    ptr.sio.is_io = 1;
+    ptr.sio.did = CVMX_OCT_DID_TAG_TAG3;
+
+    cvmx_write_io(ptr.u64, tag_req.u64); // since TAG3 is used, this store will clear the local pending switch bit
+}
+/**
+ * Performs a tag switch and then an immediate deschedule. This completes
+ * immediatly, so completion must not be waited for.  This function does NOT
+ * update the wqe in DRAM to match arguments.
+ *
+ * This function waits for any prior tag switches to complete, so the
+ * calling code may call this function with a pending tag switch.
+ *
+ * Note the following CAVEAT of the Octeon HW behavior when
+ * re-scheduling DE-SCHEDULEd items whose (next) state is
+ * ORDERED:
+ *   - If there are no switches pending at the time that the
+ *     HW executes the de-schedule, the HW will only re-schedule
+ *     the head of the FIFO associated with the given tag. This
+ *     means that in many respects, the HW treats this ORDERED
+ *     tag as an ATOMIC tag. Note that in the SWTAG_DESCH
+ *     case (to an ORDERED tag), the HW will do the switch
+ *     before the deschedule whenever it is possible to do
+ *     the switch immediately, so it may often look like
+ *     this case.
+ *   - If there is a pending switch to ORDERED at the time
+ *     the HW executes the de-schedule, the HW will perform
+ *     the switch at the time it re-schedules, and will be
+ *     able to reschedule any/all of the entries with the
+ *     same tag.
+ * Due to this behavior, the RECOMMENDATION to software is
+ * that they have a (next) state of ATOMIC when they
+ * DE-SCHEDULE. If an ORDERED tag is what was really desired,
+ * SW can choose to immediately switch to an ORDERED tag
+ * after the work (that has an ATOMIC tag) is re-scheduled.
+ * Note that since there are never any tag switches pending
+ * when the HW re-schedules, this switch can be IMMEDIATE upon
+ * the reception of the pointer during the re-schedule.
+ *
+ * @param tag      New tag value
+ * @param tag_type New tag type
+ * @param group    New group value
+ * @param no_sched Control whether this work queue entry will be rescheduled.
+ *                 - 1 : don't schedule this work
+ *                 - 0 : allow this work to be scheduled.
+ */
+static inline void cvmx_pow_tag_sw_desched(uint32_t tag, cvmx_pow_tag_type_t tag_type, uint64_t group, uint64_t no_sched)
+{
+    if (CVMX_ENABLE_POW_CHECKS)
+        __cvmx_pow_warn_if_pending_switch(__FUNCTION__);
+
+    /* Need to make sure any writes to the work queue entry are complete */
+    CVMX_SYNCWS;
+    /* Ensure that there is not a pending tag switch, as a tag switch cannot be started
+    ** if a previous switch is still pending.  */
+    cvmx_pow_tag_sw_wait();
+    cvmx_pow_tag_sw_desched_nocheck(tag, tag_type, group, no_sched);
+}
+
+
+
+
+
+/**
+ * Descchedules the current work queue entry.
+ *
+ * @param no_sched no schedule flag value to be set on the work queue entry.  If this is set
+ *                 the entry will not be rescheduled.
+ */
+static inline void cvmx_pow_desched(uint64_t no_sched)
+{
+    cvmx_addr_t ptr;
+    cvmx_pow_tag_req_t tag_req;
+
+    if (CVMX_ENABLE_POW_CHECKS)
+    {
+        cvmx_pow_tag_req_t current_tag;
+        __cvmx_pow_warn_if_pending_switch(__FUNCTION__);
+        current_tag = cvmx_pow_get_current_tag();
+        cvmx_warn_if(current_tag.s.type == CVMX_POW_TAG_TYPE_NULL_NULL, "%s called with NULL_NULL tag\n", __FUNCTION__);
+        cvmx_warn_if(current_tag.s.type == CVMX_POW_TAG_TYPE_NULL, "%s called with NULL tag. Deschedule not expected from NULL state\n", __FUNCTION__);
+    }
+
+    /* Need to make sure any writes to the work queue entry are complete */
+    CVMX_SYNCWS;
+
+    tag_req.u64 = 0;
+    tag_req.s.op = CVMX_POW_TAG_OP_DESCH;
+    tag_req.s.no_sched = no_sched;
+
+    ptr.u64 = 0;
+    ptr.sio.mem_region = CVMX_IO_SEG;
+    ptr.sio.is_io = 1;
+    ptr.sio.did = CVMX_OCT_DID_TAG_TAG3;
+
+    cvmx_write_io(ptr.u64, tag_req.u64); // since TAG3 is used, this store will clear the local pending switch bit
+}
+
+
+
+
+
+
+
+/***********************************************************************************************
+** Define usage of bits within the 32 bit tag values.
+***********************************************************************************************/
+
+/*
+ * Number of bits of the tag used by software.  The SW bits
+ * are always a contiguous block of the high starting at bit 31.
+ * The hardware bits are always the low bits.  By default, the top 8 bits
+ * of the tag are reserved for software, and the low 24 are set by the IPD unit.
+ */
+#define CVMX_TAG_SW_BITS    (8)
+#define CVMX_TAG_SW_SHIFT   (32 - CVMX_TAG_SW_BITS)
+
+/* Below is the list of values for the top 8 bits of the tag. */
+#define CVMX_TAG_SW_BITS_INTERNAL  0x1  /* Tag values with top byte of this value are reserved for internal executive uses */
+/* The executive divides the remaining 24 bits as follows:
+**  * the upper 8 bits (bits 23 - 16 of the tag) define a subgroup
+**  * the lower 16 bits (bits 15 - 0 of the tag) define are the value with the subgroup
+** Note that this section describes the format of tags generated by software - refer to the
+** hardware documentation for a description of the tags values generated by the packet input
+** hardware.
+** Subgroups are defined here */
+#define CVMX_TAG_SUBGROUP_MASK  0xFFFF /* Mask for the value portion of the tag */
+#define CVMX_TAG_SUBGROUP_SHIFT 16
+#define CVMX_TAG_SUBGROUP_PKO  0x1
+
+
+/* End of executive tag subgroup definitions */
+
+/* The remaining values software bit values 0x2 - 0xff are available for application use */
+
+
+
+/**
+ * This function creates a 32 bit tag value from the two values provided.
+ *
+ * @param sw_bits The upper bits (number depends on configuration) are set to this value.  The remainder of
+ *                bits are set by the hw_bits parameter.
+ * @param hw_bits The lower bits (number depends on configuration) are set to this value.  The remainder of
+ *                bits are set by the sw_bits parameter.
+ *
+ * @return 32 bit value of the combined hw and sw bits.
+ */
+static inline uint32_t cvmx_pow_tag_compose(uint64_t sw_bits, uint64_t hw_bits)
+{
+    return((((sw_bits & cvmx_build_mask(CVMX_TAG_SW_BITS)) << CVMX_TAG_SW_SHIFT) | (hw_bits & cvmx_build_mask(32 - CVMX_TAG_SW_BITS))));
+}
+/**
+ * Extracts the bits allocated for software use from the tag
+ *
+ * @param tag    32 bit tag value
+ *
+ * @return N bit software tag value, where N is configurable with the CVMX_TAG_SW_BITS define
+ */
+static inline uint32_t cvmx_pow_tag_get_sw_bits(uint64_t tag)
+{
+    return((tag >> (32 - CVMX_TAG_SW_BITS)) & cvmx_build_mask(CVMX_TAG_SW_BITS));
+}
+/**
+ *
+ * Extracts the bits allocated for hardware use from the tag
+ *
+ * @param tag    32 bit tag value
+ *
+ * @return (32 - N) bit software tag value, where N is configurable with the CVMX_TAG_SW_BITS define
+ */
+static inline uint32_t cvmx_pow_tag_get_hw_bits(uint64_t tag)
+{
+    return(tag & cvmx_build_mask(32 - CVMX_TAG_SW_BITS));
+}
+
+/**
+ * Store the current POW internal state into the supplied
+ * buffer. It is recommended that you pass a buffer of at least
+ * 128KB. The format of the capture may change based on SDK
+ * version and Octeon chip.
+ *
+ * @param buffer Buffer to store capture into
+ * @param buffer_size
+ *               The size of the supplied buffer
+ *
+ * @return Zero on sucess, negative on failure
+ */
+extern int cvmx_pow_capture(void *buffer, int buffer_size);
+
+/**
+ * Dump a POW capture to the console in a human readable format.
+ *
+ * @param buffer POW capture from cvmx_pow_capture()
+ * @param buffer_size
+ *               Size of the buffer
+ */
+extern void cvmx_pow_display(void *buffer, int buffer_size);
+
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif  // __CVMX_POW_H__
diff --git a/arch/mips/cavium-octeon/executive/cvmx-raid.c b/arch/mips/cavium-octeon/executive/cvmx-raid.c
new file mode 100644
index 0000000..ee1dfc5
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-raid.c
@@ -0,0 +1,140 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Interface to RAID block. This is not available on all chips.
+ *
+ * <hr>$Revision: 34528 $<hr>
+ */
+#include "executive-config.h"
+#include "cvmx-config.h"
+#include "cvmx.h"
+#include "cvmx-cmd-queue.h"
+#include "cvmx-raid.h"
+
+#ifdef CVMX_ENABLE_PKO_FUNCTIONS
+
+/**
+ * Initialize the RAID block
+ *
+ * @param polynomial Coefficients for the RAID polynomial
+ *
+ * @return Zero on success, negative on failure
+ */
+int cvmx_raid_initialize(cvmx_rad_reg_polynomial_t polynomial)
+{
+    cvmx_cmd_queue_result_t result;
+    cvmx_rad_reg_cmd_buf_t rad_reg_cmd_buf;
+
+    cvmx_write_csr(CVMX_RAD_REG_POLYNOMIAL, polynomial.u64);
+
+    result = cvmx_cmd_queue_initialize(CVMX_CMD_QUEUE_RAID, 0,
+                                       CVMX_FPA_OUTPUT_BUFFER_POOL,
+                                       CVMX_FPA_OUTPUT_BUFFER_POOL_SIZE);
+    if (result != CVMX_CMD_QUEUE_SUCCESS)
+        return -1;
+
+    rad_reg_cmd_buf.u64 = 0;
+    rad_reg_cmd_buf.s.dwb = CVMX_FPA_OUTPUT_BUFFER_POOL_SIZE/128;
+    rad_reg_cmd_buf.s.pool = CVMX_FPA_OUTPUT_BUFFER_POOL;
+    rad_reg_cmd_buf.s.size = CVMX_FPA_OUTPUT_BUFFER_POOL_SIZE/8;
+    rad_reg_cmd_buf.s.ptr = cvmx_ptr_to_phys(cvmx_cmd_queue_buffer(CVMX_CMD_QUEUE_RAID))>>7;
+    cvmx_write_csr(CVMX_RAD_REG_CMD_BUF, rad_reg_cmd_buf.u64);
+    return 0;
+}
+
+
+/**
+ * Shutdown the RAID block. RAID must be idle when
+ * this function is called.
+ *
+ * @return Zero on success, negative on failure
+ */
+int cvmx_raid_shutdown(void)
+{
+    cvmx_rad_reg_ctl_t rad_reg_ctl;
+
+    if (cvmx_cmd_queue_length(CVMX_CMD_QUEUE_RAID))
+    {
+        cvmx_dprintf("ERROR: cvmx_raid_shutdown: RAID not idle.\n");
+        return -1;
+    }
+
+    rad_reg_ctl.u64 = cvmx_read_csr(CVMX_RAD_REG_CTL);
+    rad_reg_ctl.s.reset = 1;
+    cvmx_write_csr(CVMX_RAD_REG_CTL, rad_reg_ctl.u64);
+    cvmx_wait(100);
+
+    cvmx_cmd_queue_shutdown(CVMX_CMD_QUEUE_RAID);
+    cvmx_write_csr(CVMX_RAD_REG_CMD_BUF, 0);
+    return 0;
+}
+
+
+/**
+ * Submit a command to the RAID block
+ *
+ * @param num_words Number of command words to submit
+ * @param words     Command words
+ *
+ * @return Zero on success, negative on failure
+ */
+int cvmx_raid_submit(int num_words, cvmx_raid_word_t words[])
+{
+    cvmx_cmd_queue_result_t result = cvmx_cmd_queue_write(CVMX_CMD_QUEUE_RAID, 1, num_words, (uint64_t *)words);
+    if (result == CVMX_CMD_QUEUE_SUCCESS)
+        cvmx_write_csr(CVMX_ADDR_DID(CVMX_FULL_DID(14, 0)), num_words);
+    return result;
+}
+
+#endif
diff --git a/arch/mips/cavium-octeon/executive/cvmx-raid.h b/arch/mips/cavium-octeon/executive/cvmx-raid.h
new file mode 100644
index 0000000..9869ca7
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-raid.h
@@ -0,0 +1,210 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Interface to RAID block. This is not available on all chips.
+ *
+ * <hr>$Revision: 34528 $<hr>
+ */
+
+#ifndef __CVMX_RAID_H__
+#define __CVMX_RAID_H__
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+/**
+ * This structure defines the type of command words the RAID block
+ * will accept.
+ */
+typedef union
+{
+    uint64_t u64;
+    struct
+    {
+        uint64_t    reserved_37_63  : 27;   /**< Must be zero */
+        uint64_t    q_cmp           :  1;   /**< Indicates whether the Q pipe is in normal mode (CWORD[Q_CMP]=0) or in non-zero
+                                                byte detect mode (CWORD[Q_CMP]=1).
+                                                In non-zero byte detect mode, the Q OWORD[PTR] result is the non-zero detect
+                                                result, which indicates the position of the first non-zero byte in the pipe result bytes.
+                                                CWORD[Q_CMP] must not be set when CWORD[QOUT]=0, and must not be set
+                                                when CWORD[Q_XOR] is set. */
+        uint64_t    p_cmp           :  1;   /**< Indicates whether the P pipe is in normal mode (CWORD[P_CMP]=0) or in non-zero
+                                                byte detect mode (CWORD[P_CMP]=1).
+                                                In non-zero byte detect mode, the P OWORD[PTR] result is the non-zero detect
+                                                result, which indicates the position of the first non-zero byte in the pipe result bytes.
+                                                CWORD[P_CMP] must not be set when CWORD[POUT]=0, and must not be set
+                                                when CWORD[P_XOR] is set. */
+        uint64_t    q_xor           :  1;   /**< Indicates whether the Q output buffer bytes are the normal Q pipe result or the
+                                                normal Q pipe result exclusive-OR'ed with the P pipe result.
+                                                When CWORD[Q_XOR]=0 (and CWORD[Q_CMP]=0), the Q output buffer bytes are
+                                                the normal Q pipe result, which does not include the P pipe result in any way.
+                                                When CWORD[Q_XOR]=1, the Q output buffer bytes are the normal Q pipe result
+                                                exclusive-OR'ed with the P pipe result, as if the P pipe result were another Q IWORD
+                                                for the Q pipe with QMULT=1.
+                                                CWORD[Q_XOR] must not be set unless both CWORD[POUT,QOUT] are set, and
+                                                must not be set when CWORD[Q_CMP] is set. */
+        uint64_t    p_xor           :  1;   /**< Indicates whether the P output buffer bytes are the normal P pipe result or the
+                                                normal P pipe result exclusive-OR'ed with the Q pipe result.
+                                                When CWORD[P_XOR]=0 (and CWORD[P_CMP]=0), the P output buffer bytes are
+                                                the normal P pipe result, which does not include the Q pipe result in any way.
+                                                When CWORD[P_XOR]=1, the P output buffer bytes are the normal P pipe result
+                                                exclusive-OR'ed with the Q pipe result, as if the Q pipe result were another P
+                                                IWORD for the P pipe.
+                                                CWORD[P_XOR] must not be set unless both CWORD[POUT,QOUT] are set, and
+                                                must not be set when CWORD[P_CMP] is set. */
+        uint64_t    wqe             :  1;   /**< Indicates whether RAD submits a work queue entry or writes an L2/DRAM byte to
+                                                zero after completing the instruction.
+                                                When CWORD[WQE] is set and RESP[PTR]!=0, RAD adds the work queue entry
+                                                indicated by RESP[PTR] to the selected POW input queue after completing the
+                                                instruction.
+                                                When CWORD[WQE] is clear and RESP[PTR]!=0, RAD writes the L2/DRAM byte
+                                                indicated by RESP[PTR] to zero after completing the instruction. */
+        uint64_t    qout            :  1;   /**< Indicates whether the Q pipe is used by this instruction.
+                                                If CWORD[QOUT] is set, IWORD[QEN] must be set for at least one IWORD.
+                                                At least one of CWORD[QOUT,POUT] must be set. */
+        uint64_t    pout            :  1;   /**< Indicates whether the P pipe is used by this instruction.
+                                                If CWORD[POUT] is set, IWORD[PEN] must be set for at least one IWORD.
+                                                At least one of CWORD[QOUT,POUT] must be set. */
+        uint64_t    iword           :  6;   /**< Indicates the number of input buffers used.
+                                                1 <= CWORD[IWORD] <= 32. */
+        uint64_t    size            : 24;   /**< Indicates the size in bytes of all input buffers. When CWORD[Q_CMP,P_CMP]=0,
+                                                also indicates the size of the Q/P output buffers.
+                                                CWORD[SIZE] must be a multiple of 8B (i.e. <2:0> must be zero). */
+    } cword;
+    struct
+    {
+        uint64_t    reserved_58_63  :  6;   /**< Must be zero */
+        uint64_t    fw              :  1;   /**< When set, indicates that RAD can modify any byte in any (128B) cache line touched
+                                                by L2/DRAM addresses OWORD[PTR] through OWORD[PTR]+CWORD[SIZE]1.
+                                                Setting OWORD[FW] can improve hardware performance, as some DRAM loads can
+                                                be avoided on L2 cache misses. The Q OWORD[FW] must not be set when
+                                                CWORD[Q_CMP] is set, and the P OWORD[FW] must not be set when
+                                                CWORD[P_CMP] is set. */
+        uint64_t    nc              :  1;   /**< When set, indicates that RAD should not allocate L2 cache space for the P/Q data on
+                                                L2 cache misses.
+                                                OWORD[NC] should typically be clear, though setting OWORD[NC] can improve
+                                                performance in some circumstances, as the L2 cache will not be polluted by P/Q data.
+                                                The Q OWORD[NC] must not be set when CWORD[Q_CMP] is set, and the P
+                                                OWORD[NC] must not be set when CWORD[P_CMP] is set. */
+        uint64_t    reserved_40_55  : 16;   /**< Must be zero */
+        uint64_t    addr            : 40;   /**< When CWORD[P_CMP,Q_CMP]=0, OWORD[PTR] indicates the starting address of
+                                                the L2/DRAM buffer that will receive the P/Q data. In the non-compare mode, the
+                                                output buffer receives all of the output buffer bytes.
+                                                When CWORD[P_CMP,Q_CMP]=1, the corresponding P/Q pipe is in compare mode,
+                                                and the only output of the pipe is the non-zero detect result. In this case,
+                                                OWORD[PTR] indicates the 8-byte location of the non-zero detect result. */
+    } oword;
+    struct
+    {
+        uint64_t    reserved_57_63  :  7;   /**< Must be zero */
+        uint64_t    nc              :  1;   /**< When set, indicates that RAD should not allocate L2 cache space for this input buffer
+                                                data on L2 cache misses.
+                                                Setting IWORD[NC] may improve performance in some circumstances, as the L2
+                                                cache may not be polluted with input buffer data. */
+        uint64_t    reserved_50_55  :  6;   /**< Must be zero */
+        uint64_t    qen             :  1;   /**< Indicates that this input buffer data should participate in the Q pipe result.
+                                                The Q pipe hardware multiplies each participating input byte by IWORD[QMULT]
+                                                before accumulating them by exclusive-OR'ing.
+                                                IWORD[QEN] must not be set when CWORD[QOUT] is not set.
+                                                If CWORD[QOUT] is set, IWORD[QEN] must be set for at least one IWORD. */
+        uint64_t    pen             :  1;   /**< Indicates that this input buffer data should participate in the P pipe result.
+                                                The P pipe hardware accumulates each participating input byte by bit-wise
+                                                exclusive-OR'ing it.
+                                                IWORD[PEN] must not be set when CWORD[POUT] is not set.
+                                                If CWORD[POUT] is set, IWORD[PEN] must be set for at least one IWORD. */
+        uint64_t    qmult           :  8;   /**< The Q pipe multiplier for the input buffer. Section 26.1 above describes the GF(28)
+                                                multiplication algorithm.
+                                                IWORD[QMULT] must be zero when IWORD[QEN] is not set.
+                                                IWORD[QMULT] must not be zero when IWORD[QEN] is set.
+                                                When IWORD[QMULT] is 1, the multiplication simplifies to the identity function,
+                                                and the Q pipe performs the same XOR function as the P pipe. */
+        uint64_t    addr            : 40;   /**< The starting address of the input buffer in L2/DRAM.
+                                                IWORD[PTR] must be naturally-aligned on an 8 byte boundary (i.e. <2:0> must be
+                                                zero). */
+    } iword;
+} cvmx_raid_word_t;
+
+/**
+ * Initialize the RAID block
+ *
+ * @param polynomial Coefficients for the RAID polynomial
+ *
+ * @return Zero on success, negative on failure
+ */
+int cvmx_raid_initialize(cvmx_rad_reg_polynomial_t polynomial);
+
+/**
+ * Shutdown the RAID block. RAID must be idle when
+ * this function is called.
+ *
+ * @return Zero on success, negative on failure
+ */
+int cvmx_raid_shutdown(void);
+
+/**
+ * Submit a command to the RAID block
+ *
+ * @param num_words Number of command words to submit
+ * @param words     Command words
+ *
+ * @return Zero on success, negative on failure
+ */
+int cvmx_raid_submit(int num_words, cvmx_raid_word_t words[]);
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif // __CVMX_CMD_QUEUE_H__
diff --git a/arch/mips/cavium-octeon/executive/cvmx-resources.config b/arch/mips/cavium-octeon/executive/cvmx-resources.config
new file mode 100644
index 0000000..74595f4
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-resources.config
@@ -0,0 +1,198 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+/*
+ * File version info: $Id: cvmx-resources.config 32636 2008-03-07 21:43:34Z rfranz $
+ *
+ */
+#ifndef __CVMX_RESOURCES_CONFIG__
+#define __CVMX_RESOURCES_CONFIG__
+
+
+#if (CVMX_HELPER_FIRST_MBUFF_SKIP > 256)
+#error CVMX_HELPER_FIRST_MBUFF_SKIP is greater than the maximum of 256
+#endif
+
+#if (CVMX_HELPER_NOT_FIRST_MBUFF_SKIP > 256)
+#error CVMX_HELPER_NOT_FIRST_MBUFF_SKIP is greater than the maximum of 256
+#endif
+
+
+/* Content below this point is only used by the cvmx-config tool, and is
+** not used by any C files as CAVIUM_COMPONENT_REQUIREMENT is never
+defined.
+*/
+ #ifdef CAVIUM_COMPONENT_REQUIREMENT
+    /* Define the number of LLM ports (interfaces), can be 1 or 2 */
+    cvmxconfig
+    {
+    	#if CVMX_LLM_CONFIG_NUM_PORTS == 2
+            define CVMX_LLM_NUM_PORTS value = 2;
+	#else
+            define CVMX_LLM_NUM_PORTS value = 1;
+	#endif
+    }
+    /* Control the setting of Null pointer detection, default to enabled */
+    cvmxconfig {
+    	#ifdef CVMX_CONFIG_NULL_POINTER_PROTECT
+            define CVMX_NULL_POINTER_PROTECT value = CVMX_CONFIG_NULL_POINTER_PROTECT;
+	#else
+            define CVMX_NULL_POINTER_PROTECT value = 1;
+	#endif
+    }
+    /* Control Debug prints, default to enabled */
+    cvmxconfig {
+    	#ifdef CVMX_CONFIG_ENABLE_DEBUG_PRINTS
+            define CVMX_ENABLE_DEBUG_PRINTS value = CVMX_CONFIG_ENABLE_DEBUG_PRINTS;
+	#else
+            define CVMX_ENABLE_DEBUG_PRINTS value = 1;
+	#endif
+    }
+
+    /* Define CVMX_ENABLE_DFA_FUNCTIONS to allocate resources for the DFA functions */
+    #ifdef CVMX_ENABLE_DFA_FUNCTIONS
+        cvmxconfig
+        {
+        	fpa CVMX_FPA_DFA_POOL
+                    size        = 2
+                    protected   = 1
+                    description = "DFA command buffers";
+        	fau CVMX_FAU_DFA_STATE
+                    size        = 8
+                    count       = 1
+                    description = "FAU registers for the state of the DFA command queue";
+        }
+    #endif
+
+    /* Define CVMX_ENABLE_PKO_FUNCTIONS to allocate resources for the PKO functions */
+    #ifdef CVMX_ENABLE_PKO_FUNCTIONS
+        cvmxconfig
+        {
+        #ifdef CVMX_ENABLE_PKO_LOCKLESS_OPERATION
+                define CVMX_PKO_LOCKLESS_OPERATION
+                    value       = 1
+                    description = "PKO per-core queues allowing lockless operation";
+        #endif
+		define CVMX_PKO_QUEUES_PER_PORT_INTERFACE0
+		    value       = CVMX_HELPER_PKO_QUEUES_PER_PORT_INTERFACE0
+		    description = "PKO queues per port for interface 0 (ports 0-15)";
+		define CVMX_PKO_QUEUES_PER_PORT_INTERFACE1
+		    value       = CVMX_HELPER_PKO_QUEUES_PER_PORT_INTERFACE1
+		    description = "PKO queues per port for interface 1 (ports 16-31)";
+                define CVMX_PKO_MAX_PORTS_INTERFACE0
+                    value       = CVMX_HELPER_PKO_MAX_PORTS_INTERFACE0
+                    description = "Limit on the number of PKO ports enabled for interface 0";
+                define CVMX_PKO_MAX_PORTS_INTERFACE1
+                    value       = CVMX_HELPER_PKO_MAX_PORTS_INTERFACE1
+                    description = "Limit on the number of PKO ports enabled for interface 1";
+		define CVMX_PKO_QUEUES_PER_PORT_PCI
+		    value       = 1
+		    description = "PKO queues per port for PCI (ports 32-35)";
+		define CVMX_PKO_QUEUES_PER_PORT_LOOP
+		    value       = 1
+		    description = "PKO queues per port for Loop devices (ports 36-39)";
+	#if CVMX_PKO_USE_FAU_FOR_OUTPUT_QUEUES
+        	fau CVMX_FAU_REG_OQ_ADDR_INDEX
+                    size        = 8
+                    count       = 128
+                    description = "FAU registers for the position in PKO command buffers";
+	#endif
+        	fpa CVMX_FPA_PACKET_POOL
+                    pool        = 0
+                    size        = 16
+                    priority    = 1
+                    protected   = 1
+                    description = "Packet buffers";
+        	fpa CVMX_FPA_OUTPUT_BUFFER_POOL
+                    size        = 8
+                    protected   = 1
+                    description = "PKO queue command buffers";
+        	scratch CVMX_SCR_SCRATCH
+                    size        = 8
+                    iobdma      = true
+                    permanent   = false
+                    description = "Generic scratch iobdma area";
+	#if CVMX_PKO_USE_FAU_FOR_OUTPUT_QUEUES
+        	scratch CVMX_SCR_OQ_BUF_PRE_ALLOC
+                    size        = 8
+                    iobdma      = true
+                    permanent   = true
+                    description = "Pre allocation for PKO queue command buffers";
+	#endif
+        }
+    #endif
+
+    /* Define CVMX_ENABLE_HELPER_FUNCTIONS to allocate resources for the helper functions */
+    #ifdef CVMX_ENABLE_HELPER_FUNCTIONS
+        cvmxconfig
+        {
+        	fpa CVMX_FPA_WQE_POOL
+                    size        = 1
+                    priority    = 1
+                    protected   = 1
+                    description = "Work queue entrys";
+        }
+    #endif
+
+    /* Define CVMX_ENABLE_TIMER_FUNCTIONS to allocate resources for the timer functions */
+    #ifdef CVMX_ENABLE_TIMER_FUNCTIONS
+        cvmxconfig
+        {
+        	fpa CVMX_FPA_TIMER_POOL
+                    size        = 8
+                    protected   = 1
+                    description = "TIM command buffers";
+        }
+    #endif
+
+#endif
+
+
+#endif  /* __CVMX_RESOURCES_CONFIG__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-rng.h b/arch/mips/cavium-octeon/executive/cvmx-rng.h
new file mode 100644
index 0000000..8d82b16
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-rng.h
@@ -0,0 +1,170 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Function and structure definitions for random number generator hardware
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+
+
+#ifndef __CMVX_RNG_H__
+#define __CMVX_RNG_H__
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#define CVMX_RNG_LOAD_ADDRESS   CVMX_ADD_IO_SEG(cvmx_build_io_address(CVMX_OCT_DID_RNG, 0))
+
+/**
+ * Structure describing the data format used for IOBDMA stores to the RNG.
+ */
+typedef union
+{
+    uint64_t        u64;
+    struct {
+        uint64_t    scraddr : 8;    /**< the (64-bit word) location in scratchpad to write to (if len != 0) */
+        uint64_t    len     : 8;    /**< the number of words in the response (0 => no response) */
+        uint64_t    did     : 5;    /**< the ID of the device on the non-coherent bus */
+        uint64_t    subdid  : 3;    /**< the sub ID of the device on the non-coherent bus */
+        uint64_t    addr    :40;    /**< the address that will appear in the first tick on the NCB bus */
+    } s;
+} cvmx_rng_iobdma_data_t;
+
+/**
+ * Enables the random number generator. Must be called before RNG is used
+ */
+static inline void cvmx_rng_enable(void)
+{
+    cvmx_rnm_ctl_status_t rnm_ctl_status;
+    rnm_ctl_status.u64 = 0;
+    rnm_ctl_status.s.ent_en = 1;
+    rnm_ctl_status.s.rng_en = 1;
+    cvmx_write_csr(CVMX_RNM_CTL_STATUS, rnm_ctl_status.u64);
+}
+/**
+ * Reads 8 bits of random data from Random number generator
+ *
+ * @return random data
+ */
+static inline uint8_t cvmx_rng_get_random8(void)
+{
+    return cvmx_read64_uint8(CVMX_RNG_LOAD_ADDRESS);
+}
+
+/**
+ * Reads 16 bits of random data from Random number generator
+ *
+ * @return random data
+ */
+static inline uint16_t cvmx_rng_get_random16(void)
+{
+    return cvmx_read64_uint16(CVMX_RNG_LOAD_ADDRESS);
+}
+
+/**
+ * Reads 32 bits of random data from Random number generator
+ *
+ * @return random data
+ */
+static inline uint32_t cvmx_rng_get_random32(void)
+{
+    return cvmx_read64_uint32(CVMX_RNG_LOAD_ADDRESS);
+}
+
+/**
+ * Reads 64 bits of random data from Random number generator
+ *
+ * @return random data
+ */
+static inline uint64_t cvmx_rng_get_random64(void)
+{
+    return cvmx_read64_uint64(CVMX_RNG_LOAD_ADDRESS);
+}
+
+/**
+ * Requests random data from the RNG block asynchronously using and IOBDMA operation.
+ * The random data will be written into the cores
+ * local memory at the specified address.  A SYNCIOBDMA
+ * operation should be issued to stall for completion of the write.
+ *
+ * @param scr_addr  Address in scratch memory to put the result
+ *                  MUST be a multiple of 8 bytes
+ * @param num_bytes Number of bytes of random data to write at
+ *                  scr_addr
+ *                  MUST be a multiple of 8 bytes
+ *
+ * @return 0 on success
+ *         1 on error
+ */
+static inline int cvmx_rng_request_random_async(uint64_t scr_addr, uint64_t num_bytes)
+{
+    cvmx_rng_iobdma_data_t data;
+
+    if (num_bytes & 0x7 || scr_addr & 0x7)
+        return(1);
+
+    /* scr_addr must be 8 byte aligned */
+    data.s.scraddr = scr_addr >> 3;
+    data.s.len = num_bytes >> 3;
+    data.s.did = CVMX_OCT_DID_RNG;
+    cvmx_send_single(data.u64);
+    return(0);
+}
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif /*  __CMVX_RNG_H__  */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-rtc.h b/arch/mips/cavium-octeon/executive/cvmx-rtc.h
new file mode 100644
index 0000000..33670e7
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-rtc.h
@@ -0,0 +1,168 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * This file provides support for real time clocks on some boards
+ *
+ * <hr>$Revision: 32636 $<hr>
+ *
+ */
+
+
+#ifndef __CVMX_RTC_H__
+#define __CVMX_RTC_H__
+
+#include "cvmx-sysinfo.h"
+#include "cvmx-thunder.h"
+#include "cvmx-cn3010-evb-hs5.h"
+
+/**
+ * Supported RTC options
+ */
+typedef enum
+{
+    CVMX_RTC_READ            = 0x1,  /**< Device supports read access */
+    CVMX_RTC_WRITE           = 0x2,  /**< Device supports write access */
+    CVMX_RTC_TIME_EPOCH      = 0x10, /**< Time stored as seconds from epoch */
+    CVMX_RTC_TIME_CAL        = 0x20, /**< Time stored as calendar */
+} cvmx_rtc_options_t;
+
+/**
+ * Return options supported by the RTC device
+ *
+ * @return Supported options, or 0 if RTC is not supported
+ */
+static inline cvmx_rtc_options_t cvmx_rtc_supported()
+{
+    static int supported = -1;
+
+    if (supported < 0) {
+	switch (cvmx_sysinfo_get()->board_type)
+	{
+	case CVMX_BOARD_TYPE_THUNDER:
+	    supported = CVMX_RTC_READ | CVMX_RTC_WRITE | CVMX_RTC_TIME_EPOCH;
+	    break;
+
+	case CVMX_BOARD_TYPE_EBH3000:
+	case CVMX_BOARD_TYPE_CN3010_EVB_HS5:
+	    supported = CVMX_RTC_READ | CVMX_RTC_WRITE | CVMX_RTC_TIME_CAL;
+	    break;
+
+	default:
+	    supported = 0;
+	    break;
+	}
+
+#ifdef CVMX_RTC_DEBUG
+	cvmx_dprintf("Board type: %s, RTC support: 0x%x\n",
+	       cvmx_board_type_to_string(cvmx_sysinfo_get()->board_type),
+	       supported);
+#endif
+    }
+
+    return (cvmx_rtc_options_t) supported;
+}
+
+/**
+ * Read time from RTC device.
+ *
+ * Time is expressed in seconds from epoch (Jan 1 1970 at 00:00:00 UTC)
+ *
+ * @return Time in seconds or 0 if RTC is not supported
+ */
+static inline uint32_t cvmx_rtc_read(void)
+{
+    switch (cvmx_sysinfo_get()->board_type)
+    {
+    case CVMX_BOARD_TYPE_THUNDER:
+        return cvmx_rtc_ds1374_read();
+        break;
+
+    case CVMX_BOARD_TYPE_EBH3000:
+    case CVMX_BOARD_TYPE_CN3010_EVB_HS5:
+	return cvmx_rtc_ds1337_read();
+	break;
+
+    default:
+        return 0;
+        break;
+    }
+}
+
+/**
+ * Write time to the RTC device
+ *
+ * @param time    Number of seconds from epoch (Jan 1 1970 at 00:00:00 UTC)
+ *
+ * @return Zero on success or device-specific error on failure.
+ */
+static inline uint32_t cvmx_rtc_write(uint32_t time)
+{
+    switch (cvmx_sysinfo_get()->board_type)
+    {
+    case CVMX_BOARD_TYPE_THUNDER:
+        return cvmx_rtc_ds1374_write(time);
+        break;
+
+    case CVMX_BOARD_TYPE_EBH3000:
+    case CVMX_BOARD_TYPE_CN3010_EVB_HS5:
+	return cvmx_rtc_ds1337_write(time);
+	break;
+
+    default:
+        return 0;
+        break;
+    }
+}
+
+#endif    /* __CVMX_RTC_H__  */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-rwlock.h b/arch/mips/cavium-octeon/executive/cvmx-rwlock.h
new file mode 100644
index 0000000..774d2d0
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-rwlock.h
@@ -0,0 +1,177 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * This file provides reader/writer locks.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ *
+ *
+ */
+
+
+#ifndef __CVMX_RWLOCK_H__
+#define __CVMX_RWLOCK_H__
+
+/* include to get atomic compare and store */
+#include "cvmx-atomic.h"
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+/* Flags for lock value in rw lock structure */
+#define CVMX_RWLOCK_WRITE_FLAG     0x1
+#define CVMX_RWLOCK_READ_INC       0x2
+
+
+/* Writer preference locks (wp).  Can be starved by writers.  When a writer
+ * is waiting, no readers are given the lock until all writers are done.
+ */
+typedef struct
+{
+    volatile uint32_t lock;
+    volatile uint32_t write_req;
+    volatile uint32_t write_comp;
+} cvmx_rwlock_wp_lock_t;
+
+/**
+ * Initialize a reader/writer lock.  This must be done
+ * by a single core before used.
+ *
+ * @param lock   pointer to rwlock structure
+ */
+static inline void cvmx_rwlock_wp_init(cvmx_rwlock_wp_lock_t *lock)
+{
+    lock->lock = 0;
+    lock->write_req = 0;
+    lock->write_comp = 0;
+}
+
+/**
+ * Perform a reader lock.  If a writer is pending, this
+ * will wait for that writer to complete before locking.
+ *
+ * NOTE: Each thread/process must only lock any rwlock
+ * once, or else a deadlock may result.
+ *
+ * @param lock   pointer to rwlock structure
+ */
+static inline void cvmx_rwlock_wp_read_lock(cvmx_rwlock_wp_lock_t *lock)
+{
+
+    /* Wait for outstanding write requests to be serviced */
+    while (lock->write_req != lock->write_comp)
+        ;
+    /* Add ourselves to interested reader count */
+    cvmx_atomic_add32_nosync((int32_t *)&(lock->lock), CVMX_RWLOCK_READ_INC);
+    /* Wait for writer to finish.  No writer will start again
+    ** until after we are done since we have already incremented
+    ** the reader count
+    */
+    while (lock->lock & CVMX_RWLOCK_WRITE_FLAG)
+        ;
+
+}
+
+/**
+ * Perform a reader unlock.
+ *
+ * @param lock   pointer to rwlock structure
+ */
+static inline void cvmx_rwlock_wp_read_unlock(cvmx_rwlock_wp_lock_t *lock)
+{
+    /* Remove ourselves to reader count */
+    cvmx_atomic_add32_nosync((int32_t *)&(lock->lock), -CVMX_RWLOCK_READ_INC);
+}
+
+/**
+ * Perform a writer lock.  Any readers that attempt
+ * to get a lock while there are any pending write locks
+ * will wait until all writers have completed.  Starvation
+ * of readers by writers is possible and must be avoided
+ * by the application.
+ *
+ * @param lock   pointer to rwlock structure
+ */
+static inline void cvmx_rwlock_wp_write_lock(cvmx_rwlock_wp_lock_t *lock)
+{
+    /* Get previous value of write requests */
+    uint32_t prev_writers = ((uint32_t)cvmx_atomic_fetch_and_add32((int32_t *)&(lock->write_req), 1));
+    /* Spin until our turn */
+    while (prev_writers != lock->write_comp)
+        ;
+    /* Spin until no other readers or writers, then set write flag */
+    while (!cvmx_atomic_compare_and_store32((uint32_t *)&(lock->lock), 0, CVMX_RWLOCK_WRITE_FLAG))
+        ;
+
+}
+/**
+ * Perform a writer unlock.
+ *
+ * @param lock   pointer to rwlock structure
+ */
+static inline void cvmx_rwlock_wp_write_unlock(cvmx_rwlock_wp_lock_t *lock)
+{
+    /* Remove our writer flag */
+    CVMX_SYNCWS;  /* Make sure all writes in protected region are visible before unlock */
+    cvmx_atomic_add32_nosync((int32_t *)&(lock->lock), -CVMX_RWLOCK_WRITE_FLAG);
+    cvmx_atomic_add32_nosync((int32_t *)&(lock->write_comp), 1);
+    CVMX_SYNCWS;  /* push unlock writes out, but don't stall */
+}
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif /* __CVMX_RWLOCK_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-scratch.h b/arch/mips/cavium-octeon/executive/cvmx-scratch.h
new file mode 100644
index 0000000..4c31671
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-scratch.h
@@ -0,0 +1,169 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * This file provides support for the processor local scratch memory.
+ * Scratch memory is byte addressable - all addresses are byte addresses.
+ *
+ *
+ * <hr>$Revision: 32636 $<hr>
+ *
+ *
+ */
+
+
+#ifndef __CVMX_SCRATCH_H__
+#define __CVMX_SCRATCH_H__
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+/* Note: This define must be a long, not a long long in order to compile
+        without warnings for both 32bit and 64bit. */
+#define CVMX_SCRATCH_BASE       (-32768l) /* 0xffffffffffff8000 */
+
+
+/**
+ * Reads an 8 bit value from the processor local scratchpad memory.
+ *
+ * @param address byte address to read from
+ *
+ * @return value read
+ */
+static inline uint8_t cvmx_scratch_read8(uint64_t address)
+{
+    return *CASTPTR(volatile uint8_t, CVMX_SCRATCH_BASE + address);
+}
+/**
+ * Reads a 16 bit value from the processor local scratchpad memory.
+ *
+ * @param address byte address to read from
+ *
+ * @return value read
+ */
+static inline uint16_t cvmx_scratch_read16(uint64_t address)
+{
+    return *CASTPTR(volatile uint16_t, CVMX_SCRATCH_BASE + address);
+}
+/**
+ * Reads a 32 bit value from the processor local scratchpad memory.
+ *
+ * @param address byte address to read from
+ *
+ * @return value read
+ */
+static inline uint32_t cvmx_scratch_read32(uint64_t address)
+{
+    return *CASTPTR(volatile uint32_t, CVMX_SCRATCH_BASE + address);
+}
+/**
+ * Reads a 64 bit value from the processor local scratchpad memory.
+ *
+ * @param address byte address to read from
+ *
+ * @return value read
+ */
+static inline uint64_t cvmx_scratch_read64(uint64_t address)
+{
+    return *CASTPTR(volatile uint64_t, CVMX_SCRATCH_BASE + address);
+}
+
+
+
+/**
+ * Writes an 8 bit value to the processor local scratchpad memory.
+ *
+ * @param address byte address to write to
+ * @param value   value to write
+ */
+static inline void cvmx_scratch_write8(uint64_t address, uint64_t value)
+{
+    *CASTPTR(volatile uint8_t, CVMX_SCRATCH_BASE + address) = (uint8_t)value;
+}
+/**
+ * Writes a 32 bit value to the processor local scratchpad memory.
+ *
+ * @param address byte address to write to
+ * @param value   value to write
+ */
+static inline void cvmx_scratch_write16(uint64_t address, uint64_t value)
+{
+    *CASTPTR(volatile uint16_t, CVMX_SCRATCH_BASE + address) = (uint16_t)value;
+}
+/**
+ * Writes a 16 bit value to the processor local scratchpad memory.
+ *
+ * @param address byte address to write to
+ * @param value   value to write
+ */
+static inline void cvmx_scratch_write32(uint64_t address, uint64_t value)
+{
+    *CASTPTR(volatile uint32_t, CVMX_SCRATCH_BASE + address) = (uint32_t)value;
+}
+/**
+ * Writes a 64 bit value to the processor local scratchpad memory.
+ *
+ * @param address byte address to write to
+ * @param value   value to write
+ */
+static inline void cvmx_scratch_write64(uint64_t address, uint64_t value)
+{
+    *CASTPTR(volatile uint64_t, CVMX_SCRATCH_BASE + address) = value;
+}
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif /* __CVMX_SCRATCH_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-shared-linux-n32.ld b/arch/mips/cavium-octeon/executive/cvmx-shared-linux-n32.ld
new file mode 100644
index 0000000..68d343b
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-shared-linux-n32.ld
@@ -0,0 +1,287 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+/*
+ * This was created from a template supplied by GNU binutils.
+ * Copyright (C) 2005 Cavium Networks
+ */
+
+/**
+ * @file
+ * This linker script for use in building simple executive application to run
+ * under Linux in userspace. The important difference from a standard Linux
+ * binary is the addition of the ".cvmx_shared" memory section. This script
+ * adds two symbols __cvmx_shared_start and __cvmx_shared_end before and after
+ * the CVMX_SHARED data. These are used by cvmx-app-init-linux.c to create a
+ * shared region across all application processes.
+ *
+ * The original template for this files was:
+ * ${OCTEON_ROOT}/tools/mips64-octeon-linux/lib/ldscripts/elf32btsmipn32.x
+ */
+OUTPUT_FORMAT("elf32-ntradbigmips", "elf32-ntradbigmips",
+	      "elf32-ntradlittlemips")
+OUTPUT_ARCH(mips)
+ENTRY(__start)
+SEARCH_DIR("${OCTEON_ROOT}/tools/mips64-octeon-linux/lib");
+/* Do we need any of these for elf?
+   __DYNAMIC = 0;    */
+SECTIONS
+{
+  /* Read-only sections, merged into text segment: */
+  PROVIDE (__executable_start = 0x10000000); . = 0x10000000 + SIZEOF_HEADERS;
+  .interp         : { *(.interp) }
+  .MIPS.options : { *(.MIPS.options) }
+  .dynamic        : { *(.dynamic) }
+  .hash           : { *(.hash) }
+  .dynsym         : { *(.dynsym) }
+  .dynstr         : { *(.dynstr) }
+  .gnu.version    : { *(.gnu.version) }
+  .gnu.version_d  : { *(.gnu.version_d) }
+  .gnu.version_r  : { *(.gnu.version_r) }
+  .rel.init       : { *(.rel.init) }
+  .rela.init      : { *(.rela.init) }
+  .rel.text       : { *(.rel.text .rel.text.* .rel.gnu.linkonce.t.*) }
+  .rela.text      : { *(.rela.text .rela.text.* .rela.gnu.linkonce.t.*) }
+  .rel.fini       : { *(.rel.fini) }
+  .rela.fini      : { *(.rela.fini) }
+  .rel.rodata     : { *(.rel.rodata .rel.rodata.* .rel.gnu.linkonce.r.*) }
+  .rela.rodata    : { *(.rela.rodata .rela.rodata.* .rela.gnu.linkonce.r.*) }
+  .rel.data       : { *(.rel.data .rel.data.* .rel.gnu.linkonce.d.*) }
+  .rela.data      : { *(.rela.data .rela.data.* .rela.gnu.linkonce.d.*) }
+  .rel.tdata	  : { *(.rel.tdata .rel.tdata.* .rel.gnu.linkonce.td.*) }
+  .rela.tdata	  : { *(.rela.tdata .rela.tdata.* .rela.gnu.linkonce.td.*) }
+  .rel.tbss	  : { *(.rel.tbss .rel.tbss.* .rel.gnu.linkonce.tb.*) }
+  .rela.tbss	  : { *(.rela.tbss .rela.tbss.* .rela.gnu.linkonce.tb.*) }
+  .rel.ctors      : { *(.rel.ctors) }
+  .rela.ctors     : { *(.rela.ctors) }
+  .rel.dtors      : { *(.rel.dtors) }
+  .rela.dtors     : { *(.rela.dtors) }
+  .rel.got        : { *(.rel.got) }
+  .rela.got       : { *(.rela.got) }
+  .rel.sdata      : { *(.rel.sdata .rel.sdata.* .rel.gnu.linkonce.s.*) }
+  .rela.sdata     : { *(.rela.sdata .rela.sdata.* .rela.gnu.linkonce.s.*) }
+  .rel.sbss       : { *(.rel.sbss .rel.sbss.* .rel.gnu.linkonce.sb.*) }
+  .rela.sbss      : { *(.rela.sbss .rela.sbss.* .rela.gnu.linkonce.sb.*) }
+  .rel.sdata2     : { *(.rel.sdata2 .rel.sdata2.* .rel.gnu.linkonce.s2.*) }
+  .rela.sdata2    : { *(.rela.sdata2 .rela.sdata2.* .rela.gnu.linkonce.s2.*) }
+  .rel.sbss2      : { *(.rel.sbss2 .rel.sbss2.* .rel.gnu.linkonce.sb2.*) }
+  .rela.sbss2     : { *(.rela.sbss2 .rela.sbss2.* .rela.gnu.linkonce.sb2.*) }
+  .rel.bss        : { *(.rel.bss .rel.bss.* .rel.gnu.linkonce.b.*) }
+  .rela.bss       : { *(.rela.bss .rela.bss.* .rela.gnu.linkonce.b.*) }
+  .rel.plt        : { *(.rel.plt) }
+  .rela.plt       : { *(.rela.plt) }
+  .init           :
+  {
+    KEEP (*(.init))
+  } =0
+  .plt            : { *(.plt) }
+  .text           :
+  {
+    _ftext = . ;
+    *(.text .stub .text.* .gnu.linkonce.t.*)
+    /* .gnu.warning sections are handled specially by elf32.em.  */
+    *(.gnu.warning)
+    *(.mips16.fn.*) *(.mips16.call.*)
+  } =0
+  .fini           :
+  {
+    KEEP (*(.fini))
+  } =0
+  PROVIDE (__etext = .);
+  PROVIDE (_etext = .);
+  PROVIDE (etext = .);
+  .rodata         : { *(.rodata .rodata.* .gnu.linkonce.r.*) }
+  .rodata1        : { *(.rodata1) }
+  .sdata2         : { *(.sdata2 .sdata2.* .gnu.linkonce.s2.*) }
+  .sbss2          : { *(.sbss2 .sbss2.* .gnu.linkonce.sb2.*) }
+  .eh_frame_hdr : { *(.eh_frame_hdr) }
+  /* Adjust the address for the data segment.  We want to adjust up to
+     the same address within the page on the next page up.  */
+  . = ALIGN (0x100000) - ((0x100000 - .) & (0x100000 - 1)); . = DATA_SEGMENT_ALIGN (0x100000, 0x1000);
+  /* Ensure the __preinit_array_start label is properly aligned.  We
+     could instead move the label definition inside the section, but
+     the linker would then create the section even if it turns out to
+     be empty, which isn't pretty.  */
+  . = ALIGN(32 / 8);
+  PROVIDE (__preinit_array_start = .);
+  .preinit_array     : { *(.preinit_array) }
+  PROVIDE (__preinit_array_end = .);
+  PROVIDE (__init_array_start = .);
+  .init_array     : { *(.init_array) }
+  PROVIDE (__init_array_end = .);
+  PROVIDE (__fini_array_start = .);
+  .fini_array     : { *(.fini_array) }
+  PROVIDE (__fini_array_end = .);
+  .data           :
+  {
+    _fdata = . ;
+    *(.data .data.* .gnu.linkonce.d.*)
+    SORT(CONSTRUCTORS)
+  }
+  .data1          : { *(.data1) }
+  .tdata	  : { *(.tdata .tdata.* .gnu.linkonce.td.*) }
+  .tbss		  : { *(.tbss .tbss.* .gnu.linkonce.tb.*) *(.tcommon) }
+  .eh_frame       : { KEEP (*(.eh_frame)) }
+  .gcc_except_table   : { *(.gcc_except_table) }
+  .ctors          :
+  {
+    /* gcc uses crtbegin.o to find the start of
+       the constructors, so we make sure it is
+       first.  Because this is a wildcard, it
+       doesn't matter if the user does not
+       actually link against crtbegin.o; the
+       linker won't look for a file to match a
+       wildcard.  The wildcard also means that it
+       doesn't matter which directory crtbegin.o
+       is in.  */
+    KEEP (*crtbegin*.o(.ctors))
+    /* We don't want to include the .ctor section from
+       from the crtend.o file until after the sorted ctors.
+       The .ctor section from the crtend file contains the
+       end of ctors marker and it must be last */
+    KEEP (*(EXCLUDE_FILE (*crtend*.o ) .ctors))
+    KEEP (*(SORT(.ctors.*)))
+    KEEP (*(.ctors))
+  }
+  .dtors          :
+  {
+    KEEP (*crtbegin*.o(.dtors))
+    KEEP (*(EXCLUDE_FILE (*crtend*.o ) .dtors))
+    KEEP (*(SORT(.dtors.*)))
+    KEEP (*(.dtors))
+  }
+  .jcr            : { KEEP (*(.jcr)) }
+  _gp = ALIGN(16) + 0x7ff0;
+  .got            : { *(.got.plt) *(.got) }
+  /* We want the small data sections together, so single-instruction offsets
+     can access them all, and initialized data all before uninitialized, so
+     we can shorten the on-disk segment size.  */
+  .sdata          :
+  {
+    *(.sdata .sdata.* .gnu.linkonce.s.*)
+  }
+  .lit8           : { *(.lit8) }
+  .lit4           : { *(.lit4) }
+  .srdata         : { *(.srdata) }
+
+  . = ALIGN (0x10000);
+  __cvmx_shared_start = .;
+  .cvmx_shared : {*(.cvmx_shared .cvmx_shared.linkonce.*)}
+  .cvmx_shared_bss : { *(.cvmx_shared_bss .cvmx_shared_bss.linkonce.*) }
+  . = ALIGN (0x10000);
+  __cvmx_shared_end = .;
+
+  _edata = .;
+  PROVIDE (edata = .);
+  __bss_start = .;
+  _fbss = .;
+  .sbss           :
+  {
+    PROVIDE (__sbss_start = .);
+    PROVIDE (___sbss_start = .);
+    *(.dynsbss)
+    *(.sbss .sbss.* .gnu.linkonce.sb.*)
+    *(.scommon)
+    PROVIDE (__sbss_end = .);
+    PROVIDE (___sbss_end = .);
+  }
+  .bss            :
+  {
+   *(.dynbss)
+   *(.bss .bss.* .gnu.linkonce.b.*)
+   *(COMMON)
+   /* Align here to ensure that the .bss section occupies space up to
+      _end.  Align after .bss to ensure correct alignment even if the
+      .bss section disappears because there are no input sections.  */
+   . = ALIGN(32 / 8);
+  }
+  . = ALIGN(32 / 8);
+  . = ALIGN(32M);    /* RBF added alignment of data */
+  .cvmx_shared : { *(.cvmx_shared) }
+  _end = .;
+  PROVIDE (end = .);
+  . = DATA_SEGMENT_END (.);
+  /* Stabs debugging sections.  */
+  .stab          0 : { *(.stab) }
+  .stabstr       0 : { *(.stabstr) }
+  .stab.excl     0 : { *(.stab.excl) }
+  .stab.exclstr  0 : { *(.stab.exclstr) }
+  .stab.index    0 : { *(.stab.index) }
+  .stab.indexstr 0 : { *(.stab.indexstr) }
+  .comment       0 : { *(.comment) }
+  /* DWARF debug sections.
+     Symbols in the DWARF debugging sections are relative to the beginning
+     of the section so we begin them at 0.  */
+  /* DWARF 1 */
+  .debug          0 : { *(.debug) }
+  .line           0 : { *(.line) }
+  /* GNU DWARF 1 extensions */
+  .debug_srcinfo  0 : { *(.debug_srcinfo) }
+  .debug_sfnames  0 : { *(.debug_sfnames) }
+  /* DWARF 1.1 and DWARF 2 */
+  .debug_aranges  0 : { *(.debug_aranges) }
+  .debug_pubnames 0 : { *(.debug_pubnames) }
+  /* DWARF 2 */
+  .debug_info     0 : { *(.debug_info .gnu.linkonce.wi.*) }
+  .debug_abbrev   0 : { *(.debug_abbrev) }
+  .debug_line     0 : { *(.debug_line) }
+  .debug_frame    0 : { *(.debug_frame) }
+  .debug_str      0 : { *(.debug_str) }
+  .debug_loc      0 : { *(.debug_loc) }
+  .debug_macinfo  0 : { *(.debug_macinfo) }
+  /* SGI/MIPS DWARF 2 extensions */
+  .debug_weaknames 0 : { *(.debug_weaknames) }
+  .debug_funcnames 0 : { *(.debug_funcnames) }
+  .debug_typenames 0 : { *(.debug_typenames) }
+  .debug_varnames  0 : { *(.debug_varnames) }
+  .gptab.sdata : { *(.gptab.data) *(.gptab.sdata) }
+  .gptab.sbss : { *(.gptab.bss) *(.gptab.sbss) }
+  /DISCARD/ : { *(.note.GNU-stack) }
+}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-shared-linux-o32.ld b/arch/mips/cavium-octeon/executive/cvmx-shared-linux-o32.ld
new file mode 100644
index 0000000..ace525c
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-shared-linux-o32.ld
@@ -0,0 +1,285 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+/*
+ * This was created from a template supplied by GNU binutils.
+ * Copyright (C) 2004 Cavium Networks
+ */
+
+/**
+ * @file
+ * This linker script for use in building simple executive application to run
+ * under Linux in userspace. The important difference from a standard Linux
+ * binary is the addition of the ".cvmx_shared" memory section. This script
+ * adds two symbols __cvmx_shared_start and __cvmx_sahred_end before and after
+ * the CVMX_SHARED data. These are used by cvmx-app-init-linux.c to create a
+ * shared region across all application processes.
+ *
+ * The original template for this files was:
+ * ${OCTEON_ROOT}/tools/mips64-octeon-linux/lib/ldscripts/elf32btsmip.x
+ */
+OUTPUT_FORMAT("elf32-tradbigmips", "elf32-tradbigmips",
+	      "elf32-tradlittlemips")
+OUTPUT_ARCH(mips)
+ENTRY(__start)
+SEARCH_DIR("${OCTEON_ROOT}/tools/mips64-octeon-linux/lib");
+/* Do we need any of these for elf?
+   __DYNAMIC = 0;    */
+SECTIONS
+{
+  /* Read-only sections, merged into text segment: */
+  PROVIDE (__executable_start = 0x10000000); . = 0x10000000 + SIZEOF_HEADERS;
+  .interp         : { *(.interp) }
+  .reginfo        : { *(.reginfo) }
+  .dynamic        : { *(.dynamic) }
+  .hash           : { *(.hash) }
+  .dynsym         : { *(.dynsym) }
+  .dynstr         : { *(.dynstr) }
+  .gnu.version    : { *(.gnu.version) }
+  .gnu.version_d  : { *(.gnu.version_d) }
+  .gnu.version_r  : { *(.gnu.version_r) }
+  .rel.init       : { *(.rel.init) }
+  .rela.init      : { *(.rela.init) }
+  .rel.text       : { *(.rel.text .rel.text.* .rel.gnu.linkonce.t.*) }
+  .rela.text      : { *(.rela.text .rela.text.* .rela.gnu.linkonce.t.*) }
+  .rel.fini       : { *(.rel.fini) }
+  .rela.fini      : { *(.rela.fini) }
+  .rel.rodata     : { *(.rel.rodata .rel.rodata.* .rel.gnu.linkonce.r.*) }
+  .rela.rodata    : { *(.rela.rodata .rela.rodata.* .rela.gnu.linkonce.r.*) }
+  .rel.data       : { *(.rel.data .rel.data.* .rel.gnu.linkonce.d.*) }
+  .rela.data      : { *(.rela.data .rela.data.* .rela.gnu.linkonce.d.*) }
+  .rel.tdata	  : { *(.rel.tdata .rel.tdata.* .rel.gnu.linkonce.td.*) }
+  .rela.tdata	  : { *(.rela.tdata .rela.tdata.* .rela.gnu.linkonce.td.*) }
+  .rel.tbss	  : { *(.rel.tbss .rel.tbss.* .rel.gnu.linkonce.tb.*) }
+  .rela.tbss	  : { *(.rela.tbss .rela.tbss.* .rela.gnu.linkonce.tb.*) }
+  .rel.ctors      : { *(.rel.ctors) }
+  .rela.ctors     : { *(.rela.ctors) }
+  .rel.dtors      : { *(.rel.dtors) }
+  .rela.dtors     : { *(.rela.dtors) }
+  .rel.got        : { *(.rel.got) }
+  .rela.got       : { *(.rela.got) }
+  .rel.sdata      : { *(.rel.sdata .rel.sdata.* .rel.gnu.linkonce.s.*) }
+  .rela.sdata     : { *(.rela.sdata .rela.sdata.* .rela.gnu.linkonce.s.*) }
+  .rel.sbss       : { *(.rel.sbss .rel.sbss.* .rel.gnu.linkonce.sb.*) }
+  .rela.sbss      : { *(.rela.sbss .rela.sbss.* .rela.gnu.linkonce.sb.*) }
+  .rel.sdata2     : { *(.rel.sdata2 .rel.sdata2.* .rel.gnu.linkonce.s2.*) }
+  .rela.sdata2    : { *(.rela.sdata2 .rela.sdata2.* .rela.gnu.linkonce.s2.*) }
+  .rel.sbss2      : { *(.rel.sbss2 .rel.sbss2.* .rel.gnu.linkonce.sb2.*) }
+  .rela.sbss2     : { *(.rela.sbss2 .rela.sbss2.* .rela.gnu.linkonce.sb2.*) }
+  .rel.bss        : { *(.rel.bss .rel.bss.* .rel.gnu.linkonce.b.*) }
+  .rela.bss       : { *(.rela.bss .rela.bss.* .rela.gnu.linkonce.b.*) }
+  .rel.plt        : { *(.rel.plt) }
+  .rela.plt       : { *(.rela.plt) }
+  .init           :
+  {
+    KEEP (*(.init))
+  } =0
+  .plt            : { *(.plt) }
+  .text           :
+  {
+    _ftext = . ;
+    *(.text .stub .text.* .gnu.linkonce.t.*)
+    /* .gnu.warning sections are handled specially by elf32.em.  */
+    *(.gnu.warning)
+    *(.mips16.fn.*) *(.mips16.call.*)
+  } =0
+  .fini           :
+  {
+    KEEP (*(.fini))
+  } =0
+  PROVIDE (__etext = .);
+  PROVIDE (_etext = .);
+  PROVIDE (etext = .);
+  .rodata         : { *(.rodata .rodata.* .gnu.linkonce.r.*) }
+  .rodata1        : { *(.rodata1) }
+  .sdata2         : { *(.sdata2 .sdata2.* .gnu.linkonce.s2.*) }
+  .sbss2          : { *(.sbss2 .sbss2.* .gnu.linkonce.sb2.*) }
+  .eh_frame_hdr : { *(.eh_frame_hdr) }
+  /* Adjust the address for the data segment.  We want to adjust up to
+     the same address within the page on the next page up.  */
+  . = 0x10000000;
+  /* Ensure the __preinit_array_start label is properly aligned.  We
+     could instead move the label definition inside the section, but
+     the linker would then create the section even if it turns out to
+     be empty, which isn't pretty.  */
+  . = ALIGN(32 / 8);
+  PROVIDE (__preinit_array_start = .);
+  .preinit_array     : { *(.preinit_array) }
+  PROVIDE (__preinit_array_end = .);
+  PROVIDE (__init_array_start = .);
+  .init_array     : { *(.init_array) }
+  PROVIDE (__init_array_end = .);
+  PROVIDE (__fini_array_start = .);
+  .fini_array     : { *(.fini_array) }
+  PROVIDE (__fini_array_end = .);
+  .data           :
+  {
+    _fdata = . ;
+    *(.data .data.* .gnu.linkonce.d.*)
+    SORT(CONSTRUCTORS)
+  }
+  .data1          : { *(.data1) }
+  .tdata	  : { *(.tdata .tdata.* .gnu.linkonce.td.*) }
+  .tbss		  : { *(.tbss .tbss.* .gnu.linkonce.tb.*) *(.tcommon) }
+  .eh_frame       : { KEEP (*(.eh_frame)) }
+  .gcc_except_table   : { *(.gcc_except_table) }
+  .ctors          :
+  {
+    /* gcc uses crtbegin.o to find the start of
+       the constructors, so we make sure it is
+       first.  Because this is a wildcard, it
+       doesn't matter if the user does not
+       actually link against crtbegin.o; the
+       linker won't look for a file to match a
+       wildcard.  The wildcard also means that it
+       doesn't matter which directory crtbegin.o
+       is in.  */
+    KEEP (*crtbegin*.o(.ctors))
+    /* We don't want to include the .ctor section from
+       from the crtend.o file until after the sorted ctors.
+       The .ctor section from the crtend file contains the
+       end of ctors marker and it must be last */
+    KEEP (*(EXCLUDE_FILE (*crtend*.o ) .ctors))
+    KEEP (*(SORT(.ctors.*)))
+    KEEP (*(.ctors))
+  }
+  .dtors          :
+  {
+    KEEP (*crtbegin*.o(.dtors))
+    KEEP (*(EXCLUDE_FILE (*crtend*.o ) .dtors))
+    KEEP (*(SORT(.dtors.*)))
+    KEEP (*(.dtors))
+  }
+  .jcr            : { KEEP (*(.jcr)) }
+  _gp = ALIGN(16) + 0x7ff0;
+  .got            : { *(.got.plt) *(.got) }
+  /* We want the small data sections together, so single-instruction offsets
+     can access them all, and initialized data all before uninitialized, so
+     we can shorten the on-disk segment size.  */
+  .sdata          :
+  {
+    *(.sdata .sdata.* .gnu.linkonce.s.*)
+  }
+  .lit8           : { *(.lit8) }
+  .lit4           : { *(.lit4) }
+
+  . = ALIGN (0x10000);
+  __cvmx_shared_start = .;
+  .cvmx_shared : {*(.cvmx_shared .cvmx_shared.linkonce.*)}
+  .cvmx_shared_bss : {*(.cvmx_shared_bss .cvmx_shared_bss.linkonce.*)}
+  . = ALIGN (0x10000);
+  __cvmx_shared_end = .;
+
+  _edata = .;
+  PROVIDE (edata = .);
+  __bss_start = .;
+  _fbss = .;
+  .sbss           :
+  {
+    PROVIDE (__sbss_start = .);
+    PROVIDE (___sbss_start = .);
+    *(.dynsbss)
+    *(.sbss .sbss.* .gnu.linkonce.sb.*)
+    *(.scommon)
+    PROVIDE (__sbss_end = .);
+    PROVIDE (___sbss_end = .);
+  }
+  .bss            :
+  {
+   *(.dynbss)
+   *(.bss .bss.* .gnu.linkonce.b.*)
+   *(COMMON)
+   /* Align here to ensure that the .bss section occupies space up to
+      _end.  Align after .bss to ensure correct alignment even if the
+      .bss section disappears because there are no input sections.  */
+   . = ALIGN(32 / 8);
+  }
+  . = ALIGN(32 / 8);
+  . = ALIGN(32M);    /* RBF added alignment of data */
+  .cvmx_shared : { *(.cvmx_shared) }
+  _end = .;
+  PROVIDE (end = .);
+  /* Stabs debugging sections.  */
+  .stab          0 : { *(.stab) }
+  .stabstr       0 : { *(.stabstr) }
+  .stab.excl     0 : { *(.stab.excl) }
+  .stab.exclstr  0 : { *(.stab.exclstr) }
+  .stab.index    0 : { *(.stab.index) }
+  .stab.indexstr 0 : { *(.stab.indexstr) }
+  .comment       0 : { *(.comment) }
+  /* DWARF debug sections.
+     Symbols in the DWARF debugging sections are relative to the beginning
+     of the section so we begin them at 0.  */
+  /* DWARF 1 */
+  .debug          0 : { *(.debug) }
+  .line           0 : { *(.line) }
+  /* GNU DWARF 1 extensions */
+  .debug_srcinfo  0 : { *(.debug_srcinfo) }
+  .debug_sfnames  0 : { *(.debug_sfnames) }
+  /* DWARF 1.1 and DWARF 2 */
+  .debug_aranges  0 : { *(.debug_aranges) }
+  .debug_pubnames 0 : { *(.debug_pubnames) }
+  /* DWARF 2 */
+  .debug_info     0 : { *(.debug_info .gnu.linkonce.wi.*) }
+  .debug_abbrev   0 : { *(.debug_abbrev) }
+  .debug_line     0 : { *(.debug_line) }
+  .debug_frame    0 : { *(.debug_frame) }
+  .debug_str      0 : { *(.debug_str) }
+  .debug_loc      0 : { *(.debug_loc) }
+  .debug_macinfo  0 : { *(.debug_macinfo) }
+  /* SGI/MIPS DWARF 2 extensions */
+  .debug_weaknames 0 : { *(.debug_weaknames) }
+  .debug_funcnames 0 : { *(.debug_funcnames) }
+  .debug_typenames 0 : { *(.debug_typenames) }
+  .debug_varnames  0 : { *(.debug_varnames) }
+  .gptab.sdata : { *(.gptab.data) *(.gptab.sdata) }
+  .gptab.sbss : { *(.gptab.bss) *(.gptab.sbss) }
+  /DISCARD/ : { *(.note.GNU-stack) }
+}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-shared-linux.ld b/arch/mips/cavium-octeon/executive/cvmx-shared-linux.ld
new file mode 100644
index 0000000..7cb0a9a
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-shared-linux.ld
@@ -0,0 +1,286 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+/*
+ * This was created from a template supplied by GNU binutils.
+ * Copyright (C) 2004 Cavium Networks
+ */
+ 
+/**
+ * @file
+ * This linker script for use in building simple executive application to run
+ * under Linux in userspace. The important difference from a standard Linux 
+ * binary is the addition of the ".cvmx_shared" memory section. This script 
+ * adds two symbols __cvmx_shared_start and __cvmx_sahred_end before and after
+ * the CVMX_SHARED data. These are used by cvmx-app-init-linux.c to create a
+ * shared region across all application processes.
+ *
+ * The original template for this files was:
+ * ${OCTEON_ROOT}/tools/mips64-octeon-linux/lib/ldscripts/elf64btsmip.x
+ */
+OUTPUT_FORMAT("elf64-tradbigmips", "elf64-tradbigmips",
+	      "elf64-tradlittlemips")
+OUTPUT_ARCH(mips)
+ENTRY(__start)
+SEARCH_DIR("${OCTEON_ROOT}/tools/mips64-octeon-linux/lib");
+/* Do we need any of these for elf?
+   __DYNAMIC = 0;    */
+SECTIONS
+{
+  /* Read-only sections, merged into text segment: */
+  PROVIDE (__executable_start = 0x120000000); . = 0x120000000 + SIZEOF_HEADERS;
+  .interp         : { *(.interp) }
+  .MIPS.options : { *(.MIPS.options) }
+  .dynamic        : { *(.dynamic) }
+  .hash           : { *(.hash) }
+  .dynsym         : { *(.dynsym) }
+  .dynstr         : { *(.dynstr) }
+  .gnu.version    : { *(.gnu.version) }
+  .gnu.version_d  : { *(.gnu.version_d) }
+  .gnu.version_r  : { *(.gnu.version_r) }
+  .rel.init       : { *(.rel.init) }
+  .rela.init      : { *(.rela.init) }
+  .rel.text       : { *(.rel.text .rel.text.* .rel.gnu.linkonce.t.*) }
+  .rela.text      : { *(.rela.text .rela.text.* .rela.gnu.linkonce.t.*) }
+  .rel.fini       : { *(.rel.fini) }
+  .rela.fini      : { *(.rela.fini) }
+  .rel.rodata     : { *(.rel.rodata .rel.rodata.* .rel.gnu.linkonce.r.*) }
+  .rela.rodata    : { *(.rela.rodata .rela.rodata.* .rela.gnu.linkonce.r.*) }
+  .rel.data       : { *(.rel.data .rel.data.* .rel.gnu.linkonce.d.*) }
+  .rela.data      : { *(.rela.data .rela.data.* .rela.gnu.linkonce.d.*) }
+  .rel.tdata	  : { *(.rel.tdata .rel.tdata.* .rel.gnu.linkonce.td.*) }
+  .rela.tdata	  : { *(.rela.tdata .rela.tdata.* .rela.gnu.linkonce.td.*) }
+  .rel.tbss	  : { *(.rel.tbss .rel.tbss.* .rel.gnu.linkonce.tb.*) }
+  .rela.tbss	  : { *(.rela.tbss .rela.tbss.* .rela.gnu.linkonce.tb.*) }
+  .rel.ctors      : { *(.rel.ctors) }
+  .rela.ctors     : { *(.rela.ctors) }
+  .rel.dtors      : { *(.rel.dtors) }
+  .rela.dtors     : { *(.rela.dtors) }
+  .rel.got        : { *(.rel.got) }
+  .rela.got       : { *(.rela.got) }
+  .rel.sdata      : { *(.rel.sdata .rel.sdata.* .rel.gnu.linkonce.s.*) }
+  .rela.sdata     : { *(.rela.sdata .rela.sdata.* .rela.gnu.linkonce.s.*) }
+  .rel.sbss       : { *(.rel.sbss .rel.sbss.* .rel.gnu.linkonce.sb.*) }
+  .rela.sbss      : { *(.rela.sbss .rela.sbss.* .rela.gnu.linkonce.sb.*) }
+  .rel.sdata2     : { *(.rel.sdata2 .rel.sdata2.* .rel.gnu.linkonce.s2.*) }
+  .rela.sdata2    : { *(.rela.sdata2 .rela.sdata2.* .rela.gnu.linkonce.s2.*) }
+  .rel.sbss2      : { *(.rel.sbss2 .rel.sbss2.* .rel.gnu.linkonce.sb2.*) }
+  .rela.sbss2     : { *(.rela.sbss2 .rela.sbss2.* .rela.gnu.linkonce.sb2.*) }
+  .rel.bss        : { *(.rel.bss .rel.bss.* .rel.gnu.linkonce.b.*) }
+  .rela.bss       : { *(.rela.bss .rela.bss.* .rela.gnu.linkonce.b.*) }
+  .rel.plt        : { *(.rel.plt) }
+  .rela.plt       : { *(.rela.plt) }
+  .init           :
+  {
+    KEEP (*(.init))
+  } =0
+  .plt            : { *(.plt) }
+  .text           :
+  {
+    _ftext = . ;
+    *(.text .stub .text.* .gnu.linkonce.t.*)
+    /* .gnu.warning sections are handled specially by elf32.em.  */
+    *(.gnu.warning)
+    *(.mips16.fn.*) *(.mips16.call.*)
+  } =0
+  .fini           :
+  {
+    KEEP (*(.fini))
+  } =0
+  PROVIDE (__etext = .);
+  PROVIDE (_etext = .);
+  PROVIDE (etext = .);
+  .rodata         : { *(.rodata .rodata.* .gnu.linkonce.r.*) }
+  .rodata1        : { *(.rodata1) }
+  .sdata2         : { *(.sdata2 .sdata2.* .gnu.linkonce.s2.*) }
+  .sbss2          : { *(.sbss2 .sbss2.* .gnu.linkonce.sb2.*) }
+  .eh_frame_hdr : { *(.eh_frame_hdr) }
+  /* Adjust the address for the data segment.  We want to adjust up to
+     the same address within the page on the next page up.  */
+  . = ALIGN(0x100000) + (. & (0x100000 - 1));
+  /* Ensure the __preinit_array_start label is properly aligned.  We
+     could instead move the label definition inside the section, but
+     the linker would then create the section even if it turns out to
+     be empty, which isn't pretty.  */
+  . = ALIGN(64 / 8);
+  PROVIDE (__preinit_array_start = .);
+  .preinit_array     : { *(.preinit_array) }
+  PROVIDE (__preinit_array_end = .);
+  PROVIDE (__init_array_start = .);
+  .init_array     : { *(.init_array) }
+  PROVIDE (__init_array_end = .);
+  PROVIDE (__fini_array_start = .);
+  .fini_array     : { *(.fini_array) }
+  PROVIDE (__fini_array_end = .);
+  .data           :
+  {
+    _fdata = . ;
+    *(.data .data.* .gnu.linkonce.d.*)
+    SORT(CONSTRUCTORS)
+  }
+  .data1          : { *(.data1) }
+  .tdata	  : { *(.tdata .tdata.* .gnu.linkonce.td.*) }
+  .tbss		  : { *(.tbss .tbss.* .gnu.linkonce.tb.*) *(.tcommon) }
+  .eh_frame       : { KEEP (*(.eh_frame)) }
+  .gcc_except_table   : { *(.gcc_except_table) }
+  .ctors          :
+  {
+    /* gcc uses crtbegin.o to find the start of
+       the constructors, so we make sure it is
+       first.  Because this is a wildcard, it
+       doesn't matter if the user does not
+       actually link against crtbegin.o; the
+       linker won't look for a file to match a
+       wildcard.  The wildcard also means that it
+       doesn't matter which directory crtbegin.o
+       is in.  */
+    KEEP (*crtbegin*.o(.ctors))
+    /* We don't want to include the .ctor section from
+       from the crtend.o file until after the sorted ctors.
+       The .ctor section from the crtend file contains the
+       end of ctors marker and it must be last */
+    KEEP (*(EXCLUDE_FILE (*crtend*.o ) .ctors))
+    KEEP (*(SORT(.ctors.*)))
+    KEEP (*(.ctors))
+  }
+  .dtors          :
+  {
+    KEEP (*crtbegin*.o(.dtors))
+    KEEP (*(EXCLUDE_FILE (*crtend*.o ) .dtors))
+    KEEP (*(SORT(.dtors.*)))
+    KEEP (*(.dtors))
+  }
+  .jcr            : { KEEP (*(.jcr)) }
+  _gp = ALIGN(16) + 0x7ff0;
+  .got            : { *(.got.plt) *(.got) }
+  /* We want the small data sections together, so single-instruction offsets
+     can access them all, and initialized data all before uninitialized, so
+     we can shorten the on-disk segment size.  */
+  .sdata          :
+  {
+    *(.sdata .sdata.* .gnu.linkonce.s.*)
+  }
+  .lit8           : { *(.lit8) }
+  .lit4           : { *(.lit4) }
+  .srdata         : { *(.srdata) }
+  
+  . = ALIGN (0x10000); 
+  __cvmx_shared_start = .;
+  .cvmx_shared : {*(.cvmx_shared .cvmx_shared.linkonce.*)}
+  .cvmx_shared_bss : { *(.cvmx_shared_bss .cvmx_shared_bss.linkonce.*) }
+  . = ALIGN (0x10000);
+  __cvmx_shared_end = .;
+  
+  _edata = .;
+  PROVIDE (edata = .);
+  __bss_start = .;
+  _fbss = .;
+  .sbss           :
+  {
+    PROVIDE (__sbss_start = .);
+    PROVIDE (___sbss_start = .);
+    *(.dynsbss)
+    *(.sbss .sbss.* .gnu.linkonce.sb.*)
+    *(.scommon)
+    PROVIDE (__sbss_end = .);
+    PROVIDE (___sbss_end = .);
+  }
+  .bss            :
+  {
+   *(.dynbss)
+   *(.bss .bss.* .gnu.linkonce.b.*)
+   *(COMMON)
+   /* Align here to ensure that the .bss section occupies space up to
+      _end.  Align after .bss to ensure correct alignment even if the
+      .bss section disappears because there are no input sections.  */
+   . = ALIGN(64 / 8);
+  }
+  . = ALIGN(64 / 8);
+  . = ALIGN(32M);    /* RBF added alignment of data */
+  .cvmx_shared : { *(.cvmx_shared) }
+  _end = .;
+  PROVIDE (end = .);
+  /* Stabs debugging sections.  */
+  .stab          0 : { *(.stab) }
+  .stabstr       0 : { *(.stabstr) }
+  .stab.excl     0 : { *(.stab.excl) }
+  .stab.exclstr  0 : { *(.stab.exclstr) }
+  .stab.index    0 : { *(.stab.index) }
+  .stab.indexstr 0 : { *(.stab.indexstr) }
+  .comment       0 : { *(.comment) }
+  /* DWARF debug sections.
+     Symbols in the DWARF debugging sections are relative to the beginning
+     of the section so we begin them at 0.  */
+  /* DWARF 1 */
+  .debug          0 : { *(.debug) }
+  .line           0 : { *(.line) }
+  /* GNU DWARF 1 extensions */
+  .debug_srcinfo  0 : { *(.debug_srcinfo) }
+  .debug_sfnames  0 : { *(.debug_sfnames) }
+  /* DWARF 1.1 and DWARF 2 */
+  .debug_aranges  0 : { *(.debug_aranges) }
+  .debug_pubnames 0 : { *(.debug_pubnames) }
+  /* DWARF 2 */
+  .debug_info     0 : { *(.debug_info .gnu.linkonce.wi.*) }
+  .debug_abbrev   0 : { *(.debug_abbrev) }
+  .debug_line     0 : { *(.debug_line) }
+  .debug_frame    0 : { *(.debug_frame) }
+  .debug_str      0 : { *(.debug_str) }
+  .debug_loc      0 : { *(.debug_loc) }
+  .debug_macinfo  0 : { *(.debug_macinfo) }
+  /* SGI/MIPS DWARF 2 extensions */
+  .debug_weaknames 0 : { *(.debug_weaknames) }
+  .debug_funcnames 0 : { *(.debug_funcnames) }
+  .debug_typenames 0 : { *(.debug_typenames) }
+  .debug_varnames  0 : { *(.debug_varnames) }
+  .gptab.sdata : { *(.gptab.data) *(.gptab.sdata) }
+  .gptab.sbss : { *(.gptab.bss) *(.gptab.sbss) }
+  /DISCARD/ : { *(.note.GNU-stack) }
+}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-spi.c b/arch/mips/cavium-octeon/executive/cvmx-spi.c
new file mode 100644
index 0000000..39e1b78
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-spi.c
@@ -0,0 +1,580 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Support library for the SPI
+ *
+ * <hr>$Revision: 34716 $<hr>
+ */
+#include "cvmx-config.h"
+#include "cvmx.h"
+#include "cvmx-mio.h"
+#include "cvmx-pko.h"
+#include "cvmx-spi.h"
+#include "cvmx-sysinfo.h"
+
+#define INVOKE_CB(function_p, args...) \
+        do { \
+            if (function_p) { \
+                res = function_p(args); \
+                if (res) \
+                    return res; \
+            } \
+        } while (0)
+
+#if CVMX_ENABLE_DEBUG_PRINTS
+static const char *modes[] = {"UNKNOWN", "TX Halfplex", "Rx Halfplex", "Duplex"};
+#endif
+
+/* Default callbacks, can be overridden
+ *  using cvmx_spi_get_callbacks/cvmx_spi_set_callbacks
+ */
+static cvmx_spi_callbacks_t cvmx_spi_callbacks = {
+  .reset_cb            = cvmx_spi_reset_cb,
+  .calendar_setup_cb   = cvmx_spi_calendar_setup_cb,
+  .clock_detect_cb     = cvmx_spi_clock_detect_cb,
+  .training_cb         = cvmx_spi_training_cb,
+  .calendar_sync_cb    = cvmx_spi_calendar_sync_cb,
+  .interface_up_cb     = cvmx_spi_interface_up_cb
+};
+
+/**
+ * Get current SPI4 initialization callbacks
+ *
+ * @param callbacks  Pointer to the callbacks structure.to fill
+ *
+ * @return Pointer to cvmx_spi_callbacks_t structure.
+ */
+void cvmx_spi_get_callbacks(cvmx_spi_callbacks_t * callbacks)
+{
+    memcpy(callbacks, &cvmx_spi_callbacks, sizeof(cvmx_spi_callbacks));
+}
+
+/**
+ * Set new SPI4 initialization callbacks
+ *
+ * @param new_callbacks  Pointer to an updated callbacks structure.
+ */
+void cvmx_spi_set_callbacks(cvmx_spi_callbacks_t * new_callbacks)
+{
+    memcpy(&cvmx_spi_callbacks, new_callbacks, sizeof(cvmx_spi_callbacks));
+}
+
+/**
+ * Initialize and start the SPI interface.
+ *
+ * @param interface The identifier of the packet interface to configure and
+ *                  use as a SPI interface.
+ * @param mode      The operating mode for the SPI interface. The interface
+ *                  can operate as a full duplex (both Tx and Rx data paths
+ *                  active) or as a halfplex (either the Tx data path is
+ *                  active or the Rx data path is active, but not both).
+ * @param timeout   Timeout to wait for clock synchronization in seconds
+ * @param num_ports Number of SPI ports to configure
+ *
+ * @return Zero on success, negative of failure.
+ */
+int cvmx_spi_start_interface(int interface, cvmx_spi_mode_t mode, int timeout, int num_ports)
+{
+    int res = -1;
+
+    if (!(OCTEON_IS_MODEL(OCTEON_CN38XX) || OCTEON_IS_MODEL(OCTEON_CN58XX)))
+        return res;
+
+    // Callback to perform SPI4 reset
+    INVOKE_CB( cvmx_spi_callbacks.reset_cb, interface, mode);
+
+    // Callback to perform calendar setup
+    INVOKE_CB(cvmx_spi_callbacks.calendar_setup_cb, interface, mode, num_ports);
+
+    // Callback to perform clock detection
+    INVOKE_CB(cvmx_spi_callbacks.clock_detect_cb, interface, mode, timeout);
+
+    // Callback to perform SPI4 link training
+    INVOKE_CB(cvmx_spi_callbacks.training_cb, interface, mode, timeout);
+
+    // Callback to perform calendar sync
+    INVOKE_CB(cvmx_spi_callbacks.calendar_sync_cb, interface, mode, timeout);
+
+    // Callback to handle interface coming up
+    INVOKE_CB(cvmx_spi_callbacks.interface_up_cb, interface, mode);
+
+    return res;
+}
+
+/**
+ * This routine restarts the SPI interface after it has lost synchronization
+ * with its correspondent system.
+ *
+ * @param interface The identifier of the packet interface to configure and
+ *                  use as a SPI interface.
+ * @param mode      The operating mode for the SPI interface. The interface
+ *                  can operate as a full duplex (both Tx and Rx data paths
+ *                  active) or as a halfplex (either the Tx data path is
+ *                  active or the Rx data path is active, but not both).
+ * @param timeout   Timeout to wait for clock synchronization in seconds
+ * @return Zero on success, negative of failure.
+ */
+int cvmx_spi_restart_interface(int interface, cvmx_spi_mode_t mode, int timeout)
+{
+    int res = -1;
+
+
+    if (!(OCTEON_IS_MODEL(OCTEON_CN38XX) || OCTEON_IS_MODEL(OCTEON_CN58XX)))
+        return res;
+
+    cvmx_dprintf ("SPI%d: Restart %s\n", interface, modes[mode]);
+
+    // Callback to perform SPI4 reset
+    INVOKE_CB(cvmx_spi_callbacks.reset_cb, interface,mode);
+
+    // NOTE: Calendar setup is not performed during restart
+    //       Refer to cvmx_spi_start_interface() for the full sequence
+
+    // Callback to perform clock detection
+    INVOKE_CB(cvmx_spi_callbacks.clock_detect_cb, interface, mode, timeout);
+
+    // Callback to perform SPI4 link training
+    INVOKE_CB(cvmx_spi_callbacks.training_cb, interface, mode, timeout);
+
+    // Callback to perform calendar sync
+    INVOKE_CB(cvmx_spi_callbacks.calendar_sync_cb, interface, mode, timeout);
+
+    // Callback to handle interface coming up
+    INVOKE_CB(cvmx_spi_callbacks.interface_up_cb, interface, mode);
+
+    return res;
+}
+
+/**
+ * Callback to perform SPI4 reset
+ *
+ * @param interface The identifier of the packet interface to configure and
+ *                  use as a SPI interface.
+ * @param mode      The operating mode for the SPI interface. The interface
+ *                  can operate as a full duplex (both Tx and Rx data paths
+ *                  active) or as a halfplex (either the Tx data path is
+ *                  active or the Rx data path is active, but not both).
+ * @return Zero on success, non-zero error code on failure (will cause SPI initialization to abort)
+ */
+int cvmx_spi_reset_cb(int interface, cvmx_spi_mode_t mode)
+{
+    cvmx_spxx_dbg_deskew_ctl_t spxx_dbg_deskew_ctl;
+    cvmx_spxx_clk_ctl_t spxx_clk_ctl;
+    cvmx_spxx_trn4_ctl_t spxx_trn4_ctl;
+    uint64_t MS = cvmx_sysinfo_get()->cpu_clock_hz / 1000;
+
+    // Setup the CLKDLY right in the middle
+    spxx_clk_ctl.u64 = 0;
+    spxx_clk_ctl.s.seetrn = 0;
+    spxx_clk_ctl.s.clkdly = 0x10;
+    spxx_clk_ctl.s.runbist = 0;
+    spxx_clk_ctl.s.statdrv = 1;
+    spxx_clk_ctl.s.statrcv = 1;
+    spxx_clk_ctl.s.sndtrn = 0;
+    spxx_clk_ctl.s.drptrn = 0;
+    spxx_clk_ctl.s.rcvtrn = 0;
+    spxx_clk_ctl.s.srxdlck = 0;
+    cvmx_write_csr(CVMX_SPXX_CLK_CTL(interface), spxx_clk_ctl.u64);
+    cvmx_wait (100 * MS);
+
+    // Reset SRX0 DLL
+    spxx_clk_ctl.s.srxdlck = 1;
+    cvmx_write_csr(CVMX_SPXX_CLK_CTL(interface), spxx_clk_ctl.u64);
+
+    // Waiting for Inf0 Spi4 RX DLL to lock
+    cvmx_wait (100 * MS);
+
+    // Enable dynamic alignment
+    spxx_trn4_ctl.s.trntest = 0;
+    spxx_trn4_ctl.s.jitter = 1;
+    spxx_trn4_ctl.s.clr_boot = 1;
+    spxx_trn4_ctl.s.set_boot = 0;
+    if (OCTEON_IS_MODEL(OCTEON_CN58XX))
+        spxx_trn4_ctl.s.maxdist = 3;
+    else
+        spxx_trn4_ctl.s.maxdist = 8;
+    spxx_trn4_ctl.s.macro_en = 1;
+    spxx_trn4_ctl.s.mux_en = 1;
+    cvmx_write_csr (CVMX_SPXX_TRN4_CTL(interface), spxx_trn4_ctl.u64);
+
+    spxx_dbg_deskew_ctl.u64 = 0;
+    cvmx_write_csr (CVMX_SPXX_DBG_DESKEW_CTL(interface), spxx_dbg_deskew_ctl.u64);
+
+    return 0;
+}
+
+/**
+ * Callback to setup calendar and miscellaneous settings before clock detection
+ *
+ * @param interface The identifier of the packet interface to configure and
+ *                  use as a SPI interface.
+ * @param mode      The operating mode for the SPI interface. The interface
+ *                  can operate as a full duplex (both Tx and Rx data paths
+ *                  active) or as a halfplex (either the Tx data path is
+ *                  active or the Rx data path is active, but not both).
+ * @param num_ports Number of ports to configure on SPI
+ * @return Zero on success, non-zero error code on failure (will cause SPI initialization to abort)
+ */
+int cvmx_spi_calendar_setup_cb(int interface, cvmx_spi_mode_t mode, int num_ports)
+{
+    int port;
+    int index;
+    if (mode & CVMX_SPI_MODE_RX_HALFPLEX)
+    {
+        cvmx_srxx_com_ctl_t srxx_com_ctl;
+        cvmx_srxx_spi4_stat_t srxx_spi4_stat;
+
+        // SRX0 number of Ports
+        srxx_com_ctl.u64 = 0;
+        srxx_com_ctl.s.prts = num_ports - 1;
+        srxx_com_ctl.s.st_en = 0;
+        srxx_com_ctl.s.inf_en = 0;
+        cvmx_write_csr(CVMX_SRXX_COM_CTL(interface), srxx_com_ctl.u64);
+
+        // SRX0 Calendar Table. This round robbins through all ports
+        port = 0;
+        index = 0;
+        while (port < num_ports)
+        {
+            cvmx_srxx_spi4_calx_t srxx_spi4_calx;
+            srxx_spi4_calx.u64 = 0;
+            srxx_spi4_calx.s.prt0 = port++;
+            srxx_spi4_calx.s.prt1 = port++;
+            srxx_spi4_calx.s.prt2 = port++;
+            srxx_spi4_calx.s.prt3 = port++;
+            srxx_spi4_calx.s.oddpar = ~(cvmx_dpop(srxx_spi4_calx.u64) & 1);
+            cvmx_write_csr(CVMX_SRXX_SPI4_CALX(index, interface), srxx_spi4_calx.u64);
+            index++;
+        }
+        srxx_spi4_stat.u64 = 0;
+        srxx_spi4_stat.s.len = num_ports;
+        srxx_spi4_stat.s.m = 1;
+        cvmx_write_csr(CVMX_SRXX_SPI4_STAT(interface), srxx_spi4_stat.u64);
+    }
+
+    if (mode & CVMX_SPI_MODE_TX_HALFPLEX)
+    {
+        cvmx_stxx_arb_ctl_t stxx_arb_ctl;
+        cvmx_gmxx_tx_spi_max_t gmxx_tx_spi_max;
+        cvmx_gmxx_tx_spi_thresh_t gmxx_tx_spi_thresh;
+        cvmx_gmxx_tx_spi_ctl_t gmxx_tx_spi_ctl;
+        cvmx_stxx_spi4_stat_t stxx_spi4_stat;
+        cvmx_stxx_spi4_dat_t stxx_spi4_dat;
+
+        // STX0 Config
+        stxx_arb_ctl.u64 = 0;
+        stxx_arb_ctl.s.igntpa = 0;
+        stxx_arb_ctl.s.mintrn = 0;
+        cvmx_write_csr(CVMX_STXX_ARB_CTL(interface), stxx_arb_ctl.u64);
+
+        gmxx_tx_spi_max.u64 = 0;
+        gmxx_tx_spi_max.s.max1 = 8;
+        gmxx_tx_spi_max.s.max2 = 4;
+        gmxx_tx_spi_max.s.slice = 0;
+        cvmx_write_csr(CVMX_GMXX_TX_SPI_MAX(interface), gmxx_tx_spi_max.u64);
+
+        gmxx_tx_spi_thresh.u64 = 0;
+        gmxx_tx_spi_thresh.s.thresh = 4;
+        cvmx_write_csr(CVMX_GMXX_TX_SPI_THRESH(interface), gmxx_tx_spi_thresh.u64);
+
+        gmxx_tx_spi_ctl.u64 = 0;
+        gmxx_tx_spi_ctl.s.tpa_clr = 0;
+        gmxx_tx_spi_ctl.s.cont_pkt = 0;
+        cvmx_write_csr(CVMX_GMXX_TX_SPI_CTL(interface), gmxx_tx_spi_ctl.u64);
+
+        // STX0 Training Control
+        stxx_spi4_dat.u64 = 0;
+        stxx_spi4_dat.s.alpha = 32;    /*Minimum needed by dynamic alignment*/
+        stxx_spi4_dat.s.max_t = 0xFFFF;  /*Minimum interval is 0x20*/
+        cvmx_write_csr(CVMX_STXX_SPI4_DAT(interface), stxx_spi4_dat.u64);
+
+        // STX0 Calendar Table. This round robbins through all ports
+        port = 0;
+        index = 0;
+        while (port < num_ports)
+        {
+            cvmx_stxx_spi4_calx_t stxx_spi4_calx;
+            stxx_spi4_calx.u64 = 0;
+            stxx_spi4_calx.s.prt0 = port++;
+            stxx_spi4_calx.s.prt1 = port++;
+            stxx_spi4_calx.s.prt2 = port++;
+            stxx_spi4_calx.s.prt3 = port++;
+            stxx_spi4_calx.s.oddpar = ~(cvmx_dpop(stxx_spi4_calx.u64) & 1);
+            cvmx_write_csr(CVMX_STXX_SPI4_CALX(index, interface), stxx_spi4_calx.u64);
+            index++;
+        }
+        stxx_spi4_stat.u64 = 0;
+        stxx_spi4_stat.s.len = num_ports;
+        stxx_spi4_stat.s.m = 1;
+        cvmx_write_csr(CVMX_STXX_SPI4_STAT(interface), stxx_spi4_stat.u64);
+    }
+
+    return 0;
+}
+
+/**
+ * Callback to perform clock detection
+ *
+ * @param interface The identifier of the packet interface to configure and
+ *                  use as a SPI interface.
+ * @param mode      The operating mode for the SPI interface. The interface
+ *                  can operate as a full duplex (both Tx and Rx data paths
+ *                  active) or as a halfplex (either the Tx data path is
+ *                  active or the Rx data path is active, but not both).
+ * @param timeout   Timeout to wait for clock synchronization in seconds
+ * @return Zero on success, non-zero error code on failure (will cause SPI initialization to abort)
+ */
+int cvmx_spi_clock_detect_cb(int interface, cvmx_spi_mode_t mode, int timeout)
+{
+    cvmx_spxx_clk_stat_t         stat;
+    uint64_t                     timeout_time;
+    uint64_t                     count;
+    uint64_t                     MS = cvmx_sysinfo_get()->cpu_clock_hz / 1000;
+
+    /* Regardless of operating mode, both Tx and Rx clocks must be present
+        for the SPI interface to operate. */
+    cvmx_dprintf ("SPI%d: Waiting to see TsClk...\n", interface);
+    count = 0;
+    timeout_time = cvmx_get_cycle() + 1000ull * MS * timeout;
+    do
+    {  /* Do we see the TsClk transitioning? */
+        stat.u64 = cvmx_read_csr (CVMX_SPXX_CLK_STAT(interface));
+        count = count + 1;
+#ifdef DEBUG
+        if ((count % 5000000) == 10)
+        {
+            cvmx_dprintf ("SPI%d: CLK_STAT 0x%016llX\n"
+                          "  s4 (%d,%d) d4 (%d,%d)\n",
+                          interface, (unsigned long long)stat.u64,
+                          stat.s.s4clk0, stat.s.s4clk1,
+                          stat.s.d4clk0, stat.s.d4clk1);
+        }
+#endif
+        if (cvmx_get_cycle() > timeout_time)
+        {
+            cvmx_dprintf ("SPI%d: Timeout\n", interface);
+            return -1;
+        }
+    } while (stat.s.s4clk0 == 0 || stat.s.s4clk1 == 0);
+
+    cvmx_dprintf ("SPI%d: Waiting to see RsClk...\n", interface);
+    timeout_time = cvmx_get_cycle() + 1000ull * MS * timeout;
+    do
+    {  /* Do we see the RsClk transitioning? */
+        stat.u64 = cvmx_read_csr (CVMX_SPXX_CLK_STAT(interface));
+        if (cvmx_get_cycle() > timeout_time)
+        {
+            cvmx_dprintf ("SPI%d: Timeout\n", interface);
+            return -1;
+        }
+    } while (stat.s.d4clk0 == 0 || stat.s.d4clk1 == 0);
+
+    return 0;
+}
+
+/**
+ * Callback to perform link training
+ *
+ * @param interface The identifier of the packet interface to configure and
+ *                  use as a SPI interface.
+ * @param mode      The operating mode for the SPI interface. The interface
+ *                  can operate as a full duplex (both Tx and Rx data paths
+ *                  active) or as a halfplex (either the Tx data path is
+ *                  active or the Rx data path is active, but not both).
+ * @param timeout   Timeout to wait for link to be trained (in seconds)
+ * @return Zero on success, non-zero error code on failure (will cause SPI initialization to abort)
+ */
+int cvmx_spi_training_cb(int interface, cvmx_spi_mode_t mode, int timeout)
+{
+    cvmx_spxx_trn4_ctl_t         spxx_trn4_ctl;
+    cvmx_spxx_clk_stat_t         stat;
+    uint64_t                     MS = cvmx_sysinfo_get()->cpu_clock_hz / 1000;
+    uint64_t                     timeout_time = cvmx_get_cycle() + 1000ull * MS * timeout;
+
+    // SRX0 & STX0 Inf0 Links are configured - begin training
+    cvmx_spxx_clk_ctl_t spxx_clk_ctl;
+    spxx_clk_ctl.u64 = 0;
+    spxx_clk_ctl.s.seetrn = 0;
+    spxx_clk_ctl.s.clkdly = 0x10;
+    spxx_clk_ctl.s.runbist = 0;
+    spxx_clk_ctl.s.statdrv = 1;
+    spxx_clk_ctl.s.statrcv = 1;
+    spxx_clk_ctl.s.sndtrn = 1;
+    spxx_clk_ctl.s.drptrn = 1;
+    spxx_clk_ctl.s.rcvtrn = 1;
+    spxx_clk_ctl.s.srxdlck = 1;
+    cvmx_write_csr(CVMX_SPXX_CLK_CTL(interface), spxx_clk_ctl.u64);
+    cvmx_wait (1000 * MS);
+
+    // SRX0 clear the boot bit
+    spxx_trn4_ctl.u64 = cvmx_read_csr(CVMX_SPXX_TRN4_CTL(interface));
+    spxx_trn4_ctl.s.clr_boot = 1;
+    cvmx_write_csr (CVMX_SPXX_TRN4_CTL(interface), spxx_trn4_ctl.u64);
+
+    // Wait for the training sequence to complete
+    cvmx_dprintf ("SPI%d: Waiting for training\n", interface);
+    cvmx_wait (1000 * MS);
+    timeout_time = cvmx_get_cycle() + 1000ull * MS * 600;  /* Wait a really long time here */
+    // SPX0_CLK_STAT - SPX0_CLK_STAT[SRXTRN] should be 1 (bit8)
+    do {
+        stat.u64 = cvmx_read_csr (CVMX_SPXX_CLK_STAT(interface));
+        if (cvmx_get_cycle() > timeout_time)
+        {
+            cvmx_dprintf ("SPI%d: Timeout\n", interface);
+            return -1;
+        }
+    } while (stat.s.srxtrn == 0);
+
+    return 0;
+}
+
+/**
+ * Callback to perform calendar data synchronization
+ *
+ * @param interface The identifier of the packet interface to configure and
+ *                  use as a SPI interface.
+ * @param mode      The operating mode for the SPI interface. The interface
+ *                  can operate as a full duplex (both Tx and Rx data paths
+ *                  active) or as a halfplex (either the Tx data path is
+ *                  active or the Rx data path is active, but not both).
+ * @param timeout   Timeout to wait for calendar data in seconds
+ * @return Zero on success, non-zero error code on failure (will cause SPI initialization to abort)
+ */
+int cvmx_spi_calendar_sync_cb(int interface, cvmx_spi_mode_t mode, int timeout)
+{
+    uint64_t MS = cvmx_sysinfo_get()->cpu_clock_hz / 1000;
+    if (mode & CVMX_SPI_MODE_RX_HALFPLEX) {
+        // SRX0 interface should be good, send calendar data
+        cvmx_srxx_com_ctl_t srxx_com_ctl;
+        cvmx_dprintf ("SPI%d: Rx is synchronized, start sending calendar data\n", interface);
+        srxx_com_ctl.u64 = cvmx_read_csr(CVMX_SRXX_COM_CTL(interface));
+        srxx_com_ctl.s.inf_en = 1;
+        srxx_com_ctl.s.st_en  = 1;
+        cvmx_write_csr (CVMX_SRXX_COM_CTL(interface), srxx_com_ctl.u64);
+    }
+
+    if (mode & CVMX_SPI_MODE_TX_HALFPLEX) {
+        // STX0 has achieved sync
+        // The corespondant board should be sending calendar data
+        // Enable the STX0 STAT receiver.
+        cvmx_spxx_clk_stat_t stat;
+        uint64_t timeout_time;
+        cvmx_stxx_com_ctl_t stxx_com_ctl;
+        stxx_com_ctl.u64 = 0;
+        stxx_com_ctl.s.st_en = 1;
+        cvmx_write_csr (CVMX_STXX_COM_CTL(interface), stxx_com_ctl.u64);
+
+        // Waiting for calendar sync on STX0 STAT
+        cvmx_dprintf ("SPI%d: Waiting to sync on STX[%d] STAT\n", interface, interface);
+        timeout_time = cvmx_get_cycle() + 1000ull * MS * timeout;
+        // SPX0_CLK_STAT - SPX0_CLK_STAT[STXCAL] should be 1 (bit10)
+        do {
+            stat.u64 = cvmx_read_csr (CVMX_SPXX_CLK_STAT (interface));
+            if (cvmx_get_cycle() > timeout_time)
+            {
+                cvmx_dprintf ("SPI%d: Timeout\n", interface);
+                return -1;
+            }
+        } while (stat.s.stxcal == 0);
+    }
+
+    return 0;
+}
+
+/**
+ * Callback to handle interface up
+ *
+ * @param interface The identifier of the packet interface to configure and
+ *                  use as a SPI interface.
+ * @param mode      The operating mode for the SPI interface. The interface
+ *                  can operate as a full duplex (both Tx and Rx data paths
+ *                  active) or as a halfplex (either the Tx data path is
+ *                  active or the Rx data path is active, but not both).
+ * @return Zero on success, non-zero error code on failure (will cause SPI initialization to abort)
+ */
+int cvmx_spi_interface_up_cb(int interface, cvmx_spi_mode_t mode)
+{
+    cvmx_gmxx_rxx_frm_min_t gmxx_rxx_frm_min;
+    cvmx_gmxx_rxx_frm_max_t gmxx_rxx_frm_max;
+    cvmx_gmxx_rxx_jabber_t gmxx_rxx_jabber;
+
+    if (mode & CVMX_SPI_MODE_RX_HALFPLEX) {
+        cvmx_srxx_com_ctl_t srxx_com_ctl;
+        srxx_com_ctl.u64 = cvmx_read_csr(CVMX_SRXX_COM_CTL(interface));
+        srxx_com_ctl.s.inf_en = 1;
+        cvmx_write_csr (CVMX_SRXX_COM_CTL(interface), srxx_com_ctl.u64);
+        cvmx_dprintf ("SPI%d: Rx is now up\n", interface);
+    }
+
+    if (mode & CVMX_SPI_MODE_TX_HALFPLEX) {
+        cvmx_stxx_com_ctl_t stxx_com_ctl;
+        stxx_com_ctl.u64 = cvmx_read_csr(CVMX_STXX_COM_CTL(interface));
+        stxx_com_ctl.s.inf_en = 1;
+        cvmx_write_csr (CVMX_STXX_COM_CTL(interface), stxx_com_ctl.u64);
+        cvmx_dprintf ("SPI%d: Tx is now up\n", interface);
+    }
+
+    gmxx_rxx_frm_min.u64 = 0;
+    gmxx_rxx_frm_min.s.len = 64;
+    cvmx_write_csr(CVMX_GMXX_RXX_FRM_MIN(0,interface), gmxx_rxx_frm_min.u64);
+    gmxx_rxx_frm_max.u64 = 0;
+    gmxx_rxx_frm_max.s.len = 64*1024 - 4;
+    cvmx_write_csr(CVMX_GMXX_RXX_FRM_MAX(0,interface), gmxx_rxx_frm_max.u64);
+    gmxx_rxx_jabber.u64 = 0;
+    gmxx_rxx_jabber.s.cnt = 64*1024 - 4;
+    cvmx_write_csr(CVMX_GMXX_RXX_JABBER(0,interface), gmxx_rxx_jabber.u64);
+
+    return 0;
+}
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-spi.h b/arch/mips/cavium-octeon/executive/cvmx-spi.h
new file mode 100644
index 0000000..f44e711
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-spi.h
@@ -0,0 +1,273 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * This file contains defines for the SPI interface
+
+ * <hr>$Revision: 32636 $<hr>
+ *
+ *
+ */
+#ifndef __CVMX_SPI_H__
+#define __CVMX_SPI_H__
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+/* CSR typedefs have been moved to cvmx-csr-*.h */
+
+typedef enum
+{
+    CVMX_SPI_MODE_UNKNOWN = 0,
+    CVMX_SPI_MODE_TX_HALFPLEX = 1,
+    CVMX_SPI_MODE_RX_HALFPLEX = 2,
+    CVMX_SPI_MODE_DUPLEX = 3
+} cvmx_spi_mode_t;
+
+/** Callbacks structure to customize SPI4 initialization sequence */
+typedef struct
+{
+    /** Called to reset SPI4 DLL */
+    int (*reset_cb)(int interface, cvmx_spi_mode_t mode);
+
+    /** Called to setup calendar */
+    int (*calendar_setup_cb)(int interface, cvmx_spi_mode_t mode, int num_ports);
+
+    /** Called for Tx and Rx clock detection */
+    int (*clock_detect_cb)(int interface, cvmx_spi_mode_t mode, int timeout);
+
+    /** Called to perform link training */
+    int (*training_cb)(int interface, cvmx_spi_mode_t mode, int timeout);
+
+    /** Called for calendar data synchronization */
+    int (*calendar_sync_cb)(int interface, cvmx_spi_mode_t mode, int timeout);
+
+    /** Called when interface is up */
+    int (*interface_up_cb)(int interface, cvmx_spi_mode_t mode);
+
+} cvmx_spi_callbacks_t;
+
+
+/**
+ * Return true if the supplied interface is configured for SPI
+ *
+ * @param interface Interface to check
+ * @return True if interface is SPI
+ */
+static inline int cvmx_spi_is_spi_interface(int interface)
+{
+    uint64_t gmxState = cvmx_read_csr(CVMX_GMXX_INF_MODE(interface));
+    return ((gmxState & 0x2) && (gmxState & 0x1));
+}
+
+/**
+ * Initialize and start the SPI interface.
+ *
+ * @param interface The identifier of the packet interface to configure and
+ *                  use as a SPI interface.
+ * @param mode      The operating mode for the SPI interface. The interface
+ *                  can operate as a full duplex (both Tx and Rx data paths
+ *                  active) or as a halfplex (either the Tx data path is
+ *                  active or the Rx data path is active, but not both).
+ * @param timeout   Timeout to wait for clock synchronization in seconds
+ * @param num_ports Number of SPI ports to configure
+ *
+ * @return Zero on success, negative of failure.
+ */
+extern int cvmx_spi_start_interface(int interface, cvmx_spi_mode_t mode, int timeout, int num_ports);
+
+/**
+ * This routine restarts the SPI interface after it has lost synchronization
+ * with its corespondant system.
+ *
+ * @param interface The identifier of the packet interface to configure and
+ *                  use as a SPI interface.
+ * @param mode      The operating mode for the SPI interface. The interface
+ *                  can operate as a full duplex (both Tx and Rx data paths
+ *                  active) or as a halfplex (either the Tx data path is
+ *                  active or the Rx data path is active, but not both).
+ * @param timeout   Timeout to wait for clock synchronization in seconds
+ * @return Zero on success, negative of failure.
+ */
+extern int cvmx_spi_restart_interface(int interface, cvmx_spi_mode_t mode, int timeout);
+
+/**
+ * Return non-zero if the SPI interface has a SPI4000 attached
+ *
+ * @param interface SPI interface the SPI4000 is connected to
+ *
+ * @return
+ */
+extern int cvmx_spi4000_is_present(int interface);
+
+/**
+ * Initialize the SPI4000 for use
+ *
+ * @param interface SPI interface the SPI4000 is connected to
+ */
+extern int cvmx_spi4000_initialize(int interface);
+
+/**
+ * Poll all the SPI4000 port and check its speed
+ *
+ * @param interface Interface the SPI4000 is on
+ * @param port      Port to poll (0-9)
+ * @return Status of the port. 0=down. All other values the port is up.
+ */
+extern cvmx_gmxx_rxx_rx_inbnd_t cvmx_spi4000_check_speed(int interface, int port);
+
+/**
+ * Get current SPI4 initialization callbacks
+ *
+ * @param callbacks  Pointer to the callbacks structure.to fill
+ *
+ * @return Pointer to cvmx_spi_callbacks_t structure.
+ */
+extern void cvmx_spi_get_callbacks(cvmx_spi_callbacks_t * callbacks);
+
+/**
+ * Set new SPI4 initialization callbacks
+ *
+ * @param new_callbacks  Pointer to an updated callbacks structure.
+ */
+extern void cvmx_spi_set_callbacks(cvmx_spi_callbacks_t * new_callbacks);
+
+/**
+ * Callback to perform SPI4 reset
+ *
+ * @param interface The identifier of the packet interface to configure and
+ *                  use as a SPI interface.
+ * @param mode      The operating mode for the SPI interface. The interface
+ *                  can operate as a full duplex (both Tx and Rx data paths
+ *                  active) or as a halfplex (either the Tx data path is
+ *                  active or the Rx data path is active, but not both).
+ * @return Zero on success, non-zero error code on failure (will cause SPI initialization to abort)
+ */
+extern int cvmx_spi_reset_cb(int interface, cvmx_spi_mode_t mode);
+
+/**
+ * Callback to setup calendar and miscellaneous settings before clock detection
+ *
+ * @param interface The identifier of the packet interface to configure and
+ *                  use as a SPI interface.
+ * @param mode      The operating mode for the SPI interface. The interface
+ *                  can operate as a full duplex (both Tx and Rx data paths
+ *                  active) or as a halfplex (either the Tx data path is
+ *                  active or the Rx data path is active, but not both).
+ * @param num_ports Number of ports to configure on SPI
+ *
+ * @return Zero on success, non-zero error code on failure (will cause SPI initialization to abort)
+ */
+extern int cvmx_spi_calendar_setup_cb(int interface, cvmx_spi_mode_t mode, int num_ports);
+
+/**
+ * Callback to perform clock detection
+ *
+ * @param interface The identifier of the packet interface to configure and
+ *                  use as a SPI interface.
+ * @param mode      The operating mode for the SPI interface. The interface
+ *                  can operate as a full duplex (both Tx and Rx data paths
+ *                  active) or as a halfplex (either the Tx data path is
+ *                  active or the Rx data path is active, but not both).
+ * @param timeout   Timeout to wait for clock synchronization in seconds
+ * @return Zero on success, non-zero error code on failure (will cause SPI initialization to abort)
+ */
+extern int cvmx_spi_clock_detect_cb(int interface, cvmx_spi_mode_t mode, int timeout);
+
+/**
+ * Callback to perform link training
+ *
+ * @param interface The identifier of the packet interface to configure and
+ *                  use as a SPI interface.
+ * @param mode      The operating mode for the SPI interface. The interface
+ *                  can operate as a full duplex (both Tx and Rx data paths
+ *                  active) or as a halfplex (either the Tx data path is
+ *                  active or the Rx data path is active, but not both).
+ * @param timeout   Timeout to wait for link to be trained (in seconds)
+ * @return Zero on success, non-zero error code on failure (will cause SPI initialization to abort)
+ */
+extern int cvmx_spi_training_cb(int interface, cvmx_spi_mode_t mode, int timeout);
+
+/**
+ * Callback to perform calendar data synchronization
+ *
+ * @param interface The identifier of the packet interface to configure and
+ *                  use as a SPI interface.
+ * @param mode      The operating mode for the SPI interface. The interface
+ *                  can operate as a full duplex (both Tx and Rx data paths
+ *                  active) or as a halfplex (either the Tx data path is
+ *                  active or the Rx data path is active, but not both).
+ * @param timeout   Timeout to wait for calendar data in seconds
+ * @return Zero on success, non-zero error code on failure (will cause SPI initialization to abort)
+ */
+extern int cvmx_spi_calendar_sync_cb(int interface, cvmx_spi_mode_t mode, int timeout);
+
+/**
+ * Callback to handle interface up
+ *
+ * @param interface The identifier of the packet interface to configure and
+ *                  use as a SPI interface.
+ * @param mode      The operating mode for the SPI interface. The interface
+ *                  can operate as a full duplex (both Tx and Rx data paths
+ *                  active) or as a halfplex (either the Tx data path is
+ *                  active or the Rx data path is active, but not both).
+ * @return Zero on success, non-zero error code on failure (will cause SPI initialization to abort)
+ */
+extern int cvmx_spi_interface_up_cb(int interface, cvmx_spi_mode_t mode);
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif  /* __CVMX_SPI_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-spi4000.c b/arch/mips/cavium-octeon/executive/cvmx-spi4000.c
new file mode 100644
index 0000000..cc48f67
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-spi4000.c
@@ -0,0 +1,603 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Support library for the SPI4000 card
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+#include "cvmx.h"
+#include "cvmx-mio.h"
+#include "cvmx-spi.h"
+
+/* If someone is using an old config, make the SPI4000 act like RGMII for backpressure */
+#ifndef CVMX_HELPER_DISABLE_SPI4000_BACKPRESSURE
+#ifndef CVMX_HELPER_DISABLE_RGMII_BACKPRESSURE
+#define CVMX_HELPER_DISABLE_RGMII_BACKPRESSURE 0
+#endif
+#define CVMX_HELPER_DISABLE_SPI4000_BACKPRESSURE CVMX_HELPER_DISABLE_RGMII_BACKPRESSURE
+#endif
+
+#define SPI4000_BASE(interface)             (0x8000660000000000ull + ((interface)*0x0000010000000000ull))
+#define SPI4000_READ_ADDRESS(interface)     (SPI4000_BASE(interface) | 0xf0)
+#define SPI4000_WRITE_ADDRESS(interface)    (SPI4000_BASE(interface) | 0xf2)
+#define SPI4000_WRITE_DATA(interface)       (SPI4000_BASE(interface) | 0xf8)
+#define SPI4000_DO_READ(inteface)           (SPI4000_BASE(interface) | 0xfc)
+#define SPI4000_DO_WRITE(interface)         (SPI4000_BASE(interface) | 0xfe)
+#define SPI4000_READ_DATA(interface)        (SPI4000_BASE(interface) | 0x030000f400000000ull)
+#define SPI4000_READ_DATA_CONT(interface)   (SPI4000_BASE(interface) | 0x0100000000000000ull)
+#define SPI4000_GET_READ_STATUS(interface)  (SPI4000_BASE(interface) | 0x030000fd00000000ull)
+#define SPI4000_GET_WRITE_STATUS(interface) (SPI4000_BASE(interface) | 0x030000ff00000000ull)
+#ifndef MS
+#define MS                          (400000ull) /*  Not exactly a millisecond, but close enough for our delays */
+#endif
+
+/* MDI Single Command (register 0x680) */
+typedef union
+{
+    uint32_t u32;
+    struct
+    {
+        uint32_t    reserved_21_31  : 11;
+        uint32_t    mdi_command     : 1; /**< Performs an MDIO access. When set, this bit
+                                            self clears upon completion of the access. */
+        uint32_t    reserved_18_19  : 2;
+        uint32_t    op_code         : 2; /**< MDIO Op Code
+                                            00 = Reserved
+                                            01 = Write Access
+                                            10 = Read Access
+                                            11 = Reserved */
+        uint32_t    reserved_13_15  : 3;
+        uint32_t    phy_address     : 5; /**< Address of external PHY device */
+        uint32_t    reserved_5_7    : 3;
+        uint32_t    reg_address     : 5; /**< Address of register within external PHY */
+    } s;
+} mdio_single_command_t;
+
+
+static CVMX_SHARED int interface_is_spi4000[2] = {0,0};
+
+/**
+ * @INTERNAL
+ * Read a value from the twsi interface
+ *
+ * @return Value after the read completes
+ */
+static inline uint64_t __cvmx_twsi_read(void)
+{
+    cvmx_mio_tws_sw_twsi_t result;
+    do
+    {
+        result.u64 = cvmx_read_csr(CVMX_MIO_TWS_SW_TWSI);
+    } while (result.s.v);
+
+    if (result.s.r == 0)
+    {
+#ifdef DEBUG
+        cvmx_dprintf("SPI4000: TWSI Read failed\n");
+#endif
+        result.s.d = 0;
+    }
+    return result.u64;
+}
+
+
+/**
+ * @INTERNAL
+ * Write a command to the twsi interface
+ *
+ * @param value  Value to write
+ */
+static inline int __cvmx_twsi_write(uint64_t value)
+{
+    cvmx_mio_tws_sw_twsi_t result;
+    cvmx_write_csr(CVMX_MIO_TWS_SW_TWSI, value);
+
+    result.u64 = __cvmx_twsi_read();
+    if (result.s.r == 0)
+        return -1;
+    else
+        return 0;
+}
+
+
+/**
+ * @INTERNAL
+ * Write data to the specified SPI4000 address
+ *
+ * @param interface Interface the SPI4000 is on. (0 or 1)
+ * @param address   Address to write to
+ * @param data      Data to write
+ */
+static void __cvmx_spi4000_write(int interface, int address, uint32_t data)
+{
+    uint8_t    addressh, addressl;
+    uint8_t    data0, data1, data2, data3;
+    uint64_t   start_cycle;
+
+    // Set the write address
+    addressh = (uint8_t) ((address >> 8) & 0xFF);
+    addressl = (uint8_t) (address & 0xFF);
+
+    __cvmx_twsi_write(SPI4000_WRITE_ADDRESS(interface));
+    __cvmx_twsi_write(SPI4000_BASE(interface) | addressh);
+    __cvmx_twsi_write(SPI4000_BASE(interface) | addressl);
+
+    // Set write data
+    data0 = (uint8_t) ((data >> 24) & 0xFF);
+    data1 = (uint8_t) ((data >> 16) & 0xFF);
+    data2 = (uint8_t) ((data >>  8) & 0xFF);
+    data3 = (uint8_t) ((data >>  0) & 0xFF);
+
+    __cvmx_twsi_write(SPI4000_WRITE_DATA(interface));
+    __cvmx_twsi_write(SPI4000_BASE(interface) | data0);
+    __cvmx_twsi_write(SPI4000_BASE(interface) | data1);
+    __cvmx_twsi_write(SPI4000_BASE(interface) | data2);
+    __cvmx_twsi_write(SPI4000_BASE(interface) | data3);
+
+    // Do the write
+    __cvmx_twsi_write(SPI4000_DO_WRITE(interface));
+
+    // check the status
+    start_cycle = cvmx_get_cycle();
+    do
+    {
+        if (cvmx_get_cycle() - start_cycle > 1000*MS)
+            break;
+        __cvmx_twsi_write(SPI4000_GET_WRITE_STATUS(interface));
+    } while ((__cvmx_twsi_read() & 0xFF) != 4);
+}
+
+
+/**
+ * @INTERNAL
+ * Read data from the SPI4000.
+ *
+ * @param interface Interface the SPI4000 is on. (0 or 1)
+ * @param address   Address to read from
+ *
+ * @return Value at the specified address
+ */
+static uint32_t __cvmx_spi4000_read(int interface, int address)
+{
+    uint8_t    addressh, addressl;
+    uint8_t    data0, data1, data2, data3;
+    uint64_t   result;
+
+    addressh = (uint8_t) ((address >> 8) & 0xFF);
+    addressl = (uint8_t) (address & 0xFF);
+
+    __cvmx_twsi_write(SPI4000_READ_ADDRESS(interface));
+    __cvmx_twsi_write(SPI4000_BASE(interface) | addressh);
+    __cvmx_twsi_write(SPI4000_BASE(interface) | addressl);
+
+    // Do the read
+    __cvmx_twsi_write(SPI4000_DO_READ(interface));
+    result = __cvmx_twsi_read();
+    while (result & 0xFF)
+    {
+        // check the status
+        __cvmx_twsi_write(SPI4000_GET_READ_STATUS(interface));
+        result = __cvmx_twsi_read();
+    }
+
+    __cvmx_twsi_write(SPI4000_READ_DATA(interface));
+    data0 = (uint8_t) (__cvmx_twsi_read() & 0xFF);
+
+    __cvmx_twsi_write(SPI4000_READ_DATA_CONT(interface));
+    data1 = (uint8_t) (__cvmx_twsi_read() & 0xFF);
+
+    __cvmx_twsi_write(SPI4000_READ_DATA_CONT(interface));
+    data2 = (uint8_t) (__cvmx_twsi_read() & 0xFF);
+
+    __cvmx_twsi_write(SPI4000_READ_DATA_CONT(interface));
+    data3 = (uint8_t) (__cvmx_twsi_read() & 0xFF);
+
+    return((data0<<24) | (data1<<16) | (data2<<8) | data3);
+}
+
+
+/**
+ * @INTERNAL
+ * Write to a PHY using MDIO on the SPI4000
+ *
+ * @param interface Interface the SPI4000 is on. (0 or 1)
+ * @param port      SPI4000 RGMII port to write to. (0-9)
+ * @param location  MDIO register to write
+ * @param val       Value to write
+ */
+static void __cvmx_spi4000_mdio_write(int interface, int port, int location, int val)
+{
+    static int last_value=-1;
+    mdio_single_command_t mdio;
+
+    mdio.u32 = 0;
+    mdio.s.mdi_command = 1;
+    mdio.s.op_code = 1;
+    mdio.s.phy_address = port;
+    mdio.s.reg_address = location;
+
+    /* Since the TWSI accesses are very slow, don't update the write value
+        if it is the same as the last value */
+    if (val != last_value)
+    {
+        last_value = val;
+        __cvmx_spi4000_write(interface, 0x0681, val);
+    }
+
+    __cvmx_spi4000_write(interface, 0x0680, mdio.u32);
+}
+
+
+/**
+ * @INTERNAL
+ * Read from a PHY using MDIO on the SPI4000
+ *
+ * @param interface Interface the SPI4000 is on. (0 or 1)
+ * @param port      SPI4000 RGMII port to read from. (0-9)
+ * @param location  MDIO register to read
+ * @return The MDI read result
+ */
+static int __cvmx_spi4000_mdio_read(int interface, int port, int location)
+{
+    mdio_single_command_t mdio;
+
+    mdio.u32 = 0;
+    mdio.s.mdi_command = 1;
+    mdio.s.op_code = 2;
+    mdio.s.phy_address = port;
+    mdio.s.reg_address = location;
+    __cvmx_spi4000_write(interface, 0x0680, mdio.u32);
+
+    do
+    {
+        mdio.u32 = __cvmx_spi4000_read(interface, 0x0680);
+    } while (mdio.s.mdi_command);
+
+    return __cvmx_spi4000_read(interface, 0x0681) >> 16;
+}
+
+
+/**
+ * @INTERNAL
+ * Configure the SPI4000 MACs
+ */
+static void __cvmx_spi4000_configure_mac(int interface)
+{
+    int port;
+    // IXF1010 configuration
+    // ---------------------
+    //
+    // Step 1: Apply soft reset to TxFIFO and MAC
+    //         MAC soft reset register. address=0x505
+    //         TxFIFO soft reset. address=0x620
+    __cvmx_spi4000_write(interface, 0x0505, 0x3ff);  // reset all the MACs
+    __cvmx_spi4000_write(interface, 0x0620, 0x3ff);  // reset the TX FIFOs
+
+    //         Global address and Configuration Register. address=0x500
+    //
+    // Step 2: Apply soft reset to RxFIFO and SPI.
+    __cvmx_spi4000_write(interface, 0x059e, 0x3ff);  // reset the RX FIFOs
+
+    // Step 3a: Take the MAC out of softreset
+    //          MAC soft reset register. address=0x505
+    __cvmx_spi4000_write(interface, 0x0505, 0x0);    // reset all the MACs
+
+    // Step 3b: De-assert port enables.
+    //          Global address and Configuration Register. address=0x500
+    __cvmx_spi4000_write(interface, 0x0500, 0x0);    // disable all ports
+
+    // Step 4: Assert Clock mode change En.
+    //         Clock and interface mode Change En. address=Serdes base + 0x14
+    //         Serdes (Serializer/de-serializer). address=0x780
+    //         [Can't find this one]
+
+    for (port=0; port < 10; port++)
+    {
+        int port_offset = port << 7;
+
+        // Step 5: Set MAC interface mode GMII speed.
+        //         MAC interface mode and RGMII speed register.
+        //             address=port_index+0x10
+        //
+        //         OUT port_index+0x10, 0x07     //RGMII 1000 Mbps operation.
+        __cvmx_spi4000_write(interface, port_offset | 0x0010, 0x3);
+
+        // Set the max packet size to 16383 bytes, including the CRC
+        __cvmx_spi4000_write(interface, port_offset | 0x000f, 0x3fff);
+
+        // Step 6: Change Interface to Copper mode
+        //         Interface mode register. address=0x501
+        //         [Can't find this]
+
+        // Step 7: MAC configuration
+        //         Station address configuration.
+        //         Source MAC address low register. Source MAC address 31-0.
+        //             address=port_index+0x00
+        //         Source MAC address high register. Source MAC address 47-32.
+        //             address=port_index+0x01
+        //         where Port index is 0x0 to 0x5.
+        //         This address is inserted in the source address filed when
+        //         transmitting pause frames, and is also used to compare against
+        //         unicast pause frames at the receiving side.
+        //
+        //         OUT port_index+0x00, source MAC address low.
+        __cvmx_spi4000_write(interface, port_offset | 0x0000, 0x0000);
+        //         OUT port_index+0x01, source MAC address high.
+        __cvmx_spi4000_write(interface, port_offset | 0x0001, 0x0000);
+
+        // Step 8: Set desired duplex mode
+        //         Desired duplex register. address=port_index+0x02
+        //         [Reserved]
+
+        // Step 9: Other configuration.
+        //         FC Enable Register.             address=port_index+0x12
+        //         Discard Unknown Control Frame.  address=port_index+0x15
+        //         Diverse config write register.  address=port_index+0x18
+        //         RX Packet Filter register.      address=port_index+0x19
+        //
+        // Step 9a: Tx FD FC Enabled / Rx FD FC Enabled
+        if (CVMX_HELPER_DISABLE_SPI4000_BACKPRESSURE)
+            __cvmx_spi4000_write(interface, port_offset | 0x0012, 0);
+        else
+            __cvmx_spi4000_write(interface, port_offset | 0x0012, 0x7);
+
+        // Step 9b: Discard unknown control frames
+        __cvmx_spi4000_write(interface, port_offset | 0x0015, 0x1);
+
+        // Step 9c: Enable auto-CRC and auto-padding
+        __cvmx_spi4000_write(interface, port_offset | 0x0018, 0x11cd); //??
+
+        // Step 9d: Drop bad CRC / Drop Pause / No DAF
+        __cvmx_spi4000_write(interface, port_offset | 0x0019, 0x00);
+    }
+
+    // Step 9d: Drop frames
+    __cvmx_spi4000_write(interface, 0x059f, 0x03ff);
+
+    for (port=0; port < 10; port++)
+    {
+        // Step 9e: Set the TX FIFO marks
+        __cvmx_spi4000_write(interface, port + 0x0600, 0x0900); // TXFIFO High watermark
+        __cvmx_spi4000_write(interface, port + 0x060a, 0x0800); // TXFIFO Low watermark
+        __cvmx_spi4000_write(interface, port + 0x0614, 0x0380); // TXFIFO threshold
+    }
+
+    // Step 12: De-assert RxFIFO and SPI Rx/Tx reset
+    __cvmx_spi4000_write(interface, 0x059e, 0x0);    // reset the RX FIFOs
+
+    // Step 13: De-assert TxFIFO and MAC reset
+    __cvmx_spi4000_write(interface, 0x0620, 0x0);    // reset the TX FIFOs
+
+    // Step 14: Assert port enable
+    //          Global address and Configuration Register. address=0x500
+    __cvmx_spi4000_write(interface, 0x0500, 0x03ff); // enable all ports
+
+    // Step 15: Disable loopback
+    //          [Can't find this one]
+}
+
+
+/**
+ * @INTERNAL
+ * Configure the SPI4000 PHYs
+ */
+static void __cvmx_spi4000_configure_phy(int interface)
+{
+    int port;
+
+    /* We use separate loops below since it allows us to save a write
+        to the SPI4000 for each repeated value. This adds up to a couple
+        of seconds */
+
+    /* Update the link state before resets. It takes a while for the links to
+        come back after the resets. Most likely they'll come back the same as
+        they are now */
+    for (port=0; port < 10; port++)
+        cvmx_spi4000_check_speed(interface, port);
+    /* Enable RGMII DELAYS for TX_CLK and RX_CLK (see spec) */
+    for (port=0; port < 10; port++)
+        __cvmx_spi4000_mdio_write(interface, port, 0x14, 0x00e2);
+    /* Advertise pause and 100 Full Duplex. Don't advertise half duplex or 10Mbpa */
+    for (port=0; port < 10; port++)
+        __cvmx_spi4000_mdio_write(interface, port, 0x4, 0x0d01);
+    /* Enable PHY reset */
+    for (port=0; port < 10; port++)
+        __cvmx_spi4000_mdio_write(interface, port, 0x0, 0x9140);
+}
+
+
+/**
+ * Poll all the SPI4000 port and check its speed
+ *
+ * @param interface Interface the SPI4000 is on
+ * @param port      Port to poll (0-9)
+ * @return Status of the port. 0=down. All other values the port is up.
+ */
+cvmx_gmxx_rxx_rx_inbnd_t cvmx_spi4000_check_speed(int interface, int port)
+{
+    static int phy_status[10] = {0,};
+    cvmx_gmxx_rxx_rx_inbnd_t link;
+    int read_status;
+
+    link.u64 = 0;
+
+    if (!interface_is_spi4000[interface])
+        return link;
+    if (port>=10)
+        return link;
+
+    /* Register 0x11: PHY Specific Status Register
+         Register   Function         Setting                     Mode   HW Rst SW Rst Notes
+                                                                 RO     00     Retain note
+         17.15:14   Speed            11 = Reserved
+                                                                                      17.a
+                                     10 = 1000 Mbps
+                                     01 = 100 Mbps
+                                     00 = 10 Mbps
+         17.13      Duplex           1 = Full-duplex             RO     0      Retain note
+                                     0 = Half-duplex                                  17.a
+         17.12      Page Received    1 = Page received           RO, LH 0      0
+                                     0 = Page not received
+                                     1 = Resolved                RO     0      0      note
+         17.11      Speed and
+                                     0 = Not resolved                                 17.a
+                    Duplex
+                    Resolved
+         17.10      Link (real time) 1 = Link up                 RO     0      0
+                                     0 = Link down
+                                                                 RO     000    000    note
+                                     000 = < 50m
+         17.9:7     Cable Length
+                                     001 = 50 - 80m                                   17.b
+                    (100/1000
+                                     010 = 80 - 110m
+                    modes only)
+                                     011 = 110 - 140m
+                                     100 = >140m
+         17.6       MDI Crossover    1 = MDIX                    RO     0      0      note
+                    Status           0 = MDI                                          17.a
+         17.5       Downshift Sta-   1 = Downshift               RO     0      0
+                    tus              0 = No Downshift
+         17.4       Energy Detect    1 = Sleep                   RO     0      0
+                    Status           0 = Active
+         17.3       Transmit Pause   1 = Transmit pause enabled  RO     0      0      note17.
+                    Enabled          0 = Transmit pause disabled                      a, 17.c
+         17.2       Receive Pause    1 = Receive pause enabled   RO     0      0      note17.
+                    Enabled          0 = Receive pause disabled                       a, 17.c
+         17.1       Polarity (real   1 = Reversed                RO     0      0
+                    time)            0 = Normal
+         17.0       Jabber (real     1 = Jabber                  RO     0      Retain
+                    time)            0 = No jabber
+    */
+    read_status = __cvmx_spi4000_mdio_read(interface, port, 0x11);
+    if ((read_status & (1<<10)) == 0)
+        read_status = 0; /* If the link is down, force zero */
+    else
+        read_status &= 0xe400; /* Strip off all the don't care bits */
+    if (read_status != phy_status[port])
+    {
+        phy_status[port] = read_status;
+        if (read_status & (1<<10))
+        {
+            /* If the link is up, we need to set the speed based on the PHY status */
+            if (read_status & (1<<15))
+                __cvmx_spi4000_write(interface, (port<<7) | 0x0010, 0x3); /* 1Gbps */
+            else
+                __cvmx_spi4000_write(interface, (port<<7) | 0x0010, 0x1); /* 100Mbps */
+        }
+        else
+        {
+            /* If the link is down, force 1Gbps so TX traffic dumps fast */
+            __cvmx_spi4000_write(interface, (port<<7) | 0x0010, 0x3); /* 1Gbps */
+        }
+    }
+
+    if (read_status & (1<<10))
+    {
+        link.s.status = 1; /* Link up */
+        if (read_status & (1<<15))
+            link.s.speed = 2;
+        else
+            link.s.speed = 1;
+    }
+    else
+    {
+        link.s.speed = 2; /* Use 1Gbps when down */
+        link.s.status = 0; /* Link Down */
+    }
+    link.s.duplex = ((read_status & (1<<13)) != 0);
+
+    return link;
+}
+
+
+/**
+ * Return non-zero if the SPI interface has a SPI4000 attached
+ *
+ * @param interface SPI interface the SPI4000 is connected to
+ *
+ * @return
+ */
+int cvmx_spi4000_is_present(int interface)
+{
+    if (!(OCTEON_IS_MODEL(OCTEON_CN38XX) || OCTEON_IS_MODEL(OCTEON_CN58XX)))
+        return 0;
+    // Check for the presence of a SPI4000. If it isn't there,
+    // these writes will timeout.
+    if (__cvmx_twsi_write(SPI4000_WRITE_ADDRESS(interface)))
+        return 0;
+    if (__cvmx_twsi_write(SPI4000_BASE(interface)))
+        return 0;
+    if (__cvmx_twsi_write(SPI4000_BASE(interface)))
+        return 0;
+    interface_is_spi4000[interface] = 1;
+    return 1;
+}
+
+
+/**
+ * Initialize the SPI4000 for use
+ *
+ * @param interface SPI interface the SPI4000 is connected to
+ */
+int cvmx_spi4000_initialize(int interface)
+{
+    if (!cvmx_spi4000_is_present(interface))
+        return -1;
+
+    __cvmx_spi4000_configure_mac(interface);
+    __cvmx_spi4000_configure_phy(interface);
+    return 0;
+}
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-spinlock.h b/arch/mips/cavium-octeon/executive/cvmx-spinlock.h
new file mode 100644
index 0000000..c3f7abb
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-spinlock.h
@@ -0,0 +1,438 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Implementation of spinlocks.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+
+
+#ifndef __CVMX_SPINLOCK_H__
+#define __CVMX_SPINLOCK_H__
+
+#include "cvmx-asm.h"
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+/* Spinlocks for Octeon */
+
+
+// define these to enable recursive spinlock debugging
+//#define CVMX_SPINLOCK_DEBUG
+
+
+/**
+ * Spinlocks for Octeon
+ */
+typedef struct {
+    volatile uint32_t value;
+} cvmx_spinlock_t;
+
+// note - macros not expanded in inline ASM, so values hardcoded
+#define  CVMX_SPINLOCK_UNLOCKED_VAL  0
+#define  CVMX_SPINLOCK_LOCKED_VAL    1
+
+
+#define CVMX_SPINLOCK_UNLOCKED_INITIALIZER  {CVMX_SPINLOCK_UNLOCKED_VAL}
+
+
+/**
+ * Initialize a spinlock
+ *
+ * @param lock   Lock to initialize
+ */
+static inline void cvmx_spinlock_init(cvmx_spinlock_t *lock)
+{
+    lock->value = CVMX_SPINLOCK_UNLOCKED_VAL;
+}
+
+
+/**
+ * Return non-zero if the spinlock is currently locked
+ *
+ * @param lock   Lock to check
+ * @return Non-zero if locked
+ */
+static inline int cvmx_spinlock_locked(cvmx_spinlock_t *lock)
+{
+    return (lock->value != CVMX_SPINLOCK_UNLOCKED_VAL);
+}
+
+
+/**
+ * Releases lock
+ *
+ * @param lock   pointer to lock structure
+ */
+static inline void cvmx_spinlock_unlock(cvmx_spinlock_t *lock)
+{
+    CVMX_SYNCWS;
+    lock->value = 0;
+    CVMX_SYNCWS;
+}
+
+
+/**
+ * Attempts to take the lock, but does not spin if lock is not available.
+ * May take some time to acquire the lock even if it is available
+ * due to the ll/sc not succeeding.
+ *
+ * @param lock   pointer to lock structure
+ *
+ * @return 0: lock successfully taken
+ *         1: lock not taken, held by someone else
+ * These return values match the Linux semantics.
+ */
+
+static inline unsigned int cvmx_spinlock_trylock(cvmx_spinlock_t *lock)
+{
+    unsigned int tmp;
+
+    __asm__ __volatile__(
+    ".set noreorder         \n"
+    "1: ll   %[tmp], %[val] \n"
+    "   bnez %[tmp], 2f     \n"  // if lock held, fail immediately
+    "   li   %[tmp], 1      \n"
+    "   sc   %[tmp], %[val] \n"
+    "   beqz %[tmp], 1b     \n"
+    "   li   %[tmp], 0      \n"
+    "2:                     \n"
+    ".set reorder           \n"
+    : [val] "+m" (lock->value), [tmp] "=&r" (tmp)
+    :
+    : "memory");
+
+    return (!!tmp);  /* normalize to 0 or 1 */
+}
+
+/**
+ * Gets lock, spins until lock is taken
+ *
+ * @param lock   pointer to lock structure
+ */
+static inline void cvmx_spinlock_lock(cvmx_spinlock_t *lock)
+{
+    unsigned int tmp;
+
+    __asm__ __volatile__(
+    ".set noreorder         \n"
+    "1: ll   %[tmp], %[val]  \n"
+    "   bnez %[tmp], 1b     \n"
+    "   li   %[tmp], 1      \n"
+    "   sc   %[tmp], %[val] \n"
+    "   beqz %[tmp], 1b     \n"
+    "   nop                \n"
+    ".set reorder           \n"
+    : [val] "+m" (lock->value), [tmp] "=&r" (tmp)
+    :
+    : "memory");
+
+}
+
+
+
+/** ********************************************************************
+ * Bit spinlocks
+ * These spinlocks use a single bit (bit 31) of a 32 bit word for locking.
+ * The rest of the bits in the word are left undisturbed.  This enables more
+ * compact data structures as only 1 bit is consumed for the lock.
+ *
+ */
+
+/**
+ * Gets lock, spins until lock is taken
+ * Preserves the low 31 bits of the 32 bit
+ * word used for the lock.
+ *
+ *
+ * @param word  word to lock bit 31 of
+ */
+static inline void cvmx_spinlock_bit_lock(uint32_t *word)
+{
+    unsigned int tmp;
+    unsigned int sav;
+
+    __asm__ __volatile__(
+    ".set noreorder         \n"
+    ".set noat              \n"
+    "1: ll    %[tmp], %[val]  \n"
+    "   bbit1 %[tmp], 31, 1b    \n"
+    "   li    $at, 1      \n"
+    "   ins   %[tmp], $at, 31, 1  \n"
+    "   sc    %[tmp], %[val] \n"
+    "   beqz  %[tmp], 1b     \n"
+    "   nop                \n"
+    ".set at              \n"
+    ".set reorder           \n"
+    : [val] "+m" (*word), [tmp] "=&r" (tmp), [sav] "=&r" (sav)
+    :
+    : "memory");
+
+}
+
+/**
+ * Attempts to get lock, returns immediately with success/failure
+ * Preserves the low 31 bits of the 32 bit
+ * word used for the lock.
+ *
+ *
+ * @param word  word to lock bit 31 of
+ * @return 0: lock successfully taken
+ *         1: lock not taken, held by someone else
+ * These return values match the Linux semantics.
+ */
+static inline unsigned int cvmx_spinlock_bit_trylock(uint32_t *word)
+{
+    unsigned int tmp;
+
+    __asm__ __volatile__(
+    ".set noreorder         \n"
+    ".set noat              \n"
+    "1: ll    %[tmp], %[val] \n"
+    "   bbit1 %[tmp], 31, 2f     \n"  // if lock held, fail immediately
+    "   li    $at, 1      \n"
+    "   ins   %[tmp], $at, 31, 1  \n"
+    "   sc    %[tmp], %[val] \n"
+    "   beqz  %[tmp], 1b     \n"
+    "   li    %[tmp], 0      \n"
+    "2:                     \n"
+    ".set at              \n"
+    ".set reorder           \n"
+    : [val] "+m" (*word), [tmp] "=&r" (tmp)
+    :
+    : "memory");
+
+    return (!!tmp);  /* normalize to 0 or 1 */
+}
+/**
+ * Releases bit lock
+ *
+ * Unconditionally clears bit 31 of the lock word.  Note that this is
+ * done non-atomically, as this implementation assumes that the rest
+ * of the bits in the word are protected by the lock.
+ *
+ * @param word  word to unlock bit 31 in
+ */
+static inline void cvmx_spinlock_bit_unlock(uint32_t *word)
+{
+    CVMX_SYNCWS;
+    *word &= ~(1UL << 31) ;
+    CVMX_SYNCWS;
+}
+
+
+
+/** ********************************************************************
+ * Recursive spinlocks
+ */
+typedef struct {
+	volatile unsigned int value;
+	volatile unsigned int core_num;
+} cvmx_spinlock_rec_t;
+
+
+/**
+ * Initialize a recursive spinlock
+ *
+ * @param lock   Lock to initialize
+ */
+static inline void cvmx_spinlock_rec_init(cvmx_spinlock_rec_t *lock)
+{
+    lock->value = CVMX_SPINLOCK_UNLOCKED_VAL;
+}
+
+
+/**
+ * Return non-zero if the recursive spinlock is currently locked
+ *
+ * @param lock   Lock to check
+ * @return Non-zero if locked
+ */
+static inline int cvmx_spinlock_rec_locked(cvmx_spinlock_rec_t *lock)
+{
+    return (lock->value != CVMX_SPINLOCK_UNLOCKED_VAL);
+}
+
+
+/**
+* Unlocks one level of recursive spinlock.  Lock is not unlocked
+* unless this is the final unlock call for that spinlock
+*
+* @param lock   ptr to recursive spinlock structure
+*/
+static inline void cvmx_spinlock_rec_unlock(cvmx_spinlock_rec_t *lock);
+
+#ifdef CVMX_SPINLOCK_DEBUG
+#define cvmx_spinlock_rec_unlock(x)  _int_cvmx_spinlock_rec_unlock((x), __FILE__, __LINE__)
+static inline void _int_cvmx_spinlock_rec_unlock(cvmx_spinlock_rec_t *lock, char *filename, int linenum)
+#else
+static inline void cvmx_spinlock_rec_unlock(cvmx_spinlock_rec_t *lock)
+#endif
+{
+
+	unsigned int temp, result;
+    int core_num;
+    core_num = cvmx_get_core_num();
+
+#ifdef CVMX_SPINLOCK_DEBUG
+    {
+        if (lock->core_num != core_num)
+        {
+            cvmx_dprintf("ERROR: Recursive spinlock release attemped by non-owner! file: %s, line: %d\n", filename, linenum);
+            return;
+        }
+    }
+#endif
+
+	__asm__ __volatile__(
+		".set  noreorder                 \n"
+		"     addi  %[tmp], %[pid], 0x80 \n"
+		"     sw    %[tmp], %[lid]       # set lid to invalid value\n"
+                CVMX_SYNCWS_STR
+		"1:   ll    %[tmp], %[val]       \n"
+		"     addu  %[res], %[tmp], -1   # decrement lock count\n"
+		"     sc    %[res], %[val]       \n"
+		"     beqz  %[res], 1b           \n"
+		"     nop                        \n"
+		"     beq   %[tmp], %[res], 2f   # res is 1 on successful sc       \n"
+		"     nop                        \n"
+		"     sw   %[pid], %[lid]        # set lid to pid, only if lock still held\n"
+		"2:                         \n"
+                CVMX_SYNCWS_STR
+		".set  reorder                   \n"
+		: [res] "=&r" (result), [tmp] "=&r" (temp), [val] "+m" (lock->value), [lid] "+m" (lock->core_num)
+		: [pid] "r" (core_num)
+		: "memory");
+
+
+#ifdef CVMX_SPINLOCK_DEBUG
+    {
+        if (lock->value == ~0UL)
+        {
+            cvmx_dprintf("ERROR: Recursive spinlock released too many times! file: %s, line: %d\n", filename, linenum);
+        }
+    }
+#endif
+
+
+}
+
+/**
+ * Takes recursive spinlock for a given core.  A core can take the lock multiple
+ * times, and the lock is released only when the corresponding number of
+ * unlocks have taken place.
+ *
+ * NOTE: This assumes only one thread per core, and that the core ID is used as
+ * the lock 'key'.  (This implementation cannot be generalized to allow
+ * multiple threads to use the same key (core id) .)
+ *
+ * @param lock   address of recursive spinlock structure.  Note that this is
+ *               distinct from the standard spinlock
+ */
+static inline void cvmx_spinlock_rec_lock(cvmx_spinlock_rec_t *lock);
+
+#ifdef CVMX_SPINLOCK_DEBUG
+#define cvmx_spinlock_rec_lock(x)  _int_cvmx_spinlock_rec_lock((x), __FILE__, __LINE__)
+static inline void _int_cvmx_spinlock_rec_lock(cvmx_spinlock_rec_t *lock, char *filename, int linenum)
+#else
+static inline void cvmx_spinlock_rec_lock(cvmx_spinlock_rec_t *lock)
+#endif
+{
+
+
+	volatile unsigned int tmp;
+	volatile int core_num;
+
+	core_num = cvmx_get_core_num();
+
+
+	__asm__ __volatile__(
+		".set  noreorder              \n"
+		"1: ll   %[tmp], %[val]       # load the count\n"
+		"   bnez %[tmp], 2f           # if count!=zero branch to 2\n"
+		"   addu %[tmp], %[tmp], 1    \n"
+		"   sc   %[tmp], %[val]       \n"
+		"   beqz %[tmp], 1b           # go back if not success\n"
+		"   nop                       \n"
+		"   j    3f                   # go to write core_num \n"
+		"2: lw   %[tmp], %[lid]       # load the core_num \n"
+		"   bne  %[tmp], %[pid], 1b   # core_num no match, restart\n"
+		"   nop                       \n"
+		"   lw   %[tmp], %[val]       \n"
+		"   addu %[tmp], %[tmp], 1    \n"
+		"   sw   %[tmp], %[val]       # update the count\n"
+		"3: sw   %[pid], %[lid]       # store the core_num\n"
+                CVMX_SYNCWS_STR
+		".set  reorder                \n"
+		: [tmp] "=&r" (tmp), [val] "+m" (lock->value), [lid] "+m" (lock->core_num)
+		: [pid] "r" (core_num)
+		: "memory");
+
+#ifdef CVMX_SPINLOCK_DEBUG
+    if (lock->core_num != core_num)
+    {
+        cvmx_dprintf("cvmx_spinlock_rec_lock: lock taken, but core_num is incorrect. file: %s, line: %d\n", filename, linenum);
+    }
+#endif
+
+
+}
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif /* __CVMX_SPINLOCK_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-swap.h b/arch/mips/cavium-octeon/executive/cvmx-swap.h
new file mode 100644
index 0000000..e693eb3
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-swap.h
@@ -0,0 +1,149 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Utility functions for endian swapping
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+
+#ifndef __CVMX_SWAP_H__
+#define __CVMX_SWAP_H__
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+
+/**
+ * Byte swap a 16 bit number
+ *
+ * @param x      16 bit number
+ * @return Byte swapped result
+ */
+static inline uint16_t cvmx_swap16(uint16_t x)
+{
+    return ((uint16_t)((((uint16_t)(x) & (uint16_t)0x00ffU) << 8) |
+                       (((uint16_t)(x) & (uint16_t)0xff00U) >> 8) ));
+}
+
+
+/**
+ * Byte swap a 32 bit number
+ *
+ * @param x      32 bit number
+ * @return Byte swapped result
+ */
+static inline uint32_t cvmx_swap32(uint32_t x)
+{
+    return ((uint32_t)((((uint32_t)(x) & (uint32_t)0x000000ffUL) << 24) |
+                       (((uint32_t)(x) & (uint32_t)0x0000ff00UL) <<  8) |
+                       (((uint32_t)(x) & (uint32_t)0x00ff0000UL) >>  8) |
+                       (((uint32_t)(x) & (uint32_t)0xff000000UL) >> 24) ));
+}
+
+
+/**
+ * Byte swap a 64 bit number
+ *
+ * @param x      64 bit number
+ * @return Byte swapped result
+ */
+static inline uint64_t cvmx_swap64(uint64_t x)
+{
+    return ((x >> 56) |
+            (((x >> 48) & 0xfful) << 8) |
+            (((x >> 40) & 0xfful) << 16) |
+            (((x >> 32) & 0xfful) << 24) |
+            (((x >> 24) & 0xfful) << 32) |
+            (((x >> 16) & 0xfful) << 40) |
+            (((x >>  8) & 0xfful) << 48) |
+            (((x >>  0) & 0xfful) << 56));
+}
+
+
+#if __BYTE_ORDER == __BIG_ENDIAN
+
+#define cvmx_cpu_to_le16(x) cvmx_swap16(x)
+#define cvmx_cpu_to_le32(x) cvmx_swap32(x)
+#define cvmx_cpu_to_le64(x) cvmx_swap64(x)
+
+#define cvmx_cpu_to_be16(x) (x)
+#define cvmx_cpu_to_be32(x) (x)
+#define cvmx_cpu_to_be64(x) (x)
+
+#else
+
+#define cvmx_cpu_to_le16(x) (x)
+#define cvmx_cpu_to_le32(x) (x)
+#define cvmx_cpu_to_le64(x) (x)
+
+#define cvmx_cpu_to_be16(x) cvmx_swap16(x)
+#define cvmx_cpu_to_be32(x) cvmx_swap32(x)
+#define cvmx_cpu_to_be64(x) cvmx_swap64(x)
+
+#endif
+
+#define cvmx_le16_to_cpu(x) cvmx_cpu_to_le16(x)
+#define cvmx_le32_to_cpu(x) cvmx_cpu_to_le32(x)
+#define cvmx_le64_to_cpu(x) cvmx_cpu_to_le64(x)
+
+#define cvmx_be16_to_cpu(x) cvmx_cpu_to_be16(x)
+#define cvmx_be32_to_cpu(x) cvmx_cpu_to_be32(x)
+#define cvmx_be64_to_cpu(x) cvmx_cpu_to_be64(x)
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif  /* __CVMX_SWAP_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-sysinfo.c b/arch/mips/cavium-octeon/executive/cvmx-sysinfo.c
new file mode 100644
index 0000000..9a5d16e
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-sysinfo.c
@@ -0,0 +1,215 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * This module provides system/board/application information obtained by the bootloader.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ *
+ */
+
+#include "cvmx.h"
+#include "cvmx-spinlock.h"
+#include "cvmx-sysinfo.h"
+
+/**
+ * This structure defines the private state maintained by sysinfo module.
+ *
+ */
+CVMX_SHARED static struct {
+
+    cvmx_sysinfo_t   sysinfo;      /**< system information */
+    cvmx_spinlock_t  lock;         /**< mutex spinlock */
+
+} state = {
+    .lock = CVMX_SPINLOCK_UNLOCKED_INITIALIZER
+};
+
+#ifdef CVMX_BUILD_FOR_LINUX_USER
+/* Global variable with the processor ID since we can't read it directly */
+CVMX_SHARED uint32_t cvmx_app_init_processor_id;
+#endif
+
+/* Global variables that define the min/max of the memory region set up for 32 bit userspace access */
+uint64_t linux_mem32_min = 0;
+uint64_t linux_mem32_max = 0;
+uint64_t linux_mem32_wired = 0;
+uint64_t linux_mem32_offset = 0;
+
+/**
+ * This function returns the application information as obtained
+ * by the bootloader.  This provides the core mask of the cores
+ * running the same application image, as well as the physical
+ * memory regions available to the core.
+ *
+ * @return  Pointer to the boot information structure
+ *
+ */
+cvmx_sysinfo_t * cvmx_sysinfo_get(void)
+{
+    return &(state.sysinfo);
+}
+
+
+/**
+ * This function is used in non-simple executive environments (such as Linux kernel, u-boot, etc.)
+ * to configure the minimal fields that are required to use
+ * simple executive files directly.
+ *
+ * Locking (if required) must be handled outside of this
+ * function
+ *
+ * @param phy_mem_desc_ptr
+ *                   Pointer to global physical memory descriptor (bootmem descriptor)
+ * @param board_type Octeon board type enumeration
+ *
+ * @param board_rev_major
+ *                   Board major revision
+ * @param board_rev_minor
+ *                   Board minor revision
+ * @param cpu_clock_hz
+ *                   CPU clock freqency in hertz
+ *
+ * @return 0: Failure
+ *         1: success
+ */
+int cvmx_sysinfo_minimal_initialize(void *phy_mem_desc_ptr, uint16_t board_type, uint8_t board_rev_major,
+                                    uint8_t board_rev_minor, uint32_t cpu_clock_hz)
+{
+
+    /* The sysinfo structure was already initialized */
+    if (state.sysinfo.board_type)
+        return(0);
+
+    memset(&(state.sysinfo), 0x0, sizeof(state.sysinfo));
+    state.sysinfo.phy_mem_desc_ptr = phy_mem_desc_ptr;
+    state.sysinfo.board_type = board_type;
+    state.sysinfo.board_rev_major = board_rev_major;
+    state.sysinfo.board_rev_minor = board_rev_minor;
+    state.sysinfo.cpu_clock_hz = cpu_clock_hz;
+
+    return(1);
+}
+
+#ifdef CVMX_BUILD_FOR_LINUX_USER
+/**
+ * Initialize the sysinfo structure when running on
+ * Octeon under Linux userspace
+ */
+void cvmx_sysinfo_linux_userspace_initialize(void)
+{
+    cvmx_sysinfo_t *system_info = cvmx_sysinfo_get();
+    memset(system_info, 0, sizeof(cvmx_sysinfo_t));
+
+    system_info->core_mask = 0;
+    system_info->init_core = -1;
+
+    FILE *infile = fopen("/proc/octeon_info", "r");
+    if (infile == NULL)
+    {
+        perror("Error opening /proc/octeon_info");
+        exit(-1);
+    }
+
+    while (!feof(infile))
+    {
+        char buffer[80];
+        if (fgets(buffer, sizeof(buffer), infile))
+        {
+            const char *field = strtok(buffer, " ");
+            const char *valueS = strtok(NULL, " ");
+            if (field == NULL)
+                continue;
+            if (valueS == NULL)
+                continue;
+            unsigned long long value;
+            sscanf(valueS, "%lli", &value);
+
+            if (strcmp(field, "dram_size:") == 0)
+                system_info->system_dram_size = value;
+            else if (strcmp(field, "phy_mem_desc_addr:") == 0)
+                system_info->phy_mem_desc_ptr = cvmx_phys_to_ptr(value);
+            else if (strcmp(field, "eclock_hz:") == 0)
+                system_info->cpu_clock_hz = value;
+            else if (strcmp(field, "dclock_hz:") == 0)
+                system_info->dram_data_rate_hz = value * 2;
+            else if (strcmp(field, "board_type:") == 0)
+                system_info->board_type = value;
+            else if (strcmp(field, "board_rev_major:") == 0)
+                system_info->board_rev_major = value;
+            else if (strcmp(field, "board_rev_minor:") == 0)
+                system_info->board_rev_minor = value;
+            else if (strcmp(field, "board_serial_number:") == 0)
+                strncpy(system_info->board_serial_number, valueS, sizeof(system_info->board_serial_number));
+            else if (strcmp(field, "mac_addr_base:") == 0)
+            {
+                int i;
+                int m[6];
+                sscanf(valueS, "%02x:%02x:%02x:%02x:%02x:%02x", m+0, m+1, m+2, m+3, m+4, m+5);
+                for (i=0; i<6; i++)
+                    system_info->mac_addr_base[i] = m[i];
+            }
+            else if (strcmp(field, "mac_addr_count:") == 0)
+                system_info->mac_addr_count = value;
+            else if (strcmp(field, "32bit_shared_mem_base:") == 0)
+                linux_mem32_min = value;
+            else if (strcmp(field, "32bit_shared_mem_size:") == 0)
+                linux_mem32_max = linux_mem32_min + value - 1;
+            else if (strcmp(field, "processor_id:") == 0)
+                cvmx_app_init_processor_id = value;
+            else if (strcmp(field, "32bit_shared_mem_wired:") == 0)
+                linux_mem32_wired = value;
+        }
+    }
+}
+#endif
diff --git a/arch/mips/cavium-octeon/executive/cvmx-sysinfo.h b/arch/mips/cavium-octeon/executive/cvmx-sysinfo.h
new file mode 100644
index 0000000..8ff6840
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-sysinfo.h
@@ -0,0 +1,174 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * This module provides system/board information obtained by the bootloader.
+ *
+ * <hr>$Revision: 33999 $<hr>
+ *
+ */
+
+
+#ifndef __CVMX_SYSINFO_H__
+#define __CVMX_SYSINFO_H__
+
+#include "cvmx-app-init.h"
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#define OCTEON_SERIAL_LEN 20
+/**
+ * Structure describing application specific information.
+ * __cvmx_app_init() populates this from the cvmx boot descriptor.
+ * This structure is private to simple executive applications, so
+ * no versioning is required.
+ *
+ * This structure must be provided with some fields set in order to use
+ * simple executive functions in other applications (Linux kernel, u-boot, etc.)
+ * The cvmx_sysinfo_minimal_initialize() function is provided to set the required values
+ * in these cases.
+ *
+ *
+ */
+typedef struct {
+    /* System wide variables */
+    uint64_t system_dram_size;  /**< installed DRAM in system, in bytes */
+    void *phy_mem_desc_ptr;  /**< ptr to memory descriptor block */
+
+    /* Application image specific variables */
+    uint64_t stack_top;  /**< stack top address (virtual) */
+    uint64_t heap_base;  /**< heap base address (virtual) */
+    uint32_t stack_size; /**< stack size in bytes */
+    uint32_t heap_size;  /**< heap size in bytes */
+    uint32_t core_mask;  /**< coremask defining cores running application */
+    uint32_t init_core;  /**< Deprecated, use cvmx_coremask_first_core() to select init core */
+    uint64_t exception_base_addr;  /**< exception base address, as set by bootloader */
+    uint32_t cpu_clock_hz;     /**< cpu clock speed in hz */
+    uint32_t dram_data_rate_hz;  /**< dram data rate in hz (data rate = 2 * clock rate */
+
+    uint16_t board_type;
+    uint8_t  board_rev_major;
+    uint8_t  board_rev_minor;
+    uint8_t  mac_addr_base[6];
+    uint8_t  mac_addr_count;
+    char     board_serial_number[OCTEON_SERIAL_LEN];
+    /* Several boards support compact flash on the Octeon boot bus.  The CF
+    ** memory spaces may be mapped to different addresses on different boards.
+    ** These values will be 0 if CF is not present.
+    ** Note that these addresses are physical addresses, and it is up to the application
+    ** to use the proper addressing mode (XKPHYS, KSEG0, etc.)*/
+    uint64_t compact_flash_common_base_addr;
+    uint64_t compact_flash_attribute_base_addr;
+    /* Base address of the LED display (as on EBT3000 board)
+    ** This will be 0 if LED display not present.
+    ** Note that this address is a physical address, and it is up to the application
+    ** to use the proper addressing mode (XKPHYS, KSEG0, etc.)*/
+    uint64_t led_display_base_addr;
+    uint32_t dfa_ref_clock_hz;  /**< DFA reference clock in hz (if applicable)*/
+    uint32_t bootloader_config_flags;  /**< configuration flags from bootloader */
+    uint8_t  console_uart_num;         /** < Uart number used for console */
+} cvmx_sysinfo_t;
+
+
+/**
+ * This function returns the system/board information as obtained
+ * by the bootloader.
+ *
+ *
+ * @return  Pointer to the boot information structure
+ *
+ */
+
+extern cvmx_sysinfo_t * cvmx_sysinfo_get(void);
+
+
+/**
+ * This function is used in non-simple executive environments (such as Linux kernel, u-boot, etc.)
+ * to configure the minimal fields that are required to use
+ * simple executive files directly.
+ *
+ * Locking (if required) must be handled outside of this
+ * function
+ *
+ * @param phy_mem_desc_ptr
+ *                   Pointer to global physical memory descriptor (bootmem descriptor)
+ * @param board_type Octeon board type enumeration
+ *
+ * @param board_rev_major
+ *                   Board major revision
+ * @param board_rev_minor
+ *                   Board minor revision
+ * @param cpu_clock_hz
+ *                   CPU clock freqency in hertz
+ *
+ * @return 0: Failure
+ *         1: success
+ */
+extern int cvmx_sysinfo_minimal_initialize(void *phy_mem_desc_ptr, uint16_t board_type, uint8_t board_rev_major,
+                                    uint8_t board_rev_minor, uint32_t cpu_clock_hz);
+
+#ifdef CVMX_BUILD_FOR_LINUX_USER
+/**
+ * Initialize the sysinfo structure when running on
+ * Octeon under Linux userspace
+ */
+extern void cvmx_sysinfo_linux_userspace_initialize(void);
+#endif
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif /* __CVMX_SYSINFO_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-thunder.c b/arch/mips/cavium-octeon/executive/cvmx-thunder.c
new file mode 100644
index 0000000..ea3b44e
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-thunder.c
@@ -0,0 +1,336 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Interface to the Thunder specific devices
+ *
+ * <hr>$Revision: 32636 $<hr>
+ *
+ */
+
+#include "cvmx-config.h"
+#include "cvmx.h"
+#include "cvmx-sysinfo.h"
+#include "cvmx-thunder.h"
+#include "cvmx-gpio.h"
+#include "cvmx-twsi.h"
+
+
+static const int BYPASS_STATUS = 1<<5; /* GPIO 5 */
+static const int BYPASS_EN     = 1<<6; /* GPIO 6 */
+static const int WDT_BP_CLR    = 1<<7; /* GPIO 7 */
+
+static const int RTC_CTL_ADDR = 0x7;
+static const int RTC_CTL_BIT_EOSC   = 0x80;
+static const int RTC_CTL_BIT_WACE   = 0x40;
+static const int RTC_CTL_BIT_WD_ALM = 0x20;
+static const int RTC_CTL_BIT_WDSTR  = 0x8;
+static const int RTC_CTL_BIT_AIE    = 0x1;
+static const int RTC_WD_ALM_CNT_BYTE0_ADDR = 0x4;
+
+#define CVMX_LAN_BYPASS_MSG(...)  do {} while(0)
+
+/*
+ * Board-specifc RTC read
+ * Time is expressed in seconds from epoch (Jan 1 1970 at 00:00:00 UTC)
+ */
+uint32_t cvmx_rtc_ds1374_read(void)
+{
+    int      retry;
+    uint8_t  sec;
+    uint32_t time;
+
+    for(retry=0; retry<2; retry++)
+    {
+        time = cvmx_twsi_read8(CVMX_RTC_DS1374_ADDR, 0x0);
+        time |= (cvmx_twsi_read8_cur_addr(CVMX_RTC_DS1374_ADDR) & 0xff) << 8;
+        time |= (cvmx_twsi_read8_cur_addr(CVMX_RTC_DS1374_ADDR) & 0xff) << 16;
+        time |= (cvmx_twsi_read8_cur_addr(CVMX_RTC_DS1374_ADDR) & 0xff) << 24;
+
+        sec = cvmx_twsi_read8(CVMX_RTC_DS1374_ADDR, 0x0);
+        if (sec == (time & 0xff))
+            break; /* Time did not roll-over, value is correct */
+    }
+
+    return time;
+}
+
+/*
+ * Board-specific RTC write
+ * Time is expressed in seconds from epoch (Jan 1 1970 at 00:00:00 UTC)
+ */
+int cvmx_rtc_ds1374_write(uint32_t time)
+{
+    int      rc;
+    int      retry;
+    uint8_t  sec;
+
+    for(retry=0; retry<2; retry++)
+    {
+        rc  = cvmx_twsi_write8(CVMX_RTC_DS1374_ADDR, 0x0, time & 0xff);
+        rc |= cvmx_twsi_write8(CVMX_RTC_DS1374_ADDR, 0x1, (time >> 8) & 0xff);
+        rc |= cvmx_twsi_write8(CVMX_RTC_DS1374_ADDR, 0x2, (time >> 16) & 0xff);
+        rc |= cvmx_twsi_write8(CVMX_RTC_DS1374_ADDR, 0x3, (time >> 24) & 0xff);
+        sec = cvmx_twsi_read8(CVMX_RTC_DS1374_ADDR, 0x0);
+        if (sec == (time & 0xff))
+            break; /* Time did not roll-over, value is correct */
+    }
+
+    return (rc ? -1 : 0);
+}
+
+int cvmx_rtc_ds1374_alarm_config(int WD, int WDSTR, int AIE)
+{
+    int val;
+
+    val = cvmx_twsi_read8(CVMX_RTC_DS1374_ADDR,RTC_CTL_ADDR);
+    val = val & ~RTC_CTL_BIT_EOSC; /* Make sure that oscillator is running */
+    WD?(val = val | RTC_CTL_BIT_WD_ALM):(val = val & ~RTC_CTL_BIT_WD_ALM);
+    WDSTR?(val = val | RTC_CTL_BIT_WDSTR):(val = val & ~RTC_CTL_BIT_WDSTR);
+    AIE?(val = val | RTC_CTL_BIT_AIE):(val = val & ~RTC_CTL_BIT_AIE);
+    cvmx_twsi_write8(CVMX_RTC_DS1374_ADDR,RTC_CTL_ADDR, val);
+    return 0;
+}
+
+int cvmx_rtc_ds1374_alarm_set(int alarm_on)
+{
+    uint8_t val;
+
+    if (alarm_on)
+    {
+        val = cvmx_twsi_read8(CVMX_RTC_DS1374_ADDR,RTC_CTL_ADDR);
+        cvmx_twsi_write8(CVMX_RTC_DS1374_ADDR,RTC_CTL_ADDR, val | RTC_CTL_BIT_WACE);
+    }
+    else
+    {
+        val = cvmx_twsi_read8(CVMX_RTC_DS1374_ADDR,RTC_CTL_ADDR);
+        cvmx_twsi_write8(CVMX_RTC_DS1374_ADDR,RTC_CTL_ADDR, val & ~RTC_CTL_BIT_WACE);
+    }
+    return 0;
+}
+
+
+int cvmx_rtc_ds1374_alarm_counter_set(uint32_t interval)
+{
+    int i;
+    int rc = 0;
+
+    for(i=0;i<3;i++)
+    {
+        rc |= cvmx_twsi_write8(CVMX_RTC_DS1374_ADDR, RTC_WD_ALM_CNT_BYTE0_ADDR+i, interval & 0xFF);
+        interval >>= 8;
+    }
+    return rc;
+}
+
+uint32_t cvmx_rtc_ds1374_alarm_counter_get(void)
+{
+    int i;
+    uint32_t interval = 0;
+
+    for(i=0;i<3;i++)
+    {
+        interval |= ( cvmx_twsi_read8(CVMX_RTC_DS1374_ADDR,RTC_WD_ALM_CNT_BYTE0_ADDR+i) & 0xff) << (i*8);
+    }
+    return interval;
+}
+
+
+#ifdef CVMX_RTC_DEBUG
+
+void cvmx_rtc_ds1374_dump_state(void)
+{
+    int i = 0;
+
+    cvmx_dprintf("RTC:\n");
+    cvmx_dprintf("%d : %02X ", i, cvmx_twsi_read8(CVMX_RTC_DS1374_ADDR, 0x0));
+    for(i=1; i<10; i++)
+    {
+        cvmx_dprintf("%02X ", cvmx_twsi_read8_cur_addr(CVMX_RTC_DS1374_ADDR));
+    }
+    cvmx_dprintf("\n");
+}
+
+#endif /* CVMX_RTC_DEBUG */
+
+
+/*
+ *  LAN bypass functionality
+ */
+
+/* Private initialization function */
+static int cvmx_lan_bypass_init(void)
+{
+    const int CLR_PULSE = 100;  /* Longer than 100 ns (on CPUs up to 1 GHz) */
+
+    //Clear GPIO 6
+    cvmx_gpio_clear(BYPASS_EN);
+
+    //Disable WDT
+    cvmx_rtc_ds1374_alarm_set(0);
+
+    //GPIO(7) Send a low pulse
+    cvmx_gpio_clear(WDT_BP_CLR);
+    cvmx_wait(CLR_PULSE);
+    cvmx_gpio_set(WDT_BP_CLR);
+    return 0;
+}
+
+/**
+ * Set LAN bypass mode.
+ *
+ * Supported modes are:
+ * - CVMX_LAN_BYPASS_OFF
+ *     <br>LAN ports are connected ( port 0 <--> Octeon <--> port 1 )
+ *
+ * - CVMX_LAN_BYPASS_GPIO
+ *     <br>LAN bypass is controlled by software using cvmx_lan_bypass_force() function.
+ *     When transitioning to this mode, default is LAN bypass enabled
+ *     ( port 0 <--> port 1, -- Octeon ).
+ *
+ * - CVMX_LAN_BYPASS_WATCHDOG
+ *     <br>LAN bypass is inactive as long as a watchdog is kept alive.
+ *     The default expiration time is 1 second and the function to
+ *     call periodically to prevent watchdog expiration is
+ *     cvmx_lan_bypass_keep_alive().
+ *
+ * @param mode           LAN bypass mode
+ *
+ * @return Error code, or 0 in case of success
+ */
+int cvmx_lan_bypass_mode_set(cvmx_lan_bypass_mode_t mode)
+{
+    switch(mode)
+    {
+    case CVMX_LAN_BYPASS_GPIO:
+        /* make lan bypass enable */
+        cvmx_lan_bypass_init();
+        cvmx_gpio_set(BYPASS_EN);
+        CVMX_LAN_BYPASS_MSG("Enable LAN bypass by GPIO. \n");
+        break;
+
+    case CVMX_LAN_BYPASS_WATCHDOG:
+        /* make lan bypass enable */
+        cvmx_lan_bypass_init();
+        /* Set WDT parameters and turn it on */
+        cvmx_rtc_ds1374_alarm_counter_set(0x1000);    /* 4096 ticks = 1 sec */
+        cvmx_rtc_ds1374_alarm_config(1,1,1);
+        cvmx_rtc_ds1374_alarm_set(1);
+        CVMX_LAN_BYPASS_MSG("Enable LAN bypass by WDT. \n");
+        break;
+
+    case CVMX_LAN_BYPASS_OFF:
+        /* make lan bypass disable */
+        cvmx_lan_bypass_init();
+        CVMX_LAN_BYPASS_MSG("Disable LAN bypass. \n");
+        break;
+
+    default:
+        CVMX_LAN_BYPASS_MSG("%s: LAN bypass mode %d not supported\n", __FUNCTION__, mode);
+        break;
+    }
+    return 0;
+}
+
+/**
+ * Refresh watchdog timer.
+ *
+ * Call periodically (less than 1 second) to prevent triggering LAN bypass.
+ * The alternative cvmx_lan_bypass_keep_alive_ms() is provided for cases
+ * where a variable interval is required.
+ */
+void cvmx_lan_bypass_keep_alive(void)
+{
+    cvmx_rtc_ds1374_alarm_counter_set(0x1000);    /* 4096 ticks = 1 second */
+}
+
+/**
+ * Refresh watchdog timer, setting a specific expiration interval.
+ *
+ * @param interval_ms     Interval, in milliseconds, to next watchdog expiration.
+ */
+void cvmx_lan_bypass_keep_alive_ms(uint32_t interval_ms)
+{
+    cvmx_rtc_ds1374_alarm_counter_set((interval_ms * 0x1000) / 1000);
+}
+
+/**
+ * Control LAN bypass via software.
+ *
+ * @param force_bypass   Force LAN bypass to active (1) or inactive (0)
+ *
+ * @return Error code, or 0 in case of success
+ */
+int cvmx_lan_bypass_force(int force_bypass)
+{
+    if (force_bypass)
+    {
+        //Set GPIO 6
+        cvmx_gpio_set(BYPASS_EN);
+    }
+    else
+    {
+        cvmx_lan_bypass_init();
+    }
+    return 0;
+}
+
+/**
+ * Return status of LAN bypass circuit.
+ *
+ * @return 1 if ports are in LAN bypass, or 0 if normally connected
+ */
+int cvmx_lan_bypass_is_active(void)
+{
+    return !!(cvmx_gpio_read() & BYPASS_STATUS);
+}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-thunder.h b/arch/mips/cavium-octeon/executive/cvmx-thunder.h
new file mode 100644
index 0000000..5f3adde
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-thunder.h
@@ -0,0 +1,156 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+#ifndef __CVMX_THUNDER_H__
+#define __CVMX_THUNDER_H__
+
+/**
+ * @file
+ *
+ * Interface to the Thunder specific devices
+ *
+ * <hr>$Revision: 32636 $<hr>
+ *
+ */
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#define CVMX_RTC_DS1374_ADDR   (0x68)
+
+/*
+ * Read time-of-day counter.
+ * This function is called internally by cvmx-rtc functions.
+ */
+uint32_t cvmx_rtc_ds1374_read(void);
+
+/*
+ * Write time-of-day counter.
+ * This function is called internally by cvmx-rtc functions.
+ */
+int      cvmx_rtc_ds1374_write(uint32_t time);
+
+
+/**
+ * LAN bypass modes.
+ */
+typedef enum {
+    CVMX_LAN_BYPASS_OFF = 0,   /**< LAN bypass is disabled, port 0 and port 1
+                                    are always connected to Octeon */
+    CVMX_LAN_BYPASS_GPIO,      /**< LAN bypass controlled by GPIO only */
+    CVMX_LAN_BYPASS_WATCHDOG,  /**< LAN bypass controlled by watchdog (and GPIO) */
+    CVMX_LAN_BYPASS_LAST       /* Keep as last entry */
+} cvmx_lan_bypass_mode_t;
+
+
+/**
+ * Set LAN bypass mode.
+ *
+ * Supported modes are:
+ * - CVMX_LAN_BYPASS_OFF
+ *     <br>LAN ports are connected ( port 0 <--> Octeon <--> port 1 )
+ *
+ * - CVMX_LAN_BYPASS_GPIO
+ *     <br>LAN bypass is controlled by software using cvmx_lan_bypass_force() function.
+ *     When transitioning to this mode, default is LAN bypass enabled
+ *     ( port 0 <--> port 1, disconnected from Octeon ).
+ *
+ * - CVMX_LAN_BYPASS_WATCHDOG
+ *     <br>LAN bypass is inactive as long as the watchdog is kept alive.
+ *     The default expiration time is 1 second and the function to
+ *     call periodically to prevent watchdog expiration is
+ *     cvmx_lan_bypass_keep_alive().
+ *
+ * @param mode           LAN bypass mode
+ *
+ * @return Error code, or 0 in case of success
+ */
+int  cvmx_lan_bypass_mode_set(cvmx_lan_bypass_mode_t mode);
+
+/**
+ * Return status of LAN bypass circuit.
+ *
+ * @return 1 if ports are in LAN bypass, or 0 if normally connected
+ */
+int  cvmx_lan_bypass_is_active(void);
+
+/**
+ * Refresh watchdog timer.
+ *
+ * Call periodically (less than 1 second) to prevent triggering LAN bypass.
+ * The alternative cvmx_lan_bypass_keep_alive_ms() is provided for cases
+ * where a variable interval is required.
+ */
+void cvmx_lan_bypass_keep_alive(void);
+
+/**
+ * Refresh watchdog timer, setting a specific expiration interval.
+ *
+ * @param interval_ms     Interval, in milliseconds, to next watchdog expiration.
+ */
+void cvmx_lan_bypass_keep_alive_ms(uint32_t interval_ms);
+
+/**
+ * Control LAN bypass via software.
+ *
+ * @param force_bypass   Force LAN bypass to active (1) or inactive (0)
+ *
+ * @return Error code, or 0 in case of success
+ */
+int  cvmx_lan_bypass_force(int force_bypass);
+
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif  /* __CVMX_THUNDER_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-tim.c b/arch/mips/cavium-octeon/executive/cvmx-tim.c
new file mode 100644
index 0000000..7f27e28
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-tim.c
@@ -0,0 +1,265 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Support library for the hardware work queue timers.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+#include "executive-config.h"
+#include "cvmx-config.h"
+#include "cvmx.h"
+#include "cvmx-sysinfo.h"
+#include "cvmx-tim.h"
+#include "cvmx-bootmem.h"
+
+/* CSR typedefs have been moved to cvmx-csr-*.h */
+
+/**
+ * Global structure holding the state of all timers.
+ */
+CVMX_SHARED cvmx_tim_t cvmx_tim;
+
+
+#ifdef CVMX_ENABLE_TIMER_FUNCTIONS
+/**
+ * Setup a timer for use. Must be called before the timer
+ * can be used.
+ *
+ * @param tick      Time between each bucket in microseconds. This must not be
+ *                  smaller than 1024/(clock frequency in MHz).
+ * @param max_ticks The maximum number of ticks the timer must be able
+ *                  to schedule in the future. There are guaranteed to be enough
+ *                  timer buckets such that:
+ *                  number of buckets >= max_ticks.
+ * @return Zero on success. Negative on error. Failures are possible
+ *         if the number of buckets needed is too large or memory
+ *         allocation fails for creating the buckets.
+ */
+int cvmx_tim_setup(uint64_t tick, uint64_t max_ticks)
+{
+    cvmx_tim_mem_ring0_t    config_ring0;
+    cvmx_tim_mem_ring1_t    config_ring1;
+    uint64_t                timer_id;
+    int                     error = -1;
+#if !(defined(__KERNEL__) && defined(linux))
+    cvmx_sysinfo_t         *sys_info_ptr = cvmx_sysinfo_get();
+    uint64_t                cpu_clock_hz = sys_info_ptr->cpu_clock_hz;
+#else
+    uint64_t                cpu_clock_hz = octeon_get_clock_rate();
+#endif
+    uint64_t                hw_tick_ns;
+    uint64_t                tick_ns = 1000 * tick;
+    int                     i;
+    uint32_t                temp;
+
+    /* for the simulator */
+    if (cpu_clock_hz == 0)
+      cpu_clock_hz = 333000000;
+    hw_tick_ns = 1024 * 1000000000ull / cpu_clock_hz;
+
+    /* Make sure the timers are stopped */
+    cvmx_tim_stop();
+
+    /* Reinitialize out timer state */
+    memset(&cvmx_tim, 0, sizeof(cvmx_tim));
+
+    if ((tick_ns < hw_tick_ns) || (tick_ns > 4194304 * hw_tick_ns))
+      {
+	cvmx_dprintf("init: tick wrong size\n");
+	return error;
+      }
+
+    for (i=1; i<20; i++)
+    {
+      if (tick_ns < (hw_tick_ns << i))
+	break;
+    }
+
+    cvmx_tim.max_ticks = (uint32_t)max_ticks;
+    cvmx_tim.bucket_shift = (uint32_t)(i - 1 + 10);
+    cvmx_tim.tick_cycles = tick * cpu_clock_hz / 1000000;
+
+    temp = (max_ticks * cvmx_tim.tick_cycles) >> cvmx_tim.bucket_shift;
+
+    /* round up to nearest power of 2 */
+    temp -= 1;
+    temp = temp | (temp >> 1);
+    temp = temp | (temp >> 2);
+    temp = temp | (temp >> 4);
+    temp = temp | (temp >> 8);
+    temp = temp | (temp >> 16);
+    cvmx_tim.num_buckets = temp + 1;
+
+    /* ensure input params fall into permitted ranges */
+    if ((cvmx_tim.num_buckets < 3) || cvmx_tim.num_buckets > 1048576)
+      {
+	cvmx_dprintf("init: num_buckets out of range\n");
+	return error;
+      }
+
+    /* Allocate the timer buckets from hardware addressable memory */
+    cvmx_tim.bucket = cvmx_bootmem_alloc(CVMX_TIM_NUM_TIMERS * cvmx_tim.num_buckets
+					 * sizeof(cvmx_tim_bucket_entry_t), CVMX_CACHE_LINE_SIZE);
+    if (cvmx_tim.bucket == NULL)
+      {
+	cvmx_dprintf("init: allocation problem\n");
+	return error;
+      }
+    memset(cvmx_tim.bucket, 0, CVMX_TIM_NUM_TIMERS * cvmx_tim.num_buckets * sizeof(cvmx_tim_bucket_entry_t));
+
+    cvmx_tim.start_time = 0;
+
+    /* Loop through all timers */
+    for (timer_id = 0; timer_id<CVMX_TIM_NUM_TIMERS; timer_id++)
+    {
+        cvmx_tim_bucket_entry_t *bucket = cvmx_tim.bucket + timer_id * cvmx_tim.num_buckets;
+        /* Tell the hardware where about the bucket array */
+        config_ring0.u64 = 0;
+        config_ring0.s.first_bucket = cvmx_ptr_to_phys(bucket) >> 5;
+        config_ring0.s.num_buckets = cvmx_tim.num_buckets - 1;
+        config_ring0.s.ring = timer_id;
+        cvmx_write_csr(CVMX_TIM_MEM_RING0, config_ring0.u64);
+
+        /* Tell the hardware the size of each chunk block in pointers */
+        config_ring1.u64 = 0;
+        config_ring1.s.enable = 1;
+        config_ring1.s.pool = CVMX_FPA_TIMER_POOL;
+        config_ring1.s.words_per_chunk = CVMX_FPA_TIMER_POOL_SIZE / 8;
+        config_ring1.s.interval = (1 << (cvmx_tim.bucket_shift - 10)) - 1;
+        config_ring1.s.ring = timer_id;
+        cvmx_write_csr(CVMX_TIM_MEM_RING1, config_ring1.u64);
+    }
+
+    return 0;
+}
+#endif
+
+/**
+ * Start the hardware timer processing
+ */
+void cvmx_tim_start(void)
+{
+    cvmx_tim_control_t control;
+
+    control.u64 = 0;
+    control.s.enable_dwb = 1;
+    control.s.enable_timers = 1;
+
+    /* Remember when we started the timers */
+    cvmx_tim.start_time = cvmx_get_cycle();
+    cvmx_write_csr(CVMX_TIM_REG_FLAGS, control.u64);
+}
+
+
+/**
+ * Stop the hardware timer processing. Timers stay configured.
+ */
+void cvmx_tim_stop(void)
+{
+    cvmx_tim_control_t control;
+    control.u64 = 0;
+    control.s.enable_dwb = 0;
+    control.s.enable_timers = 0;
+    cvmx_write_csr(CVMX_TIM_REG_FLAGS, control.u64);
+}
+
+
+/**
+ * Stop the timer. After this the timer must be setup again
+ * before use.
+ */
+#ifdef CVMX_ENABLE_TIMER_FUNCTIONS
+void cvmx_tim_shutdown(void)
+{
+    uint32_t                bucket;
+    uint64_t                timer_id;
+    uint64_t                entries_per_chunk;
+
+    /* Make sure the timers are stopped */
+    cvmx_tim_stop();
+
+    entries_per_chunk = CVMX_FPA_TIMER_POOL_SIZE/8 - 1;
+
+    /* Now walk all buckets freeing the chunks */
+    for (timer_id = 0; timer_id<CVMX_TIM_NUM_TIMERS; timer_id++)
+    {
+        for (bucket=0; bucket<cvmx_tim.num_buckets; bucket++)
+        {
+            uint64_t chunk_addr;
+            uint64_t next_chunk_addr;
+            cvmx_tim_bucket_entry_t *bucket_ptr = cvmx_tim.bucket + timer_id * cvmx_tim.num_buckets + bucket;
+            CVMX_PREFETCH128(CAST64(bucket_ptr));  /* prefetch the next cacheline for future buckets */
+
+            /* Each bucket contains a list of chunks */
+            chunk_addr = bucket_ptr->first_chunk_addr;
+            while (bucket_ptr->num_entries)
+            {
+#ifdef DEBUG
+                cvmx_dprintf("Freeing Timer Chunk 0x%llx\n", CAST64(chunk_addr));
+#endif
+                /* Read next chunk pointer from end of the current chunk */
+                next_chunk_addr = cvmx_read_csr(CVMX_ADD_SEG(CVMX_MIPS_SPACE_XKPHYS, chunk_addr + CVMX_FPA_TIMER_POOL_SIZE - 8));
+
+                cvmx_fpa_free(cvmx_phys_to_ptr(chunk_addr), CVMX_FPA_TIMER_POOL, 0);
+                chunk_addr = next_chunk_addr;
+                if (bucket_ptr->num_entries > entries_per_chunk)
+                    bucket_ptr->num_entries -= entries_per_chunk;
+                else
+                    bucket_ptr->num_entries = 0;
+            }
+        }
+    }
+}
+
+#endif
diff --git a/arch/mips/cavium-octeon/executive/cvmx-tim.h b/arch/mips/cavium-octeon/executive/cvmx-tim.h
new file mode 100644
index 0000000..6548d05
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-tim.h
@@ -0,0 +1,339 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Interface to the hardware work queue timers.
+ *
+`* <hr>$Revision: 32636 $<hr>
+ */
+
+#ifndef __CVMX_TIM_H__
+#define __CVMX_TIM_H__
+
+#include "cvmx-fpa.h"
+#include "cvmx-wqe.h"
+
+#include "executive-config.h"
+#ifdef CVMX_ENABLE_TIMER_FUNCTIONS
+#include "cvmx-config.h"
+#endif
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#define CVMX_TIM_NUM_TIMERS   16
+#define CVMX_TIM_NUM_BUCKETS  2048
+
+typedef enum
+{
+    CVMX_TIM_STATUS_SUCCESS = 0,
+    CVMX_TIM_STATUS_NO_MEMORY = -1,
+    CVMX_TIM_STATUS_TOO_FAR_AWAY = -2,
+    CVMX_TIM_STATUS_BUSY = -3
+} cvmx_tim_status_t;
+
+/**
+ * Each timer bucket contains a list of work queue entries to
+ * schedule when the timer fires. The list is implemented as
+ * a linked list of blocks. Each block contains an array of
+ * work queue entries followed by a next block pointer. Since
+ * these blocks are dynamically allocated off of a hardware
+ * memory pool, there actual size isn't known compile time.
+ * The next block pointer is stored in the last 8 bytes of
+ * the memory block.
+ */
+typedef struct cvmx_tim_entry_chunk
+{
+    volatile uint64_t entries[0];
+} cvmx_tim_entry_chunk_t;
+
+/**
+ * Each timer contains an array of buckets. Each bucket
+ * represents the list of work queue entries that should be
+ * scheduled when the timer fires.  The first 3 entries are used
+ * byt the hardware.
+ */
+typedef struct
+{
+   volatile uint64_t                first_chunk_addr;
+   volatile uint32_t                num_entries;    /**< Zeroed by HW after traversing list */
+   volatile uint32_t                chunk_remainder;/**< Zeroed by HW after traversing list */
+
+   // the remaining 16 bytes are not touched by hardware
+   volatile cvmx_tim_entry_chunk_t *last_chunk;
+   uint64_t                         pad;
+} cvmx_tim_bucket_entry_t;
+
+/**
+ * Structure representing an individual timer. Each timer has
+ * a timer period, a memory management pool, and a list of
+ * buckets.
+ */
+typedef struct
+{
+    cvmx_tim_bucket_entry_t*bucket;             /**< The timer buckets. Array of [CVMX_TIM_NUM_TIMERS][CVMX_TIM_NUM_BUCKETS] */
+    uint64_t                tick_cycles;        /**< How long a bucket represents */
+    uint64_t                start_time;         /**< Time the timer started in cycles */
+    uint32_t                bucket_shift;       /**< How long a bucket represents in ms */
+    uint32_t                num_buckets;        /**< How many buckets per wheel */
+    uint32_t                max_ticks;          /**< maximum number of ticks allowed for timer */
+} cvmx_tim_t;
+
+/**
+ * Structure used to store state information needed to delete
+ * an already scheduled timer entry. An instance of this
+ * structure must be passed to cvmx_tim_add_entry in order
+ * to be able to delete an entry later with
+ * cvmx_tim_delete_entry.
+ *
+ * NOTE: This structure should be considered opaque by the application,
+ * and the application should not access its members
+ */
+typedef struct
+{
+    uint64_t        commit_cycles;  /**< After this time the timer can't be changed */
+    uint64_t *      timer_entry_ptr;/**< Where the work entry is. Zero this
+                                            location to delete the entry */
+} cvmx_tim_delete_t;
+
+/**
+ * Global structure holding the state of all timers.
+ */
+extern cvmx_tim_t cvmx_tim;
+
+
+
+
+#ifdef CVMX_ENABLE_TIMER_FUNCTIONS
+/**
+ * Setup a timer for use. Must be called before the timer
+ * can be used.
+ *
+ * @param tick      Time between each bucket in microseconds. This must not be
+ *                  smaller than 1024/(clock frequency in MHz).
+ * @param max_ticks The maximum number of ticks the timer must be able
+ *                  to schedule in the future. There are guaranteed to be enough
+ *                  timer buckets such that:
+ *                  number of buckets >= max_ticks.
+ * @return Zero on success. Negative on error. Failures are possible
+ *         if the number of buckets needed is too large or memory
+ *         allocation fails for creating the buckets.
+ */
+int cvmx_tim_setup(uint64_t tick, uint64_t max_ticks);
+#endif
+
+/**
+ * Start the hardware timer processing
+ */
+extern void cvmx_tim_start(void);
+
+
+/**
+ * Stop the hardware timer processing. Timers stay configured.
+ */
+extern void cvmx_tim_stop(void);
+
+
+/**
+ * Stop the timer. After this the timer must be setup again
+ * before use.
+ */
+#ifdef CVMX_ENABLE_TIMER_FUNCTIONS
+extern void cvmx_tim_shutdown(void);
+#endif
+
+#ifdef CVMX_ENABLE_TIMER_FUNCTIONS
+/**
+ * Add a work queue entry to the timer.
+ *
+ * @param work_entry Work queue entry to add.
+ * @param ticks_from_now
+ * @param delete_info
+ *                   Optional pointer where to store information needed to
+ *                   delete the timer entry. If non NULL information needed
+ *                   to delete the timer entry before it fires is stored here.
+ *                   If you don't need to be able to delete the timer, pass
+ *                   NULL.
+ * @return Result return code
+ */
+static inline cvmx_tim_status_t cvmx_tim_add_entry(cvmx_wqe_t *work_entry, uint64_t ticks_from_now, cvmx_tim_delete_t *delete_info)
+{
+    cvmx_tim_bucket_entry_t*    work_bucket_ptr;
+    uint64_t                    current_bucket;
+    uint64_t                    work_bucket;
+    volatile uint64_t         * tim_entry_ptr;  /* pointer to wqe address in timer chunk */
+    uint64_t                    entries_per_chunk;
+
+    const uint64_t  cycles  = cvmx_get_cycle(); /* Get our reference time early for accuracy */
+    const uint64_t  core_num    = cvmx_get_core_num();  /* One timer per processor, so use this to select */
+
+    /* Make sure the specified time won't wrap our bucket list */
+    if (ticks_from_now > cvmx_tim.max_ticks)
+    {
+        cvmx_dprintf("cvmx_tim_add_entry: Tried to schedule work too far away.\n");
+        return CVMX_TIM_STATUS_TOO_FAR_AWAY;
+    }
+
+    /* Since we have no way to synchronize, we can't update a timer that is
+        being used by the hardware. Two buckets forward should be safe */
+    if (ticks_from_now < 2)
+    {
+        cvmx_dprintf("cvmx_tim_add_entry: Tried to schedule work too soon. Delaying it.\n");
+        ticks_from_now = 2;
+    }
+
+    /* Get the bucket this work queue entry should be in. Remember the bucket
+        array is circular */
+    current_bucket = ((cycles - cvmx_tim.start_time)
+		   >> cvmx_tim.bucket_shift);
+    work_bucket = (((ticks_from_now * cvmx_tim.tick_cycles) + cycles - cvmx_tim.start_time)
+		   >> cvmx_tim.bucket_shift);
+
+    work_bucket_ptr = cvmx_tim.bucket + core_num * cvmx_tim.num_buckets + (work_bucket & (cvmx_tim.num_buckets - 1));
+    entries_per_chunk = (CVMX_FPA_TIMER_POOL_SIZE/8 - 1);
+
+    /* Check if we have room to add this entry into the existing list */
+    if (work_bucket_ptr->chunk_remainder)
+    {
+        /* Adding the work entry to the end of the existing list */
+        tim_entry_ptr = &(work_bucket_ptr->last_chunk->entries[entries_per_chunk - work_bucket_ptr->chunk_remainder]);
+        *tim_entry_ptr = cvmx_ptr_to_phys(work_entry);
+        work_bucket_ptr->chunk_remainder--;
+        work_bucket_ptr->num_entries++;
+    }
+    else
+    {
+        /* Current list is either completely empty or completely full. We need
+            to allocate a new chunk for storing this work entry */
+        cvmx_tim_entry_chunk_t *new_chunk = (cvmx_tim_entry_chunk_t *)cvmx_fpa_alloc(CVMX_FPA_TIMER_POOL);
+        if (new_chunk == NULL)
+        {
+            cvmx_dprintf("cvmx_tim_add_entry: Failed to allocate memory for new chunk.\n");
+            return CVMX_TIM_STATUS_NO_MEMORY;
+        }
+
+        /* Does a chunk currently exist? We have to check num_entries since
+            the hardware doesn't NULL out the chunk pointers on free */
+        if (work_bucket_ptr->num_entries)
+        {
+            /* This chunk must be appended to an existing list by putting
+            ** its address in the last spot of the existing chunk. */
+            work_bucket_ptr->last_chunk->entries[entries_per_chunk] = cvmx_ptr_to_phys(new_chunk);
+            work_bucket_ptr->num_entries++;
+        }
+        else
+        {
+            /* This is the very first chunk. Add it */
+            work_bucket_ptr->first_chunk_addr = cvmx_ptr_to_phys(new_chunk);
+            work_bucket_ptr->num_entries = 1;
+        }
+        work_bucket_ptr->last_chunk = new_chunk;
+        work_bucket_ptr->chunk_remainder = entries_per_chunk - 1;
+        tim_entry_ptr = &(new_chunk->entries[0]);
+        *tim_entry_ptr = cvmx_ptr_to_phys(work_entry);
+    }
+
+    /* If the user supplied a delete info structure then fill it in */
+    if (delete_info)
+    {
+        /* It would be very bad to delete a timer entry after, or during the
+            timer's processing. During the processing could yield unpredicatable
+            results, but after would always be bad. Modifying the entry after
+            processing means we would be changing data in a buffer that has been
+            freed, and possible allocated again. For this reason we store a
+            commit cycle count in the delete structure. If we are after this
+            count we will refuse to delete the timer entry. */
+        delete_info->commit_cycles = cycles + (ticks_from_now - 2) * cvmx_tim.tick_cycles;
+        delete_info->timer_entry_ptr = (uint64_t *)tim_entry_ptr;  /* Cast to non-volatile type */
+    }
+
+    return CVMX_TIM_STATUS_SUCCESS;
+}
+#endif
+
+
+/**
+ * Delete a timer entry scheduled using cvmx_tim_add_entry.
+ * Deleting a timer will fail if it has already triggered or
+ * might be in progress. The actual state of the work queue
+ * entry isn't changed. You need to dispose of it properly.
+ *
+ * @param delete_info
+ *               Structure passed to cvmx_tim_add_entry to store the
+ *               information needed to delete a timer entry.
+ * @return CVMX_TIM_STATUS_BUSY if the timer was not deleted, otherwise
+ *         CVMX_TIM_STATUS_SUCCESS.
+ */
+static inline cvmx_tim_status_t cvmx_tim_delete_entry(cvmx_tim_delete_t *delete_info)
+{
+    const uint64_t cycles = cvmx_get_cycle();
+
+    if ((int64_t)(cycles - delete_info->commit_cycles) < 0)
+    {
+        /* Timer is far enough away. Safe to delete */
+        *delete_info->timer_entry_ptr = 0;
+        return CVMX_TIM_STATUS_SUCCESS;
+    }
+    else
+    {
+        /* Timer is passed the commit time. It cannot be stopped */
+        return CVMX_TIM_STATUS_BUSY;
+    }
+}
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif // __CVMX_TIM_H__
diff --git a/arch/mips/cavium-octeon/executive/cvmx-tra.c b/arch/mips/cavium-octeon/executive/cvmx-tra.c
new file mode 100644
index 0000000..34f4925
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-tra.c
@@ -0,0 +1,330 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+/**
+ * @file
+ *
+ * Interface to the Trace buffer hardware.
+ *
+ * <hr>$Revision: 30644 $<hr>
+ */
+#include "cvmx.h"
+#include "cvmx-tra.h"
+
+static const char *TYPE_ARRAY[] = {
+    "DWB - Don't write back",
+    "PL2 - Prefetch into L2",
+    "PSL1 - Dcache fill, skip L2",
+    "LDD - Dcache fill",
+    "LDI - Icache fill",
+    "LDT - Icache fill, skip L2",
+    "STF - Store full",
+    "STC - Store conditional",
+    "STP - Store partial",
+    "STT - Store full, skip L2",
+    "IOBLD8 - IOB 8bit load",
+    "IOBLD16 - IOB 16bit load",
+    "IOBLD32 - IOB 32bit load",
+    "IOBLD64 - IOB 64bit load",
+    "IOBST - IOB store",
+    "IOBDMA - Async IOB",
+    "SAA - Store atomic add",
+    "RSVD17",
+    "RSVD18",
+    "RSVD19",
+    "RSVD20",
+    "RSVD21",
+    "RSVD22",
+    "RSVD23",
+    "RSVD24",
+    "RSVD25",
+    "RSVD26",
+    "RSVD27",
+    "RSVD28",
+    "RSVD29",
+    "RSVD30",
+    "RSVD31"
+};
+
+static const char *SOURCE_ARRAY[] = {
+    "PP0",
+    "PP1",
+    "PP2",
+    "PP3",
+    "PP4",
+    "PP5",
+    "PP6",
+    "PP7",
+    "PP8",
+    "PP9",
+    "PP10",
+    "PP11",
+    "PP12",
+    "PP13",
+    "PP14",
+    "PP15",
+    "PIP/IPD",
+    "PKO-R",
+    "FPA/TIM/DFA/PCI/ZIP/POW/PKO-W",
+    "DWB",
+    "RSVD20",
+    "RSVD21",
+    "RSVD22",
+    "RSVD23",
+    "RSVD24",
+    "RSVD25",
+    "RSVD26",
+    "RSVD27",
+    "RSVD28",
+    "RSVD29",
+    "RSVD30",
+    "RSVD31"
+};
+
+static const char *DEST_ARRAY[] = {
+    "CIU/GPIO",
+    "RSVD1",
+    "RSVD2",
+    "PCI/PCIe",
+    "KEY",
+    "FPA",
+    "DFA",
+    "ZIP",
+    "RNG",
+    "IPD",
+    "PKO",
+    "RSVD11",
+    "POW",
+    "RSVD13",
+    "RSVD14",
+    "RSVD15",
+    "RSVD16",
+    "RSVD17",
+    "RSVD18",
+    "RSVD19",
+    "RSVD20",
+    "RSVD21",
+    "RSVD22",
+    "RSVD23",
+    "RSVD24",
+    "RSVD25",
+    "RSVD26",
+    "RSVD27",
+    "RSVD28",
+    "RSVD29",
+    "RSVD30",
+    "RSVD31"
+};
+
+/**
+ * Setup the TRA buffer for use
+ *
+ * @param control TRA control setup
+ * @param filter  Which events to log
+ * @param source_filter
+ *                Source match
+ * @param dest_filter
+ *                Destination match
+ * @param address Address compare
+ * @param address_mask
+ *                Address mask
+ */
+void cvmx_tra_setup(cvmx_tra_ctl_t control, cvmx_tra_filt_cmd_t filter,
+                    cvmx_tra_filt_sid_t source_filter, cvmx_tra_filt_did_t dest_filter,
+                    uint64_t address, uint64_t address_mask)
+{
+    cvmx_write_csr(CVMX_TRA_CTL,            control.u64);
+    cvmx_write_csr(CVMX_TRA_FILT_CMD,       filter.u64);
+    cvmx_write_csr(CVMX_TRA_FILT_SID,       source_filter.u64);
+    cvmx_write_csr(CVMX_TRA_FILT_DID,       dest_filter.u64);
+    cvmx_write_csr(CVMX_TRA_FILT_ADR_ADR,   address);
+    cvmx_write_csr(CVMX_TRA_FILT_ADR_MSK,   address_mask);
+}
+
+
+/**
+ * Setup a TRA trigger. How the triggers are used should be
+ * setup using cvmx_tra_setup.
+ *
+ * @param trigger Trigger to setup (0 or 1)
+ * @param filter  Which types of events to trigger on
+ * @param source_filter
+ *                Source trigger match
+ * @param dest_filter
+ *                Destination trigger match
+ * @param address Trigger address compare
+ * @param address_mask
+ *                Trigger address mask
+ */
+void cvmx_tra_trig_setup(uint64_t trigger, cvmx_tra_filt_cmd_t filter,
+                         cvmx_tra_filt_sid_t source_filter, cvmx_tra_trig0_did_t dest_filter,
+                         uint64_t address, uint64_t address_mask)
+{
+    cvmx_write_csr(CVMX_TRA_TRIG0_CMD + trigger * 64,       filter.u64);
+    cvmx_write_csr(CVMX_TRA_TRIG0_SID + trigger * 64,       source_filter.u64);
+    cvmx_write_csr(CVMX_TRA_TRIG0_DID + trigger * 64,       dest_filter.u64);
+    cvmx_write_csr(CVMX_TRA_TRIG0_ADR_ADR + trigger * 64,   address);
+    cvmx_write_csr(CVMX_TRA_TRIG0_ADR_MSK + trigger * 64,   address_mask);
+}
+
+
+/**
+ * Read an entry from the TRA buffer
+ *
+ * @return Value return. High bit will be zero if there wasn't any data
+ */
+cvmx_tra_data_t cvmx_tra_read(void)
+{
+    cvmx_tra_data_t result;
+    result.u64 = cvmx_read_csr(CVMX_TRA_READ_DAT);
+    return result;
+}
+
+
+/**
+ * Decode a TRA entry into human readable output
+ *
+ * @param tra_ctl Trace control setup
+ * @param data    Data to decode
+ */
+void cvmx_tra_decode_text(cvmx_tra_ctl_t tra_ctl, cvmx_tra_data_t data)
+{
+    /* The type is a five bit field for some entries and 4 for other. The four
+        bit entries can be mis-typed if the top is set */
+    int type = data.cmn.type;
+    if (type >= 0x1a)
+        type &= 0xf;
+    switch (type)
+    {
+        case CVMX_TRA_DATA_DWB:
+        case CVMX_TRA_DATA_PL2:
+        case CVMX_TRA_DATA_PSL1:
+        case CVMX_TRA_DATA_LDD:
+        case CVMX_TRA_DATA_LDI:
+        case CVMX_TRA_DATA_LDT:
+            cvmx_dprintf("0x%016llx %c%+10lld %s %s 0x%016llx\n",
+                   (unsigned long long)data.u64,
+                   (data.cmn.discontinuity) ? 'D' : ' ',
+                   (unsigned long long)data.cmn.timestamp << (tra_ctl.s.time_grn*3),
+                   TYPE_ARRAY[type],
+                   SOURCE_ARRAY[data.cmn.source],
+                   (unsigned long long)data.cmn.address);
+            break;
+        case CVMX_TRA_DATA_STC:
+        case CVMX_TRA_DATA_STF:
+        case CVMX_TRA_DATA_STP:
+        case CVMX_TRA_DATA_STT:
+        case CVMX_TRA_DATA_SAA:
+            cvmx_dprintf("0x%016llx %c%+10lld %s %s mask=0x%02x 0x%016llx\n",
+                   (unsigned long long)data.u64,
+                   (data.cmn.discontinuity) ? 'D' : ' ',
+                   (unsigned long long)data.cmn.timestamp << (tra_ctl.s.time_grn*3),
+                   TYPE_ARRAY[type],
+                   SOURCE_ARRAY[data.store.source],
+                   (unsigned int)data.store.mask,
+                   (unsigned long long)data.store.address << 3);
+            break;
+        case CVMX_TRA_DATA_IOBLD8:
+        case CVMX_TRA_DATA_IOBLD16:
+        case CVMX_TRA_DATA_IOBLD32:
+        case CVMX_TRA_DATA_IOBLD64:
+        case CVMX_TRA_DATA_IOBST:
+            cvmx_dprintf("0x%016llx %c%+10lld %s %s->%s subdid=0x%x 0x%016llx\n",
+                   (unsigned long long)data.u64,
+                   (data.cmn.discontinuity) ? 'D' : ' ',
+                   (unsigned long long)data.cmn.timestamp << (tra_ctl.s.time_grn*3),
+                   TYPE_ARRAY[type],
+                   SOURCE_ARRAY[data.iobld.source],
+                   DEST_ARRAY[data.iobld.dest],
+                   (unsigned int)data.iobld.subid,
+                   (unsigned long long)data.iobld.address);
+            break;
+        case CVMX_TRA_DATA_IOBDMA:
+            cvmx_dprintf("0x%016llx %c%+10lld %s %s->%s len=0x%x 0x%016llx\n",
+                   (unsigned long long)data.u64,
+                   (data.cmn.discontinuity) ? 'D' : ' ',
+                   (unsigned long long)data.cmn.timestamp << (tra_ctl.s.time_grn*3),
+                   TYPE_ARRAY[type],
+                   SOURCE_ARRAY[data.iob.source],
+                   DEST_ARRAY[data.iob.dest],
+                   (unsigned int)data.iob.mask,
+                   (unsigned long long)data.iob.address << 3);
+            break;
+        default:
+            cvmx_dprintf("0x%016llx %c%+10lld Unknown format\n",
+                   (unsigned long long)data.u64,
+                   (data.cmn.discontinuity) ? 'D' : ' ',
+                   (unsigned long long)data.cmn.timestamp << (tra_ctl.s.time_grn*3));
+            break;
+    }
+}
+
+
+/**
+ * Display the entire trace buffer. It is advised that you
+ * disable the trace buffer before calling this routine
+ * otherwise it could infinitely loop displaying trace data
+ * that it created.
+ */
+void cvmx_tra_display(void)
+{
+    cvmx_tra_ctl_t tra_ctl;
+    cvmx_tra_data_t data;
+
+    tra_ctl.u64 = cvmx_read_csr(CVMX_TRA_CTL);
+
+    do
+    {
+        data = cvmx_tra_read();
+        if (data.cmn.valid)
+            cvmx_tra_decode_text(tra_ctl, data);
+    } while (data.cmn.valid);
+}
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-tra.h b/arch/mips/cavium-octeon/executive/cvmx-tra.h
new file mode 100644
index 0000000..091da88
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-tra.h
@@ -0,0 +1,419 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Interface to the Trace buffer hardware.
+ *
+ * WRITING THE TRACE BUFFER
+ *
+ * When the trace is enabled, commands are traced continuously (wrapping) or until the buffer is filled once
+ * (no wrapping).  Additionally and independent of wrapping, tracing can be temporarily enabled and disabled
+ * by the tracing triggers.  All XMC commands can be traced except for IDLE and IOBRSP.  The subset of XMC
+ * commands that are traced is determined by the filter and the two triggers, each of which is comprised of
+ * masks for command, sid, did, and address).  If triggers are disabled, then only those commands matching
+ * the filter are traced.  If triggers are enabled, then only those commands matching the filter, the start
+ * trigger, or the stop trigger are traced during the time between a start trigger and a stop trigger.
+ *
+ * For a given command, its XMC data is written immediately to the buffer.  If the command has XMD data,
+ * then that data comes in-order at some later time.  The XMD data is accumulated across all valid
+ * XMD cycles and written to the buffer or to a shallow fifo.  Data from the fifo is written to the buffer
+ * as soon as it gets access to write the buffer (i.e. the buffer is not currently being written with XMC
+ * data).  If the fifo overflows, it simply overwrites itself and the previous XMD data is lost.
+ *
+ *
+ * READING THE TRACE BUFFER
+ *
+ * Each entry of the trace buffer is read by a CSR read command.  The trace buffer services each read in order,
+ * as soon as it has access to the (single-ported) trace buffer.
+ *
+ *
+ * OVERFLOW, UNDERFLOW AND THRESHOLD EVENTS
+ *
+ * The trace buffer maintains a write pointer and a read pointer and detects both the overflow and underflow
+ * conditions.  Each time a new trace is enabled, both pointers are reset to entry 0.  Normally, each write
+ * (traced event) increments the write pointer and each read increments the read pointer.  During the overflow
+ * condition, writing (tracing) is disabled.  Tracing will continue as soon as the overflow condition is
+ * resolved.  The first entry that is written immediately following the overflow condition may be marked to
+ * indicate that a tracing discontinuity has occurred before this entry.  During the underflow condition,
+ * reading does not increment the read pointer and the read data is marked to indicate that no read data is
+ * available.
+ *
+ * The full threshold events are defined to signal an interrupt a certain levels of "fullness" (1/2, 3/4, 4/4).
+ * "fullness" is defined as the relative distance between the write and read pointers (i.e. not defined as the
+ * absolute distance between the write pointer and entry 0).  When enabled, the full threshold event occurs
+ * every time the desired level of "fullness" is achieved.
+ *
+ *
+ * Trace buffer entry format
+ * @verbatim
+ *       6                   5                   4                   3                   2                   1                   0
+ * 3 2 1 0 9 8 7 6 5 4 3 2 1 0 9 8 7 6 5 4 3 2 1 0 9 8 7 6 5 4 3 2 1 0 9 8 7 6 5 4 3 2 1 0 9 8 7 6 5 4 3 2 1 0 9 8 7 6 5 4 3 2 1 0
+ * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
+ * |sta|                          address[35:3]                          |       0       | src id  |   0   | DWB   | diff timestamp|
+ * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
+ * |sta|                          address[35:3]                          |       0       | src id  |   0   | PL2   | diff timestamp|
+ * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
+ * |sta|                          address[35:3]                          |       0       | src id  |   0   | PSL1  | diff timestamp|
+ * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
+ * |sta|                          address[35:3]                          |       0       | src id  |   0   | LDD   | diff timestamp|
+ * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
+ * |sta|                          address[35:3]                          |       0       | src id  |   0   | LDI   | diff timestamp|
+ * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
+ * |sta|                          address[35:3]                          |       0       | src id  |   0   | LDT   | diff timestamp|
+ * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
+ * |sta|                          address[35:3]                          | * or 16B mask | src id  |   0   | STC   | diff timestamp|
+ * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
+ * |sta|                          address[35:3]                          | * or 16B mask | src id  |   0   | STF   | diff timestamp|
+ * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
+ * |sta|                          address[35:3]                          | * or 16B mask | src id  |   0   | STP   | diff timestamp|
+ * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
+ * |sta|                          address[35:3]                          | * or 16B mask | src id  |   0   | STT   | diff timestamp|
+ * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
+ * |sta|                          address[35:0]                                |    0    | src id| dest id |IOBLD8 | diff timestamp|
+ * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
+ * |sta|                          address[35:1]                              |     0     | src id| dest id |IOBLD16| diff timestamp|
+ * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
+ * |sta|                          address[35:2]                            |      0      | src id| dest id |IOBLD32| diff timestamp|
+ * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
+ * |sta|                          address[35:3]                          |       0       | src id| dest id |IOBLD64| diff timestamp|
+ * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
+ * |sta|                          address[35:3]                          | * or 16B mask | src id| dest id |IOBST  | diff timestamp|
+ * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
+ * |sta|                     * or address[35:3]                          | * or length   | src id| dest id |IOBDMA | diff timestamp|
+ * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
+ *
+ * notes:
+ * - Fields marked as '*' are first filled with '0' at XMC time and may be filled with real data later at XMD time.  Note that the
+ * XMD write may be dropped if the shallow FIFO overflows which leaves the '*' fields as '0'.
+ * - 2 bits (sta) are used not to trace, but to return global state information with each read, encoded as follows:
+ * 0x0-0x1=not valid
+ * 0x2=valid, no discontinuity
+ * 0x3=valid,    discontinuity
+ * - commands are encoded as follows:
+ * 0x0=DWB
+ * 0x1=PL2
+ * 0x2=PSL1
+ * 0x3=LDD
+ * 0x4=LDI
+ * 0x5=LDT
+ * 0x6=STC
+ * 0x7=STF
+ * 0x8=STP
+ * 0x9=STT
+ * 0xa=IOBLD8
+ * 0xb=IOBLD16
+ * 0xc=IOBLD32
+ * 0xd=IOBLD64
+ * 0xe=IOBST
+ * 0xf=IOBDMA
+ * - For non IOB* commands
+ * - source id is encoded as follows:
+ * 0x00-0x0f=PP[n]
+ * 0x10=IOB(Packet)
+ * 0x11=IOB(PKO)
+ * 0x12=IOB(ReqLoad, ReqStore)
+ * 0x13=IOB(DWB)
+ * 0x14-0x1e=illegal
+ * 0x1f=IOB(generic)
+ * - dest   id is unused (can only be L2c)
+ * - For IOB* commands
+ * - source id is encoded as follows:
+ * 0x00-0x0f = PP[n]
+ * - dest   id is encoded as follows:
+ * 0x00-0x0f=PP[n]
+ * 0x10=IOB(Packet)
+ * 0x11=IOB(PKO)
+ * 0x12=IOB(ReqLoad, ReqStore)
+ * 0x13=IOB(DWB)
+ * 0x14-0x1e=illegal
+ * 0x1f=IOB(generic)
+ *
+ * Source of data for each command
+ * command source id    dest id      address                 length/mask
+ * -------+------------+------------+-----------------------+----------------------------------------------
+ * LDI     xmc_sid[8:3] x            xmc_adr[35:3]           x
+ * LDT     xmc_sid[8:3] x            xmc_adr[35:3]           x
+ * STF     xmc_sid[8:3] x            xmc_adr[35:3]           16B mask(xmd_[wrval,eow,adr[6:4],wrmsk[15:0]])
+ * STC     xmc_sid[8:3] x            xmc_adr[35:3]           16B mask(xmd_[wrval,eow,adr[6:4],wrmsk[15:0]])
+ * STP     xmc_sid[8:3] x            xmc_adr[35:3]           16B mask(xmd_[wrval,eow,adr[6:4],wrmsk[15:0]])
+ * STT     xmc_sid[8:3] x            xmc_adr[35:3]           16B mask(xmd_[wrval,eow,adr[6:4],wrmsk[15:0]])
+ * DWB     xmc_sid[8:3] x            xmc_adr[35:3]           x
+ * PL2     xmc_sid[8:3] x            xmc_adr[35:3]           x
+ * PSL1    xmc_sid[8:3] x            xmc_adr[35:3]           x
+ * IOBLD8  xmc_sid[8:3] xmc_did[8:3] xmc_adr[35:0]           x
+ * IOBLD16 xmc_sid[8:3] xmc_did[8:3] xmc_adr[35:1]           x
+ * IOBLD32 xmc_sid[8:3] xmc_did[8:3] xmc_adr[35:2]           x
+ * IOBLD64 xmc_sid[8:3] xmc_did[8:3] xmc_adr[35:3]           x
+ * IOBST   xmc_sid[8:3] xmc_did[8:3] xmc_adr[35:3]           16B mask(xmd_[wrval,eow,adr[6:4],wrmsk[15:0]])
+ * IOBDMA  xmc_sid[8:3] xmc_did[8:3] (xmd_[wrval,eow,dat[]]) length(xmd_[wrval,eow,dat[]])
+ * IOBRSP  not traced, but monitored to keep XMC and XMD data in sync.
+ * @endverbatim
+ *
+ * <hr>$Revision: 32641 $<hr>
+ */
+
+#ifndef __CVMX_TRA_H__
+#define __CVMX_TRA_H__
+
+#include "cvmx.h"
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+
+/* CSR typedefs have been moved to cvmx-csr-*.h */
+
+/**
+ * Enumeration of the data types stored in cvmx_tra_data_t
+ */
+typedef enum
+{
+    CVMX_TRA_DATA_DWB       = 0x0,
+    CVMX_TRA_DATA_PL2       = 0x1,
+    CVMX_TRA_DATA_PSL1      = 0x2,
+    CVMX_TRA_DATA_LDD       = 0x3,
+    CVMX_TRA_DATA_LDI       = 0x4,
+    CVMX_TRA_DATA_LDT       = 0x5,
+    CVMX_TRA_DATA_STC       = 0x6,
+    CVMX_TRA_DATA_STF       = 0x7,
+    CVMX_TRA_DATA_STP       = 0x8,
+    CVMX_TRA_DATA_STT       = 0x9,
+    CVMX_TRA_DATA_IOBLD8    = 0xa,
+    CVMX_TRA_DATA_IOBLD16   = 0xb,
+    CVMX_TRA_DATA_IOBLD32   = 0xc,
+    CVMX_TRA_DATA_IOBLD64   = 0xd,
+    CVMX_TRA_DATA_IOBST     = 0xe,
+    CVMX_TRA_DATA_IOBDMA    = 0xf,
+    CVMX_TRA_DATA_SAA       = 0x10,
+} cvmx_tra_data_type_t;
+
+/**
+ * TRA data format definition. Use the type field to
+ * determine which union element to use.
+ */
+typedef union
+{
+    uint64_t u64;
+    struct
+    {
+#if __BYTE_ORDER == __BIG_ENDIAN
+        uint64_t    valid       : 1;
+        uint64_t    discontinuity:1;
+        uint64_t    address     : 36;
+        uint64_t    reserved    : 5;
+        uint64_t    source      : 5;
+        uint64_t    reserved2   : 3;
+        cvmx_tra_data_type_t type:5;
+        uint64_t    timestamp   : 8;
+#else
+        uint64_t    timestamp   : 8;
+        cvmx_tra_data_type_t type:5;
+        uint64_t    reserved2   : 3;
+        uint64_t    source      : 5;
+        uint64_t    reserved    : 5;
+        uint64_t    address     : 36;
+        uint64_t    discontinuity:1;
+        uint64_t    valid       : 1;
+#endif
+    } cmn; /**< for DWB, PL2, PSL1, LDD, LDI, LDT */
+    struct
+    {
+#if __BYTE_ORDER == __BIG_ENDIAN
+        uint64_t    valid       : 1;
+        uint64_t    discontinuity:1;
+        uint64_t    address     : 33;
+        uint64_t    mask        : 8;
+        uint64_t    source      : 5;
+        uint64_t    reserved2   : 3;
+        cvmx_tra_data_type_t type:5;
+        uint64_t    timestamp   : 8;
+#else
+        uint64_t    timestamp   : 8;
+        cvmx_tra_data_type_t type:5;
+        uint64_t    reserved2   : 3;
+        uint64_t    source      : 5;
+        uint64_t    mask        : 8;
+        uint64_t    address     : 33;
+        uint64_t    discontinuity:1;
+        uint64_t    valid       : 1;
+#endif
+    } store; /**< STC, STF, STP, STT */
+    struct
+    {
+#if __BYTE_ORDER == __BIG_ENDIAN
+        uint64_t    valid       : 1;
+        uint64_t    discontinuity:1;
+        uint64_t    address     : 36;
+        uint64_t    reserved    : 2;
+        uint64_t    subid       : 3;
+        uint64_t    source      : 4;
+        uint64_t    dest        : 5;
+        uint64_t    type        : 4;
+        uint64_t    timestamp   : 8;
+#else
+        uint64_t    timestamp   : 8;
+        uint64_t    type        : 4;
+        uint64_t    dest        : 5;
+        uint64_t    source      : 4;
+        uint64_t    subid       : 3;
+        uint64_t    reserved    : 2;
+        uint64_t    address     : 36;
+        uint64_t    discontinuity:1;
+        uint64_t    valid       : 1;
+#endif
+    } iobld; /**< for IOBLD8, IOBLD16, IOBLD32, IOBLD64, IOBST, SAA */
+    struct
+    {
+#if __BYTE_ORDER == __BIG_ENDIAN
+        uint64_t    valid       : 1;
+        uint64_t    discontinuity:1;
+        uint64_t    address     : 33;
+        uint64_t    mask        : 8;
+        uint64_t    source      : 4;
+        uint64_t    dest        : 5;
+        uint64_t    type        : 4;
+        uint64_t    timestamp   : 8;
+#else
+        uint64_t    timestamp   : 8;
+        uint64_t    type        : 4;
+        uint64_t    dest        : 5;
+        uint64_t    source      : 4;
+        uint64_t    mask        : 8;
+        uint64_t    address     : 33;
+        uint64_t    discontinuity:1;
+        uint64_t    valid       : 1;
+#endif
+    } iob; /**< for IOBDMA */
+} cvmx_tra_data_t;
+
+
+/**
+ * Setup the TRA buffer for use
+ *
+ * @param control TRA control setup
+ * @param filter  Which events to log
+ * @param source_filter
+ *                Source match
+ * @param dest_filter
+ *                Destination match
+ * @param address Address compare
+ * @param address_mask
+ *                Address mask
+ */
+extern void cvmx_tra_setup(cvmx_tra_ctl_t control, cvmx_tra_filt_cmd_t filter,
+                           cvmx_tra_filt_sid_t source_filter, cvmx_tra_filt_did_t dest_filter,
+                           uint64_t address, uint64_t address_mask);
+
+/**
+ * Setup a TRA trigger. How the triggers are used should be
+ * setup using cvmx_tra_setup.
+ *
+ * @param trigger Trigger to setup (0 or 1)
+ * @param filter  Which types of events to trigger on
+ * @param source_filter
+ *                Source trigger match
+ * @param dest_filter
+ *                Destination trigger match
+ * @param address Trigger address compare
+ * @param address_mask
+ *                Trigger address mask
+ */
+extern void cvmx_tra_trig_setup(uint64_t trigger, cvmx_tra_filt_cmd_t filter,
+                                cvmx_tra_filt_sid_t source_filter, cvmx_tra_trig0_did_t dest_filter,
+                                uint64_t address, uint64_t address_mask);
+
+/**
+ * Read an entry from the TRA buffer
+ *
+ * @return Value return. High bit will be zero if there wasn't any data
+ */
+extern cvmx_tra_data_t cvmx_tra_read(void);
+
+/**
+ * Decode a TRA entry into human readable output
+ *
+ * @param tra_ctl Trace control setup
+ * @param data    Data to decode
+ */
+extern void cvmx_tra_decode_text(cvmx_tra_ctl_t tra_ctl, cvmx_tra_data_t data);
+
+/**
+ * Display the entire trace buffer. It is advised that you
+ * disable the trace buffer before calling this routine
+ * otherwise it could infinitely loop displaying trace data
+ * that it created.
+ */
+extern void cvmx_tra_display(void);
+
+/**
+ * Enable or disable the TRA hardware
+ *
+ * @param enable 1=enable, 0=disable
+ */
+static inline void cvmx_tra_enable(int enable)
+{
+    cvmx_tra_ctl_t control;
+    control.u64 = cvmx_read_csr(CVMX_TRA_CTL);
+    control.s.ena = enable;
+    cvmx_write_csr(CVMX_TRA_CTL, control.u64);
+    cvmx_read_csr(CVMX_TRA_CTL);
+}
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-twsi.c b/arch/mips/cavium-octeon/executive/cvmx-twsi.c
new file mode 100644
index 0000000..0521d80
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-twsi.c
@@ -0,0 +1,157 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Interface to the TWSI / I2C bus
+ *
+ * <hr>$Revision: 34496 $<hr>
+ *
+ */
+
+#include "cvmx.h"
+#include "cvmx-twsi.h"
+
+
+/**
+ * Read 8-bit from a device on the TWSI / I2C bus
+ *
+ * @param twsi_id  Which TWSI bus to use. CN3XXX, CN58XX, and CN50XX only
+ *                 support 0. CN56XX and CN57XX support 0-1.
+ * @param dev_addr I2C device address (7 bit)
+ * @param internal_addr
+ *                 Internal device address
+ *
+ * @return 8-bit data or < 0 in case of error
+ */
+int cvmx_twsix_read8(int twsi_id, uint8_t dev_addr, uint8_t internal_addr)
+{
+    /* 8 bit internal address */
+    cvmx_mio_twsx_sw_twsi_t sw_twsi_val;
+
+    sw_twsi_val.u64 = 0;
+    sw_twsi_val.s.v = 1;
+    sw_twsi_val.s.a = dev_addr;
+    sw_twsi_val.s.d = internal_addr;
+    cvmx_write_csr(CVMX_MIO_TWSX_SW_TWSI(twsi_id), sw_twsi_val.u64);
+    while (((cvmx_mio_twsx_sw_twsi_t)(sw_twsi_val.u64 = cvmx_read_csr(CVMX_MIO_TWSX_SW_TWSI(twsi_id)))).s.v)
+        ;
+    if (!sw_twsi_val.s.r)
+        return -1;
+
+    return cvmx_twsix_read8_cur_addr(twsi_id, dev_addr);
+}
+
+/**
+ * Read 8-bit from a device on the TWSI / I2C bus
+ *
+ * Uses current internal address
+ *
+ * @param twsi_id  Which TWSI bus to use. CN3XXX, CN58XX, and CN50XX only
+ *                 support 0. CN56XX and CN57XX support 0-1.
+ * @param dev_addr I2C device address (7 bit)
+ *
+ * @return 8-bit value or < 0 in case of error
+ */
+int cvmx_twsix_read8_cur_addr(int twsi_id, uint8_t dev_addr)
+{
+    cvmx_mio_twsx_sw_twsi_t sw_twsi_val;
+
+    sw_twsi_val.u64 = 0;
+    sw_twsi_val.s.v = 1;
+    sw_twsi_val.s.a = dev_addr;
+    sw_twsi_val.s.r = 1;
+
+    cvmx_write_csr(CVMX_MIO_TWSX_SW_TWSI(twsi_id), sw_twsi_val.u64);
+    while (((cvmx_mio_twsx_sw_twsi_t)(sw_twsi_val.u64 = cvmx_read_csr(CVMX_MIO_TWSX_SW_TWSI(twsi_id)))).s.v)
+        ;
+    if (!sw_twsi_val.s.r)
+        return -1;
+    else
+        return(sw_twsi_val.s.d & 0xFF);
+}
+
+/**
+ * Write 8-bit to a device on the TWSI / I2C bus
+ *
+ * @param twsi_id  Which TWSI bus to use. CN3XXX, CN58XX, and CN50XX only
+ *                 support 0. CN56XX and CN57XX support 0-1.
+ * @param dev_addr I2C device address (7 bit)
+ * @param internal_addr
+ *                 Internal device address
+ * @param data     Data to be written
+ *
+ * @return 0 on success and < 0 in case of error
+ */
+int cvmx_twsix_write8(int twsi_id, uint8_t dev_addr, uint8_t internal_addr, uint8_t data)
+{
+    /* 8 bit internal address */
+    cvmx_mio_twsx_sw_twsi_t sw_twsi_val;
+
+    sw_twsi_val.u64 = 0;
+    sw_twsi_val.s.v = 1;
+    sw_twsi_val.s.op = 1;
+    sw_twsi_val.s.a = dev_addr;
+    sw_twsi_val.s.ia = internal_addr >> 3;
+    sw_twsi_val.s.eop_ia = internal_addr & 0x7;
+    sw_twsi_val.s.d = data;
+
+    cvmx_write_csr(CVMX_MIO_TWSX_SW_TWSI(twsi_id), sw_twsi_val.u64);
+    while (((cvmx_mio_twsx_sw_twsi_t)(sw_twsi_val.u64 = cvmx_read_csr(CVMX_MIO_TWSX_SW_TWSI(twsi_id)))).s.v)
+        ;
+    if (!sw_twsi_val.s.r)
+        return -1;
+
+    return 0;
+}
+
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-twsi.h b/arch/mips/cavium-octeon/executive/cvmx-twsi.h
new file mode 100644
index 0000000..5c74268
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-twsi.h
@@ -0,0 +1,163 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Interface to the TWSI / I2C bus
+ *
+ * <hr>$Revision: 32636 $<hr>
+ *
+ */
+
+#ifndef __CVMX_TWSI_H__
+#define __CVMX_TWSI_H__
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+/**
+ * Read 8-bit from a device on the TWSI / I2C bus
+ *
+ * @param twsi_id  Which TWSI bus to use. CN3XXX, CN58XX, and CN50XX only
+ *                 support 0. CN56XX and CN57XX support 0-1.
+ * @param dev_addr I2C device address (7 bit)
+ * @param internal_addr
+ *                 Internal device address
+ *
+ * @return 8-bit data or < 0 in case of error
+ */
+int cvmx_twsix_read8(int twsi_id, uint8_t dev_addr, uint8_t internal_addr);
+
+/**
+ * Read 8-bit from a device on the TWSI / I2C bus
+ *
+ * Uses current internal address
+ *
+ * @param twsi_id  Which TWSI bus to use. CN3XXX, CN58XX, and CN50XX only
+ *                 support 0. CN56XX and CN57XX support 0-1.
+ * @param dev_addr I2C device address (7 bit)
+ *
+ * @return 8-bit value or < 0 in case of error
+ */
+int cvmx_twsix_read8_cur_addr(int twsi_id, uint8_t dev_addr);
+
+/**
+ * Write 8-bit to a device on the TWSI / I2C bus
+ *
+ * @param twsi_id  Which TWSI bus to use. CN3XXX, CN58XX, and CN50XX only
+ *                 support 0. CN56XX and CN57XX support 0-1.
+ * @param dev_addr I2C device address (7 bit)
+ * @param internal_addr
+ *                 Internal device address
+ * @param data     Data to be written
+ *
+ * @return 0 on success and < 0 in case of error
+ */
+int cvmx_twsix_write8(int twsi_id, uint8_t dev_addr, uint8_t internal_addr, uint8_t data);
+
+/**
+ * Read 8-bit from a device on the TWSI / I2C bus zero.
+ *
+ * This function is for compatibility with SDK 1.6.0 and
+ * before which only supported a single TWSI bus.
+ *
+ * @param dev_addr I2C device address (7 bit)
+ * @param internal_addr
+ *                 Internal device address
+ *
+ * @return 8-bit data or < 0 in case of error
+ */
+static inline int cvmx_twsi_read8(uint8_t dev_addr, uint8_t internal_addr)
+{
+    return cvmx_twsix_read8(0, dev_addr, internal_addr);
+}
+
+/**
+ * Read 8-bit from a device on the TWSI / I2C bus zero.
+ *
+ * Uses current internal address
+ *
+ * This function is for compatibility with SDK 1.6.0 and
+ * before which only supported a single TWSI bus.
+ *
+ * @param dev_addr I2C device address (7 bit)
+ *
+ * @return 8-bit value or < 0 in case of error
+ */
+static inline int cvmx_twsi_read8_cur_addr(uint8_t dev_addr)
+{
+    return cvmx_twsix_read8_cur_addr(0, dev_addr);
+}
+
+/**
+ * Write 8-bit to a device on the TWSI / I2C bus zero.
+ * This function is for compatibility with SDK 1.6.0 and
+ * before which only supported a single TWSI bus.
+ *
+ * @param dev_addr I2C device address (7 bit)
+ * @param internal_addr
+ *                 Internal device address
+ * @param data     Data to be written
+ *
+ * @return 0 on success and < 0 in case of error
+ */
+static inline int cvmx_twsi_write8(uint8_t dev_addr, uint8_t internal_addr, uint8_t data)
+{
+    return cvmx_twsix_write8(0, dev_addr, internal_addr, data);
+}
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif  /*  __CVMX_TWSI_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-uart.h b/arch/mips/cavium-octeon/executive/cvmx-uart.h
new file mode 100644
index 0000000..94930db
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-uart.h
@@ -0,0 +1,81 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * interface to the serial port UART hardware
+ *
+ * <hr>$Revision: 32636 $<hr>
+ *
+ */
+
+#ifndef __CVMX_UART_H__
+#define __CVMX_UART_H__
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#define CVMX_UART_NUM_PORTS     2
+#define CVMX_UART_TX_FIFO_SIZE  64
+#define CVMX_UART_RX_FIFO_SIZE  64
+
+/* CSR typedefs have been moved to cvmx-csr-*.h */
+
+/* Defined in libc.  */
+unsigned __octeon_uart_trylock (void);
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif /*  __CVM_UART_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-usb.c b/arch/mips/cavium-octeon/executive/cvmx-usb.c
new file mode 100644
index 0000000..d3ae666
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-usb.c
@@ -0,0 +1,3481 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * "cvmx-usb.c" defines a set of low level USB functions to help
+ * developers create Octeon USB drivers for various operating
+ * systems. These functions provide a generic API to the Octeon
+ * USB blocks, hiding the internal hardware specific
+ * operations.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+#include "cvmx.h"
+#include "cvmx-sysinfo.h"
+#include "cvmx-usb.h"
+#include "cvmx-csr-db.h"
+#include "cvmx-swap.h"
+
+#define MAX_RETRIES         3   /* Maximum number of times to retry failed transactions */
+#define MAX_PIPES           32  /* Maximum number of pipes that can be open at once */
+#define MAX_TRANSACTIONS    256 /* Maximum number of outstanding transactions across all pipes */
+#define MAX_CHANNELS        8   /* Maximum number of hardware channels supported by the USB block */
+#define MAX_USB_ADDRESS     127 /* The highest valid USB device address */
+#define MAX_USB_ENDPOINT    15  /* The highest valid USB endpoint number */
+#define MAX_USB_HUB_PORT    15  /* The highest valid port number on a hub */
+
+/* These defines disable the normal read and write csr. This is so I can add
+    extra debug stuff to the usb specific version and I won't use the normal
+    version by mistake */
+#define cvmx_read_csr use_cvmx_usb_read_csr64_instead_of_cvmx_read_csr
+#define cvmx_write_csr use_cvmx_usb_write_csr64_instead_of_cvmx_write_csr
+
+typedef enum
+{
+    __CVMX_USB_TRANSACTION_FLAGS_IN_USE = 1<<16,
+    __CVMX_USB_TRANSACTION_FLAGS_CANCEL = 1<<17,
+} cvmx_usb_transaction_flags_t;
+
+/**
+ * Logical transactions may take numerous low level
+ * transactions, especially when splits are concerned. This
+ * enum represents all of the possible stages a transaction can
+ * be in. Note that split completes are always even. This is so
+ * the NAK handler can backup to the previous low level
+ * transaction with a simple clearing of bit 0.
+ */
+typedef enum
+{
+    CVMX_USB_STAGE_NON_CONTROL,
+    CVMX_USB_STAGE_NON_CONTROL_SPLIT_COMPLETE,
+    CVMX_USB_STAGE_SETUP,
+    CVMX_USB_STAGE_SETUP_SPLIT_COMPLETE,
+    CVMX_USB_STAGE_DATA,
+    CVMX_USB_STAGE_DATA_SPLIT_COMPLETE,
+    CVMX_USB_STAGE_STATUS,
+    CVMX_USB_STAGE_STATUS_SPLIT_COMPLETE,
+} cvmx_usb_stage_t;
+
+/**
+ * This structure describes each pending USB transaction
+ * regardless of type. These are linked together to form a list
+ * of pending requests for a pipe.
+ */
+typedef struct cvmx_usb_transaction
+{
+    struct cvmx_usb_transaction *prev;  /**< Transaction before this one in the pipe */
+    struct cvmx_usb_transaction *next;  /**< Transaction after this one in the pipe */
+    cvmx_usb_transfer_t type;           /**< Type of transaction, duplicated of the pipe */
+    cvmx_usb_transaction_flags_t flags; /**< State flags for this transaction */
+    uint64_t buffer;                    /**< User's physical buffer address to read/write */
+    int buffer_length;                  /**< Size of the user's buffer in bytes */
+    uint64_t control_header;            /**< For control transactions, physical address of the 8 byte standard header */
+    int iso_start_frame;                /**< For ISO transactions, the starting frame number */
+    int iso_number_packets;             /**< For ISO transactions, the number of packets in the request */
+    cvmx_usb_iso_packet_t *iso_packets; /**< For ISO transactions, the sub packets in the request */
+    int xfersize;
+    int pktcnt;
+    int retries;
+    int actual_bytes;                   /**< Actual bytes transfer for this transaction */
+    cvmx_usb_stage_t stage;             /**< For control transactions, the current stage */
+    cvmx_usb_callback_func_t callback;  /**< User's callback function when complete */
+    void *callback_data;                /**< User's data */
+} cvmx_usb_transaction_t;
+
+/**
+ * A pipe represents a virtual connection between Octeon and some
+ * USB device. It contains a list of pending request to the device.
+ */
+typedef struct
+{
+    cvmx_usb_transaction_t *head;       /**< The first pending transaction */
+    cvmx_usb_transaction_t *tail;       /**< The last pending transaction */
+    uint64_t interval;                  /**< For periodic pipes, the interval between packets in cycles */
+    uint64_t next_tx_cycle;             /**< The next cycle this pipe is allowed to transmit on */
+    cvmx_usb_pipe_flags_t flags;        /**< State flags for this pipe */
+    cvmx_usb_speed_t device_speed;      /**< Speed of device connected to this pipe */
+    cvmx_usb_transfer_t transfer_type;  /**< Type of transaction supported by this pipe */
+    cvmx_usb_direction_t transfer_dir;  /**< IN or OUT. Ignored for Control */
+    int multi_count;                    /**< Max packet in a row for the device */
+    uint16_t max_packet;                /**< The device's maximum packet size in bytes */
+    uint8_t device_addr;                /**< USB device address at other end of pipe */
+    uint8_t endpoint_num;               /**< USB endpoint number at other end of pipe */
+    uint8_t hub_device_addr;            /**< Hub address this device is connected to */
+    uint8_t hub_port;                   /**< Hub port this device is connected to */
+    uint8_t pid_toggle;                 /**< This toggles between 0/1 on every packet send to track the data pid needed */
+    uint8_t channel;                    /**< Hardware DMA channel for this pipe */
+} cvmx_usb_pipe_t;
+
+/**
+ * The state of the USB block is stored in this structure
+ */
+typedef struct
+{
+    int init_flags;                     /**< Flags passed to initialize */
+    int index;                          /**< Which USB block this is for */
+    int idle_hardware_channels;         /**< Bit set for every idle hardware channel */
+    int number_periodic_transactions;   /**< The number of periodic transactions scheduled. Used to determine when SOF interrupts are needed */
+    cvmx_usb_pipe_t *pipe_for_channel[MAX_CHANNELS];    /**< Map channels to pipes */
+    cvmx_usb_transaction_t *free_transaction_head;      /**< List of free transactions head */
+    cvmx_usb_transaction_t *free_transaction_tail;      /**< List of free transactions tail */
+    cvmx_usb_pipe_t pipe[MAX_PIPES];                    /**< Storage for pipes */
+    cvmx_usb_transaction_t transaction[MAX_TRANSACTIONS];       /**< Storage for transactions */
+    cvmx_usb_callback_func_t callback[__CVMX_USB_CALLBACK_END]; /**< User global callbacks */
+    void *callback_data[__CVMX_USB_CALLBACK_END];               /**< User data for each callback */
+    int indent;                         /**< Used by debug output to indent functions */
+    cvmx_usb_port_status_t port_status; /**< Last port status used for change notification */
+} cvmx_usb_internal_state_t;
+
+/* This macro logs out whenever a function is called if debugging is on */
+#define CVMX_USB_LOG_CALLED() \
+    if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_CALLS) \
+        cvmx_dprintf("%*s%s: called\n", 2*usb->indent++, "", __FUNCTION__);
+
+/* This macro logs out each function parameter if debugging is on */
+#define CVMX_USB_LOG_PARAM(format, param) \
+    if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_CALLS) \
+        cvmx_dprintf("%*s%s: param %s = " format "\n", 2*usb->indent, "", __FUNCTION__, #param, param);
+
+/* This macro logs out when a function returns a value */
+#define CVMX_USB_RETURN(v)                                              \
+    do {                                                                \
+        typeof(v) r = v;                                                \
+        if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_CALLS)    \
+            cvmx_dprintf("%*s%s: returned %s(%d)\n", 2*--usb->indent, "", __FUNCTION__, #v, r); \
+        return r;                                                       \
+    } while (0);
+
+/* This macro logs out when a function doesn't return a value */
+#define CVMX_USB_RETURN_NOTHING()                                       \
+    do {                                                                \
+        if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_CALLS)    \
+            cvmx_dprintf("%*s%s: returned\n", 2*--usb->indent, "", __FUNCTION__); \
+        return;                                                         \
+    } while (0);
+
+/* This macro spins on a field waiting for it to reach a value */
+#define CVMX_WAIT_FOR_FIELD32(address, type, field, op, value, timeout_usec)\
+    ({int result;                                                       \
+    do {                                                                \
+        uint64_t done = cvmx_get_cycle() + (uint64_t)timeout_usec *     \
+                           cvmx_sysinfo_get()->cpu_clock_hz / 1000000;  \
+        type c;                                                         \
+        while (1)                                                       \
+        {                                                               \
+            c.u32 = __cvmx_usb_read_csr32(usb, address);                \
+            if (c.s.field op (value)) {                                 \
+                result = 0;                                             \
+                break;                                                  \
+            } else if (cvmx_get_cycle() > done) {                       \
+                result = -1;                                            \
+                break;                                                  \
+            } else                                                      \
+                cvmx_wait(100);                                         \
+        }                                                               \
+    } while (0);                                                        \
+    result;})
+
+/* This macro logically sets a single field in a CSR. It does the sequence
+    read, modify, and write */
+#define USB_SET_FIELD32(address, type, field, value)\
+    do {                                            \
+        type c;                                     \
+        c.u32 = __cvmx_usb_read_csr32(usb, address);\
+        c.s.field = value;                          \
+        __cvmx_usb_write_csr32(usb, address, c.u32);\
+    } while (0)
+
+
+/**
+ * @INTERNAL
+ * Read a USB 32bit CSR. It performs the necessary address swizzle
+ * for 32bit CSRs and logs the value in a readable format if
+ * debugging is on.
+ *
+ * @param usb     USB block this access is for
+ * @param address 64bit address to read
+ *
+ * @return Result of the read
+ */
+static inline uint32_t __cvmx_usb_read_csr32(cvmx_usb_internal_state_t *usb,
+                                             uint64_t address)
+{
+    uint32_t result = cvmx_read64_uint32(address ^ 4);
+    if (cvmx_unlikely(usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_CSRS))
+    {
+        cvmx_dprintf("Read: ");
+        cvmx_csr_db_decode(cvmx_get_proc_id(), address, result);
+    }
+    return result;
+}
+
+
+/**
+ * @INTERNAL
+ * Write a USB 32bit CSR. It performs the necessary address
+ * swizzle for 32bit CSRs and logs the value in a readable format
+ * if debugging is on.
+ *
+ * @param usb     USB block this access is for
+ * @param address 64bit address to write
+ * @param value   Value to write
+ */
+static inline void __cvmx_usb_write_csr32(cvmx_usb_internal_state_t *usb,
+                                          uint64_t address, uint32_t value)
+{
+    if (cvmx_unlikely(usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_CSRS))
+    {
+        cvmx_dprintf("Write: ");
+        cvmx_csr_db_decode(cvmx_get_proc_id(), address, value);
+    }
+    cvmx_write64_uint32(address ^ 4, value);
+    /* O2P/O1P pass 1 bug workaround: A read must occur for at least
+       every 3rd write to insure that the writes do not overrun the
+       USBN. */
+    cvmx_read64_uint64(CVMX_USBNX_DMA0_INB_CHN0(usb->index));
+}
+
+
+/**
+ * @INTERNAL
+ * Read a USB 64bit CSR. It logs the value in a readable format if
+ * debugging is on.
+ *
+ * @param usb     USB block this access is for
+ * @param address 64bit address to read
+ *
+ * @return Result of the read
+ */
+static inline uint64_t __cvmx_usb_read_csr64(cvmx_usb_internal_state_t *usb,
+                                             uint64_t address)
+{
+    uint64_t result = cvmx_read64_uint64(address);
+    if (cvmx_unlikely(usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_CSRS))
+    {
+        cvmx_dprintf("Read: ");
+        cvmx_csr_db_decode(cvmx_get_proc_id(), address, result);
+    }
+    return result;
+}
+
+
+/**
+ * @INTERNAL
+ * Write a USB 64bit CSR. It logs the value in a readable format
+ * if debugging is on.
+ *
+ * @param usb     USB block this access is for
+ * @param address 64bit address to write
+ * @param value   Value to write
+ */
+static inline void __cvmx_usb_write_csr64(cvmx_usb_internal_state_t *usb,
+                                          uint64_t address, uint64_t value)
+{
+    if (cvmx_unlikely(usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_CSRS))
+    {
+        cvmx_dprintf("Write: ");
+        cvmx_csr_db_decode(cvmx_get_proc_id(), address, value);
+    }
+    cvmx_write64_uint64(address, value);
+}
+
+
+/**
+ * @INTERNAL
+ * Uitility function to convert complete codes into strings
+ *
+ * @param complete_code
+ *               Code to convert
+ *
+ * @return Human readable string
+ */
+static const char *__cvmx_usb_complete_to_string(cvmx_usb_complete_t complete_code)
+{
+    switch (complete_code)
+    {
+        case CVMX_USB_COMPLETE_SUCCESS: return "SUCCESS";
+        case CVMX_USB_COMPLETE_SHORT:   return "SHORT";
+        case CVMX_USB_COMPLETE_CANCEL:  return "CANCEL";
+        case CVMX_USB_COMPLETE_ERROR:   return "ERROR";
+        case CVMX_USB_COMPLETE_STALL:   return "STALL";
+        case CVMX_USB_COMPLETE_XACTERR: return "XACTERR";
+        case CVMX_USB_COMPLETE_DATATGLERR: return "DATATGLERR";
+        case CVMX_USB_COMPLETE_BABBLEERR: return "BABBLEERR";
+        case CVMX_USB_COMPLETE_FRAMEERR: return "FRAMEERR";
+    }
+    return "Update __cvmx_usb_complete_to_string";
+}
+
+
+/**
+ * @INTERNAL
+ * Return non zero if this pipe connects to a non HIGH speed
+ * device through a high speed hub.
+ *
+ * @param usb    USB block this access is for
+ * @param pipe   Pipe to check
+ *
+ * @return Non zero if we need to do split transactions
+ */
+static inline int __cvmx_usb_pipe_needs_split(cvmx_usb_internal_state_t *usb, cvmx_usb_pipe_t *pipe)
+{
+    cvmx_usbcx_hprt_t usbc_hprt;
+    usbc_hprt.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_HPRT(usb->index));
+    return ((pipe->device_speed != CVMX_USB_SPEED_HIGH) && (usbc_hprt.s.prtspd == CVMX_USB_SPEED_HIGH));
+}
+
+
+/**
+ * @INTERNAL
+ * Trivial utility function to return the correct PID for a pipe
+ *
+ * @param pipe   pipe to check
+ *
+ * @return PID for pipe
+ */
+static inline int __cvmx_usb_get_data_pid(cvmx_usb_pipe_t *pipe)
+{
+    if (pipe->pid_toggle)
+        return 2; /* Data1 */
+    else
+        return 0; /* Data0 */
+}
+
+
+/**
+ * Return the number of USB ports supported by this Octeon
+ * chip. If the chip doesn't support USB, or is not supported
+ * by this API, a zero will be returned. Most Octeon chips
+ * support one usb port, but some support two ports.
+ * cvmx_usb_initialize() must be called on independent
+ * cvmx_usb_state_t structures.
+ *
+ * @return Number of port, zero if usb isn't supported
+ */
+int cvmx_usb_get_num_ports(void)
+{
+    if (OCTEON_IS_MODEL(OCTEON_CN52XX))
+        return 2;
+    else if (OCTEON_IS_MODEL(OCTEON_CN31XX))
+        return 0; /* This chip has USB but it doesn't support DMA */
+    else if (OCTEON_IS_MODEL(OCTEON_CN38XX) || OCTEON_IS_MODEL(OCTEON_CN58XX))
+        return 0;
+    else
+        return 1;
+}
+
+
+/**
+ * @INTERNAL
+ * Allocate a usb transaction for use
+ *
+ * @param usb    USB device state populated by
+ *               cvmx_usb_initialize().
+ *
+ * @return Transaction or NULL
+ */
+static inline cvmx_usb_transaction_t *__cvmx_usb_alloc_transaction(cvmx_usb_internal_state_t *usb)
+{
+    cvmx_usb_transaction_t *t;
+    t = usb->free_transaction_head;
+    if (t)
+    {
+        usb->free_transaction_head = t->next;
+        if (!usb->free_transaction_head)
+            usb->free_transaction_tail = NULL;
+    }
+    else if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_INFO)
+        cvmx_dprintf("%s: Failed to allocate a transaction\n", __FUNCTION__);
+    if (t)
+    {
+        memset(t, 0, sizeof(*t));
+        t->flags = __CVMX_USB_TRANSACTION_FLAGS_IN_USE;
+    }
+    return t;
+}
+
+
+/**
+ * @INTERNAL
+ * Free a usb transaction
+ *
+ * @param usb    USB device state populated by
+ *               cvmx_usb_initialize().
+ * @param transaction
+ *               Transaction to free
+ */
+static inline void __cvmx_usb_free_transaction(cvmx_usb_internal_state_t *usb,
+                                        cvmx_usb_transaction_t *transaction)
+{
+    transaction->flags = 0;
+    transaction->prev = NULL;
+    transaction->next = NULL;
+    if (usb->free_transaction_tail)
+        usb->free_transaction_tail->next = transaction;
+    else
+        usb->free_transaction_head = transaction;
+    usb->free_transaction_tail = transaction;
+}
+
+
+/**
+ * @INTERNAL
+ * Perfrom USB device mode initialization after a reset completes.
+ * This should be called after USBC0/1_GINTSTS[USBRESET] and
+ * coresponds to section 22.6.1.1, "Initialization on USB Reset",
+ * in the manual.
+ *
+ * @param usb    USB device state populated by
+ *               cvmx_usb_initialize().
+ *
+ * @return CVMX_USB_SUCCESS or a negative error code defined in
+ *         cvmx_usb_status_t.
+ */
+static cvmx_usb_status_t __cvmx_usb_device_reset_complete(cvmx_usb_internal_state_t *usb)
+{
+    cvmx_usbcx_ghwcfg3_t usbcx_ghwcfg3;
+    int i;
+
+    CVMX_USB_LOG_CALLED();
+    CVMX_USB_LOG_PARAM("%p", usb);
+
+    /* 1. Set USBC0/1_DOEPCTLn[SNAK] = 1 (for all OUT endpoints, n = 0-4). */
+    for (i=0; i<5; i++)
+    {
+        USB_SET_FIELD32(CVMX_USBCX_DOEPCTLX(i, usb->index),
+                        cvmx_usbcx_doepctlx_t, snak, 1);
+    }
+
+    /* 2. Unmask the following interrupt bits:
+        USBC0/1_DAINTMSK[INEPMSK] = 1 (control 0 IN endpoint)
+        USBC0/1_DAINTMSK[OUTEPMSK] = 1 (control 0 OUT endpoint)
+        USBC0/1_DOEPMSK[SETUPMSK] = 1
+        USBC0/1_DOEPMSK[XFERCOMPLMSK] = 1
+        USBC0/1_DIEPMSK[XFERCOMPLMSK] = 1
+        USBC0/1_DIEPMSK[TIMEOUTMSK] = 1 */
+    USB_SET_FIELD32(CVMX_USBCX_DAINTMSK(usb->index), cvmx_usbcx_daintmsk_t,
+                    inepmsk, 1);
+    USB_SET_FIELD32(CVMX_USBCX_DAINTMSK(usb->index), cvmx_usbcx_daintmsk_t,
+                    outepmsk, 1);
+    USB_SET_FIELD32(CVMX_USBCX_DOEPMSK(usb->index), cvmx_usbcx_doepmsk_t,
+                    setupmsk, 1);
+    USB_SET_FIELD32(CVMX_USBCX_DOEPMSK(usb->index), cvmx_usbcx_doepmsk_t,
+                    xfercomplmsk, 1);
+    USB_SET_FIELD32(CVMX_USBCX_DIEPMSK(usb->index), cvmx_usbcx_diepmsk_t,
+                    xfercomplmsk, 1);
+    USB_SET_FIELD32(CVMX_USBCX_DIEPMSK(usb->index), cvmx_usbcx_diepmsk_t,
+                    timeoutmsk, 1);
+
+    /* 3. To transmit or receive data, the device must initialize more
+        registers as specified in Section 22.6.1.7 */
+    /* Nothing needed */
+
+    /* 4. Set up the data FIFO RAM for each of the FIFOs:
+        Program USBC0/1_GRXFSIZ to be able to receive control OUT data and
+        SETUP data. This must equal at least one maximum packet size of
+        control endpoint 0 + 2 Dwords (for the status of the control OUT
+        data packet) + 10 Dwords (for SETUP packets).
+        Program USBC0/1_GNPTXFSIZ to be able to transmit control IN data. This
+        must equal at least one maximum packet size of control endpoint 0. */
+
+    /* Read the HWCFG3 register so we know how much space is in the FIFO */
+    usbcx_ghwcfg3.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_GHWCFG3(usb->index));
+
+    {
+        cvmx_usbcx_gnptxfsiz_t gnptxfsiz;
+        int fifo_space = usbcx_ghwcfg3.s.dfifodepth;
+        int i;
+
+        /* Start at the top of the FIFO and assign space for each periodic
+            fifo */
+        for (i=4;i>0;i--)
+        {
+            cvmx_usbcx_dptxfsizx_t siz;
+            siz.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_DPTXFSIZX(i, usb->index));
+            fifo_space -= siz.s.dptxfsize;
+            siz.s.dptxfstaddr = fifo_space;
+            __cvmx_usb_write_csr32(usb, CVMX_USBCX_DPTXFSIZX(i, usb->index), siz.u32);
+        }
+
+        /* Assign half the leftover space to the non periodic tx fifo */
+        gnptxfsiz.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_GNPTXFSIZ(usb->index));
+        gnptxfsiz.s.nptxfdep = fifo_space / 2;
+        fifo_space -= gnptxfsiz.s.nptxfdep;
+        gnptxfsiz.s.nptxfstaddr = fifo_space;
+        __cvmx_usb_write_csr32(usb, CVMX_USBCX_GNPTXFSIZ(usb->index), gnptxfsiz.u32);
+
+        /* Assign the remain space to the RX fifo */
+        USB_SET_FIELD32(CVMX_USBCX_GRXFSIZ(usb->index), cvmx_usbcx_grxfsiz_t,
+                        rxfdep, fifo_space);
+    }
+
+    /* 5. Program the following fields in the endpoint-specific registers for
+        control OUT endpoint 0 to receive a SETUP packet
+        USBC0/1_DOEPTSIZ0[SUPCNT] = 0x3 (to receive up to three back-to-back
+        SETUP packets)
+        In DMA mode, USBC0/1_DOEPDMA0 register with a memory address to
+        store any SETUP packets received */
+    USB_SET_FIELD32(CVMX_USBCX_DOEPTSIZX(0, usb->index),
+                    cvmx_usbcx_doeptsizx_t, mc, 3);
+    // FIXME
+
+    /* At this point, all initialization required to receive SETUP packets is
+        done. */
+
+    CVMX_USB_RETURN(CVMX_USB_SUCCESS);
+}
+
+
+/**
+ * Initialize a USB port for use. This must be called before any
+ * other access to the Octeon USB port is made. The port starts
+ * off in the disabled state.
+ *
+ * @param state  Pointer to an empty cvmx_usb_state_t structure
+ *               that will be populated by the initialize call.
+ *               This structure is then passed to all other USB
+ *               functions.
+ * @param usb_port_number
+ *               Which Octeon USB port to initialize.
+ * @param flags  Flags to control hardware initialization. See
+ *               cvmx_usb_initialize_flags_t for the flag
+ *               definitions. Some flags are mandatory.
+ *
+ * @return CVMX_USB_SUCCESS or a negative error code defined in
+ *         cvmx_usb_status_t.
+ */
+cvmx_usb_status_t cvmx_usb_initialize(cvmx_usb_state_t *state,
+                                      int usb_port_number,
+                                      cvmx_usb_initialize_flags_t flags)
+{
+    cvmx_usbnx_clk_ctl_t usbn_clk_ctl;
+    cvmx_usbnx_usbp_ctl_status_t usbn_usbp_ctl_status;
+    cvmx_usb_internal_state_t *usb = (cvmx_usb_internal_state_t*)state;
+
+    usb->init_flags = flags;
+    CVMX_USB_LOG_CALLED();
+    CVMX_USB_LOG_PARAM("%p", state);
+    CVMX_USB_LOG_PARAM("%d", usb_port_number);
+    CVMX_USB_LOG_PARAM("0x%x", flags);
+
+    /* Make sure that state is large enough to store the internal state */
+    if (sizeof(*state) < sizeof(*usb))
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    /* At first allow 0-1 for the usb port number */
+    if ((usb_port_number < 0) || (usb_port_number > 1))
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    /* For all chips except 52XX there is only one port */
+    if (!OCTEON_IS_MODEL(OCTEON_CN52XX) && (usb_port_number > 0))
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    /* You must specify how the board is wired */
+    if ((flags & (CVMX_USB_INITIALIZE_FLAGS_CLOCK_XO_XI |
+                  CVMX_USB_INITIALIZE_FLAGS_CLOCK_XO_GND)) == 0)
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+
+    memset(usb, 0, sizeof(usb));
+    usb->init_flags = flags;
+
+    /* Initialize the USB state structure */
+    {
+        int i;
+        usb->index = usb_port_number;
+
+        /* Initialize the transaction double linked list */
+        usb->free_transaction_head = NULL;
+        usb->free_transaction_tail = NULL;
+        for (i=0; i<MAX_TRANSACTIONS; i++)
+            __cvmx_usb_free_transaction(usb, usb->transaction + i);
+    }
+
+    /* Power On Reset and PHY Initialization */
+
+    /* 1. Wait for DCOK to assert (nothing to do) */
+    /* 2a. Write USBN0/1_CLK_CTL[POR] = 1 and
+        USBN0/1_CLK_CTL[HRST,PRST,HCLK_RST] = 0 */
+    usbn_clk_ctl.u64 = __cvmx_usb_read_csr64(usb, CVMX_USBNX_CLK_CTL(usb->index));
+    usbn_clk_ctl.s.por = 1;
+    usbn_clk_ctl.s.hrst = 0;
+    usbn_clk_ctl.s.prst = 0;
+    usbn_clk_ctl.s.hclk_rst = 0;
+    usbn_clk_ctl.s.enable = 0;
+    /* 2b. Select the USB reference clock/crystal parameters by writing
+        appropriate values to USBN0/1_CLK_CTL[P_C_SEL, P_RTYPE, P_COM_ON] */
+    if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_CLOCK_XO_GND)
+    {
+        /* The USB port uses 12/24/48MHz 2.5V board clock
+            source at USB_XO. USB_XI should be tied to GND.
+            Most Octeon evaluation boards require this setting */
+        if (OCTEON_IS_MODEL(OCTEON_CN3XXX))
+        {
+            usbn_clk_ctl.cn31xx.p_rclk  = 1; /* From CN31XX,CN30XX manual */
+            usbn_clk_ctl.cn31xx.p_xenbn = 0;
+        }
+        else if (OCTEON_IS_MODEL(OCTEON_CN56XX) || OCTEON_IS_MODEL(OCTEON_CN50XX))
+            usbn_clk_ctl.cn56xx.p_rtype = 2; /* From CN56XX,CN50XX manual */
+        else
+            usbn_clk_ctl.cn52xx.p_rtype = 1; /* From CN52XX manual */
+    }
+    else
+    {
+        /* The USB port uses a 12MHz crystal as clock source
+            at USB_XO and USB_XI */
+        if (OCTEON_IS_MODEL(OCTEON_CN3XXX))
+        {
+            usbn_clk_ctl.cn31xx.p_rclk  = 1; /* From CN31XX,CN30XX manual */
+            usbn_clk_ctl.cn31xx.p_xenbn = 1;
+        }
+        else if (OCTEON_IS_MODEL(OCTEON_CN56XX) || OCTEON_IS_MODEL(OCTEON_CN50XX))
+            usbn_clk_ctl.cn56xx.p_rtype = 0; /* From CN56XX,CN50XX manual */
+        else
+            usbn_clk_ctl.cn52xx.p_rtype = 0; /* From CN52XX manual */
+    }
+    /* 2c. Select the HCLK via writing USBN0/1_CLK_CTL[DIVIDE, DIVIDE2] and
+        setting USBN0/1_CLK_CTL[ENABLE] = 1.  Divide the core clock down such
+        that USB is as close as possible to 125Mhz */
+    {
+        int divisor = (cvmx_sysinfo_get()->cpu_clock_hz+125000000-1)/125000000;
+        if (divisor < 4)  /* Lower than 4 doesn't seem to work properly */
+            divisor = 4;
+        usbn_clk_ctl.s.divide = divisor;
+        usbn_clk_ctl.s.divide2 = 0;
+    }
+    __cvmx_usb_write_csr64(usb, CVMX_USBNX_CLK_CTL(usb->index),
+                           usbn_clk_ctl.u64);
+    /* 2d. Write USBN0/1_CLK_CTL[HCLK_RST] = 1 */
+    usbn_clk_ctl.s.hclk_rst = 1;
+    __cvmx_usb_write_csr64(usb, CVMX_USBNX_CLK_CTL(usb->index),
+                           usbn_clk_ctl.u64);
+    /* 2e.  Wait 64 core-clock cycles for HCLK to stabilize */
+    cvmx_wait(64);
+    /* 3. Program the power-on reset field in the USBN clock-control register:
+        USBN_CLK_CTL[POR] = 0 */
+    usbn_clk_ctl.s.por = 0;
+    __cvmx_usb_write_csr64(usb, CVMX_USBNX_CLK_CTL(usb->index),
+                           usbn_clk_ctl.u64);
+    /* 4. Wait 1 ms for PHY clock to start */
+    cvmx_wait_usec(1000);
+    /* 5. Program the Reset input from automatic test equipment field in the
+        USBP control and status register: USBN_USBP_CTL_STATUS[ATE_RESET] = 1 */
+    usbn_usbp_ctl_status.u64 = __cvmx_usb_read_csr64(usb, CVMX_USBNX_USBP_CTL_STATUS(usb->index));
+    usbn_usbp_ctl_status.s.ate_reset = 1;
+    __cvmx_usb_write_csr64(usb, CVMX_USBNX_USBP_CTL_STATUS(usb->index),
+                           usbn_usbp_ctl_status.u64);
+    /* 6. Wait 10 cycles */
+    cvmx_wait(10);
+    /* 7. Clear ATE_RESET field in the USBN clock-control register:
+        USBN_USBP_CTL_STATUS[ATE_RESET] = 0 */
+    usbn_usbp_ctl_status.s.ate_reset = 0;
+    __cvmx_usb_write_csr64(usb, CVMX_USBNX_USBP_CTL_STATUS(usb->index),
+                           usbn_usbp_ctl_status.u64);
+    /* 8. Program the PHY reset field in the USBN clock-control register:
+        USBN_CLK_CTL[PRST] = 1 */
+    usbn_clk_ctl.s.prst = 1;
+    __cvmx_usb_write_csr64(usb, CVMX_USBNX_CLK_CTL(usb->index),
+                           usbn_clk_ctl.u64);
+    /* 9. Program the USBP control and status register to select host or
+        device mode. USBN_USBP_CTL_STATUS[HST_MODE] = 0 for host, = 1 for
+        device */
+    if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEVICE_MODE)
+    {
+        usbn_usbp_ctl_status.s.hst_mode = 1;
+        usbn_usbp_ctl_status.s.dm_pulld = 0;
+        usbn_usbp_ctl_status.s.dp_pulld = 0;
+    }
+    else
+    {
+        usbn_usbp_ctl_status.s.hst_mode = 0;
+    }
+    __cvmx_usb_write_csr64(usb, CVMX_USBNX_USBP_CTL_STATUS(usb->index),
+                           usbn_usbp_ctl_status.u64);
+    /* 10. Wait 1 s */
+    cvmx_wait_usec(1);
+    /* 11. Program the hreset_n field in the USBN clock-control register:
+        USBN_CLK_CTL[HRST] = 1 */
+    usbn_clk_ctl.s.hrst = 1;
+    __cvmx_usb_write_csr64(usb, CVMX_USBNX_CLK_CTL(usb->index),
+                           usbn_clk_ctl.u64);
+    /* 12. Proceed to USB core initialization */
+    usbn_clk_ctl.s.enable = 1;
+    __cvmx_usb_write_csr64(usb, CVMX_USBNX_CLK_CTL(usb->index),
+                           usbn_clk_ctl.u64);
+    cvmx_wait_usec(1);
+
+    /* USB Core Initialization */
+
+    /* 1. Read USBC_GHWCFG1, USBC_GHWCFG2, USBC_GHWCFG3, USBC_GHWCFG4 to
+        determine USB core configuration parameters. */
+    /* Nothing needed */
+    /* 2. Program the following fields in the global AHB configuration
+        register (USBC_GAHBCFG)
+        DMA mode, USBC_GAHBCFG[DMAEn]: 1 = DMA mode, 0 = slave mode
+        Burst length, USBC_GAHBCFG[HBSTLEN] = 0
+        Nonperiodic TxFIFO empty level (slave mode only),
+        USBC_GAHBCFG[NPTXFEMPLVL]
+        Periodic TxFIFO empty level (slave mode only),
+        USBC_GAHBCFG[PTXFEMPLVL]
+        Global interrupt mask, USBC_GAHBCFG[GLBLINTRMSK] = 1 */
+    {
+        cvmx_usbcx_gahbcfg_t usbcx_gahbcfg;
+        usbcx_gahbcfg.u32 = 0;
+        usbcx_gahbcfg.s.dmaen = !OCTEON_IS_MODEL(OCTEON_CN31XX);
+        /* If we are using DMA, start off with 8 idle channels. Without
+            DMA we emulate a single channel */
+        if (usbcx_gahbcfg.s.dmaen)
+            usb->idle_hardware_channels = 0xff;
+        else
+            usb->idle_hardware_channels = 0x1;
+        usbcx_gahbcfg.s.hbstlen = 0;
+        usbcx_gahbcfg.s.nptxfemplvl = 1;
+        usbcx_gahbcfg.s.ptxfemplvl = 1;
+        usbcx_gahbcfg.s.glblintrmsk = 1;
+        __cvmx_usb_write_csr32(usb, CVMX_USBCX_GAHBCFG(usb->index),
+                               usbcx_gahbcfg.u32);
+    }
+    /* 3. Program the following fields in USBC_GUSBCFG register.
+        HS/FS timeout calibration, USBC_GUSBCFG[TOUTCAL] = 0
+        ULPI DDR select, USBC_GUSBCFG[DDRSEL] = 0
+        USB turnaround time, USBC_GUSBCFG[USBTRDTIM] = 0x5
+        PHY low-power clock select, USBC_GUSBCFG[PHYLPWRCLKSEL] = 0 */
+    {
+        cvmx_usbcx_gusbcfg_t usbcx_gusbcfg;
+        usbcx_gusbcfg.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_GUSBCFG(usb->index));
+        usbcx_gusbcfg.s.toutcal = 0;
+        usbcx_gusbcfg.s.ddrsel = 0;
+        usbcx_gusbcfg.s.usbtrdtim = 0x5;
+        usbcx_gusbcfg.s.phylpwrclksel = 0;
+        __cvmx_usb_write_csr32(usb, CVMX_USBCX_GUSBCFG(usb->index),
+                               usbcx_gusbcfg.u32);
+    }
+    /* 4. The software must unmask the following bits in the USBC_GINTMSK
+        register.
+        OTG interrupt mask, USBC_GINTMSK[OTGINTMSK] = 1
+        Mode mismatch interrupt mask, USBC_GINTMSK[MODEMISMSK] = 1 */
+    {
+        cvmx_usbcx_gintmsk_t usbcx_gintmsk;
+        usbcx_gintmsk.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_GINTMSK(usb->index));
+        usbcx_gintmsk.s.otgintmsk = 1;
+        usbcx_gintmsk.s.modemismsk = 1;
+        usbcx_gintmsk.s.hchintmsk = 1;
+        usbcx_gintmsk.s.sofmsk = 0;
+        __cvmx_usb_write_csr32(usb, CVMX_USBCX_GINTMSK(usb->index),
+                               usbcx_gintmsk.u32);
+    }
+
+    if ((usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEVICE_MODE) == 0)
+    {
+        /* Host Port Initialization */
+        if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_INFO)
+            cvmx_dprintf("%s: USB%d is in host mode\n", __FUNCTION__, usb->index);
+
+        /* 1. Program the host-port interrupt-mask field to unmask,
+            USBC_GINTMSK[PRTINT] = 1 */
+        USB_SET_FIELD32(CVMX_USBCX_GINTMSK(usb->index), cvmx_usbcx_gintmsk_t,
+                        prtintmsk, 1);
+        USB_SET_FIELD32(CVMX_USBCX_GINTMSK(usb->index), cvmx_usbcx_gintmsk_t,
+                        disconnintmsk, 1);
+        /* 2. Program the USBC_HCFG register to select full-speed host or
+            high-speed host. */
+        {
+            cvmx_usbcx_hcfg_t usbcx_hcfg;
+            usbcx_hcfg.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_HCFG(usb->index));
+            usbcx_hcfg.s.fslssupp = 0;
+            usbcx_hcfg.s.fslspclksel = 0;
+            __cvmx_usb_write_csr32(usb, CVMX_USBCX_HCFG(usb->index), usbcx_hcfg.u32);
+        }
+        /* 3. Program the port power bit to drive VBUS on the USB,
+            USBC_HPRT[PRTPWR] = 1 */
+        USB_SET_FIELD32(CVMX_USBCX_HPRT(usb->index), cvmx_usbcx_hprt_t, prtpwr, 1);
+
+        /* Steps 4-15 from the manual are done later in the port enable */
+    }
+    else
+    {
+        /* Device Port Initialization */
+        if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_INFO)
+            cvmx_dprintf("%s: USB%d is in device mode\n", __FUNCTION__, usb->index);
+
+        /* 1. Program the following fields in the USBC0/1_DCFG register:
+            Device speed, USBC0/1_DCFG[DEVSPD] = 0 (high speed)
+            Non-zero-length status OUT handshake, USBC0/1_DCFG[NZSTSOUTHSHK]=0
+            Periodic frame interval (if periodic endpoints are supported),
+            USBC0/1_DCFG[PERFRINT] = 1 */
+        USB_SET_FIELD32(CVMX_USBCX_DCFG(usb->index), cvmx_usbcx_dcfg_t,
+                        devspd, 0);
+        USB_SET_FIELD32(CVMX_USBCX_DCFG(usb->index), cvmx_usbcx_dcfg_t,
+                        nzstsouthshk, 0);
+        USB_SET_FIELD32(CVMX_USBCX_DCFG(usb->index), cvmx_usbcx_dcfg_t,
+                        perfrint, 1);
+
+        /* 2. Program the USBC0/1_GINTMSK register to unmask the following
+            interrupts:
+            USB Reset, USBC0/1_GINTMSK[USBRSTMSK] = 1
+            Enumeration done, USBC0/1_GINTMSK[ENUMDONEMSK] = 1
+            SOF, USBC0/1_GINTMSK[SOFMSK] = 1 */
+        USB_SET_FIELD32(CVMX_USBCX_GINTMSK(usb->index), cvmx_usbcx_gintmsk_t,
+                        usbrstmsk, 1);
+        USB_SET_FIELD32(CVMX_USBCX_GINTMSK(usb->index), cvmx_usbcx_gintmsk_t,
+                        enumdonemsk, 1);
+        USB_SET_FIELD32(CVMX_USBCX_GINTMSK(usb->index), cvmx_usbcx_gintmsk_t,
+                        sofmsk, 1);
+
+        /* 3. Wait for the USBC0/1_GINTSTS[USBRESET] interrupt, which
+            indicates a reset has been detected on the USB and lasts for
+            about 10 ms. On receiving this interrupt, the application must
+            perform the steps listed in Section 22.6.1.1, "Initialization on
+            USB Reset". */
+        /* Handled in cvmx_poll() usbc_gintsts.s.usbrst processing */
+
+        /* 4. Wait for the USBC0/1_GINTSTS[ENUMERATIONDONE] interrupt, which
+            indicates the end of reset on the USB. On receiving this interrupt,
+            the application must read the USBC0/1_DSTS register to determine
+            the enumeration speed and perform the steps listed in Section
+            22.6.1.2, "Initialization on Enumeration Completion". */
+        /* Handled in cvmx_poll() usbc_gintsts.s.enumdone processing */
+    }
+
+    CVMX_USB_RETURN(CVMX_USB_SUCCESS);
+}
+
+
+/**
+ * Shutdown a USB port after a call to cvmx_usb_initialize().
+ * The port should be disabled with all pipes closed when this
+ * function is called.
+ *
+ * @param state  USB device state populated by
+ *               cvmx_usb_initialize().
+ *
+ * @return CVMX_USB_SUCCESS or a negative error code defined in
+ *         cvmx_usb_status_t.
+ */
+cvmx_usb_status_t cvmx_usb_shutdown(cvmx_usb_state_t *state)
+{
+    int pipe_handle;
+    cvmx_usbnx_clk_ctl_t usbn_clk_ctl;
+    cvmx_usb_internal_state_t *usb = (cvmx_usb_internal_state_t*)state;
+
+    CVMX_USB_LOG_CALLED();
+    CVMX_USB_LOG_PARAM("%p", state);
+
+    /* Make sure all pipes are closed */
+    for (pipe_handle = 0; pipe_handle<MAX_PIPES; pipe_handle++)
+        if (usb->pipe[pipe_handle].flags & __CVMX_USB_PIPE_FLAGS_OPEN)
+            CVMX_USB_RETURN(CVMX_USB_BUSY);
+
+    /* Disable the clocks and put them in power on reset */
+    usbn_clk_ctl.u64 = __cvmx_usb_read_csr64(usb, CVMX_USBNX_CLK_CTL(usb->index));
+    usbn_clk_ctl.s.enable = 0;
+    usbn_clk_ctl.s.por = 1;
+    __cvmx_usb_write_csr64(usb, CVMX_USBNX_CLK_CTL(usb->index),
+                           usbn_clk_ctl.u64);
+    CVMX_USB_RETURN(CVMX_USB_SUCCESS);
+}
+
+
+/**
+ * Enable a USB port. After this call succeeds, the USB port is
+ * online and servicing requests.
+ *
+ * @param state  USB device state populated by
+ *               cvmx_usb_initialize().
+ *
+ * @return CVMX_USB_SUCCESS or a negative error code defined in
+ *         cvmx_usb_status_t.
+ */
+cvmx_usb_status_t cvmx_usb_enable(cvmx_usb_state_t *state)
+{
+    cvmx_usbcx_hprt_t usbcx_hprt;
+    cvmx_usbcx_ghwcfg3_t usbcx_ghwcfg3;
+    cvmx_usb_internal_state_t *usb = (cvmx_usb_internal_state_t*)state;
+
+    CVMX_USB_LOG_CALLED();
+    CVMX_USB_LOG_PARAM("%p", state);
+    if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEVICE_MODE)
+        CVMX_USB_RETURN(CVMX_USB_INCORRECT_MODE);
+
+    usbcx_hprt.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_HPRT(usb->index));
+
+    /* If the port is already enabled the just return. We don't need to do
+        anything */
+    if (usbcx_hprt.s.prtena)
+        CVMX_USB_RETURN(CVMX_USB_SUCCESS);
+
+    /* If there is nothing plugged into the port then fail immediately */
+    if (!usbcx_hprt.s.prtconnsts)
+    {
+        if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_INFO)
+            cvmx_dprintf("%s: USB%d Nothing plugged into the port\n", __FUNCTION__, usb->index);
+        CVMX_USB_RETURN(CVMX_USB_TIMEOUT);
+    }
+
+    /* Program the port reset bit to start the reset process */
+    USB_SET_FIELD32(CVMX_USBCX_HPRT(usb->index), cvmx_usbcx_hprt_t, prtrst, 1);
+
+    /* Wait at least 50ms (high speed), or 10ms (full speed) for the reset
+        process to complete. */
+    cvmx_wait_usec(50000);
+
+    /* Program the port reset bit to 0, USBC_HPRT[PRTRST] = 0 */
+    USB_SET_FIELD32(CVMX_USBCX_HPRT(usb->index), cvmx_usbcx_hprt_t, prtrst, 0);
+
+    /* Wait for the USBC_HPRT[PRTENA]. */
+    if (CVMX_WAIT_FOR_FIELD32(CVMX_USBCX_HPRT(usb->index), cvmx_usbcx_hprt_t,
+                              prtena, ==, 1, 100000))
+    {
+        if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_INFO)
+            cvmx_dprintf("%s: Timeout waiting for the port to finish reset\n",
+                         __FUNCTION__);
+        CVMX_USB_RETURN(CVMX_USB_TIMEOUT);
+    }
+
+    /* Read the port speed field to get the enumerated speed, USBC_HPRT[PRTSPD]. */
+    usbcx_hprt.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_HPRT(usb->index));
+    if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_INFO)
+        cvmx_dprintf("%s: USB%d is in %s speed mode\n", __FUNCTION__, usb->index,
+                     (usbcx_hprt.s.prtspd == CVMX_USB_SPEED_HIGH) ? "high" :
+                     (usbcx_hprt.s.prtspd == CVMX_USB_SPEED_FULL) ? "full" :
+                     "low");
+
+    usbcx_ghwcfg3.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_GHWCFG3(usb->index));
+
+    /* 13. Program the USBC_GRXFSIZ register to select the size of the receive
+        FIFO (25%). */
+    USB_SET_FIELD32(CVMX_USBCX_GRXFSIZ(usb->index), cvmx_usbcx_grxfsiz_t,
+                    rxfdep, usbcx_ghwcfg3.s.dfifodepth / 4);
+    /* 14. Program the USBC_GNPTXFSIZ register to select the size and the
+        start address of the non- periodic transmit FIFO for nonperiodic
+        transactions (50%). */
+    {
+        cvmx_usbcx_gnptxfsiz_t siz;
+        siz.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_GNPTXFSIZ(usb->index));
+        siz.s.nptxfdep = usbcx_ghwcfg3.s.dfifodepth / 2;
+        siz.s.nptxfstaddr = usbcx_ghwcfg3.s.dfifodepth / 4;
+        __cvmx_usb_write_csr32(usb, CVMX_USBCX_GNPTXFSIZ(usb->index), siz.u32);
+    }
+    /* 15. Program the USBC_HPTXFSIZ register to select the size and start
+        address of the periodic transmit FIFO for periodic transactions (25%). */
+    {
+        cvmx_usbcx_hptxfsiz_t siz;
+        siz.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_HPTXFSIZ(usb->index));
+        siz.s.ptxfsize = usbcx_ghwcfg3.s.dfifodepth / 4;
+        siz.s.ptxfstaddr = 3 * usbcx_ghwcfg3.s.dfifodepth / 4;
+        __cvmx_usb_write_csr32(usb, CVMX_USBCX_HPTXFSIZ(usb->index), siz.u32);
+    }
+    /* Flush all FIFOs */
+    USB_SET_FIELD32(CVMX_USBCX_GRSTCTL(usb->index), cvmx_usbcx_grstctl_t, txfnum, 0x10);
+    USB_SET_FIELD32(CVMX_USBCX_GRSTCTL(usb->index), cvmx_usbcx_grstctl_t, txfflsh, 1);
+    CVMX_WAIT_FOR_FIELD32(CVMX_USBCX_GRSTCTL(usb->index), cvmx_usbcx_grstctl_t,
+                          txfflsh, ==, 0, 100);
+    USB_SET_FIELD32(CVMX_USBCX_GRSTCTL(usb->index), cvmx_usbcx_grstctl_t, rxfflsh, 1);
+    CVMX_WAIT_FOR_FIELD32(CVMX_USBCX_GRSTCTL(usb->index), cvmx_usbcx_grstctl_t,
+                          rxfflsh, ==, 0, 100);
+
+    CVMX_USB_RETURN(CVMX_USB_SUCCESS);
+}
+
+
+/**
+ * Disable a USB port. After this call the USB port will not
+ * generate data transfers and will not generate events.
+ * Transactions in process will fail and call their
+ * associated callbacks.
+ *
+ * @param state  USB device state populated by
+ *               cvmx_usb_initialize().
+ *
+ * @return CVMX_USB_SUCCESS or a negative error code defined in
+ *         cvmx_usb_status_t.
+ */
+cvmx_usb_status_t cvmx_usb_disable(cvmx_usb_state_t *state)
+{
+    cvmx_usb_internal_state_t *usb = (cvmx_usb_internal_state_t*)state;
+
+    CVMX_USB_LOG_CALLED();
+    CVMX_USB_LOG_PARAM("%p", state);
+    if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEVICE_MODE)
+        CVMX_USB_RETURN(CVMX_USB_INCORRECT_MODE);
+
+    /* Disable the port */
+    USB_SET_FIELD32(CVMX_USBCX_HPRT(usb->index), cvmx_usbcx_hprt_t, prtena, 1);
+    CVMX_USB_RETURN(CVMX_USB_SUCCESS);
+}
+
+
+/**
+ * Get the current state of the USB port. Use this call to
+ * determine if the usb port has anything connected, is enabled,
+ * or has some sort of error condition. The return value of this
+ * call has "changed" bits to signal of the value of some fields
+ * have changed between calls. These "changed" fields are based
+ * on the last call to cvmx_usb_set_status(). In order to clear
+ * them, you must update the status through cvmx_usb_set_status().
+ *
+ * @param state  USB device state populated by
+ *               cvmx_usb_initialize().
+ *
+ * @return Port status information
+ */
+cvmx_usb_port_status_t cvmx_usb_get_status(cvmx_usb_state_t *state)
+{
+    cvmx_usbcx_hprt_t usbc_hprt;
+    cvmx_usb_port_status_t result;
+    cvmx_usb_internal_state_t *usb = (cvmx_usb_internal_state_t*)state;
+
+    memset(&result, 0, sizeof(result));
+
+    CVMX_USB_LOG_CALLED();
+    CVMX_USB_LOG_PARAM("%p", state);
+
+    usbc_hprt.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_HPRT(usb->index));
+    result.port_enabled = usbc_hprt.s.prtena;
+    result.port_over_current = usbc_hprt.s.prtovrcurract;
+    result.port_powered = usbc_hprt.s.prtpwr;
+    result.port_speed = usbc_hprt.s.prtspd;
+    result.connected = usbc_hprt.s.prtconnsts;
+    result.connect_change = (result.connected != usb->port_status.connected);
+
+    if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_CALLS)
+        cvmx_dprintf("%*s%s: returned port enabled=%d, over_current=%d, powered=%d, speed=%d, connected=%d, connect_change=%d\n",
+                     2*(--usb->indent), "", __FUNCTION__,
+                     result.port_enabled,
+                     result.port_over_current,
+                     result.port_powered,
+                     result.port_speed,
+                     result.connected,
+                     result.connect_change);
+    return result;
+}
+
+
+/**
+ * Set the current state of the USB port. The status is used as
+ * a reference for the "changed" bits returned by
+ * cvmx_usb_get_status(). Other than serving as a reference, the
+ * status passed to this function is not used. No fields can be
+ * changed through this call.
+ *
+ * @param state  USB device state populated by
+ *               cvmx_usb_initialize().
+ * @param port_status
+ *               Port status to set, most like returned by cvmx_usb_get_status()
+ */
+void cvmx_usb_set_status(cvmx_usb_state_t *state, cvmx_usb_port_status_t port_status)
+{
+    cvmx_usb_internal_state_t *usb = (cvmx_usb_internal_state_t*)state;
+    CVMX_USB_LOG_CALLED();
+    CVMX_USB_LOG_PARAM("%p", state);
+    usb->port_status = port_status;
+    CVMX_USB_RETURN_NOTHING();
+}
+
+
+/**
+ * Open a virtual pipe between the host and a USB device. A pipe
+ * must be opened before data can be transferred between a device
+ * and Octeon.
+ *
+ * @param state      USB device state populated by
+ *                   cvmx_usb_initialize().
+ * @param flags      Optional pipe flags defined in
+ *                   cvmx_usb_pipe_flags_t.
+ * @param device_addr
+ *                   USB device address to open the pipe to
+ *                   (0-127).
+ * @param endpoint_num
+ *                   USB endpoint number to open the pipe to
+ *                   (0-15).
+ * @param device_speed
+ *                   The speed of the device the pipe is going
+ *                   to. This must match the device's speed,
+ *                   which may be different than the port speed.
+ * @param max_packet The maximum packet length the device can
+ *                   transmit/receive (low speed=0-8, full
+ *                   speed=0-1023, high speed=0-1024). This value
+ *                   comes from the stadnard endpoint descriptor
+ *                   field wMaxPacketSize bits <10:0>.
+ * @param transfer_type
+ *                   The type of transfer this pipe is for.
+ * @param transfer_dir
+ *                   The direction the pipe is in. This is not
+ *                   used for control pipes.
+ * @param interval   For ISOCHRONOUS and INTERRUPT transfers,
+ *                   this is how often the transfer is scheduled
+ *                   for. All other transfers should specify
+ *                   zero. The units are in frames (8000/sec at
+ *                   high speed, 1000/sec for full speed).
+ * @param multi_count
+ *                   For high speed devices, this is the maximum
+ *                   allowed number of packet per microframe.
+ *                   Specify zero for non high speed devices. This
+ *                   value comes from the stadnard endpoint descriptor
+ *                   field wMaxPacketSize bits <12:11>.
+ * @param hub_device_addr
+ *                   Hub device address this device is connected
+ *                   to. Devices connected directly to Octeon
+ *                   use zero. This is only used when the device
+ *                   is full/low speed behind a high speed hub.
+ *                   The address will be of the high speed hub,
+ *                   not and full speed hubs after it.
+ * @param hub_port   Which port on the hub the device is
+ *                   connected. Use zero for devices connected
+ *                   directly to Octeon. Like hub_device_addr,
+ *                   this is only used for full/low speed
+ *                   devices behind a high speed hub.
+ *
+ * @return A non negative value is a pipe handle. Negative
+ *         values are failure codes from cvmx_usb_status_t.
+ */
+int cvmx_usb_open_pipe(cvmx_usb_state_t *state, cvmx_usb_pipe_flags_t flags,
+                       int device_addr, int endpoint_num,
+                       cvmx_usb_speed_t device_speed, int max_packet,
+                       cvmx_usb_transfer_t transfer_type,
+                       cvmx_usb_direction_t transfer_dir, int interval,
+                       int multi_count, int hub_device_addr, int hub_port)
+{
+    int pipe_handle;
+    cvmx_usb_pipe_t *pipe;
+    cvmx_usb_internal_state_t *usb = (cvmx_usb_internal_state_t*)state;
+
+    CVMX_USB_LOG_CALLED();
+    CVMX_USB_LOG_PARAM("%p", state);
+    CVMX_USB_LOG_PARAM("0x%x", flags);
+    CVMX_USB_LOG_PARAM("%d", device_addr);
+    CVMX_USB_LOG_PARAM("%d", endpoint_num);
+    CVMX_USB_LOG_PARAM("%d", device_speed);
+    CVMX_USB_LOG_PARAM("%d", max_packet);
+    CVMX_USB_LOG_PARAM("%d", transfer_type);
+    CVMX_USB_LOG_PARAM("%d", transfer_dir);
+    CVMX_USB_LOG_PARAM("%d", interval);
+    CVMX_USB_LOG_PARAM("%d", multi_count);
+    CVMX_USB_LOG_PARAM("%d", hub_device_addr);
+    CVMX_USB_LOG_PARAM("%d", hub_port);
+
+    if ((device_addr < 0) || (device_addr > MAX_USB_ADDRESS))
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    if ((endpoint_num < 0) || (endpoint_num > MAX_USB_ENDPOINT))
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    if (device_speed > CVMX_USB_SPEED_LOW)
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    if ((max_packet <= 0) || (max_packet > 1024))
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    if (transfer_type > CVMX_USB_TRANSFER_INTERRUPT)
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    if ((transfer_dir != CVMX_USB_DIRECTION_OUT) &&
+        (transfer_dir != CVMX_USB_DIRECTION_IN))
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    if (interval < 0)
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    if (!((transfer_type == CVMX_USB_TRANSFER_ISOCHRONOUS) ||
+          (transfer_type == CVMX_USB_TRANSFER_INTERRUPT)) && (interval != 0))
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    if (multi_count < 0)
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    if ((device_speed != CVMX_USB_SPEED_HIGH) &&
+        (multi_count != 0))
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    if ((hub_device_addr < 0) || (hub_device_addr > MAX_USB_ADDRESS))
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    if ((hub_port < 0) || (hub_port > MAX_USB_HUB_PORT))
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEVICE_MODE)
+        CVMX_USB_RETURN(CVMX_USB_INCORRECT_MODE);
+
+    /* Find a closed pipe */
+    for (pipe_handle = 0; pipe_handle<MAX_PIPES; pipe_handle++)
+        if ((usb->pipe[pipe_handle].flags & __CVMX_USB_PIPE_FLAGS_OPEN) == 0)
+            break;
+    if (pipe_handle >= MAX_PIPES)
+        CVMX_USB_RETURN(CVMX_USB_NO_MEMORY);
+
+    pipe = usb->pipe + pipe_handle;
+    pipe->flags = flags | __CVMX_USB_PIPE_FLAGS_OPEN;
+    if ((device_speed == CVMX_USB_SPEED_HIGH) &&
+        (transfer_dir == CVMX_USB_DIRECTION_OUT) &&
+        (transfer_type == CVMX_USB_TRANSFER_BULK))
+        pipe->flags |= __CVMX_USB_PIPE_FLAGS_NEED_PING;
+    pipe->device_addr = device_addr;
+    pipe->endpoint_num = endpoint_num;
+    pipe->device_speed = device_speed;
+    pipe->max_packet = max_packet;
+    pipe->transfer_type = transfer_type;
+    pipe->transfer_dir = transfer_dir;
+    if (device_speed == CVMX_USB_SPEED_HIGH)
+        pipe->interval = (uint64_t)interval * cvmx_sysinfo_get()->cpu_clock_hz / 8000;
+    else
+        pipe->interval = (uint64_t)interval * cvmx_sysinfo_get()->cpu_clock_hz / 1000;
+    pipe->multi_count = multi_count;
+    pipe->hub_device_addr = hub_device_addr;
+    pipe->hub_port = hub_port;
+    pipe->pid_toggle = 0;
+    pipe->next_tx_cycle = cvmx_read64_uint64(CVMX_IPD_CLK_COUNT) + pipe->interval;
+
+    /* We don't need to tell the hardware about this pipe yet since
+        it doesn't have any submitted requests */
+
+    CVMX_USB_RETURN(pipe_handle);
+}
+
+
+/**
+ * @INTERNAL
+ * Convert a USB transaction into a handle
+ *
+ * @param usb    USB device state populated by
+ *               cvmx_usb_initialize().
+ * @param transaction
+ *               Transaction to get handle for
+ *
+ * @return Handle
+ */
+static inline int __cvmx_usb_get_submit_handle(cvmx_usb_internal_state_t *usb,
+                                        cvmx_usb_transaction_t *transaction)
+{
+    return ((unsigned long)transaction - (unsigned long)usb->transaction) /
+            sizeof(*transaction);
+}
+
+
+/**
+ * @INTERNAL
+ * Convert a USB pipe into a handle
+ *
+ * @param usb    USB device state populated by
+ *               cvmx_usb_initialize().
+ * @param pipe   Pipe to get handle for
+ *
+ * @return Handle
+ */
+static inline int __cvmx_usb_get_pipe_handle(cvmx_usb_internal_state_t *usb,
+                                        cvmx_usb_pipe_t *pipe)
+{
+    return ((unsigned long)pipe - (unsigned long)usb->pipe) / sizeof(*pipe);
+}
+
+
+/**
+ * @INTERNAL
+ * Perform channel specific setup for Control transactions. All
+ * the generic stuff will already have been done in
+ * __cvmx_usb_start_channel()
+ *
+ * @param usb     USB device state populated by
+ *                cvmx_usb_initialize().
+ * @param channel Channel to setup
+ * @param pipe    Pipe for control transaction
+ */
+static void __cvmx_usb_start_channel_control(cvmx_usb_internal_state_t *usb,
+                                             int channel,
+                                             cvmx_usb_pipe_t *pipe)
+{
+    cvmx_usb_transaction_t *transaction = pipe->head;
+    cvmx_usb_control_header_t *header = cvmx_phys_to_ptr(transaction->control_header);
+    int bytes_to_transfer = transaction->buffer_length - transaction->actual_bytes;
+    cvmx_usbcx_hctsizx_t usbc_hctsiz;
+
+    CVMX_USB_LOG_CALLED();
+    CVMX_USB_LOG_PARAM("%p", usb);
+    CVMX_USB_LOG_PARAM("%d", channel);
+    CVMX_USB_LOG_PARAM("%p", pipe);
+
+    usbc_hctsiz.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_HCTSIZX(channel, usb->index));
+    usbc_hctsiz.s.pktcnt = 1;
+
+    switch (transaction->stage)
+    {
+        case CVMX_USB_STAGE_NON_CONTROL:
+        case CVMX_USB_STAGE_NON_CONTROL_SPLIT_COMPLETE:
+            cvmx_dprintf("%s: ERROR - Non control stage\n", __FUNCTION__);
+            break;
+        case CVMX_USB_STAGE_SETUP:
+            usbc_hctsiz.s.pid = 3; /* Setup */
+            usbc_hctsiz.s.xfersize = sizeof(*header);
+            /* All Control operations start with a setup going OUT */
+            USB_SET_FIELD32(CVMX_USBCX_HCCHARX(channel, usb->index), cvmx_usbcx_hccharx_t, epdir, CVMX_USB_DIRECTION_OUT);
+            /* Setup send the control header instead of the buffer data. The
+                buffer data will be used in the next stage */
+            __cvmx_usb_write_csr64(usb, CVMX_USBNX_DMA0_OUTB_CHN0(usb->index) + channel*8, transaction->control_header);
+            break;
+        case CVMX_USB_STAGE_SETUP_SPLIT_COMPLETE:
+            usbc_hctsiz.s.pid = 3; /* Setup */
+            usbc_hctsiz.s.xfersize = 0;
+            /* All Control operations start with a setup going OUT */
+            USB_SET_FIELD32(CVMX_USBCX_HCCHARX(channel, usb->index), cvmx_usbcx_hccharx_t, epdir, CVMX_USB_DIRECTION_OUT);
+            USB_SET_FIELD32(CVMX_USBCX_HCSPLTX(channel, usb->index), cvmx_usbcx_hcspltx_t, compsplt, 1);
+            break;
+        case CVMX_USB_STAGE_DATA:
+            usbc_hctsiz.s.pid = __cvmx_usb_get_data_pid(pipe);
+            if (__cvmx_usb_pipe_needs_split(usb, pipe))
+                usbc_hctsiz.s.xfersize = (header->s.request_type & 0x80) ? 0 : cvmx_le16_to_cpu(header->s.length);
+            else
+                usbc_hctsiz.s.xfersize = (header->s.request_type & 0x80) ? bytes_to_transfer : cvmx_le16_to_cpu(header->s.length);
+            USB_SET_FIELD32(CVMX_USBCX_HCCHARX(channel, usb->index),
+                            cvmx_usbcx_hccharx_t, epdir,
+                            ((header->s.request_type & 0x80) ?
+                             CVMX_USB_DIRECTION_IN :
+                             CVMX_USB_DIRECTION_OUT));
+            break;
+        case CVMX_USB_STAGE_DATA_SPLIT_COMPLETE:
+            usbc_hctsiz.s.pid = __cvmx_usb_get_data_pid(pipe);
+            usbc_hctsiz.s.xfersize = (header->s.request_type & 0x80) ? bytes_to_transfer : 0;
+            USB_SET_FIELD32(CVMX_USBCX_HCCHARX(channel, usb->index),
+                            cvmx_usbcx_hccharx_t, epdir,
+                            ((header->s.request_type & 0x80) ?
+                             CVMX_USB_DIRECTION_IN :
+                             CVMX_USB_DIRECTION_OUT));
+            USB_SET_FIELD32(CVMX_USBCX_HCSPLTX(channel, usb->index), cvmx_usbcx_hcspltx_t, compsplt, 1);
+            break;
+        case CVMX_USB_STAGE_STATUS:
+            usbc_hctsiz.s.pid = __cvmx_usb_get_data_pid(pipe);
+            usbc_hctsiz.s.xfersize = 0;
+            USB_SET_FIELD32(CVMX_USBCX_HCCHARX(channel, usb->index), cvmx_usbcx_hccharx_t, epdir,
+                            ((header->s.request_type & 0x80) ?
+                             CVMX_USB_DIRECTION_OUT :
+                             CVMX_USB_DIRECTION_IN));
+            break;
+        case CVMX_USB_STAGE_STATUS_SPLIT_COMPLETE:
+            usbc_hctsiz.s.pid = __cvmx_usb_get_data_pid(pipe);
+            usbc_hctsiz.s.xfersize = 0;
+            USB_SET_FIELD32(CVMX_USBCX_HCCHARX(channel, usb->index), cvmx_usbcx_hccharx_t, epdir,
+                            ((header->s.request_type & 0x80) ?
+                             CVMX_USB_DIRECTION_OUT :
+                             CVMX_USB_DIRECTION_IN));
+            USB_SET_FIELD32(CVMX_USBCX_HCSPLTX(channel, usb->index), cvmx_usbcx_hcspltx_t, compsplt, 1);
+            break;
+    }
+    __cvmx_usb_write_csr32(usb, CVMX_USBCX_HCTSIZX(channel, usb->index), usbc_hctsiz.u32);
+    CVMX_USB_RETURN_NOTHING();
+}
+
+
+/**
+ * @INTERNAL
+ * Start a channel to perform the pipe's head transaction
+ *
+ * @param usb     USB device state populated by
+ *                cvmx_usb_initialize().
+ * @param channel Channel to setup
+ * @param pipe    Pipe to start
+ */
+static void __cvmx_usb_start_channel(cvmx_usb_internal_state_t *usb,
+                                     int channel,
+                                     cvmx_usb_pipe_t *pipe)
+{
+    cvmx_usb_transaction_t *transaction = pipe->head;
+
+    CVMX_USB_LOG_CALLED();
+    CVMX_USB_LOG_PARAM("%p", usb);
+    CVMX_USB_LOG_PARAM("%d", channel);
+    CVMX_USB_LOG_PARAM("%p", pipe);
+
+    if ((usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_TRANSFERS) ||
+        (pipe->flags & CVMX_USB_PIPE_FLAGS_DEBUG_TRANSFERS))
+        cvmx_dprintf("%s: Channel %d started. Pipe %d transaction %d stage %d\n",
+                     __FUNCTION__, channel, __cvmx_usb_get_pipe_handle(usb, pipe),
+                     __cvmx_usb_get_submit_handle(usb, transaction),
+                     transaction->stage);
+
+    /* Make sure all writes to the DMA region get flushed */
+    CVMX_SYNCW;
+
+    /* Attach the channel to the pipe */
+    usb->pipe_for_channel[channel] = pipe;
+    pipe->channel = channel;
+    pipe->flags |= __CVMX_USB_PIPE_FLAGS_SCHEDULED;
+
+    /* Mark this channel as in use */
+    usb->idle_hardware_channels &= ~(1<<channel);
+
+    /* Enable the channel interrupt bits */
+    {
+        cvmx_usbcx_hcintx_t usbc_hcint;
+        cvmx_usbcx_hcintmskx_t usbc_hcintmsk;
+        cvmx_usbcx_haintmsk_t usbc_haintmsk;
+
+        /* Clear all channel status bits */
+        usbc_hcint.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_HCINTX(channel, usb->index));
+        __cvmx_usb_write_csr32(usb, CVMX_USBCX_HCINTX(channel, usb->index), usbc_hcint.u32);
+
+        /* Enable the channel halt interrupt */
+        usbc_hcintmsk.u32 = 0;
+        usbc_hcintmsk.s.chhltdmsk = 1;
+        __cvmx_usb_write_csr32(usb, CVMX_USBCX_HCINTMSKX(channel, usb->index), usbc_hcintmsk.u32);
+
+        /* Enable the channel interrupt to propagate */
+        usbc_haintmsk.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_HAINTMSK(usb->index));
+        usbc_haintmsk.s.haintmsk |= 1<<channel;
+        __cvmx_usb_write_csr32(usb, CVMX_USBCX_HAINTMSK(usb->index), usbc_haintmsk.u32);
+    }
+
+    /* Setup the locations the DMA engines use  */
+    {
+        uint64_t dma_address = transaction->buffer + transaction->actual_bytes;
+        if (transaction->type == CVMX_USB_TRANSFER_ISOCHRONOUS)
+            dma_address = transaction->buffer + transaction->iso_packets[0].offset + transaction->actual_bytes;
+        __cvmx_usb_write_csr64(usb, CVMX_USBNX_DMA0_OUTB_CHN0(usb->index) + channel*8, dma_address);
+        __cvmx_usb_write_csr64(usb, CVMX_USBNX_DMA0_INB_CHN0(usb->index) + channel*8, dma_address);
+    }
+
+    /* Setup both the size of the transfer and the SPLIT characteristics */
+    {
+        cvmx_usbcx_hcspltx_t usbc_hcsplt = {.u32 = 0};
+        cvmx_usbcx_hctsizx_t usbc_hctsiz = {.u32 = 0};
+        int bytes_to_transfer = transaction->buffer_length - transaction->actual_bytes;
+
+        /* ISOCHRONOUS transactions store each individual transfer size in the
+            packet structure, not the global buffer_length */
+        if (transaction->type == CVMX_USB_TRANSFER_ISOCHRONOUS)
+            bytes_to_transfer = transaction->iso_packets[0].length - transaction->actual_bytes;
+
+        /* We need to do split transactions when we are talking to non high
+            speed devices that are behind a high speed hub */
+        if (__cvmx_usb_pipe_needs_split(usb, pipe))
+        {
+            usbc_hcsplt.s.spltena = 1;
+            usbc_hcsplt.s.hubaddr = pipe->hub_device_addr;
+            usbc_hcsplt.s.prtaddr = pipe->hub_port;
+            usbc_hcsplt.s.compsplt = (transaction->stage == CVMX_USB_STAGE_NON_CONTROL_SPLIT_COMPLETE);
+
+            /* SPLIT transactions can only ever transmit one data packet so
+                limit the transfer size to the max packet size */
+            if (bytes_to_transfer > pipe->max_packet)
+                bytes_to_transfer = pipe->max_packet;
+
+            /* ISOCHRONOUS OUT splits are unique in that they limit
+                data transfers to 188 byte chunks representing the
+                begin/middle/end of the data or all */
+            if (!usbc_hcsplt.s.compsplt &&
+                (pipe->transfer_dir == CVMX_USB_DIRECTION_OUT) &&
+                (pipe->transfer_type == CVMX_USB_TRANSFER_ISOCHRONOUS))
+            {
+                /* See if we've started this tranfer and sent data */
+                if (transaction->actual_bytes == 0)
+                {
+                    /* Nothing sent yet, this is either a begin or the
+                        entire payload */
+                    if (bytes_to_transfer <= 188)
+                        usbc_hcsplt.s.xactpos = 3; /* Entire payload in one go */
+                    else
+                        usbc_hcsplt.s.xactpos = 2; /* First part of payload */
+                }
+                else
+                {
+                    /* Continuing the previous data, we must either be
+                        in the middle or at the end */
+                    if (bytes_to_transfer <= 188)
+                        usbc_hcsplt.s.xactpos = 1; /* End of payload */
+                    else
+                        usbc_hcsplt.s.xactpos = 0; /* Middle of payload */
+                }
+                /* Again, the transfer size is limited to 188 bytes */
+                if (bytes_to_transfer > 188)
+                    bytes_to_transfer = 188;
+            }
+        }
+
+        usbc_hctsiz.s.xfersize = bytes_to_transfer;
+        usbc_hctsiz.s.pktcnt = (bytes_to_transfer + pipe->max_packet - 1) / pipe->max_packet;
+        if (!usbc_hctsiz.s.pktcnt)
+            usbc_hctsiz.s.pktcnt = 1;
+
+        /* Update the DATA0/DATA1 toggle */
+        usbc_hctsiz.s.pid = __cvmx_usb_get_data_pid(pipe);
+        /* High speed pipes may need a hardware ping before they start */
+        if (pipe->flags & __CVMX_USB_PIPE_FLAGS_NEED_PING)
+            usbc_hctsiz.s.dopng = 1;
+
+        __cvmx_usb_write_csr32(usb, CVMX_USBCX_HCSPLTX(channel, usb->index), usbc_hcsplt.u32);
+        __cvmx_usb_write_csr32(usb, CVMX_USBCX_HCTSIZX(channel, usb->index), usbc_hctsiz.u32);
+    }
+
+    /* Setup the Host Channel Characteristics Register */
+    {
+        cvmx_usbcx_hccharx_t usbc_hcchar = {.u32 = 0};
+        /* Setup periodic transactions to go out the next frame by setting
+            the even/odd flag to not match the current frame */
+        if ((pipe->transfer_type == CVMX_USB_TRANSFER_INTERRUPT) ||
+            (pipe->transfer_type == CVMX_USB_TRANSFER_ISOCHRONOUS))
+        {
+            cvmx_usbcx_hfnum_t usbc_hfnum;
+            usbc_hfnum.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_HFNUM(usb->index));
+            usbc_hcchar.s.oddfrm = !(usbc_hfnum.s.frnum&1);
+        }
+
+        /* Set the number of back to back packets allowed by this endpoint */
+        if (pipe->multi_count < 1)
+            usbc_hcchar.s.ec = 1;
+        else if (pipe->multi_count > 3)
+            usbc_hcchar.s.ec = 3;
+        else
+            usbc_hcchar.s.ec = pipe->multi_count;
+
+        /* Set the rest of the endpoint specific settings */
+        usbc_hcchar.s.devaddr = pipe->device_addr;
+        usbc_hcchar.s.eptype = transaction->type;
+        usbc_hcchar.s.lspddev = (pipe->device_speed == CVMX_USB_SPEED_LOW);
+        usbc_hcchar.s.epdir = pipe->transfer_dir;
+        usbc_hcchar.s.epnum = pipe->endpoint_num;
+        usbc_hcchar.s.mps = pipe->max_packet;
+        __cvmx_usb_write_csr32(usb, CVMX_USBCX_HCCHARX(channel, usb->index), usbc_hcchar.u32);
+    }
+
+    /* Do transaction type specific fixups as needed */
+    switch (transaction->type)
+    {
+        case CVMX_USB_TRANSFER_CONTROL:
+            __cvmx_usb_start_channel_control(usb, channel, pipe);
+            break;
+        case CVMX_USB_TRANSFER_BULK:
+        case CVMX_USB_TRANSFER_INTERRUPT:
+            break;
+        case CVMX_USB_TRANSFER_ISOCHRONOUS:
+            if (!__cvmx_usb_pipe_needs_split(usb, pipe))
+            {
+                /* ISO transactions require differnet PIDs depending on direction
+                    and how many packets are needed */
+                if (pipe->transfer_dir == CVMX_USB_DIRECTION_OUT)
+                {
+                    if (pipe->multi_count < 2) /* Need DATA0 */
+                        USB_SET_FIELD32(CVMX_USBCX_HCTSIZX(channel, usb->index), cvmx_usbcx_hctsizx_t, pid, 0);
+                    else /* Need MDATA */
+                        USB_SET_FIELD32(CVMX_USBCX_HCTSIZX(channel, usb->index), cvmx_usbcx_hctsizx_t, pid, 3);
+                }
+            }
+            break;
+    }
+    {
+        cvmx_usbcx_hctsizx_t usbc_hctsiz = {.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_HCTSIZX(channel, usb->index))};
+        transaction->xfersize = usbc_hctsiz.s.xfersize;
+        transaction->pktcnt = usbc_hctsiz.s.pktcnt;
+    }
+    USB_SET_FIELD32(CVMX_USBCX_HCCHARX(channel, usb->index), cvmx_usbcx_hccharx_t, chena, 1);
+    CVMX_USB_RETURN_NOTHING();
+}
+
+
+/**
+ * @INTERNAL
+ * Called whenever a pipe might need to be scheduled to the
+ * hardware.
+ *
+ * @param usb    USB device state populated by
+ *               cvmx_usb_initialize().
+ */
+static void __cvmx_usb_schedule(cvmx_usb_internal_state_t *usb)
+{
+    int pipe_handle;
+    int channel;
+    cvmx_usb_pipe_t *pipe;
+    uint64_t current_cycle = cvmx_read64_uint64(CVMX_IPD_CLK_COUNT);
+
+    CVMX_USB_LOG_CALLED();
+    CVMX_USB_LOG_PARAM("%p", usb);
+
+    while (usb->idle_hardware_channels)
+    {
+        /* Find an idle channel */
+        CVMX_CLZ(channel, usb->idle_hardware_channels);
+        channel = 31 - channel;
+        if (channel > 7)
+        {
+            if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_INFO)
+                cvmx_dprintf("%s: Idle hardware channels has a channel higher than 7. This is wrong\n", __FUNCTION__);
+            break;
+        }
+
+        /* Find a pipe needing service. FIXME: This needs to be replaced with a
+            priority heap */
+        pipe = NULL;
+        for (pipe_handle=0; pipe_handle<MAX_PIPES; pipe_handle++)
+        {
+            if (!(usb->pipe[pipe_handle].flags & __CVMX_USB_PIPE_FLAGS_SCHEDULED) &&
+                usb->pipe[pipe_handle].head && (usb->pipe[pipe_handle].next_tx_cycle <= current_cycle))
+            {
+                pipe = usb->pipe + pipe_handle;
+                break;
+            }
+        }
+        if (!pipe)
+            break;
+
+        CVMX_USB_LOG_PARAM("%d", channel);
+        CVMX_USB_LOG_PARAM("%d", pipe_handle);
+        CVMX_USB_LOG_PARAM("%p", pipe);
+
+        pipe->next_tx_cycle += pipe->interval;
+        if (pipe->next_tx_cycle < current_cycle)
+            pipe->next_tx_cycle = current_cycle + pipe->interval;
+
+        if ((usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_TRANSFERS) ||
+            (pipe->flags & CVMX_USB_PIPE_FLAGS_DEBUG_TRANSFERS))
+        {
+            cvmx_usb_transaction_t *transaction = pipe->head;
+            const cvmx_usb_control_header_t *header = (transaction->control_header) ? cvmx_phys_to_ptr(transaction->control_header) : NULL;
+            const char *dir = (pipe->transfer_dir == CVMX_USB_DIRECTION_IN) ? "IN" : "OUT";
+            const char *type;
+            switch (pipe->transfer_type)
+            {
+                case CVMX_USB_TRANSFER_CONTROL:
+                    type = "SETUP";
+                    dir = (header->s.request_type & 0x80) ? "IN" : "OUT";
+                    break;
+                case CVMX_USB_TRANSFER_ISOCHRONOUS:
+                    type = "ISOCHRONOUS";
+                    break;
+                case CVMX_USB_TRANSFER_BULK:
+                    type = "BULK";
+                    break;
+                default: /* CVMX_USB_TRANSFER_INTERRUPT */
+                    type = "INTERRUPT";
+                    break;
+            }
+            cvmx_dprintf("%s: Starting pipe %d, transaction %d on channel %d. %s %s len=%d header=0x%llx\n",
+                         __FUNCTION__, __cvmx_usb_get_pipe_handle(usb, pipe),
+                         __cvmx_usb_get_submit_handle(usb, transaction),
+                         channel, type, dir,
+                         transaction->buffer_length,
+                         (header) ? (unsigned long long)header->u64 : 0ull);
+        }
+        __cvmx_usb_start_channel(usb, channel, pipe);
+    }
+    CVMX_USB_RETURN_NOTHING();
+}
+
+
+/**
+ * @INTERNAL
+ * Call a user's callback for a specific reason.
+ *
+ * @param usb    USB device state populated by
+ *               cvmx_usb_initialize().
+ * @param pipe   Pipe the callback is for or NULL
+ * @param transaction
+ *               Transaction the callback is for or NULL
+ * @param reason Reason this callback is being called
+ * @param complete_code
+ *               Completion code for the transaction, if any
+ */
+static void __cvmx_usb_perform_callback(cvmx_usb_internal_state_t *usb,
+                                        cvmx_usb_pipe_t *pipe,
+                                        cvmx_usb_transaction_t *transaction,
+                                        cvmx_usb_callback_t reason,
+                                        cvmx_usb_complete_t complete_code)
+{
+    cvmx_usb_callback_func_t callback = usb->callback[reason];
+    void *user_data = usb->callback_data[reason];
+    int submit_handle = -1;
+    int pipe_handle = -1;
+    int bytes_transferred = 0;
+
+    if (pipe)
+        pipe_handle = __cvmx_usb_get_pipe_handle(usb, pipe);
+
+    if (transaction)
+    {
+        submit_handle = __cvmx_usb_get_submit_handle(usb, transaction);
+        bytes_transferred = transaction->actual_bytes;
+        /* Transactions are allowed to override the default callback */
+        if ((reason == CVMX_USB_CALLBACK_TRANSFER_COMPLETE) && transaction->callback)
+        {
+            callback = transaction->callback;
+            user_data = transaction->callback_data;
+        }
+    }
+
+    if (!callback)
+        return;
+
+    if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_CALLBACKS)
+        cvmx_dprintf("%*s%s: calling callback %p(usb=%p, complete_code=%s, "
+                     "pipe_handle=%d, submit_handle=%d, bytes_transferred=%d, user_data=%p);\n",
+                     2*usb->indent, "", __FUNCTION__, callback, usb,
+                     __cvmx_usb_complete_to_string(complete_code),
+                     pipe_handle, submit_handle, bytes_transferred, user_data);
+
+    callback((cvmx_usb_state_t *)usb, reason, complete_code, pipe_handle, submit_handle,
+             bytes_transferred, user_data);
+
+    if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_CALLBACKS)
+        cvmx_dprintf("%*s%s: callback %p complete\n", 2*usb->indent, "",
+                      __FUNCTION__, callback);
+}
+
+
+/**
+ * @INTERNAL
+ * Signal the completion of a transaction and free it. The
+ * transaction will be removed from the pipe transaction list.
+ *
+ * @param usb    USB device state populated by
+ *               cvmx_usb_initialize().
+ * @param pipe   Pipe the transaction is on
+ * @param transaction
+ *               Transaction that completed
+ * @param complete_code
+ *               Completion code
+ */
+static void __cvmx_usb_perform_complete(cvmx_usb_internal_state_t * usb,
+                                        cvmx_usb_pipe_t *pipe,
+                                        cvmx_usb_transaction_t *transaction,
+                                        cvmx_usb_complete_t complete_code)
+{
+    int was_scheduled;
+    CVMX_USB_LOG_CALLED();
+    CVMX_USB_LOG_PARAM("%p", usb);
+    CVMX_USB_LOG_PARAM("%p", pipe);
+    CVMX_USB_LOG_PARAM("%p", transaction);
+    CVMX_USB_LOG_PARAM("%d", complete_code);
+
+    /* Determine if this transaction was scheduled when complete was called */
+    was_scheduled = (pipe->flags & __CVMX_USB_PIPE_FLAGS_SCHEDULED) && (pipe->head == transaction);
+
+    /* Isochronous transactions need extra processing as they might not be done
+        after a single data transfer */
+    if (transaction->type == CVMX_USB_TRANSFER_ISOCHRONOUS)
+    {
+        /* Update the number of bytes transfered in this ISO packet */
+        transaction->iso_packets[0].length = transaction->actual_bytes;
+        transaction->iso_packets[0].status = complete_code;
+
+        /* If there are more ISOs pending and we suceeded, schedule the next
+            one */
+        if ((transaction->iso_number_packets > 1) && (complete_code == CVMX_USB_COMPLETE_SUCCESS))
+        {
+            transaction->actual_bytes = 0;      /* No bytes transfered for this packet as of yet */
+            transaction->iso_number_packets--;  /* One less ISO waiting to transfer */
+            transaction->iso_packets++;         /* Increment to the next location in our packet array */
+            transaction->stage = CVMX_USB_STAGE_NON_CONTROL;
+            /* We only need to schedule if we were scheduled. This should
+                always be the case */
+            if (was_scheduled)
+            {
+                pipe->flags &= ~__CVMX_USB_PIPE_FLAGS_SCHEDULED;
+                usb->pipe_for_channel[pipe->channel] = NULL;
+                usb->idle_hardware_channels |= (1<<pipe->channel);
+                __cvmx_usb_schedule(usb);
+            }
+            goto done;
+        }
+    }
+
+    /* Count periodic transactions submitted. Once the last one is done, then
+        we don't need SOF interrupts */
+    if ((transaction->type == CVMX_USB_TRANSFER_ISOCHRONOUS) ||
+        (transaction->type == CVMX_USB_TRANSFER_INTERRUPT))
+    {
+        usb->number_periodic_transactions--;
+        if (usb->number_periodic_transactions == 0)
+            USB_SET_FIELD32(CVMX_USBCX_GINTMSK(usb->index), cvmx_usbcx_gintmsk_t, sofmsk, 0);
+    }
+
+    /* Remove the transaction from the pipe list */
+    if (transaction->next)
+        transaction->next->prev = transaction->prev;
+    else
+        pipe->tail = transaction->prev;
+    if (transaction->prev)
+        transaction->prev->next = transaction->next;
+    else
+        pipe->head = transaction->next;
+
+    if (was_scheduled)
+    {
+        pipe->flags &= ~__CVMX_USB_PIPE_FLAGS_SCHEDULED;
+        usb->pipe_for_channel[pipe->channel] = NULL;
+        usb->idle_hardware_channels |= (1<<pipe->channel);
+        __cvmx_usb_schedule(usb);
+    }
+    __cvmx_usb_perform_callback(usb, pipe, transaction,
+                                CVMX_USB_CALLBACK_TRANSFER_COMPLETE,
+                                complete_code);
+    __cvmx_usb_free_transaction(usb, transaction);
+done:
+    CVMX_USB_RETURN_NOTHING();
+}
+
+
+/**
+ * @INTERNAL
+ * Submit a usb transaction to a pipe. Called for all types
+ * of transactions.
+ *
+ * @param usb
+ * @param pipe_handle
+ *                  Which pipe to submit to. Will be validated in this function.
+ * @param type      Transaction type
+ * @param flags     Flags for the transaction
+ * @param buffer    User buffer for the transaction
+ * @param buffer_length
+ *                  User buffer's length in bytes
+ * @param control_header
+ *                  For control transactions, the 8 byte standard header
+ * @param iso_start_frame
+ *                  For ISO transactiosn, the start frame
+ * @param iso_number_packets
+ *                  For ISO, the number of packet in the transaction.
+ * @param iso_packets
+ *                  A description of each ISO packet
+ * @param callback  User callback to call when the transaction completes
+ * @param user_data User's data for the callback
+ *
+ * @return Submit handle or negative on failure. Matches the result
+ *         in the external API.
+ */
+static int __cvmx_usb_submit_transaction(cvmx_usb_internal_state_t *usb,
+                                         int pipe_handle,
+                                         cvmx_usb_transfer_t type,
+                                         int flags,
+                                         uint64_t buffer,
+                                         int buffer_length,
+                                         uint64_t control_header,
+                                         int iso_start_frame,
+                                         int iso_number_packets,
+                                         cvmx_usb_iso_packet_t *iso_packets,
+                                         cvmx_usb_callback_func_t callback,
+                                         void *user_data)
+{
+    int submit_handle;
+    cvmx_usb_transaction_t *transaction;
+    cvmx_usb_pipe_t *pipe = usb->pipe + pipe_handle;
+
+    CVMX_USB_LOG_CALLED();
+    if ((pipe_handle < 0) || (pipe_handle >= MAX_PIPES))
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    /* Fail if the pipe isn't open */
+    if ((pipe->flags & __CVMX_USB_PIPE_FLAGS_OPEN) == 0)
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    if (pipe->transfer_type != type)
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+
+    transaction = __cvmx_usb_alloc_transaction(usb);
+    if (!transaction)
+        CVMX_USB_RETURN(CVMX_USB_NO_MEMORY);
+
+    /* Count periodic transactions submitted. Once one is submitted, then
+        we need SOF interrupts */
+    if ((type == CVMX_USB_TRANSFER_ISOCHRONOUS) || (type == CVMX_USB_TRANSFER_INTERRUPT))
+    {
+        if (usb->number_periodic_transactions == 0)
+            USB_SET_FIELD32(CVMX_USBCX_GINTMSK(usb->index), cvmx_usbcx_gintmsk_t, sofmsk, 1);
+        usb->number_periodic_transactions++;
+    }
+
+    transaction->type = type;
+    transaction->flags |= flags;
+    transaction->buffer = buffer;
+    transaction->buffer_length = buffer_length;
+    transaction->control_header = control_header;
+    transaction->iso_start_frame = iso_start_frame; // FIXME: This is not used, implement it
+    transaction->iso_number_packets = iso_number_packets;
+    transaction->iso_packets = iso_packets;
+    transaction->callback = callback;
+    transaction->callback_data = user_data;
+    if (transaction->type == CVMX_USB_TRANSFER_CONTROL)
+        transaction->stage = CVMX_USB_STAGE_SETUP;
+    else
+        transaction->stage = CVMX_USB_STAGE_NON_CONTROL;
+
+    transaction->next = NULL;
+    if (pipe->tail)
+    {
+        transaction->prev = pipe->tail;
+        transaction->prev->next = transaction;
+    }
+    else
+    {
+        transaction->prev = NULL;
+        pipe->head = transaction;
+    }
+    pipe->tail = transaction;
+
+    submit_handle = __cvmx_usb_get_submit_handle(usb, transaction);
+
+    /* We may need to schedule the pipe if this was the head of the pipe */
+    if (!transaction->prev)
+        __cvmx_usb_schedule(usb);
+
+    CVMX_USB_RETURN(submit_handle);
+}
+
+
+/**
+ * Call to submit a USB Bulk transfer to a pipe.
+ *
+ * @param state     USB device state populated by
+ *                  cvmx_usb_initialize().
+ * @param pipe_handle
+ *                  Handle to the pipe for the transfer.
+ * @param buffer    Physical address of the data buffer in
+ *                  memory. Note that this is NOT A POINTER, but
+ *                  the full 64bit physical address of the
+ *                  buffer. This may be zero if buffer_length is
+ *                  zero.
+ * @param buffer_length
+ *                  Length of buffer in bytes.
+ * @param callback  Function to call when this transaction
+ *                  completes. If the return value of this
+ *                  function isn't an error, then this function
+ *                  is guaranteed to be called when the
+ *                  transaction completes. If this parameter is
+ *                  NULL, then the generic callback registered
+ *                  through cvmx_usb_register_callback is
+ *                  called. If both are NULL, then there is no
+ *                  way to know when a transaction completes.
+ * @param user_data User supplied data returned when the
+ *                  callback is called. This is only used if
+ *                  callback in not NULL.
+ *
+ * @return A submitted transaction handle or negative on
+ *         failure. Negative values are failure codes from
+ *         cvmx_usb_status_t.
+ */
+int cvmx_usb_submit_bulk(cvmx_usb_state_t *state, int pipe_handle,
+                                uint64_t buffer, int buffer_length,
+                                cvmx_usb_callback_func_t callback,
+                                void *user_data)
+{
+    int submit_handle;
+    cvmx_usb_internal_state_t *usb = (cvmx_usb_internal_state_t*)state;
+
+    CVMX_USB_LOG_CALLED();
+    CVMX_USB_LOG_PARAM("%p", state);
+    CVMX_USB_LOG_PARAM("%d", pipe_handle);
+    CVMX_USB_LOG_PARAM("0x%llx", (unsigned long long)buffer);
+    CVMX_USB_LOG_PARAM("%d", buffer_length);
+
+    /* Pipe handle checking is done later in a common place */
+    if (!buffer)
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    if (buffer_length < 0)
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEVICE_MODE)
+        CVMX_USB_RETURN(CVMX_USB_INCORRECT_MODE);
+
+    submit_handle = __cvmx_usb_submit_transaction(usb, pipe_handle,
+                                         CVMX_USB_TRANSFER_BULK,
+                                         0, /* flags */
+                                         buffer,
+                                         buffer_length,
+                                         0, /* control_header */
+                                         0, /* iso_start_frame */
+                                         0, /* iso_number_packets */
+                                         NULL, /* iso_packets */
+                                         callback,
+                                         user_data);
+    CVMX_USB_RETURN(submit_handle);
+}
+
+
+/**
+ * Call to submit a USB Interrupt transfer to a pipe.
+ *
+ * @param state     USB device state populated by
+ *                  cvmx_usb_initialize().
+ * @param pipe_handle
+ *                  Handle to the pipe for the transfer.
+ * @param buffer    Physical address of the data buffer in
+ *                  memory. Note that this is NOT A POINTER, but
+ *                  the full 64bit physical address of the
+ *                  buffer. This may be zero if buffer_length is
+ *                  zero.
+ * @param buffer_length
+ *                  Length of buffer in bytes.
+ * @param callback  Function to call when this transaction
+ *                  completes. If the return value of this
+ *                  function isn't an error, then this function
+ *                  is guaranteed to be called when the
+ *                  transaction completes. If this parameter is
+ *                  NULL, then the generic callback registered
+ *                  through cvmx_usb_register_callback is
+ *                  called. If both are NULL, then there is no
+ *                  way to know when a transaction completes.
+ * @param user_data User supplied data returned when the
+ *                  callback is called. This is only used if
+ *                  callback in not NULL.
+ *
+ * @return A submitted transaction handle or negative on
+ *         failure. Negative values are failure codes from
+ *         cvmx_usb_status_t.
+ */
+int cvmx_usb_submit_interrupt(cvmx_usb_state_t *state, int pipe_handle,
+                              uint64_t buffer, int buffer_length,
+                              cvmx_usb_callback_func_t callback,
+                              void *user_data)
+{
+    int submit_handle;
+    cvmx_usb_internal_state_t *usb = (cvmx_usb_internal_state_t*)state;
+
+    CVMX_USB_LOG_CALLED();
+    CVMX_USB_LOG_PARAM("%p", state);
+    CVMX_USB_LOG_PARAM("%d", pipe_handle);
+    CVMX_USB_LOG_PARAM("0x%llx", (unsigned long long)buffer);
+    CVMX_USB_LOG_PARAM("%d", buffer_length);
+
+    /* Pipe handle checking is done later in a common place */
+    if (!buffer)
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    if (buffer_length < 0)
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEVICE_MODE)
+        CVMX_USB_RETURN(CVMX_USB_INCORRECT_MODE);
+
+    submit_handle = __cvmx_usb_submit_transaction(usb, pipe_handle,
+                                         CVMX_USB_TRANSFER_INTERRUPT,
+                                         0, /* flags */
+                                         buffer,
+                                         buffer_length,
+                                         0, /* control_header */
+                                         0, /* iso_start_frame */
+                                         0, /* iso_number_packets */
+                                         NULL, /* iso_packets */
+                                         callback,
+                                         user_data);
+    CVMX_USB_RETURN(submit_handle);
+}
+
+
+/**
+ * Call to submit a USB Control transfer to a pipe.
+ *
+ * @param state     USB device state populated by
+ *                  cvmx_usb_initialize().
+ * @param pipe_handle
+ *                  Handle to the pipe for the transfer.
+ * @param control_header
+ *                  USB 8 byte control header physical address.
+ *                  Note that this is NOT A POINTER, but the
+ *                  full 64bit physical address of the buffer.
+ * @param buffer    Physical address of the data buffer in
+ *                  memory. Note that this is NOT A POINTER, but
+ *                  the full 64bit physical address of the
+ *                  buffer. This may be zero if buffer_length is
+ *                  zero.
+ * @param buffer_length
+ *                  Length of buffer in bytes.
+ * @param callback  Function to call when this transaction
+ *                  completes. If the return value of this
+ *                  function isn't an error, then this function
+ *                  is guaranteed to be called when the
+ *                  transaction completes. If this parameter is
+ *                  NULL, then the generic callback registered
+ *                  through cvmx_usb_register_callback is
+ *                  called. If both are NULL, then there is no
+ *                  way to know when a transaction completes.
+ * @param user_data User supplied data returned when the
+ *                  callback is called. This is only used if
+ *                  callback in not NULL.
+ *
+ * @return A submitted transaction handle or negative on
+ *         failure. Negative values are failure codes from
+ *         cvmx_usb_status_t.
+ */
+int cvmx_usb_submit_control(cvmx_usb_state_t *state, int pipe_handle,
+                            uint64_t control_header,
+                            uint64_t buffer, int buffer_length,
+                            cvmx_usb_callback_func_t callback,
+                            void *user_data)
+{
+    int submit_handle;
+    cvmx_usb_internal_state_t *usb = (cvmx_usb_internal_state_t*)state;
+
+    CVMX_USB_LOG_CALLED();
+    CVMX_USB_LOG_PARAM("%p", state);
+    CVMX_USB_LOG_PARAM("%d", pipe_handle);
+    CVMX_USB_LOG_PARAM("0x%llx", (unsigned long long)control_header);
+    CVMX_USB_LOG_PARAM("0x%llx", (unsigned long long)buffer);
+    CVMX_USB_LOG_PARAM("%d", buffer_length);
+
+    /* Pipe handle checking is done later in a common place */
+    if (!control_header)
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    if (buffer && (buffer_length <= 0))
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    if (!buffer && (buffer_length != 0))
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEVICE_MODE)
+        CVMX_USB_RETURN(CVMX_USB_INCORRECT_MODE);
+
+    submit_handle = __cvmx_usb_submit_transaction(usb, pipe_handle,
+                                         CVMX_USB_TRANSFER_CONTROL,
+                                         0, /* flags */
+                                         buffer,
+                                         buffer_length,
+                                         control_header,
+                                         0, /* iso_start_frame */
+                                         0, /* iso_number_packets */
+                                         NULL, /* iso_packets */
+                                         callback,
+                                         user_data);
+    CVMX_USB_RETURN(submit_handle);
+}
+
+
+/**
+ * Call to submit a USB Isochronous transfer to a pipe.
+ *
+ * @param state     USB device state populated by
+ *                  cvmx_usb_initialize().
+ * @param pipe_handle
+ *                  Handle to the pipe for the transfer.
+ * @param start_frame
+ *                  Number of frames into the future to schedule
+ *                  this transaction.
+ * @param flags     Flags to control the transfer. See
+ *                  cvmx_usb_isochronous_flags_t for the flag
+ *                  definitions.
+ * @param number_packets
+ *                  Number of sequential packets to transfer.
+ *                  "packets" is a pointer to an array of this
+ *                  many packet structures.
+ * @param packets   Description of each transfer packet as
+ *                  defined by cvmx_usb_iso_packet_t. The array
+ *                  pointed to here must stay valid until the
+ *                  complete callback is called.
+ * @param buffer    Physical address of the data buffer in
+ *                  memory. Note that this is NOT A POINTER, but
+ *                  the full 64bit physical address of the
+ *                  buffer. This may be zero if buffer_length is
+ *                  zero.
+ * @param buffer_length
+ *                  Length of buffer in bytes.
+ * @param callback  Function to call when this transaction
+ *                  completes. If the return value of this
+ *                  function isn't an error, then this function
+ *                  is guaranteed to be called when the
+ *                  transaction completes. If this parameter is
+ *                  NULL, then the generic callback registered
+ *                  through cvmx_usb_register_callback is
+ *                  called. If both are NULL, then there is no
+ *                  way to know when a transaction completes.
+ * @param user_data User supplied data returned when the
+ *                  callback is called. This is only used if
+ *                  callback in not NULL.
+ *
+ * @return A submitted transaction handle or negative on
+ *         failure. Negative values are failure codes from
+ *         cvmx_usb_status_t.
+ */
+int cvmx_usb_submit_isochronous(cvmx_usb_state_t *state, int pipe_handle,
+                                int start_frame, int flags,
+                                int number_packets,
+                                cvmx_usb_iso_packet_t packets[],
+                                uint64_t buffer, int buffer_length,
+                                cvmx_usb_callback_func_t callback,
+                                void *user_data)
+{
+    int submit_handle;
+    cvmx_usb_internal_state_t *usb = (cvmx_usb_internal_state_t*)state;
+
+    CVMX_USB_LOG_CALLED();
+    CVMX_USB_LOG_PARAM("%p", state);
+    CVMX_USB_LOG_PARAM("%d", pipe_handle);
+    CVMX_USB_LOG_PARAM("%d", start_frame);
+    CVMX_USB_LOG_PARAM("0x%x", flags);
+    CVMX_USB_LOG_PARAM("%d", number_packets);
+    CVMX_USB_LOG_PARAM("%p", packets);
+    CVMX_USB_LOG_PARAM("0x%llx", (unsigned long long)buffer);
+    CVMX_USB_LOG_PARAM("%d", buffer_length);
+
+    /* Pipe handle checking is done later in a common place */
+    if (start_frame < 0)
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    if (flags & ~(CVMX_USB_ISOCHRONOUS_FLAGS_ALLOW_SHORT | CVMX_USB_ISOCHRONOUS_FLAGS_ASAP))
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    if (number_packets < 1)
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    if (!packets)
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    if (!buffer)
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    if (buffer_length < 0)
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEVICE_MODE)
+        CVMX_USB_RETURN(CVMX_USB_INCORRECT_MODE);
+
+    submit_handle = __cvmx_usb_submit_transaction(usb, pipe_handle,
+                                         CVMX_USB_TRANSFER_ISOCHRONOUS,
+                                         flags,
+                                         buffer,
+                                         buffer_length,
+                                         0, /* control_header */
+                                         start_frame,
+                                         number_packets,
+                                         packets,
+                                         callback,
+                                         user_data);
+    CVMX_USB_RETURN(submit_handle);
+}
+
+
+/**
+ * Cancel one outstanding request in a pipe. Canceling a request
+ * can fail if the transaction has already completed before cancel
+ * is called. Even after a successful cancel call, it may take
+ * a frame or two for the cvmx_usb_poll() function to call the
+ * associated callback.
+ *
+ * @param state  USB device state populated by
+ *               cvmx_usb_initialize().
+ * @param pipe_handle
+ *               Pipe handle to cancel requests in.
+ * @param submit_handle
+ *               Handle to transaction to cancel, returned by the submit function.
+ *
+ * @return CVMX_USB_SUCCESS or a negative error code defined in
+ *         cvmx_usb_status_t.
+ */
+cvmx_usb_status_t cvmx_usb_cancel(cvmx_usb_state_t *state, int pipe_handle,
+                                  int submit_handle)
+{
+    cvmx_usb_transaction_t *transaction;
+    cvmx_usb_internal_state_t *usb = (cvmx_usb_internal_state_t*)state;
+    cvmx_usb_pipe_t *pipe = usb->pipe + pipe_handle;
+
+    CVMX_USB_LOG_CALLED();
+    CVMX_USB_LOG_PARAM("%p", state);
+    CVMX_USB_LOG_PARAM("%d", pipe_handle);
+    CVMX_USB_LOG_PARAM("%d", submit_handle);
+
+    if ((pipe_handle < 0) || (pipe_handle >= MAX_PIPES))
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    if ((submit_handle < 0) || (submit_handle >= MAX_TRANSACTIONS))
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEVICE_MODE)
+        CVMX_USB_RETURN(CVMX_USB_INCORRECT_MODE);
+
+    /* Fail if the pipe isn't open */
+    if ((pipe->flags & __CVMX_USB_PIPE_FLAGS_OPEN) == 0)
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+
+    transaction = usb->transaction + submit_handle;
+
+    /* Fail if this transaction already completed */
+    if ((transaction->flags & __CVMX_USB_TRANSACTION_FLAGS_IN_USE) == 0)
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+
+    /* If the transaction is the HEAD of the queue and scheduled. We need to
+        treat it special */
+    if ((pipe->head == transaction) &&
+        (pipe->flags & __CVMX_USB_PIPE_FLAGS_SCHEDULED))
+    {
+        cvmx_usbcx_hccharx_t usbc_hcchar;
+        transaction->flags |= __CVMX_USB_TRANSACTION_FLAGS_CANCEL;
+        usbc_hcchar.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_HCCHARX(pipe->channel, usb->index));
+        /* If the channel isn't enabled then the transaction already completed */
+        if (usbc_hcchar.s.chena)
+        {
+            usbc_hcchar.s.chdis = 1;
+            __cvmx_usb_write_csr32(usb, CVMX_USBCX_HCCHARX(pipe->channel, usb->index), usbc_hcchar.u32);
+        }
+        /* The complete callback should be called in the next poll */
+    }
+    else
+        __cvmx_usb_perform_complete(usb, pipe, transaction, CVMX_USB_COMPLETE_CANCEL);
+    CVMX_USB_RETURN(CVMX_USB_SUCCESS);
+}
+
+
+/**
+ * Cancel all outstanding requests in a pipe. Logically all this
+ * does is call cvmx_usb_cancel() in a loop.
+ *
+ * @param state  USB device state populated by
+ *               cvmx_usb_initialize().
+ * @param pipe_handle
+ *               Pipe handle to cancel requests in.
+ *
+ * @return CVMX_USB_SUCCESS or a negative error code defined in
+ *         cvmx_usb_status_t.
+ */
+cvmx_usb_status_t cvmx_usb_cancel_all(cvmx_usb_state_t *state, int pipe_handle)
+{
+    cvmx_usb_internal_state_t *usb = (cvmx_usb_internal_state_t*)state;
+    cvmx_usb_pipe_t *pipe = usb->pipe + pipe_handle;
+
+    CVMX_USB_LOG_CALLED();
+    CVMX_USB_LOG_PARAM("%p", state);
+    CVMX_USB_LOG_PARAM("%d", pipe_handle);
+    if ((pipe_handle < 0) || (pipe_handle >= MAX_PIPES))
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEVICE_MODE)
+        CVMX_USB_RETURN(CVMX_USB_INCORRECT_MODE);
+
+    /* Fail if the pipe isn't open */
+    if ((pipe->flags & __CVMX_USB_PIPE_FLAGS_OPEN) == 0)
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+
+    /* Simply loop through and attempt to cancel each transaction */
+    while (pipe->tail)
+    {
+        cvmx_usb_status_t result = cvmx_usb_cancel(state, pipe_handle,
+            __cvmx_usb_get_submit_handle(usb, pipe->tail));
+        if (result != CVMX_USB_SUCCESS)
+            CVMX_USB_RETURN(result);
+        /* The head transaction may take some time to cancel, so we need to
+            bail otherwise we might loop forever */
+        if (pipe->head == pipe->tail)
+            break;
+    }
+    CVMX_USB_RETURN(CVMX_USB_SUCCESS);
+}
+
+
+/**
+ * Close a pipe created with cvmx_usb_open_pipe().
+ *
+ * @param state  USB device state populated by
+ *               cvmx_usb_initialize().
+ * @param pipe_handle
+ *               Pipe handle to close.
+ *
+ * @return CVMX_USB_SUCCESS or a negative error code defined in
+ *         cvmx_usb_status_t. CVMX_USB_BUSY is returned if the
+ *         pipe has outstanding transfers.
+ */
+cvmx_usb_status_t cvmx_usb_close_pipe(cvmx_usb_state_t *state, int pipe_handle)
+{
+    cvmx_usb_internal_state_t *usb = (cvmx_usb_internal_state_t*)state;
+    cvmx_usb_pipe_t *pipe = usb->pipe + pipe_handle;
+
+    CVMX_USB_LOG_CALLED();
+    CVMX_USB_LOG_PARAM("%p", state);
+    CVMX_USB_LOG_PARAM("%d", pipe_handle);
+    if ((pipe_handle < 0) || (pipe_handle >= MAX_PIPES))
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEVICE_MODE)
+        CVMX_USB_RETURN(CVMX_USB_INCORRECT_MODE);
+
+    /* Fail if the pipe isn't open */
+    if ((pipe->flags & __CVMX_USB_PIPE_FLAGS_OPEN) == 0)
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+
+    /* Fail if the pipe has pending transactions */
+    if (pipe->head)
+        CVMX_USB_RETURN(CVMX_USB_BUSY);
+
+    pipe->flags = 0;
+
+    CVMX_USB_RETURN(CVMX_USB_SUCCESS);
+}
+
+
+/**
+ * Register a function to be called when various USB events occur.
+ *
+ * @param state     USB device state populated by
+ *                  cvmx_usb_initialize().
+ * @param reason    Which event to register for.
+ * @param callback  Function to call when the event occurs.
+ * @param user_data User data parameter to the function.
+ *
+ * @return CVMX_USB_SUCCESS or a negative error code defined in
+ *         cvmx_usb_status_t.
+ */
+cvmx_usb_status_t cvmx_usb_register_callback(cvmx_usb_state_t *state,
+                                             cvmx_usb_callback_t reason,
+                                             cvmx_usb_callback_func_t callback,
+                                             void *user_data)
+{
+    cvmx_usb_internal_state_t *usb = (cvmx_usb_internal_state_t*)state;
+
+    CVMX_USB_LOG_CALLED();
+    CVMX_USB_LOG_PARAM("%p", state);
+    CVMX_USB_LOG_PARAM("%d", reason);
+    CVMX_USB_LOG_PARAM("%p", callback);
+    CVMX_USB_LOG_PARAM("%p", user_data);
+    if (reason >= __CVMX_USB_CALLBACK_END)
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    if (!callback)
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+
+    usb->callback[reason] = callback;
+    usb->callback_data[reason] = user_data;
+
+    CVMX_USB_RETURN(CVMX_USB_SUCCESS);
+}
+
+
+/**
+ * Get the current USB protocol level frame number. The frame
+ * number is always in the range of 0-0x7ff.
+ *
+ * @param state  USB device state populated by
+ *               cvmx_usb_initialize().
+ *
+ * @return USB frame number
+ */
+int cvmx_usb_get_frame_number(cvmx_usb_state_t *state)
+{
+    int frame_number;
+    cvmx_usb_internal_state_t *usb = (cvmx_usb_internal_state_t*)state;
+
+    CVMX_USB_LOG_CALLED();
+    CVMX_USB_LOG_PARAM("%p", state);
+
+    if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEVICE_MODE)
+    {
+        cvmx_usbcx_dsts_t usbc_dsts;
+        usbc_dsts.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_DSTS(usb->index));
+        frame_number = usbc_dsts.s.soffn;
+    }
+    else
+    {
+        cvmx_usbcx_hfnum_t usbc_hfnum;
+        usbc_hfnum.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_HFNUM(usb->index));
+        frame_number = usbc_hfnum.s.frnum;
+    }
+
+    CVMX_USB_RETURN(frame_number);
+}
+
+
+/**
+ * @INTERNAL
+ * Poll a channel for status
+ *
+ * @param usb     USB device
+ * @param channel Channel to poll
+ *
+ * @return Zero on success
+ */
+static int __cvmx_usb_poll_channel(cvmx_usb_internal_state_t *usb, int channel)
+{
+    cvmx_usbcx_hcintx_t usbc_hcint;
+    cvmx_usbcx_hctsizx_t usbc_hctsiz;
+    cvmx_usbcx_hccharx_t usbc_hcchar;
+    cvmx_usb_pipe_t *pipe;
+    cvmx_usb_transaction_t *transaction;
+    int bytes_this_transfer;
+    int buffer_space_left;
+    CVMX_USB_LOG_CALLED();
+    CVMX_USB_LOG_PARAM("%p", usb);
+    CVMX_USB_LOG_PARAM("%d", channel);
+
+    /* Read the interrupt status bits for the channel */
+    usbc_hcint.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_HCINTX(channel, usb->index));
+
+    /* We ignore any interrupts where the channel hasn't halted yet. These
+        should be impossible since we don't enable any interrupts except for
+        channel halted */
+    if (!usbc_hcint.s.chhltd)
+        CVMX_USB_RETURN(0);
+
+    /* Now that the channel has halted, clear all status bits before
+        processing. This way we don't have any race conditions caused by the
+        channel starting up and finishing before we clear the bits */
+    __cvmx_usb_write_csr32(usb, CVMX_USBCX_HCINTX(channel, usb->index), usbc_hcint.u32);
+    //cvmx_csr_db_decode(cvmx_get_proc_id(), CVMX_USBCX_HCINTX(channel, usb->index), usbc_hcint.u32);
+
+    /* Make sure this channel is tied to a valid pipe */
+    pipe = usb->pipe_for_channel[channel];
+    if (!pipe)
+        CVMX_USB_RETURN(0);
+    transaction = pipe->head;
+
+    /* Read the channel config info so we can figure out how much data
+        transfered */
+    usbc_hcchar.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_HCCHARX(channel, usb->index));
+    usbc_hctsiz.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_HCTSIZX(channel, usb->index));
+
+    /* Calculating the number of bytes successfully transfered is dependent on
+        the transfer direction */
+    if (usbc_hcchar.s.epdir)
+    {
+        /* IN transactions are easy. For every byte received the hardware
+            decrements xfersize. All we need to do is subtract the current
+            value of xfersize from its starting value and we know how many
+            bytes were written to the buffer */
+        bytes_this_transfer = transaction->xfersize - usbc_hctsiz.s.xfersize;
+    }
+    else
+    {
+        /* OUT transaction don't decrement xfersize. Instead pktcnt is
+            decremented on every successful packet send. The hardware does
+            this when it receives an ACK, or NYET. If it doesn't
+            receive one of these responses pktcnt doesn't change */
+        int packets_sent = transaction->pktcnt - usbc_hctsiz.s.pktcnt;
+        bytes_this_transfer = packets_sent * usbc_hcchar.s.mps;
+        /* The last packet may not be a full transfer if we didn't have
+            enough data */
+        if (bytes_this_transfer > transaction->xfersize)
+            bytes_this_transfer = transaction->xfersize;
+    }
+
+    /* As a special case, setup transactions output the setup header, not
+        the user's data. For this reason we don't count setup data as bytes
+        transfered */
+    if ((transaction->stage == CVMX_USB_STAGE_SETUP) ||
+        (transaction->stage == CVMX_USB_STAGE_SETUP_SPLIT_COMPLETE))
+        bytes_this_transfer = 0;
+
+    /* Optional debug output */
+    if ((usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_TRANSFERS) ||
+        (pipe->flags & CVMX_USB_PIPE_FLAGS_DEBUG_TRANSFERS))
+        cvmx_dprintf("%s: Channel %d halted. Pipe %d transaction %d stage %d bytes=%d\n",
+                     __FUNCTION__, channel,
+                     __cvmx_usb_get_pipe_handle(usb, pipe),
+                     __cvmx_usb_get_submit_handle(usb, transaction),
+                     transaction->stage, bytes_this_transfer);
+
+    /* Add the bytes transfered to the running total. It is important that
+        bytes_this_transfer doesn't count any data that needs to be
+        retransmitted */
+    transaction->actual_bytes += bytes_this_transfer;
+    if (transaction->type == CVMX_USB_TRANSFER_ISOCHRONOUS)
+        buffer_space_left = transaction->iso_packets[0].length - transaction->actual_bytes;
+    else
+        buffer_space_left = transaction->buffer_length - transaction->actual_bytes;
+
+    /* We need to remember the PID toggle state for the next transaction. The
+        hardware already updated it for the next transaction */
+    pipe->pid_toggle = !(usbc_hctsiz.s.pid == 0);
+
+    /* For high speed bulk out, assume the next transaction will need to do a
+        ping before proceeding. If this isn't true the ACK processing below
+        will clear this flag */
+    if ((pipe->device_speed == CVMX_USB_SPEED_HIGH) &&
+        (pipe->transfer_type == CVMX_USB_TRANSFER_BULK) &&
+        (pipe->transfer_dir == CVMX_USB_DIRECTION_OUT))
+        pipe->flags |= __CVMX_USB_PIPE_FLAGS_NEED_PING;
+
+    /* Ignore status of canceled transactions and just tell the user they are
+        done */
+    if (transaction->flags & __CVMX_USB_TRANSACTION_FLAGS_CANCEL)
+    {
+        __cvmx_usb_perform_complete(usb, pipe, transaction, CVMX_USB_COMPLETE_CANCEL);
+    }
+    else if (usbc_hcint.s.stall)
+    {
+        /* STALL as a response means this transaction cannot be completed
+            because the device can't process transactions. Tell the user. Any
+            data that was transfered will be counted on the actual bytes
+            transfered */
+        __cvmx_usb_perform_complete(usb, pipe, transaction, CVMX_USB_COMPLETE_STALL);
+    }
+    else if (0 && usbc_hcint.s.xfercompl)
+    {
+        /* XferCompl is only useful in non DMA mode */
+    }
+    else if (usbc_hcint.s.xacterr)
+    {
+        transaction->retries++;
+        if (transaction->retries > MAX_RETRIES)
+        {
+            /* XactErr as a response means the device signaled something wrong with
+                the transfer. For example, PID toggle errors cause these */
+            __cvmx_usb_perform_complete(usb, pipe, transaction, CVMX_USB_COMPLETE_XACTERR);
+        }
+        else
+        {
+            /* Rewind to the beginning of the transaction by anding off the
+                split complete bit */
+            transaction->stage &= ~1;
+            __cvmx_usb_start_channel(usb, channel, pipe);
+        }
+    }
+    else if (usbc_hcint.s.datatglerr)
+    {
+        /* Data Toggle Error (DataTglErr) */
+        transaction->retries++;
+        if (transaction->retries > MAX_RETRIES)
+        {
+            __cvmx_usb_perform_complete(usb, pipe, transaction, CVMX_USB_COMPLETE_DATATGLERR);
+        }
+        else
+        {
+            /* Rewind to the beginning of the transaction by anding off the
+                split complete bit */
+            transaction->stage &= ~1;
+            __cvmx_usb_start_channel(usb, channel, pipe);
+        }
+    }
+    else if (usbc_hcint.s.bblerr)
+    {
+        /* Babble Error (BblErr) */
+        __cvmx_usb_perform_complete(usb, pipe, transaction, CVMX_USB_COMPLETE_BABBLEERR);
+    }
+    else if (usbc_hcint.s.frmovrun)
+    {
+        /* Frame Overrun (FrmOvrun) */
+        /* Rewind to the beginning of the transaction by anding off the
+            split complete bit */
+        transaction->stage &= ~1;
+        __cvmx_usb_start_channel(usb, channel, pipe);
+    }
+    else if (usbc_hcint.s.nyet)
+    {
+        transaction->retries = 0;
+        /* NYET as a response is only allowed in three cases: as a response to
+            a ping, as a response to a split transaction, and as a response to
+            a bulk out. The ping case is handled by hardware, so we only have
+            splits and bulk out */
+        if (__cvmx_usb_pipe_needs_split(usb, pipe))
+        {
+            /* NYET on periodic splits need to be retried at the next interval.
+                Other transactions must be retried immediately */
+            if (pipe->transfer_type == CVMX_USB_TRANSFER_INTERRUPT)
+            {
+                /* NYET as a response to a split means we need to try again later.
+                    Just resubmit it, but do a full schedule. This way periodic
+                    requests wait for their next interval and other pipes might
+                    get a chance */
+                pipe->flags &= ~__CVMX_USB_PIPE_FLAGS_SCHEDULED;
+                usb->pipe_for_channel[pipe->channel] = NULL;
+                usb->idle_hardware_channels |= (1<<pipe->channel);
+                /* Rewind to the beginning of the transaction by anding off the
+                    split complete bit */
+                transaction->stage &= ~1;
+                __cvmx_usb_schedule(usb);
+            }
+            else
+                __cvmx_usb_start_channel(usb, channel, pipe);
+        }
+        else
+        {
+            /* If there is more data to go then we need to try again. Otherwise
+                this transaction is complete */
+            if (buffer_space_left && (bytes_this_transfer >= pipe->max_packet))
+                __cvmx_usb_start_channel(usb, channel, pipe);
+            else
+                __cvmx_usb_perform_complete(usb, pipe, transaction, CVMX_USB_COMPLETE_SUCCESS);
+        }
+    }
+    else if (usbc_hcint.s.ack)
+    {
+        transaction->retries = 0;
+        /* The ACK bit can only be checked after the other error bits. This is
+            because a multi packet transfer may succeed in a number of packets
+            and then get a different response on the last packet. In this case
+            both ACK and the last response bit will be set. If none of the
+            other response bits is set, then the last packet must have been an
+            ACK */
+
+        /* Since we got an ACK, we know we don't need to do a ping on this
+            pipe */
+        pipe->flags &= ~__CVMX_USB_PIPE_FLAGS_NEED_PING;
+
+        switch (transaction->type)
+        {
+            case CVMX_USB_TRANSFER_CONTROL:
+                switch (transaction->stage)
+                {
+                    case CVMX_USB_STAGE_NON_CONTROL:
+                    case CVMX_USB_STAGE_NON_CONTROL_SPLIT_COMPLETE:
+                        /* This should be impossible */
+                        __cvmx_usb_perform_complete(usb, pipe, transaction, CVMX_USB_COMPLETE_ERROR);
+                        break;
+                    case CVMX_USB_STAGE_SETUP:
+                        pipe->pid_toggle = 1;
+                        if (__cvmx_usb_pipe_needs_split(usb, pipe))
+                            transaction->stage = CVMX_USB_STAGE_SETUP_SPLIT_COMPLETE;
+                        else
+                        {
+                            cvmx_usb_control_header_t *header = cvmx_phys_to_ptr(transaction->control_header);
+                            if (header->s.length)
+                                transaction->stage = CVMX_USB_STAGE_DATA;
+                            else
+                                transaction->stage = CVMX_USB_STAGE_STATUS;
+                        }
+                        __cvmx_usb_start_channel(usb, channel, pipe);
+                        break;
+                    case CVMX_USB_STAGE_SETUP_SPLIT_COMPLETE:
+                        {
+                            cvmx_usb_control_header_t *header = cvmx_phys_to_ptr(transaction->control_header);
+                            if (header->s.length)
+                                transaction->stage = CVMX_USB_STAGE_DATA;
+                            else
+                                transaction->stage = CVMX_USB_STAGE_STATUS;
+                        }
+                        __cvmx_usb_start_channel(usb, channel, pipe);
+                        break;
+                    case CVMX_USB_STAGE_DATA:
+                        if (__cvmx_usb_pipe_needs_split(usb, pipe))
+                            transaction->stage = CVMX_USB_STAGE_DATA_SPLIT_COMPLETE;
+                        else if ((buffer_space_left == 0) || (bytes_this_transfer < pipe->max_packet))
+                        {
+                            pipe->pid_toggle = 1;
+                            transaction->stage = CVMX_USB_STAGE_STATUS;
+                        }
+                        __cvmx_usb_start_channel(usb, channel, pipe);
+                        break;
+                    case CVMX_USB_STAGE_DATA_SPLIT_COMPLETE:
+                        if ((buffer_space_left == 0) || (bytes_this_transfer < pipe->max_packet))
+                        {
+                            pipe->pid_toggle = 1;
+                            transaction->stage = CVMX_USB_STAGE_STATUS;
+                        }
+                        else
+                        {
+                            transaction->stage = CVMX_USB_STAGE_DATA;
+                        }
+                        __cvmx_usb_start_channel(usb, channel, pipe);
+                        break;
+                    case CVMX_USB_STAGE_STATUS:
+                        if (__cvmx_usb_pipe_needs_split(usb, pipe))
+                        {
+                            transaction->stage = CVMX_USB_STAGE_STATUS_SPLIT_COMPLETE;
+                            __cvmx_usb_start_channel(usb, channel, pipe);
+                        }
+                        else
+                            __cvmx_usb_perform_complete(usb, pipe, transaction, CVMX_USB_COMPLETE_SUCCESS);
+                        break;
+                    case CVMX_USB_STAGE_STATUS_SPLIT_COMPLETE:
+                        __cvmx_usb_perform_complete(usb, pipe, transaction, CVMX_USB_COMPLETE_SUCCESS);
+                        break;
+                }
+                break;
+            case CVMX_USB_TRANSFER_BULK:
+            case CVMX_USB_TRANSFER_INTERRUPT:
+                /* The only time a bulk/interrupt transfer isn't complete when
+                    it  finishes with an ACK is during a split transaction. For
+                    splits we need to continue the transfer */
+                if (__cvmx_usb_pipe_needs_split(usb, pipe))
+                {
+                    if (transaction->stage == CVMX_USB_STAGE_NON_CONTROL_SPLIT_COMPLETE)
+                    {
+                        if (buffer_space_left && (bytes_this_transfer >= pipe->max_packet))
+                        {
+                            transaction->stage = CVMX_USB_STAGE_NON_CONTROL;
+                            __cvmx_usb_start_channel(usb, channel, pipe);
+                        }
+                        else
+                            __cvmx_usb_perform_complete(usb, pipe, transaction, CVMX_USB_COMPLETE_SUCCESS);
+                    }
+                    else
+                    {
+                        transaction->stage = CVMX_USB_STAGE_NON_CONTROL_SPLIT_COMPLETE;
+                        __cvmx_usb_start_channel(usb, channel, pipe);
+                    }
+                }
+                else
+                    __cvmx_usb_perform_complete(usb, pipe, transaction, CVMX_USB_COMPLETE_SUCCESS);
+                break;
+            case CVMX_USB_TRANSFER_ISOCHRONOUS:
+                if (__cvmx_usb_pipe_needs_split(usb, pipe))
+                {
+                    /* ISOCHRONOUS OUT splits don't require a complete split stage.
+                        Instead they use a sequence of begin OUT splits to transfer
+                        the data 188 bytes at a time. Once the transfer is complete,
+                        the pipe sleeps until the next schedule interval */
+                    if (pipe->transfer_dir == CVMX_USB_DIRECTION_OUT)
+                    {
+                        /* If no space left or this wasn't a max size packet then
+                            this transfer is complete. Otherwise start it again
+                            to send the next 188 bytes */
+                        if (!buffer_space_left || (bytes_this_transfer < 188))
+                            __cvmx_usb_perform_complete(usb, pipe, transaction, CVMX_USB_COMPLETE_SUCCESS);
+                        else
+                            __cvmx_usb_start_channel(usb, channel, pipe);
+                    }
+                    else
+                    {
+                        if (transaction->stage == CVMX_USB_STAGE_NON_CONTROL_SPLIT_COMPLETE)
+                        {
+                            /* We are in the incomming data phase. Keep getting
+                                data until we run out of space or get a small
+                                packet */
+                            if (buffer_space_left && (bytes_this_transfer >= pipe->max_packet))
+                                __cvmx_usb_start_channel(usb, channel, pipe);
+                            else
+                                __cvmx_usb_perform_complete(usb, pipe, transaction, CVMX_USB_COMPLETE_SUCCESS);
+                        }
+                        else
+                        {
+                            transaction->stage = CVMX_USB_STAGE_NON_CONTROL_SPLIT_COMPLETE;
+                            __cvmx_usb_start_channel(usb, channel, pipe);
+                        }
+                    }
+                }
+                else
+                    __cvmx_usb_perform_complete(usb, pipe, transaction, CVMX_USB_COMPLETE_SUCCESS);
+                break;
+        }
+    }
+    else if (usbc_hcint.s.nak)
+    {
+        /* NAK as a response means the device couldn't accept the transaction,
+            but it should be retried in the future. Just resubmit it, but do a
+            full schedule. This way periodic requests wait for their next
+            interval and other pipes might get a chance */
+        pipe->flags &= ~__CVMX_USB_PIPE_FLAGS_SCHEDULED;
+        usb->pipe_for_channel[pipe->channel] = NULL;
+        usb->idle_hardware_channels |= (1<<pipe->channel);
+        /* Rewind to the beginning of the transaction by anding off the
+            split complete bit */
+        transaction->stage &= ~1;
+        __cvmx_usb_schedule(usb);
+    }
+    else
+    {
+        /* We get channel halted interrupts with no result bits sets when the
+            cable is unplugged */
+        __cvmx_usb_perform_complete(usb, pipe, transaction, CVMX_USB_COMPLETE_ERROR);
+    }
+    CVMX_USB_RETURN(0);
+}
+
+
+/**
+ * Poll a device mode endpoint for status
+ *
+ * @param usb    USB device state populated by
+ *               cvmx_usb_initialize().
+ * @param endpoint_num
+ *               Endpoint to poll
+ *
+ * @return Zero on success
+ */
+static int __cvmx_usb_poll_endpoint(cvmx_usb_internal_state_t *usb, int endpoint_num)
+{
+    cvmx_usbcx_diepintx_t usbc_diepint;
+    cvmx_usbcx_doepintx_t usbc_doepint;
+
+    CVMX_USB_LOG_CALLED();
+    CVMX_USB_LOG_PARAM("%p", usb);
+    CVMX_USB_LOG_PARAM("%d", endpoint_num);
+
+    usbc_diepint.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_DIEPINTX(endpoint_num, usb->index));
+    __cvmx_usb_write_csr32(usb, CVMX_USBCX_DIEPINTX(endpoint_num, usb->index), usbc_diepint.u32);
+    cvmx_csr_db_decode(cvmx_get_proc_id(), CVMX_USBCX_DIEPINTX(endpoint_num, usb->index), usbc_diepint.u32);
+    if (usbc_diepint.s.inepnakeff)
+    {
+        /* IN Endpoint NAK Effective (INEPNakEff)
+            Applies to periodic IN endpoints only.
+            Indicates that the IN endpoint NAK bit set by the application has
+            taken effect in the core. This bit can be cleared when the
+            application clears the IN endpoint NAK by writing to
+            DIEPCTLn.CNAK.
+            This interrupt indicates that the core has sampled the NAK bit
+            set (either by the application or by the core).
+            This interrupt does not necessarily mean that a NAK handshake
+            is sent on the USB. A STALL bit takes priority over a NAK bit. */
+        /* Nothing to do */
+    }
+    if (usbc_diepint.s.intknepmis)
+    {
+        /* IN Token Received with EP Mismatch (INTknEPMis)
+            Applies to non-periodic IN endpoints only.
+            Indicates that the data in the top of the non-periodic TxFIFO
+            belongs to an endpoint other than the one for which the IN
+            token was received. This interrupt is asserted on the endpoint
+            for which the IN token was received. */
+        if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_INFO)
+            cvmx_dprintf("%s: Endpoint %d mismatch\n", __FUNCTION__, endpoint_num);
+    }
+    if (usbc_diepint.s.intkntxfemp)
+    {
+        /* IN Token Received When TxFIFO is Empty (INTknTXFEmp)
+            Applies only to non-periodic IN endpoints.
+            Indicates that an IN token was received when the associated
+            TxFIFO (periodic/non-periodic) was empty. This interrupt is
+            asserted on the endpoint for which the IN token was received. */
+        if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_INFO)
+            cvmx_dprintf("%s: Received IN token on endpoint %d without data\n", __FUNCTION__, endpoint_num);
+    }
+    if (usbc_diepint.s.timeout)
+    {
+        /* Timeout Condition (TimeOUT)
+            Applies to non-isochronous IN endpoints only.
+            Indicates that the core has detected a timeout condition on the
+            USB for the last IN token on this endpoint. */
+        if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_INFO)
+            cvmx_dprintf("%s: Received timeout on endpoint %d\n", __FUNCTION__, endpoint_num);
+    }
+    if (usbc_diepint.s.ahberr)
+    {
+        /* AHB Error (AHBErr)
+            This is generated only in Internal DMA mode when there is an
+            AHB error during an AHB read/write. The application can read
+            the corresponding endpoint DMA address register to get the
+            error address. */
+        if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_INFO)
+            cvmx_dprintf("%s: AHB error on endpoint %d\n", __FUNCTION__, endpoint_num);
+    }
+    if (usbc_diepint.s.epdisbld)
+    {
+        /* Endpoint Disabled Interrupt (EPDisbld)
+            This bit indicates that the endpoint is disabled per the
+            application's request. */
+        /* Nothing to do */
+    }
+    if (usbc_diepint.s.xfercompl)
+    {
+        /* Transfer Completed Interrupt (XferCompl)
+            Indicates that the programmed transfer is complete on the AHB
+            as well as on the USB, for this endpoint. */
+        __cvmx_usb_perform_callback(usb, usb->pipe + endpoint_num, NULL,
+                                    CVMX_USB_CALLBACK_TRANSFER_COMPLETE,
+                                    CVMX_USB_COMPLETE_SUCCESS);
+    }
+
+    usbc_doepint.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_DOEPINTX(endpoint_num, usb->index));
+    __cvmx_usb_write_csr32(usb, CVMX_USBCX_DOEPINTX(endpoint_num, usb->index), usbc_doepint.u32);
+    cvmx_csr_db_decode(cvmx_get_proc_id(), CVMX_USBCX_DOEPINTX(endpoint_num, usb->index), usbc_doepint.u32);
+    if (usbc_doepint.s.outtknepdis)
+    {
+        /* OUT Token Received When Endpoint Disabled (OUTTknEPdis)
+            Applies only to control OUT endpoints.
+            Indicates that an OUT token was received when the endpoint
+            was not yet enabled. This interrupt is asserted on the endpoint
+            for which the OUT token was received. */
+        if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_INFO)
+            cvmx_dprintf("%s: Received OUT token on disabled endpoint %d\n", __FUNCTION__, endpoint_num);
+    }
+    if (usbc_doepint.s.setup)
+    {
+        /* SETUP Phase Done (SetUp)
+            Applies to control OUT endpoints only.
+            Indicates that the SETUP phase for the control endpoint is
+            complete and no more back-to-back SETUP packets were
+            received for the current control transfer. On this interrupt, the
+            application can decode the received SETUP data packet. */
+        __cvmx_usb_perform_callback(usb, usb->pipe + endpoint_num, NULL,
+                                    CVMX_USB_CALLBACK_DEVICE_SETUP,
+                                    CVMX_USB_COMPLETE_SUCCESS);
+    }
+    if (usbc_doepint.s.ahberr)
+    {
+        /* AHB Error (AHBErr)
+            This is generated only in Internal DMA mode when there is an
+            AHB error during an AHB read/write. The application can read
+            the corresponding endpoint DMA address register to get the
+            error address. */
+        if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_INFO)
+            cvmx_dprintf("%s: AHB error on endpoint %d\n", __FUNCTION__, endpoint_num);
+    }
+    if (usbc_doepint.s.epdisbld)
+    {
+        /* Endpoint Disabled Interrupt (EPDisbld)
+            This bit indicates that the endpoint is disabled per the
+            application's request. */
+        /* Nothing to do */
+    }
+    if (usbc_doepint.s.xfercompl)
+    {
+        /* Transfer Completed Interrupt (XferCompl)
+            Indicates that the programmed transfer is complete on the AHB
+            as well as on the USB, for this endpoint. */
+        __cvmx_usb_perform_callback(usb, usb->pipe + endpoint_num, NULL,
+                                    CVMX_USB_CALLBACK_TRANSFER_COMPLETE,
+                                    CVMX_USB_COMPLETE_SUCCESS);
+    }
+
+    CVMX_USB_RETURN(0);
+}
+
+
+/**
+ * Poll the device mode endpoints for status
+ *
+ * @param usb  USB device state populated by
+ *               cvmx_usb_initialize().
+ *
+ * @return Zero on success
+ */
+static int __cvmx_usb_poll_endpoints(cvmx_usb_internal_state_t *usb)
+{
+    cvmx_usbcx_daint_t usbc_daint;
+    int active_endpoints;
+
+    CVMX_USB_LOG_CALLED();
+    CVMX_USB_LOG_PARAM("%p", usb);
+
+    usbc_daint.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_DAINT(usb->index));
+    active_endpoints = usbc_daint.s.inepint | usbc_daint.s.outepint;
+
+    while (active_endpoints)
+    {
+        int endpoint;
+        CVMX_CLZ(endpoint, active_endpoints);
+        endpoint = 31 - endpoint;
+        __cvmx_usb_poll_endpoint(usb, endpoint);
+        active_endpoints ^= 1<<endpoint;
+    }
+
+    CVMX_USB_RETURN(0);
+}
+
+
+/**
+ * Poll the USB block for status and call all needed callback
+ * handlers. This function is meant to be called in the interrupt
+ * handler for the USB controller. It can also be called
+ * periodically in a loop for non-interrupt based operation.
+ *
+ * @param state  USB device state populated by
+ *               cvmx_usb_initialize().
+ *
+ * @return CVMX_USB_SUCCESS or a negative error code defined in
+ *         cvmx_usb_status_t.
+ */
+cvmx_usb_status_t cvmx_usb_poll(cvmx_usb_state_t *state)
+{
+    cvmx_usbcx_gintsts_t usbc_gintsts;
+    cvmx_usb_internal_state_t *usb = (cvmx_usb_internal_state_t*)state;
+
+    CVMX_USB_LOG_CALLED();
+    CVMX_USB_LOG_PARAM("%p", state);
+
+    /* Read the pending interrupts */
+    usbc_gintsts.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_GINTSTS(usb->index));
+
+    if (usbc_gintsts.s.wkupint)
+    {
+        /* Resume/Remote Wakeup Detected Interrupt (WkUpInt)
+            In Device mode, this interrupt is asserted when a resume is
+            detected on the USB. In Host mode, this interrupt is asserted
+            when a remote wakeup is detected on the USB. */
+        /* Octeon doesn't support suspend / resume */
+    }
+    if (usbc_gintsts.s.sessreqint)
+    {
+        /* Session Request/New Session Detected Interrupt (SessReqInt)
+            In Host mode, this interrupt is asserted when a session request
+            is detected from the device. In Device mode, this interrupt is
+            asserted when the utmiotg_bvalid signal goes high. */
+        /* Octeon doesn't support OTG */
+    }
+    if (usbc_gintsts.s.disconnint || usbc_gintsts.s.prtint)
+    {
+        cvmx_usbcx_hprt_t usbc_hprt;
+        /* Disconnect Detected Interrupt (DisconnInt)
+            Asserted when a device disconnect is detected. */
+
+        /* Host Port Interrupt (PrtInt)
+            The core sets this bit to indicate a change in port status of one
+            of the O2P USB core ports in Host mode. The application must
+            read the Host Port Control and Status (HPRT) register to
+            determine the exact event that caused this interrupt. The
+            application must clear the appropriate status bit in the Host Port
+            Control and Status register to clear this bit. */
+
+        /* Call the user's port callback */
+        __cvmx_usb_perform_callback(usb, NULL, NULL,
+                                    CVMX_USB_CALLBACK_PORT_CHANGED,
+                                    CVMX_USB_COMPLETE_SUCCESS);
+        /* Clear the port change bits */
+        usbc_hprt.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_HPRT(usb->index));
+        usbc_hprt.s.prtena = 0;
+        __cvmx_usb_write_csr32(usb, CVMX_USBCX_HPRT(usb->index), usbc_hprt.u32);
+    }
+    if (usbc_gintsts.s.conidstschng)
+    {
+        /* Connector ID Status Change (ConIDStsChng)
+            The core sets this bit when there is a change in connector ID
+            status. */
+        /* The USB core currently doesn't support dynamically changing from
+            host to device mode */
+    }
+    if (usbc_gintsts.s.ptxfemp)
+    {
+        /* Periodic TxFIFO Empty (PTxFEmp)
+            Asserted when the Periodic Transmit FIFO is either half or
+            completely empty and there is space for at least one entry to be
+            written in the Periodic Request Queue. The half or completely
+            empty status is determined by the Periodic TxFIFO Empty Level
+            bit in the Core AHB Configuration register
+            (GAHBCFG.PTxFEmpLvl). */
+        /* In DMA mode we don't care */
+    }
+    if (usbc_gintsts.s.hchint)
+    {
+        /* Host Channels Interrupt (HChInt)
+            The core sets this bit to indicate that an interrupt is pending on
+            one of the channels of the core (in Host mode). The application
+            must read the Host All Channels Interrupt (HAINT) register to
+            determine the exact number of the channel on which the
+            interrupt occurred, and then read the corresponding Host
+            Channel-n Interrupt (HCINTn) register to determine the exact
+            cause of the interrupt. The application must clear the
+            appropriate status bit in the HCINTn register to clear this bit. */
+        cvmx_usbcx_haint_t usbc_haint;
+        usbc_haint.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_HAINT(usb->index));
+        while (usbc_haint.u32)
+        {
+            int channel;
+            CVMX_CLZ(channel, usbc_haint.u32);
+            channel = 31 - channel;
+            __cvmx_usb_poll_channel(usb, channel);
+            usbc_haint.u32 ^= 1<<channel;
+        }
+    }
+    if (usbc_gintsts.s.fetsusp)
+    {
+        /* Data Fetch Suspended (FetSusp)
+            This interrupt is valid only in DMA mode. This interrupt indicates
+            that the core has stopped fetching data for IN endpoints due to
+            the unavailability of TxFIFO space or Request Queue space.
+            This interrupt is used by the application for an endpoint
+            mismatch algorithm. */
+        // FIXME
+    }
+    if (usbc_gintsts.s.incomplp)
+    {
+        /* Incomplete Periodic Transfer (incomplP)
+            In Host mode, the core sets this interrupt bit when there are
+            incomplete periodic transactions still pending which are
+            scheduled for the current microframe.
+            Incomplete Isochronous OUT Transfer (incompISOOUT)
+            The Device mode, the core sets this interrupt to indicate that
+            there is at least one isochronous OUT endpoint on which the
+            transfer is not completed in the current microframe. This
+            interrupt is asserted along with the End of Periodic Frame
+            Interrupt (EOPF) bit in this register. */
+        // FIXME
+    }
+    if (usbc_gintsts.s.incompisoin)
+    {
+        /* Incomplete Isochronous IN Transfer (incompISOIN)
+            The core sets this interrupt to indicate that there is at least one
+            isochronous IN endpoint on which the transfer is not completed
+            in the current microframe. This interrupt is asserted along with
+            the End of Periodic Frame Interrupt (EOPF) bit in this register. */
+        // FIXME
+    }
+    if (usbc_gintsts.s.oepint)
+    {
+        /* OUT Endpoints Interrupt (OEPInt)
+            The core sets this bit to indicate that an interrupt is pending on
+            one of the OUT endpoints of the core (in Device mode). The
+            application must read the Device All Endpoints Interrupt
+            (DAINT) register to determine the exact number of the OUT
+            endpoint on which the interrupt occurred, and then read the
+            corresponding Device OUT Endpoint-n Interrupt (DOEPINTn)
+            register to determine the exact cause of the interrupt. The
+            application must clear the appropriate status bit in the
+            corresponding DOEPINTn register to clear this bit. */
+        __cvmx_usb_poll_endpoints(usb);
+    }
+    if (usbc_gintsts.s.iepint)
+    {
+        /* IN Endpoints Interrupt (IEPInt)
+            The core sets this bit to indicate that an interrupt is pending on
+            one of the IN endpoints of the core (in Device mode). The
+            application must read the Device All Endpoints Interrupt
+            (DAINT) register to determine the exact number of the IN
+            endpoint on which the interrupt occurred, and then read the
+            corresponding Device IN Endpoint-n Interrupt (DIEPINTn)
+            register to determine the exact cause of the interrupt. The
+            application must clear the appropriate status bit in the
+            corresponding DIEPINTn register to clear this bit. */
+        __cvmx_usb_poll_endpoints(usb);
+    }
+    if (usbc_gintsts.s.epmis)
+    {
+        /* Endpoint Mismatch Interrupt (EPMis)
+            Indicates that an IN token has been received for a non-periodic
+            endpoint, but the data for another endpoint is present in the top
+            of the Non-Periodic Transmit FIFO and the IN endpoint
+            mismatch count programmed by the application has expired. */
+        // FIXME
+    }
+    if (usbc_gintsts.s.eopf)
+    {
+        /* End of Periodic Frame Interrupt (EOPF)
+            Indicates that the period specified in the Periodic Frame Interval
+            field of the Device Configuration register (DCFG.PerFrInt) has
+            been reached in the current microframe. */
+        // FIXME
+    }
+    if (usbc_gintsts.s.isooutdrop)
+    {
+        /* Isochronous OUT Packet Dropped Interrupt (ISOOutDrop)
+            The core sets this bit when it fails to write an isochronous OUT
+            packet into the RxFIFO because the RxFIFO doesn't have
+            enough space to accommodate a maximum packet size packet
+            for the isochronous OUT endpoint. */
+        // FIXME
+    }
+    if (usbc_gintsts.s.enumdone)
+    {
+        /* Enumeration Done (EnumDone)
+            The core sets this bit to indicate that speed enumeration is
+            complete. The application must read the Device Status (DSTS)
+            register to obtain the enumerated speed. */
+        if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEVICE_MODE)
+        {
+            cvmx_usbcx_dsts_t usbc_dsts;
+            usbc_dsts.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_DSTS(usb->index));
+            if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_INFO)
+                cvmx_dprintf("%s: USB%d Enumeration complete with %s speed\n",
+                             __FUNCTION__, usb->index,
+                             (usbc_dsts.s.enumspd == CVMX_USB_SPEED_HIGH) ? "high" :
+                             (usbc_dsts.s.enumspd == CVMX_USB_SPEED_FULL) ? "full" :
+                             "low");
+            USB_SET_FIELD32(CVMX_USBCX_DIEPCTLX(0, usb->index),
+                            cvmx_usbcx_diepctlx_t, mps,
+                            (usbc_dsts.s.enumspd == CVMX_USB_SPEED_LOW) ? 3 : 0);
+            USB_SET_FIELD32(CVMX_USBCX_DOEPCTLX(0, usb->index),
+                            cvmx_usbcx_doepctlx_t, epena, 1);
+        }
+    }
+    if (usbc_gintsts.s.usbrst)
+    {
+        /* USB Reset (USBRst)
+            The core sets this bit to indicate that a reset is
+            detected on the USB. */
+        if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEVICE_MODE)
+        {
+            if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_INFO)
+                cvmx_dprintf("%s: USB%d Reset complete\n", __FUNCTION__, usb->index);
+            __cvmx_usb_device_reset_complete(usb);
+        }
+    }
+    if (usbc_gintsts.s.nptxfemp)
+    {
+        /* Non-Periodic TxFIFO Empty (NPTxFEmp)
+            This interrupt is asserted when the Non-Periodic TxFIFO is
+            either half or completely empty, and there is space for at least
+            one entry to be written to the Non-Periodic Transmit Request
+            Queue. The half or completely empty status is determined by
+            the Non-Periodic TxFIFO Empty Level bit in the Core AHB
+            Configuration register (GAHBCFG.NPTxFEmpLvl). */
+        /* In DMA mode this is handled by hardware */
+    }
+    if (usbc_gintsts.s.rxflvl)
+    {
+        /* RxFIFO Non-Empty (RxFLvl)
+            Indicates that there is at least one packet pending to be read
+            from the RxFIFO. */
+        /* In DMA mode this is handled by hardware */
+    }
+    if (usbc_gintsts.s.sof)
+    {
+        /* Start of (micro)Frame (Sof)
+            In Host mode, the core sets this bit to indicate that an SOF
+            (FS), micro-SOF (HS), or Keep-Alive (LS) is transmitted on the
+            USB. The application must write a 1 to this bit to clear the
+            interrupt.
+            In Device mode, in the core sets this bit to indicate that an SOF
+            token has been received on the USB. The application can read
+            the Device Status register to get the current (micro)frame
+            number. This interrupt is seen only when the core is operating
+            at either HS or FS. */
+        __cvmx_usb_schedule(usb);
+    }
+    if (usbc_gintsts.s.otgint)
+    {
+        /* OTG Interrupt (OTGInt)
+            The core sets this bit to indicate an OTG protocol event. The
+            application must read the OTG Interrupt Status (GOTGINT)
+            register to determine the exact event that caused this interrupt.
+            The application must clear the appropriate status bit in the
+            GOTGINT register to clear this bit. */
+        /* Octeon doesn't support OTG, so ignore */
+    }
+    if (usbc_gintsts.s.modemis)
+    {
+        /* Mode Mismatch Interrupt (ModeMis)
+            The core sets this bit when the application is trying to access:
+            * A Host mode register, when the core is operating in Device
+            mode
+            * A Device mode register, when the core is operating in Host
+            mode
+            The register access is completed on the AHB with an OKAY
+            response, but is ignored by the core internally and doesn't
+            affect the operation of the core. */
+        /* Ignored for now */
+    }
+
+    /* Clear the interrupts now that we know about them */
+    __cvmx_usb_write_csr32(usb, CVMX_USBCX_GINTSTS(usb->index), usbc_gintsts.u32);
+
+    CVMX_USB_RETURN(CVMX_USB_SUCCESS);
+}
+
+
+/**
+ * Enable an endpoint for use in device mode. After this call
+ * transactions will be allowed over the endpoint. This must be
+ * called after every usb reset.
+ *
+ * @param state  USB device state populated by
+ *               cvmx_usb_initialize().
+ * @param endpoint_num
+ *               The endpoint number to enable (0-4)
+ * @param transfer_type
+ *               USB transfer type of this endpoint
+ * @param transfer_dir
+ *               Direction of transfer relative to Octeon
+ * @param max_packet_size
+ *               Maximum packet size support by this endpoint
+ * @param buffer Buffer to send/receive
+ * @param buffer_length
+ *               Length of the buffer in bytes
+ *
+ * @return CVMX_USB_SUCCESS or a negative error code defined in
+ *         cvmx_usb_status_t.
+ */
+cvmx_usb_status_t cvmx_usb_device_enable_endpoint(cvmx_usb_state_t *state,
+                                                  int endpoint_num,
+                                                  cvmx_usb_transfer_t transfer_type,
+                                                  cvmx_usb_direction_t transfer_dir,
+                                                  int max_packet_size,
+                                                  uint64_t buffer,
+                                                  int buffer_length)
+{
+    cvmx_usb_internal_state_t *usb = (cvmx_usb_internal_state_t*)state;
+
+    CVMX_USB_LOG_CALLED();
+    CVMX_USB_LOG_PARAM("%p", state);
+    CVMX_USB_LOG_PARAM("%d", endpoint_num);
+    CVMX_USB_LOG_PARAM("%d", transfer_type);
+    CVMX_USB_LOG_PARAM("%d", transfer_dir);
+    CVMX_USB_LOG_PARAM("%d", max_packet_size);
+    CVMX_USB_LOG_PARAM("0x%llx", (unsigned long long)buffer);
+    CVMX_USB_LOG_PARAM("%d", buffer_length);
+
+    if ((endpoint_num < 0) || (endpoint_num > 4))
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    if (transfer_type > CVMX_USB_TRANSFER_INTERRUPT)
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    if ((transfer_dir != CVMX_USB_DIRECTION_OUT) &&
+        (transfer_dir != CVMX_USB_DIRECTION_IN))
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    if ((max_packet_size < 0) || (max_packet_size > 512))
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    if (!buffer)
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    if (buffer_length < 0)
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    if ((usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEVICE_MODE) == 0)
+        CVMX_USB_RETURN(CVMX_USB_INCORRECT_MODE);
+
+    /* Setup the locations the DMA engines use  */
+    __cvmx_usb_write_csr64(usb, CVMX_USBNX_DMA0_OUTB_CHN0(usb->index) + endpoint_num*8, buffer);
+    __cvmx_usb_write_csr64(usb, CVMX_USBNX_DMA0_INB_CHN0(usb->index) + endpoint_num*8, buffer);
+
+    if (transfer_dir == CVMX_USB_DIRECTION_IN)
+    {
+        cvmx_usbcx_doepctlx_t usbc_doepctl;
+        cvmx_usbcx_doeptsizx_t usbc_doeptsiz;
+
+        usbc_doeptsiz.u32 = 0;
+        usbc_doeptsiz.s.mc = 1; // FIXME
+        usbc_doeptsiz.s.pktcnt = (buffer_length + max_packet_size - 1) / max_packet_size;
+        if (usbc_doeptsiz.s.pktcnt == 0)
+            usbc_doeptsiz.s.pktcnt = 1;
+        usbc_doeptsiz.s.xfersize = buffer_length;
+        __cvmx_usb_write_csr32(usb, CVMX_USBCX_DOEPTSIZX(endpoint_num, usb->index), usbc_doeptsiz.u32);
+
+        usbc_doepctl.u32 = 0;
+        usbc_doepctl.s.epena = 1;
+        usbc_doepctl.s.setd1pid = 0; // FIXME
+        usbc_doepctl.s.setd0pid = 0; // FIXME
+        usbc_doepctl.s.cnak = 1;
+        usbc_doepctl.s.eptype = transfer_type;
+        usbc_doepctl.s.usbactep = 1;
+        usbc_doepctl.s.mps = max_packet_size;
+        __cvmx_usb_write_csr32(usb, CVMX_USBCX_DOEPCTLX(endpoint_num, usb->index), usbc_doepctl.u32);
+    }
+    else
+    {
+        cvmx_usbcx_diepctlx_t usbc_diepctl;
+        cvmx_usbcx_dieptsizx_t usbc_dieptsiz;
+
+        usbc_dieptsiz.u32 = 0;
+        usbc_dieptsiz.s.mc = 1; // FIXME
+        usbc_dieptsiz.s.pktcnt = (buffer_length + max_packet_size - 1) / max_packet_size;
+        if (usbc_dieptsiz.s.pktcnt == 0)
+            usbc_dieptsiz.s.pktcnt = 1;
+        usbc_dieptsiz.s.xfersize = buffer_length;
+        __cvmx_usb_write_csr32(usb, CVMX_USBCX_DIEPTSIZX(endpoint_num, usb->index), usbc_dieptsiz.u32);
+
+        usbc_diepctl.u32 = 0;
+        usbc_diepctl.s.epena = 1;
+        usbc_diepctl.s.setd1pid = 0; // FIXME
+        usbc_diepctl.s.setd0pid = 0; // FIXME
+        usbc_diepctl.s.cnak = 1;
+        if ((transfer_type == CVMX_USB_TRANSFER_INTERRUPT) ||
+            (transfer_type == CVMX_USB_TRANSFER_ISOCHRONOUS))
+            usbc_diepctl.s.txfnum = endpoint_num; // FIXME
+        else
+            usbc_diepctl.s.txfnum = 0;
+        usbc_diepctl.s.eptype = transfer_type;
+        usbc_diepctl.s.usbactep = 1;
+        usbc_diepctl.s.nextep = endpoint_num - 1; // FIXME
+        usbc_diepctl.s.mps = max_packet_size;
+        __cvmx_usb_write_csr32(usb, CVMX_USBCX_DIEPCTLX(endpoint_num, usb->index), usbc_diepctl.u32);
+    }
+
+    CVMX_USB_RETURN(CVMX_USB_SUCCESS);
+}
+
+
+/**
+ * Disable an endpoint in device mode.
+ *
+ * @param state  USB device state populated by
+ *               cvmx_usb_initialize().
+ * @param endpoint_num
+ *               The endpoint number to disable (0-4)
+ *
+ * @return CVMX_USB_SUCCESS or a negative error code defined in
+ *         cvmx_usb_status_t.
+ */
+cvmx_usb_status_t cvmx_usb_device_disable_endpoint(cvmx_usb_state_t *state,
+                                                   int endpoint_num)
+{
+    cvmx_usb_internal_state_t *usb = (cvmx_usb_internal_state_t*)state;
+
+    CVMX_USB_LOG_CALLED();
+    CVMX_USB_LOG_PARAM("%p", state);
+    CVMX_USB_LOG_PARAM("%d", endpoint_num);
+
+    if ((endpoint_num < 0) || (endpoint_num > 4))
+        CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
+    if ((usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEVICE_MODE) == 0)
+        CVMX_USB_RETURN(CVMX_USB_INCORRECT_MODE);
+
+    USB_SET_FIELD32(CVMX_USBCX_DOEPCTLX(endpoint_num, usb->index),
+                    cvmx_usbcx_doepctlx_t, epdis, 1);
+    USB_SET_FIELD32(CVMX_USBCX_DIEPCTLX(endpoint_num, usb->index),
+                    cvmx_usbcx_diepctlx_t, epdis, 1);
+
+    CVMX_USB_RETURN(CVMX_USB_SUCCESS);
+}
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-usb.h b/arch/mips/cavium-octeon/executive/cvmx-usb.h
new file mode 100644
index 0000000..7ce62b0
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-usb.h
@@ -0,0 +1,1133 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * "cvmx-usb.h" defines a set of low level USB functions to help
+ * developers create Octeon USB drivers for various operating
+ * systems. These functions provide a generic API to the Octeon
+ * USB blocks, hiding the internal hardware specific
+ * operations.
+ *
+ * At a high level the device driver needs to:
+ *
+ * -# Call cvmx_usb_get_num_ports() to get the number of
+ *  supported ports.
+ * -# Call cvmx_usb_initialize() for each Octeon USB port.
+ * -# Enable the port using cvmx_usb_enable().
+ * -# Either periodically, or in an interrupt handler, call
+ *  cvmx_usb_poll() to service USB events.
+ * -# Manage pipes using cvmx_usb_open_pipe() and
+ *  cvmx_usb_close_pipe().
+ * -# Manage transfers using cvmx_usb_submit_*() and
+ *  cvmx_usb_cancel*().
+ * -# Shutdown USB on unload using cvmx_usb_shutdown().
+ *
+ * To monitor USB status changes, the device driver must use
+ * cvmx_usb_register_callback() to register for events that it
+ * is interested in. Below are a few hints on successfully
+ * implementing a driver on top of this API.
+ *
+ * <h2>Initialization</h2>
+ *
+ * When a driver is first loaded, it is normally not necessary
+ * to bring up the USB port completely. Most operating systems
+ * expect to initialize and enable the port in two independent
+ * steps. Normally an operating system will probe hardware,
+ * initialize anything found, and then enable the hardware.
+ *
+ * In the probe phase you should:
+ * -# Use cvmx_usb_get_num_ports() to determine the number of
+ *  USB port to be supported.
+ * -# Allocate space for a cvmx_usb_state_t structure for each
+ *  port.
+ * -# Tell the operating system about each port
+ *
+ * In the initialization phase you should:
+ * -# Use cvmx_usb_initialize() on each port.
+ * -# Do not call cvmx_usb_enable(). This leaves the USB port in
+ *  the disabled state until the operating system is ready.
+ *
+ * Finally, in the enable phase you should:
+ * -# Call cvmx_usb_enable() on the appropriate port.
+ * -# Note that some operating system use a RESET instead of an
+ *  enable call. To implement RESET, you should call
+ *  cvmx_usb_disable() followed by cvmx_usb_enable().
+ *
+ * <h2>Locking</h2>
+ *
+ * All of the functions in the cvmx-usb API assume exclusive
+ * access to the USB hardware and internal data structures. This
+ * means that the driver must provide locking as necessary.
+ *
+ * In the single CPU state it is normally enough to disable
+ * interrupts before every call to cvmx_usb*() and enable them
+ * again after the call is complete. Keep in mind that it is
+ * very common for the callback handlers to make additional
+ * calls into cvmx-usb, so the disable/enable must be protected
+ * against recursion. As an example, the Linux kernel
+ * local_irq_save() and local_irq_restore() are perfect for this
+ * in the non SMP case.
+ *
+ * In the SMP case, locking is more complicated. For SMP you not
+ * only need to disable interrupts on the local core, but also
+ * take a lock to make sure that another core cannot call
+ * cvmx-usb.
+ *
+ * <h2>Port callback</h2>
+ *
+ * The port callback prototype needs to look as follows:
+ *
+ * void port_callback(cvmx_usb_state_t *usb,
+ *                    cvmx_usb_callback_t reason,
+ *                    cvmx_usb_complete_t status,
+ *                    int pipe_handle,
+ *                    int submit_handle,
+ *                    int bytes_transferred,
+ *                    void *user_data);
+ * - @b usb is the cvmx_usb_state_t for the port.
+ * - @b reason will always be
+ *   CVMX_USB_CALLBACK_PORT_CHANGED.
+ * - @b status will always be CVMX_USB_COMPLETE_SUCCESS.
+ * - @b pipe_handle will always be -1.
+ * - @b submit_handle will always be -1.
+ * - @b bytes_transferred will always be 0.
+ * - @b user_data is the void pointer originally passed along
+ *   with the callback. Use this for any state information you
+ *   need.
+ *
+ * The port callback will be called whenever the user plugs /
+ * unplugs a device from the port. It will not be called when a
+ * device is plugged / unplugged from a hub connected to the
+ * root port. Normally all the callback needs to do is tell the
+ * operating system to poll the root hub for status. Under
+ * Linux, this is performed by calling usb_hcd_poll_rh_status().
+ * In the Linux driver we use @b user_data. to pass around the
+ * Linux "hcd" structure. Once the port callback completes,
+ * Linux automatically calls octeon_usb_hub_status_data() which
+ * uses cvmx_usb_get_status() to determine the root port status.
+ *
+ * <h2>Complete callback</h2>
+ *
+ * The completion callback prototype needs to look as follows:
+ *
+ * void complete_callback(cvmx_usb_state_t *usb,
+ *                        cvmx_usb_callback_t reason,
+ *                        cvmx_usb_complete_t status,
+ *                        int pipe_handle,
+ *                        int submit_handle,
+ *                        int bytes_transferred,
+ *                        void *user_data);
+ * - @b usb is the cvmx_usb_state_t for the port.
+ * - @b reason will always be
+ *   CVMX_USB_CALLBACK_TRANSFER_COMPLETE.
+ * - @b status will be one of the cvmx_usb_complete_t
+ *   enumerations.
+ * - @b pipe_handle is the handle to the pipe the transaction
+ *   was originally submitted on.
+ * - @b submit_handle is the handle returned by the original
+ *   cvmx_usb_submit_* call.
+ * - @b bytes_transferred is the number of bytes successfully
+ *   transferred in the transaction. This will be zero on most
+ *   error conditions.
+ * - @b user_data is the void pointer originally passed along
+ *   with the callback. Use this for any state information you
+ *   need. For example, the Linux "urb" is stored in here in the
+ *   Linux driver.
+ *
+ * In general your callback handler should use @b status and @b
+ * bytes_transferred to tell the operating system the how the
+ * transaction completed. Normally the pipe is not changed in
+ * this callback.
+ *
+ * <h2>Canceling transactions</h2>
+ *
+ * When a transaction is cancelled using cvmx_usb_cancel*(), the
+ * actual length of time until the complete callback is called
+ * can vary greatly. It may be called before cvmx_usb_cancel*()
+ * returns, or it may be called a number of usb frames in the
+ * future once the hardware frees the transaction. In either of
+ * these cases, the complete handler will receive
+ * CVMX_USB_COMPLETE_CANCEL.
+ *
+ * <h2>Handling pipes</h2>
+ *
+ * USB "pipes" is a software construct created by this API to
+ * enable the ordering of usb transactions to a device endpoint.
+ * Octeon's underlying hardware doesn't have any concept
+ * equivalent to "pipes". The hardware instead has eight
+ * channels that can be used simultaneously to have up to eight
+ * transaction in process at the same time. In order to maintain
+ * ordering in a pipe, the transactions for a pipe will only be
+ * active in one hardware channel at a time. From an API user's
+ * perspective, this doesn't matter but it can be helpful to
+ * keep this in mind when you are probing hardware while
+ * debugging.
+ *
+ * Also keep in mind that usb transactions contain state
+ * information about the previous transaction to the same
+ * endpoint. Each transaction has a PID toggle that changes 0/1
+ * between each sub packet. This is maintained in the pipe data
+ * structures. For this reason, you generally cannot create and
+ * destroy a pipe for every transaction. A sequence of
+ * transaction to the same endpoint must use the same pipe.
+ *
+ * <h2>Root Hub</h2>
+ *
+ * Some operating systems view the usb root port as a normal usb
+ * hub. These systems attempt to control the root hub with
+ * messages similar to the usb 2.0 spec for hub control and
+ * status. For these systems it may be necessary to write
+ * function to decode standard usb control messages into
+ * equivalent cvmx-usb API calls. As an example, the following
+ * code is used under Linux for some of the basic hub control
+ * messages.
+ *
+ * @code
+ * static int octeon_usb_hub_control(struct usb_hcd *hcd, u16 typeReq, u16 wValue, u16 wIndex, char *buf, u16 wLength)
+ * {
+ *     cvmx_usb_state_t *usb = (cvmx_usb_state_t *)hcd->hcd_priv;
+ *     cvmx_usb_port_status_t usb_port_status;
+ *     int port_status;
+ *     struct usb_hub_descriptor *desc;
+ *     unsigned long flags;
+ *
+ *     switch (typeReq)
+ *     {
+ *         case ClearHubFeature:
+ *             DEBUG_ROOT_HUB("OcteonUSB: ClearHubFeature\n");
+ *             switch (wValue)
+ *             {
+ *                 case C_HUB_LOCAL_POWER:
+ *                 case C_HUB_OVER_CURRENT:
+ *                     // Nothing required here
+ *                     break;
+ *                 default:
+ *                     return -EINVAL;
+ *             }
+ *             break;
+ *         case ClearPortFeature:
+ *             DEBUG_ROOT_HUB("OcteonUSB: ClearPortFeature");
+ *             if (wIndex != 1)
+ *             {
+ *                 DEBUG_ROOT_HUB(" INVALID\n");
+ *                 return -EINVAL;
+ *             }
+ *
+ *             switch (wValue)
+ *             {
+ *                 case USB_PORT_FEAT_ENABLE:
+ *                     DEBUG_ROOT_HUB(" ENABLE");
+ *                     local_irq_save(flags);
+ *                     cvmx_usb_disable(usb);
+ *                     local_irq_restore(flags);
+ *                     break;
+ *                 case USB_PORT_FEAT_SUSPEND:
+ *                     DEBUG_ROOT_HUB(" SUSPEND");
+ *                     // Not supported on Octeon
+ *                     break;
+ *                 case USB_PORT_FEAT_POWER:
+ *                     DEBUG_ROOT_HUB(" POWER");
+ *                     // Not supported on Octeon
+ *                     break;
+ *                 case USB_PORT_FEAT_INDICATOR:
+ *                     DEBUG_ROOT_HUB(" INDICATOR");
+ *                     // Port inidicator not supported
+ *                     break;
+ *                 case USB_PORT_FEAT_C_CONNECTION:
+ *                     DEBUG_ROOT_HUB(" C_CONNECTION");
+ *                     // Clears drivers internal connect status change flag
+ *                     cvmx_usb_set_status(usb, cvmx_usb_get_status(usb));
+ *                     break;
+ *                 case USB_PORT_FEAT_C_RESET:
+ *                     DEBUG_ROOT_HUB(" C_RESET");
+ *                     // Clears the driver's internal Port Reset Change flag
+ *                     cvmx_usb_set_status(usb, cvmx_usb_get_status(usb));
+ *                     break;
+ *                 case USB_PORT_FEAT_C_ENABLE:
+ *                     DEBUG_ROOT_HUB(" C_ENABLE");
+ *                     // Clears the driver's internal Port Enable/Disable Change flag
+ *                     cvmx_usb_set_status(usb, cvmx_usb_get_status(usb));
+ *                     break;
+ *                 case USB_PORT_FEAT_C_SUSPEND:
+ *                     DEBUG_ROOT_HUB(" C_SUSPEND");
+ *                     // Clears the driver's internal Port Suspend Change flag,
+ *                         which is set when resume signaling on the host port is
+ *                         complete
+ *                     break;
+ *                 case USB_PORT_FEAT_C_OVER_CURRENT:
+ *                     DEBUG_ROOT_HUB(" C_OVER_CURRENT");
+ *                     // Clears the driver's overcurrent Change flag
+ *                     cvmx_usb_set_status(usb, cvmx_usb_get_status(usb));
+ *                     break;
+ *                 default:
+ *                     DEBUG_ROOT_HUB(" UNKNOWN\n");
+ *                     return -EINVAL;
+ *             }
+ *             DEBUG_ROOT_HUB("\n");
+ *             break;
+ *         case GetHubDescriptor:
+ *             DEBUG_ROOT_HUB("OcteonUSB: GetHubDescriptor\n");
+ *             desc = (struct usb_hub_descriptor *)buf;
+ *             desc->bDescLength = 9;
+ *             desc->bDescriptorType = 0x29;
+ *             desc->bNbrPorts = 1;
+ *             desc->wHubCharacteristics = 0x08;
+ *             desc->bPwrOn2PwrGood = 1;
+ *             desc->bHubContrCurrent = 0;
+ *             desc->bitmap[0] = 0;
+ *             desc->bitmap[1] = 0xff;
+ *             break;
+ *         case GetHubStatus:
+ *             DEBUG_ROOT_HUB("OcteonUSB: GetHubStatus\n");
+ *             *(__le32 *)buf = 0;
+ *             break;
+ *         case GetPortStatus:
+ *             DEBUG_ROOT_HUB("OcteonUSB: GetPortStatus");
+ *             if (wIndex != 1)
+ *             {
+ *                 DEBUG_ROOT_HUB(" INVALID\n");
+ *                 return -EINVAL;
+ *             }
+ *
+ *             usb_port_status = cvmx_usb_get_status(usb);
+ *             port_status = 0;
+ *
+ *             if (usb_port_status.connect_change)
+ *             {
+ *                 port_status |= (1 << USB_PORT_FEAT_C_CONNECTION);
+ *                 DEBUG_ROOT_HUB(" C_CONNECTION");
+ *             }
+ *
+ *             if (usb_port_status.port_enabled)
+ *             {
+ *                 port_status |= (1 << USB_PORT_FEAT_C_ENABLE);
+ *                 DEBUG_ROOT_HUB(" C_ENABLE");
+ *             }
+ *
+ *             if (usb_port_status.connected)
+ *             {
+ *                 port_status |= (1 << USB_PORT_FEAT_CONNECTION);
+ *                 DEBUG_ROOT_HUB(" CONNECTION");
+ *             }
+ *
+ *             if (usb_port_status.port_enabled)
+ *             {
+ *                 port_status |= (1 << USB_PORT_FEAT_ENABLE);
+ *                 DEBUG_ROOT_HUB(" ENABLE");
+ *             }
+ *
+ *             if (usb_port_status.port_over_current)
+ *             {
+ *                 port_status |= (1 << USB_PORT_FEAT_OVER_CURRENT);
+ *                 DEBUG_ROOT_HUB(" OVER_CURRENT");
+ *             }
+ *
+ *             if (usb_port_status.port_powered)
+ *             {
+ *                 port_status |= (1 << USB_PORT_FEAT_POWER);
+ *                 DEBUG_ROOT_HUB(" POWER");
+ *             }
+ *
+ *             if (usb_port_status.port_speed == CVMX_USB_SPEED_HIGH)
+ *             {
+ *                 port_status |= (1 << USB_PORT_FEAT_HIGHSPEED);
+ *                 DEBUG_ROOT_HUB(" HIGHSPEED");
+ *             }
+ *             else if (usb_port_status.port_speed == CVMX_USB_SPEED_LOW)
+ *             {
+ *                 port_status |= (1 << USB_PORT_FEAT_LOWSPEED);
+ *                 DEBUG_ROOT_HUB(" LOWSPEED");
+ *             }
+ *
+ *             *((__le32 *)buf) = cpu_to_le32(port_status);
+ *             DEBUG_ROOT_HUB("\n");
+ *             break;
+ *         case SetHubFeature:
+ *             DEBUG_ROOT_HUB("OcteonUSB: SetHubFeature\n");
+ *             // No HUB features supported
+ *             break;
+ *         case SetPortFeature:
+ *             DEBUG_ROOT_HUB("OcteonUSB: SetPortFeature");
+ *             if (wIndex != 1)
+ *             {
+ *                 DEBUG_ROOT_HUB(" INVALID\n");
+ *                 return -EINVAL;
+ *             }
+ *
+ *             switch (wValue)
+ *             {
+ *                 case USB_PORT_FEAT_SUSPEND:
+ *                     DEBUG_ROOT_HUB(" SUSPEND\n");
+ *                     return -EINVAL;
+ *                 case USB_PORT_FEAT_POWER:
+ *                     DEBUG_ROOT_HUB(" POWER\n");
+ *                     return -EINVAL;
+ *                 case USB_PORT_FEAT_RESET:
+ *                     DEBUG_ROOT_HUB(" RESET\n");
+ *                     local_irq_save(flags);
+ *                     cvmx_usb_disable(usb);
+ *                     if (cvmx_usb_enable(usb))
+ *                         DEBUG_ERROR("Failed to enable the port\n");
+ *                     local_irq_restore(flags);
+ *                     return 0;
+ *                 case USB_PORT_FEAT_INDICATOR:
+ *                     DEBUG_ROOT_HUB(" INDICATOR\n");
+ *                     // Not supported
+ *                     break;
+ *                 default:
+ *                     DEBUG_ROOT_HUB(" UNKNOWN\n");
+ *                     return -EINVAL;
+ *             }
+ *             break;
+ *         default:
+ *             DEBUG_ROOT_HUB("OcteonUSB: Unknown root hub request\n");
+ *             return -EINVAL;
+ *     }
+ *     return 0;
+ * }
+ * @endcode
+ *
+ * <h2>Interrupts</h2>
+ *
+ * If you plan on using usb interrupts, cvmx_usb_poll() must be
+ * called on every usb interrupt. It will read the usb state,
+ * call any needed callbacks, and schedule transactions as
+ * needed. Your device driver needs only to hookup an interrupt
+ * handler and call cvmx_usb_poll(). Octeon's usb port 0 causes
+ * CIU bit CIU_INT*_SUM0[USB] to be set (bit 56). For port 1,
+ * CIU bit CIU_INT_SUM1[USB1] is set (bit 17). How these bits
+ * are turned into interrupt numbers is operating system
+ * specific. For Linux, there are the convenient defines
+ * OCTEON_IRQ_USB0 and OCTEON_IRQ_USB1 for the IRQ numbers.
+ *
+ * If you aren't using interrupts, simple call cvmx_usb_poll()
+ * in your main processing loop.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+
+#ifndef __CVMX_USB_H__
+#define __CVMX_USB_H__
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+/**
+ * Enumerations representing the status of function calls.
+ */
+typedef enum
+{
+    CVMX_USB_SUCCESS = 0,           /**< There were no errors */
+    CVMX_USB_INVALID_PARAM = -1,    /**< A parameter to the function was invalid */
+    CVMX_USB_NO_MEMORY = -2,        /**< Insufficient resources were available for the request */
+    CVMX_USB_BUSY = -3,             /**< The resource is busy and cannot service the request */
+    CVMX_USB_TIMEOUT = -4,          /**< Waiting for an action timed out */
+    CVMX_USB_INCORRECT_MODE = -5,   /**< The function call doesn't work in the current USB
+                                         mode. This happens when host only functions are
+                                         called in device mode or vice versa */
+} cvmx_usb_status_t;
+
+/**
+ * Enumerations representing the possible USB device speeds
+ */
+typedef enum
+{
+    CVMX_USB_SPEED_HIGH = 0,        /**< Device is operation at 480Mbps */
+    CVMX_USB_SPEED_FULL = 1,        /**< Device is operation at 12Mbps */
+    CVMX_USB_SPEED_LOW = 2,         /**< Device is operation at 1.5Mbps */
+} cvmx_usb_speed_t;
+
+/**
+ * Enumeration representing the possible USB transfer types.
+ */
+typedef enum
+{
+    CVMX_USB_TRANSFER_CONTROL = 0,      /**< USB transfer type control for hub and status transfers */
+    CVMX_USB_TRANSFER_ISOCHRONOUS = 1,  /**< USB transfer type isochronous for low priority periodic transfers */
+    CVMX_USB_TRANSFER_BULK = 2,         /**< USB transfer type bulk for large low priority transfers */
+    CVMX_USB_TRANSFER_INTERRUPT = 3,    /**< USB transfer type interrupt for high priority periodic transfers */
+} cvmx_usb_transfer_t;
+
+/**
+ * Enumeration of the transfer directions
+ */
+typedef enum
+{
+    CVMX_USB_DIRECTION_OUT,         /**< Data is transferring from Octeon to the device/host */
+    CVMX_USB_DIRECTION_IN,          /**< Data is transferring from the device/host to Octeon */
+} cvmx_usb_direction_t;
+
+/**
+ * Enumeration of all possible status codes passed to callback
+ * functions.
+ */
+typedef enum
+{
+    CVMX_USB_COMPLETE_SUCCESS,      /**< The transaction / operation finished without any errors */
+    CVMX_USB_COMPLETE_SHORT,        /**< FIXME: This is currently not implemented */
+    CVMX_USB_COMPLETE_CANCEL,       /**< The transaction was canceled while in flight by a user call to cvmx_usb_cancel* */
+    CVMX_USB_COMPLETE_ERROR,        /**< The transaction aborted with an unexpected error status */
+    CVMX_USB_COMPLETE_STALL,        /**< The transaction received a USB STALL response from the device */
+    CVMX_USB_COMPLETE_XACTERR,      /**< The transaction failed with an error from the device even after a number of retries */
+    CVMX_USB_COMPLETE_DATATGLERR,   /**< The transaction failed with a data toggle error even after a number of retries */
+    CVMX_USB_COMPLETE_BABBLEERR,    /**< The transaction failed with a babble error */
+    CVMX_USB_COMPLETE_FRAMEERR,     /**< The transaction failed with a frame error even after a number of retries */
+} cvmx_usb_complete_t;
+
+/**
+ * Structure returned containing the USB port status information.
+ */
+typedef struct
+{
+    uint32_t reserved           : 25;
+    uint32_t port_enabled       : 1; /**< 1 = Usb port is enabled, 0 = disabled */
+    uint32_t port_over_current  : 1; /**< 1 = Over current detected, 0 = Over current not detected. Octeon doesn't support over current detection */
+    uint32_t port_powered       : 1; /**< 1 = Port power is being supplied to the device, 0 = power is off. Octeon doesn't support turning port power off */
+    cvmx_usb_speed_t port_speed : 2; /**< Current port speed */
+    uint32_t connected          : 1; /**< 1 = A device is connected to the port, 0 = No device is connected */
+    uint32_t connect_change     : 1; /**< 1 = Device connected state changed since the last set status call */
+} cvmx_usb_port_status_t;
+
+/**
+ * This is the structure of a Control packet header
+ */
+typedef union
+{
+    uint64_t u64;
+    struct
+    {
+        uint64_t request_type   : 8;  /**< Bit 7 tells the direction: 1=IN, 0=OUT */
+        uint64_t request        : 8;  /**< The standard usb request to make */
+        uint64_t value          : 16; /**< Value parameter for the request in little endian format */
+        uint64_t index          : 16; /**< Index for the request in little endian format */
+        uint64_t length         : 16; /**< Length of the data associated with this request in little endian format */
+    } s;
+} cvmx_usb_control_header_t;
+
+/**
+ * Descriptor for Isochronous packets
+ */
+typedef struct
+{
+    int offset;                     /**< This is the offset in bytes into the main buffer where this data is stored */
+    int length;                     /**< This is the length in bytes of the data */
+    cvmx_usb_complete_t status;     /**< This is the status of this individual packet transfer */
+} cvmx_usb_iso_packet_t;
+
+/**
+ * Possible callback reasons for the USB API.
+ */
+typedef enum
+{
+    CVMX_USB_CALLBACK_TRANSFER_COMPLETE,
+                                    /**< A callback of this type is called when a submitted transfer
+                                        completes. The completion callback will be called even if the
+                                        transfer fails or is canceled. The status parameter will
+                                        contain details of why he callback was called. */
+    CVMX_USB_CALLBACK_PORT_CHANGED, /**< The status of the port changed. For example, someone may have
+                                        plugged a device in. The status parameter contains
+                                        CVMX_USB_COMPLETE_SUCCESS. Use cvmx_usb_get_status() to get
+                                        the new port status. */
+    CVMX_USB_CALLBACK_DEVICE_SETUP, /**< This is called in device mode when a control channels receives
+                                        a setup header */
+    __CVMX_USB_CALLBACK_END         /**< Do not use. Used internally for array bounds */
+} cvmx_usb_callback_t;
+
+/**
+ * USB state internal data. The contents of this structure
+ * may change in future SDKs. No data in it should be referenced
+ * by user's of this API.
+ */
+typedef struct
+{
+    char data[65536];
+} cvmx_usb_state_t;
+
+/**
+ * USB callback functions are always of the following type.
+ * The parameters are as follows:
+ *      - state = USB device state populated by
+ *        cvmx_usb_initialize().
+ *      - reason = The cvmx_usb_callback_t used to register
+ *        the callback.
+ *      - status = The cvmx_usb_complete_t representing the
+ *        status code of a transaction.
+ *      - pipe_handle = The Pipe that caused this callback, or
+ *        -1 if this callback wasn't associated with a pipe.
+ *      - submit_handle = Transfer submit handle causing this
+ *        callback, or -1 if this callback wasn't associated
+ *        with a transfer.
+ *      - Actual number of bytes transfer.
+ *      - user_data = The user pointer supplied to the
+ *        function cvmx_usb_submit() or
+ *        cvmx_usb_register_callback() */
+typedef void (*cvmx_usb_callback_func_t)(cvmx_usb_state_t *state,
+                                         cvmx_usb_callback_t reason,
+                                         cvmx_usb_complete_t status,
+                                         int pipe_handle, int submit_handle,
+                                         int bytes_transferred, void *user_data);
+
+/**
+ * Flags to pass the initialization function.
+ */
+typedef enum
+{
+    CVMX_USB_INITIALIZE_FLAGS_CLOCK_XO_XI = 1<<0,       /**< The USB port uses a 12MHz crystal as clock source
+                                                            at USB_XO and USB_XI. This or CVMX_USB_INITIALIZE_FLAGS_CLOCK_XO_GND
+                                                            must be specified. */
+    CVMX_USB_INITIALIZE_FLAGS_CLOCK_XO_GND = 1<<1,      /**< The USB port uses 12/24/48MHz 2.5V board clock
+                                                            source at USB_XO. USB_XI should be tied to GND.
+                                                            This or CVMX_USB_INITIALIZE_FLAGS_CLOCK_XO_XI must
+                                                            be specified. Most Octeon evaluation boards require this setting */
+    CVMX_USB_INITIALIZE_FLAGS_DEVICE_MODE = 1<<2,       /**< Program the USB port for device mode instead of host mode */
+    CVMX_USB_INITIALIZE_FLAGS_DEBUG_TRANSFERS = 1<<16,  /**< Enable extra console output for debugging USB transfers */
+    CVMX_USB_INITIALIZE_FLAGS_DEBUG_CALLBACKS = 1<<17,  /**< Enable extra console output for debugging USB callbacks */
+    CVMX_USB_INITIALIZE_FLAGS_DEBUG_INFO = 1<<18,       /**< Enable extra console output for USB informational data */
+    CVMX_USB_INITIALIZE_FLAGS_DEBUG_CALLS = 1<<19,      /**< Enable extra console output for every function call */
+    CVMX_USB_INITIALIZE_FLAGS_DEBUG_CSRS = 1<<20,       /**< Enable extra console output for every CSR access */
+    CVMX_USB_INITIALIZE_FLAGS_DEBUG_ALL = ((CVMX_USB_INITIALIZE_FLAGS_DEBUG_CSRS<<1)-1) - (CVMX_USB_INITIALIZE_FLAGS_DEBUG_TRANSFERS-1),
+} cvmx_usb_initialize_flags_t;
+
+/**
+ * Flags for passing when a pipe is created. Currently no flags
+ * need to be passed.
+ */
+typedef enum
+{
+    CVMX_USB_PIPE_FLAGS_DEBUG_TRANSFERS = 1<<15,/**< Used to display CVMX_USB_INITIALIZE_FLAGS_DEBUG_TRANSFERS for a specific pipe only */
+    __CVMX_USB_PIPE_FLAGS_OPEN = 1<<16,         /**< Used internally to determine if a pipe is open. Do not use */
+    __CVMX_USB_PIPE_FLAGS_SCHEDULED = 1<<17,    /**< Used internally to determine if a pipe is actively using hardware. Do not use */
+    __CVMX_USB_PIPE_FLAGS_NEED_PING = 1<<18,    /**< Used internally to determine if a high speed pipe is in the ping state. Do not use */
+} cvmx_usb_pipe_flags_t;
+
+/**
+ * Return the number of USB ports supported by this Octeon
+ * chip. If the chip doesn't support USB, or is not supported
+ * by this API, a zero will be returned. Most Octeon chips
+ * support one usb port, but some support two ports.
+ * cvmx_usb_initialize() must be called on independent
+ * cvmx_usb_state_t structures.
+ *
+ * @return Number of port, zero if usb isn't supported
+ */
+extern int cvmx_usb_get_num_ports(void);
+
+/**
+ * Initialize a USB port for use. This must be called before any
+ * other access to the Octeon USB port is made. The port starts
+ * off in the disabled state.
+ *
+ * @param state  Pointer to an empty cvmx_usb_state_t structure
+ *               that will be populated by the initialize call.
+ *               This structure is then passed to all other USB
+ *               functions.
+ * @param usb_port_number
+ *               Which Octeon USB port to initialize.
+ * @param flags  Flags to control hardware initialization. See
+ *               cvmx_usb_initialize_flags_t for the flag
+ *               definitions. Some flags are mandatory.
+ *
+ * @return CVMX_USB_SUCCESS or a negative error code defined in
+ *         cvmx_usb_status_t.
+ */
+extern cvmx_usb_status_t cvmx_usb_initialize(cvmx_usb_state_t *state,
+                                             int usb_port_number,
+                                             cvmx_usb_initialize_flags_t flags);
+
+/**
+ * Shutdown a USB port after a call to cvmx_usb_initialize().
+ * The port should be disabled with all pipes closed when this
+ * function is called.
+ *
+ * @param state  USB device state populated by
+ *               cvmx_usb_initialize().
+ *
+ * @return CVMX_USB_SUCCESS or a negative error code defined in
+ *         cvmx_usb_status_t.
+ */
+extern cvmx_usb_status_t cvmx_usb_shutdown(cvmx_usb_state_t *state);
+
+/**
+ * Enable a USB port. After this call succeeds, the USB port is
+ * online and servicing requests.
+ *
+ * @param state  USB device state populated by
+ *               cvmx_usb_initialize().
+ *
+ * @return CVMX_USB_SUCCESS or a negative error code defined in
+ *         cvmx_usb_status_t.
+ */
+extern cvmx_usb_status_t cvmx_usb_enable(cvmx_usb_state_t *state);
+
+/**
+ * Disable a USB port. After this call the USB port will not
+ * generate data transfers and will not generate events.
+ * Transactions in process will fail and call their
+ * associated callbacks.
+ *
+ * @param state  USB device state populated by
+ *               cvmx_usb_initialize().
+ *
+ * @return CVMX_USB_SUCCESS or a negative error code defined in
+ *         cvmx_usb_status_t.
+ */
+extern cvmx_usb_status_t cvmx_usb_disable(cvmx_usb_state_t *state);
+
+/**
+ * Get the current state of the USB port. Use this call to
+ * determine if the usb port has anything connected, is enabled,
+ * or has some sort of error condition. The return value of this
+ * call has "changed" bits to signal of the value of some fields
+ * have changed between calls. These "changed" fields are based
+ * on the last call to cvmx_usb_set_status(). In order to clear
+ * them, you must update the status through cvmx_usb_set_status().
+ *
+ * @param state  USB device state populated by
+ *               cvmx_usb_initialize().
+ *
+ * @return Port status information
+ */
+extern cvmx_usb_port_status_t cvmx_usb_get_status(cvmx_usb_state_t *state);
+
+/**
+ * Set the current state of the USB port. The status is used as
+ * a reference for the "changed" bits returned by
+ * cvmx_usb_get_status(). Other than serving as a reference, the
+ * status passed to this function is not used. No fields can be
+ * changed through this call.
+ *
+ * @param state  USB device state populated by
+ *               cvmx_usb_initialize().
+ * @param port_status
+ *               Port status to set, most like returned by cvmx_usb_get_status()
+ */
+extern void cvmx_usb_set_status(cvmx_usb_state_t *state, cvmx_usb_port_status_t port_status);
+
+/**
+ * Open a virtual pipe between the host and a USB device. A pipe
+ * must be opened before data can be transferred between a device
+ * and Octeon.
+ *
+ * @param state      USB device state populated by
+ *                   cvmx_usb_initialize().
+ * @param flags      Optional pipe flags defined in
+ *                   cvmx_usb_pipe_flags_t.
+ * @param device_addr
+ *                   USB device address to open the pipe to
+ *                   (0-127).
+ * @param endpoint_num
+ *                   USB endpoint number to open the pipe to
+ *                   (0-15).
+ * @param device_speed
+ *                   The speed of the device the pipe is going
+ *                   to. This must match the device's speed,
+ *                   which may be different than the port speed.
+ * @param max_packet The maximum packet length the device can
+ *                   transmit/receive (low speed=0-8, full
+ *                   speed=0-1023, high speed=0-1024). This value
+ *                   comes from the stadnard endpoint descriptor
+ *                   field wMaxPacketSize bits <10:0>.
+ * @param transfer_type
+ *                   The type of transfer this pipe is for.
+ * @param transfer_dir
+ *                   The direction the pipe is in. This is not
+ *                   used for control pipes.
+ * @param interval   For ISOCHRONOUS and INTERRUPT transfers,
+ *                   this is how often the transfer is scheduled
+ *                   for. All other transfers should specify
+ *                   zero. The units are in frames (8000/sec at
+ *                   high speed, 1000/sec for full speed).
+ * @param multi_count
+ *                   For high speed devices, this is the maximum
+ *                   allowed number of packet per microframe.
+ *                   Specify zero for non high speed devices. This
+ *                   value comes from the stadnard endpoint descriptor
+ *                   field wMaxPacketSize bits <12:11>.
+ * @param hub_device_addr
+ *                   Hub device address this device is connected
+ *                   to. Devices connected directly to Octeon
+ *                   use zero. This is only used when the device
+ *                   is full/low speed behind a high speed hub.
+ *                   The address will be of the high speed hub,
+ *                   not and full speed hubs after it.
+ * @param hub_port   Which port on the hub the device is
+ *                   connected. Use zero for devices connected
+ *                   directly to Octeon. Like hub_device_addr,
+ *                   this is only used for full/low speed
+ *                   devices behind a high speed hub.
+ *
+ * @return A non negative value is a pipe handle. Negative
+ *         values are failure codes from cvmx_usb_status_t.
+ */
+extern int cvmx_usb_open_pipe(cvmx_usb_state_t *state,
+                              cvmx_usb_pipe_flags_t flags,
+                              int device_addr, int endpoint_num,
+                              cvmx_usb_speed_t device_speed, int max_packet,
+                              cvmx_usb_transfer_t transfer_type,
+                              cvmx_usb_direction_t transfer_dir, int interval,
+                              int multi_count, int hub_device_addr,
+                              int hub_port);
+
+/**
+ * Call to submit a USB Bulk transfer to a pipe.
+ *
+ * @param state     USB device state populated by
+ *                  cvmx_usb_initialize().
+ * @param pipe_handle
+ *                  Handle to the pipe for the transfer.
+ * @param buffer    Physical address of the data buffer in
+ *                  memory. Note that this is NOT A POINTER, but
+ *                  the full 64bit physical address of the
+ *                  buffer. This may be zero if buffer_length is
+ *                  zero.
+ * @param buffer_length
+ *                  Length of buffer in bytes.
+ * @param callback  Function to call when this transaction
+ *                  completes. If the return value of this
+ *                  function isn't an error, then this function
+ *                  is guaranteed to be called when the
+ *                  transaction completes. If this parameter is
+ *                  NULL, then the generic callback registered
+ *                  through cvmx_usb_register_callback is
+ *                  called. If both are NULL, then there is no
+ *                  way to know when a transaction completes.
+ * @param user_data User supplied data returned when the
+ *                  callback is called. This is only used if
+ *                  callback in not NULL.
+ *
+ * @return A submitted transaction handle or negative on
+ *         failure. Negative values are failure codes from
+ *         cvmx_usb_status_t.
+ */
+extern int cvmx_usb_submit_bulk(cvmx_usb_state_t *state, int pipe_handle,
+                                uint64_t buffer, int buffer_length,
+                                cvmx_usb_callback_func_t callback,
+                                void *user_data);
+
+/**
+ * Call to submit a USB Interrupt transfer to a pipe.
+ *
+ * @param state     USB device state populated by
+ *                  cvmx_usb_initialize().
+ * @param pipe_handle
+ *                  Handle to the pipe for the transfer.
+ * @param buffer    Physical address of the data buffer in
+ *                  memory. Note that this is NOT A POINTER, but
+ *                  the full 64bit physical address of the
+ *                  buffer. This may be zero if buffer_length is
+ *                  zero.
+ * @param buffer_length
+ *                  Length of buffer in bytes.
+ * @param callback  Function to call when this transaction
+ *                  completes. If the return value of this
+ *                  function isn't an error, then this function
+ *                  is guaranteed to be called when the
+ *                  transaction completes. If this parameter is
+ *                  NULL, then the generic callback registered
+ *                  through cvmx_usb_register_callback is
+ *                  called. If both are NULL, then there is no
+ *                  way to know when a transaction completes.
+ * @param user_data User supplied data returned when the
+ *                  callback is called. This is only used if
+ *                  callback in not NULL.
+ *
+ * @return A submitted transaction handle or negative on
+ *         failure. Negative values are failure codes from
+ *         cvmx_usb_status_t.
+ */
+extern int cvmx_usb_submit_interrupt(cvmx_usb_state_t *state, int pipe_handle,
+                                     uint64_t buffer, int buffer_length,
+                                     cvmx_usb_callback_func_t callback,
+                                     void *user_data);
+
+/**
+ * Call to submit a USB Control transfer to a pipe.
+ *
+ * @param state     USB device state populated by
+ *                  cvmx_usb_initialize().
+ * @param pipe_handle
+ *                  Handle to the pipe for the transfer.
+ * @param control_header
+ *                  USB 8 byte control header physical address.
+ *                  Note that this is NOT A POINTER, but the
+ *                  full 64bit physical address of the buffer.
+ * @param buffer    Physical address of the data buffer in
+ *                  memory. Note that this is NOT A POINTER, but
+ *                  the full 64bit physical address of the
+ *                  buffer. This may be zero if buffer_length is
+ *                  zero.
+ * @param buffer_length
+ *                  Length of buffer in bytes.
+ * @param callback  Function to call when this transaction
+ *                  completes. If the return value of this
+ *                  function isn't an error, then this function
+ *                  is guaranteed to be called when the
+ *                  transaction completes. If this parameter is
+ *                  NULL, then the generic callback registered
+ *                  through cvmx_usb_register_callback is
+ *                  called. If both are NULL, then there is no
+ *                  way to know when a transaction completes.
+ * @param user_data User supplied data returned when the
+ *                  callback is called. This is only used if
+ *                  callback in not NULL.
+ *
+ * @return A submitted transaction handle or negative on
+ *         failure. Negative values are failure codes from
+ *         cvmx_usb_status_t.
+ */
+extern int cvmx_usb_submit_control(cvmx_usb_state_t *state, int pipe_handle,
+                                   uint64_t control_header,
+                                   uint64_t buffer, int buffer_length,
+                                   cvmx_usb_callback_func_t callback,
+                                   void *user_data);
+
+/**
+ * Flags to pass the cvmx_usb_submit_isochronous() function.
+ */
+typedef enum
+{
+    CVMX_USB_ISOCHRONOUS_FLAGS_ALLOW_SHORT = 1<<0,  /**< Do not return an error if a transfer is less than the maximum packet size of the device */
+    CVMX_USB_ISOCHRONOUS_FLAGS_ASAP = 1<<1,         /**< Schedule the transaction as soon as possible */
+} cvmx_usb_isochronous_flags_t;
+
+/**
+ * Call to submit a USB Isochronous transfer to a pipe.
+ *
+ * @param state     USB device state populated by
+ *                  cvmx_usb_initialize().
+ * @param pipe_handle
+ *                  Handle to the pipe for the transfer.
+ * @param start_frame
+ *                  Number of frames into the future to schedule
+ *                  this transaction.
+ * @param flags     Flags to control the transfer. See
+ *                  cvmx_usb_isochronous_flags_t for the flag
+ *                  definitions.
+ * @param number_packets
+ *                  Number of sequential packets to transfer.
+ *                  "packets" is a pointer to an array of this
+ *                  many packet structures.
+ * @param packets   Description of each transfer packet as
+ *                  defined by cvmx_usb_iso_packet_t. The array
+ *                  pointed to here must stay valid until the
+ *                  complete callback is called.
+ * @param buffer    Physical address of the data buffer in
+ *                  memory. Note that this is NOT A POINTER, but
+ *                  the full 64bit physical address of the
+ *                  buffer. This may be zero if buffer_length is
+ *                  zero.
+ * @param buffer_length
+ *                  Length of buffer in bytes.
+ * @param callback  Function to call when this transaction
+ *                  completes. If the return value of this
+ *                  function isn't an error, then this function
+ *                  is guaranteed to be called when the
+ *                  transaction completes. If this parameter is
+ *                  NULL, then the generic callback registered
+ *                  through cvmx_usb_register_callback is
+ *                  called. If both are NULL, then there is no
+ *                  way to know when a transaction completes.
+ * @param user_data User supplied data returned when the
+ *                  callback is called. This is only used if
+ *                  callback in not NULL.
+ *
+ * @return A submitted transaction handle or negative on
+ *         failure. Negative values are failure codes from
+ *         cvmx_usb_status_t.
+ */
+extern int cvmx_usb_submit_isochronous(cvmx_usb_state_t *state, int pipe_handle,
+                                       int start_frame, int flags,
+                                       int number_packets,
+                                       cvmx_usb_iso_packet_t packets[],
+                                       uint64_t buffer, int buffer_length,
+                                       cvmx_usb_callback_func_t callback,
+                                       void *user_data);
+
+/**
+ * Cancel one outstanding request in a pipe. Canceling a request
+ * can fail if the transaction has already completed before cancel
+ * is called. Even after a successful cancel call, it may take
+ * a frame or two for the cvmx_usb_poll() function to call the
+ * associated callback.
+ *
+ * @param state  USB device state populated by
+ *               cvmx_usb_initialize().
+ * @param pipe_handle
+ *               Pipe handle to cancel requests in.
+ * @param submit_handle
+ *               Handle to transaction to cancel, returned by the submit function.
+ *
+ * @return CVMX_USB_SUCCESS or a negative error code defined in
+ *         cvmx_usb_status_t.
+ */
+extern cvmx_usb_status_t cvmx_usb_cancel(cvmx_usb_state_t *state,
+                                         int pipe_handle, int submit_handle);
+
+
+/**
+ * Cancel all outstanding requests in a pipe. Logically all this
+ * does is call cvmx_usb_cancel() in a loop.
+ *
+ * @param state  USB device state populated by
+ *               cvmx_usb_initialize().
+ * @param pipe_handle
+ *               Pipe handle to cancel requests in.
+ *
+ * @return CVMX_USB_SUCCESS or a negative error code defined in
+ *         cvmx_usb_status_t.
+ */
+extern cvmx_usb_status_t cvmx_usb_cancel_all(cvmx_usb_state_t *state,
+                                             int pipe_handle);
+
+/**
+ * Close a pipe created with cvmx_usb_open_pipe().
+ *
+ * @param state  USB device state populated by
+ *               cvmx_usb_initialize().
+ * @param pipe_handle
+ *               Pipe handle to close.
+ *
+ * @return CVMX_USB_SUCCESS or a negative error code defined in
+ *         cvmx_usb_status_t. CVMX_USB_BUSY is returned if the
+ *         pipe has outstanding transfers.
+ */
+extern cvmx_usb_status_t cvmx_usb_close_pipe(cvmx_usb_state_t *state,
+                                             int pipe_handle);
+
+/**
+ * Register a function to be called when various USB events occur.
+ *
+ * @param state     USB device state populated by
+ *                  cvmx_usb_initialize().
+ * @param reason    Which event to register for.
+ * @param callback  Function to call when the event occurs.
+ * @param user_data User data parameter to the function.
+ *
+ * @return CVMX_USB_SUCCESS or a negative error code defined in
+ *         cvmx_usb_status_t.
+ */
+extern cvmx_usb_status_t cvmx_usb_register_callback(cvmx_usb_state_t *state,
+                                                    cvmx_usb_callback_t reason,
+                                                    cvmx_usb_callback_func_t callback,
+                                                    void *user_data);
+
+/**
+ * Get the current USB protocol level frame number. The frame
+ * number is always in the range of 0-0x7ff.
+ *
+ * @param state  USB device state populated by
+ *               cvmx_usb_initialize().
+ *
+ * @return USB frame number
+ */
+extern int cvmx_usb_get_frame_number(cvmx_usb_state_t *state);
+
+/**
+ * Poll the USB block for status and call all needed callback
+ * handlers. This function is meant to be called in the interrupt
+ * handler for the USB controller. It can also be called
+ * periodically in a loop for non-interrupt based operation.
+ *
+ * @param state  USB device state populated by
+ *               cvmx_usb_initialize().
+ *
+ * @return CVMX_USB_SUCCESS or a negative error code defined in
+ *         cvmx_usb_status_t.
+ */
+extern cvmx_usb_status_t cvmx_usb_poll(cvmx_usb_state_t *state);
+
+/**
+ * Enable an endpoint for use in device mode. After this call
+ * transactions will be allowed over the endpoint. This must be
+ * called after every usb reset.
+ *
+ * @param state  USB device state populated by
+ *               cvmx_usb_initialize().
+ * @param endpoint_num
+ *               The endpoint number to enable (0-4)
+ * @param transfer_type
+ *               USB transfer type of this endpoint
+ * @param transfer_dir
+ *               Direction of transfer relative to Octeon
+ * @param max_packet_size
+ *               Maximum packet size support by this endpoint
+ * @param buffer Buffer to send/receive
+ * @param buffer_length
+ *               Length of the buffer in bytes
+ *
+ * @return CVMX_USB_SUCCESS or a negative error code defined in
+ *         cvmx_usb_status_t.
+ */
+extern cvmx_usb_status_t cvmx_usb_device_enable_endpoint(cvmx_usb_state_t *state,
+                                                  int endpoint_num,
+                                                  cvmx_usb_transfer_t transfer_type,
+                                                  cvmx_usb_direction_t transfer_dir,
+                                                  int max_packet_size,
+                                                  uint64_t buffer,
+                                                  int buffer_length);
+
+/**
+ * Disable an endpoint in device mode.
+ *
+ * @param state  USB device state populated by
+ *               cvmx_usb_initialize().
+ * @param endpoint_num
+ *               The endpoint number to disable (0-4)
+ *
+ * @return CVMX_USB_SUCCESS or a negative error code defined in
+ *         cvmx_usb_status_t.
+ */
+extern cvmx_usb_status_t cvmx_usb_device_disable_endpoint(cvmx_usb_state_t *state,
+                                                          int endpoint_num);
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif  /* __CVMX_USB_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-version.h b/arch/mips/cavium-octeon/executive/cvmx-version.h
new file mode 100644
index 0000000..3196469
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-version.h
@@ -0,0 +1,12 @@
+/* Version information is made available at compile time in two forms:
+** 1) a version string for printing
+** 2) a combined SDK version and build number, suitable for comparisons
+**    to determine what SDK version is being used.
+**    SDK 1.2.3 build 567 => 102030567
+**    Note that 2 digits are used for each version number, so that:
+**     1.9.0  == 01.09.00 < 01.10.00 == 1.10.0 
+**     10.9.0 == 10.09.00 > 09.10.00 == 9.10.0 
+** 
+*/ 
+#define OCTEON_SDK_VERSION_NUM  108000270ull
+#define OCTEON_SDK_VERSION_STRING   "Cavium Networks Octeon SDK version 1.8.0, build 270"
diff --git a/arch/mips/cavium-octeon/executive/cvmx-warn.c b/arch/mips/cavium-octeon/executive/cvmx-warn.c
new file mode 100644
index 0000000..b48931f
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-warn.c
@@ -0,0 +1,87 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Functions for warning users about errors and such.
+ *
+ * <hr>$Revision: 33871 $<hr>
+ *
+ */
+#include "cvmx.h"
+#include "cvmx-warn.h"
+
+void cvmx_warn(const char *format, ...)
+{
+#ifdef CVMX_BUILD_FOR_UBOOT
+    DECLARE_GLOBAL_DATA_PTR;
+    if (!gd->have_console)
+    {
+        /* If the serial port is not set up yet,
+        ** save pointer to error message (most likely a constant in flash)
+        ** to print out once we can. */
+        gd->err_msg = (void *)format;
+        return;
+    }
+#endif
+    va_list args;
+    va_start(args, format);
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+    printk("WARNING:");
+    vprintk(format, args);
+#else
+    printf("WARNING:\n");
+    vprintf(format, args);
+#endif
+    va_end(args);
+}
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-warn.h b/arch/mips/cavium-octeon/executive/cvmx-warn.h
new file mode 100644
index 0000000..c0b4b28
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-warn.h
@@ -0,0 +1,72 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Functions for warning users about errors and such.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ *
+ */
+#ifndef __CVMX_WARN_H__
+#define __CVMX_WARN_H__
+
+#ifdef printf
+extern void cvmx_warn(const char *format, ...);
+#else
+extern void cvmx_warn(const char *format, ...) __attribute__ ((format(printf, 1, 2)));
+#endif
+
+#define cvmx_warn_if(expression, format, ...) if (expression) cvmx_warn(format, ##__VA_ARGS__)
+
+#endif /* __CVMX_WARN_H__ */
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-wqe.h b/arch/mips/cavium-octeon/executive/cvmx-wqe.h
new file mode 100644
index 0000000..fa4e0be
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-wqe.h
@@ -0,0 +1,321 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * This header file defines the work queue entry (wqe) data structure.
+ * Since this is a commonly used structure that depends on structures
+ * from several hardware blocks, those definitions have been placed
+ * in this file to create a single point of definition of the wqe
+ * format.
+ * Data structures are still named according to the block that they
+ * relate to.
+ *
+ * This file must not depend on any other header files, except for cvmx.h!!!
+ *
+ *
+ * <hr>$Revision: 32636 $<hr>
+ *
+ *
+ */
+
+
+#ifndef __CVMX_WQE_H__
+#define __CVMX_WQE_H__
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#define OCT_TAG_TYPE_STRING(x) (((x) == CVMX_POW_TAG_TYPE_ORDERED) ?  "ORDERED" : \
+                                (((x) == CVMX_POW_TAG_TYPE_ATOMIC) ?  "ATOMIC" : \
+                                (((x) == CVMX_POW_TAG_TYPE_NULL) ?  "NULL" : \
+                                "NULL_NULL")))
+
+
+/**
+ * HW decode / err_code in work queue entry
+ */
+typedef union
+{
+    uint64_t                 u64;
+
+    /** Use this struct if the hardware determines that the packet is IP */
+    struct
+    {
+        uint64_t               bufs          : 8; /**< HW sets this to the number of buffers used by this packet */
+        uint64_t               ip_offset     : 8; /**< HW sets to the number of L2 bytes prior to the IP */
+        uint64_t               vlan_valid    : 1; /**< set to 1 if we found VLAN in the L2 */
+        uint64_t               unassigned    : 2;
+        uint64_t               vlan_cfi      : 1; /**< HW sets to the VLAN CFI flag (valid when vlan_valid) */
+        uint64_t               vlan_id       :12; /**< HW sets to the VLAN_ID field (valid when vlan_valid) */
+
+        uint64_t               unassigned2   :12;
+        uint64_t               dec_ipcomp    : 1; /**< the packet needs to be decompressed */
+        uint64_t               tcp_or_udp    : 1; /**< the packet is either TCP or UDP */
+        uint64_t               dec_ipsec     : 1; /**< the packet needs to be decrypted (ESP or AH) */
+        uint64_t               is_v6         : 1; /**< the packet is IPv6 */
+
+        // (rcv_error, not_IP, IP_exc, is_frag, L4_error, software, etc.)
+
+        uint64_t               software      : 1; /**< reserved for software use, hardware will clear on packet creation */
+        // exceptional conditions below
+        uint64_t               L4_error      : 1; /**< the receive interface hardware detected an L4 error (only applies if !is_frag)
+                                                    (only applies if !rcv_error && !not_IP && !IP_exc && !is_frag)
+                                                    failure indicated in err_code below, decode:
+                                                    - 1 = Malformed L4
+                                                    - 2 = L4 Checksum Error: the L4 checksum value is
+                                                    - 3 = UDP Length Error: The UDP length field would make the UDP data longer than what
+                                                        remains in the IP packet (as defined by the IP header length field).
+                                                    - 4 = Bad L4 Port: either the source or destination TCP/UDP port is 0.
+                                                    - 8 = TCP FIN Only: the packet is TCP and only the FIN flag set.
+                                                    - 9 = TCP No Flags: the packet is TCP and no flags are set.
+                                                    - 10 = TCP FIN RST: the packet is TCP and both FIN and RST are set.
+                                                    - 11 = TCP SYN URG: the packet is TCP and both SYN and URG are set.
+                                                    - 12 = TCP SYN RST: the packet is TCP and both SYN and RST are set.
+                                                    - 13 = TCP SYN FIN: the packet is TCP and both SYN and FIN are set. */
+
+
+
+        uint64_t               is_frag       : 1; /**< set if the packet is a fragment */
+        uint64_t               IP_exc        : 1; /**< the receive interface hardware detected an IP error / exception
+                                                    (only applies if !rcv_error && !not_IP) failure indicated in err_code below, decode:
+                                                    - 1 = Not IP: the IP version field is neither 4 nor 6.
+                                                    - 2 = IPv4 Header Checksum Error: the IPv4 header has a checksum violation.
+                                                    - 3 = IP Malformed Header: the packet is not long enough to contain the IP header.
+                                                    - 4 = IP Malformed: the packet is not long enough to contain the bytes indicated by the IP
+                                                        header. Pad is allowed.
+                                                    - 5 = IP TTL Hop: the IPv4 TTL field or the IPv6 Hop Count field are zero.
+                                                    - 6 = IP Options */
+
+        uint64_t               is_bcast      : 1; /**< set if the hardware determined that the packet is a broadcast */
+        uint64_t               is_mcast      : 1; /**< set if the hardware determined that the packet is a multi-cast */
+        uint64_t               not_IP        : 1; /**< set if the packet may not be IP (must be zero in this case) */
+        uint64_t               rcv_error     : 1; /**< the receive interface hardware detected a receive error (must be zero in this case) */
+        /* lower err_code = first-level descriptor of the work */
+        /* zero for packet submitted by hardware that isn't on the slow path */
+
+        uint64_t               err_code      : 8; /**< type is cvmx_pip_err_t */
+    } s;
+
+    /**< use this to get at the 16 vlan bits */
+    struct
+    {
+        uint64_t               unused1       :16;
+        uint64_t               vlan          :16;
+        uint64_t               unused2       :32;
+    } svlan;
+
+    /**< use this struct if the hardware could not determine that the packet is ip */
+    struct
+    {
+        uint64_t               bufs          : 8; /**< HW sets this to the number of buffers used by this packet */
+        uint64_t               unused        : 8;
+        uint64_t               vlan_valid    : 1; /**< set to 1 if we found VLAN in the L2 */
+        uint64_t               unassigned    : 2;
+        uint64_t               vlan_cfi      : 1; /**< HW sets to the VLAN CFI flag (valid when vlan_valid) */
+        uint64_t               vlan_id       :12; /**< HW sets to the VLAN_ID field (valid when vlan_valid) */
+
+        uint64_t               unassigned2   :16;
+        uint64_t               software      : 1; /**< reserved for software use, hardware will clear on packet creation */
+        uint64_t               unassigned3   : 1;
+        uint64_t               is_rarp       : 1; /**< set if the hardware determined that the packet is rarp */
+        uint64_t               is_arp        : 1; /**< set if the hardware determined that the packet is arp */
+        uint64_t               is_bcast      : 1; /**< set if the hardware determined that the packet is a broadcast */
+        uint64_t               is_mcast      : 1; /**< set if the hardware determined that the packet is a multi-cast */
+        uint64_t               not_IP        : 1; /**< set if the packet may not be IP (must be one in this case) */
+        uint64_t               rcv_error     : 1; /**< the receive interface hardware detected a receive error.
+                                                    Failure indicated in err_code below, decode:
+                                                    - 1 = partial error: a packet was partially received, but internal
+                                                        buffering / bandwidth was not adequate to receive the entire packet.
+                                                    - 2 = jabber error: the RGMII packet was too large and is truncated.
+                                                    - 3 = overrun error: the RGMII packet is longer than allowed and had
+                                                        an FCS error.
+                                                    - 4 = oversize error: the RGMII packet is longer than allowed.
+                                                    - 5 = alignment error: the RGMII packet is not an integer number of bytes
+                                                        and had an FCS error (100M and 10M only).
+                                                    - 6 = fragment error: the RGMII packet is shorter than allowed and had an
+                                                        FCS error.
+                                                    - 7 = GMX FCS error: the RGMII packet had an FCS error.
+                                                    - 8 = undersize error: the RGMII packet is shorter than allowed.
+                                                    - 9 = extend error: the RGMII packet had an extend error.
+                                                    - 10 = length mismatch error: the RGMII packet had a length that did not
+                                                        match the length field in the L2 HDR.
+                                                    - 11 = RGMII RX error/SPI4 DIP4 Error: the RGMII packet had one or more
+                                                        data reception errors (RXERR) or the SPI4 packet had one or more DIP4
+                                                        errors.
+                                                    - 12 = RGMII skip error/SPI4 Abort Error: the RGMII packet was not large
+                                                        enough to cover the skipped bytes or the SPI4 packet was terminated
+                                                        with an About EOPS.
+                                                    - 13 = RGMII nibble error/SPI4 Port NXA Error: the RGMII packet had a
+                                                        studder error (data not repeated - 10/100M only) or the SPI4 packet
+                                                        was sent to an NXA.
+                                                    - 16 = FCS error: a SPI4.2 packet had an FCS error.
+                                                    - 17 = Skip error: a packet was not large enough to cover the skipped bytes.
+                                                    - 18 = L2 header malformed: the packet is not long enough to contain the L2 */
+
+
+        /* lower err_code = first-level descriptor of the work */
+        /* zero for packet submitted by hardware that isn't on the slow path */
+        uint64_t               err_code       : 8; // type is cvmx_pip_err_t (union, so can't use directly
+    } snoip;
+
+} cvmx_pip_wqe_word2;
+
+
+
+
+
+
+
+
+/**
+ * Work queue entry format
+ *
+ * must be 8-byte aligned
+ */
+typedef struct
+{
+
+    /*****************************************************************
+     * WORD 0
+     *  HW WRITE: the following 64 bits are filled by HW when a packet arrives
+     */
+
+    /**
+     * raw chksum result generated by the HW
+     */
+    uint16_t                   hw_chksum;
+    /**
+     * Field unused by hardware - available for software
+     */
+    uint8_t                    unused;
+    /**
+     * Next pointer used by hardware for list maintenance.
+     * May be written/read by HW before the work queue
+     *           entry is scheduled to a PP
+     * (Only 36 bits used in Octeon 1)
+     */
+    uint64_t                   next_ptr      : 40;
+
+
+    /*****************************************************************
+     * WORD 1
+     *  HW WRITE: the following 64 bits are filled by HW when a packet arrives
+     */
+
+    /**
+     * HW sets to the total number of bytes in the packet
+     */
+    uint64_t                   len           :16;
+    /**
+     * HW sets this to input physical port
+     */
+    uint64_t                   ipprt         : 6;
+
+    /**
+     * HW sets this to what it thought the priority of the input packet was
+     */
+    uint64_t                   qos           : 3;
+
+    /**
+     * the group that the work queue entry will be scheduled to
+     */
+    uint64_t                   grp           : 4;
+    /**
+     * the type of the tag (ORDERED, ATOMIC, NULL)
+     */
+    cvmx_pow_tag_type_t        tag_type      : 3;
+    /**
+     * the synchronization/ordering tag
+     */
+    uint64_t                   tag           :32;
+
+    /**
+     * WORD 2
+     *   HW WRITE: the following 64-bits are filled in by hardware when a packet arrives
+     *   This indicates a variety of status and error conditions.
+     */
+    cvmx_pip_wqe_word2       word2;
+
+    /**
+     * Pointer to the first segment of the packet.
+     */
+    cvmx_buf_ptr_t             packet_ptr;
+
+    /**
+     *   HW WRITE: octeon will fill in a programmable amount from the
+     *             packet, up to (at most, but perhaps less) the amount
+     *             needed to fill the work queue entry to 128 bytes
+     *   If the packet is recognized to be IP, the hardware starts (except that
+     *   the IPv4 header is padded for appropriate alignment) writing here where
+     *   the IP header starts.
+     *   If the packet is not recognized to be IP, the hardware starts writing
+     *   the beginning of the packet here.
+     */
+    uint8_t packet_data[96];
+
+
+    /**
+     * If desired, SW can make the work Q entry any length. For the
+     * purposes of discussion here, Assume 128B always, as this is all that
+     * the hardware deals with.
+     *
+     */
+
+}  CVMX_CACHE_LINE_ALIGNED cvmx_wqe_t;
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif /* __CVMX_WQE_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-zip.c b/arch/mips/cavium-octeon/executive/cvmx-zip.c
new file mode 100644
index 0000000..c729d0f
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-zip.c
@@ -0,0 +1,135 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Source file for the zip (deflate) block
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+
+#include "executive-config.h"
+#include "cvmx-config.h"
+#include "cvmx.h"
+#include "cvmx-cmd-queue.h"
+#include "cvmx-zip.h"
+
+#ifdef CVMX_ENABLE_PKO_FUNCTIONS
+
+/**
+ * Initialize the ZIP block
+ *
+ * @return Zero on success, negative on failure
+ */
+int cvmx_zip_initialize(void)
+{
+    cvmx_zip_cmd_buf_t zip_cmd_buf;
+    cvmx_cmd_queue_result_t result;
+    result = cvmx_cmd_queue_initialize(CVMX_CMD_QUEUE_ZIP, 0,
+                                       CVMX_FPA_OUTPUT_BUFFER_POOL,
+                                       CVMX_FPA_OUTPUT_BUFFER_POOL_SIZE);
+    if (result != CVMX_CMD_QUEUE_SUCCESS)
+        return -1;
+
+    zip_cmd_buf.u64 = 0;
+    zip_cmd_buf.s.dwb = CVMX_FPA_OUTPUT_BUFFER_POOL_SIZE/128;
+    zip_cmd_buf.s.pool = CVMX_FPA_OUTPUT_BUFFER_POOL;
+    zip_cmd_buf.s.size = CVMX_FPA_OUTPUT_BUFFER_POOL_SIZE/8;
+    zip_cmd_buf.s.ptr =  cvmx_ptr_to_phys(cvmx_cmd_queue_buffer(CVMX_CMD_QUEUE_ZIP))>>7;
+    cvmx_write_csr(CVMX_ZIP_CMD_BUF, zip_cmd_buf.u64);
+    cvmx_write_csr(CVMX_ZIP_ERROR, 1);
+    cvmx_read_csr(CVMX_ZIP_CMD_BUF); /* Read to make sure setup is complete */
+    return 0;
+}
+
+/**
+ * Shutdown the ZIP block. ZIP must be idle when
+ * this function is called.
+ *
+ * @return Zero on success, negative on failure
+ */
+int cvmx_zip_shutdown(void)
+{
+    cvmx_zip_cmd_ctl_t zip_cmd_ctl;
+
+    if (cvmx_cmd_queue_length(CVMX_CMD_QUEUE_ZIP))
+    {
+        cvmx_dprintf("ERROR: cvmx_zip_shutdown: ZIP not idle.\n");
+        return -1;
+    }
+
+    zip_cmd_ctl.u64 = cvmx_read_csr(CVMX_ZIP_CMD_CTL);
+    zip_cmd_ctl.s.reset = 1;
+    cvmx_write_csr(CVMX_ZIP_CMD_CTL, zip_cmd_ctl.u64);
+    cvmx_wait(100);
+
+    cvmx_cmd_queue_shutdown(CVMX_CMD_QUEUE_ZIP);
+    return 0;
+}
+
+/**
+ * Submit a command to the ZIP block
+ *
+ * @param command Zip command to submit
+ *
+ * @return Zero on success, negative on failure
+ */
+int cvmx_zip_submit(cvmx_zip_command_t *command)
+{
+    cvmx_cmd_queue_result_t result = cvmx_cmd_queue_write(CVMX_CMD_QUEUE_ZIP, 1, 8, command->u64);
+    if (result == CVMX_CMD_QUEUE_SUCCESS)
+        cvmx_write_csr(CVMX_ADDR_DID(CVMX_FULL_DID(7, 0)), 8);
+    return result;
+}
+
+#endif
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-zip.h b/arch/mips/cavium-octeon/executive/cvmx-zip.h
new file mode 100644
index 0000000..041100e
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-zip.h
@@ -0,0 +1,255 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Header file for the zip (deflate) block
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+
+#ifndef __CVMX_ZIP_H__
+#define __CVMX_ZIP_H__
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+typedef union {
+   uint64_t u64;
+   struct {
+#if __BYTE_ORDER == __BIG_ENDIAN
+      uint64_t unused              :  5;
+      uint64_t full_block_write    :  1;
+      uint64_t no_l2_alloc         :  1;
+      uint64_t little_endian       :  1;
+      uint64_t length              : 16;
+      uint64_t ptr                 : 40;
+#else
+      uint64_t ptr                 : 40;
+      uint64_t length              : 16;
+      uint64_t little_endian       :  1;
+      uint64_t no_l2_alloc         :  1;
+      uint64_t full_block_write    :  1;
+      uint64_t unused              :  5;
+#endif
+   } s;
+} cvmx_zip_ptr_t;
+#define CVMX_ZIP_PTR_MAX_LEN    ((1 << 16) - 1)
+
+
+typedef enum {
+   CVMX_ZIP_COMPLETION_NOTDONE  = 0,
+   CVMX_ZIP_COMPLETION_SUCCESS  = 1,
+   CVMX_ZIP_COMPLETION_OTRUNC   = 2,
+   CVMX_ZIP_COMPLETION_STOP     = 3,
+   CVMX_ZIP_COMPLETION_ITRUNC   = 4,
+   CVMX_ZIP_COMPLETION_RBLOCK   = 5,
+   CVMX_ZIP_COMPLETION_NLEN     = 6,
+   CVMX_ZIP_COMPLETION_BADCODE  = 7,
+   CVMX_ZIP_COMPLETION_BADCODE2 = 8,
+   CVMX_ZIP_COMPLETION_ZERO_LEN = 9,
+   CVMX_ZIP_COMPLETION_PARITY   = 10
+} cvmx_zip_completion_code_t;
+
+typedef union {
+   uint64_t u64[3];
+   struct {
+
+      // WORD 0
+#if __BYTE_ORDER == __BIG_ENDIAN
+      uint64_t crc32               : 32;
+      uint64_t adler               : 32;
+#else
+      uint64_t adler               : 32;
+      uint64_t crc32               : 32;
+#endif
+
+      // WORD 1
+#if __BYTE_ORDER == __BIG_ENDIAN
+      uint64_t totalbyteswritten   : 32;
+      uint64_t totalbytesread      : 32;
+#else
+      uint64_t totalbytesread      : 32;
+      uint64_t totalbyteswritten   : 32;
+#endif
+
+      // WORD 2
+#if __BYTE_ORDER == __BIG_ENDIAN
+      uint64_t                      totalbitsprocessed  : 32; // decompression only
+      uint64_t                      unused20            :  5;
+      uint64_t                      exnum               :  3; // compression only
+      uint64_t                      unused21            :  1;
+      uint64_t                      exbits              :  7; // compression only
+      uint64_t                      unused22            :  7;
+      uint64_t                      eof                 :  1; // decompression only
+      cvmx_zip_completion_code_t    completioncode      :  8; // If polling, SW should set this to zero and wait for non-zero
+#else
+      cvmx_zip_completion_code_t    completioncode      :  8; // If polling, SW should set this to zero and wait for non-zero
+      uint64_t                      eof                 :  1; // decompression only
+      uint64_t                      unused22            :  7;
+      uint64_t                      exbits              :  7; // compression only
+      uint64_t                      unused21            :  1;
+      uint64_t                      exnum               :  3; // compression only
+      uint64_t                      unused20            :  5;
+      uint64_t                      totalbitsprocessed  : 32; // decompression only
+#endif
+   } s;
+} cvmx_zip_result_t;
+
+typedef union {
+   uint64_t u64[8];
+   struct {
+
+      // WORD 0
+#if __BYTE_ORDER == __BIG_ENDIAN
+      uint64_t unused00            :  8;
+      uint64_t totaloutputlength   : 24;
+      uint64_t unused01            :  5;
+      uint64_t exnum               :  3;
+      uint64_t unused02            :  1;
+      uint64_t exbits              :  7;
+      uint64_t unused03            :  6;
+      uint64_t speed               :  1;
+      uint64_t forcefixed          :  1;
+      uint64_t forcedynamic        :  1;
+      uint64_t eof                 :  1;
+      uint64_t bof                 :  1;
+      uint64_t compress            :  1;
+      uint64_t unused04            :  1;
+      uint64_t dscatter            :  1;
+      uint64_t dgather             :  1;
+      uint64_t hgather             :  1;
+#else
+      uint64_t hgather             :  1;
+      uint64_t dgather             :  1;
+      uint64_t dscatter            :  1;
+      uint64_t unused04            :  1;
+      uint64_t compress            :  1;
+      uint64_t bof                 :  1;
+      uint64_t eof                 :  1;
+      uint64_t forcedynamic        :  1;
+      uint64_t forcefixed          :  1;
+      uint64_t speed               :  1;
+      uint64_t unused03            :  6;
+      uint64_t exbits              :  7;
+      uint64_t unused02            :  1;
+      uint64_t exnum               :  3;
+      uint64_t unused01            :  5;
+      uint64_t totaloutputlength   : 24;
+      uint64_t unused00            :  8;
+#endif
+
+      // WORD 1
+#if __BYTE_ORDER == __BIG_ENDIAN
+      uint64_t historylength       : 16;
+      uint64_t unused10            : 16;
+      uint64_t adler32             : 32;
+#else
+      uint64_t adler32             : 32;
+      uint64_t unused10            : 16;
+      uint64_t historylength       : 16;
+#endif
+
+      // WORD 2
+      cvmx_zip_ptr_t ctx_ptr;
+
+      // WORD 3
+      cvmx_zip_ptr_t hist_ptr;
+
+      // WORD 4
+      cvmx_zip_ptr_t in_ptr;
+
+      // WORD 5
+      cvmx_zip_ptr_t out_ptr;
+
+      // WORD 6
+      cvmx_zip_ptr_t result_ptr;
+
+      // WORD 7
+      cvmx_zip_ptr_t wq_ptr;
+
+   } s;
+} cvmx_zip_command_t;
+
+
+/**
+ * Initialize the ZIP block
+ *
+ * @return Zero on success, negative on failure
+ */
+int cvmx_zip_initialize(void);
+
+/**
+ * Shutdown the ZIP block. ZIP must be idle when
+ * this function is called.
+ *
+ * @return Zero on success, negative on failure
+ */
+int cvmx_zip_shutdown(void);
+
+/**
+ * Submit a command to the ZIP block
+ *
+ * @param command Zip command to submit
+ *
+ * @return Zero on success, negative on failure
+ */
+int cvmx_zip_submit(cvmx_zip_command_t *command);
+
+/* CSR typedefs have been moved to cvmx-csr-*.h */
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif  /* __CVMX_ZIP_H__ */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-zone.c b/arch/mips/cavium-octeon/executive/cvmx-zone.c
new file mode 100644
index 0000000..18bebd5
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-zone.c
@@ -0,0 +1,177 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Support library for the Zone Allocator.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+
+
+#include "cvmx-config.h"
+#include "cvmx.h"
+#include "cvmx-spinlock.h"
+#include "cvmx-malloc.h"
+
+
+
+
+cvmx_zone_t cvmx_zone_create_from_addr(char *name, uint32_t elem_size, uint32_t num_elem,
+                             void* mem_ptr, uint64_t mem_size, uint32_t flags)
+{
+    cvmx_zone_t zone;
+    unsigned int i;
+
+    if ((unsigned long)mem_ptr & (sizeof(void *) -1))
+    {
+        return(NULL);  //invalid alignment
+    }
+    if (mem_size  < sizeof(struct cvmx_zone) + elem_size * num_elem)
+    {
+        return(NULL);  // not enough room
+    }
+
+    zone = (cvmx_zone_t) ((char *)mem_ptr + elem_size * num_elem);
+    zone->elem_size = elem_size;
+    zone->num_elem = num_elem;
+    zone->name = name;
+    zone->align = 0;  // not used
+    zone->baseptr = NULL;
+    zone->freelist = NULL;
+    zone->lock.value = CVMX_SPINLOCK_UNLOCKED_VAL;
+
+    zone->baseptr = (char *)mem_ptr;
+
+    for(i=0;i<num_elem;i++)
+    {
+        *(void **)(zone->baseptr + (i*elem_size)) = zone->freelist;
+        zone->freelist = (void *)(zone->baseptr + (i*elem_size));
+    }
+
+    return(zone);
+
+}
+
+cvmx_zone_t cvmx_zone_create_from_arena(char *name, uint32_t elem_size, uint32_t num_elem, uint32_t align, cvmx_arena_list_t arena_list, uint32_t flags)
+{
+    unsigned int i;
+    cvmx_zone_t zone;
+
+    zone = (cvmx_zone_t)cvmx_malloc(arena_list, sizeof(struct cvmx_zone));
+
+    if (NULL == zone)
+    {
+        return(NULL);
+    }
+    zone->elem_size = elem_size;
+    zone->num_elem = num_elem;
+    zone->name = name;
+    zone->align = align;
+    zone->baseptr = NULL;
+    zone->freelist = NULL;
+    zone->lock.value = CVMX_SPINLOCK_UNLOCKED_VAL;
+
+    zone->baseptr = (char *)cvmx_memalign(arena_list, align, num_elem * elem_size);
+    if (NULL == zone->baseptr)
+    {
+        return(NULL);
+    }
+
+    for(i=0;i<num_elem;i++)
+    {
+        *(void **)(zone->baseptr + (i*elem_size)) = zone->freelist;
+        zone->freelist = (void *)(zone->baseptr + (i*elem_size));
+    }
+
+    return(zone);
+
+}
+
+
+
+void * cvmx_zone_alloc(cvmx_zone_t zone, uint32_t flags)
+{
+    cvmx_zone_t item;
+
+    assert(zone != NULL);
+    assert(zone->baseptr != NULL);
+    cvmx_spinlock_lock(&zone->lock);
+
+	item = (cvmx_zone_t)zone->freelist;
+	if(item != NULL)
+	{
+		zone->freelist = *(void **)item;
+	}
+	else
+	{
+//		cvmx_dprintf("No more elements in zone %s\n", zone->name);
+	}
+
+    cvmx_spinlock_unlock(&zone->lock);
+    return(item);
+}
+
+void cvmx_zone_free(cvmx_zone_t zone, void *ptr)
+{
+
+    assert(zone != NULL);
+    assert(zone->baseptr != NULL);
+    assert((unsigned long)ptr - (unsigned long)zone->baseptr < zone->num_elem * zone->elem_size);
+
+    cvmx_spinlock_lock(&zone->lock);
+	*(void **)ptr = zone->freelist;
+	zone->freelist = ptr;
+    cvmx_spinlock_unlock(&zone->lock);
+}
+
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx.h b/arch/mips/cavium-octeon/executive/cvmx.h
new file mode 100644
index 0000000..6129c60
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx.h
@@ -0,0 +1,1134 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * Main Octeon executive header file (This should be the second header
+ * file included by an application).
+ *
+ * <hr>$Revision: 35609 $<hr>
+*/
+#ifndef __CVMX_H__
+#define __CVMX_H__
+
+/* Control whether simple executive applications use 1-1 TLB mappings to access physical
+** memory addresses.  This must be disabled to allow large programs that use more than
+** the 0x10000000 - 0x20000000 virtual address range.
+*/
+#ifndef CVMX_USE_1_TO_1_TLB_MAPPINGS
+#define CVMX_USE_1_TO_1_TLB_MAPPINGS 1
+#endif
+
+
+#include "cvmx-platform.h"
+#include "cvmx-asm.h"
+#include "cvmx-packet.h"
+#include "cvmx-warn.h"
+#include "cvmx-sysinfo.h"
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+/* To have a global variable be shared among all cores,
+ * declare with the CVMX_SHARED attribute.  Ex:
+ * CVMX_SHARED int myglobal;
+ * This will cause the variable to be placed in a special
+ * section that the loader will map as shared for all cores
+ * This is for data structures use by software ONLY,
+ * as it is not 1-1 VA-PA mapped.
+ */
+#define CVMX_SHARED      __attribute__ ((cvmx_shared))
+
+#ifdef __cplusplus
+#define EXTERN_ASM extern "C"
+#else
+#define EXTERN_ASM extern
+#endif
+
+#ifndef CVMX_ENABLE_PARAMETER_CHECKING
+#define CVMX_ENABLE_PARAMETER_CHECKING 1
+#endif
+
+#ifndef CVMX_ENABLE_DEBUG_PRINTS
+#define CVMX_ENABLE_DEBUG_PRINTS 1
+#endif
+
+#if CVMX_ENABLE_DEBUG_PRINTS
+    #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+        #define cvmx_dprintf        printk
+    #else
+        #define cvmx_dprintf        printf
+    #endif
+#else
+    #define cvmx_dprintf(...)   {}
+#endif
+
+#define CVMX_MAX_CORES          (16)
+#define CVMX_CACHE_LINE_SIZE    (128)   // In bytes
+#define CVMX_CACHE_LINE_MASK    (CVMX_CACHE_LINE_SIZE - 1)   // In bytes
+#define CVMX_CACHE_LINE_ALIGNED __attribute__ ((aligned (CVMX_CACHE_LINE_SIZE)))
+#define CAST64(v) ((long long)(long)(v))
+#define CASTPTR(type, v) ((type *)(long)(v))
+
+
+/* simprintf uses simulator tricks to speed up printouts.  The format
+** and args are passed to the simulator and processed natively on the host.
+** Simprintf is limited to 7 arguments, and they all must use %ll (long long)
+** format specifiers to be displayed correctly.
+*/
+EXTERN_ASM void simprintf(const char *format, ...);
+
+
+/**
+ * This function performs some default initialization of the Octeon executive.  It initializes
+ * the cvmx_bootmem memory allocator with the list of physical memory provided by the bootloader,
+ * and creates 1-1 TLB mappings for this memory.
+ * This function should be called on all cores that will use either the bootmem allocator or the 1-1 TLB
+ * mappings.
+ * Applications which require a different configuration can replace this function with a suitable application
+ * specific one.
+ *
+ * @return 0 on success
+ *         -1 on failure
+ */
+int cvmx_user_app_init(void);
+
+/* Returns processor ID, different Linux and simple exec versions provided in the
+** cvmx-app-init*.c files */
+static inline uint32_t cvmx_get_proc_id(void) __attribute__ ((pure));
+static inline uint32_t cvmx_get_proc_id(void)
+{
+#ifdef CVMX_BUILD_FOR_LINUX_USER
+    extern uint32_t cvmx_app_init_processor_id;
+    return cvmx_app_init_processor_id;
+#else
+    uint32_t id;
+    asm ("mfc0 %0, $15,0" : "=r" (id));
+    return id;
+#endif
+}
+
+/* turn the variable name into a string */
+#define CVMX_TMP_STR(x) CVMX_TMP_STR2(x)
+#define CVMX_TMP_STR2(x) #x
+
+/*
+ * The macros cvmx_likely and cvmx_unlikely use the
+ * __builtin_expect GCC operation to control branch
+ * probabilities for a conditional. For example, an "if"
+ * statement in the code that will almost always be
+ * executed should be written as "if (cvmx_likely(...))".
+ * If the "else" section of an if statement is more
+ * probable, use "if (cvmx_unlikey(...))".
+ */
+#define cvmx_likely(x)      __builtin_expect(!!(x), 1)
+#define cvmx_unlikely(x)    __builtin_expect(!!(x), 0)
+
+
+/**
+ * Builds a bit mask given the required size in bits.
+ *
+ * @param bits   Number of bits in the mask
+ * @return The mask
+ */
+static inline uint64_t cvmx_build_mask(uint64_t bits)
+{
+    return ~((~0x0ull) << bits);
+}
+
+
+/**
+ * Builds a memory address for I/O based on the Major and Sub DID.
+ *
+ * @param major_did 5 bit major did
+ * @param sub_did   3 bit sub did
+ * @return I/O base address
+ */
+static inline uint64_t cvmx_build_io_address(uint64_t major_did, uint64_t sub_did)
+{
+    return ((0x1ull << 48) | (major_did << 43) | (sub_did << 40));
+}
+
+
+/**
+ * Perform mask and shift to place the supplied value into
+ * the supplied bit rage.
+ *
+ * Example: cvmx_build_bits(39,24,value)
+ * <pre>
+ * 6       5       4       3       3       2       1
+ * 3       5       7       9       1       3       5       7      0
+ * +-------+-------+-------+-------+-------+-------+-------+------+
+ * 000000000000000000000000___________value000000000000000000000000
+ * </pre>
+ *
+ * @param high_bit Highest bit value can occupy (inclusive) 0-63
+ * @param low_bit  Lowest bit value can occupy inclusive 0-high_bit
+ * @param value    Value to use
+ * @return Value masked and shifted
+ */
+static inline uint64_t cvmx_build_bits(uint64_t high_bit, uint64_t low_bit, uint64_t value)
+{
+    return ((value & cvmx_build_mask(high_bit - low_bit + 1)) << low_bit);
+}
+
+
+#ifndef TRUE
+#define FALSE   0
+#define TRUE    (!(FALSE))
+#endif
+
+typedef enum {
+   CVMX_MIPS_SPACE_XKSEG = 3LL,
+   CVMX_MIPS_SPACE_XKPHYS = 2LL,
+   CVMX_MIPS_SPACE_XSSEG = 1LL,
+   CVMX_MIPS_SPACE_XUSEG = 0LL
+} cvmx_mips_space_t;
+
+typedef enum {
+   CVMX_MIPS_XKSEG_SPACE_KSEG0 = 0LL,
+   CVMX_MIPS_XKSEG_SPACE_KSEG1 = 1LL,
+   CVMX_MIPS_XKSEG_SPACE_SSEG = 2LL,
+   CVMX_MIPS_XKSEG_SPACE_KSEG3 = 3LL
+} cvmx_mips_xkseg_space_t;
+
+// decodes <14:13> of a kseg3 window address
+typedef enum {
+   CVMX_ADD_WIN_SCR = 0L,
+   CVMX_ADD_WIN_DMA = 1L,   // see cvmx_add_win_dma_dec_t for further decode
+   CVMX_ADD_WIN_UNUSED = 2L,
+   CVMX_ADD_WIN_UNUSED2 = 3L
+} cvmx_add_win_dec_t;
+
+// decode within DMA space
+typedef enum {
+   CVMX_ADD_WIN_DMA_ADD = 0L,     // add store data to the write buffer entry, allocating it if necessary
+   CVMX_ADD_WIN_DMA_SENDMEM = 1L, // send out the write buffer entry to DRAM
+                                     // store data must be normal DRAM memory space address in this case
+   CVMX_ADD_WIN_DMA_SENDDMA = 2L, // send out the write buffer entry as an IOBDMA command
+                                     // see CVMX_ADD_WIN_DMA_SEND_DEC for data contents
+   CVMX_ADD_WIN_DMA_SENDIO = 3L,  // send out the write buffer entry as an IO write
+                                     // store data must be normal IO space address in this case
+   CVMX_ADD_WIN_DMA_SENDSINGLE = 4L, // send out a single-tick command on the NCB bus
+                                        // no write buffer data needed/used
+} cvmx_add_win_dma_dec_t;
+
+
+
+/**
+ *   Physical Address Decode
+ *
+ * Octeon-I HW never interprets this X (<39:36> reserved
+ * for future expansion), software should set to 0.
+ *
+ *  - 0x0 XXX0 0000 0000 to      DRAM         Cached
+ *  - 0x0 XXX0 0FFF FFFF
+ *
+ *  - 0x0 XXX0 1000 0000 to      Boot Bus     Uncached  (Converted to 0x1 00X0 1000 0000
+ *  - 0x0 XXX0 1FFF FFFF         + EJTAG                           to 0x1 00X0 1FFF FFFF)
+ *
+ *  - 0x0 XXX0 2000 0000 to      DRAM         Cached
+ *  - 0x0 XXXF FFFF FFFF
+ *
+ *  - 0x1 00X0 0000 0000 to      Boot Bus     Uncached
+ *  - 0x1 00XF FFFF FFFF
+ *
+ *  - 0x1 01X0 0000 0000 to      Other NCB    Uncached
+ *  - 0x1 FFXF FFFF FFFF         devices
+ *
+ * Decode of all Octeon addresses
+ */
+typedef union {
+
+   uint64_t         u64;
+
+   struct {
+      cvmx_mips_space_t          R   : 2;
+      uint64_t               offset :62;
+   } sva; // mapped or unmapped virtual address
+
+   struct {
+      uint64_t               zeroes :33;
+      uint64_t               offset :31;
+   } suseg; // mapped USEG virtual addresses (typically)
+
+   struct {
+      uint64_t                ones  :33;
+      cvmx_mips_xkseg_space_t   sp   : 2;
+      uint64_t               offset :29;
+   } sxkseg; // mapped or unmapped virtual address
+
+   struct {
+      cvmx_mips_space_t          R   : 2; // CVMX_MIPS_SPACE_XKPHYS in this case
+      uint64_t                 cca  : 3; // ignored by octeon
+      uint64_t                 mbz  :10;
+      uint64_t                  pa  :49; // physical address
+   } sxkphys; // physical address accessed through xkphys unmapped virtual address
+
+   struct {
+      uint64_t                 mbz  :15;
+      uint64_t                is_io : 1; // if set, the address is uncached and resides on MCB bus
+      uint64_t                 did  : 8; // the hardware ignores this field when is_io==0, else device ID
+      uint64_t                unaddr: 4; // the hardware ignores <39:36> in Octeon I
+      uint64_t               offset :36;
+   } sphys; // physical address
+
+   struct {
+      uint64_t               zeroes :24; // techically, <47:40> are dont-cares
+      uint64_t                unaddr: 4; // the hardware ignores <39:36> in Octeon I
+      uint64_t               offset :36;
+   } smem; // physical mem address
+
+   struct {
+      uint64_t                 mem_region  :2;
+      uint64_t                 mbz  :13;
+      uint64_t                is_io : 1; // 1 in this case
+      uint64_t                 did  : 8; // the hardware ignores this field when is_io==0, else device ID
+      uint64_t                unaddr: 4; // the hardware ignores <39:36> in Octeon I
+      uint64_t               offset :36;
+   } sio; // physical IO address
+
+   struct {
+      uint64_t                ones   : 49;
+      cvmx_add_win_dec_t   csrdec : 2;    // CVMX_ADD_WIN_SCR (0) in this case
+      uint64_t                addr   : 13;
+   } sscr; // scratchpad virtual address - accessed through a window at the end of kseg3
+
+   // there should only be stores to IOBDMA space, no loads
+   struct {
+      uint64_t                ones   : 49;
+      cvmx_add_win_dec_t   csrdec : 2;    // CVMX_ADD_WIN_DMA (1) in this case
+      uint64_t                unused2: 3;
+      cvmx_add_win_dma_dec_t type : 3;
+      uint64_t                addr   : 7;
+   } sdma; // IOBDMA virtual address - accessed through a window at the end of kseg3
+
+   struct {
+      uint64_t                didspace : 24;
+      uint64_t                unused   : 40;
+   } sfilldidspace;
+
+} cvmx_addr_t;
+
+/* These macros for used by 32 bit applications */
+
+#define CVMX_MIPS32_SPACE_KSEG0 1l
+#define CVMX_ADD_SEG32(segment, add)          (((int32_t)segment << 31) | (int32_t)(add))
+
+/* Currently all IOs are performed using XKPHYS addressing. Linux uses the
+    CvmMemCtl register to enable XKPHYS addressing to IO space from user mode.
+    Future OSes may need to change the upper bits of IO addresses. The
+    following define controls the upper two bits for all IO addresses generated
+    by the simple executive library */
+#define CVMX_IO_SEG CVMX_MIPS_SPACE_XKPHYS
+
+/* These macros simplify the process of creating common IO addresses */
+#define CVMX_ADD_SEG(segment, add)          ((((uint64_t)segment) << 62) | (add))
+#ifndef CVMX_ADD_IO_SEG
+#define CVMX_ADD_IO_SEG(add)                CVMX_ADD_SEG(CVMX_IO_SEG, (add))
+#endif
+#define CVMX_ADDR_DIDSPACE(did)             (((CVMX_IO_SEG) << 22) | ((1ULL) << 8) | (did))
+#define CVMX_ADDR_DID(did)                  (CVMX_ADDR_DIDSPACE(did) << 40)
+#define CVMX_FULL_DID(did,subdid)           (((did) << 3) | (subdid))
+
+
+// from include/ncb_rsl_id.v
+#define CVMX_OCT_DID_MIS 0ULL   // misc stuff
+#define CVMX_OCT_DID_GMX0 1ULL
+#define CVMX_OCT_DID_GMX1 2ULL
+#define CVMX_OCT_DID_PCI 3ULL
+#define CVMX_OCT_DID_KEY 4ULL
+#define CVMX_OCT_DID_FPA 5ULL
+#define CVMX_OCT_DID_DFA 6ULL
+#define CVMX_OCT_DID_ZIP 7ULL
+#define CVMX_OCT_DID_RNG 8ULL
+#define CVMX_OCT_DID_IPD 9ULL
+#define CVMX_OCT_DID_PKT 10ULL
+#define CVMX_OCT_DID_TIM 11ULL
+#define CVMX_OCT_DID_TAG 12ULL
+// the rest are not on the IO bus
+#define CVMX_OCT_DID_L2C 16ULL
+#define CVMX_OCT_DID_LMC 17ULL
+#define CVMX_OCT_DID_SPX0 18ULL
+#define CVMX_OCT_DID_SPX1 19ULL
+#define CVMX_OCT_DID_PIP 20ULL
+#define CVMX_OCT_DID_ASX0 22ULL
+#define CVMX_OCT_DID_ASX1 23ULL
+#define CVMX_OCT_DID_IOB 30ULL
+
+#define CVMX_OCT_DID_PKT_SEND       CVMX_FULL_DID(CVMX_OCT_DID_PKT,2ULL)
+#define CVMX_OCT_DID_TAG_SWTAG      CVMX_FULL_DID(CVMX_OCT_DID_TAG,0ULL)
+#define CVMX_OCT_DID_TAG_TAG1       CVMX_FULL_DID(CVMX_OCT_DID_TAG,1ULL)
+#define CVMX_OCT_DID_TAG_TAG2       CVMX_FULL_DID(CVMX_OCT_DID_TAG,2ULL)
+#define CVMX_OCT_DID_TAG_TAG3       CVMX_FULL_DID(CVMX_OCT_DID_TAG,3ULL)
+#define CVMX_OCT_DID_TAG_NULL_RD    CVMX_FULL_DID(CVMX_OCT_DID_TAG,4ULL)
+#define CVMX_OCT_DID_TAG_CSR        CVMX_FULL_DID(CVMX_OCT_DID_TAG,7ULL)
+#define CVMX_OCT_DID_FAU_FAI        CVMX_FULL_DID(CVMX_OCT_DID_IOB,0ULL)
+#define CVMX_OCT_DID_TIM_CSR        CVMX_FULL_DID(CVMX_OCT_DID_TIM,0ULL)
+#define CVMX_OCT_DID_KEY_RW         CVMX_FULL_DID(CVMX_OCT_DID_KEY,0ULL)
+#define CVMX_OCT_DID_PCI_6          CVMX_FULL_DID(CVMX_OCT_DID_PCI,6ULL)
+#define CVMX_OCT_DID_MIS_BOO        CVMX_FULL_DID(CVMX_OCT_DID_MIS,0ULL)
+#define CVMX_OCT_DID_PCI_RML        CVMX_FULL_DID(CVMX_OCT_DID_PCI,0ULL)
+#define CVMX_OCT_DID_IPD_CSR        CVMX_FULL_DID(CVMX_OCT_DID_IPD,7ULL)
+#define CVMX_OCT_DID_DFA_CSR        CVMX_FULL_DID(CVMX_OCT_DID_DFA,7ULL)
+#define CVMX_OCT_DID_MIS_CSR        CVMX_FULL_DID(CVMX_OCT_DID_MIS,7ULL)
+#define CVMX_OCT_DID_ZIP_CSR        CVMX_FULL_DID(CVMX_OCT_DID_ZIP,0ULL)
+
+
+/**
+ * Convert a memory pointer (void*) into a hardware compatable
+ * memory address (uint64_t). Octeon hardware widgets don't
+ * understand logical addresses.
+ *
+ * @param ptr    C style memory pointer
+ * @return Hardware physical address
+ */
+static inline uint64_t cvmx_ptr_to_phys(void *ptr)
+{
+    if (CVMX_ENABLE_PARAMETER_CHECKING)
+        cvmx_warn_if(ptr==NULL, "cvmx_ptr_to_phys() passed a NULL pointer\n");
+#ifdef __linux__
+    if (sizeof(void*) == 8)
+    {
+        /* We're running in 64 bit mode. Normally this means that we can use
+            40 bits of address space (the hardware limit). Unfortunately there
+            is one case were we need to limit this to 30 bits, sign extended
+            32 bit. Although these are 64 bits wide, only 30 bits can be used */
+        if ((CAST64(ptr) >> 62) == 3)
+            return CAST64(ptr) & cvmx_build_mask(30);
+        else
+            return CAST64(ptr) & cvmx_build_mask(40);
+    }
+    else
+    {
+#ifdef __KERNEL__
+	return (long)(ptr) & 0x1fffffff;
+#else
+        extern uint64_t linux_mem32_offset;
+        if (cvmx_likely(ptr))
+            return CAST64(ptr) - linux_mem32_offset;
+        else
+            return 0;
+#endif
+    }
+#elif defined(_WRS_KERNEL)
+	return (long)(ptr) & 0x7fffffff;
+#elif defined(VXWORKS_USER_MAPPINGS)
+    /* This mapping mode is used in vxWorks 5.5 to support 2GB of ram. The
+        2nd 256MB is mapped at 0x10000000 and the rest of memory is 1:1 */
+    uint64_t address = (long)ptr;
+    if (address & 0x80000000)
+        return address & 0x1fffffff;    /* KSEG pointers directly map the lower 256MB and bootbus */
+    else if ((address >= 0x10000000) && (address < 0x20000000))
+        return address + 0x400000000ull;   /* 256MB-512MB is a virtual mapping for the 2nd 256MB */
+    else
+        return address; /* Looks to be a 1:1 mapped userspace pointer */
+#else
+#if CVMX_USE_1_TO_1_TLB_MAPPINGS
+    /* We are assumung we're running the Simple Executive standalone. In this
+        mode the TLB is setup to perform 1:1 mapping and 32 bit sign extended
+        addresses are never used. Since we know all this, save the masking
+        cycles and do nothing */
+    return CAST64(ptr);
+#else
+
+    if (sizeof(void*) == 8)
+    {
+        /* We're running in 64 bit mode. Normally this means that we can use
+            40 bits of address space (the hardware limit). Unfortunately there
+            is one case were we need to limit this to 30 bits, sign extended
+            32 bit. Although these are 64 bits wide, only 30 bits can be used */
+        if ((CAST64(ptr) >> 62) == 3)
+            return CAST64(ptr) & cvmx_build_mask(30);
+        else
+            return CAST64(ptr) & cvmx_build_mask(40);
+    }
+    else
+	return (long)(ptr) & 0x7fffffff;
+
+#endif
+#endif
+}
+
+
+/**
+ * Convert a hardware physical address (uint64_t) into a
+ * memory pointer (void *).
+ *
+ * @param physical_address
+ *               Hardware physical address to memory
+ * @return Pointer to memory
+ */
+static inline void *cvmx_phys_to_ptr(uint64_t physical_address)
+{
+    if (CVMX_ENABLE_PARAMETER_CHECKING)
+        cvmx_warn_if(physical_address==0, "cvmx_phys_to_ptr() passed a zero address\n");
+#ifdef __linux__
+    if (sizeof(void*) == 8)
+    {
+        /* Just set the top bit, avoiding any TLB uglyness */
+        return CASTPTR(void, CVMX_ADD_SEG(CVMX_MIPS_SPACE_XKPHYS, physical_address));
+    }
+    else
+    {
+#ifdef __KERNEL__
+	return CASTPTR(void, CVMX_ADD_SEG32(CVMX_MIPS32_SPACE_KSEG0, physical_address));
+#else
+        extern uint64_t linux_mem32_offset;
+        if (cvmx_likely(physical_address))
+            return CASTPTR(void, physical_address + linux_mem32_offset);
+        else
+            return NULL;
+#endif
+    }
+#elif defined(_WRS_KERNEL)
+	return CASTPTR(void, CVMX_ADD_SEG32(CVMX_MIPS32_SPACE_KSEG0, physical_address));
+#elif defined(VXWORKS_USER_MAPPINGS)
+    /* This mapping mode is used in vxWorks 5.5 to support 2GB of ram. The
+        2nd 256MB is mapped at 0x10000000 and the rest of memory is 1:1 */
+    if ((physical_address >= 0x10000000) && (physical_address < 0x20000000))
+        return CASTPTR(void, CVMX_ADD_SEG32(CVMX_MIPS32_SPACE_KSEG0, physical_address));
+    else if ((physical_address >= 0x410000000ull) && (physical_address < 0x420000000ull))
+        return CASTPTR(void, physical_address - 0x400000000ull);
+    else
+        return CASTPTR(void, physical_address);
+#else
+
+#if CVMX_USE_1_TO_1_TLB_MAPPINGS
+        /* We are assumung we're running the Simple Executive standalone. In this
+            mode the TLB is setup to perform 1:1 mapping and 32 bit sign extended
+            addresses are never used. Since we know all this, save bit insert
+            cycles and do nothing */
+    return CASTPTR(void, physical_address);
+#else
+    /* Set the XKPHYS/KSEG0 bit as appropriate based on ABI */
+    if (sizeof(void*) == 8)
+        return CASTPTR(void, CVMX_ADD_SEG(CVMX_MIPS_SPACE_XKPHYS, physical_address));
+    else
+	return CASTPTR(void, CVMX_ADD_SEG32(CVMX_MIPS32_SPACE_KSEG0, physical_address));
+
+#endif
+
+#endif
+}
+
+
+#include "cvmx-csr.h"
+
+/* The following #if controls the definition of the macro
+    CVMX_BUILD_WRITE64. This macro is used to build a store operation to
+    a full 64bit address. With a 64bit ABI, this can be done with a simple
+    pointer access. 32bit ABIs require more complicated assembly */
+#if defined(CVMX_ABI_N64) || defined(CVMX_ABI_EABI)
+
+/* We have a full 64bit ABI. Writing to a 64bit address can be done with
+    a simple volatile pointer */
+#define CVMX_BUILD_WRITE64(TYPE, ST)                                    \
+static inline void cvmx_write64_##TYPE(uint64_t addr, TYPE##_t val)     \
+{                                                                       \
+    *CASTPTR(volatile TYPE##_t, addr) = val;                            \
+}
+
+#elif defined(CVMX_ABI_N32)
+
+/* The N32 ABI passes all 64bit quantities in a single register, so it is
+    possible to use the arguments directly. We have to use inline assembly
+    for the actual store since a pointer would truncate the address */
+#define CVMX_BUILD_WRITE64(TYPE, ST)                                    \
+static inline void cvmx_write64_##TYPE(uint64_t addr, TYPE##_t val)     \
+{                                                                       \
+    asm volatile (ST " %[v], 0(%[c])" ::[v] "r" (val), [c] "r" (addr)); \
+}
+
+#elif defined(CVMX_ABI_O32)
+
+#ifdef __KERNEL__
+#define CVMX_BUILD_WRITE64(TYPE, LT) extern void cvmx_write64_##TYPE(uint64_t csr_addr, TYPE##_t val);
+#else
+
+/* Ok, now the ugly stuff starts. O32 splits 64bit quantities into two
+    separate registers. Assembly must be used to put them back together
+    before they're used. What should be a simple store becomes a
+    convoluted mess of shifts and ors */
+#define CVMX_BUILD_WRITE64(TYPE, ST)                                    \
+static inline void cvmx_write64_##TYPE(uint64_t csr_addr, TYPE##_t val) \
+{                                                                       \
+    if (sizeof(TYPE##_t) == 8)                                          \
+    {                                                                   \
+        uint32_t csr_addrh = csr_addr>>32;                              \
+        uint32_t csr_addrl = csr_addr;                                  \
+        uint32_t valh = (uint64_t)val>>32;                              \
+        uint32_t vall = val;                                            \
+        uint32_t tmp1;                                                  \
+        uint32_t tmp2;                                                  \
+        uint32_t tmp3;                                                  \
+                                                                        \
+        asm volatile (                                                  \
+            ".set push\n"                                             \
+            ".set mips64\n"                                             \
+            "dsll   %[tmp1], %[valh], 32\n"                             \
+            "dsll   %[tmp2], %[csrh], 32\n"                             \
+            "dsll   %[tmp3], %[vall], 32\n"                             \
+            "dsrl   %[tmp3], %[tmp3], 32\n"                             \
+            "or     %[tmp1], %[tmp1], %[tmp3]\n"                        \
+            "dsll   %[tmp3], %[csrl], 32\n"                             \
+            "dsrl   %[tmp3], %[tmp3], 32\n"                             \
+            "or     %[tmp2], %[tmp2], %[tmp3]\n"                        \
+            ST "    %[tmp1], 0(%[tmp2])\n"                              \
+            ".set pop\n"                                             \
+            : [tmp1] "=&r" (tmp1), [tmp2] "=&r" (tmp2), [tmp3] "=&r" (tmp3)\
+            : [valh] "r" (valh), [vall] "r" (vall),                     \
+              [csrh] "r" (csr_addrh), [csrl] "r" (csr_addrl)            \
+        );                                                              \
+    }                                                                   \
+    else                                                                \
+    {                                                                   \
+        uint32_t csr_addrh = csr_addr>>32;                              \
+        uint32_t csr_addrl = csr_addr;                                  \
+        uint32_t tmp1;                                                  \
+        uint32_t tmp2;                                                  \
+                                                                        \
+        asm volatile (                                                  \
+            ".set push\n"                                             \
+            ".set mips64\n"                                             \
+            "dsll   %[tmp1], %[csrh], 32\n"                             \
+            "dsll   %[tmp2], %[csrl], 32\n"                             \
+            "dsrl   %[tmp2], %[tmp2], 32\n"                             \
+            "or     %[tmp1], %[tmp1], %[tmp2]\n"                        \
+            ST "    %[val], 0(%[tmp1])\n"                               \
+            ".set pop\n"                                             \
+            : [tmp1] "=&r" (tmp1), [tmp2] "=&r" (tmp2)                  \
+            : [val] "r" (val), [csrh] "r" (csr_addrh),                  \
+              [csrl] "r" (csr_addrl)                                    \
+        );                                                              \
+    }                                                                   \
+}
+
+#endif
+
+#else
+
+/* cvmx-abi.h didn't recognize the ABI. Force the compile to fail. */
+#error: Unsupported ABI
+
+#endif
+
+/* The following #if controls the definition of the macro
+    CVMX_BUILD_READ64. This macro is used to build a load operation from
+    a full 64bit address. With a 64bit ABI, this can be done with a simple
+    pointer access. 32bit ABIs require more complicated assembly */
+#if defined(CVMX_ABI_N64) || defined(CVMX_ABI_EABI)
+
+/* We have a full 64bit ABI. Writing to a 64bit address can be done with
+    a simple volatile pointer */
+#define CVMX_BUILD_READ64(TYPE, LT)                                     \
+static inline TYPE##_t cvmx_read64_##TYPE(uint64_t addr)                \
+{                                                                       \
+    return *CASTPTR(volatile TYPE##_t, addr);                           \
+}
+
+#elif defined(CVMX_ABI_N32)
+
+/* The N32 ABI passes all 64bit quantities in a single register, so it is
+    possible to use the arguments directly. We have to use inline assembly
+    for the actual store since a pointer would truncate the address */
+#define CVMX_BUILD_READ64(TYPE, LT)                                     \
+static inline TYPE##_t cvmx_read64_##TYPE(uint64_t addr)                \
+{                                                                       \
+    TYPE##_t val;                                                       \
+    asm volatile (LT " %[v], 0(%[c])": [v] "=r" (val) : [c] "r" (addr));\
+    return val;                                                         \
+}
+
+#elif defined(CVMX_ABI_O32)
+
+#ifdef __KERNEL__
+#define CVMX_BUILD_READ64(TYPE, LT) extern TYPE##_t cvmx_read64_##TYPE(uint64_t csr_addr);
+#else
+
+/* Ok, now the ugly stuff starts. O32 splits 64bit quantities into two
+    separate registers. Assembly must be used to put them back together
+    before they're used. What should be a simple load becomes a
+    convoluted mess of shifts and ors */
+#define CVMX_BUILD_READ64(TYPE, LT)                                     \
+static inline TYPE##_t cvmx_read64_##TYPE(uint64_t csr_addr)            \
+{                                                                       \
+    if (sizeof(TYPE##_t) == 8)                                          \
+    {                                                                   \
+        uint32_t csr_addrh = csr_addr>>32;                              \
+        uint32_t csr_addrl = csr_addr;                                  \
+        uint32_t valh;                                                  \
+        uint32_t vall;                                                  \
+                                                                        \
+        asm volatile (                                                  \
+            ".set push\n"                                               \
+            ".set mips64\n"                                             \
+            "dsll   %[valh], %[csrh], 32\n"                             \
+            "dsll   %[vall], %[csrl], 32\n"                             \
+            "dsrl   %[vall], %[vall], 32\n"                             \
+            "or     %[valh], %[valh], %[vall]\n"                        \
+            LT "    %[vall], 0(%[valh])\n"                              \
+            "dsrl   %[valh], %[vall], 32\n"                             \
+            "sll    %[vall], 0\n"                                       \
+            "sll    %[valh], 0\n"                                       \
+            ".set pop\n"                                                \
+            : [valh] "=&r" (valh), [vall] "=&r" (vall)                  \
+            : [csrh] "r" (csr_addrh), [csrl] "r" (csr_addrl)            \
+        );                                                              \
+        return ((uint64_t)valh<<32) | vall;                             \
+    }                                                                   \
+    else                                                                \
+    {                                                                   \
+        uint32_t csr_addrh = csr_addr>>32;                              \
+        uint32_t csr_addrl = csr_addr;                                  \
+        TYPE##_t val;                                                   \
+        uint32_t tmp;                                                   \
+                                                                        \
+        asm volatile (                                                  \
+            ".set push\n"                                             \
+            ".set mips64\n"                                             \
+            "dsll   %[val], %[csrh], 32\n"                              \
+            "dsll   %[tmp], %[csrl], 32\n"                              \
+            "dsrl   %[tmp], %[tmp], 32\n"                               \
+            "or     %[val], %[val], %[tmp]\n"                           \
+            LT "    %[val], 0(%[val])\n"                                \
+            ".set pop\n"                                             \
+            : [val] "=&r" (val), [tmp] "=&r" (tmp)                      \
+            : [csrh] "r" (csr_addrh), [csrl] "r" (csr_addrl)            \
+        );                                                              \
+        return val;                                                     \
+    }                                                                   \
+}
+
+#endif /* __KERNEL__ */
+
+#else
+
+/* cvmx-abi.h didn't recognize the ABI. Force the compile to fail. */
+#error: Unsupported ABI
+
+#endif
+
+/* The following defines 8 functions for writing to a 64bit address. Each
+    takes two arguments, the address and the value to write.
+    cvmx_write64_int64      cvmx_write64_uint64
+    cvmx_write64_int32      cvmx_write64_uint32
+    cvmx_write64_int16      cvmx_write64_uint16
+    cvmx_write64_int8       cvmx_write64_uint8 */
+CVMX_BUILD_WRITE64(int64, "sd");
+CVMX_BUILD_WRITE64(int32, "sw");
+CVMX_BUILD_WRITE64(int16, "sh");
+CVMX_BUILD_WRITE64(int8, "sb");
+CVMX_BUILD_WRITE64(uint64, "sd");
+CVMX_BUILD_WRITE64(uint32, "sw");
+CVMX_BUILD_WRITE64(uint16, "sh");
+CVMX_BUILD_WRITE64(uint8, "sb");
+#define cvmx_write64 cvmx_write64_uint64
+
+/* The following defines 8 functions for reading from a 64bit address. Each
+    takes the address as the only argument
+    cvmx_read64_int64       cvmx_read64_uint64
+    cvmx_read64_int32       cvmx_read64_uint32
+    cvmx_read64_int16       cvmx_read64_uint16
+    cvmx_read64_int8        cvmx_read64_uint8 */
+CVMX_BUILD_READ64(int64, "ld");
+CVMX_BUILD_READ64(int32, "lw");
+CVMX_BUILD_READ64(int16, "lh");
+CVMX_BUILD_READ64(int8, "lb");
+CVMX_BUILD_READ64(uint64, "ld");
+CVMX_BUILD_READ64(uint32, "lw");
+CVMX_BUILD_READ64(uint16, "lhu");
+CVMX_BUILD_READ64(uint8, "lbu");
+#define cvmx_read64 cvmx_read64_uint64
+
+#ifdef CVMX_BUILD_FOR_LINUX_HOST
+
+static inline void cvmx_write_csr(uint64_t csr_addr, uint64_t val)
+{
+    extern void octeon_pci_write_csr(uint64_t address, uint64_t value);
+    octeon_pci_write_csr(csr_addr, val);
+}
+
+static inline void cvmx_write_io(uint64_t io_addr, uint64_t val)
+{
+    extern void octeon_pci_write_csr(uint64_t address, uint64_t value);
+    octeon_pci_write_csr(io_addr, val);
+}
+
+static inline uint64_t cvmx_read_csr(uint64_t csr_addr)
+{
+    extern uint64_t octeon_pci_read_csr(uint64_t address);
+    return octeon_pci_read_csr(csr_addr);
+}
+
+#else /* CVMX_BUILD_FOR_LINUX_HOST */
+
+static inline void cvmx_write_csr(uint64_t csr_addr, uint64_t val)
+{
+#if 0
+    simprintf("CSR WRITE: 0x%llx <- 0x%llx\n", csr_addr & ~(1ULL<<63), val);
+#endif
+
+    cvmx_write64(csr_addr, val);
+
+    /* Perform an immediate read after every write to an RSL register to force
+        the write to complete. It doesn't matter what RSL read we do, so we
+        choose CVMX_MIO_BOOT_BIST_STAT because it is fast and harmless */
+    if ((csr_addr >> 40) == (0x800118))
+        cvmx_read64(CVMX_MIO_BOOT_BIST_STAT);
+}
+
+static inline void cvmx_write_io(uint64_t io_addr, uint64_t val)
+{
+#if 0
+    simprintf("CSR WRITE: 0x%llx <- 0x%llx\n", io_addr & ~(1ULL<<63), val);
+#endif
+    cvmx_write64(io_addr, val);
+
+}
+
+static inline uint64_t cvmx_read_csr(uint64_t csr_addr)
+{
+    uint64_t val = cvmx_read64(csr_addr);
+#if 0
+    simprintf("CSR READ: 0x%llx -> 0x%llx\n", csr_addr & ~(1ULL<<63), val);
+#endif
+    return(val);
+}
+
+#endif /* CVMX_BUILD_FOR_LINUX_HOST */
+
+
+static inline void cvmx_send_single(uint64_t data)
+{
+    const uint64_t CVMX_IOBDMA_SENDSINGLE = 0xffffffffffffa200ull;
+    cvmx_write64(CVMX_IOBDMA_SENDSINGLE, data);
+}
+
+static inline void cvmx_read_csr_async(uint64_t scraddr, uint64_t csr_addr)
+{
+    union
+    {
+        uint64_t    u64;
+        struct {
+            uint64_t    scraddr : 8;
+            uint64_t    len     : 8;
+            uint64_t    addr    :48;
+        } s;
+    } addr;
+    addr.u64 = csr_addr;
+    addr.s.scraddr = scraddr >> 3;
+    addr.s.len = 1;
+    cvmx_send_single(addr.u64);
+}
+
+/* Return true if Octeon is CN38XX pass 1 */
+static inline int cvmx_octeon_is_pass1(void)
+{
+#if OCTEON_IS_COMMON_BINARY()
+    return 0; /* Pass 1 isn't supported for common binaries */
+#else
+/* Now that we know we're built for a specific model, only check CN38XX */
+#if OCTEON_IS_MODEL(OCTEON_CN38XX)
+    return (cvmx_get_proc_id() == OCTEON_CN38XX_PASS1);
+#else
+    return 0; /* Built for non CN38XX chip, we're not CN38XX pass1 */
+#endif
+#endif
+}
+
+static inline unsigned int cvmx_get_core_num(void)
+{
+    unsigned int core_num;
+    CVMX_RDHWRNV(core_num, 0);
+    return core_num;
+}
+
+/**
+ * Returns the number of bits set in the provided value.
+ * Simple wrapper for POP instruction.
+ *
+ * @param val    32 bit value to count set bits in
+ *
+ * @return Number of bits set
+ */
+static inline uint32_t cvmx_pop(uint32_t val)
+{
+    uint32_t pop;
+    CVMX_POP(pop, val);
+    return pop;
+}
+/**
+ * Returns the number of bits set in the provided value.
+ * Simple wrapper for DPOP instruction.
+ *
+ * @param val    64 bit value to count set bits in
+ *
+ * @return Number of bits set
+ */
+static inline int cvmx_dpop(uint64_t val)
+{
+    int pop;
+    CVMX_DPOP(pop, val);
+    return pop;
+}
+
+/**
+ * Provide current cycle counter as a return value
+ *
+ * @return current cycle counter
+ */
+
+#if defined(CVMX_ABI_O32)
+static inline uint64_t cvmx_get_cycle(void)
+{
+    uint32_t tmp_low, tmp_hi;
+
+    asm volatile (
+               "   .set push                    \n"
+               "   .set mips64r2                \n"
+               "   .set noreorder               \n"
+               "   rdhwr %[tmpl], $31           \n"
+               "   dsrl  %[tmph], %[tmpl], 32   \n"
+               "   sll   %[tmpl], 0             \n"
+               "   sll   %[tmph], 0             \n"
+               "   .set pop                 \n"
+                  : [tmpl] "=&r" (tmp_low), [tmph] "=&r" (tmp_hi) : );
+
+    return(((uint64_t)tmp_hi << 32) + tmp_low);
+}
+#else
+static inline uint64_t cvmx_get_cycle(void)
+{
+    uint64_t cycle;
+#ifdef CVMX_BUILD_FOR_LINUX_HOST
+    cycle = cvmx_read_csr(CVMX_IPD_CLK_COUNT);
+#else
+    CVMX_RDHWR(cycle, 31);
+#endif
+    return(cycle);
+}
+#endif
+
+/**
+ * Reads a chip global cycle counter.  This counts CPU cycles since
+ * chip reset.  The counter is 64 bit.
+ * This register does not exist on CN38XX pass 1 silicion
+ *
+ * @return Global chip cycle count since chip reset.
+ */
+static inline uint64_t cvmx_get_cycle_global(void)
+{
+    if (cvmx_octeon_is_pass1())
+        return 0;
+    else
+        return cvmx_read64(CVMX_IPD_CLK_COUNT);
+}
+/**
+ * Wait for the specified number of cycle
+ *
+ * @param cycles
+ */
+static inline void cvmx_wait(uint64_t cycles)
+{
+    uint64_t done = cvmx_get_cycle() + cycles;
+
+    while (cvmx_get_cycle() < done)
+    {
+        /* Spin */
+    }
+}
+
+/**
+ * Wait for the specified number of micro seconds
+ *
+ * @param usec   micro seconds to wait
+ */
+static inline void cvmx_wait_usec(uint64_t usec)
+{
+    uint64_t done = cvmx_get_cycle() + usec * cvmx_sysinfo_get()->cpu_clock_hz / 1000000;
+    while (cvmx_get_cycle() < done)
+    {
+        /* Spin */
+    }
+}
+
+/**
+ * This macro spins on a field waiting for it to reach a value. It
+ * is common in code to need to wait for a specific field in a CSR
+ * to match a specific value. Conceptually this macro expands to:
+ *
+ * 1) read csr at "address" with a csr typedef of "type"
+ * 2) Check if ("type".s."field" "op" "value")
+ * 3) If #2 isn't true loop to #1 unless too much time has passed.
+ */
+#define CVMX_WAIT_FOR_FIELD64(address, type, field, op, value, timeout_usec)\
+    ({int result;                                                       \
+    do {                                                                \
+        uint64_t done = cvmx_get_cycle() + (uint64_t)timeout_usec *     \
+                           cvmx_sysinfo_get()->cpu_clock_hz / 1000000;  \
+        type c;                                                         \
+        while (1)                                                       \
+        {                                                               \
+            c.u64 = cvmx_read_csr(address);                             \
+            if ((c.s.field) op (value)) {                               \
+                result = 0;                                             \
+                break;                                                  \
+            } else if (cvmx_get_cycle() > done) {                       \
+                result = -1;                                            \
+                break;                                                  \
+            } else                                                      \
+                cvmx_wait(100);                                         \
+        }                                                               \
+    } while (0);                                                        \
+    result;})
+
+/***************************************************************************/
+
+/* Watchdog defines, to be moved.... */
+typedef enum {
+   CVMX_CIU_WDOG_MODE_OFF = 0,
+   CVMX_CIU_WDOG_MODE_INT = 1,
+   CVMX_CIU_WDOG_MODE_INT_NMI = 2,
+   CVMX_CIU_WDOG_MODE_INT_NMI_SR = 3
+} cvmx_ciu_wdog_mode_t;
+
+
+static inline void cvmx_reset_octeon(void)
+{
+    cvmx_ciu_soft_rst_t ciu_soft_rst;
+    ciu_soft_rst.u64 = 0;
+    ciu_soft_rst.s.soft_rst = 1;
+    cvmx_write_csr(CVMX_CIU_SOFT_RST, ciu_soft_rst.u64);
+}
+
+/* Return the number of cores available in the chip */
+static inline uint32_t cvmx_octeon_num_cores(void)
+{
+    uint32_t ciu_fuse = (uint32_t)cvmx_read_csr(CVMX_CIU_FUSE) & 0xffff;
+#ifdef CVMX_BUILD_FOR_LINUX_HOST
+    int bit_count = 0;
+    while (ciu_fuse)
+    {
+        if (ciu_fuse&1)
+            bit_count++;
+        ciu_fuse>>=1;
+    }
+    return bit_count;
+#else
+    return cvmx_pop(ciu_fuse);
+#endif
+}
+
+/**
+ * Read a byte of fuse data
+ * @param byte_addr   address to read
+ *
+ * @return fuse value: 0 or 1
+ */
+static uint8_t cvmx_fuse_read_byte(int byte_addr)
+{
+    cvmx_mio_fus_rcmd_t read_cmd;
+
+    read_cmd.u64 = 0;
+    read_cmd.s.addr = byte_addr;
+    read_cmd.s.pend = 1;
+    cvmx_write_csr(CVMX_MIO_FUS_RCMD, read_cmd.u64);
+    while ((read_cmd.u64 = cvmx_read_csr(CVMX_MIO_FUS_RCMD)) && read_cmd.s.pend)
+        ;
+    return(read_cmd.s.dat);
+}
+/**
+ * Read a single fuse bit
+ *
+ * @param fuse   Fuse number (0-1024)
+ *
+ * @return fuse value: 0 or 1
+ */
+static inline int cvmx_fuse_read(int fuse)
+{
+    return((cvmx_fuse_read_byte(fuse >> 3) >> (fuse & 0x7)) & 1);
+}
+
+static inline int cvmx_octeon_model_CN36XX(void)
+{
+    return(OCTEON_IS_MODEL(OCTEON_CN38XX)
+           && !cvmx_octeon_is_pass1()
+           &&cvmx_fuse_read(264));
+}
+static inline int cvmx_octeon_zip_present(void)
+{
+    return octeon_has_feature(OCTEON_FEATURE_ZIP);
+}
+static inline int cvmx_octeon_dfa_present(void)
+{
+    if (!OCTEON_IS_MODEL(OCTEON_CN38XX) && !OCTEON_IS_MODEL(OCTEON_CN31XX) && !OCTEON_IS_MODEL(OCTEON_CN58XX))
+        return 0;
+    else if (OCTEON_IS_MODEL(OCTEON_CN3020))
+        return 0;
+    else if (cvmx_octeon_is_pass1())
+        return 1;
+    else
+        return(!cvmx_fuse_read(120));
+}
+static inline int cvmx_octeon_crypto_present(void)
+{
+    return octeon_has_feature(OCTEON_FEATURE_CRYPTO);
+}
+
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif  /*  __CVMX_H__  */
diff --git a/arch/mips/cavium-octeon/executive/cvmx.mk b/arch/mips/cavium-octeon/executive/cvmx.mk
new file mode 100644
index 0000000..2e22018
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx.mk
@@ -0,0 +1,151 @@
+#/***********************license start***************
+# * Copyright (c) 2003-2007  Cavium Networks (support@cavium.com). All rights
+# * reserved.
+# *
+# *
+# * Redistribution and use in source and binary forms, with or without
+# * modification, are permitted provided that the following conditions are
+# * met:
+# *
+# *     * Redistributions of source code must retain the above copyright
+# *       notice, this list of conditions and the following disclaimer.
+# *
+# *     * Redistributions in binary form must reproduce the above
+# *       copyright notice, this list of conditions and the following
+# *       disclaimer in the documentation and/or other materials provided
+# *       with the distribution.
+# *
+# *     * Neither the name of Cavium Networks nor the names of
+# *       its contributors may be used to endorse or promote products
+# *       derived from this software without specific prior written
+# *       permission.
+# *
+# * This Software, including technical data, may be subject to U.S.  export
+# * control laws, including the U.S.  Export Administration Act and its
+# * associated regulations, and may be subject to export or import regulations
+# * in other countries.  You warrant that You will comply strictly in all
+# * respects with all such regulations and acknowledge that you have the
+# * responsibility to obtain licenses to export, re-export or import the
+# * Software.
+# *
+# * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+# * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+# * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+# * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+# * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+# * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+# * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+# * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+# * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+# * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+# *
+# *
+# * For any questions regarding licensing please contact marketing@caviumnetworks.com
+# *
+# ***********************license end**************************************/
+
+#
+#  component Makefile fragment
+#
+
+#  standard component Makefile header
+sp              :=  $(sp).x
+dirstack_$(sp)  :=  $(d)
+d               :=  $(dir)
+
+#  component specification
+
+LIBRARY := $(OBJ_DIR)/libcvmx.a
+
+OBJS_$(d)  :=  \
+	$(OBJ_DIR)/cvmx-bootmem.o \
+	$(OBJ_DIR)/cvmx-cn3010-evb-hs5.o \
+	$(OBJ_DIR)/cvmx-core.o \
+	$(OBJ_DIR)/cvmx-coremask.o \
+	$(OBJ_DIR)/cvmx-cmd-queue.o \
+	$(OBJ_DIR)/cvmx-csr-db.o \
+	$(OBJ_DIR)/cvmx-csr-db-support.o \
+	$(OBJ_DIR)/cvmx-dfa.o \
+	$(OBJ_DIR)/cvmx-dma-engine.o \
+	$(OBJ_DIR)/cvmx-ebt3000.o \
+	$(OBJ_DIR)/cvmx-flash.o \
+	$(OBJ_DIR)/cvmx-fpa.o \
+	$(OBJ_DIR)/cvmx-helper-board.o \
+	$(OBJ_DIR)/cvmx-helper-errata.o \
+	$(OBJ_DIR)/cvmx-helper-fpa.o \
+	$(OBJ_DIR)/cvmx-helper-loop.o \
+	$(OBJ_DIR)/cvmx-helper-npi.o \
+	$(OBJ_DIR)/cvmx-helper-rgmii.o \
+	$(OBJ_DIR)/cvmx-helper-sgmii.o \
+	$(OBJ_DIR)/cvmx-helper-spi.o \
+	$(OBJ_DIR)/cvmx-helper-util.o \
+	$(OBJ_DIR)/cvmx-helper-xaui.o \
+	$(OBJ_DIR)/cvmx-helper.o \
+	$(OBJ_DIR)/cvmx-interrupt-rsl.o \
+	$(OBJ_DIR)/cvmx-interrupt-decodes.o \
+	$(OBJ_DIR)/cvmx-l2c.o \
+	$(OBJ_DIR)/cvmx-llm.o \
+	$(OBJ_DIR)/cvmx-log-arc.o \
+	$(OBJ_DIR)/cvmx-log.o \
+	$(OBJ_DIR)/cvmx-mgmt-port.o \
+	$(OBJ_DIR)/cvmx-pcie.o \
+	$(OBJ_DIR)/cvmx-pko.o \
+	$(OBJ_DIR)/cvmx-pow.o \
+	$(OBJ_DIR)/cvmx-raid.o \
+	$(OBJ_DIR)/cvmx-spi.o \
+	$(OBJ_DIR)/cvmx-spi4000.o \
+	$(OBJ_DIR)/cvmx-sysinfo.o \
+	$(OBJ_DIR)/cvmx-thunder.o \
+	$(OBJ_DIR)/cvmx-tim.o \
+	$(OBJ_DIR)/cvmx-tra.o \
+	$(OBJ_DIR)/cvmx-twsi.o \
+	$(OBJ_DIR)/cvmx-usb.o \
+	$(OBJ_DIR)/cvmx-warn.o \
+	$(OBJ_DIR)/cvmx-zip.o \
+	$(OBJ_DIR)/cvmx-zone.o \
+	$(OBJ_DIR)/octeon-model.o \
+	$(OBJ_DIR)/octeon-pci-console.o
+ifeq (linux,$(findstring linux,$(OCTEON_TARGET)))
+OBJS_$(d)  +=  \
+	$(OBJ_DIR)/cvmx-app-init-linux.o
+else
+OBJS_$(d)  +=  \
+	$(OBJ_DIR)/cvmx-interrupt.o \
+	$(OBJ_DIR)/cvmx-interrupt-handler.o \
+	$(OBJ_DIR)/cvmx-app-init.o \
+	$(OBJ_DIR)/cvmx-malloc.o
+endif
+
+$(OBJS_$(d)):  CFLAGS_LOCAL := -I$(d) -O2 -g -W -Wall -Wno-unused-parameter -Wundef
+
+#  standard component Makefile rules
+
+DEPS_$(d)   :=  $(OBJS_$(d):.o=.d)
+
+LIBS_LIST   :=  $(LIBS_LIST) $(LIBRARY)
+
+CLEAN_LIST  :=  $(CLEAN_LIST) $(OBJS_$(d)) $(DEPS_$(d)) $(LIBRARY)
+
+-include $(DEPS_$(d))
+
+$(LIBRARY): $(OBJS_$(d))
+	$(AR) -cr $@ $^
+
+$(OBJ_DIR)/%.o:	$(d)/%.c
+	$(COMPILE)
+
+$(OBJ_DIR)/%.o:	$(d)/%.S
+	$(ASSEMBLE)
+
+$(OBJ_DIR)/cvmx-app-init-linux.o: $(d)/cvmx-app-init-linux.c
+	$(CC) $(CFLAGS_GLOBAL) $(CFLAGS_LOCAL) -MD -c -Umain -o $@ $<
+
+CFLAGS_SPECIAL := -I$(d) -I$(d)/cvmx-malloc -O2 -g -DUSE_CVM_THREADS=1 -D_REENTRANT
+
+$(OBJ_DIR)/cvmx-malloc.o: $(d)/cvmx-malloc/malloc.c
+	$(CC) $(CFLAGS_GLOBAL) $(CFLAGS_SPECIAL) -MD -c -o $@ $<
+
+#  standard component Makefile footer
+
+d   :=  $(dirstack_$(sp))
+sp  :=  $(basename $(sp))
diff --git a/arch/mips/cavium-octeon/executive/executive-config.h.template b/arch/mips/cavium-octeon/executive/executive-config.h.template
new file mode 100644
index 0000000..c7c58c6
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/executive-config.h.template
@@ -0,0 +1,188 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+/*! 
+ * @file executive-config.h.template
+ *
+ * This file is a template for the executive-config.h file that each
+ * application that uses the simple exec must provide.  Each application
+ * should have an executive-config.h file in a directory named 'config'.
+ * If the application uses other components, config files for those
+ * components should be placed in the config directory as well.  The 
+ * macros defined in this file control the configuration and functionality
+ * provided by the simple executive.  Available macros are commented out
+ * and documented in this file.
+ */
+
+/*
+ * File version info: $Id: executive-config.h.template 32636 2008-03-07 21:43:34Z rfranz $
+ *
+ */
+#ifndef __EXECUTIVE_CONFIG_H__
+#define __EXECUTIVE_CONFIG_H__
+
+/* Define to enable the use of simple executive DFA functions */
+//#define CVMX_ENABLE_DFA_FUNCTIONS
+
+/* Define to enable the use of simple executive packet output functions.
+** For packet I/O setup enable the helper functions below. 
+*/ 
+//#define CVMX_ENABLE_PKO_FUNCTIONS
+
+/* Define to enable the use of simple executive timer bucket functions. 
+** Refer to cvmx-tim.[ch] for more information
+*/
+//#define CVMX_ENABLE_TIMER_FUNCTIONS
+
+/* Define to enable the use of simple executive helper functions. These
+** include many harware setup functions.  See cvmx-helper.[ch] for
+** details.
+*/
+//#define CVMX_ENABLE_HELPER_FUNCTIONS
+
+/* CVMX_HELPER_FIRST_MBUFF_SKIP is the number of bytes to reserve before
+** the beginning of the packet. If necessary, override the default  
+** here.  See the IPD section of the hardware manual for MBUFF SKIP 
+** details.*/ 
+#define CVMX_HELPER_FIRST_MBUFF_SKIP 184
+
+/* CVMX_HELPER_NOT_FIRST_MBUFF_SKIP is the number of bytes to reserve in each
+** chained packet element. If necessary, override the default here */
+#define CVMX_HELPER_NOT_FIRST_MBUFF_SKIP 0
+
+/* CVMX_HELPER_ENABLE_BACK_PRESSURE controls whether back pressure is enabled
+** for all input ports. This controls if IPD sends backpressure to all ports if
+** Octeon's FPA pools don't have enough packet or work queue entries. Even when
+** this is off, it is still possible to get backpressure from individual
+** hardware ports. When configuring backpressure, also check
+** CVMX_HELPER_DISABLE_*_BACKPRESSURE below. If necessary, override the default
+** here */
+#define CVMX_HELPER_ENABLE_BACK_PRESSURE 1
+
+/* CVMX_HELPER_ENABLE_IPD controls if the IPD is enabled in the helper
+**  function. Once it is enabled the hardware starts accepting packets. You
+**  might want to skip the IPD enable if configuration changes are need
+**  from the default helper setup. If necessary, override the default here */
+#define CVMX_HELPER_ENABLE_IPD 1
+
+/* CVMX_HELPER_INPUT_TAG_TYPE selects the type of tag that the IPD assigns
+** to incoming packets. */
+#define CVMX_HELPER_INPUT_TAG_TYPE CVMX_POW_TAG_TYPE_ORDERED
+
+/* The following select which fields are used by the PIP to generate
+** the tag on INPUT
+** 0: don't include
+** 1: include */
+#define CVMX_HELPER_INPUT_TAG_IPV6_SRC_IP	0
+#define CVMX_HELPER_INPUT_TAG_IPV6_DST_IP   	0
+#define CVMX_HELPER_INPUT_TAG_IPV6_SRC_PORT 	0
+#define CVMX_HELPER_INPUT_TAG_IPV6_DST_PORT 	0
+#define CVMX_HELPER_INPUT_TAG_IPV6_NEXT_HEADER 	0
+#define CVMX_HELPER_INPUT_TAG_IPV4_SRC_IP	0
+#define CVMX_HELPER_INPUT_TAG_IPV4_DST_IP   	0
+#define CVMX_HELPER_INPUT_TAG_IPV4_SRC_PORT 	0
+#define CVMX_HELPER_INPUT_TAG_IPV4_DST_PORT 	0
+#define CVMX_HELPER_INPUT_TAG_IPV4_PROTOCOL	0
+#define CVMX_HELPER_INPUT_TAG_INPUT_PORT	1
+
+/* Select skip mode for input ports */
+#define CVMX_HELPER_INPUT_PORT_SKIP_MODE	CVMX_PIP_PORT_CFG_MODE_SKIPL2
+
+/* Define the number of queues per output port */
+#define CVMX_HELPER_PKO_QUEUES_PER_PORT_INTERFACE0	1
+#define CVMX_HELPER_PKO_QUEUES_PER_PORT_INTERFACE1	1
+
+/* Configure PKO to use per-core queues (PKO lockless operation). 
+** Please see the related SDK documentation for PKO that illustrates 
+** how to enable and configure this option. */
+//#define CVMX_ENABLE_PKO_LOCKLESS_OPERATION 1
+//#define CVMX_HELPER_PKO_MAX_PORTS_INTERFACE0 8
+//#define CVMX_HELPER_PKO_MAX_PORTS_INTERFACE1 8
+
+/* Force backpressure to be disabled.  This overrides all other
+** backpressure configuration */
+#define CVMX_HELPER_DISABLE_RGMII_BACKPRESSURE 1
+
+/* Disable the SPI4000's processing of backpressure packets and backpressure
+** generation. When this is 1, the SPI4000 will not stop sending packets when
+** receiving backpressure. It will also not generate backpressure packets when
+** its internal FIFOs are full. */
+#define CVMX_HELPER_DISABLE_SPI4000_BACKPRESSURE 1
+
+/* CVMX_HELPER_SPI_TIMEOUT is used to determine how long the SPI initialization
+** routines wait for SPI training. You can override the value using
+** executive-config.h if necessary */
+#define CVMX_HELPER_SPI_TIMEOUT 10
+
+/* Select the number of low latency memory ports (interfaces) that
+** will be configured.  Valid values are 1 and 2.
+*/
+#define CVMX_LLM_CONFIG_NUM_PORTS 2
+
+/* Enable the fix for PKI-100 errata ("Size field is 8 too large in WQE and next
+** pointers"). If CVMX_ENABLE_LEN_M8_FIX is set to 0, the fix for this errata will 
+** not be enabled. 
+** 0: Fix is not enabled
+** 1: Fix is enabled, if supported by hardware
+*/
+#define CVMX_ENABLE_LEN_M8_FIX  1
+
+#if defined(CVMX_ENABLE_HELPER_FUNCTIONS) && !defined(CVMX_ENABLE_PKO_FUNCTIONS)
+#define CVMX_ENABLE_PKO_FUNCTIONS
+#endif
+
+/* Enable setting up of TLB entries to trap NULL pointer references */
+#define CVMX_CONFIG_NULL_POINTER_PROTECT	1
+
+/* Enable debug and informational printfs */
+#define CVMX_CONFIG_ENABLE_DEBUG_PRINTS 	1
+
+/* Executive resource descriptions provided in cvmx-resources.config */
+#include "cvmx-resources.config"
+
+#endif
diff --git a/arch/mips/cavium-octeon/executive/octeon-feature.h b/arch/mips/cavium-octeon/executive/octeon-feature.h
new file mode 100644
index 0000000..dd9e1c3
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/octeon-feature.h
@@ -0,0 +1,137 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+/**
+ * @file
+ *
+ * File defining checks for different Octeon features.
+ *
+ * <hr>$Revision: 30468 $<hr>
+ */
+
+#ifndef __OCTEON_FEATURE_H__
+#define __OCTEON_FEATURE_H__
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+typedef enum
+{
+    OCTEON_FEATURE_SAAD,        /* Octeon models in the CN5XXX family and higher support atomic add instructions to memory (saa/saad) */
+    OCTEON_FEATURE_ZIP,         /* Does this Octeon support the ZIP offload engine? */
+    OCTEON_FEATURE_CRYPTO,      /* Does this Octeon support crypto acceleration using COP2? */
+    OCTEON_FEATURE_PCIE,        /* Does this Octeon support PCI express? */
+    OCTEON_FEATURE_KEY_MEMORY,  /* Some Octeon models support internal memory for storing cryptographic keys */
+    OCTEON_FEATURE_LED_CONTROLLER, /* Octeon has a LED controller for banks of external LEDs */
+    OCTEON_FEATURE_TRA,         /* Octeon has a trace buffer */
+    OCTEON_FEATURE_MGMT_PORT,   /* Octeon has a management port */
+    OCTEON_FEATURE_RAID,        /* Octeon has a raid unit */
+    OCTEON_FEATURE_USB,         /* Octeon has a builtin USB */
+} octeon_feature_t;
+
+#ifdef __mips__
+static inline int cvmx_fuse_read(int fuse);
+#endif
+
+/**
+ * Determine if the current Octeon supports a specific feature. These
+ * checks have been optimized to be fairly quick, but they should still
+ * be kept out of fast path code.
+ *
+ * @param feature Feature to check for. This should always be a constant so the
+ *                compiler can remove the switch statement through optimization.
+ *
+ * @return Non zero if the feature exists. Zero if the feature does not
+ *         exist.
+ */
+static inline int octeon_has_feature(octeon_feature_t feature)
+{
+#ifdef __mips__
+    switch (feature)
+    {
+        case OCTEON_FEATURE_SAAD:
+            return !OCTEON_IS_MODEL(OCTEON_CN3XXX);
+
+        case OCTEON_FEATURE_ZIP:
+            if (OCTEON_IS_MODEL(OCTEON_CN30XX) || OCTEON_IS_MODEL(OCTEON_CN50XX) || OCTEON_IS_MODEL(OCTEON_CN52XX))
+                return 0;
+            else if (OCTEON_IS_MODEL(OCTEON_CN38XX_PASS1))
+                return 1;
+            else
+                return (!cvmx_fuse_read(121));
+
+        case OCTEON_FEATURE_CRYPTO:
+            return (!cvmx_fuse_read(90));
+
+        case OCTEON_FEATURE_PCIE:
+            return (OCTEON_IS_MODEL(OCTEON_CN56XX) || OCTEON_IS_MODEL(OCTEON_CN52XX));
+
+        case OCTEON_FEATURE_KEY_MEMORY:
+        case OCTEON_FEATURE_LED_CONTROLLER:
+            return (OCTEON_IS_MODEL(OCTEON_CN38XX) || OCTEON_IS_MODEL(OCTEON_CN58XX) || OCTEON_IS_MODEL(OCTEON_CN56XX));
+        case OCTEON_FEATURE_TRA:
+            return !(OCTEON_IS_MODEL(OCTEON_CN30XX) || OCTEON_IS_MODEL(OCTEON_CN50XX));
+        case OCTEON_FEATURE_MGMT_PORT:
+            return (OCTEON_IS_MODEL(OCTEON_CN56XX) || OCTEON_IS_MODEL(OCTEON_CN52XX));
+        case OCTEON_FEATURE_RAID:
+            return (OCTEON_IS_MODEL(OCTEON_CN56XX) || OCTEON_IS_MODEL(OCTEON_CN52XX));
+        case OCTEON_FEATURE_USB:
+            return !(OCTEON_IS_MODEL(OCTEON_CN38XX) || OCTEON_IS_MODEL(OCTEON_CN58XX));
+    }
+#endif
+    return 0;
+}
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif    /* __OCTEON_FEATURE_H__ */
diff --git a/arch/mips/cavium-octeon/executive/octeon-model.c b/arch/mips/cavium-octeon/executive/octeon-model.c
new file mode 100644
index 0000000..d93fa20
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/octeon-model.c
@@ -0,0 +1,395 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+/**
+ * @file
+ *
+ * File defining functions for working with different Octeon
+ * models.
+ *
+ * <hr>$Revision: 34419 $<hr>
+ */
+#include "cvmx.h"
+#include "cvmx-pow.h"
+#include "cvmx-warn.h"
+
+#if defined(CVMX_BUILD_FOR_LINUX_USER) || defined(CVMX_BUILD_FOR_STANDALONE)
+#include <octeon-app-init.h>
+#include "cvmx-sysinfo.h"
+
+/**
+ * This function checks to see if the software is compatible with the
+ * chip it is running on.  This is called in the application startup code
+ * and does not need to be called directly by the application.
+ * Does not return if software is incompatible.
+ *
+ * @param chip_id chip id that the software is being run on.
+ *
+ * @return 0: runtime checking or exact version match
+ *         1: chip is newer revision than compiled for, but software will run properly.
+ */
+int octeon_model_version_check(uint32_t chip_id)
+{
+    //printf("Model Number: %s\n", octeon_model_get_string(chip_id));
+#if OCTEON_IS_COMMON_BINARY()
+    if (chip_id == OCTEON_CN38XX_PASS1)
+    {
+        printf("Runtime Octeon Model checking binaries do not support OCTEON_CN38XX_PASS1 chips\n");
+#ifdef CVMX_BUILD_FOR_STANDALONE
+        if (cvmx_sysinfo_get()->board_type == CVMX_BOARD_TYPE_SIM)
+            CVMX_BREAK;
+        while (1);
+#else
+        exit(-1);
+#endif
+    }
+#else
+    /* Check for special case of mismarked 3005 samples, and adjust cpuid */
+    if (chip_id == OCTEON_CN3010_PASS1 && (cvmx_read_csr(0x80011800800007B8ull) & (1ull << 34)))
+        chip_id |= 0x10;
+
+    if ((OCTEON_MODEL & 0xffffff) != chip_id)
+    {
+        if (!OCTEON_IS_MODEL((OM_IGNORE_REVISION | chip_id)) || (OCTEON_MODEL & 0xffffff) > chip_id || (((OCTEON_MODEL & 0xffffff) ^ chip_id) & 0x10))
+        {
+            printf("ERROR: Software not configured for this chip\n"
+                   "         Expecting ID=0x%08x, Chip is 0x%08x\n", (OCTEON_MODEL & 0xffffff), (unsigned int)chip_id);
+            if ((OCTEON_MODEL & 0xffffff) > chip_id)
+                printf("Refusing to run on older revision than program was compiled for.\n");
+#ifdef CVMX_BUILD_FOR_STANDALONE
+            if (cvmx_sysinfo_get()->board_type == CVMX_BOARD_TYPE_SIM)
+                CVMX_BREAK;
+            while (1);
+#else
+            exit(-1);
+#endif
+        }
+        else
+        {
+            printf("\n###################################################\n");
+            printf("WARNING: Software configured for older revision than running on.\n"
+                   "         Compiled for ID=0x%08x, Chip is 0x%08x\n", (OCTEON_MODEL & 0xffffff), (unsigned int)chip_id);
+            printf("###################################################\n\n");
+            return(1);
+        }
+    }
+#endif
+
+    cvmx_warn_if(CVMX_ENABLE_PARAMETER_CHECKING, "Parameter checks are enabled. Expect some performance loss due to the extra checking\n");
+    cvmx_warn_if(CVMX_ENABLE_CSR_ADDRESS_CHECKING, "CSR address checks are enabled. Expect some performance loss due to the extra checking\n");
+    cvmx_warn_if(CVMX_ENABLE_POW_CHECKS, "POW state checks are enabled. Expect some performance loss due to the extra checking\n");
+
+    return(0);
+}
+
+#endif
+/**
+ * Given the chip processor ID from COP0, this function returns a
+ * string representing the chip model number. The string is of the
+ * form CNXXXXpX.X-FREQ-SUFFIX.
+ * - XXXX = The chip model number
+ * - X.X = Chip pass number
+ * - FREQ = Current frequency in Mhz
+ * - SUFFIX = NSP, EXP, SCP, SSP, or CP
+ *
+ * @param chip_id Chip ID
+ *
+ * @return Model string
+ */
+const char *octeon_model_get_string(uint32_t chip_id)
+{
+    static char         buffer[32];
+    return octeon_model_get_string_buffer(chip_id,buffer);
+}
+
+/* Version of octeon_model_get_string() that takes buffer as argument, as
+** running early in u-boot static/global variables don't work when running from
+** flash
+*/
+const char *octeon_model_get_string_buffer(uint32_t chip_id, char * buffer)
+{
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+    extern uint64_t octeon_get_clock_rate(void);
+#endif
+    const char *        family;
+    const char *        core_model;
+    char                pass[4];
+    int                 clock_mhz;
+    const char *        suffix;
+    cvmx_l2d_fus3_t     fus3;
+    int                 num_cores;
+    cvmx_mio_fus_dat2_t fus_dat2;
+    cvmx_mio_fus_dat3_t fus_dat3;
+    char fuse_model[10];
+    uint32_t fuse_data = 0;
+
+    fus3.u64 = cvmx_read_csr(CVMX_L2D_FUS3);
+    fus_dat2.u64 = cvmx_read_csr(CVMX_MIO_FUS_DAT2);
+    fus_dat3.u64 = cvmx_read_csr(CVMX_MIO_FUS_DAT3);
+    num_cores = __builtin_popcount(cvmx_read_csr(CVMX_CIU_FUSE));
+
+    /* Make sure the non existant devices look disabled */
+    switch ((chip_id >> 8) & 0xff)
+    {
+        case 6: /* CN50XX */
+        case 2: /* CN30XX */
+            fus_dat3.s.nodfa_dte = 1;
+            fus_dat3.s.nozip = 1;
+            break;
+        case 4: /* CN57XX or CN56XX */
+            fus_dat3.s.nodfa_dte = 1;
+            break;
+        default:
+            break;
+    }
+
+    /* Make a guess at the suffix */
+    /* NSP = everything */
+    /* EXP = No crypto */
+    /* SCP = No DFA, No zip */
+    /* CP = No DFA, No crypto, No zip */
+    if (fus_dat3.s.nodfa_dte)
+    {
+        if (fus_dat2.s.nocrypto)
+            suffix = "CP";
+        else
+            suffix = "SCP";
+    }
+    else if (fus_dat2.s.nocrypto)
+        suffix = "EXP";
+    else
+        suffix = "NSP";
+
+    /* Assume pass number is encoded using <5:3><2:0>. Exceptions will be
+        fixed later */
+    sprintf(pass, "%d.%d", (int)((chip_id>>3)&7)+1, (int)chip_id&7);
+
+    /* Use the number of cores to determine the last 2 digits of the model
+        number. There are some exceptions that are fixed later */
+    switch (num_cores)
+    {
+        case 16: core_model = "60"; break;
+        case 15:
+        case 14: core_model = "55"; break;
+        case 13:
+        case 12: core_model = "50"; break;
+        case 11:
+        case 10: core_model = "45"; break;
+        case  9: core_model = "42"; break;
+        case  8: core_model = "40"; break;
+        case  7:
+        case  6: core_model = "34"; break;
+        case  5: core_model = "32"; break;
+        case  4: core_model = "30"; break;
+        case  3: core_model = "25"; break;
+        case  2: core_model = "20"; break;
+        case  1: core_model = "10"; break;
+        default: core_model = "XX"; break;
+    }
+
+    /* Now figure out the family, the first two digits */
+    switch ((chip_id >> 8) & 0xff)
+    {
+        case 0: /* CN38XX, CN37XX or CN36XX */
+            if (fus3.cn38xx.crip_512k)
+            {
+                /* For some unknown reason, the 16 core one is called 37 instead of 36 */
+                if (num_cores >= 16)
+                    family = "37";
+                else
+                    family = "36";
+            }
+            else
+                family = "38";
+            /* This series of chips didn't follow the standard pass numbering */
+            switch (chip_id & 0xf)
+            {
+                case 0: strcpy(pass, "1.X"); break;
+                case 1: strcpy(pass, "2.X"); break;
+                case 3: strcpy(pass, "3.X"); break;
+                default:strcpy(pass, "X.X"); break;
+            }
+            break;
+        case 1: /* CN31XX or CN3020 */
+            if ((chip_id & 0x10) || fus3.cn31xx.crip_128k)
+                family = "30";
+            else
+                family = "31";
+            /* This series of chips didn't follow the standard pass numbering */
+            switch (chip_id & 0xf)
+            {
+                case 0: strcpy(pass, "1.0"); break;
+                case 2: strcpy(pass, "1.1"); break;
+                default:strcpy(pass, "X.X"); break;
+            }
+            break;
+        case 2: /* CN3010 or CN3005 */
+            family = "30";
+            /* A chip with half cache is an 05 */
+            if (fus3.cn30xx.crip_64k)
+                core_model = "05";
+            /* This series of chips didn't follow the standard pass numbering */
+            switch (chip_id & 0xf)
+            {
+                case 0: strcpy(pass, "1.0"); break;
+                case 2: strcpy(pass, "1.1"); break;
+                default:strcpy(pass, "X.X"); break;
+            }
+            break;
+        case 3: /* CN58XX */
+            family = "58";
+            /* Special case. 4 core, no crypto */
+            if ((num_cores == 4) && fus_dat2.cn38xx.nocrypto)
+                core_model = "29";
+
+            /* Pass 1 uses different encodings for pass numbers */
+            if ((chip_id & 0xFF)< 0x8)
+            {
+                switch (chip_id & 0x3)
+                {
+                    case 0: strcpy(pass, "1.0"); break;
+                    case 1: strcpy(pass, "1.1"); break;
+                    case 3: strcpy(pass, "1.2"); break;
+                    default:strcpy(pass, "1.X"); break;
+                }
+            }
+            break;
+        case 4: /* CN57XX, CN56XX, CN55XX, CN54XX */
+            if (fus_dat2.cn56xx.raid_en)
+            {
+                if (fus3.cn56xx.crip_1024k)
+                    family = "55";
+                else
+                    family = "57";
+                if (fus_dat2.cn56xx.nocrypto)
+                    suffix = "SP";
+                else
+                    suffix = "SSP";
+            }
+            else 
+            {
+                if (fus_dat2.cn56xx.nocrypto)
+                    suffix = "CP";
+                else
+                {
+                    suffix = "NSP";
+                    if (fus_dat3.s.nozip)
+                        suffix = "SCP";
+                }
+                if (fus3.cn56xx.crip_1024k)
+                    family = "54";
+                else
+                    family = "56";
+            }
+            break;
+        case 6: /* CN50XX */
+            family = "50";
+            break;
+        case 7: /* CN52XX */
+            family = "52";
+            break;
+        case 8: /* CN51XX */
+            family = "51";
+            break;
+        default:
+            family = "XX";
+            core_model = "XX";
+            strcpy(pass, "X.X");
+            suffix = "XXX";
+            break;
+    }
+
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+    clock_mhz = octeon_get_clock_rate() / 1000000;
+#else
+    if (cvmx_sysinfo_get())
+        clock_mhz = cvmx_sysinfo_get()->cpu_clock_hz / 1000000;
+    else
+        clock_mhz = 0;
+#endif
+
+    /* Check for model in fuses, overrides normal decode */
+    fuse_data |= cvmx_fuse_read_byte(51);
+    fuse_data = fuse_data << 8;
+    fuse_data |= cvmx_fuse_read_byte(50);
+    fuse_data = fuse_data << 8;
+    fuse_data |= cvmx_fuse_read_byte(49);
+    fuse_data = fuse_data << 8;
+    fuse_data |= cvmx_fuse_read_byte(48);
+    if (fuse_data & 0x7ffff)
+    {
+        int model = fuse_data & 0x3fff;
+        int suffix = (fuse_data >> 14) & 0x1f;
+        if (suffix && model)  /* Have both number and suffix in fuses, so both */
+        {
+            sprintf(fuse_model, "%d%c",model, 'A' + suffix - 1);
+            core_model = "";
+            family = fuse_model;
+        }
+        else if (suffix && !model)   /* Only have suffix, so add suffix to 'normal' model number */
+        {
+            sprintf(fuse_model, "%s%c", core_model, 'A' + suffix - 1);
+            core_model = fuse_model;
+        }
+        else /* Don't have suffix, so just use model from fuses */
+        {
+            sprintf(fuse_model, "%d",model);
+            core_model = "";
+            family = fuse_model;
+        }
+    }
+#ifdef CVMX_BUILD_FOR_UBOOT
+    sprintf(buffer, "CN%s%s-%s pass %s", family, core_model, suffix, pass);
+#else
+    sprintf(buffer, "CN%s%sp%s-%d-%s", family, core_model, pass, clock_mhz, suffix);
+#endif
+    return buffer;
+}
diff --git a/arch/mips/cavium-octeon/executive/octeon-model.h b/arch/mips/cavium-octeon/executive/octeon-model.h
new file mode 100644
index 0000000..afa5b4e
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/octeon-model.h
@@ -0,0 +1,254 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+/**
+ * @file
+ *
+ * File defining different Octeon model IDs and macros to
+ * compare them.
+ *
+ * <hr>$Revision: 32636 $<hr>
+ */
+
+#ifndef __OCTEON_MODEL_H__
+#define __OCTEON_MODEL_H__
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+
+/* NOTE: These must match what is checked in common-config.mk */
+/* Defines to represent the different versions of Octeon.  */
+
+
+/* IMPORTANT: When the default pass is updated for an Octeon Model,
+** the corresponding change must also be made in the oct-sim script. */
+
+
+/* The defines below should be used with the OCTEON_IS_MODEL() macro to
+** determine what model of chip the software is running on.  Models ending
+** in 'XX' match multiple models (families), while specific models match only
+** that model.  If a pass (revision) is specified, then only that revision
+** will be matched.  Care should be taken when checking for both specific
+** models and families that the specific models are checked for first.
+** While these defines are similar to the processor ID, they are not intended
+** to be used by anything other that the OCTEON_IS_MODEL framework, and
+** the values are subject to change at anytime without notice.
+**
+** NOTE: only the OCTEON_IS_MODEL() macro/function and the OCTEON_CN* macros
+** should be used outside of this file.  All other macros are for internal
+** use only, and may change without notice.
+*/
+
+
+/* Flag bits in top byte */
+#define OM_IGNORE_REVISION        0x01000000      /* Ignores revision in model checks */
+#define OM_IGNORE_SUBMODEL        0x02000000      /* Ignores submodels  */
+#define OM_MATCH_PREVIOUS_MODELS  0x04000000      /* Match all models previous than the one specified */
+
+#define OCTEON_CN56XX_PASS1     0x000d0400
+#define OCTEON_CN56XX_PASS1_1   0x000d0401
+#define OCTEON_CN56XX_PASS2     0x000d0408
+#define OCTEON_CN56XX           (OCTEON_CN56XX_PASS1 | OM_IGNORE_REVISION | OM_IGNORE_SUBMODEL)
+
+/* NOTE: Octeon CN57XX, CN55XX, and CN54XX models are not identifiable using the
+    OCTEON_IS_MODEL() functions, but are treated as CN56XX */
+
+#define OCTEON_CN58XX_PASS1     0x000d0300
+#define OCTEON_CN58XX_PASS1_1   0x000d0301
+#define OCTEON_CN58XX_PASS1_2   0x000d0303
+#define OCTEON_CN58XX_PASS2     0x000d0308
+#define OCTEON_CN58XX           (OCTEON_CN58XX_PASS1 | OM_IGNORE_REVISION | OM_IGNORE_SUBMODEL)
+
+#define OCTEON_CN50XX_PASS1     0x000d0600
+#define OCTEON_CN50XX           (OCTEON_CN50XX_PASS1 | OM_IGNORE_REVISION | OM_IGNORE_SUBMODEL)
+
+/* NOTE: Octeon CN5000F model is not identifiable using the OCTEON_IS_MODEL()
+    functions, but are treated as CN50XX */
+
+#define OCTEON_CN52XX_PASS1     0x000d0700
+#define OCTEON_CN52XX           (OCTEON_CN52XX_PASS1 | OM_IGNORE_REVISION | OM_IGNORE_SUBMODEL)
+
+#define OCTEON_CN38XX_PASS1     0x000d0000
+#define OCTEON_CN38XX_PASS2     0x000d0001
+#define OCTEON_CN38XX_PASS3     0x000d0003
+#define OCTEON_CN38XX       	(OCTEON_CN38XX_PASS2 | OM_IGNORE_REVISION | OM_IGNORE_SUBMODEL)
+
+/* NOTE: OCTEON CN36XX models are not identifiable using the OCTEON_IS_MODEL() functions,
+** but are treated as 38XX with a smaller L2 cache.  Setting OCTEON_MODEL to
+** OCTEON_CN36XX will not affect how the program is built (it will be built for OCTEON_CN38XX)
+** but does cause the simulator to properly simulate the smaller L2 cache. */
+
+
+/* The OCTEON_CN31XX matches CN31XX models and the CN3020 */
+#define OCTEON_CN31XX_PASS1    	0x000d0100
+#define OCTEON_CN31XX_PASS1_1  	0x000d0102
+#define OCTEON_CN31XX       	(OCTEON_CN31XX_PASS1 | OM_IGNORE_REVISION | OM_IGNORE_SUBMODEL)
+
+#define OCTEON_CN3005_PASS1    	0x000d0210
+#define OCTEON_CN3005_PASS1_1  	0x000d0212
+#define OCTEON_CN3005       	(OCTEON_CN3005_PASS1 | OM_IGNORE_REVISION)
+
+#define OCTEON_CN3010_PASS1    	0x000d0200
+#define OCTEON_CN3010_PASS1_1  	0x000d0202
+#define OCTEON_CN3010       	(OCTEON_CN3010_PASS1 | OM_IGNORE_REVISION)
+
+#define OCTEON_CN3020_PASS1    	0x000d0110
+#define OCTEON_CN3020_PASS1_1  	0x000d0112
+#define OCTEON_CN3020       	(OCTEON_CN3020_PASS1 | OM_IGNORE_REVISION)
+
+
+/* This model is only used for internal checks, it
+** is not valid model for the OCTEON_MODEL environment variable.
+** This matches the CN3010 and CN3005 but NOT the CN3020*/
+#define OCTEON_CN30XX       	(OCTEON_CN3010_PASS1   | OM_IGNORE_REVISION | OM_IGNORE_SUBMODEL)
+#define OCTEON_CN30XX_PASS1   	(OCTEON_CN3010_PASS1   | OM_IGNORE_SUBMODEL)
+#define OCTEON_CN30XX_PASS1_1  	(OCTEON_CN3010_PASS1_1 | OM_IGNORE_SUBMODEL)
+
+/* This matches the complete family of CN3xxx CPUs, and not subsequent models */
+#define OCTEON_CN3XXX           (OCTEON_CN58XX_PASS1 | OM_MATCH_PREVIOUS_MODELS | OM_IGNORE_REVISION | OM_IGNORE_SUBMODEL )
+
+/* The revision byte (low byte) has two different encodings.
+** CN3XXX:
+** 
+**     bits
+**     <7:5>: reserved (0)
+**     <4>:   alternate package
+**     <3:0>: revision
+**     
+** CN5XXX:
+** 
+**     bits
+**     <7>:   reserved (0)
+**     <6>:   alternate package
+**     <5:3>: major revision
+**     <2:0>: minor revision
+** 
+*/ 
+
+/* Masks used for the various types of model/family/revision matching */
+#define OCTEON_38XX_FAMILY_MASK      0x00ffff00
+#define OCTEON_38XX_FAMILY_REV_MASK  0x00ffff0f
+#define OCTEON_38XX_MODEL_MASK       0x00ffff10
+#define OCTEON_38XX_MODEL_REV_MASK   (OCTEON_38XX_FAMILY_REV_MASK | OCTEON_38XX_MODEL_MASK)
+
+/* CN5XXX and use different layout of bits in the revision ID field */
+#define OCTEON_58XX_FAMILY_MASK      OCTEON_38XX_FAMILY_MASK
+#define OCTEON_58XX_FAMILY_REV_MASK  0x00ffff3f
+#define OCTEON_58XX_MODEL_MASK       0x00ffffc0
+#define OCTEON_58XX_MODEL_REV_MASK   (OCTEON_58XX_FAMILY_REV_MASK | OCTEON_58XX_MODEL_MASK)
+
+
+#define __OCTEON_MATCH_MASK__(x,y,z) (((x) & (z)) == ((y) & (z)))
+
+
+/* NOTE: This for internal use only!!!!! */
+#define __OCTEON_IS_MODEL_COMPILE__(arg_model, chip_model) \
+    ((((arg_model & OCTEON_38XX_FAMILY_MASK) <= OCTEON_CN3010_PASS1)  && (\
+     ((((arg_model) & (OM_MATCH_PREVIOUS_MODELS | OM_IGNORE_REVISION | OM_IGNORE_SUBMODEL)) == OM_IGNORE_REVISION) && __OCTEON_MATCH_MASK__((chip_model), (arg_model), OCTEON_38XX_MODEL_MASK)) || \
+     ((((arg_model) & (OM_MATCH_PREVIOUS_MODELS | OM_IGNORE_REVISION | OM_IGNORE_SUBMODEL)) == OM_IGNORE_SUBMODEL) && __OCTEON_MATCH_MASK__((chip_model), (arg_model), OCTEON_38XX_FAMILY_REV_MASK)) || \
+     ((((arg_model) & (OM_MATCH_PREVIOUS_MODELS | OM_IGNORE_REVISION | OM_IGNORE_SUBMODEL)) == (OM_IGNORE_REVISION | OM_IGNORE_SUBMODEL)) && __OCTEON_MATCH_MASK__((chip_model), (arg_model), OCTEON_38XX_FAMILY_MASK)) || \
+     ((((arg_model) & (OM_MATCH_PREVIOUS_MODELS | OM_IGNORE_REVISION | OM_IGNORE_SUBMODEL)) == 0) && __OCTEON_MATCH_MASK__((chip_model), (arg_model), OCTEON_38XX_MODEL_REV_MASK)) || \
+     ((((arg_model) & (OM_MATCH_PREVIOUS_MODELS)) == OM_MATCH_PREVIOUS_MODELS) && (((chip_model) & OCTEON_38XX_MODEL_MASK) < ((arg_model) & OCTEON_38XX_MODEL_MASK))) \
+    )) || \
+    (((arg_model & OCTEON_38XX_FAMILY_MASK) > OCTEON_CN3010_PASS1)  && (\
+     ((((arg_model) & (OM_MATCH_PREVIOUS_MODELS | OM_IGNORE_REVISION | OM_IGNORE_SUBMODEL)) == OM_IGNORE_REVISION) && __OCTEON_MATCH_MASK__((chip_model), (arg_model), OCTEON_58XX_MODEL_MASK)) || \
+     ((((arg_model) & (OM_MATCH_PREVIOUS_MODELS | OM_IGNORE_REVISION | OM_IGNORE_SUBMODEL)) == OM_IGNORE_SUBMODEL) && __OCTEON_MATCH_MASK__((chip_model), (arg_model), OCTEON_58XX_FAMILY_REV_MASK)) || \
+     ((((arg_model) & (OM_MATCH_PREVIOUS_MODELS | OM_IGNORE_REVISION | OM_IGNORE_SUBMODEL)) == (OM_IGNORE_REVISION | OM_IGNORE_SUBMODEL)) && __OCTEON_MATCH_MASK__((chip_model), (arg_model), OCTEON_58XX_FAMILY_MASK)) || \
+     ((((arg_model) & (OM_MATCH_PREVIOUS_MODELS | OM_IGNORE_REVISION | OM_IGNORE_SUBMODEL)) == 0) && __OCTEON_MATCH_MASK__((chip_model), (arg_model), OCTEON_58XX_MODEL_REV_MASK)) || \
+     ((((arg_model) & (OM_MATCH_PREVIOUS_MODELS)) == OM_MATCH_PREVIOUS_MODELS) && (((chip_model) & OCTEON_58XX_MODEL_MASK) < ((arg_model) & OCTEON_58XX_MODEL_MASK))) \
+    )))
+
+
+#if defined(USE_RUNTIME_MODEL_CHECKS) || defined(__U_BOOT__) || (defined(__linux__) && defined(__KERNEL__))
+/* forward declarations */
+static inline uint32_t cvmx_get_proc_id(void) __attribute__ ((pure));
+static inline uint64_t cvmx_read_csr(uint64_t csr_addr);
+
+/* NOTE: This for internal use only!!!!! */
+static inline int __octeon_is_model_runtime__(uint32_t model)
+{
+    uint32_t cpuid = cvmx_get_proc_id();
+
+    /* Check for special case of mismarked 3005 samples. We only need to check
+        if the sub model isn't being ignored */
+    if ((model & OM_IGNORE_SUBMODEL) == 0)
+    {
+        if (cpuid == OCTEON_CN3010_PASS1 && (cvmx_read_csr(0x80011800800007B8ull) & (1ull << 34)))
+            cpuid |= 0x10;
+    }
+    return(__OCTEON_IS_MODEL_COMPILE__(model, cpuid));
+
+}
+
+/* The OCTEON_IS_MODEL macro should be used for all Octeon model checking done in a program.
+** This should be kept runtime if at all possible.  Any compile time (#if OCTEON_IS_MODEL) usage
+** must be condtionalized with OCTEON_IS_COMMON_BINARY() if runtime checking support is required.
+**
+*/
+#define OCTEON_IS_MODEL(x) __octeon_is_model_runtime__(x)
+#define OCTEON_IS_COMMON_BINARY() 1
+#undef OCTEON_MODEL
+#else
+#define OCTEON_IS_MODEL(x) __OCTEON_IS_MODEL_COMPILE__(x, OCTEON_MODEL)
+#define OCTEON_IS_COMMON_BINARY() 0
+#endif
+
+const char *octeon_model_get_string(uint32_t chip_id);
+const char *octeon_model_get_string_buffer(uint32_t chip_id, char * buffer);
+
+#include "octeon-feature.h"
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif    /* __OCTEON_MODEL_H__ */
diff --git a/arch/mips/cavium-octeon/executive/octeon-pci-console.c b/arch/mips/cavium-octeon/executive/octeon-pci-console.c
new file mode 100644
index 0000000..27e7ad7
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/octeon-pci-console.c
@@ -0,0 +1,488 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+#define CVMX_USE_1_TO_1_TLB_MAPPINGS 0
+
+#include "cvmx-platform.h"
+
+#include "cvmx.h"
+#include "cvmx-spinlock.h"
+#define	MIN(a,b) (((a)<(b))?(a):(b))
+
+#if !defined(CVMX_BUILD_FOR_LINUX_KERNEL)
+#include "cvmx-bootmem.h"
+#endif
+
+#include "octeon-pci-console.h"
+
+#if defined(__linux__) && !defined(__KERNEL__) && !defined(OCTEON_TARGET)
+#include "octeon-pci.h"
+#endif
+
+
+/* The following code is only used in standalone CVMX applications. It does
+    not apply for kernel or Linux programming */
+#if defined(OCTEON_TARGET) && !defined(__linux__)
+
+static int cvmx_pci_console_num = 0;
+static int per_core_pci_consoles = 0;
+static uint64_t pci_console_desc_addr = 0;
+/* This function for simple executive internal use only - do not use in any application */
+int  __cvmx_pci_console_write (int fd, char *buf, int nbytes)
+{
+    int console_num;
+    if (fd >= 0x10000000)
+    {
+        console_num = fd & 0xFFFF;
+    }
+    else if (per_core_pci_consoles)
+    {
+        console_num = cvmx_get_core_num();
+    }
+    else
+        console_num = cvmx_pci_console_num;
+
+    if (!pci_console_desc_addr)
+    {
+        cvmx_bootmem_named_block_desc_t *block_desc = cvmx_bootmem_find_named_block(OCTEON_PCI_CONSOLE_BLOCK_NAME);
+        pci_console_desc_addr = block_desc->base_addr;
+    }
+
+
+    return octeon_pci_console_write(pci_console_desc_addr, console_num, buf, nbytes, 0);
+
+}
+
+#endif
+
+
+#if !defined(CONFIG_OCTEON_U_BOOT) || (defined(CONFIG_OCTEON_U_BOOT) && defined(CFG_PCI_CONSOLE))
+int octeon_pci_console_buffer_free_bytes(uint32_t buffer_size, uint32_t wr_idx, uint32_t rd_idx)
+{
+    if (rd_idx >= buffer_size || wr_idx >= buffer_size)
+        return -1;
+
+    return (((buffer_size -1) - (wr_idx - rd_idx))%buffer_size);
+}
+int octeon_pci_console_buffer_avail_bytes(uint32_t buffer_size, uint32_t wr_idx, uint32_t rd_idx)
+{
+    if (rd_idx >= buffer_size || wr_idx >= buffer_size)
+        return -1;
+
+    return (buffer_size - 1 - octeon_pci_console_buffer_free_bytes(buffer_size, wr_idx, rd_idx));
+}
+#endif
+
+
+
+/* The following code is only used under Linux userspace when you are using
+    CVMX */
+#if defined(__linux__) && !defined(__KERNEL__) && !defined(OCTEON_TARGET)
+int octeon_pci_console_host_write(uint64_t console_desc_addr, unsigned int console_num, const char * buffer, int write_reqest_size, uint32_t flags)
+{
+    if (!console_desc_addr)
+        return -1;
+
+    /* Get global pci console information and look up specific console structure. */
+    uint32_t num_consoles = octeon_read_mem32(console_desc_addr + offsetof(octeon_pci_console_desc_t, num_consoles));
+//    printf("Num consoles: %d, buf size: %d\n", num_consoles, console_buffer_size);
+    if (console_num >= num_consoles)
+    {
+        printf("ERROR: attempting to read non-existant console: %d\n", console_num);
+        return(-1);
+    }
+    uint64_t console_addr = octeon_read_mem64(console_desc_addr + offsetof(octeon_pci_console_desc_t, console_addr_array) + console_num *8);
+//    printf("Console %d is at 0x%llx\n", console_num, (long long)console_addr);
+
+    uint32_t console_buffer_size = octeon_read_mem32(console_addr + offsetof(octeon_pci_console_t, buf_size));
+    /* Check to see if any data is available */
+    uint32_t rd_idx, wr_idx;
+    uint64_t base_addr;
+
+    base_addr = octeon_read_mem64(console_addr + offsetof(octeon_pci_console_t, input_base_addr));
+    rd_idx = octeon_read_mem32(console_addr + offsetof(octeon_pci_console_t, input_read_index));
+    wr_idx = octeon_read_mem32(console_addr + offsetof(octeon_pci_console_t, input_write_index));
+
+//    printf("Input base: 0x%llx, rd: %d(0x%x), wr: %d(0x%x)\n", (long long)base_addr, rd_idx, rd_idx, wr_idx, wr_idx);
+    int bytes_to_write = octeon_pci_console_buffer_free_bytes(console_buffer_size, wr_idx, rd_idx);
+    if (bytes_to_write <= 0)
+        return bytes_to_write;
+    bytes_to_write = MIN(bytes_to_write, write_reqest_size);
+    /* Check to see if what we want to write is not contiguous, and limit ourselves to the contiguous block*/
+    if (wr_idx + bytes_to_write >= console_buffer_size)
+        bytes_to_write = console_buffer_size - wr_idx;
+
+//    printf("Attempting to write %d bytes, (buf size: %d)\n", bytes_to_write, write_reqest_size);
+
+    octeon_pci_write_mem(base_addr + wr_idx, buffer, bytes_to_write, OCTEON_PCI_ENDIAN_64BIT_SWAP);
+    octeon_write_mem32(console_addr + offsetof(octeon_pci_console_t, input_write_index), (wr_idx + bytes_to_write)%console_buffer_size);
+
+    return bytes_to_write;
+
+}
+
+int octeon_pci_console_host_read(uint64_t console_desc_addr, unsigned int console_num, char * buffer, int buf_size, uint32_t flags)
+{
+    if (!console_desc_addr)
+        return -1;
+
+    /* Get global pci console information and look up specific console structure. */
+    uint32_t num_consoles = octeon_read_mem32(console_desc_addr + offsetof(octeon_pci_console_desc_t, num_consoles));
+//    printf("Num consoles: %d, buf size: %d\n", num_consoles, console_buffer_size);
+    if (console_num >= num_consoles)
+    {
+        printf("ERROR: attempting to read non-existant console: %d\n", console_num);
+        return(-1);
+    }
+    uint64_t console_addr = octeon_read_mem64(console_desc_addr + offsetof(octeon_pci_console_desc_t, console_addr_array) + console_num *8);
+    uint32_t console_buffer_size = octeon_read_mem32(console_addr + offsetof(octeon_pci_console_t, buf_size));
+//    printf("Console %d is at 0x%llx\n", console_num, (long long)console_addr);
+
+    /* Check to see if any data is available */
+    uint32_t rd_idx, wr_idx;
+    uint64_t base_addr;
+
+    base_addr = octeon_read_mem64(console_addr + offsetof(octeon_pci_console_t, output_base_addr));
+    rd_idx = octeon_read_mem32(console_addr + offsetof(octeon_pci_console_t, output_read_index));
+    wr_idx = octeon_read_mem32(console_addr + offsetof(octeon_pci_console_t, output_write_index));
+
+//    printf("Read buffer base: 0x%llx, rd: %d(0x%x), wr: %d(0x%x)\n", (long long)base_addr, rd_idx, rd_idx, wr_idx, wr_idx);
+    int bytes_to_read = octeon_pci_console_buffer_avail_bytes(console_buffer_size, wr_idx, rd_idx);
+    if (bytes_to_read <= 0)
+        return bytes_to_read;
+
+
+    bytes_to_read = MIN(bytes_to_read, buf_size);
+    /* Check to see if what we want to read is not contiguous, and limit ourselves to the contiguous block*/
+    if (rd_idx + bytes_to_read >= console_buffer_size)
+        bytes_to_read = console_buffer_size - rd_idx;
+
+
+    octeon_pci_read_mem(buffer, base_addr + rd_idx, bytes_to_read,OCTEON_PCI_ENDIAN_64BIT_SWAP);
+    octeon_write_mem32(console_addr + offsetof(octeon_pci_console_t, output_read_index), (rd_idx + bytes_to_read)%console_buffer_size);
+
+    return bytes_to_read;
+}
+
+
+int octeon_pci_console_host_write_avail(uint64_t console_desc_addr, unsigned int console_num)
+{
+    if (!console_desc_addr)
+        return -1;
+
+    /* Get global pci console information and look up specific console structure. */
+    uint32_t num_consoles = octeon_read_mem32(console_desc_addr + offsetof(octeon_pci_console_desc_t, num_consoles));
+//    printf("Num consoles: %d, buf size: %d\n", num_consoles, console_buffer_size);
+    if (console_num >= num_consoles)
+    {
+        printf("ERROR: attempting to read non-existant console: %d\n", console_num);
+        return -1;
+    }
+    uint64_t console_addr = octeon_read_mem64(console_desc_addr + offsetof(octeon_pci_console_desc_t, console_addr_array) + console_num *8);
+//    printf("Console %d is at 0x%llx\n", console_num, (long long)console_addr);
+
+    uint32_t console_buffer_size = octeon_read_mem32(console_addr + offsetof(octeon_pci_console_t, buf_size));
+    /* Check to see if any data is available */
+    uint32_t rd_idx, wr_idx;
+    uint64_t base_addr;
+
+    base_addr = octeon_read_mem64(console_addr + offsetof(octeon_pci_console_t, input_base_addr));
+    rd_idx = octeon_read_mem32(console_addr + offsetof(octeon_pci_console_t, input_read_index));
+    wr_idx = octeon_read_mem32(console_addr + offsetof(octeon_pci_console_t, input_write_index));
+
+//    printf("Input base: 0x%llx, rd: %d(0x%x), wr: %d(0x%x)\n", (long long)base_addr, rd_idx, rd_idx, wr_idx, wr_idx);
+    return octeon_pci_console_buffer_free_bytes(console_buffer_size, wr_idx, rd_idx);
+}
+
+
+int octeon_pci_console_host_read_avail(uint64_t console_desc_addr, unsigned int console_num)
+{
+    if (!console_desc_addr)
+        return -1;
+
+    /* Get global pci console information and look up specific console structure. */
+    uint32_t num_consoles = octeon_read_mem32(console_desc_addr + offsetof(octeon_pci_console_desc_t, num_consoles));
+//    printf("Num consoles: %d, buf size: %d\n", num_consoles, console_buffer_size);
+    if (console_num >= num_consoles)
+    {
+        printf("ERROR: attempting to read non-existant console: %d\n", console_num);
+        return(-1);
+    }
+    uint64_t console_addr = octeon_read_mem64(console_desc_addr + offsetof(octeon_pci_console_desc_t, console_addr_array) + console_num *8);
+    uint32_t console_buffer_size = octeon_read_mem32(console_addr + offsetof(octeon_pci_console_t, buf_size));
+//    printf("Console %d is at 0x%llx\n", console_num, (long long)console_addr);
+
+    /* Check to see if any data is available */
+    uint32_t rd_idx, wr_idx;
+    uint64_t base_addr;
+
+    base_addr = octeon_read_mem64(console_addr + offsetof(octeon_pci_console_t, output_base_addr));
+    rd_idx = octeon_read_mem32(console_addr + offsetof(octeon_pci_console_t, output_read_index));
+    wr_idx = octeon_read_mem32(console_addr + offsetof(octeon_pci_console_t, output_write_index));
+
+//    printf("Read buffer base: 0x%llx, rd: %d(0x%x), wr: %d(0x%x)\n", (long long)base_addr, rd_idx, rd_idx, wr_idx, wr_idx);
+    return octeon_pci_console_buffer_avail_bytes(console_buffer_size, wr_idx, rd_idx);
+}
+
+
+#endif /* TARGET_HOST */
+
+
+
+
+
+
+/* This code is only available in a kernel or CVMX standalone. It can't be used
+    from userspace */
+#if (!defined(CONFIG_OCTEON_U_BOOT) && (!defined(__linux__) || defined(__KERNEL__))) || (defined(CONFIG_OCTEON_U_BOOT) && defined(CFG_PCI_CONSOLE))
+
+static octeon_pci_console_t *octeon_pci_console_get_ptr(uint64_t console_desc_addr, unsigned int console_num)
+{
+    octeon_pci_console_desc_t *cons_desc_ptr;
+
+    if (!console_desc_addr)
+        return NULL;
+
+    cons_desc_ptr = (octeon_pci_console_desc_t *)cvmx_phys_to_ptr(console_desc_addr);
+    if (console_num >= cons_desc_ptr->num_consoles)
+        return NULL;
+
+    return (octeon_pci_console_t *)cvmx_phys_to_ptr(cons_desc_ptr->console_addr_array[console_num]);
+}
+
+
+int octeon_pci_console_write(uint64_t console_desc_addr, unsigned int console_num, const char * buffer, int bytes_to_write, uint32_t flags)
+{
+    octeon_pci_console_t *cons_ptr;
+    cvmx_spinlock_t *lock;
+    int bytes_available;
+    char *buf_ptr;
+    int bytes_written;
+
+    cons_ptr = octeon_pci_console_get_ptr(console_desc_addr, console_num);
+    if (!cons_ptr)
+        return -1;
+
+    lock = (cvmx_spinlock_t *)&cons_ptr->lock;
+
+    buf_ptr = (char*)cvmx_phys_to_ptr(cons_ptr->output_base_addr);
+    bytes_written = 0;
+    cvmx_spinlock_lock(lock);
+    while (bytes_to_write > 0)
+    {
+        bytes_available = octeon_pci_console_buffer_free_bytes(cons_ptr->buf_size, cons_ptr->output_write_index, cons_ptr->output_read_index);
+//        printf("Console %d has %d bytes available for writes\n", console_num, bytes_available);
+        if (bytes_available > 0)
+        {
+            int write_size = MIN(bytes_available, bytes_to_write);
+            /* Limit ourselves to what we can output in a contiguous block */
+            if (cons_ptr->output_write_index + write_size >= cons_ptr->buf_size)
+                write_size = cons_ptr->buf_size - cons_ptr->output_write_index;
+
+            memcpy(buf_ptr + cons_ptr->output_write_index, buffer + bytes_written, write_size);
+            CVMX_SYNCW;  /* Make sure data is visible before changing write index */
+            cons_ptr->output_write_index = (cons_ptr->output_write_index + write_size)%cons_ptr->buf_size;
+            bytes_to_write -= write_size;
+            bytes_written += write_size;
+        }
+        else if (bytes_available == 0)
+        {
+            /* Check to see if we should wait for room, or return after a partial write */
+            if (flags & OCT_PCI_CON_FLAG_NONBLOCK)
+                goto done;
+
+            cvmx_wait(1000000);  /* Delay if we are spinning */
+        }
+        else
+        {
+            bytes_written = -1;
+            goto done;
+        }
+    }
+
+done:
+    cvmx_spinlock_unlock(lock);
+    return(bytes_written);
+}
+
+int octeon_pci_console_read(uint64_t console_desc_addr, unsigned int console_num, char * buffer, int buffer_size, uint32_t flags)
+{
+    int bytes_available;
+    char *buf_ptr;
+    cvmx_spinlock_t *lock;
+    int bytes_read;
+    int read_size;
+    octeon_pci_console_t *cons_ptr = octeon_pci_console_get_ptr(console_desc_addr, console_num);
+    if (!cons_ptr)
+        return -1;
+
+    buf_ptr = (char*)cvmx_phys_to_ptr(cons_ptr->input_base_addr);
+
+    bytes_available = octeon_pci_console_buffer_avail_bytes(cons_ptr->buf_size, cons_ptr->input_write_index, cons_ptr->input_read_index);
+    if (bytes_available < 0)
+        return bytes_available;
+
+    lock = (cvmx_spinlock_t *)&cons_ptr->lock;
+    cvmx_spinlock_lock(lock);
+
+    if (!(flags & OCT_PCI_CON_FLAG_NONBLOCK))
+    {
+        /* Wait for some data to be available */
+        while (0 == (bytes_available = octeon_pci_console_buffer_avail_bytes(cons_ptr->buf_size, cons_ptr->input_write_index, cons_ptr->input_read_index)))
+            cvmx_wait(1000000);
+    }
+
+    bytes_read = 0;
+//        printf("Console %d has %d bytes available for writes\n", console_num, bytes_available);
+
+    /* Don't overflow the buffer passed to us */
+    read_size = MIN(bytes_available, buffer_size);
+
+    /* Limit ourselves to what we can input in a contiguous block */
+    if (cons_ptr->input_read_index + read_size >= cons_ptr->buf_size)
+        read_size = cons_ptr->buf_size - cons_ptr->input_read_index;
+
+    memcpy(buffer, buf_ptr + cons_ptr->input_read_index, read_size);
+    cons_ptr->input_read_index = (cons_ptr->input_read_index + read_size)%cons_ptr->buf_size;
+    bytes_read += read_size;
+
+    cvmx_spinlock_unlock(lock);
+    return(bytes_read);
+}
+
+
+int octeon_pci_console_write_avail(uint64_t console_desc_addr, unsigned int console_num)
+{
+    int bytes_available;
+    octeon_pci_console_t *cons_ptr = octeon_pci_console_get_ptr(console_desc_addr, console_num);
+    if (!cons_ptr)
+        return -1;
+
+    bytes_available = octeon_pci_console_buffer_free_bytes(cons_ptr->buf_size, cons_ptr->input_write_index, cons_ptr->input_read_index);
+    if (bytes_available >= 0)
+        return(bytes_available);
+    else
+        return 0;
+}
+
+
+int octeon_pci_console_read_avail(uint64_t console_desc_addr, unsigned int console_num)
+{
+    int bytes_available;
+    octeon_pci_console_t *cons_ptr = octeon_pci_console_get_ptr(console_desc_addr, console_num);
+    if (!cons_ptr)
+        return -1;
+
+    bytes_available = octeon_pci_console_buffer_avail_bytes(cons_ptr->buf_size, cons_ptr->input_write_index, cons_ptr->input_read_index);
+    if (bytes_available >= 0)
+        return(bytes_available);
+    else
+        return 0;
+}
+
+#endif
+
+
+/* This code can only be used in the bootloader */
+#if defined(CONFIG_OCTEON_U_BOOT) && defined(CFG_PCI_CONSOLE)
+#define DDR0_TOP        0x10000000
+#define DDR2_BASE       0x20000000
+uint64_t  octeon_pci_console_init(int num_consoles, int buffer_size)
+{
+    octeon_pci_console_desc_t *cons_desc_ptr;
+    octeon_pci_console_t *cons_ptr;
+
+    /* Compute size required for pci console structure */
+    int alloc_size = num_consoles * (buffer_size * 2 + sizeof(octeon_pci_console_t) + sizeof(uint64_t)) + sizeof(octeon_pci_console_desc_t);
+
+    /* Allocate memory for the consoles.  This must be in the range addresssible by the bootloader.
+    ** Try to do so in a manner which minimizes fragmentation.  We try to put it at the top of DDR0 or bottom of
+    ** DDR2 first, and only do generic allocation if those fail */
+    int64_t console_block_addr = cvmx_bootmem_phy_named_block_alloc(alloc_size, DDR0_TOP - alloc_size - 128, DDR0_TOP, 128, OCTEON_PCI_CONSOLE_BLOCK_NAME, CVMX_BOOTMEM_FLAG_END_ALLOC);
+    if (console_block_addr < 0)
+        console_block_addr = cvmx_bootmem_phy_named_block_alloc(alloc_size, DDR2_BASE + 1, DDR2_BASE + alloc_size + 128, 128, OCTEON_PCI_CONSOLE_BLOCK_NAME, CVMX_BOOTMEM_FLAG_END_ALLOC);
+    if (console_block_addr < 0)
+        console_block_addr = cvmx_bootmem_phy_named_block_alloc(alloc_size, 0, 0x7fffffff, 128, OCTEON_PCI_CONSOLE_BLOCK_NAME, CVMX_BOOTMEM_FLAG_END_ALLOC);
+    if (console_block_addr < 0)
+        return 0;
+
+    cons_desc_ptr = (void *)(uint32_t)console_block_addr;
+
+    memset(cons_desc_ptr, 0, alloc_size);  /* Clear entire alloc'ed memory */
+
+    cons_desc_ptr->lock = 1; /* initialize as locked until we are done */
+    CVMX_SYNCW;
+    cons_desc_ptr->num_consoles = num_consoles;
+    cons_desc_ptr->flags = 0;
+    cons_desc_ptr->major_version = OCTEON_PCI_CONSOLE_MAJOR_VERSION;
+    cons_desc_ptr->minor_version = OCTEON_PCI_CONSOLE_MINOR_VERSION;
+
+    int i;
+    uint64_t avail_addr = console_block_addr + sizeof(octeon_pci_console_desc_t) + num_consoles * sizeof(uint64_t);
+    for (i = 0; i < num_consoles;i++)
+    {
+        cons_desc_ptr->console_addr_array[i] = avail_addr;
+        cons_ptr = (void *)(uint32_t)cons_desc_ptr->console_addr_array[i];
+        avail_addr += sizeof(octeon_pci_console_t);
+        cons_ptr->input_base_addr = avail_addr;
+        avail_addr += buffer_size;
+        cons_ptr->output_base_addr = avail_addr;
+        avail_addr += buffer_size;
+        cons_ptr->buf_size = buffer_size;
+    }
+    CVMX_SYNCW;
+    cons_desc_ptr->lock = 0;
+
+    return console_block_addr;
+
+
+}
+#endif
diff --git a/arch/mips/cavium-octeon/executive/octeon-pci-console.h b/arch/mips/cavium-octeon/executive/octeon-pci-console.h
new file mode 100644
index 0000000..d59cf79
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/octeon-pci-console.h
@@ -0,0 +1,146 @@
+/***********************license start***************
+ * Copyright (c) 2003-2008  Cavium Networks (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *
+ *     * Neither the name of Cavium Networks nor the names of
+ *       its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written
+ *       permission.
+ *
+ * This Software, including technical data, may be subject to U.S.  export
+ * control laws, including the U.S.  Export Administration Act and its
+ * associated regulations, and may be subject to export or import regulations
+ * in other countries.  You warrant that You will comply strictly in all
+ * respects with all such regulations and acknowledge that you have the
+ * responsibility to obtain licenses to export, re-export or import the
+ * Software.
+ *
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+ * OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+ * RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+ * REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+ * DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+ * OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+ * PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+ * POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+ * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ *
+ *
+ * For any questions regarding licensing please contact marketing@caviumnetworks.com
+ *
+ ***********************license end**************************************/
+
+
+
+
+
+
+
+#ifndef __OCTEON_PCI_CONSOLE_H__
+#define __OCTEON_PCI_CONSOLE_H__
+
+#include "cvmx-platform.h"
+
+/* Current versions */
+#define OCTEON_PCI_CONSOLE_MAJOR_VERSION    1
+#define OCTEON_PCI_CONSOLE_MINOR_VERSION    0
+
+#define OCTEON_PCI_CONSOLE_BLOCK_NAME   "__pci_console"
+
+
+/* Structure that defines a single console.
+
+
+* Note: when read_index == write_index, the buffer is empty.  The actual usable size
+*       of each console is console_buf_size -1;
+*/
+typedef struct {
+    uint64_t input_base_addr;
+    uint32_t input_read_index;
+    uint32_t input_write_index;
+    uint64_t output_base_addr;
+    uint32_t output_read_index;
+    uint32_t output_write_index;
+    uint32_t lock;
+    uint32_t buf_size;
+} octeon_pci_console_t;
+
+
+/* This is the main container structure that contains all the information
+about all PCI consoles.  The address of this structure is passed to various
+routines that operation on PCI consoles.
+*/
+typedef struct {
+    uint32_t major_version;
+    uint32_t minor_version;
+    uint32_t lock;
+    uint32_t flags;
+    uint32_t num_consoles;
+    uint32_t pad;
+    /* must be 64 bit aligned here... */
+    uint64_t console_addr_array[0];  /* Array of addresses of octeon_pci_console_t structures */
+    /* Implicit storage for console_addr_array */
+} octeon_pci_console_desc_t;
+
+
+/* Flag definitions for octeon_pci_console_desc_t */
+enum {
+    OCT_PCI_CON_DESC_FLAG_PERCPU = 1 << 0,  /* If set, output from core N will be sent to console N */
+};
+
+#if defined(OCTEON_TARGET) && !defined(__linux__)
+/**
+ * This is an internal-only function that is called from within the simple executive
+ * C library, and is not intended for any other use.
+ *
+ * @param fd
+ * @param buf
+ * @param nbytes
+ *
+ * @return
+ */
+int  __cvmx_pci_console_write (int fd, char *buf, int nbytes);
+#endif
+
+
+#ifdef CVMX_BUILD_FOR_UBOOT
+uint64_t octeon_pci_console_init(int num_consoles, int buffer_size);
+#endif
+
+/* Flag definitions for read/write functions */
+enum {
+    OCT_PCI_CON_FLAG_NONBLOCK = 1 << 0,  /* If set, read/write functions won't block waiting for space or data.
+                                          * For reads, 0 bytes may be read, and for writes not all of the
+                                          * supplied data may be written.*/
+};
+
+#if !defined(__linux__) || defined(__KERNEL__)
+int octeon_pci_console_write(uint64_t console_desc_addr, unsigned int console_num, const char * buffer, int bytes_to_write, uint32_t flags);
+int octeon_pci_console_write_avail(uint64_t console_desc_addr, unsigned int console_num);
+
+int octeon_pci_console_read(uint64_t console_desc_addr, unsigned int console_num, char * buffer, int buffer_size, uint32_t flags);
+int octeon_pci_console_read_avail(uint64_t console_desc_addr, unsigned int console_num);
+#endif
+
+#if !defined(OCTEON_TARGET) && defined(__linux__) && !defined(__KERNEL__)
+int octeon_pci_console_host_write(uint64_t console_desc_addr, unsigned int console_num, const char * buffer, int write_reqest_size, uint32_t flags);
+int octeon_pci_console_host_write_avail(uint64_t console_desc_addr, unsigned int console_num);
+
+int octeon_pci_console_host_read(uint64_t console_desc_addr, unsigned int console_num, char * buffer, int buf_size, uint32_t flags);
+int octeon_pci_console_host_read_avail(uint64_t console_desc_addr, unsigned int console_num);
+#endif
+#endif
-- 
1.5.5.1

