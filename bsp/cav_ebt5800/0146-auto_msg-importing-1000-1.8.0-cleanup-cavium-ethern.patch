From ae8cde48685982c69f5698513e13bf92f9e81f49 Mon Sep 17 00:00:00 2001
From: auto commit <unknown@unknown>
Date: Fri, 24 Oct 2008 12:23:11 -0700
Subject: [PATCH] auto_msg: importing 1000-1.8.0-cleanup-cavium-ethernet.patch

This is an automatic import of patch 1000-1.8.0-cleanup-cavium-ethernet.patch, no headers were
detected and a default message was constructed
---
 drivers/net/cavium-ethernet/cavium-ethernet.h  |   64 +-
 drivers/net/cavium-ethernet/ethernet-common.c  |  337 +++++-----
 drivers/net/cavium-ethernet/ethernet-defines.h |   24 +-
 drivers/net/cavium-ethernet/ethernet-mdio.c    |  157 ++---
 drivers/net/cavium-ethernet/ethernet-mem.c     |  161 ++---
 drivers/net/cavium-ethernet/ethernet-proc.c    |  291 ++++----
 drivers/net/cavium-ethernet/ethernet-rgmii.c   |  489 +++++++-------
 drivers/net/cavium-ethernet/ethernet-rx.c      |  665 +++++++++---------
 drivers/net/cavium-ethernet/ethernet-sgmii.c   |  108 ++--
 drivers/net/cavium-ethernet/ethernet-spi.c     |  373 ++++++-----
 drivers/net/cavium-ethernet/ethernet-tx.c      |  858 ++++++++++++------------
 drivers/net/cavium-ethernet/ethernet-util.h    |   34 +-
 drivers/net/cavium-ethernet/ethernet-xaui.c    |    4 +-
 drivers/net/cavium-ethernet/ethernet.c         |  864 ++++++++++++------------
 14 files changed, 2201 insertions(+), 2228 deletions(-)

diff --git a/drivers/net/cavium-ethernet/cavium-ethernet.h b/drivers/net/cavium-ethernet/cavium-ethernet.h
index e236f03..9e81fc4 100644
--- a/drivers/net/cavium-ethernet/cavium-ethernet.h
+++ b/drivers/net/cavium-ethernet/cavium-ethernet.h
@@ -58,32 +58,31 @@
  * the ethernet driver will continue processing in different
  * ways.
  */
-typedef enum
-{
-    CVM_OCT_PASS,               /**< The ethernet driver will pass the packet
-                                    to the kernel, just as if the intercept
-                                    callback didn't exist */
-    CVM_OCT_DROP,               /**< The ethernet driver will drop the packet,
-                                    cleaning of the work queue entry and the
-                                    skbuff */
-    CVM_OCT_TAKE_OWNERSHIP_WORK,/**< The intercept callback takes over
-                                    ownership of the work queue entry. It is
-                                    the responsibility of the callback to free
-                                    the work queue entry and all associated
-                                    packet buffers. The ethernet driver will
-                                    dispose of the skbuff without affecting the
-                                    work queue entry */
-    CVM_OCT_TAKE_OWNERSHIP_SKB  /**< The intercept callback takes over
-                                    ownership of the skbuff. The work queue
-                                    entry and packet buffer will be disposed of
-                                    in a way keeping the skbuff valid */
+typedef enum {
+	CVM_OCT_PASS,               /**< The ethernet driver will pass the packet
+					to the kernel, just as if the intercept
+					callback didn't exist */
+	CVM_OCT_DROP,               /**< The ethernet driver will drop the packet,
+					cleaning of the work queue entry and the
+					skbuff */
+	CVM_OCT_TAKE_OWNERSHIP_WORK,/**< The intercept callback takes over
+					ownership of the work queue entry. It is
+					the responsibility of the callback to free
+					the work queue entry and all associated
+					packet buffers. The ethernet driver will
+					dispose of the skbuff without affecting the
+					work queue entry */
+	CVM_OCT_TAKE_OWNERSHIP_SKB  /**< The intercept callback takes over
+					ownership of the skbuff. The work queue
+					entry and packet buffer will be disposed of
+					in a way keeping the skbuff valid */
 } cvm_oct_callback_result_t;
 
 
 /**
  * The is the definition of the Ethernet driver intercept
  * callback. The callback receives three parameters and
- * returns a cvm_oct_callback_result_t code.
+ * returns a struct cvm_oct_callback_result code.
  *
  * The first parameter is the linux device for the ethernet
  * port the packet came in on.
@@ -98,18 +97,17 @@ typedef cvm_oct_callback_result_t (*cvm_oct_callback_t)(struct net_device *dev,
  * This is the definition of the Ethernet driver's private
  * driver state stored in netdev_priv(dev).
  */
-typedef struct
-{
-    int                     port;           /* PKO hardware output port */
-    int                     queue;          /* PKO hardware queue for the port */
-    int                     fau;            /* Hardware fetch and add to count outstanding tx buffers */
-    int                     imode;          /* Type of port. This is one of the enums in cvmx_helper_interface_mode_t */
-    struct sk_buff_head     tx_free_list[16];/* List of outstanding tx buffers per queue */
-    struct net_device_stats stats;          /* Device statistics */
-    struct mii_if_info      mii_info;       /* Generic MII info structure */
-    cvm_oct_callback_t      intercept_cb;   /* Optional intecept callback defined above */
-    uint64_t                link_info;      /* Last negotiated link state */
-    void (*poll)(struct net_device *dev);   /* Called periodically to check link status */
+typedef struct {
+	int                     port;           /* PKO hardware output port */
+	int                     queue;          /* PKO hardware queue for the port */
+	int                     fau;            /* Hardware fetch and add to count outstanding tx buffers */
+	int                     imode;          /* Type of port. This is one of the enums in cvmx_helper_interface_mode_t */
+	struct sk_buff_head     tx_free_list[16];/* List of outstanding tx buffers per queue */
+	struct net_device_stats stats;          /* Device statistics */
+	struct mii_if_info      mii_info;       /* Generic MII info structure */
+	cvm_oct_callback_t      intercept_cb;   /* Optional intecept callback defined above */
+	uint64_t                link_info;      /* Last negotiated link state */
+	void (*poll)(struct net_device *dev);   /* Called periodically to check link status */
 } cvm_oct_private_t;
 
 
@@ -177,7 +175,7 @@ int cvm_oct_transmit_qos(struct net_device *dev, void *work_queue_entry, int do_
  */
 static inline int cvm_oct_transmit(struct net_device *dev, void *work_queue_entry, int do_free)
 {
-    return cvm_oct_transmit_qos(dev, work_queue_entry, do_free, 0);
+	return cvm_oct_transmit_qos(dev, work_queue_entry, do_free, 0);
 }
 
 #endif
diff --git a/drivers/net/cavium-ethernet/ethernet-common.c b/drivers/net/cavium-ethernet/ethernet-common.c
index 25b9ed0..8bbd075 100644
--- a/drivers/net/cavium-ethernet/ethernet-common.c
+++ b/drivers/net/cavium-ethernet/ethernet-common.c
@@ -64,42 +64,38 @@ extern char pow_send_list[];
  */
 static struct net_device_stats *cvm_oct_common_get_stats(struct net_device *dev)
 {
-    cvmx_pip_port_status_t rx_status;
-    cvmx_pko_port_status_t tx_status;
-    cvm_oct_private_t* priv = (cvm_oct_private_t*)netdev_priv(dev);
-
-    if (priv->port < CVMX_PIP_NUM_INPUT_PORTS)
-    {
-        if (octeon_is_simulation())
-        {
-            /* The simulator doesn't support statistics */
-            memset(&rx_status, 0, sizeof(rx_status));
-            memset(&tx_status, 0, sizeof(tx_status));
-        }
-        else
-        {
-            cvmx_pip_get_port_status(priv->port, 1, &rx_status);
-            cvmx_pko_get_port_status(priv->port, 1, &tx_status);
-        }
-
-        priv->stats.rx_packets      += rx_status.inb_packets;
-        priv->stats.tx_packets      += tx_status.packets;
-        priv->stats.rx_bytes        += rx_status.inb_octets;
-        priv->stats.tx_bytes        += tx_status.octets;
-        priv->stats.multicast       += rx_status.multicast_packets;
-        priv->stats.rx_crc_errors   += rx_status.inb_errors;
-        priv->stats.rx_frame_errors += rx_status.fcs_align_err_packets;
-
-        /* The drop counter must be incremented atomically since the RX
-            tasklet also increments it */
+	cvmx_pip_port_status_t rx_status;
+	cvmx_pko_port_status_t tx_status;
+	cvm_oct_private_t *priv = (cvm_oct_private_t *)netdev_priv(dev);
+
+	if (priv->port < CVMX_PIP_NUM_INPUT_PORTS) {
+		if (octeon_is_simulation()) {
+			/* The simulator doesn't support statistics */
+			memset(&rx_status, 0, sizeof(rx_status));
+			memset(&tx_status, 0, sizeof(tx_status));
+		} else {
+		cvmx_pip_get_port_status(priv->port, 1, &rx_status);
+		cvmx_pko_get_port_status(priv->port, 1, &tx_status);
+		}
+
+		priv->stats.rx_packets      += rx_status.inb_packets;
+		priv->stats.tx_packets      += tx_status.packets;
+		priv->stats.rx_bytes        += rx_status.inb_octets;
+		priv->stats.tx_bytes        += tx_status.octets;
+		priv->stats.multicast       += rx_status.multicast_packets;
+		priv->stats.rx_crc_errors   += rx_status.inb_errors;
+		priv->stats.rx_frame_errors += rx_status.fcs_align_err_packets;
+
+		/* The drop counter must be incremented atomically since the RX
+		   tasklet also increments it */
 #ifdef CONFIG_64BIT
-        cvmx_atomic_add64_nosync(&priv->stats.rx_dropped, rx_status.dropped_packets);
+		cvmx_atomic_add64_nosync(&priv->stats.rx_dropped, rx_status.dropped_packets);
 #else
-        cvmx_atomic_add32_nosync((int32_t*)&priv->stats.rx_dropped, rx_status.dropped_packets);
+		cvmx_atomic_add32_nosync((int32_t *)&priv->stats.rx_dropped, rx_status.dropped_packets);
 #endif
-    }
+	}
 
-    return &priv->stats;
+	return &priv->stats;
 }
 
 
@@ -110,38 +106,38 @@ static struct net_device_stats *cvm_oct_common_get_stats(struct net_device *dev)
  */
 static void cvm_oct_common_set_multicast_list(struct net_device *dev)
 {
-    cvmx_gmxx_prtx_cfg_t gmx_cfg;
-    cvm_oct_private_t* priv = (cvm_oct_private_t*)netdev_priv(dev);
-    int interface = INTERFACE(priv->port);
-    int index = INDEX(priv->port);
-
-    if ((interface < 2) && (cvmx_helper_interface_get_mode(interface) != CVMX_HELPER_INTERFACE_MODE_SPI))
-    {
-        cvmx_gmxx_rxx_adr_ctl_t control;
-        control.u64 = 0;
-        control.s.bcst = 1;     /* Allow broadcast MAC addresses */
-
-        if (dev->mc_list || (dev->flags&IFF_ALLMULTI) || (dev->flags&IFF_PROMISC))
-            control.s.mcst = 2; /* Force accept multicast packets */
-        else
-            control.s.mcst = 1; /* Force reject multicat packets */
-
-        if(dev->flags&IFF_PROMISC)
-            control.s.cam_mode = 0; /* Reject matches if promisc. Since CAM is shut off, should accept everything */
-        else
-            control.s.cam_mode = 1; /* Filter packets based on the CAM */
-
-        gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
-        cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmx_cfg.u64 & ~1ull);
-
-        cvmx_write_csr(CVMX_GMXX_RXX_ADR_CTL(index, interface), control.u64);
-        if (dev->flags&IFF_PROMISC)
-            cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM_EN(index, interface), 0);
-        else
-            cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM_EN(index, interface), 1);
-
-        cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmx_cfg.u64);
-    }
+	cvmx_gmxx_prtx_cfg_t gmx_cfg;
+	cvm_oct_private_t *priv = (cvm_oct_private_t *)netdev_priv(dev);
+	int interface = INTERFACE(priv->port);
+	int index = INDEX(priv->port);
+
+	if ((interface < 2) && (cvmx_helper_interface_get_mode(interface) != CVMX_HELPER_INTERFACE_MODE_SPI)) {
+		cvmx_gmxx_rxx_adr_ctl_t control;
+		control.u64 = 0;
+		control.s.bcst = 1;     /* Allow broadcast MAC addresses */
+
+		if (dev->mc_list || (dev->flags&IFF_ALLMULTI) ||
+		    (dev->flags & IFF_PROMISC))
+			control.s.mcst = 2; /* Force accept multicast packets */
+		else
+			control.s.mcst = 1; /* Force reject multicat packets */
+
+		if (dev->flags & IFF_PROMISC)
+			control.s.cam_mode = 0; /* Reject matches if promisc. Since CAM is shut off, should accept everything */
+		else
+			control.s.cam_mode = 1; /* Filter packets based on the CAM */
+
+		gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
+		cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmx_cfg.u64 & ~1ull);
+
+		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CTL(index, interface), control.u64);
+		if (dev->flags&IFF_PROMISC)
+			cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM_EN(index, interface), 0);
+		else
+			cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM_EN(index, interface), 1);
+
+		cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmx_cfg.u64);
+	}
 }
 
 
@@ -154,35 +150,34 @@ static void cvm_oct_common_set_multicast_list(struct net_device *dev)
  */
 static int cvm_oct_common_set_mac_address(struct net_device *dev, void *addr)
 {
-    cvm_oct_private_t* priv = (cvm_oct_private_t*)netdev_priv(dev);
-    cvmx_gmxx_prtx_cfg_t gmx_cfg;
-    int interface = INTERFACE(priv->port);
-    int index = INDEX(priv->port);
-
-    memcpy(dev->dev_addr, addr + 2, 6);
-
-    if ((interface < 2) && (cvmx_helper_interface_get_mode(interface) != CVMX_HELPER_INTERFACE_MODE_SPI))
-    {
-        int i;
-        uint8_t *ptr = addr;
-        uint64_t mac = 0;
-        for (i=0; i<6; i++)
-            mac = (mac<<8) | (uint64_t)(ptr[i+2]);
-
-        gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
-        cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmx_cfg.u64 & ~1ull);
-
-        cvmx_write_csr(CVMX_GMXX_SMACX(index, interface), mac);
-        cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM0(index, interface), ptr[2]);
-        cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM1(index, interface), ptr[3]);
-        cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM2(index, interface), ptr[4]);
-        cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM3(index, interface), ptr[5]);
-        cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM4(index, interface), ptr[6]);
-        cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM5(index, interface), ptr[7]);
-        cvm_oct_common_set_multicast_list(dev);
-        cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmx_cfg.u64);
-    }
-    return 0;
+	cvm_oct_private_t *priv = (cvm_oct_private_t *)netdev_priv(dev);
+	cvmx_gmxx_prtx_cfg_t gmx_cfg;
+	int interface = INTERFACE(priv->port);
+	int index = INDEX(priv->port);
+
+	memcpy(dev->dev_addr, addr + 2, 6);
+
+	if ((interface < 2) && (cvmx_helper_interface_get_mode(interface) != CVMX_HELPER_INTERFACE_MODE_SPI)) {
+		int i;
+		uint8_t *ptr = addr;
+		uint64_t mac = 0;
+		for (i = 0; i < 6; i++)
+			mac = (mac<<8) | (uint64_t)(ptr[i+2]);
+
+		gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
+		cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmx_cfg.u64 & ~1ull);
+
+		cvmx_write_csr(CVMX_GMXX_SMACX(index, interface), mac);
+		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM0(index, interface), ptr[2]);
+		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM1(index, interface), ptr[3]);
+		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM2(index, interface), ptr[4]);
+		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM3(index, interface), ptr[5]);
+		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM4(index, interface), ptr[6]);
+		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM5(index, interface), ptr[7]);
+		cvm_oct_common_set_multicast_list(dev);
+		cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmx_cfg.u64);
+	}
+	return 0;
 }
 
 
@@ -195,43 +190,38 @@ static int cvm_oct_common_set_mac_address(struct net_device *dev, void *addr)
  */
 static int cvm_oct_common_change_mtu(struct net_device *dev, int new_mtu)
 {
-    cvm_oct_private_t* priv = (cvm_oct_private_t*)netdev_priv(dev);
-    int interface = INTERFACE(priv->port);
-    int index = INDEX(priv->port);
-
-    /* Limit the MTU to make sure the ethernet packets are between 64 bytes
-        and 65535 bytes */
-    if ((new_mtu + 14 + 4 < 64) || (new_mtu + 14 + 4 > 65392))
-    {
-        printk("MTU must be between %d and %d.\n", 64-14-4, 65392-14-4);
-        return -EINVAL;
-    }
-    dev->mtu = new_mtu;
-
-    if ((interface < 2) && (cvmx_helper_interface_get_mode(interface) != CVMX_HELPER_INTERFACE_MODE_SPI))
-    {
-        int max_packet = new_mtu + 14 + 4; /* Add ethernet header and possible VLAN */
-
-        if (OCTEON_IS_MODEL(OCTEON_CN3XXX) || OCTEON_IS_MODEL(OCTEON_CN58XX))
-        {
-            /* Signal errors on packets larger than the MTU */
-            cvmx_write_csr(CVMX_GMXX_RXX_FRM_MAX(index, interface), max_packet);
-        }
-        else
-        {
-            /* Set the hardware to truncate packets larger than the MTU and
-                smaller the 64 bytes */
-            cvmx_pip_frm_len_chkx_t frm_len_chk;
-            frm_len_chk.u64 = 0;
-            frm_len_chk.s.minlen = 64;
-            frm_len_chk.s.maxlen = max_packet;
-            cvmx_write_csr(CVMX_PIP_FRM_LEN_CHKX(interface), frm_len_chk.u64);
-        }
-        /* Set the hardware to truncate packets larger than the MTU. The
-            jabber register must be set to a multiple of 8 bytes, so round up */
-        cvmx_write_csr(CVMX_GMXX_RXX_JABBER(index, interface), (max_packet + 7) & ~7u);
-    }
-    return 0;
+	cvm_oct_private_t *priv = (cvm_oct_private_t *)netdev_priv(dev);
+	int interface = INTERFACE(priv->port);
+	int index = INDEX(priv->port);
+
+	/* Limit the MTU to make sure the ethernet packets are between 64 bytes
+	   and 65535 bytes */
+	if ((new_mtu + 14 + 4 < 64) || (new_mtu + 14 + 4 > 65392)) {
+		printk("MTU must be between %d and %d.\n", 64-14-4, 65392-14-4);
+		return -EINVAL;
+	}
+	dev->mtu = new_mtu;
+
+	if ((interface < 2) && (cvmx_helper_interface_get_mode(interface) != CVMX_HELPER_INTERFACE_MODE_SPI)) {
+		int max_packet = new_mtu + 14 + 4; /* Add ethernet header and possible VLAN */
+
+		if (OCTEON_IS_MODEL(OCTEON_CN3XXX) || OCTEON_IS_MODEL(OCTEON_CN58XX)) {
+			/* Signal errors on packets larger than the MTU */
+			cvmx_write_csr(CVMX_GMXX_RXX_FRM_MAX(index, interface), max_packet);
+		} else {
+			/* Set the hardware to truncate packets larger than the MTU and
+				smaller the 64 bytes */
+			cvmx_pip_frm_len_chkx_t frm_len_chk;
+			frm_len_chk.u64 = 0;
+			frm_len_chk.s.minlen = 64;
+			frm_len_chk.s.maxlen = max_packet;
+			cvmx_write_csr(CVMX_PIP_FRM_LEN_CHKX(interface), frm_len_chk.u64);
+		}
+		/* Set the hardware to truncate packets larger than the MTU. The
+		   jabber register must be set to a multiple of 8 bytes, so round up */
+		cvmx_write_csr(CVMX_GMXX_RXX_JABBER(index, interface), (max_packet + 7) & ~7u);
+	}
+	return 0;
 }
 
 
@@ -243,56 +233,53 @@ static int cvm_oct_common_change_mtu(struct net_device *dev, int new_mtu)
  */
 int cvm_oct_common_init(struct net_device *dev)
 {
-    static int count = 0;
-    char mac[8] = {0x00,0x00,
-        octeon_bootinfo->mac_addr_base[0],
-        octeon_bootinfo->mac_addr_base[1],
-        octeon_bootinfo->mac_addr_base[2],
-        octeon_bootinfo->mac_addr_base[3],
-        octeon_bootinfo->mac_addr_base[4],
-        octeon_bootinfo->mac_addr_base[5] + count};
-    cvm_oct_private_t* priv = (cvm_oct_private_t*)netdev_priv(dev);
-
-
-    /* Force the interface to use the POW send if always_use_pow was
-        specified or it is in the pow send list */
-    if ((pow_send_group != -1) && (always_use_pow || strstr(pow_send_list, dev->name)))
-        priv->queue = -1;
-
-    if (priv->queue != -1)
-    {
-        dev->hard_start_xmit= cvm_oct_xmit;
-        if (USE_HW_TCPUDP_CHECKSUM)
-            dev->features |= NETIF_F_IP_CSUM;
-        count++;
-    }
-    else
-    {
-        dev->hard_start_xmit= cvm_oct_xmit_pow;
-        memset(mac, 0, sizeof(mac));
-        mac[7] = pow_send_group;
-    }
-
-    dev->get_stats          = cvm_oct_common_get_stats;
-    dev->set_mac_address    = cvm_oct_common_set_mac_address;
-    dev->set_multicast_list = cvm_oct_common_set_multicast_list;
-    dev->change_mtu         = cvm_oct_common_change_mtu;
-    dev->do_ioctl           = cvm_oct_ioctl;
-    dev->features           |= NETIF_F_LLTX; /* We do our own locking, Linux doesn't need to */
-    SET_ETHTOOL_OPS(dev, &cvm_oct_ethtool_ops);
+	static int count;
+	char mac[8] = {0x00, 0x00,
+		octeon_bootinfo->mac_addr_base[0],
+		octeon_bootinfo->mac_addr_base[1],
+		octeon_bootinfo->mac_addr_base[2],
+		octeon_bootinfo->mac_addr_base[3],
+		octeon_bootinfo->mac_addr_base[4],
+		octeon_bootinfo->mac_addr_base[5] + count};
+	cvm_oct_private_t *priv = (cvm_oct_private_t *)netdev_priv(dev);
+
+
+	/* Force the interface to use the POW send if always_use_pow was
+	   specified or it is in the pow send list */
+	if ((pow_send_group != -1) && (always_use_pow || strstr(pow_send_list, dev->name)))
+		priv->queue = -1;
+
+	if (priv->queue != -1) {
+		dev->hard_start_xmit = cvm_oct_xmit;
+		if (USE_HW_TCPUDP_CHECKSUM)
+			dev->features |= NETIF_F_IP_CSUM;
+		count++;
+	} else {
+		dev->hard_start_xmit = cvm_oct_xmit_pow;
+		memset(mac, 0, sizeof(mac));
+		mac[7] = pow_send_group;
+	}
+
+	dev->get_stats          = cvm_oct_common_get_stats;
+	dev->set_mac_address    = cvm_oct_common_set_mac_address;
+	dev->set_multicast_list = cvm_oct_common_set_multicast_list;
+	dev->change_mtu         = cvm_oct_common_change_mtu;
+	dev->do_ioctl           = cvm_oct_ioctl;
+	dev->features           |= NETIF_F_LLTX; /* We do our own locking, Linux doesn't need to */
+	SET_ETHTOOL_OPS(dev, &cvm_oct_ethtool_ops);
 #ifdef CONFIG_NET_POLL_CONTROLLER
-    dev->poll_controller    = cvm_oct_poll_controller;
+	dev->poll_controller    = cvm_oct_poll_controller;
 #endif
 
-    cvm_oct_mdio_setup_device(dev);
-    dev->set_mac_address(dev, mac);
-    dev->change_mtu(dev, dev->mtu);
+	cvm_oct_mdio_setup_device(dev);
+	dev->set_mac_address(dev, mac);
+	dev->change_mtu(dev, dev->mtu);
 
-    /* Zero out stats for port so we won't mistakenly show counters from the
-        bootloader */
-    memset(dev->get_stats(dev), 0, sizeof(struct net_device_stats));
+	/* Zero out stats for port so we won't mistakenly show counters from the
+	   bootloader */
+	memset(dev->get_stats(dev), 0, sizeof(struct net_device_stats));
 
-    return 0;
+	return 0;
 }
 
 void cvm_oct_common_uninit(struct net_device *dev)
diff --git a/drivers/net/cavium-ethernet/ethernet-defines.h b/drivers/net/cavium-ethernet/ethernet-defines.h
index 5b14697..e8501c7 100644
--- a/drivers/net/cavium-ethernet/ethernet-defines.h
+++ b/drivers/net/cavium-ethernet/ethernet-defines.h
@@ -90,17 +90,17 @@
 #endif
 
 #if CONFIG_CAVIUM_RESERVE32
-    #define USE_32BIT_SHARED            1
-    #define USE_SKBUFFS_IN_HW           0
-    #define REUSE_SKBUFFS_WITHOUT_FREE  0
+	#define USE_32BIT_SHARED            1
+	#define USE_SKBUFFS_IN_HW           0
+	#define REUSE_SKBUFFS_WITHOUT_FREE  0
 #else
-    #define USE_32BIT_SHARED            0
-    #define USE_SKBUFFS_IN_HW           1
-    #ifdef CONFIG_NETFILTER
-        #define REUSE_SKBUFFS_WITHOUT_FREE  0
-    #else
-        #define REUSE_SKBUFFS_WITHOUT_FREE  1
-    #endif
+	#define USE_32BIT_SHARED            0
+	#define USE_SKBUFFS_IN_HW           1
+	#ifdef CONFIG_NETFILTER
+		#define REUSE_SKBUFFS_WITHOUT_FREE  0
+	#else
+		#define REUSE_SKBUFFS_WITHOUT_FREE  1
+	#endif
 #endif
 
 #if defined(CONFIG_CAVIUM_OCTEON_NUM_PACKET_BUFFERS) && CONFIG_CAVIUM_OCTEON_NUM_PACKET_BUFFERS
@@ -110,14 +110,14 @@
 #endif
 
 #define INTERRUPT_LIMIT             10000   /* Max interrupts per second per core */
-//#define INTERRUPT_LIMIT             0       /* Don't limit the number of interrupts */
+/*#define INTERRUPT_LIMIT             0     *//* Don't limit the number of interrupts */
 #define USE_HW_TCPUDP_CHECKSUM      1
 #define USE_MULTICORE_RECEIVE       1
 #define USE_RED                     1	/* Enable Random Early Dropping under load */
 #define USE_ASYNC_IOBDMA            (CONFIG_CAVIUM_OCTEON_CVMSEG_SIZE > 0)
 #define USE_10MBPS_PREAMBLE_WORKAROUND 1    /* Allow SW based preamble removal at 10Mbps to workaround PHYs giving us bad preambles */
 #define DONT_WRITEBACK(x)           (x) /* Use this to have all FPA frees also tell the L2 not to write data to memory */
-//#define DONT_WRITEBACK(x)         0   /* Use this to not have FPA frees control L2 */
+/*#define DONT_WRITEBACK(x)         0   *//* Use this to not have FPA frees control L2 */
 
 #ifndef CONFIG_SMP
 #undef USE_MULTICORE_RECEIVE
diff --git a/drivers/net/cavium-ethernet/ethernet-mdio.c b/drivers/net/cavium-ethernet/ethernet-mdio.c
index d41dd7f..985ccc1 100644
--- a/drivers/net/cavium-ethernet/ethernet-mdio.c
+++ b/drivers/net/cavium-ethernet/ethernet-mdio.c
@@ -62,26 +62,25 @@
  */
 static int cvm_oct_mdio_read(struct net_device *dev, int phy_id, int location)
 {
-    cvmx_smi_cmd_t          smi_cmd;
-    cvmx_smi_rd_dat_t       smi_rd;
-
-    smi_cmd.u64 = 0;
-    smi_cmd.s.phy_op = 1;
-    smi_cmd.s.phy_adr = phy_id;
-    smi_cmd.s.reg_adr = location;
-    cvmx_write_csr(CVMX_SMI_CMD, smi_cmd.u64);
-
-    do
-    {
-        if (!in_interrupt())
-            yield();
-        smi_rd.u64 = cvmx_read_csr(CVMX_SMI_RD_DAT);
-    } while (smi_rd.s.pending);
-
-    if (smi_rd.s.val)
-        return smi_rd.s.dat;
-    else
-        return 0;
+	cvmx_smi_cmd_t          smi_cmd;
+	cvmx_smi_rd_dat_t       smi_rd;
+
+	smi_cmd.u64 = 0;
+	smi_cmd.s.phy_op = 1;
+	smi_cmd.s.phy_adr = phy_id;
+	smi_cmd.s.reg_adr = location;
+	cvmx_write_csr(CVMX_SMI_CMD, smi_cmd.u64);
+
+	do {
+		if (!in_interrupt())
+			yield();
+		smi_rd.u64 = cvmx_read_csr(CVMX_SMI_RD_DAT);
+	} while (smi_rd.s.pending);
+
+	if (smi_rd.s.val)
+		return smi_rd.s.dat;
+	else
+		return 0;
 }
 
 static int cvm_oct_mdio_dummy_read(struct net_device *dev, int phy_id, int location)
@@ -100,25 +99,24 @@ static int cvm_oct_mdio_dummy_read(struct net_device *dev, int phy_id, int locat
  */
 static void cvm_oct_mdio_write(struct net_device *dev, int phy_id, int location, int val)
 {
-    cvmx_smi_cmd_t          smi_cmd;
-    cvmx_smi_wr_dat_t       smi_wr;
-
-    smi_wr.u64 = 0;
-    smi_wr.s.dat = val;
-    cvmx_write_csr(CVMX_SMI_WR_DAT, smi_wr.u64);
-
-    smi_cmd.u64 = 0;
-    smi_cmd.s.phy_op = 0;
-    smi_cmd.s.phy_adr = phy_id;
-    smi_cmd.s.reg_adr = location;
-    cvmx_write_csr(CVMX_SMI_CMD, smi_cmd.u64);
-
-    do
-    {
-        if (!in_interrupt())
-            yield();
-        smi_wr.u64 = cvmx_read_csr(CVMX_SMI_WR_DAT);
-    } while (smi_wr.s.pending);
+	cvmx_smi_cmd_t          smi_cmd;
+	cvmx_smi_wr_dat_t       smi_wr;
+
+	smi_wr.u64 = 0;
+	smi_wr.s.dat = val;
+	cvmx_write_csr(CVMX_SMI_WR_DAT, smi_wr.u64);
+
+	smi_cmd.u64 = 0;
+	smi_cmd.s.phy_op = 0;
+	smi_cmd.s.phy_adr = phy_id;
+	smi_cmd.s.reg_adr = location;
+	cvmx_write_csr(CVMX_SMI_CMD, smi_cmd.u64);
+
+	do {
+		if (!in_interrupt())
+			yield();
+		smi_wr.u64 = cvmx_read_csr(CVMX_SMI_WR_DAT);
+	} while (smi_wr.s.pending);
 }
 
 static void cvm_oct_mdio_dummy_write(struct net_device *dev, int phy_id, int location, int val)
@@ -128,48 +126,48 @@ static void cvm_oct_mdio_dummy_write(struct net_device *dev, int phy_id, int loc
 
 static void cvm_oct_get_drvinfo(struct net_device *dev, struct ethtool_drvinfo *info)
 {
-    strcpy(info->driver, "cavium-ethernet");
-    strcpy(info->version, OCTEON_SDK_VERSION_STRING);
-    strcpy(info->bus_info, "Builtin");
+	strcpy(info->driver, "cavium-ethernet");
+	strcpy(info->version, OCTEON_SDK_VERSION_STRING);
+	strcpy(info->bus_info, "Builtin");
 }
 
 
 static int cvm_oct_get_settings(struct net_device *dev, struct ethtool_cmd *cmd)
 {
-    cvm_oct_private_t* priv = (cvm_oct_private_t*)netdev_priv(dev);
-    return mii_ethtool_gset(&priv->mii_info, cmd);
+	cvm_oct_private_t *priv = (cvm_oct_private_t *)netdev_priv(dev);
+	return mii_ethtool_gset(&priv->mii_info, cmd);
 }
 
 
 static int cvm_oct_set_settings(struct net_device *dev, struct ethtool_cmd *cmd)
 {
-    cvm_oct_private_t* priv = (cvm_oct_private_t*)netdev_priv(dev);
-    return mii_ethtool_sset(&priv->mii_info, cmd);
+	cvm_oct_private_t *priv = (cvm_oct_private_t *)netdev_priv(dev);
+	return mii_ethtool_sset(&priv->mii_info, cmd);
 }
 
 
 static int cvm_oct_nway_reset(struct net_device *dev)
 {
-    cvm_oct_private_t* priv = (cvm_oct_private_t*)netdev_priv(dev);
-    return mii_nway_restart(&priv->mii_info);
+	cvm_oct_private_t *priv = (cvm_oct_private_t *)netdev_priv(dev);
+	return mii_nway_restart(&priv->mii_info);
 }
 
 
 static u32 cvm_oct_get_link(struct net_device *dev)
 {
-    cvm_oct_private_t* priv = (cvm_oct_private_t*)netdev_priv(dev);
-    return mii_link_ok(&priv->mii_info);
+	cvm_oct_private_t *priv = (cvm_oct_private_t *)netdev_priv(dev);
+	return mii_link_ok(&priv->mii_info);
 }
 
 
 struct ethtool_ops cvm_oct_ethtool_ops = {
-    .get_drvinfo    = cvm_oct_get_drvinfo,
-    .get_settings   = cvm_oct_get_settings,
-    .set_settings   = cvm_oct_set_settings,
-    .nway_reset     = cvm_oct_nway_reset,
-    .get_link       = cvm_oct_get_link,
-    .get_sg         = ethtool_op_get_sg,
-    .get_tx_csum    = ethtool_op_get_tx_csum,
+	.get_drvinfo    = cvm_oct_get_drvinfo,
+	.get_settings   = cvm_oct_get_settings,
+	.set_settings   = cvm_oct_set_settings,
+	.nway_reset     = cvm_oct_nway_reset,
+	.get_link       = cvm_oct_get_link,
+	.get_sg         = ethtool_op_get_sg,
+	.get_tx_csum    = ethtool_op_get_tx_csum,
 };
 
 
@@ -183,10 +181,10 @@ struct ethtool_ops cvm_oct_ethtool_ops = {
  */
 int cvm_oct_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)
 {
-    cvm_oct_private_t*      priv = (cvm_oct_private_t*)netdev_priv(dev);
-    struct mii_ioctl_data*  data = if_mii(rq);
-    unsigned int            duplex_chg;
-    return generic_mii_ioctl(&priv->mii_info, data, cmd, &duplex_chg);
+	cvm_oct_private_t      *priv = (cvm_oct_private_t *)netdev_priv(dev);
+	struct mii_ioctl_data  *data = if_mii(rq);
+	unsigned int            duplex_chg;
+	return generic_mii_ioctl(&priv->mii_info, data, cmd, &duplex_chg);
 }
 
 
@@ -199,25 +197,22 @@ int cvm_oct_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)
  */
 int cvm_oct_mdio_setup_device(struct net_device *dev)
 {
-    cvm_oct_private_t* priv = (cvm_oct_private_t*)netdev_priv(dev);
-    int phy_id = cvmx_helper_board_get_mii_address(priv->port);
-    if (phy_id != -1)
-    {
-        priv->mii_info.dev = dev;
-        priv->mii_info.phy_id = phy_id;
-        priv->mii_info.phy_id_mask = 0xff;
-        priv->mii_info.supports_gmii = 1;
-        priv->mii_info.reg_num_mask = 0x1f;
-        priv->mii_info.mdio_read = cvm_oct_mdio_read;
-        priv->mii_info.mdio_write = cvm_oct_mdio_write;
-    }
-    else
-    {
-        /* Supply dummy MDIO routines so the kernel won't crash
-            if the user tries to read them */
-        priv->mii_info.mdio_read = cvm_oct_mdio_dummy_read;
-        priv->mii_info.mdio_write = cvm_oct_mdio_dummy_write;
-    }
-    return 0;
+	cvm_oct_private_t *priv = (cvm_oct_private_t *)netdev_priv(dev);
+	int phy_id = cvmx_helper_board_get_mii_address(priv->port);
+	if (phy_id != -1) {
+		priv->mii_info.dev = dev;
+		priv->mii_info.phy_id = phy_id;
+		priv->mii_info.phy_id_mask = 0xff;
+		priv->mii_info.supports_gmii = 1;
+		priv->mii_info.reg_num_mask = 0x1f;
+		priv->mii_info.mdio_read = cvm_oct_mdio_read;
+		priv->mii_info.mdio_write = cvm_oct_mdio_write;
+	} else {
+		/* Supply dummy MDIO routines so the kernel won't crash
+		   if the user tries to read them */
+		priv->mii_info.mdio_read = cvm_oct_mdio_dummy_read;
+		priv->mii_info.mdio_write = cvm_oct_mdio_dummy_write;
+	}
+	return 0;
 }
 
diff --git a/drivers/net/cavium-ethernet/ethernet-mem.c b/drivers/net/cavium-ethernet/ethernet-mem.c
index 99bcf6f..773d512 100644
--- a/drivers/net/cavium-ethernet/ethernet-mem.c
+++ b/drivers/net/cavium-ethernet/ethernet-mem.c
@@ -60,16 +60,16 @@
  */
 static void cvm_oct_fill_hw_skbuff(int pool, int size, int elements)
 {
-    while (elements--)
-    {
-        struct sk_buff *skb = dev_alloc_skb(size + 128);
-        if (unlikely(skb == NULL))
-            panic("Failed to allocate skb for hardware pool %d\n", pool);
-
-        skb_reserve(skb, 128 - (((unsigned long)skb->data) & 0x7f));
-        *(struct sk_buff **)(skb->data - sizeof(void*)) = skb;
-        cvmx_fpa_free(skb->data, pool, DONT_WRITEBACK(size/128));
-    }
+	while (elements--) {
+
+		struct sk_buff *skb = dev_alloc_skb(size + 128);
+		if (unlikely(skb == NULL))
+			panic("Failed to allocate skb for hardware pool %d\n", pool);
+
+		skb_reserve(skb, 128 - (((unsigned long)skb->data) & 0x7f));
+		*(struct sk_buff **)(skb->data - sizeof(void *)) = skb;
+		cvmx_fpa_free(skb->data, pool, DONT_WRITEBACK(size/128));
+	}
 }
 
 
@@ -82,23 +82,21 @@ static void cvm_oct_fill_hw_skbuff(int pool, int size, int elements)
  */
 static void cvm_oct_free_hw_skbuff(int pool, int size, int elements)
 {
-    char *memory;
-
-    do
-    {
-        memory = cvmx_fpa_alloc(pool);
-        if (memory)
-        {
-            struct sk_buff *skb = *(struct sk_buff **)(memory - sizeof(void*));
-            elements--;
-            dev_kfree_skb(skb);
-        }
-    } while (memory);
-
-    if (elements < 0)
-        printk("Warning: Freeing of pool %u had too many skbuffs (%d)\n", pool, elements);
-    else if (elements > 0)
-        printk("Warning: Freeing of pool %u is missing %d skbuffs\n", pool, elements);
+	char *memory;
+
+	do {
+		memory = cvmx_fpa_alloc(pool);
+		if (memory) {
+			struct sk_buff *skb = *(struct sk_buff **)(memory - sizeof(void *));
+			elements--;
+			dev_kfree_skb(skb);
+		}
+	} while (memory);
+
+	if (elements < 0)
+		printk("Warning: Freeing of pool %u had too many skbuffs (%d)\n", pool, elements);
+	else if (elements > 0)
+		printk("Warning: Freeing of pool %u is missing %d skbuffs\n", pool, elements);
 }
 
 
@@ -114,35 +112,31 @@ static void cvm_oct_free_hw_skbuff(int pool, int size, int elements)
  */
 void cvm_oct_fill_hw_memory(int pool, int size, int elements)
 {
-    char *memory;
-
-    if (USE_32BIT_SHARED)
-    {
-        extern uint64_t octeon_reserve32_memory;
-
-        memory = cvmx_bootmem_alloc_range(elements*size, 128, octeon_reserve32_memory,
-                                          octeon_reserve32_memory + (CONFIG_CAVIUM_RESERVE32<<20) - 1);
-        if (memory == NULL)
-            panic("Unable to allocate %u bytes for FPA pool %d\n", elements*size, pool);
-        printk("Memory range %p - %p reserved for hardware\n", memory, memory + elements*size - 1);
-        while (elements--)
-        {
-            cvmx_fpa_free(memory, pool, 0);
-            memory += size;
-        }
-    }
-    else
-    {
-        while (elements--)
-        {
-            /* We need to force alignment to 128 bytes here */
-            memory = kmalloc(size + 127, GFP_ATOMIC);
-            memory = (char*)(((unsigned long)memory+127) & -128);
-            if (memory == NULL)
-                panic("Unable to allocate %u bytes for FPA pool %d\n", elements*size, pool);
-            cvmx_fpa_free(memory, pool, 0);
-        }
-    }
+	char *memory;
+
+	if (USE_32BIT_SHARED) {
+		extern uint64_t octeon_reserve32_memory;
+
+		memory = cvmx_bootmem_alloc_range(elements*size, 128, octeon_reserve32_memory,
+						octeon_reserve32_memory + (CONFIG_CAVIUM_RESERVE32<<20) - 1);
+		if (memory == NULL)
+			panic("Unable to allocate %u bytes for FPA pool %d\n", elements*size, pool);
+		printk("Memory range %p - %p reserved for hardware\n", memory, memory + elements*size - 1);
+
+		while (elements--) {
+			cvmx_fpa_free(memory, pool, 0);
+			memory += size;
+		}
+	} else {
+		while (elements--) {
+			/* We need to force alignment to 128 bytes here */
+			memory = kmalloc(size + 127, GFP_ATOMIC);
+			memory = (char *)(((unsigned long)memory+127) & -128);
+			if (memory == NULL)
+				panic("Unable to allocate %u bytes for FPA pool %d\n", elements*size, pool);
+			cvmx_fpa_free(memory, pool, 0);
+		}
+	}
 }
 
 
@@ -155,44 +149,39 @@ void cvm_oct_fill_hw_memory(int pool, int size, int elements)
  */
 static void cvm_oct_free_hw_memory(int pool, int size, int elements)
 {
-    if (USE_32BIT_SHARED)
-    {
-        printk("Warning: 32 shared memory is not freeable\n");
-    }
-    else
-    {
-        char *memory;
-        do
-        {
-            memory = cvmx_fpa_alloc(pool);
-            if (memory)
-            {
-                elements--;
-                kfree(phys_to_virt(cvmx_ptr_to_phys(memory)));
-            }
-        } while (memory);
-
-        if (elements < 0)
-            printk("Freeing of pool %u had too many buffers (%d)\n", pool, elements);
-        else if (elements > 0)
-            printk("Warning: Freeing of pool %u is missing %d buffers\n", pool, elements);
-    }
+	if (USE_32BIT_SHARED) {
+		printk("Warning: 32 shared memory is not freeable\n");
+	} else {
+		char *memory;
+		do {
+			memory = cvmx_fpa_alloc(pool);
+			if (memory) {
+				elements--;
+				kfree(phys_to_virt(cvmx_ptr_to_phys(memory)));
+			}
+		} while (memory);
+
+		if (elements < 0)
+			printk("Freeing of pool %u had too many buffers (%d)\n", pool, elements);
+		else if (elements > 0)
+			printk("Warning: Freeing of pool %u is missing %d buffers\n", pool, elements);
+	}
 }
 
 
 void cvm_oct_mem_fill_fpa(int pool, int size, int elements)
 {
-    if (USE_SKBUFFS_IN_HW)
-        cvm_oct_fill_hw_skbuff(pool, size, elements);
-    else
-        cvm_oct_fill_hw_memory(pool, size, elements);
+	if (USE_SKBUFFS_IN_HW)
+		cvm_oct_fill_hw_skbuff(pool, size, elements);
+	else
+		cvm_oct_fill_hw_memory(pool, size, elements);
 }
 
 void cvm_oct_mem_empty_fpa(int pool, int size, int elements)
 {
-    if (USE_SKBUFFS_IN_HW)
-        cvm_oct_free_hw_skbuff(pool, size, elements);
-    else
-        cvm_oct_free_hw_memory(pool, size, elements);
+	if (USE_SKBUFFS_IN_HW)
+		cvm_oct_free_hw_skbuff(pool, size, elements);
+	else
+		cvm_oct_free_hw_memory(pool, size, elements);
 }
 
diff --git a/drivers/net/cavium-ethernet/ethernet-proc.c b/drivers/net/cavium-ethernet/ethernet-proc.c
index 0298770..8ad051b 100644
--- a/drivers/net/cavium-ethernet/ethernet-proc.c
+++ b/drivers/net/cavium-ethernet/ethernet-proc.c
@@ -56,97 +56,97 @@ extern struct net_device *cvm_oct_device[];
 
 static unsigned long long cvm_oct_stats_read_switch(struct net_device *dev, int phy_id, int offset)
 {
-    cvm_oct_private_t* priv = (cvm_oct_private_t*)netdev_priv(dev);
-    priv->mii_info.mdio_write(dev, phy_id, 0x1d, 0xcc00 | offset);
-    return ((uint64_t)priv->mii_info.mdio_read(dev, phy_id, 0x1e)<<16) | (uint64_t)priv->mii_info.mdio_read(dev, phy_id, 0x1f);
+	cvm_oct_private_t *priv = (cvm_oct_private_t *)netdev_priv(dev);
+	priv->mii_info.mdio_write(dev, phy_id, 0x1d, 0xcc00 | offset);
+	return ((uint64_t)priv->mii_info.mdio_read(dev, phy_id, 0x1e)<<16) | (uint64_t)priv->mii_info.mdio_read(dev, phy_id, 0x1f);
 }
 
 static int cvm_oct_stats_switch_show(struct seq_file *m, void *v)
 {
-    static const int ports[] = {0,1,2,3,9,-1};
-    struct net_device *dev = cvm_oct_device[0];
-    int index = 0;
+	static const int ports[] = {0, 1, 2, 3, 9, -1};
+	struct net_device *dev = cvm_oct_device[0];
+	int index = 0;
 
-    while (ports[index] != -1)
-    {
-        /* Latch port */
-        cvm_oct_private_t* priv = (cvm_oct_private_t*)netdev_priv(dev);
-        priv->mii_info.mdio_write(dev, 0x1b, 0x1d, 0xdc00 | ports[index]);
-        seq_printf(m, "\nSwitch Port %d\n", ports[index]);
-        seq_printf(m, "InGoodOctets:   %12llu\t"
-                   "OutOctets:      %12llu\t"
-                   "64 Octets:      %12llu\n",
-                   cvm_oct_stats_read_switch(dev, 0x1b, 0x00) | (cvm_oct_stats_read_switch(dev, 0x1b, 0x01)<<32),
-                   cvm_oct_stats_read_switch(dev, 0x1b, 0x0E) | (cvm_oct_stats_read_switch(dev, 0x1b, 0x0F)<<32),
-                   cvm_oct_stats_read_switch(dev, 0x1b, 0x08));
+	while (ports[index] != -1) {
 
-        seq_printf(m, "InBadOctets:    %12llu\t"
-                   "OutUnicast:     %12llu\t"
-                   "65-127 Octets:  %12llu\n",
-                   cvm_oct_stats_read_switch(dev, 0x1b, 0x02),
-                   cvm_oct_stats_read_switch(dev, 0x1b, 0x10),
-                   cvm_oct_stats_read_switch(dev, 0x1b, 0x09));
+		/* Latch port */
+		cvm_oct_private_t *priv = (cvm_oct_private_t *)netdev_priv(dev);
+		priv->mii_info.mdio_write(dev, 0x1b, 0x1d, 0xdc00 | ports[index]);
+		seq_printf(m, "\nSwitch Port %d\n", ports[index]);
+		seq_printf(m, "InGoodOctets:   %12llu\t"
+			   "OutOctets:      %12llu\t"
+			   "64 Octets:      %12llu\n",
+			   cvm_oct_stats_read_switch(dev, 0x1b, 0x00) | (cvm_oct_stats_read_switch(dev, 0x1b, 0x01)<<32),
+			   cvm_oct_stats_read_switch(dev, 0x1b, 0x0E) | (cvm_oct_stats_read_switch(dev, 0x1b, 0x0F)<<32),
+			   cvm_oct_stats_read_switch(dev, 0x1b, 0x08));
 
-        seq_printf(m, "InUnicast:      %12llu\t"
-                   "OutBroadcasts:  %12llu\t"
-                   "128-255 Octets: %12llu\n",
-                   cvm_oct_stats_read_switch(dev, 0x1b, 0x04),
-                   cvm_oct_stats_read_switch(dev, 0x1b, 0x13),
-                   cvm_oct_stats_read_switch(dev, 0x1b, 0x0A));
+		seq_printf(m, "InBadOctets:    %12llu\t"
+			   "OutUnicast:     %12llu\t"
+			   "65-127 Octets:  %12llu\n",
+			   cvm_oct_stats_read_switch(dev, 0x1b, 0x02),
+			   cvm_oct_stats_read_switch(dev, 0x1b, 0x10),
+			   cvm_oct_stats_read_switch(dev, 0x1b, 0x09));
 
-        seq_printf(m, "InBroadcasts:   %12llu\t"
-                   "OutMulticasts:  %12llu\t"
-                   "256-511 Octets: %12llu\n",
-                   cvm_oct_stats_read_switch(dev, 0x1b, 0x06),
-                   cvm_oct_stats_read_switch(dev, 0x1b, 0x12),
-                   cvm_oct_stats_read_switch(dev, 0x1b, 0x0B));
+		seq_printf(m, "InUnicast:      %12llu\t"
+			   "OutBroadcasts:  %12llu\t"
+			   "128-255 Octets: %12llu\n",
+			   cvm_oct_stats_read_switch(dev, 0x1b, 0x04),
+			   cvm_oct_stats_read_switch(dev, 0x1b, 0x13),
+			   cvm_oct_stats_read_switch(dev, 0x1b, 0x0A));
 
-        seq_printf(m, "InMulticasts:   %12llu\t"
-                   "OutPause:       %12llu\t"
-                   "512-1023 Octets:%12llu\n",
-                   cvm_oct_stats_read_switch(dev, 0x1b, 0x07),
-                   cvm_oct_stats_read_switch(dev, 0x1b, 0x15),
-                   cvm_oct_stats_read_switch(dev, 0x1b, 0x0C));
+		seq_printf(m, "InBroadcasts:   %12llu\t"
+			   "OutMulticasts:  %12llu\t"
+			   "256-511 Octets: %12llu\n",
+			   cvm_oct_stats_read_switch(dev, 0x1b, 0x06),
+			   cvm_oct_stats_read_switch(dev, 0x1b, 0x12),
+			   cvm_oct_stats_read_switch(dev, 0x1b, 0x0B));
 
-        seq_printf(m, "InPause:        %12llu\t"
-                   "Excessive:      %12llu\t"
-                   "1024-Max Octets:%12llu\n",
-                   cvm_oct_stats_read_switch(dev, 0x1b, 0x16),
-                   cvm_oct_stats_read_switch(dev, 0x1b, 0x11),
-                   cvm_oct_stats_read_switch(dev, 0x1b, 0x0D));
+		seq_printf(m, "InMulticasts:   %12llu\t"
+			   "OutPause:       %12llu\t"
+			   "512-1023 Octets:%12llu\n",
+			   cvm_oct_stats_read_switch(dev, 0x1b, 0x07),
+			   cvm_oct_stats_read_switch(dev, 0x1b, 0x15),
+			   cvm_oct_stats_read_switch(dev, 0x1b, 0x0C));
 
-        seq_printf(m, "InUndersize:    %12llu\t"
-                   "Collisions:     %12llu\n",
-                   cvm_oct_stats_read_switch(dev, 0x1b, 0x18),
-                   cvm_oct_stats_read_switch(dev, 0x1b, 0x1E));
+		seq_printf(m, "InPause:        %12llu\t"
+			   "Excessive:      %12llu\t"
+			   "1024-Max Octets:%12llu\n",
+			   cvm_oct_stats_read_switch(dev, 0x1b, 0x16),
+			   cvm_oct_stats_read_switch(dev, 0x1b, 0x11),
+			   cvm_oct_stats_read_switch(dev, 0x1b, 0x0D));
 
-        seq_printf(m, "InFragments:    %12llu\t"
-                   "Deferred:       %12llu\n",
-                   cvm_oct_stats_read_switch(dev, 0x1b, 0x19),
-                   cvm_oct_stats_read_switch(dev, 0x1b, 0x05));
+		seq_printf(m, "InUndersize:    %12llu\t"
+			   "Collisions:     %12llu\n",
+			   cvm_oct_stats_read_switch(dev, 0x1b, 0x18),
+			   cvm_oct_stats_read_switch(dev, 0x1b, 0x1E));
 
-        seq_printf(m, "InOversize:     %12llu\t"
-                   "Single:         %12llu\n",
-                   cvm_oct_stats_read_switch(dev, 0x1b, 0x1A),
-                   cvm_oct_stats_read_switch(dev, 0x1b, 0x14));
+		seq_printf(m, "InFragments:    %12llu\t"
+			   "Deferred:       %12llu\n",
+			   cvm_oct_stats_read_switch(dev, 0x1b, 0x19),
+			   cvm_oct_stats_read_switch(dev, 0x1b, 0x05));
 
-        seq_printf(m, "InJabber:       %12llu\t"
-                   "Multiple:       %12llu\n",
-                   cvm_oct_stats_read_switch(dev, 0x1b, 0x1B),
-                   cvm_oct_stats_read_switch(dev, 0x1b, 0x17));
+		seq_printf(m, "InOversize:     %12llu\t"
+			   "Single:         %12llu\n",
+			   cvm_oct_stats_read_switch(dev, 0x1b, 0x1A),
+			   cvm_oct_stats_read_switch(dev, 0x1b, 0x14));
 
-        seq_printf(m, "In RxErr:       %12llu\t"
-                   "OutFCSErr:      %12llu\n",
-                   cvm_oct_stats_read_switch(dev, 0x1b, 0x1C),
-                   cvm_oct_stats_read_switch(dev, 0x1b, 0x03));
+		seq_printf(m, "InJabber:       %12llu\t"
+			   "Multiple:       %12llu\n",
+			   cvm_oct_stats_read_switch(dev, 0x1b, 0x1B),
+			   cvm_oct_stats_read_switch(dev, 0x1b, 0x17));
 
-        seq_printf(m, "InFCSErr:       %12llu\t"
-                   "Late:           %12llu\n",
-                   cvm_oct_stats_read_switch(dev, 0x1b, 0x1D),
-                   cvm_oct_stats_read_switch(dev, 0x1b, 0x1F));
-        index++;
-    }
-    return 0;
+		seq_printf(m, "In RxErr:       %12llu\t"
+			   "OutFCSErr:      %12llu\n",
+			   cvm_oct_stats_read_switch(dev, 0x1b, 0x1C),
+			   cvm_oct_stats_read_switch(dev, 0x1b, 0x03));
+
+		seq_printf(m, "InFCSErr:       %12llu\t"
+			   "Late:           %12llu\n",
+			   cvm_oct_stats_read_switch(dev, 0x1b, 0x1D),
+			   cvm_oct_stats_read_switch(dev, 0x1b, 0x1F));
+		index++;
+	}
+	return 0;
 }
 
 /**
@@ -158,75 +158,74 @@ static int cvm_oct_stats_switch_show(struct seq_file *m, void *v)
  */
 static int cvm_oct_stats_show(struct seq_file *m, void *v)
 {
-    cvm_oct_private_t* priv;
-    int port;
+	cvm_oct_private_t *priv;
+	int port;
+
+	for (port = 0; port < TOTAL_NUMBER_OF_PORTS; port++) {
+
+		if (cvm_oct_device[port]) {
 
-    for (port=0; port<TOTAL_NUMBER_OF_PORTS; port++)
-    {
-        if (cvm_oct_device[port])
-        {
-            priv = (cvm_oct_private_t*)netdev_priv(cvm_oct_device[port]);
-            seq_printf(m, "\nOcteon Port %d (%s)\n", port, cvm_oct_device[port]->name);
-            seq_printf(m,
-                       "rx_packets:             %12lu\t"
-                       "tx_packets:             %12lu\n",
-                       priv->stats.rx_packets,
-                       priv->stats.tx_packets);
-            seq_printf(m,
-                       "rx_bytes:               %12lu\t"
-                       "tx_bytes:               %12lu\n",
-                       priv->stats.rx_bytes,
-                       priv->stats.tx_bytes);
-            seq_printf(m,
-                       "rx_errors:              %12lu\t"
-                       "tx_errors:              %12lu\n",
-                       priv->stats.rx_errors,
-                       priv->stats.tx_errors);
-            seq_printf(m,
-                       "rx_dropped:             %12lu\t"
-                       "tx_dropped:             %12lu\n",
-                       priv->stats.rx_dropped,
-                       priv->stats.tx_dropped);
-            seq_printf(m,
-                       "rx_length_errors:       %12lu\t"
-                       "tx_aborted_errors:      %12lu\n",
-                       priv->stats.rx_length_errors,
-                       priv->stats.tx_aborted_errors);
-            seq_printf(m,
-                       "rx_over_errors:         %12lu\t"
-                       "tx_carrier_errors:      %12lu\n",
-                       priv->stats.rx_over_errors,
-                       priv->stats.tx_carrier_errors);
-            seq_printf(m,
-                       "rx_crc_errors:          %12lu\t"
-                       "tx_fifo_errors:         %12lu\n",
-                       priv->stats.rx_crc_errors,
-                       priv->stats.tx_fifo_errors);
-            seq_printf(m,
-                       "rx_frame_errors:        %12lu\t"
-                       "tx_heartbeat_errors:    %12lu\n",
-                       priv->stats.rx_frame_errors,
-                       priv->stats.tx_heartbeat_errors);
-            seq_printf(m,
-                       "rx_fifo_errors:         %12lu\t"
-                       "tx_window_errors:       %12lu\n",
-                       priv->stats.rx_fifo_errors,
-                       priv->stats.tx_window_errors);
-            seq_printf(m,
-                       "rx_missed_errors:       %12lu\t"
-                       "multicast:              %12lu\n",
-                       priv->stats.rx_missed_errors,
-                       priv->stats.multicast);
-        }
-    }
+			priv = (cvm_oct_private_t *)netdev_priv(cvm_oct_device[port]);
+			seq_printf(m, "\nOcteon Port %d (%s)\n", port, cvm_oct_device[port]->name);
+			seq_printf(m,
+				   "rx_packets:             %12lu\t"
+				   "tx_packets:             %12lu\n",
+				   priv->stats.rx_packets,
+				   priv->stats.tx_packets);
+			seq_printf(m,
+				   "rx_bytes:               %12lu\t"
+				   "tx_bytes:               %12lu\n",
+				   priv->stats.rx_bytes,
+				   priv->stats.tx_bytes);
+			seq_printf(m,
+				   "rx_errors:              %12lu\t"
+				   "tx_errors:              %12lu\n",
+				   priv->stats.rx_errors,
+				   priv->stats.tx_errors);
+			seq_printf(m,
+				   "rx_dropped:             %12lu\t"
+				   "tx_dropped:             %12lu\n",
+				   priv->stats.rx_dropped,
+				   priv->stats.tx_dropped);
+			seq_printf(m,
+				   "rx_length_errors:       %12lu\t"
+				   "tx_aborted_errors:      %12lu\n",
+				   priv->stats.rx_length_errors,
+				   priv->stats.tx_aborted_errors);
+			seq_printf(m,
+				   "rx_over_errors:         %12lu\t"
+				   "tx_carrier_errors:      %12lu\n",
+				   priv->stats.rx_over_errors,
+				   priv->stats.tx_carrier_errors);
+			seq_printf(m,
+				   "rx_crc_errors:          %12lu\t"
+				   "tx_fifo_errors:         %12lu\n",
+				   priv->stats.rx_crc_errors,
+				   priv->stats.tx_fifo_errors);
+			seq_printf(m,
+				   "rx_frame_errors:        %12lu\t"
+				   "tx_heartbeat_errors:    %12lu\n",
+				   priv->stats.rx_frame_errors,
+				   priv->stats.tx_heartbeat_errors);
+			seq_printf(m,
+				   "rx_fifo_errors:         %12lu\t"
+				   "tx_window_errors:       %12lu\n",
+				   priv->stats.rx_fifo_errors,
+				   priv->stats.tx_window_errors);
+			seq_printf(m,
+				   "rx_missed_errors:       %12lu\t"
+				   "multicast:              %12lu\n",
+				   priv->stats.rx_missed_errors,
+				   priv->stats.multicast);
+		}
+	}
 
-    if (cvm_oct_device[0])
-    {
-        priv = (cvm_oct_private_t*)netdev_priv(cvm_oct_device[0]);
-        if (priv->imode == CVMX_HELPER_INTERFACE_MODE_GMII)
-            cvm_oct_stats_switch_show(m, v);
-    }
-    return 0;
+	if (cvm_oct_device[0]) {
+		priv = (cvm_oct_private_t *)netdev_priv(cvm_oct_device[0]);
+		if (priv->imode == CVMX_HELPER_INTERFACE_MODE_GMII)
+			cvm_oct_stats_switch_show(m, v);
+	}
+	return 0;
 }
 
 
@@ -253,13 +252,13 @@ static struct file_operations cvm_oct_stats_operations = {
 
 void cvm_oct_proc_initialize(void)
 {
-    struct proc_dir_entry *entry = create_proc_entry("octeon_ethernet_stats", 0, NULL);
-    if (entry)
-        entry->proc_fops = &cvm_oct_stats_operations;
+	struct proc_dir_entry *entry = create_proc_entry("octeon_ethernet_stats", 0, NULL);
+	if (entry)
+		entry->proc_fops = &cvm_oct_stats_operations;
 }
 
 void cvm_oct_proc_shutdown(void)
 {
-    remove_proc_entry("octeon_ethernet_stats", NULL);
+	remove_proc_entry("octeon_ethernet_stats", NULL);
 }
 
diff --git a/drivers/net/cavium-ethernet/ethernet-rgmii.c b/drivers/net/cavium-ethernet/ethernet-rgmii.c
index ea53a1c..e01787e 100644
--- a/drivers/net/cavium-ethernet/ethernet-rgmii.c
+++ b/drivers/net/cavium-ethernet/ethernet-rgmii.c
@@ -52,275 +52,278 @@
 
 extern int octeon_is_simulation(void);
 extern struct net_device *cvm_oct_device[];
+
 DEFINE_SPINLOCK(global_register_lock);
 
-static int number_rgmii_ports = 0;
+static int number_rgmii_ports;
 
 static void cvm_oct_rgmii_poll(struct net_device *dev)
 {
-    cvm_oct_private_t* priv = (cvm_oct_private_t*)netdev_priv(dev);
-    unsigned long flags;
-    cvmx_helper_link_info_t link_info;
-
-    /* Take the global register lock since we are going to touch
-        registers that affect more than one port */
-    spin_lock_irqsave(&global_register_lock, flags);
-
-    link_info = cvmx_helper_link_get(priv->port);
-    if (link_info.u64 == priv->link_info)
-    {
-        /* If the 10Mbps preamble workaround is supported and we're at 10Mbps
-            we may need to do some special checking */
-        if (USE_10MBPS_PREAMBLE_WORKAROUND && (link_info.s.speed == 10))
-        {
-            /* Read the GMXX_RXX_INT_REG[PCTERR] bit and see if we are getting
-                preamble errors */
-            int interface = INTERFACE(priv->port);
-            int index = INDEX(priv->port);
-            cvmx_gmxx_rxx_int_reg_t gmxx_rxx_int_reg;
-            gmxx_rxx_int_reg.u64 = cvmx_read_csr(CVMX_GMXX_RXX_INT_REG(index, interface));
-            if (gmxx_rxx_int_reg.s.pcterr)
-            {
-                /* We are getting preamble errors at 10Mbps. Most likely the
-                    PHY is giving us packets with mis aligned preambles. In
-                    order to get these packets we need to disable preamble
-                    checking and do it in software */
-                cvmx_gmxx_rxx_frm_ctl_t gmxx_rxx_frm_ctl;
-                cvmx_ipd_sub_port_fcs_t ipd_sub_port_fcs;
-
-                /* Disable preamble checking */
-                gmxx_rxx_frm_ctl.u64 = cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(index, interface));
-                gmxx_rxx_frm_ctl.s.pre_chk = 0;
-                cvmx_write_csr(CVMX_GMXX_RXX_FRM_CTL(index, interface), gmxx_rxx_frm_ctl.u64);
-                /* Disable FCS stripping */
-                ipd_sub_port_fcs.u64 = cvmx_read_csr(CVMX_IPD_SUB_PORT_FCS);
-                ipd_sub_port_fcs.s.port_bit &= 0xffffffffull ^ (1ull<<priv->port);
-                cvmx_write_csr(CVMX_IPD_SUB_PORT_FCS, ipd_sub_port_fcs.u64);
-                /* Clear any error bits */
-                cvmx_write_csr(CVMX_GMXX_RXX_INT_REG(index, interface), gmxx_rxx_int_reg.u64);
-                DEBUGPRINT("%s: Using 10Mbps with software preamble removal\n", dev->name);
-            }
-        }
-        spin_unlock_irqrestore(&global_register_lock, flags);
-        return;
-    }
-
-    /* If the 10Mbps preamble workaround is allowed we need to on preamble checking,
-        FCS stripping, and clear error bits on every speed change. If errors occur
-        during 10Mbps operation the above cod will change this stuff */
-    if (USE_10MBPS_PREAMBLE_WORKAROUND)
-    {
-        cvmx_gmxx_rxx_frm_ctl_t gmxx_rxx_frm_ctl;
-        cvmx_ipd_sub_port_fcs_t ipd_sub_port_fcs;
-        cvmx_gmxx_rxx_int_reg_t gmxx_rxx_int_reg;
-        int interface = INTERFACE(priv->port);
-        int index = INDEX(priv->port);
-
-        /* Enable preamble checking */
-        gmxx_rxx_frm_ctl.u64 = cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(index, interface));
-        gmxx_rxx_frm_ctl.s.pre_chk = 1;
-        cvmx_write_csr(CVMX_GMXX_RXX_FRM_CTL(index, interface), gmxx_rxx_frm_ctl.u64);
-        /* Enable FCS stripping */
-        ipd_sub_port_fcs.u64 = cvmx_read_csr(CVMX_IPD_SUB_PORT_FCS);
-        ipd_sub_port_fcs.s.port_bit |= 1ull<<priv->port;
-        cvmx_write_csr(CVMX_IPD_SUB_PORT_FCS, ipd_sub_port_fcs.u64);
-        /* Clear any error bits */
-        gmxx_rxx_int_reg.u64 = cvmx_read_csr(CVMX_GMXX_RXX_INT_REG(index, interface));
-        cvmx_write_csr(CVMX_GMXX_RXX_INT_REG(index, interface), gmxx_rxx_int_reg.u64);
-    }
-
-    link_info = cvmx_helper_link_autoconf(priv->port);
-    priv->link_info = link_info.u64;
-    spin_unlock_irqrestore(&global_register_lock, flags);
-
-    /* Tell Linux */
-    if (link_info.s.link_up)
-    {
-        if (!netif_carrier_ok(dev))
-            netif_carrier_on(dev);
-        if (priv->queue != -1)
-            DEBUGPRINT("%s: %u Mbps %s duplex, port %2d, queue %2d\n",
-                       dev->name, link_info.s.speed,
-                       (link_info.s.full_duplex) ? "Full" : "Half",
-                       priv->port, priv->queue);
-        else
-            DEBUGPRINT("%s: %u Mbps %s duplex, port %2d, POW\n",
-                       dev->name, link_info.s.speed,
-                       (link_info.s.full_duplex) ? "Full" : "Half",
-                       priv->port);
-    }
-    else
-    {
-        if (netif_carrier_ok(dev))
-            netif_carrier_off(dev);
-        DEBUGPRINT("%s: Link down\n", dev->name);
-    }
+	cvm_oct_private_t *priv = (cvm_oct_private_t *)netdev_priv(dev);
+	unsigned long flags;
+	cvmx_helper_link_info_t link_info;
+
+	/* Take the global register lock since we are going to touch
+	   registers that affect more than one port */
+	spin_lock_irqsave(&global_register_lock, flags);
+
+	link_info = cvmx_helper_link_get(priv->port);
+	if (link_info.u64 == priv->link_info) {
+
+		/* If the 10Mbps preamble workaround is supported and we're
+		   at 10Mbps we may need to do some special checking */
+		if (USE_10MBPS_PREAMBLE_WORKAROUND && (link_info.s.speed == 10)) {
+
+			/* Read the GMXX_RXX_INT_REG[PCTERR] bit and
+			   see if we are getting preamble errors */
+			int interface = INTERFACE(priv->port);
+			int index = INDEX(priv->port);
+			cvmx_gmxx_rxx_int_reg_t gmxx_rxx_int_reg;
+			gmxx_rxx_int_reg.u64 = cvmx_read_csr(CVMX_GMXX_RXX_INT_REG(index, interface));
+			if (gmxx_rxx_int_reg.s.pcterr) {
+
+				/* We are getting preamble errors at 10Mbps.
+				   Most likely the PHY is giving us packets
+				   with mis aligned preambles. In order to get
+				   these packets we need to disable preamble
+				   checking and do it in software */
+				cvmx_gmxx_rxx_frm_ctl_t gmxx_rxx_frm_ctl;
+				cvmx_ipd_sub_port_fcs_t ipd_sub_port_fcs;
+
+				/* Disable preamble checking */
+				gmxx_rxx_frm_ctl.u64 = cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(index, interface));
+				gmxx_rxx_frm_ctl.s.pre_chk = 0;
+				cvmx_write_csr(CVMX_GMXX_RXX_FRM_CTL(index, interface), gmxx_rxx_frm_ctl.u64);
+
+				/* Disable FCS stripping */
+				ipd_sub_port_fcs.u64 = cvmx_read_csr(CVMX_IPD_SUB_PORT_FCS);
+				ipd_sub_port_fcs.s.port_bit &= 0xffffffffull ^ (1ull<<priv->port);
+				cvmx_write_csr(CVMX_IPD_SUB_PORT_FCS, ipd_sub_port_fcs.u64);
+
+				/* Clear any error bits */
+				cvmx_write_csr(CVMX_GMXX_RXX_INT_REG(index, interface), gmxx_rxx_int_reg.u64);
+				DEBUGPRINT("%s: Using 10Mbps with software preamble removal\n", dev->name);
+			}
+		}
+		spin_unlock_irqrestore(&global_register_lock, flags);
+		return;
+	}
+
+	/* If the 10Mbps preamble workaround is allowed we need to on
+	   preamble checking, FCS stripping, and clear error bits on
+	   every speed change. If errors occur during 10Mbps operation
+	   the above code will change this stuff */
+	if (USE_10MBPS_PREAMBLE_WORKAROUND) {
+
+		cvmx_gmxx_rxx_frm_ctl_t gmxx_rxx_frm_ctl;
+		cvmx_ipd_sub_port_fcs_t ipd_sub_port_fcs;
+		cvmx_gmxx_rxx_int_reg_t gmxx_rxx_int_reg;
+		int interface = INTERFACE(priv->port);
+		int index = INDEX(priv->port);
+
+		/* Enable preamble checking */
+		gmxx_rxx_frm_ctl.u64 = cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(index, interface));
+		gmxx_rxx_frm_ctl.s.pre_chk = 1;
+		cvmx_write_csr(CVMX_GMXX_RXX_FRM_CTL(index, interface), gmxx_rxx_frm_ctl.u64);
+		/* Enable FCS stripping */
+		ipd_sub_port_fcs.u64 = cvmx_read_csr(CVMX_IPD_SUB_PORT_FCS);
+		ipd_sub_port_fcs.s.port_bit |= 1ull<<priv->port;
+		cvmx_write_csr(CVMX_IPD_SUB_PORT_FCS, ipd_sub_port_fcs.u64);
+		/* Clear any error bits */
+		gmxx_rxx_int_reg.u64 = cvmx_read_csr(CVMX_GMXX_RXX_INT_REG(index, interface));
+		cvmx_write_csr(CVMX_GMXX_RXX_INT_REG(index, interface), gmxx_rxx_int_reg.u64);
+	}
+
+	link_info = cvmx_helper_link_autoconf(priv->port);
+	priv->link_info = link_info.u64;
+	spin_unlock_irqrestore(&global_register_lock, flags);
+
+	/* Tell Linux */
+	if (link_info.s.link_up) {
+
+		if (!netif_carrier_ok(dev))
+			netif_carrier_on(dev);
+		if (priv->queue != -1)
+			DEBUGPRINT("%s: %u Mbps %s duplex, port %2d, queue %2d\n",
+				   dev->name, link_info.s.speed,
+				   (link_info.s.full_duplex) ? "Full" : "Half",
+				   priv->port, priv->queue);
+		else
+			DEBUGPRINT("%s: %u Mbps %s duplex, port %2d, POW\n",
+				   dev->name, link_info.s.speed,
+				   (link_info.s.full_duplex) ? "Full" : "Half",
+				   priv->port);
+	} else {
+
+		if (netif_carrier_ok(dev))
+			netif_carrier_off(dev);
+		DEBUGPRINT("%s: Link down\n", dev->name);
+	}
 }
 
 
 static irqreturn_t cvm_oct_rgmii_rml_interrupt(int cpl, void *dev_id)
 {
-    cvmx_npi_rsl_int_blocks_t rsl_int_blocks;
-    int index;
-    irqreturn_t return_status = IRQ_NONE;
-
-    rsl_int_blocks.u64 = cvmx_read_csr(CVMX_NPI_RSL_INT_BLOCKS);
-
-    /* Check and see if this interrupt was caused by the GMX0 block */
-    if (rsl_int_blocks.s.gmx0)
-    {
-        int interface = 0;
-        /* Loop through every port of this interface */
-        for (index=0; index<cvmx_helper_ports_on_interface(interface); index++)
-        {
-            /* Read the GMX interrupt status bits */
-            cvmx_gmxx_rxx_int_reg_t gmx_rx_int_reg;
-            gmx_rx_int_reg.u64 = cvmx_read_csr(CVMX_GMXX_RXX_INT_REG(index, interface));
-            gmx_rx_int_reg.u64 &= cvmx_read_csr(CVMX_GMXX_RXX_INT_EN(index, interface));
-            /* Poll the port if inband status changed */
-            if (gmx_rx_int_reg.s.phy_dupx || gmx_rx_int_reg.s.phy_link || gmx_rx_int_reg.s.phy_spd)
-            {
-                struct net_device *dev = cvm_oct_device[cvmx_helper_get_ipd_port(interface, index)];
-                if (dev)
-                    cvm_oct_rgmii_poll(dev);
-                gmx_rx_int_reg.u64 = 0;
-                gmx_rx_int_reg.s.phy_dupx = 1;
-                gmx_rx_int_reg.s.phy_link = 1;
-                gmx_rx_int_reg.s.phy_spd = 1;
-                cvmx_write_csr(CVMX_GMXX_RXX_INT_REG(index, interface), gmx_rx_int_reg.u64);
-                return_status = IRQ_HANDLED;
-            }
-        }
-    }
-
-    /* Check and see if this interrupt was caused by the GMX1 block */
-    if (rsl_int_blocks.s.gmx1)
-    {
-        int interface = 1;
-        /* Loop through every port of this interface */
-        for (index=0; index<cvmx_helper_ports_on_interface(interface); index++)
-        {
-            /* Read the GMX interrupt status bits */
-            cvmx_gmxx_rxx_int_reg_t gmx_rx_int_reg;
-            gmx_rx_int_reg.u64 = cvmx_read_csr(CVMX_GMXX_RXX_INT_REG(index, interface));
-            gmx_rx_int_reg.u64 &= cvmx_read_csr(CVMX_GMXX_RXX_INT_EN(index, interface));
-            /* Poll the port if inband status changed */
-            if (gmx_rx_int_reg.s.phy_dupx || gmx_rx_int_reg.s.phy_link || gmx_rx_int_reg.s.phy_spd)
-            {
-                struct net_device *dev = cvm_oct_device[cvmx_helper_get_ipd_port(interface, index)];
-                if (dev)
-                    cvm_oct_rgmii_poll(dev);
-                gmx_rx_int_reg.u64 = 0;
-                gmx_rx_int_reg.s.phy_dupx = 1;
-                gmx_rx_int_reg.s.phy_link = 1;
-                gmx_rx_int_reg.s.phy_spd = 1;
-                cvmx_write_csr(CVMX_GMXX_RXX_INT_REG(index, interface), gmx_rx_int_reg.u64);
-                return_status = IRQ_HANDLED;
-            }
-        }
-    }
-    return return_status;
+	cvmx_npi_rsl_int_blocks_t rsl_int_blocks;
+	int index;
+	irqreturn_t return_status = IRQ_NONE;
+
+	rsl_int_blocks.u64 = cvmx_read_csr(CVMX_NPI_RSL_INT_BLOCKS);
+
+	/* Check and see if this interrupt was caused by the GMX0 block */
+	if (rsl_int_blocks.s.gmx0) {
+
+		int interface = 0;
+		/* Loop through every port of this interface */
+		for (index = 0; index < cvmx_helper_ports_on_interface(interface); index++) {
+
+			/* Read the GMX interrupt status bits */
+			cvmx_gmxx_rxx_int_reg_t gmx_rx_int_reg;
+			gmx_rx_int_reg.u64 = cvmx_read_csr(CVMX_GMXX_RXX_INT_REG(index, interface));
+			gmx_rx_int_reg.u64 &= cvmx_read_csr(CVMX_GMXX_RXX_INT_EN(index, interface));
+			/* Poll the port if inband status changed */
+			if (gmx_rx_int_reg.s.phy_dupx || gmx_rx_int_reg.s.phy_link || gmx_rx_int_reg.s.phy_spd) {
+
+				struct net_device *dev = cvm_oct_device[cvmx_helper_get_ipd_port(interface, index)];
+				if (dev)
+					cvm_oct_rgmii_poll(dev);
+				gmx_rx_int_reg.u64 = 0;
+				gmx_rx_int_reg.s.phy_dupx = 1;
+				gmx_rx_int_reg.s.phy_link = 1;
+				gmx_rx_int_reg.s.phy_spd = 1;
+				cvmx_write_csr(CVMX_GMXX_RXX_INT_REG(index, interface), gmx_rx_int_reg.u64);
+				return_status = IRQ_HANDLED;
+			}
+		}
+	}
+
+	/* Check and see if this interrupt was caused by the GMX1 block */
+	if (rsl_int_blocks.s.gmx1) {
+
+		int interface = 1;
+		/* Loop through every port of this interface */
+		for (index = 0; index < cvmx_helper_ports_on_interface(interface); index++) {
+
+			/* Read the GMX interrupt status bits */
+			cvmx_gmxx_rxx_int_reg_t gmx_rx_int_reg;
+			gmx_rx_int_reg.u64 = cvmx_read_csr(CVMX_GMXX_RXX_INT_REG(index, interface));
+			gmx_rx_int_reg.u64 &= cvmx_read_csr(CVMX_GMXX_RXX_INT_EN(index, interface));
+			/* Poll the port if inband status changed */
+			if (gmx_rx_int_reg.s.phy_dupx || gmx_rx_int_reg.s.phy_link || gmx_rx_int_reg.s.phy_spd) {
+
+				struct net_device *dev = cvm_oct_device[cvmx_helper_get_ipd_port(interface, index)];
+				if (dev)
+					cvm_oct_rgmii_poll(dev);
+				gmx_rx_int_reg.u64 = 0;
+				gmx_rx_int_reg.s.phy_dupx = 1;
+				gmx_rx_int_reg.s.phy_link = 1;
+				gmx_rx_int_reg.s.phy_spd = 1;
+				cvmx_write_csr(CVMX_GMXX_RXX_INT_REG(index, interface), gmx_rx_int_reg.u64);
+				return_status = IRQ_HANDLED;
+			}
+		}
+	}
+	return return_status;
 }
 
 
 static int cvm_oct_rgmii_open(struct net_device *dev)
 {
-    cvmx_gmxx_prtx_cfg_t gmx_cfg;
-    cvm_oct_private_t* priv = (cvm_oct_private_t*)netdev_priv(dev);
-    int interface = INTERFACE(priv->port);
-    int index = INDEX(priv->port);
-
-    gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
-    gmx_cfg.s.en = 1;
-    cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmx_cfg.u64);
-    return 0;
+	cvmx_gmxx_prtx_cfg_t gmx_cfg;
+	cvm_oct_private_t *priv = (cvm_oct_private_t *)netdev_priv(dev);
+	int interface = INTERFACE(priv->port);
+	int index = INDEX(priv->port);
+
+	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
+	gmx_cfg.s.en = 1;
+	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmx_cfg.u64);
+	return 0;
 }
 
 static int cvm_oct_rgmii_stop(struct net_device *dev)
 {
-    cvmx_gmxx_prtx_cfg_t gmx_cfg;
-    cvm_oct_private_t* priv = (cvm_oct_private_t*)netdev_priv(dev);
-    int interface = INTERFACE(priv->port);
-    int index = INDEX(priv->port);
-
-    gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
-    gmx_cfg.s.en = 0;
-    cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmx_cfg.u64);
-    return 0;
+	cvmx_gmxx_prtx_cfg_t gmx_cfg;
+	cvm_oct_private_t *priv = (cvm_oct_private_t *)netdev_priv(dev);
+	int interface = INTERFACE(priv->port);
+	int index = INDEX(priv->port);
+
+	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
+	gmx_cfg.s.en = 0;
+	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmx_cfg.u64);
+	return 0;
 }
 
 int cvm_oct_rgmii_init(struct net_device *dev)
 {
-    cvm_oct_private_t* priv = (cvm_oct_private_t*)netdev_priv(dev);
-
-    cvm_oct_common_init(dev);
-    dev->open = cvm_oct_rgmii_open;
-    dev->stop = cvm_oct_rgmii_stop;
-    dev->stop(dev);
-
-    /* Due to GMX errata in CN3XXX series chips, it is necessary to take the
-        link down immediately whne the PHY changes state. In order to do this
-        we call the poll function every time the RGMII inband status changes.
-        This may cause problems if the PHY doesn't implement inband status
-        properly */
-    if (number_rgmii_ports == 0)
-    {
-        request_irq(OCTEON_IRQ_RML, cvm_oct_rgmii_rml_interrupt, IRQF_SHARED, "RGMII", &number_rgmii_ports);
-    }
-    number_rgmii_ports++;
-
-    /* Only true RGMII ports need to be polled. In GMII mode, port 0 is really
-        a RGMII port */
-    if (((priv->imode == CVMX_HELPER_INTERFACE_MODE_GMII) && (priv->port == 0)) ||
-        (priv->imode == CVMX_HELPER_INTERFACE_MODE_RGMII))
-    {
-        if (!octeon_is_simulation())
-        {
-            cvmx_gmxx_rxx_int_en_t gmx_rx_int_en;
-            int interface = INTERFACE(priv->port);
-            int index = INDEX(priv->port);
-
-            /* Enable interrupts on inband status changes for this port */
-            gmx_rx_int_en.u64 = cvmx_read_csr(CVMX_GMXX_RXX_INT_EN(index, interface));
-            gmx_rx_int_en.s.phy_dupx = 1;
-            gmx_rx_int_en.s.phy_link = 1;
-            gmx_rx_int_en.s.phy_spd = 1;
-            cvmx_write_csr(CVMX_GMXX_RXX_INT_EN(index, interface), gmx_rx_int_en.u64);
-            priv->poll = cvm_oct_rgmii_poll;
-        }
-    }
-
-    return 0;
+	cvm_oct_private_t *priv = (cvm_oct_private_t *)netdev_priv(dev);
+
+	cvm_oct_common_init(dev);
+	dev->open = cvm_oct_rgmii_open;
+	dev->stop = cvm_oct_rgmii_stop;
+	dev->stop(dev);
+
+	/* Due to GMX errata in CN3XXX series chips, it is necessary to take the
+	   link down immediately whne the PHY changes state. In order to do this
+	   we call the poll function every time the RGMII inband status changes.
+	   This may cause problems if the PHY doesn't implement inband status
+	   properly */
+	if (number_rgmii_ports == 0) {
+		request_irq(OCTEON_IRQ_RML, cvm_oct_rgmii_rml_interrupt, IRQF_SHARED, "RGMII", &number_rgmii_ports);
+	}
+	number_rgmii_ports++;
+
+	/* Only true RGMII ports need to be polled. In GMII mode, port 0 is really
+	   a RGMII port */
+	if (((priv->imode == CVMX_HELPER_INTERFACE_MODE_GMII) && (priv->port == 0)) ||
+	    (priv->imode == CVMX_HELPER_INTERFACE_MODE_RGMII)) {
+
+		if (!octeon_is_simulation()) {
+
+			cvmx_gmxx_rxx_int_en_t gmx_rx_int_en;
+			int interface = INTERFACE(priv->port);
+			int index = INDEX(priv->port);
+
+			/* Enable interrupts on inband status changes for this port */
+			gmx_rx_int_en.u64 = cvmx_read_csr(CVMX_GMXX_RXX_INT_EN(index, interface));
+			gmx_rx_int_en.s.phy_dupx = 1;
+			gmx_rx_int_en.s.phy_link = 1;
+			gmx_rx_int_en.s.phy_spd = 1;
+			cvmx_write_csr(CVMX_GMXX_RXX_INT_EN(index, interface), gmx_rx_int_en.u64);
+			priv->poll = cvm_oct_rgmii_poll;
+		}
+	}
+
+	return 0;
 }
 
 void cvm_oct_rgmii_uninit(struct net_device *dev)
 {
-    cvm_oct_private_t* priv = (cvm_oct_private_t*)netdev_priv(dev);
-    cvm_oct_common_uninit(dev);
-
-    /* Only true RGMII ports need to be polled. In GMII mode, port 0 is really
-        a RGMII port */
-    if (((priv->imode == CVMX_HELPER_INTERFACE_MODE_GMII) && (priv->port == 0)) ||
-        (priv->imode == CVMX_HELPER_INTERFACE_MODE_RGMII))
-    {
-        if (!octeon_is_simulation())
-        {
-            cvmx_gmxx_rxx_int_en_t gmx_rx_int_en;
-            int interface = INTERFACE(priv->port);
-            int index = INDEX(priv->port);
-
-            /* Disable interrupts on inband status changes for this port */
-            gmx_rx_int_en.u64 = cvmx_read_csr(CVMX_GMXX_RXX_INT_EN(index, interface));
-            gmx_rx_int_en.s.phy_dupx = 0;
-            gmx_rx_int_en.s.phy_link = 0;
-            gmx_rx_int_en.s.phy_spd = 0;
-            cvmx_write_csr(CVMX_GMXX_RXX_INT_EN(index, interface), gmx_rx_int_en.u64);
-        }
-    }
-
-    /* Remove the interrupt handler when the last port is removed */
-    number_rgmii_ports--;
-    if (number_rgmii_ports == 0)
-        free_irq(OCTEON_IRQ_RML, &number_rgmii_ports);
+	cvm_oct_private_t *priv = (cvm_oct_private_t *)netdev_priv(dev);
+	cvm_oct_common_uninit(dev);
+
+	/* Only true RGMII ports need to be polled. In GMII mode, port 0 is really
+	   a RGMII port */
+	if (((priv->imode == CVMX_HELPER_INTERFACE_MODE_GMII) && (priv->port == 0)) ||
+	    (priv->imode == CVMX_HELPER_INTERFACE_MODE_RGMII)) {
+
+		if (!octeon_is_simulation()) {
+
+			cvmx_gmxx_rxx_int_en_t gmx_rx_int_en;
+			int interface = INTERFACE(priv->port);
+			int index = INDEX(priv->port);
+
+			/* Disable interrupts on inband status changes for this port */
+			gmx_rx_int_en.u64 = cvmx_read_csr(CVMX_GMXX_RXX_INT_EN(index, interface));
+			gmx_rx_int_en.s.phy_dupx = 0;
+			gmx_rx_int_en.s.phy_link = 0;
+			gmx_rx_int_en.s.phy_spd = 0;
+			cvmx_write_csr(CVMX_GMXX_RXX_INT_EN(index, interface), gmx_rx_int_en.u64);
+		}
+	}
+
+	/* Remove the interrupt handler when the last port is removed */
+	number_rgmii_ports--;
+	if (number_rgmii_ports == 0)
+		free_irq(OCTEON_IRQ_RML, &number_rgmii_ports);
 }
 
diff --git a/drivers/net/cavium-ethernet/ethernet-rx.c b/drivers/net/cavium-ethernet/ethernet-rx.c
index e8058f8..8ce7ed0 100644
--- a/drivers/net/cavium-ethernet/ethernet-rx.c
+++ b/drivers/net/cavium-ethernet/ethernet-rx.c
@@ -77,15 +77,15 @@ static struct tasklet_struct cvm_oct_tasklet[NR_CPUS];
  */
 irqreturn_t cvm_oct_do_interrupt(int cpl, void *dev_id)
 {
-    /* Acknowledge the interrupt */
-    if (INTERRUPT_LIMIT)
-        cvmx_write_csr(CVMX_POW_WQ_INT, 1<<pow_receive_group);
-    else
-        cvmx_write_csr(CVMX_POW_WQ_INT, 0x10001<<pow_receive_group);
-    preempt_disable();
-    tasklet_schedule(cvm_oct_tasklet + smp_processor_id());
-    preempt_enable();
-    return IRQ_HANDLED;
+	/* Acknowledge the interrupt */
+	if (INTERRUPT_LIMIT)
+		cvmx_write_csr(CVMX_POW_WQ_INT, 1<<pow_receive_group);
+	else
+		cvmx_write_csr(CVMX_POW_WQ_INT, 0x10001<<pow_receive_group);
+	preempt_disable();
+	tasklet_schedule(cvm_oct_tasklet + smp_processor_id());
+	preempt_enable();
+	return IRQ_HANDLED;
 }
 
 
@@ -100,12 +100,83 @@ irqreturn_t cvm_oct_do_interrupt(int cpl, void *dev_id)
  */
 void cvm_oct_poll_controller(struct net_device *dev)
 {
-    preempt_disable();
-    tasklet_schedule(cvm_oct_tasklet + smp_processor_id());
-    preempt_enable();
+	preempt_disable();
+	tasklet_schedule(cvm_oct_tasklet + smp_processor_id());
+	preempt_enable();
 }
 #endif
 
+/**
+ * This is called on receive errors, and determines if the packet
+ * can be dropped early-on in cvm_oct_tasklet_rx().
+ *
+ * @param work Work queue entry pointing to the packet.
+ * @return Non-zero if the packet can be dropped, zero otherwise.
+ */
+static inline int cvm_oct_check_rcv_error(cvmx_wqe_t *work)
+{
+	if ((work->word2.snoip.err_code == 10) && (work->len <= 64)) {
+		/* Ignore length errors on min size packets. Some equipment
+		   incorrectly pads packets to 64+4FCS instead of 60+4FCS.
+		   Note these packets still get counted as frame errors. */
+	} else
+	if (USE_10MBPS_PREAMBLE_WORKAROUND && ((work->word2.snoip.err_code == 5) || (work->word2.snoip.err_code == 7))) {
+
+		/* We received a packet with either an alignment error or a
+		   FCS error. This may be signalling that we are running
+		   10Mbps with GMXX_RXX_FRM_CTL[PRE_CHK} off. If this is the
+		   case we need to parse the packet to determine if we can
+		   remove a non spec preamble and generate a correct packet */
+		int interface = cvmx_helper_get_interface_num(work->ipprt);
+		int index = cvmx_helper_get_interface_index_num(work->ipprt);
+		cvmx_gmxx_rxx_frm_ctl_t gmxx_rxx_frm_ctl;
+		gmxx_rxx_frm_ctl.u64 = cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(index, interface));
+		if (gmxx_rxx_frm_ctl.s.pre_chk == 0) {
+
+			uint8_t *ptr = cvmx_phys_to_ptr(work->packet_ptr.s.addr);
+			int i = 0;
+
+			while (i < work->len-1) {
+				if (*ptr != 0x55)
+					break;
+				ptr++;
+				i++;
+			}
+
+			if (*ptr == 0xd5) {
+				/*
+				DEBUGPRINT("Port %d received 0xd5 preamble\n", work->ipprt);
+				*/
+				work->packet_ptr.s.addr += i+1;
+				work->len -= i+5;
+			} else
+			if ((*ptr & 0xf) == 0xd) {
+				/*
+				DEBUGPRINT("Port %d received 0x?d preamble\n", work->ipprt);
+				*/
+				work->packet_ptr.s.addr += i;
+				work->len -= i+4;
+				for (i = 0; i < work->len; i++) {
+					*ptr = ((*ptr&0xf0)>>4) | ((*(ptr+1)&0xf)<<4);
+					ptr++;
+				}
+			} else {
+				DEBUGPRINT("Port %d unknown preamble, packet dropped\n", work->ipprt);
+				/*
+				cvmx_helper_dump_packet(work);
+				*/
+				cvm_oct_free_work(work);
+				return 1;
+			}
+		}
+	} else {
+		DEBUGPRINT("Port %d receive error code %d, packet dropped\n", work->ipprt, work->word2.snoip.err_code);
+		cvm_oct_free_work(work);
+		return 1;
+	}
+
+	return 0;
+}
 
 /**
  * Tasklet function that is scheduled on a core when an interrupt occurs.
@@ -114,340 +185,278 @@ void cvm_oct_poll_controller(struct net_device *dev)
  */
 void cvm_oct_tasklet_rx(unsigned long unused)
 {
-    const int           coreid = cvmx_get_core_num();
-    uint64_t            old_group_mask;
-    uint64_t            old_scratch;
-    int                 rx_count = 0;
-    int                 number_to_free;
-    int                 packet_not_copied;
-
-    /* Prefetch cvm_oct_device since we know we need it soon */
-    CVMX_PREFETCH(cvm_oct_device, 0);
-
-    if (USE_ASYNC_IOBDMA)
-    {
-        /* Save scratch in case userspace is using it */
-        CVMX_SYNCIOBDMA;
-        old_scratch = cvmx_scratch_read64(CVMX_SCR_SCRATCH);
-    }
-
-    /* Only allow work for our group (and preserve priorities) */
-    old_group_mask = cvmx_read_csr(CVMX_POW_PP_GRP_MSKX(coreid));
-    cvmx_write_csr(CVMX_POW_PP_GRP_MSKX(coreid),
-        (old_group_mask & ~0xFFFFull) | 1<<pow_receive_group);
-
-    if (USE_ASYNC_IOBDMA)
-        cvmx_pow_work_request_async(CVMX_SCR_SCRATCH, CVMX_POW_NO_WAIT);
-
-    while (1)
-    {
-        void *start_of_buffer;
-        struct sk_buff *skb;
-        cvm_oct_callback_result_t callback_result;
-        cvmx_wqe_t *work;
-        if (USE_ASYNC_IOBDMA)
-        {
-            work = cvmx_pow_work_response_async(CVMX_SCR_SCRATCH);
-        }
-        else
-        {
-            if ((INTERRUPT_LIMIT == 0) || likely(rx_count < 60))
-                work = cvmx_pow_work_request_sync(CVMX_POW_NO_WAIT);
-            else
-                work = NULL;
-        }
-        if (work == NULL)
-            break;
-
-        /* Limit each core to processing 60 packets without a break. This way
-            the RX can't starve the TX task. */
-        if (USE_ASYNC_IOBDMA)
-        {
-            if ((INTERRUPT_LIMIT == 0) || likely(rx_count < 60))
-                cvmx_pow_work_request_async_nocheck(CVMX_SCR_SCRATCH, CVMX_POW_NO_WAIT);
-            else
-            {
-                cvmx_scratch_write64(CVMX_SCR_SCRATCH, 0x8000000000000000ull);
-                cvmx_pow_tag_sw_null_nocheck();
-            }
-        }
-        rx_count++;
-
-        CVMX_PREFETCH(cvm_oct_device[work->ipprt], 0);
-        start_of_buffer = cvm_oct_get_buffer_ptr(work->packet_ptr);
-        CVMX_PREFETCH(start_of_buffer, -8); /* Should be -sizeof(void*), but you can't use that with this macro. */
-
-        /* Immediately throw away all packets with receive errors */
-        if (unlikely(work->word2.snoip.rcv_error))
-        {
-            if ((work->word2.snoip.err_code == 10) && (work->len <= 64))
-            {
-                /* Ignore length errors on min size packets. Some equipment
-                    incorrectly pads packets to 64+4FCS instead of 60+4FCS.
-                    Note these packets still get counted as frame errors. */
-            }
-            else if (USE_10MBPS_PREAMBLE_WORKAROUND && ((work->word2.snoip.err_code == 5) || (work->word2.snoip.err_code == 7)))
-            {
-                /* We received a packet with either an alignment error or a
-                    FCS error. This may be signalling that we are running
-                    10Mbps with GMXX_RXX_FRM_CTL[PRE_CHK} off. If this is the
-                    case we need to parse the packet to determine if we can
-                    remove a non spec preamble and generate a correct packet */
-                int interface = cvmx_helper_get_interface_num(work->ipprt);
-                int index = cvmx_helper_get_interface_index_num(work->ipprt);
-                cvmx_gmxx_rxx_frm_ctl_t gmxx_rxx_frm_ctl;
-                gmxx_rxx_frm_ctl.u64 = cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(index, interface));
-                if (gmxx_rxx_frm_ctl.s.pre_chk == 0)
-                {
-                    uint8_t *ptr = cvmx_phys_to_ptr(work->packet_ptr.s.addr);
-                    int i = 0;
-                    while (i<work->len-1)
-                    {
-                        if (*ptr != 0x55)
-                            break;
-                        ptr++;
-                        i++;
-                    }
-                    if (*ptr == 0xd5)
-                    {
-                        //DEBUGPRINT("Port %d received 0xd5 preamble\n", work->ipprt);
-                        work->packet_ptr.s.addr += i+1;
-                        work->len -= i+5;
-                    }
-                    else if ((*ptr & 0xf) == 0xd)
-                    {
-                        //DEBUGPRINT("Port %d received 0x?d preamble\n", work->ipprt);
-                        work->packet_ptr.s.addr += i;
-                        work->len -= i+4;
-                        for (i=0; i<work->len; i++)
-                        {
-                            *ptr = ((*ptr&0xf0)>>4) | ((*(ptr+1)&0xf)<<4);
-                            ptr++;
-                        }
-                    }
-                    else
-                    {
-                        DEBUGPRINT("Port %d unknown preamble, packet dropped\n", work->ipprt);
-                        //cvmx_helper_dump_packet(work);
-                        cvm_oct_free_work(work);
-                        continue;
-                    }
-                }
-            }
-            else
-            {
-                DEBUGPRINT("Port %d receive error code %d, packet dropped\n", work->ipprt, work->word2.snoip.err_code);
-                cvm_oct_free_work(work);
-                continue;
-            }
-        }
-
-        /* We can only use the zero copy path if skbuffs are in the FPA pool
-            and the packet fit in a single buffer */
-        if (USE_SKBUFFS_IN_HW && likely(work->word2.s.bufs == 1))
-        {
-            skb = *(struct sk_buff **)(start_of_buffer - sizeof(void*));
-            /* This calculation was changed in case the skb header is using a
-                different address aliasing type than the buffer. It doesn't make
-                any differnece now, but the new one is more correct */
-            skb->data = skb->head + work->packet_ptr.s.addr - cvmx_ptr_to_phys(skb->head);
-            CVMX_PREFETCH(skb->data, 0);
-            skb->len = work->len;
-            skb->tail = skb->data + skb->len;
-            packet_not_copied = 1;
-        }
-        else
-        {
-            /* We have to copy the packet. First allocate an skbuff for it */
-            skb = dev_alloc_skb(work->len);
-            if (!skb)
-            {
-                DEBUGPRINT("Port %d failed to allocate skbuff, packet dropped\n", work->ipprt);
-                cvm_oct_free_work(work);
-                continue;
-            }
-
-            /* Check if we've received a packet that was entirely stored the
-                work entry. This is untested */
-            if (unlikely(work->word2.s.bufs == 0))
-            {
-                uint8_t *ptr = work->packet_data;
-                if (cvmx_likely(!work->word2.s.not_IP))
-                {
-                    /* The beginning of the packet moves for IP packets */
-                    if (work->word2.s.is_v6)
-                        ptr += 2;
-                    else
-                        ptr += 6;
-                }
-                memcpy(skb_put(skb, work->len), ptr, work->len);
-                /* No packet buffers to free */
-            }
-            else
-            {
-                int segments = work->word2.s.bufs;
-                cvmx_buf_ptr_t segment_ptr = work->packet_ptr;
-                int len = work->len;
-                while (segments--)
-                {
-                    cvmx_buf_ptr_t next_ptr = *(cvmx_buf_ptr_t*)cvmx_phys_to_ptr(segment_ptr.s.addr-8);
-                    /* Octeon Errata PKI-100: The segment size is wrong. Until it
-                        is fixed, calculate the segment size based on the packet
-                        pool buffer size. When it is fixed, the following line should
-                        be replaced with this one:
-                            int segment_size = segment_ptr.s.size; */
-                    int segment_size = CVMX_FPA_PACKET_POOL_SIZE - (segment_ptr.s.addr - (((segment_ptr.s.addr >> 7) - segment_ptr.s.back) << 7));
-                    /* Don't copy more than what is left in the packet */
-                    if (segment_size > len)
-                        segment_size = len;
-                    /* Copy the data into the packet */
-                    memcpy(skb_put(skb, segment_size), cvmx_phys_to_ptr(segment_ptr.s.addr), segment_size);
-                    /* Reduce the amount of bytes left to copy */
-                    len -= segment_size;
-                    segment_ptr = next_ptr;
-                }
-            }
-            packet_not_copied = 0;
-        }
-
-        if (likely((work->ipprt < TOTAL_NUMBER_OF_PORTS) && cvm_oct_device[work->ipprt]))
-        {
-            struct net_device *dev = cvm_oct_device[work->ipprt];
-            cvm_oct_private_t* priv = (cvm_oct_private_t*)netdev_priv(dev);
-
-            /* Only accept packets for devices that are currently up */
-            if (likely(dev->flags & IFF_UP))
-            {
-                skb->protocol = eth_type_trans(skb, dev);
-                skb->dev = dev;
-
-                if (unlikely(work->word2.s.not_IP || work->word2.s.IP_exc || work->word2.s.L4_error))
-                    skb->ip_summed = CHECKSUM_NONE;
-                else
-                    skb->ip_summed = CHECKSUM_UNNECESSARY;
-                /* Increment RX stats for virtual ports */
-                if (work->ipprt >= CVMX_PIP_NUM_INPUT_PORTS)
-                {
+	const int           coreid = cvmx_get_core_num();
+	uint64_t            old_group_mask;
+	uint64_t            old_scratch;
+	int                 rx_count = 0;
+	int                 number_to_free;
+	int                 packet_not_copied;
+
+	/* Prefetch cvm_oct_device since we know we need it soon */
+	CVMX_PREFETCH(cvm_oct_device, 0);
+
+	if (USE_ASYNC_IOBDMA) {
+		/* Save scratch in case userspace is using it */
+		CVMX_SYNCIOBDMA;
+		old_scratch = cvmx_scratch_read64(CVMX_SCR_SCRATCH);
+	}
+
+	/* Only allow work for our group (and preserve priorities) */
+	old_group_mask = cvmx_read_csr(CVMX_POW_PP_GRP_MSKX(coreid));
+	cvmx_write_csr(CVMX_POW_PP_GRP_MSKX(coreid),
+		       (old_group_mask & ~0xFFFFull) | 1<<pow_receive_group);
+
+	if (USE_ASYNC_IOBDMA)
+		cvmx_pow_work_request_async(CVMX_SCR_SCRATCH, CVMX_POW_NO_WAIT);
+
+	while (1) {
+
+		void *start_of_buffer;
+		struct sk_buff *skb;
+		cvm_oct_callback_result_t callback_result;
+		cvmx_wqe_t *work;
+
+		if (USE_ASYNC_IOBDMA) {
+			work = cvmx_pow_work_response_async(CVMX_SCR_SCRATCH);
+		} else {
+			if ((INTERRUPT_LIMIT == 0) || likely(rx_count < 60))
+				work = cvmx_pow_work_request_sync(CVMX_POW_NO_WAIT);
+			else
+				work = NULL;
+		}
+		if (work == NULL)
+			break;
+
+		/* Limit each core to processing 60 packets without a break.
+		   This way the RX can't starve the TX task. */
+		if (USE_ASYNC_IOBDMA) {
+
+			if ((INTERRUPT_LIMIT == 0) || likely(rx_count < 60))
+				cvmx_pow_work_request_async_nocheck(CVMX_SCR_SCRATCH, CVMX_POW_NO_WAIT);
+			else {
+				cvmx_scratch_write64(CVMX_SCR_SCRATCH, 0x8000000000000000ull);
+				cvmx_pow_tag_sw_null_nocheck();
+			}
+		}
+		rx_count++;
+
+		CVMX_PREFETCH(cvm_oct_device[work->ipprt], 0);
+		start_of_buffer = cvm_oct_get_buffer_ptr(work->packet_ptr);
+		CVMX_PREFETCH(start_of_buffer, -8); /* Should be -sizeof(void*), but you can't use that with this macro. */
+
+		/* Immediately throw away all packets with receive errors */
+		if (unlikely(work->word2.snoip.rcv_error)) {
+			if (cvm_oct_check_rcv_error(work))
+				continue;
+		}
+
+		/* We can only use the zero copy path if skbuffs are in the FPA pool
+		   and the packet fits in a single buffer */
+		if (USE_SKBUFFS_IN_HW && likely(work->word2.s.bufs == 1)) {
+
+			skb = *(struct sk_buff **)(start_of_buffer - sizeof(void *));
+			/* This calculation was changed in case the skb header is using a
+			   different address aliasing type than the buffer. It doesn't make
+			   any differnece now, but the new one is more correct */
+			skb->data = skb->head + work->packet_ptr.s.addr - cvmx_ptr_to_phys(skb->head);
+			CVMX_PREFETCH(skb->data, 0);
+			skb->len = work->len;
+			skb->tail = skb->data + skb->len;
+			packet_not_copied = 1;
+		} else {
+
+			/* We have to copy the packet. First allocate an
+			   skbuff for it */
+			skb = dev_alloc_skb(work->len);
+			if (!skb) {
+				DEBUGPRINT("Port %d failed to allocate skbuff, packet dropped\n", work->ipprt);
+				cvm_oct_free_work(work);
+				continue;
+			}
+
+			/* Check if we've received a packet that was entirely
+			   stored in the work entry. This is untested */
+			if (unlikely(work->word2.s.bufs == 0)) {
+				uint8_t *ptr = work->packet_data;
+
+				if (cvmx_likely(!work->word2.s.not_IP)) {
+					/* The beginning of the packet moves
+					   for IP packets */
+					if (work->word2.s.is_v6)
+						ptr += 2;
+					else
+						ptr += 6;
+				}
+				memcpy(skb_put(skb, work->len), ptr, work->len);
+				/* No packet buffers to free */
+			} else {
+				int segments = work->word2.s.bufs;
+				cvmx_buf_ptr_t segment_ptr = work->packet_ptr;
+				int len = work->len;
+
+				while (segments--) {
+					cvmx_buf_ptr_t next_ptr = *(cvmx_buf_ptr_t *)cvmx_phys_to_ptr(segment_ptr.s.addr-8);
+					/* Octeon Errata PKI-100: The segment
+					   size is wrong. Until it is fixed,
+					   calculate the segment size based on
+					   the packet pool buffer size. When
+					   it is fixed, the following line
+					   should be replaced with this one:
+					int segment_size = segment_ptr.s.size; */
+					int segment_size = CVMX_FPA_PACKET_POOL_SIZE - (segment_ptr.s.addr - (((segment_ptr.s.addr >> 7) - segment_ptr.s.back) << 7));
+					/* Don't copy more than what is left
+					   in the packet */
+					if (segment_size > len)
+						segment_size = len;
+					/* Copy the data into the packet */
+					memcpy(skb_put(skb, segment_size), cvmx_phys_to_ptr(segment_ptr.s.addr), segment_size);
+					/* Reduce the amount of bytes left
+					   to copy */
+					len -= segment_size;
+					segment_ptr = next_ptr;
+				}
+			}
+			packet_not_copied = 0;
+		}
+
+		if (likely((work->ipprt < TOTAL_NUMBER_OF_PORTS) &&
+		    cvm_oct_device[work->ipprt])) {
+			struct net_device *dev = cvm_oct_device[work->ipprt];
+			cvm_oct_private_t *priv = (cvm_oct_private_t *)netdev_priv(dev);
+
+			/* Only accept packets for devices
+			   that are currently up */
+			if (likely(dev->flags & IFF_UP)) {
+				skb->protocol = eth_type_trans(skb, dev);
+				skb->dev = dev;
+
+				if (unlikely(work->word2.s.not_IP || work->word2.s.IP_exc || work->word2.s.L4_error))
+					skb->ip_summed = CHECKSUM_NONE;
+				else
+					skb->ip_summed = CHECKSUM_UNNECESSARY;
+
+				/* Increment RX stats for virtual ports */
+				if (work->ipprt >= CVMX_PIP_NUM_INPUT_PORTS) {
 #ifdef CONFIG_64BIT
-                    cvmx_atomic_add64_nosync(&priv->stats.rx_packets, 1);
-                    cvmx_atomic_add64_nosync(&priv->stats.rx_bytes, skb->len);
+					cvmx_atomic_add64_nosync(&priv->stats.rx_packets, 1);
+					cvmx_atomic_add64_nosync(&priv->stats.rx_bytes, skb->len);
 #else
-                    cvmx_atomic_add32_nosync((int32_t*)&priv->stats.rx_packets, 1);
-                    cvmx_atomic_add32_nosync((int32_t*)&priv->stats.rx_bytes, skb->len);
+					cvmx_atomic_add32_nosync((int32_t *)&priv->stats.rx_packets, 1);
+					cvmx_atomic_add32_nosync((int32_t *)&priv->stats.rx_bytes, skb->len);
 #endif
-                }
-
-                if (priv->intercept_cb)
-                {
-                    callback_result = priv->intercept_cb(dev, work, skb);
-                    switch (callback_result)
-                    {
-                        case CVM_OCT_PASS:
-                            netif_receive_skb(skb);
-                            break;
-                        case CVM_OCT_DROP:
-                            dev_kfree_skb_irq(skb);
+				}
+
+				if (priv->intercept_cb) {
+					callback_result = priv->intercept_cb(dev, work, skb);
+
+					switch (callback_result) {
+					case CVM_OCT_PASS:
+						netif_receive_skb(skb);
+						break;
+					case CVM_OCT_DROP:
+						dev_kfree_skb_irq(skb);
 #ifdef CONFIG_64BIT
-                            cvmx_atomic_add64_nosync(&priv->stats.rx_dropped, 1);
+						cvmx_atomic_add64_nosync(&priv->stats.rx_dropped, 1);
 #else
-                            cvmx_atomic_add32_nosync((int32_t*)&priv->stats.rx_dropped, 1);
+						cvmx_atomic_add32_nosync((int32_t *)&priv->stats.rx_dropped, 1);
 #endif
-                            break;
-                        case CVM_OCT_TAKE_OWNERSHIP_WORK:
-                            /* Interceptor stole our work, but we need to free
-                                the skbuff */
-                            if (USE_SKBUFFS_IN_HW && likely(packet_not_copied))
-                            {
-                                /* We can't free the skbuff since its data is
-                                    the same as the work. In this case we don't
-                                    do anything */
-                            }
-                            else
-                                dev_kfree_skb_irq(skb);
-                            break;
-                        case CVM_OCT_TAKE_OWNERSHIP_SKB:
-                            /* Interceptor stole our packet */
-                            break;
-                    }
-                }
-                else
-                {
-                    netif_receive_skb(skb);
-                    callback_result = CVM_OCT_PASS;
-                }
-            }
-            else
-            {
-                /* Drop any packet received for a device that isn't up */
-                //DEBUGPRINT("%s: Device not up, packet dropped\n", dev->name);
+						break;
+					case CVM_OCT_TAKE_OWNERSHIP_WORK:
+						/* Interceptor stole our work, but
+						   we need to free the skbuff */
+						if (USE_SKBUFFS_IN_HW && likely(packet_not_copied)) {
+							/* We can't free the skbuff since its data is
+							the same as the work. In this case we don't
+							do anything */
+						} else
+							dev_kfree_skb_irq(skb);
+						break;
+					case CVM_OCT_TAKE_OWNERSHIP_SKB:
+						/* Interceptor stole our packet */
+						break;
+					}
+				} else {
+					netif_receive_skb(skb);
+					callback_result = CVM_OCT_PASS;
+				}
+			} else {
+				/* Drop any packet received for a device that isn't up */
+				/*
+				DEBUGPRINT("%s: Device not up, packet dropped\n",
+					   dev->name);
+				*/
 #ifdef CONFIG_64BIT
-                cvmx_atomic_add64_nosync(&priv->stats.rx_dropped, 1);
+				cvmx_atomic_add64_nosync(&priv->stats.rx_dropped, 1);
 #else
-                cvmx_atomic_add32_nosync((int32_t*)&priv->stats.rx_dropped, 1);
+				cvmx_atomic_add32_nosync((int32_t *)&priv->stats.rx_dropped, 1);
 #endif
-                dev_kfree_skb_irq(skb);
-                callback_result = CVM_OCT_DROP;
-            }
-        }
-        else
-        {
-            /* Drop any packet received for a device that doesn't exist */
-            DEBUGPRINT("Port %d not controlled by Linux, packet dropped\n", work->ipprt);
-            dev_kfree_skb_irq(skb);
-            callback_result = CVM_OCT_DROP;
-        }
-
-        /* We only need to free the work if the interceptor didn't take over
-            ownership of it */
-        if (callback_result != CVM_OCT_TAKE_OWNERSHIP_WORK)
-        {
-            /* Check to see if the skbuff and work share the same packet buffer */
-            if (USE_SKBUFFS_IN_HW && likely(packet_not_copied))
-            {
-                /* This buffer needs to be replaced, increment the number of buffers we need to free by one */
-                cvmx_fau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE, 1);
-                cvmx_fpa_free(work, CVMX_FPA_WQE_POOL, DONT_WRITEBACK(1));
-            }
-            else
-                cvm_oct_free_work(work);
-        }
-    }
-
-    /* Restore the original POW group mask */
-    cvmx_write_csr(CVMX_POW_PP_GRP_MSKX(coreid), old_group_mask);
-    if (USE_ASYNC_IOBDMA)
-    {
-        /* Restore the scratch area */
-        cvmx_scratch_write64(CVMX_SCR_SCRATCH, old_scratch);
-    }
-
-    if (USE_SKBUFFS_IN_HW)
-    {
-        /* Refill the packet buffer pool */
-        number_to_free = cvmx_fau_fetch_and_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE, 0);
-        if (number_to_free>0)
-        {
-            cvmx_fau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE, -number_to_free);
-            cvm_oct_mem_fill_fpa(CVMX_FPA_PACKET_POOL, CVMX_FPA_PACKET_POOL_SIZE, number_to_free);
-        }
-    }
+				dev_kfree_skb_irq(skb);
+				callback_result = CVM_OCT_DROP;
+			}
+		} else {
+			/* Drop any packet received for a device that
+			   doesn't exist */
+			DEBUGPRINT("Port %d not controlled by Linux, packet dropped\n", work->ipprt);
+			dev_kfree_skb_irq(skb);
+			callback_result = CVM_OCT_DROP;
+		}
+
+		/* We only need to free the work if the interceptor didn't
+		   take over ownership of it */
+		if (callback_result != CVM_OCT_TAKE_OWNERSHIP_WORK) {
+
+			/* Check to see if the skbuff and work share
+			   the same packet buffer */
+			if (USE_SKBUFFS_IN_HW && likely(packet_not_copied)) {
+				/* This buffer needs to be replaced, increment
+				the number of buffers we need to free by one */
+				cvmx_fau_atomic_add32(
+					FAU_NUM_PACKET_BUFFERS_TO_FREE, 1);
+
+				cvmx_fpa_free(work, CVMX_FPA_WQE_POOL,
+					      DONT_WRITEBACK(1));
+			} else
+				cvm_oct_free_work(work);
+		}
+	}
+
+	/* Restore the original POW group mask */
+	cvmx_write_csr(CVMX_POW_PP_GRP_MSKX(coreid), old_group_mask);
+	if (USE_ASYNC_IOBDMA) {
+		/* Restore the scratch area */
+		cvmx_scratch_write64(CVMX_SCR_SCRATCH, old_scratch);
+	}
+
+	if (USE_SKBUFFS_IN_HW) {
+		/* Refill the packet buffer pool */
+		number_to_free =
+		  cvmx_fau_fetch_and_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE, 0);
+
+		if (number_to_free > 0) {
+			cvmx_fau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE,
+					      -number_to_free);
+			cvm_oct_mem_fill_fpa(CVMX_FPA_PACKET_POOL,
+					     CVMX_FPA_PACKET_POOL_SIZE,
+					     number_to_free);
+		}
+	}
 }
 
 
 
 void cvm_oct_rx_initialize(void)
 {
-    int i;
-    /* Initialize all of the tasklets */
-    for (i=0; i<NR_CPUS; i++)
-        tasklet_init(cvm_oct_tasklet + i, cvm_oct_tasklet_rx, 0);
+	int i;
+	/* Initialize all of the tasklets */
+	for (i = 0; i < NR_CPUS; i++)
+		tasklet_init(cvm_oct_tasklet + i, cvm_oct_tasklet_rx, 0);
 }
 
 void cvm_oct_rx_shutdown(void)
 {
-    int i;
-    /* Shutdown all of the tasklets */
-    for (i=0; i<NR_CPUS; i++)
-        tasklet_kill(cvm_oct_tasklet + i);
+	int i;
+	/* Shutdown all of the tasklets */
+	for (i = 0; i < NR_CPUS; i++)
+		tasklet_kill(cvm_oct_tasklet + i);
 }
 
diff --git a/drivers/net/cavium-ethernet/ethernet-sgmii.c b/drivers/net/cavium-ethernet/ethernet-sgmii.c
index d87e803..c117479 100644
--- a/drivers/net/cavium-ethernet/ethernet-sgmii.c
+++ b/drivers/net/cavium-ethernet/ethernet-sgmii.c
@@ -54,82 +54,80 @@ extern int octeon_is_simulation(void);
 
 static int cvm_oct_sgmii_open(struct net_device *dev)
 {
-    cvmx_gmxx_prtx_cfg_t gmx_cfg;
-    cvm_oct_private_t* priv = (cvm_oct_private_t*)netdev_priv(dev);
-    int interface = INTERFACE(priv->port);
-    int index = INDEX(priv->port);
+	cvmx_gmxx_prtx_cfg_t gmx_cfg;
+	cvm_oct_private_t *priv = (cvm_oct_private_t *)netdev_priv(dev);
+	int interface = INTERFACE(priv->port);
+	int index = INDEX(priv->port);
 
-    gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
-    gmx_cfg.s.en = 1;
-    cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmx_cfg.u64);
-    return 0;
+	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
+	gmx_cfg.s.en = 1;
+	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmx_cfg.u64);
+	return 0;
 }
 
 static int cvm_oct_sgmii_stop(struct net_device *dev)
 {
-    cvmx_gmxx_prtx_cfg_t gmx_cfg;
-    cvm_oct_private_t* priv = (cvm_oct_private_t*)netdev_priv(dev);
-    int interface = INTERFACE(priv->port);
-    int index = INDEX(priv->port);
+	cvmx_gmxx_prtx_cfg_t gmx_cfg;
+	cvm_oct_private_t *priv = (cvm_oct_private_t *)netdev_priv(dev);
+	int interface = INTERFACE(priv->port);
+	int index = INDEX(priv->port);
 
-    gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
-    gmx_cfg.s.en = 0;
-    cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmx_cfg.u64);
-    return 0;
+	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
+	gmx_cfg.s.en = 0;
+	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmx_cfg.u64);
+	return 0;
 }
 
 static void cvm_oct_sgmii_poll(struct net_device *dev)
 {
-    cvm_oct_private_t* priv = (cvm_oct_private_t*)netdev_priv(dev);
-    cvmx_helper_link_info_t link_info;
+	cvm_oct_private_t *priv = (cvm_oct_private_t *)netdev_priv(dev);
+	cvmx_helper_link_info_t link_info;
 
-    link_info = cvmx_helper_link_get(priv->port);
-    if (link_info.u64 == priv->link_info)
-        return;
+	link_info = cvmx_helper_link_get(priv->port);
+	if (link_info.u64 == priv->link_info)
+		return;
 
-    link_info = cvmx_helper_link_autoconf(priv->port);
-    priv->link_info = link_info.u64;
+	link_info = cvmx_helper_link_autoconf(priv->port);
+	priv->link_info = link_info.u64;
 
-    /* Tell Linux */
-    if (link_info.s.link_up)
-    {
-        if (!netif_carrier_ok(dev))
-            netif_carrier_on(dev);
-        if (priv->queue != -1)
-            DEBUGPRINT("%s: %u Mbps %s duplex, port %2d, queue %2d\n",
-                       dev->name, link_info.s.speed,
-                       (link_info.s.full_duplex) ? "Full" : "Half",
-                       priv->port, priv->queue);
-        else
-            DEBUGPRINT("%s: %u Mbps %s duplex, port %2d, POW\n",
-                       dev->name, link_info.s.speed,
-                       (link_info.s.full_duplex) ? "Full" : "Half",
-                       priv->port);
-    }
-    else
-    {
-        if (netif_carrier_ok(dev))
-            netif_carrier_off(dev);
-        DEBUGPRINT("%s: Link down\n", dev->name);
-    }
+	/* Tell Linux */
+	if (link_info.s.link_up) {
+
+		if (!netif_carrier_ok(dev))
+			netif_carrier_on(dev);
+		if (priv->queue != -1)
+			DEBUGPRINT("%s: %u Mbps %s duplex, port %2d, queue %2d\n",
+				   dev->name, link_info.s.speed,
+				   (link_info.s.full_duplex) ? "Full" : "Half",
+				   priv->port, priv->queue);
+		else
+			DEBUGPRINT("%s: %u Mbps %s duplex, port %2d, POW\n",
+				   dev->name, link_info.s.speed,
+				   (link_info.s.full_duplex) ? "Full" : "Half",
+				   priv->port);
+	} else {
+		if (netif_carrier_ok(dev))
+			netif_carrier_off(dev);
+		DEBUGPRINT("%s: Link down\n", dev->name);
+	}
 }
 
 int cvm_oct_sgmii_init(struct net_device *dev)
 {
-    cvm_oct_private_t* priv = (cvm_oct_private_t*)netdev_priv(dev);
-    cvm_oct_common_init(dev);
-    dev->open = cvm_oct_sgmii_open;
-    dev->stop = cvm_oct_sgmii_stop;
-    dev->stop(dev);
-    if (!octeon_is_simulation())
-        priv->poll = cvm_oct_sgmii_poll;
+	cvm_oct_private_t *priv = (cvm_oct_private_t *)netdev_priv(dev);
+	cvm_oct_common_init(dev);
+	dev->open = cvm_oct_sgmii_open;
+	dev->stop = cvm_oct_sgmii_stop;
+	dev->stop(dev);
+	if (!octeon_is_simulation())
+		priv->poll = cvm_oct_sgmii_poll;
 
-    /* FIXME: Need autoneg logic */
-    return 0;
+	/* FIXME: Need autoneg logic */
+	return 0;
 }
 
 void cvm_oct_sgmii_uninit(struct net_device *dev)
 {
-    cvm_oct_common_uninit(dev);
+	cvm_oct_common_uninit(dev);
 }
 
diff --git a/drivers/net/cavium-ethernet/ethernet-spi.c b/drivers/net/cavium-ethernet/ethernet-spi.c
index e60b94f..3240a03 100644
--- a/drivers/net/cavium-ethernet/ethernet-spi.c
+++ b/drivers/net/cavium-ethernet/ethernet-spi.c
@@ -50,208 +50,243 @@
 #include "wrapper-cvmx-includes.h"
 #include "ethernet-headers.h"
 
-static int number_spi_ports = 0;
-static int need_retrain[2] = {0,0};
+static int number_spi_ports;
+static int need_retrain[2] = {0, 0};
 
 static irqreturn_t cvm_oct_spi_rml_interrupt(int cpl, void *dev_id)
 {
-    irqreturn_t return_status = IRQ_NONE;
-    cvmx_npi_rsl_int_blocks_t rsl_int_blocks;
+	irqreturn_t return_status = IRQ_NONE;
+	cvmx_npi_rsl_int_blocks_t rsl_int_blocks;
 
-    /* Check and see if this interrupt was caused by the GMX block */
-    rsl_int_blocks.u64 = cvmx_read_csr(CVMX_NPI_RSL_INT_BLOCKS);
-    if (rsl_int_blocks.s.spx1) /* 19 - SPX1_INT_REG & STX1_INT_REG */
-    {
-        cvmx_spxx_int_reg_t spx_int_reg;
-        cvmx_stxx_int_reg_t stx_int_reg;
+	/* Check and see if this interrupt was caused by the GMX block */
+	rsl_int_blocks.u64 = cvmx_read_csr(CVMX_NPI_RSL_INT_BLOCKS);
+	if (rsl_int_blocks.s.spx1) { /* 19 - SPX1_INT_REG & STX1_INT_REG */
 
-        spx_int_reg.u64 = cvmx_read_csr(CVMX_SPXX_INT_REG(1));
-        cvmx_write_csr(CVMX_SPXX_INT_REG(1), spx_int_reg.u64);
-        if (!need_retrain[1])
-        {
-            spx_int_reg.u64 &= cvmx_read_csr(CVMX_SPXX_INT_MSK(1));
-            if (spx_int_reg.s.spf)      printk("SPI1: SRX Spi4 interface down\n");
-            if (spx_int_reg.s.calerr)   printk("SPI1: SRX Spi4 Calendar table parity error\n");
-            if (spx_int_reg.s.syncerr)  printk("SPI1: SRX Consecutive Spi4 DIP4 errors have exceeded SPX_ERR_CTL[ERRCNT]\n");
-            if (spx_int_reg.s.diperr)   printk("SPI1: SRX Spi4 DIP4 error\n");
-            if (spx_int_reg.s.tpaovr)   printk("SPI1: SRX Selected port has hit TPA overflow\n");
-            if (spx_int_reg.s.rsverr)   printk("SPI1: SRX Spi4 reserved control word detected\n");
-            if (spx_int_reg.s.drwnng)   printk("SPI1: SRX Spi4 receive FIFO drowning/overflow\n");
-            if (spx_int_reg.s.clserr)   printk("SPI1: SRX Spi4 packet closed on non-16B alignment without EOP\n");
-            if (spx_int_reg.s.spiovr)   printk("SPI1: SRX Spi4 async FIFO overflow\n");
-            if (spx_int_reg.s.abnorm)   printk("SPI1: SRX Abnormal packet termination (ERR bit)\n");
-            if (spx_int_reg.s.prtnxa)   printk("SPI1: SRX Port out of range\n");
-        }
+		cvmx_spxx_int_reg_t spx_int_reg;
+		cvmx_stxx_int_reg_t stx_int_reg;
 
-        stx_int_reg.u64 = cvmx_read_csr(CVMX_STXX_INT_REG(1));
-        cvmx_write_csr(CVMX_STXX_INT_REG(1), spx_int_reg.u64);
-        if (!need_retrain[1])
-        {
-            stx_int_reg.u64 &= cvmx_read_csr(CVMX_STXX_INT_MSK(1));
-            if (stx_int_reg.s.syncerr)  printk("SPI1: STX Interface encountered a fatal error\n");
-            if (stx_int_reg.s.frmerr)   printk("SPI1: STX FRMCNT has exceeded STX_DIP_CNT[MAXFRM]\n");
-            if (stx_int_reg.s.unxfrm)   printk("SPI1: STX Unexpected framing sequence\n");
-            if (stx_int_reg.s.nosync)   printk("SPI1: STX ERRCNT has exceeded STX_DIP_CNT[MAXDIP]\n");
-            if (stx_int_reg.s.diperr)   printk("SPI1: STX DIP2 error on the Spi4 Status channel\n");
-            if (stx_int_reg.s.datovr)   printk("SPI1: STX Spi4 FIFO overflow error\n");
-            if (stx_int_reg.s.ovrbst)   printk("SPI1: STX Transmit packet burst too big\n");
-            if (stx_int_reg.s.calpar1)  printk("SPI1: STX Calendar Table Parity Error Bank1\n");
-            if (stx_int_reg.s.calpar0)  printk("SPI1: STX Calendar Table Parity Error Bank0\n");
-        }
+		spx_int_reg.u64 = cvmx_read_csr(CVMX_SPXX_INT_REG(1));
+		cvmx_write_csr(CVMX_SPXX_INT_REG(1), spx_int_reg.u64);
+		if (!need_retrain[1]) {
 
-        cvmx_write_csr(CVMX_SPXX_INT_MSK(1), 0);
-        cvmx_write_csr(CVMX_STXX_INT_MSK(1), 0);
-        need_retrain[1] = 1;
-        return_status = IRQ_HANDLED;
-    }
+			spx_int_reg.u64 &= cvmx_read_csr(CVMX_SPXX_INT_MSK(1));
+			if (spx_int_reg.s.spf)
+				pr_err("SPI1: SRX Spi4 interface down\n");
+			if (spx_int_reg.s.calerr)
+				pr_err("SPI1: SRX Spi4 Calendar table parity error\n");
+			if (spx_int_reg.s.syncerr)
+				pr_err("SPI1: SRX Consecutive Spi4 DIP4 errors have exceeded SPX_ERR_CTL[ERRCNT]\n");
+			if (spx_int_reg.s.diperr)
+				pr_err("SPI1: SRX Spi4 DIP4 error\n");
+			if (spx_int_reg.s.tpaovr)
+				pr_err("SPI1: SRX Selected port has hit TPA overflow\n");
+			if (spx_int_reg.s.rsverr)
+				pr_err("SPI1: SRX Spi4 reserved control word detected\n");
+			if (spx_int_reg.s.drwnng)
+				pr_err("SPI1: SRX Spi4 receive FIFO drowning/overflow\n");
+			if (spx_int_reg.s.clserr)
+				pr_err("SPI1: SRX Spi4 packet closed on non-16B alignment without EOP\n");
+			if (spx_int_reg.s.spiovr)
+				pr_err("SPI1: SRX Spi4 async FIFO overflow\n");
+			if (spx_int_reg.s.abnorm)
+				pr_err("SPI1: SRX Abnormal packet termination (ERR bit)\n");
+			if (spx_int_reg.s.prtnxa)
+				pr_err("SPI1: SRX Port out of range\n");
+		}
 
-    if (rsl_int_blocks.s.spx0) /* 18 - SPX0_INT_REG & STX0_INT_REG */
-    {
-        cvmx_spxx_int_reg_t spx_int_reg;
-        cvmx_stxx_int_reg_t stx_int_reg;
+		stx_int_reg.u64 = cvmx_read_csr(CVMX_STXX_INT_REG(1));
+		cvmx_write_csr(CVMX_STXX_INT_REG(1), spx_int_reg.u64);
+		if (!need_retrain[1]) {
 
-        spx_int_reg.u64 = cvmx_read_csr(CVMX_SPXX_INT_REG(0));
-        cvmx_write_csr(CVMX_SPXX_INT_REG(0), spx_int_reg.u64);
-        if (!need_retrain[0])
-        {
-            spx_int_reg.u64 &= cvmx_read_csr(CVMX_SPXX_INT_MSK(0));
-            if (spx_int_reg.s.spf)      printk("SPI0: SRX Spi4 interface down\n");
-            if (spx_int_reg.s.calerr)   printk("SPI0: SRX Spi4 Calendar table parity error\n");
-            if (spx_int_reg.s.syncerr)  printk("SPI0: SRX Consecutive Spi4 DIP4 errors have exceeded SPX_ERR_CTL[ERRCNT]\n");
-            if (spx_int_reg.s.diperr)   printk("SPI0: SRX Spi4 DIP4 error\n");
-            if (spx_int_reg.s.tpaovr)   printk("SPI0: SRX Selected port has hit TPA overflow\n");
-            if (spx_int_reg.s.rsverr)   printk("SPI0: SRX Spi4 reserved control word detected\n");
-            if (spx_int_reg.s.drwnng)   printk("SPI0: SRX Spi4 receive FIFO drowning/overflow\n");
-            if (spx_int_reg.s.clserr)   printk("SPI0: SRX Spi4 packet closed on non-16B alignment without EOP\n");
-            if (spx_int_reg.s.spiovr)   printk("SPI0: SRX Spi4 async FIFO overflow\n");
-            if (spx_int_reg.s.abnorm)   printk("SPI0: SRX Abnormal packet termination (ERR bit)\n");
-            if (spx_int_reg.s.prtnxa)   printk("SPI0: SRX Port out of range\n");
-        }
+			stx_int_reg.u64 &= cvmx_read_csr(CVMX_STXX_INT_MSK(1));
+			if (stx_int_reg.s.syncerr)
+				pr_err("SPI1: STX Interface encountered a fatal error\n");
+			if (stx_int_reg.s.frmerr)
+				pr_err("SPI1: STX FRMCNT has exceeded STX_DIP_CNT[MAXFRM]\n");
+			if (stx_int_reg.s.unxfrm)
+				pr_err("SPI1: STX Unexpected framing sequence\n");
+			if (stx_int_reg.s.nosync)
+				pr_err("SPI1: STX ERRCNT has exceeded STX_DIP_CNT[MAXDIP]\n");
+			if (stx_int_reg.s.diperr)
+				pr_err("SPI1: STX DIP2 error on the Spi4 Status channel\n");
+			if (stx_int_reg.s.datovr)
+				pr_err("SPI1: STX Spi4 FIFO overflow error\n");
+			if (stx_int_reg.s.ovrbst)
+				pr_err("SPI1: STX Transmit packet burst too big\n");
+			if (stx_int_reg.s.calpar1)
+				pr_err("SPI1: STX Calendar Table Parity Error Bank1\n");
+			if (stx_int_reg.s.calpar0)
+				pr_err("SPI1: STX Calendar Table Parity Error Bank0\n");
+		}
 
-        stx_int_reg.u64 = cvmx_read_csr(CVMX_STXX_INT_REG(0));
-        cvmx_write_csr(CVMX_STXX_INT_REG(0), spx_int_reg.u64);
-        if (!need_retrain[0])
-        {
-            stx_int_reg.u64 &= cvmx_read_csr(CVMX_STXX_INT_MSK(0));
-            if (stx_int_reg.s.syncerr)  printk("SPI0: STX Interface encountered a fatal error\n");
-            if (stx_int_reg.s.frmerr)   printk("SPI0: STX FRMCNT has exceeded STX_DIP_CNT[MAXFRM]\n");
-            if (stx_int_reg.s.unxfrm)   printk("SPI0: STX Unexpected framing sequence\n");
-            if (stx_int_reg.s.nosync)   printk("SPI0: STX ERRCNT has exceeded STX_DIP_CNT[MAXDIP]\n");
-            if (stx_int_reg.s.diperr)   printk("SPI0: STX DIP2 error on the Spi4 Status channel\n");
-            if (stx_int_reg.s.datovr)   printk("SPI0: STX Spi4 FIFO overflow error\n");
-            if (stx_int_reg.s.ovrbst)   printk("SPI0: STX Transmit packet burst too big\n");
-            if (stx_int_reg.s.calpar1)  printk("SPI0: STX Calendar Table Parity Error Bank1\n");
-            if (stx_int_reg.s.calpar0)  printk("SPI0: STX Calendar Table Parity Error Bank0\n");
-        }
+		cvmx_write_csr(CVMX_SPXX_INT_MSK(1), 0);
+		cvmx_write_csr(CVMX_STXX_INT_MSK(1), 0);
+		need_retrain[1] = 1;
+		return_status = IRQ_HANDLED;
+	}
 
-        cvmx_write_csr(CVMX_SPXX_INT_MSK(0), 0);
-        cvmx_write_csr(CVMX_STXX_INT_MSK(0), 0);
-        need_retrain[0] = 1;
-        return_status = IRQ_HANDLED;
-    }
+	if (rsl_int_blocks.s.spx0) { /* 18 - SPX0_INT_REG & STX0_INT_REG */
+		cvmx_spxx_int_reg_t spx_int_reg;
+		cvmx_stxx_int_reg_t stx_int_reg;
 
-    return return_status;
+		spx_int_reg.u64 = cvmx_read_csr(CVMX_SPXX_INT_REG(0));
+		cvmx_write_csr(CVMX_SPXX_INT_REG(0), spx_int_reg.u64);
+		if (!need_retrain[0]) {
+
+			spx_int_reg.u64 &= cvmx_read_csr(CVMX_SPXX_INT_MSK(0));
+			if (spx_int_reg.s.spf)
+				pr_err("SPI0: SRX Spi4 interface down\n");
+			if (spx_int_reg.s.calerr)
+				pr_err("SPI0: SRX Spi4 Calendar table parity error\n");
+			if (spx_int_reg.s.syncerr)
+				pr_err("SPI0: SRX Consecutive Spi4 DIP4 errors have exceeded SPX_ERR_CTL[ERRCNT]\n");
+			if (spx_int_reg.s.diperr)
+				pr_err("SPI0: SRX Spi4 DIP4 error\n");
+			if (spx_int_reg.s.tpaovr)
+				pr_err("SPI0: SRX Selected port has hit TPA overflow\n");
+			if (spx_int_reg.s.rsverr)
+				pr_err("SPI0: SRX Spi4 reserved control word detected\n");
+			if (spx_int_reg.s.drwnng)
+				pr_err("SPI0: SRX Spi4 receive FIFO drowning/overflow\n");
+			if (spx_int_reg.s.clserr)
+				pr_err("SPI0: SRX Spi4 packet closed on non-16B alignment without EOP\n");
+			if (spx_int_reg.s.spiovr)
+				pr_err("SPI0: SRX Spi4 async FIFO overflow\n");
+			if (spx_int_reg.s.abnorm)
+				pr_err("SPI0: SRX Abnormal packet termination (ERR bit)\n");
+			if (spx_int_reg.s.prtnxa)
+				pr_err("SPI0: SRX Port out of range\n");
+		}
+
+		stx_int_reg.u64 = cvmx_read_csr(CVMX_STXX_INT_REG(0));
+		cvmx_write_csr(CVMX_STXX_INT_REG(0), spx_int_reg.u64);
+		if (!need_retrain[0]) {
+
+			stx_int_reg.u64 &= cvmx_read_csr(CVMX_STXX_INT_MSK(0));
+			if (stx_int_reg.s.syncerr)
+				pr_err("SPI0: STX Interface encountered a fatal error\n");
+			if (stx_int_reg.s.frmerr)
+				pr_err("SPI0: STX FRMCNT has exceeded STX_DIP_CNT[MAXFRM]\n");
+			if (stx_int_reg.s.unxfrm)
+				pr_err("SPI0: STX Unexpected framing sequence\n");
+			if (stx_int_reg.s.nosync)
+				pr_err("SPI0: STX ERRCNT has exceeded STX_DIP_CNT[MAXDIP]\n");
+			if (stx_int_reg.s.diperr)
+				pr_err("SPI0: STX DIP2 error on the Spi4 Status channel\n");
+			if (stx_int_reg.s.datovr)
+				pr_err("SPI0: STX Spi4 FIFO overflow error\n");
+			if (stx_int_reg.s.ovrbst)
+				pr_err("SPI0: STX Transmit packet burst too big\n");
+			if (stx_int_reg.s.calpar1)
+				pr_err("SPI0: STX Calendar Table Parity Error Bank1\n");
+			if (stx_int_reg.s.calpar0)
+				pr_err("SPI0: STX Calendar Table Parity Error Bank0\n");
+		}
+
+		cvmx_write_csr(CVMX_SPXX_INT_MSK(0), 0);
+		cvmx_write_csr(CVMX_STXX_INT_MSK(0), 0);
+		need_retrain[0] = 1;
+		return_status = IRQ_HANDLED;
+	}
+
+	return return_status;
 }
 
 static void cvm_oct_spi_enable_error_reporting(int interface)
 {
-    cvmx_spxx_int_msk_t spxx_int_msk;
-    cvmx_stxx_int_msk_t stxx_int_msk;
+	cvmx_spxx_int_msk_t spxx_int_msk;
+	cvmx_stxx_int_msk_t stxx_int_msk;
 
-    spxx_int_msk.u64 = cvmx_read_csr(CVMX_SPXX_INT_MSK(interface));
-    spxx_int_msk.s.calerr = 1;
-    spxx_int_msk.s.syncerr = 1;
-    spxx_int_msk.s.diperr = 1;
-    spxx_int_msk.s.tpaovr = 1;
-    spxx_int_msk.s.rsverr = 1;
-    spxx_int_msk.s.drwnng = 1;
-    spxx_int_msk.s.clserr = 1;
-    spxx_int_msk.s.spiovr = 1;
-    spxx_int_msk.s.abnorm = 1;
-    spxx_int_msk.s.prtnxa = 1;
-    cvmx_write_csr(CVMX_SPXX_INT_MSK(interface), spxx_int_msk.u64);
+	spxx_int_msk.u64 = cvmx_read_csr(CVMX_SPXX_INT_MSK(interface));
+	spxx_int_msk.s.calerr = 1;
+	spxx_int_msk.s.syncerr = 1;
+	spxx_int_msk.s.diperr = 1;
+	spxx_int_msk.s.tpaovr = 1;
+	spxx_int_msk.s.rsverr = 1;
+	spxx_int_msk.s.drwnng = 1;
+	spxx_int_msk.s.clserr = 1;
+	spxx_int_msk.s.spiovr = 1;
+	spxx_int_msk.s.abnorm = 1;
+	spxx_int_msk.s.prtnxa = 1;
+	cvmx_write_csr(CVMX_SPXX_INT_MSK(interface), spxx_int_msk.u64);
 
-    stxx_int_msk.u64 = cvmx_read_csr(CVMX_STXX_INT_MSK(interface));
-    stxx_int_msk.s.frmerr = 1;
-    stxx_int_msk.s.unxfrm = 1;
-    stxx_int_msk.s.nosync = 1;
-    stxx_int_msk.s.diperr = 1;
-    stxx_int_msk.s.datovr = 1;
-    stxx_int_msk.s.ovrbst = 1;
-    stxx_int_msk.s.calpar1 = 1;
-    stxx_int_msk.s.calpar0 = 1;
-    cvmx_write_csr(CVMX_STXX_INT_MSK(interface), stxx_int_msk.u64);
+	stxx_int_msk.u64 = cvmx_read_csr(CVMX_STXX_INT_MSK(interface));
+	stxx_int_msk.s.frmerr = 1;
+	stxx_int_msk.s.unxfrm = 1;
+	stxx_int_msk.s.nosync = 1;
+	stxx_int_msk.s.diperr = 1;
+	stxx_int_msk.s.datovr = 1;
+	stxx_int_msk.s.ovrbst = 1;
+	stxx_int_msk.s.calpar1 = 1;
+	stxx_int_msk.s.calpar0 = 1;
+	cvmx_write_csr(CVMX_STXX_INT_MSK(interface), stxx_int_msk.u64);
 }
 
 static void cvm_oct_spi_poll(struct net_device *dev)
 {
-    static int spi4000_port = 0;
-    cvm_oct_private_t* priv = (cvm_oct_private_t*)netdev_priv(dev);
-    int interface;
+	static int spi4000_port;
+	cvm_oct_private_t *priv = (cvm_oct_private_t *)netdev_priv(dev);
+	int interface;
 
-    for (interface=0; interface<2; interface++)
-    {
-        if ((priv->port == interface*16) && need_retrain[interface])
-        {
-            if (cvmx_spi_restart_interface(interface, CVMX_SPI_MODE_DUPLEX, 10) == 0)
-            {
-                need_retrain[interface] = 0;
-                cvm_oct_spi_enable_error_reporting(interface);
-            }
-        }
-        /* The SPI4000 TWSI interface is very slow. In order not to bring the
-            system to a crawl, we only poll a single port every second. This
-            means negotiation speed changes take up to 10 seconds, but at least
-            we don't waste absurd amounts of time waiting for TWSI */
-        if (priv->port == spi4000_port)
-        {
-            /* This function does nothing if it is called on an interface
-                without a SPI4000 */
-            cvmx_spi4000_check_speed(interface, priv->port);
-            /* Normal ordering increments. By decrimenting we only match once
-                per iteration */
-            spi4000_port--;
-            if (spi4000_port < 0)
-                spi4000_port = 10;
-        }
-    }
+	for (interface = 0; interface < 2; interface++) {
+
+		if ((priv->port == interface*16) && need_retrain[interface]) {
+
+			if (cvmx_spi_restart_interface(interface, CVMX_SPI_MODE_DUPLEX, 10) == 0) {
+				need_retrain[interface] = 0;
+				cvm_oct_spi_enable_error_reporting(interface);
+			}
+		}
+
+		/* The SPI4000 TWSI interface is very slow. In order not to
+		   bring the system to a crawl, we only poll a single port
+		   every second. This means negotiation speed changes
+		   take up to 10 seconds, but at least we don't waste
+		   absurd amounts of time waiting for TWSI */
+		if (priv->port == spi4000_port) {
+			/* This function does nothing if it is called on an
+			   interface without a SPI4000 */
+			cvmx_spi4000_check_speed(interface, priv->port);
+			/* Normal ordering increments. By decrementing
+			   we only match once per iteration */
+			spi4000_port--;
+			if (spi4000_port < 0)
+				spi4000_port = 10;
+		}
+	}
 }
 
 
 int cvm_oct_spi_init(struct net_device *dev)
 {
-    int r;
-    cvm_oct_private_t* priv = (cvm_oct_private_t*)netdev_priv(dev);
+	int r;
+	cvm_oct_private_t *priv = (cvm_oct_private_t *)netdev_priv(dev);
 
-    if (number_spi_ports == 0)
-    {
-        r = request_irq(8 + 46, cvm_oct_spi_rml_interrupt, IRQF_SHARED, "SPI", &number_spi_ports);
-    }
-    number_spi_ports++;
+	if (number_spi_ports == 0) {
+		r = request_irq(8 + 46, cvm_oct_spi_rml_interrupt, IRQF_SHARED,
+				"SPI", &number_spi_ports);
+	}
+	number_spi_ports++;
 
-    if ((priv->port == 0) || (priv->port == 16))
-    {
-        cvm_oct_spi_enable_error_reporting(INTERFACE(priv->port));
-        priv->poll = cvm_oct_spi_poll;
-    }
-    cvm_oct_common_init(dev);
-    return 0;
+	if ((priv->port == 0) || (priv->port == 16)) {
+		cvm_oct_spi_enable_error_reporting(INTERFACE(priv->port));
+		priv->poll = cvm_oct_spi_poll;
+	}
+	cvm_oct_common_init(dev);
+	return 0;
 }
 
 void cvm_oct_spi_uninit(struct net_device *dev)
 {
-    int interface;
+	int interface;
 
-    cvm_oct_common_uninit(dev);
-    number_spi_ports--;
-    if (number_spi_ports == 0)
-    {
-        for (interface=0; interface<2; interface++)
-        {
-            cvmx_write_csr(CVMX_SPXX_INT_MSK(interface), 0);
-            cvmx_write_csr(CVMX_STXX_INT_MSK(interface), 0);
-        }
-        free_irq(8 + 46, &number_spi_ports);
-    }
+	cvm_oct_common_uninit(dev);
+	number_spi_ports--;
+	if (number_spi_ports == 0) {
+		for (interface = 0; interface < 2; interface++) {
+			cvmx_write_csr(CVMX_SPXX_INT_MSK(interface), 0);
+			cvmx_write_csr(CVMX_STXX_INT_MSK(interface), 0);
+		}
+		free_irq(8 + 46, &number_spi_ports);
+	}
 }
-
diff --git a/drivers/net/cavium-ethernet/ethernet-tx.c b/drivers/net/cavium-ethernet/ethernet-tx.c
index 7a7ea4e..db04300 100644
--- a/drivers/net/cavium-ethernet/ethernet-tx.c
+++ b/drivers/net/cavium-ethernet/ethernet-tx.c
@@ -63,10 +63,10 @@
 #include "ethernet-headers.h"
 
 /* You can define GET_SKBUFF_QOS() to override how the skbuff output function
-    determines which output queue is used. The default implementation
-    always uses the base queue for the port. If, for example, you wanted
-    to use the skb->priority fieid, define GET_SKBUFF_QOS as:
-    #define GET_SKBUFF_QOS(skb) ((skb)->priority) */
+   determines which output queue is used. The default implementation
+   always uses the base queue for the port. If, for example, you wanted
+   to use the skb->priority fieid, define GET_SKBUFF_QOS as:
+   #define GET_SKBUFF_QOS(skb) ((skb)->priority) */
 #ifndef GET_SKBUFF_QOS
     #define GET_SKBUFF_QOS(skb) 0
 #endif
@@ -83,275 +83,267 @@ extern int pow_send_group;
  */
 int cvm_oct_xmit(struct sk_buff *skb, struct net_device *dev)
 {
-    cvmx_pko_command_word0_t    pko_command;
-    cvmx_buf_ptr_t              hw_buffer;
-    uint64_t                    old_scratch;
-    uint64_t                    old_scratch2;
-    int                         dropped;
-    int                         qos;
-    cvm_oct_private_t*          priv = (cvm_oct_private_t*)netdev_priv(dev);
-    int32_t in_use;
-    int32_t buffers_to_free;
+	cvmx_pko_command_word0_t    pko_command;
+	cvmx_buf_ptr_t              hw_buffer;
+	uint64_t                    old_scratch;
+	uint64_t                    old_scratch2;
+	int                         dropped;
+	int                         qos;
+	cvm_oct_private_t          *priv = (cvm_oct_private_t *)netdev_priv(dev);
+	int32_t in_use;
+	int32_t buffers_to_free;
 #if REUSE_SKBUFFS_WITHOUT_FREE
-    unsigned char *fpa_head;
+	unsigned char *fpa_head;
 #endif
 
-    /* Prefetch the private data structure. It is larger that one cache line */
-    CVMX_PREFETCH(priv, 0);
-
-    /* Start off assuming no drop */
-    dropped = 0;
-
-    /* The check on CVMX_PKO_QUEUES_PER_PORT_* is designed to completely
-        remove "qos" in the event neither interface supports multiple queues
-        per port */
-    if ((CVMX_PKO_QUEUES_PER_PORT_INTERFACE0 > 1) ||
-        (CVMX_PKO_QUEUES_PER_PORT_INTERFACE1 > 1))
-    {
-        qos = GET_SKBUFF_QOS(skb);
-        if (qos <= 0)
-            qos = 0;
-        else if (qos >= cvmx_pko_get_num_queues(priv->port))
-            qos = 0;
-    }
-    else
-        qos = 0;
-
-    if (USE_ASYNC_IOBDMA)
-    {
-        /* Save scratch in case userspace is using it */
-        CVMX_SYNCIOBDMA;
-        old_scratch = cvmx_scratch_read64(CVMX_SCR_SCRATCH);
-        old_scratch2 = cvmx_scratch_read64(CVMX_SCR_SCRATCH+8);
-
-        /* Assume we're going to be able t osend this packet. Fetch and increment
-            the number of pending packets for output */
-        cvmx_fau_async_fetch_and_add32(CVMX_SCR_SCRATCH+8, FAU_NUM_PACKET_BUFFERS_TO_FREE, 0);
-        cvmx_fau_async_fetch_and_add32(CVMX_SCR_SCRATCH, priv->fau+qos*4, 1);
-    }
-
-    /* The CN3XXX series of parts has an errata (GMX-401) which causes the GMX
-        block to hang if a collision occurs towards the end of a <68 byte
-        packet. As a workaround for this, we pad packets to be 68 bytes
-        whenever we are in half duplex mode. We don't handle the case of having
-        a small packet but no room to add the padding. The kernel should always
-        give us at least a cache line */
-    if ((skb->len < 64) && OCTEON_IS_MODEL(OCTEON_CN3XXX))
-    {
-        cvmx_gmxx_prtx_cfg_t gmx_prt_cfg;
-        int interface = INTERFACE(priv->port);
-        int index = INDEX(priv->port);
-
-        /* We only need to pad packet in half duplex mode */
-        gmx_prt_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
-        if (gmx_prt_cfg.s.duplex == 0)
-        {
-            int add_bytes = 64 - skb->len;
-            if (skb->tail + add_bytes <= skb->end)
-                memset(__skb_put(skb, add_bytes), 0, add_bytes);
-        }
-    }
-
-    /* Build the PKO buffer pointer */
-    hw_buffer.u64 = 0;
-    hw_buffer.s.addr = cvmx_ptr_to_phys(skb->data);
-    hw_buffer.s.pool = 0;
-    hw_buffer.s.size = (unsigned long)skb->end - (unsigned long)skb->head;
-
-    /* Build the PKO command */
-    pko_command.u64 = 0;
-    pko_command.s.n2 = 1; /* Don't pollute L2 with the outgoing packet */
-    pko_command.s.segs = 1;
-    pko_command.s.total_bytes = skb->len;
-    pko_command.s.size0 = CVMX_FAU_OP_SIZE_32;
-    pko_command.s.subone0 = 1;
-
-    pko_command.s.dontfree = 1;
-    pko_command.s.reg0 = priv->fau+qos*4;
-    /* See if we can put this skb in the FPA pool. Any strange behavior from
-        the Linux networking stack will most likely be caused by a bug in the
-        following code. If some field is in use by the network stack and get
-        carried over when a buffer is reused, bad thing may happen. If in
-        doubt and you dont need the absolute best performance, disable the
-        define REUSE_SKBUFFS_WITHOUT_FREE. The reuse of buffers has showen
-        a 25% increase in performance under some loads */
+	/* Prefetch the private data structure.
+	   It is larger that one cache line */
+	CVMX_PREFETCH(priv, 0);
+
+	/* Start off assuming no drop */
+	dropped = 0;
+
+	/* The check on CVMX_PKO_QUEUES_PER_PORT_* is designed to completely
+	   remove "qos" in the event neither interface supports multiple queues
+	   per port */
+	if ((CVMX_PKO_QUEUES_PER_PORT_INTERFACE0 > 1) ||
+	    (CVMX_PKO_QUEUES_PER_PORT_INTERFACE1 > 1)) {
+		qos = GET_SKBUFF_QOS(skb);
+		if (qos <= 0)
+			qos = 0;
+		else if (qos >= cvmx_pko_get_num_queues(priv->port))
+			qos = 0;
+	} else
+		qos = 0;
+
+	if (USE_ASYNC_IOBDMA) {
+		/* Save scratch in case userspace is using it */
+		CVMX_SYNCIOBDMA;
+		old_scratch = cvmx_scratch_read64(CVMX_SCR_SCRATCH);
+		old_scratch2 = cvmx_scratch_read64(CVMX_SCR_SCRATCH+8);
+
+		/* Assume we're going to be able t osend this packet. Fetch and increment
+		   the number of pending packets for output */
+		cvmx_fau_async_fetch_and_add32(CVMX_SCR_SCRATCH+8, FAU_NUM_PACKET_BUFFERS_TO_FREE, 0);
+		cvmx_fau_async_fetch_and_add32(CVMX_SCR_SCRATCH, priv->fau+qos*4, 1);
+	}
+
+	/* The CN3XXX series of parts has an errata (GMX-401) which causes the
+	   GMX block to hang if a collision occurs towards the end of a
+	   <68 byte packet. As a workaround for this, we pad packets to be
+	   68 bytes whenever we are in half duplex mode. We don't handle
+	   the case of having a small packet but no room to add the padding.
+	   The kernel should always give us at least a cache line */
+	if ((skb->len < 64) && OCTEON_IS_MODEL(OCTEON_CN3XXX)) {
+		cvmx_gmxx_prtx_cfg_t gmx_prt_cfg;
+		int interface = INTERFACE(priv->port);
+		int index = INDEX(priv->port);
+
+		/* We only need to pad packet in half duplex mode */
+		gmx_prt_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
+		if (gmx_prt_cfg.s.duplex == 0) {
+			int add_bytes = 64 - skb->len;
+			if (skb->tail + add_bytes <= skb->end)
+				memset(__skb_put(skb, add_bytes), 0, add_bytes);
+		}
+	}
+
+	/* Build the PKO buffer pointer */
+	hw_buffer.u64 = 0;
+	hw_buffer.s.addr = cvmx_ptr_to_phys(skb->data);
+	hw_buffer.s.pool = 0;
+	hw_buffer.s.size = (unsigned long)skb->end - (unsigned long)skb->head;
+
+	/* Build the PKO command */
+	pko_command.u64 = 0;
+	pko_command.s.n2 = 1; /* Don't pollute L2 with the outgoing packet */
+	pko_command.s.segs = 1;
+	pko_command.s.total_bytes = skb->len;
+	pko_command.s.size0 = CVMX_FAU_OP_SIZE_32;
+	pko_command.s.subone0 = 1;
+
+	pko_command.s.dontfree = 1;
+	pko_command.s.reg0 = priv->fau+qos*4;
+	/* See if we can put this skb in the FPA pool. Any strange behavior
+	   from the Linux networking stack will most likely be caused by a bug
+	   in the following code. If some field is in use by the network stack
+	   and get carried over when a buffer is reused, bad thing may happen.
+	   If in doubt and you dont need the absolute best performance, disable
+	   the define REUSE_SKBUFFS_WITHOUT_FREE. The reuse of buffers has
+	   shown a 25% increase in performance under some loads */
 #if REUSE_SKBUFFS_WITHOUT_FREE
-    fpa_head = skb->head + 128 - ((unsigned long)skb->head&0x7f);
-    if (unlikely(skb->data < fpa_head))
-    {
-        //printk("TX buffer beginning can't meet FPA alignment constraints\n");
-        goto dont_put_skbuff_in_hw;
-    }
+	fpa_head = skb->head + 128 - ((unsigned long)skb->head&0x7f);
+	if (unlikely(skb->data < fpa_head)) {
+		/*
+		printk("TX buffer beginning can't meet FPA alignment constraints\n");
+		*/
+		goto dont_put_skbuff_in_hw;
+	}
 #ifdef NET_SKBUFF_DATA_USES_OFFSET
 #warning math on skb->* is not recommended
 #endif
-    if (unlikely(skb->end >=  fpa_head + CVMX_FPA_PACKET_POOL_SIZE))
-    {
-        //printk("TX buffer isn't large enough for the FPA\n");
-        goto dont_put_skbuff_in_hw;
-    }
-    if (unlikely(skb_shared(skb)))
-    {
-        //printk("TX buffer sharing data with someone else\n");
-        goto dont_put_skbuff_in_hw;
-    }
-    if (unlikely(skb_cloned(skb)))
-    {
-        //printk("TX buffer has been cloned\n");
-        goto dont_put_skbuff_in_hw;
-    }
-    if (unlikely(skb_header_cloned(skb)))
-    {
-        //printk("TX buffer header has been cloned\n");
-        goto dont_put_skbuff_in_hw;
-    }
-    if (unlikely(skb->destructor))
-    {
-        //printk("TX buffer has a destructor\n");
-        goto dont_put_skbuff_in_hw;
-    }
-    if (unlikely(skb_shinfo(skb)->nr_frags))
-    {
-        //printk("TX buffer has fragments\n");
-        goto dont_put_skbuff_in_hw;
-    }
+	if (unlikely(skb->end >=  fpa_head + CVMX_FPA_PACKET_POOL_SIZE)) {
+		/*
+		printk("TX buffer isn't large enough for the FPA\n");
+		*/
+		goto dont_put_skbuff_in_hw;
+	}
+	if (unlikely(skb_shared(skb))) {
+		/*
+		printk("TX buffer sharing data with someone else\n");
+		*/
+		goto dont_put_skbuff_in_hw;
+	}
+	if (unlikely(skb_cloned(skb))) {
+		/*
+		printk("TX buffer has been cloned\n");
+		*/
+		goto dont_put_skbuff_in_hw;
+	}
+	if (unlikely(skb_header_cloned(skb))) {
+		/*
+		printk("TX buffer header has been cloned\n");
+		*/
+		goto dont_put_skbuff_in_hw;
+	}
+	if (unlikely(skb->destructor)) {
+		/*
+		printk("TX buffer has a destructor\n");
+		*/
+		goto dont_put_skbuff_in_hw;
+	}
+	if (unlikely(skb_shinfo(skb)->nr_frags)) {
+		/*
+		printk("TX buffer has fragments\n");
+		*/
+		goto dont_put_skbuff_in_hw;
+	}
 #ifdef NET_SKBUFF_DATA_USES_OFFSET
 #warning math on skb->* is not recommended
 #else
-    if (unlikely(skb->truesize != sizeof(*skb) + skb->end - skb->head))
-    {
-        //printk("TX buffer truesize has been changed\n");
-        goto dont_put_skbuff_in_hw;
-    }
+	if (unlikely(skb->truesize != sizeof(*skb) + skb->end - skb->head)) {
+		/*
+		printk("TX buffer truesize has been changed\n");
+		*/
+		goto dont_put_skbuff_in_hw;
+	}
 #endif
 
-    /* We can use this buffer in the FPA. We don't need the FAU update anymore */
-    pko_command.s.reg0 = 0;
-    pko_command.s.dontfree = 0;
+	/* We can use this buffer in the FPA.
+	   We don't need the FAU update anymore */
+	pko_command.s.reg0 = 0;
+	pko_command.s.dontfree = 0;
 
-    hw_buffer.s.back = (skb->data - fpa_head)>>7;
-    *(struct sk_buff **)(fpa_head-sizeof(void*)) = skb;
+	hw_buffer.s.back = (skb->data - fpa_head)>>7;
+	*(struct sk_buff **)(fpa_head-sizeof(void *)) = skb;
 
-    /* The skbuff will be reused without ever being freed. We must cleanup a
-        bunch of Linux stuff */
-    dst_release(skb->dst);
-    skb->dst = NULL;
+	/* The skbuff will be reused without ever being freed. We must cleanup a
+	   bunch of Linux stuff */
+	dst_release(skb->dst);
+	skb->dst = NULL;
 #ifdef CONFIG_XFRM
-    secpath_put(skb->sp);
-    skb->sp = NULL;
+	secpath_put(skb->sp);
+	skb->sp = NULL;
 #endif
-    nf_reset(skb);
+	nf_reset(skb);
 #ifdef CONFIG_BRIDGE_NETFILTER
-    /* The next two lines are done in nf_reset() for 2.6.21. 2.6.16 needs
-        them. I'm leaving it for all versions since the compiler will optimize
-        them away when they aren't needed. It can tell that skb->nf_bridge
-        was set to NULL in the inlined nf_reset(). */
-    nf_bridge_put(skb->nf_bridge);
-    skb->nf_bridge = NULL;
+	/* The next two lines are done in nf_reset() for 2.6.21. 2.6.16 needs
+	   them. I'm leaving it for all versions since the compiler will
+	   optimize them away when they aren't needed. It can tell that
+	   skb->nf_bridge was set to NULL in the inlined nf_reset(). */
+	nf_bridge_put(skb->nf_bridge);
+	skb->nf_bridge = NULL;
 #endif /* CONFIG_BRIDGE_NETFILTER */
 #ifdef CONFIG_NET_SCHED
-    skb->tc_index = 0;
+	skb->tc_index = 0;
 #ifdef CONFIG_NET_CLS_ACT
-    skb->tc_verd = 0;
+	skb->tc_verd = 0;
 #endif /* CONFIG_NET_CLS_ACT */
 #endif /* CONFIG_NET_SCHED */
 
 dont_put_skbuff_in_hw:
 #endif /* REUSE_SKBUFFS_WITHOUT_FREE */
 
-    /* Check if we can use the hardware checksumming */
-    if (USE_HW_TCPUDP_CHECKSUM && (skb->protocol == htons(ETH_P_IP)) &&
-        (ip_hdr(skb)->version == 4) && (ip_hdr(skb)->ihl == 5) &&
-        ((ip_hdr(skb)->frag_off == 0) || (ip_hdr(skb)->frag_off == 1<<14)) &&
-        ((ip_hdr(skb)->protocol == IP_PROTOCOL_TCP) || (ip_hdr(skb)->protocol == IP_PROTOCOL_UDP)))
-    {
-        /* Use hardware checksum calc */
-        pko_command.s.ipoffp1 = sizeof(struct ethhdr) + 1;
-    }
-
-    if (USE_ASYNC_IOBDMA)
-    {
-        /* Get the number of skbuffs in use by the hardware */
-        CVMX_SYNCIOBDMA;
-        in_use = cvmx_scratch_read64(CVMX_SCR_SCRATCH);
-        buffers_to_free = cvmx_scratch_read64(CVMX_SCR_SCRATCH+8);
-    }
-    else
-    {
-        /* Get the number of skbuffs in use by the hardware */
-        in_use = cvmx_fau_fetch_and_add32(priv->fau+qos*4, 1);
-        buffers_to_free = cvmx_fau_fetch_and_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE, 0);
-    }
-
-    /* If we're sending faster than the receive can free them then don't do
-        the HW free */
-    if ((buffers_to_free<-100) && !pko_command.s.dontfree)
-    {
-        pko_command.s.dontfree = 1;
-        pko_command.s.reg0 = priv->fau+qos*4;
-    }
-
-    cvmx_pko_send_packet_prepare(priv->port, priv->queue + qos, CVMX_PKO_LOCK_CMD_QUEUE);
-
-    /* Drop this packet if we have too many already queued to the HW */
-    if (unlikely(skb_queue_len(&priv->tx_free_list[qos]) >= dev->tx_queue_len))
-    {
-        //DEBUGPRINT("%s: Tx dropped. Too many queued\n", dev->name);
-        dropped=1;
-    }
-    /* Send the packet to the output queue */
-    else if (unlikely(cvmx_pko_send_packet_finish(priv->port, priv->queue + qos, pko_command, hw_buffer, CVMX_PKO_LOCK_CMD_QUEUE)))
-    {
-        DEBUGPRINT("%s: Failed to send the packet\n", dev->name);
-        dropped=1;
-    }
-
-    if (USE_ASYNC_IOBDMA)
-    {
-        /* Restore the scratch area */
-        cvmx_scratch_write64(CVMX_SCR_SCRATCH, old_scratch);
-        cvmx_scratch_write64(CVMX_SCR_SCRATCH+8, old_scratch2);
-    }
-
-    if (unlikely(dropped))
-    {
-        dev_kfree_skb_any(skb);
-        cvmx_fau_atomic_add32(priv->fau+qos*4, -1);
-        priv->stats.tx_dropped++;
-    }
-    else
-    {
-        if (USE_SKBUFFS_IN_HW)
-        {
-            /* Put this packet on the queue to be freed later */
-            if (pko_command.s.dontfree)
-                skb_queue_tail(&priv->tx_free_list[qos], skb);
-            else
-            {
-                cvmx_fau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE, -1);
-                cvmx_fau_atomic_add32(priv->fau+qos*4, -1);
-            }
-        }
-        else
-        {
-            /* Put this packet on the queue to be freed later */
-            skb_queue_tail(&priv->tx_free_list[qos], skb);
-        }
-    }
-
-    /* Free skbuffs not in use by the hardware, possibly two at a time */
-    if (skb_queue_len(&priv->tx_free_list[qos]) > in_use)
-    {
-        spin_lock(&priv->tx_free_list[qos].lock);
-        /* Check again now that we have the lock. It might have changed */
-        if (skb_queue_len(&priv->tx_free_list[qos]) > in_use)
-            dev_kfree_skb(__skb_dequeue(&priv->tx_free_list[qos]));
-        if (skb_queue_len(&priv->tx_free_list[qos]) > in_use)
-            dev_kfree_skb(__skb_dequeue(&priv->tx_free_list[qos]));
-        spin_unlock(&priv->tx_free_list[qos].lock);
-    }
-
-    return 0;
+	/* Check if we can use the hardware checksumming */
+	if (USE_HW_TCPUDP_CHECKSUM && (skb->protocol == htons(ETH_P_IP)) &&
+	    (ip_hdr(skb)->version == 4) && (ip_hdr(skb)->ihl == 5) &&
+	    ((ip_hdr(skb)->frag_off == 0) || (ip_hdr(skb)->frag_off == 1<<14)) &&
+	    ((ip_hdr(skb)->protocol == IP_PROTOCOL_TCP) || (ip_hdr(skb)->protocol == IP_PROTOCOL_UDP))) {
+		/* Use hardware checksum calc */
+		pko_command.s.ipoffp1 = sizeof(struct ethhdr) + 1;
+	}
+
+	if (USE_ASYNC_IOBDMA) {
+		/* Get the number of skbuffs in use by the hardware */
+		CVMX_SYNCIOBDMA;
+		in_use = cvmx_scratch_read64(CVMX_SCR_SCRATCH);
+		buffers_to_free = cvmx_scratch_read64(CVMX_SCR_SCRATCH+8);
+	} else {
+		/* Get the number of skbuffs in use by the hardware */
+		in_use = cvmx_fau_fetch_and_add32(priv->fau+qos*4, 1);
+		buffers_to_free = cvmx_fau_fetch_and_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE, 0);
+	}
+
+	/* If we're sending faster than the receive can free them then don't do
+	   the HW free */
+	if ((buffers_to_free < -100) && !pko_command.s.dontfree) {
+		pko_command.s.dontfree = 1;
+		pko_command.s.reg0 = priv->fau+qos*4;
+	}
+
+	cvmx_pko_send_packet_prepare(priv->port, priv->queue + qos, CVMX_PKO_LOCK_CMD_QUEUE);
+
+	/* Drop this packet if we have too many already queued to the HW */
+	if (unlikely(skb_queue_len(&priv->tx_free_list[qos]) >= dev->tx_queue_len)) {
+		/*
+		DEBUGPRINT("%s: Tx dropped. Too many queued\n", dev->name);
+		*/
+		dropped = 1;
+	}
+	/* Send the packet to the output queue */
+	else
+	if (unlikely(cvmx_pko_send_packet_finish(priv->port, priv->queue + qos, pko_command, hw_buffer, CVMX_PKO_LOCK_CMD_QUEUE))) {
+		DEBUGPRINT("%s: Failed to send the packet\n", dev->name);
+		dropped = 1;
+	}
+
+	if (USE_ASYNC_IOBDMA) {
+		/* Restore the scratch area */
+		cvmx_scratch_write64(CVMX_SCR_SCRATCH, old_scratch);
+		cvmx_scratch_write64(CVMX_SCR_SCRATCH+8, old_scratch2);
+	}
+
+	if (unlikely(dropped)) {
+		dev_kfree_skb_any(skb);
+		cvmx_fau_atomic_add32(priv->fau+qos*4, -1);
+		priv->stats.tx_dropped++;
+	} else {
+		if (USE_SKBUFFS_IN_HW) {
+			/* Put this packet on the queue to be freed later */
+			if (pko_command.s.dontfree)
+				skb_queue_tail(&priv->tx_free_list[qos], skb);
+			else {
+				cvmx_fau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE, -1);
+				cvmx_fau_atomic_add32(priv->fau+qos*4, -1);
+			}
+		} else {
+			/* Put this packet on the queue to be freed later */
+			skb_queue_tail(&priv->tx_free_list[qos], skb);
+		}
+	}
+
+	/* Free skbuffs not in use by the hardware, possibly two at a time */
+	if (skb_queue_len(&priv->tx_free_list[qos]) > in_use) {
+		spin_lock(&priv->tx_free_list[qos].lock);
+		/* Check again now that we have the lock. It might have changed */
+		if (skb_queue_len(&priv->tx_free_list[qos]) > in_use)
+			dev_kfree_skb(__skb_dequeue(&priv->tx_free_list[qos]));
+		if (skb_queue_len(&priv->tx_free_list[qos]) > in_use)
+			dev_kfree_skb(__skb_dequeue(&priv->tx_free_list[qos]));
+		spin_unlock(&priv->tx_free_list[qos].lock);
+	}
+
+	return 0;
 }
 
 
@@ -364,107 +356,115 @@ dont_put_skbuff_in_hw:
  */
 int cvm_oct_xmit_pow(struct sk_buff *skb, struct net_device *dev)
 {
-    cvm_oct_private_t*  priv = (cvm_oct_private_t*)netdev_priv(dev);
-    void *              packet_buffer;
-    void *              copy_location;
-
-    /* Get a work queue entry */
-    cvmx_wqe_t *work = cvmx_fpa_alloc(CVMX_FPA_WQE_POOL);
-    if (unlikely(work == NULL))
-    {
-        DEBUGPRINT("%s: Failed to allocate a work queue entry\n", dev->name);
-        priv->stats.tx_dropped++;
-        dev_kfree_skb(skb);
-        return 0;
-    }
-
-    /* Get a packet buffer */
-    packet_buffer = cvmx_fpa_alloc(CVMX_FPA_PACKET_POOL);
-    if (unlikely(packet_buffer == NULL))
-    {
-        DEBUGPRINT("%s: Failed to allocate a packet buffer\n", dev->name);
-        cvmx_fpa_free(work, CVMX_FPA_WQE_POOL, DONT_WRITEBACK(1));
-        priv->stats.tx_dropped++;
-        dev_kfree_skb(skb);
-        return 0;
-    }
-
-    /* Calculate where we need to copy the data to. We need to leave 8 bytes
-        for a next pointer (unused). We also need to include any configure
-        skip. Then we need to align the IP packet src and dest into the same
-        64bit word. The below calculation may add a little extra, but that
-        doesn't hurt */
-    copy_location = packet_buffer + sizeof(uint64_t);
-    copy_location += ((CVMX_HELPER_FIRST_MBUFF_SKIP+7)&0xfff8) + 6;
-
-    /* We have to copy the packet since whoever processes this packet
-        will free it to a hardware pool. We can't use the trick of
-        counting outstanding packets like in cvm_oct_xmit */
-    memcpy(copy_location, skb->data, skb->len);
-
-    /* Fill in some of the work queue fields. We may need to add more
-        if the software at the other end needs them */
-    work->hw_chksum     = skb->csum;
-    work->len           = skb->len;
-    work->ipprt         = priv->port;
-    work->qos           = priv->port & 0x7;
-    work->grp           = pow_send_group;
-    work->tag_type      = CVMX_HELPER_INPUT_TAG_TYPE;
-    work->tag           = pow_send_group; /* FIXME */
-    work->word2.u64     = 0;    /* Default to zero. Sets of zero later are commented out */
-    work->word2.s.bufs  = 1;
-    work->packet_ptr.u64 = 0;
-    work->packet_ptr.s.addr = cvmx_ptr_to_phys(copy_location);
-    work->packet_ptr.s.pool = CVMX_FPA_PACKET_POOL;
-    work->packet_ptr.s.size = CVMX_FPA_PACKET_POOL_SIZE;
-    work->packet_ptr.s.back = (copy_location - packet_buffer)>>7;
-
-    if (skb->protocol == htons(ETH_P_IP))
-    {
-        work->word2.s.ip_offset     = 14;
-        //work->word2.s.vlan_valid  = 0; /* FIXME */
-        //work->word2.s.vlan_cfi    = 0; /* FIXME */
-        //work->word2.s.vlan_id     = 0; /* FIXME */
-        //work->word2.s.dec_ipcomp  = 0; /* FIXME */
-        work->word2.s.tcp_or_udp    = (ip_hdr(skb)->protocol == IP_PROTOCOL_TCP) || (ip_hdr(skb)->protocol == IP_PROTOCOL_UDP);
-        //work->word2.s.dec_ipsec   = 0; /* FIXME */
-        //work->word2.s.is_v6       = 0; /* We only support IPv4 right now */
-        //work->word2.s.software    = 0; /* Hardware would set to zero */
-        //work->word2.s.L4_error    = 0; /* No error, packet is internal */
-        work->word2.s.is_frag       = !((ip_hdr(skb)->frag_off == 0) || (ip_hdr(skb)->frag_off == 1<<14));
-        //work->word2.s.IP_exc      = 0;  /* Assume Linux is sending a good packet */
-        work->word2.s.is_bcast      = (skb->pkt_type==PACKET_BROADCAST);
-        work->word2.s.is_mcast      = (skb->pkt_type==PACKET_MULTICAST);
-        //work->word2.s.not_IP      = 0; /* This is an IP packet */
-        //work->word2.s.rcv_error   = 0; /* No error, packet is internal */
-        //work->word2.s.err_code    = 0; /* No error, packet is internal */
-
-        /* When copying the data, include 4 bytes of the ethernet header to
-            align the same way hardware does */
-        memcpy(work->packet_data, skb->data + 10, sizeof(work->packet_data));
-    }
-    else
-    {
-        //work->word2.snoip.vlan_valid  = 0; /* FIXME */
-        //work->word2.snoip.vlan_cfi    = 0; /* FIXME */
-        //work->word2.snoip.vlan_id     = 0; /* FIXME */
-        //work->word2.snoip.software    = 0; /* Hardware would set to zero */
-        work->word2.snoip.is_rarp       = skb->protocol == htons(ETH_P_RARP);
-        work->word2.snoip.is_arp        = skb->protocol == htons(ETH_P_ARP);
-        work->word2.snoip.is_bcast      = (skb->pkt_type==PACKET_BROADCAST);
-        work->word2.snoip.is_mcast      = (skb->pkt_type==PACKET_MULTICAST);
-        work->word2.snoip.not_IP        = 1; /* IP was done up above */
-        //work->word2.snoip.rcv_error   = 0; /* No error, packet is internal */
-        //work->word2.snoip.err_code    = 0; /* No error, packet is internal */
-        memcpy(work->packet_data, skb->data, sizeof(work->packet_data));
-    }
-
-    /* Submit the packet to the POW */
-    cvmx_pow_work_submit(work, work->tag, work->tag_type, work->qos, work->grp);
-    priv->stats.tx_packets++;
-    priv->stats.tx_bytes += skb->len;
-    dev_kfree_skb(skb);
-    return 0;
+	cvm_oct_private_t  *priv = (cvm_oct_private_t *)netdev_priv(dev);
+	void               *packet_buffer;
+	void               *copy_location;
+
+	/* Get a work queue entry */
+	cvmx_wqe_t *work = cvmx_fpa_alloc(CVMX_FPA_WQE_POOL);
+	if (unlikely(work == NULL)) {
+		DEBUGPRINT("%s: Failed to allocate a work queue entry\n", dev->name);
+		priv->stats.tx_dropped++;
+		dev_kfree_skb(skb);
+		return 0;
+	}
+
+	/* Get a packet buffer */
+	packet_buffer = cvmx_fpa_alloc(CVMX_FPA_PACKET_POOL);
+	if (unlikely(packet_buffer == NULL)) {
+		DEBUGPRINT("%s: Failed to allocate a packet buffer\n",
+			   dev->name);
+		cvmx_fpa_free(work, CVMX_FPA_WQE_POOL, DONT_WRITEBACK(1));
+		priv->stats.tx_dropped++;
+		dev_kfree_skb(skb);
+		return 0;
+	}
+
+	/* Calculate where we need to copy the data to. We need to leave 8 bytes
+	   for a next pointer (unused). We also need to include any configure
+	   skip. Then we need to align the IP packet src and dest into the same
+	   64bit word. The below calculation may add a little extra, but that
+	   doesn't hurt */
+	copy_location = packet_buffer + sizeof(uint64_t);
+	copy_location += ((CVMX_HELPER_FIRST_MBUFF_SKIP+7)&0xfff8) + 6;
+
+	/* We have to copy the packet since whoever processes this packet
+	   will free it to a hardware pool. We can't use the trick of
+	   counting outstanding packets like in cvm_oct_xmit */
+	memcpy(copy_location, skb->data, skb->len);
+
+	/* Fill in some of the work queue fields. We may need to add more
+	   if the software at the other end needs them */
+	work->hw_chksum     = skb->csum;
+	work->len           = skb->len;
+	work->ipprt         = priv->port;
+	work->qos           = priv->port & 0x7;
+	work->grp           = pow_send_group;
+	work->tag_type      = CVMX_HELPER_INPUT_TAG_TYPE;
+	work->tag           = pow_send_group; /* FIXME */
+	work->word2.u64     = 0;    /* Default to zero. Sets of zero later are commented out */
+	work->word2.s.bufs  = 1;
+	work->packet_ptr.u64 = 0;
+	work->packet_ptr.s.addr = cvmx_ptr_to_phys(copy_location);
+	work->packet_ptr.s.pool = CVMX_FPA_PACKET_POOL;
+	work->packet_ptr.s.size = CVMX_FPA_PACKET_POOL_SIZE;
+	work->packet_ptr.s.back = (copy_location - packet_buffer)>>7;
+
+	if (skb->protocol == htons(ETH_P_IP)) {
+		work->word2.s.ip_offset     = 14;
+		#if 0
+		work->word2.s.vlan_valid  = 0; /* FIXME */
+		work->word2.s.vlan_cfi    = 0; /* FIXME */
+		work->word2.s.vlan_id     = 0; /* FIXME */
+		work->word2.s.dec_ipcomp  = 0; /* FIXME */
+		#endif
+		work->word2.s.tcp_or_udp    = (ip_hdr(skb)->protocol == IP_PROTOCOL_TCP) || (ip_hdr(skb)->protocol == IP_PROTOCOL_UDP);
+		#if 0
+		work->word2.s.dec_ipsec   = 0; /* FIXME */
+		work->word2.s.is_v6       = 0; /* We only support IPv4 right now */
+		work->word2.s.software    = 0; /* Hardware would set to zero */
+		work->word2.s.L4_error    = 0; /* No error, packet is internal */
+		#endif
+		work->word2.s.is_frag       = !((ip_hdr(skb)->frag_off == 0) || (ip_hdr(skb)->frag_off == 1<<14));
+		#if 0
+		work->word2.s.IP_exc      = 0;  /* Assume Linux is sending a good packet */
+		#endif
+		work->word2.s.is_bcast      = (skb->pkt_type == PACKET_BROADCAST);
+		work->word2.s.is_mcast      = (skb->pkt_type == PACKET_MULTICAST);
+		#if 0
+		work->word2.s.not_IP      = 0; /* This is an IP packet */
+		work->word2.s.rcv_error   = 0; /* No error, packet is internal */
+		work->word2.s.err_code    = 0; /* No error, packet is internal */
+		#endif
+
+		/* When copying the data, include 4 bytes of the ethernet header to
+		   align the same way hardware does */
+		memcpy(work->packet_data, skb->data + 10, sizeof(work->packet_data));
+	} else {
+		#if 0
+		work->word2.snoip.vlan_valid  = 0; /* FIXME */
+		work->word2.snoip.vlan_cfi    = 0; /* FIXME */
+		work->word2.snoip.vlan_id     = 0; /* FIXME */
+		work->word2.snoip.software    = 0; /* Hardware would set to zero */
+		#endif
+		work->word2.snoip.is_rarp       = skb->protocol == htons(ETH_P_RARP);
+		work->word2.snoip.is_arp        = skb->protocol == htons(ETH_P_ARP);
+		work->word2.snoip.is_bcast      = (skb->pkt_type == PACKET_BROADCAST);
+		work->word2.snoip.is_mcast      = (skb->pkt_type == PACKET_MULTICAST);
+		work->word2.snoip.not_IP        = 1; /* IP was done up above */
+		#if 0
+		work->word2.snoip.rcv_error   = 0; /* No error, packet is internal */
+		work->word2.snoip.err_code    = 0; /* No error, packet is internal */
+		#endif
+		memcpy(work->packet_data, skb->data, sizeof(work->packet_data));
+	}
+
+	/* Submit the packet to the POW */
+	cvmx_pow_work_submit(work, work->tag, work->tag_type, work->qos, work->grp);
+	priv->stats.tx_packets++;
+	priv->stats.tx_bytes += skb->len;
+	dev_kfree_skb(skb);
+	return 0;
 }
 
 
@@ -488,79 +488,74 @@ int cvm_oct_xmit_pow(struct sk_buff *skb, struct net_device *dev)
  */
 int cvm_oct_transmit_qos(struct net_device *dev, void *work_queue_entry, int do_free, int qos)
 {
-    unsigned long               flags;
-    cvmx_buf_ptr_t              hw_buffer;
-    cvmx_pko_command_word0_t    pko_command;
-    int                         dropped;
-    cvm_oct_private_t*          priv = (cvm_oct_private_t*)netdev_priv(dev);
-    cvmx_wqe_t *                work = work_queue_entry;
-
-    if (!(dev->flags & IFF_UP))
-    {
-        DEBUGPRINT("%s: Device not up\n", dev->name);
-        if (do_free)
-            cvm_oct_free_work(work);
-        return -1;
-    }
-
-    /* The check on CVMX_PKO_QUEUES_PER_PORT_* is designed to completely
-        remove "qos" in the event neither interface supports multiple queues
-        per port */
-    if ((CVMX_PKO_QUEUES_PER_PORT_INTERFACE0 > 1) ||
-        (CVMX_PKO_QUEUES_PER_PORT_INTERFACE1 > 1))
-    {
-        if (qos <= 0)
-            qos = 0;
-        else if (qos >= cvmx_pko_get_num_queues(priv->port))
-            qos = 0;
-    }
-    else
-        qos = 0;
-
-    /* Start off assuming no drop */
-    dropped = 0;
-
-    local_irq_save(flags);
-    cvmx_pko_send_packet_prepare(priv->port, priv->queue + qos, CVMX_PKO_LOCK_CMD_QUEUE);
-
-    /* Build the PKO buffer pointer */
-    hw_buffer.u64 = 0;
-    hw_buffer.s.addr = work->packet_ptr.s.addr;
-    hw_buffer.s.pool = CVMX_FPA_PACKET_POOL;
-    hw_buffer.s.size = CVMX_FPA_PACKET_POOL_SIZE;
-    hw_buffer.s.back = work->packet_ptr.s.back;
-
-    /* Build the PKO command */
-    pko_command.u64 = 0;
-    pko_command.s.n2 = 1; /* Don't pollute L2 with the outgoing packet */
-    pko_command.s.dontfree = !do_free;
-    pko_command.s.segs = work->word2.s.bufs;
-    pko_command.s.total_bytes = work->len;
-
-    /* Check if we can use the hardware checksumming */
-    if (unlikely(work->word2.s.not_IP || work->word2.s.IP_exc))
-        pko_command.s.ipoffp1 = 0;
-    else
-        pko_command.s.ipoffp1 = sizeof(struct ethhdr) + 1;
-
-    /* Send the packet to the output queue */
-    if (unlikely(cvmx_pko_send_packet_finish(priv->port, priv->queue + qos, pko_command, hw_buffer, CVMX_PKO_LOCK_CMD_QUEUE)))
-    {
-        DEBUGPRINT("%s: Failed to send the packet\n", dev->name);
-        dropped=-1;
-    }
-    local_irq_restore(flags);
-
-    if (unlikely(dropped))
-    {
-        if (do_free)
-            cvm_oct_free_work(work);
-        priv->stats.tx_dropped++;
-    }
-    else if (do_free)
-        cvmx_fpa_free(work, CVMX_FPA_WQE_POOL, DONT_WRITEBACK(1));
-
-    return dropped;
+	unsigned long              flags;
+	cvmx_buf_ptr_t             hw_buffer;
+	cvmx_pko_command_word0_t   pko_command;
+	int                        dropped;
+	cvm_oct_private_t         *priv = (cvm_oct_private_t *)netdev_priv(dev);
+	cvmx_wqe_t                *work = work_queue_entry;
+
+	if (!(dev->flags & IFF_UP)) {
+		DEBUGPRINT("%s: Device not up\n", dev->name);
+		if (do_free)
+			cvm_oct_free_work(work);
+		return -1;
+	}
+
+	/* The check on CVMX_PKO_QUEUES_PER_PORT_* is designed to completely
+	   remove "qos" in the event neither interface supports
+	   multiple queues per port */
+	if ((CVMX_PKO_QUEUES_PER_PORT_INTERFACE0 > 1) ||
+		(CVMX_PKO_QUEUES_PER_PORT_INTERFACE1 > 1)) {
+		if (qos <= 0)
+			qos = 0;
+		else if (qos >= cvmx_pko_get_num_queues(priv->port))
+			qos = 0;
+	} else
+		qos = 0;
+
+	/* Start off assuming no drop */
+	dropped = 0;
+
+	local_irq_save(flags);
+	cvmx_pko_send_packet_prepare(priv->port, priv->queue + qos, CVMX_PKO_LOCK_CMD_QUEUE);
+
+	/* Build the PKO buffer pointer */
+	hw_buffer.u64 = 0;
+	hw_buffer.s.addr = work->packet_ptr.s.addr;
+	hw_buffer.s.pool = CVMX_FPA_PACKET_POOL;
+	hw_buffer.s.size = CVMX_FPA_PACKET_POOL_SIZE;
+	hw_buffer.s.back = work->packet_ptr.s.back;
+
+	/* Build the PKO command */
+	pko_command.u64 = 0;
+	pko_command.s.n2 = 1; /* Don't pollute L2 with the outgoing packet */
+	pko_command.s.dontfree = !do_free;
+	pko_command.s.segs = work->word2.s.bufs;
+	pko_command.s.total_bytes = work->len;
+
+	/* Check if we can use the hardware checksumming */
+	if (unlikely(work->word2.s.not_IP || work->word2.s.IP_exc))
+		pko_command.s.ipoffp1 = 0;
+	else
+		pko_command.s.ipoffp1 = sizeof(struct ethhdr) + 1;
+
+	/* Send the packet to the output queue */
+	if (unlikely(cvmx_pko_send_packet_finish(priv->port, priv->queue + qos, pko_command, hw_buffer, CVMX_PKO_LOCK_CMD_QUEUE))) {
+		DEBUGPRINT("%s: Failed to send the packet\n", dev->name);
+		dropped = -1;
+	}
+	local_irq_restore(flags);
+
+	if (unlikely(dropped)) {
+		if (do_free)
+			cvm_oct_free_work(work);
+		priv->stats.tx_dropped++;
+	} else
+	if (do_free)
+		cvmx_fpa_free(work, CVMX_FPA_WQE_POOL, DONT_WRITEBACK(1));
+
+	return dropped;
 }
 EXPORT_SYMBOL(cvm_oct_transmit_qos);
 
@@ -572,15 +567,14 @@ EXPORT_SYMBOL(cvm_oct_transmit_qos);
  */
 void cvm_oct_tx_shutdown(struct net_device *dev)
 {
-    cvm_oct_private_t *priv = (cvm_oct_private_t*)netdev_priv(dev);
-    unsigned long flags;
-    int qos;
-
-    for (qos=0; qos<16; qos++)
-    {
-        spin_lock_irqsave(&priv->tx_free_list[qos].lock, flags);
-        while (skb_queue_len(&priv->tx_free_list[qos]))
-            dev_kfree_skb_any(__skb_dequeue(&priv->tx_free_list[qos]));
-        spin_unlock_irqrestore(&priv->tx_free_list[qos].lock, flags);
-    }
+	cvm_oct_private_t *priv = (cvm_oct_private_t *)netdev_priv(dev);
+	unsigned long flags;
+	int qos;
+
+	for (qos = 0; qos < 16; qos++) {
+		spin_lock_irqsave(&priv->tx_free_list[qos].lock, flags);
+		while (skb_queue_len(&priv->tx_free_list[qos]))
+			dev_kfree_skb_any(__skb_dequeue(&priv->tx_free_list[qos]));
+		spin_unlock_irqrestore(&priv->tx_free_list[qos].lock, flags);
+	}
 }
diff --git a/drivers/net/cavium-ethernet/ethernet-util.h b/drivers/net/cavium-ethernet/ethernet-util.h
index a63da44..0ffe087 100644
--- a/drivers/net/cavium-ethernet/ethernet-util.h
+++ b/drivers/net/cavium-ethernet/ethernet-util.h
@@ -43,7 +43,9 @@
 * OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
 *************************************************************************/
 
-#define DEBUGPRINT(format, ...) do{if (printk_ratelimit()) printk(format, ##__VA_ARGS__);} while (0)
+#define DEBUGPRINT(format, ...) do { if (printk_ratelimit()) 		\
+					printk(format, ##__VA_ARGS__);	\
+				} while (0)
 
 /**
  * Given a packet data address, return a pointer to the
@@ -54,7 +56,7 @@
  */
 static inline void *cvm_oct_get_buffer_ptr(cvmx_buf_ptr_t packet_ptr)
 {
-    return cvmx_phys_to_ptr(((packet_ptr.s.addr >> 7) - packet_ptr.s.back) << 7);
+	return cvmx_phys_to_ptr(((packet_ptr.s.addr >> 7) - packet_ptr.s.back) << 7);
 }
 
 
@@ -68,16 +70,16 @@ static inline void *cvm_oct_get_buffer_ptr(cvmx_buf_ptr_t packet_ptr)
  */
 static inline int INTERFACE(int ipd_port)
 {
-    if (ipd_port<32)    /* Interface 0 or 1 for RGMII,GMII,SPI, etc */
-        return ipd_port>>4;
-    else if (ipd_port<36)   /* Interface 2 for NPI */
-        return 2;
-    else if (ipd_port<40)   /* Interface 3 for loopback */
-        return 3;
-    else if (ipd_port==40)  /* Non existant interface for POW0 */
-        return 4;
-    else
-        panic("Illegal ipd_port %d passed to INTERFACE\n", ipd_port);
+	if (ipd_port < 32)    /* Interface 0 or 1 for RGMII,GMII,SPI, etc */
+		return ipd_port>>4;
+	else if (ipd_port < 36)   /* Interface 2 for NPI */
+		return 2;
+	else if (ipd_port < 40)   /* Interface 3 for loopback */
+		return 3;
+	else if (ipd_port == 40)  /* Non existant interface for POW0 */
+		return 4;
+	else
+		panic("Illegal ipd_port %d passed to INTERFACE\n", ipd_port);
 }
 
 
@@ -91,9 +93,9 @@ static inline int INTERFACE(int ipd_port)
  */
 static inline int INDEX(int ipd_port)
 {
-    if (ipd_port<32)
-        return ipd_port&15;
-    else
-        return ipd_port&3;
+	if (ipd_port < 32)
+		return ipd_port & 15;
+	else
+		return ipd_port & 3;
 }
 
diff --git a/drivers/net/cavium-ethernet/ethernet-xaui.c b/drivers/net/cavium-ethernet/ethernet-xaui.c
index b402e30..e0fa881 100644
--- a/drivers/net/cavium-ethernet/ethernet-xaui.c
+++ b/drivers/net/cavium-ethernet/ethernet-xaui.c
@@ -53,7 +53,7 @@
 static int cvm_oct_xaui_open(struct net_device *dev)
 {
     cvmx_gmxx_prtx_cfg_t gmx_cfg;
-    cvm_oct_private_t* priv = (cvm_oct_private_t*)netdev_priv(dev);
+    cvm_oct_private_t *priv = (cvm_oct_private_t *)netdev_priv(dev);
     int interface = INTERFACE(priv->port);
     int index = INDEX(priv->port);
 
@@ -66,7 +66,7 @@ static int cvm_oct_xaui_open(struct net_device *dev)
 static int cvm_oct_xaui_stop(struct net_device *dev)
 {
     cvmx_gmxx_prtx_cfg_t gmx_cfg;
-    cvm_oct_private_t* priv = (cvm_oct_private_t*)netdev_priv(dev);
+    cvm_oct_private_t *priv = (cvm_oct_private_t *)netdev_priv(dev);
     int interface = INTERFACE(priv->port);
     int index = INDEX(priv->port);
 
diff --git a/drivers/net/cavium-ethernet/ethernet.c b/drivers/net/cavium-ethernet/ethernet.c
index eca27f9..fe4a4b8 100644
--- a/drivers/net/cavium-ethernet/ethernet.c
+++ b/drivers/net/cavium-ethernet/ethernet.c
@@ -54,39 +54,39 @@
 int pow_receive_group = 15;
 module_param(pow_receive_group, int, 0444);
 MODULE_PARM_DESC(pow_receive_group, "\n"
-                 "\t\tPOW group to receive packets from. All ethernet hardware\n"
-                 "\t\twill be configured to send incomming packets to this POW\n"
-                 "\t\tgroup. Also any other software can submit packets to this\n"
-                 "\t\tgroup for the kernel to process.");
+		 "\t\tPOW group to receive packets from. All ethernet hardware\n"
+		 "\t\twill be configured to send incomming packets to this POW\n"
+		 "\t\tgroup. Also any other software can submit packets to this\n"
+		 "\t\tgroup for the kernel to process.");
 
 int pow_send_group = -1;
 module_param(pow_send_group, int, 0644);
 MODULE_PARM_DESC(pow_send_group, "\n"
-                 "\t\tPOW group to send packets to other software on. This\n"
-                 "\t\tcontrols the creation of the virtual device pow0.\n"
-                 "\t\talways_use_pow also depends on this value.");
+		 "\t\tPOW group to send packets to other software on. This\n"
+		 "\t\tcontrols the creation of the virtual device pow0.\n"
+		 "\t\talways_use_pow also depends on this value.");
 
-int always_use_pow = 0;
+int always_use_pow;
 module_param(always_use_pow, int, 0444);
 MODULE_PARM_DESC(always_use_pow, "\n"
-                 "\t\tWhen set, always send to the pow group. This will cause\n"
-                 "\t\tpackets sent to real ethernet devices to be sent to the\n"
-                 "\t\tPOW group instead of the hardware. Unless some other\n"
-                 "\t\tapplication changes the config, packets will still be\n"
-                 "\t\treceived from the low level hardware. Use this option\n"
-                 "\t\tto allow a CVMX app to intercept all packets from the\n"
-                 "\t\tlinux kernel. You must specify pow_send_group along with\n"
-                 "\t\tthis option.");
+		 "\t\tWhen set, always send to the pow group. This will cause\n"
+		 "\t\tpackets sent to real ethernet devices to be sent to the\n"
+		 "\t\tPOW group instead of the hardware. Unless some other\n"
+		 "\t\tapplication changes the config, packets will still be\n"
+		 "\t\treceived from the low level hardware. Use this option\n"
+		 "\t\tto allow a CVMX app to intercept all packets from the\n"
+		 "\t\tlinux kernel. You must specify pow_send_group along with\n"
+		 "\t\tthis option.");
 
 char pow_send_list[128] = "";
 module_param_string(pow_send_list, pow_send_list, sizeof(pow_send_list), 0444);
 MODULE_PARM_DESC(pow_send_list, "\n"
-                 "\t\tComma separated list of ethernet devices that should use the\n"
-                 "\t\tPOW for transmit instead of the actual ethernet hardware. This\n"
-                 "\t\tis a per port version of always_use_pow. always_use_pow takes\n"
-                 "\t\tprecedence over this list. For example, setting this to\n"
-                 "\t\t\"eth2,spi3,spi7\" would cause these three devices to transmit\n"
-                 "\t\tusing the pow_send_group.");
+		 "\t\tComma separated list of ethernet devices that should use the\n"
+		 "\t\tPOW for transmit instead of the actual ethernet hardware. This\n"
+		 "\t\tis a per port version of always_use_pow. always_use_pow takes\n"
+		 "\t\tprecedence over this list. For example, setting this to\n"
+		 "\t\t\"eth2,spi3,spi7\" would cause these three devices to transmit\n"
+		 "\t\tusing the pow_send_group.");
 
 extern int octeon_is_simulation(void);
 
@@ -115,43 +115,37 @@ struct net_device *cvm_oct_device[TOTAL_NUMBER_OF_PORTS];
  */
 static void cvm_do_timer(unsigned long arg)
 {
-    static int port = 0;
-    if (port < CVMX_PIP_NUM_INPUT_PORTS)
-    {
-        if (cvm_oct_device[port])
-        {
-            int queues_per_port;
-            int qos;
-            cvm_oct_private_t *priv = (cvm_oct_private_t*)netdev_priv(cvm_oct_device[port]);
-            if (priv->poll)
-                priv->poll(cvm_oct_device[port]);
-
-            queues_per_port = cvmx_pko_get_num_queues(port);
-            /* Drain any pending packets in the free list */
-            for (qos=0; qos<queues_per_port; qos++)
-            {
-                if (skb_queue_len(&priv->tx_free_list[qos]))
-                {
-                    spin_lock(&priv->tx_free_list[qos].lock);
-                    while (skb_queue_len(&priv->tx_free_list[qos]) > cvmx_fau_fetch_and_add32(priv->fau+qos*4, 0))
-                        dev_kfree_skb(__skb_dequeue(&priv->tx_free_list[qos]));
-                    spin_unlock(&priv->tx_free_list[qos].lock);
-                }
-            }
-            cvm_oct_device[port]->get_stats(cvm_oct_device[port]);
-        }
-        port++;
-        /* Poll the next port in a 50th of a second. This spreads the polling
-            of ports out a little bit */
-        mod_timer(&cvm_oct_poll_timer, jiffies + HZ/50);
-    }
-    else
-    {
-        port = 0;
-        /* All ports have been polled. Start the next iteration through the
-            ports in one second */
-        mod_timer(&cvm_oct_poll_timer, jiffies + HZ);
-    }
+	static int port;
+	if (port < CVMX_PIP_NUM_INPUT_PORTS) {
+		if (cvm_oct_device[port]) {
+			int queues_per_port;
+			int qos;
+			cvm_oct_private_t *priv = (cvm_oct_private_t *)netdev_priv(cvm_oct_device[port]);
+			if (priv->poll)
+				priv->poll(cvm_oct_device[port]);
+
+			queues_per_port = cvmx_pko_get_num_queues(port);
+			/* Drain any pending packets in the free list */
+			for (qos = 0; qos < queues_per_port; qos++) {
+				if (skb_queue_len(&priv->tx_free_list[qos])) {
+					spin_lock(&priv->tx_free_list[qos].lock);
+					while (skb_queue_len(&priv->tx_free_list[qos]) > cvmx_fau_fetch_and_add32(priv->fau+qos*4, 0))
+						dev_kfree_skb(__skb_dequeue(&priv->tx_free_list[qos]));
+					spin_unlock(&priv->tx_free_list[qos].lock);
+				}
+			}
+			cvm_oct_device[port]->get_stats(cvm_oct_device[port]);
+		}
+		port++;
+		/* Poll the next port in a 50th of a second.
+		   This spreads the polling of ports out a little bit */
+		mod_timer(&cvm_oct_poll_timer, jiffies + HZ/50);
+	} else {
+		port = 0;
+		/* All ports have been polled. Start the next iteration through
+		   the ports in one second */
+		mod_timer(&cvm_oct_poll_timer, jiffies + HZ);
+	}
 }
 
 
@@ -160,43 +154,41 @@ static void cvm_do_timer(unsigned long arg)
  */
 static __init void cvm_oct_configure_common_hw(void)
 {
-    int r;
-    /* Setup the FPA */
-    cvmx_fpa_enable();
-    cvm_oct_mem_fill_fpa(CVMX_FPA_PACKET_POOL, CVMX_FPA_PACKET_POOL_SIZE, NUM_PACKET_BUFFERS);
-    cvm_oct_mem_fill_fpa(CVMX_FPA_WQE_POOL, CVMX_FPA_WQE_POOL_SIZE, NUM_PACKET_BUFFERS);
-    if (CVMX_FPA_OUTPUT_BUFFER_POOL != CVMX_FPA_PACKET_POOL)
-        cvm_oct_mem_fill_fpa(CVMX_FPA_OUTPUT_BUFFER_POOL, CVMX_FPA_OUTPUT_BUFFER_POOL_SIZE, 128);
+	int r;
+	/* Setup the FPA */
+	cvmx_fpa_enable();
+	cvm_oct_mem_fill_fpa(CVMX_FPA_PACKET_POOL, CVMX_FPA_PACKET_POOL_SIZE, NUM_PACKET_BUFFERS);
+	cvm_oct_mem_fill_fpa(CVMX_FPA_WQE_POOL, CVMX_FPA_WQE_POOL_SIZE, NUM_PACKET_BUFFERS);
+	if (CVMX_FPA_OUTPUT_BUFFER_POOL != CVMX_FPA_PACKET_POOL)
+		cvm_oct_mem_fill_fpa(CVMX_FPA_OUTPUT_BUFFER_POOL, CVMX_FPA_OUTPUT_BUFFER_POOL_SIZE, 128);
 
-    if (USE_RED)
-        cvmx_helper_setup_red(NUM_PACKET_BUFFERS/4, NUM_PACKET_BUFFERS/8);
+	if (USE_RED)
+		cvmx_helper_setup_red(NUM_PACKET_BUFFERS/4, NUM_PACKET_BUFFERS/8);
 
-    /* Enable the MII interface */
-    if (!octeon_is_simulation())
-        cvmx_write_csr(CVMX_SMI_EN, 1);
+	/* Enable the MII interface */
+	if (!octeon_is_simulation())
+		cvmx_write_csr(CVMX_SMI_EN, 1);
 
-    /* Register an IRQ hander for to receive POW interrupts */
-    r = request_irq(8 + pow_receive_group, cvm_oct_do_interrupt, IRQF_SHARED, "Ethernet", cvm_oct_device);
+	/* Register an IRQ hander for to receive POW interrupts */
+	r = request_irq(8 + pow_receive_group, cvm_oct_do_interrupt, IRQF_SHARED, "Ethernet", cvm_oct_device);
 
 #ifdef CONFIG_SMP
-    if (USE_MULTICORE_RECEIVE)
-    {
-        preempt_disable();
-        {
-            int cpu;
-            for (cpu=0; cpu<NR_CPUS; cpu++)
-            {
-                if (cpu_online(cpu) && (cpu != smp_processor_id()))
-                {
-                    cvmx_ciu_intx0_t en;
-                    en.u64 = cvmx_read_csr(CVMX_CIU_INTX_EN0(cpu_logical_map(cpu)*2));
-                    en.s.workq |= (1<<pow_receive_group);
-                    cvmx_write_csr(CVMX_CIU_INTX_EN0(cpu_logical_map(cpu)*2), en.u64);
-                }
-            }
-        }
-        preempt_enable();
-    }
+	if (USE_MULTICORE_RECEIVE) {
+		preempt_disable();
+		{
+			int cpu;
+			for (cpu = 0; cpu < NR_CPUS; cpu++) {
+				if (cpu_online(cpu) &&
+				   (cpu != smp_processor_id())) {
+					cvmx_ciu_intx0_t en;
+					en.u64 = cvmx_read_csr(CVMX_CIU_INTX_EN0(cpu_logical_map(cpu)*2));
+					en.s.workq |= (1<<pow_receive_group);
+					cvmx_write_csr(CVMX_CIU_INTX_EN0(cpu_logical_map(cpu)*2), en.u64);
+				}
+			}
+		}
+		preempt_enable();
+	}
 #endif
 }
 
@@ -217,20 +209,19 @@ static __init void cvm_oct_configure_common_hw(void)
  */
 struct net_device *cvm_oct_register_callback(const char *device_name, cvm_oct_callback_t callback)
 {
-    int port;
-
-    for (port=0; port<TOTAL_NUMBER_OF_PORTS; port++)
-    {
-        if (cvm_oct_device[port] && (strcmp(device_name, cvm_oct_device[port]->name) == 0))
-        {
-            cvm_oct_private_t* priv = (cvm_oct_private_t*)netdev_priv(cvm_oct_device[port]);
-            priv->intercept_cb = callback;
-            wmb();
-            return cvm_oct_device[port];
-        }
-    }
-
-    return NULL;
+	int port;
+
+	for (port = 0; port < TOTAL_NUMBER_OF_PORTS; port++) {
+		if (cvm_oct_device[port] &&
+		    (strcmp(device_name, cvm_oct_device[port]->name) == 0)) {
+			cvm_oct_private_t *priv = (cvm_oct_private_t *)netdev_priv(cvm_oct_device[port]);
+			priv->intercept_cb = callback;
+			wmb();
+			return cvm_oct_device[port];
+		}
+	}
+
+	return NULL;
 }
 EXPORT_SYMBOL(cvm_oct_register_callback);
 
@@ -244,21 +235,20 @@ EXPORT_SYMBOL(cvm_oct_register_callback);
  */
 int cvm_oct_free_work(void *work_queue_entry)
 {
-    cvmx_wqe_t *work = work_queue_entry;
+	cvmx_wqe_t *work = work_queue_entry;
 
-    int segments = work->word2.s.bufs;
-    cvmx_buf_ptr_t segment_ptr = work->packet_ptr;
+	int segments = work->word2.s.bufs;
+	cvmx_buf_ptr_t segment_ptr = work->packet_ptr;
 
-    while (segments--)
-    {
-        cvmx_buf_ptr_t next_ptr = *(cvmx_buf_ptr_t*)cvmx_phys_to_ptr(segment_ptr.s.addr-8);
-        if (unlikely(!segment_ptr.s.i))
-            cvmx_fpa_free(cvm_oct_get_buffer_ptr(segment_ptr), segment_ptr.s.pool, DONT_WRITEBACK(CVMX_FPA_PACKET_POOL_SIZE/128));
-        segment_ptr = next_ptr;
-    }
-    cvmx_fpa_free(work, CVMX_FPA_WQE_POOL, DONT_WRITEBACK(1));
+	while (segments--) {
+		cvmx_buf_ptr_t next_ptr = *(cvmx_buf_ptr_t *)cvmx_phys_to_ptr(segment_ptr.s.addr-8);
+		if (unlikely(!segment_ptr.s.i))
+			cvmx_fpa_free(cvm_oct_get_buffer_ptr(segment_ptr), segment_ptr.s.pool, DONT_WRITEBACK(CVMX_FPA_PACKET_POOL_SIZE/128));
+		segment_ptr = next_ptr;
+	}
+	cvmx_fpa_free(work, CVMX_FPA_WQE_POOL, DONT_WRITEBACK(1));
 
-    return 0;
+	return 0;
 }
 EXPORT_SYMBOL(cvm_oct_free_work);
 
@@ -271,194 +261,180 @@ EXPORT_SYMBOL(cvm_oct_free_work);
  */
 static int __init cvm_oct_init_module(void)
 {
-    extern int cvm_oct_rgmii_init(struct net_device *dev);
-    extern void cvm_oct_rgmii_uninit(struct net_device *dev);
-    extern int cvm_oct_sgmii_init(struct net_device *dev);
-    extern void cvm_oct_sgmii_uninit(struct net_device *dev);
-    extern int cvm_oct_spi_init(struct net_device *dev);
-    extern void cvm_oct_spi_uninit(struct net_device *dev);
-    extern int cvm_oct_xaui_init(struct net_device *dev);
-    extern void cvm_oct_xaui_uninit(struct net_device *dev);
-
-    int num_interfaces;
-    int interface;
-    int fau = FAU_NUM_PACKET_BUFFERS_TO_FREE;
-    int qos;
-
-    printk("cavium-ethernet: %s\n", OCTEON_SDK_VERSION_STRING);
-
-    cvm_oct_proc_initialize();
-    cvm_oct_rx_initialize();
-    cvm_oct_configure_common_hw();
-
-    cvmx_helper_initialize_packet_io_global();
-
-    /* Change the input group for all ports before input is enabled */
-    num_interfaces = cvmx_helper_get_number_of_interfaces();
-    for (interface=0; interface<num_interfaces; interface++)
-    {
-        int num_ports = cvmx_helper_ports_on_interface(interface);
-        int port;
-
-        for (port=cvmx_helper_get_ipd_port(interface, 0); port<cvmx_helper_get_ipd_port(interface, num_ports); port++)
-        {
-            cvmx_pip_prt_tagx_t pip_prt_tagx;
-            pip_prt_tagx.u64 = cvmx_read_csr(CVMX_PIP_PRT_TAGX(port));
-            pip_prt_tagx.s.grp = pow_receive_group;
-            cvmx_write_csr(CVMX_PIP_PRT_TAGX(port), pip_prt_tagx.u64);
-        }
-    }
-
-    cvmx_helper_ipd_and_packet_input_enable();
-
-    memset(cvm_oct_device, 0, sizeof(cvm_oct_device));
-
-    /* Initialize the FAU used for counting packet buffers that need to be freed */
-    cvmx_fau_atomic_write32(FAU_NUM_PACKET_BUFFERS_TO_FREE, 0);
-
-    if ((pow_send_group != -1))
-    {
-        struct net_device *dev;
-        printk("\tConfiguring device for POW only access\n");
-        dev = alloc_etherdev(sizeof(cvm_oct_private_t));
-        if (dev)
-        {
-            /* Initialize the device private structure. */
-            cvm_oct_private_t* priv = (cvm_oct_private_t*)netdev_priv(dev);
-            memset(priv, 0, sizeof(cvm_oct_private_t));
-
-            dev->init = cvm_oct_common_init;
-            priv->imode = CVMX_HELPER_INTERFACE_MODE_DISABLED;
-            priv->port = CVMX_PIP_NUM_INPUT_PORTS;
-            priv->queue = -1;
-            strcpy(dev->name, "pow%d");
-            for (qos=0; qos<16; qos++)
-                skb_queue_head_init(&priv->tx_free_list[qos]);
-
-            if (register_netdev(dev)<0)
-            {
-                printk("\t\tFailed to register ethernet device for POW\n");
-                kfree(dev);
-            }
-            else
-            {
-                cvm_oct_device[CVMX_PIP_NUM_INPUT_PORTS] = dev;
-                printk("\t\t%s: POW send group %d, receive group %d\n",
-                       dev->name, pow_send_group, pow_receive_group);
-            }
-        }
-        else
-        {
-            printk("\t\tFailed to allocate ethernet device for POW\n");
-        }
-    }
-
-    num_interfaces = cvmx_helper_get_number_of_interfaces();
-    for (interface=0; interface<num_interfaces; interface++)
-    {
-        cvmx_helper_interface_mode_t imode = cvmx_helper_interface_get_mode(interface);
-        int num_ports = cvmx_helper_ports_on_interface(interface);
-        int port;
-
-        for (port=cvmx_helper_get_ipd_port(interface, 0); port<cvmx_helper_get_ipd_port(interface, num_ports); port++)
-        {
-            cvm_oct_private_t* priv;
-            struct net_device *dev = alloc_etherdev(sizeof(cvm_oct_private_t));
-            if (!dev)
-            {
-                printk("\t\tFailed to allocate ethernet device for port %d\n", port);
-                continue;
-            }
-
-            /* Initialize the device private structure. */
-            priv = (cvm_oct_private_t*)netdev_priv(dev);
-            memset(priv, 0, sizeof(cvm_oct_private_t));
-
-            priv->imode = imode;
-            priv->port = port;
-            priv->queue = cvmx_pko_get_base_queue(priv->port);
-            priv->intercept_cb = NULL;
-            priv->fau = fau - cvmx_pko_get_num_queues(port) * 4;
-            for (qos=0; qos<16; qos++)
-                skb_queue_head_init(&priv->tx_free_list[qos]);
-            for (qos=0; qos<cvmx_pko_get_num_queues(port); qos++)
-                cvmx_fau_atomic_write32(priv->fau+qos*4, 0);
-
-            switch (priv->imode)
-            {
-                /* These types don't support ports to IPD/PKO */
-                case CVMX_HELPER_INTERFACE_MODE_DISABLED:
-                case CVMX_HELPER_INTERFACE_MODE_PCIE:
-                case CVMX_HELPER_INTERFACE_MODE_PICMG:
-                case CVMX_HELPER_INTERFACE_MODE_NPI:
-                    break;
-                case CVMX_HELPER_INTERFACE_MODE_XAUI:
-                    dev->init = cvm_oct_xaui_init;
-                    dev->uninit = cvm_oct_xaui_uninit;
-                    strcpy(dev->name, "xaui%d");
-                    break;
-                case CVMX_HELPER_INTERFACE_MODE_LOOP:
-                    dev->init = cvm_oct_common_init;
-                    dev->uninit = cvm_oct_common_uninit;
-                    strcpy(dev->name, "loop%d");
-                    break;
-                case CVMX_HELPER_INTERFACE_MODE_SGMII:
-                    dev->init = cvm_oct_sgmii_init;
-                    dev->uninit = cvm_oct_sgmii_uninit;
-                    strcpy(dev->name, "eth%d");
-                    break;
-                case CVMX_HELPER_INTERFACE_MODE_SPI:
-                    dev->init = cvm_oct_spi_init;
-                    dev->uninit = cvm_oct_spi_uninit;
-                    strcpy(dev->name, "spi%d");
-                    break;
-                case CVMX_HELPER_INTERFACE_MODE_RGMII:
-                case CVMX_HELPER_INTERFACE_MODE_GMII:
-                    dev->init = cvm_oct_rgmii_init;
-                    dev->uninit = cvm_oct_rgmii_uninit;
-                    strcpy(dev->name, "eth%d");
-                    break;
-            }
-
-            if (!dev->init)
-            {
-                kfree(dev);
-            }
-            else if (register_netdev(dev)<0)
-            {
-                printk("\t\tFailed to register ethernet device for interface %d, port %d\n",
-                       interface, priv->port);
-                kfree(dev);
-            }
-            else
-            {
-                cvm_oct_device[priv->port] = dev;
-                fau -= cvmx_pko_get_num_queues(priv->port) * sizeof(uint32_t);
-            }
-        }
-    }
-
-    if (INTERRUPT_LIMIT)
-    {
-        /* Set the POW timer rate to give an interrupt at most INTERRUPT_LIMIT times per second */
-        cvmx_write_csr(CVMX_POW_WQ_INT_PC, octeon_bootinfo->eclock_hz/(INTERRUPT_LIMIT*16*256)<<8);
-
-        /* Enable POW timer interrupt. It will count when there are packets available */
-        cvmx_write_csr(CVMX_POW_WQ_INT_THRX(pow_receive_group), 0x1ful<<24);
-    }
-    else
-    {
-        /* Enable POW interrupt when our port has at least one packet */
-        cvmx_write_csr(CVMX_POW_WQ_INT_THRX(pow_receive_group), 0x1001);
-    }
-
-    /* Enable the poll timer for checking RGMII status */
-    init_timer(&cvm_oct_poll_timer);
-    cvm_oct_poll_timer.data = 0;
-    cvm_oct_poll_timer.function = cvm_do_timer;
-    mod_timer(&cvm_oct_poll_timer, jiffies + HZ);
-
-    return 0;
+	extern int cvm_oct_rgmii_init(struct net_device *dev);
+	extern void cvm_oct_rgmii_uninit(struct net_device *dev);
+	extern int cvm_oct_sgmii_init(struct net_device *dev);
+	extern void cvm_oct_sgmii_uninit(struct net_device *dev);
+	extern int cvm_oct_spi_init(struct net_device *dev);
+	extern void cvm_oct_spi_uninit(struct net_device *dev);
+	extern int cvm_oct_xaui_init(struct net_device *dev);
+	extern void cvm_oct_xaui_uninit(struct net_device *dev);
+
+	int num_interfaces;
+	int interface;
+	int fau = FAU_NUM_PACKET_BUFFERS_TO_FREE;
+	int qos;
+
+	printk("cavium-ethernet: %s\n", OCTEON_SDK_VERSION_STRING);
+
+	cvm_oct_proc_initialize();
+	cvm_oct_rx_initialize();
+	cvm_oct_configure_common_hw();
+
+	cvmx_helper_initialize_packet_io_global();
+
+	/* Change the input group for all ports before input is enabled */
+	num_interfaces = cvmx_helper_get_number_of_interfaces();
+	for (interface = 0; interface < num_interfaces; interface++) {
+		int num_ports = cvmx_helper_ports_on_interface(interface);
+		int port;
+
+		for (port = cvmx_helper_get_ipd_port(interface, 0); port < cvmx_helper_get_ipd_port(interface, num_ports); port++) {
+			cvmx_pip_prt_tagx_t pip_prt_tagx;
+			pip_prt_tagx.u64 = cvmx_read_csr(CVMX_PIP_PRT_TAGX(port));
+			pip_prt_tagx.s.grp = pow_receive_group;
+			cvmx_write_csr(CVMX_PIP_PRT_TAGX(port), pip_prt_tagx.u64);
+		}
+	}
+
+	cvmx_helper_ipd_and_packet_input_enable();
+
+	memset(cvm_oct_device, 0, sizeof(cvm_oct_device));
+
+	/* Initialize the FAU used for counting packet buffers that need to be freed */
+	cvmx_fau_atomic_write32(FAU_NUM_PACKET_BUFFERS_TO_FREE, 0);
+
+	if ((pow_send_group != -1)) {
+		struct net_device *dev;
+		printk("\tConfiguring device for POW only access\n");
+		dev = alloc_etherdev(sizeof(cvm_oct_private_t));
+		if (dev) {
+			/* Initialize the device private structure. */
+			cvm_oct_private_t *priv = (cvm_oct_private_t *)netdev_priv(dev);
+			memset(priv, 0, sizeof(cvm_oct_private_t));
+
+			dev->init = cvm_oct_common_init;
+			priv->imode = CVMX_HELPER_INTERFACE_MODE_DISABLED;
+			priv->port = CVMX_PIP_NUM_INPUT_PORTS;
+			priv->queue = -1;
+			strcpy(dev->name, "pow%d");
+			for (qos = 0; qos < 16; qos++)
+				skb_queue_head_init(&priv->tx_free_list[qos]);
+
+			if (register_netdev(dev) < 0) {
+				printk("\t\tFailed to register ethernet device for POW\n");
+				kfree(dev);
+			} else {
+				cvm_oct_device[CVMX_PIP_NUM_INPUT_PORTS] = dev;
+				printk("\t\t%s: POW send group %d, receive group %d\n",
+				dev->name, pow_send_group, pow_receive_group);
+			}
+		} else {
+			printk("\t\tFailed to allocate ethernet device for POW\n");
+		}
+	}
+
+	num_interfaces = cvmx_helper_get_number_of_interfaces();
+	for (interface = 0; interface < num_interfaces; interface++) {
+		cvmx_helper_interface_mode_t imode = cvmx_helper_interface_get_mode(interface);
+		int num_ports = cvmx_helper_ports_on_interface(interface);
+		int port;
+
+		for (port = cvmx_helper_get_ipd_port(interface, 0); port < cvmx_helper_get_ipd_port(interface, num_ports); port++) {
+			cvm_oct_private_t *priv;
+			struct net_device *dev = alloc_etherdev(sizeof(cvm_oct_private_t));
+			if (!dev) {
+				printk("\t\tFailed to allocate ethernet device for port %d\n", port);
+				continue;
+			}
+
+			/* Initialize the device private structure. */
+			priv = (cvm_oct_private_t *)netdev_priv(dev);
+			memset(priv, 0, sizeof(cvm_oct_private_t));
+
+			priv->imode = imode;
+			priv->port = port;
+			priv->queue = cvmx_pko_get_base_queue(priv->port);
+			priv->intercept_cb = NULL;
+			priv->fau = fau - cvmx_pko_get_num_queues(port) * 4;
+			for (qos = 0; qos < 16; qos++)
+				skb_queue_head_init(&priv->tx_free_list[qos]);
+			for (qos = 0; qos < cvmx_pko_get_num_queues(port); qos++)
+				cvmx_fau_atomic_write32(priv->fau+qos*4, 0);
+
+			switch (priv->imode) {
+
+			/* These types don't support ports to IPD/PKO */
+			case CVMX_HELPER_INTERFACE_MODE_DISABLED:
+			case CVMX_HELPER_INTERFACE_MODE_PCIE:
+			case CVMX_HELPER_INTERFACE_MODE_PICMG:
+			case CVMX_HELPER_INTERFACE_MODE_NPI:
+				break;
+
+			case CVMX_HELPER_INTERFACE_MODE_XAUI:
+				dev->init = cvm_oct_xaui_init;
+				dev->uninit = cvm_oct_xaui_uninit;
+				strcpy(dev->name, "xaui%d");
+				break;
+
+			case CVMX_HELPER_INTERFACE_MODE_LOOP:
+				dev->init = cvm_oct_common_init;
+				dev->uninit = cvm_oct_common_uninit;
+				strcpy(dev->name, "loop%d");
+				break;
+
+			case CVMX_HELPER_INTERFACE_MODE_SGMII:
+				dev->init = cvm_oct_sgmii_init;
+				dev->uninit = cvm_oct_sgmii_uninit;
+				strcpy(dev->name, "eth%d");
+				break;
+
+			case CVMX_HELPER_INTERFACE_MODE_SPI:
+				dev->init = cvm_oct_spi_init;
+				dev->uninit = cvm_oct_spi_uninit;
+				strcpy(dev->name, "spi%d");
+				break;
+
+			case CVMX_HELPER_INTERFACE_MODE_RGMII:
+			case CVMX_HELPER_INTERFACE_MODE_GMII:
+				dev->init = cvm_oct_rgmii_init;
+				dev->uninit = cvm_oct_rgmii_uninit;
+				strcpy(dev->name, "eth%d");
+				break;
+			}
+
+			if (!dev->init) {
+				kfree(dev);
+			} else
+			if (register_netdev(dev) < 0) {
+				printk("\t\tFailed to register ethernet device for interface %d, port %d\n",
+				interface, priv->port);
+				kfree(dev);
+			} else {
+				cvm_oct_device[priv->port] = dev;
+				fau -= cvmx_pko_get_num_queues(priv->port) * sizeof(uint32_t);
+			}
+		}
+	}
+
+	if (INTERRUPT_LIMIT) {
+		/* Set the POW timer rate to give an interrupt at most INTERRUPT_LIMIT times per second */
+		cvmx_write_csr(CVMX_POW_WQ_INT_PC, octeon_bootinfo->eclock_hz/(INTERRUPT_LIMIT*16*256)<<8);
+
+		/* Enable POW timer interrupt. It will count when there are packets available */
+		cvmx_write_csr(CVMX_POW_WQ_INT_THRX(pow_receive_group), 0x1ful<<24);
+	} else {
+		/* Enable POW interrupt when our port has at least one packet */
+		cvmx_write_csr(CVMX_POW_WQ_INT_THRX(pow_receive_group), 0x1001);
+	}
+
+	/* Enable the poll timer for checking RGMII status */
+	init_timer(&cvm_oct_poll_timer);
+	cvm_oct_poll_timer.data = 0;
+	cvm_oct_poll_timer.function = cvm_do_timer;
+	mod_timer(&cvm_oct_poll_timer, jiffies + HZ);
+
+	return 0;
 }
 
 
@@ -469,151 +445,139 @@ static int __init cvm_oct_init_module(void)
  */
 static void __exit cvm_oct_cleanup_module(void)
 {
-    int port;
-
-    /* Disable POW interrupt */
-    cvmx_write_csr(CVMX_POW_WQ_INT_THRX(pow_receive_group), 0);
-
-    cvmx_ipd_disable();
-
-    /* Free the interrupt handler */
-    free_irq(8 + pow_receive_group, cvm_oct_device);
-
-    del_timer(&cvm_oct_poll_timer);
-    cvm_oct_rx_shutdown();
-    cvmx_pko_disable();
-
-    /* Free the ethernet devices */
-    for (port=0; port<TOTAL_NUMBER_OF_PORTS; port++)
-    {
-        if (cvm_oct_device[port])
-        {
-            cvm_oct_tx_shutdown(cvm_oct_device[port]);
-            unregister_netdev(cvm_oct_device[port]);
-            kfree(cvm_oct_device[port]);
-            cvm_oct_device[port] = NULL;
-        }
-    }
-
-    cvmx_pko_shutdown();
-    cvm_oct_proc_shutdown();
-
-    /* Only CN38XXp{1,2} cannot read pointer out of the IPD */
-    if (!OCTEON_IS_MODEL(OCTEON_CN38XX_PASS1) && !OCTEON_IS_MODEL(OCTEON_CN38XX_PASS2))
-    {
-        cvmx_ipd_ptr_count_t ipd_ptr_count;
-        ipd_ptr_count.u64 = cvmx_read_csr(CVMX_IPD_PTR_COUNT);
-
-        /* Free the prefetched WQE */
-        if (ipd_ptr_count.s.wqev_cnt)
-        {
-            cvmx_ipd_wqe_ptr_valid_t ipd_wqe_ptr_valid;
-            ipd_wqe_ptr_valid.u64 = cvmx_read_csr(CVMX_IPD_WQE_PTR_VALID);
-            cvmx_fpa_free(cvmx_phys_to_ptr((uint64_t)ipd_wqe_ptr_valid.s.ptr<<7), CVMX_FPA_WQE_POOL, 0);
-        }
-
-        /* Free all WQE in the fifo */
-        if (ipd_ptr_count.s.wqe_pcnt)
-        {
-            int i;
-            cvmx_ipd_pwp_ptr_fifo_ctl_t ipd_pwp_ptr_fifo_ctl;
-            ipd_pwp_ptr_fifo_ctl.u64 = cvmx_read_csr(CVMX_IPD_PWP_PTR_FIFO_CTL);
-            for (i=0; i<ipd_ptr_count.s.wqe_pcnt; i++)
-            {
-                ipd_pwp_ptr_fifo_ctl.s.cena = 0;
-                ipd_pwp_ptr_fifo_ctl.s.raddr = ipd_pwp_ptr_fifo_ctl.s.max_cnts + (ipd_pwp_ptr_fifo_ctl.s.wraddr+i) % ipd_pwp_ptr_fifo_ctl.s.max_cnts;
-                cvmx_write_csr(CVMX_IPD_PWP_PTR_FIFO_CTL, ipd_pwp_ptr_fifo_ctl.u64);
-                ipd_pwp_ptr_fifo_ctl.u64 = cvmx_read_csr(CVMX_IPD_PWP_PTR_FIFO_CTL);
-                cvmx_fpa_free(cvmx_phys_to_ptr((uint64_t)ipd_pwp_ptr_fifo_ctl.s.ptr<<7), CVMX_FPA_WQE_POOL, 0);
-            }
-            ipd_pwp_ptr_fifo_ctl.s.cena = 1;
-            cvmx_write_csr(CVMX_IPD_PWP_PTR_FIFO_CTL, ipd_pwp_ptr_fifo_ctl.u64);
-        }
-
-        /* Free the prefetched packet */
-        if (ipd_ptr_count.s.pktv_cnt)
-        {
-            cvmx_ipd_pkt_ptr_valid_t ipd_pkt_ptr_valid;
-            ipd_pkt_ptr_valid.u64 = cvmx_read_csr(CVMX_IPD_PKT_PTR_VALID);
-            cvmx_fpa_free(cvmx_phys_to_ptr(ipd_pkt_ptr_valid.s.ptr<<7), CVMX_FPA_PACKET_POOL, 0);
-        }
-
-        /* Free the per port prefetched packets */
-        if (1)
-        {
-            int i;
-            cvmx_ipd_prc_port_ptr_fifo_ctl_t ipd_prc_port_ptr_fifo_ctl;
-            ipd_prc_port_ptr_fifo_ctl.u64 = cvmx_read_csr(CVMX_IPD_PRC_PORT_PTR_FIFO_CTL);
-            for (i=0; i<ipd_prc_port_ptr_fifo_ctl.s.max_pkt; i++)
-            {
-                ipd_prc_port_ptr_fifo_ctl.s.cena = 0;
-                ipd_prc_port_ptr_fifo_ctl.s.raddr = i % ipd_prc_port_ptr_fifo_ctl.s.max_pkt;
-                cvmx_write_csr(CVMX_IPD_PRC_PORT_PTR_FIFO_CTL, ipd_prc_port_ptr_fifo_ctl.u64);
-                ipd_prc_port_ptr_fifo_ctl.u64 = cvmx_read_csr(CVMX_IPD_PRC_PORT_PTR_FIFO_CTL);
-                cvmx_fpa_free(cvmx_phys_to_ptr((uint64_t)ipd_prc_port_ptr_fifo_ctl.s.ptr<<7), CVMX_FPA_PACKET_POOL, 0);
-            }
-            ipd_prc_port_ptr_fifo_ctl.s.cena = 1;
-            cvmx_write_csr(CVMX_IPD_PRC_PORT_PTR_FIFO_CTL, ipd_prc_port_ptr_fifo_ctl.u64);
-        }
-
-        /* Free all packets in the holding fifo */
-        if (ipd_ptr_count.s.pfif_cnt)
-        {
-            int i;
-            cvmx_ipd_prc_hold_ptr_fifo_ctl_t ipd_prc_hold_ptr_fifo_ctl;
-            ipd_prc_hold_ptr_fifo_ctl.u64 = cvmx_read_csr(CVMX_IPD_PRC_HOLD_PTR_FIFO_CTL);
-            for (i=0; i<ipd_ptr_count.s.pfif_cnt; i++)
-            {
-                ipd_prc_hold_ptr_fifo_ctl.s.cena = 0;
-                ipd_prc_hold_ptr_fifo_ctl.s.raddr = (ipd_prc_hold_ptr_fifo_ctl.s.praddr + i) % ipd_prc_hold_ptr_fifo_ctl.s.max_pkt;
-                cvmx_write_csr(CVMX_IPD_PRC_HOLD_PTR_FIFO_CTL, ipd_prc_hold_ptr_fifo_ctl.u64);
-                ipd_prc_hold_ptr_fifo_ctl.u64 = cvmx_read_csr(CVMX_IPD_PRC_HOLD_PTR_FIFO_CTL);
-                cvmx_fpa_free(cvmx_phys_to_ptr((uint64_t)ipd_prc_hold_ptr_fifo_ctl.s.ptr<<7), CVMX_FPA_PACKET_POOL, 0);
-            }
-            ipd_prc_hold_ptr_fifo_ctl.s.cena = 1;
-            cvmx_write_csr(CVMX_IPD_PRC_HOLD_PTR_FIFO_CTL, ipd_prc_hold_ptr_fifo_ctl.u64);
-        }
-
-        /* Free all packets in the fifo */
-        if (ipd_ptr_count.s.pkt_pcnt)
-        {
-            int i;
-            cvmx_ipd_pwp_ptr_fifo_ctl_t ipd_pwp_ptr_fifo_ctl;
-            ipd_pwp_ptr_fifo_ctl.u64 = cvmx_read_csr(CVMX_IPD_PWP_PTR_FIFO_CTL);
-            for (i=0; i<ipd_ptr_count.s.pkt_pcnt; i++)
-            {
-                ipd_pwp_ptr_fifo_ctl.s.cena = 0;
-                ipd_pwp_ptr_fifo_ctl.s.raddr = (ipd_pwp_ptr_fifo_ctl.s.praddr+i) % ipd_pwp_ptr_fifo_ctl.s.max_cnts;
-                cvmx_write_csr(CVMX_IPD_PWP_PTR_FIFO_CTL, ipd_pwp_ptr_fifo_ctl.u64);
-                ipd_pwp_ptr_fifo_ctl.u64 = cvmx_read_csr(CVMX_IPD_PWP_PTR_FIFO_CTL);
-                cvmx_fpa_free(cvmx_phys_to_ptr((uint64_t)ipd_pwp_ptr_fifo_ctl.s.ptr<<7), CVMX_FPA_PACKET_POOL, 0);
-            }
-            ipd_pwp_ptr_fifo_ctl.s.cena = 1;
-            cvmx_write_csr(CVMX_IPD_PWP_PTR_FIFO_CTL, ipd_pwp_ptr_fifo_ctl.u64);
-        }
-
-        /* Reset the IPD to get all buffers out of it */
-        {
-            cvmx_ipd_ctl_status_t ipd_ctl_status;
-            ipd_ctl_status.u64 = cvmx_read_csr(CVMX_IPD_CTL_STATUS);
-            ipd_ctl_status.s.reset = 1;
-            cvmx_write_csr(CVMX_IPD_CTL_STATUS, ipd_ctl_status.u64);
-        }
-
-        /* Reset the PIP */
-        {
-            cvmx_pip_sft_rst_t pip_sft_rst;
-            pip_sft_rst.u64 = cvmx_read_csr(CVMX_PIP_SFT_RST);
-            pip_sft_rst.s.rst = 1;
-            cvmx_write_csr(CVMX_PIP_SFT_RST, pip_sft_rst.u64);
-        }
-    }
-
-    /* Free the HW pools */
-    cvm_oct_mem_empty_fpa(CVMX_FPA_PACKET_POOL, CVMX_FPA_PACKET_POOL_SIZE, NUM_PACKET_BUFFERS);
-    cvm_oct_mem_empty_fpa(CVMX_FPA_WQE_POOL, CVMX_FPA_WQE_POOL_SIZE, NUM_PACKET_BUFFERS);
-    if (CVMX_FPA_OUTPUT_BUFFER_POOL != CVMX_FPA_PACKET_POOL)
-        cvm_oct_mem_empty_fpa(CVMX_FPA_OUTPUT_BUFFER_POOL, CVMX_FPA_OUTPUT_BUFFER_POOL_SIZE, 128);
+	int port;
+
+	/* Disable POW interrupt */
+	cvmx_write_csr(CVMX_POW_WQ_INT_THRX(pow_receive_group), 0);
+
+	cvmx_ipd_disable();
+
+	/* Free the interrupt handler */
+	free_irq(8 + pow_receive_group, cvm_oct_device);
+
+	del_timer(&cvm_oct_poll_timer);
+	cvm_oct_rx_shutdown();
+	cvmx_pko_disable();
+
+	/* Free the ethernet devices */
+	for (port = 0; port < TOTAL_NUMBER_OF_PORTS; port++) {
+		if (cvm_oct_device[port]) {
+			cvm_oct_tx_shutdown(cvm_oct_device[port]);
+			unregister_netdev(cvm_oct_device[port]);
+			kfree(cvm_oct_device[port]);
+			cvm_oct_device[port] = NULL;
+		}
+	}
+
+	cvmx_pko_shutdown();
+	cvm_oct_proc_shutdown();
+
+	/* Only CN38XXp{1,2} cannot read pointer out of the IPD */
+	if (!OCTEON_IS_MODEL(OCTEON_CN38XX_PASS1) && !OCTEON_IS_MODEL(OCTEON_CN38XX_PASS2)) {
+		cvmx_ipd_ptr_count_t ipd_ptr_count;
+		ipd_ptr_count.u64 = cvmx_read_csr(CVMX_IPD_PTR_COUNT);
+
+		/* Free the prefetched WQE */
+		if (ipd_ptr_count.s.wqev_cnt) {
+			cvmx_ipd_wqe_ptr_valid_t ipd_wqe_ptr_valid;
+			ipd_wqe_ptr_valid.u64 = cvmx_read_csr(CVMX_IPD_WQE_PTR_VALID);
+			cvmx_fpa_free(cvmx_phys_to_ptr((uint64_t)ipd_wqe_ptr_valid.s.ptr<<7), CVMX_FPA_WQE_POOL, 0);
+		}
+
+		/* Free all WQE in the fifo */
+		if (ipd_ptr_count.s.wqe_pcnt) {
+			int i;
+			cvmx_ipd_pwp_ptr_fifo_ctl_t ipd_pwp_ptr_fifo_ctl;
+			ipd_pwp_ptr_fifo_ctl.u64 = cvmx_read_csr(CVMX_IPD_PWP_PTR_FIFO_CTL);
+			for (i = 0; i < ipd_ptr_count.s.wqe_pcnt; i++) {
+				ipd_pwp_ptr_fifo_ctl.s.cena = 0;
+				ipd_pwp_ptr_fifo_ctl.s.raddr = ipd_pwp_ptr_fifo_ctl.s.max_cnts + (ipd_pwp_ptr_fifo_ctl.s.wraddr+i) % ipd_pwp_ptr_fifo_ctl.s.max_cnts;
+				cvmx_write_csr(CVMX_IPD_PWP_PTR_FIFO_CTL, ipd_pwp_ptr_fifo_ctl.u64);
+				ipd_pwp_ptr_fifo_ctl.u64 = cvmx_read_csr(CVMX_IPD_PWP_PTR_FIFO_CTL);
+				cvmx_fpa_free(cvmx_phys_to_ptr((uint64_t)ipd_pwp_ptr_fifo_ctl.s.ptr<<7), CVMX_FPA_WQE_POOL, 0);
+			}
+			ipd_pwp_ptr_fifo_ctl.s.cena = 1;
+			cvmx_write_csr(CVMX_IPD_PWP_PTR_FIFO_CTL, ipd_pwp_ptr_fifo_ctl.u64);
+		}
+
+		/* Free the prefetched packet */
+		if (ipd_ptr_count.s.pktv_cnt) {
+			cvmx_ipd_pkt_ptr_valid_t ipd_pkt_ptr_valid;
+			ipd_pkt_ptr_valid.u64 = cvmx_read_csr(CVMX_IPD_PKT_PTR_VALID);
+			cvmx_fpa_free(cvmx_phys_to_ptr(ipd_pkt_ptr_valid.s.ptr<<7), CVMX_FPA_PACKET_POOL, 0);
+		}
+
+		/* Free the per port prefetched packets */
+		if (1) {
+			int i;
+			cvmx_ipd_prc_port_ptr_fifo_ctl_t ipd_prc_port_ptr_fifo_ctl;
+			ipd_prc_port_ptr_fifo_ctl.u64 = cvmx_read_csr(CVMX_IPD_PRC_PORT_PTR_FIFO_CTL);
+			for (i = 0; i < ipd_prc_port_ptr_fifo_ctl.s.max_pkt; i++) {
+				ipd_prc_port_ptr_fifo_ctl.s.cena = 0;
+				ipd_prc_port_ptr_fifo_ctl.s.raddr = i % ipd_prc_port_ptr_fifo_ctl.s.max_pkt;
+				cvmx_write_csr(CVMX_IPD_PRC_PORT_PTR_FIFO_CTL, ipd_prc_port_ptr_fifo_ctl.u64);
+				ipd_prc_port_ptr_fifo_ctl.u64 = cvmx_read_csr(CVMX_IPD_PRC_PORT_PTR_FIFO_CTL);
+				cvmx_fpa_free(cvmx_phys_to_ptr((uint64_t)ipd_prc_port_ptr_fifo_ctl.s.ptr<<7), CVMX_FPA_PACKET_POOL, 0);
+			}
+			ipd_prc_port_ptr_fifo_ctl.s.cena = 1;
+			cvmx_write_csr(CVMX_IPD_PRC_PORT_PTR_FIFO_CTL, ipd_prc_port_ptr_fifo_ctl.u64);
+		}
+
+		/* Free all packets in the holding fifo */
+		if (ipd_ptr_count.s.pfif_cnt) {
+			int i;
+			cvmx_ipd_prc_hold_ptr_fifo_ctl_t ipd_prc_hold_ptr_fifo_ctl;
+
+			ipd_prc_hold_ptr_fifo_ctl.u64 = cvmx_read_csr(CVMX_IPD_PRC_HOLD_PTR_FIFO_CTL);
+			for (i = 0; i < ipd_ptr_count.s.pfif_cnt; i++) {
+				ipd_prc_hold_ptr_fifo_ctl.s.cena = 0;
+				ipd_prc_hold_ptr_fifo_ctl.s.raddr = (ipd_prc_hold_ptr_fifo_ctl.s.praddr + i) % ipd_prc_hold_ptr_fifo_ctl.s.max_pkt;
+				cvmx_write_csr(CVMX_IPD_PRC_HOLD_PTR_FIFO_CTL, ipd_prc_hold_ptr_fifo_ctl.u64);
+				ipd_prc_hold_ptr_fifo_ctl.u64 = cvmx_read_csr(CVMX_IPD_PRC_HOLD_PTR_FIFO_CTL);
+				cvmx_fpa_free(cvmx_phys_to_ptr((uint64_t)ipd_prc_hold_ptr_fifo_ctl.s.ptr<<7), CVMX_FPA_PACKET_POOL, 0);
+			}
+			ipd_prc_hold_ptr_fifo_ctl.s.cena = 1;
+			cvmx_write_csr(CVMX_IPD_PRC_HOLD_PTR_FIFO_CTL, ipd_prc_hold_ptr_fifo_ctl.u64);
+		}
+
+		/* Free all packets in the fifo */
+		if (ipd_ptr_count.s.pkt_pcnt) {
+			int i;
+			cvmx_ipd_pwp_ptr_fifo_ctl_t ipd_pwp_ptr_fifo_ctl;
+			ipd_pwp_ptr_fifo_ctl.u64 = cvmx_read_csr(CVMX_IPD_PWP_PTR_FIFO_CTL);
+			for (i = 0; i < ipd_ptr_count.s.pkt_pcnt; i++) {
+				ipd_pwp_ptr_fifo_ctl.s.cena = 0;
+				ipd_pwp_ptr_fifo_ctl.s.raddr = (ipd_pwp_ptr_fifo_ctl.s.praddr+i) % ipd_pwp_ptr_fifo_ctl.s.max_cnts;
+				cvmx_write_csr(CVMX_IPD_PWP_PTR_FIFO_CTL, ipd_pwp_ptr_fifo_ctl.u64);
+				ipd_pwp_ptr_fifo_ctl.u64 = cvmx_read_csr(CVMX_IPD_PWP_PTR_FIFO_CTL);
+				cvmx_fpa_free(cvmx_phys_to_ptr((uint64_t)ipd_pwp_ptr_fifo_ctl.s.ptr<<7), CVMX_FPA_PACKET_POOL, 0);
+			}
+			ipd_pwp_ptr_fifo_ctl.s.cena = 1;
+			cvmx_write_csr(CVMX_IPD_PWP_PTR_FIFO_CTL, ipd_pwp_ptr_fifo_ctl.u64);
+		}
+
+		/* Reset the IPD to get all buffers out of it */
+		{
+		cvmx_ipd_ctl_status_t ipd_ctl_status;
+		ipd_ctl_status.u64 = cvmx_read_csr(CVMX_IPD_CTL_STATUS);
+		ipd_ctl_status.s.reset = 1;
+		cvmx_write_csr(CVMX_IPD_CTL_STATUS, ipd_ctl_status.u64);
+		}
+
+		/* Reset the PIP */
+		{
+		cvmx_pip_sft_rst_t pip_sft_rst;
+		pip_sft_rst.u64 = cvmx_read_csr(CVMX_PIP_SFT_RST);
+		pip_sft_rst.s.rst = 1;
+		cvmx_write_csr(CVMX_PIP_SFT_RST, pip_sft_rst.u64);
+		}
+	}
+
+	/* Free the HW pools */
+	cvm_oct_mem_empty_fpa(CVMX_FPA_PACKET_POOL, CVMX_FPA_PACKET_POOL_SIZE, NUM_PACKET_BUFFERS);
+	cvm_oct_mem_empty_fpa(CVMX_FPA_WQE_POOL, CVMX_FPA_WQE_POOL_SIZE, NUM_PACKET_BUFFERS);
+	if (CVMX_FPA_OUTPUT_BUFFER_POOL != CVMX_FPA_PACKET_POOL)
+		cvm_oct_mem_empty_fpa(CVMX_FPA_OUTPUT_BUFFER_POOL, CVMX_FPA_OUTPUT_BUFFER_POOL_SIZE, 128);
 }
 
 /* Note that this module is covered by the GPL even though the files are
-- 
1.5.5.1

