From b3589b2d23fb5673d106ebec2495eaab356acf3d Mon Sep 17 00:00:00 2001
From: auto commit <unknown@unknown>
Date: Fri, 24 Oct 2008 12:22:32 -0700
Subject: [PATCH] cavium: add Cavium platform specific files

Bring in the new platform specific files that are only used by
the Cavium boards.  Direct import of files as found in the drop
of OCTEON-LINUX-1.8.0-270.

Signed-off-by: Tomaso Paoletti <tpaoletti@caviumnetworks.com>
---
 arch/mips/cavium-octeon/Kconfig                    |  223 ++
 arch/mips/cavium-octeon/Makefile                   |   20 +
 arch/mips/cavium-octeon/cs89x0.c                   | 2264 ++++++++++++++++++++
 arch/mips/cavium-octeon/cs89x0.h                   |  486 +++++
 arch/mips/cavium-octeon/dma-octeon.c               |  494 +++++
 arch/mips/cavium-octeon/gpl-executive/Makefile     |   62 +
 .../gpl-executive/config/executive-config.h        |  115 +
 .../gpl-executive/cvmx-linux-kernel-exports.c      |  102 +
 arch/mips/cavium-octeon/hal.c                      |  545 +++++
 arch/mips/cavium-octeon/hal.h                      |  139 ++
 arch/mips/cavium-octeon/i8259.c                    |  178 ++
 arch/mips/cavium-octeon/irq.c                      |   59 +
 arch/mips/cavium-octeon/octeon-memcpy.S            |  521 +++++
 arch/mips/cavium-octeon/octeon_info.c              |  126 ++
 arch/mips/cavium-octeon/perf_counters.c            |  786 +++++++
 arch/mips/cavium-octeon/serial.c                   |  167 ++
 arch/mips/cavium-octeon/setup.c                    |  422 ++++
 arch/mips/cavium-octeon/smp.c                      |  208 ++
 arch/mips/cavium-octeon/userio.c                   |  162 ++
 arch/mips/kernel/irq-octeon.c                      |  472 ++++
 arch/mips/kernel/octeon_switch.S                   |  503 +++++
 arch/mips/mm/c-octeon.c                            |  293 +++
 arch/mips/mm/cex-oct.S                             |   73 +
 arch/mips/mm/pg-octeon.c                           |  120 +
 .../mach-cavium-octeon/cpu-feature-overrides.h     |   68 +
 include/asm-mips/mach-cavium-octeon/irq.h          |  252 +++
 .../mach-cavium-octeon/kernel-entry-init.h         |  116 +
 include/asm-mips/mach-cavium-octeon/mc146818rtc.h  |   14 +
 .../mach-cavium-octeon/octeon-hal-read-write.h     |   38 +
 .../asm-mips/mach-cavium-octeon/perf_counters.h    |   24 +
 30 files changed, 9052 insertions(+), 0 deletions(-)
 create mode 100644 arch/mips/cavium-octeon/Kconfig
 create mode 100644 arch/mips/cavium-octeon/Makefile
 create mode 100644 arch/mips/cavium-octeon/cs89x0.c
 create mode 100644 arch/mips/cavium-octeon/cs89x0.h
 create mode 100644 arch/mips/cavium-octeon/dma-octeon.c
 create mode 100644 arch/mips/cavium-octeon/gpl-executive/Makefile
 create mode 100644 arch/mips/cavium-octeon/gpl-executive/config/executive-config.h
 create mode 100644 arch/mips/cavium-octeon/gpl-executive/cvmx-linux-kernel-exports.c
 create mode 100644 arch/mips/cavium-octeon/hal.c
 create mode 100644 arch/mips/cavium-octeon/hal.h
 create mode 100644 arch/mips/cavium-octeon/i8259.c
 create mode 100644 arch/mips/cavium-octeon/irq.c
 create mode 100644 arch/mips/cavium-octeon/octeon-memcpy.S
 create mode 100644 arch/mips/cavium-octeon/octeon_info.c
 create mode 100644 arch/mips/cavium-octeon/perf_counters.c
 create mode 100644 arch/mips/cavium-octeon/serial.c
 create mode 100644 arch/mips/cavium-octeon/setup.c
 create mode 100644 arch/mips/cavium-octeon/smp.c
 create mode 100644 arch/mips/cavium-octeon/userio.c
 create mode 100644 arch/mips/kernel/irq-octeon.c
 create mode 100644 arch/mips/kernel/octeon_switch.S
 create mode 100644 arch/mips/mm/c-octeon.c
 create mode 100644 arch/mips/mm/cex-oct.S
 create mode 100644 arch/mips/mm/pg-octeon.c
 create mode 100644 include/asm-mips/mach-cavium-octeon/cpu-feature-overrides.h
 create mode 100644 include/asm-mips/mach-cavium-octeon/irq.h
 create mode 100644 include/asm-mips/mach-cavium-octeon/kernel-entry-init.h
 create mode 100644 include/asm-mips/mach-cavium-octeon/mc146818rtc.h
 create mode 100644 include/asm-mips/mach-cavium-octeon/octeon-hal-read-write.h
 create mode 100644 include/asm-mips/mach-cavium-octeon/perf_counters.h

diff --git a/arch/mips/cavium-octeon/Kconfig b/arch/mips/cavium-octeon/Kconfig
new file mode 100644
index 0000000..dcd51cc
--- /dev/null
+++ b/arch/mips/cavium-octeon/Kconfig
@@ -0,0 +1,223 @@
+config CAVIUM_OCTEON_SPECIFIC_OPTIONS
+	bool "Enable Octeon specific options"
+	depends on CPU_CAVIUM_OCTEON
+	default "y"
+
+config USE_RI_XI_PAGE_BITS
+	bool "Enable RI/XI extended page table bits"
+	depends on CPU_CAVIUM_OCTEON
+	default "n"
+	help
+	  This option enables the use of the Read Inhibit (RI) and Execute
+	  Inhibit (XI) on page table entries. These bits are only effective
+	  on processors that support them. Currently, only the CN5XXX series
+	  of Octeon processors support them.
+
+config CAVIUM_OCTEON_2ND_KERNEL
+	bool "Build the kernel to be used as a 2nd kernel on the same chip"
+	depends on CAVIUM_OCTEON_SPECIFIC_OPTIONS
+	default "n"
+	help
+	  This option configures this kernel to be linked at a different
+	  address and use the 2nd uart for output. This allows a kernel built
+	  with this option to be run at the same time as one built without this
+	  option.
+
+config CAVIUM_OCTEON_HW_FIX_UNALIGNED
+	bool "Enable hardware fixups of unaligned loads and stores"
+	depends on CAVIUM_OCTEON_SPECIFIC_OPTIONS
+	default "y"
+	help
+	  Configure the Octeon hardware to automatically fix unaligned loads
+	  and stores. Normally unaligned accesses are fixed using a kernel
+	  exception handler. This option enables the hardware automatic fixups,
+	  which requires only an extra 3 cycles. Disable this option if you
+	  are running code that relies on address exceptions on unaligned
+	  accesses.
+
+config CAVIUM_OCTEON_CVMSEG_SIZE
+	int "Number of L1 cache lines reserved for CVMSEG memory"
+	depends on CAVIUM_OCTEON_SPECIFIC_OPTIONS
+	range 0 54
+	default 1
+	help
+	  CVMSEG LM is a segment that accesses portions of the dcache as a
+	  local memory; the larger CVMSEG is, the smaller the cache is.
+	  This selects the size of CVMSEG LM, which is in cache blocks. The
+	  legally range is from zero to 54 cache blocks (i.e. CVMSEG LM is
+	  between zero and 6192 bytes).
+
+config CAVIUM_OCTEON_LOCK_L2
+	bool "Lock often used kernel code in the L2"
+	depends on CAVIUM_OCTEON_SPECIFIC_OPTIONS
+	default "y"
+	help
+	  Enable locking parts of the kernel into the L2 cache.
+
+config CAVIUM_OCTEON_LOCK_L2_TLB
+	bool "Lock the TLB handler in L2"
+	depends on CAVIUM_OCTEON_LOCK_L2
+	default "y"
+	help
+	  Lock the low level TLB fast path into L2.
+
+config CAVIUM_OCTEON_LOCK_L2_EXCEPTION
+	bool "Lock the exception handler in L2"
+	depends on CAVIUM_OCTEON_LOCK_L2
+	default "y"
+	help
+	  Lock the low level exception handler into L2.
+
+config CAVIUM_OCTEON_LOCK_L2_LOW_LEVEL_INTERRUPT
+	bool "Lock the interrupt handler in L2"
+	depends on CAVIUM_OCTEON_LOCK_L2
+	default "y"
+	help
+	  Lock the low level interrupt handler into L2.
+
+config CAVIUM_OCTEON_LOCK_L2_INTERRUPT
+	bool "Lock the 2nd level interrupt handler in L2"
+	depends on CAVIUM_OCTEON_LOCK_L2
+	default "y"
+	help
+	  Lock the 2nd level interrupt handler in L2.
+
+config CAVIUM_OCTEON_LOCK_L2_MEMCPY
+	bool "Lock memcpy() in L2"
+	depends on CAVIUM_OCTEON_LOCK_L2
+	default "y"
+	help
+	  Lock the kernel's implementation of memcpy() into L2.
+
+config CAVIUM_OCTEON_USER_IO
+	bool "Allow User space to access hardware IO directly"
+	depends on CAVIUM_OCTEON_SPECIFIC_OPTIONS
+	help
+	  Allows user applications to directly access the Octeon hardware
+	  IO addresses (0x1000000000000 - 0x1ffffffffffff). This allows high
+	  performance networking applications to run in user space with minimal
+	  performance penalties. This also means a user application can bring
+	  down the entire system. Only use this option on embedded devices
+	  where all user applications are strictly controlled.
+
+config CAVIUM_OCTEON_USER_MEM
+	bool "Allow User space to access memory directly"
+	depends on CAVIUM_OCTEON_SPECIFIC_OPTIONS
+	depends on CAVIUM_OCTEON_USER_IO
+	help
+	  Allows user applications to use XKPHYS addresses directly to memory.
+	  This allows user space direct access to shared memory not in use by
+	  Linux. This memory is suitable for use with the Octeon hardware.
+	  Cavium simple executive applications also share this memory. Since
+	  this bypass all of the Linux memory protection, only use this option
+	  on embedded devices where all user applications are strictly
+	  controlled.
+
+config CAVIUM_RESERVE32
+	int "Memory to reserve for user processes shared region (MB)"
+	range 0 1536
+	depends on CAVIUM_OCTEON_SPECIFIC_OPTIONS
+	depends on CAVIUM_OCTEON_USER_IO
+	default "0"
+	help
+	  Reserve a shared memory region for user processes to use for hardware
+	  memory buffers. This is required for 32bit applications to be able to
+	  send and receive packets directly. Applications access this memory by
+	  memory mapping /dev/mem for the addresses in /proc/octeon_info. For
+	  optimal performance with HugeTLBs, keep this size an even number of
+	  megabytes.
+
+config CAVIUM_RESERVE32_USE_WIRED_TLB
+	bool "Use wired TLB entries to access the reserved memory region"
+	depends on CAVIUM_OCTEON_SPECIFIC_OPTIONS
+	depends on CAVIUM_OCTEON_USER_IO
+	default "n"
+	help
+	  When this option is set, the memory reserved for the user process
+	  shared region (CONFIG_CAVIUM_RESERVE32) is globally mapped to all
+	  userspace programs using wired TLB entries. The userspace addresses
+	  beginning at 2GB-CONFIG_CAVIUM_RESERVE32 and ending at 2GB are hard
+	  wired to always map to the reserved memory region. This has the
+	  benefit of completely eliminating TLB overhead to the region, but may
+	  cause unwanted side affect. Since all memory in this region is shared
+	  across all userspace applications, any application attempting to
+	  mmap() these virtual address will fail in strange ways. In
+	  particular, be careful of shared libraries being mapped into this
+	  region. If this happens, the entire system may become unstable.
+
+	  Note: When this option is selected, CONFIG_CAVIUM_RESERVE32 must be
+	  512MB, 1024MB, or 1536MB. Any other valid will fail on boot.
+
+config CAVIUM_OCTEON_NUM_PACKET_BUFFERS
+	int "Number of packet buffers (and work queue entries) for the Ethernet driver"
+	range 0 8192
+	depends on CAVIUM_OCTEON_SPECIFIC_OPTIONS
+	default "1024"
+	help
+	  Number of packet buffers (and work queue entries) to allocate for
+	  the ethernet driver. Zero is treated as 1024.
+
+config ARCH_SPARSEMEM_ENABLE
+	def_bool y
+	select SPARSEMEM_STATIC
+	depends on CPU_CAVIUM_OCTEON
+
+config CAVIUM_OCTEON_POW_ONLY_ETHERNET
+	tristate "POW based internal only ethernet driver"
+	default m
+	help
+	  This option enables a very simple ethernet driver for internal core
+	  to core traffic. It relies on another driver, cavium-ethernet,
+	  to perform all hardware setup. This driver's purpose is to supply
+	  basic networking between different Linux images running on the same
+	  chip. A single core loads the cavium-ethernet module, all other cores
+	  load this driver. On load, the driver waits for some other core to
+	  perform hardware setup.
+
+config CAVIUM_OCTEON_MGMT_PORT_ETHERNET
+	tristate "Management port ethernet driver (CN5XXX)"
+	default y
+	help
+	  This option enables the ethernet driver for the management port on
+	  CN57XX, CN56XX, CN55XX, and CN54XX chips.
+
+config CAVIUM_OCTEON_WATCHDOG
+	tristate "Octeon watchdog driver"
+	default n
+	help
+	  This option enables a watchdog driver for all cores running Linux. It
+	  installs a NMI handler and pokes the watchdog based on an interrupt.
+	  On first expiration of the watchdog, the interrupt handler pokes it.
+	  The second expiration causes an NMI that prints a message and resets
+	  the chip. The third expiration causes a global soft reset.
+
+config CAVIUM_OCTEON_TRA
+	tristate "Octeon trace buffer (TRA) driver"
+	default n
+	help
+	  This option enables a driver for the Octeon trace buffer. By default
+	  it enables interrupts on some illegal memory accesses. See
+	  octeon-tra.c for information on customizing this driver to find
+	  specific problems.
+
+config CAVIUM_OCTEON_IPSEC
+   bool "Enable enhancements to the IPSec stack to allow procotol offload."
+   depends on CAVIUM_OCTEON_SPECIFIC_OPTIONS
+   default "n"
+   help
+     This enables enhancements to the IPSec stack to allow some of the
+     processing required for IPSec to be performed on another processor
+     which must be running the ipsec-filter application.
+
+config FORCE_MAX_ZONEORDER
+	int
+	depends on HUGETLB_PAGE && (PAGE_SIZE_16KB || PAGE_SIZE_32KB || PAGE_SIZE_64KB)
+	default 12 if PAGE_SIZE_16KB
+	default 13 if PAGE_SIZE_32KB
+	default 14 if PAGE_SIZE_32KB
+
+config CAVIUM_OCTEON_IPFWD_OFFLOAD
+	bool "Enable Cavium Octeon ip-offload module"
+	default "n"
+	help
+	  This enables Cavium Octeon ip-offload module.
diff --git a/arch/mips/cavium-octeon/Makefile b/arch/mips/cavium-octeon/Makefile
new file mode 100644
index 0000000..519be03
--- /dev/null
+++ b/arch/mips/cavium-octeon/Makefile
@@ -0,0 +1,20 @@
+#
+# Makefile for the Cavium Octeon specific kernel interface routines
+# under Linux.
+#
+# This file is subject to the terms and conditions of the GNU General Public
+# License.  See the file "COPYING" in the main directory of this archive
+# for more details.
+#
+# Copyright (C) 2005-2007 Cavium Networks
+#
+
+obj-y := setup.o serial.o irq.o hal.o perf_counters.o octeon_info.o
+obj-y += dma-octeon.o userio.o
+obj-y += octeon-memcpy.o
+
+obj-$(CONFIG_SMP)                     += smp.o
+
+clean:
+	rm -f *.o
+
diff --git a/arch/mips/cavium-octeon/cs89x0.c b/arch/mips/cavium-octeon/cs89x0.c
new file mode 100644
index 0000000..0a74af8
--- /dev/null
+++ b/arch/mips/cavium-octeon/cs89x0.c
@@ -0,0 +1,2264 @@
+/*
+   Copyright (C) 2006 Lanbird Technology. All rights reserved.
+
+   This program is free software; you can redistribute it and/or modify it
+   under the terms of the GNU General Public License as published by the Free
+   Software Foundation; either version 2 of the License, or (at your option)
+   any later version. */
+/* cs89x0.c: A Crystal Semiconductor (Now Cirrus Logic) CS89[02]0 driver for
+   linux. */
+
+/*
+   Written 1996 by Russell Nelson, with reference to skeleton.c written
+   1993-1994 by Donald Becker.
+
+   This software may be used and distributed according to the terms of the GNU
+   General Public License, incorporated herein by reference.
+
+   The author may be reached at nelson@crynwr.com, Crynwr Software, 521
+   Pleasant Valley Rd., Potsdam, NY 13676
+
+   Changelog:
+
+   Mike Cruse : mcruse@cti-ltd.com : Changes for Linux 2.0 compatibility. :
+   Added dev_id parameter in net_interrupt(), : request_irq() and free_irq().
+   Just NULL for now.
+
+   Mike Cruse : Added MOD_INC_USE_COUNT and MOD_DEC_USE_COUNT macros : in
+   net_open() and net_close() so kerneld would know : that the module is in use
+   and wouldn't eject the : driver prematurely.
+
+   Mike Cruse : Rewrote init_module() and cleanup_module using 8390.c : as an
+   example. Disabled autoprobing in init_module(), : not a good thing to do to
+   other devices while Linux : is running from all accounts.
+
+   Russ Nelson : Jul 13 1998.  Added RxOnly DMA support.
+
+   Melody Lee : Aug 10 1999.  Changes for Linux 2.2.5 compatibility. : email:
+   ethernet@crystal.cirrus.com
+
+   Alan Cox : Removed 1.2 support, added 2.1 extra counters.
+
+   Andrew Morton : andrewm@uow.edu.au : Kernel 2.3.48 : Handle kmalloc()
+   failures : Other resource allocation fixes : Add SMP locks : Integrate Russ
+   Nelson's ALLOW_DMA functionality back in. : If ALLOW_DMA is true, make DMA
+   runtime selectable : Folded in changes from Cirrus (Melody Lee :
+   <klee@crystal.cirrus.com>) : Don't call netif_wake_queue() in
+   net_send_packet() : Fixed an out-of-mem bug in dma_rx() : Updated
+   Documentation/networking/cs89x0.txt
+
+   Andrew Morton : andrewm@uow.edu.au / Kernel 2.3.99-pre1 : Use skb_reserve to
+   longword align IP header (two places) : Remove a delay loop from dma_rx() :
+   Replace '100' with HZ : Clean up a couple of skb API abuses : Added
+   'cs89x0_dma=N' kernel boot option : Correctly initialise lp->lock in
+   non-module compile
+
+   Andrew Morton : andrewm@uow.edu.au / Kernel 2.3.99-pre4-1 : MOD_INC/DEC race
+   fix (see :
+   http://www.uwsg.indiana.edu/hypermail/linux/kernel/0003.3/1532.html)
+
+   Andrew Morton : andrewm@uow.edu.au / Kernel 2.4.0-test7-pre2 : Enhanced
+   EEPROM support to cover more devices, : abstracted IRQ mapping to support
+   CONFIG_ARCH_CLPS7500 arch : (Jason Gunthorpe <jgg@ualberta.ca>)
+
+   Andrew Morton : Kernel 2.4.0-test11-pre4 : Use dev->name in request_*()
+   (Andrey Panin) : Fix an error-path memleak in init_module() : Preserve
+   return value from request_irq() : Fix type of `media' module parm (Keith
+   Owens) : Use SET_MODULE_OWNER() : Tidied up strange request_irq() abuse in
+   net_open().
+
+   Andrew Morton : Kernel 2.4.3-pre1 : Request correct number of pages for DMA
+   (Hugh Dickens) : Select PP_ChipID _after_ unregister_netdev in
+   cleanup_module() : because unregister_netdev() calls get_stats. : Make
+   `version[]' __initdata : Uninlined the read/write reg/word functions.
+
+   Oskar Schirmer : oskar@scara.com : HiCO.SH4 (superh) support added (irq#1,
+   cs89x0_media=)
+
+   Deepak Saxena : dsaxena@plexity.net : Intel IXDP2x01 (XScale ixp2x00 NPU)
+   platform support
+
+ */
+
+/* Always include 'config.h' first in case the user wants to turn on or
+   override something. */
+#include <linux/config.h>
+#include <linux/module.h>
+
+/*
+ * Set this to zero to disable DMA code
+ *
+ * Note that even if DMA is turned off we still support the 'dma' and  'use_dma'
+ * module options so we don't break any startup scripts.
+ */
+#ifndef CONFIG_ARCH_IXDP2X01
+#define ALLOW_DMA	0
+#else
+#define ALLOW_DMA	1
+#endif
+
+/*
+ * Set this to zero to remove all the debug statements via
+ * dead code elimination
+ */
+#if 0				// kihbingo
+#define DEBUGGING	1
+#endif
+
+/*
+   Sources:
+
+   Crynwr packet driver epktisa.
+
+   Crystal Semiconductor data sheets.
+
+ */
+
+#include <linux/errno.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/fcntl.h>
+#include <linux/interrupt.h>
+#include <linux/ioport.h>
+#include <linux/in.h>
+#include <linux/skbuff.h>
+#include <linux/slab.h>
+#include <linux/spinlock.h>
+#include <linux/string.h>
+#include <linux/init.h>
+#include <linux/bitops.h>
+#include <linux/delay.h>
+
+#include <asm/system.h>
+#include <asm/io.h>
+#if ALLOW_DMA
+#include <asm/dma.h>
+#endif
+
+#include "cs89x0.h"
+
+static char version[] __initdata =
+	"cs89x0.c: v2.4.3-pre1 Russell Nelson <nelson@crynwr.com>, Andrew Morton <andrewm@uow.edu.au>\n";
+
+#define DRV_NAME "cs89x0"
+
+#if 0				// kihbingo
+/* First, a few definitions that the brave might change. A zero-terminated list
+   of I/O addresses to be probed. Some special flags.. Addr & 1 = Read back the
+   address port, look for signature and reset the page window before probing
+   Addr & 3 = Reset the page window and probe The CLPS eval board has the
+   Cirrus chip at 0x80090300, in ARM IO space, but it is possible that a Cirrus
+   board could be plugged into the ISA slots. */
+/* The cs8900 has 4 IRQ pins, software selectable. cs8900_irq_map maps them to
+   system IRQ numbers. This mapping is card specific and is set to the
+   configuration of the Cirrus Eval board for this chip. */
+#ifdef CONFIG_ARCH_CLPS7500
+static unsigned int netcard_portlist[] __initdata =
+	{ 0x80090303, 0x300, 0x320, 0x340, 0x360, 0x200, 0x220, 0x240, 0x260,
+0x280, 0x2a0, 0x2c0, 0x2e0, 0 };
+static unsigned int cs8900_irq_map[] = { 12, 0, 0, 0 };
+#elif defined(CONFIG_SH_HICOSH4)
+static unsigned int netcard_portlist[] __initdata = { 0x0300, 0 };
+static unsigned int cs8900_irq_map[] = { 1, 0, 0, 0 };
+#elif defined(CONFIG_ARCH_IXDP2X01)
+#include <asm/irq.h>
+static unsigned int netcard_portlist[] __initdata =
+	{ IXDP2X01_CS8900_VIRT_BASE, 0 };
+static unsigned int cs8900_irq_map[] = { IRQ_IXDP2X01_CS8900, 0, 0, 0 };
+#elif defined(CONFIG_ARCH_PNX0105)
+#include <asm/irq.h>
+#include <asm/arch/gpio.h>
+#define CIRRUS_DEFAULT_BASE	IO_ADDRESS(EXT_STATIC2_s0_BASE + 0x200000)	/* =
+										   Physical
+										   address
+										   0x48200000
+										 */
+#define CIRRUS_DEFAULT_IRQ	VH_INTC_INT_NUM_CASCADED_INTERRUPT_1	/* Event
+									   inputs
+									   bank
+									   1 -
+									   ID
+									   35/bit
+									   3 */
+static unsigned int netcard_portlist[] __initdata = { CIRRUS_DEFAULT_BASE, 0 };
+static unsigned int cs8900_irq_map[] = { CIRRUS_DEFAULT_IRQ, 0, 0, 0 };
+#else
+static unsigned int netcard_portlist[] __initdata =
+	{ 0x300, 0x320, 0x340, 0x360, 0x200, 0x220, 0x240, 0x260, 0x280, 0x2a0,
+0x2c0, 0x2e0, 0 };
+static unsigned int cs8900_irq_map[] = { 10, 11, 12, 5 };
+#endif
+#endif
+
+#if DEBUGGING
+static unsigned int net_debug = DEBUGGING;
+#else
+#define net_debug 0		/* gcc will remove all the debug code for us */
+#endif
+
+/* The number of low I/O ports used by the ethercard. */
+#define NETCARD_IO_EXTENT	16
+
+/* we allow the user to override various values normally set in the EEPROM */
+#define FORCE_RJ45	0x0001	/* pick one of these three */
+#define FORCE_AUI	0x0002
+#define FORCE_BNC	0x0004
+
+#define FORCE_AUTO	0x0010	/* pick one of these three */
+#define FORCE_HALF	0x0020
+#define FORCE_FULL	0x0030
+
+/* Information that need to be kept for each board. */
+struct net_local {
+	struct net_device_stats stats;
+	int chip_type;		/* one of: CS8900, CS8920, CS8920M */
+	char chip_revision;	/* revision letter of the chip ('A'...) */
+	int send_cmd;		/* the proper send command: TX_NOW,
+				   TX_AFTER_381, or TX_AFTER_ALL */
+	int auto_neg_cnf;	/* auto-negotiation word from EEPROM */
+	int adapter_cnf;	/* adapter configuration from EEPROM */
+	int isa_config;		/* ISA configuration from EEPROM */
+	int irq_map;		/* IRQ map from EEPROM */
+	int rx_mode;		/* what mode are we in? 0, RX_MULTCAST_ACCEPT,
+				   or RX_ALL_ACCEPT */
+	int curr_rx_cfg;	/* a copy of PP_RxCFG */
+	int linectl;		/* either 0 or LOW_RX_SQUELCH, depending on
+				   configuration. */
+	int send_underrun;	/* keep track of how many underruns in a row we
+				   get */
+	int force;		/* force various values; see FORCE* above. */
+	spinlock_t lock;
+#if ALLOW_DMA
+	int use_dma;		/* Flag: we're using dma */
+	int dma;		/* DMA channel */
+	int dmasize;		/* 16 or 64 */
+	unsigned char *dma_buff;	/* points to the beginning of the
+					   buffer */
+	unsigned char *end_dma_buff;	/* points to the end of the buffer */
+	unsigned char *rx_dma_ptr;	/* points to the next packet */
+#endif
+};
+
+/* Index to functions, as function prototypes. */
+
+static int cs89x0_probe1(struct net_device *dev, int ioaddr, int modular);
+static int net_open(struct net_device *dev);
+static int net_send_packet(struct sk_buff *skb, struct net_device *dev);
+static irqreturn_t net_interrupt(int irq, void *dev_id, struct pt_regs *regs);
+static void set_multicast_list(struct net_device *dev);
+static void net_timeout(struct net_device *dev);
+static void net_rx(struct net_device *dev);
+static int net_close(struct net_device *dev);
+static struct net_device_stats *net_get_stats(struct net_device *dev);
+static void reset_chip(struct net_device *dev);
+static int get_eeprom_data(struct net_device *dev, int off, int len,
+			   int *buffer);
+static int get_eeprom_cksum(int off, int len, int *buffer);
+static int set_mac_address(struct net_device *dev, void *addr);
+static void count_rx_errors(int status, struct net_local *lp);
+#if ALLOW_DMA
+static void get_dma_channel(struct net_device *dev);
+static void release_dma_buff(struct net_local *lp);
+#endif
+#if 1				// kihbingo
+static inline unsigned short INW_swap(unsigned short *addr)
+{
+	// swap
+	unsigned short result;
+	result = *(unsigned short *) (addr);
+	return (((result >> 8) & 0xFF) | ((result << 8) & 0xFF00));
+}
+static inline unsigned short INW(unsigned short *addr)
+{
+	// no swap
+	return (*(unsigned short *) (addr));
+}
+static inline void OUTW_swap(unsigned short value, unsigned short *addr)
+{
+	// swap
+	*(volatile unsigned char *) (addr) = value;
+	*(volatile unsigned char *) (addr) = value >> 8;
+}
+static inline void OUTW(unsigned short value, unsigned short *addr)
+{
+	// no swap
+	*(volatile unsigned char *) (addr) = value >> 8;
+	*(volatile unsigned char *) (addr) = value;
+}
+#endif
+
+/* Example routines you must write ;->. */
+#define tx_done(dev) 1
+
+#if 0				// kihbingo
+/*
+ * Permit 'cs89x0_dma=N' in the kernel boot environment
+ */
+#if !defined(MODULE) && (ALLOW_DMA != 0)
+static int g_cs89x0_dma;
+
+static int __init dma_fn(char *str)
+{
+	g_cs89x0_dma = simple_strtol(str, NULL, 0);
+	return 1;
+}
+
+__setup("cs89x0_dma=", dma_fn);
+#endif				/* !defined(MODULE) && (ALLOW_DMA != 0) */
+
+#ifndef MODULE
+static int g_cs89x0_media__force;
+
+static int __init media_fn(char *str)
+{
+	if (!strcmp(str, "rj45"))
+		g_cs89x0_media__force = FORCE_RJ45;
+	else if (!strcmp(str, "aui"))
+		g_cs89x0_media__force = FORCE_AUI;
+	else if (!strcmp(str, "bnc"))
+		g_cs89x0_media__force = FORCE_BNC;
+	return 1;
+}
+
+__setup("cs89x0_media=", media_fn);
+
+
+/* Check for a network adaptor of this type, and return '0' iff one exists. If
+   dev->base_addr == 0, probe all likely locations. If dev->base_addr == 1,
+   always return failure. If dev->base_addr == 2, allocate space for the device
+   and return success (detachable devices only). Return 0 on success. */
+
+struct net_device *__init cs89x0_probe(int unit)
+{
+	struct net_device *dev = alloc_etherdev(sizeof(struct net_local));
+	unsigned *port;
+	int err = 0;
+	int irq;
+	int io;
+
+	if (!dev)
+		return ERR_PTR(-ENODEV);
+
+	sprintf(dev->name, "eth%d", unit);
+	netdev_boot_setup_check(dev);
+	io = dev->base_addr;
+	irq = dev->irq;
+
+	if (net_debug)
+		printk("cs89x0:cs89x0_probe(0x%x)\n", io);
+
+	if (io > 0x1ff) {	/* Check a single specified location. */
+		err = cs89x0_probe1(dev, io, 0);
+	} else if (io != 0) {	/* Don't probe at all. */
+		err = -ENXIO;
+	} else {
+		for (port = netcard_portlist; *port; port++) {
+			if (cs89x0_probe1(dev, *port, 0) == 0)
+				break;
+			dev->irq = irq;
+		}
+		if (!*port)
+			err = -ENODEV;
+	}
+	if (err)
+		goto out;
+	return dev;
+      out:
+	free_netdev(dev);
+	printk(KERN_WARNING
+	       "cs89x0: no cs8900 or cs8920 detected.  Be sure to disable PnP with SETUP\n");
+	return ERR_PTR(err);
+}
+#endif
+#endif
+
+static int readreg(struct net_device *dev, int portno)
+{
+#if 0				// kihbingo
+	outw(portno, dev->base_addr + ADD_PORT);
+	return inw(dev->base_addr + DATA_PORT);
+#else
+	OUTW_swap((unsigned short) portno,
+		  (unsigned short *) (dev->base_addr + ADD_PORT));
+	return INW_swap((unsigned short *) (dev->base_addr + DATA_PORT));
+#endif
+}
+
+static void writereg(struct net_device *dev, int portno, int value)
+{
+#if 0				// kihbingo
+	outw(portno, dev->base_addr + ADD_PORT);
+	outw(value, dev->base_addr + DATA_PORT);
+#else
+	OUTW_swap((unsigned short) portno,
+		  (unsigned short *) (dev->base_addr + ADD_PORT));
+	OUTW_swap((unsigned short) value,
+		  (unsigned short *) (dev->base_addr + DATA_PORT));
+#endif
+}
+
+static int readword(struct net_device *dev, int portno)
+{
+#if 0				// kihbingo
+	return inw(dev->base_addr + portno);
+#else
+	return INW_swap((unsigned short *) (dev->base_addr + portno));
+#endif
+}
+
+#if 1				// kihbingo
+static void readstring(struct net_device *dev, int portno, void *Ptr, int len)
+{
+	unsigned short *ptr = (unsigned short *) Ptr;
+
+	while (len > 0) {
+		*ptr = INW((unsigned short *) (dev->base_addr + portno));
+		ptr++;
+		len--;
+	}
+}
+#endif
+
+static void writeword(struct net_device *dev, int portno, int value)
+{
+#if 0				// kihbingo
+	outw(value, dev->base_addr + portno);
+#else
+	OUTW_swap((unsigned short) value,
+		  (unsigned short *) (dev->base_addr + portno));
+#endif
+}
+
+#if 1				// kihbingo
+static void writestring(struct net_device *dev, int portno, void *Ptr, int len)
+{
+	unsigned short *ptr = (unsigned short *) Ptr;
+
+	// no swap
+	while (len > 0) {
+		OUTW((unsigned short) *ptr,
+		     (unsigned short *) (dev->base_addr + portno));
+		ptr++;
+		len--;
+	}
+}
+#endif
+
+static int __init wait_eeprom_ready(struct net_device *dev)
+{
+	int timeout = jiffies;
+	/* check to see if the EEPROM is ready, a timeout is used - just in
+	   case EEPROM is ready when SI_BUSY in the PP_SelfST is clear */
+	while (readreg(dev, PP_SelfST) & SI_BUSY)
+		if (jiffies - timeout >= 40)
+			return -1;
+	return 0;
+}
+
+static int __init
+get_eeprom_data(struct net_device *dev, int off, int len, int *buffer)
+{
+	int i;
+
+	if (net_debug > 3)
+		printk("EEPROM data from %x for %x:\n", off, len);
+	for (i = 0; i < len; i++) {
+		if (wait_eeprom_ready(dev) < 0)
+			return -1;
+		/* Now send the EEPROM read command and EEPROM location to read
+		 */
+		writereg(dev, PP_EECMD, (off + i) | EEPROM_READ_CMD);
+		if (wait_eeprom_ready(dev) < 0)
+			return -1;
+		buffer[i] = readreg(dev, PP_EEData);
+		if (net_debug > 3)
+			printk("%04x ", buffer[i]);
+	}
+	if (net_debug > 3)
+		printk("\n");
+	return 0;
+}
+
+static int __init get_eeprom_cksum(int off, int len, int *buffer)
+{
+	int i, cksum;
+
+	cksum = 0;
+	for (i = 0; i < len; i++)
+		cksum += buffer[i];
+	cksum &= 0xffff;
+	if (cksum == 0)
+		return 0;
+	return -1;
+}
+
+/* This is the real probe routine.  Linux has a history of friendly device
+   probes on the ISA bus.  A good device probes avoids doing writes, and
+   verifies that the correct device exists and functions. Return 0 on success. */
+
+static int __init cs89x0_probe1(struct net_device *dev, int ioaddr, int modular)
+{
+	struct net_local *lp = netdev_priv(dev);
+	static unsigned version_printed;
+	int i;
+	int tmp;
+	unsigned rev_type = 0;
+	int eeprom_buff[CHKSUM_LEN];
+	int retval;
+
+	SET_MODULE_OWNER(dev);
+	/* Initialize the device structure. */
+	if (!modular) {
+		memset(lp, 0, sizeof(*lp));
+		spin_lock_init(&lp->lock);
+#ifndef MODULE
+#if ALLOW_DMA
+		if (g_cs89x0_dma) {
+			lp->use_dma = 1;
+			lp->dma = g_cs89x0_dma;
+			lp->dmasize = 16;	/* Could make this an option...
+						 */
+		}
+#endif
+#if 0				// kihbingo
+		lp->force = g_cs89x0_media__force;
+#else
+		lp->force = FORCE_RJ45;
+#endif
+#endif
+	}
+#ifdef CONFIG_ARCH_PNX0105
+	initialize_ebi();
+
+	/* Map GPIO registers for the pins connected to the CS8900a. */
+	if (map_cirrus_gpio() < 0)
+		return -ENODEV;
+
+	reset_cirrus();
+
+	/* Map event-router registers. */
+	if (map_event_router() < 0)
+		return -ENODEV;
+
+	enable_cirrus_irq();
+
+	unmap_cirrus_gpio();
+	unmap_event_router();
+
+	dev->base_addr = ioaddr;
+
+	for (i = 0; i < 3; i++)
+		readreg(dev, 0);
+#endif
+
+#if 0				// kihbingo
+	/* Grab the region so we can find another board if autoIRQ fails. */
+	/* WTF is going on here? */
+	if (!request_region(ioaddr & ~3, NETCARD_IO_EXTENT, DRV_NAME)) {
+		printk(KERN_ERR "%s: request_region(0x%x, 0x%x) failed\n",
+		       DRV_NAME, ioaddr, NETCARD_IO_EXTENT);
+		retval = -EBUSY;
+		goto out1;
+	}
+#ifdef CONFIG_SH_HICOSH4
+	/* truely reset the chip */
+	outw(0x0114, ioaddr + ADD_PORT);
+	outw(0x0040, ioaddr + DATA_PORT);
+#endif
+
+	/* if they give us an odd I/O address, then do ONE write to the address
+	   port, to get it back to address zero, where we expect to find the
+	   EISA signature word. An IO with a base of 0x3 will skip the test for
+	   the ADD_PORT. */
+	if (ioaddr & 1) {
+		if (net_debug > 1)
+			printk(KERN_INFO "%s: odd ioaddr 0x%x\n", dev->name,
+			       ioaddr);
+		if ((ioaddr & 2) != 2)
+			if ((inw((ioaddr & ~3) + ADD_PORT) & ADD_MASK) !=
+			    ADD_SIG) {
+				printk(KERN_ERR "%s: bad signature 0x%x\n",
+				       dev->name,
+				       inw((ioaddr & ~3) + ADD_PORT));
+				retval = -ENODEV;
+				goto out2;
+			}
+	}
+	printk(KERN_DEBUG "PP_addr at %x: 0x%x\n",
+	       ioaddr + ADD_PORT, inw(ioaddr + ADD_PORT));
+
+	ioaddr &= ~3;
+	outw(PP_ChipID, ioaddr + ADD_PORT);
+
+	tmp = inw(ioaddr + DATA_PORT);
+	if (tmp != CHIP_EISA_ID_SIG) {
+		printk(KERN_DEBUG "%s: incorrect signature at %x: 0x%x!="
+		       CHIP_EISA_ID_SIG_STR "\n",
+		       dev->name, ioaddr + DATA_PORT, tmp);
+		retval = -ENODEV;
+		goto out2;
+	}
+
+	/* Fill in the 'dev' fields. */
+	dev->base_addr = ioaddr;
+#else				// kihbingo
+	tmp = readreg(dev, PP_ChipID);
+	if (tmp != CHIP_EISA_ID_SIG) {
+		printk(KERN_INFO "%s: incorrect signature at %x: 0x%x!="
+		       CHIP_EISA_ID_SIG_STR "\n",
+		       dev->name, (unsigned int) (dev->base_addr + DATA_PORT),
+		       tmp);
+		retval = -ENODEV;
+		goto out2;
+	}
+#endif
+
+	/* get the chip type */
+	rev_type = readreg(dev, PRODUCT_ID_ADD);
+	lp->chip_type = rev_type & ~REVISON_BITS;
+	lp->chip_revision = ((rev_type & REVISON_BITS) >> 8) + 'A';
+
+	/* Check the chip type and revision in order to set the correct send
+	   command CS8920 revision C and CS8900 revision F can use the faster
+	   send. */
+	lp->send_cmd = TX_AFTER_381;
+	if (lp->chip_type == CS8900 && lp->chip_revision >= 'F')
+		lp->send_cmd = TX_NOW;
+	if (lp->chip_type != CS8900 && lp->chip_revision >= 'C')
+		lp->send_cmd = TX_NOW;
+
+	if (net_debug && version_printed++ == 0)
+		printk(version);
+
+	printk(KERN_INFO "%s: cs89%c0%s rev %c found at %#3lx ",
+	       dev->name,
+	       lp->chip_type == CS8900 ? '0' : '2',
+	       lp->chip_type == CS8920M ? "M" : "",
+	       lp->chip_revision, dev->base_addr);
+
+	reset_chip(dev);
+
+	/* Here we read the current configuration of the chip. If there is no
+	   Extended EEPROM then the idea is to not disturb the chip
+	   configuration, it should have been correctly setup by automatic
+	   EEPROM read on reset. So, if the chip says it read the EEPROM the
+	   driver will always do *something* instead of complain that
+	   adapter_cnf is 0. */
+
+#ifdef CONFIG_SH_HICOSH4
+	if (1) {
+		/* For the HiCO.SH4 board, things are different: we don't have
+		   EEPROM, but there is some data in flash, so we go get it
+		   there directly (MAC). */
+		__u16 *confd;
+		short cnt;
+		if (((*(volatile __u32 *) 0xa0013ff0) & 0x00ffffff)
+		    == 0x006c3000) {
+			confd = (__u16 *) 0xa0013fc0;
+		} else {
+			confd = (__u16 *) 0xa001ffc0;
+		}
+		cnt = (*confd++ & 0x00ff) >> 1;
+		while (--cnt > 0) {
+			__u16 j = *confd++;
+
+			switch (j & 0x0fff) {
+			case PP_IA:
+				for (i = 0; i < ETH_ALEN / 2; i++) {
+					dev->dev_addr[i * 2] = confd[i] & 0xFF;
+					dev->dev_addr[i * 2 + 1] =
+						confd[i] >> 8;
+				}
+				break;
+			}
+			j = (j >> 12) + 1;
+			confd += j;
+			cnt -= j;
+		}
+	} else
+#endif
+
+	if ((readreg(dev, PP_SelfST) & (EEPROM_OK | EEPROM_PRESENT)) ==
+		    (EEPROM_OK | EEPROM_PRESENT)) {
+		/* Load the MAC. */
+		for (i = 0; i < ETH_ALEN / 2; i++) {
+			unsigned int Addr;
+			Addr = readreg(dev, PP_IA + i * 2);
+			dev->dev_addr[i * 2] = Addr & 0xFF;
+			dev->dev_addr[i * 2 + 1] = Addr >> 8;
+		}
+
+		/* Load the Adapter Configuration. Note: Barring any more
+		   specific information from some other source (ie
+		   EEPROM+Schematics), we would not know how to operate a
+		   10Base2 interface on the AUI port. However, since we do
+		   read the status of HCB1 and use settings that always result
+		   in calls to control_dc_dc(dev,0) a BNC interface should
+		   work if the enable pin (dc/dc converter) is on HCB1. It
+		   will be called AUI however. */
+
+		lp->adapter_cnf = 0;
+		i = readreg(dev, PP_LineCTL);
+		/* Preserve the setting of the HCB1 pin. */
+		if ((i & (HCB1 | HCB1_ENBL)) == (HCB1 | HCB1_ENBL))
+			lp->adapter_cnf |= A_CNF_DC_DC_POLARITY;
+		/* Save the sqelch bit */
+		if ((i & LOW_RX_SQUELCH) == LOW_RX_SQUELCH)
+			lp->adapter_cnf |=
+				A_CNF_EXTND_10B_2 | A_CNF_LOW_RX_SQUELCH;
+		/* Check if the card is in 10Base-t only mode */
+		if ((i & (AUI_ONLY | AUTO_AUI_10BASET)) == 0)
+			lp->adapter_cnf |= A_CNF_10B_T | A_CNF_MEDIA_10B_T;
+		/* Check if the card is in AUI only mode */
+		if ((i & (AUI_ONLY | AUTO_AUI_10BASET)) == AUI_ONLY)
+			lp->adapter_cnf |= A_CNF_AUI | A_CNF_MEDIA_AUI;
+		/* Check if the card is in Auto mode. */
+		if ((i & (AUI_ONLY | AUTO_AUI_10BASET)) == AUTO_AUI_10BASET)
+			lp->adapter_cnf |= A_CNF_AUI | A_CNF_10B_T |
+				A_CNF_MEDIA_AUI | A_CNF_MEDIA_10B_T |
+				A_CNF_MEDIA_AUTO;
+
+		if (net_debug > 1)
+			printk(KERN_INFO
+			       "%s: PP_LineCTL=0x%x, adapter_cnf=0x%x\n",
+			       dev->name, i, lp->adapter_cnf);
+
+		/* IRQ. Other chips already probe, see below. */
+		if (lp->chip_type == CS8900)
+			lp->isa_config =
+				readreg(dev, PP_CS8900_ISAINT) & INT_NO_MASK;
+
+		printk("[Cirrus EEPROM] ");
+	}
+
+	printk("\n");
+
+	/* First check to see if an EEPROM is attached. */
+#ifdef CONFIG_SH_HICOSH4	/* no EEPROM on HiCO, don't hazzle with it here
+				 */
+	if (1) {
+		printk(KERN_NOTICE "cs89x0: No EEPROM on HiCO.SH4\n");
+	} else
+#endif
+	if ((readreg(dev, PP_SelfST) & EEPROM_PRESENT) == 0)
+		printk(KERN_WARNING
+		       "cs89x0: No EEPROM, relying on command line....\n");
+	else if (get_eeprom_data
+		 (dev, START_EEPROM_DATA, CHKSUM_LEN, eeprom_buff) < 0) {
+#if 0				// kihbingo
+		printk(KERN_WARNING
+		       "\ncs89x0: EEPROM read failed, relying on command line.\n");
+#else
+		printk(KERN_WARNING
+		       "cs89x0: EEPROM read failed, relying on command line.\n");
+#endif
+	} else if (get_eeprom_cksum(START_EEPROM_DATA, CHKSUM_LEN, eeprom_buff)
+		   < 0) {
+		/* Check if the chip was able to read its own configuration
+		   starting at 0 in the EEPROM */
+		if ((readreg(dev, PP_SelfST) & (EEPROM_OK | EEPROM_PRESENT)) !=
+		    (EEPROM_OK | EEPROM_PRESENT))
+			printk(KERN_WARNING
+			       "cs89x0: Extended EEPROM checksum bad and no Cirrus EEPROM, relying on command line\n");
+
+	} else {
+		/* This reads an extended EEPROM that is not documented in the
+		   CS8900 datasheet. */
+
+		/* get transmission control word but keep the autonegotiation
+		   bits */
+		if (!lp->auto_neg_cnf)
+			lp->auto_neg_cnf = eeprom_buff[AUTO_NEG_CNF_OFFSET / 2];
+		/* Store adapter configuration */
+		if (!lp->adapter_cnf)
+			lp->adapter_cnf = eeprom_buff[ADAPTER_CNF_OFFSET / 2];
+		/* Store ISA configuration */
+		lp->isa_config = eeprom_buff[ISA_CNF_OFFSET / 2];
+		dev->mem_start = eeprom_buff[PACKET_PAGE_OFFSET / 2] << 8;
+
+		/* eeprom_buff has 32-bit ints, so we can't just memcpy it */
+		/* store the initial memory base address */
+		for (i = 0; i < ETH_ALEN / 2; i++) {
+			dev->dev_addr[i * 2] = eeprom_buff[i];
+			dev->dev_addr[i * 2 + 1] = eeprom_buff[i] >> 8;
+		}
+		if (net_debug > 1)
+			printk(KERN_DEBUG "%s: new adapter_cnf: 0x%x\n",
+			       dev->name, lp->adapter_cnf);
+	}
+
+	/* allow them to force multiple transceivers.  If they force multiple,
+	   autosense */
+	{
+		int count = 0;
+		if (lp->force & FORCE_RJ45) {
+			lp->adapter_cnf |= A_CNF_10B_T;
+			count++;
+		}
+		if (lp->force & FORCE_AUI) {
+			lp->adapter_cnf |= A_CNF_AUI;
+			count++;
+		}
+		if (lp->force & FORCE_BNC) {
+			lp->adapter_cnf |= A_CNF_10B_2;
+			count++;
+		}
+		if (count > 1) {
+			lp->adapter_cnf |= A_CNF_MEDIA_AUTO;
+		} else if (lp->force & FORCE_RJ45) {
+			lp->adapter_cnf |= A_CNF_MEDIA_10B_T;
+		} else if (lp->force & FORCE_AUI) {
+			lp->adapter_cnf |= A_CNF_MEDIA_AUI;
+		} else if (lp->force & FORCE_BNC) {
+			lp->adapter_cnf |= A_CNF_MEDIA_10B_2;
+		}
+	}
+
+	if (net_debug > 1)
+		printk(KERN_DEBUG "%s: after force 0x%x, adapter_cnf=0x%x\n",
+		       dev->name, lp->force, lp->adapter_cnf);
+
+	/* FIXME: We don't let you set dc-dc polarity or low RX squelch from
+	   the command line: add it here */
+
+	/* FIXME: We don't let you set the IMM bit from the command line: add
+	   it to lp->auto_neg_cnf here */
+
+	/* FIXME: we don't set the Ethernet address on the command line.  Use
+	   ifconfig IFACE hw ether AABBCCDDEEFF */
+
+	printk(KERN_INFO "cs89x0 media %s%s%s",
+	       (lp->adapter_cnf & A_CNF_10B_T) ? "RJ-45," : "",
+	       (lp->adapter_cnf & A_CNF_AUI) ? "AUI," : "",
+	       (lp->adapter_cnf & A_CNF_10B_2) ? "BNC," : "");
+
+#if 0				// kihbingo
+	lp->irq_map = 0xffff;
+
+	/* If this is a CS8900 then no pnp soft */
+	if (lp->chip_type != CS8900 &&
+	    /* Check if the ISA IRQ has been set */
+	    (i = readreg(dev, PP_CS8920_ISAINT) & 0xff,
+	     (i != 0 && i < CS8920_NO_INTS))) {
+		if (!dev->irq)
+			dev->irq = i;
+	} else {
+		i = lp->isa_config & INT_NO_MASK;
+		if (lp->chip_type == CS8900) {
+#if defined(CONFIG_ARCH_IXDP2X01) || defined(CONFIG_ARCH_PNX0105)
+			i = cs8900_irq_map[0];
+#else
+			/* Translate the IRQ using the IRQ mapping table. */
+			if (i >=
+			    sizeof(cs8900_irq_map) / sizeof(cs8900_irq_map[0]))
+				printk("\ncs89x0: invalid ISA interrupt number %d\n", i);
+			else
+				i = cs8900_irq_map[i];
+
+			lp->irq_map = CS8900_IRQ_MAP;	/* fixed IRQ map for
+							   CS8900 */
+		} else {
+			int irq_map_buff[IRQ_MAP_LEN / 2];
+
+			if (get_eeprom_data(dev, IRQ_MAP_EEPROM_DATA,
+					    IRQ_MAP_LEN / 2,
+					    irq_map_buff) >= 0) {
+				if ((irq_map_buff[0] & 0xff) == PNP_IRQ_FRMT)
+					lp->irq_map =
+						(irq_map_buff[0] >> 8) |
+						(irq_map_buff[1] << 8);
+			}
+#endif
+		}
+		if (!dev->irq)
+			dev->irq = i;
+	}
+#endif
+
+	printk(" IRQ %d", dev->irq);
+
+#if ALLOW_DMA
+	if (lp->use_dma) {
+		get_dma_channel(dev);
+		printk(", DMA %d", dev->dma);
+	} else
+#endif
+	{
+		printk(", programmed I/O");
+	}
+
+#if 1				// kihbingo
+	/* Set the MAC. */
+	extern unsigned char *octeon_get_mac_addr_base(void);
+	unsigned char *mac_tmp = octeon_get_mac_addr_base();
+	for (i = 0; i < 6; i++) {
+		dev->dev_addr[i] = mac_tmp[i];
+	}
+	dev->dev_addr[5] += 0x1F;
+#endif
+	/* print the ethernet address. */
+	printk(", MAC");
+	for (i = 0; i < ETH_ALEN; i++) {
+		printk("%c%02x", i ? ':' : ' ', dev->dev_addr[i]);
+	}
+
+	dev->open = net_open;
+	dev->stop = net_close;
+	dev->tx_timeout = net_timeout;
+	dev->watchdog_timeo = HZ;
+	dev->hard_start_xmit = net_send_packet;
+	dev->get_stats = net_get_stats;
+	dev->set_multicast_list = set_multicast_list;
+	dev->set_mac_address = set_mac_address;
+
+	printk("\n");
+	if (net_debug)
+		printk("cs89x0_probe1() successful\n");
+
+	retval = register_netdev(dev);
+	if (retval)
+		goto out3;
+#if 1				// kihbingo
+	netif_carrier_off(dev);
+	netif_stop_queue(dev);
+#endif
+	return 0;
+      out3:
+	outw(PP_ChipID, dev->base_addr + ADD_PORT);
+      out2:
+#if 0				// kihbingo
+	release_region(ioaddr & ~3, NETCARD_IO_EXTENT);
+      out1:
+#endif
+	return retval;
+}
+
+
+/*********************************
+ * This page contains DMA routines
+**********************************/
+
+#if ALLOW_DMA
+
+#define dma_page_eq(ptr1, ptr2) ((long)(ptr1)>>17 == (long)(ptr2)>>17)
+
+static void get_dma_channel(struct net_device *dev)
+{
+	struct net_local *lp = netdev_priv(dev);
+
+	if (lp->dma) {
+		dev->dma = lp->dma;
+		lp->isa_config |= ISA_RxDMA;
+	} else {
+		if ((lp->isa_config & ANY_ISA_DMA) == 0)
+			return;
+		dev->dma = lp->isa_config & DMA_NO_MASK;
+		if (lp->chip_type == CS8900)
+			dev->dma += 5;
+		if (dev->dma < 5 || dev->dma > 7) {
+			lp->isa_config &= ~ANY_ISA_DMA;
+			return;
+		}
+	}
+	return;
+}
+
+static void write_dma(struct net_device *dev, int chip_type, int dma)
+{
+	struct net_local *lp = netdev_priv(dev);
+	if ((lp->isa_config & ANY_ISA_DMA) == 0)
+		return;
+	if (chip_type == CS8900) {
+		writereg(dev, PP_CS8900_ISADMA, dma - 5);
+	} else {
+		writereg(dev, PP_CS8920_ISADMA, dma);
+	}
+}
+
+static void set_dma_cfg(struct net_device *dev)
+{
+	struct net_local *lp = netdev_priv(dev);
+
+	if (lp->use_dma) {
+		if ((lp->isa_config & ANY_ISA_DMA) == 0) {
+			if (net_debug > 3)
+				printk("set_dma_cfg(): no DMA\n");
+			return;
+		}
+		if (lp->isa_config & ISA_RxDMA) {
+			lp->curr_rx_cfg |= RX_DMA_ONLY;
+			if (net_debug > 3)
+				printk("set_dma_cfg(): RX_DMA_ONLY\n");
+		} else {
+			lp->curr_rx_cfg |= AUTO_RX_DMA;	/* not that we support
+							   it... */
+			if (net_debug > 3)
+				printk("set_dma_cfg(): AUTO_RX_DMA\n");
+		}
+	}
+}
+
+static int dma_bufcfg(struct net_device *dev)
+{
+	struct net_local *lp = netdev_priv(dev);
+	if (lp->use_dma)
+		return (lp->isa_config & ANY_ISA_DMA) ? RX_DMA_ENBL : 0;
+	else
+		return 0;
+}
+
+static int dma_busctl(struct net_device *dev)
+{
+	int retval = 0;
+	struct net_local *lp = netdev_priv(dev);
+	if (lp->use_dma) {
+		if (lp->isa_config & ANY_ISA_DMA)
+			retval |= RESET_RX_DMA;	/* Reset the DMA pointer */
+		if (lp->isa_config & DMA_BURST)
+			retval |= DMA_BURST_MODE;	/* Does ISA config
+							   specify DMA burst ? */
+		if (lp->dmasize == 64)
+			retval |= RX_DMA_SIZE_64K;	/* did they ask for
+							   64K? */
+		retval |= MEMORY_ON;	/* we need memory enabled to use DMA. */
+	}
+	return retval;
+}
+
+static void dma_rx(struct net_device *dev)
+{
+	struct net_local *lp = netdev_priv(dev);
+	struct sk_buff *skb;
+	int status, length;
+	unsigned char *bp = lp->rx_dma_ptr;
+
+	status = bp[0] + (bp[1] << 8);
+	length = bp[2] + (bp[3] << 8);
+	bp += 4;
+	if (net_debug > 5) {
+		printk("%s: receiving DMA packet at %lx, status %x, length %x\n", dev->name, (unsigned long) bp, status, length);
+	}
+	if ((status & RX_OK) == 0) {
+		count_rx_errors(status, lp);
+		goto skip_this_frame;
+	}
+
+	/* Malloc up new buffer. */
+	skb = dev_alloc_skb(length + 2);
+	if (skb == NULL) {
+		if (net_debug)	/* I don't think we want to do this to a
+				   stressed system */
+			printk("%s: Memory squeeze, dropping packet.\n",
+			       dev->name);
+		lp->stats.rx_dropped++;
+
+		/* AKPM: advance bp to the next frame */
+	      skip_this_frame:
+		bp += (length + 3) & ~3;
+		if (bp >= lp->end_dma_buff)
+			bp -= lp->dmasize * 1024;
+		lp->rx_dma_ptr = bp;
+		return;
+	}
+	skb_reserve(skb, 2);	/* longword align L3 header */
+	skb->dev = dev;
+
+	if (bp + length > lp->end_dma_buff) {
+		int semi_cnt = lp->end_dma_buff - bp;
+		memcpy(skb_put(skb, semi_cnt), bp, semi_cnt);
+		memcpy(skb_put(skb, length - semi_cnt), lp->dma_buff,
+		       length - semi_cnt);
+	} else {
+		memcpy(skb_put(skb, length), bp, length);
+	}
+	bp += (length + 3) & ~3;
+	if (bp >= lp->end_dma_buff)
+		bp -= lp->dmasize * 1024;
+	lp->rx_dma_ptr = bp;
+
+	if (net_debug > 3) {
+		printk("%s: received %d byte DMA packet of type %x\n",
+		       dev->name, length,
+		       (skb->data[ETH_ALEN + ETH_ALEN] << 8) | skb->
+		       data[ETH_ALEN + ETH_ALEN + 1]);
+	}
+	skb->protocol = eth_type_trans(skb, dev);
+	netif_rx(skb);
+	dev->last_rx = jiffies;
+	lp->stats.rx_packets++;
+	lp->stats.rx_bytes += length;
+}
+
+#endif				/* ALLOW_DMA */
+
+void __init reset_chip(struct net_device *dev)
+{
+#ifndef CONFIG_ARCH_IXDP2X01
+	struct net_local *lp = netdev_priv(dev);
+	int ioaddr = dev->base_addr;
+#endif
+	int reset_start_time;
+
+	writereg(dev, PP_SelfCTL, readreg(dev, PP_SelfCTL) | POWER_ON_RESET);
+
+	/* wait 30 ms */
+	msleep(30);
+
+#ifndef CONFIG_ARCH_IXDP2X01
+	if (lp->chip_type != CS8900) {
+		/* Hardware problem requires PNP registers to be reconfigured
+		   after a reset */
+		outw(PP_CS8920_ISAINT, ioaddr + ADD_PORT);
+		outb(dev->irq, ioaddr + DATA_PORT);
+		outb(0, ioaddr + DATA_PORT + 1);
+
+		outw(PP_CS8920_ISAMemB, ioaddr + ADD_PORT);
+		outb((dev->mem_start >> 16) & 0xff, ioaddr + DATA_PORT);
+		outb((dev->mem_start >> 8) & 0xff, ioaddr + DATA_PORT + 1);
+	}
+#endif				/* IXDP2x01 */
+
+	/* Wait until the chip is reset */
+	reset_start_time = jiffies;
+	while ((readreg(dev, PP_SelfST) & INIT_DONE) == 0 &&
+	       jiffies - reset_start_time < 2);
+}
+
+
+static void control_dc_dc(struct net_device *dev, int on_not_off)
+{
+	struct net_local *lp = netdev_priv(dev);
+	unsigned int selfcontrol;
+	int timenow = jiffies;
+	/* control the DC to DC convertor in the SelfControl register. Note:
+	   This is hooked up to a general purpose pin, might not always be a DC
+	   to DC convertor. */
+
+	selfcontrol = HCB1_ENBL;	/* Enable the HCB1 bit as an output */
+	if (((lp->adapter_cnf & A_CNF_DC_DC_POLARITY) != 0) ^ on_not_off)
+		selfcontrol |= HCB1;
+	else
+		selfcontrol &= ~HCB1;
+	writereg(dev, PP_SelfCTL, selfcontrol);
+
+	/* Wait for the DC/DC converter to power up - 500ms */
+	while (jiffies - timenow < HZ);
+}
+
+#define DETECTED_NONE  0
+#define DETECTED_RJ45H 1
+#define DETECTED_RJ45F 2
+#define DETECTED_AUI   3
+#define DETECTED_BNC   4
+
+static int detect_tp(struct net_device *dev)
+{
+	struct net_local *lp = netdev_priv(dev);
+	int timenow = jiffies;
+	int fdx;
+
+	if (net_debug > 1)
+		printk("%s: Attempting TP\n", dev->name);
+
+	/* If connected to another full duplex capable 10-Base-T card the link
+	   pulses seem to be lost when the auto detect bit in the LineCTL is
+	   set. To overcome this the auto detect bit will be cleared whilst
+	   testing the 10-Base-T interface.  This would not be necessary for
+	   the sparrow chip but is simpler to do it anyway. */
+	writereg(dev, PP_LineCTL, lp->linectl & ~AUI_ONLY);
+	control_dc_dc(dev, 0);
+
+	/* Delay for the hardware to work out if the TP cable is present -
+	   150ms */
+	for (timenow = jiffies; jiffies - timenow < 15;);
+	if ((readreg(dev, PP_LineST) & LINK_OK) == 0)
+		return DETECTED_NONE;
+
+	if (lp->chip_type == CS8900) {
+		switch (lp->force & 0xf0) {
+#if 0
+		case FORCE_AUTO:
+			printk("%s: cs8900 doesn't autonegotiate\n", dev->name);
+			return DETECTED_NONE;
+#endif
+			/* CS8900 doesn't support AUTO, change to HALF */
+		case FORCE_AUTO:
+			lp->force &= ~FORCE_AUTO;
+			lp->force |= FORCE_HALF;
+			break;
+		case FORCE_HALF:
+			break;
+		case FORCE_FULL:
+			writereg(dev, PP_TestCTL,
+				 readreg(dev, PP_TestCTL) | FDX_8900);
+			break;
+		}
+		fdx = readreg(dev, PP_TestCTL) & FDX_8900;
+	} else {
+		switch (lp->force & 0xf0) {
+		case FORCE_AUTO:
+			lp->auto_neg_cnf = AUTO_NEG_ENABLE;
+			break;
+		case FORCE_HALF:
+			lp->auto_neg_cnf = 0;
+			break;
+		case FORCE_FULL:
+			lp->auto_neg_cnf = RE_NEG_NOW | ALLOW_FDX;
+			break;
+		}
+
+		writereg(dev, PP_AutoNegCTL, lp->auto_neg_cnf & AUTO_NEG_MASK);
+
+		if ((lp->auto_neg_cnf & AUTO_NEG_BITS) == AUTO_NEG_ENABLE) {
+			printk(KERN_INFO "%s: negotiating duplex...\n",
+			       dev->name);
+			while (readreg(dev, PP_AutoNegST) & AUTO_NEG_BUSY) {
+				if (jiffies - timenow > 4000) {
+					printk(KERN_ERR
+					       "**** Full / half duplex auto-negotiation timed out ****\n");
+					break;
+				}
+			}
+		}
+		fdx = readreg(dev, PP_AutoNegST) & FDX_ACTIVE;
+	}
+	if (fdx)
+		return DETECTED_RJ45F;
+	else
+		return DETECTED_RJ45H;
+}
+
+/* send a test packet - return true if carrier bits are ok */
+static int send_test_pkt(struct net_device *dev)
+{
+	char test_packet[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+		0, 46,		/* A 46 in network order */
+		0, 0,		/* DSAP=0 & SSAP=0 fields */
+		0xf3, 0 /* Control (Test Req + P bit set) */
+	};
+	long timenow = jiffies;
+
+	writereg(dev, PP_LineCTL, readreg(dev, PP_LineCTL) | SERIAL_TX_ON);
+
+	memcpy(test_packet, dev->dev_addr, ETH_ALEN);
+	memcpy(test_packet + ETH_ALEN, dev->dev_addr, ETH_ALEN);
+
+	writeword(dev, TX_CMD_PORT, TX_AFTER_ALL);
+	writeword(dev, TX_LEN_PORT, ETH_ZLEN);
+
+	/* Test to see if the chip has allocated memory for the packet */
+	while (jiffies - timenow < 5)
+		if (readreg(dev, PP_BusST) & READY_FOR_TX_NOW)
+			break;
+	if (jiffies - timenow >= 5)
+		return 0;	/* this shouldn't happen */
+
+	/* Write the contents of the packet */
+#if 0				// kihbingo
+	outsw(dev->base_addr + TX_FRAME_PORT, test_packet, (ETH_ZLEN + 1) >> 1);
+#else
+	writestring(dev, TX_FRAME_PORT, test_packet, (ETH_ZLEN + 1) >> 1);
+#endif
+
+	if (net_debug > 1)
+		printk("Sending test packet ");
+	/* wait a couple of jiffies for packet to be received */
+	for (timenow = jiffies; jiffies - timenow < 3;);
+	if ((readreg(dev, PP_TxEvent) & TX_SEND_OK_BITS) == TX_OK) {
+		if (net_debug > 1)
+			printk("succeeded\n");
+		return 1;
+	}
+	if (net_debug > 1)
+		printk("failed\n");
+	return 0;
+}
+
+
+static int detect_aui(struct net_device *dev)
+{
+	struct net_local *lp = netdev_priv(dev);
+
+	if (net_debug > 1)
+		printk("%s: Attempting AUI\n", dev->name);
+	control_dc_dc(dev, 0);
+
+	writereg(dev, PP_LineCTL, (lp->linectl & ~AUTO_AUI_10BASET) | AUI_ONLY);
+
+	if (send_test_pkt(dev))
+		return DETECTED_AUI;
+	else
+		return DETECTED_NONE;
+}
+
+static int detect_bnc(struct net_device *dev)
+{
+	struct net_local *lp = netdev_priv(dev);
+
+	if (net_debug > 1)
+		printk("%s: Attempting BNC\n", dev->name);
+	control_dc_dc(dev, 1);
+
+	writereg(dev, PP_LineCTL, (lp->linectl & ~AUTO_AUI_10BASET) | AUI_ONLY);
+
+	if (send_test_pkt(dev))
+		return DETECTED_BNC;
+	else
+		return DETECTED_NONE;
+}
+
+#if 0				// kihbingo
+
+static void write_irq(struct net_device *dev, int chip_type, int irq)
+{
+	int i;
+
+	if (chip_type == CS8900) {
+		/* Search the mapping table for the corresponding IRQ pin. */
+		for (i = 0;
+		     i != sizeof(cs8900_irq_map) / sizeof(cs8900_irq_map[0]);
+		     i++)
+			if (cs8900_irq_map[i] == irq)
+				break;
+		/* Not found */
+		if (i == sizeof(cs8900_irq_map) / sizeof(cs8900_irq_map[0]))
+			i = 3;
+		writereg(dev, PP_CS8900_ISAINT, i);
+	} else {
+		writereg(dev, PP_CS8920_ISAINT, irq);
+	}
+}
+#endif
+#if 1				// kihbingo
+static struct timer_list cs8900_poll_timer;
+
+static void cs8900_do_timer(unsigned long arg)
+{
+	struct net_device *dev = (struct net_device *) arg;
+	int link;
+
+	// printk("%s(): %d, %x\n", __FUNCTION__,
+	// (int)jiffies, readreg(dev, PP_LineST));
+	link = readreg(dev, PP_LineST) & LINK_OK;
+	// printk("%s(): %d\n", __FUNCTION__, (int)netif_carrier_ok(dev));
+	if (link) {
+		if (!netif_carrier_ok(dev)) {
+			printk("%s: Link is Up\n", dev->name);
+			netif_carrier_on(dev);
+			netif_wake_queue(dev);
+		}
+	} else {
+		if (netif_carrier_ok(dev)) {
+			printk("%s: Link is Down\n", dev->name);
+			netif_carrier_off(dev);
+			netif_stop_queue(dev);
+		}
+	}
+	/* Repeat every two seconds */
+	mod_timer(&cs8900_poll_timer, jiffies + HZ * 2);
+}
+#endif
+
+/* Open/initialize the board.  This is called (in the current kernel) sometime
+   after booting when the 'ifconfig' program is run.
+
+   This routine should set everything up anew at each open, even registers that
+   "should" only need to be set once at boot, so that there is non-reboot way
+   to recover if something goes wrong. */
+
+/* AKPM: do we need to do any locking here? */
+
+static int net_open(struct net_device *dev)
+{
+	struct net_local *lp = netdev_priv(dev);
+	int result = 0;
+	int i;
+	int ret;
+
+#if 0				// kihbingo
+#if !defined(CONFIG_SH_HICOSH4) && !defined(CONFIG_ARCH_PNX0105)	/* uses
+									   irq#1,
+									   so
+									   this
+									   won't
+									   work
+									 */
+	if (dev->irq < 2) {
+		/* Allow interrupts to be generated by the chip */
+		/* Cirrus' release had this: */
+#if 0
+		writereg(dev, PP_BusCTL, readreg(dev, PP_BusCTL) | ENABLE_IRQ);
+#endif
+		/* And 2.3.47 had this: */
+		writereg(dev, PP_BusCTL, ENABLE_IRQ | MEMORY_ON);
+
+		for (i = 2; i < CS8920_NO_INTS; i++) {
+			if ((1 << i) & lp->irq_map) {
+				if (request_irq
+				    (i, net_interrupt, 0, dev->name,
+				     dev) == 0) {
+					dev->irq = i;
+					write_irq(dev, lp->chip_type, i);
+					/* writereg(dev, PP_BufCFG,
+					   GENERATE_SW_INTERRUPT); */
+					break;
+				}
+			}
+		}
+
+		if (i >= CS8920_NO_INTS) {
+			writereg(dev, PP_BusCTL, 0);	/* disable interrupts. */
+			printk(KERN_ERR "cs89x0: can't get an interrupt\n");
+			ret = -EAGAIN;
+			goto bad_out;
+		}
+	} else
+#endif
+	{
+#if !defined(CONFIG_ARCH_IXDP2X01) && !defined(CONFIG_ARCH_PNX0105)
+		if (((1 << dev->irq) & lp->irq_map) == 0) {
+			printk(KERN_ERR
+			       "%s: IRQ %d is not in our map of allowable IRQs, which is %x\n",
+			       dev->name, dev->irq, lp->irq_map);
+			ret = -EAGAIN;
+			goto bad_out;
+		}
+#endif
+		/* FIXME: Cirrus' release had this: */
+		writereg(dev, PP_BusCTL, readreg(dev, PP_BusCTL) | ENABLE_IRQ);
+		/* And 2.3.47 had this: */
+#if 0
+		writereg(dev, PP_BusCTL, ENABLE_IRQ | MEMORY_ON);
+#endif
+		write_irq(dev, lp->chip_type, dev->irq);
+		ret = request_irq(dev->irq, &net_interrupt, 0, dev->name, dev);
+		if (ret) {
+			if (net_debug)
+				printk(KERN_DEBUG
+				       "cs89x0: request_irq(%d) failed\n",
+				       dev->irq);
+			goto bad_out;
+		}
+	}
+#else				// kihbingo
+	writereg(dev, PP_CS8900_ISAINT, 0);
+	ret = request_irq(dev->irq, &net_interrupt, 0, dev->name, dev);
+	if (ret) {
+		if (net_debug)
+			printk(KERN_DEBUG "cs89x0: request_irq(%d) failed\n",
+			       dev->irq);
+		goto bad_out;
+	}
+	extern void octeon_dbg_set_interrupt(void);
+	octeon_dbg_set_interrupt();
+#endif
+
+#if ALLOW_DMA
+	if (lp->use_dma) {
+		if (lp->isa_config & ANY_ISA_DMA) {
+			unsigned long flags;
+			lp->dma_buff =
+				(unsigned char *) __get_dma_pages(GFP_KERNEL,
+								  get_order(lp->
+									    dmasize
+									    *
+									    1024));
+
+			if (!lp->dma_buff) {
+				printk(KERN_ERR
+				       "%s: cannot get %dK memory for DMA\n",
+				       dev->name, lp->dmasize);
+				goto release_irq;
+			}
+			if (net_debug > 1) {
+				printk("%s: dma %lx %lx\n",
+				       dev->name,
+				       (unsigned long) lp->dma_buff,
+				       (unsigned long) isa_virt_to_bus(lp->
+								       dma_buff));
+			}
+			if ((unsigned long) lp->dma_buff >= MAX_DMA_ADDRESS ||
+			    !dma_page_eq(lp->dma_buff,
+					 lp->dma_buff + lp->dmasize * 1024 -
+					 1)) {
+				printk(KERN_ERR
+				       "%s: not usable as DMA buffer\n",
+				       dev->name);
+				goto release_irq;
+			}
+			memset(lp->dma_buff, 0, lp->dmasize * 1024);	/* Why?
+									 */
+			if (request_dma(dev->dma, dev->name)) {
+				printk(KERN_ERR
+				       "%s: cannot get dma channel %d\n",
+				       dev->name, dev->dma);
+				goto release_irq;
+			}
+			write_dma(dev, lp->chip_type, dev->dma);
+			lp->rx_dma_ptr = lp->dma_buff;
+			lp->end_dma_buff = lp->dma_buff + lp->dmasize * 1024;
+			spin_lock_irqsave(&lp->lock, flags);
+			disable_dma(dev->dma);
+			clear_dma_ff(dev->dma);
+			set_dma_mode(dev->dma, 0x14);	/* auto_init as well */
+			set_dma_addr(dev->dma, isa_virt_to_bus(lp->dma_buff));
+			set_dma_count(dev->dma, lp->dmasize * 1024);
+			enable_dma(dev->dma);
+			spin_unlock_irqrestore(&lp->lock, flags);
+		}
+	}
+#endif				/* ALLOW_DMA */
+
+	/* set the Ethernet address */
+	for (i = 0; i < ETH_ALEN / 2; i++)
+		writereg(dev, PP_IA + i * 2,
+			 dev->dev_addr[i *
+				       2] | (dev->dev_addr[i * 2 + 1] << 8));
+
+	/* while we're testing the interface, leave interrupts disabled */
+	writereg(dev, PP_BusCTL, MEMORY_ON);
+
+	/* Set the LineCTL quintuplet based on adapter configuration read from
+	   EEPROM */
+	if ((lp->adapter_cnf & A_CNF_EXTND_10B_2) &&
+	    (lp->adapter_cnf & A_CNF_LOW_RX_SQUELCH))
+		lp->linectl = LOW_RX_SQUELCH;
+	else
+		lp->linectl = 0;
+
+	/* check to make sure that they have the "right" hardware available */
+	switch (lp->adapter_cnf & A_CNF_MEDIA_TYPE) {
+	case A_CNF_MEDIA_10B_T:
+		result = lp->adapter_cnf & A_CNF_10B_T;
+		break;
+	case A_CNF_MEDIA_AUI:
+		result = lp->adapter_cnf & A_CNF_AUI;
+		break;
+	case A_CNF_MEDIA_10B_2:
+		result = lp->adapter_cnf & A_CNF_10B_2;
+		break;
+	default:
+		result = lp->
+			adapter_cnf & (A_CNF_10B_T | A_CNF_AUI | A_CNF_10B_2);
+	}
+#ifdef CONFIG_ARCH_PNX0105
+	result = A_CNF_10B_T;
+#endif
+	if (!result) {
+		printk(KERN_ERR
+		       "%s: EEPROM is configured for unavailable media\n",
+		       dev->name);
+	      release_irq:
+#if ALLOW_DMA
+		release_dma_buff(lp);
+#endif
+		writereg(dev, PP_LineCTL,
+			 readreg(dev,
+				 PP_LineCTL) & ~(SERIAL_TX_ON | SERIAL_RX_ON));
+		free_irq(dev->irq, dev);
+		ret = -EAGAIN;
+		goto bad_out;
+	}
+
+	/* set the hardware to the configured choice */
+	switch (lp->adapter_cnf & A_CNF_MEDIA_TYPE) {
+	case A_CNF_MEDIA_10B_T:
+		result = detect_tp(dev);
+		if (result == DETECTED_NONE) {
+			printk(KERN_WARNING
+			       "%s: 10Base-T (RJ-45) has no cable\n",
+			       dev->name);
+			if (lp->auto_neg_cnf & IMM_BIT)	/* check "ignore
+							   missing media" bit */
+				result = DETECTED_RJ45H;	/* Yes! I don't
+								   care if I
+								   see a link
+								   pulse */
+		}
+		break;
+	case A_CNF_MEDIA_AUI:
+		result = detect_aui(dev);
+		if (result == DETECTED_NONE) {
+			printk(KERN_WARNING "%s: 10Base-5 (AUI) has no cable\n",
+			       dev->name);
+			if (lp->auto_neg_cnf & IMM_BIT)	/* check "ignore
+							   missing media" bit */
+				result = DETECTED_AUI;	/* Yes! I don't care if
+							   I see a carrrier */
+		}
+		break;
+	case A_CNF_MEDIA_10B_2:
+		result = detect_bnc(dev);
+		if (result == DETECTED_NONE) {
+			printk(KERN_WARNING "%s: 10Base-2 (BNC) has no cable\n",
+			       dev->name);
+			if (lp->auto_neg_cnf & IMM_BIT)	/* check "ignore
+							   missing media" bit */
+				result = DETECTED_BNC;	/* Yes! I don't care if
+							   I can xmit a packet */
+		}
+		break;
+	case A_CNF_MEDIA_AUTO:
+		writereg(dev, PP_LineCTL, lp->linectl | AUTO_AUI_10BASET);
+		if (lp->adapter_cnf & A_CNF_10B_T)
+			if ((result = detect_tp(dev)) != DETECTED_NONE)
+				break;
+		if (lp->adapter_cnf & A_CNF_AUI)
+			if ((result = detect_aui(dev)) != DETECTED_NONE)
+				break;
+		if (lp->adapter_cnf & A_CNF_10B_2)
+			if ((result = detect_bnc(dev)) != DETECTED_NONE)
+				break;
+		printk(KERN_ERR "%s: no media detected\n", dev->name);
+		goto release_irq;
+	}
+	switch (result) {
+	case DETECTED_NONE:
+		printk(KERN_ERR
+		       "%s: no network cable attached to configured media\n",
+		       dev->name);
+		goto release_irq;
+	case DETECTED_RJ45H:
+		printk(KERN_INFO "%s: using half-duplex 10Base-T (RJ-45)\n",
+		       dev->name);
+		break;
+	case DETECTED_RJ45F:
+		printk(KERN_INFO "%s: using full-duplex 10Base-T (RJ-45)\n",
+		       dev->name);
+		break;
+	case DETECTED_AUI:
+		printk(KERN_INFO "%s: using 10Base-5 (AUI)\n", dev->name);
+		break;
+	case DETECTED_BNC:
+		printk(KERN_INFO "%s: using 10Base-2 (BNC)\n", dev->name);
+		break;
+	}
+
+	/* Turn on both receive and transmit operations */
+	writereg(dev, PP_LineCTL,
+		 readreg(dev, PP_LineCTL) | SERIAL_RX_ON | SERIAL_TX_ON);
+
+	/* Receive only error free packets addressed to this card */
+	lp->rx_mode = 0;
+	writereg(dev, PP_RxCTL, DEF_RX_ACCEPT);
+
+	lp->curr_rx_cfg = RX_OK_ENBL | RX_CRC_ERROR_ENBL;
+
+	if (lp->isa_config & STREAM_TRANSFER)
+		lp->curr_rx_cfg |= RX_STREAM_ENBL;
+#if ALLOW_DMA
+	set_dma_cfg(dev);
+#endif
+	writereg(dev, PP_RxCFG, lp->curr_rx_cfg);
+
+	writereg(dev, PP_TxCFG,
+		 TX_LOST_CRS_ENBL | TX_SQE_ERROR_ENBL | TX_OK_ENBL |
+		 TX_LATE_COL_ENBL | TX_JBR_ENBL | TX_ANY_COL_ENBL |
+		 TX_16_COL_ENBL);
+
+	writereg(dev, PP_BufCFG,
+		 READY_FOR_TX_ENBL | RX_MISS_COUNT_OVRFLOW_ENBL |
+#if ALLOW_DMA
+		 dma_bufcfg(dev) |
+#endif
+		 TX_COL_COUNT_OVRFLOW_ENBL | TX_UNDERRUN_ENBL);
+
+	/* now that we've got our act together, enable everything */
+	writereg(dev, PP_BusCTL, ENABLE_IRQ | (dev->mem_start ? MEMORY_ON : 0)	/* turn
+										   memory
+										   on
+										 */
+#if ALLOW_DMA
+		 | dma_busctl(dev)
+#endif
+		);
+#if 0				// kihbingo
+	netif_start_queue(dev);
+#endif
+	if (net_debug > 1)
+		printk("cs89x0: net_open() succeeded\n");
+#if 1				// kihbingo
+	/* Enable the poll timer for checking link status */
+	init_timer(&cs8900_poll_timer);
+	cs8900_poll_timer.data = (unsigned long) dev;
+	cs8900_poll_timer.function = cs8900_do_timer;
+	mod_timer(&cs8900_poll_timer, jiffies + HZ * 2);
+#endif
+	return 0;
+      bad_out:
+	return ret;
+}
+
+static void net_timeout(struct net_device *dev)
+{
+	/* If we get here, some higher level has decided we are broken. There
+	   should really be a "kick me" function call instead. */
+	if (net_debug > 0)
+		printk("%s: transmit timed out, %s?\n", dev->name,
+		       tx_done(dev) ? "IRQ conflict ?" :
+		       "network cable problem");
+	/* Try to restart the adaptor. */
+	netif_wake_queue(dev);
+}
+
+static int net_send_packet(struct sk_buff *skb, struct net_device *dev)
+{
+	struct net_local *lp = netdev_priv(dev);
+
+	if (net_debug > 3) {
+		printk("%s: sent %d byte packet of type %x\n",
+		       dev->name, skb->len,
+		       (skb->data[ETH_ALEN + ETH_ALEN] << 8) | skb->
+		       data[ETH_ALEN + ETH_ALEN + 1]);
+	}
+
+	/* keep the upload from being interrupted, since we ask the chip to
+	   start transmitting before the whole packet has been completely
+	   uploaded. */
+
+	spin_lock_irq(&lp->lock);
+	netif_stop_queue(dev);
+
+	/* initiate a transmit sequence */
+	writeword(dev, TX_CMD_PORT, lp->send_cmd);
+	writeword(dev, TX_LEN_PORT, skb->len);
+
+	/* Test to see if the chip has allocated memory for the packet */
+	if ((readreg(dev, PP_BusST) & READY_FOR_TX_NOW) == 0) {
+		/*
+		 * Gasp!  It hasn't.  But that shouldn't happen since
+		 * we're waiting for TxOk, so return 1 and requeue this packet.
+		 */
+
+		spin_unlock_irq(&lp->lock);
+		if (net_debug)
+			printk("cs89x0: Tx buffer not free!\n");
+		return 1;
+	}
+	/* Write the contents of the packet */
+#if 0				// kihbingo
+	outsw(dev->base_addr + TX_FRAME_PORT, skb->data, (skb->len + 1) >> 1);
+#else
+	writestring(dev, TX_FRAME_PORT, skb->data, (skb->len + 1) >> 1);
+#endif
+	spin_unlock_irq(&lp->lock);
+	lp->stats.tx_bytes += skb->len;
+	dev->trans_start = jiffies;
+	dev_kfree_skb(skb);
+
+	/*
+	 * We DO NOT call netif_wake_queue() here.
+	 * We also DO NOT call netif_start_queue().
+	 *
+	 * Either of these would cause another bottom half run through
+	 * net_send_packet() before this packet has fully gone out.  That causes
+	 * us to hit the "Gasp!" above and the send is rescheduled.  it runs like
+	 * a dog.  We just return and wait for the Tx completion interrupt handler
+	 * to restart the netdevice layer
+	 */
+
+	return 0;
+}
+
+/* The typical workload of the driver: Handle the network interface interrupts.
+ */
+
+static irqreturn_t net_interrupt(int irq, void *dev_id, struct pt_regs *regs)
+{
+	struct net_device *dev = dev_id;
+	struct net_local *lp;
+	int ioaddr, status;
+	int handled = 0;
+
+	ioaddr = dev->base_addr;
+	lp = netdev_priv(dev);
+
+#if 0				// kihbingo
+	printk("%s(): %d\n", __FUNCTION__, irq);
+#endif
+	/* we MUST read all the events out of the ISQ, otherwise we'll never
+	   get interrupted again.  As a consequence, we can't have any limit on
+	   the number of times we loop in the interrupt handler.  The hardware
+	   guarantees that eventually we'll run out of events.  Of course, if
+	   you're on a slow machine, and packets are arriving faster than you
+	   can read them off, you're screwed.  Hasta la vista, baby! */
+	while ((status = readword(dev, ISQ_PORT))) {
+		if (net_debug > 4)
+			printk("%s: event=%04x\n", dev->name, status);
+		handled = 1;
+		switch (status & ISQ_EVENT_MASK) {
+		case ISQ_RECEIVER_EVENT:
+			/* Got a packet(s). */
+			net_rx(dev);
+			break;
+		case ISQ_TRANSMITTER_EVENT:
+			lp->stats.tx_packets++;
+			netif_wake_queue(dev);	/* Inform upper layers. */
+			if ((status & (TX_OK |
+				       TX_LOST_CRS |
+				       TX_SQE_ERROR |
+				       TX_LATE_COL | TX_16_COL)) != TX_OK) {
+				if ((status & TX_OK) == 0)
+					lp->stats.tx_errors++;
+				if (status & TX_LOST_CRS)
+					lp->stats.tx_carrier_errors++;
+				if (status & TX_SQE_ERROR)
+					lp->stats.tx_heartbeat_errors++;
+				if (status & TX_LATE_COL)
+					lp->stats.tx_window_errors++;
+				if (status & TX_16_COL)
+					lp->stats.tx_aborted_errors++;
+			}
+			break;
+		case ISQ_BUFFER_EVENT:
+			if (status & READY_FOR_TX) {
+				/* we tried to transmit a packet earlier, but
+				   inexplicably ran out of buffers. That
+				   shouldn't happen since we only ever load one
+				   packet.  Shrug.  Do the right thing anyway. */
+				netif_wake_queue(dev);	/* Inform upper layers.
+							 */
+			}
+			if (status & TX_UNDERRUN) {
+				if (net_debug > 0)
+					printk("%s: transmit underrun\n",
+					       dev->name);
+				lp->send_underrun++;
+				if (lp->send_underrun == 3)
+					lp->send_cmd = TX_AFTER_381;
+				else if (lp->send_underrun == 6)
+					lp->send_cmd = TX_AFTER_ALL;
+				/* transmit cycle is done, although frame
+				   wasn't transmitted - this avoids having to
+				   wait for the upper layers to timeout on us,
+				   in the event of a tx underrun */
+				netif_wake_queue(dev);	/* Inform upper layers.
+							 */
+			}
+#if ALLOW_DMA
+			if (lp->use_dma && (status & RX_DMA)) {
+				int count = readreg(dev, PP_DmaFrameCnt);
+				while (count) {
+					if (net_debug > 5)
+						printk("%s: receiving %d DMA frames\n", dev->name, count);
+					if (net_debug > 2 && count > 1)
+						printk("%s: receiving %d DMA frames\n", dev->name, count);
+					dma_rx(dev);
+					if (--count == 0)
+						count = readreg(dev,
+								PP_DmaFrameCnt);
+					if (net_debug > 2 && count > 0)
+						printk("%s: continuing with %d DMA frames\n", dev->name, count);
+				}
+			}
+#endif
+			break;
+		case ISQ_RX_MISS_EVENT:
+			lp->stats.rx_missed_errors += (status >> 6);
+			break;
+		case ISQ_TX_COL_EVENT:
+			lp->stats.collisions += (status >> 6);
+			break;
+		}
+	}
+	return IRQ_RETVAL(handled);
+}
+
+static void count_rx_errors(int status, struct net_local *lp)
+{
+	lp->stats.rx_errors++;
+	if (status & RX_RUNT)
+		lp->stats.rx_length_errors++;
+	if (status & RX_EXTRA_DATA)
+		lp->stats.rx_length_errors++;
+	if (status & RX_CRC_ERROR)
+		if (!(status & (RX_EXTRA_DATA | RX_RUNT)))
+			/* per str 172 */
+			lp->stats.rx_crc_errors++;
+	if (status & RX_DRIBBLE)
+		lp->stats.rx_frame_errors++;
+	return;
+}
+
+/* We have a good packet(s), get it/them out of the buffers. */
+static void net_rx(struct net_device *dev)
+{
+	struct net_local *lp = netdev_priv(dev);
+	struct sk_buff *skb;
+	volatile int status, length;
+
+#if 0				// kihbingo
+	int ioaddr = dev->base_addr;
+	status = inw(ioaddr + RX_FRAME_PORT);
+	length = inw(ioaddr + RX_FRAME_PORT);
+#else
+	status = readword(dev, RX_FRAME_PORT);
+	length = readword(dev, RX_FRAME_PORT);
+#endif
+
+	if ((status & RX_OK) == 0) {
+		count_rx_errors(status, lp);
+		return;
+	}
+
+	/* Malloc up new buffer. */
+	skb = dev_alloc_skb(length + 2);
+	if (skb == NULL) {
+#if 0				/* Again, this seems a cruel thing to do */
+		printk(KERN_WARNING "%s: Memory squeeze, dropping packet.\n",
+		       dev->name);
+#endif
+		lp->stats.rx_dropped++;
+		return;
+	}
+	skb_reserve(skb, 2);	/* longword align L3 header */
+	skb->dev = dev;
+
+#if 0				// kihbingo
+	insw(ioaddr + RX_FRAME_PORT, skb_put(skb, length), length >> 1);
+#else
+	readstring(dev, RX_FRAME_PORT, skb_put(skb, length), length >> 1);
+#endif
+	if (length & 1)
+#if 0				// kihbingo
+		skb->data[length - 1] = inw(ioaddr + RX_FRAME_PORT);
+#else
+		skb->data[length - 1] = readword(dev, RX_FRAME_PORT);
+#endif
+
+	if (net_debug > 3) {
+		printk("%s: received %d byte packet of type %x\n",
+		       dev->name, length,
+		       (skb->data[ETH_ALEN + ETH_ALEN] << 8) | skb->
+		       data[ETH_ALEN + ETH_ALEN + 1]);
+	}
+
+	skb->protocol = eth_type_trans(skb, dev);
+	netif_rx(skb);
+	dev->last_rx = jiffies;
+	lp->stats.rx_packets++;
+	lp->stats.rx_bytes += length;
+}
+
+#if ALLOW_DMA
+static void release_dma_buff(struct net_local *lp)
+{
+	if (lp->dma_buff) {
+		free_pages((unsigned long) (lp->dma_buff),
+			   get_order(lp->dmasize * 1024));
+		lp->dma_buff = NULL;
+	}
+}
+#endif
+
+/* The inverse routine to net_open(). */
+static int net_close(struct net_device *dev)
+{
+#if ALLOW_DMA
+	struct net_local *lp = netdev_priv(dev);
+#endif
+#if 1				// kihbingo
+	del_timer_sync(&cs8900_poll_timer);
+#endif
+
+	netif_stop_queue(dev);
+
+	writereg(dev, PP_RxCFG, 0);
+	writereg(dev, PP_TxCFG, 0);
+	writereg(dev, PP_BufCFG, 0);
+	writereg(dev, PP_BusCTL, 0);
+
+	free_irq(dev->irq, dev);
+
+#if ALLOW_DMA
+	if (lp->use_dma && lp->dma) {
+		free_dma(dev->dma);
+		release_dma_buff(lp);
+	}
+#endif
+
+	/* Update the statistics here. */
+	return 0;
+}
+
+/* Get the current statistics. This may be called with the card open or closed.
+ */
+static struct net_device_stats *net_get_stats(struct net_device *dev)
+{
+	struct net_local *lp = netdev_priv(dev);
+	unsigned long flags;
+
+	spin_lock_irqsave(&lp->lock, flags);
+	/* Update the statistics from the device registers. */
+	lp->stats.rx_missed_errors += (readreg(dev, PP_RxMiss) >> 6);
+	lp->stats.collisions += (readreg(dev, PP_TxCol) >> 6);
+	spin_unlock_irqrestore(&lp->lock, flags);
+
+	return &lp->stats;
+}
+
+static void set_multicast_list(struct net_device *dev)
+{
+	struct net_local *lp = netdev_priv(dev);
+	unsigned long flags;
+
+	spin_lock_irqsave(&lp->lock, flags);
+	if (dev->flags & IFF_PROMISC) {
+		lp->rx_mode = RX_ALL_ACCEPT;
+	} else if ((dev->flags & IFF_ALLMULTI) || dev->mc_list) {
+		/* The multicast-accept list is initialized to accept-all, and
+		   we rely on higher-level filtering for now. */
+		lp->rx_mode = RX_MULTCAST_ACCEPT;
+	} else
+		lp->rx_mode = 0;
+
+	writereg(dev, PP_RxCTL, DEF_RX_ACCEPT | lp->rx_mode);
+
+	/* in promiscuous mode, we accept errored packets, so we have to enable
+	   interrupts on them also */
+	writereg(dev, PP_RxCFG, lp->curr_rx_cfg |
+		 (lp->rx_mode ==
+		  RX_ALL_ACCEPT ? (RX_CRC_ERROR_ENBL | RX_RUNT_ENBL |
+				   RX_EXTRA_DATA_ENBL) : 0));
+	spin_unlock_irqrestore(&lp->lock, flags);
+}
+
+
+static int set_mac_address(struct net_device *dev, void *p)
+{
+	int i;
+	struct sockaddr *addr = p;
+
+
+	if (netif_running(dev))
+		return -EBUSY;
+
+	memcpy(dev->dev_addr, addr->sa_data, dev->addr_len);
+
+	if (net_debug) {
+		printk("%s: Setting MAC address to ", dev->name);
+		for (i = 0; i < dev->addr_len; i++)
+			printk(" %2.2x", dev->dev_addr[i]);
+		printk(".\n");
+	}
+	/* set the Ethernet address */
+	for (i = 0; i < ETH_ALEN / 2; i++)
+		writereg(dev, PP_IA + i * 2,
+			 dev->dev_addr[i *
+				       2] | (dev->dev_addr[i * 2 + 1] << 8));
+
+	return 0;
+}
+
+#if 0				// kihbingo
+#ifdef MODULE
+
+static struct net_device *dev_cs89x0;
+
+/*
+ * Support the 'debug' module parm even if we're compiled for non-debug to
+ * avoid breaking someone's startup scripts
+ */
+
+static int io;
+static int irq;
+static int debug;
+static char media[8];
+static int duplex = -1;
+
+static int use_dma;		/* These generate unused var warnings if
+				   ALLOW_DMA = 0 */
+static int dma;
+static int dmasize = 16;	/* or 64 */
+
+module_param(io, int, 0);
+module_param(irq, int, 0);
+module_param(debug, int, 0);
+module_param_string(media, media, sizeof(media), 0);
+module_param(duplex, int, 0);
+module_param(dma, int, 0);
+module_param(dmasize, int, 0);
+module_param(use_dma, int, 0);
+MODULE_PARM_DESC(io, "cs89x0 I/O base address");
+MODULE_PARM_DESC(irq, "cs89x0 IRQ number");
+#if DEBUGGING
+MODULE_PARM_DESC(debug, "cs89x0 debug level (0-6)");
+#else
+MODULE_PARM_DESC(debug, "(ignored)");
+#endif
+MODULE_PARM_DESC(media, "Set cs89x0 adapter(s) media type(s) (rj45,bnc,aui)");
+/* No other value than -1 for duplex seems to be currently interpreted */
+MODULE_PARM_DESC(duplex, "(ignored)");
+#if ALLOW_DMA
+MODULE_PARM_DESC(dma, "cs89x0 ISA DMA channel; ignored if use_dma=0");
+MODULE_PARM_DESC(dmasize,
+		 "cs89x0 DMA size in kB (16,64); ignored if use_dma=0");
+MODULE_PARM_DESC(use_dma, "cs89x0 using DMA (0-1)");
+#else
+MODULE_PARM_DESC(dma, "(ignored)");
+MODULE_PARM_DESC(dmasize, "(ignored)");
+MODULE_PARM_DESC(use_dma, "(ignored)");
+#endif
+
+MODULE_AUTHOR
+	("Mike Cruse, Russwll Nelson <nelson@crynwr.com>, Andrew Morton <andrewm@uow.edu.au>");
+MODULE_LICENSE("GPL");
+
+
+/*
+ * media=t             - specify media type
+ or media=2
+ or media=aui
+ or medai=auto
+ * duplex=0            - specify forced half/full/autonegotiate duplex
+ * debug=#             - debug level
+
+
+ * Default Chip Configuration:
+ * DMA Burst = enabled
+ * IOCHRDY Enabled = enabled
+ * UseSA = enabled
+ * CS8900 defaults to half-duplex if not specified on command-line
+ * CS8920 defaults to autoneg if not specified on command-line
+ * Use reset defaults for other config parameters
+
+ * Assumptions:
+ * media type specified is supported (circuitry is present)
+ * if memory address is > 1MB, then required mem decode hw is present
+ * if 10B-2, then agent other than driver will enable DC/DC converter
+ (hw or software util)
+
+
+ */
+
+int init_module(void)
+{
+	struct net_device *dev = alloc_etherdev(sizeof(struct net_local));
+	struct net_local *lp;
+	int ret = 0;
+
+#if DEBUGGING
+	net_debug = debug;
+#else
+	debug = 0;
+#endif
+	if (!dev)
+		return -ENOMEM;
+
+	dev->irq = irq;
+	dev->base_addr = io;
+	lp = netdev_priv(dev);
+
+#if ALLOW_DMA
+	if (use_dma) {
+		lp->use_dma = use_dma;
+		lp->dma = dma;
+		lp->dmasize = dmasize;
+	}
+#endif
+
+	spin_lock_init(&lp->lock);
+
+	/* boy, they'd better get these right */
+	if (!strcmp(media, "rj45"))
+		lp->adapter_cnf = A_CNF_MEDIA_10B_T | A_CNF_10B_T;
+	else if (!strcmp(media, "aui"))
+		lp->adapter_cnf = A_CNF_MEDIA_AUI | A_CNF_AUI;
+	else if (!strcmp(media, "bnc"))
+		lp->adapter_cnf = A_CNF_MEDIA_10B_2 | A_CNF_10B_2;
+	else
+		lp->adapter_cnf = A_CNF_MEDIA_10B_T | A_CNF_10B_T;
+
+	if (duplex == -1)
+		lp->auto_neg_cnf = AUTO_NEG_ENABLE;
+
+	if (io == 0) {
+		printk(KERN_ERR "cs89x0.c: Module autoprobing not allowed.\n");
+		printk(KERN_ERR "cs89x0.c: Append io=0xNNN\n");
+		ret = -EPERM;
+		goto out;
+	} else if (io <= 0x1ff) {
+		ret = -ENXIO;
+		goto out;
+	}
+#if ALLOW_DMA
+	if (use_dma && dmasize != 16 && dmasize != 64) {
+		printk(KERN_ERR
+		       "cs89x0.c: dma size must be either 16K or 64K, not %dK\n",
+		       dmasize);
+		ret = -EPERM;
+		goto out;
+	}
+#endif
+	ret = cs89x0_probe1(dev, io, 1);
+	if (ret)
+		goto out;
+
+	dev_cs89x0 = dev;
+	return 0;
+      out:
+	free_netdev(dev);
+	return ret;
+}
+
+void cleanup_module(void)
+{
+	unregister_netdev(dev_cs89x0);
+	outw(PP_ChipID, dev_cs89x0->base_addr + ADD_PORT);
+	release_region(dev_cs89x0->base_addr, NETCARD_IO_EXTENT);
+	free_netdev(dev_cs89x0);
+}
+#endif				/* MODULE */
+#endif
+
+/*
+ * Local variables:
+ *  version-control: t
+ *  kept-new-versions: 5
+ *  c-indent-level: 8
+ *  tab-width: 8
+ * End:
+ *
+ */
+#if 1				// kihbingo
+
+static struct net_device *dev_cs89x0;
+
+static int __init cs89x0_init(void)
+{
+	struct net_device *dev;
+	int ret = 0;
+
+	// printk("cs89x0 init...\n");
+
+	dev = alloc_etherdev(sizeof(struct net_local));
+	sprintf(dev->name, "dbg%d", 0);
+	dev->base_addr = 0xffffffffb8200000ULL;
+	dev->irq = 8 + 31;
+
+	// printk(" %04x, %04x\n", readreg(dev, 0), readreg(dev, 2));
+	ret = cs89x0_probe1(dev, 0, 0);
+	if (ret)
+		goto out;
+
+	dev_cs89x0 = dev;
+	return 0;
+      out:
+	free_netdev(dev);
+	return ret;
+}
+
+static void __init cs89x0_exit(void)
+{
+	printk("cs89x0 exit...\n");
+	unregister_netdev(dev_cs89x0);
+	free_netdev(dev_cs89x0);
+	del_timer_sync(&cs8900_poll_timer);
+}
+
+module_init(cs89x0_init);
+module_exit(cs89x0_exit);
+
+#endif
diff --git a/arch/mips/cavium-octeon/cs89x0.h b/arch/mips/cavium-octeon/cs89x0.h
new file mode 100644
index 0000000..8a38bd7
--- /dev/null
+++ b/arch/mips/cavium-octeon/cs89x0.h
@@ -0,0 +1,486 @@
+/* Copyright, 1988-1992, Russell Nelson, Crynwr Software
+
+   This program is free software; you can redistribute it and/or modify it
+   under the terms of the GNU General Public License as published by the Free
+   Software Foundation, version 1.
+
+   This program is distributed in the hope that it will be useful, but WITHOUT
+   ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+   FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+   more details.
+
+   You should have received a copy of the GNU General Public License along with
+   this program; if not, write to the Free Software Foundation, Inc., 675 Mass
+   Ave, Cambridge, MA 02139, USA. */
+
+#include <linux/config.h>
+
+#if defined(CONFIG_ARCH_IXDP2X01) || defined(CONFIG_ARCH_PNX0105)
+/* IXDP2401/IXDP2801 uses dword-aligned register addressing */
+#define CS89x0_PORT(reg) ((reg) * 2)
+#else
+#define CS89x0_PORT(reg) (reg)
+#endif
+
+#define PP_ChipID 0x0000	/* offset 0h -> Corp -ID */
+				/* offset 2h -> Model/Product Number */
+				/* offset 3h -> Chip Revision Number */
+
+#define PP_ISAIOB 0x0020	/* IO base address */
+#define PP_CS8900_ISAINT 0x0022	/* ISA interrupt select */
+#define PP_CS8920_ISAINT 0x0370	/* ISA interrupt select */
+#define PP_CS8900_ISADMA 0x0024	/* ISA Rec DMA channel */
+#define PP_CS8920_ISADMA 0x0374	/* ISA Rec DMA channel */
+#define PP_ISASOF 0x0026	/* ISA DMA offset */
+#define PP_DmaFrameCnt 0x0028	/* ISA DMA Frame count */
+#define PP_DmaByteCnt 0x002A	/* ISA DMA Byte count */
+#define PP_CS8900_ISAMemB 0x002C	/* Memory base */
+#define PP_CS8920_ISAMemB 0x0348	/* */
+
+#define PP_ISABootBase 0x0030	/* Boot Prom base */
+#define PP_ISABootMask 0x0034	/* Boot Prom Mask */
+
+/* EEPROM data and command registers */
+#define PP_EECMD 0x0040		/* NVR Interface Command register */
+#define PP_EEData 0x0042	/* NVR Interface Data Register */
+#define PP_DebugReg 0x0044	/* Debug Register */
+
+#define PP_RxCFG 0x0102		/* Rx Bus config */
+#define PP_RxCTL 0x0104		/* Receive Control Register */
+#define PP_TxCFG 0x0106		/* Transmit Config Register */
+#define PP_TxCMD 0x0108		/* Transmit Command Register */
+#define PP_BufCFG 0x010A	/* Bus configuration Register */
+#define PP_LineCTL 0x0112	/* Line Config Register */
+#define PP_SelfCTL 0x0114	/* Self Command Register */
+#define PP_BusCTL 0x0116	/* ISA bus control Register */
+#define PP_TestCTL 0x0118	/* Test Register */
+#define PP_AutoNegCTL 0x011C	/* Auto Negotiation Ctrl */
+
+#define PP_ISQ 0x0120		/* Interrupt Status */
+#define PP_RxEvent 0x0124	/* Rx Event Register */
+#define PP_TxEvent 0x0128	/* Tx Event Register */
+#define PP_BufEvent 0x012C	/* Bus Event Register */
+#define PP_RxMiss 0x0130	/* Receive Miss Count */
+#define PP_TxCol 0x0132		/* Transmit Collision Count */
+#define PP_LineST 0x0134	/* Line State Register */
+#define PP_SelfST 0x0136	/* Self State register */
+#define PP_BusST 0x0138		/* Bus Status */
+#define PP_TDR 0x013C		/* Time Domain Reflectometry */
+#define PP_AutoNegST 0x013E	/* Auto Neg Status */
+#define PP_TxCommand 0x0144	/* Tx Command */
+#define PP_TxLength 0x0146	/* Tx Length */
+#define PP_LAF 0x0150		/* Hash Table */
+#define PP_IA 0x0158		/* Physical Address Register */
+
+#define PP_RxStatus 0x0400	/* Receive start of frame */
+#define PP_RxLength 0x0402	/* Receive Length of frame */
+#define PP_RxFrame 0x0404	/* Receive frame pointer */
+#define PP_TxFrame 0x0A00	/* Transmit frame pointer */
+
+/* Primary I/O Base Address. If no I/O base is supplied by the user, then this */
+/* can be used as the default I/O base to access the PacketPage Area. */
+#define DEFAULTIOBASE 0x0300
+#define FIRST_IO 0x020C		/* First I/O port to check */
+#define LAST_IO 0x037C		/* Last I/O port to check (+10h) */
+#define ADD_MASK 0x3000		/* Mask it use of the ADD_PORT register */
+#define ADD_SIG 0x3000		/* Expected ID signature */
+
+/* On Macs, we only need use the ISA I/O stuff until we do MEMORY_ON */
+#ifdef CONFIG_MAC
+#define LCSLOTBASE 0xfee00000
+#define MMIOBASE 0x40000
+#endif
+
+#define CHIP_EISA_ID_SIG 0x630E	/* Product ID Code for Crystal Chip (CS8900
+				   spec 4.3) */
+#define CHIP_EISA_ID_SIG_STR "0x630E"
+
+#ifdef IBMEIPKT
+#define EISA_ID_SIG 0x4D24	/* IBM */
+#define PART_NO_SIG 0x1010	/* IBM */
+#define MONGOOSE_BIT 0x0000	/* IBM */
+#else
+#define EISA_ID_SIG 0x630E	/* PnP Vendor ID (same as chip id for Crystal
+				   board) */
+#define PART_NO_SIG 0x4000	/* ID code CS8920 board (PnP Vendor Product
+				   code) */
+#define MONGOOSE_BIT 0x2000	/* PART_NO_SIG + MONGOOSE_BUT => ID of mongoose
+				 */
+#endif
+
+#define PRODUCT_ID_ADD 0x0002	/* Address of product ID */
+
+/* Mask to find out the types of registers */
+#define REG_TYPE_MASK 0x001F
+
+/* Eeprom Commands */
+#define ERSE_WR_ENBL 0x00F0
+#define ERSE_WR_DISABLE 0x0000
+
+/* Defines Control/Config register quintuplet numbers */
+#define RX_BUF_CFG 0x0003
+#define RX_CONTROL 0x0005
+#define TX_CFG 0x0007
+#define TX_COMMAND 0x0009
+#define BUF_CFG 0x000B
+#define LINE_CONTROL 0x0013
+#define SELF_CONTROL 0x0015
+#define BUS_CONTROL 0x0017
+#define TEST_CONTROL 0x0019
+
+/* Defines Status/Count registers quintuplet numbers */
+#define RX_EVENT 0x0004
+#define TX_EVENT 0x0008
+#define BUF_EVENT 0x000C
+#define RX_MISS_COUNT 0x0010
+#define TX_COL_COUNT 0x0012
+#define LINE_STATUS 0x0014
+#define SELF_STATUS 0x0016
+#define BUS_STATUS 0x0018
+#define TDR 0x001C
+
+/* PP_RxCFG - Receive Configuration and Interrupt Mask bit definition -
+   Read/write */
+#define SKIP_1 0x0040
+#define RX_STREAM_ENBL 0x0080
+#define RX_OK_ENBL 0x0100
+#define RX_DMA_ONLY 0x0200
+#define AUTO_RX_DMA 0x0400
+#define BUFFER_CRC 0x0800
+#define RX_CRC_ERROR_ENBL 0x1000
+#define RX_RUNT_ENBL 0x2000
+#define RX_EXTRA_DATA_ENBL 0x4000
+
+/* PP_RxCTL - Receive Control bit definition - Read/write */
+#define RX_IA_HASH_ACCEPT 0x0040
+#define RX_PROM_ACCEPT 0x0080
+#define RX_OK_ACCEPT 0x0100
+#define RX_MULTCAST_ACCEPT 0x0200
+#define RX_IA_ACCEPT 0x0400
+#define RX_BROADCAST_ACCEPT 0x0800
+#define RX_BAD_CRC_ACCEPT 0x1000
+#define RX_RUNT_ACCEPT 0x2000
+#define RX_EXTRA_DATA_ACCEPT 0x4000
+#define RX_ALL_ACCEPT (RX_PROM_ACCEPT|RX_BAD_CRC_ACCEPT|RX_RUNT_ACCEPT|RX_EXTRA_DATA_ACCEPT)
+/* Default receive mode - individually addressed, broadcast, and error free */
+#define DEF_RX_ACCEPT (RX_IA_ACCEPT | RX_BROADCAST_ACCEPT | RX_OK_ACCEPT)
+
+/* PP_TxCFG - Transmit Configuration Interrupt Mask bit definition - Read/write
+ */
+#define TX_LOST_CRS_ENBL 0x0040
+#define TX_SQE_ERROR_ENBL 0x0080
+#define TX_OK_ENBL 0x0100
+#define TX_LATE_COL_ENBL 0x0200
+#define TX_JBR_ENBL 0x0400
+#define TX_ANY_COL_ENBL 0x0800
+#define TX_16_COL_ENBL 0x8000
+
+/* PP_TxCMD - Transmit Command bit definition - Read-only */
+#define TX_START_4_BYTES 0x0000
+#define TX_START_64_BYTES 0x0040
+#define TX_START_128_BYTES 0x0080
+#define TX_START_ALL_BYTES 0x00C0
+#define TX_FORCE 0x0100
+#define TX_ONE_COL 0x0200
+#define TX_TWO_PART_DEFF_DISABLE 0x0400
+#define TX_NO_CRC 0x1000
+#define TX_RUNT 0x2000
+
+/* PP_BufCFG - Buffer Configuration Interrupt Mask bit definition - Read/write */
+#define GENERATE_SW_INTERRUPT 0x0040
+#define RX_DMA_ENBL 0x0080
+#define READY_FOR_TX_ENBL 0x0100
+#define TX_UNDERRUN_ENBL 0x0200
+#define RX_MISS_ENBL 0x0400
+#define RX_128_BYTE_ENBL 0x0800
+#define TX_COL_COUNT_OVRFLOW_ENBL 0x1000
+#define RX_MISS_COUNT_OVRFLOW_ENBL 0x2000
+#define RX_DEST_MATCH_ENBL 0x8000
+
+/* PP_LineCTL - Line Control bit definition - Read/write */
+#define SERIAL_RX_ON 0x0040
+#define SERIAL_TX_ON 0x0080
+#define AUI_ONLY 0x0100
+#define AUTO_AUI_10BASET 0x0200
+#define MODIFIED_BACKOFF 0x0800
+#define NO_AUTO_POLARITY 0x1000
+#define TWO_PART_DEFDIS 0x2000
+#define LOW_RX_SQUELCH 0x4000
+
+/* PP_SelfCTL - Software Self Control bit definition - Read/write */
+#define POWER_ON_RESET 0x0040
+#define SW_STOP 0x0100
+#define SLEEP_ON 0x0200
+#define AUTO_WAKEUP 0x0400
+#define HCB0_ENBL 0x1000
+#define HCB1_ENBL 0x2000
+#define HCB0 0x4000
+#define HCB1 0x8000
+
+/* PP_BusCTL - ISA Bus Control bit definition - Read/write */
+#define RESET_RX_DMA 0x0040
+#define MEMORY_ON 0x0400
+#define DMA_BURST_MODE 0x0800
+#define IO_CHANNEL_READY_ON 0x1000
+#define RX_DMA_SIZE_64K 0x2000
+#define ENABLE_IRQ 0x8000
+
+/* PP_TestCTL - Test Control bit definition - Read/write */
+#define LINK_OFF 0x0080
+#define ENDEC_LOOPBACK 0x0200
+#define AUI_LOOPBACK 0x0400
+#define BACKOFF_OFF 0x0800
+#define FDX_8900 0x4000
+#define FAST_TEST 0x8000
+
+/* PP_RxEvent - Receive Event Bit definition - Read-only */
+#define RX_IA_HASHED 0x0040
+#define RX_DRIBBLE 0x0080
+#define RX_OK 0x0100
+#define RX_HASHED 0x0200
+#define RX_IA 0x0400
+#define RX_BROADCAST 0x0800
+#define RX_CRC_ERROR 0x1000
+#define RX_RUNT 0x2000
+#define RX_EXTRA_DATA 0x4000
+
+#define HASH_INDEX_MASK 0x0FC00
+
+/* PP_TxEvent - Transmit Event Bit definition - Read-only */
+#define TX_LOST_CRS 0x0040
+#define TX_SQE_ERROR 0x0080
+#define TX_OK 0x0100
+#define TX_LATE_COL 0x0200
+#define TX_JBR 0x0400
+#define TX_16_COL 0x8000
+#define TX_SEND_OK_BITS (TX_OK|TX_LOST_CRS)
+#define TX_COL_COUNT_MASK 0x7800
+
+/* PP_BufEvent - Buffer Event Bit definition - Read-only */
+#define SW_INTERRUPT 0x0040
+#define RX_DMA 0x0080
+#define READY_FOR_TX 0x0100
+#define TX_UNDERRUN 0x0200
+#define RX_MISS 0x0400
+#define RX_128_BYTE 0x0800
+#define TX_COL_OVRFLW 0x1000
+#define RX_MISS_OVRFLW 0x2000
+#define RX_DEST_MATCH 0x8000
+
+/* PP_LineST - Ethernet Line Status bit definition - Read-only */
+#define LINK_OK 0x0080
+#define AUI_ON 0x0100
+#define TENBASET_ON 0x0200
+#define POLARITY_OK 0x1000
+#define CRS_OK 0x4000
+
+/* PP_SelfST - Chip Software Status bit definition */
+#define ACTIVE_33V 0x0040
+#define INIT_DONE 0x0080
+#define SI_BUSY 0x0100
+#define EEPROM_PRESENT 0x0200
+#define EEPROM_OK 0x0400
+#define EL_PRESENT 0x0800
+#define EE_SIZE_64 0x1000
+
+/* PP_BusST - ISA Bus Status bit definition */
+#define TX_BID_ERROR 0x0080
+#define READY_FOR_TX_NOW 0x0100
+
+/* PP_AutoNegCTL - Auto Negotiation Control bit definition */
+#define RE_NEG_NOW 0x0040
+#define ALLOW_FDX 0x0080
+#define AUTO_NEG_ENABLE 0x0100
+#define NLP_ENABLE 0x0200
+#define FORCE_FDX 0x8000
+#define AUTO_NEG_BITS (FORCE_FDX|NLP_ENABLE|AUTO_NEG_ENABLE)
+#define AUTO_NEG_MASK (FORCE_FDX|NLP_ENABLE|AUTO_NEG_ENABLE|ALLOW_FDX|RE_NEG_NOW)
+
+/* PP_AutoNegST - Auto Negotiation Status bit definition */
+#define AUTO_NEG_BUSY 0x0080
+#define FLP_LINK 0x0100
+#define FLP_LINK_GOOD 0x0800
+#define LINK_FAULT 0x1000
+#define HDX_ACTIVE 0x4000
+#define FDX_ACTIVE 0x8000
+
+/* The following block defines the ISQ event types */
+#define ISQ_RECEIVER_EVENT 0x04
+#define ISQ_TRANSMITTER_EVENT 0x08
+#define ISQ_BUFFER_EVENT 0x0c
+#define ISQ_RX_MISS_EVENT 0x10
+#define ISQ_TX_COL_EVENT 0x12
+
+#define ISQ_EVENT_MASK 0x003F	/* ISQ mask to find out type of event */
+#define ISQ_HIST 16		/* small history buffer */
+#define AUTOINCREMENT 0x8000	/* Bit mask to set bit-15 for autoincrement */
+
+#define TXRXBUFSIZE 0x0600
+#define RXDMABUFSIZE 0x8000
+#define RXDMASIZE 0x4000
+#define TXRX_LENGTH_MASK 0x07FF
+
+/* rx options bits */
+#define RCV_WITH_RXON	1	/* Set SerRx ON */
+#define RCV_COUNTS	2	/* Use Framecnt1 */
+#define RCV_PONG	4	/* Pong respondent */
+#define RCV_DONG	8	/* Dong operation */
+#define RCV_POLLING	0x10	/* Poll RxEvent */
+#define RCV_ISQ		0x20	/* Use ISQ, int */
+#define RCV_AUTO_DMA	0x100	/* Set AutoRxDMAE */
+#define RCV_DMA		0x200	/* Set RxDMA only */
+#define RCV_DMA_ALL	0x400	/* Copy all DMA'ed */
+#define RCV_FIXED_DATA	0x800	/* Every frame same */
+#define RCV_IO		0x1000	/* Use ISA IO only */
+#define RCV_MEMORY	0x2000	/* Use ISA Memory */
+
+#define RAM_SIZE	0x1000	/* The card has 4k bytes or RAM */
+#define PKT_START PP_TxFrame	/* Start of packet RAM */
+
+#define RX_FRAME_PORT	CS89x0_PORT(0x0000)
+#define TX_FRAME_PORT RX_FRAME_PORT
+#define TX_CMD_PORT	CS89x0_PORT(0x0004)
+#define TX_NOW		0x0000	/* Tx packet after 5 bytes copied */
+#define TX_AFTER_381	0x0040	/* Tx packet after 381 bytes copied */
+#define TX_AFTER_ALL	0x00c0	/* Tx packet after all bytes copied */
+#define TX_LEN_PORT	CS89x0_PORT(0x0006)
+#define ISQ_PORT	CS89x0_PORT(0x0008)
+#define ADD_PORT	CS89x0_PORT(0x000A)
+#define DATA_PORT	CS89x0_PORT(0x000C)
+
+#define EEPROM_WRITE_EN		0x00F0
+#define EEPROM_WRITE_DIS	0x0000
+#define EEPROM_WRITE_CMD	0x0100
+#define EEPROM_READ_CMD		0x0200
+
+/* Receive Header */
+/* Description of header of each packet in receive area of memory */
+#define RBUF_EVENT_LOW	0	/* Low byte of RxEvent - status of received
+				   frame */
+#define RBUF_EVENT_HIGH	1	/* High byte of RxEvent - status of received
+				   frame */
+#define RBUF_LEN_LOW	2	/* Length of received data - low byte */
+#define RBUF_LEN_HI	3	/* Length of received data - high byte */
+#define RBUF_HEAD_LEN	4	/* Length of this header */
+
+#define CHIP_READ 0x1		/* Used to mark state of the repins code (chip
+				   or dma) */
+#define DMA_READ 0x2		/* Used to mark state of the repins code (chip
+				   or dma) */
+
+/* for bios scan */
+/* */
+#ifdef CSDEBUG
+/* use these values for debugging bios scan */
+#define BIOS_START_SEG 0x00000
+#define BIOS_OFFSET_INC 0x0010
+#else
+#define BIOS_START_SEG 0x0c000
+#define BIOS_OFFSET_INC 0x0200
+#endif
+
+#define BIOS_LAST_OFFSET 0x0fc00
+
+/* Byte offsets into the EEPROM configuration buffer */
+#define ISA_CNF_OFFSET 0x6
+#define TX_CTL_OFFSET (ISA_CNF_OFFSET + 8)	/* 8900 eeprom */
+#define AUTO_NEG_CNF_OFFSET (ISA_CNF_OFFSET + 8)	/* 8920 eeprom */
+
+  /* the assumption here is that the bits in the eeprom are generally */
+  /* in the same position as those in the autonegctl register. */
+  /* Of course the IMM bit is not in that register so it must be */
+  /* masked out */
+#define EE_FORCE_FDX  0x8000
+#define EE_NLP_ENABLE 0x0200
+#define EE_AUTO_NEG_ENABLE 0x0100
+#define EE_ALLOW_FDX 0x0080
+#define EE_AUTO_NEG_CNF_MASK (EE_FORCE_FDX|EE_NLP_ENABLE|EE_AUTO_NEG_ENABLE|EE_ALLOW_FDX)
+
+#define IMM_BIT 0x0040		/* ignore missing media */
+
+#define ADAPTER_CNF_OFFSET (AUTO_NEG_CNF_OFFSET + 2)
+#define A_CNF_10B_T 0x0001
+#define A_CNF_AUI 0x0002
+#define A_CNF_10B_2 0x0004
+#define A_CNF_MEDIA_TYPE 0x0070
+#define A_CNF_MEDIA_AUTO 0x0070
+#define A_CNF_MEDIA_10B_T 0x0020
+#define A_CNF_MEDIA_AUI 0x0040
+#define A_CNF_MEDIA_10B_2 0x0010
+#define A_CNF_DC_DC_POLARITY 0x0080
+#define A_CNF_NO_AUTO_POLARITY 0x2000
+#define A_CNF_LOW_RX_SQUELCH 0x4000
+#define A_CNF_EXTND_10B_2 0x8000
+
+#define PACKET_PAGE_OFFSET 0x8
+
+/* Bit definitions for the ISA configuration word from the EEPROM */
+#define INT_NO_MASK 0x000F
+#define DMA_NO_MASK 0x0070
+#define ISA_DMA_SIZE 0x0200
+#define ISA_AUTO_RxDMA 0x0400
+#define ISA_RxDMA 0x0800
+#define DMA_BURST 0x1000
+#define STREAM_TRANSFER 0x2000
+#define ANY_ISA_DMA (ISA_AUTO_RxDMA | ISA_RxDMA)
+
+/* DMA controller registers */
+#define DMA_BASE 0x00		/* DMA controller base */
+#define DMA_BASE_2 0x0C0	/* DMA controller base */
+
+#define DMA_STAT 0x0D0		/* DMA controller status register */
+#define DMA_MASK 0x0D4		/* DMA controller mask register */
+#define DMA_MODE 0x0D6		/* DMA controller mode register */
+#define DMA_RESETFF 0x0D8	/* DMA controller first/last flip flop */
+
+/* DMA data */
+#define DMA_DISABLE 0x04	/* Disable channel n */
+#define DMA_ENABLE 0x00		/* Enable channel n */
+/* Demand transfers, incr. address, auto init, writes, ch. n */
+#define DMA_RX_MODE 0x14
+/* Demand transfers, incr. address, auto init, reads, ch. n */
+#define DMA_TX_MODE 0x18
+
+#define DMA_SIZE (16*1024)	/* Size of dma buffer - 16k */
+
+#define CS8900 0x0000
+#define CS8920 0x4000
+#define CS8920M 0x6000
+#define REVISON_BITS 0x1F00
+#define EEVER_NUMBER 0x12
+#define CHKSUM_LEN 0x14
+#define CHKSUM_VAL 0x0000
+#define START_EEPROM_DATA 0x001c	/* Offset into eeprom for start of data
+					 */
+#define IRQ_MAP_EEPROM_DATA 0x0046	/* Offset into eeprom for the IRQ map */
+#define IRQ_MAP_LEN 0x0004	/* No of bytes to read for the IRQ map */
+#define PNP_IRQ_FRMT 0x0022	/* PNP small item IRQ format */
+#ifdef CONFIG_SH_HICOSH4
+#define CS8900_IRQ_MAP 0x0002	/* HiCO-SH4 board has its IRQ on #1 */
+#else
+#define CS8900_IRQ_MAP 0x1c20	/* This IRQ map is fixed */
+#endif
+
+#define CS8920_NO_INTS 0x0F	/* Max CS8920 interrupt select # */
+
+#define PNP_ADD_PORT 0x0279
+#define PNP_WRITE_PORT 0x0A79
+
+#define GET_PNP_ISA_STRUCT 0x40
+#define PNP_ISA_STRUCT_LEN 0x06
+#define PNP_CSN_CNT_OFF 0x01
+#define PNP_RD_PORT_OFF 0x02
+#define PNP_FUNCTION_OK 0x00
+#define PNP_WAKE 0x03
+#define PNP_RSRC_DATA 0x04
+#define PNP_RSRC_READY 0x01
+#define PNP_STATUS 0x05
+#define PNP_ACTIVATE 0x30
+#define PNP_CNF_IO_H 0x60
+#define PNP_CNF_IO_L 0x61
+#define PNP_CNF_INT 0x70
+#define PNP_CNF_DMA 0x74
+#define PNP_CNF_MEM 0x48
+
+#define BIT0 1
+#define BIT15 0x8000
diff --git a/arch/mips/cavium-octeon/dma-octeon.c b/arch/mips/cavium-octeon/dma-octeon.c
new file mode 100644
index 0000000..3cfed46
--- /dev/null
+++ b/arch/mips/cavium-octeon/dma-octeon.c
@@ -0,0 +1,494 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2000  Ani Joshi <ajoshi@unixbox.com>
+ * Copyright (C) 2000, 2001  Ralf Baechle <ralf@gnu.org>
+ * Copyright (C) 2005 Ilya A. Volynets-Evenbakh <ilya@total-knowledge.com>
+ * swiped from i386, and cloned for MIPS by Geert, polished by Ralf.
+ * IP32 changes by Ilya.
+ * Cavium Networks: Create new dma setup for Cavium Networks Octeon based on
+ * the kernels original.
+ */
+#include <linux/types.h>
+#include <linux/mm.h>
+#include <linux/module.h>
+#include <linux/string.h>
+#include <linux/dma-mapping.h>
+#include <linux/platform_device.h>
+
+#include <asm/cache.h>
+#include <asm/io.h>
+#include <asm/ip32/crime.h>
+
+#include "hal.h"
+#include "pci-common.h"
+
+#define BAR2_PCI_ADDRESS 0x8000000000ul
+
+typedef struct {
+	int16_t ref_count;	/* Number of PCI mappings using this index */
+	int16_t address_bits;	/* Upper bits of physical address. This is
+				   shifted 22 bits */
+} bar1_index_state_t;
+
+#ifdef CONFIG_PCI
+static spinlock_t bar1_lock = SPIN_LOCK_UNLOCKED;
+static bar1_index_state_t bar1_state[32] = { {0, 0}, };
+#endif
+
+void *dma_alloc_noncoherent(struct device *dev, size_t size,
+			    dma_addr_t * dma_handle, unsigned int __nocast gfp)
+	__attribute__ ((alias("dma_alloc_coherent")));
+
+EXPORT_SYMBOL(dma_alloc_noncoherent);
+
+
+void *dma_alloc_coherent(struct device *dev, size_t size,
+			 dma_addr_t * dma_handle, unsigned int __nocast gfp)
+{
+	void *ret;
+	/* Ignore region specifiers. The Octeon Bar1 register can be used to
+	   map any memory on the system. As long as we don't fill up the 32
+	   entry table, memory can be anywhere */
+	gfp &= ~(__GFP_DMA | __GFP_HIGHMEM);
+
+	ret = (void *) __get_free_pages(gfp, get_order(size));
+	if (likely(ret != NULL)) {
+		memset(ret, 0, size);
+		/* By default, PCI devices can't get to memory. dma_map_single
+		   changes the Bar mapping to allow access to this region */
+		*dma_handle = dma_map_single(dev, ret, size, DMA_BIDIRECTIONAL);
+	}
+
+	return ret;
+}
+
+EXPORT_SYMBOL(dma_alloc_coherent);
+
+
+void dma_free_noncoherent(struct device *dev, size_t size, void *vaddr,
+			  dma_addr_t dma_handle)
+	__attribute__ ((alias("dma_free_coherent")));
+
+EXPORT_SYMBOL(dma_free_noncoherent);
+
+
+void dma_free_coherent(struct device *dev, size_t size, void *vaddr,
+		       dma_addr_t dma_handle)
+{
+	/* Free the Bar mappings */
+	dma_unmap_single(dev, dma_handle, size, DMA_BIDIRECTIONAL);
+	free_pages((unsigned long) vaddr, get_order(size));
+}
+
+EXPORT_SYMBOL(dma_free_coherent);
+
+
+dma_addr_t dma_map_single(struct device *dev, void *ptr, size_t size,
+			  enum dma_data_direction direction)
+{
+#ifndef CONFIG_PCI
+	/* Without PCI/PCIe this function can be called for Octeon internal
+	   devices such as USB. These devices all support 64bit addressing */
+	return virt_to_phys(ptr);
+#else
+	unsigned long flags;
+	uint64_t dma_mask;
+	int64_t start_index;
+	dma_addr_t result = -1;
+	uint64_t physical = virt_to_phys(ptr);
+	int64_t index;
+
+	BUG_ON(direction == DMA_NONE);
+
+	/* Use the DMA masks to determine the allowed memory region. For us it
+	   doesn't limit the actual memory, just the address visible over PCI.
+	   Devices with limits need to use lower indexed Bar1 entries. */
+	if (dev) {
+		dma_mask = dev->coherent_dma_mask;
+		if (dev->dma_mask)
+			dma_mask = *dev->dma_mask;
+	} else
+		dma_mask = 0xfffffffful;
+
+	/* Platform devices, such as the internal USB, skip all translation and
+	   use Octeon physical addresses directly */
+	if (dev->bus == &platform_bus_type)
+		return physical;
+
+	switch (octeon_dma_bar_type) {
+	case OCTEON_DMA_BAR_TYPE_PCIE:
+		if (unlikely(physical < (16ul << 10)))
+			panic("dma_map_single: Not allowed to map first 16KB. It interferes with BAR0 special area\n");
+		else if ((physical + size >= (256ul << 20)) &&
+			 (physical < (512ul << 20)))
+			panic("dma_map_single: Not allowed to map bootbus\n");
+		else if ((physical + size >= 0x400000000ull) &&
+			 physical < 0x410000000ull)
+			panic("dma_map_single: Attempt to map illegal memory address 0x%lx\n", physical);
+		else if (physical >= 0x420000000ull)
+			panic("dma_map_single: Attempt to map illegal memory address 0x%lx\n", physical);
+                else if ((physical + size >= (4ull<<30) - (OCTEON_PCI_BAR1_HOLE_SIZE<<20)) &&
+                         physical < (4ull<<30))
+                    printk("dma_map_single: Warning: Maping memory address that might conflict with devices 0x%lx-0x%lx\n", physical, physical+size-1);
+		/* The 2nd 256MB is mapped at 256<<20 instead of 0x410000000 */
+		if ((physical >= 0x410000000ull) && physical < 0x420000000ull)
+			result = physical - 0x400000000ull;
+		else
+			result = physical;
+                if (((result+size-1) & dma_mask) != result+size-1)
+                    panic("dma_map_single: Attempt to map address 0x%lx-0x%lx, which can't be accessed according to the dma mask 0x%lx\n", physical, physical+size-1, dma_mask);
+		goto done;
+
+	case OCTEON_DMA_BAR_TYPE_BIG:
+#ifdef CONFIG_64BIT
+		/* If the device supports 64bit addressing, then use BAR2 */
+		if (dma_mask > BAR2_PCI_ADDRESS) {
+			result = physical + BAR2_PCI_ADDRESS;
+			goto done;
+		}
+#endif
+		if (unlikely(physical < (4ul << 10))) {
+			panic("dma_map_single: Not allowed to map first 4KB. It interferes with BAR0 special area\n");
+		} else if (physical < (256ul << 20)) {
+			if (unlikely(physical + size > (256ul << 20)))
+				panic("dma_map_single: Requested memory spans Bar0 0:256MB and bootbus\n");
+			result = physical;
+			goto done;
+		} else if (unlikely(physical < (512ul << 20))) {
+			panic("dma_map_single: Not allowed to map bootbus\n");
+		} else if (physical < (2ul << 30)) {
+			if (unlikely(physical + size > (2ul << 30)))
+				panic("dma_map_single: Requested memory spans Bar0 512MB:2GB and BAR1\n");
+			result = physical;
+			goto done;
+		} else if (physical < (2ul << 30) + (128 << 20)) {
+			/* Fall through */
+		} else if (physical <
+			   (4ul << 30) - (OCTEON_PCI_BAR1_HOLE_SIZE << 20)) {
+			if (unlikely
+			    (physical + size >
+			     (4ul << 30) - (OCTEON_PCI_BAR1_HOLE_SIZE << 20)))
+				panic("dma_map_single: Requested memory extends past Bar1 (4GB-%luMB)\n", OCTEON_PCI_BAR1_HOLE_SIZE);
+			result = physical;
+			goto done;
+		} else if ((physical >= 0x410000000ull) &&
+			   (physical < 0x420000000ull)) {
+			if (unlikely(physical + size > 0x420000000ull))
+				panic("dma_map_single: Requested memory spans non existant memory\n");
+			result = physical - 0x400000000ull;	/* BAR0 fixed
+								   mapping
+								   256MB:512MB
+								   ->
+								   16GB+256MB:16GB+512MB
+								 */
+			goto done;
+		} else {
+			/* Continued below switch statement */
+		}
+		break;
+
+	case OCTEON_DMA_BAR_TYPE_SMALL:
+#ifdef CONFIG_64BIT
+		/* If the device supports 64bit addressing, then use BAR2 */
+		if (dma_mask > BAR2_PCI_ADDRESS) {
+			result = physical + BAR2_PCI_ADDRESS;
+			goto done;
+		}
+#endif
+		/* Continued below switch statement */
+		break;
+
+	default:
+		panic("dma_map_single: Invalid octeon_dma_bar_type\n");
+	}
+
+	/* Don't allow mapping to span multiple Bar entries. The hardware guys
+	   won't guarantee that DMA across boards work */
+	if (unlikely((physical >> 22) != ((physical + size - 1) >> 22)))
+		panic("dma_map_single: Requested memory spans more than one Bar1 entry\n");
+
+	if (octeon_dma_bar_type == OCTEON_DMA_BAR_TYPE_BIG)
+		start_index = 31;
+	else if (unlikely(dma_mask < (1ul << 27)))
+		start_index = (dma_mask >> 22);
+	else
+		start_index = 31;
+
+	/* Only one processor can access the Bar register at once */
+	spin_lock_irqsave(&bar1_lock, flags);
+
+	/* Look through Bar1 for existing mapping that will work */
+	for (index = start_index; index >= 0; index--) {
+		if ((bar1_state[index].address_bits == physical >> 22) &&
+		    (bar1_state[index].ref_count)) {
+			/* An existing mapping will work, use it */
+			bar1_state[index].ref_count++;
+			if (unlikely(bar1_state[index].ref_count < 0))
+				panic("dma_map_single: Bar1[%d] reference count overflowed\n", (int) index);
+			result = (index << 22) | (physical & ((1 << 22) - 1));
+			/* Large BAR1 is offset at 2GB */
+			if (octeon_dma_bar_type == OCTEON_DMA_BAR_TYPE_BIG)
+				result += 2ul << 30;
+			goto done_unlock;
+		}
+	}
+
+	/* No existing mappings, look for a free entry */
+	for (index = start_index; index >= 0; index--) {
+		if (unlikely(bar1_state[index].ref_count == 0)) {
+			cvmx_pci_bar1_indexx_t bar1_index;
+			/* We have a free entry, use it */
+			bar1_state[index].ref_count = 1;
+			bar1_state[index].address_bits = physical >> 22;
+			bar1_index.u32 = 0;
+			bar1_index.s.addr_idx = physical >> 22;	/* Address bits
+								   [35:22] sent
+								   to L2C */
+			bar1_index.s.ca = 1;	/* Don't put PCI accesses in
+						   L2. */
+			bar1_index.s.end_swp = 1;	/* Endian Swap Mode */
+			bar1_index.s.addr_v = 1;	/* Set '1' when the
+							   selected address
+							   range is valid. */
+			octeon_npi_write32(CVMX_NPI_PCI_BAR1_INDEXX(index),
+					   bar1_index.u32);
+			/* An existing mapping will work, use it */
+			result = (index << 22) | (physical & ((1 << 22) - 1));
+			/* Large BAR1 is offset at 2GB */
+			if (octeon_dma_bar_type == OCTEON_DMA_BAR_TYPE_BIG)
+				result += 2ul << 30;
+			goto done_unlock;
+		}
+	}
+
+	printk("dma_map_single: Can't find empty BAR1 index for physical mapping 0x%llx\n", (unsigned long long) physical);
+
+      done_unlock:
+	spin_unlock_irqrestore(&bar1_lock, flags);
+      done:
+	// printk("dma_map_single 0x%lx->0x%lx\n", physical, result);
+	return result;
+#endif
+}
+
+EXPORT_SYMBOL(dma_map_single);
+
+
+void dma_unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
+		      enum dma_data_direction direction)
+{
+#ifndef CONFIG_PCI
+	/* Without PCI/PCIe this function can be called for Octeon internal
+	   devices such as USB. These devices all support 64bit addressing */
+	return;
+#else
+	unsigned long flags;
+	uint64_t index;
+
+	BUG_ON(direction == DMA_NONE);
+
+	/* Platform devices, such as the internal USB, skip all translation and
+	   use Octeon physical addresses directly */
+	if (dev->bus == &platform_bus_type)
+		return;
+
+	switch (octeon_dma_bar_type) {
+	case OCTEON_DMA_BAR_TYPE_PCIE:
+		/* Nothing to do, all mappings are static */
+		goto done;
+
+	case OCTEON_DMA_BAR_TYPE_BIG:
+#ifdef CONFIG_64BIT
+		/* Nothing to do for addresses using BAR2 */
+		if (dma_addr >= BAR2_PCI_ADDRESS)
+			goto done;
+#endif
+		if (unlikely(dma_addr < (4ul << 10)))
+			panic("dma_unmap_single: Unexpect DMA address 0x%lx\n",
+			      dma_addr);
+		else if (dma_addr < (2ul << 30))
+			goto done;	/* Nothing to do for addresses using
+					   BAR0 */
+		else if (dma_addr < (2ul << 30) + (128ul << 20))
+			index = (dma_addr - (2ul << 30)) >> 22;	/* Need to
+								   unmap, fall
+								   through */
+		else if (dma_addr <
+			 (4ul << 30) - (OCTEON_PCI_BAR1_HOLE_SIZE << 20))
+			goto done;	/* Nothing to do for the rest of BAR1 */
+		else
+			panic("dma_unmap_single: Unexpect DMA address 0x%lx\n",
+			      dma_addr);
+		/* Continued below switch statement */
+		break;
+
+	case OCTEON_DMA_BAR_TYPE_SMALL:
+#ifdef CONFIG_64BIT
+		/* Nothing to do for addresses using BAR2 */
+		if (dma_addr >= BAR2_PCI_ADDRESS)
+			goto done;
+#endif
+		index = dma_addr >> 22;
+		/* Continued below switch statement */
+		break;
+
+	default:
+		panic("dma_unmap_single: Invalid octeon_dma_bar_type\n");
+	}
+
+	if (unlikely(index > 31))
+		panic("dma_unmap_single: Attempt to unmap an invalid address (0x%llx)\n", (unsigned long long) dma_addr);
+
+	spin_lock_irqsave(&bar1_lock, flags);
+	bar1_state[index].ref_count--;
+	if (bar1_state[index].ref_count == 0)
+		octeon_npi_write32(CVMX_NPI_PCI_BAR1_INDEXX(index), 0);
+	else if (unlikely(bar1_state[index].ref_count < 0))
+		panic("dma_unmap_single: Bar1[%u] reference count < 0\n",
+		      (int) index);
+	spin_unlock_irqrestore(&bar1_lock, flags);
+      done:
+	// printk("dma_unmap_single 0x%lx\n", dma_addr);
+	return;
+#endif
+}
+
+EXPORT_SYMBOL(dma_unmap_single);
+
+
+int dma_map_sg(struct device *dev, struct scatterlist *sg, int nents,
+	       enum dma_data_direction direction)
+{
+	int i;
+	for (i = 0; i < nents; i++, sg++)
+		sg->dma_address =
+			dma_map_page(dev, sg->page, sg->offset, sg->length,
+				     direction);
+
+	return nents;
+}
+
+EXPORT_SYMBOL(dma_map_sg);
+
+
+dma_addr_t dma_map_page(struct device * dev, struct page * page,
+			unsigned long offset, size_t size,
+			enum dma_data_direction direction)
+{
+	return dma_map_single(dev, page_address(page) + offset, size,
+			      direction);
+}
+
+EXPORT_SYMBOL(dma_map_page);
+
+
+void dma_unmap_page(struct device *dev, dma_addr_t dma_address, size_t size,
+		    enum dma_data_direction direction)
+{
+	dma_unmap_single(dev, dma_address, size, direction);
+}
+
+EXPORT_SYMBOL(dma_unmap_page);
+
+
+void dma_unmap_sg(struct device *dev, struct scatterlist *sg, int nhwentries,
+		  enum dma_data_direction direction)
+{
+	int i;
+	for (i = 0; i < nhwentries; i++, sg++)
+		dma_unmap_page(dev, sg->dma_address, sg->length, direction);
+}
+
+EXPORT_SYMBOL(dma_unmap_sg);
+
+
+void dma_sync_single_for_cpu(struct device *dev, dma_addr_t dma_handle,
+			     size_t size, enum dma_data_direction direction)
+{
+	mb();
+}
+
+EXPORT_SYMBOL(dma_sync_single_for_cpu);
+
+
+void dma_sync_single_for_device(struct device *dev, dma_addr_t dma_handle,
+				size_t size, enum dma_data_direction direction)
+{
+	mb();
+}
+
+EXPORT_SYMBOL(dma_sync_single_for_device);
+
+
+void dma_sync_single_range_for_cpu(struct device *dev, dma_addr_t dma_handle,
+				   unsigned long offset, size_t size,
+				   enum dma_data_direction direction)
+{
+	mb();
+}
+
+EXPORT_SYMBOL(dma_sync_single_range_for_cpu);
+
+
+void dma_sync_single_range_for_device(struct device *dev, dma_addr_t dma_handle,
+				      unsigned long offset, size_t size,
+				      enum dma_data_direction direction)
+{
+	mb();
+}
+
+EXPORT_SYMBOL(dma_sync_single_range_for_device);
+
+
+void dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg, int nelems,
+			 enum dma_data_direction direction)
+{
+	mb();
+}
+
+EXPORT_SYMBOL(dma_sync_sg_for_cpu);
+
+
+void dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg,
+			    int nelems, enum dma_data_direction direction)
+{
+	mb();
+}
+
+EXPORT_SYMBOL(dma_sync_sg_for_device);
+
+
+int dma_mapping_error(dma_addr_t dma_addr)
+{
+	return (dma_addr == -1);
+}
+
+EXPORT_SYMBOL(dma_mapping_error);
+
+
+int dma_supported(struct device *dev, u64 mask)
+{
+	return 1;
+}
+
+EXPORT_SYMBOL(dma_supported);
+
+
+int dma_is_consistent(struct device *dev, dma_addr_t dma_addr)
+{
+	return 1;
+}
+
+EXPORT_SYMBOL(dma_is_consistent);
+
+
+void dma_cache_sync(struct device *dev, void *vaddr, size_t size,
+		    enum dma_data_direction direction)
+{
+	mb();
+}
+
+EXPORT_SYMBOL(dma_cache_sync);
diff --git a/arch/mips/cavium-octeon/gpl-executive/Makefile b/arch/mips/cavium-octeon/gpl-executive/Makefile
new file mode 100644
index 0000000..4b6e788
--- /dev/null
+++ b/arch/mips/cavium-octeon/gpl-executive/Makefile
@@ -0,0 +1,62 @@
+#
+# Makefile for the Cavium Octeon specific kernel interface routines
+# under Linux.
+#
+# This file is subject to the terms and conditions of the GNU General Public
+# License.  See the file "COPYING" in the main directory of this archive
+# for more details.
+#
+# Copyright (C) 2005-2007 Cavium Networks
+#
+
+source:=$(srctree)/$(src)
+
+EXTRA_CFLAGS += -I $(obj)/config -I $(source)/config
+
+executive-files := cvmx-bootmem.o
+executive-files += cvmx-cmd-queue.o
+executive-files += cvmx-dma-engine.o
+executive-files += cvmx-fpa.o
+executive-files += cvmx-helper.o
+executive-files += cvmx-helper-board.o
+executive-files += cvmx-helper-errata.o
+executive-files += cvmx-helper-fpa.o
+executive-files += cvmx-helper-loop.o
+executive-files += cvmx-helper-npi.o
+executive-files += cvmx-helper-rgmii.o
+executive-files += cvmx-helper-sgmii.o
+executive-files += cvmx-helper-spi.o
+executive-files += cvmx-helper-util.o
+executive-files += cvmx-helper-xaui.o
+executive-files += cvmx-interrupt-decodes.o
+executive-files += cvmx-interrupt-rsl.o
+executive-files += cvmx-l2c.o
+executive-files += cvmx-mgmt-port.o
+executive-files += cvmx-pcie.o
+executive-files += cvmx-pko.o
+executive-files += cvmx-raid.o
+executive-files += cvmx-spi4000.o
+executive-files += cvmx-spi.o
+executive-files += cvmx-sysinfo.o
+executive-files += cvmx-tra.o
+executive-files += cvmx-twsi.o
+executive-files += cvmx-warn.o
+executive-files += cvmx-compactflash.o
+executive-files += octeon-model.o
+executive-files += octeon-pci-console.o
+
+obj-y := $(executive-files) cvmx-linux-kernel-exports.o 
+
+executive-obj-files := $(executive-files:%=$(obj)/%)
+executive-src-files := $(executive-obj-files:%.o=%.c)
+cvmx-linux-kernel-exports.o $(executive-src-files): $(obj)/config/cvmx-config.h
+
+$(executive-src-files):
+	$(Q)ln -fs $(@:$(obj)/%.c=$(OCTEON_ROOT)/executive/%.c) $@
+
+$(obj)/config/cvmx-config.h: $(source)/config/executive-config.h
+	cd $(obj) && cvmx-config $< 
+
+clean:
+	rm -f *.o $(executive-src-files)
+
diff --git a/arch/mips/cavium-octeon/gpl-executive/config/executive-config.h b/arch/mips/cavium-octeon/gpl-executive/config/executive-config.h
new file mode 100644
index 0000000..2fe77fd
--- /dev/null
+++ b/arch/mips/cavium-octeon/gpl-executive/config/executive-config.h
@@ -0,0 +1,115 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2005-2007 Cavium Networks
+ */
+/*
+ * File version info: $Id: executive-config.h 25756 2007-07-02 23:25:50Z kreese $
+ */
+#ifndef __EXECUTIVE_CONFIG_H__
+#define __EXECUTIVE_CONFIG_H__
+
+/* Define to enable the use of simple executive DFA functions */
+//#define CVMX_ENABLE_DFA_FUNCTIONS
+
+/* Define to enable the use of simple executive packet output functions.
+** For packet I/O setup enable the helper functions below.
+*/
+#define CVMX_ENABLE_PKO_FUNCTIONS
+
+/* Define to enable the use of simple executive timer bucket functions.
+** Refer to cvmx-tim.[ch] for more information
+*/
+//#define CVMX_ENABLE_TIMER_FUNCTIONS
+
+/* Define to enable the use of simple executive helper functions. These
+** include many harware setup functions.  See cvmx-helper.[ch] for
+** details.
+*/
+#define CVMX_ENABLE_HELPER_FUNCTIONS
+
+/* CVMX_HELPER_FIRST_MBUFF_SKIP is the number of bytes to reserve before
+** the beginning of the packet. If necessary, override the default
+** here.  See the IPD section of the hardware manual for MBUFF SKIP
+** details.*/
+#define CVMX_HELPER_FIRST_MBUFF_SKIP 184
+
+/* CVMX_HELPER_NOT_FIRST_MBUFF_SKIP is the number of bytes to reserve in each
+** chained packet element. If necessary, override the default here */
+#define CVMX_HELPER_NOT_FIRST_MBUFF_SKIP 0
+
+/* CVMX_HELPER_ENABLE_BACK_PRESSURE controls whether back pressure is enabled
+** for all input ports. This controls if IPD sends backpressure to all ports if
+** Octeon's FPA pools don't have enough packet or work queue entries. Even when
+** this is off, it is still possible to get backpressure from individual
+** hardware ports. When configuring backpressure, also check
+** CVMX_HELPER_DISABLE_*_BACKPRESSURE below. If necessary, override the default
+** here */
+#define CVMX_HELPER_ENABLE_BACK_PRESSURE 1
+
+/* CVMX_HELPER_ENABLE_IPD controls if the IPD is enabled in the helper
+**  function. Once it is enabled the hardware starts accepting packets. You
+**  might want to skip the IPD enable if configuration changes are need
+**  from the default helper setup. If necessary, override the default here */
+#define CVMX_HELPER_ENABLE_IPD 0
+
+/* CVMX_HELPER_INPUT_TAG_TYPE selects the type of tag that the IPD assigns
+** to incoming packets. */
+#define CVMX_HELPER_INPUT_TAG_TYPE CVMX_POW_TAG_TYPE_ORDERED
+
+/* The following select which fields are used by the PIP to generate
+** the tag on INPUT
+** 0: don't include
+** 1: include */
+#define CVMX_HELPER_INPUT_TAG_IPV6_SRC_IP	0
+#define CVMX_HELPER_INPUT_TAG_IPV6_DST_IP   	0
+#define CVMX_HELPER_INPUT_TAG_IPV6_SRC_PORT 	0
+#define CVMX_HELPER_INPUT_TAG_IPV6_DST_PORT 	0
+#define CVMX_HELPER_INPUT_TAG_IPV6_NEXT_HEADER 	0
+#define CVMX_HELPER_INPUT_TAG_IPV4_SRC_IP	0
+#define CVMX_HELPER_INPUT_TAG_IPV4_DST_IP   	0
+#define CVMX_HELPER_INPUT_TAG_IPV4_SRC_PORT 	0
+#define CVMX_HELPER_INPUT_TAG_IPV4_DST_PORT 	0
+#define CVMX_HELPER_INPUT_TAG_IPV4_PROTOCOL	0
+#define CVMX_HELPER_INPUT_TAG_INPUT_PORT	1
+
+/* Select skip mode for input ports */
+#define CVMX_HELPER_INPUT_PORT_SKIP_MODE	CVMX_PIP_PORT_CFG_MODE_SKIPL2
+
+/* Define the number of queues per output port */
+#define CVMX_HELPER_PKO_QUEUES_PER_PORT_INTERFACE0	1
+#define CVMX_HELPER_PKO_QUEUES_PER_PORT_INTERFACE1	1
+
+/* Force backpressure to be disabled.  This overrides all other
+** backpressure configuration */
+#define CVMX_HELPER_DISABLE_RGMII_BACKPRESSURE 0
+
+/* Disable the SPI4000's processing of backpressure packets and backpressure
+** generation. When this is 1, the SPI4000 will not stop sending packets when
+** receiving backpressure. It will also not generate backpressure packets when
+** its internal FIFOs are full. */
+#define CVMX_HELPER_DISABLE_SPI4000_BACKPRESSURE 0
+
+/* Select the number of low latency memory ports (interfaces) that
+** will be configured.  Valid values are 1 and 2.
+*/
+#define CVMX_LLM_CONFIG_NUM_PORTS 1
+
+/* Enable the fix for PKI-100 errata ("Size field is 8 too large in WQE and next
+** pointers"). If CVMX_ENABLE_LEN_M8_FIX is not enabled the fix for this errats will 
+** not be enabled. 
+** 0: Fix is not enabled
+** 1: Fix is enabled, if supported by hardware
+*/
+#define CVMX_ENABLE_LEN_M8_FIX  1
+
+#if defined(CVMX_ENABLE_HELPER_FUNCTIONS) && !defined(CVMX_ENABLE_PKO_FUNCTIONS)
+#define CVMX_ENABLE_PKO_FUNCTIONS
+#endif
+
+/* Executive resource descriptions provided in cvmx-resources.config */
+#include "cvmx-resources.config"
+
+#endif
diff --git a/arch/mips/cavium-octeon/gpl-executive/cvmx-linux-kernel-exports.c b/arch/mips/cavium-octeon/gpl-executive/cvmx-linux-kernel-exports.c
new file mode 100644
index 0000000..6148bf5
--- /dev/null
+++ b/arch/mips/cavium-octeon/gpl-executive/cvmx-linux-kernel-exports.c
@@ -0,0 +1,102 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2005-2007 Cavium Networks
+ */
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include "cvmx.h"
+#include "cvmx-bootmem.h"
+#include "cvmx-cmd-queue.h"
+#include "cvmx-helper.h"
+#include "cvmx-helper-board.h"
+#include "cvmx-helper-util.h"
+#include "cvmx-mgmt-port.h"
+#include "cvmx-pko.h"
+#include "cvmx-spi.h"
+#include "cvmx-sysinfo.h"
+#include "cvmx-tra.h"
+#include "cvmx-warn.h"
+
+extern CVMX_SHARED __cvmx_cmd_queue_all_state_t *__cvmx_cmd_queue_state_ptr;
+
+/* Exports for cvmx-bootmem.c */
+EXPORT_SYMBOL(cvmx_bootmem_alloc);
+EXPORT_SYMBOL(cvmx_bootmem_alloc_address);
+EXPORT_SYMBOL(cvmx_bootmem_alloc_range);
+EXPORT_SYMBOL(cvmx_bootmem_alloc_named);
+EXPORT_SYMBOL(cvmx_bootmem_alloc_named_address);
+EXPORT_SYMBOL(cvmx_bootmem_alloc_named_range);
+EXPORT_SYMBOL(cvmx_bootmem_free_named);
+EXPORT_SYMBOL(cvmx_bootmem_find_named_block);
+EXPORT_SYMBOL(cvmx_bootmem_available_mem);
+
+/* Exports for cvmx-cmd-queue.c */
+EXPORT_SYMBOL(cvmx_cmd_queue_initialize);
+EXPORT_SYMBOL(cvmx_cmd_queue_shutdown);
+EXPORT_SYMBOL(cvmx_cmd_queue_length);
+EXPORT_SYMBOL(cvmx_cmd_queue_buffer);
+EXPORT_SYMBOL(__cvmx_cmd_queue_state_ptr);
+
+/* Exports for cvmx-helper.c */
+EXPORT_SYMBOL(cvmx_helper_ipd_and_packet_input_enable);
+EXPORT_SYMBOL(cvmx_helper_initialize_packet_io_global);
+EXPORT_SYMBOL(cvmx_helper_initialize_packet_io_local);
+EXPORT_SYMBOL(cvmx_helper_ports_on_interface);
+EXPORT_SYMBOL(cvmx_helper_get_number_of_interfaces);
+EXPORT_SYMBOL(cvmx_helper_interface_get_mode);
+EXPORT_SYMBOL(cvmx_helper_link_autoconf);
+EXPORT_SYMBOL(cvmx_helper_link_get);
+EXPORT_SYMBOL(cvmx_helper_link_set);
+
+/* Exports for cvmx-helper-board.c */
+EXPORT_SYMBOL(cvmx_helper_board_get_mii_address);
+
+/* Exports for cvmx-helper-util.c */
+EXPORT_SYMBOL(cvmx_helper_interface_mode_to_string);
+EXPORT_SYMBOL(cvmx_helper_dump_packet);
+EXPORT_SYMBOL(cvmx_helper_setup_red_queue);
+EXPORT_SYMBOL(cvmx_helper_setup_red);
+EXPORT_SYMBOL(cvmx_helper_get_version);
+EXPORT_SYMBOL(cvmx_helper_get_ipd_port);
+EXPORT_SYMBOL(cvmx_helper_get_interface_num);
+EXPORT_SYMBOL(cvmx_helper_get_interface_index_num);
+
+/* Exports for cvmx-mgmt-port.c */
+EXPORT_SYMBOL(cvmx_mgmt_port_initialize);
+EXPORT_SYMBOL(cvmx_mgmt_port_shutdown);
+EXPORT_SYMBOL(cvmx_mgmt_port_enable);
+EXPORT_SYMBOL(cvmx_mgmt_port_disable);
+EXPORT_SYMBOL(cvmx_mgmt_port_send);
+EXPORT_SYMBOL(cvmx_mgmt_port_receive);
+EXPORT_SYMBOL(cvmx_mgmt_port_get_link);
+EXPORT_SYMBOL(cvmx_mgmt_port_set_mac);
+EXPORT_SYMBOL(cvmx_mgmt_port_get_mac);
+
+/* Exports for cvmx-pko.c */
+EXPORT_SYMBOL(cvmx_pko_initialize_global);
+EXPORT_SYMBOL(cvmx_pko_initialize_local);
+EXPORT_SYMBOL(cvmx_pko_enable);
+EXPORT_SYMBOL(cvmx_pko_disable);
+EXPORT_SYMBOL(cvmx_pko_shutdown);
+EXPORT_SYMBOL(cvmx_pko_config_port);
+
+/* Exports for cvmx-spi.c and cvmx-spi4000.c */
+EXPORT_SYMBOL(cvmx_spi_restart_interface);
+EXPORT_SYMBOL(cvmx_spi4000_check_speed);
+
+/* Exports for cvmx-sysinfo.c */
+EXPORT_SYMBOL(cvmx_sysinfo_get);
+
+/* Exports for cvmx-tra.c */
+EXPORT_SYMBOL(cvmx_tra_setup);
+EXPORT_SYMBOL(cvmx_tra_trig_setup);
+EXPORT_SYMBOL(cvmx_tra_read);
+EXPORT_SYMBOL(cvmx_tra_decode_text);
+EXPORT_SYMBOL(cvmx_tra_display);
+
+/* Exports for cvmx-warn.c */
+EXPORT_SYMBOL(cvmx_warn);
+
diff --git a/arch/mips/cavium-octeon/hal.c b/arch/mips/cavium-octeon/hal.c
new file mode 100644
index 0000000..1d0080f
--- /dev/null
+++ b/arch/mips/cavium-octeon/hal.c
@@ -0,0 +1,545 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2004 - 2007 Cavium Networks
+ */
+#include <linux/kernel.h>
+#include <linux/string.h>
+#include <linux/module.h>
+#include <asm/time.h>
+#include <octeon-app-init.h>
+
+#ifndef CONFIG_CAVIUM_RESERVE32
+#define CONFIG_CAVIUM_RESERVE32 0
+#endif
+
+#include "hal.h"
+
+#include "cvmx-bootmem.h"
+#include "cvmx-app-init.h"
+#include "cvmx-sysinfo.h"
+#include "cvmx-l2c.h"
+#include "cvmx-spinlock.h"
+
+/* Set to non-zero, so it is not in .bss section and is not zeroed */
+volatile octeon_boot_descriptor_t *octeon_boot_desc_ptr = (void *) 0xEADBEEFULL;
+cvmx_bootinfo_t *octeon_bootinfo;
+
+/* This must not be static since inline functions access it */
+spinlock_t octeon_led_lock;
+
+#if CONFIG_CAVIUM_RESERVE32
+uint64_t octeon_reserve32_memory = 0;
+#endif
+
+
+/**
+ * Write to the LCD display connected to the bootbus. This display
+ * exists on most Cavium evaluation boards. If it doesn't exist, then
+ * this function doesn't do anything.
+ *
+ * @param s      String to write
+ */
+void octeon_write_lcd(const char *s)
+{
+	if (octeon_bootinfo->led_display_base_addr) {
+		volatile char *lcd_address =
+			cvmx_phys_to_ptr(octeon_bootinfo->
+					 led_display_base_addr);
+		int i;
+		for (i = 0; i < 8; i++) {
+			if (*s)
+				lcd_address[i] = *s++;
+			else
+				lcd_address[i] = ' ';
+		}
+	}
+}
+
+
+/**
+ * Check the hardware BIST results for a CPU
+ */
+void octeon_check_cpu_bist(void)
+{
+	const int coreid = cvmx_get_core_num();
+	unsigned long long mask;
+	unsigned long long bist_val;
+
+	/* Check BIST results for COP0 registers */
+	mask = 0x1f00000000ull;
+	bist_val = __read_64bit_c0_register($27, 0);
+	if (bist_val & mask)
+		printk("Core%d BIST Failure: CacheErr(icache) = 0x%llx\n",
+		       coreid, bist_val);
+
+	bist_val = __read_64bit_c0_register($27, 1);
+	if (bist_val & 1)
+		printk("Core%d L1 Dcache parity error: CacheErr(dcache) = 0x%llx\n", coreid, bist_val);
+
+	mask = 0xfc00000000000000ull;
+	bist_val = __read_64bit_c0_register($11, 7);
+	if (bist_val & mask)
+		printk("Core%d BIST Failure: COP0_CVM_MEM_CTL = 0x%llx\n",
+		       coreid, bist_val);
+
+	__write_64bit_c0_register($27, 1, 0);
+}
+
+
+/**
+ * Return non zero of we are currently running in the Octeon simulator
+ *
+ * @return
+ */
+int octeon_is_simulation(void)
+{
+	return (octeon_bootinfo->board_type == CVMX_BOARD_TYPE_SIM);
+}
+
+
+/**
+ * Return true if Octeon is in PCI Host mode. This means
+ * Linux can control the PCI bus.
+ *
+ * @return Non zero if Octeon in host mode
+ */
+int octeon_is_pci_host(void)
+{
+#ifdef CONFIG_PCI
+	return (octeon_bootinfo->
+		config_flags & CVMX_BOOTINFO_CFG_FLAG_PCI_HOST);
+#else
+	return 0;
+#endif
+}
+
+
+/**
+ * This function returns if the USB clock uses 12/24/48MHz 3.3V
+ * reference clock at the USB_REF_CLK pin. Result is non zero
+ * if it does, If it uses 12MHz crystal as clock source at USB_XO
+ * and USB_XI, then the return value is zero. This function will
+ * need update for new boards.
+ *
+ * @return True is USB is using a reference clock
+ */
+int octeon_usb_is_ref_clk(void)
+{
+	switch (octeon_bootinfo->board_type) {
+	case CVMX_BOARD_TYPE_BBGW_REF:
+		return 0;
+	}
+	return 1;
+}
+
+
+/**
+ * Get the clock rate of Octeon
+ *
+ * @return Clock rate in HZ
+ */
+uint64_t octeon_get_clock_rate(void)
+{
+	if (octeon_is_simulation())
+		octeon_bootinfo->eclock_hz = 6000000;
+	return octeon_bootinfo->eclock_hz;
+}
+
+
+/**
+ * Return the board name as a constant string
+ *
+ * @return board name
+ */
+const char *octeon_board_type_string(void)
+{
+	static char name[80];
+	sprintf(name, "%s (%s)",
+		cvmx_board_type_to_string(octeon_bootinfo->board_type),
+		octeon_model_get_string(read_c0_prid()));
+	return name;
+}
+
+
+/**
+ * Return the mapping of PCI device number to IRQ line. Each
+ * character in the return string represents the interrupt
+ * line for the device at that position. Device 1 maps to the
+ * first character, etc. The characters A-D are used for PCI
+ * interrupts.
+ *
+ * @return PCI interrupt mapping
+ */
+const char *octeon_get_pci_interrupts(void)
+{
+	/* Returning an empty string causes the interrupts to be routed based
+	   on the PCI specification. From the PCI spec:
+
+	   INTA# of Device Number 0 is connected to IRQW on the system board.
+	   (Device Number has no significance regarding being located on the
+	   system board or in a connector.) INTA# of Device Number 1 is
+	   connected to IRQX on the system board. INTA# of Device Number 2 is
+	   connected to IRQY on the system board. INTA# of Device Number 3 is
+	   connected to IRQZ on the system board. The table below describes how
+	   each agent's INTx# lines are connected to the system board interrupt
+	   lines. The following equation can be used to determine to which
+	   INTx# signal on the system board a given device's INTx# line(s) is
+	   connected.
+
+	   MB = (D + I) MOD 4 MB = System board Interrupt (IRQW = 0, IRQX = 1,
+	   IRQY = 2, and IRQZ = 3) D = Device Number I = Interrupt Number
+	   (INTA# = 0, INTB# = 1, INTC# = 2, and INTD# = 3) */
+	switch (octeon_bootinfo->board_type) {	/* Device ID
+		   1111111111222222222233 *//* 01234567890123456789012345678901 */
+		// case CVMX_BOARD_TYPE_NAO38: return
+		// "AAAAAADBAAAAAAAAAAAAAAAAAAAAAAAA";
+	case CVMX_BOARD_TYPE_NAO38:
+		return "AAAAADABAAAAAAAAAAAAAAAAAAAAAAAA";	/* This is
+								   really the
+								   NAC38 */
+	case CVMX_BOARD_TYPE_THUNDER:
+		return "";
+	case CVMX_BOARD_TYPE_EBH3000:
+		return "";
+	case CVMX_BOARD_TYPE_EBH3100:
+	case CVMX_BOARD_TYPE_CN3010_EVB_HS5:
+	case CVMX_BOARD_TYPE_CN3005_EVB_HS5:
+		return "AAABAAAAAAAAAAAAAAAAAAAAAAAAAAAA";
+        case CVMX_BOARD_TYPE_BBGW_REF:
+            return "AABCD";
+	default:
+		return "";
+	}
+}
+
+
+/**
+ * Return the interrupt line for the i8259 in the southbridge
+ *
+ * @return
+ */
+int octeon_get_southbridge_interrupt(void)
+{
+	switch (octeon_bootinfo->board_type) {
+	case CVMX_BOARD_TYPE_EBH3000:
+		return 47;	/* PCI INDD */
+	case CVMX_BOARD_TYPE_NAC38:
+		return 39;	/* GPIO 15 */
+	default:
+		return 0;	/* No southbridge */
+	}
+}
+
+
+/**
+ * Get the coremask Linux was booted on.
+ *
+ * @return Core mask
+ */
+int octeon_get_boot_coremask(void)
+{
+	return octeon_boot_desc_ptr->core_mask;
+}
+
+
+/**
+ * Return the number of arguments we got from the bootloader
+ *
+ * @return argc
+ */
+int octeon_get_boot_num_arguments(void)
+{
+	return octeon_boot_desc_ptr->argc;
+}
+
+
+/**
+ * Return the console uart passed by the bootloader
+ *
+ * @return uart   (0 or 1)
+ */
+int octeon_get_boot_uart(void)
+{
+#if OCTEON_APP_INIT_H_VERSION >= 1	/* The UART1 flag is new */
+	return !!(octeon_boot_desc_ptr->flags & OCTEON_BL_FLAG_CONSOLE_UART1);
+#else
+	return 0;
+#endif
+}
+
+/**
+ * Return the debug flag passed by the bootloader
+ *
+ * @return debug flag (0 or 1)
+ */
+int octeon_get_boot_debug_flag(void)
+{
+	return !!(octeon_boot_desc_ptr->flags & OCTEON_BL_FLAG_DEBUG);
+}
+
+/**
+ * Get an argument from the bootloader
+ *
+ * @param arg    argument to get
+ * @return argument
+ */
+const char *octeon_get_boot_argument(int arg)
+{
+	return cvmx_phys_to_ptr(octeon_boot_desc_ptr->argv[arg]);
+}
+
+
+/**
+ * Called very early in the initial C code to initialize the Octeon
+ * HAL layer.
+ */
+void octeon_hal_init(void)
+{
+	cvmx_sysinfo_t *sysinfo;
+
+	/* Make sure we got the boot descriptor block */
+	if ((octeon_boot_desc_ptr == (void *) 0xEADBEEFULL))
+		panic("Boot descriptor block wasn't passed properly\n");
+
+	octeon_bootinfo =
+		cvmx_phys_to_ptr(octeon_boot_desc_ptr->cvmx_desc_vaddr);
+	cvmx_bootmem_init(cvmx_phys_to_ptr(octeon_bootinfo->phy_mem_desc_addr));
+
+	spin_lock_init(&octeon_led_lock);
+	/* Only enable the LED controller if we're running on a CN38XX, CN58XX,
+	   or CN56XX. The CN30XX and CN31XX don't have an LED controller */
+	if (!octeon_is_simulation() &&
+	    octeon_has_feature(OCTEON_FEATURE_LED_CONTROLLER)) {
+		cvmx_write_csr(CVMX_LED_EN, 0);
+		cvmx_write_csr(CVMX_LED_PRT, 0);
+		cvmx_write_csr(CVMX_LED_DBG, 0);
+		cvmx_write_csr(CVMX_LED_PRT_FMT, 0);
+		cvmx_write_csr(CVMX_LED_UDD_CNTX(0), 32);
+		cvmx_write_csr(CVMX_LED_UDD_CNTX(1), 32);
+		cvmx_write_csr(CVMX_LED_UDD_DATX(0), 0);
+		cvmx_write_csr(CVMX_LED_UDD_DATX(1), 0);
+		cvmx_write_csr(CVMX_LED_EN, 1);
+	}
+#if CONFIG_CAVIUM_RESERVE32
+	{
+		int64_t addr = -1;
+		/* We need to temporarily allocate all memory in the reserve32
+		   region. This makes sure the kernel doesn't allocate this
+		   memory when it is getting memory from the bootloader. Later,
+		   after the memory allocations are complete, the reserve32
+		   will be freed */
+#ifdef CONFIG_CAVIUM_RESERVE32_USE_WIRED_TLB
+		if (CONFIG_CAVIUM_RESERVE32 & 0x1ff)
+			printk("CAVIUM_RESERVE32 isn't a multiple of 512MB. This is required if CAVIUM_RESERVE32_USE_WIRED_TLB is set\n");
+		else
+			addr = cvmx_bootmem_phy_named_block_alloc
+				(CONFIG_CAVIUM_RESERVE32 << 20, 0, 0, 512 << 20,
+				 "CAVIUM_RESERVE32", 0);
+#else
+		/* Allocate memory for RESERVED32 aligned on 2MB boundary. This
+		   is in case we later use hugetlb entries with it */
+		addr = cvmx_bootmem_phy_named_block_alloc
+			(CONFIG_CAVIUM_RESERVE32 << 20, 0, 0, 2 << 20,
+			 "CAVIUM_RESERVE32", 0);
+#endif
+		if (addr < 0)
+			printk("Failed to allocate CAVIUM_RESERVE32 memory area\n");
+		else
+			octeon_reserve32_memory = addr;
+
+	}
+#endif
+
+#ifdef CONFIG_CAVIUM_OCTEON_LOCK_L2
+	if (cvmx_read_csr(CVMX_L2D_FUS3) & (3ull << 34)) {
+		printk("Skipping L2 locking due to reduced L2 cache size\n");
+	} else {
+		extern asmlinkage void handle_int(void);
+		extern asmlinkage void plat_irq_dispatch(void);
+		uint32_t ebase = read_c0_ebase() & 0x3ffff000;
+#ifdef CONFIG_CAVIUM_OCTEON_LOCK_L2_TLB
+		cvmx_l2c_lock_mem_region(ebase, 0x100);	/* TLB refill */
+#endif
+#ifdef CONFIG_CAVIUM_OCTEON_LOCK_L2_EXCEPTION
+		cvmx_l2c_lock_mem_region(ebase + 0x180, 0x80);	/* General
+								   exception */
+#endif
+#ifdef CONFIG_CAVIUM_OCTEON_LOCK_L2_LOW_LEVEL_INTERRUPT
+		cvmx_l2c_lock_mem_region(ebase + 0x200, 0x80);	/* Interrupt
+								   handler */
+#endif
+#ifdef CONFIG_CAVIUM_OCTEON_LOCK_L2_INTERRUPT
+		cvmx_l2c_lock_mem_region(__pa_symbol(handle_int), 0x100);
+		cvmx_l2c_lock_mem_region(__pa_symbol(plat_irq_dispatch), 0x80);
+#endif
+#ifdef CONFIG_CAVIUM_OCTEON_LOCK_L2_MEMCPY
+		cvmx_l2c_lock_mem_region(__pa_symbol(memcpy), 0x480);
+#endif
+	}
+#endif
+
+	sysinfo = cvmx_sysinfo_get();
+	memset(sysinfo, 0, sizeof(*sysinfo));
+	sysinfo->system_dram_size = octeon_bootinfo->dram_size << 20;
+	sysinfo->phy_mem_desc_ptr =
+		cvmx_phys_to_ptr(octeon_bootinfo->phy_mem_desc_addr);
+	sysinfo->core_mask = octeon_bootinfo->core_mask;
+	sysinfo->exception_base_addr = octeon_bootinfo->exception_base_addr;
+	sysinfo->cpu_clock_hz = octeon_bootinfo->eclock_hz;
+	sysinfo->dram_data_rate_hz = octeon_bootinfo->dclock_hz * 2;
+	sysinfo->board_type = octeon_bootinfo->board_type;
+	sysinfo->board_rev_major = octeon_bootinfo->board_rev_major;
+	sysinfo->board_rev_minor = octeon_bootinfo->board_rev_minor;
+	memcpy(sysinfo->mac_addr_base, octeon_bootinfo->mac_addr_base,
+	       sizeof(sysinfo->mac_addr_base));
+	sysinfo->mac_addr_count = octeon_bootinfo->mac_addr_count;
+	memcpy(sysinfo->board_serial_number,
+	       octeon_bootinfo->board_serial_number,
+	       sizeof(sysinfo->board_serial_number));
+	sysinfo->compact_flash_common_base_addr =
+		octeon_bootinfo->compact_flash_common_base_addr;
+	sysinfo->compact_flash_attribute_base_addr =
+		octeon_bootinfo->compact_flash_attribute_base_addr;
+	sysinfo->led_display_base_addr = octeon_bootinfo->led_display_base_addr;
+	sysinfo->dfa_ref_clock_hz = octeon_bootinfo->dfa_ref_clock_hz;
+	sysinfo->bootloader_config_flags = octeon_bootinfo->config_flags;
+}
+
+
+#ifdef CONFIG_CAVIUM_RESERVE32_USE_WIRED_TLB
+/**
+ * Called on every core to setup the wired tlb entry needed
+ * if CONFIG_CAVIUM_RESERVE32_USE_WIRED_TLB is set.
+ *
+ * @param unused
+ */
+static void octeon_hal_setup_per_cpu_reserved32(void *unused)
+{
+	/* The config has selected to wire the reserve32 memory for all
+	   userspace applications. We need to put a wired TLB entry in for each
+	   512MB of reserve32 memory. We only handle double 256MB pages here,
+	   so reserve32 must be multiple of 512MB */
+	extern void add_wired_entry(unsigned long entrylo0,
+				    unsigned long entrylo1,
+				    unsigned long entryhi,
+				    unsigned long pagemask);
+	uint32_t size = CONFIG_CAVIUM_RESERVE32;
+	uint32_t entrylo0 =
+		0x7 | ((octeon_reserve32_memory & ((1ul << 40) - 1)) >> 6);
+	uint32_t entrylo1 = entrylo0 + (256 << 14);
+	uint32_t entryhi = (0x80000000UL - (CONFIG_CAVIUM_RESERVE32 << 20));
+	while (size >= 512) {
+		// printk("CPU%d: Adding double wired TLB entry for 0x%lx\n",
+		// smp_processor_id(), entryhi);
+		add_wired_entry(entrylo0, entrylo1, entryhi, PM_256M);
+		entrylo0 += 512 << 14;
+		entrylo1 += 512 << 14;
+		entryhi += 512 << 20;
+		size -= 512;
+	}
+}
+#endif				/* CONFIG_CAVIUM_RESERVE32_USE_WIRED_TLB */
+
+/**
+ * Called to release the named block which was used to made sure
+ * that nobody used the memory for something else during
+ * init. Now we'll free it so userspace apps can use this
+ * memory region with bootmem_alloc.
+ *
+ * This function is called only once from prom_free_prom_memory().
+ */
+void octeon_hal_setup_reserved32(void)
+{
+#ifdef CONFIG_CAVIUM_RESERVE32_USE_WIRED_TLB
+	on_each_cpu(octeon_hal_setup_per_cpu_reserved32, NULL, 0, 1);
+#endif
+}
+
+
+/**
+ * Poweroff the Octeon board if possible.
+ */
+void octeon_poweroff(void)
+{
+	switch (octeon_bootinfo->board_type) {
+	case CVMX_BOARD_TYPE_NAO38:
+		/* Driving a 1 to GPIO 12 shuts off this board */
+		cvmx_write_csr(CVMX_GPIO_BIT_CFGX(12), 1);
+		cvmx_write_csr(CVMX_GPIO_TX_SET, 0x1000);
+		break;
+	default:
+		octeon_write_lcd("PowerOff");
+		break;
+	}
+}
+
+
+/**
+ * Enable access to Octeon's COP2 crypto hardware for kernel use.
+ * Wrap any crypto operations in calls to
+ * octeon_crypto_enable/disable in order to make sure the state of
+ * COP2 isn't corrupted if userspace is also performing hardware
+ * crypto operations. Allocate the state parameter on the stack.
+ *
+ * @param state  State structure to store current COP2 state in
+ *
+ * @return Flags to be passed to octeon_crypto_disable()
+ */
+unsigned long octeon_crypto_enable(struct octeon_cop2_state *state)
+{
+	extern void octeon_cop2_save(struct octeon_cop2_state *);
+	int status;
+	unsigned long flags;
+
+	local_irq_save(flags);
+	status = read_c0_status();
+	write_c0_status(status | ST0_CU2);
+	if (KSTK_STATUS(current) & ST0_CU2)
+	{
+		octeon_cop2_save(&(current->thread.cp2));
+		KSTK_STATUS(current) &= ~ST0_CU2;
+		status &= ~ST0_CU2;
+	} else if (status & ST0_CU2)
+		octeon_cop2_save(state);
+	local_irq_restore(flags);
+	return status & ST0_CU2;
+}
+EXPORT_SYMBOL(octeon_crypto_enable);
+
+
+/**
+ * Disable access to Octeon's COP2 crypto hardware in the kernel.
+ * This must be called after an octeon_crypto_enable() before any
+ * context switch or return to userspace.
+ *
+ * @param state  COP2 state to restore
+ * @param flags  Return value from octeon_crypto_enable()
+ */
+void octeon_crypto_disable(struct octeon_cop2_state *state, unsigned long crypto_flags)
+{
+	extern void octeon_cop2_restore(struct octeon_cop2_state *);
+	unsigned long flags;
+
+	local_irq_save(flags);
+	if (crypto_flags & ST0_CU2)
+		octeon_cop2_restore(state);
+	else
+                write_c0_status(read_c0_status() & ~ST0_CU2);
+	local_irq_restore(flags);
+}
+EXPORT_SYMBOL(octeon_crypto_disable);
+
+
+
+/* Misc exports */
+EXPORT_SYMBOL(octeon_is_simulation);
+EXPORT_SYMBOL(octeon_bootinfo);
+EXPORT_SYMBOL(mips_hpt_frequency);
+#if CONFIG_CAVIUM_RESERVE32
+EXPORT_SYMBOL(octeon_reserve32_memory);
+#endif
+
+EXPORT_SYMBOL(octeon_get_clock_rate);
diff --git a/arch/mips/cavium-octeon/hal.h b/arch/mips/cavium-octeon/hal.h
new file mode 100644
index 0000000..5b04d94
--- /dev/null
+++ b/arch/mips/cavium-octeon/hal.h
@@ -0,0 +1,139 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2004-2007 Cavium Networks
+ */
+#ifndef __CAVIUM_OCTEON_HAL_H
+#define __CAVIUM_OCTEON_HAL_H
+
+#include "octeon-hal-read-write.h"
+
+#ifndef __BYTE_ORDER
+#  error "__BYTE_ORDER not set"
+#endif
+#ifndef __BIG_ENDIAN
+#  error "__BIG_ENDIAN not set"
+#endif
+
+extern uint64_t octeon_bootmem_alloc_range_phys(uint64_t size,
+						uint64_t alignment,
+						uint64_t min_addr,
+						uint64_t max_addr,
+						int do_locking);
+extern void *octeon_bootmem_alloc(uint64_t size, uint64_t alignment,
+				  int do_locking);
+extern void *octeon_bootmem_alloc_range(uint64_t size, uint64_t alignment,
+					uint64_t min_addr, uint64_t max_addr,
+					int do_locking);
+extern void *octeon_bootmem_alloc_named(uint64_t size, uint64_t alignment,
+					char *name);
+extern void *octeon_bootmem_alloc_named_range(uint64_t size, uint64_t min_addr,
+					      uint64_t max_addr, uint64_t align,
+					      char *name);
+extern void *octeon_bootmem_alloc_named_address(uint64_t size, uint64_t address,
+						char *name);
+extern int octeon_bootmem_free_named(char *name);
+extern void octeon_bootmem_lock(void);
+extern void octeon_bootmem_unlock(void);
+
+extern int octeon_is_simulation(void);
+extern int octeon_is_pci_host(void);
+extern int octeon_usb_is_ref_clk(void);
+extern uint64_t octeon_get_clock_rate(void);
+extern const char *octeon_board_type_string(void);
+extern const char *octeon_get_pci_interrupts(void);
+extern int octeon_get_southbridge_interrupt(void);
+extern int octeon_get_boot_coremask(void);
+extern int octeon_get_boot_num_arguments(void);
+extern const char *octeon_get_boot_argument(int arg);
+extern void octeon_hal_setup_reserved32(void);
+extern unsigned long octeon_crypto_enable(struct octeon_cop2_state *state);
+extern void octeon_crypto_disable(struct octeon_cop2_state *state, unsigned long flags);
+
+typedef union {
+	uint64_t u64;
+	struct {
+		uint64_t tlbbist:1;	    /**< RO 1 = BIST fail, 0 = BIST pass */
+		uint64_t l1cbist:1;	    /**< RO 1 = BIST fail, 0 = BIST pass */
+		uint64_t l1dbist:1;	    /**< RO 1 = BIST fail, 0 = BIST pass */
+		uint64_t dcmbist:1;	    /**< RO 1 = BIST fail, 0 = BIST pass */
+		uint64_t ptgbist:1;	    /**< RO 1 = BIST fail, 0 = BIST pass */
+		uint64_t wbfbist:1;	    /**< RO 1 = BIST fail, 0 = BIST pass */
+		uint64_t reserved:22;	    /**< Reserved */
+		uint64_t dismarkwblongto:1; /**< R/W If set, marked write-buffer entries time out the same as
+                                                as other entries; if clear, marked write-buffer entries use the
+                                                maximum timeout. */
+		uint64_t dismrgclrwbto:1;   /**< R/W If set, a merged store does not clear the write-buffer entry
+                                                timeout state. */
+		uint64_t iobdmascrmsb:2;    /**< R/W Two bits that are the MSBs of the resultant CVMSEG LM word
+                                                location for an IOBDMA. The other 8 bits come from the SCRADDR
+                                                field of the IOBDMA. */
+		uint64_t syncwsmarked:1;    /**< R/W If set, SYNCWS and SYNCS only order marked stores; if clear,
+                                                SYNCWS and SYNCS only order unmarked stores. SYNCWSMARKED has no
+                                                effect when DISSYNCWS is set. */
+		uint64_t dissyncws:1;	    /**< R/W If set, SYNCWS acts as SYNCW and SYNCS acts as SYNC. */
+		uint64_t diswbfst:1;	    /**< R/W If set, no stall happens on write buffer full. */
+		uint64_t xkmemenas:1;	    /**< R/W If set (and SX set), supervisor-level loads/stores can use
+                                                XKPHYS addresses with VA<48>==0 */
+		uint64_t xkmemenau:1;	    /**< R/W If set (and UX set), user-level loads/stores can use XKPHYS
+                                                addresses with VA<48>==0 */
+		uint64_t xkioenas:1;	    /**< R/W If set (and SX set), supervisor-level loads/stores can use
+                                                XKPHYS addresses with VA<48>==1 */
+		uint64_t xkioenau:1;	    /**< R/W If set (and UX set), user-level loads/stores can use XKPHYS
+                                                addresses with VA<48>==1 */
+		uint64_t allsyncw:1;	    /**< R/W If set, all stores act as SYNCW (NOMERGE must be set when
+                                                this is set) RW, reset to 0. */
+		uint64_t nomerge:1;	    /**< R/W If set, no stores merge, and all stores reach the coherent
+                                                bus in order. */
+		uint64_t didtto:2;	    /**< R/W Selects the bit in the counter used for DID time-outs
+                                                0 = 231, 1 = 230, 2 = 229, 3 = 214. Actual time-out is between
+                                                1� and 2� this interval. For example, with DIDTTO=3, expiration
+                                                interval is between 16K and 32K. */
+		uint64_t csrckalwys:1;	    /**< R/W If set, the (mem) CSR clock never turns off. */
+		uint64_t mclkalwys:1;	    /**< R/W If set, mclk never turns off. */
+		uint64_t wbfltime:3;	    /**< R/W Selects the bit in the counter used for write buffer flush
+                                                time-outs (WBFLT+11) is the bit position in an internal counter
+                                                used to determine expiration. The write buffer expires between
+                                                1� and 2� this interval. For example, with WBFLT = 0, a write
+                                                buffer expires between 2K and 4K cycles after the write buffer
+                                                entry is allocated. */
+		uint64_t istrnol2:1;	    /**< R/W If set, do not put Istream in the L2 cache. */
+		uint64_t wbthresh:4;	    /**< R/W The write buffer threshold. */
+		uint64_t reserved2:2;	    /**< Reserved */
+		uint64_t cvmsegenak:1;	    /**< R/W If set, CVMSEG is available for loads/stores in kernel/debug mode. */
+		uint64_t cvmsegenas:1;	    /**< R/W If set, CVMSEG is available for loads/stores in supervisor mode. */
+		uint64_t cvmsegenau:1;	    /**< R/W If set, CVMSEG is available for loads/stores in user mode. */
+		uint64_t lmemsz:6;	    /**< R/W Size of local memory in cache blocks, 54 (6912 bytes) is max legal value. */
+	} s;
+} octeon_cvmemctl_t;
+
+static inline void octeon_led_write(int bank, uint32_t data)
+{
+	cvmx_write_csr(CVMX_LED_UDD_DATX(bank), data);
+}
+
+static inline uint32_t octeon_led_read(int bank)
+{
+	return cvmx_read_csr(CVMX_LED_UDD_DATX(bank));
+}
+
+static inline void octeon_led_set(int bank, int bit)
+{
+	cvmx_write_csr(CVMX_LED_UDD_DAT_SETX(bank), 1 << bit);
+}
+
+static inline void octeon_led_clear(int bank, int bit)
+{
+	cvmx_write_csr(CVMX_LED_UDD_DAT_CLRX(bank), 1 << bit);
+}
+
+extern void octeon_write_lcd(const char *s);
+extern void octeon_check_cpu_bist(void);
+extern void octeon_hal_init(void);
+extern int octeon_get_boot_uart(void);
+extern int octeon_get_boot_debug_flag(void);
+extern void octeon_poweroff(void);
+
+#endif
diff --git a/arch/mips/cavium-octeon/i8259.c b/arch/mips/cavium-octeon/i8259.c
new file mode 100644
index 0000000..fc57a1e
--- /dev/null
+++ b/arch/mips/cavium-octeon/i8259.c
@@ -0,0 +1,178 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2006-2007 Cavium Networks
+ */
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/pci.h>
+#include <linux/interrupt.h>
+#include "hal.h"
+
+#define SLAVE           (0xa0 - 0x20)
+
+/* Initialization Command Word 1 (ICW1 address 0x20 or 0xa0) */
+/* 7:5 Interrupt Vector Addresses for MCS-80/85 Mode. */
+#define ICW1_ADDRESS    0x20
+#define ICW1            0x10	// 4 Must be set to 1 for ICW1
+#define ICW1_LEVEL_TRIG (1<<3)	// 3 1 Level Triggered Interrupts, 0 Edge
+				// Triggered Interrupts
+#define ICW1_INTERVAL4  (1<<2)	// 2 1 Call Address Interval of 4, 0 Call
+				// Address Interval of 8
+#define ICW1_SINGLE_PIC (1<<1)	// 1 1 Single PIC, 0 Cascaded PICs
+#define ICW1_NEED_ICW4  (1<<0)	// 0 1 Will be Sending ICW4, 0 Don't need ICW4
+
+/* Initialization Command Word 2 (ICW2 address 0x21 or 0xa1) */
+#define ICW2_ADDRESS    0x21
+/* Bit 8086/8080 Mode MCS 80/85 Mode 7 I7 A15 6 I6 A14 5 I5 A13 4 I4 A12 3 I3
+   A11 2 - A10 1 - A9 0 - A8 */
+
+/* Initialization Command Word 3 (ICW3 address 0x21 or 0xa1) For the master,
+   this is a bitfield saying which line is hooked to a slave. For a slave, this
+   is the slave's ID, the line it is hooked to */
+#define ICW3_ADDRESS    0x21
+
+/* Initialization Command Word 4 (ICW4 address 0x21 or 0xa1) */
+/* Bits 7-5 are reserved */
+#define ICW4_ADDRESS        0x21
+#define ICW4_FULLY_NESTED   (1<<4)	// 4 1 Special Fully Nested Mode, 0 Not
+					// Special Fully Nested Mode
+#define ICW4_BUFFERED       (3<<2)	// 3 1 Buffered Mode, 0 Unbuffered
+#define ICW4_MASTER         (2<<2)	// 2 1 Master, 0 Slave
+#define ICW4_AUTO_EOI       (1<<1)	// 1 1 Auto EOI, 0 Normal EOI
+#define ICW4_8086           (1<<0)	// 0 1 8086/8080 Mode, 0 MCS-80/85
+
+/* Operation Control Word 1 (OCW1 address 0x21 or 0xa1) This is a bitmask for
+   each interrupt */
+#define OCW1_ADDRESS    0x21
+
+/* Operation Control Word 2 (OCW2 address 0x20 or 0xa0) */
+#define OCW2_ADDRESS    0x20
+#define OCW2            0x00	// Bits 4:3 must be zero
+#define OCW2_ROTATE_AUTO_EOI_CLEAR  (0<<5)	// 7:5 000 Rotate in Auto EOI
+						// Mode (Clear)
+#define OCW2_NON_SPECIFIC_EOI       (1<<5)	// 001 Non Specific EOI
+#define OCW2_NOP                    (2<<5)	// 010 NOP
+#define OCW2_SPECIFIC_EOI           (3<<5)	// 011 Specific EOI
+#define OCW2_ROTATE_AUTO_EOI_SET    (4<<5)	// 100 Rotate in Auto EOI Mode
+						// (Set)
+#define OCW2_ROTATE_NON_SPECIFIC_EOI (5<<5)	// 101 Rotate on Non-Specific
+						// EOI
+#define OCW2_SET_PRIORITY           (6<<5)	// 110 Set Priority Command
+						// (Use Bits 2:0)
+#define OCW2_ROTATE_SPECIFIC_EOI    (7<<5)	// 111 Rotate on Specific EOI
+						// (Use Bits 2:0)
+
+/* Operation Control Word 3 (OCW3 address 0x20 or 0xa0) */
+/* Bit 7 Must be set to 0 */
+#define OCW3_ADDRESS    0x20
+#define OCW3            0x08	// 4:3 Must be set to 01
+#define OCW3_RESET_SPECIAL_MASK (2<<5)	// 6:5 00 Reserved, 01 Reserved, 10
+					// Reset Special Mask
+#define OCW3_SET_SPECIAL_MASK   (3<<5)	// 11 Set Special Mask
+#define OCW3_POLL               (1<<2)	// 2 1 Poll Command, 0 No Poll Command
+#define OCW3_READ_IRR           (2<<0)	// 1:0 00 Reserved, 01 Reserved, 10
+					// Next Read Returns Interrupt Request
+					// Register
+#define OCW3_READ_ISR           (3<<0)	// 11 Next Read Returns In-Service
+					// Register
+
+
+static irqreturn_t octeon_i8259_interrupt(int cpl, void *dev_id)
+{
+	u8 master_isr;
+	u8 slave_isr;
+
+	outb(OCW3 | OCW3_POLL, OCW3_ADDRESS);
+	master_isr = inb(OCW3_ADDRESS);
+	if (master_isr & 0x80) {	/* Top bit is set if the master
+					   requested the interrupt */
+		if ((master_isr & 0x7) == 2) {
+			outb(OCW3 | OCW3_POLL, OCW3_ADDRESS + SLAVE);
+			slave_isr = inb(OCW3_ADDRESS + SLAVE);
+			if (slave_isr & 0x80) {	/* Top bit is set if the slave
+						   requested the interrupt */
+				int irq = (slave_isr & 7) + OCTEON_IRQ_I8259S0;
+
+				// printk("8259: Interrupt %d from slave\n",
+				// irq);
+				if (irq_desc[irq].action)
+					do_IRQ(irq);
+
+				/* Ack the slave */
+				outb(OCW2 | OCW2_SPECIFIC_EOI | (slave_isr & 7),
+				     OCW2_ADDRESS + SLAVE);
+			} else
+				printk("8259: Spurious interrupt from master for slave\n");
+		} else {
+			int irq = (master_isr & 7) + OCTEON_IRQ_I8259M0;
+			// printk("8259: Interrupt %d from master\n", irq);
+			if (irq_desc[irq].action)
+				do_IRQ(irq);
+		}
+
+		/* Ack the master */
+		outb(OCW2 | OCW2_SPECIFIC_EOI | (master_isr & 7), OCW2_ADDRESS);
+
+		return IRQ_HANDLED;
+	} else {
+		printk("8259: Spurious interrupt from master\n");
+		return IRQ_NONE;
+	}
+}
+
+void octeon_i8259_setup(int irq_line)
+{
+	printk("8259: Initializing\n");
+
+	/* Setup the Master 8259 */
+	outb(ICW1 | ICW1_NEED_ICW4, ICW1_ADDRESS);	/* Begin the init
+							   sequence */
+	outb(0, ICW2_ADDRESS);	/* Master base address is zero, interrupts 0-7 */
+	outb(1 << 2, ICW3_ADDRESS);	/* Slave is connected to line 2 */
+	outb(ICW4_FULLY_NESTED | ICW4_MASTER | ICW4_BUFFERED | ICW4_8086, ICW4_ADDRESS);	/* Set
+												   the
+												   mode
+												   to
+												   buffered
+												   with
+												   edge
+												   triggering
+												 */
+	outb(OCW3 | OCW3_READ_ISR, OCW3_ADDRESS);	/* Read ISR */
+
+	/* Setup the Slave 8259 */
+	outb(ICW1 | ICW1_NEED_ICW4, ICW1_ADDRESS + SLAVE);	/* Begin the
+								   init
+								   sequence */
+	outb(8, ICW2_ADDRESS + SLAVE);	/* Slave base address is 8, interrupts
+					   8-15 */
+	outb(2, ICW3_ADDRESS + SLAVE);	/* Slave is connected to line 2 */
+	outb(ICW4_BUFFERED | ICW4_8086, ICW4_ADDRESS + SLAVE);	/* Set the mode
+								   to buffered
+								   with edge
+								   triggering */
+	outb(OCW3 | OCW3_READ_ISR, OCW3_ADDRESS + SLAVE);	/* Read ISR */
+
+	/* Set interrupt mask to disable all interrupts */
+	outb(0xfb, OCW1_ADDRESS);
+	outb(0xff, OCW1_ADDRESS + SLAVE);
+
+	/* Setup the GPIO pin if the interrupt is hooked to it */
+	if ((irq_line >= 24) && (irq_line <= 39)) {
+		printk("8259: Setting GPIO %d for the interrupt\n",
+		       irq_line - 24);
+		cvmx_write_csr(CVMX_GPIO_BIT_CFGX(irq_line - 24), 0x114);
+		request_irq(irq_line, octeon_i8259_interrupt, SA_SHIRQ, "8259",
+			    octeon_i8259_interrupt);
+	} else if ((irq_line >= 44) && (irq_line <= 47)) {
+		printk("8259: Using PCI INT-%c\n", irq_line - 44 + 'A');
+		request_irq(irq_line, octeon_i8259_interrupt, SA_SHIRQ, "8259",
+			    octeon_i8259_interrupt);
+	} else {
+		panic("8259: Don't know how to setup the interrupt IRQ %d\n",
+		      irq_line);
+	}
+}
diff --git a/arch/mips/cavium-octeon/irq.c b/arch/mips/cavium-octeon/irq.c
new file mode 100644
index 0000000..7b71b51
--- /dev/null
+++ b/arch/mips/cavium-octeon/irq.c
@@ -0,0 +1,59 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2004-2008 Cavium Networks
+ */
+#include "linux/irq.h"
+#include "linux/hardirq.h"
+#include "linux/kernel_stat.h"
+#include "hal.h"
+
+asmlinkage void plat_irq_dispatch(void)
+{
+	const unsigned long core_id = cvmx_get_core_num();
+	const uint64_t ciu_sum0_address = CVMX_CIU_INTX_SUM0(core_id * 2);
+	const uint64_t ciu_en0_address = CVMX_CIU_INTX_EN0(core_id * 2);
+	const uint64_t ciu_sum1_address = CVMX_CIU_INT_SUM1;
+	const uint64_t ciu_en1_address = CVMX_CIU_INTX_EN1(core_id * 2 + 1);
+	unsigned long cop0_cause;
+	unsigned long cop0_status;
+	uint64_t ciu_en;
+	uint64_t ciu_sum;
+
+	while (1) {
+		cop0_cause = read_c0_cause();
+		cop0_status = read_c0_status();
+		cop0_cause &= cop0_status;
+		cop0_cause &= ST0_IM;
+
+		if (unlikely(cop0_cause & STATUSF_IP2)) {
+			asm volatile ("ld	%[sum], 0(%[sum_address])\n"
+				      "ld	%[en], 0(%[en_address])\n":
+				      [sum] "=r"(ciu_sum),[en] "=r"(ciu_en)
+				      :[sum_address] "r"(ciu_sum0_address),
+				      [en_address] "r"(ciu_en0_address));
+			ciu_sum &= ciu_en;
+			if (likely(ciu_sum))
+				do_IRQ(fls64(ciu_sum) + OCTEON_IRQ_WORKQ0 - 1);
+			else
+				spurious_interrupt();
+		} else if (unlikely(cop0_cause & STATUSF_IP3)) {
+			asm volatile ("ld	%[sum], 0(%[sum_address])\n"
+				      "ld	%[en], 0(%[en_address])\n":
+				      [sum] "=r"(ciu_sum),[en] "=r"(ciu_en)
+				      :[sum_address] "r"(ciu_sum1_address),
+				      [en_address] "r"(ciu_en1_address));
+			ciu_sum &= ciu_en;
+			if (likely(ciu_sum))
+				do_IRQ(fls64(ciu_sum) + OCTEON_IRQ_WDOG0 - 1);
+			else
+				spurious_interrupt();
+		} else if (likely(cop0_cause)) {
+			do_IRQ(fls(cop0_cause) - 9);
+		} else {
+			break;
+		}
+	}
+}
diff --git a/arch/mips/cavium-octeon/octeon-memcpy.S b/arch/mips/cavium-octeon/octeon-memcpy.S
new file mode 100644
index 0000000..e083d5c
--- /dev/null
+++ b/arch/mips/cavium-octeon/octeon-memcpy.S
@@ -0,0 +1,521 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Unified implementation of memcpy, memmove and the __copy_user backend.
+ *
+ * Copyright (C) 1998, 99, 2000, 01, 2002 Ralf Baechle (ralf@gnu.org)
+ * Copyright (C) 1999, 2000, 01, 2002 Silicon Graphics, Inc.
+ * Copyright (C) 2002 Broadcom, Inc.
+ *   memcpy/copy_user author: Mark Vandevoorde
+ *
+ * Mnemonic names for arguments to memcpy/__copy_user
+ */
+
+#include <asm/asm.h>
+#include <asm/asm-offsets.h>
+#include <asm/regdef.h>
+
+#define dst a0
+#define src a1
+#define len a2
+
+/*
+ * Spec
+ *
+ * memcpy copies len bytes from src to dst and sets v0 to dst.
+ * It assumes that
+ *   - src and dst don't overlap
+ *   - src is readable
+ *   - dst is writable
+ * memcpy uses the standard calling convention
+ *
+ * __copy_user copies up to len bytes from src to dst and sets a2 (len) to
+ * the number of uncopied bytes due to an exception caused by a read or write.
+ * __copy_user assumes that src and dst don't overlap, and that the call is
+ * implementing one of the following:
+ *   copy_to_user
+ *     - src is readable  (no exceptions when reading src)
+ *   copy_from_user
+ *     - dst is writable  (no exceptions when writing dst)
+ * __copy_user uses a non-standard calling convention; see
+ * include/asm-mips/uaccess.h
+ *
+ * When an exception happens on a load, the handler must
+ # ensure that all of the destination buffer is overwritten to prevent
+ * leaking information to user mode programs.
+ */
+
+/*
+ * Implementation
+ */
+
+/*
+ * The exception handler for loads requires that:
+ *  1- AT contain the address of the byte just past the end of the source
+ *     of the copy,
+ *  2- src_entry <= src < AT, and
+ *  3- (dst - src) == (dst_entry - src_entry),
+ * The _entry suffix denotes values when __copy_user was called.
+ *
+ * (1) is set up up by uaccess.h and maintained by not writing AT in copy_user
+ * (2) is met by incrementing src by the number of bytes copied
+ * (3) is met by not doing loads between a pair of increments of dst and src
+ *
+ * The exception handlers for stores adjust len (if necessary) and return.
+ * These handlers do not need to overwrite any data.
+ *
+ * For __rmemcpy and memmove an exception is always a kernel bug, therefore
+ * they're not protected.
+ */
+
+#define EXC(inst_reg,addr,handler)		\
+9:	inst_reg, addr;				\
+	.section __ex_table,"a";		\
+	PTR	9b, handler;			\
+	.previous
+
+/*
+ * Only on the 64-bit kernel we can made use of 64-bit registers.
+ */
+#ifdef CONFIG_64BIT
+#define USE_DOUBLE
+#endif
+
+#ifdef USE_DOUBLE
+
+#define LOAD   ld
+#define LOADL  ldl
+#define LOADR  ldr
+#define STOREL sdl
+#define STORER sdr
+#define STORE  sd
+#define ADD    daddu
+#define SUB    dsubu
+#define SRL    dsrl
+#define SRA    dsra
+#define SLL    dsll
+#define SLLV   dsllv
+#define SRLV   dsrlv
+#define NBYTES 8
+#define LOG_NBYTES 3
+
+/*
+ * As we are sharing code base with the mips32 tree (which use the o32 ABI
+ * register definitions). We need to redefine the register definitions from
+ * the n64 ABI register naming to the o32 ABI register naming.
+ */
+#undef t0
+#undef t1
+#undef t2
+#undef t3
+#define t0	$8
+#define t1	$9
+#define t2	$10
+#define t3	$11
+#define t4	$12
+#define t5	$13
+#define t6	$14
+#define t7	$15
+
+#else
+
+#define LOAD   lw
+#define LOADL  lwl
+#define LOADR  lwr
+#define STOREL swl
+#define STORER swr
+#define STORE  sw
+#define ADD    addu
+#define SUB    subu
+#define SRL    srl
+#define SLL    sll
+#define SRA    sra
+#define SLLV   sllv
+#define SRLV   srlv
+#define NBYTES 4
+#define LOG_NBYTES 2
+
+#endif /* USE_DOUBLE */
+
+#ifdef CONFIG_CPU_LITTLE_ENDIAN
+#define LDFIRST LOADR
+#define LDREST  LOADL
+#define STFIRST STORER
+#define STREST  STOREL
+#define SHIFT_DISCARD SLLV
+#else
+#define LDFIRST LOADL
+#define LDREST  LOADR
+#define STFIRST STOREL
+#define STREST  STORER
+#define SHIFT_DISCARD SRLV
+#endif
+
+#define FIRST(unit) ((unit)*NBYTES)
+#define REST(unit)  (FIRST(unit)+NBYTES-1)
+#define UNIT(unit)  FIRST(unit)
+
+#define ADDRMASK (NBYTES-1)
+
+	.text
+	.set	noreorder
+	.set	noat
+
+/*
+ * A combined memcpy/__copy_user
+ * __copy_user sets len to 0 for success; else to an upper bound of
+ * the number of uncopied bytes.
+ * memcpy sets v0 to dst.
+ */
+	.align	5
+LEAF(memcpy)					/* a0=dst a1=src a2=len */
+	move	v0, dst				/* return value */
+__memcpy:
+FEXPORT(__copy_user)
+	/*
+	 * Note: dst & src may be unaligned, len may be 0
+	 * Temps
+	 */
+	#
+	# Octeon doesn't care if the destination is unaligned. The hardware
+	# can fix it faster than we can special case the assembly.
+	#
+	pref	0, 0(src)
+	sltu	t0, len, NBYTES		# Check if < 1 word
+	bnez	t0, copy_bytes_checklen
+	 and	t0, src, ADDRMASK	# Check if src unaligned
+	bnez	t0, src_unaligned
+	 sltu	t0, len, 4*NBYTES	# Check if < 4 words
+	bnez	t0, less_than_4units
+	 sltu	t0, len, 8*NBYTES	# Check if < 8 words
+	bnez	t0, less_than_8units
+	 sltu	t0, len, 16*NBYTES	# Check if < 16 words
+	bnez	t0, cleanup_both_aligned
+	 sltu	t0, len, 128+1		# Check if len < 129
+	bnez	t0, 1f			# Skip prefetch if len is too short
+	 sltu	t0, len, 256+1		# Check if len < 257
+	bnez	t0, 1f			# Skip prefetch if len is too short
+	 pref	0, 128(src)		# We must not prefetch invalid addresses
+	#
+	# This is where we loop if there is more than 128 bytes left
+2:	pref	0, 256(src)		# We must not prefetch invalid addresses
+	#
+	# This is where we loop if we can't prefetch anymore
+1:
+EXC(	LOAD	t0, UNIT(0)(src),	l_exc)
+EXC(	LOAD	t1, UNIT(1)(src),	l_exc_copy)
+EXC(	LOAD	t2, UNIT(2)(src),	l_exc_copy)
+EXC(	LOAD	t3, UNIT(3)(src),	l_exc_copy)
+	SUB	len, len, 16*NBYTES
+EXC(	STORE	t0, UNIT(0)(dst),	s_exc_p16u)
+EXC(	STORE	t1, UNIT(1)(dst),	s_exc_p15u)
+EXC(	STORE	t2, UNIT(2)(dst),	s_exc_p14u)
+EXC(	STORE	t3, UNIT(3)(dst),	s_exc_p13u)
+EXC(	LOAD	t0, UNIT(4)(src),	l_exc_copy)
+EXC(	LOAD	t1, UNIT(5)(src),	l_exc_copy)
+EXC(	LOAD	t2, UNIT(6)(src),	l_exc_copy)
+EXC(	LOAD	t3, UNIT(7)(src),	l_exc_copy)
+EXC(	STORE	t0, UNIT(4)(dst),	s_exc_p12u)
+EXC(	STORE	t1, UNIT(5)(dst),	s_exc_p11u)
+EXC(	STORE	t2, UNIT(6)(dst),	s_exc_p10u)
+	ADD	src, src, 16*NBYTES
+EXC(	STORE	t3, UNIT(7)(dst),	s_exc_p9u)
+	ADD	dst, dst, 16*NBYTES
+EXC(	LOAD	t0, UNIT(-8)(src),	l_exc_copy)
+EXC(	LOAD	t1, UNIT(-7)(src),	l_exc_copy)
+EXC(	LOAD	t2, UNIT(-6)(src),	l_exc_copy)
+EXC(	LOAD	t3, UNIT(-5)(src),	l_exc_copy)
+EXC(	STORE	t0, UNIT(-8)(dst),	s_exc_p8u)
+EXC(	STORE	t1, UNIT(-7)(dst),	s_exc_p7u)
+EXC(	STORE	t2, UNIT(-6)(dst),	s_exc_p6u)
+EXC(	STORE	t3, UNIT(-5)(dst),	s_exc_p5u)
+EXC(	LOAD	t0, UNIT(-4)(src),	l_exc_copy)
+EXC(	LOAD	t1, UNIT(-3)(src),	l_exc_copy)
+EXC(	LOAD	t2, UNIT(-2)(src),	l_exc_copy)
+EXC(	LOAD	t3, UNIT(-1)(src),	l_exc_copy)
+EXC(	STORE	t0, UNIT(-4)(dst),	s_exc_p4u)
+EXC(	STORE	t1, UNIT(-3)(dst),	s_exc_p3u)
+EXC(	STORE	t2, UNIT(-2)(dst),	s_exc_p2u)
+EXC(	STORE	t3, UNIT(-1)(dst),	s_exc_p1u)
+	sltu	t0, len, 256+1		# See if we can prefetch more
+	beqz	t0, 2b
+	 sltu	t0, len, 128		# See if we can loop more time
+	beqz	t0, 1b
+	 nop
+	#
+	# Jump here if there are less than 16*NBYTES left.
+	#
+cleanup_both_aligned:
+	beqz	len, done
+	 sltu	t0, len, 8*NBYTES
+	bnez	t0, less_than_8units
+	 nop
+EXC(	LOAD	t0, UNIT(0)(src),	l_exc)
+EXC(	LOAD	t1, UNIT(1)(src),	l_exc_copy)
+EXC(	LOAD	t2, UNIT(2)(src),	l_exc_copy)
+EXC(	LOAD	t3, UNIT(3)(src),	l_exc_copy)
+	SUB	len, len, 8*NBYTES
+EXC(	STORE	t0, UNIT(0)(dst),	s_exc_p8u)
+EXC(	STORE	t1, UNIT(1)(dst),	s_exc_p7u)
+EXC(	STORE	t2, UNIT(2)(dst),	s_exc_p6u)
+EXC(	STORE	t3, UNIT(3)(dst),	s_exc_p5u)
+EXC(	LOAD	t0, UNIT(4)(src),	l_exc_copy)
+EXC(	LOAD	t1, UNIT(5)(src),	l_exc_copy)
+EXC(	LOAD	t2, UNIT(6)(src),	l_exc_copy)
+EXC(	LOAD	t3, UNIT(7)(src),	l_exc_copy)
+EXC(	STORE	t0, UNIT(4)(dst),	s_exc_p4u)
+EXC(	STORE	t1, UNIT(5)(dst),	s_exc_p3u)
+EXC(	STORE	t2, UNIT(6)(dst),	s_exc_p2u)
+EXC(	STORE	t3, UNIT(7)(dst),	s_exc_p1u)
+	ADD	src, src, 8*NBYTES
+	beqz	len, done
+	 ADD	dst, dst, 8*NBYTES
+	#
+	# Jump here if there are less than 8*NBYTES left.
+	#
+less_than_8units:
+	sltu	t0, len, 4*NBYTES
+	bnez	t0, less_than_4units
+	 nop
+EXC(	LOAD	t0, UNIT(0)(src),	l_exc)
+EXC(	LOAD	t1, UNIT(1)(src),	l_exc_copy)
+EXC(	LOAD	t2, UNIT(2)(src),	l_exc_copy)
+EXC(	LOAD	t3, UNIT(3)(src),	l_exc_copy)
+	SUB	len, len, 4*NBYTES
+EXC(	STORE	t0, UNIT(0)(dst),	s_exc_p4u)
+EXC(	STORE	t1, UNIT(1)(dst),	s_exc_p3u)
+EXC(	STORE	t2, UNIT(2)(dst),	s_exc_p2u)
+EXC(	STORE	t3, UNIT(3)(dst),	s_exc_p1u)
+	ADD	src, src, 4*NBYTES
+	beqz	len, done
+	 ADD	dst, dst, 4*NBYTES
+	#
+	# Jump here if there are less than 4*NBYTES left. This means
+	# we may need to copy up to 3 NBYTES words.
+	#
+less_than_4units:
+	sltu	t0, len, 1*NBYTES
+	bnez	t0, copy_bytes_checklen
+	 nop
+	#
+	# 1) Copy NBYTES, then check length again
+	#
+EXC(	LOAD	t0, 0(src),		l_exc)
+	SUB	len, len, NBYTES
+	sltu	t1, len, 8
+EXC(	STORE	t0, 0(dst),		s_exc_p1u)
+	ADD	src, src, NBYTES
+	bnez	t1, copy_bytes_checklen
+	 ADD	dst, dst, NBYTES
+	#
+	# 2) Copy NBYTES, then check length again
+	#
+EXC(	LOAD	t0, 0(src),		l_exc)
+	SUB	len, len, NBYTES
+	sltu	t1, len, 8
+EXC(	STORE	t0, 0(dst),		s_exc_p1u)
+	ADD	src, src, NBYTES
+	bnez	t1, copy_bytes_checklen
+	 ADD	dst, dst, NBYTES
+	#
+	# 3) Copy NBYTES, then check length again
+	#
+EXC(	LOAD	t0, 0(src),		l_exc)
+	SUB	len, len, NBYTES
+	ADD	src, src, NBYTES
+	ADD	dst, dst, NBYTES
+	b copy_bytes_checklen
+EXC(	 STORE	t0, -8(dst),		s_exc_p1u)
+
+src_unaligned:
+#define rem t8
+	SRL	t0, len, LOG_NBYTES+2    # +2 for 4 units/iter
+	beqz	t0, cleanup_src_unaligned
+	 and	rem, len, (4*NBYTES-1)   # rem = len % 4*NBYTES
+1:
+/*
+ * Avoid consecutive LD*'s to the same register since some mips
+ * implementations can't issue them in the same cycle.
+ * It's OK to load FIRST(N+1) before REST(N) because the two addresses
+ * are to the same unit (unless src is aligned, but it's not).
+ */
+EXC(	LDFIRST	t0, FIRST(0)(src),	l_exc)
+EXC(	LDFIRST	t1, FIRST(1)(src),	l_exc_copy)
+	SUB     len, len, 4*NBYTES
+EXC(	LDREST	t0, REST(0)(src),	l_exc_copy)
+EXC(	LDREST	t1, REST(1)(src),	l_exc_copy)
+EXC(	LDFIRST	t2, FIRST(2)(src),	l_exc_copy)
+EXC(	LDFIRST	t3, FIRST(3)(src),	l_exc_copy)
+EXC(	LDREST	t2, REST(2)(src),	l_exc_copy)
+EXC(	LDREST	t3, REST(3)(src),	l_exc_copy)
+	ADD	src, src, 4*NBYTES
+EXC(	STORE	t0, UNIT(0)(dst),	s_exc_p4u)
+EXC(	STORE	t1, UNIT(1)(dst),	s_exc_p3u)
+EXC(	STORE	t2, UNIT(2)(dst),	s_exc_p2u)
+EXC(	STORE	t3, UNIT(3)(dst),	s_exc_p1u)
+	bne	len, rem, 1b
+	 ADD	dst, dst, 4*NBYTES
+
+cleanup_src_unaligned:
+	beqz	len, done
+	 and	rem, len, NBYTES-1  # rem = len % NBYTES
+	beq	rem, len, copy_bytes
+	 nop
+1:
+EXC(	LDFIRST t0, FIRST(0)(src),	l_exc)
+EXC(	LDREST	t0, REST(0)(src),	l_exc_copy)
+	SUB	len, len, NBYTES
+EXC(	STORE	t0, 0(dst),		s_exc_p1u)
+	ADD	src, src, NBYTES
+	bne	len, rem, 1b
+	 ADD	dst, dst, NBYTES
+
+copy_bytes_checklen:
+	beqz	len, done
+	 nop
+copy_bytes:
+	/* 0 < len < NBYTES  */
+#define COPY_BYTE(N)			\
+EXC(	lb	t0, N(src), l_exc);	\
+	SUB	len, len, 1;		\
+	beqz	len, done;		\
+EXC(	 sb	t0, N(dst), s_exc_p1)
+
+	COPY_BYTE(0)
+	COPY_BYTE(1)
+#ifdef USE_DOUBLE
+	COPY_BYTE(2)
+	COPY_BYTE(3)
+	COPY_BYTE(4)
+	COPY_BYTE(5)
+#endif
+EXC(	lb	t0, NBYTES-2(src), l_exc)
+	SUB	len, len, 1
+	jr	ra
+EXC(	 sb	t0, NBYTES-2(dst), s_exc_p1)
+done:
+	jr	ra
+	 nop
+	END(memcpy)
+
+l_exc_copy:
+	/*
+	 * Copy bytes from src until faulting load address (or until a
+	 * lb faults)
+	 *
+	 * When reached by a faulting LDFIRST/LDREST, THREAD_BUADDR($28)
+	 * may be more than a byte beyond the last address.
+	 * Hence, the lb below may get an exception.
+	 *
+	 * Assumes src < THREAD_BUADDR($28)
+	 */
+	LOAD	t0, TI_TASK($28)
+	 nop
+	LOAD	t0, THREAD_BUADDR(t0)
+1:
+EXC(	lb	t1, 0(src),	l_exc)
+	ADD	src, src, 1
+	sb	t1, 0(dst)	# can't fault -- we're copy_from_user
+	bne	src, t0, 1b
+	 ADD	dst, dst, 1
+l_exc:
+	LOAD	t0, TI_TASK($28)
+	 nop
+	LOAD	t0, THREAD_BUADDR(t0)	# t0 is just past last good address
+	 nop
+	SUB	len, AT, t0		# len number of uncopied bytes
+	/*
+	 * Here's where we rely on src and dst being incremented in tandem,
+	 *   See (3) above.
+	 * dst += (fault addr - src) to put dst at first byte to clear
+	 */
+	ADD	dst, t0			# compute start address in a1
+	SUB	dst, src
+	/*
+	 * Clear len bytes starting at dst.  Can't call __bzero because it
+	 * might modify len.  An inefficient loop for these rare times...
+	 */
+	beqz	len, done
+	 SUB	src, len, 1
+1:	sb	zero, 0(dst)
+	ADD	dst, dst, 1
+	bnez	src, 1b
+	 SUB	src, src, 1
+	jr	ra
+	 nop
+
+
+#define SEXC(n)				\
+s_exc_p ## n ## u:			\
+	jr	ra;			\
+	 ADD	len, len, n*NBYTES
+
+SEXC(16)
+SEXC(15)
+SEXC(14)
+SEXC(13)
+SEXC(12)
+SEXC(11)
+SEXC(10)
+SEXC(9)
+SEXC(8)
+SEXC(7)
+SEXC(6)
+SEXC(5)
+SEXC(4)
+SEXC(3)
+SEXC(2)
+SEXC(1)
+
+s_exc_p1:
+	jr	ra
+	 ADD	len, len, 1
+s_exc:
+	jr	ra
+	 nop
+
+	.align	5
+LEAF(memmove)
+	ADD	t0, a0, a2
+	ADD	t1, a1, a2
+	sltu	t0, a1, t0			# dst + len <= src -> memcpy
+	sltu	t1, a0, t1			# dst >= src + len -> memcpy
+	and	t0, t1
+	beqz	t0, __memcpy
+	 move	v0, a0				/* return value */
+	beqz	a2, r_out
+	END(memmove)
+
+	/* fall through to __rmemcpy */
+LEAF(__rmemcpy)					/* a0=dst a1=src a2=len */
+	 sltu	t0, a1, a0
+	beqz	t0, r_end_bytes_up		# src >= dst
+	 nop
+	ADD	a0, a2				# dst = dst + len
+	ADD	a1, a2				# src = src + len
+
+r_end_bytes:
+	lb	t0, -1(a1)
+	SUB	a2, a2, 0x1
+	sb	t0, -1(a0)
+	SUB	a1, a1, 0x1
+	bnez	a2, r_end_bytes
+	 SUB	a0, a0, 0x1
+
+r_out:
+	jr	ra
+	 move	a2, zero
+
+r_end_bytes_up:
+	lb	t0, (a1)
+	SUB	a2, a2, 0x1
+	sb	t0, (a0)
+	ADD	a1, a1, 0x1
+	bnez	a2, r_end_bytes_up
+	 ADD	a0, a0, 0x1
+
+	jr	ra
+	 move	a2, zero
+	END(__rmemcpy)
diff --git a/arch/mips/cavium-octeon/octeon_info.c b/arch/mips/cavium-octeon/octeon_info.c
new file mode 100644
index 0000000..301e1a0
--- /dev/null
+++ b/arch/mips/cavium-octeon/octeon_info.c
@@ -0,0 +1,126 @@
+/*
+ * Simple /proc interface to Octeon Information
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2004-2007 Cavium Networks
+ */
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/fs.h>
+#include <linux/seq_file.h>
+#include <linux/proc_fs.h>
+#include "hal.h"
+#include "cvmx-app-init.h"
+
+/**
+ * User is reading /proc/octeon_info
+ *
+ * @param m
+ * @param v
+ * @return
+ */
+static int octeon_info_show(struct seq_file *m, void *v)
+{
+	extern cvmx_bootinfo_t *octeon_bootinfo;
+#if defined(CONFIG_CAVIUM_RESERVE32) && CONFIG_CAVIUM_RESERVE32
+	extern uint64_t octeon_reserve32_memory;
+#endif
+
+	seq_printf(m, "processor_id:        0x%x\n", read_c0_prid());
+	seq_printf(m, "boot_flags:          0x%x\n", octeon_bootinfo->flags);
+	seq_printf(m, "dram_size:           %u\n", octeon_bootinfo->dram_size);
+	seq_printf(m, "phy_mem_desc_addr:   0x%x\n",
+		   octeon_bootinfo->phy_mem_desc_addr);
+	seq_printf(m, "eclock_hz:           %u\n", octeon_bootinfo->eclock_hz);
+	seq_printf(m, "dclock_hz:           %u\n", octeon_bootinfo->dclock_hz);
+	seq_printf(m, "board_type:          %u\n", octeon_bootinfo->board_type);
+	seq_printf(m, "board_rev_major:     %u\n",
+		   octeon_bootinfo->board_rev_major);
+	seq_printf(m, "board_rev_minor:     %u\n",
+		   octeon_bootinfo->board_rev_minor);
+	seq_printf(m, "board_serial_number: %s\n",
+		   octeon_bootinfo->board_serial_number);
+	seq_printf(m, "mac_addr_base:       %02x:%02x:%02x:%02x:%02x:%02x\n",
+		   (int) octeon_bootinfo->mac_addr_base[0],
+		   (int) octeon_bootinfo->mac_addr_base[1],
+		   (int) octeon_bootinfo->mac_addr_base[2],
+		   (int) octeon_bootinfo->mac_addr_base[3],
+		   (int) octeon_bootinfo->mac_addr_base[4],
+		   (int) octeon_bootinfo->mac_addr_base[5]);
+	seq_printf(m, "mac_addr_count:      %u\n",
+		   octeon_bootinfo->mac_addr_count);
+#if CONFIG_CAVIUM_RESERVE32
+	seq_printf(m, "32bit_shared_mem_base: 0x%lx\n",
+		   octeon_reserve32_memory);
+	seq_printf(m, "32bit_shared_mem_size: 0x%x\n",
+		   octeon_reserve32_memory ? CONFIG_CAVIUM_RESERVE32 << 20 : 0);
+#else
+	seq_printf(m, "32bit_shared_mem_base: 0x%lx\n", 0ul);
+	seq_printf(m, "32bit_shared_mem_size: 0x%x\n", 0);
+#endif
+#ifdef CONFIG_CAVIUM_RESERVE32_USE_WIRED_TLB
+	seq_printf(m, "32bit_shared_mem_wired: 1\n");
+#else
+	seq_printf(m, "32bit_shared_mem_wired: 0\n");
+#endif
+	return 0;
+}
+
+
+/**
+ * /proc/octeon_info was openned. Use the single_open iterator
+ *
+ * @param inode
+ * @param file
+ * @return
+ */
+static int octeon_info_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, octeon_info_show, NULL);
+}
+
+
+static struct file_operations octeon_info_operations = {
+	.open = octeon_info_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+
+
+/**
+ * Module initialization
+ *
+ * @return
+ */
+static int __init octeon_info_init(void)
+{
+	struct proc_dir_entry *entry =
+		create_proc_entry("octeon_info", 0, NULL);
+	if (entry == NULL)
+		return -1;
+
+	entry->proc_fops = &octeon_info_operations;
+	return 0;
+}
+
+
+/**
+ * Module cleanup
+ *
+ * @return
+ */
+static void __exit octeon_info_cleanup(void)
+{
+	remove_proc_entry("octeon_info", NULL);
+}
+
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Cavium Networks <support@caviumnetworks.com>");
+MODULE_DESCRIPTION("Cavium Networks Octeon information interface.");
+module_init(octeon_info_init);
+module_exit(octeon_info_cleanup);
diff --git a/arch/mips/cavium-octeon/perf_counters.c b/arch/mips/cavium-octeon/perf_counters.c
new file mode 100644
index 0000000..3b6939c
--- /dev/null
+++ b/arch/mips/cavium-octeon/perf_counters.c
@@ -0,0 +1,786 @@
+/*
+ * Simple /proc interface to the Octeon Performance Counters
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2004-2007 Cavium Networks
+ */
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/fs.h>
+#include <linux/seq_file.h>
+#include <linux/proc_fs.h>
+#include <asm/ioctl.h>
+#include <asm/mach-cavium-octeon/perf_counters.h>
+#include "hal.h"
+
+/**
+ * The types of counters supported per cpu
+ */
+typedef enum {
+	PROC_PERF_CORE_NONE = 0,	/* Turn off the performance counter */
+	PROC_PERF_CORE_CLK = 1,	/* Conditionally clocked cycles (as opposed to
+				   count/cvm_count which count even with no
+				   clocks) */
+	PROC_PERF_CORE_ISSUE = 2,	/* Instructions issued but not retired */
+	PROC_PERF_CORE_RET = 3,	/* Instructions retired */
+	PROC_PERF_CORE_NISSUE = 4,	/* Cycles no issue */
+	PROC_PERF_CORE_SISSUE = 5,	/* Cycles single issue */
+	PROC_PERF_CORE_DISSUE = 6,	/* Cycles dual issue */
+	PROC_PERF_CORE_IFI = 7,	/* Cycle ifetch issued (but not necessarily
+				   commit to pp_mem) */
+	PROC_PERF_CORE_BR = 8,	/* Branches retired */
+	PROC_PERF_CORE_BRMIS = 9,	/* Branch mispredicts */
+	PROC_PERF_CORE_J = 10,	/* Jumps retired */
+	PROC_PERF_CORE_JMIS = 11,	/* Jumps mispredicted */
+	PROC_PERF_CORE_REPLAY = 12,	/* Mem Replays */
+	PROC_PERF_CORE_IUNA = 13,	/* Cycles idle due to unaligned_replays
+					 */
+	PROC_PERF_CORE_TRAP = 14,	/* trap_6a signal */
+	PROC_PERF_CORE_UULOAD = 16,	/* Unexpected unaligned loads (REPUN=1)
+					 */
+	PROC_PERF_CORE_UUSTORE = 17,	/* Unexpected unaligned store (REPUN=1)
+					 */
+	PROC_PERF_CORE_ULOAD = 18,	/* Unaligned loads (REPUN=1 or USEUN=1)
+					 */
+	PROC_PERF_CORE_USTORE = 19,	/* Unaligned store (REPUN=1 or USEUN=1)
+					 */
+	PROC_PERF_CORE_EC = 20,	/* Exec clocks(must set CvmCtl[DISCE] for
+				   accurate timing) */
+	PROC_PERF_CORE_MC = 21,	/* Mul clocks(must set CvmCtl[DISCE] for
+				   accurate timing) */
+	PROC_PERF_CORE_CC = 22,	/* Crypto clocks(must set CvmCtl[DISCE] for
+				   accurate timing) */
+	PROC_PERF_CORE_CSRC = 23,	/* Issue_csr clocks(must set
+					   CvmCtl[DISCE] for accurate timing) */
+	PROC_PERF_CORE_CFETCH = 24,	/* Icache committed fetches
+					   (demand+prefetch) */
+	PROC_PERF_CORE_CPREF = 25,	/* Icache committed prefetches */
+	PROC_PERF_CORE_ICA = 26,	/* Icache aliases */
+	PROC_PERF_CORE_II = 27,	/* Icache invalidates */
+	PROC_PERF_CORE_IP = 28,	/* Icache parity error */
+	PROC_PERF_CORE_CIMISS = 29,	/* Cycles idle due to imiss (must set
+					   CvmCtl[DISCE] for accurate timing) */
+	PROC_PERF_CORE_WBUF = 32,	/* Number of write buffer entries
+					   created */
+	PROC_PERF_CORE_WDAT = 33,	/* Number of write buffer data cycles
+					   used (may need to set CvmCtl[DISCE]
+					   for accurate counts) */
+	PROC_PERF_CORE_WBUFLD = 34,	/* Number of write buffer entries
+					   forced out by loads */
+	PROC_PERF_CORE_WBUFFL = 35,	/* Number of cycles that there was no
+					   available write buffer entry (may
+					   need to set CvmCtl[DISCE] and
+					   CvmMemCtl[MCLK] for accurate counts)
+					 */
+	PROC_PERF_CORE_WBUFTR = 36,	/* Number of stores that found no
+					   available write buffer entries */
+	PROC_PERF_CORE_BADD = 37,	/* Number of address bus cycles used
+					   (may need to set CvmCtl[DISCE] for
+					   accurate counts) */
+	PROC_PERF_CORE_BADDL2 = 38,	/* Number of address bus cycles not
+					   reflected (i.e. destined for L2)
+					   (may need to set CvmCtl[DISCE] for
+					   accurate counts) */
+	PROC_PERF_CORE_BFILL = 39,	/* Number of fill bus cycles used (may
+					   need to set CvmCtl[DISCE] for
+					   accurate counts) */
+	PROC_PERF_CORE_DDIDS = 40,	/* Number of Dstream DIDs created */
+	PROC_PERF_CORE_IDIDS = 41,	/* Number of Istream DIDs created */
+	PROC_PERF_CORE_DIDNA = 42,	/* Number of cycles that no DIDs were
+					   available (may need to set
+					   CvmCtl[DISCE] and CvmMemCtl[MCLK]
+					   for accurate counts) */
+	PROC_PERF_CORE_LDS = 43,	/* Number of load issues */
+	PROC_PERF_CORE_LMLDS = 44,	/* Number of local memory load */
+	PROC_PERF_CORE_IOLDS = 45,	/* Number of I/O load issues */
+	PROC_PERF_CORE_DMLDS = 46,	/* Number of loads that were not
+					   prefetches and missed in the cache */
+	PROC_PERF_CORE_STS = 48,	/* Number of store issues */
+	PROC_PERF_CORE_LMSTS = 49,	/* Number of local memory store issues */
+	PROC_PERF_CORE_IOSTS = 50,	/* Number of I/O store issues */
+	PROC_PERF_CORE_IOBDMA = 51,	/* Number of IOBDMAs */
+	PROC_PERF_CORE_DTLB = 53,	/* Number of dstream TLB refill,
+					   invalid, or modified exceptions */
+	PROC_PERF_CORE_DTLBAD = 54,	/* Number of dstream TLB address errors
+					 */
+	PROC_PERF_CORE_ITLB = 55,	/* Number of istream TLB refill,
+					   invalid, or address error exceptions
+					 */
+	PROC_PERF_CORE_SYNC = 56,	/* Number of SYNC stall cycles (may
+					   need to set CvmCtl[DISCE] for
+					   accurate counts) */
+	PROC_PERF_CORE_SYNCIOB = 57,	/* Number of SYNCIOBDMA stall cycles
+					   (may need to set CvmCtl[DISCE] for
+					   accurate counts) */
+	PROC_PERF_CORE_SYNCW = 58,	/* Number of SYNCWs */
+	PROC_PERF_CORE_MAX
+} proc_perf_core_t;
+
+/**
+ * The types of counters supported for L2
+ */
+typedef enum {
+	PROC_PERF_L2_CYCLES,	/* Cycles */
+	PROC_PERF_L2_IMISS,	/* L2 Instruction Miss */
+	PROC_PERF_L2_IHIT,	/* L2 Instruction Hit */
+	PROC_PERF_L2_DMISS,	/* L2 Data Miss */
+	PROC_PERF_L2_DHIT,	/* L2 Data Hit */
+	PROC_PERF_L2_MISS,	/* L2 Miss (I/D) */
+	PROC_PERF_L2_HIT,	/* L2 Hit (I/D) */
+	PROC_PERF_L2_VICTIM_BUFFER_HIT,	/* L2 Victim Buffer Hit (Retry Probe) */
+	PROC_PERF_L2_LFB_NQ_INDEX_CONFLICT,	/* LFB-NQ Index Conflict */
+	PROC_PERF_L2_TAG_PROBE,	/* L2 Tag Probe (issued - could be VB-Retried) */
+	PROC_PERF_L2_TAG_UPDATE,	/* L2 Tag Update (completed). Note:
+					   Some CMD types do not update */
+	PROC_PERF_L2_TAG_PROBE_COMPLETED,	/* L2 Tag Probe Completed
+						   (beyond VB-RTY window) */
+	PROC_PERF_L2_TAG_DIRTY_VICTIM,	/* L2 Tag Dirty Victim */
+	PROC_PERF_L2_DATA_STORE_NOP,	/* L2 Data Store NOP */
+	PROC_PERF_L2_DATA_STORE_READ,	/* L2 Data Store READ */
+	PROC_PERF_L2_DATA_STORE_WRITE,	/* L2 Data Store WRITE */
+	PROC_PERF_L2_MEMORY_FILL_DATA_VALID,	/* Memory Fill Data valid */
+	PROC_PERF_L2_MEMORY_WRITE_REQUEST,	/* Memory Write Request */
+	PROC_PERF_L2_MEMORY_READ_REQUEST,	/* Memory Read Request */
+	PROC_PERF_L2_MEMORY_WRITE_DATA_VALID,	/* Memory Write Data valid */
+	PROC_PERF_L2_XMC_NOP,	/* XMC NOP */
+	PROC_PERF_L2_XMC_LDT,	/* XMC LDT */
+	PROC_PERF_L2_XMC_LDI,	/* XMC LDI */
+	PROC_PERF_L2_XMC_LDD,	/* XMC LDD */
+	PROC_PERF_L2_XMC_STF,	/* XMC STF */
+	PROC_PERF_L2_XMC_STT,	/* XMC STT */
+	PROC_PERF_L2_XMC_STP,	/* XMC STP */
+	PROC_PERF_L2_XMC_STC,	/* XMC STC */
+	PROC_PERF_L2_XMC_DWB,	/* XMC DWB */
+	PROC_PERF_L2_XMC_PL2,	/* XMC PL2 */
+	PROC_PERF_L2_XMC_PSL1,	/* XMC PSL1 */
+	PROC_PERF_L2_XMC_IOBLD,	/* XMC IOBLD */
+	PROC_PERF_L2_XMC_IOBST,	/* XMC IOBST */
+	PROC_PERF_L2_XMC_IOBDMA,	/* XMC IOBDMA */
+	PROC_PERF_L2_XMC_IOBRSP,	/* XMC IOBRSP */
+	PROC_PERF_L2_XMD_BUS_VALID,	/* XMD Bus valid (all) */
+	PROC_PERF_L2_XMD_BUS_VALID_DST_L2C,	/* XMD Bus valid (DST=L2C)
+						   Memory */
+	PROC_PERF_L2_XMD_BUS_VALID_DST_IOB,	/* XMD Bus valid (DST=IOB) REFL
+						   Data */
+	PROC_PERF_L2_XMD_BUS_VALID_DST_PP,	/* XMD Bus valid (DST=PP)
+						   IOBRSP Data */
+	PROC_PERF_L2_RSC_NOP,	/* RSC NOP */
+	PROC_PERF_L2_RSC_STDN,	/* RSC STDN */
+	PROC_PERF_L2_RSC_FILL,	/* RSC FILL */
+	PROC_PERF_L2_RSC_REFL,	/* RSC REFL */
+	PROC_PERF_L2_RSC_STIN,	/* RSC STIN */
+	PROC_PERF_L2_RSC_SCIN,	/* RSC SCIN */
+	PROC_PERF_L2_RSC_SCFL,	/* RSC SCFL */
+	PROC_PERF_L2_RSC_SCDN,	/* RSC SCDN */
+	PROC_PERF_L2_RSD_DATA_VALID,	/* RSD Data Valid */
+	PROC_PERF_L2_RSD_DATA_VALID_FILL,	/* RSD Data Valid (FILL) */
+	PROC_PERF_L2_RSD_DATA_VALID_STRSP,	/* RSD Data Valid (STRSP) */
+	PROC_PERF_L2_RSD_DATA_VALID_REFL,	/* RSD Data Valid (REFL) */
+	PROC_PERF_L2_LRF_REQ,	/* LRF-REQ (LFB-NQ) */
+	PROC_PERF_L2_DT_RD_ALLOC,	/* DT RD-ALLOC */
+	PROC_PERF_L2_DT_WR_INVA,	/* DT WR-INVA */
+	PROC_PERF_L2_MAX
+} proc_perf_l2_t;
+
+/**
+ * IO addresses for L2 registers
+ */
+#define  OCTEON_L2C_PFCTL   0x8001180080000090ull
+#define  OCTEON_L2C_PFC0    0x8001180080000098ull
+#define  OCTEON_L2C_PFC1    0x80011800800000A0ull
+#define  OCTEON_L2C_PFC2    0x80011800800000A8ull
+#define  OCTEON_L2C_PFC3    0x80011800800000B0ull
+#define  OCTEON_LMC_DCLK_CNT_HI 0x8001180088000070ull
+#define  OCTEON_LMC_DCLK_CNT_LO 0x8001180088000068ull
+#define  OCTEON_LMC_OPS_CNT_HI  0x8001180088000060ull
+#define  OCTEON_LMC_OPS_CNT_LO  0x8001180088000058ull
+
+/**
+ * Bit description of the core counters control register
+ */
+typedef union {
+	uint32_t u32;
+	struct {
+		uint32_t M:1;
+		uint32_t W:1;
+		uint32_t reserved:19;
+		proc_perf_core_t event:6;
+		uint32_t IE:1;
+		uint32_t U:1;
+		uint32_t S:1;
+		uint32_t K:1;
+		uint32_t EX:1;
+	} s;
+} proc_perf_core_control_t;
+
+/**
+ * Bit description of the L2 counters control register
+ */
+typedef union {
+	uint64_t u64;
+	struct {
+		uint64_t reserved:32;
+		uint64_t cnt3ena:1;
+		uint64_t cnt3clr:1;
+		proc_perf_l2_t cnt3sel:6;
+		uint64_t cnt2ena:1;
+		uint64_t cnt2clr:1;
+		proc_perf_l2_t cnt2sel:6;
+		uint64_t cnt1ena:1;
+		uint64_t cnt1clr:1;
+		proc_perf_l2_t cnt1sel:6;
+		uint64_t cnt0ena:1;
+		uint64_t cnt0clr:1;
+		proc_perf_l2_t cnt0sel:6;
+	} s;
+} proc_perf_l2_control_t;
+
+/**
+ * Module parameters used to control the counters. Can be
+ * changed on the fly through sysfs or ioctls.
+ */
+static char counter0[32] = "sissue";
+static char counter1[32] = "dissue";
+module_param_string(counter0, counter0, sizeof(counter0), 0644);
+module_param_string(counter1, counter1, sizeof(counter1), 0644);
+
+static char l2counter0[32] = "imiss";
+static char l2counter1[32] = "ihit";
+static char l2counter2[32] = "dmiss";
+static char l2counter3[32] = "dhit";
+module_param_string(l2counter0, l2counter0, sizeof(l2counter0), 0644);
+module_param_string(l2counter1, l2counter1, sizeof(l2counter1), 0644);
+module_param_string(l2counter2, l2counter2, sizeof(l2counter2), 0644);
+module_param_string(l2counter3, l2counter3, sizeof(l2counter3), 0644);
+
+static struct proc_dir_entry *proc_perf_entry;
+static uint64_t proc_perf_counter_control[2];
+static uint64_t proc_perf_counter_data[NR_CPUS][2];
+static uint64_t proc_perf_l2counter_control[4];
+static uint64_t proc_perf_l2counter_data[4];
+static const char *proc_perf_label[PROC_PERF_CORE_MAX];
+static const char *proc_perf_l2label[PROC_PERF_L2_MAX];
+static uint64_t proc_perf_dram_clocks;
+static uint64_t proc_perf_dram_operations;
+static int proc_perf_in_use = 0;
+
+
+/**
+ * Setup the core counters. Called on each core
+ *
+ * @param arg
+ */
+static void proc_perf_setup_counters(void *arg)
+{
+	proc_perf_core_control_t control;
+	uint64_t cvmctl;
+
+	if (proc_perf_in_use) {
+		/* Disable the issue and exec conditional clock support so we get
+			better results */
+		cvmctl = __read_64bit_c0_register($9, 7);
+		cvmctl |= 3 << 16;
+		__write_64bit_c0_register($9, 7, cvmctl);
+	}
+
+	control.u32 = 0;
+	control.s.event = proc_perf_counter_control[0];
+	control.s.U = 1;
+	control.s.S = 1;
+	control.s.K = 1;
+	control.s.EX = 1;
+	__write_32bit_c0_register($25, 0, control.u32);
+
+	control.s.event = proc_perf_counter_control[1];
+	__write_32bit_c0_register($25, 2, control.u32);
+
+	__write_32bit_c0_register($25, 1, 0);
+	__write_32bit_c0_register($25, 3, 0);
+}
+
+
+/**
+ * Update the counters for each core.
+ *
+ * @param arg
+ */
+static void proc_perf_update_counters(void *arg)
+{
+	int cpu = smp_processor_id();
+
+	proc_perf_counter_data[cpu][0] = __read_64bit_c0_register($25, 1);
+	proc_perf_counter_data[cpu][1] = __read_64bit_c0_register($25, 3);
+	mb();
+}
+
+
+/**
+ * Cleanup the input of sysfs
+ *
+ * @param str
+ * @param len
+ */
+static inline void clean_string(char *str, int len)
+{
+	int i;
+	for (i = 0; i < len; i++)
+		if (str[i] <= 32)
+			str[i] = 0;
+}
+
+
+/**
+ * Setup the counters using the current config
+ */
+static void proc_perf_setup(void)
+{
+	int i;
+	proc_perf_l2_control_t l2control;
+
+	proc_perf_counter_control[0] = 0;
+	proc_perf_counter_control[1] = 0;
+	proc_perf_l2counter_control[0] = 0;
+	proc_perf_l2counter_control[1] = 0;
+	proc_perf_l2counter_control[2] = 0;
+	proc_perf_l2counter_control[3] = 0;
+
+	/* Cleanup junk on end of param strings */
+	clean_string(counter0, sizeof(counter0));
+	clean_string(counter1, sizeof(counter1));
+	clean_string(l2counter0, sizeof(l2counter0));
+	clean_string(l2counter1, sizeof(l2counter1));
+	clean_string(l2counter2, sizeof(l2counter2));
+	clean_string(l2counter3, sizeof(l2counter3));
+
+	/* Set the core counters to match the string parameters */
+	for (i = 0; i < PROC_PERF_CORE_MAX; i++) {
+		if (proc_perf_label[i]) {
+			if (strcmp(proc_perf_label[i], counter0) == 0)
+				proc_perf_counter_control[0] = i;
+			if (strcmp(proc_perf_label[i], counter1) == 0)
+				proc_perf_counter_control[1] = i;
+		}
+	}
+
+	/* Set the L2 counters to match the string parameters */
+	for (i = 0; i < PROC_PERF_L2_MAX; i++) {
+		if (proc_perf_l2label[i]) {
+			if (strcmp(proc_perf_l2label[i], l2counter0) == 0)
+				proc_perf_l2counter_control[0] = i;
+			if (strcmp(proc_perf_l2label[i], l2counter1) == 0)
+				proc_perf_l2counter_control[1] = i;
+			if (strcmp(proc_perf_l2label[i], l2counter2) == 0)
+				proc_perf_l2counter_control[2] = i;
+			if (strcmp(proc_perf_l2label[i], l2counter3) == 0)
+				proc_perf_l2counter_control[3] = i;
+		}
+	}
+
+	/* Update strings to match final config */
+	strcpy(counter0, proc_perf_label[proc_perf_counter_control[0]]);
+	strcpy(counter1, proc_perf_label[proc_perf_counter_control[1]]);
+	strcpy(l2counter0, proc_perf_l2label[proc_perf_l2counter_control[0]]);
+	strcpy(l2counter1, proc_perf_l2label[proc_perf_l2counter_control[1]]);
+	strcpy(l2counter2, proc_perf_l2label[proc_perf_l2counter_control[2]]);
+	strcpy(l2counter3, proc_perf_l2label[proc_perf_l2counter_control[3]]);
+
+	on_each_cpu(proc_perf_setup_counters, NULL, 1, 1);
+
+	l2control.u64 = 0;
+	l2control.s.cnt3ena = 1;
+	l2control.s.cnt3clr = 1;
+	l2control.s.cnt3sel = proc_perf_l2counter_control[3];
+	l2control.s.cnt2ena = 1;
+	l2control.s.cnt2clr = 1;
+	l2control.s.cnt2sel = proc_perf_l2counter_control[2];
+	l2control.s.cnt1ena = 1;
+	l2control.s.cnt1clr = 1;
+	l2control.s.cnt1sel = proc_perf_l2counter_control[1];
+	l2control.s.cnt0ena = 1;
+	l2control.s.cnt0clr = 1;
+	l2control.s.cnt0sel = proc_perf_l2counter_control[0];
+
+	cvmx_write_csr(OCTEON_L2C_PFCTL, l2control.u64);
+}
+
+
+static void proc_perf_update(void)
+{
+	on_each_cpu(proc_perf_update_counters, NULL, 1, 1);
+	mb();
+	proc_perf_l2counter_data[0] = cvmx_read_csr(OCTEON_L2C_PFC0);
+	proc_perf_l2counter_data[1] = cvmx_read_csr(OCTEON_L2C_PFC1);
+	proc_perf_l2counter_data[2] = cvmx_read_csr(OCTEON_L2C_PFC2);
+	proc_perf_l2counter_data[3] = cvmx_read_csr(OCTEON_L2C_PFC3);
+}
+
+
+/**
+ * Show the counters to the user
+ *
+ * @param m
+ * @param v
+ * @return
+ */
+static int proc_perf_show(struct seq_file *m, void *v)
+{
+	int cpu;
+	int i;
+	uint64_t dram_clocks;
+	uint64_t dram_operations;
+	proc_perf_core_control_t control0;
+	proc_perf_core_control_t control1;
+
+	proc_perf_update();
+
+	control0.u32 = __read_32bit_c0_register($25, 0);
+	control1.u32 = __read_32bit_c0_register($25, 2);
+	seq_printf(m, "       %16s %16s\n",
+		   proc_perf_label[control0.s.event],
+		   proc_perf_label[control1.s.event]);
+	for (cpu = 0; cpu < NR_CPUS; cpu++) {
+		if (cpu_online(cpu))
+			seq_printf(m, "CPU%2d: %16llu %16llu\n", cpu,
+				   (unsigned long long)
+				   proc_perf_counter_data[cpu][0],
+				   (unsigned long long)
+				   proc_perf_counter_data[cpu][1]);
+	}
+
+	seq_printf(m, "\n");
+	for (i = 0; i < 4; i++)
+		seq_printf(m, "%s: %llu\n",
+			   proc_perf_l2label[proc_perf_l2counter_control[i]],
+			   (unsigned long long) proc_perf_l2counter_data[i]);
+
+	/* Compute DRAM utilization */
+	dram_operations =
+		(cvmx_read_csr(OCTEON_LMC_OPS_CNT_HI) << 32) |
+		cvmx_read_csr(OCTEON_LMC_OPS_CNT_LO);
+	dram_clocks =
+		(cvmx_read_csr(OCTEON_LMC_DCLK_CNT_HI) << 32) |
+		cvmx_read_csr(OCTEON_LMC_DCLK_CNT_LO);
+#ifndef _ABIO32
+	if (dram_clocks > proc_perf_dram_clocks) {
+		uint64_t delta_clocks = dram_clocks - proc_perf_dram_clocks;
+		uint64_t delta_operations =
+			dram_operations - proc_perf_dram_operations;
+		uint64_t percent_x100 = 10000 * delta_operations / delta_clocks;
+		seq_printf(m,
+			   "\nDRAM ops count: %lu, dclk count: %lu, utilization: %lu.%02lu%%\n",
+			   delta_operations, delta_clocks, percent_x100 / 100,
+			   percent_x100 % 100);
+	}
+#endif
+	proc_perf_dram_operations = dram_operations;
+	proc_perf_dram_clocks = dram_clocks;
+
+	seq_printf(m,
+		   "\n"
+		   "Configuration of the performance counters is controller by writing\n"
+		   "one of the following values to:\n"
+		   "    /sys/module/perf_counters/parameters/counter{0,1}\n"
+		   "    /sys/module/perf_counters/parameters/l2counter{0-3}\n"
+		   "\n" "Possible CPU counters:");
+	for (i = 0; i < PROC_PERF_CORE_MAX; i++) {
+		if ((i & 7) == 0)
+			seq_printf(m, "\n    ");
+		if (proc_perf_label[i])
+			seq_printf(m, "%s ", proc_perf_label[i]);
+	}
+
+	seq_printf(m, "\n\nPossible L2 counters:");
+	for (i = 0; i < PROC_PERF_L2_MAX; i++) {
+		if ((i & 3) == 0)
+			seq_printf(m, "\n    ");
+		if (proc_perf_l2label[i])
+			seq_printf(m, "%s ", proc_perf_l2label[i]);
+	}
+	seq_printf(m,
+		   "\nWarning: Counter configuration doesn't update till you access /proc/octeon_perf.\n");
+
+	proc_perf_setup();
+	return 0;
+}
+
+
+/**
+ * /proc/octeon_perf was openned. Use the single_open iterator
+ *
+ * @param inode
+ * @param file
+ * @return
+ */
+static int proc_perf_open(struct inode *inode, struct file *file)
+{
+	proc_perf_in_use = 1;
+	return single_open(file, proc_perf_show, NULL);
+}
+
+
+/**
+ * IOCTL on /proc/octeon_perf
+ *
+ * @param inode
+ * @param file
+ * @param cmd
+ * @param arg
+ * @return
+ */
+static int proc_perf_ioctl(struct inode *inode, struct file *file,
+			   unsigned int cmd, unsigned long arg)
+{
+	// printk("proc_perf_ioctl(cmd=0x%x(%u), arg=0x%lx)\n", cmd, cmd, arg);
+	switch (cmd) {
+	case PROC_PERF_IOCTL_SETUP_COUNTER0:
+		if ((arg <= PROC_PERF_CORE_MAX) && proc_perf_label[arg]) {
+			strcpy(counter0, proc_perf_label[arg]);
+			proc_perf_setup();
+			return 0;
+		}
+		return -EINVAL;
+	case PROC_PERF_IOCTL_SETUP_COUNTER1:
+		if ((arg <= PROC_PERF_CORE_MAX) && proc_perf_label[arg]) {
+			strcpy(counter1, proc_perf_label[arg]);
+			proc_perf_setup();
+			return 0;
+		}
+		return -EINVAL;
+	case PROC_PERF_IOCTL_SETUP_L2COUNTER0:
+		if ((arg <= PROC_PERF_L2_MAX) && proc_perf_l2label[arg]) {
+			strcpy(l2counter0, proc_perf_l2label[arg]);
+			proc_perf_setup();
+			return 0;
+		}
+		return -EINVAL;
+	case PROC_PERF_IOCTL_SETUP_L2COUNTER1:
+		if ((arg <= PROC_PERF_L2_MAX) && proc_perf_l2label[arg]) {
+			strcpy(l2counter1, proc_perf_l2label[arg]);
+			proc_perf_setup();
+			return 0;
+		}
+		return -EINVAL;
+	case PROC_PERF_IOCTL_SETUP_L2COUNTER2:
+		if ((arg <= PROC_PERF_L2_MAX) && proc_perf_l2label[arg]) {
+			strcpy(l2counter2, proc_perf_l2label[arg]);
+			proc_perf_setup();
+			return 0;
+		}
+		return -EINVAL;
+	case PROC_PERF_IOCTL_SETUP_L2COUNTER3:
+		if ((arg <= PROC_PERF_L2_MAX) && proc_perf_l2label[arg]) {
+			strcpy(l2counter3, proc_perf_l2label[arg]);
+			proc_perf_setup();
+			return 0;
+		}
+		return -EINVAL;
+	case PROC_PERF_IOCTL_READ_COUNTER0:
+		proc_perf_update();
+		copy_to_user((void *) arg,
+			     proc_perf_counter_data[smp_processor_id()] + 0,
+			     sizeof(long long));
+		return 0;
+	case PROC_PERF_IOCTL_READ_COUNTER1:
+		proc_perf_update();
+		copy_to_user((void *) arg,
+			     proc_perf_counter_data[smp_processor_id()] + 1,
+			     sizeof(long long));
+		return 0;
+	case PROC_PERF_IOCTL_READ_L2COUNTER0:
+		proc_perf_update();
+		copy_to_user((void *) arg, proc_perf_l2counter_data + 0,
+			     sizeof(long long));
+		return 0;
+	case PROC_PERF_IOCTL_READ_L2COUNTER1:
+		proc_perf_update();
+		copy_to_user((void *) arg, proc_perf_l2counter_data + 1,
+			     sizeof(long long));
+		return 0;
+	case PROC_PERF_IOCTL_READ_L2COUNTER2:
+		proc_perf_update();
+		copy_to_user((void *) arg, proc_perf_l2counter_data + 2,
+			     sizeof(long long));
+		return 0;
+	case PROC_PERF_IOCTL_READ_L2COUNTER3:
+		proc_perf_update();
+		copy_to_user((void *) arg, proc_perf_l2counter_data + 3,
+			     sizeof(long long));
+		return 0;
+	default:
+		return -EINVAL;
+	}
+}
+
+
+static struct file_operations proc_perf_operations = {
+	.open = proc_perf_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+	.ioctl = proc_perf_ioctl,
+};
+
+
+/**
+ * Module initialization
+ *
+ * @return
+ */
+static int __init proc_perf_init(void)
+{
+	printk("/proc/octeon_perf: Octeon performace counter interface loaded\n");
+
+	memset(proc_perf_label, 0, sizeof(proc_perf_label));
+	memset(proc_perf_l2label, 0, sizeof(proc_perf_l2label));
+
+	proc_perf_label[PROC_PERF_CORE_NONE] = "none";
+	proc_perf_label[PROC_PERF_CORE_CLK] = "clk";
+	proc_perf_label[PROC_PERF_CORE_ISSUE] = "issue";
+	proc_perf_label[PROC_PERF_CORE_RET] = "ret";
+	proc_perf_label[PROC_PERF_CORE_NISSUE] = "nissue";
+	proc_perf_label[PROC_PERF_CORE_SISSUE] = "sissue";
+	proc_perf_label[PROC_PERF_CORE_DISSUE] = "dissue";
+	proc_perf_label[PROC_PERF_CORE_IFI] = "ifi";
+	proc_perf_label[PROC_PERF_CORE_BR] = "br";
+	proc_perf_label[PROC_PERF_CORE_BRMIS] = "brmis";
+	proc_perf_label[PROC_PERF_CORE_J] = "j";
+	proc_perf_label[PROC_PERF_CORE_JMIS] = "jmis";
+	proc_perf_label[PROC_PERF_CORE_REPLAY] = "replay";
+	proc_perf_label[PROC_PERF_CORE_IUNA] = "iuna";
+	proc_perf_label[PROC_PERF_CORE_TRAP] = "trap";
+	proc_perf_label[PROC_PERF_CORE_UULOAD] = "uuload";
+	proc_perf_label[PROC_PERF_CORE_UUSTORE] = "uustore";
+	proc_perf_label[PROC_PERF_CORE_ULOAD] = "uload";
+	proc_perf_label[PROC_PERF_CORE_USTORE] = "ustore";
+	proc_perf_label[PROC_PERF_CORE_EC] = "ec";
+	proc_perf_label[PROC_PERF_CORE_MC] = "mc";
+	proc_perf_label[PROC_PERF_CORE_CC] = "cc";
+	proc_perf_label[PROC_PERF_CORE_CSRC] = "csrc";
+	proc_perf_label[PROC_PERF_CORE_CFETCH] = "cfetch";
+	proc_perf_label[PROC_PERF_CORE_CPREF] = "cpref";
+	proc_perf_label[PROC_PERF_CORE_ICA] = "ica";
+	proc_perf_label[PROC_PERF_CORE_II] = "ii";
+	proc_perf_label[PROC_PERF_CORE_IP] = "ip";
+	proc_perf_label[PROC_PERF_CORE_CIMISS] = "cimiss";
+	proc_perf_label[PROC_PERF_CORE_WBUF] = "wbuf";
+	proc_perf_label[PROC_PERF_CORE_WDAT] = "wdat";
+	proc_perf_label[PROC_PERF_CORE_WBUFLD] = "wbufld";
+	proc_perf_label[PROC_PERF_CORE_WBUFFL] = "wbuffl";
+	proc_perf_label[PROC_PERF_CORE_WBUFTR] = "wbuftr";
+	proc_perf_label[PROC_PERF_CORE_BADD] = "badd";
+	proc_perf_label[PROC_PERF_CORE_BADDL2] = "baddl2";
+	proc_perf_label[PROC_PERF_CORE_BFILL] = "bfill";
+	proc_perf_label[PROC_PERF_CORE_DDIDS] = "ddids";
+	proc_perf_label[PROC_PERF_CORE_IDIDS] = "idids";
+	proc_perf_label[PROC_PERF_CORE_DIDNA] = "didna";
+	proc_perf_label[PROC_PERF_CORE_LDS] = "lds";
+	proc_perf_label[PROC_PERF_CORE_LMLDS] = "lmlds";
+	proc_perf_label[PROC_PERF_CORE_IOLDS] = "iolds";
+	proc_perf_label[PROC_PERF_CORE_DMLDS] = "dmlds";
+	proc_perf_label[PROC_PERF_CORE_STS] = "sts";
+	proc_perf_label[PROC_PERF_CORE_LMSTS] = "lmsts";
+	proc_perf_label[PROC_PERF_CORE_IOSTS] = "iosts";
+	proc_perf_label[PROC_PERF_CORE_IOBDMA] = "iobdma";
+	proc_perf_label[PROC_PERF_CORE_DTLB] = "dtlb";
+	proc_perf_label[PROC_PERF_CORE_DTLBAD] = "dtlbad";
+	proc_perf_label[PROC_PERF_CORE_ITLB] = "itlb";
+	proc_perf_label[PROC_PERF_CORE_SYNC] = "sync";
+	proc_perf_label[PROC_PERF_CORE_SYNCIOB] = "synciob";
+	proc_perf_label[PROC_PERF_CORE_SYNCW] = "syncw";
+
+	proc_perf_l2label[PROC_PERF_L2_CYCLES] = "cycles";
+	proc_perf_l2label[PROC_PERF_L2_IMISS] = "imiss";
+	proc_perf_l2label[PROC_PERF_L2_IHIT] = "ihit";
+	proc_perf_l2label[PROC_PERF_L2_DMISS] = "dmiss";
+	proc_perf_l2label[PROC_PERF_L2_DHIT] = "dhit";
+	proc_perf_l2label[PROC_PERF_L2_MISS] = "miss";
+	proc_perf_l2label[PROC_PERF_L2_HIT] = "hit";
+	proc_perf_l2label[PROC_PERF_L2_VICTIM_BUFFER_HIT] = "victim-buffer-hit";
+	proc_perf_l2label[PROC_PERF_L2_LFB_NQ_INDEX_CONFLICT] =
+		"lfb-nq-index-conflict";
+	proc_perf_l2label[PROC_PERF_L2_TAG_PROBE] = "tag-probe";
+	proc_perf_l2label[PROC_PERF_L2_TAG_UPDATE] = "tag-update";
+	proc_perf_l2label[PROC_PERF_L2_TAG_PROBE_COMPLETED] =
+		"tag-probe-completed";
+	proc_perf_l2label[PROC_PERF_L2_TAG_DIRTY_VICTIM] = "tag-dirty-victim";
+	proc_perf_l2label[PROC_PERF_L2_DATA_STORE_NOP] = "data-store-nop";
+	proc_perf_l2label[PROC_PERF_L2_DATA_STORE_READ] = "data-store-read";
+	proc_perf_l2label[PROC_PERF_L2_DATA_STORE_WRITE] = "data-store-write";
+	proc_perf_l2label[PROC_PERF_L2_MEMORY_FILL_DATA_VALID] =
+		"memory-fill-data-valid";
+	proc_perf_l2label[PROC_PERF_L2_MEMORY_WRITE_REQUEST] =
+		"memory-write-request";
+	proc_perf_l2label[PROC_PERF_L2_MEMORY_READ_REQUEST] =
+		"memory-read-request";
+	proc_perf_l2label[PROC_PERF_L2_MEMORY_WRITE_DATA_VALID] =
+		"memory-write-data-valid";
+	proc_perf_l2label[PROC_PERF_L2_XMC_NOP] = "xmc-nop";
+	proc_perf_l2label[PROC_PERF_L2_XMC_LDT] = "xmc-ldt";
+	proc_perf_l2label[PROC_PERF_L2_XMC_LDI] = "xmc-ldi";
+	proc_perf_l2label[PROC_PERF_L2_XMC_LDD] = "xmc-ldd";
+	proc_perf_l2label[PROC_PERF_L2_XMC_STF] = "xmc-stf";
+	proc_perf_l2label[PROC_PERF_L2_XMC_STT] = "xmc-stt";
+	proc_perf_l2label[PROC_PERF_L2_XMC_STP] = "xmc-stp";
+	proc_perf_l2label[PROC_PERF_L2_XMC_STC] = "xmc-stc";
+	proc_perf_l2label[PROC_PERF_L2_XMC_DWB] = "xmc-dwb";
+	proc_perf_l2label[PROC_PERF_L2_XMC_PL2] = "xmc-pl2";
+	proc_perf_l2label[PROC_PERF_L2_XMC_PSL1] = "xmc-psl1";
+	proc_perf_l2label[PROC_PERF_L2_XMC_IOBLD] = "xmc-iobld";
+	proc_perf_l2label[PROC_PERF_L2_XMC_IOBST] = "xmc-iobst";
+	proc_perf_l2label[PROC_PERF_L2_XMC_IOBDMA] = "xmc-iobdma";
+	proc_perf_l2label[PROC_PERF_L2_XMC_IOBRSP] = "xmc-iobrsp";
+	proc_perf_l2label[PROC_PERF_L2_XMD_BUS_VALID] = "xmd-bus-valid";
+	proc_perf_l2label[PROC_PERF_L2_XMD_BUS_VALID_DST_L2C] =
+		"xmd-bus-valid-dst-l2c";
+	proc_perf_l2label[PROC_PERF_L2_XMD_BUS_VALID_DST_IOB] =
+		"xmd-bus-valid-dst-iob";
+	proc_perf_l2label[PROC_PERF_L2_XMD_BUS_VALID_DST_PP] =
+		"xmd-bus-valid-dst-pp";
+	proc_perf_l2label[PROC_PERF_L2_RSC_NOP] = "rsc-nop";
+	proc_perf_l2label[PROC_PERF_L2_RSC_STDN] = "rsc-stdn";
+	proc_perf_l2label[PROC_PERF_L2_RSC_FILL] = "rsc-fill";
+	proc_perf_l2label[PROC_PERF_L2_RSC_REFL] = "rsc-refl";
+	proc_perf_l2label[PROC_PERF_L2_RSC_STIN] = "rsc-stin";
+	proc_perf_l2label[PROC_PERF_L2_RSC_SCIN] = "rsc-scin";
+	proc_perf_l2label[PROC_PERF_L2_RSC_SCFL] = "rsc-scfl";
+	proc_perf_l2label[PROC_PERF_L2_RSC_SCDN] = "rsc-scdn";
+	proc_perf_l2label[PROC_PERF_L2_RSD_DATA_VALID] = "rsd-data-valid";
+	proc_perf_l2label[PROC_PERF_L2_RSD_DATA_VALID_FILL] =
+		"rsd-data-valid-fill";
+	proc_perf_l2label[PROC_PERF_L2_RSD_DATA_VALID_STRSP] =
+		"rsd-data-valid-strsp";
+	proc_perf_l2label[PROC_PERF_L2_RSD_DATA_VALID_REFL] =
+		"rsd-data-valid-refl";
+	proc_perf_l2label[PROC_PERF_L2_LRF_REQ] = "lrf-req";
+	proc_perf_l2label[PROC_PERF_L2_DT_RD_ALLOC] = "dt-rd-alloc";
+	proc_perf_l2label[PROC_PERF_L2_DT_WR_INVA] = "dt-wr-inva";
+
+	proc_perf_entry = create_proc_entry("octeon_perf", 0, NULL);
+	if (proc_perf_entry)
+		proc_perf_entry->proc_fops = &proc_perf_operations;
+
+	proc_perf_setup();
+	return 0;
+}
+
+
+/**
+ * Module cleanup
+ *
+ * @return
+ */
+static void __exit proc_perf_cleanup(void)
+{
+	if (proc_perf_entry)
+		remove_proc_entry("octeon_perf", NULL);
+}
+
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Cavium Networks <support@caviumnetworks.com>");
+MODULE_DESCRIPTION("Cavium Networks Octeon performance counter interface.");
+module_init(proc_perf_init);
+module_exit(proc_perf_cleanup);
diff --git a/arch/mips/cavium-octeon/serial.c b/arch/mips/cavium-octeon/serial.c
new file mode 100644
index 0000000..6446af6
--- /dev/null
+++ b/arch/mips/cavium-octeon/serial.c
@@ -0,0 +1,167 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2004-2007 Cavium Networks
+ */
+#include <linux/console.h>
+#include <linux/serial.h>
+#include <linux/tty.h>
+#include <asm/time.h>
+#include <linux/serial_8250.h>
+#include <asm/gdb-stub.h>
+#include "hal.h"
+
+#ifdef CONFIG_GDB_CONSOLE
+#define DEBUG_UART 0
+#else
+#define DEBUG_UART 1
+#endif
+
+#ifdef CONFIG_KGDB
+
+extern void breakpoint(void);
+
+char getDebugChar(void)
+{
+	unsigned long lsrval;
+
+	octeon_write_lcd("kgdb");
+
+	/* Spin until data is available */
+	do {
+		lsrval = cvmx_read_csr(CVMX_MIO_UARTX_LSR(DEBUG_UART));
+	} while ((lsrval & 0x1) == 0);
+
+	octeon_write_lcd("");
+
+	/* Read and return the data */
+	return cvmx_read_csr(CVMX_MIO_UARTX_RBR(DEBUG_UART));
+}
+
+void putDebugChar(char ch)
+{
+	unsigned long lsrval;
+
+	/* Spin until there is room */
+	do {
+		lsrval = cvmx_read_csr(CVMX_MIO_UARTX_LSR(DEBUG_UART));
+	}
+	while ((lsrval & 0x20) == 0);
+
+	/* Write the byte */
+	cvmx_write_csr(CVMX_MIO_UARTX_THR(DEBUG_UART), ch);
+}
+
+#endif
+
+#if defined(CONFIG_KGDB) || defined(CONFIG_CAVIUM_GDB)
+
+static irqreturn_t interruptDebugChar(int cpl, void *dev_id)
+{
+	unsigned long lsrval;
+	lsrval = cvmx_read_csr(CVMX_MIO_UARTX_LSR(1));
+	if (lsrval & 1) {
+#ifdef CONFIG_KGDB
+		struct pt_regs *regs = get_irq_regs();
+
+		putDebugChar(getDebugChar());
+		set_async_breakpoint(&regs->cp0_epc);
+#else
+		unsigned long tmp;
+		/* Pulse MCD0 signal on Ctrl-C to stop all the cores. Also set
+		   the MCD0 to be not masked by this core so we know the signal
+		   is received by someone */
+		octeon_write_lcd("brk");
+		asm volatile ("dmfc0 %0, $22\n"
+			      "ori   %0, %0, 0x10\n"
+			      "dmtc0 %0, $22\n":"=r" (tmp));
+		octeon_write_lcd("");
+#endif
+		return IRQ_HANDLED;
+	}
+	return IRQ_NONE;
+}
+
+#endif
+
+static int octeon_serial_init(void)
+{
+	struct uart_port octeon_port;
+	int enable_uart0;
+	int enable_uart1;
+	int enable_uart2;
+
+#ifdef CONFIG_CAVIUM_OCTEON_2ND_KERNEL
+	/* If we are configured to run as the second of two kernels, disable
+	   uart0 and enable uart1. Uart0 is owned by the first kernel */
+	enable_uart0 = 0;
+	enable_uart1 = 1;
+#else
+	/* We are configured for the first kernel. We'll enable uart0 if the
+	   bootloader told us to use 0, otherwise will enable uart 1 */
+	enable_uart0 = (octeon_get_boot_uart() == 0);
+	enable_uart1 = (octeon_get_boot_uart() == 1);
+	/* Uncomment the following line if you'd like uart1 to be enable as
+	   well as uart 0 when the bootloader tells us to use uart0 */
+	// enable_uart1 = 1;
+#endif
+
+#if defined(CONFIG_KGDB) || defined(CONFIG_CAVIUM_GDB)
+	/* As a special case disable uart1 if KGDB is in use */
+	enable_uart1 = 0;
+#endif
+
+	/* Right now CN52XX is the only chip with a third uart */
+	enable_uart2 = OCTEON_IS_MODEL(OCTEON_CN52XX);
+
+	/* These fields are common to all Octeon UARTs */
+	memset(&octeon_port, 0, sizeof(octeon_port));
+	octeon_port.flags = ASYNC_SKIP_TEST | UPF_SHARE_IRQ;
+	octeon_port.iotype = UPIO_MEM;
+	octeon_port.regshift = 3;	/* I/O addresses are every 8 bytes */
+	octeon_port.uartclk = mips_hpt_frequency;
+	octeon_port.fifosize = 64;
+
+	/* Add a ttyS device for hardware uart 0 */
+	if (enable_uart0) {
+		octeon_port.membase = (void *) CVMX_MIO_UARTX_RBR(0);
+		octeon_port.mapbase =
+			CVMX_MIO_UARTX_RBR(0) & ((1ull << 49) - 1);
+		/* Only CN38XXp{1,2} has errata with uart interrupt */
+		if (!OCTEON_IS_MODEL(OCTEON_CN38XX_PASS2))
+			octeon_port.irq = OCTEON_IRQ_UART0;
+		serial8250_register_port(&octeon_port);
+	}
+
+	/* Add a ttyS device for hardware uart 1 */
+	if (enable_uart1) {
+		octeon_port.membase = (void *) CVMX_MIO_UARTX_RBR(1);
+		octeon_port.mapbase =
+			CVMX_MIO_UARTX_RBR(1) & ((1ull << 49) - 1);
+		/* Only CN38XXp{1,2} has errata with uart interrupt */
+		if (!OCTEON_IS_MODEL(OCTEON_CN38XX_PASS2))
+			octeon_port.irq = OCTEON_IRQ_UART1;
+		serial8250_register_port(&octeon_port);
+	}
+
+	/* Add a ttyS device for hardware uart 2 */
+	if (enable_uart2) {
+		octeon_port.membase = (void *) CVMX_MIO_UART2_RBR;
+		octeon_port.mapbase = CVMX_MIO_UART2_RBR & ((1ull << 49) - 1);
+		octeon_port.irq = OCTEON_IRQ_UART2;
+		serial8250_register_port(&octeon_port);
+	}
+#if defined(CONFIG_KGDB) || defined(CONFIG_CAVIUM_GDB)
+	request_irq(OCTEON_IRQ_UART0 + DEBUG_UART, interruptDebugChar,
+		    SA_SHIRQ, "KGDB", interruptDebugChar);
+
+	/* Enable uart1 interrupts for debugger Control-C processing */
+	cvmx_write_csr(CVMX_MIO_UARTX_IER(DEBUG_UART),
+		       cvmx_read_csr(CVMX_MIO_UARTX_IER(DEBUG_UART)) | 1);
+#endif
+	return 0;
+}
+
+late_initcall(octeon_serial_init);
diff --git a/arch/mips/cavium-octeon/setup.c b/arch/mips/cavium-octeon/setup.c
new file mode 100644
index 0000000..ff56a2c
--- /dev/null
+++ b/arch/mips/cavium-octeon/setup.c
@@ -0,0 +1,422 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2004-2007 Cavium Networks
+ */
+#include <linux/init.h>
+#include <linux/delay.h>
+#include <linux/irq.h>
+#include <linux/interrupt.h>
+#include <linux/serial.h>
+#include <linux/types.h>
+#include <linux/string.h>	/* for memset */
+#include <linux/console.h>
+#include <linux/serial.h>
+#include <linux/tty.h>
+#include <asm/time.h>
+#include <linux/serial_core.h>
+#include <linux/string.h>
+
+#include <asm/reboot.h>
+#include <asm/io.h>
+#include <asm/time.h>
+#include <asm/processor.h>
+#include <asm/reboot.h>
+#include <asm/system.h>
+#include <asm/irq_cpu.h>
+#include <asm/mipsregs.h>
+#include <asm/bootinfo.h>
+#include <asm/gdb-stub.h>
+#include "hal.h"
+#include "cvmx-l2c.h"
+#include "cvmx-bootmem.h"
+
+extern void octeon_user_io_init(void);
+#ifdef CONFIG_CAVIUM_OCTEON_BOOTBUS_COMPACT_FLASH
+extern void ebt3000_cf_enable_dma(void);
+#endif
+static unsigned long CYCLES_PER_JIFFY;
+static unsigned long long MAX_MEMORY = 512ull << 20;
+
+
+/**
+ * Reboot Octeon
+ *
+ * @param command Command to pass to the bootloader. Currently ignored.
+ */
+static void octeon_restart(char *command)
+{
+	/* Disable all watchdogs before soft reset. They don't get cleared */
+#ifdef CONFIG_SMP
+	int cpu;
+	for (cpu = 0; cpu < NR_CPUS; cpu++)
+		if (cpu_online(cpu))
+			cvmx_write_csr(CVMX_CIU_WDOGX(cpu_logical_map(cpu)), 0);
+#else
+	cvmx_write_csr(CVMX_CIU_WDOGX(cvmx_get_core_num()), 0);
+#endif
+
+	mb();
+	while (1)
+		cvmx_write_csr(CVMX_CIU_SOFT_RST, 1);
+}
+
+
+/**
+ * Permanently stop a core.
+ *
+ * @param arg
+ */
+static void octeon_kill_core(void *arg)
+{
+	mb();
+	if (octeon_is_simulation()) {
+		/* The simulator needs the watchdog to stop for dead cores */
+		cvmx_write_csr(CVMX_CIU_WDOGX(cvmx_get_core_num()), 0);
+		/* A break instruction causes the simulator stop a core */
+		asm volatile ("sync\nbreak");
+	}
+}
+
+
+/**
+ * Halt the system
+ */
+static void octeon_halt(void)
+{
+	smp_call_function(octeon_kill_core, NULL, 0, 0);
+	octeon_poweroff();
+	octeon_kill_core(NULL);
+}
+
+
+/**
+ * Read the Octeon high performance counter
+ *
+ * @return The counter value. For some brain dead reason, the kernel
+ *         uses a 32bit number here.
+ */
+static cycles_t octeon_hpt_read(void)
+{
+	cycles_t cycles;
+      asm("rdhwr %0,$31":"=r"(cycles));
+	return cycles;
+}
+
+
+/**
+ * Acknowledge a timer tick. We don't use the standard Mips
+ * one because it confuses the timer ticks and the HPT clock.
+ */
+static void octeon_timer_ack(void)
+{
+	uint32_t count;
+	uint32_t next_compare = read_c0_compare() + CYCLES_PER_JIFFY;
+	write_c0_compare(next_compare);
+	count = read_c0_count();
+	if ((count - next_compare) < 0x7fffffff) {
+		next_compare = count + CYCLES_PER_JIFFY;
+		write_c0_compare(next_compare);
+	}
+}
+
+
+/**
+ * Interrupt entry point for timer ticks
+ *
+ * @param irq
+ * @param dev_id
+ * @return
+ */
+static irqreturn_t octeon_main_timer_interrupt(int irq, void *dev_id)
+{
+	if (read_c0_cause() & (1 << 30)) {
+		if (smp_processor_id() == 0) {
+			/* This function calls the timer ack internally */
+			timer_interrupt(irq, dev_id);
+		} else {
+			octeon_timer_ack();
+			local_timer_interrupt(irq, dev_id);
+		}
+		return IRQ_HANDLED;
+	} else
+		return IRQ_NONE;
+}
+
+
+/**
+ * Setup the first cores timer interrupt
+ *
+ * @param irq
+ * @return
+ */
+void __init plat_timer_setup(struct irqaction *irq)
+{
+	irq->handler = octeon_main_timer_interrupt;
+	irq->flags |= SA_SHIRQ;
+	setup_irq(7, irq);
+}
+
+
+/**
+ * Handle all the error condition interrupts that might occur.
+ *
+ * @param cpl
+ * @param dev_id
+ * @return
+ */
+static irqreturn_t octeon_rlm_interrupt(int cpl, void *dev_id)
+{
+	extern void cvmx_interrupt_rsl_decode(void);
+	cvmx_interrupt_rsl_decode();
+	return IRQ_HANDLED;
+}
+
+
+/**
+ * Return a string representing the system type
+ *
+ * @return
+ */
+const char *get_system_type(void)
+{
+	return octeon_board_type_string();
+}
+
+
+/**
+ * Early entry point for arch setup
+ */
+void prom_init(void)
+{
+	const int coreid = cvmx_get_core_num();
+	int i;
+	int argc;
+	struct uart_port octeon_port;
+	int octeon_uart;
+	extern void pci_console_init(const char *arg);
+
+	octeon_hal_init();
+	octeon_check_cpu_bist();
+#ifdef CONFIG_CAVIUM_OCTEON_2ND_KERNEL
+	octeon_uart = 1;
+#else
+	octeon_uart = octeon_get_boot_uart();
+#endif
+
+	/* Disable All CIU Interrupts. The ones we need will be enabled later.
+	   Read the SUM register so we know the write completed. */
+	cvmx_write_csr(CVMX_CIU_INTX_EN0((coreid * 2)), 0);
+	cvmx_write_csr(CVMX_CIU_INTX_EN0((coreid * 2 + 1)), 0);
+	cvmx_write_csr(CVMX_CIU_INTX_EN1((coreid * 2)), 0);
+	cvmx_write_csr(CVMX_CIU_INTX_EN1((coreid * 2 + 1)), 0);
+	cvmx_read_csr(CVMX_CIU_INTX_SUM0((coreid * 2)));
+
+#ifdef CONFIG_SMP
+	octeon_write_lcd("LinuxSMP");
+#else
+	octeon_write_lcd("Linux");
+#endif
+
+#ifdef CONFIG_CAVIUM_GDB
+	/* When debugging the linux kernel, force the cores to enter the debug
+	   exception handler to break in.  */
+	if (octeon_get_boot_debug_flag()) {
+		cvmx_write_csr(CVMX_CIU_DINT, 1 << cvmx_get_core_num());
+		cvmx_read_csr(CVMX_CIU_DINT);
+	}
+#endif
+
+	/* BIST should always be enabled when doing a soft reset. L2 Cache
+	   locking for instance is not cleared unless BIST is enabled.
+	   Unfortunately due to a chip errata G-200 for Cn38XX and CN31XX, BIST 
+	   msut be disabled on these parts */
+	if (OCTEON_IS_MODEL(OCTEON_CN38XX_PASS2) ||
+	    OCTEON_IS_MODEL(OCTEON_CN31XX))
+		cvmx_write_csr(CVMX_CIU_SOFT_BIST, 0);
+	else
+		cvmx_write_csr(CVMX_CIU_SOFT_BIST, 1);
+
+	/* Default to 64MB in the simulator to speed things up */
+	if (octeon_is_simulation())
+		MAX_MEMORY = 64ull << 20;
+
+	arcs_cmdline[0] = 0;
+	argc = octeon_get_boot_num_arguments();
+	for (i = 0; i < argc; i++) {
+		const char *arg = octeon_get_boot_argument(i);
+		if ((strncmp(arg, "MEM=", 4) == 0) ||
+		    (strncmp(arg, "mem=", 4) == 0)) {
+			sscanf(arg + 4, "%llu", &MAX_MEMORY);
+			MAX_MEMORY <<= 20;
+			if (MAX_MEMORY == 0)
+				MAX_MEMORY = 32ull << 30;
+		} else if (strcmp(arg, "ecc_verbose") == 0) {
+			extern int __cvmx_interrupt_ecc_report_single_bit_errors;
+			__cvmx_interrupt_ecc_report_single_bit_errors = 1;
+			printk("Reporting of single bit ECC errors is turned on\n");
+		} else if (strlen(arcs_cmdline) + strlen(arg) + 1 <
+			   sizeof(arcs_cmdline) - 1) {
+			strcat(arcs_cmdline, " ");
+			strcat(arcs_cmdline, arg);
+		}
+	}
+#ifdef CONFIG_CAVIUM_OCTEON_BOOTBUS_COMPACT_FLASH
+	if (strstr(arcs_cmdline, "use_cf_dma"))
+		ebt3000_cf_enable_dma();
+#endif
+
+
+	if (strstr(arcs_cmdline, "console=pci"))
+		pci_console_init(strstr(arcs_cmdline, "console=pci") + 8);
+
+	if (strstr(arcs_cmdline, "console=") == NULL) {
+#ifdef CONFIG_GDB_CONSOLE
+		strcat(arcs_cmdline, " console=gdb");
+#else
+#ifdef CONFIG_CAVIUM_OCTEON_2ND_KERNEL
+		strcat(arcs_cmdline, " console=ttyS0,115200");
+#else
+		if (octeon_uart == 1)
+			strcat(arcs_cmdline, " console=ttyS1,115200");
+		else
+			strcat(arcs_cmdline, " console=ttyS0,115200");
+#endif
+#endif
+	}
+
+	if (octeon_is_simulation()) {
+		/* The simulator uses a mtdram device pre filled with the
+		   filesystem. Also specify the calibration delay to avoid
+		   calculating it every time */
+		strcat(arcs_cmdline,
+		       " rw root=1f00 lpj=60176 slram=root,0x40000000,+1073741824");
+	}
+
+	/* you should these macros defined in include/asm/bootinfo.h */
+	mips_machgroup = MACH_GROUP_CAVIUM;
+	mips_machtype = MACH_CAVIUM_OCTEON;
+
+	mips_hpt_frequency = octeon_get_clock_rate();
+	clocksource_mips.read = octeon_hpt_read;
+	mips_timer_ack = octeon_timer_ack;
+	CYCLES_PER_JIFFY = ((mips_hpt_frequency + HZ / 2) / HZ);
+
+	_machine_restart = octeon_restart;
+	_machine_halt = octeon_halt;
+
+	memset(&octeon_port, 0, sizeof(octeon_port));
+	octeon_port.flags = ASYNC_SKIP_TEST | UPF_SHARE_IRQ;
+	octeon_port.iotype = UPIO_MEM;
+	octeon_port.regshift = 3;	/* I/O addresses are every 8 bytes */
+	octeon_port.uartclk = mips_hpt_frequency;	/* Clock rate of the
+							   chip */
+	octeon_port.fifosize = 64;
+	octeon_port.mapbase = 0x0001180000000800ull + (1024 * octeon_uart);
+	octeon_port.membase = cvmx_phys_to_ptr(octeon_port.mapbase);
+#ifdef CONFIG_CAVIUM_OCTEON_2ND_KERNEL
+	octeon_port.line = 0;
+#else
+	octeon_port.line = octeon_uart;
+#endif
+	if (!OCTEON_IS_MODEL(OCTEON_CN38XX_PASS2))	/* Only CN38XXp{1,2}
+							   has errata with uart
+							   interrupt */
+		octeon_port.irq = 42 + octeon_uart;
+	early_serial_setup(&octeon_port);
+
+	octeon_user_io_init();
+
+#ifdef CONFIG_KGDB
+	{
+		extern void putDebugChar(char ch);
+		const char *s = "\r\nConnect GDB to this port\r\n";
+		while (*s)
+			putDebugChar(*s++);
+	}
+#endif
+}
+
+
+
+void __init plat_mem_setup(void)
+{
+#if CONFIG_CAVIUM_RESERVE32
+	extern uint64_t octeon_reserve32_memory;
+#endif
+	extern asmlinkage void kernel_entry(void);
+	uint64_t mem_alloc_size;
+	uint64_t total;
+
+	/* The Mips memory init uses the first memory location for some memory
+	   vectors. When SPARSEMEM is in use, it doesn't verify that the size
+	   is big enough for the final vectors. Making the smallest chuck 4MB
+	   seems to be enough to consistantly work. This needs to be debugged
+	   more */
+	mem_alloc_size = 4 << 20;
+	total = 0;
+	if (mem_alloc_size > MAX_MEMORY)
+		mem_alloc_size = MAX_MEMORY;
+
+	/* When allocating memory, we want incrementing addresses from
+	   bootmem_alloc so the code in add_memory_region can merge regions
+	   next to each other */
+	cvmx_bootmem_lock();
+	while ((boot_mem_map.nr_map < BOOT_MEM_MAP_MAX) && (total < MAX_MEMORY)) {
+#if defined(CONFIG_64BIT) || defined(CONFIG_64BIT_PHYS_ADDR)
+		int64_t memory =
+			cvmx_bootmem_phy_alloc(mem_alloc_size,
+					       __pa_symbol(kernel_entry), -1,
+					       0x100000,
+					       CVMX_BOOTMEM_FLAG_NO_LOCKING);
+#elif defined(CONFIG_HIGHMEM)
+		int64_t memory =
+			cvmx_bootmem_phy_alloc(mem_alloc_size, 0, 1ull << 31,
+					       0x100000,
+					       CVMX_BOOTMEM_FLAG_NO_LOCKING);
+#else
+		int64_t memory =
+			cvmx_bootmem_phy_alloc(mem_alloc_size, 0, 512 << 20,
+					       0x100000,
+					       CVMX_BOOTMEM_FLAG_NO_LOCKING);
+#endif
+		if (memory >= 0) {
+			/* This function automatically merges address regions
+			   next to each other if they are received in
+			   incrementing order */
+			add_memory_region(memory, mem_alloc_size, BOOT_MEM_RAM);
+			total += mem_alloc_size;
+		} else
+			break;
+	}
+	cvmx_bootmem_unlock();
+
+#if CONFIG_CAVIUM_RESERVE32
+	/* Now that we've allocated the kernel memory it is safe to free the
+		reserved region. We free it here so that builtin drivers can
+		use the memory */
+	if (octeon_reserve32_memory)
+		cvmx_bootmem_free_named("CAVIUM_RESERVE32");
+#endif /* CONFIG_CAVIUM_RESERVE32 */
+
+	if (total == 0)
+		panic("Unable to allocate memory from cvmx_bootmem_phy_alloc\n");
+}
+
+
+void prom_free_prom_memory(void)
+{
+	extern void cvmx_interrupt_rsl_enable(void);
+	cvmx_interrupt_rsl_enable();
+
+	/* Add an interrupt handler for general failures. */
+	request_irq(OCTEON_IRQ_RML, octeon_rlm_interrupt, SA_SHIRQ, "RML/RSL",
+		    octeon_rlm_interrupt);
+
+	/* This call is here so that it is performed after any TLB
+	   initializations. It needs to be after these in case the
+	   CONFIG_CAVIUM_RESERVE32_USE_WIRED_TLB option is set */
+	octeon_hal_setup_reserved32();
+}
diff --git a/arch/mips/cavium-octeon/smp.c b/arch/mips/cavium-octeon/smp.c
new file mode 100644
index 0000000..196673f
--- /dev/null
+++ b/arch/mips/cavium-octeon/smp.c
@@ -0,0 +1,208 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2004-2007 Cavium Networks
+ */
+#include <linux/init.h>
+#include <linux/delay.h>
+#include <linux/smp.h>
+#include <linux/kernel_stat.h>
+#include <linux/sched.h>
+#include <linux/module.h>
+#include <asm/mmu_context.h>
+#include <asm/time.h>
+#include <asm/system.h>
+
+#include "hal.h"
+
+extern void octeon_user_io_init(void);
+
+volatile unsigned long octeon_processor_boot = 0xff;
+volatile unsigned long octeon_processor_cycle;
+volatile unsigned long octeon_processor_sp;
+volatile unsigned long octeon_processor_gp;
+
+
+static irqreturn_t mailbox_interrupt(int irq, void *dev_id)
+{
+	const int coreid = cvmx_get_core_num();
+	uint64_t action;
+
+	/* Load the mailbox register to figure out what we're supposed to do */
+	action = cvmx_read_csr(CVMX_CIU_MBOX_CLRX(coreid));
+
+	/* Clear the mailbox to clear the interrupt */
+	cvmx_write_csr(CVMX_CIU_MBOX_CLRX(coreid), action);
+
+	if (action & SMP_CALL_FUNCTION)
+		smp_call_function_interrupt();
+
+	/* Check if we've been told to flush the icache */
+	if (action & SMP_ICACHE_FLUSH)
+		asm volatile ("synci 0($0)\n");
+	return IRQ_HANDLED;
+}
+
+
+/**
+ * Cause the function described by call_data to be executed on the passed
+ * cpu.  When the function has finished, increment the finished field of
+ * call_data.
+ *
+ * @param cpu
+ * @param action
+ */
+void core_send_ipi(int cpu, unsigned int action)
+{
+	int coreid = cpu_logical_map(cpu);
+	// printk("SMP: Mailbox send cpu=%d, coreid=%d, action=%u\n", cpu,
+	// coreid, action);
+	cvmx_write_csr(CVMX_CIU_MBOX_SETX(coreid), action);
+}
+
+
+/**
+ * Detect available CPUs, populate phys_cpu_present_map
+ */
+void plat_smp_setup(void)
+{
+	const int coreid = cvmx_get_core_num();
+	int cpus;
+	int id;
+
+	int core_mask = octeon_get_boot_coremask();
+
+	cpus_clear(phys_cpu_present_map);
+	__cpu_number_map[coreid] = 0;
+	__cpu_logical_map[0] = coreid;
+	cpu_set(0, phys_cpu_present_map);
+
+	cpus = 1;
+	for (id = 0; id < 16; id++) {
+		if ((id != coreid) && (core_mask & (1 << id))) {
+			cpu_set(cpus, phys_cpu_present_map);
+			__cpu_number_map[id] = cpus;
+			__cpu_logical_map[cpus] = id;
+			cpus++;
+		}
+	}
+}
+
+
+/**
+ * Firmware CPU startup hook
+ *
+ * @param cpu
+ * @param idle
+ */
+void prom_boot_secondary(int cpu, struct task_struct *idle)
+{
+	int count;
+
+	printk("SMP: Booting CPU%02d (CoreId %2d)...", cpu,
+	       cpu_logical_map(cpu));
+
+	octeon_processor_sp = __KSTK_TOS(idle);
+	octeon_processor_gp = (unsigned long) idle->thread_info;
+	__sync();		/* Use sync so all ops are done. This makes the
+				   cycle counter propagate in a more bounded
+				   amount of time */
+	octeon_processor_cycle = get_cycles();
+	octeon_processor_boot = cpu_logical_map(cpu);
+	mb();
+
+	count = 10000;
+	while (octeon_processor_sp && count) {
+		/* Waiting for processor to get the SP and GP */
+		udelay(1);
+		count--;
+	}
+	if (count == 0)
+		printk("Timeout\n");
+}
+
+
+/**
+ * After we've done initial boot, this function is called to allow the
+ * board code to clean up state, if needed
+ */
+void prom_init_secondary(void)
+{
+	const int coreid = cvmx_get_core_num();
+	cvmx_ciu_intx0_t interrupt_enable;
+
+	octeon_check_cpu_bist();
+
+	// printk("SMP: CPU%d (CoreId %lu) started\n", cpu, coreid);
+
+	/* Enable Mailbox interrupts to this core. These are the only
+	   interrupts allowed on line 3 */
+	cvmx_write_csr(CVMX_CIU_MBOX_CLRX(coreid), 0xffffffff);
+	interrupt_enable.u64 = 0;
+	interrupt_enable.s.mbox = 0x3;
+	cvmx_write_csr(CVMX_CIU_INTX_EN0((coreid * 2)), interrupt_enable.u64);
+	cvmx_write_csr(CVMX_CIU_INTX_EN0((coreid * 2 + 1)), 0);
+	cvmx_write_csr(CVMX_CIU_INTX_EN1((coreid * 2)), 0);
+	cvmx_write_csr(CVMX_CIU_INTX_EN1((coreid * 2 + 1)), 0);
+	set_c0_status(0x8c01);	/* Enable core interrupt processing for 2,3 and
+				   7 */
+}
+
+
+/**
+ * Callout to firmware before smp_init
+ *
+ * @param max_cpus
+ */
+void plat_prepare_cpus(unsigned int max_cpus)
+{
+	cvmx_write_csr(CVMX_CIU_MBOX_CLRX(cvmx_get_core_num()), 0xffffffff);
+	request_irq(OCTEON_IRQ_MBOX0, mailbox_interrupt, SA_SHIRQ, "mailbox0",
+		    mailbox_interrupt);
+	request_irq(OCTEON_IRQ_MBOX1, mailbox_interrupt, SA_SHIRQ, "mailbox1",
+		    mailbox_interrupt);
+}
+
+
+/**
+ * Last chance for the board code to finish SMP initialization before
+ * the CPU is "online".
+ */
+void prom_smp_finish(void)
+{
+#ifdef CONFIG_CAVIUM_GDB
+	unsigned long tmp;
+	/* Pulse MCD0 signal on Ctrl-C to stop all the cores. Also set the MCD0
+	   to be not masked by this core so we know the signal is received by
+	   someone */
+	asm volatile ("dmfc0 %0, $22\n"
+		      "ori   %0, %0, 0x9100\n" "dmtc0 %0, $22\n":"=r" (tmp));
+#endif
+
+#ifdef CONFIG_CAVIUM_OCTEON_USER_MEM
+	octeon_user_io_init();
+#endif
+
+	/* to generate the first CPU timer interrupt */
+	write_c0_compare(read_c0_count() + mips_hpt_frequency / HZ);
+}
+
+
+/**
+ * Hook for after all CPUs are online
+ */
+void prom_cpus_done(void)
+{
+#ifdef CONFIG_CAVIUM_GDB
+	unsigned long tmp;
+	/* Pulse MCD0 signal on Ctrl-C to stop all the cores. Also set the MCD0
+	   to be not masked by this core so we know the signal is received by
+	   someone */
+	asm volatile ("dmfc0 %0, $22\n"
+		      "ori   %0, %0, 0x9100\n" "dmtc0 %0, $22\n":"=r" (tmp));
+#endif
+}
+
+EXPORT_SYMBOL(__cpu_logical_map);
diff --git a/arch/mips/cavium-octeon/userio.c b/arch/mips/cavium-octeon/userio.c
new file mode 100644
index 0000000..4bd58e1
--- /dev/null
+++ b/arch/mips/cavium-octeon/userio.c
@@ -0,0 +1,162 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2004-2007 Cavium Networks
+ */
+#include <linux/init.h>
+#include <linux/delay.h>
+#include <linux/irq.h>
+#include <linux/interrupt.h>
+#include <linux/serial.h>
+#include <linux/types.h>
+#include <linux/string.h>	/* for memset */
+#include <linux/console.h>
+#include <linux/serial.h>
+#include <linux/tty.h>
+#include <asm/time.h>
+#include <linux/serial_core.h>
+
+#include <asm/reboot.h>
+#include <asm/io.h>
+#include <asm/time.h>
+#include <asm/processor.h>
+#include <asm/reboot.h>
+#include <asm/system.h>
+#include <asm/irq_cpu.h>
+#include <asm/mipsregs.h>
+#include <asm/gdb-stub.h>
+#include <asm/bootinfo.h>
+
+#include "hal.h"
+
+
+/**
+ *
+ * @return
+ */
+void octeon_user_io_init(void)
+{
+	octeon_cvmemctl_t cvmmemctl;
+	cvmx_iob_fau_timeout_t fau_timeout;
+	cvmx_pow_nw_tim_t nm_tim;
+	uint64_t cvmctl;
+
+	/* Get the current settings for CP0_CVMMEMCTL_REG */
+	cvmmemctl.u64 = __read_64bit_c0_register($11, 7);
+
+	cvmmemctl.s.dismarkwblongto = 1;	/* R/W If set, marked
+						   write-buffer entries time
+						   out the same as as other
+						   entries; if clear, marked
+						   write-buffer entries use the
+						   maximum timeout. */
+	cvmmemctl.s.dismrgclrwbto = 0;	/* R/W If set, a merged store does not
+					   clear the write-buffer entry timeout
+					   state. */
+	cvmmemctl.s.iobdmascrmsb = 0;	/* R/W Two bits that are the MSBs of
+					   the resultant CVMSEG LM word
+					   location for an IOBDMA. The other 8
+					   bits come from the SCRADDR field of
+					   the IOBDMA. */
+	cvmmemctl.s.syncwsmarked = 0;	/* R/W If set, SYNCWS and SYNCS only
+					   order marked stores; if clear,
+					   SYNCWS and SYNCS only order unmarked
+					   stores. SYNCWSMARKED has no effect
+					   when DISSYNCWS is set. */
+	cvmmemctl.s.dissyncws = 0;	/* R/W If set, SYNCWS acts as SYNCW and
+					   SYNCS acts as SYNC. */
+	if (OCTEON_IS_MODEL(OCTEON_CN38XX_PASS2))
+		cvmmemctl.s.diswbfst = 1;	/* R/W If set, no stall happens
+						   on write buffer full. */
+	else
+		cvmmemctl.s.diswbfst = 0;	/* R/W If set, no stall happens
+						   on write buffer full. */
+	cvmmemctl.s.xkmemenas = 0;	/* R/W If set (and SX set),
+					   supervisor-level loads/stores can
+					   use XKPHYS addresses with VA<48>==0 */
+#ifdef CONFIG_CAVIUM_OCTEON_USER_MEM
+	cvmmemctl.s.xkmemenau = 1;	/* R/W If set (and UX set), user-level
+					   loads/stores can use XKPHYS
+					   addresses with VA<48>==0 */
+#else
+	cvmmemctl.s.xkmemenau = 0;
+#endif
+	cvmmemctl.s.xkioenas = 0;	/* R/W If set (and SX set),
+					   supervisor-level loads/stores can
+					   use XKPHYS addresses with VA<48>==1 */
+#ifdef CONFIG_CAVIUM_OCTEON_USER_IO
+	cvmmemctl.s.xkioenau = 1;	/* R/W If set (and UX set), user-level
+					   loads/stores can use XKPHYS
+					   addresses with VA<48>==1 */
+#else
+	cvmmemctl.s.xkioenau = 0;
+#endif
+	cvmmemctl.s.allsyncw = 0;	/* R/W If set, all stores act as SYNCW
+					   (NOMERGE must be set when this is
+					   set) RW, reset to 0. */
+	cvmmemctl.s.nomerge = 0;	/* R/W If set, no stores merge, and all
+					   stores reach the coherent bus in
+					   order. */
+	cvmmemctl.s.didtto = 0;	/* R/W Selects the bit in the counter used for
+				   DID time-outs 0 = 231, 1 = 230, 2 = 229, 3 =
+				   214. Actual time-out is between 1� and 2�
+				   this interval. For example, with DIDTTO=3,
+				   expiration interval is between 16K and 32K. */
+	cvmmemctl.s.csrckalwys = 0;	/* R/W If set, the (mem) CSR clock
+					   never turns off. */
+	cvmmemctl.s.mclkalwys = 0;	/* R/W If set, mclk never turns off. */
+	cvmmemctl.s.wbfltime = 0;	/* R/W Selects the bit in the counter
+					   used for write buffer flush
+					   time-outs (WBFLT+11) is the bit
+					   position in an internal counter used
+					   to determine expiration. The write
+					   buffer expires between 1� and 2�
+					   this interval. For example, with
+					   WBFLT = 0, a write buffer expires
+					   between 2K and 4K cycles after the
+					   write buffer entry is allocated. */
+	cvmmemctl.s.istrnol2 = 0;	/* R/W If set, do not put Istream in
+					   the L2 cache. */
+	cvmmemctl.s.wbthresh = 10;	/* R/W The write buffer threshold. */
+#if CONFIG_CAVIUM_OCTEON_CVMSEG_SIZE > 0
+	cvmmemctl.s.cvmsegenak = 1;	/* R/W If set, CVMSEG is available for
+					   loads/stores in kernel/debug mode. */
+#else
+	cvmmemctl.s.cvmsegenak = 0;
+#endif
+	cvmmemctl.s.cvmsegenas = 0;	/* R/W If set, CVMSEG is available for
+					   loads/stores in supervisor mode. */
+	cvmmemctl.s.cvmsegenau = 0;	/* R/W If set, CVMSEG is available for
+					   loads/stores in user mode. */
+	cvmmemctl.s.lmemsz = CONFIG_CAVIUM_OCTEON_CVMSEG_SIZE;
+	/* R/W Size of local memory in cache blocks, 54 (6912 bytes) is max
+	   legal value. */
+
+	if (smp_processor_id() == 0)
+		printk("CVMSEG size: %d cache lines (%d bytes)\n",
+		       CONFIG_CAVIUM_OCTEON_CVMSEG_SIZE,
+		       CONFIG_CAVIUM_OCTEON_CVMSEG_SIZE * 128);
+
+	__write_64bit_c0_register($11, 7, cvmmemctl.u64);
+
+	/* Move the performance counter interrupts to IRQ 6 */
+	cvmctl = __read_64bit_c0_register($9, 7);
+	cvmctl &= ~(7 << 7);
+	cvmctl |= 6 << 7;
+	__write_64bit_c0_register($9, 7, cvmctl);
+
+	/* Set a default for the hardware timeouts */
+	fau_timeout.u64 = 0;
+	fau_timeout.s.tout_val = 0xfff;
+	fau_timeout.s.tout_enb = 0;	/* Disable tagwait FAU timeout */
+	cvmx_write_csr(CVMX_IOB_FAU_TIMEOUT, fau_timeout.u64);
+
+	nm_tim.u64 = 0;
+	nm_tim.s.nw_tim = 3;	/* 4096 cycles */
+	cvmx_write_csr(CVMX_POW_NW_TIM, nm_tim.u64);
+
+	write_c0_cacheerr(0);
+	write_c0_derraddr1(0);
+}
diff --git a/arch/mips/kernel/irq-octeon.c b/arch/mips/kernel/irq-octeon.c
new file mode 100644
index 0000000..ccecfd2
--- /dev/null
+++ b/arch/mips/kernel/irq-octeon.c
@@ -0,0 +1,472 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2004-2007 Cavium Networks
+ */
+#include <linux/init.h>
+#include <linux/interrupt.h>
+#include <linux/kernel.h>
+#include <linux/pci.h>
+
+#include <asm/irq_cpu.h>
+#include <asm/mipsregs.h>
+#include <asm/system.h>
+
+#include "../cavium-octeon/hal.h"
+
+DEFINE_RWLOCK(octeon_irq_ciu0_rwlock);
+DEFINE_RWLOCK(octeon_irq_ciu1_rwlock);
+DEFINE_SPINLOCK(octeon_irq_msi_lock);
+
+static void octeon_irq_core_ack(unsigned int irq)
+{
+	/* We don't need to disable IRQs to make these atomic since they are
+	   already disabled earlier in the low level interrupt code */
+	clear_c0_status(0x100 << irq);
+	/* The two user interrupts must be cleared manually */
+	if (irq < 2)
+		clear_c0_cause(0x100 << irq);
+}
+
+static void octeon_irq_core_eoi(unsigned int irq)
+{
+	irq_desc_t *desc = irq_desc + irq;
+	/* If an IRQ is being processed while we are disabling it the handler
+	   will attempt to unmask the interrupt after it has been disabled */
+	if (desc->status & IRQ_DISABLED)
+		return;
+	/* We don't need to disable IRQs to make these atomic since they are
+	   already disabled earlier in the low level interrupt code */
+	set_c0_status(0x100 << irq);
+}
+
+static void octeon_irq_core_enable(unsigned int irq)
+{
+	/* We need to disable interrupts to make sure our updates are atomic */
+	unsigned long flags;
+	local_irq_save(flags);
+	set_c0_status(0x100 << irq);
+	local_irq_restore(flags);
+}
+
+static void octeon_irq_core_disable_local(unsigned int irq)
+{
+	/* We need to disable interrupts to make sure our updates are atomic */
+	unsigned long flags;
+	local_irq_save(flags);
+	clear_c0_status(0x100 << irq);
+	local_irq_restore(flags);
+}
+
+static void octeon_irq_core_disable(unsigned int irq)
+{
+#ifdef CONFIG_SMP
+	on_each_cpu((void (*)(void *)) octeon_irq_core_disable_local,
+		    (void *) (long) irq, 0, 1);
+#else
+	octeon_irq_core_disable_local(irq);
+#endif
+}
+
+struct irq_chip octeon_irq_chip_core = {
+	.name = "Core",
+	.enable = octeon_irq_core_enable,
+	.disable = octeon_irq_core_disable,
+	.ack = octeon_irq_core_ack,
+	.eoi = octeon_irq_core_eoi,
+};
+
+
+static void octeon_irq_ciu0_ack(unsigned int irq)
+{
+	/* In order to avoid any locking accessing the CIU, we acknowledge CIU
+	   interrupts by disabling all of them. This way we can use a per core
+	   register and avoid any out of core locking requirements. This has
+	   the side affect that CIU interrupts can't be processed recursively */
+	/* We don't need to disable IRQs to make these atomic since they are
+	   already disabled earlier in the low level interrupt code */
+	clear_c0_status(0x100 << 2);
+}
+
+static void octeon_irq_ciu0_eoi(unsigned int irq)
+{
+	/* Enable all CIU interrupts again */
+	/* We don't need to disable IRQs to make these atomic since they are
+	   already disabled earlier in the low level interrupt code */
+	set_c0_status(0x100 << 2);
+}
+
+static void octeon_irq_ciu0_enable(unsigned int irq)
+{
+	int coreid = cvmx_get_core_num();
+	unsigned long flags;
+	uint64_t en0;
+	int bit = irq - OCTEON_IRQ_WORKQ0;	/* Bit 0-63 of EN0 */
+
+	/* A read lock is used here to make sure only one core is ever updating
+	   the CIU enable bits at a time. During an enable the cores don't
+	   interfere with each other. During a disable the write lock stops any
+	   enables that might cause a problem */
+	read_lock_irqsave(&octeon_irq_ciu0_rwlock, flags);
+	en0 = cvmx_read_csr(CVMX_CIU_INTX_EN0(coreid * 2));
+	en0 |= 1ull << bit;
+	cvmx_write_csr(CVMX_CIU_INTX_EN0(coreid * 2), en0);
+	cvmx_read_csr(CVMX_CIU_INTX_EN0(coreid * 2));
+	read_unlock_irqrestore(&octeon_irq_ciu0_rwlock, flags);
+}
+
+static void octeon_irq_ciu0_disable(unsigned int irq)
+{
+	int bit = irq - OCTEON_IRQ_WORKQ0;	/* Bit 0-63 of EN0 */
+	unsigned long flags;
+	uint64_t en0;
+#ifdef CONFIG_SMP
+	int cpu;
+	write_lock_irqsave(&octeon_irq_ciu0_rwlock, flags);
+	for (cpu = 0; cpu < NR_CPUS; cpu++) {
+		if (cpu_present(cpu)) {
+			int coreid = cpu_logical_map(cpu);
+			en0 = cvmx_read_csr(CVMX_CIU_INTX_EN0(coreid * 2));
+			en0 &= ~(1ull << bit);
+			cvmx_write_csr(CVMX_CIU_INTX_EN0(coreid * 2), en0);
+		}
+	}
+	/* We need to do a read after the last update to make sure all of them
+	   are done */
+	cvmx_read_csr(CVMX_CIU_INTX_EN0(cvmx_get_core_num() * 2));
+	write_unlock_irqrestore(&octeon_irq_ciu0_rwlock, flags);
+#else
+	int coreid = cvmx_get_core_num();
+	local_irq_save(flags);
+	en0 = cvmx_read_csr(CVMX_CIU_INTX_EN0(coreid * 2));
+	en0 &= ~(1ull << bit);
+	cvmx_write_csr(CVMX_CIU_INTX_EN0(coreid * 2), en0);
+	cvmx_read_csr(CVMX_CIU_INTX_EN0(coreid * 2));
+	local_irq_restore(flags);
+#endif
+}
+
+#ifdef CONFIG_SMP
+static void octeon_irq_ciu0_set_affinity(unsigned int irq, cpumask_t dest)
+{
+	int cpu;
+	unsigned long flags;
+	irq_desc_t *desc = irq_desc + irq;
+	int bit = irq - OCTEON_IRQ_WORKQ0;	/* Bit 0-63 of EN0 */
+
+	spin_lock_irqsave(&desc->lock, flags);
+	write_lock(&octeon_irq_ciu0_rwlock);
+	for (cpu = 0; cpu < NR_CPUS; cpu++) {
+		if (cpu_present(cpu)) {
+			int coreid = cpu_logical_map(cpu);
+			uint64_t en0 =
+				cvmx_read_csr(CVMX_CIU_INTX_EN0(coreid * 2));
+			if (cpu_isset(cpu, dest))
+				en0 |= 1ull << bit;
+			else
+				en0 &= ~(1ull << bit);
+			cvmx_write_csr(CVMX_CIU_INTX_EN0(coreid * 2), en0);
+		}
+	}
+	/* We need to do a read after the last update to make sure all of them
+	   are done */
+	cvmx_read_csr(CVMX_CIU_INTX_EN0(cvmx_get_core_num() * 2));
+	write_unlock(&octeon_irq_ciu0_rwlock);
+	spin_unlock_irqrestore(&desc->lock, flags);
+}
+#endif
+
+struct irq_chip octeon_irq_chip_ciu0 = {
+	.name = "CIU0",
+	.enable = octeon_irq_ciu0_enable,
+	.disable = octeon_irq_ciu0_disable,
+	.ack = octeon_irq_ciu0_ack,
+	.eoi = octeon_irq_ciu0_eoi,
+#ifdef CONFIG_SMP
+	.set_affinity = octeon_irq_ciu0_set_affinity,
+#endif
+};
+
+
+static void octeon_irq_ciu1_ack(unsigned int irq)
+{
+	/* In order to avoid any locking accessing the CIU, we acknowledge CIU
+	   interrupts by disabling all of them. This way we can use a per core
+	   register and avoid any out of core locking requirements. This has
+	   the side affect that CIU interrupts can't be processed recursively */
+	/* We don't need to disable IRQs to make these atomic since they are
+	   already disabled earlier in the low level interrupt code */
+	clear_c0_status(0x100 << 3);
+}
+
+static void octeon_irq_ciu1_eoi(unsigned int irq)
+{
+	/* Enable all CIU interrupts again */
+	/* We don't need to disable IRQs to make these atomic since they are
+	   already disabled earlier in the low level interrupt code */
+	set_c0_status(0x100 << 3);
+}
+
+static void octeon_irq_ciu1_enable(unsigned int irq)
+{
+	int coreid = cvmx_get_core_num();
+	unsigned long flags;
+	uint64_t en1;
+	int bit = irq - OCTEON_IRQ_WDOG0;	/* Bit 0-63 of EN1 */
+
+	/* A read lock is used here to make sure only one core is ever updating
+	   the CIU enable bits at a time. During an enable the cores don't
+	   interfere with each other. During a disable the write lock stops any
+	   enables that might cause a problem */
+	read_lock_irqsave(&octeon_irq_ciu1_rwlock, flags);
+	en1 = cvmx_read_csr(CVMX_CIU_INTX_EN1(coreid * 2 + 1));
+	en1 |= 1ull << bit;
+	cvmx_write_csr(CVMX_CIU_INTX_EN1(coreid * 2 + 1), en1);
+	cvmx_read_csr(CVMX_CIU_INTX_EN1(coreid * 2 + 1));
+	read_unlock_irqrestore(&octeon_irq_ciu1_rwlock, flags);
+}
+
+static void octeon_irq_ciu1_disable(unsigned int irq)
+{
+	int bit = irq - OCTEON_IRQ_WDOG0;	/* Bit 0-63 of EN1 */
+	unsigned long flags;
+	uint64_t en1;
+#ifdef CONFIG_SMP
+	int cpu;
+	write_lock_irqsave(&octeon_irq_ciu1_rwlock, flags);
+	for (cpu = 0; cpu < NR_CPUS; cpu++) {
+		if (cpu_present(cpu)) {
+			int coreid = cpu_logical_map(cpu);
+			en1 = cvmx_read_csr(CVMX_CIU_INTX_EN1(coreid * 2 + 1));
+			en1 &= ~(1ull << bit);
+			cvmx_write_csr(CVMX_CIU_INTX_EN1(coreid * 2 + 1), en1);
+		}
+	}
+	/* We need to do a read after the last update to make sure all of them
+	   are done */
+	cvmx_read_csr(CVMX_CIU_INTX_EN1(cvmx_get_core_num() * 2 + 1));
+	write_unlock_irqrestore(&octeon_irq_ciu1_rwlock, flags);
+#else
+	int coreid = cvmx_get_core_num();
+	local_irq_save(flags);
+	en1 = cvmx_read_csr(CVMX_CIU_INTX_EN1(coreid * 2 + 1));
+	en1 &= ~(1ull << bit);
+	cvmx_write_csr(CVMX_CIU_INTX_EN1(coreid * 2 + 1), en1);
+	cvmx_read_csr(CVMX_CIU_INTX_EN1(coreid * 2 + 1));
+	local_irq_restore(flags);
+#endif
+}
+
+#ifdef CONFIG_SMP
+static void octeon_irq_ciu1_set_affinity(unsigned int irq, cpumask_t dest)
+{
+	int cpu;
+	unsigned long flags;
+	irq_desc_t *desc = irq_desc + irq;
+	int bit = irq - OCTEON_IRQ_WDOG0;	/* Bit 0-63 of EN1 */
+
+	spin_lock_irqsave(&desc->lock, flags);
+	write_lock(&octeon_irq_ciu1_rwlock);
+	for (cpu = 0; cpu < NR_CPUS; cpu++) {
+		if (cpu_present(cpu)) {
+			int coreid = cpu_logical_map(cpu);
+			uint64_t en1 =
+				cvmx_read_csr(CVMX_CIU_INTX_EN1
+					      (coreid * 2 + 1));
+			if (cpu_isset(cpu, dest))
+				en1 |= 1ull << bit;
+			else
+				en1 &= ~(1ull << bit);
+			cvmx_write_csr(CVMX_CIU_INTX_EN1(coreid * 2 + 1), en1);
+		}
+	}
+	/* We need to do a read after the last update to make sure all of them
+	   are done */
+	cvmx_read_csr(CVMX_CIU_INTX_EN1(cvmx_get_core_num() * 2 + 1));
+	write_unlock(&octeon_irq_ciu1_rwlock);
+	spin_unlock_irqrestore(&desc->lock, flags);
+}
+#endif
+
+struct irq_chip octeon_irq_chip_ciu1 = {
+	.name = "CIU1",
+	.enable = octeon_irq_ciu1_enable,
+	.disable = octeon_irq_ciu1_disable,
+	.ack = octeon_irq_ciu1_ack,
+	.eoi = octeon_irq_ciu1_eoi,
+#ifdef CONFIG_SMP
+	.set_affinity = octeon_irq_ciu1_set_affinity,
+#endif
+};
+
+
+static void octeon_irq_i8289_master_unmask(unsigned int irq)
+{
+	unsigned long flags;
+	local_irq_save(flags);
+	outb(inb(0x21) & ~(1 << (irq - OCTEON_IRQ_I8259M0)), 0x21);
+	local_irq_restore(flags);
+}
+
+static void octeon_irq_i8289_master_mask(unsigned int irq)
+{
+	unsigned long flags;
+	local_irq_save(flags);
+	outb(inb(0x21) | (1 << (irq - OCTEON_IRQ_I8259M0)), 0x21);
+	local_irq_restore(flags);
+}
+
+struct irq_chip octeon_irq_chip_i8259_master = {
+	.name = "i8259M",
+	.mask = octeon_irq_i8289_master_mask,
+	.mask_ack = octeon_irq_i8289_master_mask,
+	.unmask = octeon_irq_i8289_master_unmask,
+	.eoi = octeon_irq_i8289_master_unmask,
+};
+
+
+static void octeon_irq_i8289_slave_unmask(unsigned int irq)
+{
+	outb(inb(0xa1) & ~(1 << (irq - OCTEON_IRQ_I8259S0)), 0xa1);
+}
+
+static void octeon_irq_i8289_slave_mask(unsigned int irq)
+{
+	outb(inb(0xa1) | (1 << (irq - OCTEON_IRQ_I8259S0)), 0xa1);
+}
+
+struct irq_chip octeon_irq_chip_i8259_slave = {
+	.name = "i8259S",
+	.mask = octeon_irq_i8289_slave_mask,
+	.mask_ack = octeon_irq_i8289_slave_mask,
+	.unmask = octeon_irq_i8289_slave_unmask,
+	.eoi = octeon_irq_i8289_slave_unmask,
+};
+
+#ifdef CONFIG_PCI_MSI
+
+static void octeon_irq_msi_ack(unsigned int irq)
+{
+	if (!octeon_has_feature(OCTEON_FEATURE_PCIE)) {
+		/* These chips have PCI */
+		cvmx_write_csr(CVMX_NPI_NPI_MSI_RCV,
+			       1ull << (irq - OCTEON_IRQ_MSI_BIT0));
+	} else {
+		/* These chips have PCIe. Thankfully the ACK doesn't need any
+		   locking */
+		cvmx_write_csr(CVMX_PEXP_NPEI_MSI_RCV0,
+			       1ull << (irq - OCTEON_IRQ_MSI_BIT0));
+	}
+}
+
+static void octeon_irq_msi_eoi(unsigned int irq)
+{
+	/* Nothing needed */
+}
+
+static void octeon_irq_msi_enable(unsigned int irq)
+{
+	if (!octeon_has_feature(OCTEON_FEATURE_PCIE)) {
+		/* Octeon PCI doesn't have the ability to mask/unmask MSI
+		   interrupts individually. Instead of masking/unmasking them
+		   in groups of 16, we simple assume MSI devices are well
+		   behaved. MSI interrupts are always enable and the ACK is
+		   assumed to be enough */
+	} else {
+		/* These chips have PCIe. Note that we only support the first
+		   64 MSI interrupts. Unfortunately all the MSI enables are in
+		   the same register. We use MSI0's lock to control access to
+		   them all. */
+		uint64_t en;
+		unsigned long flags;
+		spin_lock_irqsave(&octeon_irq_msi_lock, flags);
+		en = cvmx_read_csr(CVMX_PEXP_NPEI_MSI_ENB0);
+		en |= 1ull << (irq - OCTEON_IRQ_MSI_BIT0);
+		cvmx_write_csr(CVMX_PEXP_NPEI_MSI_ENB0, en);
+		cvmx_read_csr(CVMX_PEXP_NPEI_MSI_ENB0);
+		spin_unlock_irqrestore(&octeon_irq_msi_lock, flags);
+	}
+}
+
+static void octeon_irq_msi_disable(unsigned int irq)
+{
+	if (!octeon_has_feature(OCTEON_FEATURE_PCIE)) {
+		/* See comment in enable */
+	} else {
+		/* These chips have PCIe. Note that we only support the first
+		   64 MSI interrupts. Unfortunately all the MSI enables are in
+		   the same register. We use MSI0's lock to control access to
+		   them all. */
+		uint64_t en;
+		unsigned long flags;
+		spin_lock_irqsave(&octeon_irq_msi_lock, flags);
+		en = cvmx_read_csr(CVMX_PEXP_NPEI_MSI_ENB0);
+		en &= ~(1ull << (irq - OCTEON_IRQ_MSI_BIT0));
+		cvmx_write_csr(CVMX_PEXP_NPEI_MSI_ENB0, en);
+		cvmx_read_csr(CVMX_PEXP_NPEI_MSI_ENB0);
+		spin_unlock_irqrestore(&octeon_irq_msi_lock, flags);
+	}
+}
+
+struct irq_chip octeon_irq_chip_msi = {
+	.name = "MSI",
+	.enable = octeon_irq_msi_enable,
+	.disable = octeon_irq_msi_disable,
+	.ack = octeon_irq_msi_ack,
+	.eoi = octeon_irq_msi_eoi,
+};
+#endif
+
+void __init arch_init_irq(void)
+{
+	int irq;
+
+	if (NR_IRQS < OCTEON_IRQ_LAST)
+		printk("octeon_irq_init: NR_IRQS is set too low\n");
+
+	/* 0-7 Mips internal */
+	for (irq = OCTEON_IRQ_SW0; irq <= OCTEON_IRQ_TIMER; irq++) {
+		set_irq_chip_and_handler(irq, &octeon_irq_chip_core,
+					 handle_percpu_irq);
+	}
+
+	/* 8-71 CIU_INT_SUM0 */
+	for (irq = OCTEON_IRQ_WORKQ0; irq <= OCTEON_IRQ_BOOTDMA; irq++) {
+		set_irq_chip_and_handler(irq, &octeon_irq_chip_ciu0,
+					 handle_percpu_irq);
+	}
+
+	/* 72-135 CIU_INT_SUM1 */
+	for (irq = OCTEON_IRQ_WDOG0; irq <= OCTEON_IRQ_RESERVED135; irq++) {
+		set_irq_chip_and_handler(irq, &octeon_irq_chip_ciu1,
+					 handle_percpu_irq);
+	}
+
+	/* 136 - 143 are reserved to align the i8259 in a multiple of 16. This
+	   alignment is necessary since old style ISA interrupts hanging off
+	   the i8259 have internal alignment assumptions */
+
+	/* 144-151 i8259 master controller */
+	for (irq = OCTEON_IRQ_I8259M0; irq <= OCTEON_IRQ_I8259M7; irq++) {
+		set_irq_chip_and_handler(irq, &octeon_irq_chip_i8259_master,
+					 handle_level_irq);
+	}
+
+	/* 152-159 i8259 slave controller */
+	for (irq = OCTEON_IRQ_I8259S0; irq <= OCTEON_IRQ_I8259S7; irq++) {
+		set_irq_chip_and_handler(irq, &octeon_irq_chip_i8259_slave,
+					 handle_level_irq);
+	}
+
+#ifdef CONFIG_PCI_MSI
+	/* 160-223 PCI/PCIe MSI interrupts */
+	for (irq = OCTEON_IRQ_MSI_BIT0; irq <= OCTEON_IRQ_MSI_BIT63; irq++) {
+		set_irq_chip_and_handler(irq, &octeon_irq_chip_msi,
+					 handle_percpu_irq);
+	}
+#endif
+
+	set_c0_status(0x300 << 2);
+}
diff --git a/arch/mips/kernel/octeon_switch.S b/arch/mips/kernel/octeon_switch.S
new file mode 100644
index 0000000..314a59e
--- /dev/null
+++ b/arch/mips/kernel/octeon_switch.S
@@ -0,0 +1,503 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 1994, 1995, 1996, 1998, 1999, 2002, 2003 Ralf Baechle
+ * Copyright (C) 1996 David S. Miller (dm@engr.sgi.com)
+ * Copyright (C) 1994, 1995, 1996, by Andreas Busse
+ * Copyright (C) 1999 Silicon Graphics, Inc.
+ * Copyright (C) 2000 MIPS Technologies, Inc.
+ *    written by Carsten Langgaard, carstenl@mips.com
+ */
+#include <asm/asm.h>
+#include <asm/cachectl.h>
+#include <asm/fpregdef.h>
+#include <asm/mipsregs.h>
+#include <asm/asm-offsets.h>
+#include <asm/page.h>
+#include <asm/pgtable-bits.h>
+#include <asm/regdef.h>
+#include <asm/stackframe.h>
+#include <asm/thread_info.h>
+
+#include <asm/asmmacro.h>
+
+/*
+ * Offset to the current process status flags, the first 32 bytes of the
+ * stack are not used.
+ */
+#define ST_OFF (_THREAD_SIZE - 32 - PT_SIZE + PT_STATUS)
+
+/*
+ * task_struct *resume(task_struct *prev, task_struct *next,
+ *                     struct thread_info *next_ti)
+ */
+	.align	7
+	LEAF(resume)
+	.set arch=octeon
+#ifndef CONFIG_CPU_HAS_LLSC
+	sw	zero, ll_bit
+#endif
+	mfc0	t1, CP0_STATUS
+	LONG_S	t1, THREAD_STATUS(a0)
+	cpu_save_nonscratch a0
+	LONG_S	ra, THREAD_REG31(a0)
+
+	/* check if we need to save COP2 registers */
+	PTR_L	t2, TASK_THREAD_INFO(a0)
+	LONG_L	t0, ST_OFF(t2)
+	bbit0	t0, 30, 1f
+
+	/* Disable COP2 in the stored process state */
+	li	t1, ST0_CU2
+	xor	t0, t1
+	LONG_S	t0, ST_OFF(t2)
+
+	/* Enable COP2 so we can save it */
+	mfc0	t0, CP0_STATUS
+	or	t0, t1
+	mtc0	t0, CP0_STATUS
+
+	/* Save COP2 */
+	daddu	a0, THREAD_CP2
+	jal octeon_cop2_save
+	dsubu	a0, THREAD_CP2
+
+	/* Disable COP2 now that we are done */
+	mfc0	t0, CP0_STATUS
+	li	t1, ST0_CU2
+	xor	t0, t1
+	mtc0	t0, CP0_STATUS
+
+1:
+#if CONFIG_CAVIUM_OCTEON_CVMSEG_SIZE > 0
+	/* Check if we need to store CVMSEG state */
+	mfc0	t0, $11,7 	/* CvmMemCtl */
+	bbit0	t0, 6, 3f	/* Is user access enabled? */
+
+	/* Store the CVMSEG state */
+	andi	t0, 0x3f	/* Extract the size of CVMSEG */
+	sll	t0, 7-LONGLOG-1	/* Multiply * (cache line size/sizeof(long)/2) */
+	li	t1, -32768 	/* Base address of CVMSEG */
+	LONG_ADDI t2, a0, THREAD_CVMSEG	/* Where to store CVMSEG to */
+	synciobdma
+2:
+	.set noreorder
+	LONG_L	t8, 0(t1)	/* Load from CVMSEG */
+	subu	t0, 1		/* Decrement loop var */
+	LONG_L	t9, LONGSIZE(t1)/* Load from CVMSEG */
+	LONG_ADDU t1, LONGSIZE*2 /* Increment loc in CVMSEG */
+	LONG_S	t8, 0(t2)	/* Store CVMSEG to thread storage */
+	LONG_ADDU t2, LONGSIZE*2 /* Increment loc in thread storage */
+	bnez	t0, 2b		/* Loop until we've copied it all */
+	 LONG_S	t9, -LONGSIZE(t2)/* Store CVMSEG to thread storage */
+	.set reorder
+
+	/* Disable access to CVMSEG */
+	mfc0	t0, $11,7 	/* CvmMemCtl */
+	xori	t0, t0, 0x40	/* Bit 6 is CVMSEG user enable */
+	mtc0	t0, $11,7 	/* CvmMemCtl */
+#endif
+3:
+	/*
+	 * The order of restoring the registers takes care of the race
+	 * updating $28, $29 and kernelsp without disabling ints.
+	 */
+	move	$28, a2
+	cpu_restore_nonscratch a1
+
+#if (_THREAD_SIZE - 32) < 0x8000
+	PTR_ADDIU	t0, $28, _THREAD_SIZE - 32
+#else
+	PTR_LI		t0, _THREAD_SIZE - 32
+	PTR_ADDU	t0, $28
+#endif
+	set_saved_sp	t0, t1, t2
+
+	mfc0	t1, CP0_STATUS		/* Do we really need this? */
+	li	a3, 0xff01
+	and	t1, a3
+	LONG_L	a2, THREAD_STATUS(a1)
+	nor	a3, $0, a3
+	and	a2, a3
+	or	a2, t1
+	mtc0	a2, CP0_STATUS
+	move	v0, a0
+	jr	ra
+	END(resume)
+
+/*
+ * void octeon_cop2_save(struct octeon_cop2_state *a0)
+ */
+	.align	7
+	LEAF(octeon_cop2_save)
+
+	dmfc0	t9, $9,7	/* CvmCtl register. */
+
+        /* Save the COP2 CRC state */
+	dmfc2	t0, 0x0201
+	dmfc2	t1, 0x0202
+	dmfc2	t2, 0x0200
+	sd	t0, OCTEON_CP2_CRC_IV(a0)
+	sd	t1, OCTEON_CP2_CRC_LENGTH(a0)
+	sd	t2, OCTEON_CP2_CRC_POLY(a0)
+
+	bbit1	t9, 28, 1f	/* Skip next instructions if CvmCtl[NODFA_CP2] set */
+
+	/* Save the LLM state */
+	dmfc2	t0, 0x0402
+	dmfc2	t1, 0x040A
+	sd	t0, OCTEON_CP2_LLM_DAT(a0)
+	sd	t1, OCTEON_CP2_LLM_DAT+8(a0)
+
+1:      bbit1	t9, 26, 3f	/* done if CvmCtl[NOCRYPTO] set */
+
+	/* Save the COP2 crypto state */
+        /* this part is mostly common to both pass 1 and later revisions */
+	dmfc2 	t0, 0x0084
+	dmfc2 	t1, 0x0080
+	dmfc2 	t2, 0x0081
+	dmfc2 	t3, 0x0082
+	sd	t0, OCTEON_CP2_3DES_IV(a0)
+	dmfc2 	t0, 0x0088
+	sd	t1, OCTEON_CP2_3DES_KEY(a0)
+	dmfc2 	t1, 0x0111                      /* only necessary for pass 1 */
+	sd	t2, OCTEON_CP2_3DES_KEY+8(a0)
+	dmfc2 	t2, 0x0102
+	sd	t3, OCTEON_CP2_3DES_KEY+16(a0)
+	dmfc2 	t3, 0x0103
+	sd	t0, OCTEON_CP2_3DES_RESULT(a0)
+	dmfc2 	t0, 0x0104
+	sd	t1, OCTEON_CP2_AES_INP0(a0)     /* only necessary for pass 1 */
+	dmfc2 	t1, 0x0105
+	sd	t2, OCTEON_CP2_AES_IV(a0)
+	dmfc2	t2, 0x0106
+	sd	t3, OCTEON_CP2_AES_IV+8(a0)
+	dmfc2 	t3, 0x0107
+	sd	t0, OCTEON_CP2_AES_KEY(a0)
+	dmfc2	t0, 0x0110
+	sd	t1, OCTEON_CP2_AES_KEY+8(a0)
+	dmfc2	t1, 0x0100
+	sd	t2, OCTEON_CP2_AES_KEY+16(a0)
+	dmfc2	t2, 0x0101
+	sd	t3, OCTEON_CP2_AES_KEY+24(a0)
+	mfc0	t3, $15,0 	/* Get the processor ID register */
+	sd	t0, OCTEON_CP2_AES_KEYLEN(a0)
+	li	t0, 0x000d0000	/* This is the processor ID of Octeon Pass1 */
+	sd	t1, OCTEON_CP2_AES_RESULT(a0)
+	sd	t2, OCTEON_CP2_AES_RESULT+8(a0)
+
+	beq	t3, t0, 2f	/* Skip to the Pass1 version of the remainder of the COP2 state */
+
+        /* the non-pass1 state when !CvmCtl[NOCRYPTO] */
+	dmfc2	t1, 0x0240
+	dmfc2	t2, 0x0241
+	dmfc2	t3, 0x0242
+	dmfc2	t0, 0x0243
+	sd	t1, OCTEON_CP2_HSH_DATW(a0)
+	dmfc2	t1, 0x0244
+	sd	t2, OCTEON_CP2_HSH_DATW+8(a0)
+	dmfc2	t2, 0x0245
+	sd	t3, OCTEON_CP2_HSH_DATW+16(a0)
+	dmfc2	t3, 0x0246
+	sd	t0, OCTEON_CP2_HSH_DATW+24(a0)
+	dmfc2	t0, 0x0247
+	sd	t1, OCTEON_CP2_HSH_DATW+32(a0)
+	dmfc2	t1, 0x0248
+	sd	t2, OCTEON_CP2_HSH_DATW+40(a0)
+	dmfc2	t2, 0x0249
+	sd	t3, OCTEON_CP2_HSH_DATW+48(a0)
+	dmfc2	t3, 0x024A
+	sd	t0, OCTEON_CP2_HSH_DATW+56(a0)
+	dmfc2	t0, 0x024B
+	sd	t1, OCTEON_CP2_HSH_DATW+64(a0)
+	dmfc2	t1, 0x024C
+	sd	t2, OCTEON_CP2_HSH_DATW+72(a0)
+	dmfc2	t2, 0x024D
+	sd	t3, OCTEON_CP2_HSH_DATW+80(a0)
+	dmfc2 	t3, 0x024E
+	sd	t0, OCTEON_CP2_HSH_DATW+88(a0)
+	dmfc2	t0, 0x0250
+	sd	t1, OCTEON_CP2_HSH_DATW+96(a0)
+	dmfc2	t1, 0x0251
+	sd	t2, OCTEON_CP2_HSH_DATW+104(a0)
+	dmfc2	t2, 0x0252
+	sd	t3, OCTEON_CP2_HSH_DATW+112(a0)
+	dmfc2	t3, 0x0253
+	sd	t0, OCTEON_CP2_HSH_IVW(a0)
+	dmfc2	t0, 0x0254
+	sd	t1, OCTEON_CP2_HSH_IVW+8(a0)
+	dmfc2	t1, 0x0255
+	sd	t2, OCTEON_CP2_HSH_IVW+16(a0)
+	dmfc2	t2, 0x0256
+	sd	t3, OCTEON_CP2_HSH_IVW+24(a0)
+	dmfc2	t3, 0x0257
+	sd	t0, OCTEON_CP2_HSH_IVW+32(a0)
+	dmfc2 	t0, 0x0258
+	sd	t1, OCTEON_CP2_HSH_IVW+40(a0)
+	dmfc2 	t1, 0x0259
+	sd	t2, OCTEON_CP2_HSH_IVW+48(a0)
+	dmfc2	t2, 0x025E
+	sd	t3, OCTEON_CP2_HSH_IVW+56(a0)
+	dmfc2	t3, 0x025A
+	sd	t0, OCTEON_CP2_GFM_MULT(a0)
+	dmfc2	t0, 0x025B
+	sd	t1, OCTEON_CP2_GFM_MULT+8(a0)
+	sd	t2, OCTEON_CP2_GFM_POLY(a0)
+	sd	t3, OCTEON_CP2_GFM_RESULT(a0)
+	sd	t0, OCTEON_CP2_GFM_RESULT+8(a0)
+	jr	ra
+
+2:      /* pass 1 special stuff when !CvmCtl[NOCRYPTO] */
+	dmfc2	t3, 0x0040
+	dmfc2	t0, 0x0041
+	dmfc2	t1, 0x0042
+	dmfc2	t2, 0x0043
+	sd	t3, OCTEON_CP2_HSH_DATW(a0)
+	dmfc2	t3, 0x0044
+	sd	t0, OCTEON_CP2_HSH_DATW+8(a0)
+	dmfc2	t0, 0x0045
+	sd	t1, OCTEON_CP2_HSH_DATW+16(a0)
+	dmfc2	t1, 0x0046
+	sd	t2, OCTEON_CP2_HSH_DATW+24(a0)
+	dmfc2	t2, 0x0048
+	sd	t3, OCTEON_CP2_HSH_DATW+32(a0)
+	dmfc2	t3, 0x0049
+	sd	t0, OCTEON_CP2_HSH_DATW+40(a0)
+	dmfc2	t0, 0x004A
+	sd	t1, OCTEON_CP2_HSH_DATW+48(a0)
+	sd	t2, OCTEON_CP2_HSH_IVW(a0)
+	sd	t3, OCTEON_CP2_HSH_IVW+8(a0)
+	sd	t0, OCTEON_CP2_HSH_IVW+16(a0)
+
+3:      /* pass 1 or CvmCtl[NOCRYPTO] set */
+	jr	ra
+	END(octeon_cop2_save)
+
+/*
+ * void octeon_cop2_restore(struct octeon_cop2_state *a0)
+ */
+	.align	7
+	.set push
+	.set noreorder
+	LEAF(octeon_cop2_restore)
+        /* First cache line was prefetched before the call */
+        pref    4,  128(a0)
+	dmfc0	t9, $9,7	/* CvmCtl register. */
+
+        pref    4,  256(a0)
+	ld	t0, OCTEON_CP2_CRC_IV(a0)
+        pref    4,  384(a0)
+	ld	t1, OCTEON_CP2_CRC_LENGTH(a0)
+	ld	t2, OCTEON_CP2_CRC_POLY(a0)
+
+	/* Restore the COP2 CRC state */
+	dmtc2	t0, 0x0201
+	dmtc2 	t1, 0x1202
+	bbit1	t9, 28, 2f	/* Skip LLM if CvmCtl[NODFA_CP2] is set */
+	 dmtc2	t2, 0x4200
+
+	/* Restore the LLM state */
+	ld	t0, OCTEON_CP2_LLM_DAT(a0)
+	ld	t1, OCTEON_CP2_LLM_DAT+8(a0)
+	dmtc2	t0, 0x0402
+	dmtc2	t1, 0x040A
+
+2:
+	bbit1	t9, 26, done_restore	/* done if CvmCtl[NOCRYPTO] set */
+	 nop
+
+	/* Restore the COP2 crypto state common to pass 1 and pass 2 */
+	ld	t0, OCTEON_CP2_3DES_IV(a0)
+	ld	t1, OCTEON_CP2_3DES_KEY(a0)
+	ld	t2, OCTEON_CP2_3DES_KEY+8(a0)
+	dmtc2 	t0, 0x0084
+	ld	t0, OCTEON_CP2_3DES_KEY+16(a0)
+	dmtc2 	t1, 0x0080
+	ld	t1, OCTEON_CP2_3DES_RESULT(a0)
+	dmtc2 	t2, 0x0081
+	ld	t2, OCTEON_CP2_AES_INP0(a0)       /* only really needed for pass 1 */
+	dmtc2	t0, 0x0082
+	ld	t0, OCTEON_CP2_AES_IV(a0)
+	dmtc2 	t1, 0x0098
+	ld	t1, OCTEON_CP2_AES_IV+8(a0)
+	dmtc2 	t2, 0x010A                        /* only really needed for pass 1 */
+	ld	t2, OCTEON_CP2_AES_KEY(a0)
+	dmtc2 	t0, 0x0102
+	ld	t0, OCTEON_CP2_AES_KEY+8(a0)
+	dmtc2	t1, 0x0103
+	ld	t1, OCTEON_CP2_AES_KEY+16(a0)
+	dmtc2	t2, 0x0104
+	ld	t2, OCTEON_CP2_AES_KEY+24(a0)
+	dmtc2	t0, 0x0105
+	ld	t0, OCTEON_CP2_AES_KEYLEN(a0)
+	dmtc2	t1, 0x0106
+	ld	t1, OCTEON_CP2_AES_RESULT(a0)
+	dmtc2	t2, 0x0107
+	ld	t2, OCTEON_CP2_AES_RESULT+8(a0)
+	mfc0	t3, $15,0 	/* Get the processor ID register */
+	dmtc2	t0, 0x0110
+	li	t0, 0x000d0000	/* This is the processor ID of Octeon Pass1 */
+	dmtc2	t1, 0x0100
+	bne	t0, t3, 3f	/* Skip the next stuff for non-pass1 */
+	 dmtc2	t2, 0x0101
+
+        /* this code is specific for pass 1 */
+	ld	t0, OCTEON_CP2_HSH_DATW(a0)
+	ld	t1, OCTEON_CP2_HSH_DATW+8(a0)
+	ld	t2, OCTEON_CP2_HSH_DATW+16(a0)
+	dmtc2	t0, 0x0040
+	ld	t0, OCTEON_CP2_HSH_DATW+24(a0)
+	dmtc2	t1, 0x0041
+	ld	t1, OCTEON_CP2_HSH_DATW+32(a0)
+	dmtc2	t2, 0x0042
+	ld	t2, OCTEON_CP2_HSH_DATW+40(a0)
+	dmtc2	t0, 0x0043
+	ld	t0, OCTEON_CP2_HSH_DATW+48(a0)
+	dmtc2	t1, 0x0044
+	ld	t1, OCTEON_CP2_HSH_IVW(a0)
+	dmtc2	t2, 0x0045
+	ld	t2, OCTEON_CP2_HSH_IVW+8(a0)
+	dmtc2	t0, 0x0046
+	ld	t0, OCTEON_CP2_HSH_IVW+16(a0)
+	dmtc2	t1, 0x0048
+	dmtc2	t2, 0x0049
+        b done_restore   /* unconditional branch */
+	 dmtc2	t0, 0x004A
+
+3:      /* this is post-pass1 code */
+	ld	t2, OCTEON_CP2_HSH_DATW(a0)
+	ld	t0, OCTEON_CP2_HSH_DATW+8(a0)
+	ld	t1, OCTEON_CP2_HSH_DATW+16(a0)
+	dmtc2	t2, 0x0240
+	ld	t2, OCTEON_CP2_HSH_DATW+24(a0)
+	dmtc2	t0, 0x0241
+	ld	t0, OCTEON_CP2_HSH_DATW+32(a0)
+	dmtc2	t1, 0x0242
+	ld	t1, OCTEON_CP2_HSH_DATW+40(a0)
+	dmtc2	t2, 0x0243
+	ld	t2, OCTEON_CP2_HSH_DATW+48(a0)
+	dmtc2	t0, 0x0244
+	ld	t0, OCTEON_CP2_HSH_DATW+56(a0)
+	dmtc2	t1, 0x0245
+	ld	t1, OCTEON_CP2_HSH_DATW+64(a0)
+	dmtc2	t2, 0x0246
+	ld	t2, OCTEON_CP2_HSH_DATW+72(a0)
+	dmtc2	t0, 0x0247
+	ld	t0, OCTEON_CP2_HSH_DATW+80(a0)
+	dmtc2	t1, 0x0248
+	ld	t1, OCTEON_CP2_HSH_DATW+88(a0)
+	dmtc2	t2, 0x0249
+	ld	t2, OCTEON_CP2_HSH_DATW+96(a0)
+	dmtc2	t0, 0x024A
+	ld	t0, OCTEON_CP2_HSH_DATW+104(a0)
+	dmtc2	t1, 0x024B
+	ld	t1, OCTEON_CP2_HSH_DATW+112(a0)
+	dmtc2	t2, 0x024C
+	ld	t2, OCTEON_CP2_HSH_IVW(a0)
+	dmtc2	t0, 0x024D
+	ld	t0, OCTEON_CP2_HSH_IVW+8(a0)
+	dmtc2	t1, 0x024E
+	ld	t1, OCTEON_CP2_HSH_IVW+16(a0)
+	dmtc2	t2, 0x0250
+	ld	t2, OCTEON_CP2_HSH_IVW+24(a0)
+	dmtc2	t0, 0x0251
+	ld	t0, OCTEON_CP2_HSH_IVW+32(a0)
+	dmtc2	t1, 0x0252
+	ld	t1, OCTEON_CP2_HSH_IVW+40(a0)
+	dmtc2	t2, 0x0253
+	ld	t2, OCTEON_CP2_HSH_IVW+48(a0)
+	dmtc2	t0, 0x0254
+	ld	t0, OCTEON_CP2_HSH_IVW+56(a0)
+	dmtc2	t1, 0x0255
+	ld	t1, OCTEON_CP2_GFM_MULT(a0)
+	dmtc2	t2, 0x0256
+	ld	t2, OCTEON_CP2_GFM_MULT+8(a0)
+	dmtc2	t0, 0x0257
+	ld	t0, OCTEON_CP2_GFM_POLY(a0)
+	dmtc2	t1, 0x0258
+	ld	t1, OCTEON_CP2_GFM_RESULT(a0)
+	dmtc2	t2, 0x0259
+	ld	t2, OCTEON_CP2_GFM_RESULT+8(a0)
+	dmtc2	t0, 0x025E
+	dmtc2	t1, 0x025A
+	dmtc2	t2, 0x025B
+
+done_restore:
+	jr	ra
+	 nop
+	END(octeon_cop2_restore)
+	.set pop
+
+/*
+ * void octeon_mult_save()
+ * sp is assumed to point to a struct pt_regs
+ *
+ * NOTE: This is called in SAVE_SOME in stackframe.h. It can only
+ *       safely modify k0 and k1.
+ */
+	.align	7
+	.set push
+	.set noreorder
+	LEAF(octeon_mult_save)
+	dmfc0	k0, $9,7	/* CvmCtl register. */
+	bbit1	k0, 27, 1f	/* Skip CvmCtl[NOMUL] */
+	 nop
+
+	/* Save the multiplier state */
+	v3mulu	k0, $0, $0
+	v3mulu	k1, $0, $0
+	sd	k0, PT_MTP(sp)        /* PT_MTP    has P0 */
+	v3mulu	k0, $0, $0
+	sd	k1, PT_MTP+8(sp)      /* PT_MTP+8  has P1 */
+	ori	k1, $0, 1
+	v3mulu	k1, k1, $0
+	sd	k0, PT_MTP+16(sp)     /* PT_MTP+16 has P2 */
+	v3mulu	k0, $0, $0
+	sd	k1, PT_MPL(sp)        /* PT_MPL    has MPL0 */
+	v3mulu	k1, $0, $0
+	sd	k0, PT_MPL+8(sp)      /* PT_MPL+8  has MPL1 */
+	jr	ra
+	 sd	k1, PT_MPL+16(sp)     /* PT_MPL+16 has MPL2 */
+
+1:	/* Resume here if CvmCtl[NOMUL] */
+	jr	ra
+	END(octeon_mult_save)
+	.set pop
+
+/*
+ * void octeon_mult_restore()
+ * sp is assumed to point to a struct pt_regs
+ *
+ * NOTE: This is called in RESTORE_SOME in stackframe.h.
+ */
+	.align	7
+	.set push
+	.set noreorder
+	LEAF(octeon_mult_restore)
+	dmfc0	k1, $9,7		/* CvmCtl register. */
+	ld	v0, PT_MPL(sp)        	/* MPL0 */
+	ld	v1, PT_MPL+8(sp)      	/* MPL1 */
+	ld	k0, PT_MPL+16(sp)     	/* MPL2 */
+	bbit1	k1, 27, 1f		/* Skip CvmCtl[NOMUL] */
+	 nop				/* Normally falls through, so no time wasted here */
+
+	/* Restore the multiplier state */
+	ld	k1, PT_MTP+16(sp)     	/* P2 */
+	MTM0	v0			/* MPL0 */
+	ld	v0, PT_MTP+8(sp)	/* P1 */
+	MTM1	v1			/* MPL1 */
+	ld	v1, PT_MTP(sp)   	/* P0 */
+	MTM2	k0			/* MPL2 */
+	MTP2	k1			/* P2 */
+	MTP1	v0			/* P1 */
+	jr	ra
+	 MTP0	v1			/* P0 */
+
+1:	/* Resume here if CvmCtl[NOMUL] */
+	jr	ra
+	 nop
+	END(octeon_mult_restore)
+	.set pop
+
diff --git a/arch/mips/mm/c-octeon.c b/arch/mips/mm/c-octeon.c
new file mode 100644
index 0000000..d373ea6
--- /dev/null
+++ b/arch/mips/mm/c-octeon.c
@@ -0,0 +1,293 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2005-2007 Cavium Networks
+ */
+#include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/mm.h>
+#include <linux/bitops.h>
+
+#include <asm/bcache.h>
+#include <asm/bootinfo.h>
+#include <asm/cacheops.h>
+#include <asm/cpu.h>
+#include <asm/cpu-features.h>
+#include <asm/io.h>
+#include <asm/page.h>
+#include <asm/pgtable.h>
+#include <asm/r4kcache.h>
+#include <asm/system.h>
+#include <asm/mmu_context.h>
+#include <asm/war.h>
+#include "../cavium-octeon/hal.h"
+
+unsigned long long cache_err_dcache[NR_CPUS];
+
+/**
+ * Octeon automatically flushes the dcache on tlb changes, so
+ * from Linux's viewpoint it acts much like a physically
+ * tagged cache. No flushing is needed
+ *
+ * @param addr
+ */
+static void octeon_flush_data_cache_page(unsigned long addr)
+{
+    /* Nothing to do */
+}
+
+
+/**
+ * Flush caches as necessary for all cores affected by a
+ * vma. If no vma is supplied, all cores are flushed.
+ *
+ * @param vma    VMA to flush or NULL to flush all icaches.
+ */
+static void octeon_flush_icache_all_cores(struct vm_area_struct *vma)
+{
+#ifdef CONFIG_SMP
+    int i;
+    int cpu;
+#endif
+    preempt_disable();
+#ifdef CONFIG_SMP
+    cpu = smp_processor_id();
+#endif
+    mb();
+
+    /* If we have a vma structure, we only need to worry about cores it
+        has been used on */
+    if (vma)
+    {
+#ifdef CONFIG_SMP
+        for (i = 0; i < NR_CPUS; i++)
+            if (cpu_isset(i, vma->vm_mm->cpu_vm_mask) && i != cpu)
+                core_send_ipi(i, SMP_ICACHE_FLUSH);
+#endif
+        asm volatile ("synci 0($0)\n");
+    }
+    else
+    {
+        /* No extra info available. Flush the icache on all cores that
+            are online */
+#ifdef CONFIG_SMP
+        for (i = 0; i < NR_CPUS; i++)
+            if (cpu_online(i) && i != cpu)
+                core_send_ipi(i, SMP_ICACHE_FLUSH);
+#endif
+        asm volatile ("synci 0($0)\n");
+    }
+    preempt_enable();
+}
+
+
+/**
+ * Called to flush the icache on all cores
+ */
+static void octeon_flush_icache_all(void)
+{
+	octeon_flush_icache_all_cores(NULL);
+}
+
+
+/**
+ * Called to flush all memory associated with a memory
+ * context.
+ *
+ * @param mm     Memory context to flush
+ */
+static void octeon_flush_cache_mm(struct mm_struct *mm)
+{
+    /* According to the R4K version of this file, CPUs without
+        dcache aliases don't need to do anything here */
+}
+
+
+/**
+ * Flush a range of kernel addresses out of the icache
+ *
+ * @param start
+ * @param end
+ */
+static void octeon_flush_icache_range(unsigned long start, unsigned long end)
+{
+	octeon_flush_icache_all_cores(NULL);
+}
+
+
+/**
+ * Flush the icache for a trampoline. These are used for interrupt
+ * and exception hooking.
+ *
+ * @param addr   Address to flush
+ */
+static void octeon_flush_cache_sigtramp(unsigned long addr)
+{
+    /* Only flush trampolines on the current core */
+    mb();
+    asm volatile ("synci 0(%0)\n":: "r" (addr));
+}
+
+
+/**
+ * Flush a range out of a vma
+ *
+ * @param vma    VMA to flush
+ * @param start
+ * @param end
+ */
+static void octeon_flush_cache_range(struct vm_area_struct *vma, unsigned long start, unsigned long end)
+{
+    if (vma->vm_flags & (VM_EXEC|VM_EXECUTABLE))
+        octeon_flush_icache_all_cores(vma);
+}
+
+
+/**
+ * Flush a specific page of a vma
+ *
+ * @param vma    VMA to flush page for
+ * @param page   Page to flush
+ * @param pfn
+ */
+static void octeon_flush_cache_page(struct vm_area_struct *vma, unsigned long page, unsigned long pfn)
+{
+    if (vma->vm_flags & (VM_EXEC|VM_EXECUTABLE))
+        octeon_flush_icache_all_cores(vma);
+}
+
+
+/**
+ * Probe Octeon's caches
+ *
+ * @return
+ */
+static void __init probe_octeon(void)
+{
+    unsigned long icache_size;
+    unsigned long dcache_size;
+    unsigned int config1;
+    struct cpuinfo_mips *c = &current_cpu_data;
+
+    switch (c->cputype)
+    {
+        case CPU_CAVIUM_OCTEON:
+            config1 = read_c0_config1();
+            c->icache.linesz = 2 << ((config1 >> 19) & 7);
+            c->icache.sets = 64 << ((config1 >> 22) & 7);
+            c->icache.ways = 1 + ((config1 >> 16) & 7);
+            c->icache.flags |= MIPS_CACHE_VTAG;
+            icache_size = c->icache.sets * c->icache.ways * c->icache.linesz;
+            c->icache.waybit = ffs(icache_size / c->icache.ways) - 1;
+            c->dcache.linesz = 128;
+            if (OCTEON_IS_MODEL(OCTEON_CN3XXX))
+                c->dcache.sets = 1; /* CN3XXX has one Dcache set */
+            else
+                c->dcache.sets = 2; /* CN5XXX has two Dcache sets */
+            c->dcache.ways = 64;
+            dcache_size = c->dcache.sets * c->dcache.ways * c->dcache.linesz;
+            c->dcache.waybit = ffs(dcache_size / c->dcache.ways) - 1;
+            c->options |= MIPS_CPU_PREFETCH;
+            break;
+
+        default:
+            panic("Unsupported Cavium Networks CPU type\n");
+            break;
+    }
+
+    /* compute a couple of other cache variables */
+    c->icache.waysize = icache_size / c->icache.ways;
+    c->dcache.waysize = dcache_size / c->dcache.ways;
+
+    c->icache.sets = icache_size / (c->icache.linesz * c->icache.ways);
+    c->dcache.sets = dcache_size / (c->dcache.linesz * c->dcache.ways);
+
+    if (smp_processor_id() == 0)
+    {
+        printk("Primary instruction cache %ldkB, %s, %d way, %d sets, linesize %d bytes.\n",
+               icache_size >> 10,
+               cpu_has_vtag_icache ? "virtually tagged" : "physically tagged",
+               c->icache.ways, c->icache.sets, c->icache.linesz);
+
+        printk("Primary data cache %ldkB, %d-way, %d sets, linesize %d bytes.\n",
+               dcache_size >> 10, c->dcache.ways, c->dcache.sets, c->dcache.linesz);
+    }
+}
+
+
+/**
+ * Setup the Octeon cache flush routines
+ *
+ * @return
+ */
+void __init octeon_cache_init(void)
+{
+    extern unsigned long ebase;
+    extern char except_vec2_octeon;
+
+    memcpy((void *)(ebase + 0x100), &except_vec2_octeon, 0x80);
+    octeon_flush_cache_sigtramp(ebase + 0x100);
+
+    probe_octeon();
+
+    shm_align_mask = PAGE_SIZE - 1;
+
+    flush_cache_all         = octeon_flush_icache_all;
+    __flush_cache_all       = octeon_flush_icache_all;
+    flush_cache_mm          = octeon_flush_cache_mm;
+    flush_cache_page        = octeon_flush_cache_page;
+    flush_cache_range       = octeon_flush_cache_range;
+    flush_cache_sigtramp    = octeon_flush_cache_sigtramp;
+    flush_icache_all        = octeon_flush_icache_all;
+    flush_data_cache_page   = octeon_flush_data_cache_page;
+    flush_icache_range      = octeon_flush_icache_range;
+}
+
+/**
+ * Handle a cache error exception
+ */
+
+static void  cache_parity_error_octeon(int non_recoverable)
+{
+    unsigned long coreid = cvmx_get_core_num();
+    uint64_t icache_err = read_c0_cacheerr();
+
+    printk("Cache error exception:\n");
+    printk("cp0_errorepc == %lx\n", read_c0_errorepc());
+    if (icache_err & 1)
+    {
+        printk("CacheErr (Icache) == %llx\n", (unsigned long long)icache_err);
+        write_c0_cacheerr(0);
+    }
+    if (cache_err_dcache[coreid] & 1)
+    {
+        printk("CacheErr (Dcache) == %llx\n", (unsigned long long)cache_err_dcache[coreid]);
+        cache_err_dcache[coreid] = 0;
+    }
+
+
+    if (non_recoverable)
+        panic("Can't handle cache error: nested exception");
+}
+
+/**
+ * Called when the the exception is not recoverable
+ */
+
+asmlinkage void cache_parity_error_octeon_recoverable(void)
+{
+    cache_parity_error_octeon(0);
+}
+
+/**
+ * Called when the the exception is recoverable
+ */
+
+asmlinkage void cache_parity_error_octeon_non_recoverable(void)
+{
+    cache_parity_error_octeon(1);
+}
+
diff --git a/arch/mips/mm/cex-oct.S b/arch/mips/mm/cex-oct.S
new file mode 100644
index 0000000..6546f26
--- /dev/null
+++ b/arch/mips/mm/cex-oct.S
@@ -0,0 +1,73 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2006 Cavium Networks
+ * Cache error handler
+ */
+
+#include <asm/asm.h>
+#include <asm/regdef.h>
+#include <asm/mipsregs.h>
+#include <asm/stackframe.h>
+
+/*
+ * Handle cache error. Indicate to the second level handler whether
+ * the exception is recoverable.
+ */
+	LEAF(except_vec2_octeon)
+
+	.set    push
+	.set	mips64r2
+	.set	noreorder
+	.set	noat
+
+
+	/* due to an errata we need to read the COP0 CacheErr (Dcache) before any
+	 * cache/DRAM access
+	 */
+
+	rdhwr   k0, $0        /* get core_id */
+	PTR_LA  k1, cache_err_dcache
+	sll     k0, k0, 3
+	PTR_ADDU k1, k0, k1    /* k1 = &cache_err_dcache[core_id] */
+
+	dmfc0   k0, CP0_CACHEERR, 1
+	sd      k0, (k1)
+	dmtc0   $0, CP0_CACHEERR, 1
+
+        /* check whether this is a nested exception */
+	mfc0    k1, CP0_STATUS
+	andi    k1, k1, ST0_EXL
+	beqz    k1, 1f
+	 nop
+	j	cache_parity_error_octeon_non_recoverable
+	 nop
+
+	/* exception is recoverable */
+1:	j	handle_cache_err
+	 nop
+
+	.set    pop
+	END(except_vec2_octeon)
+
+/*
+ * We need to jump to handle_cache_err so that the previous handler can fit
+ * within 0x80 bytes. We also move from 0xFFFFFFFFAXXXXXXX space (uncached) to the
+ * 0xFFFFFFFF8XXXXXXX space (cached).
+ */
+	LEAF(handle_cache_err)
+	.set    push
+        .set    noreorder
+        .set    noat
+
+	SAVE_ALL
+	KMODE
+	jal     cache_parity_error_octeon_recoverable
+	nop
+	j       ret_from_exception
+	nop
+
+	.set pop
+	END(handle_cache_err)
diff --git a/arch/mips/mm/pg-octeon.c b/arch/mips/mm/pg-octeon.c
new file mode 100644
index 0000000..ce3f4cb
--- /dev/null
+++ b/arch/mips/mm/pg-octeon.c
@@ -0,0 +1,120 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2005-2007 Cavium Networks
+ */
+#include <linux/module.h>
+
+
+void clear_page(void *page)
+{
+	void *end = page + PAGE_SIZE;
+
+    asm volatile (
+        "   .set	push		\n"
+        "   .set	mips64		\n"
+        "   .set	noreorder	\n"
+        "1: sd      $0, 0(%0)   \n" /* Write zeros to the cache line */
+        "   sd      $0, 8(%0)   \n"
+        "   sd      $0, 16(%0)  \n"
+        "   sd      $0, 24(%0)  \n"
+        "   sd      $0, 32(%0)  \n"
+        "   sd      $0, 40(%0)  \n"
+        "   sd      $0, 48(%0)  \n"
+        "   sd      $0, 56(%0)  \n"
+        "   sd      $0, 64(%0)  \n"
+        "   sd      $0, 72(%0)  \n"
+        "   sd      $0, 80(%0)  \n"
+        "   sd      $0, 88(%0)  \n"
+        "   sd      $0, 96(%0)  \n"
+        "   sd      $0, 104(%0) \n"
+#ifdef CONFIG_64BIT
+        "   daddu   %0, 128     \n" /* Increment to the next address. Will be dual issued */
+#else
+        "   addu    %0, 128     \n"
+#endif
+        "   sd      $0, -16(%0) \n"
+        "   blt     %0, %1, 1b  \n" /* Loop until we've completed the page */
+        "    sd     $0, -8(%0)  \n"
+        "   .set	pop		\n"
+        : "+r" (page)
+        : "r" (end)
+        : "memory"
+    );
+}
+
+
+void copy_page(void *to, void *from)
+{
+#ifdef _ABIO32
+    memcpy(to, from, PAGE_SIZE);
+#else
+	void *end = to + PAGE_SIZE;
+
+    /* Warning: Bad thing can happen if you prefetch an address that doesn't
+        exist in memory. With Octeon, this can happen at 256MB, 0x420000000,
+        and the top of memory. To avoid this, we stop prefetching during the
+        last two iterations of this loop */
+    asm volatile (
+        "   .set	push		\n"
+        "   .set	mips64		\n"
+        "   .set	noreorder	\n"
+        "   pref    0,  0(%1)   \n" /* Prefetch the first cache line of "from" */
+        "   pref    0,  128(%1) \n" /* Prefetch the next "from" cache line for the next iteration */
+        "1: pref    0,  256(%1) \n" /* Prefetch for two loops ahead */
+        "2:                     \n" /* This is the entry point for the last two loops to skip the prefetch */
+        "   ld      $12, 0(%1)  \n" /* Copy 32 bytes at a time */
+        "   ld      $13, 8(%1)  \n"
+        "   ld      $14, 16(%1) \n"
+        "   ld      $15, 24(%1) \n"
+        "   daddu   %1, 32      \n" /* Dual issued */
+        "   sd      $12, 0(%0)  \n"
+        "   sd      $13, 8(%0)  \n"
+        "   sd      $14, 16(%0) \n"
+        "   sd      $15, 24(%0) \n"
+        "   daddu   %0, 32      \n" /* Dual issued */
+        "   ld      $12, 0(%1)  \n" /* Copy 32 bytes at a time */
+        "   ld      $13, 8(%1)  \n"
+        "   ld      $14, 16(%1) \n"
+        "   ld      $15, 24(%1) \n"
+        "   daddu   %1, 32      \n" /* Dual issued */
+        "   sd      $12, 0(%0)  \n"
+        "   sd      $13, 8(%0)  \n"
+        "   sd      $14, 16(%0) \n"
+        "   sd      $15, 24(%0) \n"
+        "   daddu   %0, 32      \n" /* Dual issued */
+        "   ld      $12, 0(%1)  \n" /* Copy 32 bytes at a time */
+        "   ld      $13, 8(%1)  \n"
+        "   ld      $14, 16(%1) \n"
+        "   ld      $15, 24(%1) \n"
+        "   daddu   %1, 32      \n" /* Dual issued */
+        "   sd      $12, 0(%0)  \n"
+        "   sd      $13, 8(%0)  \n"
+        "   sd      $14, 16(%0) \n"
+        "   sd      $15, 24(%0) \n"
+        "   daddu   %0, 32      \n" /* Dual issued */
+        "   ld      $12, 0(%1)  \n" /* Copy 32 bytes at a time */
+        "   ld      $13, 8(%1)  \n"
+        "   ld      $14, 16(%1) \n"
+        "   ld      $15, 24(%1) \n"
+        "   daddu   %1, 32      \n" /* Dual issued */
+        "   sd      $12, 0(%0)  \n"
+        "   sd      $13, 8(%0)  \n"
+        "   daddu   %0, 32      \n" /* Dual issued */
+        "   sd      $14, -16(%0)\n"
+        "   blt     %0, %3, 1b  \n" /* Loop until we've gotten to the last 256 bytes */
+        "    sd     $15, -8(%0) \n"
+        "   blt     %0, %2, 2b  \n" /* Loop until we've completed the last 256 bytes, skip the prefetch */
+        "    nop                \n"
+        "   .set	pop		\n"
+        : "+r" (to), "+r" (from)
+        : "r" (end), "r" (end-256)
+        : "$12", "$13", "$14", "$15", "memory"
+    );
+#endif
+}
+
+EXPORT_SYMBOL(clear_page);
+EXPORT_SYMBOL(copy_page);
diff --git a/include/asm-mips/mach-cavium-octeon/cpu-feature-overrides.h b/include/asm-mips/mach-cavium-octeon/cpu-feature-overrides.h
new file mode 100644
index 0000000..f1ecf81
--- /dev/null
+++ b/include/asm-mips/mach-cavium-octeon/cpu-feature-overrides.h
@@ -0,0 +1,68 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2004 Cavium Networks
+ */
+#ifndef __ASM_MACH_CAVIUM_OCTEON_CPU_FEATURE_OVERRIDES_H
+#define __ASM_MACH_CAVIUM_OCTEON_CPU_FEATURE_OVERRIDES_H
+
+#include <linux/types.h>
+#include <asm/mipsregs.h>
+
+/*
+ * Cavium Octeons are MIPS64v2 processors
+ */
+#define cpu_dcache_line_size()	128
+#define cpu_icache_line_size()	128
+
+#ifdef CONFIG_SMP
+#define cpu_has_llsc		1
+#else
+/* Disable LL/SC on non SMP systems. It is faster to disable interrupts for
+   atomic access than a LL/SC */
+#define cpu_has_llsc		0
+#endif
+#define cpu_has_prefetch  	1
+#define cpu_has_dc_aliases      0
+#define cpu_has_fpu             0
+#define cpu_has_64bits          1
+#define cpu_has_octeon_cache    1
+#define cpu_has_saa             octeon_has_saa()
+#define cpu_has_mips64r2        1
+#define cpu_has_counter         1
+/* We actually use two syncw instructions in a row when we need a write memory
+   barrier. This is because the CN3XXX series of Octeons have errata Core-401.
+   This can cause a single syncw to not enforce ordering under very rare
+   conditions. Even if it is rare, better safe than sorry */
+#define OCTEON_SYNCW_STR ".set push\n.set arch=octeon\nsyncw\nsyncw\n.set pop\n"
+
+#define ARCH_HAS_READ_CURRENT_TIMER 1
+#define ARCH_HAS_IRQ_PER_CPU 1
+#define ARCH_HAS_SPINLOCK_PREFETCH 1
+#define spin_lock_prefetch(x) prefetch(x)
+#define PREFETCH_STRIDE 128
+
+static inline unsigned long read_current_timer(unsigned long *result)
+{
+	asm volatile ("rdhwr %0,$31\n"
+#ifndef CONFIG_64BIT
+		      "sll %0, 0\n"
+#endif
+		      :"=r" (*result));
+	return *result;
+}
+
+static inline int octeon_has_saa(void)
+{
+#ifdef CONFIG_CPU_CAVIUM_OCTEON
+	int id;
+	asm volatile ("mfc0 %0, $15,0":"=r" (id));
+	return (id >= 0x000d0300);
+#else
+	return 0;
+#endif
+}
+
+#endif
diff --git a/include/asm-mips/mach-cavium-octeon/irq.h b/include/asm-mips/mach-cavium-octeon/irq.h
new file mode 100644
index 0000000..43d5b68
--- /dev/null
+++ b/include/asm-mips/mach-cavium-octeon/irq.h
@@ -0,0 +1,252 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2004-2007 Cavium Networks
+ */
+#ifndef __OCTEON_IRQ_H__
+#define __OCTEON_IRQ_H__
+
+#define NR_IRQS OCTEON_IRQ_LAST
+
+/* 0 - 7 represent the 8 MIPS standard interrupt sources */
+#define OCTEON_IRQ_SW0          0
+#define OCTEON_IRQ_SW1          1
+#define OCTEON_IRQ_CIU0         2
+#define OCTEON_IRQ_CIU1         3
+#define OCTEON_IRQ_CIU4         4
+#define OCTEON_IRQ_5            5
+#define OCTEON_IRQ_PERF         6
+#define OCTEON_IRQ_TIMER        7
+/* 8 - 71 represent the sources in CIU_INTX_EN0 */
+#define OCTEON_IRQ_WORKQ0       8
+#define OCTEON_IRQ_WORKQ1       9
+#define OCTEON_IRQ_WORKQ2       10
+#define OCTEON_IRQ_WORKQ3       11
+#define OCTEON_IRQ_WORKQ4       12
+#define OCTEON_IRQ_WORKQ5       13
+#define OCTEON_IRQ_WORKQ6       14
+#define OCTEON_IRQ_WORKQ7       15
+#define OCTEON_IRQ_WORKQ8       16
+#define OCTEON_IRQ_WORKQ9       17
+#define OCTEON_IRQ_WORKQ10      18
+#define OCTEON_IRQ_WORKQ11      19
+#define OCTEON_IRQ_WORKQ12      20
+#define OCTEON_IRQ_WORKQ13      21
+#define OCTEON_IRQ_WORKQ14      22
+#define OCTEON_IRQ_WORKQ15      23
+#define OCTEON_IRQ_GPIO0        24
+#define OCTEON_IRQ_GPIO1        25
+#define OCTEON_IRQ_GPIO2        26
+#define OCTEON_IRQ_GPIO3        27
+#define OCTEON_IRQ_GPIO4        28
+#define OCTEON_IRQ_GPIO5        29
+#define OCTEON_IRQ_GPIO6        30
+#define OCTEON_IRQ_GPIO7        31
+#define OCTEON_IRQ_GPIO8        32
+#define OCTEON_IRQ_GPIO9        33
+#define OCTEON_IRQ_GPIO10       34
+#define OCTEON_IRQ_GPIO11       35
+#define OCTEON_IRQ_GPIO12       36
+#define OCTEON_IRQ_GPIO13       37
+#define OCTEON_IRQ_GPIO14       38
+#define OCTEON_IRQ_GPIO15       39
+#define OCTEON_IRQ_MBOX0        40
+#define OCTEON_IRQ_MBOX1        41
+#define OCTEON_IRQ_UART0        42
+#define OCTEON_IRQ_UART1        43
+#define OCTEON_IRQ_PCI_INT0     44
+#define OCTEON_IRQ_PCI_INT1     45
+#define OCTEON_IRQ_PCI_INT2     46
+#define OCTEON_IRQ_PCI_INT3     47
+#define OCTEON_IRQ_PCI_MSI0     48
+#define OCTEON_IRQ_PCI_MSI1     49
+#define OCTEON_IRQ_PCI_MSI2     50
+#define OCTEON_IRQ_PCI_MSI3     51
+#define OCTEON_IRQ_RESERVED52   52	/* Summary of CIU_INT_SUM1 */
+#define OCTEON_IRQ_TWSI         53
+#define OCTEON_IRQ_RML          54
+#define OCTEON_IRQ_TRACE        55
+#define OCTEON_IRQ_GMX_DRP0     56
+#define OCTEON_IRQ_GMX_DRP1     57
+#define OCTEON_IRQ_IPD_DRP      58
+#define OCTEON_IRQ_KEY_ZERO     59
+#define OCTEON_IRQ_TIMER0       60
+#define OCTEON_IRQ_TIMER1       61
+#define OCTEON_IRQ_TIMER2       62
+#define OCTEON_IRQ_TIMER3       63
+#define OCTEON_IRQ_USB0         64
+#define OCTEON_IRQ_PCM          65
+#define OCTEON_IRQ_MPI          66
+#define OCTEON_IRQ_TWSI2        67
+#define OCTEON_IRQ_POWIQ        68
+#define OCTEON_IRQ_IPDPPTHR     69
+#define OCTEON_IRQ_MII0         70
+#define OCTEON_IRQ_BOOTDMA      71
+/* 72 - 135 represent the sources in CIU_INTX_EN1 */
+#define OCTEON_IRQ_WDOG0        72
+#define OCTEON_IRQ_WDOG1        73
+#define OCTEON_IRQ_WDOG2        74
+#define OCTEON_IRQ_WDOG3        75
+#define OCTEON_IRQ_WDOG4        76
+#define OCTEON_IRQ_WDOG5        77
+#define OCTEON_IRQ_WDOG6        78
+#define OCTEON_IRQ_WDOG7        79
+#define OCTEON_IRQ_WDOG8        80
+#define OCTEON_IRQ_WDOG9        81
+#define OCTEON_IRQ_WDOG10       82
+#define OCTEON_IRQ_WDOG11       83
+#define OCTEON_IRQ_WDOG12       84
+#define OCTEON_IRQ_WDOG13       85
+#define OCTEON_IRQ_WDOG14       86
+#define OCTEON_IRQ_WDOG15       87
+#define OCTEON_IRQ_UART2        88
+#define OCTEON_IRQ_USB1         89
+#define OCTEON_IRQ_MII1         90
+#define OCTEON_IRQ_RESERVED91   91
+#define OCTEON_IRQ_RESERVED92   92
+#define OCTEON_IRQ_RESERVED93   93
+#define OCTEON_IRQ_RESERVED94   94
+#define OCTEON_IRQ_RESERVED95   95
+#define OCTEON_IRQ_RESERVED96   96
+#define OCTEON_IRQ_RESERVED97   97
+#define OCTEON_IRQ_RESERVED98   98
+#define OCTEON_IRQ_RESERVED99   99
+#define OCTEON_IRQ_RESERVED100  100
+#define OCTEON_IRQ_RESERVED101  101
+#define OCTEON_IRQ_RESERVED102  102
+#define OCTEON_IRQ_RESERVED103  103
+#define OCTEON_IRQ_RESERVED104  104
+#define OCTEON_IRQ_RESERVED105  105
+#define OCTEON_IRQ_RESERVED106  106
+#define OCTEON_IRQ_RESERVED107  107
+#define OCTEON_IRQ_RESERVED108  108
+#define OCTEON_IRQ_RESERVED109  109
+#define OCTEON_IRQ_RESERVED110  110
+#define OCTEON_IRQ_RESERVED111  111
+#define OCTEON_IRQ_RESERVED112  112
+#define OCTEON_IRQ_RESERVED113  113
+#define OCTEON_IRQ_RESERVED114  114
+#define OCTEON_IRQ_RESERVED115  115
+#define OCTEON_IRQ_RESERVED116  116
+#define OCTEON_IRQ_RESERVED117  117
+#define OCTEON_IRQ_RESERVED118  118
+#define OCTEON_IRQ_RESERVED119  119
+#define OCTEON_IRQ_RESERVED120  120
+#define OCTEON_IRQ_RESERVED121  121
+#define OCTEON_IRQ_RESERVED122  122
+#define OCTEON_IRQ_RESERVED123  123
+#define OCTEON_IRQ_RESERVED124  124
+#define OCTEON_IRQ_RESERVED125  125
+#define OCTEON_IRQ_RESERVED126  126
+#define OCTEON_IRQ_RESERVED127  127
+#define OCTEON_IRQ_RESERVED128  128
+#define OCTEON_IRQ_RESERVED129  129
+#define OCTEON_IRQ_RESERVED130  130
+#define OCTEON_IRQ_RESERVED131  131
+#define OCTEON_IRQ_RESERVED132  132
+#define OCTEON_IRQ_RESERVED133  133
+#define OCTEON_IRQ_RESERVED134  134
+#define OCTEON_IRQ_RESERVED135  135
+/* 136 - 143 are reserved to align the i8259 in a multiple of 16. This
+   alignment is necessary since old style ISA interrupts hanging off the i8259
+   have internal alignment assumptions */
+#define OCTEON_IRQ_RESERVED136  136
+#define OCTEON_IRQ_RESERVED137  137
+#define OCTEON_IRQ_RESERVED138  138
+#define OCTEON_IRQ_RESERVED139  139
+#define OCTEON_IRQ_RESERVED140  140
+#define OCTEON_IRQ_RESERVED141  141
+#define OCTEON_IRQ_RESERVED142  142
+#define OCTEON_IRQ_RESERVED143  143
+/* 144 - 151 represent the i8259 master */
+#define OCTEON_IRQ_I8259M0      144
+#define OCTEON_IRQ_I8259M1      145
+#define OCTEON_IRQ_I8259M2      146
+#define OCTEON_IRQ_I8259M3      147
+#define OCTEON_IRQ_I8259M4      148
+#define OCTEON_IRQ_I8259M5      149
+#define OCTEON_IRQ_I8259M6      150
+#define OCTEON_IRQ_I8259M7      151
+/* 152 - 159 represent the i8259 slave */
+#define OCTEON_IRQ_I8259S0      152
+#define OCTEON_IRQ_I8259S1      153
+#define OCTEON_IRQ_I8259S2      154
+#define OCTEON_IRQ_I8259S3      155
+#define OCTEON_IRQ_I8259S4      156
+#define OCTEON_IRQ_I8259S5      157
+#define OCTEON_IRQ_I8259S6      158
+#define OCTEON_IRQ_I8259S7      159
+#ifdef CONFIG_PCI_MSI
+/* 160 - 223 represent the MSI interrupts 0-63 */
+#define OCTEON_IRQ_MSI_BIT0     160
+#define OCTEON_IRQ_MSI_BIT1     161
+#define OCTEON_IRQ_MSI_BIT2     162
+#define OCTEON_IRQ_MSI_BIT3     163
+#define OCTEON_IRQ_MSI_BIT4     164
+#define OCTEON_IRQ_MSI_BIT5     165
+#define OCTEON_IRQ_MSI_BIT6     166
+#define OCTEON_IRQ_MSI_BIT7     167
+#define OCTEON_IRQ_MSI_BIT8     168
+#define OCTEON_IRQ_MSI_BIT9     169
+#define OCTEON_IRQ_MSI_BIT10    170
+#define OCTEON_IRQ_MSI_BIT11    171
+#define OCTEON_IRQ_MSI_BIT12    172
+#define OCTEON_IRQ_MSI_BIT13    173
+#define OCTEON_IRQ_MSI_BIT14    174
+#define OCTEON_IRQ_MSI_BIT15    175
+#define OCTEON_IRQ_MSI_BIT16    176
+#define OCTEON_IRQ_MSI_BIT17    177
+#define OCTEON_IRQ_MSI_BIT18    178
+#define OCTEON_IRQ_MSI_BIT19    179
+#define OCTEON_IRQ_MSI_BIT20    180
+#define OCTEON_IRQ_MSI_BIT21    181
+#define OCTEON_IRQ_MSI_BIT22    182
+#define OCTEON_IRQ_MSI_BIT23    183
+#define OCTEON_IRQ_MSI_BIT24    184
+#define OCTEON_IRQ_MSI_BIT25    185
+#define OCTEON_IRQ_MSI_BIT26    186
+#define OCTEON_IRQ_MSI_BIT27    187
+#define OCTEON_IRQ_MSI_BIT28    188
+#define OCTEON_IRQ_MSI_BIT29    189
+#define OCTEON_IRQ_MSI_BIT30    190
+#define OCTEON_IRQ_MSI_BIT31    191
+#define OCTEON_IRQ_MSI_BIT32    192
+#define OCTEON_IRQ_MSI_BIT33    193
+#define OCTEON_IRQ_MSI_BIT34    194
+#define OCTEON_IRQ_MSI_BIT35    195
+#define OCTEON_IRQ_MSI_BIT36    196
+#define OCTEON_IRQ_MSI_BIT37    197
+#define OCTEON_IRQ_MSI_BIT38    198
+#define OCTEON_IRQ_MSI_BIT39    199
+#define OCTEON_IRQ_MSI_BIT40    200
+#define OCTEON_IRQ_MSI_BIT41    201
+#define OCTEON_IRQ_MSI_BIT42    202
+#define OCTEON_IRQ_MSI_BIT43    203
+#define OCTEON_IRQ_MSI_BIT44    204
+#define OCTEON_IRQ_MSI_BIT45    205
+#define OCTEON_IRQ_MSI_BIT46    206
+#define OCTEON_IRQ_MSI_BIT47    207
+#define OCTEON_IRQ_MSI_BIT48    208
+#define OCTEON_IRQ_MSI_BIT49    209
+#define OCTEON_IRQ_MSI_BIT50    210
+#define OCTEON_IRQ_MSI_BIT51    211
+#define OCTEON_IRQ_MSI_BIT52    212
+#define OCTEON_IRQ_MSI_BIT53    213
+#define OCTEON_IRQ_MSI_BIT54    214
+#define OCTEON_IRQ_MSI_BIT55    215
+#define OCTEON_IRQ_MSI_BIT56    216
+#define OCTEON_IRQ_MSI_BIT57    217
+#define OCTEON_IRQ_MSI_BIT58    218
+#define OCTEON_IRQ_MSI_BIT59    219
+#define OCTEON_IRQ_MSI_BIT60    220
+#define OCTEON_IRQ_MSI_BIT61    221
+#define OCTEON_IRQ_MSI_BIT62    222
+#define OCTEON_IRQ_MSI_BIT63    223
+#define OCTEON_IRQ_LAST         224
+#else
+#define OCTEON_IRQ_LAST         160
+#endif
+
+#endif
diff --git a/include/asm-mips/mach-cavium-octeon/kernel-entry-init.h b/include/asm-mips/mach-cavium-octeon/kernel-entry-init.h
new file mode 100644
index 0000000..16a8a2a
--- /dev/null
+++ b/include/asm-mips/mach-cavium-octeon/kernel-entry-init.h
@@ -0,0 +1,116 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2005 Cavium Networks, Inc
+ */
+#ifndef __ASM_MACH_GENERIC_KERNEL_ENTRY_H
+#define __ASM_MACH_GENERIC_KERNEL_ENTRY_H
+
+
+#define CP0_CYCLE_COUNTER $9,6
+#define CP0_CVMCTL_REG $9,7
+#define CP0_CVMMEMCTL_REG $11,7
+#define CP0_PRID_REG $15,0
+#define CP0_PRID_OCTEON_PASS1 0x000d0000
+#define CP0_PRID_OCTEON_CN30XX 0x000d0200
+
+.macro  kernel_entry_setup
+    # Registers set by bootloader:
+    # (only 32 bits set by bootloader, all addresses are physical addresses, and need
+    #   to have the appropriate memory region set by the kernel
+    # a0 = argc
+    # a1 = argv (kseg0 compat addr )
+    # a2 = 1 if init core, zero otherwise
+    # a3 = address of boot descriptor block
+    .set push
+    .set arch=octeon
+    dmfc0   v0, CP0_CVMMEMCTL_REG       # Read the cavium mem control register
+    dins    v0, $0, 0, 6                # Clear the lower 6 bits, the CVMSEG size
+    ori     v0, CONFIG_CAVIUM_OCTEON_CVMSEG_SIZE
+    dmtc0   v0, CP0_CVMMEMCTL_REG       # Write the cavium mem control register
+    dmfc0   v0, CP0_CVMCTL_REG          # Read the cavium control register
+#ifdef CONFIG_CAVIUM_OCTEON_HW_FIX_UNALIGNED
+    or  v0, v0, 0x5001                  # Disable unaligned load/store support but leave HW fixup enabled
+    xor v0, v0, 0x1001
+#else
+    or  v0, v0, 0x5001                  # Disable unaligned load/store and HW fixup support
+    xor v0, v0, 0x5001
+#endif
+    mfc0 v1, CP0_PRID_REG               # Read the processor ID register
+    or  v0, v0, 0x2000                  # Disable instruction prefetching (Octeon Pass1 errata)
+    beq v1, CP0_PRID_OCTEON_PASS1,skip  # Skip reenable of prefetching for Octeon Pass1
+     nop
+    xor v0, v0, 0x2000                  # Reenable instruction prefetching, not on Pass1
+    srl v1, 8                           # Strip off pass number off of processor id
+    sll v1, 8
+    bne v1, CP0_PRID_OCTEON_CN30XX,skip # CN30XX needs some extra stuff turned off for better performance
+     nop
+    or  v0, v0, 0x400                   # CN30XX Use random Icache replacement
+    or  v0, v0, 0x2000                  # CN30XX Disable instruction prefetching
+skip:
+    dmtc0   v0, CP0_CVMCTL_REG          # Write the cavium control register
+    sync
+    cache   9, 0($0)                    # Flush dcache after config change
+
+    PTR_LA  t2, octeon_boot_desc_ptr    # Store the boot descriptor pointer
+    LONG_S  a3, (t2)
+
+    rdhwr   v0, $0                      # Get my core id
+    bne     a2, zero, octeon_main_processor # Jump the master to kernel_entry
+    nop
+
+#ifdef CONFIG_SMP
+
+    #
+    # All cores other than the master need to wait here for SMP bootstrap
+    # to begin
+    #
+    PTR_LA  t0, octeon_processor_boot   # This is the variable where the next core to boot os stored
+octeon_spin_wait_boot:
+    LONG_L  t1, (t0)                    # Get the core id of the next to be booted
+    bne t1, v0, octeon_spin_wait_boot   # Keep looping if it isn't me
+    nop
+    PTR_LA  t0, octeon_processor_cycle  # Synchronize the cycle counters
+    LONG_L  t0, (t0)
+    LONG_ADDU t0, 122                   # Aproximately how many cycles we will be off
+    MTC0    t0, CP0_CYCLE_COUNTER
+    PTR_LA  t0, octeon_processor_gp     # Get my GP from the global variable
+    LONG_L  gp, (t0)
+    PTR_LA  t0, octeon_processor_sp     # Get my SP from the global variable
+    LONG_L  sp, (t0)
+    LONG_S  zero, (t0)                  # Set the SP global variable to zero so the master knows we've started
+#ifdef __OCTEON__
+    syncw
+    syncw
+#else
+    sync
+#endif
+    b   smp_bootstrap                   # Jump to the normal Linux SMP entry point
+    nop
+
+#else /* CONFIG_SMP */
+
+    #
+    # Someone tried to boot SMP with a non SMP kernel. All extra cores
+    # will halt here.
+    #
+octeon_wait_forever:
+    wait
+    b   octeon_wait_forever
+    nop
+
+#endif /* CONFIG_SMP */
+octeon_main_processor:
+    .set pop
+.endm
+
+/*
+ * Do SMP slave processor setup necessary before we can savely execute C code.
+ */
+    .macro  smp_slave_setup
+    .endm
+
+
+#endif /* __ASM_MACH_GENERIC_KERNEL_ENTRY_H */
diff --git a/include/asm-mips/mach-cavium-octeon/mc146818rtc.h b/include/asm-mips/mach-cavium-octeon/mc146818rtc.h
new file mode 100644
index 0000000..f49ece1
--- /dev/null
+++ b/include/asm-mips/mach-cavium-octeon/mc146818rtc.h
@@ -0,0 +1,14 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2004 Cavium Networks
+ */
+#ifndef __ASM_MACH_CAVIUM_OCTEON_MC146818RTC_H__
+#define __ASM_MACH_CAVIUM_OCTEON_MC146818RTC_H__
+
+#define RTC_IRQ 88
+#include <asm/mach-generic/mc146818rtc.h>
+
+#endif
diff --git a/include/asm-mips/mach-cavium-octeon/octeon-hal-read-write.h b/include/asm-mips/mach-cavium-octeon/octeon-hal-read-write.h
new file mode 100644
index 0000000..8c1dfc8
--- /dev/null
+++ b/include/asm-mips/mach-cavium-octeon/octeon-hal-read-write.h
@@ -0,0 +1,38 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2004-2007 Cavium Networks
+ */
+#ifndef __CAVIUM_OCTEON_HAL_READ_WRITE_H__
+#define __CAVIUM_OCTEON_HAL_READ_WRITE_H__
+
+#include "cvmx.h"
+
+
+/**
+ * Write a 32bit value to the Octeon NPI register space
+ *
+ * @param address Address to write to
+ * @param val     Value to write
+ */
+static inline void octeon_npi_write32(uint64_t address, uint32_t val)
+{
+	cvmx_write64_uint32(address ^ 4, val);
+	cvmx_read64_uint32(address ^ 4);
+}
+
+
+/**
+ * Read a 32bit value from the Octeon NPI register space
+ *
+ * @param address Address to read
+ * @return The result
+ */
+static inline uint32_t octeon_npi_read32(uint64_t address)
+{
+	return cvmx_read64_uint32(address ^ 4);
+}
+
+#endif
diff --git a/include/asm-mips/mach-cavium-octeon/perf_counters.h b/include/asm-mips/mach-cavium-octeon/perf_counters.h
new file mode 100644
index 0000000..f52a511
--- /dev/null
+++ b/include/asm-mips/mach-cavium-octeon/perf_counters.h
@@ -0,0 +1,24 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2004 Cavium Networks
+ */
+
+/**
+ * The IOCTL numbers supported on /proc/octeon_perf
+ */
+#define PROC_PERF_IOCTL_SETUP_COUNTER0      _IOW(0x81, 0, int)
+#define PROC_PERF_IOCTL_SETUP_COUNTER1      _IOW(0x81, 1, int)
+#define PROC_PERF_IOCTL_SETUP_L2COUNTER0    _IOW(0x81, 2, int)
+#define PROC_PERF_IOCTL_SETUP_L2COUNTER1    _IOW(0x81, 3, int)
+#define PROC_PERF_IOCTL_SETUP_L2COUNTER2    _IOW(0x81, 4, int)
+#define PROC_PERF_IOCTL_SETUP_L2COUNTER3    _IOW(0x81, 5, int)
+#define PROC_PERF_IOCTL_READ_COUNTER0       _IOR(0x81, 6, long long)
+#define PROC_PERF_IOCTL_READ_COUNTER1       _IOR(0x81, 7, long long)
+#define PROC_PERF_IOCTL_READ_L2COUNTER0     _IOR(0x81, 8, long long)
+#define PROC_PERF_IOCTL_READ_L2COUNTER1     _IOR(0x81, 9, long long)
+#define PROC_PERF_IOCTL_READ_L2COUNTER2     _IOR(0x81, 10, long long)
+#define PROC_PERF_IOCTL_READ_L2COUNTER3     _IOR(0x81, 11, long long)
+
-- 
1.6.0.3

