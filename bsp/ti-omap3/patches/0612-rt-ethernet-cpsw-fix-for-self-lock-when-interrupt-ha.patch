From ccca7372b404936bb61eae2dd9be51722eb4742e Mon Sep 17 00:00:00 2001
From: Zumeng Chen <zumeng.chen@windriver.com>
Date: Sun, 11 Nov 2012 08:07:06 +0800
Subject: [PATCH 612/612] rt: ethernet: cpsw: fix for self-lock when interrupt happen

The following callback from the NET protocol, cpsw_ndo_start_xmit->
cpsw_tx_packet_submit->cpdma_chan_submit->spin_lock_irqsave(&chan->lock,
Meanwhile, if a hardware interrupt is received during this time, the
interrupt handler wants to hold the chan->lock. As a result, we need to
disable irq for chan->lock and change it into raw_spin_lock from spin_lock.

Signed-off-by: Zumeng Chen <zumeng.chen@windriver.com>
---
 drivers/net/ethernet/ti/davinci_cpdma.c |   44 +++++++++++++++---------------
 1 files changed, 22 insertions(+), 22 deletions(-)

diff --git a/drivers/net/ethernet/ti/davinci_cpdma.c b/drivers/net/ethernet/ti/davinci_cpdma.c
index b608831..496a807 100644
--- a/drivers/net/ethernet/ti/davinci_cpdma.c
+++ b/drivers/net/ethernet/ti/davinci_cpdma.c
@@ -112,7 +112,7 @@ struct cpdma_chan {
 	enum cpdma_state		state;
 	struct cpdma_ctlr		*ctlr;
 	int				chan_num;
-	spinlock_t			lock;
+	raw_spinlock_t			lock;
 	int				count;
 	u32				mask;
 	cpdma_handler_fn		handler;
@@ -534,7 +534,7 @@ struct cpdma_chan *cpdma_chan_create(struct cpdma_ctlr *ctlr, int chan_num,
 	}
 	chan->mask = BIT(chan_linear(chan));
 
-	spin_lock_init(&chan->lock);
+	raw_spin_lock_init(&chan->lock);
 
 	ctlr->channels[chan_num] = chan;
 	spin_unlock_irqrestore(&ctlr->lock, flags);
@@ -572,9 +572,9 @@ int cpdma_chan_get_stats(struct cpdma_chan *chan,
 	unsigned long flags;
 	if (!chan)
 		return -EINVAL;
-	spin_lock_irqsave(&chan->lock, flags);
+	raw_spin_lock_irqsave(&chan->lock, flags);
 	memcpy(stats, &chan->stats, sizeof(*stats));
-	spin_unlock_irqrestore(&chan->lock, flags);
+	raw_spin_unlock_irqrestore(&chan->lock, flags);
 	return 0;
 }
 EXPORT_SYMBOL_GPL(cpdma_chan_get_stats);
@@ -584,7 +584,7 @@ int cpdma_chan_dump(struct cpdma_chan *chan)
 	unsigned long flags;
 	struct device *dev = chan->ctlr->dev;
 
-	spin_lock_irqsave(&chan->lock, flags);
+	raw_spin_lock_irqsave(&chan->lock, flags);
 
 	dev_info(dev, "channel %d (%s %d) state %s",
 		 chan->chan_num, is_rx_chan(chan) ? "rx" : "tx",
@@ -623,7 +623,7 @@ int cpdma_chan_dump(struct cpdma_chan *chan)
 	dev_info(dev, "\tstats teardown_dequeue: %d\n",
 		 chan->stats.teardown_dequeue);
 
-	spin_unlock_irqrestore(&chan->lock, flags);
+	raw_spin_unlock_irqrestore(&chan->lock, flags);
 	return 0;
 }
 
@@ -674,7 +674,7 @@ int cpdma_chan_submit(struct cpdma_chan *chan, void *token, void *data,
 	int				ret = 0;
 	bool                            is_rx;
 
-	spin_lock_irqsave(&chan->lock, flags);
+	raw_spin_lock_irqsave(&chan->lock, flags);
 
 	if (chan->state == CPDMA_STATE_TEARDOWN) {
 		ret = -EINVAL;
@@ -715,7 +715,7 @@ int cpdma_chan_submit(struct cpdma_chan *chan, void *token, void *data,
 	chan->count++;
 
 unlock_ret:
-	spin_unlock_irqrestore(&chan->lock, flags);
+	raw_spin_unlock_irqrestore(&chan->lock, flags);
 	return ret;
 }
 EXPORT_SYMBOL_GPL(cpdma_chan_submit);
@@ -806,13 +806,13 @@ int cpdma_chan_start(struct cpdma_chan *chan)
 	struct cpdma_desc_pool	*pool = ctlr->pool;
 	unsigned long		flags;
 
-	spin_lock_irqsave(&chan->lock, flags);
+	raw_spin_lock_irqsave(&chan->lock, flags);
 	if (chan->state != CPDMA_STATE_IDLE) {
-		spin_unlock_irqrestore(&chan->lock, flags);
+		raw_spin_unlock_irqrestore(&chan->lock, flags);
 		return -EBUSY;
 	}
 	if (ctlr->state != CPDMA_STATE_ACTIVE) {
-		spin_unlock_irqrestore(&chan->lock, flags);
+		raw_spin_unlock_irqrestore(&chan->lock, flags);
 		return -EINVAL;
 	}
 	dma_reg_write(ctlr, chan->int_set, chan->mask);
@@ -823,7 +823,7 @@ int cpdma_chan_start(struct cpdma_chan *chan)
 			chan_write(chan, rxfree, chan->count);
 	}
 
-	spin_unlock_irqrestore(&chan->lock, flags);
+	raw_spin_unlock_irqrestore(&chan->lock, flags);
 	return 0;
 }
 EXPORT_SYMBOL_GPL(cpdma_chan_start);
@@ -836,9 +836,9 @@ int cpdma_chan_stop(struct cpdma_chan *chan)
 	int			ret;
 	unsigned long		timeout;
 
-	spin_lock_irqsave(&chan->lock, flags);
+	raw_spin_lock_irqsave(&chan->lock, flags);
 	if (chan->state != CPDMA_STATE_ACTIVE) {
-		spin_unlock_irqrestore(&chan->lock, flags);
+		raw_spin_unlock_irqrestore(&chan->lock, flags);
 		return -EINVAL;
 	}
 
@@ -860,13 +860,13 @@ int cpdma_chan_stop(struct cpdma_chan *chan)
 	chan_write(chan, cp, CPDMA_TEARDOWN_VALUE);
 
 	/* handle completed packets */
-	spin_unlock_irqrestore(&chan->lock, flags);
+	raw_spin_unlock_irqrestore(&chan->lock, flags);
 	do {
 		ret = __cpdma_chan_process(chan);
 		if (ret < 0)
 			break;
 	} while ((ret & CPDMA_DESC_TD_COMPLETE) == 0);
-	spin_lock_irqsave(&chan->lock, flags);
+	raw_spin_lock_irqsave(&chan->lock, flags);
 
 	/* remaining packets haven't been tx/rx'ed, clean them up */
 	while (chan->head) {
@@ -879,13 +879,13 @@ int cpdma_chan_stop(struct cpdma_chan *chan)
 		chan->stats.teardown_dequeue++;
 
 		/* issue callback without locks held */
-		spin_unlock_irqrestore(&chan->lock, flags);
+		raw_spin_unlock_irqrestore(&chan->lock, flags);
 		__cpdma_chan_free(chan, desc, 0, -ENOSYS);
-		spin_lock_irqsave(&chan->lock, flags);
+		raw_spin_lock_irqsave(&chan->lock, flags);
 	}
 
 	chan->state = CPDMA_STATE_IDLE;
-	spin_unlock_irqrestore(&chan->lock, flags);
+	raw_spin_unlock_irqrestore(&chan->lock, flags);
 	return 0;
 }
 EXPORT_SYMBOL_GPL(cpdma_chan_stop);
@@ -894,15 +894,15 @@ int cpdma_chan_int_ctrl(struct cpdma_chan *chan, bool enable)
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&chan->lock, flags);
+	raw_spin_lock_irqsave(&chan->lock, flags);
 	if (chan->state != CPDMA_STATE_ACTIVE) {
-		spin_unlock_irqrestore(&chan->lock, flags);
+		raw_spin_unlock_irqrestore(&chan->lock, flags);
 		return -EINVAL;
 	}
 
 	dma_reg_write(chan->ctlr, enable ? chan->int_set : chan->int_clear,
 		      chan->mask);
-	spin_unlock_irqrestore(&chan->lock, flags);
+	raw_spin_unlock_irqrestore(&chan->lock, flags);
 
 	return 0;
 }
-- 
1.7.3.5

