From 7d00c4339a443ca66fefa8950a609262604ab68d Mon Sep 17 00:00:00 2001
From: Yonghong Song <ysong@netlogicmicro.com>
Date: Mon, 19 Sep 2011 16:09:05 -0700
Subject: [PATCH 328/761] NUMA support

Based on Broadcom SDK 2.3.

Signed-off-by: Yonghong Song <ysong@netlogicmicro.com>
Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 arch/mips/Kconfig                              |    3 +
 arch/mips/include/asm/mach-netlogic/mmu.h      |    4 +
 arch/mips/include/asm/mach-netlogic/mmzone.h   |   66 ++++
 arch/mips/include/asm/mach-netlogic/topology.h |   44 +++
 arch/mips/include/asm/netlogic/xlp.h           |    3 +-
 arch/mips/kernel/scall64-64.S                  |    5 +
 arch/mips/kernel/scall64-n32.S                 |    6 +
 arch/mips/kernel/scall64-o32.S                 |    6 +
 arch/mips/kernel/setup.c                       |   39 +-
 arch/mips/netlogic/xlp/Makefile                |    1 +
 arch/mips/netlogic/xlp/numa.c                  |  457 ++++++++++++++++++++++++
 arch/mips/netlogic/xlp/setup.c                 |   90 +++++
 arch/mips/netlogic/xlp/smp.c                   |    4 +-
 init/main.c                                    |   18 +
 14 files changed, 739 insertions(+), 7 deletions(-)
 create mode 100644 arch/mips/include/asm/mach-netlogic/mmzone.h
 create mode 100644 arch/mips/include/asm/mach-netlogic/topology.h
 create mode 100644 arch/mips/netlogic/xlp/numa.c

diff --git a/arch/mips/Kconfig b/arch/mips/Kconfig
index 6872c48..2f75c69 100644
--- a/arch/mips/Kconfig
+++ b/arch/mips/Kconfig
@@ -808,6 +808,7 @@ config NLM_XLP_EVP_BOARD
         select ZONE_DMA if 64BIT
         select SYNC_R4K
         select SYS_HAS_EARLY_PRINTK
+	select SYS_SUPPORTS_NUMA
         help
           This board is based on Netlogic XLP Processor.
           Say Y here to support this machine type
@@ -2249,6 +2250,7 @@ config ARCH_FLATMEM_ENABLE
 
 config ARCH_DISCONTIGMEM_ENABLE
 	bool
+	default y if NLM_XLP_EVP_BOARD
 	default y if SGI_IP27
 	help
 	  Say Y to support efficient handling of discontiguous physical memory,
@@ -2275,6 +2277,7 @@ config SYS_SUPPORTS_NUMA
 
 config NODES_SHIFT
 	int
+	default "2" if NLM_XLP_EVP_BOARD
 	default "6"
 	depends on NEED_MULTIPLE_NODES
 
diff --git a/arch/mips/include/asm/mach-netlogic/mmu.h b/arch/mips/include/asm/mach-netlogic/mmu.h
index 3478715..c866b59 100644
--- a/arch/mips/include/asm/mach-netlogic/mmu.h
+++ b/arch/mips/include/asm/mach-netlogic/mmu.h
@@ -60,6 +60,10 @@ nlm_write_os_scratch_3(~(((1ULL << HUGETLB_PAGE_ORDER) - 1) << ENTRYLO_PFN_SHIFT
 extern void setup_mapped_kernel_tlbs(int index, int secondary_cpu);
 extern unsigned long recalculate_max_low_pfn(unsigned long max_low_pfn);
 
+#if defined(CONFIG_NLM_XLP) && defined(CONFIG_NUMA)
+extern void __init nlm_numa_bootmem_init(unsigned long);
+#endif
+
 #ifndef CONFIG_NLM_XLP
 #define disable_pgwalker(flags) (void)flags
 #define enable_pgwalker(flags) (void) flags
diff --git a/arch/mips/include/asm/mach-netlogic/mmzone.h b/arch/mips/include/asm/mach-netlogic/mmzone.h
new file mode 100644
index 0000000..245c770
--- /dev/null
+++ b/arch/mips/include/asm/mach-netlogic/mmzone.h
@@ -0,0 +1,66 @@
+#ifndef _ASM_MACH_MMZONE_H
+#define _ASM_MACH_MMZONE_H
+
+#if defined(CONFIG_NEED_MULTIPLE_NODES) && defined(CONFIG_NUMA)
+
+#include <linux/cpumask.h>
+
+#ifndef NLM_MAX_CPU_NODE
+#define NLM_MAX_CPU_NODE	4
+#endif
+
+struct nlm_mem_info {
+	unsigned long low_pfn;
+	unsigned long high_pfn;
+	unsigned long map_pfn;
+	unsigned long bootmem_size;
+};
+struct nlm_node_mem_frag {
+	unsigned long start_pfn;
+	unsigned long end_pfn;
+};
+
+#define NLM_MAX_MEM_FRAGS_PER_NODE 16
+struct nlm_node_mem_info {
+	struct nlm_node_mem_frag mem[NLM_MAX_MEM_FRAGS_PER_NODE];
+	unsigned long free_addr; /* for node_data */
+	int frags;
+};
+
+struct nlm_cpu_info {
+	struct cpumask mask;
+};
+
+extern struct nlm_mem_info __node_mem_data[];
+
+struct nlm_node_data {
+	struct pglist_data pg_data;
+	struct nlm_cpu_info cpu;
+};
+
+extern struct nlm_node_data *__node_data[];
+
+#define NODE_DATA(nid)		(&__node_data[nid]->pg_data)
+#define NODE_CPU_MASK(nid)	(&__node_data[nid]->cpu.mask)
+#define NODE_MEM_DATA(nid)	(&__node_mem_data[nid])
+
+static inline unsigned int pa_to_nid(unsigned long addr)
+{
+	unsigned int  i;
+	unsigned long pfn = addr >> PAGE_SHIFT;
+
+	/* TODO: Implement this using NODE_DATA */
+	for (i = 0; i < NLM_MAX_CPU_NODE; i++) {
+		if (pfn >= NODE_MEM_DATA(i)->low_pfn && pfn <= NODE_MEM_DATA(i)->high_pfn)
+			return i;
+	}
+
+	/* it should not really reach here */
+	printk("Invalid address is %lx\n", addr);
+	panic("Invalid address in pa_to_nid\n");
+	return 0;
+}
+
+#endif
+
+#endif /* _ASM_MACH_MMZONE_H */
diff --git a/arch/mips/include/asm/mach-netlogic/topology.h b/arch/mips/include/asm/mach-netlogic/topology.h
new file mode 100644
index 0000000..56e66ab
--- /dev/null
+++ b/arch/mips/include/asm/mach-netlogic/topology.h
@@ -0,0 +1,44 @@
+#ifndef _ASM_MACH_TOPOLOGY_H
+#define _ASM_MACH_TOPOLOGY_H
+
+#ifdef CONFIG_NUMA
+
+#include <asm/mmzone.h>
+
+/* FIXME_XLP: only works for all cpus up */
+#define cpu_to_node(cpu)	(cpu >> 5)
+#define hardcpu_to_node(cpu)	(cpu >> 5)
+
+#define cpumask_of_node(node)	(NODE_CPU_MASK(node))
+
+#define parent_node(node)	(node)
+
+#ifdef CONFIG_PCI
+#define cpumask_of_pcibus(bus)  (cpu_online_mask)
+/* FIXME_XLP: to be implemented */
+#define pcibus_to_node(bus)     (0)
+#endif
+
+/* sched_domains SD_NODE_INIT for multi-node XLP machines */
+/* FIXME_XLP: the number needs to be fine tuned later */
+#define SD_NODE_INIT (struct sched_domain) {		\
+	.parent			= NULL,			\
+	.child			= NULL,			\
+	.groups			= NULL,			\
+	.min_interval		= 8,			\
+	.max_interval		= 32,			\
+	.busy_factor		= 32,			\
+	.imbalance_pct		= 125,			\
+	.cache_nice_tries	= 1,			\
+	.flags			= SD_LOAD_BALANCE |	\
+				  SD_BALANCE_EXEC,	\
+	.last_balance		= jiffies,		\
+	.balance_interval	= 1,			\
+	.nr_balance_failed	= 0,			\
+}
+
+#endif
+
+#include <asm-generic/topology.h>
+
+#endif /* _ASM_MACH_TOPOLOGY_H */
diff --git a/arch/mips/include/asm/netlogic/xlp.h b/arch/mips/include/asm/netlogic/xlp.h
index f6cb03f..b2786be 100644
--- a/arch/mips/include/asm/netlogic/xlp.h
+++ b/arch/mips/include/asm/netlogic/xlp.h
@@ -32,7 +32,8 @@ THE POSSIBILITY OF SUCH DAMAGE.
 #include <asm/netlogic/hal/nlm_hal_xlp_dev.h>
 
 #define MAX_CPU_REV_LEN		100
-#define NLM_MAX_CPU_NODE		4
+#define NLM_MAX_CPU_NODE	4
+#define NLM_MAX_DRAM_REGION	8
 #define NLM_MAX_CPU_PER_NODE	32
 #define NLM_MAX_THREADS_PER_CPU	4
 #define NLM_MAX_VC_PER_THREAD	4
diff --git a/arch/mips/kernel/scall64-64.S b/arch/mips/kernel/scall64-64.S
index 8ee1420..bc193a7 100644
--- a/arch/mips/kernel/scall64-64.S
+++ b/arch/mips/kernel/scall64-64.S
@@ -366,8 +366,13 @@ EXPORT(sys_call_table)
 	PTR	sys_tgkill			/* 5225 */
 	PTR	sys_utimes
 	PTR	sys_mbind
+#ifdef CONFIG_NUMA
+	PTR	sys_get_mempolicy
+	PTR	sys_set_mempolicy
+#else
 	PTR	sys_ni_syscall			/* sys_get_mempolicy */
 	PTR	sys_ni_syscall			/* sys_set_mempolicy */
+#endif
 	PTR	sys_mq_open			/* 5230 */
 	PTR	sys_mq_unlink
 	PTR	sys_mq_timedsend
diff --git a/arch/mips/kernel/scall64-n32.S b/arch/mips/kernel/scall64-n32.S
index dc18ddd..397b9ef 100644
--- a/arch/mips/kernel/scall64-n32.S
+++ b/arch/mips/kernel/scall64-n32.S
@@ -365,9 +365,15 @@ EXPORT(sysn32_call_table)
 	PTR	compat_sys_clock_nanosleep
 	PTR	sys_tgkill
 	PTR	compat_sys_utimes		/* 6230 */
+#ifdef CONFIG_NUMA
+	PTR	sys_mbind
+	PTR	sys_get_mempolicy
+	PTR	sys_set_mempolicy
+#else
 	PTR	sys_ni_syscall			/* sys_mbind */
 	PTR	sys_ni_syscall			/* sys_get_mempolicy */
 	PTR	sys_ni_syscall			/* sys_set_mempolicy */
+#endif
 	PTR	compat_sys_mq_open
 	PTR	sys_mq_unlink			/* 6235 */
 	PTR	compat_sys_mq_timedsend
diff --git a/arch/mips/kernel/scall64-o32.S b/arch/mips/kernel/scall64-o32.S
index 3cab205..e8e4190 100644
--- a/arch/mips/kernel/scall64-o32.S
+++ b/arch/mips/kernel/scall64-o32.S
@@ -500,9 +500,15 @@ EXPORT(syso32_call_table)
 	PTR	compat_sys_clock_nanosleep	/* 4265 */
 	PTR	sys_tgkill
 	PTR	compat_sys_utimes
+#ifdef CONFIG_NUMA
+	PTR	sys_mbind
+	PTR	sys_get_mempolicy
+	PTR	sys_set_mempolicy
+#else
 	PTR	sys_ni_syscall			/* sys_mbind */
 	PTR	sys_ni_syscall			/* sys_get_mempolicy */
 	PTR	sys_ni_syscall			/* 4270 sys_set_mempolicy */
+#endif
 	PTR	compat_sys_mq_open
 	PTR	sys_mq_unlink
 	PTR	compat_sys_mq_timedsend
diff --git a/arch/mips/kernel/setup.c b/arch/mips/kernel/setup.c
index 6ee84fe..b488df2 100644
--- a/arch/mips/kernel/setup.c
+++ b/arch/mips/kernel/setup.c
@@ -98,7 +98,9 @@ static struct resource data_resource = { .name = "Kernel data", };
 void __init add_memory_region(uint64_t start, uint64_t size, long type)
 {
 	int x = boot_mem_map.nr_map;
+#ifndef CONFIG_NUMA
 	struct boot_mem_map_entry *prev = boot_mem_map.map + x - 1;
+#endif
 
 	/* Sanity check */
 	if (start + size < start) {
@@ -106,6 +108,8 @@ void __init add_memory_region(uint64_t start, uint64_t size, long type)
 		return;
 	}
 
+	/* For numa, we want to avoid merging memories from different nodes */
+#ifndef CONFIG_NUMA
 	/*
 	 * Try to merge with previous entry if any.  This is far less than
 	 * perfect but is sufficient for most real world cases.
@@ -114,6 +118,7 @@ void __init add_memory_region(uint64_t start, uint64_t size, long type)
 		prev->size += size;
 		return;
 	}
+#endif
 
 	if (x == BOOT_MEM_MAP_MAX) {
 		pr_err("Ooops! Too many entries in the memory map!\n");
@@ -272,7 +277,34 @@ static void __init bootmem_init(void)
 	finalize_initrd();
 }
 
-#else  /* !CONFIG_SGI_IP27 */
+#elif defined(CONFIG_NLM_XLP) && defined(CONFIG_NUMA)
+
+static void __init bootmem_init(void)
+{
+	unsigned long reserved_end;
+
+	/*
+	 * Init any data related to initrd. It's a nop if INITRD is
+	 * not selected. Once that done we can determine the low bound
+	 * of usable memory.
+	 */
+#ifdef CONFIG_XEN
+	reserved_end = max(init_initrd(),
+			   (unsigned long) PFN_UP(__pa_symbol(&_end) + PAGE_SIZE));
+#else
+	reserved_end = max(init_initrd(),
+			   (unsigned long) PFN_UP(__pa_symbol(&_end)));
+#endif
+
+	nlm_numa_bootmem_init(reserved_end);
+
+	/*
+	 * Reserve initrd memory if needed.
+	 */
+	finalize_initrd();
+}
+
+#else  /* !CONFIG_SGI_IP27 && !(defined(CONFIG_NLM_XLP) && defined(CONFIG_NUMA)) */
 
 static void __init bootmem_init(void)
 {
@@ -553,11 +585,10 @@ static void __init arch_mem_init(char **cmdline_p)
 		print_memory_map();
 	}
     
-	setup_mapped_kernel_tlbs(TRUE, TRUE);	
 	bootmem_init();
-/*
+#ifndef CONFIG_NUMA
 	setup_mapped_kernel_tlbs(FALSE, TRUE);
-*/
+#endif
 	device_tree_init();
 	sparse_init();
 	plat_swiotlb_setup();
diff --git a/arch/mips/netlogic/xlp/Makefile b/arch/mips/netlogic/xlp/Makefile
index 9449750..2cfd9b0 100644
--- a/arch/mips/netlogic/xlp/Makefile
+++ b/arch/mips/netlogic/xlp/Makefile
@@ -15,3 +15,4 @@ obj-y += xenbootinfo.o
 endif
 
 obj-$(CONFIG_NLM_XLP) += cpu_control.o cpu_control_asm.o
+obj-$(CONFIG_NUMA) += numa.o
diff --git a/arch/mips/netlogic/xlp/numa.c b/arch/mips/netlogic/xlp/numa.c
new file mode 100644
index 0000000..ad45c20
--- /dev/null
+++ b/arch/mips/netlogic/xlp/numa.c
@@ -0,0 +1,457 @@
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/bootmem.h>
+#include <linux/mm.h>
+#include <linux/percpu.h>
+#include <linux/mmzone.h>
+#include <linux/pfn.h>
+#include <linux/highmem.h>
+#include <linux/swap.h>
+
+#include <asm/addrspace.h>
+#include <asm/pgalloc.h>
+#include <asm/sections.h>
+#include <asm/bootinfo.h>
+#include <asm/mach-netlogic/mmu.h>
+#include <asm/netlogic/xlp.h>
+
+extern unsigned long setup_zero_pages(void);
+
+struct nlm_node_data *__node_data[NLM_MAX_CPU_NODE];
+struct nlm_node_data __node_data_holder[NLM_MAX_CPU_NODE];
+struct nlm_mem_info __node_mem_data[MAX_NUMNODES];
+
+struct xlp_dram_mapping {
+	unsigned long low_pfn;
+	unsigned long high_pfn;
+	int node;
+};
+#define NLM_NODES_MAX_DRAM_REGION (NLM_MAX_DRAM_REGION * MAX_NUMNODES)
+static struct xlp_dram_mapping	dram_map[NLM_NODES_MAX_DRAM_REGION];
+extern int hcpu_to_lcpu[];
+
+#define NLM_DRAM_BASE_REG_0	20
+#define NLM_DRAM_LIMIT_REG_0	28
+#define NLM_DRAM_NODEADDR_XLAT	36
+
+
+#define HDR_OFFSET	0x100
+#define BRIDGE  (0x00<<20) | (0x00<<15) | (0x00<<12)
+#define cpu_io_mmio(node,offset)  ((__u32 *)(DEFAULT_NETLOGIC_IO_BASE + \
+                         (node<<18) + (offset) + HDR_OFFSET))
+
+#if defined (CONFIG_64BIT) && defined (CONFIG_MAPPED_KERNEL)
+#define _low_virt_to_phys(addr) ((unsigned long)(addr) & ~CKSSEG)
+#endif
+
+static uint8_t _node_map_mem[MAX_NUMNODES][PAGE_SIZE];
+
+int is_xlp832_step_A_chip(void)
+{
+	uint32_t prid    = read_c0_prid();
+	uint32_t chip_id = (prid >> 8) & 0xff;
+	uint32_t rev_id  = prid & 0xff;
+
+	if (chip_id == CHIP_PROCESSOR_ID_XLP_8XX
+	    || chip_id == CHIP_PROCESSOR_ID_XLP_832) {
+		if (rev_id == XLP_REVISION_A0 || rev_id == XLP_REVISION_A1
+		    || rev_id == XLP_REVISION_A2)
+			return 1;
+	}
+
+	return 0;
+
+}
+
+static void read_node_bars(int node)
+{
+	int i, idx;
+	uint32_t *membase = cpu_io_mmio(node, BRIDGE);
+
+	for (i = 0; i < NLM_MAX_DRAM_REGION; i++) {
+		uint64_t base_reg  = *(membase + NLM_DRAM_BASE_REG_0  + i);
+		uint64_t limit_reg = *(membase + NLM_DRAM_LIMIT_REG_0 + i);
+		uint32_t node_reg = *(membase + NLM_DRAM_NODEADDR_XLAT + i);
+
+		if(((node_reg >> 1) & 0x3) != node) {
+			continue;
+		}
+		if(((limit_reg >> 12) << 20) == 0) {
+			continue;
+		}
+		idx = (node * NLM_MAX_DRAM_REGION) + i;
+		dram_map[idx].low_pfn = ((base_reg >> 12) << 20) >> PAGE_SHIFT;
+		dram_map[idx].high_pfn = 
+			((limit_reg >> 12) << 20) >> PAGE_SHIFT;
+		dram_map[idx].node = node;
+	}
+}
+
+static void nlm_get_dram_mapping(void)
+{
+	int  node;
+
+	for(node=0; node < MAX_NUMNODES; node++) {
+		read_node_bars(node);
+	}
+}
+
+extern cpumask_t fdt_cpumask;
+static void nlm_init_bootmem_node (unsigned long mapstart, unsigned long min_pfn, unsigned long max_pfn)
+{
+	int i;
+
+	for (i = 0; i < NLM_MAX_CPU_NODE; i++) {
+		unsigned long map_pfn, start_pfn, end_pfn, bootmem_size;
+		int j;
+
+		if(!node_online(i))
+			continue;
+
+		start_pfn = NODE_MEM_DATA(i)->low_pfn;
+		end_pfn   = NODE_MEM_DATA(i)->high_pfn;
+
+		if (start_pfn && start_pfn < min_pfn)
+			start_pfn = min_pfn;
+
+		if (end_pfn > max_pfn)
+			end_pfn = max_pfn;
+
+		/* in general, never hit the condition */
+		if (start_pfn && start_pfn >= end_pfn) {
+			NODE_MEM_DATA(i)->map_pfn = 0; /* indicate a bad map_pfn */
+			continue;
+		}
+
+		if (start_pfn > mapstart)
+			map_pfn = start_pfn;
+		else
+			map_pfn = mapstart;
+
+		if((start_pfn == 0) && (end_pfn == 0)) {
+			map_pfn = 
+			_low_virt_to_phys(&_node_map_mem[i][0]) >> PAGE_SHIFT;
+			__node_data[i] = __va(map_pfn << PAGE_SHIFT);
+		} else {
+			__node_data[i] = __va(map_pfn << PAGE_SHIFT);
+			map_pfn  += PFN_UP(sizeof(struct nlm_node_data));
+		}
+
+		NODE_DATA(i)->bdata = &bootmem_node_data[i];
+		NODE_DATA(i)->node_start_pfn = start_pfn;
+		NODE_DATA(i)->node_spanned_pages = end_pfn - start_pfn;
+
+		/* Set this up with logical cpu map :
+		   Assuming here that there are 32 cpus per node
+		   */
+		cpumask_clear(NODE_CPU_MASK(i));
+		for (j = 0; j < 32; j++)
+			if (cpumask_test_cpu((j + i * 32), &fdt_cpumask))
+				cpumask_set_cpu(hcpu_to_lcpu[(j+i*32)], 
+						NODE_CPU_MASK(i));
+
+		NODE_MEM_DATA(i)->map_pfn = map_pfn;
+
+		bootmem_size = init_bootmem_node(NODE_DATA(i), map_pfn, start_pfn, end_pfn);
+		NODE_MEM_DATA(i)->bootmem_size = bootmem_size;
+	}
+}
+
+static void nlm_reserve_bootmem(void)
+{
+	int i;
+	unsigned long size;
+
+	for (i = 0; i < NLM_MAX_CPU_NODE; i++) {
+
+		if(!node_online(i))
+			continue;
+		if(NODE_DATA(i)->node_spanned_pages == 0)
+			continue;
+		size = NODE_MEM_DATA(i)->map_pfn - NODE_DATA(i)->node_start_pfn;
+		size = PFN_PHYS(size);
+		reserve_bootmem_node(NODE_DATA(i), 
+			PFN_PHYS(NODE_DATA(i)->node_start_pfn),
+			(NODE_MEM_DATA(i)->bootmem_size + size),
+			 BOOTMEM_DEFAULT);
+	}
+}
+
+static int dram_get_node_id (unsigned long start_pfn, unsigned long end_pfn)
+{
+	int i;
+
+	for (i = 0; i < NLM_MAX_CPU_NODE; i++) {
+		if (NODE_MEM_DATA(i)->low_pfn <= start_pfn && end_pfn <= NODE_MEM_DATA(i)->high_pfn)
+			return i;
+	}
+
+	printk("Invalid start %lx end %lx\n", start_pfn, end_pfn);
+	/* should not reach here */
+	panic("dram_get_node_id: incorrect memory region\n");
+}
+
+/* This is used very early in boot process to build the node mem regions */
+int get_node(unsigned long pfn)
+{
+	int i;
+	for(i=0; i < NLM_NODES_MAX_DRAM_REGION; i++) {
+		if((pfn >= dram_map[i].low_pfn) && 
+				(pfn <= dram_map[i].high_pfn))
+			return dram_map[i].node;
+	}
+	panic("Invalid PFN Passed: Cannot get node id\n");
+}
+/**
+ * boot memory initialization for NUMA architecture.
+ *
+ * The implementation here copies the implementation in
+ * arch/mips/kernel/setup, but added numa support.
+ */
+extern struct nlm_node_mem_info node_mem_info[];
+extern void prom_meminit(void);
+void __init nlm_numa_bootmem_init(unsigned long reserved_end)
+{
+	unsigned long mapstart = ~0UL;
+	int i;
+	int node, seg;
+
+	/* Get the hardware dram region info */
+	nlm_get_dram_mapping();
+
+	/*
+	 * max_low_pfn is not a number of pages. The number of pages
+	 * of the system is given by 'max_low_pfn - min_low_pfn'.
+	 */
+	min_low_pfn = ~0UL;
+	max_low_pfn = 0;
+
+	/*
+	 * Find the highest page frame number we have available.
+	 */
+	for (i = 0; i < boot_mem_map.nr_map; i++) {
+		unsigned long start, end;
+
+		if (boot_mem_map.map[i].type != BOOT_MEM_RAM)
+			continue;
+
+		start = PFN_UP(boot_mem_map.map[i].addr);
+		end = PFN_DOWN(boot_mem_map.map[i].addr
+				+ boot_mem_map.map[i].size);
+
+		node = get_node(start);
+		seg = node_mem_info[node].frags;
+		node_mem_info[node].mem[seg].start_pfn = start;
+		node_mem_info[node].mem[seg].end_pfn = end;
+		node_mem_info[node].frags++;
+
+		if (end > max_low_pfn)
+			max_low_pfn = end;
+		if (start < min_low_pfn)
+			min_low_pfn = start;
+		if (end <= reserved_end)
+			continue;
+		if (start >= mapstart)
+			continue;
+		mapstart = max(reserved_end, start);
+	}
+
+	if (min_low_pfn >= max_low_pfn)
+		panic("Incorrect memory mapping !!!");
+	if (min_low_pfn > ARCH_PFN_OFFSET) {
+		pr_info("Wasting %lu bytes for tracking %lu unused pages\n",
+			(min_low_pfn - ARCH_PFN_OFFSET) * sizeof(struct page),
+			min_low_pfn - ARCH_PFN_OFFSET);
+	} else if (min_low_pfn < ARCH_PFN_OFFSET) {
+		pr_info("%lu free pages won't be used\n",
+			ARCH_PFN_OFFSET - min_low_pfn);
+	}
+	min_low_pfn = ARCH_PFN_OFFSET;
+
+	/*
+	 * Determine low and high memory ranges
+	 */
+	max_pfn = max_low_pfn;
+	if (max_low_pfn > PFN_DOWN(HIGHMEM_START)) {
+#ifdef CONFIG_HIGHMEM
+		highstart_pfn = PFN_DOWN(HIGHMEM_START);
+		highend_pfn = max_low_pfn;
+#endif
+		max_low_pfn = PFN_DOWN(HIGHMEM_START);
+	}
+
+	max_low_pfn = recalculate_max_low_pfn(max_low_pfn);
+
+#ifdef DEBUG_MAPPED_KERNEL
+	printk("max_low_pfn = 0x%lx\n", max_low_pfn);
+#endif
+	setup_mapped_kernel_tlbs(FALSE, TRUE);
+	prom_meminit();
+	/*
+	 * Initialize the boot-time allocator with low memory only.
+	 */
+	nlm_init_bootmem_node(mapstart, min_low_pfn, max_low_pfn);
+
+	for (i = 0; i < boot_mem_map.nr_map; i++) {
+		unsigned long start, end;
+
+		start = PFN_UP(boot_mem_map.map[i].addr);
+		end = PFN_DOWN(boot_mem_map.map[i].addr
+				+ boot_mem_map.map[i].size);
+
+		if (start <= min_low_pfn)
+			start = min_low_pfn;
+		if (start >= end)
+			continue;
+
+#ifndef CONFIG_HIGHMEM
+		if (end > max_low_pfn)
+			end = max_low_pfn;
+
+		/*
+		 * ... finally, is the area going away?
+		 */
+		if (end <= start)
+			continue;
+#endif
+		add_active_range(dram_get_node_id(start, end), start, end);
+	}
+
+	/*
+	 * Register fully available low RAM pages with the bootmem allocator.
+	 */
+	for (i = 0; i < boot_mem_map.nr_map; i++) {
+		unsigned long start, end, size;
+
+		/*
+		 * Reserve usable memory.
+		 */
+		if (boot_mem_map.map[i].type != BOOT_MEM_RAM)
+			continue;
+
+		start = PFN_UP(boot_mem_map.map[i].addr);
+		end   = PFN_DOWN(boot_mem_map.map[i].addr
+				    + boot_mem_map.map[i].size);
+		/*
+		 * We are rounding up the start address of usable memory
+		 * and at the end of the usable range downwards.
+		 */
+		if (start >= max_low_pfn)
+			continue;
+		if (start < reserved_end)
+			start = reserved_end;
+		if (end > max_low_pfn)
+			end = max_low_pfn;
+
+		/*
+		 * ... finally, is the area going away?
+		 */
+		if (end <= start)
+			continue;
+		size = end - start;
+
+		/* Register lowmem ranges */
+		free_bootmem(PFN_PHYS(start), size << PAGE_SHIFT);
+		memory_present(dram_get_node_id(start, end), start, end);
+	}
+
+	/*
+	 * Reserve the bootmap memory.
+	 */
+	nlm_reserve_bootmem();
+}
+
+static int __init page_is_ram(unsigned long pagenr)
+{
+	int i;
+
+#if 0
+	for (i = 0; i < NLM_NODES_MAX_DRAM_REGION; i++) {
+		if (pagenr >= dram_map[i].low_pfn && 
+				pagenr <= dram_map[i].high_pfn) {
+			return 1;
+		}
+	}
+#endif
+	for (i = 0; i < boot_mem_map.nr_map; i++) {
+		unsigned long start, end;
+
+		if (boot_mem_map.map[i].type != BOOT_MEM_RAM)
+			continue;
+
+		start = PFN_UP(boot_mem_map.map[i].addr);
+		end   = PFN_DOWN(boot_mem_map.map[i].addr
+				    + boot_mem_map.map[i].size);
+		if(pagenr >= start && pagenr <= end)
+			return 1;
+	}
+	return 0;
+}
+
+void __init paging_init(void)
+{
+	unsigned long zones_size[MAX_NR_ZONES] = {0, };
+	unsigned node;
+
+	pagetable_init();
+
+#ifdef CONFIG_ZONE_DMA
+	zones_size[ZONE_DMA] = MAX_DMA_PFN;
+#endif
+
+
+	for_each_online_node(node) {
+		unsigned long start_pfn, end_pfn;
+
+		get_pfn_range_for_nid(node, &start_pfn, &end_pfn);
+
+		if (end_pfn > max_low_pfn)
+			max_low_pfn = end_pfn;
+	}
+	zones_size[ZONE_NORMAL] = max_low_pfn;
+	free_area_init_nodes(zones_size);
+}
+
+void __init mem_init(void)
+{
+	unsigned long codesize, reservedpages, datasize, initsize, tmp, ram;
+	unsigned int  node;
+
+	high_memory = (void *) __va(max_low_pfn<< PAGE_SHIFT);
+	for_each_online_node(node) {
+		/*
+		 * This will free up the bootmem, ie, slot 0 memory.
+		 */
+		//if((NODE_MEM_DATA(node)->low_pfn !=0) && 
+				//(NODE_MEM_DATA(node)->high_pfn !=0))
+			totalram_pages += 
+				free_all_bootmem_node(NODE_DATA(node));
+	}
+
+	totalram_pages -= setup_zero_pages();   /* This comes from node 0 */
+
+	reservedpages = ram = 0;
+	for (tmp = 0; tmp < max_low_pfn; tmp++)
+		if (page_is_ram(tmp)) {
+			ram++;
+			if (PageReserved(pfn_to_page(tmp)))
+				reservedpages++;
+		}
+	num_physpages = ram;
+
+	codesize =  (unsigned long) &_etext - (unsigned long) &_text;
+	datasize =  (unsigned long) &_edata - (unsigned long) &_etext;
+	initsize =  (unsigned long) &__init_end - (unsigned long) &__init_begin;
+
+	printk(KERN_INFO "Memory: %luk/%luk available (%ldk kernel code, "
+		"%ldk reserved, %ldk data, %ldk init, %ldk highmem)\n",
+		nr_free_pages() << (PAGE_SHIFT - 10),
+		ram << (PAGE_SHIFT - 10),
+		codesize >> 10,
+		reservedpages << (PAGE_SHIFT - 10),
+		datasize >> 10,
+		initsize >> 10,
+		(unsigned long) (totalhigh_pages << (PAGE_SHIFT - 10)));
+}
diff --git a/arch/mips/netlogic/xlp/setup.c b/arch/mips/netlogic/xlp/setup.c
index 6deb993..42197b3 100644
--- a/arch/mips/netlogic/xlp/setup.c
+++ b/arch/mips/netlogic/xlp/setup.c
@@ -99,6 +99,9 @@ extern unsigned long _text[];
 extern cpumask_t fdt_cpumask;
 extern cpumask_t fdt_loadermask;
 int xlp_loader_support = 0;
+#ifdef CONFIG_NUMA
+int hcpu_to_lcpu[NR_CPUS];
+#endif
 
 struct proc_dir_entry *nlm_root_proc;
 EXPORT_SYMBOL(nlm_root_proc);
@@ -114,6 +117,12 @@ static char prop_buf[MAX_PROP_LEN];
 void *nlm_common_psb_shm = 0;
 unsigned long nlm_common_psb_shm_size = 0;
 
+#ifdef CONFIG_NUMA
+struct nlm_node_mem_info node_mem_info[NLM_MAX_CPU_NODE];
+/* number of nodes */
+static int nlm_nodes=1;
+#endif
+
 #ifdef CONFIG_NLMCOMMON_GLOBAL_TLB_SPLIT_ASID
 unsigned long nlm_asid_mask = 0x3f;
 unsigned int nlm_shtlb = 1; /* by default shared TLB is enabled */
@@ -498,6 +507,13 @@ static int fdt_process(void)
 	 */
 	node = finddevice("/doms/dom@0");
 	if (node) {
+#ifdef CONFIG_NUMA
+		if (getprop(node, "#nodes", &nlm_nodes, sizeof(nlm_nodes)) < 0)
+			nlm_nodes = 1;
+		else
+			nlm_nodes = fdt32_to_cpu(nlm_nodes);
+#endif
+
 		if (getprop(node, "#address-cells", &na, sizeof(na)) < 0)
 			na = 1;
 		else
@@ -841,6 +857,76 @@ extern void xen_init(void);
 static void xen_init(void) {}
 #endif
 
+#ifdef CONFIG_NUMA
+static void sort_mem_info(struct nlm_node_mem_info *info, unsigned long *spfn,
+	unsigned long *epfn)
+{
+	struct nlm_node_mem_frag *list = info->mem;
+	int i,j;
+
+	uint64_t start_pfn = 0;
+	uint64_t end_pfn = 0;
+
+	*spfn = *epfn = 0;
+	if(info->frags == 0)
+		return;
+
+	for(i=0; i < info->frags; i++) {
+		for (j = i; j < info->frags; j++) {
+			if (list[i].start_pfn > list[j].start_pfn) {
+				start_pfn = list[i].start_pfn;
+				end_pfn = list[i].end_pfn;
+				list[i].start_pfn = list[j].start_pfn;
+				list[i].end_pfn = list[j].end_pfn;
+				list[j].start_pfn = start_pfn;
+				list[j].end_pfn = end_pfn;
+			}
+		}
+	}
+	*spfn = list[0].start_pfn;
+	*epfn = list[info->frags-1].end_pfn;
+}
+
+void __init prom_meminit(void)
+{
+	int node=0;
+	unsigned long start_pfn, end_pfn;
+	struct nlm_mem_info *minfo;
+
+	/* sort the node_mem_map */
+	for(node=0; node < nlm_nodes; node++) {
+        	sort_mem_info(&node_mem_info[node], &start_pfn, &end_pfn);
+        	minfo = NODE_MEM_DATA(node);
+        	minfo->low_pfn = start_pfn;
+        	minfo->high_pfn = end_pfn;
+	}
+}
+
+extern struct nlm_node_data __node_data_holder[];
+void __init build_node_cpu_map(void)
+{
+	int cpu, node,i;
+
+	/* kernel expects all node_data to initialized
+ 	 * If a node has its own memory, we will overwrite this pointer
+ 	 */
+	for(node=0; node < MAX_NUMNODES; node++) {
+		__node_data[node] = &__node_data_holder[node];
+	}
+
+	i=0;
+	for_each_cpu(cpu, &fdt_cpumask) {
+		node = hardcpu_to_node(cpu);
+		hcpu_to_lcpu[cpu] = i;
+		if(!node_online(node)) {
+			node_set_online(num_online_nodes());
+		}
+		i++;
+	}
+	printk("Number of online nodes = %d\n", num_online_nodes());
+}
+#endif
+
 void __init prom_init(void)
 {
 #ifdef CONFIG_NLM_ENABLE_COP2
@@ -850,6 +936,10 @@ void __init prom_init(void)
 
 	fdt_process();
 
+#ifdef CONFIG_NUMA
+	build_node_cpu_map();
+#endif
+
 	xen_init();
 
 	nlm_common_ebase = read_c0_ebase() & (~((1 << 12) - 1));
diff --git a/arch/mips/netlogic/xlp/smp.c b/arch/mips/netlogic/xlp/smp.c
index 2aa3612..8c2b5e2 100644
--- a/arch/mips/netlogic/xlp/smp.c
+++ b/arch/mips/netlogic/xlp/smp.c
@@ -108,7 +108,7 @@ static void __cpuinit nlm_init_secondary(void)
     /* Time init for this cpu is done in mips_clockevent_init() */
     nlm_smp_irq_init();
     enable_msgconfig_int();
-#ifdef CONFIG_NLM_XLP_A0_WORKAROUNDS
+
     /* Workaround for XLP A0 Multi-Node bug */
     {
 	    int cpu = hard_smp_processor_id();
@@ -118,7 +118,7 @@ static void __cpuinit nlm_init_secondary(void)
 		    nlm_common_timer_setup();
 	    }
     }
-#endif
+
     /* Enable vc interupts for this thread*/
     nlm_enable_vc_intr();
 }
diff --git a/init/main.c b/init/main.c
index 9caced7..256e284 100644
--- a/init/main.c
+++ b/init/main.c
@@ -876,6 +876,24 @@ static int __init kernel_init(void * unused)
 	smp_init();
 	sched_init_smp();
 
+#if defined(CONFIG_NLM_XLP) && defined(CONFIG_NUMA)
+	/* This is really a workaround for XLP A0/1/2 chips.
+	 * On these chips, PIC device specific registers cannot be accessed
+	 * cross different nodes. So the kernel_init has to run on node 0.
+	 * For simplicity, just let is run on vcpu 0.
+	 */
+	{
+		extern int is_xlp832_step_A_chip(void);
+
+                if (is_xlp832_step_A_chip()) {
+			struct cpumask new_mask;
+			cpumask_clear(&new_mask);
+			cpumask_set_cpu(0, &new_mask);
+			set_cpus_allowed_ptr(current, &new_mask);
+		}
+	}
+#endif
+
 	do_basic_setup();
 
 	/* Use /dev/console to infer if the rootfs is setup properly */
-- 
1.7.10.4

