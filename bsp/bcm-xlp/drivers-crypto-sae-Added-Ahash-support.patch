From 0fa1c767a4e564778dd9a03852be986508e16691 Mon Sep 17 00:00:00 2001
From: reshmic <reshmic@broadcom.com>
Date: Wed, 12 Dec 2012 10:10:38 +0530
Subject: [PATCH 744/761] drivers/crypto/sae: Added Ahash support.

Linux 3.0 suppport asynchronous hash apart from linux 2.6.32 `s
synchronous hash.

Based on Broadcom SDK 2.3.

Signed-off-by: reshmic <reshmic@broadcom.com>
Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 drivers/crypto/sae/nlm_auth.c   |  578 ++++++++++++++++++++++-----------------
 drivers/crypto/sae/nlm_crypto.c |   17 --
 2 files changed, 321 insertions(+), 274 deletions(-)

diff --git a/drivers/crypto/sae/nlm_auth.c b/drivers/crypto/sae/nlm_auth.c
index aea787e..44b3e0b 100644
--- a/drivers/crypto/sae/nlm_auth.c
+++ b/drivers/crypto/sae/nlm_auth.c
@@ -19,6 +19,7 @@
 #include <crypto/internal/hash.h>
 #include "nlm_async.h"
 #include <asm/netlogic/msgring.h>
+#include <crypto/scatterwalk.h>
 
 #define XLP_AUTH_PRIORITY      300
 #define XLP_HMAC_PRIORITY      300
@@ -28,40 +29,36 @@
 #define MD5_DIGEST_SIZE		16
 #define MD5_BLOCK_SIZE		64
 
-//#define AUTH_BUFFER_SIZE	(16 * 1024)
-void hex_dump(char * description,unsigned char *in, int num);
-extern int auth_mode_key_len[NLM_HASH_MAX][NLM_HASH_MODE_MAX];
+#undef NLM_CRYPTO_DEBUG
 
-#define ASYNC_PTR_SIZE 128
-#define ASYNC_PTR_OFFSET (sizeof(struct auth_pkt_desc ) + (NLM_AUTH_MAX_FRAGS* 16))
 
-//#define SEC_DEBUG
-
-
-#ifdef SEC_DEBUG
-#ifdef __KERNEL__
-#define debug_print(fmt, args...) printk(fmt, ##args)
-#else				/* __KERNEL__ */
-#define debug_print(fmt, args...) printf(fmt, ##args)
-#endif				/* __KERNEL__ */
-#else				/* SEC_DEBUG */
-#define debug_print(fmt, args...)
-#endif				/* SEC_DEBUG */
-
-#define malloc(a) kmalloc(a, GFP_KERNEL)
-#define free kfree
-#define NLM_AUTH_MAX_FRAGS	(20)
-
-#define MAX_AUTH_DATA 64000
+#ifdef NLM_CRYPTO_DEBUG
+extern void hex_dump(char * description,unsigned char *in, int num);
+#endif
 
+int auth_mode_key_len[NLM_HASH_MAX][NLM_HASH_MODE_MAX] = {
+/*	               SHA1 SHA224 SHA256 SHA384 SHA512  CMAC  XCBC CBC_MAC CCM  GCM*/
+/* BYPASS */		{0,    0,     0,     0,     0,     0,   0,    0,     0,    0, },
+/* MD5 */		{64,   64,    64,   64,    64,    64,  64,   64,    64,   64, },
+/* SHA */		{64,   64,    64,   128,   128,    0,   0,    0,     0,    0, },
+/* 3 */			{0,    0,     0,     0,     0,     0,   0,    0,     0,    0, },
+/* AES128 */		{0,    0,     0,     0,     0,    16,  16,   16,    16,   16, },
+/* AES192 */		{0,    0,     0,     0,     0,    24,  24,   24,    24,   24, },
+/* AES256 */		{0,    0,     0,     0,     0,    32,  32,   32,    32,   32, },
+/* KASUMI_F9 */		{16,  16,    16,    16,    16,    16,  16,   16,    16,   16, },
+/* SNOW3G_F9 */		{16,  16,    16,    16,    16,    16,  16,   16,    16,   16, }, //sandip -> verify
+/* CAMELLIA128 */	{0,    0,     0,     0,     0,    16,  16,   16,    16,   16, },
+/* CAMELLIA192 */	{0,    0,     0,     0,     0,    24,  24,   24,    24,   24, },
+/* CAMELLIA256 */	{0,    0,     0,     0,     0,    32,  32,   32,    32,   32, },
+/* GHASH */		{0,    0,     0,     0,     0,    32,  32,   32,    32,   32, }, //todo:
+};
 
 struct nlm_auth_ctx
 {
 	struct nlm_crypto_pkt_ctrl ctrl;
 	uint16_t stat;
-	uint8_t hashed_key[128];
-	struct crypto_shash * fallback_tfm;
-	uint8_t data[MAX_AUTH_DATA];
+	uint8_t hashed_key[128]; // can be replace by a local varibale */
+	struct crypto_ahash * fallback_tfm;
 	/*Don't change the order of this strucutre*/
 };
 
@@ -70,19 +67,17 @@ struct nlm_auth_ctx
 
 struct auth_pkt_desc
 {
-	uint16_t curr_index;
-	uint16_t total_len;
-	uint16_t stat;
-	uint16_t is_allocated;
-	int max_frags;
-	struct shash_desc * fallback;
-	uint8_t  * alloc_pkt_param;
+	uint32_t total_len;
+	uint32_t index;
+	uint16_t max_frags;
+	struct ahash_request * fallback_req;
 	struct nlm_crypto_pkt_param * pkt_param; /* maintain at the end */ 
 };
 
-#define PACKET_DESC_SIZE   (64+sizeof(struct auth_pkt_desc) + MAX_FRAGS*(2*8) ) /* should be less than PAGE_SIZE/8 */ 
+#define PACKET_DESC_SIZE   (64+sizeof(struct auth_pkt_desc) + MAX_FRAGS*(2*8) + sizeof(struct nlm_async_crypto) + 128 ) /* should be less than PAGE_SIZE/8 */ 
 #define NLM_CRYPTO_PKT_PARAM_OFFSET(addr)       (((unsigned long)addr + 64) & ~0x3fUL)
-#define NLM_ASYNC_PTR_PARAM_OFFSET(addr)        (((unsigned long)(addr + 64 + sizeof(struct nlm_crypto_pkt_param) + MAX_FRAGS*(2*8)) + 64) & ~0x3fUL)
+#define NLM_ASYNC_PTR_PARAM_OFFSET(addr)        ((unsigned long)(addr + 64 + sizeof(struct auth_pkt_desc) + MAX_FRAGS*(2*8) + 64) & ~0x3fUL)
+#define NLM_CRYPTO_CTRL_DESC(addr ) ( ((unsigned long)addr + 64) & ~(0x3fUL))
 /*
    All extern declaration goes here.
  */
@@ -102,264 +97,236 @@ static inline void print_info(const char *func)
 	printk("\n*********[%s Dumpstack ends]***********\n\n\n",func);
 	return;
 }
-#ifdef NLM_CRYPTO_DEBUG
-static void print_buf(unsigned char *msg, unsigned char *buf, int len)
+static struct nlm_auth_ctx * ctrl_desc_crypto_ahash_ctx(struct crypto_ahash * ahash)
 {
-#define TMP_BUF		50
-	char tmp_buf[TMP_BUF + 1];
-	int i, index = 0;
-
-	printk("**********%s************\n",msg);
-	for(i=0; i<len; i++){
-		sprintf(&tmp_buf[index*2], "%02x", buf[i]);
-		index++;
-		if(index == (TMP_BUF/2)){
-			tmp_buf[index*2] = '\0';
-			printk("[%s]\n",tmp_buf);
-			index = 0;
-		}
-	}
-	if(index){
-		tmp_buf[index*2] = '\0';
-		printk("[%s]\n",tmp_buf);
-	}
+	struct crypto_tfm * tfm = crypto_ahash_tfm(ahash);
+	struct nlm_auth_ctx * ctx = (struct nlm_auth_ctx * )NLM_CRYPTO_CTRL_DESC(crypto_tfm_ctx(tfm));
+	return ctx;
 }
-#endif
 
-static struct nlm_auth_ctx *nlm_shash_auth_ctx(struct crypto_shash *shash)
+
+static struct nlm_auth_ctx * ctrl_desc_req_ahash_ctx(struct ahash_request * req) 
 {
-	uint8_t *ctx = crypto_tfm_ctx(crypto_shash_tfm(shash));
-	ctx = (uint8_t *)(((unsigned long)ctx + 63) & ~(0x3fUL));
-	return (struct nlm_auth_ctx *)ctx;
+	struct crypto_ahash * ahash = crypto_ahash_reqtfm(req);
+	return ctrl_desc_crypto_ahash_ctx(ahash);
 }
 
-static struct nlm_auth_ctx *pkt_ctrl_auth_ctx(struct shash_desc *desc)
+struct auth_pkt_desc *pkt_desc_req_ctx(struct ahash_request *req) 
 {
-	return nlm_shash_auth_ctx(desc->tfm);
+	struct auth_pkt_desc *auth_pkt_desc = (struct auth_pkt_desc * )NLM_CRYPTO_PKT_PARAM_OFFSET(ahash_request_ctx(req));
+	return auth_pkt_desc; 
 }
 
 static int
 xlp_cra_xcbc_init(struct crypto_tfm *tfm)
 {
-	struct crypto_shash * hash = __crypto_shash_cast(tfm);
-	struct nlm_auth_ctx * nlm_ctx = nlm_shash_auth_ctx(hash);
-	nlm_ctx->fallback_tfm = crypto_alloc_shash("xcbc(aes-generic)",CRYPTO_ALG_TYPE_SHASH ,0 );
+	struct nlm_auth_ctx * nlm_ctx  = (struct nlm_auth_ctx *)NLM_CRYPTO_CTRL_DESC(crypto_tfm_ctx(tfm));
+	nlm_ctx->fallback_tfm = crypto_alloc_ahash("xcbc(aes-generic)",CRYPTO_ALG_TYPE_AHASH ,0 );
+	crypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),PACKET_DESC_SIZE);
 	return 0;
 }
 
 static int
 xlp_cra_hmac_sha1_init(struct crypto_tfm *tfm)
 {
-	struct crypto_shash * hash = __crypto_shash_cast(tfm);
-	struct nlm_auth_ctx * nlm_ctx = nlm_shash_auth_ctx(hash);
-	nlm_ctx->fallback_tfm = crypto_alloc_shash("hmac(sha1-generic)",CRYPTO_ALG_TYPE_SHASH ,0 );
+	struct nlm_auth_ctx * nlm_ctx  = (struct nlm_auth_ctx *)NLM_CRYPTO_CTRL_DESC(crypto_tfm_ctx(tfm));
+	nlm_ctx->fallback_tfm = crypto_alloc_ahash("hmac(sha1-generic)",CRYPTO_ALG_TYPE_AHASH ,0 );
+	crypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),PACKET_DESC_SIZE);
 	return 0;
 }
 static int
 xlp_cra_hmac_sha256_init(struct crypto_tfm *tfm)
 {
-	struct crypto_shash * hash = __crypto_shash_cast(tfm);
-	struct nlm_auth_ctx * nlm_ctx = nlm_shash_auth_ctx(hash);
-	nlm_ctx->fallback_tfm = crypto_alloc_shash("hmac(sha256-generic)",CRYPTO_ALG_TYPE_SHASH ,0 );
+	struct nlm_auth_ctx * nlm_ctx  = (struct nlm_auth_ctx *)NLM_CRYPTO_CTRL_DESC(crypto_tfm_ctx(tfm));
+	nlm_ctx->fallback_tfm = crypto_alloc_ahash("hmac(sha256-generic)",CRYPTO_ALG_TYPE_AHASH ,0 );
+	crypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),PACKET_DESC_SIZE);
 	return 0;
 }
 
 static int
 xlp_cra_md5_init(struct crypto_tfm *tfm)
 {
-	struct crypto_shash * hash = __crypto_shash_cast(tfm);
-	struct nlm_auth_ctx * nlm_ctx = nlm_shash_auth_ctx(hash);
-	nlm_ctx->fallback_tfm = crypto_alloc_shash("hmac(md5-generic)",CRYPTO_ALG_TYPE_SHASH ,0 );
-	
+	struct nlm_auth_ctx * nlm_ctx  = (struct nlm_auth_ctx *)NLM_CRYPTO_CTRL_DESC(crypto_tfm_ctx(tfm));
+	nlm_ctx->fallback_tfm = crypto_alloc_ahash("hmac(md5-generic)",CRYPTO_ALG_TYPE_AHASH ,0 );
+	crypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),PACKET_DESC_SIZE);
 	return 0;
 }
 static int
-xlp_auth_init(struct shash_desc *desc)
+xlp_auth_init(struct ahash_request *req)
 {
-	struct auth_pkt_desc *auth_pkt_desc = (struct auth_pkt_desc * )shash_desc_ctx(desc);
-	auth_pkt_desc->curr_index = 0;
+	struct auth_pkt_desc *auth_pkt_desc = pkt_desc_req_ctx(req);
+	struct nlm_async_crypto * async =  (struct nlm_async_crypto *)NLM_ASYNC_PTR_PARAM_OFFSET((unsigned long)auth_pkt_desc);;
 	auth_pkt_desc->total_len = 0;
-	auth_pkt_desc->fallback = NULL;
-	auth_pkt_desc->pkt_param = ( struct nlm_crypto_pkt_param  * )NLM_CRYPTO_PKT_PARAM_OFFSET((unsigned long )auth_pkt_desc + sizeof(struct auth_pkt_desc ));
+	async->stat = 0;
+	auth_pkt_desc->fallback_req = NULL;
 	auth_pkt_desc->max_frags = MAX_FRAGS;
-	auth_pkt_desc->is_allocated = 0;
+	auth_pkt_desc->index = 0;
+	auth_pkt_desc->pkt_param = ( struct nlm_crypto_pkt_param  * )NLM_CRYPTO_PKT_PARAM_OFFSET((unsigned long )auth_pkt_desc + sizeof(struct auth_pkt_desc ));
+	async->pkt_param = NULL;
 	return 0;
 }
+extern 
+int nlm_crypto_calc_rem_len(struct scatterlist *sg, unsigned int cipher_len);
+extern int fill_src_dst_sg(struct scatterlist *src_sg, struct scatterlist *dst_sg, unsigned int cipher_len,
+                struct nlm_crypto_pkt_param *param, 
+                int seg, unsigned int max_frags, int op);
 static int
-xlp_auth_update(struct shash_desc *desc,
-		const uint8_t * data, unsigned int length)
+xlp_auth_update(struct ahash_request *areq)
 {
-	struct auth_pkt_desc *auth_pkt_desc = (struct auth_pkt_desc *)shash_desc_ctx(desc);
-	struct nlm_auth_ctx * nlm_ctx = nlm_shash_auth_ctx(desc->tfm);
-	int index = auth_pkt_desc->curr_index;
-	unsigned char * data_index = &nlm_ctx->data[auth_pkt_desc->total_len];
+	struct auth_pkt_desc *auth_pkt_desc = pkt_desc_req_ctx(areq);
 	struct nlm_crypto_pkt_param  *pkt_param  = auth_pkt_desc->pkt_param;
+	int max_frags = auth_pkt_desc->max_frags;
+	int index = auth_pkt_desc->index;
+	unsigned int auth_len;
+	struct nlm_async_crypto * async =  (struct nlm_async_crypto *)NLM_ASYNC_PTR_PARAM_OFFSET((unsigned long)auth_pkt_desc);;
 
-	auth_pkt_desc->total_len += length;
-	if  (auth_pkt_desc->total_len >= MAX_AUTH_DATA)  {
-		if (auth_pkt_desc->fallback == NULL ) {
-			auth_pkt_desc->fallback = kmalloc((sizeof(struct shash_desc ) + 
-				crypto_shash_descsize(nlm_ctx->fallback_tfm))
-									,GFP_KERNEL);
-			auth_pkt_desc->fallback->tfm = nlm_ctx->fallback_tfm;
-			auth_pkt_desc->fallback->flags = desc->flags & CRYPTO_TFM_REQ_MAY_SLEEP;
-			crypto_shash_init(auth_pkt_desc->fallback);
-			auth_pkt_desc->total_len -= length;
-			crypto_shash_update(auth_pkt_desc->fallback,nlm_ctx->data,auth_pkt_desc->total_len);
-			auth_pkt_desc->total_len += length;
-		}
-		crypto_shash_update(auth_pkt_desc->fallback,data,length);
-		
+	if ( areq->nbytes == 0 )
 		return 0;
-	}
+	do {
+		auth_len = areq->nbytes;
+		index = fill_src_dst_sg(areq->src,areq->src,auth_len,pkt_param,index,max_frags,0);
+
+		if ( index > max_frags ) {
+			struct nlm_crypto_pkt_param * param  = auth_pkt_desc->pkt_param;
+			int filled_index = auth_pkt_desc->index;
+			int i;
+			alloc_pkt_param(async,&pkt_param,index);
+			for(i = 0; i < filled_index; i++) {
+				pkt_param->segment[i][0] =  param->segment[i][0];
+				pkt_param->segment[i][1] =  param->segment[i][1];
+			}
+			max_frags = index;
+			index = auth_pkt_desc->index;
+			auth_pkt_desc->pkt_param = pkt_param;
+		}
+	}while(index == auth_pkt_desc->index );
 
-	memcpy(data_index,data,length);
-	if ( (auth_pkt_desc->curr_index + 1 ) > auth_pkt_desc->max_frags ) {
-		uint8_t * mem = kmalloc(sizeof (struct nlm_crypto_pkt_param) 
-					+ ((auth_pkt_desc->max_frags + MAX_FRAGS) * 2 * 8 ) + 64 , GFP_KERNEL);
-		auth_pkt_desc->alloc_pkt_param = mem;
-		mem = (uint8_t *)NLM_CRYPTO_PKT_PARAM_OFFSET((unsigned long )mem);
-		memcpy(mem,pkt_param,sizeof (struct nlm_crypto_pkt_param)+(auth_pkt_desc->max_frags * 2 * 8));
-		
-		if ( auth_pkt_desc->is_allocated )
-			kfree(auth_pkt_desc->alloc_pkt_param);
-		pkt_param = auth_pkt_desc->pkt_param = ( struct nlm_crypto_pkt_param  * )mem;
-		auth_pkt_desc->is_allocated = 0;
-		auth_pkt_desc->max_frags += MAX_FRAGS;
-	} 
-		
-	auth_pkt_desc->curr_index = nlm_crypto_fill_src_dst_seg(pkt_param, index , auth_pkt_desc->max_frags, data_index, length);
+
+	auth_pkt_desc->total_len += areq->nbytes;
+	auth_pkt_desc->index = index;
+	auth_pkt_desc->max_frags = max_frags;
 
 	return 0;
 }
-static int
-crypto_get_sync_fb_vc(int * node)
+static void auth_request_callback(struct nlm_async_crypto *async, uint64_t msg1)
 {
-    int cpu;
-    int node_id;
-    extern int ipsec_sync_vc;
+	struct crypto_async_request *base = (struct crypto_async_request *)async->args;
+	int err = 0;
+	int cpu = hard_smp_processor_id();
+	int stat = async->stat & 0xff;
 
-    cpu = hard_smp_processor_id();      //processor_id();
-    node_id = (cpu >> NODE_ID_SHIFT_BIT);
-    cpu = (node_id << NODE_BASE_SHIFT_BIT) | (((cpu & 0x1f) * 4) + ipsec_sync_vc);
-    *node = node_id;
+	if (msg1 & 0x7ff80) {
+		printk("\n Error: entry1 is %llx",msg1);
+		err = -EIO;
+		base->complete(base, err);
+		return ;
+	}
+	crypto_stat[cpu].auth[stat] ++;
+	crypto_stat[cpu].auth_tbytes[stat] += async->bytes;
 
-    return cpu;
+	if ( async->pkt_param)
+		kfree(async->pkt_param);
+	base->complete(base, err);
+	return;
 }
 
+
+
+
+
 static int
-xlp_auth_final(struct shash_desc *desc, uint8_t *out)
+xlp_auth_final(struct ahash_request *areq)
 {
-	struct auth_pkt_desc *auth_pkt_desc = (struct auth_pkt_desc *)shash_desc_ctx(desc);
-	int index = auth_pkt_desc->curr_index;
+	struct auth_pkt_desc *auth_pkt_desc = pkt_desc_req_ctx(areq); 
+	struct nlm_auth_ctx  * auth_ctx   = ctrl_desc_req_ahash_ctx(areq);	
 	struct nlm_crypto_pkt_param  *pkt_param  = auth_pkt_desc->pkt_param;
-	struct nlm_auth_ctx  * auth_ctx   = pkt_ctrl_auth_ctx(desc);
 	int fb_vc ;
+	int index = auth_pkt_desc->index ;
 	uint64_t entry0, entry1, tx_id=0x12345678ULL;
-	uint64_t  timeout = 0;
 	struct nlm_crypto_pkt_ctrl *ctrl = &auth_ctx->ctrl;
-	uint32_t size,code,src;
-	uint16_t stat = auth_ctx->stat;
-	int cpu = hard_smp_processor_id();
-        extern int ipsec_sync_vc;
 #ifdef CONFIG_32BIT
 	unsigned long msgrng_flags;
 #endif
 	int node_sae_base;
 	int node;
+	int err;
+	struct nlm_async_crypto * async =  (struct nlm_async_crypto *)NLM_ASYNC_PTR_PARAM_OFFSET((unsigned long)auth_pkt_desc);;
 
-	if ( (auth_pkt_desc->total_len == 0 ) ||  ( auth_pkt_desc->total_len > MAX_AUTH_DATA)) { 
+	if ( auth_pkt_desc->total_len == 0 )  { 
 
-		if (auth_pkt_desc->fallback == NULL ) {
-			auth_pkt_desc->fallback = kmalloc((sizeof(struct shash_desc ) + 
-				crypto_shash_descsize(auth_ctx->fallback_tfm))
-									,GFP_KERNEL);
-			auth_pkt_desc->fallback->tfm = auth_ctx->fallback_tfm;
-			auth_pkt_desc->fallback->flags = desc->flags & CRYPTO_TFM_REQ_MAY_SLEEP;
-			crypto_shash_init(auth_pkt_desc->fallback);
-		}
-		crypto_shash_final(auth_pkt_desc->fallback,out);
+
+		auth_pkt_desc->fallback_req = ahash_request_alloc(auth_ctx->fallback_tfm,GFP_KERNEL);
+		auth_pkt_desc->fallback_req->nbytes = areq->nbytes;
+		auth_pkt_desc->fallback_req->src = areq->src;
+		auth_pkt_desc->fallback_req->result = areq->result;
+		auth_pkt_desc->fallback_req->base.flags =  areq->base.flags;
+		
+		crypto_ahash_init(auth_pkt_desc->fallback_req);
+		crypto_ahash_final(auth_pkt_desc->fallback_req);
+		kfree(auth_pkt_desc->fallback_req);
+		auth_pkt_desc->fallback_req = NULL;
 		return 0;
 	}
-	if ( auth_pkt_desc->fallback != NULL ) {
-		kfree(auth_pkt_desc->fallback);
-		auth_pkt_desc->fallback = NULL;
-	}
 		
 	nlm_crypto_fill_auth_pkt_param(ctrl,pkt_param,
-			0,auth_pkt_desc->total_len,0,out); 
+			0,auth_pkt_desc->total_len,0,areq->result); 
 
-	preempt_disable();
-#ifdef CONFIG_32BIT
-	msgrng_access_enable(msgrng_flags);
-#endif
 
-	fb_vc = crypto_get_sync_fb_vc(&node);
+	fb_vc = crypto_get_fb_vc(&node);
 	entry0 = nlm_crypto_form_pkt_fmn_entry0(fb_vc, 0, 0, 0, virt_to_phys(ctrl));
 	entry1 = nlm_crypto_form_pkt_fmn_entry1(0, ctrl->hashkeylen, (32 + index * 16 ), virt_to_phys(pkt_param));
-	node_sae_base = (node << NODE_BASE_SHIFT_BIT) | crypto_vc_base;
 
-	while (nlm_hal_send_msg3(node_sae_base, 0 /*code */ , entry0, entry1, tx_id) != 0 );
+
+	async->callback =  auth_request_callback;
+	async->args = (void *)&areq->base; 
+	async->hash_addr = areq->result;
+	async->stat = auth_ctx->stat; 
+	async->bytes = auth_pkt_desc->total_len + ctrl->taglen; 
+	tx_id = (uint64_t)(unsigned long)async;
+
 
 #ifdef NLM_CRYPTO_DEBUG
 	print_crypto_msg_desc(entry0, entry1, tx_id);
 	print_cntl_instr(ctrl->desc0);
+	hex_dump("key",ctrl->key,16);
 	print_pkt_desc(pkt_param,index);
 #endif
 
-	//construct pkt, send to engine and receive reply
-	timeout = 0;
-	do {
-		timeout++;
-		nlm_hal_recv_msg2(ipsec_sync_vc, &src, &size, &code, &entry0, &entry1);
-	} while(entry0 != tx_id && timeout < 0xffffffffULL) ;
-	
 
-	if (timeout >= 0xffffffffULL) {
-		printk("\nError: FreeBack message is not received");
 #ifdef CONFIG_32BIT
-		msgrng_access_disable(msgrng_flags);
-#endif
-		if ( auth_pkt_desc->is_allocated )
-			kfree(auth_pkt_desc->pkt_param);
-		preempt_enable();
-		return -EAGAIN;
-	}
-
-#ifdef NLM_CRYPTO_DEBUG
-	print_buf("AUTH:", out, 16);
+	msgrng_access_enable(msgrng_flags);
 #endif
+	node_sae_base = (node << NODE_BASE_SHIFT_BIT) | crypto_vc_base;
+	err = nlm_hal_send_msg3(node_sae_base, 0 /*code */ , entry0, entry1, tx_id);
 #ifdef CONFIG_32BIT
 	msgrng_access_disable(msgrng_flags);
 #endif
-	preempt_enable();
-	if ( auth_pkt_desc->is_allocated )
-		kfree(auth_pkt_desc->pkt_param);
-	crypto_stat[cpu].auth[stat] ++;
-	crypto_stat[cpu].auth_tbytes[stat] += auth_pkt_desc->total_len + ctrl->taglen;
- 	return 0;
-}
-static int xlp_auth_export(struct shash_desc *desc, void *out)
-{
-	struct auth_pkt_desc *auth_pkt_desc = (struct auth_pkt_desc *)shash_desc_ctx(desc);
-	if ( auth_pkt_desc->fallback == NULL )
-		return 0;
-	return crypto_shash_export(auth_pkt_desc->fallback,out);
-}
-
-static int xlp_auth_import(struct shash_desc *desc, const void *in)
-{
-	struct auth_pkt_desc *auth_pkt_desc = (struct auth_pkt_desc *)shash_desc_ctx(desc);
-	if ( auth_pkt_desc->fallback == NULL )
-		return 0;
-	return crypto_shash_import(auth_pkt_desc->fallback, in);
-}
+	if(err){
+		printk("error\n");
+		return -EAGAIN;
+	}
 
+ 	return -EINPROGRESS;
 
+}
 /*
    All Setkey goes here.
  */
 
+static int
+crypto_get_sync_fb_vc(int * node)
+{
+    int cpu;
+    int node_id;
+    extern int ipsec_sync_vc;
+
+    cpu = hard_smp_processor_id();      //processor_id();
+    node_id = (cpu >> NODE_ID_SHIFT_BIT);
+    cpu = (node_id << NODE_BASE_SHIFT_BIT) | (((cpu & 0x1f) * 4) + ipsec_sync_vc);
+    *node = node_id;
+
+    return cpu;
+}
 int hash_key(int alg, int mode, const uint8_t * key, unsigned int keylen, uint8_t * new_key)
 {
 
@@ -383,7 +350,7 @@ int hash_key(int alg, int mode, const uint8_t * key, unsigned int keylen, uint8_
 			sizeof(struct nlm_crypto_pkt_param ) + 63)  & ~(0x3fUL));
 
 	memcpy(tmp_key,key,keylen);
-        nlm_crypto_fill_pkt_ctrl(ctrl,0,alg,mode,0,0,0,NULL,0,NULL,0,1);
+        nlm_crypto_fill_pkt_ctrl(ctrl,0,alg,mode,0,0,0,NULL,0,NULL,0,0);
         nlm_crypto_fill_auth_pkt_param(ctrl,pkt_param,0,keylen,0,new_key);
         nlm_crypto_fill_src_dst_seg(pkt_param,0,MAX_FRAGS,tmp_key,keylen);
 
@@ -421,9 +388,9 @@ int hash_key(int alg, int mode, const uint8_t * key, unsigned int keylen, uint8_
 
 }
 
-static int xlp_auth_aes_xcbc_setkey(struct crypto_shash *tfm, const u8 * key, unsigned int keylen)
+static int xlp_auth_aes_xcbc_setkey(struct crypto_ahash *tfm, const u8 * key, unsigned int keylen)
 {
-	struct nlm_auth_ctx * nlm_ctx = nlm_shash_auth_ctx(tfm);
+	struct nlm_auth_ctx * nlm_ctx = ctrl_desc_crypto_ahash_ctx(tfm);
 	uint32_t hash_alg = NLM_HASH_AES128;
 
         switch (keylen) {
@@ -446,9 +413,9 @@ static int xlp_auth_aes_xcbc_setkey(struct crypto_shash *tfm, const u8 * key, un
 
 	/*setup ctrl descriptor*/
 	nlm_crypto_fill_pkt_ctrl(&nlm_ctx->ctrl,0,hash_alg,NLM_HASH_MODE_XCBC,
-		NLM_CIPHER_BYPASS,NLM_CIPHER_MODE_ECB,0,NULL,0,(unsigned char *)key,keylen,1);
+		NLM_CIPHER_BYPASS,NLM_CIPHER_MODE_ECB,0,NULL,0,(unsigned char *)key,keylen,0);
 	
-	crypto_shash_setkey(nlm_ctx->fallback_tfm,key,keylen);
+	crypto_ahash_setkey(nlm_ctx->fallback_tfm,key,keylen);
 
 	
 	return 0;
@@ -457,60 +424,60 @@ static int xlp_auth_aes_xcbc_setkey(struct crypto_shash *tfm, const u8 * key, un
 
 
 static int
-xlp_auth_hmac_sha256_setkey(struct crypto_shash *tfm, const u8 * key, unsigned int keylen)
+xlp_auth_hmac_sha256_setkey(struct crypto_ahash *tfm, const u8 * key, unsigned int keylen)
 {
-	struct nlm_auth_ctx * nlm_ctx = nlm_shash_auth_ctx(tfm);
+	struct nlm_auth_ctx * nlm_ctx = ctrl_desc_crypto_ahash_ctx(tfm);
 	struct nlm_crypto_pkt_ctrl * ctrl = &nlm_ctx->ctrl; 
 	
 	nlm_ctx->stat = H_SHA256_STAT;
 	if ( keylen > 64 ) {
 		hash_key(NLM_HASH_SHA,NLM_HASH_MODE_SHA256,key,keylen,&nlm_ctx->hashed_key[0]);
 		nlm_crypto_fill_pkt_ctrl(ctrl,1,NLM_HASH_SHA,NLM_HASH_MODE_SHA256,
-			NLM_CIPHER_BYPASS,NLM_CIPHER_MODE_ECB,0,NULL,0,(unsigned char*)nlm_ctx->hashed_key,64,1);
-		crypto_shash_setkey(nlm_ctx->fallback_tfm,nlm_ctx->hashed_key,64);
+			NLM_CIPHER_BYPASS,NLM_CIPHER_MODE_ECB,0,NULL,0,(unsigned char*)nlm_ctx->hashed_key,64,0);
+		crypto_ahash_setkey(nlm_ctx->fallback_tfm,nlm_ctx->hashed_key,64);
 		return 0;
 	}
 
 	/*setup ctrl descriptor*/
 	nlm_crypto_fill_pkt_ctrl(ctrl,1,NLM_HASH_SHA,NLM_HASH_MODE_SHA256,
-		NLM_CIPHER_BYPASS,NLM_CIPHER_MODE_ECB,0,NULL,0,(unsigned char*)key,keylen,1);
+		NLM_CIPHER_BYPASS,NLM_CIPHER_MODE_ECB,0,NULL,0,(unsigned char*)key,keylen,0);
 	if ( ctrl->hashkeylen < auth_mode_key_len[NLM_HASH_SHA][NLM_HASH_MODE_SHA256]) 
 		ctrl->hashkeylen = auth_mode_key_len[NLM_HASH_SHA][NLM_HASH_MODE_SHA256];
-	crypto_shash_setkey(nlm_ctx->fallback_tfm,key,keylen);
+	crypto_ahash_setkey(nlm_ctx->fallback_tfm,key,keylen);
 	return 0;
 	
 }
 
 
 static int
-xlp_auth_hmac_md5_setkey(struct crypto_shash *tfm, const u8 * key, unsigned int keylen)
+xlp_auth_hmac_md5_setkey(struct crypto_ahash *tfm, const u8 * key, unsigned int keylen)
 {
-	struct nlm_auth_ctx * nlm_ctx = nlm_shash_auth_ctx(tfm);
+	struct nlm_auth_ctx * nlm_ctx = ctrl_desc_crypto_ahash_ctx(tfm);
 	struct nlm_crypto_pkt_ctrl * ctrl = &nlm_ctx->ctrl; 
 
 	nlm_ctx->stat = MD5_STAT;
 	if ( keylen > 64 ) {
 		hash_key(NLM_HASH_MD5,NLM_HASH_MODE_SHA1,key,keylen,&nlm_ctx->hashed_key[0]);
 		nlm_crypto_fill_pkt_ctrl(ctrl,1,NLM_HASH_MD5,NLM_HASH_MODE_SHA1,
-			NLM_CIPHER_BYPASS,NLM_CIPHER_MODE_ECB,0,NULL,0,(unsigned char *)nlm_ctx->hashed_key,64,1);
-		crypto_shash_setkey(nlm_ctx->fallback_tfm,nlm_ctx->hashed_key,64);
+			NLM_CIPHER_BYPASS,NLM_CIPHER_MODE_ECB,0,NULL,0,(unsigned char *)nlm_ctx->hashed_key,64,0);
+		crypto_ahash_setkey(nlm_ctx->fallback_tfm,nlm_ctx->hashed_key,64);
 		return 0;
 	}
 
 	/*setup ctrl descriptor*/
 	nlm_crypto_fill_pkt_ctrl(ctrl,1,NLM_HASH_MD5,NLM_HASH_MODE_SHA1,
-		NLM_CIPHER_BYPASS,NLM_CIPHER_MODE_ECB,0,NULL,0,(unsigned char *)key,keylen,1);
+		NLM_CIPHER_BYPASS,NLM_CIPHER_MODE_ECB,0,NULL,0,(unsigned char *)key,keylen,0);
 	if ( ctrl->hashkeylen < auth_mode_key_len[NLM_HASH_MD5][NLM_HASH_MODE_SHA1]) 
 		ctrl->hashkeylen = auth_mode_key_len[NLM_HASH_MD5][NLM_HASH_MODE_SHA1];
-	crypto_shash_setkey(nlm_ctx->fallback_tfm,key,keylen);
+	crypto_ahash_setkey(nlm_ctx->fallback_tfm,key,keylen);
 	return 0;
 	
 }
 
 static int
-xlp_auth_hmac_sha1_setkey(struct crypto_shash *tfm, const u8 * key, unsigned int keylen)
+xlp_auth_hmac_sha1_setkey(struct crypto_ahash *tfm, const u8 * key, unsigned int keylen)
 {
-	struct nlm_auth_ctx * nlm_ctx = nlm_shash_auth_ctx(tfm);
+	struct nlm_auth_ctx * nlm_ctx = ctrl_desc_crypto_ahash_ctx(tfm);
 	struct nlm_crypto_pkt_ctrl * ctrl = &nlm_ctx->ctrl; 
 
 	nlm_ctx->stat = H_SHA1_STAT;
@@ -518,103 +485,200 @@ xlp_auth_hmac_sha1_setkey(struct crypto_shash *tfm, const u8 * key, unsigned int
 
 		hash_key(NLM_HASH_SHA,NLM_HASH_MODE_SHA1,key,keylen,&nlm_ctx->hashed_key[0]);
 		nlm_crypto_fill_pkt_ctrl(ctrl,1,NLM_HASH_SHA,NLM_HASH_MODE_SHA1,
-			NLM_CIPHER_BYPASS,NLM_CIPHER_MODE_ECB,0,NULL,0,(unsigned char *)nlm_ctx->hashed_key,64,1);
-		crypto_shash_setkey(nlm_ctx->fallback_tfm,nlm_ctx->hashed_key,64);
+			NLM_CIPHER_BYPASS,NLM_CIPHER_MODE_ECB,0,NULL,0,(unsigned char *)nlm_ctx->hashed_key,64,0);
+		crypto_ahash_setkey(nlm_ctx->fallback_tfm,nlm_ctx->hashed_key,64);
 		return 0;
 	}
 
 	/*setup ctrl descriptor*/
 	nlm_crypto_fill_pkt_ctrl(ctrl,1,NLM_HASH_SHA,NLM_HASH_MODE_SHA1,
-		NLM_CIPHER_BYPASS,NLM_CIPHER_MODE_ECB,0,NULL,0,(unsigned char *)key,keylen,1);
+		NLM_CIPHER_BYPASS,NLM_CIPHER_MODE_ECB,0,NULL,0,(unsigned char *)key,keylen,0);
 	if ( ctrl->hashkeylen < auth_mode_key_len[NLM_HASH_SHA][NLM_HASH_MODE_SHA1]) 
 		ctrl->hashkeylen = auth_mode_key_len[NLM_HASH_SHA][NLM_HASH_MODE_SHA1];
-	crypto_shash_setkey(nlm_ctx->fallback_tfm,key,keylen);
+	crypto_ahash_setkey(nlm_ctx->fallback_tfm,key,keylen);
 	return 0;
 	 
 	
 }
+static int xlp_auth_digest(struct ahash_request *areq)
+{
+	unsigned int auth_len;
+	int fb_vc ;
+	int node_sae_base;
+	int node;
+	int err;
+	uint64_t entry0, entry1;
+	int max_frags = MAX_FRAGS;
+	int index = 0;
+	uint64_t tx_id=0x12345678ULL;
+	struct nlm_crypto_pkt_param  *pkt_param  = NULL;
+	struct auth_pkt_desc *auth_pkt_desc = pkt_desc_req_ctx(areq);
+	struct nlm_auth_ctx  * auth_ctx   = ctrl_desc_req_ahash_ctx(areq);	
+	struct nlm_crypto_pkt_ctrl *ctrl = &auth_ctx->ctrl;
+	struct nlm_async_crypto * async =  (struct nlm_async_crypto *)NLM_ASYNC_PTR_PARAM_OFFSET((unsigned long)auth_pkt_desc);;
 
-static struct shash_alg xcbc_mac_alg = {
-	.digestsize = XCBC_DIGEST_SIZE,
+	auth_pkt_desc->max_frags = MAX_FRAGS;
+
+	if ( areq->nbytes == 0 )  { 
+
+		auth_pkt_desc->fallback_req = ahash_request_alloc(auth_ctx->fallback_tfm,GFP_KERNEL);
+		auth_pkt_desc->fallback_req->nbytes = areq->nbytes;
+		auth_pkt_desc->fallback_req->src = areq->src;
+		auth_pkt_desc->fallback_req->result = areq->result;
+		auth_pkt_desc->fallback_req->base.flags =  areq->base.flags;
+		
+		crypto_ahash_init(auth_pkt_desc->fallback_req);
+		crypto_ahash_final(auth_pkt_desc->fallback_req);
+		kfree(auth_pkt_desc->fallback_req);
+		auth_pkt_desc->fallback_req = NULL;
+		return 0;
+	}
+
+	async->stat = 0;
+	auth_pkt_desc->pkt_param = ( struct nlm_crypto_pkt_param  * )NLM_CRYPTO_PKT_PARAM_OFFSET((unsigned long )auth_pkt_desc + sizeof(struct auth_pkt_desc ));
+	pkt_param  = auth_pkt_desc->pkt_param;
+	async->pkt_param = NULL;
+
+	do {
+		auth_len = areq->nbytes;
+		index = fill_src_dst_sg(areq->src,areq->src,auth_len,pkt_param,index,max_frags,0);
+
+		if ( index > max_frags ) {
+			max_frags = index;
+			index = alloc_pkt_param(async,&pkt_param,max_frags);
+			index = 0;
+		}
+	}while(index == 0 );
+
+	auth_pkt_desc->max_frags = max_frags;
+
+#ifdef CONFIG_32BIT
+	unsigned long msgrng_flags;
+#endif
+		
+	nlm_crypto_fill_auth_pkt_param(ctrl,pkt_param,
+			0,areq->nbytes,0,areq->result); 
+
+
+	fb_vc = crypto_get_fb_vc(&node);
+	entry0 = nlm_crypto_form_pkt_fmn_entry0(fb_vc, 0, 0, 0, virt_to_phys(ctrl));
+	entry1 = nlm_crypto_form_pkt_fmn_entry1(0, ctrl->hashkeylen, (32 + index * 16 ), virt_to_phys(pkt_param));
+
+
+	async->callback =  auth_request_callback;
+	async->args = (void *)&areq->base; 
+	async->hash_addr = areq->result;
+	async->stat = auth_ctx->stat; 
+	async->bytes = areq->nbytes + ctrl->taglen; 
+	tx_id = (uint64_t)(unsigned long)async;
+
+
+#ifdef NLM_CRYPTO_DEBUG
+	print_crypto_msg_desc(entry0, entry1, tx_id);
+	print_cntl_instr(ctrl->desc0);
+	hex_dump("key",ctrl->key,16);
+	print_pkt_desc(pkt_param,index);
+#endif
+
+
+#ifdef CONFIG_32BIT
+	msgrng_access_enable(msgrng_flags);
+#endif
+	node_sae_base = (node << NODE_BASE_SHIFT_BIT) | crypto_vc_base;
+	err = nlm_hal_send_msg3(node_sae_base, 0 /*code */ , entry0, entry1, tx_id);
+#ifdef CONFIG_32BIT
+	msgrng_access_disable(msgrng_flags);
+#endif
+	if(err){
+		printk("failed\n");
+		return -EAGAIN;
+	}
+
+ 	return -EINPROGRESS;
+}
+
+
+static struct ahash_alg xcbc_mac_alg = {
 	.init = xlp_auth_init,
 	.update = xlp_auth_update,
-	.export = xlp_auth_export,
-	.import = xlp_auth_import,
 	.final = xlp_auth_final,
 	.setkey = xlp_auth_aes_xcbc_setkey,
-	.descsize = PACKET_DESC_SIZE,
-	.base = {
+	.digest = xlp_auth_digest,
+	.halg = {
+		.digestsize = XCBC_DIGEST_SIZE, 
+		.base = {
 		 .cra_name = "xcbc(aes)",
 		 .cra_driver_name = "xcbc-aes-xlp",
 		 .cra_priority = XLP_HMAC_PRIORITY,
-		 .cra_flags = CRYPTO_ALG_TYPE_SHASH,
+		 .cra_flags = CRYPTO_ALG_TYPE_AHASH,
 		 .cra_blocksize = AES_BLOCK_SIZE,
 		 .cra_module = THIS_MODULE,
 		 .cra_ctxsize = CTRL_DESC_SIZE, 
 		 .cra_init = xlp_cra_xcbc_init,
 		 }
+	}
 };
 
-static struct shash_alg sha256_hmac_alg = {
-	.digestsize = SHA256_DIGEST_SIZE,
+static struct ahash_alg sha256_hmac_alg = {
 	.init = xlp_auth_init,
 	.update = xlp_auth_update,
-	.export = xlp_auth_export,
-	.import = xlp_auth_import,
 	.final = xlp_auth_final,
 	.setkey = xlp_auth_hmac_sha256_setkey,
-	.descsize = PACKET_DESC_SIZE,
+	.digest = xlp_auth_digest,
+	.halg = {
+	.digestsize = SHA256_DIGEST_SIZE,
 	.base = {
 		 .cra_name = "hmac(sha256)",
 		 .cra_driver_name = "hmac-sha256-xlp",
 		 .cra_priority = XLP_HMAC_PRIORITY,
-		 .cra_flags = CRYPTO_ALG_TYPE_SHASH,
+		 .cra_flags = CRYPTO_ALG_TYPE_AHASH,
 		 .cra_blocksize = SHA256_BLOCK_SIZE,
 		 .cra_module = THIS_MODULE,
 		 .cra_ctxsize = CTRL_DESC_SIZE, 
 		 .cra_init = xlp_cra_hmac_sha256_init,
 		 }
+	}
 };
 
-static struct shash_alg md5_hmac_alg = {
-	.digestsize = MD5_DIGEST_SIZE,
+static struct ahash_alg md5_hmac_alg = {
 	.init = xlp_auth_init,
 	.update = xlp_auth_update,
 	.final = xlp_auth_final,
-	.export = xlp_auth_export,
-	.import = xlp_auth_import,
 	.setkey = xlp_auth_hmac_md5_setkey,
-	.descsize = PACKET_DESC_SIZE,
+	.digest = xlp_auth_digest,
+	.halg = {
+	.digestsize = MD5_DIGEST_SIZE,
 	.base = {
 		 .cra_name = "hmac(md5)",
 		 .cra_driver_name = "hmac-md5-xlp",
 		 .cra_priority = XLP_HMAC_PRIORITY,
-		 .cra_flags = CRYPTO_ALG_TYPE_SHASH,
+		 .cra_flags = CRYPTO_ALG_TYPE_AHASH,
 		 .cra_blocksize = MD5_BLOCK_SIZE,
 		 .cra_module = THIS_MODULE,
 		 .cra_ctxsize = CTRL_DESC_SIZE, 
 		 .cra_init = xlp_cra_md5_init,
 		 }
+	}
 };
-static struct shash_alg sha1_hmac_alg = {
-	.digestsize = SHA1_DIGEST_SIZE,
+static struct ahash_alg sha1_hmac_alg = {
 	.init = xlp_auth_init,
 	.update = xlp_auth_update,
 	.final = xlp_auth_final,
-	.export = xlp_auth_export,
-	.import = xlp_auth_import,
 	.setkey = xlp_auth_hmac_sha1_setkey,
-	.descsize = PACKET_DESC_SIZE,
+	.digest = xlp_auth_digest,
+	.halg = {
+	.digestsize = SHA1_DIGEST_SIZE,
 	.base = {
 		 .cra_name = "hmac(sha1)",
 		 .cra_driver_name = "hmac-sha1-xlp",
 		 .cra_priority = XLP_HMAC_PRIORITY,
-		 .cra_flags = CRYPTO_ALG_TYPE_SHASH,
+		 .cra_flags = CRYPTO_ALG_TYPE_AHASH,
 		 .cra_blocksize = SHA1_BLOCK_SIZE,
 		 .cra_module = THIS_MODULE,
 		 .cra_ctxsize = CTRL_DESC_SIZE, 
 		 .cra_init = xlp_cra_hmac_sha1_init,
 		 }
+	}
 };
 
 int
@@ -623,19 +687,19 @@ xlp_auth_alg_init(void)
 	int rc = -ENODEV;
 	int no_of_alg_registered = 0;
 
-	rc = crypto_register_shash(&sha1_hmac_alg);
+	rc = crypto_register_ahash(&sha1_hmac_alg);
 	if (rc)
 		goto out;
 	no_of_alg_registered++;
-	rc = crypto_register_shash(&sha256_hmac_alg);
+	rc = crypto_register_ahash(&sha256_hmac_alg);
 	if (rc)
 		goto out;
 	no_of_alg_registered++;
-	rc = crypto_register_shash(&md5_hmac_alg);
+	rc = crypto_register_ahash(&md5_hmac_alg);
 	if (rc)
 		goto out;
 	no_of_alg_registered++;
-	rc = crypto_register_shash(&xcbc_mac_alg);
+	rc = crypto_register_ahash(&xcbc_mac_alg);
 	if (rc)
 		goto out;
 	no_of_alg_registered++; 
@@ -651,10 +715,10 @@ out:
 void
 xlp_auth_alg_fini(void)
 {
-	crypto_unregister_shash(&xcbc_mac_alg);
-	crypto_unregister_shash(&md5_hmac_alg);
-	crypto_unregister_shash(&sha256_hmac_alg);
-	crypto_unregister_shash(&sha1_hmac_alg);
+	crypto_unregister_ahash(&xcbc_mac_alg);
+	crypto_unregister_ahash(&md5_hmac_alg);
+	crypto_unregister_ahash(&sha256_hmac_alg);
+	crypto_unregister_ahash(&sha1_hmac_alg);
 }
 
 EXPORT_SYMBOL(xlp_auth_alg_init);
diff --git a/drivers/crypto/sae/nlm_crypto.c b/drivers/crypto/sae/nlm_crypto.c
index 0b6b149..d56ef8f 100644
--- a/drivers/crypto/sae/nlm_crypto.c
+++ b/drivers/crypto/sae/nlm_crypto.c
@@ -92,23 +92,6 @@ int cipher_mode_iv_len[NLM_CIPHER_MAX][NLM_CIPHER_MODE_MAX] = {
 /* CAMELLIA256 */  {   0,    16,   16,   16,   16,   16      -1,   0,   0,  0,  16,   16,},
 };
 
-int auth_mode_key_len[NLM_HASH_MAX][NLM_HASH_MODE_MAX] = {
-/*	               SHA1 SHA224 SHA256 SHA384 SHA512  CMAC  XCBC CBC_MAC CCM  GCM*/
-/* BYPASS */		{0,    0,     0,     0,     0,     0,   0,    0,     0,    0, },
-/* MD5 */		{64,   64,    64,   64,    64,    64,  64,   64,    64,   64, },
-/* SHA */		{64,   64,    64,   128,   128,    0,   0,    0,     0,    0, },
-/* 3 */			{0,    0,     0,     0,     0,     0,   0,    0,     0,    0, },
-/* AES128 */		{0,    0,     0,     0,     0,    16,  16,   16,    16,   16, },
-/* AES192 */		{0,    0,     0,     0,     0,    24,  24,   24,    24,   24, },
-/* AES256 */		{0,    0,     0,     0,     0,    32,  32,   32,    32,   32, },
-/* KASUMI_F9 */		{16,  16,    16,    16,    16,    16,  16,   16,    16,   16, },
-/* SNOW3G_F9 */		{16,  16,    16,    16,    16,    16,  16,   16,    16,   16, }, //sandip -> verify
-/* CAMELLIA128 */	{0,    0,     0,     0,     0,    16,  16,   16,    16,   16, },
-/* CAMELLIA192 */	{0,    0,     0,     0,     0,    24,  24,   24,    24,   24, },
-/* CAMELLIA256 */	{0,    0,     0,     0,     0,    32,  32,   32,    32,   32, },
-/* GHASH */		{0,    0,     0,     0,     0,    32,  32,   32,    32,   32, }, //todo:
-};
-
 #define NLM_CRYPTO_MAX_STR_LEN 200
 #ifdef NLM_CRYPTO_DEBUG
 static char str_cipher_alg[NLM_CIPHER_MAX + 1][NLM_CRYPTO_MAX_STR_LEN] = {
-- 
1.7.10.4

