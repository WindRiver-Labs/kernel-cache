From 69171bbb8e762f33f964934eb42e653ee1ec8f30 Mon Sep 17 00:00:00 2001
From: Yanjiang Jin <yanjiang.jin@windriver.com>
Date: Thu, 5 Dec 2013 15:54:35 +0800
Subject: [PATCH 2/3] bcm-xlp: implement flush_cache_page

XLP processors doesn't implement I-cache coherency in hardware so that the
I-cache has to be invalidated in all cases of writes to executable pages.
The old implementation of the function does nothing, the pointer redirects
to nlm_common_noflush() that has an empty body. Therefore when ptrace(2) is
called to poke at an instruction, the old contents of the I-cache are not
invalidated and consequently out of sync with the D-cache.
If an instruction is then executed at the address previously poked at,
then the stale contents of the I-cache are used(here causing a BREAK
instruction to reexecute) rather than new contents just stored in the
D-cache/RAM(unless the I-cache line has been already replaced).

Now import flush_cache_page() from c-r4k.c to c-phoenix.c to make all caches
and RAM consistent.

Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 arch/mips/mm/c-phoenix.c |  194 +++++++++++++++++++++++++++++++++++++++++++--
 1 files changed, 185 insertions(+), 9 deletions(-)

diff --git a/arch/mips/mm/c-phoenix.c b/arch/mips/mm/c-phoenix.c
index c81b40c..cce44a8 100644
--- a/arch/mips/mm/c-phoenix.c
+++ b/arch/mips/mm/c-phoenix.c
@@ -32,10 +32,13 @@
 #include <asm/cacheops.h>
 #include <asm/cpu.h>
 #include <asm/uaccess.h>
+#include <asm/cacheflush.h>
+#include <asm/r4kcache.h>
 #include <linux/smp.h>
 #include <linux/kallsyms.h>
 #include <linux/mm.h>
 #include <linux/module.h>
+#include <linux/highmem.h>
 
 #ifdef CONFIG_NLM_XLP
 #include <asm/mach-netlogic/xlp-mmu.h>
@@ -60,6 +63,179 @@ static unsigned int icache_lines;
 		       : : "i" (op), "r" (base));          \
   } while (0)
 
+/*
+ * Must die.
+ */
+static unsigned long icache_size __read_mostly;
+static unsigned long dcache_size __read_mostly;
+static unsigned long scache_size __read_mostly;
+
+/*
+ * Dummy cache handling routines for machines without boardcaches
+ */
+static void cache_noop(void) {}
+
+static void (*r4k_blast_icache_page)(unsigned long addr);
+
+static void __cpuinit r4k_blast_icache_page_setup(void)
+{
+	unsigned long ic_lsize = cpu_icache_line_size();
+
+	if (ic_lsize == 0)
+		r4k_blast_icache_page = (void *)cache_noop;
+	else if (ic_lsize == 16)
+		r4k_blast_icache_page = blast_icache16_page;
+	else if (ic_lsize == 32)
+		r4k_blast_icache_page = blast_icache32_page;
+	else if (ic_lsize == 64)
+		r4k_blast_icache_page = blast_icache64_page;
+}
+
+static void (*r4k_blast_scache_page)(unsigned long addr);
+
+static void __cpuinit r4k_blast_scache_page_setup(void)
+{
+	unsigned long sc_lsize = cpu_scache_line_size();
+
+	if (scache_size == 0)
+		r4k_blast_scache_page = (void *)cache_noop;
+	else if (sc_lsize == 16)
+		r4k_blast_scache_page = blast_scache16_page;
+	else if (sc_lsize == 32)
+		r4k_blast_scache_page = blast_scache32_page;
+	else if (sc_lsize == 64)
+		r4k_blast_scache_page = blast_scache64_page;
+	else if (sc_lsize == 128)
+		r4k_blast_scache_page = blast_scache128_page;
+}
+
+/*
+ * Special Variant of smp_call_function for use by cache functions:
+ *
+ *  o No return value
+ *  o collapses to normal function call on UP kernels
+ *  o collapses to normal function call on systems with a single shared
+ *    primary cache.
+ *  o doesn't disable interrupts on the local CPU
+ */
+static inline void r4k_on_each_cpu(void (*func) (void *info), void *info)
+{
+	preempt_disable();
+
+#if !defined(CONFIG_MIPS_MT_SMP) && !defined(CONFIG_MIPS_MT_SMTC)
+	smp_call_function(func, info, 1);
+#endif
+	func(info);
+	preempt_enable();
+}
+
+static inline int has_valid_asid(const struct mm_struct *mm)
+{
+#if defined(CONFIG_MIPS_MT_SMP) || defined(CONFIG_MIPS_MT_SMTC)
+	int i;
+
+	for_each_online_cpu(i)
+		if (cpu_context(i, mm))
+			return 1;
+
+	return 0;
+#else
+	return cpu_context(smp_processor_id(), mm);
+#endif
+}
+
+struct flush_cache_page_args {
+	struct vm_area_struct *vma;
+	unsigned long addr;
+	unsigned long pfn;
+};
+
+static inline void local_r4k_flush_cache_page(void *args)
+{
+	struct flush_cache_page_args *fcp_args = args;
+	struct vm_area_struct *vma = fcp_args->vma;
+	unsigned long addr = fcp_args->addr;
+	struct page *page = pfn_to_page(fcp_args->pfn);
+	int exec = vma->vm_flags & VM_EXEC;
+	struct mm_struct *mm = vma->vm_mm;
+	int map_coherent = 0;
+	pgd_t *pgdp;
+	pud_t *pudp;
+	pmd_t *pmdp;
+	pte_t *ptep;
+	void *vaddr;
+
+	/*
+	 * If ownes no valid ASID yet, cannot possibly have gotten
+	 * this page into the cache.
+	 */
+	if (!has_valid_asid(mm))
+		return;
+
+	addr &= PAGE_MASK;
+	pgdp = pgd_offset(mm, addr);
+	pudp = pud_offset(pgdp, addr);
+	pmdp = pmd_offset(pudp, addr);
+	ptep = pte_offset(pmdp, addr);
+
+	/*
+	 * If the page isn't marked valid, the page cannot possibly be
+	 * in the cache.
+	 */
+	if (!(pte_present(*ptep)))
+		return;
+
+	if ((mm == current->active_mm) && (pte_val(*ptep) & _PAGE_VALID))
+		vaddr = NULL;
+	else {
+		/*
+		 * Use kmap_coherent or kmap_atomic to do flushes for
+		 * another ASID than the current one.
+		 */
+		map_coherent = (cpu_has_dc_aliases &&
+				page_mapped(page) && !Page_dcache_dirty(page));
+		if (map_coherent)
+			vaddr = kmap_coherent(page, addr);
+		else
+			vaddr = kmap_atomic(page);
+		addr = (unsigned long)vaddr;
+	}
+
+	if (cpu_has_dc_aliases || (exec && !cpu_has_ic_fills_f_dc)) {
+		r4k_blast_icache_page(addr);
+		if (exec && !cpu_icache_snoops_remote_store)
+			r4k_blast_scache_page(addr);
+	}
+	if (exec) {
+		if (vaddr && cpu_has_vtag_icache && mm == current->active_mm) {
+			int cpu = smp_processor_id();
+
+			if (cpu_context(cpu, mm) != 0)
+				drop_mmu_context(mm, cpu);
+		} else
+			r4k_blast_icache_page(addr);
+	}
+
+	if (vaddr) {
+		if (map_coherent)
+			kunmap_coherent();
+		else
+			kunmap_atomic(vaddr);
+	}
+}
+
+static void r4k_flush_cache_page(struct vm_area_struct *vma,
+	unsigned long addr, unsigned long pfn)
+{
+	struct flush_cache_page_args args;
+
+	args.vma = vma;
+	args.addr = addr;
+	args.pfn = pfn;
+
+	r4k_on_each_cpu(local_r4k_flush_cache_page, &args);
+}
+
 #ifdef CONFIG_NLM_XLP
 
 static __inline__ void sync_istream(void)
@@ -325,7 +501,6 @@ static __cpuinit void probe_l1_cache(void)
   struct cpuinfo_mips *c = &current_cpu_data;
   unsigned int config1 = read_c0_config1();
   int lsize = 0;
-  int icache_size=0, dcache_size=0;
 
   if ((lsize = ((config1 >> 19) & 7)))
     c->icache.linesz = 2 << lsize;
@@ -353,14 +528,13 @@ static __cpuinit void probe_l1_cache(void)
     c->dcache.linesz;
   c->dcache.waybit = ffs(dcache_size/c->dcache.ways) - 1;
 
-  if (smp_processor_id()==0) {
-    printk("Primary instruction cache %dkB, %d-way, linesize %d bytes.\n",
-	   icache_size >> 10,
-	   c->icache.ways, c->icache.linesz);
+	if (smp_processor_id() == 0) {
+		pr_info("Primary instruction cache %ldkB, %d-way, linesize %d bytes.\n",
+			icache_size >> 10, c->icache.ways, c->icache.linesz);
 
-    printk("Primary data cache %dkB %d-way, linesize %d bytes.\n",
-	   dcache_size >> 10, c->dcache.ways, c->dcache.linesz);
-  }
+		pr_info("Primary data cache %ldkB %d-way, linesize %d bytes.\n",
+			dcache_size >> 10, c->dcache.ways, c->dcache.linesz);
+	}
 
 }
 
@@ -395,6 +569,8 @@ void __cpuinit ld_mmu_xlr(void)
 	/* update cpu_data */
 
 	probe_l1_cache();
+	r4k_blast_icache_page_setup();
+	r4k_blast_scache_page_setup();
 
 	if (smp_processor_id()) {
 
@@ -440,7 +616,7 @@ void __cpuinit ld_mmu_xlr(void)
 	 */
 	flush_cache_mm = (void (*)(struct mm_struct *))nlm_common_noflush;
 	flush_cache_range = (void *) nlm_common_noflush;
-	flush_cache_page = (void *) nlm_common_noflush;
+	flush_cache_page	= r4k_flush_cache_page;
 
 	/* flush_icache_page: flush_dcache_page + update_mmu_cache takes care of this
 	 *
-- 
1.7.5.4

