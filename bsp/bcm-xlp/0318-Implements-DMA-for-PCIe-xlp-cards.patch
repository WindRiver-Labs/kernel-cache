From ba2a60f2e7f9b369f256e9ed8291d371721f7454 Mon Sep 17 00:00:00 2001
From: Om Narasimhan <onarasimhan@netlogicmicro.com>
Date: Thu, 4 Aug 2011 17:08:31 -0700
Subject: [PATCH 318/762] Implements DMA for PCIe xlp cards

This commit implements the following:
1. DMA driver for XLP PCIe cards. Adds configuration options required for
this as well.
2. Ways to interrupt the host from XLP PCIe device
3. Documentation of interrupt subsystem (irq.c and pci-xlp.c and header files)
4. Many warnings during compilation are removed; some more remains.
5. Bug fixes -- Interrupts were concentrated on CPU0, now all CPUs get
interrupts on RC mode.

Based on Broadcom SDK 2.3.

Signed-off-by: Om Narasimhan <onarasimhan@netlogicmicro.com>
Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 arch/mips/include/asm/netlogic/nlm_dma.h |   59 +++
 arch/mips/include/asm/netlogic/xlp_irq.h |    2 +-
 arch/mips/netlogic/Kconfig               |    9 +
 arch/mips/netlogic/xlp/irq.c             |  153 +++++-
 arch/mips/netlogic/xlp/on_chip.c         |   58 +++-
 arch/mips/netlogic/xlp/xlp_hal_pic.c     |    1 -
 arch/mips/pci/pci-xlp.c                  |  287 +++++++++---
 drivers/char/Kconfig                     |   10 +
 drivers/char/Makefile                    |    1 +
 drivers/char/nlm_xlp_dma.c               |  744 ++++++++++++++++++++++++++++++
 drivers/i2c/busses/i2c-xlp.c             |   10 +-
 11 files changed, 1228 insertions(+), 106 deletions(-)
 create mode 100644 arch/mips/include/asm/netlogic/nlm_dma.h
 create mode 100644 drivers/char/nlm_xlp_dma.c

diff --git a/arch/mips/include/asm/netlogic/nlm_dma.h b/arch/mips/include/asm/netlogic/nlm_dma.h
new file mode 100644
index 0000000..afacc42
--- /dev/null
+++ b/arch/mips/include/asm/netlogic/nlm_dma.h
@@ -0,0 +1,59 @@
+#ifndef _NLM_DMA_H
+#define _NLM_DMA_H
+
+#include <linux/types.h>
+#include <linux/spinlock.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/pci.h>
+#include <linux/string.h>
+#include <linux/delay.h>
+#include <linux/sched.h>
+#include <linux/init.h>
+#include <linux/io.h>
+#include <linux/fs.h>
+#include <linux/cdev.h>
+#include <linux/interrupt.h>
+#include <asm/uaccess.h>
+#include <linux/delay.h>
+#include <linux/debugfs.h>
+#include <linux/seq_file.h>
+#include <linux/signal.h>
+#include <linux/poll.h>
+#include <linux/percpu.h>
+
+#ifdef CONFIG_NLM_XLP
+#include <asm/netlogic/iomap.h>
+#include <asm/netlogic/xlp_hal_pic.h>
+#include <hal/nlm_hal.h>
+#include <hal/nlm_hal_macros.h>
+#include <hal/nlm_hal_fmn.h>
+extern u32 xlp_get_power_on_reset_cfg(int);
+#endif
+u64 setup_pcie_shared_memspace(u64 *);
+void raise_host_interrupt(int);
+int xlp_async_request_dma(uint64_t src, uint64_t dest, uint32_t len,
+	void (*func)(void *,uint64_t),void *data, enum dma_data_direction dir);
+extern struct proc_dir_entry *dma_procfs_dir;
+int xlp_init_dma(void);
+
+#define NLM_XLP_PCIE_SHARED_MEMSIZE		(32 * 1024 * 1024)
+#define DEFAULT_NLMXLP_IO_BASE DEFAULT_NETLOGIC_IO_BASE
+
+#define DPRINTK(level,fmt,args...)\
+do{\
+	printk(level "%s()@%s:%d " fmt,__func__, __FILE__, __LINE__, ##args);\
+}while(0)
+
+#define XLP_PCIE0_TX_BUCKET	(256)	// PRM 10.8, FMN message addressing.
+#define XLP_PCIE0_RX_BUCKET	(257)	// PRM 10.8, FMN message addressing.
+#define XLP_PCIE1_TX_BUCKET	(258)	// PRM 10.8, FMN message addressing.
+#define XLP_PCIE1_RX_BUCKET	(259)	// PRM 10.8, FMN message addressing.
+#define XLP_PCIE2_TX_BUCKET	(260)	// PRM 10.8, FMN message addressing.
+#define XLP_PCIE2_RX_BUCKET	(261)	// PRM 10.8, FMN message addressing.
+#define XLP_PCIE3_TX_BUCKET	(262)	// PRM 10.8, FMN message addressing.
+#define XLP_PCIE3_RX_BUCKET	(263)	// PRM 10.8, FMN message addressing.
+
+#define XLP_DMA_MSGSIZE		(3)	// PRM 22.4.1
+#define XLP_P2D_MAX_MSGSIZE	(4)	// PRM 10.2, for more, use P2P
+#endif // _XLP_DMA_H_
diff --git a/arch/mips/include/asm/netlogic/xlp_irq.h b/arch/mips/include/asm/netlogic/xlp_irq.h
index 3c67269..39d6e1a 100644
--- a/arch/mips/include/asm/netlogic/xlp_irq.h
+++ b/arch/mips/include/asm/netlogic/xlp_irq.h
@@ -196,7 +196,7 @@ THE POSSIBILITY OF SUCH DAMAGE.
 
 #define XLP_MSI_ADDR_SIZE	0x00002000
 #define XLP_MSIX_ADDR_SIZE	0x00008000
-
+#define XLP_BDF_BASE(b,d,f)	(0x18000000 + ((b) << 20) + ((d) << 15) + ((f) << 12))
 #define XLP_MAX_SLOTS		4	/* Only 4 slots now */
 #define XLP_PCIE_CTRL_DEVFN(node, ctr)	PCI_DEVFN((node + 1), ctr)
 #ifdef CONFIG_PCI_MSI_XLP
diff --git a/arch/mips/netlogic/Kconfig b/arch/mips/netlogic/Kconfig
index 9881de1..ef4aa44 100644
--- a/arch/mips/netlogic/Kconfig
+++ b/arch/mips/netlogic/Kconfig
@@ -221,6 +221,15 @@ config NLM_ENABLE_COP2
 	help
 	  This option enables cop2 access for both user and kernel space.
 
+config NLM_XLP_DEVMODE
+	depends on NLM_XLP
+	bool "Enable XLP device mode"
+	default n
+	help
+		Some XLP specific drivers need be compiled as host or device
+		mode. If selected, this option enables device mode compilation.
+		Otherwise, these drivers are compiled as host mode drivers.
+
 config NLM_RIXI
        bool "Enable Read Inhibit/ Execute Inhibit support"
        depends on NLM_XLP && 64BIT
diff --git a/arch/mips/netlogic/xlp/irq.c b/arch/mips/netlogic/xlp/irq.c
index 0c1a244..c2e45fa 100644
--- a/arch/mips/netlogic/xlp/irq.c
+++ b/arch/mips/netlogic/xlp/irq.c
@@ -48,11 +48,12 @@ THE POSSIBILITY OF SUCH DAMAGE.
 #include <asm/netlogic/debug.h>
 #include <asm/thread_info.h>
 
-/*
- * These are the routines that handle all the low level interrupt stuff.
+/* About this file: irq.c
+ * This file contains routines that handle all the low level interrupt stuff.
+ * Some of the platform specific portions are moved to arch/mips/pci/pci-xlp.c
  * Actions handled here are: initialization of the interrupt map, requesting of
- * interrupt lines by handlers, dispatching if interrupts to handlers, probing
- * for interrupt lines
+ * interrupt lines by handlers, dispatching interrupts to handlers, probing
+ * for interrupt lines..etc.
  */
 
 /* Externs */
@@ -65,8 +66,8 @@ extern int is_msi_set(int);
 extern int calc_msi_vector_offset(int);
 extern void xlp_msi_disable(int, int);
 extern u32 xlp_msi_set_mask(int, int, int);
-extern u64 xlp_msix_addr_start(int);
-extern u64 xlp_msi_addr_start(int);
+volatile const void *xlp_msix_addr_start(int);
+volatile const void *xlp_msi_addr_start(int);
 extern int xlp_ctrl_fn_from_dev(const struct pci_dev *);
 extern u32 xlp_msix_status_clear(int);
 extern u32 xlp_msix_set_mask(int, int, int);
@@ -80,7 +81,16 @@ extern void xlp_intx_enable(int);
 extern void xlp_intx_disable(int);
 
 /* own variables */
+
+/* xlp_irq_mask is retained for legacy. It can be removed at a later point of
+ * time. Initially it was meant to keep a copy of present interrupt mask; with
+ * multi cpus each having its own mask register, we might not need this variable
+ */
 static volatile uint64_t xlp_irq_mask;
+
+/* spin lock for all interrupt related data structures
+ * This variable is used in timer init, so we export it
+ */
 spinlock_t xlp_pic_lock = SPIN_LOCK_UNLOCKED;
 EXPORT_SYMBOL(xlp_pic_lock);
 /*
@@ -93,17 +103,46 @@ struct msix_alloc_bitmap {
 };
 static struct msix_alloc_bitmap msix_vec[XLP_MAX_SLOTS];
 
-// #define pr_err fdebug
-
 /*
- * rvec_map is meant to map IRT numbers (index of IRT) to RVEC numbers.
- * RVEC is just a number for s/w; which is the bit that is set in EIRR
- * Refer PRM : 9.3
+ * There are two data structures pivotal for interrupt delivery mechanism
+ * irq_map[] and rvec_map[]
+ * irq_map[] : An array of struct irq_map_elem.
+ * Number of elements in this array is equal to NR_IRQ => there should be an
+ * entry corresponding to each interrupt in the system.
+ * Initial 8 elements (0-XLP_IRQ_RESERVED_MAX) are unpopulated. They are
+ * reserved for system internal interrupts which are not explicitly handled
+ * by plat_irq_dispatch; the handlers for these interrupts are called
+ * differently.
+ *
+ * All other entries are handled by plat_irq_dispatch()
+ * An entry would contain the rvec entry for this interrupt. The offset of this
+ * entry would be presented as the interrupt number for any requests
+ * E.g, for UART1, index is 141. This is the value of uart1 interrupt.
+ * asm/netlogic/xlp_irq.h defines this value as XLP_UART_IRQ(1)
+ *
+ * Each irq_map_elem has two members : rvec -> the rvec entry for this entry,
+ * usage : the number of successful irq_request() called on this IRQ.
+ *
+ * rvec_map is meant to map rvec numbers back to Interrupts.
+ * RVEC is just a number for s/w; which is the bit offset that is set in EIRR
+ * Refer PRM : 9.3 for details.
  *
  * irq_map : {<IERR RVEC>, <#of s/w vector multiplexed on this RVEC> }
  * Some RVECs are reserved : so use only 9 through 63
  * Any irt index can be derived from irq using the macros
  * xlp_irt_to_irq() or xlp_irq_to_irt()
+ * These macros are required because of the imposed 64 bits (if RVEC)
+ * to 160 entries (size of IRT table)
+ *
+ * It is further complicated by the fact that PCI LINK interrupts are
+ * multiplexed with MSI interrupts. In that mode, each PCI Link interrupts
+ * can be caused by 32 MSI interrupts. That means, we need a meachinsm to map
+ * 32 msi interrupts * 4 pci slots (128 interrupts) to 4 possible RVECs.
+ * the irq_map table serves this purpose as well as follows
+ *
+ * We limit the per pci slot interrupt (for the time being) to XLP_MSI_PER_SLOT
+ * (currently 8). This is done to keep total number of interrupts to NR_IRQ;
+ *
  */
 struct irq_map_elem {
 	int rvec;
@@ -389,10 +428,16 @@ void xlp_clear_eimr(void *param)
 
 void __xlp_setup_one_irq(u64 irq)
 {
-	int cpu = smp_processor_id();
+	int cpu;
+	preempt_disable();
+	cpu = smp_processor_id();
 	__nlm_hal_set_irt_to_cpu(xlp_irq_to_irt(irq), cpu);
+	preempt_enable();
 }
 
+/*
+ * Returns the base IRQ (index of irq_map) from an rvec
+ */
 static inline int __irqbase_from_rvec(int rvec)
 {
 	int irt;
@@ -405,6 +450,9 @@ static inline int __irqbase_from_rvec(int rvec)
 }
 
 
+/*
+ * Returns the base IRQ from an rvec
+ */
 static inline int irqbase_from_rvec(int rvec)
 {
 	int ret;
@@ -433,13 +481,12 @@ EXPORT_SYMBOL(xlp_rvec_from_irq);
 
 /*
  * Masks out one IRQ in the EIMR register
- * Must be called with xlp_pic_lock held
+ * Must NOT be called with xlp_pic_lock held
  * @irq : IRQ number
  */
 static void __nlm_irq_mask(unsigned int irq)
 {
 	int rvec;
-	u64 mask;
 
 	rvec = xlp_rvec_from_irq(irq);
 	if (rvec < 0) {
@@ -457,7 +504,7 @@ static void __nlm_irq_mask(unsigned int irq)
  */
 static void nlm_irq_mask(unsigned int irq)
 {
-	unsigned long flags;
+	//unsigned long flags;
 
 	if((irq < XLP_IRQ_RESERVED_MAX) && (irq >= 0)) {
 		return;
@@ -478,7 +525,6 @@ static void nlm_irq_mask(unsigned int irq)
  */
 static void __nlm_irq_unmask(int irq)
 {
-	volatile u64 mask;
 	int rvec = xlp_rvec_from_irq(irq);
 
 	if (rvec < 0) {
@@ -496,7 +542,7 @@ static void __nlm_irq_unmask(int irq)
  */
 static void nlm_irq_unmask(unsigned int irq)
 {
-	unsigned long flags;
+	//unsigned long flags;
 
 	if((irq < XLP_IRQ_RESERVED_MAX) && (irq >= 0)) {
 		return;
@@ -550,6 +596,11 @@ static void nlm_irq_end(unsigned int irq)
 /*
  * Startup function for normal IRQ
  * @irq: irq number
+ *
+ * This function is called as chip->startup() for all IRQs.
+ * In case of XLP, all normal interrupts must fall below XLP_MSI_IRQ_OFFSET
+ * When an interrupt is started, we force it to be enabled only in cpu0, it can
+ * be changed later by calling nlm_irq_set_affinity()
  */
 static unsigned int nlm_irq_startup(unsigned int irq)
 {
@@ -557,7 +608,8 @@ static unsigned int nlm_irq_startup(unsigned int irq)
 	int ret = 0;
 	unsigned long flags;
 	int idx, rvec;
-	struct cpumask m, *n;
+	struct cpumask m;
+	struct cpumask const *n;
 
 	cpumask_clear(&m);
 	cpumask_set_cpu(0, &m);
@@ -582,7 +634,10 @@ static unsigned int nlm_irq_startup(unsigned int irq)
 		irq_map[irq].usage++;
 		/* At this point, make sure that each CPU has eimr bit
 		 * corresponding to this IRQ set. Later the driver can set
-		 * the cpu affinity of this interrupt */
+		 * the cpu affinity of this interrupt. The rationale for
+		 * setting up EIMR here is that it can be moved to any CPUs
+		 * (well, a subset of any CPUs) later
+		 */
 		__nlm_irq_unmask(irq);
 	} else if (irq_map[irq].usage > 0) {
 		/* already being used. No need to check mask
@@ -599,11 +654,16 @@ __failure:
 	spin_unlock_irqrestore(&xlp_pic_lock, flags);
 	return ret;
 }
+
 /*
  * IRQ shut down function
  * Disables one IRQ
  *
  * @irq : irq to shut down
+ *
+ * This function is called whenever release_irq() is called by means of
+ * chip->shutdown(). In this function, the rvec bit in every EIMR is cleared if
+ * usage falls to zero (in case of shared interrupts)
  */
 static void nlm_irq_shutdown(unsigned int irq)
 {
@@ -642,16 +702,25 @@ static void nlm_irq_shutdown(unsigned int irq)
  *
  * When an interrupt is setup, its EIMR bit is set in all online cpus. That is,
  * any cpu _can_ receive that interrupt. But it is the IRT entry that decides
- * whether to send that interrupt (i.e, whether to set EIRR bit or not) to that
+ * whether to send that interrupt (i.e, whether to set EIRR bit or not) to any
  * particular CPU.
  *
- * This function sets up the IRT entries according to cpumask.
+ * IRT has two modes to decide the target CPUs for one interrupt.
+ * Method 1 : Using IRT table entry bits DT and DTE
+ * If DT==1, this interrupt can be routed to a max of 16 CPUs (well, hw threads)
+ * If DT==1, there is one more level of indirection called DTE. Each DTE entry
+ * has 128 bits and there are a total of 8 DTE entries. Each DTE entry contains
+ * the bitmask of target CPU for an interrupt. One of them is chosen based
+ * on the specified cpumask.
+ *
+ * The actual bitmask can be different from the specified bitmask based
+ * on the logic of xlp_closest_match_cpumask()
  */
 static int nlm_irq_set_affinity(unsigned int irq, const struct cpumask *mask)
 {
 	unsigned long flags;
-	int cpu;
-	const struct cpumask *m, n;
+	const struct cpumask *m;
+	struct cpumask n;
 
 	if((irq < XLP_IRQ_RESERVED_MAX) && (irq >= 0)) {
 		return 0;
@@ -770,6 +839,7 @@ void do_nlm_common_IRQ(unsigned int irq, struct pt_regs *regs)
 	}
 }
 
+/* Unused function? Remove later */
 void __cpuinit nlm_smp_irq_init(void)
 {
 #ifdef XLP_MERGE_TODO
@@ -788,6 +858,20 @@ void destroy_irq(unsigned int irq)
 #ifdef CONFIG_PCI_MSI_XLP
 
 /*
+ * The MSI and MSI-X functionality is supported only by the PCIe Controller.
+ * Whenever there is a request for MSI/MSI-X, we need to find out the
+ * controller on which that request is made. But in the PCI structure, I could
+ * not find a place where that information can be kept. Moreover, a hack job
+ * is not justified for the reason that only a small subset of PCI devices
+ * (no onboard ones) require this. So, we have the file arch/mips/pci/pci-xlp.c
+ * for XLP specific functionality.
+ *
+ * In case of MSI, we have to share one interrupt (XLP_PCIE_LINK_IRQ(x)) for
+ * for 32 possible MSI on a slot.
+ * We work around this problem by limiting the #of MSI per slot to
+ * XLP_MSI_PER_SLOT. MSI IRQ vectors start from XLP_MSI_IRQ_OFFSET.
+ * plat_irq_dispatch checks the vector number and dispatch it correctly.
+ *
  * These bunch of functions would find out the controller function using the
  * passed parameter and use nlm_irq_* function to operate on that IRT
  */
@@ -906,7 +990,7 @@ struct irq_chip nlm_msi_pic = {
 };
 
 /*
- * These bunch of functions would find out the controller function using the
+ * These functions would find out the controller function using the
  * passed parameter and use nlm_irq_* function to operate on that IRT
  */
 
@@ -1005,6 +1089,9 @@ struct irq_chip nlm_msix_pic = {
 };
 
 
+/*
+ * Composes MSI/MSI-X messages
+ */
 static int xlp_msi_compose_msg(struct pci_dev *pdev, struct msi_desc *desc,
 		unsigned int irq, struct msi_msg *msg)
 {
@@ -1052,6 +1139,9 @@ u32 __xlp_msix_bitmask(int fn)
 	return ret;
 }
 
+/*
+ * Back end of disable_msi()/ disable_msix()
+ */
 void arch_teardown_msi_irq(unsigned int msi)
 {
 	unsigned long flags;
@@ -1105,6 +1195,9 @@ static int xlp_perf_irq(void)
 	return IRQ_HANDLED;
 }
 
+/*
+ * Entry function for interrupts
+ */
 asmlinkage void plat_irq_dispatch(void)
 {
 	volatile u64 eirr;
@@ -1121,7 +1214,8 @@ asmlinkage void plat_irq_dispatch(void)
 		nlm_common_timer_interrupt(pt_regs, XLP_IRQ_TIMER);
 		return;
 	}
-	while (eirr) {		/* LOOP STARTS HERE */
+	/* Loop till all bits of eirr is cleared */
+	while (eirr) {
 
 	rvec = __ilog2_u64(eirr);
 	if (rvec == -1) {
@@ -1173,6 +1267,7 @@ asmlinkage void plat_irq_dispatch(void)
 			break;
 		}
 		while (bitmap) {
+			/* now that we have bitmap, serve all of them */
 			idx = ffs(bitmap) - 1;	/* man ffs */
 			bitmap &= ~(1 << idx);
 			irq = base_irq + idx;
@@ -1200,7 +1295,6 @@ EXPORT_SYMBOL(nlm_xlp_request_irq);
 /*
  * Arch specific setup functions and helpers
  */
-
 static int __xlp_alloc_msi(int base, int max_base, int req, int *max_avail)
 {
 	int ret;
@@ -1246,6 +1340,13 @@ static int __xlp_reserve_nvec(int base, int req, int *max)
 	return ret;
 }
 
+/*
+ * Backend function that sets up MSI.
+ * Called from arch_setup_msi_irqs()
+ *
+ * On XLP, we can have more than one MSI per device. But no device requests it
+ * because of possible x86 influence (one MSI per device limitation)
+ */
 int xlp_setup_msi_irq(struct pci_dev *dev, struct msi_desc *desc, int nvec)
 {
 	struct msi_msg msg;
diff --git a/arch/mips/netlogic/xlp/on_chip.c b/arch/mips/netlogic/xlp/on_chip.c
index ba9d53b..0cfd08b 100644
--- a/arch/mips/netlogic/xlp/on_chip.c
+++ b/arch/mips/netlogic/xlp/on_chip.c
@@ -44,6 +44,7 @@ THE POSSIBILITY OF SUCH DAMAGE.
 #define MAX_VC	4096
 
 extern int xlp_rvec_from_irt(int);
+extern int xlp_rvec_from_irq(int);
 unsigned long netlogic_io_base = (unsigned long)(DEFAULT_NETLOGIC_IO_BASE);
 EXPORT_SYMBOL(netlogic_io_base);
 
@@ -60,6 +61,7 @@ uint32_t nlm_l1_lock[NR_CPUS/4] = {0};
 
 /* make this a read/write spinlock */
 spinlock_t msgrng_lock;
+static nlm_common_atomic_t msgring_registered;
 
 struct msgstn_handler {
         void (*action)(uint32_t, uint32_t, uint32_t, uint32_t, uint64_t, uint64_t, uint64_t, uint64_t, void *);
@@ -186,10 +188,12 @@ static uint16_t xlp3xx_vc_to_handle_map[MAX_VC] = {
 void dummy_handler(uint32_t vc, uint32_t src_id, uint32_t size, uint32_t code, 
 		   uint64_t msg0, uint64_t msg1, uint64_t msg2, uint64_t msg3, void *dev_id)
 {
+#if 0
 	printk("[%s]: No Handler for message from stn_id=%d, bucket=%d, "
 	       "size=%d, msg0=%llx, msg1=%llx dropping message\n",
 	       __FUNCTION__, src_id, vc, size,
 	       (unsigned long long)msg0, (unsigned long long)msg1);
+#endif
 }
 
 /******************************************************************************************
@@ -211,6 +215,7 @@ struct msgstn_handler msg_handler_map[XLP_MSG_HANDLE_MAX] = {
  ********************************************************************/
 void nlm_xlp_msgring_int_handler(unsigned int irq, struct pt_regs *regs)
 {
+	unsigned long mflags;
 	int vc = 0;
 	uint32_t size = 0, code = 0, src_id = 0, cycles = 0;
 	struct msgstn_handler *handler = 0;
@@ -223,7 +228,7 @@ void nlm_xlp_msgring_int_handler(unsigned int irq, struct pt_regs *regs)
 
 	msg0 = msg1 = msg2 = msg3 = 0;
 
-        if (irq == XLP_IRQ_MSGRING) {
+	if (irq == XLP_IRQ_MSGRING) {
                 /* normal message ring interrupt */
                 /* xlr_inc_counter(MSGRNG_INT);  */
                 nlm_cpu_stat_update_msgring_int();
@@ -305,6 +310,44 @@ void init_msg_bkp_timer(void *data)
 	timer->function = msg_timer_handler;
 	add_timer(timer);
 }
+
+void xlp_poll_vc0_messages(void)
+{
+        int vc = 0;
+        uint32_t size = 0, code = 0, src_id = 0;
+        struct msgstn_handler *handler = 0;
+        unsigned int status = 0;
+        uint64_t msg0, msg1, msg2, msg3;
+        unsigned int msg_status1 = 0, vc_empty_status = 0;
+        int loop = 0;
+        int pop_vc_mask = 0x1;
+	unsigned long mflags;
+#if 0
+	if (hard_smp_processor_id() != 0)
+		printk("Called handler on cpu %d from %s msgstatus: 0x%x\n",
+			       hard_smp_processor_id(),
+			       __FUNCTION__,xlp_read_status1());
+#endif
+        msg0 = msg1 = msg2 = msg3 = 0;
+        msgrng_access_enable(mflags);
+        for (loop = 0; loop < 16; loop++) {
+                /* Read latest VC empty mask */
+                msg_status1 = xlp_read_status1();
+                vc_empty_status = (msg_status1 >> 24) & pop_vc_mask;
+                if (vc_empty_status == pop_vc_mask)
+                        break;
+                status = xlp_message_receive( vc, &src_id, &size, &code, &msg0, &msg1, &msg2, &msg3);
+                if(status != 0)
+                        continue;
+                handler = &msg_handler_map[vc_to_handle_map[src_id]];
+                /* Execute device driver fmn handler */
+                (handler->action)(vc, src_id, size, code,
+                                msg0, msg1, msg2, msg3, handler->dev_id);
+        }
+        msgrng_access_disable(mflags);
+}
+EXPORT_SYMBOL(xlp_poll_vc0_messages);
+
 /*******************************************************************************************
  *  register_xlp_msgring_handler 
  *
@@ -342,6 +385,8 @@ int register_xlp_msgring_handler(int major,
 	msg_handler_map[major].dev_id = dev_id;
 
 	ret = 0;
+	msgring_registered.value = 1;
+
 	spin_unlock_irqrestore(&msgrng_lock, flags);
 
 	return ret;
@@ -367,6 +412,7 @@ int unregister_xlp_msgring_handler(int major, void *dev_id)
 	spin_unlock_irqrestore(&msgrng_lock, flags);
 	return 0;
 }
+
 EXPORT_SYMBOL(unregister_xlp_msgring_handler);
 
 #include <asm/netlogic/cpumask.h>
@@ -399,6 +445,8 @@ void nlm_nmi_cpus(unsigned int mask)
  ********************************************************************/
 void enable_msgconfig_int(void)
 {
+	uint32_t flags;
+
 	/* Need write interrupt vector to cp2 msgconfig register */
 	msgrng_access_enable(flags);
 	nlm_hal_set_fmn_interrupt(XLP_IRQ_MSGRING);
@@ -410,7 +458,6 @@ void enable_msgconfig_int(void)
  *  pic_init
  *  
  ********************************************************************/
-extern int xlp_rvec_from_irq(int);
 static void pic_init(void)
 {
 	int i = 0;
@@ -418,13 +465,16 @@ static void pic_init(void)
 	uint32_t thread_mask;
 
 	vcpu = hard_smp_processor_id() & 0x1F;
+
 	thread_mask = (1 << vcpu);
+
 	for (i = XLP_IRQ_RESERVED_MAX; i < XLP_IRT_NUM; i++) {
 		level = PIC_IRQ_IS_EDGE_TRIGGERED(i);
 		/* Use local scheduling and high polarity for all IRTs
 		 * Invalidate all IRTs, by default */
 		nlm_hal_pic_write_irt(xlp_irq_to_irt(i), 0, 0, 1, xlp_rvec_from_irq(i), 1, 0, thread_mask);
 	}
+
 	/* On XLP, MSGRING config register is per hw-thread */
 	enable_msgconfig_int();
 }
@@ -475,6 +525,8 @@ void nlm_enable_vc_intr(void)
 			vc_index = (i + cpu*NLM_MAX_VC_PER_THREAD) & 0x7f;
 			/*enable interrupts*/
 			nlm_hal_enable_vc_intr(vc_index);
+		}else{
+			nlm_hal_disable_vc_intr(vc_index);
 		}
 	}
 }
@@ -492,6 +544,8 @@ void on_chip_init(void)
 	/* Set netlogic_io_base to the run time value */
 	spin_lock_init(&msgrng_lock);
 
+	msgring_registered.value = 0;
+
 	nlm_hal_init();
 
 	pic_init(); 
diff --git a/arch/mips/netlogic/xlp/xlp_hal_pic.c b/arch/mips/netlogic/xlp/xlp_hal_pic.c
index 7c52470..3a0520a 100644
--- a/arch/mips/netlogic/xlp/xlp_hal_pic.c
+++ b/arch/mips/netlogic/xlp/xlp_hal_pic.c
@@ -49,7 +49,6 @@ void __nlm_hal_release_irq(int irt)
 void __nlm_hal_set_irt_to_cpu(int irt, int cpu)
 {
         uint64_t val;
-	uint node;
 	uint cpuid, threadid;
 	uint nodeid = 0;	/* TBD */
 	/* DT is set 1 ==> Destination thread is specificed in DB and
diff --git a/arch/mips/pci/pci-xlp.c b/arch/mips/pci/pci-xlp.c
index df8b22b..10dbb96 100644
--- a/arch/mips/pci/pci-xlp.c
+++ b/arch/mips/pci/pci-xlp.c
@@ -23,6 +23,10 @@ ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
 THE POSSIBILITY OF SUCH DAMAGE.
 *****************************#NETL_2#********************************/
 
+/*
+ * This file contains specific functions for XLP chipsets and
+ * EVP boards.
+ */
 #include <linux/types.h>
 #include <linux/pci.h>
 #include <linux/kernel.h>
@@ -43,7 +47,7 @@ THE POSSIBILITY OF SUCH DAMAGE.
 
 extern int pci_probe_only;
 static void *pci_config_base;
-static void *pci_io_base;
+static const volatile void *pci_io_base;
 
 void xlp_intx_enable(int fn);
 void xlp_intx_disable(int fn);
@@ -52,35 +56,44 @@ int xlp_msix_enable(struct pci_dev *dev);
 void xlp_msi_disable(int fn, u32 bitmap);
 void xlp_msix_disable(int fn);
 
-#define SWAP32(x)				\
-        (((x) & 0xff000000) >> 24) |		\
-        (((x) & 0x000000ff) << 24) |		\
-        (((x) & 0x0000ff00) << 8)  |		\
-        (((x) & 0x00ff0000) >> 8)
-
 /*
  * Possible values are no more hard coded.
  * For mapping of these values to IRT, refer
  * arch/mips/netlogic/xlp/irq.c
  *
+ * Here a table is defined to figure out the interrupt assignments to different
+ * cards placed on any of the 4 PCI slots.
+ *
  * We have some unique problems here.
  * 1. Board could be configured in different lane widths. That means, the cards
  * could be controlled by different functions of the controller on board
  * Eg. 2x8 config can have two cards (fn 0 and fn 2)
  *	4x4 config can also have two cards (under fn0 through fn 3)
+ * 2. Cards can be placed on any available slot
+ * 3. The card can have a switch built in, thus giving rise to multiple devices
+ * on the slot.
+ *
  * So, it is important to figure out the lanes on which cards are placed.
  * First we read the lane config from POWER_ON_RESET_CFG
  * Then each line's LTSSM state would give the card presence
- * Based on that we have to assign interrupt values
+ * Based on that we have to assign interrupt values; while keeping the
+ * possibility of same interrupt assigned to multiple devices open.
  *
- * XLP irq map is as follows
+ * So, we have a map: XLP irq map is as follows
  *  \fn 0	1	2	3
  *plc\
  * 0	86	0	88	89
  * 1	86	87	88	0
  * 2	86	0	88	89
  * 3	86	87	88	89
- * This map changes from processor to processor. check PRM or RTL
+ * This map changes from processor to processor. check PRM or RTL because
+ * the values are a function of XLP_PCIE_LINK_IRT_OFFSET. To make them
+ * somewhat independent, I have defined macros and used them here.
+ *
+ * This map is dynamically populated based on card presence in the slot.
+ * If a card is present, and is a switch, then the secondary and subordinate
+ * numbers would be different. Based on this fact, we can figure out from
+ * pci_dev structure the slot where a card is placed at run time.
  */
 static int xlp_irq_map[4][4][3] = {
 	{{XLP_PCIE_LINK_IRQ(0), 0, 0}, {0, 0, 0},
@@ -93,12 +106,17 @@ static int xlp_irq_map[4][4][3] = {
 		{XLP_PCIE_LINK_IRQ(2), 0, 0}, {XLP_PCIE_LINK_IRQ(3), 0, 0}},
 };
 
+/* Just a helper function to fill up xlp_irq_map table's entries
+ * This function checks whether a PCIe slot is populated and if yes,
+ * fills up the table with subordinate and secondary bus numbers. These
+ * numbers would be different only if the PCIe device has a switch inside.
+ */
 static int xlp_map_helper(int row, int fn)
 {
 	u64 xlp_pci_base;
 	u32 reg6, ltssm;
 
-	xlp_pci_base = 0x18000000 + (0 << 20) + (1 << 15) + ( fn << 12);
+	xlp_pci_base = XLP_BDF_BASE(0,1,fn);
 	ltssm = nlm_hal_read_32bit_reg(xlp_pci_base, 0x25E);
 	if (ltssm != 0x00446000) {
 		printk(KERN_WARNING "LTSSM state is %#x. Fn %x link not up\n",
@@ -111,6 +129,9 @@ static int xlp_map_helper(int row, int fn)
 	return 0;
 }
 
+/*
+ * Iterates over buses to find out the slot (thus pci controller fn)
+ */
 int xlp_ctrl_fn_from_dev(const struct pci_dev *dev)
 {
 	__label__ out;
@@ -134,57 +155,41 @@ out:
 	return fn;
 }
 
+/*
+ * We discard the idea of a fixed address for MSI. But if that is ever required,
+ * define CONFIG_XLP_MSI_ADDRESSES
+ */
 #ifndef CONFIG_XLP_MSI_ADDRESSES
 static u64 XLP_MSI_ADDR = 0;
-static u64 xlp_msix_addr_array[XLP_MAX_SLOTS];
-static u64 xlp_msi_addr_array[XLP_MAX_SLOTS];
-#else
-static u64 xlp_msi_addr_array[] = {
-	XLP_MSI_ADDR + (0 * XLP_MSI_ADDR_SIZE),
-	XLP_MSI_ADDR + (1 * XLP_MSI_ADDR_SIZE),
-	XLP_MSI_ADDR + (2 * XLP_MSI_ADDR_SIZE),
-	XLP_MSI_ADDR + (3 * XLP_MSI_ADDR_SIZE),
-};
-
-static u64 xlp_msix_addr_array[] = {
-	XLP_MSI_ADDR + (0 * XLP_MSIX_ADDR_SIZE),
-	XLP_MSI_ADDR + (1 * XLP_MSIX_ADDR_SIZE),
-	XLP_MSI_ADDR + (2 * XLP_MSIX_ADDR_SIZE),
-	XLP_MSI_ADDR + (3 * XLP_MSIX_ADDR_SIZE),
-};
 #endif
 
-u64 xlp_msix_addr_start(int fn)
+volatile const void *xlp_msix_addr_start(int fn)
 {
-#ifdef CONFIG_XLP_MSI_ADDRESSES
-	return xlp_msix_addr_array[fn];
-#else
 	if (XLP_MSI_ADDR == 0) {
 		return 0;
 	}
-	return (XLP_MSI_ADDR + (fn * XLP_MSIX_ADDR_SIZE));
-#endif
+	return (volatile const void *)(XLP_MSI_ADDR + (fn * XLP_MSIX_ADDR_SIZE));
 }
 
-u64 xlp_msi_addr_start(int fn)
+volatile const void *xlp_msi_addr_start(int fn)
 {
-#ifdef CONFIG_XLP_MSI_ADDRESSES
-	return xlp_msi_addr_array[fn];
-#else
 	if (XLP_MSI_ADDR == 0) {
 		return 0;
 	}
-	return (XLP_MSI_ADDR + (fn * XLP_MSI_ADDR_SIZE));
-#endif
+	return (volatile const void *)(XLP_MSI_ADDR + (fn * XLP_MSI_ADDR_SIZE));
 }
 
+/* Irrespective of any device requesting MSI/MSI-X, we keep the controller
+ * ready by programming the corresponding registers. This action, per se,
+ * does not start MSI/MSI-X for they have to be enabled explicitly.
+ */
 static void xlp_msi_controller_init(int fn)
 {
 	u64 xlp_pci_base;
 	u8 mmc;
-	u32 pci, msi;
+	u32 msi;
 
-	xlp_pci_base = 0x18000000 + (0 << 20) + (1 << 15) + ( fn << 12);
+	xlp_pci_base = XLP_BDF_BASE(0,1,fn);
 	if (XLP_MSI_ADDR == 0) {
 		printk(KERN_ERR "MSI/MSI-X CANNOT be programmed\n");
 		return;
@@ -192,16 +197,26 @@ static void xlp_msi_controller_init(int fn)
 	msi = nlm_hal_read_32bit_reg(xlp_pci_base, 0x14);
 	mmc = (msi >> 17) & 0x7;
 	/* Initialize MSI Base register */
-	nlm_hal_write_32bit_reg(xlp_pci_base, 0x15, virt_to_phys(xlp_msi_addr_start(fn)) & 0xffffffff);
-	nlm_hal_write_32bit_reg(xlp_pci_base, 0x16, (virt_to_phys(xlp_msi_addr_start(fn)) >> 32) & 0xffffffff);
+	nlm_hal_write_32bit_reg(xlp_pci_base, 0x15,
+		virt_to_phys(xlp_msi_addr_start(fn)) & 0xffffffff);
+	nlm_hal_write_32bit_reg(xlp_pci_base, 0x16,
+		(virt_to_phys(xlp_msi_addr_start(fn)) >> 32) & 0xffffffff);
 	nlm_hal_write_32bit_reg(xlp_pci_base, 0x17, 0x0);
 	msi |= ((mmc << 10) | (1 << 16));
 	nlm_hal_write_32bit_reg(xlp_pci_base, 0x14, msi);
-	/* Initialize MSI-X Base and Address reg*/
-	nlm_hal_write_32bit_reg(xlp_pci_base, 0x24F, (virt_to_phys(xlp_msix_addr_start(fn)) >> 8));
-	nlm_hal_write_32bit_reg(xlp_pci_base, 0x250, (virt_to_phys((xlp_msix_addr_start(fn) + XLP_MSIX_ADDR_SIZE)) >> 8));
+	/* Initialize MSI-X Base and Address reg. Note >> 8 in the address.
+	 * This is how 40bit address goes in 32bit registers.*/
+	nlm_hal_write_32bit_reg(xlp_pci_base, 0x24F,
+		(virt_to_phys(xlp_msix_addr_start(fn)) >> 8));
+	nlm_hal_write_32bit_reg(xlp_pci_base, 0x250,
+		(virt_to_phys(xlp_msix_addr_start(fn) + XLP_MSIX_ADDR_SIZE) >> 8));
 }
 
+/*
+ * Controller is initialized and explicity disabled
+ *
+ * @fn : controller function no.
+ */
 void xlp_pcie_controller_setup(int fn)
 {
 	xlp_msi_controller_init(fn);
@@ -211,12 +226,22 @@ void xlp_pcie_controller_setup(int fn)
 	xlp_intx_enable(fn);
 }
 
-u32 xlp_get_power_on_reset_cfg(void)
+/*
+ * Utility function to get syscfg
+ *
+ * @node : node id in multi chip config
+ */
+u32 xlp_get_power_on_reset_cfg(int node)
 {
-	u64 xlp_syscfg_base = 0x18000000 + (0 << 20) + (6 << 15) + ( 5 << 12);
+	u64 xlp_syscfg_base = XLP_BDF_BASE(0,6,5);
 	return nlm_hal_read_32bit_reg(xlp_syscfg_base, 0x41);
 }
+EXPORT_SYMBOL(xlp_get_power_on_reset_cfg);
+
 
+/*
+ * Called from system startup routine
+ */
 static void pcie_controller_init_done(void)
 {
 	u32 plc, syscfg, mode, count = 0;
@@ -231,7 +256,7 @@ static void pcie_controller_init_done(void)
 		printk(KERN_WARNING "PCIe bus IRQs configured incorrectly\n");
 		return;
 	}
-	syscfg = xlp_get_power_on_reset_cfg();
+	syscfg = xlp_get_power_on_reset_cfg(0);
 	/* We don't manipulate pci_address space.
 	 * Get the link status from pcie lane config from 34.9.7.2 XLP PRM */
 	mode = (syscfg >> 19) & 0xf;
@@ -243,9 +268,11 @@ static void pcie_controller_init_done(void)
 	plc = (syscfg >> 23) & 0x3;
 	printk(KERN_WARNING "PLC = %#x, mode = %#x\n", plc, mode);
 	switch (plc) {
-		/* In each case find subordinate and primary numbers */
+	/* The correlation between plc and lane config is very specific to XLP
+	 * and not very clear in PRM
+	 */
 	case 0:
-		/* controller 0 and 2 are active */
+		/* controller 0 and 2 are active with 8lanes each */
 		if (mode & 0x1){
 			if (xlp_map_helper(plc, 0) == 0) {
 				xlp_pcie_controller_setup(0);
@@ -258,6 +285,7 @@ static void pcie_controller_init_done(void)
 		}
 		break;
 	case 1:
+		/* controllers 0,1 and 2 are active */
 		if (mode & 0x1){
 			if (xlp_map_helper(plc, 0) == 0) {
 				xlp_pcie_controller_setup(0);
@@ -275,6 +303,7 @@ static void pcie_controller_init_done(void)
 		}
 		break;
 	case 2:
+		/* controllers 0,2 and 3 are active */
 		if (mode & 0x1){
 			if (xlp_map_helper(plc, 0) == 0) {
 				xlp_pcie_controller_setup(0);
@@ -292,6 +321,7 @@ static void pcie_controller_init_done(void)
 		}
 		break;
 	case 3:
+		/* All four controllers are active with 4 lanes each */
 		if (mode & 0x1){
 			if (xlp_map_helper(plc, 0) == 0) {
 				xlp_pcie_controller_setup(0);
@@ -428,6 +458,8 @@ struct pci_controller xlp_controller = {
  * Apparently this function is called for all pci controller functions
  * viz. 0:1.0, 0:1.1, 0:1.2 and 0:1.3
  * In fact, we need not assign them any interrupt.
+ * But for any devices connected on them, consult the populated table
+ * and return corresponding interrupt.
  */
 int __init pcibios_map_irq(const struct pci_dev *dev, u8 slot, u8 pin)
 {
@@ -439,18 +471,21 @@ int __init pcibios_map_irq(const struct pci_dev *dev, u8 slot, u8 pin)
 	default:
 		break;
 	}
-	row = (xlp_get_power_on_reset_cfg() >> 23) & 0x3;
+	row = (xlp_get_power_on_reset_cfg(0) >> 23) & 0x3;
 	fn = xlp_ctrl_fn_from_dev(dev);
 	dev_printk(KERN_WARNING, &dev->dev, "Assigning interrupt %#x\n", xlp_irq_map[row][fn][0]);
 	return xlp_irq_map[row][fn][0];
 }
 
+/*
+ * Enables INTx on a controller
+ */
 void xlp_intx_enable(int fn)
 {
 	u64 xlp_pci_base;
 	u32 pci;
 
-	xlp_pci_base = 0x18000000 + (0 << 20) + (1 << 15) + ( fn << 12);
+	xlp_pci_base = XLP_BDF_BASE(0,1,fn);
 	pci = nlm_hal_read_32bit_reg(xlp_pci_base, 0x1);
 	pci &= ~(1 << 10);	/* Enable IntX assertion */
 	nlm_hal_write_32bit_reg(xlp_pci_base, 0x1, pci);
@@ -460,12 +495,15 @@ void xlp_intx_enable(int fn)
 	return;
 }
 
+/*
+ * Disables INTx on a controller
+ */
 void xlp_intx_disable(int fn)
 {
 	u64 xlp_pci_base;
 	u32 pci;
 
-	xlp_pci_base = 0x18000000 + (0 << 20) + (1 << 15) + ( fn << 12);
+	xlp_pci_base = XLP_BDF_BASE(0,1,fn);
 	pci = nlm_hal_read_32bit_reg(xlp_pci_base, 0x1);
 	pci |= (1 << 10);
 	nlm_hal_write_32bit_reg(xlp_pci_base, 0x1, pci);
@@ -477,7 +515,7 @@ void xlp_intx_disable(int fn)
 
 /*
  * Finds the slot on which this device is placed and enables corresponding
- * MSI enable register on the controller
+ * MSI enable register on the _controller_ if not already enabled
  * @dev : pci device corresponding to this device
  */
 int xlp_msi_enable(struct pci_dev *dev, u32 bitmap)
@@ -487,7 +525,7 @@ int xlp_msi_enable(struct pci_dev *dev, u32 bitmap)
 	u32 msi_en;
 
 	fn = xlp_ctrl_fn_from_dev(dev);
-	xlp_pci_base = 0x18000000 + (0 << 20) + (1 << 15) + ( fn << 12);
+	xlp_pci_base = XLP_BDF_BASE(0,1,fn);
 	/* First, set PCIe Int Enable register. __KEEP_THIS_ORDER__ */
 	msi_en = nlm_hal_read_32bit_reg(xlp_pci_base, 0x261);
 	if (!(msi_en & (1 << 9))) {
@@ -504,6 +542,11 @@ int xlp_msi_enable(struct pci_dev *dev, u32 bitmap)
 	return 0;
 }
 
+/*
+ * Finds the slot on which this device is placed and enables corresponding
+ * MSI-X enable register on the controller
+ * @dev : pci device corresponding to this device
+ */
 int xlp_msix_enable(struct pci_dev *dev)
 {
 	int fn = 0;
@@ -511,7 +554,7 @@ int xlp_msix_enable(struct pci_dev *dev)
 	u32 msix_ctrl;
 
 	fn = xlp_ctrl_fn_from_dev(dev);
-	xlp_pci_base = 0x18000000 + (0 << 20) + (1 << 15) + ( fn << 12);
+	xlp_pci_base = XLP_BDF_BASE(0,1,fn);
 	msix_ctrl = nlm_hal_read_32bit_reg(xlp_pci_base, 0x2C);
 	if (!(msix_ctrl & 0x80000000)) {
 		/* disable MSI and intx first */
@@ -532,7 +575,7 @@ void xlp_msi_disable(int fn, u32 bitmap)
 	u64 xlp_pci_base;
 	u32 msi_en;
 
-	xlp_pci_base = 0x18000000 + (0 << 20) + (1 << 15) + ( fn << 12);
+	xlp_pci_base = XLP_BDF_BASE(0,1,fn);
 	/*set PCIe Int Enable register */
 	msi_en = nlm_hal_read_32bit_reg(xlp_pci_base, 0x261);
 	if (msi_en & (1 << 9)) {
@@ -542,12 +585,15 @@ void xlp_msi_disable(int fn, u32 bitmap)
 	}
 }
 
+/*
+ * Disables MSI-X on a controller function
+ */
 void xlp_msix_disable(int fn)
 {
 	u64 xlp_pci_base;
 	u32 msix_ctrl;
 
-	xlp_pci_base = 0x18000000 + (0 << 20) + (1 << 15) + ( fn << 12);
+	xlp_pci_base = XLP_BDF_BASE(0,1,fn);
 	msix_ctrl = nlm_hal_read_32bit_reg(xlp_pci_base, 0x2C);
 	msix_ctrl &= ~(0x80000000);	/* MSI-X disable */
 	nlm_hal_write_32bit_reg(xlp_pci_base, 0x2C, msix_ctrl);
@@ -564,7 +610,7 @@ int is_msi_set(int fn)
 	u64 xlp_pci_base;
 	u32 msi_en, status;
 
-	xlp_pci_base = 0x18000000 + (0 << 20) + (1 << 15) + ( fn << 12);
+	xlp_pci_base = XLP_BDF_BASE(0,1,fn);
 	msi_en = nlm_hal_read_32bit_reg(xlp_pci_base, 0x261);
 	status = (msi_en >> 9) & 1 ;
 	return status;
@@ -576,7 +622,7 @@ u32 calc_msi_vector_offset(int fn)
 	u64 xlp_pci_base;
 	u32 msi_en, msi_stat;
 
-	xlp_pci_base = 0x18000000 + (0 << 20) + (1 << 15) + ( fn << 12);
+	xlp_pci_base = XLP_BDF_BASE(0,1,fn);
 	msi_en = nlm_hal_read_32bit_reg(xlp_pci_base, 0x25B);
 	msi_stat = nlm_hal_read_32bit_reg(xlp_pci_base, 0x25A);
 	nlm_hal_write_32bit_reg(xlp_pci_base, 0x25A, msi_stat);
@@ -584,13 +630,21 @@ u32 calc_msi_vector_offset(int fn)
 	return msi_stat;
 }
 
+/*
+ * Clears MSI-X status bits for a controller
+ * @fn : controller number
+ *
+ * status register is Read, Write 1 to clear.
+ * Figure out the mask (the bits corresponding to fn), read register, clear
+ * them and return the bits corresponding to fn
+ */
 u32 xlp_msix_status_clear(int fn)
 {
 	u64 xlp_pci_base;
 	u32 msix_stat;
 	u32 mask = ((XLP_MSIX_PER_SLOT - 1) << (fn * XLP_MSIX_PER_SLOT));
 
-	xlp_pci_base = 0x18000000 + (0 << 20) + (1 << 15) + ( fn << 12);
+	xlp_pci_base = XLP_BDF_BASE(0,1,fn);
 	msix_stat = nlm_hal_read_32bit_reg(xlp_pci_base, 0x25D);
 	//fdebug("mask = %#x, fn = %d, MSIX status = %#x\n", mask, fn, msix_stat);
 	msix_stat &= mask;
@@ -615,12 +669,15 @@ int xlp_msix_base_vector(struct pci_dev *dev)
 
 #endif
 
+/*
+ * Masks the bit corresponding to an MSI
+ */
 u32 xlp_msi_set_mask(int fn, int bit, int val)
 {
 	u64 xlp_pci_base;
 	u32 msi_en;
 
-	xlp_pci_base = 0x18000000 + (0 << 20) + (1 << 15) + ( fn << 12);
+	xlp_pci_base = XLP_BDF_BASE(0,1,fn);
 	msi_en = nlm_hal_read_32bit_reg(xlp_pci_base, 0x25B);
 	if (val == 0) {	/* Make the bit 0 */
 		msi_en &= ~( 1 << bit);
@@ -657,7 +714,7 @@ out:
 	if (fn >= 4) {
 		return -ENODEV;
 	}
-	xlp_pci_base = 0x18000000 + (0 << 20) + (1 << 15) + ( fn << 12);
+	xlp_pci_base = XLP_BDF_BASE(0,1,fn);
 	msi_en = 1 << bit;
 	nlm_hal_write_32bit_reg(xlp_pci_base, 0x25B, msi_en);
 	return 0;
@@ -692,13 +749,15 @@ __setup("xlp_nopci", xlp_nopci_setup);
  * 5 => cpu6-7 on all nodes; mask = 0xff000000 & online_cpu_mask on all nodes
  * 6 => cpu0-15 on all nodes; mask = 0x0000ffff & online_cpu_mask on all nodes
  * 7 => cpu15-31 on all nodes; mask = 0xffff0000 & online_cpu_mask on all nodes
+ *
+ * These are programmer defined groups and can be changed as warranted.
  */
 static struct cpumask xlp_ite_cpumask[XLP_ITE_ENTRIES];
 void xlp_pic_ite_init(void)
 {
 	int i;
 	struct cpumask m;
-	u64 xlp_pic_base = 0x18000000 + (0 << 20) + (0 << 15) + ( 4 << 12);
+	u64 xlp_pic_base = XLP_BDF_BASE(0,0,4);
 	char buf[140];
 	u64 bitmask = 0;
 
@@ -753,7 +812,6 @@ void xlp_pic_ite_init(void)
 	nlm_hal_write_64bit_reg(xlp_pic_base, 0xAC >> 1, (0x0000ffff & bitmask));
 	nlm_hal_write_64bit_reg(xlp_pic_base, 0xB0 >> 1, (0xffff0000 & bitmask));
 	/* We don't populate redirection to other nodes now */
-
 }
 
 /*
@@ -778,15 +836,14 @@ const struct cpumask *xlp_closest_match_cpumask(struct cpumask *m)
 
 /*
  * This function sets the cpumask for an interrupt vector
- *
- * m	: CPU mask resulting from xlp_closest_match_cpumask() call
+ * @m	: CPU mask resulting from xlp_closest_match_cpumask() call
  */
 void xlp_set_cpumask(const struct cpumask *m, int irt)
 {
-	int i, pic_idx = XLP_PIC_IRTREG_START;
-	u64 xlp_pic_base = 0x18000000 + (0 << 20) + (0 << 15) + ( 4 << 12);
+	int i;
+	u64 xlp_pic_base = XLP_BDF_BASE(0,0,4);
 	u64 val;
-	u32 offset = (XLP_PIC_IRTREG_START + (irt << 1) >> 1);	// Hal requires this nasty right shi(f)t
+	u32 offset = ((XLP_PIC_IRTREG_START + (irt << 1)) >> 1);	// Hal requires this nasty right shift
 
 	/* We set the following in IRT entry
 	 * 28 : clear to indicate global delivery
@@ -834,11 +891,15 @@ static int __init pcibios_init(void)
 	if (!pci_io_base) {
 		printk("[%s]: Unable to IO-Remap phys=%lx, size=%lx\n",
 		       __FUNCTION__, phys, size);
+		/* Eventually this is going to panic() */
 	}
 	else {
 		printk("[%s]: IO-Remapped phys=%lx, size=%lx to vaddr=%p\n",
 		       __FUNCTION__, phys, size, pci_io_base);
 	}
+	set_io_port_base((unsigned long) pci_io_base);
+	xlp_controller.io_map_base = (unsigned long) pci_io_base;
+	xlp_controller.io_map_base -= xlp_controller.io_offset;
 
 	/* IO Range for 16MB from where the MEM Range Ends */
 	ioport_resource.start =  0;
@@ -858,3 +919,87 @@ struct pci_fixup pcibios_fixups[] = {
 	{0}
 };
 
+
+/*
+ * some ide specific io routines on PCI
+ */
+#define pci_ide_phys_to_virt(x) (((x) - (xlp_io_resource.start)) + (unsigned long)pci_io_base )
+
+inline void nlm_ide_mm_insw(unsigned long port, void *addr, u32 count)
+{
+	unsigned long v_port = pci_ide_phys_to_virt(port);
+	while (count--) {
+		*(u16 *)addr = (readw((const volatile void *)v_port));
+		addr += 2;
+	}
+}
+
+EXPORT_SYMBOL(nlm_ide_mm_insw);
+
+inline void nlm_ide_mm_insl(unsigned long port, void *addr, unsigned int count)
+{
+	unsigned long v_port = pci_ide_phys_to_virt(port);
+	while (count--) {
+		*(u32 *)addr = readl((const volatile void *) v_port);
+		addr += 4;
+	}
+}
+EXPORT_SYMBOL(nlm_ide_mm_insl);
+
+inline void nlm_ide_mm_outsw(unsigned long port, void *addr, unsigned int count)
+{
+	unsigned long v_port = pci_ide_phys_to_virt(port);
+	while (count--) {
+		writew(*(u16 *)addr, (volatile void *)v_port);
+		addr += 2;
+	}
+}
+EXPORT_SYMBOL(nlm_ide_mm_outsw);
+
+inline void nlm_ide_mm_outsl(unsigned long port, void *addr, unsigned int count)
+{
+	unsigned long v_port = pci_ide_phys_to_virt(port);
+	while (count--) {
+		writel(*(u32 *)addr, (volatile void *)v_port);
+		addr += 4;
+	}
+}
+EXPORT_SYMBOL(nlm_ide_mm_outsl);
+
+u8 nlm_ide_mm_inb (unsigned long port)
+{
+	return((u8)readb((const volatile void *)pci_ide_phys_to_virt(port)));
+}
+
+EXPORT_SYMBOL(nlm_ide_mm_inb);
+u16 nlm_ide_mm_inw (unsigned long port)
+{
+	return ((u16) (readw((const volatile void *)pci_ide_phys_to_virt(port))));
+}
+
+EXPORT_SYMBOL(nlm_ide_mm_inw);
+/* Not part of hwif anymore; remove static declaration */
+u32 nlm_ide_mm_inl (unsigned long port)
+{
+	return ((u32)readl((const volatile void *)pci_ide_phys_to_virt(port)));
+}
+
+EXPORT_SYMBOL(nlm_ide_mm_inl);
+void nlm_ide_mm_outb (u8 value, unsigned long port)
+{
+	writeb(value, (volatile void *)pci_ide_phys_to_virt(port));
+}
+
+EXPORT_SYMBOL(nlm_ide_mm_outb);
+void nlm_ide_mm_outw (u16 value, unsigned long port)
+{
+	writew(value, (volatile void *)pci_ide_phys_to_virt((u64)port));
+}
+
+EXPORT_SYMBOL(nlm_ide_mm_outw);
+/* Not part of hwif anymore; remove static declaration */
+void nlm_ide_mm_outl (u32 value, unsigned long port)
+{
+	writel((value), (volatile void *)pci_ide_phys_to_virt(port));
+}
+EXPORT_SYMBOL(nlm_ide_mm_outl);
diff --git a/drivers/char/Kconfig b/drivers/char/Kconfig
index 0c0ef72..95e8fbc 100644
--- a/drivers/char/Kconfig
+++ b/drivers/char/Kconfig
@@ -640,6 +640,16 @@ config TILE_SROM
 	  device appear much like a simple EEPROM, and knows
 	  how to partition a single ROM for multiple purposes.
 
+config NLM_XLP_DMA
+        depends on NLM_XLP && NLM_XLP_DEVMODE
+        tristate "Enable netlogic XLP DMA driver"
+        default n
+        help
+            Netlogic XLP PCIe DMA driver. When target is device mode,
+	    this driver is required for PCI DMAs
+
+	    If you want this as a module, choose M here. The driver
+	    will be called nlm_xlp_dma.ko
 endmenu
 
 config RMICDE
diff --git a/drivers/char/Makefile b/drivers/char/Makefile
index 95fa9b0..cfe9cca 100644
--- a/drivers/char/Makefile
+++ b/drivers/char/Makefile
@@ -63,6 +63,7 @@ obj-$(CONFIG_NLMCOMMON_PCIX_GEN_DRIVER) += nlm_pcix_gen_dev.o
 obj-$(CONFIG_NLMCOMMON_PCIX_GEN_DRIVER) += nlm_pcix_gen_host.o
 obj-$(CONFIG_NLMCOMMON_CONSOLE_OVER_PCI) += xlr_pcix_console_dev.o
 obj-$(CONFIG_NLMCOMMON_CONSOLE_OVER_PCI) += xlr_pcix_console_host.o
+obj-$(CONFIG_NLM_XLP_DMA)           += nlm_xlp_dma.o
 
 obj-$(CONFIG_HANGCHECK_TIMER)	+= hangcheck-timer.o
 obj-$(CONFIG_TCG_TPM)		+= tpm/
diff --git a/drivers/char/nlm_xlp_dma.c b/drivers/char/nlm_xlp_dma.c
new file mode 100644
index 0000000..fe1cf86
--- /dev/null
+++ b/drivers/char/nlm_xlp_dma.c
@@ -0,0 +1,744 @@
+/*********************************************************************
+
+  Copyright 2003-2006 Raza Microelectronics, Inc. (RMI). All rights
+  reserved.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+  1. Redistributions of source code must retain the above copyright
+  notice, this list of conditions and the following disclaimer.
+  2. Redistributions in binary form must reproduce the above copyright
+  notice, this list of conditions and the following disclaimer in
+  the documentation and/or other materials provided with the
+  distribution.
+
+  THIS SOFTWARE IS PROVIDED BY Raza Microelectronics, Inc. ``AS IS'' AND
+  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL RMI OR CONTRIBUTORS BE LIABLE
+  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+  INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+  CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+  THE POSSIBILITY OF SUCH DAMAGE.
+
+  *****************************#RMI_2#**********************************/
+
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/smp.h>
+#include <linux/interrupt.h>
+#include <linux/spinlock.h>
+#include <linux/delay.h>
+
+#ifdef CONFIG_NLM_XLP
+#include <asm/netlogic/iomap.h>
+#include <asm/netlogic/cpumask.h>
+#include <asm/netlogic/msgring.h>
+#include <asm/netlogic/xlp_hal_pic.h>
+#include <asm/netlogic/nlm_dma.h>
+#include <hal/nlm_hal.h>
+#include <hal/nlm_hal_macros.h>
+#include <hal/nlm_hal_fmn.h>
+#include <hal/nlm_hal_xlp_dev.h>
+#endif
+
+
+#define CH0_CONTROL 8
+#define MSGRNG_CODE_DMA 8
+#define XLP_DMA_RESP_TIMEOUT 500
+#define MAX_DMA_QUEUE_LEN 256 
+#define PCIX_ACK_TIMER_VAL 0x18
+
+#define MAX_DMA_TRANS_PER_CPU 256 
+#define XLP_MAX_DMA_LEN_PER_DESC ((1 << 20) - 1) /* 1 MB - 1 */
+
+#define NEXT_SEQ_NUM(x) ((x->sequence_number + 1) & (MAX_DMA_TRANS_PER_CPU - 1))
+#define INC_SEQ_NUM(x) x->sequence_number = \
+		((x->sequence_number + 1) & (MAX_DMA_TRANS_PER_CPU - 1))
+
+#define DMA_SLOT_BUSY(x) (x->trans[x->sequence_number].pending)
+
+#define DMA_RESP_PENDING(x, seq) (x->trans[seq].pending)
+
+#define DMA_SLOT_GET(x) (x->trans[x->sequence_number].pending = 1); \
+				INC_SEQ_NUM(ctrl);
+
+#define DMA_SLOT_PUT(x, seq) (x->trans[seq].pending = 0)
+
+#define DMA_GET_RESP(x, seq) (x->trans[seq].dma_resp)
+
+#define DMA_PUT_RESP(x, seq, msg) x->trans[seq].dma_resp = msg; \
+				x->trans[seq].pending = 0;
+
+#define XLP_DMA_DONE(x, seq) (x->trans[seq].pending == 0)
+
+/* XLP DMA Global structures */
+static u64 pcie_shared_membase;
+static struct workqueue_struct *dma_wq;
+/* We use 10 bit transaction id in the DMA message to uniquely identify a DMA
+   response.
+   0-7 indicate a sequence number (0 to 255)
+   8-9 bits encode the CPU thread id (0 to 3)
+   */
+typedef struct dma_trans {
+	volatile int pending;
+	uint64_t dma_resp;
+	void (*func) (void *,uint64_t);
+	void *data;
+}dma_trans_t;
+
+typedef struct xlp_dma_ctrl {
+	spinlock_t q_lock;
+	int sequence_number;
+	dma_trans_t trans[MAX_DMA_TRANS_PER_CPU];
+}xlp_dma_ctrl_t;
+
+volatile static int xlp_dma_producer=0;
+volatile static int xlp_dma_consumer=0;
+struct msgrng_msg xlp_dma_queue[MAX_DMA_QUEUE_LEN];
+static int xlp_dma_init_done = 0;
+xlp_dma_ctrl_t xlp_dma_ctrl[32];	// XXX change to percpu
+
+spinlock_t xlp_dma_lock = SPIN_LOCK_UNLOCKED;
+spinlock_t xlp_enqueue_dma_spin=SPIN_LOCK_UNLOCKED;
+uint32_t xlp_total_dma_reqs, xlp_total_dma_bytes;
+uint32_t xlp_dma_req_failed, xlp_dma_timeout_errors, xlp_dma_errors, 
+	 xlp_dma_stale_resp;
+uint32_t xlp_dma_msg_send_failed;
+
+void xlp_async_dma_task(unsigned long data);
+#define CONFIG_PROC_FS 1
+#ifdef CONFIG_PROC_FS
+#include <linux/proc_fs.h>
+
+extern int xlp_loader_own_dma;
+extern struct proc_dir_entry proc_root;
+extern struct proc_dir_entry *nlm_root_proc;
+
+static int xlp_dma_proc_read(char *page, char **start, off_t off,
+		                        int count, int *eof, void *data)
+{
+	int len, total_len;
+	char *ptr = page;
+
+	if(count < 512) /* Need minimum of this space */
+		return -EINVAL;
+        memset(ptr, 0, count);
+
+	total_len = 0;
+	len  = sprintf(ptr, "Total DMA Requests = %d\n", xlp_total_dma_reqs);
+	ptr += len;
+	total_len += len;
+
+	len  = sprintf(ptr, "Total DMA Bytes = %d\n", xlp_total_dma_bytes);
+	ptr += len;
+	total_len += len;
+
+	len  = sprintf(ptr, "DMA Requests failed = %d\n", xlp_dma_req_failed);
+	ptr += len;
+	total_len += len;
+
+	len  = sprintf(ptr, "DMA Timeout errors = %d\n", xlp_dma_timeout_errors);
+	ptr += len;
+	total_len += len;
+
+	len  = sprintf(ptr, "DMA errors = %d\n", xlp_dma_errors);
+	ptr += len;
+	total_len += len;
+
+	len  = sprintf(ptr, "DMA Stale responses = %d\n", xlp_dma_stale_resp);
+	total_len += len;
+
+	len = sprintf(ptr,"DMA Message Send Failed = %d\n",
+			xlp_dma_msg_send_failed);
+	total_len += len;
+
+	return total_len;
+}
+
+struct proc_dir_entry *dma_procfs_dir;
+EXPORT_SYMBOL(dma_procfs_dir);
+static void xlp_init_dma_proc(void)
+{
+	struct proc_dir_entry *entry;
+
+        dma_procfs_dir = proc_mkdir("dma_debug", NULL);
+        if (dma_procfs_dir == NULL) {
+                printk(" **********  cannot create proc dir ******* \n");
+                return;
+        }
+        printk("CREATED dma_procfs_dir\n");
+
+#if 0
+	if (!(entry = create_proc_entry("xlp_dma_stats", 0444, dma_procfs_dir))) {
+		printk("%s: create_proc_entry failed\n", __FUNCTION__);
+		return;
+	}
+	entry->read_proc = xlp_dma_proc_read;
+#else
+        entry = create_proc_read_entry("xlp_dma_stats", 0444, dma_procfs_dir,
+                                        xlp_dma_proc_read, NULL);
+        if (!entry) {
+                printk("%s: create_proc_entry failed\n", __FUNCTION__);
+                return;
+        }
+#endif
+        printk("xlp_dma_stats create_proc_entry passed\n");
+
+}
+void xlp_uninit_dma_proc(void)
+{
+	remove_proc_entry("xlp_dma_stat", dma_procfs_dir);
+}
+#endif
+
+/* DMA message handler - Called from interrupt context */
+static void xlp_dma_msgring_handler(uint32_t bucket, uint32_t stid,
+		uint32_t size, uint32_t code, uint64_t msg0, uint64_t msg1,
+		uint64_t msg2, uint64_t msg3, void *data/* ignored */)
+{
+	int cpu, tx_id, seq;
+	xlp_dma_ctrl_t *ctrl;
+	unsigned long flags;
+
+	tx_id = msg1;
+	cpu = (tx_id >> 8) & 0x1f;
+	seq = (tx_id & 0xff);
+
+	ctrl = &xlp_dma_ctrl[cpu];
+
+	spin_lock_irqsave(&ctrl->q_lock, flags);
+	/* Check if there was a pending request. This can happen if the
+	   requestor times out and gives up the request. So in that case
+	   do not update the response
+	NOTE: One corner case that is not handled here is that when seq no
+	wraps around and request was pending for the new one and this response
+	was for the old request. This ideally must not happen.
+	*/
+	if(DMA_RESP_PENDING(ctrl, seq)){
+
+		DMA_PUT_RESP(ctrl, seq, msg1);
+		if(ctrl->trans[seq].func){
+			ctrl->trans[seq].func(ctrl->trans[seq].data,msg1);
+		}
+		spin_unlock_irqrestore(&ctrl->q_lock, flags);
+		return;
+	}
+	spin_unlock_irqrestore(&ctrl->q_lock, flags);
+	printk("ERROR: Stale response from DMA engine for transaction id %d\n",
+			seq);
+	spin_lock_irqsave(&ctrl->q_lock, flags);
+	xlp_dma_stale_resp++;
+	spin_unlock_irqrestore(&ctrl->q_lock, flags);
+	return;
+}
+
+#define NLM_HAL_PCI_TXRXMSG0(src, reten, ep, attr, l2alloc, rdx)\
+	((0xFFFFFFFFFFULL & (u64)src) | (((u64)(reten & 1)) << 45) |\
+	(((u64) (ep & 1))<< 44) | (((u64)(attr & 3)) << 42) |\
+	(((u64)(l2alloc & 0x1)) << 41) | (((u64)(rdx & 1)) << 40))
+
+#define NLM_HAL_PCI_TXRXMSG1(dst, len)\
+(\
+	((u64)dst & 0xFFFFFFFFFFULL) | (((u64)(len) & 0xFFFFF) << 40)\
+)
+
+#define NLM_HAL_PCI_TXRXMSG2(id)\
+(\
+	(u64)id\
+)
+
+inline void xlp_build_xfer_msg(struct msgrng_msg *msg, uint64_t src, 
+				uint64_t dest, uint32_t len, int tx_id,
+				unsigned int station)
+{
+//	msg->msg0 = (1ULL << 63) | ((uint64_t)len << 40) | 
+//				(src & 0xffffffffffULL);
+//	msg->msg1 = (1ULL << 58) | ((uint64_t)tx_id << 48) |
+//			((uint64_t)resp_bkt << 40) | (dest & 0xffffffffffull);
+
+	msg->msg0 = NLM_HAL_PCI_TXRXMSG0(src, 1, 0, 0, 0, 0);
+	msg->msg1 = NLM_HAL_PCI_TXRXMSG1(dest, (len-1));
+	msg->msg2 = NLM_HAL_PCI_TXRXMSG2(tx_id);
+	msg->msg3 = (uint64_t)station;
+}
+
+int xlp_enqueue_dma_msg(struct msgrng_msg  *msg)
+{
+	unsigned long mflags;
+	spin_lock_irqsave(&xlp_enqueue_dma_spin,mflags);
+	if(((xlp_dma_producer+1)%(MAX_DMA_QUEUE_LEN)) == xlp_dma_consumer){
+		spin_unlock_irqrestore(&xlp_enqueue_dma_spin,mflags);
+		return -ENOMEM;
+	}
+	//Enqueue Msg 
+	xlp_dma_queue[xlp_dma_producer].msg0 = msg->msg0;
+	xlp_dma_queue[xlp_dma_producer].msg1 = msg->msg1;
+	xlp_dma_queue[xlp_dma_producer].msg2 = msg->msg2;
+	xlp_dma_queue[xlp_dma_producer].msg3 = msg->msg3;
+	xlp_dma_producer = (xlp_dma_producer + 1)%(MAX_DMA_QUEUE_LEN);
+	spin_unlock_irqrestore(&xlp_enqueue_dma_spin,mflags);
+	return 0;
+}
+
+DECLARE_TASKLET(xlp_dma_task,xlp_async_dma_task , 0);
+void xlp_async_dma_task(unsigned long data)
+{
+	struct msgrng_msg *msg;
+	static int last_msg_send_success=1;
+	int msg_send_success=0;
+
+	while((xlp_dma_consumer != xlp_dma_producer)){
+		msg = &xlp_dma_queue[xlp_dma_consumer];
+		msgrng_access_enable(flags);
+		if(xlp_dma_consumer == xlp_dma_producer){
+		}
+		if(xlp_message_send((uint32_t)msg->msg3, 3, MSGRNG_CODE_DMA, 
+				    (uint64_t *)msg)) {
+			xlp_dma_msg_send_failed++;
+			msgrng_access_disable(flags);
+			last_msg_send_success = 1;
+			msg_send_success = 0;
+			tasklet_schedule(&xlp_dma_task);
+			break;
+		}
+		msg_send_success = 1;
+		last_msg_send_success = 1;
+		xlp_dma_consumer = (xlp_dma_consumer + 1) % (MAX_DMA_QUEUE_LEN);
+		msgrng_access_disable(flags);
+	}
+}
+
+/* Returns 0 on success, -1 otherwise */
+int xlp_async_request_dma(uint64_t src, uint64_t dest, uint32_t len,
+			void (*func)(void *,uint64_t),void *data,
+			enum dma_data_direction dir)
+{
+	int cpu;
+	volatile xlp_dma_ctrl_t *ctrl;
+	int tx_id, seq ;
+	struct msgrng_msg  msg;
+	unsigned long flags;
+	unsigned int station;
+
+	/* Driver does not support multiple descriptor DMA yet */
+	if(len > XLP_MAX_DMA_LEN_PER_DESC) {
+		fdebug("%s: Cannot do DMA for more than %d bytes\n",
+				__FUNCTION__, XLP_MAX_DMA_LEN_PER_DESC);
+		spin_lock_irqsave(&xlp_dma_lock, flags);
+		xlp_dma_req_failed++;
+		spin_unlock_irqrestore(&xlp_dma_lock, flags);
+		return -1;
+	}
+	if(xlp_dma_init_done == 0) {
+		fdebug("%s: XLR DMA engine is not initialized\n", __FUNCTION__);
+		spin_lock_irqsave(&xlp_dma_lock, flags);
+		xlp_dma_req_failed++;
+		spin_unlock_irqrestore(&xlp_dma_lock, flags);
+		return -1;
+	}
+
+	if (dir == DMA_TO_DEVICE)
+		station = XLP_PCIE0_RX_BUCKET;
+	else if (dir == DMA_FROM_DEVICE)
+		station = XLP_PCIE0_TX_BUCKET;
+	else {
+		printk("Invalid DMA direction specified\n");
+		spin_lock_irqsave(&xlp_dma_lock, flags);
+		xlp_dma_req_failed++;
+		spin_unlock_irqrestore(&xlp_dma_lock, flags);
+		return -1;
+	}
+
+	preempt_disable();
+	cpu = hard_smp_processor_id();
+	ctrl = (volatile xlp_dma_ctrl_t *)&xlp_dma_ctrl[cpu];
+	preempt_enable();
+	spin_lock_irqsave( (spinlock_t *)&(ctrl->q_lock),flags);
+
+	if(DMA_SLOT_BUSY(ctrl)) {
+		spin_unlock_irqrestore((spinlock_t *)&ctrl->q_lock,flags);
+		spin_lock_irqsave(&xlp_dma_lock, flags);
+		xlp_dma_req_failed++;
+		spin_unlock_irqrestore(&xlp_dma_lock, flags);
+		return -1;
+	}
+	tx_id = (cpu << 8) | ctrl->sequence_number;
+	seq = ctrl->sequence_number;
+	DMA_SLOT_GET(ctrl);
+
+	/*CallBack For Async Call.*/
+	ctrl->trans[seq].func = func;
+	ctrl->trans[seq].data = data;
+
+	spin_unlock_irqrestore((spinlock_t *)&ctrl->q_lock,flags);
+	/* Form the DMA simple xfer request and send to Channel 0 */
+
+	xlp_build_xfer_msg(&msg, src, dest, len, tx_id, station);
+
+	if(xlp_enqueue_dma_msg(&msg)){
+		spin_lock_irqsave((spinlock_t *)&ctrl->q_lock, flags);
+		DMA_SLOT_PUT(ctrl, seq);
+		spin_unlock_irqrestore((spinlock_t *)&ctrl->q_lock, flags);
+		tasklet_schedule(&xlp_dma_task);
+		spin_lock_irqsave(&xlp_dma_lock, flags);
+		xlp_dma_req_failed++;
+		spin_unlock_irqrestore(&xlp_dma_lock, flags);
+		return -1;
+	}
+	spin_lock_irqsave(&xlp_dma_lock, flags);
+	xlp_total_dma_reqs++;
+	xlp_total_dma_bytes += len;
+	spin_unlock_irqrestore(&xlp_dma_lock, flags);
+	tasklet_schedule(&xlp_dma_task);
+	return 0;
+}
+EXPORT_SYMBOL(xlp_async_request_dma);
+
+/* Returns 0 on success, -1 otherwise */
+int xlp_request_dma(uint64_t src, uint64_t dest, uint32_t len, unsigned char dir)
+{
+	int cpu, i;
+	xlp_dma_ctrl_t *ctrl;
+	int tx_id, seq;
+	struct msgrng_msg  msg = {0}, r_msg = {0};
+	unsigned long flags;
+	unsigned int station;
+
+
+	/* Driver does not support multiple descriptor DMA yet */
+	if(len > XLP_MAX_DMA_LEN_PER_DESC) {
+		printk("%s: Cannot do DMA for more than %d bytes\n",
+				__FUNCTION__, XLP_MAX_DMA_LEN_PER_DESC);
+		spin_lock_irqsave(&xlp_dma_lock, flags);
+		xlp_dma_req_failed++;
+		spin_unlock_irqrestore(&xlp_dma_lock, flags);
+		return -1;
+	}
+	if(xlp_dma_init_done == 0) {
+		printk("%s: XLR DMA engine is not initialized\n", __FUNCTION__);
+		spin_lock_irqsave(&xlp_dma_lock, flags);
+		xlp_dma_req_failed++;
+		spin_unlock_irqrestore(&xlp_dma_lock, flags);
+		return -1;
+	}
+	if (dir == DMA_TO_DEVICE)
+		station = XLP_PCIE0_RX_BUCKET;
+	else if (dir == DMA_FROM_DEVICE)
+		station = XLP_PCIE0_TX_BUCKET;
+	else {
+		printk("Invalid DMA direction specified\n");
+		return -1;
+	}
+
+	preempt_disable();
+	cpu = hard_smp_processor_id();
+	ctrl = &xlp_dma_ctrl[cpu];
+	preempt_enable();
+
+	spin_lock_irqsave(&ctrl->q_lock, flags);
+	if(DMA_SLOT_BUSY(ctrl)) {
+		printk("%s: No space to enqueue this request\n", __FUNCTION__);
+		spin_unlock_irqrestore(&ctrl->q_lock, flags);
+
+		spin_lock_irqsave(&xlp_dma_lock, flags);
+		xlp_dma_req_failed++;
+		spin_unlock_irqrestore(&xlp_dma_lock, flags);
+		return -1;
+	}
+	tx_id = (cpu << 8) | ctrl->sequence_number;
+	seq = ctrl->sequence_number;
+	DMA_SLOT_GET(ctrl);
+
+	spin_unlock_irqrestore(&ctrl->q_lock, flags);
+
+	/*Reset Callback - As this is not async call*/	
+	ctrl->trans[seq].func = NULL;
+	ctrl->trans[seq].data= NULL;
+
+	/* Form the DMA simple xfer request and send to Channel 0 */
+	xlp_build_xfer_msg(&msg, src, dest, len, tx_id, station);
+	msgrng_access_enable(flags);
+//	if(message_send_retry(2, MSGRNG_CODE_DMA, MSGRNG_STNID_DMA_0, &msg)) {
+	if (xlp_message_send(station, XLP_DMA_MSGSIZE, MSGRNG_CODE_DMA, (uint64_t *)&msg) != 0){
+		printk("Message_send failed: Cannot submit DMA request to engine\n");
+		msgrng_access_disable(flags);
+		spin_lock_irqsave(&ctrl->q_lock, flags);
+		DMA_SLOT_PUT(ctrl, seq);
+		spin_unlock_irqrestore(&ctrl->q_lock, flags);
+
+		spin_lock_irqsave(&xlp_dma_lock, flags);
+		xlp_dma_req_failed++;
+		spin_unlock_irqrestore(&xlp_dma_lock, flags);
+		return -1;
+	}
+	msgrng_access_disable(flags);
+	/* wait for the response here */
+	for(i=0; i < XLP_DMA_RESP_TIMEOUT; i++) {
+		if(XLP_DMA_DONE(ctrl, seq)) break;
+		udelay(50);
+	}
+	if(i == XLP_DMA_RESP_TIMEOUT) {
+		printk("%s:Did not get response from DMA engine\n", 
+						__FUNCTION__);
+		spin_lock_irqsave(&ctrl->q_lock, flags);
+		DMA_SLOT_PUT(ctrl, seq);
+		spin_unlock_irqrestore(&ctrl->q_lock, flags);
+
+		spin_lock_irqsave(&xlp_dma_lock, flags);
+		xlp_dma_timeout_errors++;
+		spin_unlock_irqrestore(&xlp_dma_lock, flags);
+		return -1;
+	}
+	/* Do some error checks */
+
+	r_msg.msg0 = DMA_GET_RESP(ctrl, seq);
+	if(r_msg.msg0 & 0x2) {
+		printk("%s:DMA engine reported Message load error\n", 
+				__FUNCTION__);
+		spin_lock_irqsave(&xlp_dma_lock, flags);
+		xlp_dma_errors++;
+		spin_unlock_irqrestore(&xlp_dma_lock, flags);
+		return -1;
+	}
+	spin_lock_irqsave(&xlp_dma_lock, flags);
+	xlp_total_dma_reqs++;
+	xlp_total_dma_bytes += len;
+	spin_unlock_irqrestore(&xlp_dma_lock, flags);
+	return 0;
+}
+
+extern void xlp_poll_vc0_messages (void);
+struct delayed_work msg_poll_work[NR_CPUS];
+#define NLM_DMA_MSG_POLL_DELAY 50
+static void msg_poll_func(struct work_struct *work)
+{
+        int cpu;
+
+        preempt_disable();
+        xlp_poll_vc0_messages();
+        cpu = smp_processor_id();
+        preempt_enable();
+        schedule_delayed_work_on(cpu,  &msg_poll_work[cpu], NLM_DMA_MSG_POLL_DELAY);
+}
+
+/*
+ * This function is taken from nae driver
+ */
+static void config_fmn(void)
+{
+	struct cpumask cpumask;
+
+	/* bind cpu to n0c0t0 and drain all leftover firmware messages */
+	sched_bindto_save_affinity(0, &cpumask);
+	/* Configure FMN again but only cpu credits */
+	msgrng_access_enable(mflags);
+	//nlm_xlp_msgring_int_handler(IRQ_MSGRING, NULL);
+	/* Configure credits to non-n0c0 cores */
+	printk(KERN_ERR "Potential collission. Change params of nlm_hal_fmn_init\n");
+	nlm_hal_fmn_init(0x10000000, 0x02000000, 50);
+	msgrng_access_disable(mflags);
+	sched_bindto_restore_affinity(&cpumask);
+}
+
+static int is_xlp_rc(int node, int fn)
+{
+	u32 mode = xlp_get_power_on_reset_cfg(node);
+
+	mode &= (0xF << 19);
+	mode >>= 19;
+	return (mode & (1 << fn));
+}
+
+static int nlm_init_dma(void)
+{
+	int i;
+	xlp_dma_ctrl_t *ctrl;
+
+	if(is_xlp_rc(0, 0)){ /* Only node 0, fn 0 is configured as EP */
+		printk(KERN_ERR "ERROR: Controller 0 is RC\n");
+		return -ENODEV;
+	}
+	for_each_online_cpu(i){
+		ctrl = &xlp_dma_ctrl[i];
+		spin_lock_init(&ctrl->q_lock);
+		ctrl->sequence_number = 0;
+	}
+	dma_wq = create_workqueue("xlp_dma");
+	if (dma_wq == NULL) {
+		return -EFAULT;
+	}
+	config_fmn();
+	if (register_xlp_msgring_handler(XLP_MSG_HANDLE_PCIE0,
+			xlp_dma_msgring_handler, NULL)) {
+		printk("Couldn't register DMA msgring handler\n");
+		return -1;
+	}
+	xlp_init_dma_proc();
+	xlp_dma_init_done = 1;
+	printk("Initialized XLR DMA Controller, Channel 0 \n");
+	for_each_online_cpu(i) {
+		INIT_DELAYED_WORK(&msg_poll_work[i], msg_poll_func);
+		schedule_delayed_work_on(i, &msg_poll_work[i], NLM_DMA_MSG_POLL_DELAY);
+	}
+	tasklet_schedule(&xlp_dma_task);
+	return 0;
+}
+
+/*
+ * This function sets up the memory that is exposed as PCI memory to the host
+ * PRM 22.8.7
+ */
+u64 setup_pcie_shared_memspace(u64 * len)
+{
+	u64 phys;
+	u64 xlp_pci_base = XLP_BDF_BASE(0,1,0); /* fn = 0 for EP */
+	u32 r;
+
+	if (pcie_shared_membase == 0) {
+		pcie_shared_membase = (u64) __get_free_pages(GFP_KERNEL, get_order(NLM_XLP_PCIE_SHARED_MEMSIZE));/* Reserve 32 MB */
+		if (pcie_shared_membase == 0) {
+			fdebug("Alloc failed\n");
+			return -ENOMEM;
+		}
+		phys = virt_to_phys((const volatile void *)pcie_shared_membase);
+		if (phys == 0) {
+			free_pages(pcie_shared_membase, get_order(NLM_XLP_PCIE_SHARED_MEMSIZE));
+			fdebug("virt_to_phys failed\n");
+			return -ENOMEM;
+		}
+		/* program the PCIe address map registers : 22.8.7 PRM */
+		nlm_hal_write_32bit_reg(xlp_pci_base, 0x251,
+				((phys + (0 * 8 * 1024 * 1024)) >> 23));
+		nlm_hal_write_32bit_reg(xlp_pci_base, 0x252,
+				((phys + (1 * 8 * 1024 * 1024)) >> 23));
+		nlm_hal_write_32bit_reg(xlp_pci_base, 0x253,
+				((phys + (2 * 8 * 1024 * 1024)) >> 23));
+		nlm_hal_write_32bit_reg(xlp_pci_base, 0x254,
+				((phys + (3 * 8 * 1024 * 1024)) >> 23));
+		*len = NLM_XLP_PCIE_SHARED_MEMSIZE;
+		r = nlm_hal_read_32bit_reg(xlp_pci_base, 0x240);
+		r |= 0x02000000;
+		nlm_hal_write_32bit_reg(xlp_pci_base, 0x240, r);
+	}
+	return pcie_shared_membase;
+}
+
+EXPORT_SYMBOL(setup_pcie_shared_memspace);
+
+#ifdef TEST_DMA
+void xlp_dma_test(void)
+{
+	int i;
+	uint8_t *ptr1, *ptr2;
+	unsigned long s_jiffy, e_jiffy;
+
+	ptr1 = (uint8_t *)kmalloc(0x1000, GFP_KERNEL);
+	ptr2 = (uint8_t *)kmalloc(0x1000, GFP_KERNEL);
+	if(!ptr1 || !ptr2){
+		if(ptr1)
+			kfree(ptr1);
+		if(ptr2)
+			kfree(ptr2);
+
+		printk("DMA test buffer alloc failed\n");
+		return;
+	}
+	memset(ptr1, 0xa5, 0x1000);
+	s_jiffy = read_c0_count();
+	for(i=0; i < 512; i++) {
+		xlp_request_dma((uint64_t)virt_to_phys(ptr1),
+				(uint64_t)virt_to_phys(ptr2), 0x1000);
+	}
+	e_jiffy = read_c0_count();
+	if(memcmp(ptr1, ptr2, 0x1000)) {
+		printk("DMA Data does not match. Test failed\n");
+	}else
+		printk("DMA Data Matches. Test Successful\n");
+
+	printk("Start time = %lx end time = %lx\n", s_jiffy, e_jiffy);
+	kfree(ptr1);
+	kfree(ptr2);
+}
+#endif
+EXPORT_SYMBOL(xlp_request_dma);
+
+static void nlm_exit_dma(void)
+{
+	free_pages(pcie_shared_membase, get_order(NLM_XLP_PCIE_SHARED_MEMSIZE));
+	return;
+}
+
+static inline void raise_intx(int ignore)
+{
+	/* Currently the device could be configured in INTx mode,
+	 * MSI or MSI-X mode. Find out the mode before raising int.*/
+	u64 xlp_pci_base = XLP_BDF_BASE(0,1,0); /* fn = 0 for EP */
+	u32 r;
+
+	/* We dont' have to read reg1 to check if INTx are disabled.
+	 * PCIe core would take care of this (I hope) */
+	r = nlm_hal_read_32bit_reg(xlp_pci_base, 0x240);
+	r |= (1 << 22);
+	nlm_hal_write_32bit_reg(xlp_pci_base, 0x240, r);
+}
+
+static inline void raise_msix(int vec)
+{
+	u64 xlp_pci_base = XLP_BDF_BASE(0,1,0); /* fn = 0 for EP */
+	u32 r;
+
+	r = ((1 << 5) | vec);
+	nlm_hal_write_32bit_reg(xlp_pci_base, 0x25c, r);
+}
+
+static inline void raise_msi(int vec)
+{
+	u64 xlp_pci_base = XLP_BDF_BASE(0,1,0); /* fn = 0 for EP */
+	u32 r;
+
+	r = ((1 << 5) | vec);
+	nlm_hal_write_32bit_reg(xlp_pci_base, 0x258, r);
+	while (nlm_hal_read_32bit_reg(xlp_pci_base, 0x259) != 1){
+		if (printk_ratelimit()){
+			fdebug("Still looping\n");
+		}
+	}
+	nlm_hal_write_32bit_reg(xlp_pci_base, 0x259, 1);
+}
+
+void raise_host_interrupt(int vec)
+{
+	int msi, msix;
+	u64 xlp_pci_base = XLP_BDF_BASE(0,1,0); /* fn = 0 for EP */
+
+	msix = nlm_hal_read_32bit_reg(xlp_pci_base, 0x2C);
+	msix = (msix >> 31) & 1;
+	/* We don't care if fmask == 1 or not */
+	if (msix != 0) {
+		raise_msix(vec);
+		return;
+	}
+
+	msi = (nlm_hal_read_32bit_reg(xlp_pci_base, 0x14) >> 16) & 1;
+	if (msi != 0) {
+		raise_msi(vec);
+		return;
+	}
+
+	/* if MSI and MSI-X are not enabled, raise INTx, even if it is disabled
+	 * in reg1, bit 10. We cannot distinguish between intX not enabled and
+	 * intX disabled on purpose (e.g NAPI) */
+	raise_intx(vec);
+	return;
+}
+EXPORT_SYMBOL(raise_host_interrupt);
+
+module_init(nlm_init_dma);
+module_exit(nlm_exit_dma);
+MODULE_LICENSE("GPL");
diff --git a/drivers/i2c/busses/i2c-xlp.c b/drivers/i2c/busses/i2c-xlp.c
index 9fd2cfa..ece53c4 100644
--- a/drivers/i2c/busses/i2c-xlp.c
+++ b/drivers/i2c/busses/i2c-xlp.c
@@ -54,10 +54,10 @@
         #define I2C_COMMAND_RDNACK       0x28
         #define I2C_COMMAND_IACK         0x01    /* Not used */
 #define I2C_STATUS                       4               /* Same as 'command' */
-        #define I2C_STATUS_NACK          0x80    /* Did not get an ACK */                                
-        #define I2C_STATUS_BUSY          0x40                            
-        #define I2C_STATUS_AL            0x20    /* Arbitration Lost */                          
-        #define I2C_STATUS_TIP           0x02    /* Transfer in Progress  */                             
+        #define I2C_STATUS_NACK          0x80    /* Did not get an ACK */
+        #define I2C_STATUS_BUSY          0x40
+        #define I2C_STATUS_AL            0x20    /* Arbitration Lost */
+        #define I2C_STATUS_TIP           0x02    /* Transfer in Progress  */
         #define I2C_STATUS_IF            0x01    /* Intr. Pending Flag, not used */
 #define I2C_WRITE_BIT			0x00
 #define I2C_READ_BIT			0x01
@@ -326,7 +326,7 @@ xlp_i2c_xfer(struct i2c_adapter *i2c_adap, struct i2c_msg *msgs, int num)
 		len = p->len;
 	}
 	else {
-		printk("%s ERR: msg num =%d large than 2\n");
+		printk("ERR: msg num =%d large than 2\n", num);
 		return -1;
 	}
 	if (p->flags & I2C_M_RD)
-- 
1.7.0.4

