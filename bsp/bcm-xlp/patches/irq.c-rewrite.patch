From db00494ea5d261f202c58c92fad619635208a59f Mon Sep 17 00:00:00 2001
From: Zi Shen Lim <zlim@netlogicmicro.com>
Date: Thu, 3 Nov 2011 14:36:44 -0700
Subject: [PATCH 295/565] irq.c rewrite

irq.c rewrite

Based on Broadcom SDK 2.3.

Signed-off-by: Zi Shen Lim <zlim@netlogicmicro.com>
Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 arch/mips/include/asm/netlogic/xlp_irq.h |   20 +-
 arch/mips/netlogic/xlp/irq.c             | 1631 +++---------------------------
 arch/mips/netlogic/xlp/on_chip.c         |   35 +-
 arch/mips/netlogic/xlp/platform.c        |    4 +-
 arch/mips/netlogic/xlp/smp.c             |    2 -
 drivers/usb/host/ehci-pci.c              |    3 +-
 6 files changed, 175 insertions(+), 1520 deletions(-)

diff --git a/arch/mips/include/asm/netlogic/xlp_irq.h b/arch/mips/include/asm/netlogic/xlp_irq.h
index 42684ad..d759b9b 100644
--- a/arch/mips/include/asm/netlogic/xlp_irq.h
+++ b/arch/mips/include/asm/netlogic/xlp_irq.h
@@ -34,8 +34,6 @@ THE POSSIBILITY OF SUCH DAMAGE.
 #define NR_IRQS			384
 /* Maximum IRQ vector numbers supported by MIPS */
 #define XLP_EIRR_SIZE		64
-#define XLP_IRT_NUM	160
-#define XLP_IRQ_MAX	168	/* 0-7 are reserved + 160 IRT entries */
 
 /* The following interrupt assignments (0-7) are special.
  * I need to find out what governs these assignments
@@ -51,6 +49,10 @@ THE POSSIBILITY OF SUCH DAMAGE.
 #define XLP_IRQ_MSGRING              6
 #define XLP_IRQ_TIMER                7
 #define XLP_IRQ_RESERVED_MAX		8
+#define XLP_PIC_IRQ_BASE		XLP_IRQ_RESERVED_MAX
+
+#define xlp_pic_irt_to_irq(irt) ((irt) + XLP_PIC_IRQ_BASE)
+#define xlp_pic_irq_to_irt(irq) ((irq) - XLP_PIC_IRQ_BASE)
 
 #define XLP_IRQ_IPI_SMP_KGDB	     50
 #define XLP_IRQ_IPI_OPROFILE         51
@@ -82,10 +84,7 @@ THE POSSIBILITY OF SUCH DAMAGE.
 /*
  *    IRT Map
  */
-
-#define arch_setup_msi_irqs	arch_setup_msi_irqs /* defines arch. specific msi setup function */
-#define xlp_irq_to_irt(x)	((x) - XLP_IRQ_RESERVED_MAX)
-#define xlp_irt_to_irq(x)	((x) + XLP_IRQ_RESERVED_MAX)
+#define XLP_PIC_NUM_IRTS	160
 
 #define XLP_WD_BASE			(0 + XLP_IRQ_RESERVED_MAX)
 #define XLP_WD_IRQ_IRQ(x) (XLP_WD_BASE + (x))
@@ -138,8 +137,13 @@ THE POSSIBILITY OF SUCH DAMAGE.
 #define	XLP_KBP_IRT_OFFSET			(132 + XLP_IRQ_RESERVED_MAX)
 #define XLP_KBP_IRQ(x)	(XLP_KBP_IRT_OFFSET + (x))
 
-#define XLP_UART_IRT_OFFSET			(133 + XLP_IRQ_RESERVED_MAX)
-#define XLP_UART_IRQ(x)	(XLP_UART_IRT_OFFSET + (x))
+#define XLP_PIC_IRT_UART_0		133
+#define XLP_PIC_IRT_UART_1		134
+#define XLP_PIC_IRT_UART(x)		((x) + XLP_PIC_IRT_UART_0)
+
+#define XLP_PIC_IRQ_UART_0		(XLP_PIC_IRT_UART_0 + XLP_PIC_IRQ_BASE)
+#define XLP_PIC_IRQ_UART_1		(XLP_PIC_IRT_UART_1 + XLP_PIC_IRQ_BASE)
+#define XLP_PIC_IRQ_UART(x)		(XLP_PIC_IRT_UART(x) + XLP_PIC_IRQ_BASE)
 
 #define XLP_I2C_IRT_OFFSET			(135 + XLP_IRQ_RESERVED_MAX)
 #define XLP_I2C_IRQ(x)	(XLP_I2C_IRT_OFFSET + (x))
diff --git a/arch/mips/netlogic/xlp/irq.c b/arch/mips/netlogic/xlp/irq.c
index 8d7b74a..e716f82 100644
--- a/arch/mips/netlogic/xlp/irq.c
+++ b/arch/mips/netlogic/xlp/irq.c
@@ -30,1598 +30,281 @@ THE POSSIBILITY OF SUCH DAMAGE.
 #include <linux/spinlock.h>
 #include <linux/mm.h>
 #include <linux/slab.h>
-#include <linux/pci.h>
-#include <linux/msi.h>
-#include <linux/export.h>
+#include <linux/irq.h>
+
 #include <asm/errno.h>
 #include <asm/signal.h>
 #include <asm/ptrace.h>
-#include <asm/kgdb.h>
 #include <asm/mipsregs.h>
+#include <asm/thread_info.h>
 
+#include <asm/netlogic/mips-exts.h>
 #include <asm/netlogic/xlp_hal_pic.h>
 #include <asm/netlogic/xlp.h>
 #include <asm/netlogic/xlp_irq.h>
-#include <asm/netlogic/msidef.h>
-#include <asm/netlogic/mips-exts.h>
 #include <asm/netlogic/pic.h>
 #include <asm/netlogic/debug.h>
-#include <asm/thread_info.h>
-
-/* About this file: irq.c
- * This file contains routines that handle all the low level interrupt stuff.
- * Some of the platform specific portions are moved to arch/mips/pci/pci-xlp.c
- * Actions handled here are: initialization of the interrupt map, requesting of
- * interrupt lines by handlers, dispatching interrupts to handlers, probing
- * for interrupt lines..etc.
- */
 
-/* Externs */
-extern void nlm_common_timer_interrupt(struct pt_regs *, int);
-extern void nlm_xlp_msgring_int_handler(int , struct pt_regs *);
-extern int xlp_ctrl_fn_from_dev(const struct pci_dev *);
-extern const struct cpumask *xlp_closest_match_cpumask(struct cpumask *);
-extern void xlp_intx_enable(int);
-extern void xlp_intx_disable(int);
-extern void xlp_set_cpumask(const struct cpumask *m, int irt);
-#if defined CONFIG_PCI_MSI
-extern int xlp_msi_status_clear(struct pci_dev *, int);
-extern int xlp_msi_enable(struct pci_dev *, u32);
-extern int xlp_msi_base_vector(struct pci_dev *);
-extern int is_msi_set(int);
-extern int calc_msi_vector_offset(int);
-extern void xlp_msi_disable(int, int);
-extern u32 xlp_msi_set_mask(int, int, int);
-volatile const void *xlp_msix_addr_start(int);
-volatile const void *xlp_msi_addr_start(int);
-extern u32 xlp_msix_status_clear(int);
-extern u32 xlp_msix_set_mask(int, int, int);
-extern int xlp_msix_enable(struct pci_dev *);
-extern void xlp_msix_disable(int);
-void mask_msi_irq(unsigned int);
-void unmask_msi_irq(unsigned int);
-#endif
-
-/* own variables */
-
-/* xlp_irq_mask is retained for legacy. It can be removed at a later point of
- * time. Initially it was meant to keep a copy of present interrupt mask; with
- * multi cpus each having its own mask register, we might not need this variable
- */
-static volatile uint64_t xlp_irq_mask;
-
-/* spin lock for all interrupt related data structures
- * This variable is used in timer init, so we export it
- */
 DEFINE_SPINLOCK(xlp_pic_lock);
-EXPORT_SYMBOL(xlp_pic_lock);
-#if defined CONFIG_PCI_MSI
-/*
- * This bitmap keeps track of the MSI vectors allocated from
- * XLP_MSIX_IRQ_START(x)
- */
-struct msix_alloc_bitmap {
-	u64 bitmap;	/* Can be any data structure to keep bits */
-	u32 count;	/* #of bits set at any point of time */
-};
-static struct msix_alloc_bitmap msix_vec[XLP_MAX_SLOTS];
-#endif
 
-/*
- * There are two data structures pivotal for interrupt delivery mechanism
- * irq_map[] and rvec_map[]
- * irq_map[] : An array of struct irq_map_elem.
- * Number of elements in this array is equal to NR_IRQ => there should be an
- * entry corresponding to each interrupt in the system.
- * Initial 8 elements (0-XLP_IRQ_RESERVED_MAX) are unpopulated. They are
- * reserved for system internal interrupts which are not explicitly handled
- * by plat_irq_dispatch; the handlers for these interrupts are called
- * differently.
- *
- * All other entries are handled by plat_irq_dispatch()
- * An entry would contain the rvec entry for this interrupt. The offset of this
- * entry would be presented as the interrupt number for any requests
- * E.g, for UART1, index is 141. This is the value of uart1 interrupt.
- * asm/netlogic/xlp_irq.h defines this value as XLP_UART_IRQ(1)
- *
- * Each irq_map_elem has two members : rvec -> the rvec entry for this entry,
- * usage : the number of successful irq_request() called on this IRQ.
- *
- * rvec_map is meant to map rvec numbers back to Interrupts.
- * RVEC is just a number for s/w; which is the bit offset that is set in EIRR
- * Refer PRM : 9.3 for details.
- *
- * irq_map : {<IERR RVEC>, <#of s/w vector multiplexed on this RVEC> }
- * Some RVECs are reserved : so use only 9 through 63
- * Any irt index can be derived from irq using the macros
- * xlp_irt_to_irq() or xlp_irq_to_irt()
- * These macros are required because of the imposed 64 bits (if RVEC)
- * to 160 entries (size of IRT table)
- *
- * It is further complicated by the fact that PCI LINK interrupts are
- * multiplexed with MSI interrupts. In that mode, each PCI Link interrupts
- * can be caused by 32 MSI interrupts. That means, we need a meachinsm to map
- * 32 msi interrupts * 4 pci slots (128 interrupts) to 4 possible RVECs.
- * the irq_map table serves this purpose as well as follows
- *
- * We limit the per pci slot interrupt (for the time being) to XLP_MSI_PER_SLOT
- * (currently 8). This is done to keep total number of interrupts to NR_IRQ;
- *
- */
-struct irq_map_elem {
-	int rvec;
-	int usage;
-};
+static uint64_t xlp_irq_mask;
+static int xlp_rvec_to_irq_map[XLP_EIRR_SIZE];
+static int xlp_irq_to_rvec_map[NR_IRQS];
 
-static struct irq_map_elem irq_map[NR_IRQS] = {
-	{0, 0},	/* Dummy			:	0 */
-	{0, 0},	/* Dummy			:	1 */
-	{0, 0},	/* Dummy			:	2 */
-	{0, 0},	/* Dummy			:	3 */
-	{0, 0},	/* Dummy			:	4 */
-	{0, 0},	/* Dummy			:	5 */
-	{0, 0},	/* Dummy			:	6 */
-	{0, 0},	/* Dummy			:	7 */
-        {9, 0}, /*XLP_WD_IDX(0)			:	8 */
-        {9, 0}, /*XLP_WD_IDX(1)			:	9 */
-        {19, 0}, /*XLP_WD_NMI_IDX(0)		:	10 */
-        {19, 0}, /*XLP_WD_NMI_IDX(1)		:	11 */
-        {10, 0}, /*XLP_TIMER_IDX(0)		:	12 */
-        {10, 0}, /*XLP_TIMER_IDX(1)		:	13 */
-        {10, 0}, /*XLP_TIMER_IDX(2)		:	14 */
-        {10, 0}, /*XLP_TIMER_IDX(3)		:	15 */
-        {10, 0}, /*XLP_TIMER_IDX(4)		:	16 */
-        {10, 0}, /*XLP_TIMER_IDX(5)		:	17 */
-        {10, 0}, /*XLP_TIMER_IDX(6)		:	18 */
-        {10, 0}, /*XLP_TIMER_IDX(7)		:	19 */
-        {59, 0}, /*XLP_MSGQ_IDX(0)		:	20 */
-        {59, 0}, /*XLP_MSGQ_IDX(1)		:	21 */
-        {59, 0}, /*XLP_MSGQ_IDX(2)		:	22 */
-        {59, 0}, /*XLP_MSGQ_IDX(3)		:	23 */
-        {59, 0}, /*XLP_MSGQ_IDX(4)		:	24 */
-        {59, 0}, /*XLP_MSGQ_IDX(5)		:	25 */
-        {59, 0}, /*XLP_MSGQ_IDX(6)		:	26 */
-        {59, 0}, /*XLP_MSGQ_IDX(7)		:	27 */
-        {59, 0}, /*XLP_MSGQ_IDX(8)		:	28 */
-        {59, 0}, /*XLP_MSGQ_IDX(9)		:	29 */
-        {59, 0}, /*XLP_MSGQ_IDX(10)		:	30 */
-        {59, 0}, /*XLP_MSGQ_IDX(11)		:	31 */
-        {59, 0}, /*XLP_MSGQ_IDX(12)		:	32 */
-        {59, 0}, /*XLP_MSGQ_IDX(13)		:	33 */
-        {59, 0}, /*XLP_MSGQ_IDX(14)		:	34 */
-        {59, 0}, /*XLP_MSGQ_IDX(15)		:	35 */
-        {59, 0}, /*XLP_MSGQ_IDX(16)		:	36 */
-        {59, 0}, /*XLP_MSGQ_IDX(17)		:	37 */
-        {59, 0}, /*XLP_MSGQ_IDX(18)		:	38 */
-        {59, 0}, /*XLP_MSGQ_IDX(19)		:	39 */
-        {59, 0}, /*XLP_MSGQ_IDX(20)		:	40 */
-        {59, 0}, /*XLP_MSGQ_IDX(21)		:	41 */
-        {59, 0}, /*XLP_MSGQ_IDX(22)		:	42 */
-        {59, 0}, /*XLP_MSGQ_IDX(23)		:	43 */
-        {59, 0}, /*XLP_MSGQ_IDX(24)		:	44 */
-        {59, 0}, /*XLP_MSGQ_IDX(25)		:	45 */
-        {59, 0}, /*XLP_MSGQ_IDX(26)		:	46 */
-        {59, 0}, /*XLP_MSGQ_IDX(27)		:	47 */
-        {59, 0}, /*XLP_MSGQ_IDX(28)		:	48 */
-        {59, 0}, /*XLP_MSGQ_IDX(29)		:	49 */
-        {59, 0}, /*XLP_MSGQ_IDX(30)		:	50 */
-        {59, 0}, /*XLP_MSGQ_IDX(31)		:	51 */
-        {49, 0}, /*XLP_MSG_IDX(0)		:	52 */
-        {48, 0}, /*XLP_MSG_IDX(1)		:	53 */
-        {32, 0}, /*XLP_PCIE_MSIX_IDX(0)		:	54 */
-        {32, 0}, /*XLP_PCIE_MSIX_IDX(1)		:	55 */
-        {32, 0}, /*XLP_PCIE_MSIX_IDX(2)		:	56 */
-        {32, 0}, /*XLP_PCIE_MSIX_IDX(3)		:	57 */
-        {32, 0}, /*XLP_PCIE_MSIX_IDX(4)		:	58 */
-        {32, 0}, /*XLP_PCIE_MSIX_IDX(5)		:	59 */
-        {32, 0}, /*XLP_PCIE_MSIX_IDX(6)		:	60 */
-        {32, 0}, /*XLP_PCIE_MSIX_IDX(7)		:	61 */
-        {33, 0}, /*XLP_PCIE_MSIX_IDX(8)		:	62 */
-        {33, 0}, /*XLP_PCIE_MSIX_IDX(9)		:	63 */
-        {33, 0}, /*XLP_PCIE_MSIX_IDX(10)	:	64 */
-        {33, 0}, /*XLP_PCIE_MSIX_IDX(11)	:	65 */
-        {33, 0}, /*XLP_PCIE_MSIX_IDX(12)	:	66 */
-        {33, 0}, /*XLP_PCIE_MSIX_IDX(13)	:	67 */
-        {33, 0}, /*XLP_PCIE_MSIX_IDX(14)	:	68 */
-        {33, 0}, /*XLP_PCIE_MSIX_IDX(15)	:	69 */
-        {34, 0}, /*XLP_PCIE_MSIX_IDX(16)	:	70 */
-        {34, 0}, /*XLP_PCIE_MSIX_IDX(17)	:	71 */
-        {34, 0}, /*XLP_PCIE_MSIX_IDX(18)	:	72 */
-        {34, 0}, /*XLP_PCIE_MSIX_IDX(19)	:	73 */
-        {34, 0}, /*XLP_PCIE_MSIX_IDX(20)	:	74 */
-        {34, 0}, /*XLP_PCIE_MSIX_IDX(21)	:	75 */
-        {34, 0}, /*XLP_PCIE_MSIX_IDX(22)	:	76 */
-        {34, 0}, /*XLP_PCIE_MSIX_IDX(23)	:	77 */
-        {35, 0}, /*XLP_PCIE_MSIX_IDX(24)	:	78 */
-        {35, 0}, /*XLP_PCIE_MSIX_IDX(25)	:	79 */
-        {35, 0}, /*XLP_PCIE_MSIX_IDX(26)	:	80 */
-        {35, 0}, /*XLP_PCIE_MSIX_IDX(27)	:	81 */
-        {35, 0}, /*XLP_PCIE_MSIX_IDX(28)	:	82 */
-        {35, 0}, /*XLP_PCIE_MSIX_IDX(29)	:	83 */
-        {35, 0}, /*XLP_PCIE_MSIX_IDX(30)	:	84 */
-        {35, 0}, /*XLP_PCIE_MSIX_IDX(31)	:	85 */
-        {41, 0}, /*XLP_PCIE_LINK_IRQ(0)		:	86 */
-        {42, 0}, /*XLP_PCIE_LINK_IRQ(1)		:	87 */
-        {43, 0}, /*XLP_PCIE_LINK_IRQ(2)		:	88 */
-        {44, 0}, /*XLP_PCIE_LINK_IRQ(3)		:	89 */
-        {58, 0}, /*XLP_NAE_IDX(0)		:	90 */
-        {58, 0}, /*XLP_NAE_IDX(1)		:	91 */
-        {58, 0}, /*XLP_NAE_IDX(2)		:	92 */
-        {58, 0}, /*XLP_NAE_IDX(3)		:	93 */
-        {58, 0}, /*XLP_NAE_IDX(4)		:	94 */
-        {58, 0}, /*XLP_NAE_IDX(5)		:	95 */
-        {58, 0}, /*XLP_NAE_IDX(6)		:	96 */
-        {58, 0}, /*XLP_NAE_IDX(7)		:	97 */
-        {58, 0}, /*XLP_NAE_IDX(8)		:	98 */
-        {58, 0}, /*XLP_NAE_IDX(9)		:	99 */
-        {58, 0}, /*XLP_NAE_IDX(10)		:	100 */
-        {58, 0}, /*XLP_NAE_IDX(11)		:	101 */
-        {58, 0}, /*XLP_NAE_IDX(12)		:	102 */
-        {58, 0}, /*XLP_NAE_IDX(13)		:	103 */
-        {58, 0}, /*XLP_NAE_IDX(14)		:	104 */
-        {58, 0}, /*XLP_NAE_IDX(15)		:	105 */
-        {58, 0}, /*XLP_NAE_IDX(16)		:	106 */
-        {58, 0}, /*XLP_NAE_IDX(17)		:	107 */
-        {58, 0}, /*XLP_NAE_IDX(18)		:	108 */
-        {58, 0}, /*XLP_NAE_IDX(19)		:	109 */
-        {58, 0}, /*XLP_NAE_IDX(20)		:	110 */
-        {58, 0}, /*XLP_NAE_IDX(21)		:	111 */
-        {58, 0}, /*XLP_NAE_IDX(22)		:	112 */
-        {58, 0}, /*XLP_NAE_IDX(23)		:	113 */
-        {58, 0}, /*XLP_NAE_IDX(24)		:	114 */
-        {58, 0}, /*XLP_NAE_IDX(25)		:	115 */
-        {58, 0}, /*XLP_NAE_IDX(26)		:	116 */
-        {58, 0}, /*XLP_NAE_IDX(27)		:	117 */
-        {58, 0}, /*XLP_NAE_IDX(28)		:	118 */
-        {58, 0}, /*XLP_NAE_IDX(29)		:	119 */
-        {58, 0}, /*XLP_NAE_IDX(30)		:	120 */
-        {58, 0}, /*XLP_NAE_IDX(31)		:	121 */
-        {60, 0}, /*XLP_POE_IDX			:	122 */
-        {24, 0}, /*XLP_USB_IDX(0)		:	123 */
-        {24, 0}, /*XLP_USB_IDX(1)		:	124 */
-        {24, 0}, /*XLP_USB_IDX(2)		:	125 */
-        {25, 0}, /*XLP_USB_IDX(3)		:	126 */
-        {25, 0}, /*XLP_USB_IDX(4)		:	127 */
-        {25, 0}, /*XLP_USB_IDX(5)		:	128 */
-        {61, 0}, /*XLP_GDX_IDX			:	129 */
-        {63, 0}, /*XLP_SEC_IDX			:	130 */
-        {62, 0}, /*XLP_RSA_IDX			:	131 */
-        {39, 0}, /*XLP_COMP_IDX(0)		:	132 */
-        {39, 0}, /*XLP_COMP_IDX(1)		:	133 */
-        {39, 0}, /*XLP_COMP_IDX(2)		:	134 */
-        {39, 0}, /*XLP_COMP_IDX(3)		:	135 */
-        {0, 0}, /*RESERVED_IDX			:	136 */
-        {37, 0}, /*XLP_ICC_IDX(0)		:	137  ICC - Inter Chip Coherency*/
-        {37, 0}, /*XLP_ICC_IDX(1)		:	138 */
-        {37, 0}, /*XLP_ICC_IDX(2)		:	139 */
-        {36, 0}, /*XLP_CAM_IDX			:	140 */
-        {17, 0}, /*XLP_UART_IDX(0)		:	141 */
-        {18, 0}, /*XLP_UART_IDX(0)		:	142 */
-        {11, 0}, /*XLP_I2C_IDX(0)		:	143 */
-        {11, 0}, /*XLP_I2C_IDX(0)		:	144 */
-        {12, 0}, /*XLP_SYS_IDX(0)		:	145 */
-        {12, 0}, /*XLP_SYS_IDX(1)		:	146 */
-        {55, 0}, /*XLP_JTAG_IDX			:	147 */
-        {50, 0}, /*XLP_PIC_IDX			:	148 */
-        {54, 0}, /*XLP_NBU_IDX			:	149 */
-        {53, 0}, /*XLP_TCU_IDX			:	150 */
-        {52, 0}, /*XLP_GCU_IDX			:	151  GBC - Global Coherency*/
-        {38, 0}, /*XLP_DMC_IDX			:	152 */	/* collision */
-        {38, 0}, /*XLP_DMC_IDX			:	153 */
-        {13, 0}, /*XLP_GPIO_IDX(0)		:	154 */
-        {14, 0}, /*XLP_GPIO_IDX(1)		:	155 */
-        {15, 0}, /*XLP_GPIO_IDX(2)		:	156 */
-        {16, 0}, /*XLP_GPIO_IDX(3)		:	157 */
-        {20, 0}, /*XLP_NOR_IDX			:	158 */
-        {21, 0}, /*XLP_NAND_IDX			:	159 */
-        {22, 0}, /*XLP_SPI_IDX			:	160 */
-        {23, 0}, /*XLP_MMC_IDX			:	161 */
-        {0, 0}, /*			    162 */
-        {0, 0}, /*                          163 */
-        {0, 0}, /*                          164 */
-        {0, 0}, /*                          165 */
-        {0, 0}, /*                          166 */
-        {0, 0}, /*                          167 */
-};
-
-/*
- * When startup function is called on an IRQ, that IRT's rvec map's bitmap
- * would be set. This serves as a quick reverse lookup at the time of dispatch
- */
-struct rvec_map_elem {
-	/* irt = elem.irt + ffs(bitmap), where bitmap != 0 */
-	int irt;	/* The first IRT corresponding to this rvec */
-	volatile unsigned long bitmap;	/* bit set is the offset from irt */
-};
-static struct rvec_map_elem rvec_map[XLP_EIRR_SIZE] = {
-	{-1, 0},			/* 0 */
-	{-1, 0},			/* 1 */
-	{-1, 0},			/* 2 */
-	{-1, 0},			/* 3 */
-	{-1, 0},			/* 4 */
-	{-1, 0},			/* 5 */
-	{-1, 0},			/* 6 */
-	{-1, 0},			/* 7 */
-	{-1, 0},			/* 8 */
-	{0, 0},				/* 9  Watchdog timer */
-	{4, 0},				/* 10 PIC Timter */
-	{135, 0},			/* 11 */
-	{137, 0},			/* 12 */
-	{146, 0},			/* 13 */
-	{147, 0},			/* 14 */
-	{148, 0},			/* 15 */
-	{149, 0},			/* 16 */
-	{133, 0},			/* 17 */
-	{134, 0},			/* 18 */
-	{2, 0},				/* 19 , Watchdog NMI */
-	{150, 0},			/* 20 */
-	{151, 0},			/* 21 */
-	{152, 0},			/* 22 */
-	{153, 0},			/* 23 */
-	{115, 0},			/* 24 */
-	{118, 0},			/* 25 */
-	{-1, 0},			/* 26 */
-	{-1, 0},			/* 27 */
-	{-1, 0},			/* 28 */
-	{-1, 0},			/* 29 */
-	{-1, 0},			/* 30 */
-	{-1, 0},			/* 31 */
-	{46, 0},			/* 32  MSIX - FN(0)*/
-	{54, 0},			/* 33  MSIX - FN(1)*/
-	{62, 0},			/* 34  MSIX - FN(2)*/
-	{70, 0},			/* 35  MSIX - FN(3)*/
-	{132, 0},			/* 36 */
-	{129, 0},			/* 37 */
-	{144, 0},			/* 38 */
-	{124, 0},			/* 39 */
-	{-1, 0},			/* 40 */
-	{78, 0},			/* 41 */
-	{79, 0},			/* 42 */
-	{80, 0},			/* 43 */
-	{81, 0},			/* 44 */
-	{-1, 0},			/* 45 */
-	{-1, 0},			/* 46 */
-	{-1, 0},			/* 47 */
-	{45, 0},			/* 48 */
-	{44, 0},			/* 49 XLP_MSG_IDX*/
-	{140, 0},			/* 50 */
-	{-1, 0},			/* 51 */
-	{143, 0},			/* 52 */
-	{142, 0},			/* 53 */
-	{141, 0},			/* 54 */
-	{139, 0},			/* 55 */
-	{-1, 0},			/* 56 */
-	{-1, 0},			/* 57 */
-	{82, 0},			/* 58 */
-	{12, 0},			/* 59 , MSGQ*/
-	{114, 0},			/* 60 */
-	{121, 0},			/* 61 */
-	{123, 0},			/* 62 */
-	{122, 0},			/* 63 */
-};
-
-/*
- * Set some eimr bits on each cpu
- * This function will be called on each cpu by on_each_cpu()
- * @bitmask	: bitmask to set in EIMR
- */
-void xlp_set_eimr(void *param)
+static void set_irq_mapping(int irq, int rvec)
 {
-	u64 bitmask = (u64) param;
-	u64 eimr;
+	BUG_ON((irq < 0) || (irq >= NR_IRQS));
+	BUG_ON((rvec < 0) || (rvec >= XLP_EIRR_SIZE));
 
-	eimr = read_64bit_cp0_eimr();
-	eimr |= bitmask;
-	write_64bit_cp0_eimr(eimr);
-	return;
-}
+	xlp_irq_to_rvec_map[irq] = rvec;
+	xlp_rvec_to_irq_map[rvec] = irq;
 
-/*
- * Clear some eimr bits on each cpu
- * This function will be called on each cpu by on_each_cpu()
- * @bitmask	: bitmask to clear in EIMR
- */
-void xlp_clear_eimr(void *param)
-{
-	u64 bitmask = (u64) param;
-	u64 eimr = read_64bit_cp0_eimr();
-	eimr &= ~bitmask;
-	write_64bit_cp0_eimr(eimr);
-	return;
+	/* TODO: handle shared rvec case */
 }
 
-void __xlp_setup_one_irq(u64 irq)
+static void set_pic_irq_mapping(int irq, int rvec)
 {
-	int cpu;
-	preempt_disable();
-	cpu = smp_processor_id();
-	__nlm_hal_set_irt_to_cpu(xlp_irq_to_irt(irq), cpu);
-	preempt_enable();
-}
+	BUG_ON((rvec < XLP_IRQ_RESERVED_MAX) || (irq < XLP_PIC_IRQ_BASE));
 
-/*
- * Returns the base IRQ (index of irq_map) from an rvec
- */
-static inline int __irqbase_from_rvec(int rvec)
-{
-	int irt;
-
-	irt = rvec_map[rvec].irt;
-	if (irt < 0) {
-		return -EINVAL;
-	}
-	return(irt + XLP_IRQ_RESERVED_MAX);
-}
-
-
-/*
- * Returns the base IRQ from an rvec
- */
-static inline int irqbase_from_rvec(int rvec)
-{
-	int ret;
-	unsigned long flags;
-
-	spin_lock_irqsave(&xlp_pic_lock, flags);
-	ret = __irqbase_from_rvec(rvec);
-	spin_unlock_irqrestore(&xlp_pic_lock, flags);
-	return ret;
-}
-
-/*
- * returns rvec from an IRQ entry
- * An IRQ entry is (irt table index + reserved max)
- *
- * @irq : irq number
- */
-int xlp_rvec_from_irq(int irq)
-{
-	if ((irq < XLP_IRQ_RESERVED_MAX) || (irq >= XLP_IRQ_MAX)) {
-		return -EINVAL;
-	}
-	return(irq_map[irq].rvec);
+	set_irq_mapping(irq, rvec);
 }
-EXPORT_SYMBOL(xlp_rvec_from_irq);
 
-/*
- * Masks out one IRQ in the EIMR register
- * Must NOT be called with xlp_pic_lock held
- * @irq : IRQ number
- */
-static void __nlm_irq_mask(unsigned int irq)
+static void __init init_xlp_irq_map(void)
 {
-	int rvec;
-
-	rvec = xlp_rvec_from_irq(irq);
-	if (rvec < 0) {
-		return;
-	}
-	if (read_64bit_cp0_eimr() & (1ULL << rvec)) {
-		/* We do not clear eimr, this is a TODO for later time */
-		//on_each_cpu(xlp_clear_eimr, (void *) (1ULL << rvec), 1);
-	}
-	return;
-}
-
-#if 0 /* DEPRECATED */
-/*
- * Interface function (unlocked version) to mask an IRQ
- * Calls helper function after input tests and spin_lock holds
- *
- * @irq : IRQ number
- */
-static void nlm_irq_mask(struct irq_data *d)
-{
-	unsigned int irq = d->irq;
-	//unsigned long flags;
-
-	if((irq < XLP_IRQ_RESERVED_MAX) && (irq >= 0)) {
-		return;
-	} else if(irq >= XLP_IRQ_MAX) {
-		pr_err("irq = %d. Invalid irq requested\n", irq);
-		return;
-	}
-	// Once enabled, we don't mask it out
-	//spin_lock_irqsave(&xlp_pic_lock, flags);	// Remove XXX
-	__nlm_irq_mask(irq);				// XXX
-	//spin_unlock_irqrestore(&xlp_pic_lock, flags);	// XXX remove
-	return;
-}
-#endif
-
-/*
- * Changes eimr bit value corresponding to IRT
- * @irq : IRQ number
- */
-static void __nlm_irq_unmask(int irq)
-{
-	int rvec = xlp_rvec_from_irq(irq);
-
-	if (rvec < 0) {
-		return;
-	} else if (((1ULL << rvec) & read_64bit_cp0_eimr()) == 0) {
-		/* This is only for those interrupts which are not statically
-		 * set in EIMR. Could dump stack if spin lock held */
-		 on_each_cpu(xlp_set_eimr, (void *) (1ULL << rvec), 1);
-	}
-	return;
-}
-
-#if 0 /* DEPRECATED */
-/*
- * Interface function (unlocked version) to mask an IRQ
- * Calls helper function after input tests and spin_lock holds
- *
- * @irq : IRQ number
- */
-static void nlm_irq_unmask(struct irq_data *d)
-{
-	unsigned int irq = d->irq;
-	//unsigned long flags;
-
-	if((irq < XLP_IRQ_RESERVED_MAX) && (irq >= 0)) {
-		return;
-	} else if(irq >= XLP_IRQ_MAX) {
-		pr_err("irq = %d. Invalid irq requested\n", irq);
-		return;
-	}
-	//spin_lock_irqsave(&xlp_pic_lock, flags);
-	__nlm_irq_unmask(irq);
-	//spin_unlock_irqrestore(&xlp_pic_lock, flags);
-	return;
-}
-#endif
+	int i;
 
-static void xlp_pic_unmask(struct irq_data *d)
-{
-	unsigned int irq = d->irq;
-	unsigned long flags;
+	for (i = 0; i < XLP_EIRR_SIZE; i++)
+		xlp_rvec_to_irq_map[i] = -1;
 
-	if((irq < XLP_IRQ_RESERVED_MAX) && (irq >= 0)) {
-		return;
-	} else if(irq >= XLP_IRQ_MAX) {
-		pr_err("irq = %d. Invalid irq requested\n", irq);
-		return;
-	}
-	spin_lock_irqsave(&xlp_pic_lock, flags);
-	nlm_hal_ack_pic(xlp_irq_to_irt(irq));
-	spin_unlock_irqrestore(&xlp_pic_lock, flags);
-}
+	for (i = 0; i < NR_IRQS; i++)
+		xlp_irq_to_rvec_map[i] = -1;
 
-/*
- * Startup function for normal IRQ
- * @irq: irq number
- *
- * This function is called as chip->startup() for all IRQs.
- * In case of XLP, all normal interrupts must fall below XLP_MSI_IRQ_OFFSET
- * When an interrupt is started, we force it to be enabled only in cpu0, it can
- * be changed later by calling nlm_irq_set_affinity()
- */
-static void nlm_irq_startup(struct irq_data *d)
-{
-	__label__ __failure;
-	unsigned int irq = d->irq;
-	int ret = 0;
-	unsigned long flags;
-	int idx, rvec;
-	struct cpumask m;
-	struct cpumask const *n;
+	for (i = 0; i < XLP_PIC_IRQ_BASE; i++)
+		set_irq_mapping(i, i);
 
-	cpumask_clear(&m);
-	cpumask_set_cpu(0, &m);
-	if((irq < XLP_IRQ_RESERVED_MAX) && (irq >= 0)) {
-		return;
-	} else if(irq >= XLP_IRQ_MAX) {
-		return;
-	}
-	n = xlp_closest_match_cpumask(&m);
-	spin_lock_irqsave(&xlp_pic_lock, flags);
-	rvec = xlp_rvec_from_irq(irq);
-	if (irq_map[irq].usage == 0) {
-		/* Currently unused => not enabled. So, setup and enable */
-		xlp_set_cpumask(n, xlp_irq_to_irt(irq));
-		ret = __nlm_hal_request_irq(xlp_irq_to_irt(irq) , rvec);
-		if (ret != 0) {
-			printk(KERN_WARNING "Failed to setup IRQ %d\n", irq);
-			goto __failure;
+	for (i = XLP_PIC_IRQ_BASE; i < NR_IRQS; i++) {
+		switch (i) {
+		case XLP_PIC_IRQ_UART_0:
+			set_pic_irq_mapping(i, 17); break;
+		case XLP_PIC_IRQ_UART_1:
+			set_pic_irq_mapping(i, 18); break;
+		default:
+			break;
 		}
-		idx = irq - __irqbase_from_rvec(rvec);
-		set_bit(idx, &(rvec_map[rvec].bitmap));
-		irq_map[irq].usage++;
-		/* At this point, make sure that each CPU has eimr bit
-		 * corresponding to this IRQ set. Later the driver can set
-		 * the cpu affinity of this interrupt. The rationale for
-		 * setting up EIMR here is that it can be moved to any CPUs
-		 * (well, a subset of any CPUs) later
-		 */
-		__nlm_irq_unmask(irq);
-	} else if (irq_map[irq].usage > 0) {
-		/* already being used. No need to check mask
-		 * if masked, will be unmasked later
-		 */
-		irq_map[irq].usage++;
-		ret = 0;
-	} else {
-		pr_err("Error irq = %d, rvec = %d, usage count %d\n", irq,
-				irq_map[irq].rvec, irq_map[irq].usage);
-		ret = -EFAULT;
 	}
-__failure:
-	spin_unlock_irqrestore(&xlp_pic_lock, flags);
-	return;
 }
 
-/*
- * IRQ shut down function
- * Disables one IRQ
- *
- * @irq : irq to shut down
- *
- * This function is called whenever release_irq() is called by means of
- * chip->shutdown(). In this function, the rvec bit in every EIMR is cleared if
- * usage falls to zero (in case of shared interrupts)
- */
-static void nlm_irq_shutdown(struct irq_data *d)
+static void xlp_pic_enable(struct irq_data *d)
 {
-	unsigned int irq = d->irq;
 	unsigned long flags;
-	int idx, rvec;
+	int irt = xlp_pic_irq_to_irt(d->irq);
+	BUG_ON(irt < 0);
 
-	if((irq < XLP_IRQ_RESERVED_MAX) && (irq >= 0)) {
-		return;
-	} else if(irq >= XLP_IRQ_MAX) {
-		pr_err("irq = %d. Invalid irq requested\n", irq);
-		return;
-	}
 	spin_lock_irqsave(&xlp_pic_lock, flags);
-	if (irq_map[irq].usage == 0) {
-		//fdebug("irq = %d, usage = %d\n", irq, irq_map[irq].usage);
-		spin_unlock_irqrestore(&xlp_pic_lock, flags);
-		return;
-	} else if (irq_map[irq].usage > 0) {
-		irq_map[irq].usage--;
-	}
-	if (irq_map[irq].usage == 0) {
-		//fdebug("irq = %d, usage = %d\n", irq, irq_map[irq].usage);
-		rvec = xlp_rvec_from_irq(irq);
-		idx = irq - __irqbase_from_rvec(rvec);
-		clear_bit(idx, &(rvec_map[rvec].bitmap));
-		spin_unlock_irqrestore(&xlp_pic_lock, flags);
-		__nlm_irq_mask(irq); /* masks this IRQ */
-	} else {
-		spin_unlock_irqrestore(&xlp_pic_lock, flags);
-	}
-	return;
+	nlm_hal_pic_enable_irt(irt);
+	spin_unlock_irqrestore(&xlp_pic_lock, flags);
 }
 
-/*
- * Set affinity for the irq for chips
- *
- * When an interrupt is setup, its EIMR bit is set in all online cpus. That is,
- * any cpu _can_ receive that interrupt. But it is the IRT entry that decides
- * whether to send that interrupt (i.e, whether to set EIRR bit or not) to any
- * particular CPU.
- *
- * IRT has two modes to decide the target CPUs for one interrupt.
- * Method 1 : Using IRT table entry bits DT and DTE
- * If DT==1, this interrupt can be routed to a max of 16 CPUs (well, hw threads)
- * If DT==1, there is one more level of indirection called DTE. Each DTE entry
- * has 128 bits and there are a total of 8 DTE entries. Each DTE entry contains
- * the bitmask of target CPU for an interrupt. One of them is chosen based
- * on the specified cpumask.
- *
- * The actual bitmask can be different from the specified bitmask based
- * on the logic of xlp_closest_match_cpumask()
- */
-static int nlm_irq_set_affinity(struct irq_data *d, const struct cpumask *mask, bool force)
+static void xlp_pic_disable(struct irq_data *d)
 {
-	unsigned int irq = d->irq;
 	unsigned long flags;
-	const struct cpumask *m;
-	struct cpumask n;
+	int irt = xlp_pic_irq_to_irt(d->irq);
+	BUG_ON(irt < 0);
 
-	if((irq < XLP_IRQ_RESERVED_MAX) && (irq >= 0)) {
-		return 0;
-	} else if(irq >= XLP_IRQ_MAX) {
-		pr_err("irq = %d. Invalid irq requested\n", irq);
-		return -EINVAL;
-	}
-	cpumask_and(&n, mask, cpu_online_mask);	/* check include/linux/cpumask.h */
-	m = xlp_closest_match_cpumask(&n);
-	if (m == NULL) {
-		printk(KERN_WARNING "Could not find a match for specified cpumask\n");
-		return -EINVAL;
-	}
 	spin_lock_irqsave(&xlp_pic_lock, flags);
-	xlp_set_cpumask(m, xlp_irq_to_irt(irq));
+	nlm_hal_pic_disable_irt(irt);
 	spin_unlock_irqrestore(&xlp_pic_lock, flags);
-	return 0;
 }
 
-/* For default handle_level_irq, the flow is as follows:
-	desc->irq_data.chip->irq_mask_ack();
-	handle_irq_event(desc->action);
-	desc->irq_data.chip->irq_unmask();
-*/
 static void xlp_pic_mask_ack(struct irq_data *d)
 {
-	/* Do nothing here. */
-	/* Since rvec is multiplexed, we do not ack eirr here. */
-}
-
-static struct irq_chip nlm_irq_pic = {
-	.name = "XLP-PIC",
-	.irq_mask_ack = xlp_pic_mask_ack,
-	.irq_unmask = xlp_pic_unmask,
-	.irq_enable = nlm_irq_startup,
-	.irq_disable = nlm_irq_shutdown,
-	.irq_set_affinity = nlm_irq_set_affinity
-};
+	int rvec = xlp_irq_to_rvec_map[d->irq];
+	BUG_ON(rvec == -1);
 
-static void rsvd_pic_handler_1(struct irq_data *d)
-{
-	unsigned int irq = d->irq;
-	if((irq < XLP_IRQ_RESERVED_MAX) && (irq >= 0)) {
-		return;
-	} else if(irq >= XLP_IRQ_MAX) {
-		pr_err("irq = %d. Invalid irq requested\n", irq);
-		return;
-	}
-	pr_err("Requesting a reserved irq (%d)??", irq);
-	return;
+	write_64bit_cp0_eirr(1ULL << rvec);
+	/* TODO: handle shared eirr case */
 }
 
-static int rsvd_pic_handler_2(struct irq_data *d, const struct cpumask *mask, bool force)
+static void xlp_pic_unmask(struct irq_data *d)
 {
-	unsigned int irq = d->irq;
-	if((irq < XLP_IRQ_RESERVED_MAX) && (irq >= 0)) {
-		return 0;
-	} else if(irq >= XLP_IRQ_MAX) {
-		pr_err("irq = %d. Invalid irq requested\n", irq);
-		return -EINVAL;
-	}
-	pr_err("Requesting a reserved irq (%d)??", irq);
-	return 0;
-}
-
-struct irq_chip nlm_common_rsvd_pic = {
-	.name = "Netlogic-RSVD-PIC",
-	.irq_unmask = rsvd_pic_handler_1,
-	.irq_mask = rsvd_pic_handler_1,
-	.irq_ack = rsvd_pic_handler_1,
-	// .end = rsvd_pic_handler_1, /* deprecated */
-	.irq_set_affinity = rsvd_pic_handler_2
-};
+	int irt = xlp_pic_irq_to_irt(d->irq);
+	BUG_ON(irt < 0);
 
-static irqreturn_t nlm_common_rsvd_irq_handler(int irq, void *dev_id)
-{
-	if((irq < XLP_IRQ_RESERVED_MAX) && (irq >= 0)) {
-		return 0;
-	} else if(irq >= XLP_IRQ_MAX) {
-		pr_err("irq = %d. Invalid irq requested\n", irq);
-		return -EINVAL;
-	}
-	if(irq == XLP_IRQ_TIMER) {
-		return IRQ_HANDLED;
-	}
-	pr_err("Requesting a reserved irq (%d)??", irq);
-	return IRQ_NONE;
+	nlm_hal_ack_pic(irt);
 }
 
-struct irqaction nlm_common_rsvd_action = {
-	.handler = nlm_common_rsvd_irq_handler,
-	.flags = 0,
-	.name = "nlm_common_rsvd_action",
-	.dev_id = 0,
-	.next = 0
+static struct irq_chip xlp_pic_intr = {
+	.name = "XLP-PIC",
+	.irq_enable	= xlp_pic_enable,
+	.irq_disable	= xlp_pic_disable,
+	.irq_mask_ack	= xlp_pic_mask_ack,
+	.irq_unmask	= xlp_pic_unmask,
 };
 
-void do_nlm_common_IRQ(unsigned int irq, struct pt_regs *regs)
-{
-	if (irq == XLP_IRQ_IPI_SMP_FUNCTION || irq == XLP_IRQ_IPI_SMP_RESCHEDULE) {
-		/* do nothing */
-	}
-	if (irq == XLP_IRQ_MSGRING) {
-		nlm_xlp_msgring_int_handler(irq, regs);
-	}
-	else if (irq == XLP_IRQ_IPI_SMP_KGDB) {
-		/* ignore now */
-	}
-	else if (irq == XLP_IRQ_IPI_OPROFILE) {
-		/* ignore now */
-	}
-	else {
-		do_IRQ(irq);
-	}
-}
-
-/* Unused function? Remove later */
-void __cpuinit nlm_smp_irq_init(void)
-{
-#ifdef XLP_MERGE_TODO
-	/* Set up kseg0 to be cachable coherent */
-	change_c0_config(CONF_CM_CMASK, CONF_CM_DEFAULT);
-#endif
-	write_64bit_cp0_eimr(xlp_irq_mask);
-}
-
-void destroy_irq(unsigned int irq)
+static void clear_cp0_eimr_bit(unsigned int bit)
 {
-    /* no-op */
-}
+	uint64_t mask;
 
-#ifdef CONFIG_PCI_MSI
-
-/*
- * The MSI and MSI-X functionality is supported only by the PCIe Controller.
- * Whenever there is a request for MSI/MSI-X, we need to find out the
- * controller on which that request is made. But in the PCI structure, I could
- * not find a place where that information can be kept. Moreover, a hack job
- * is not justified for the reason that only a small subset of PCI devices
- * (no onboard ones) require this. So, we have the file arch/mips/pci/pci-xlp.c
- * for XLP specific functionality.
- *
- * In case of MSI, we have to share one interrupt (XLP_PCIE_LINK_IRQ(x)) for
- * for 32 possible MSI on a slot.
- * We work around this problem by limiting the #of MSI per slot to
- * XLP_MSI_PER_SLOT. MSI IRQ vectors start from XLP_MSI_IRQ_OFFSET.
- * plat_irq_dispatch checks the vector number and dispatch it correctly.
- *
- * These bunch of functions would find out the controller function using the
- * passed parameter and use nlm_irq_* function to operate on that IRT
- */
-static unsigned int nlm_msi_startup(unsigned int msi)
-{
-	if ((msi < XLP_MSI_INDEX_START) || (msi > XLP_MSI_INDEX_END)){
-		return -EINVAL;
-	}
-	return nlm_irq_startup(XLP_PCIE_LINK_IRQ(XLP_MSI_TO_CTRL_FN(msi)));
+	mask = read_64bit_cp0_eimr();
+	mask &= ~(1ULL << bit);
+	write_64bit_cp0_eimr(mask);
 }
 
-static int nlm_msi_set_affinity(unsigned int msi, const struct cpumask *mask)
+static void set_cp0_eimr_bit(unsigned int bit)
 {
-	struct cpumask m;
-	if ((msi < XLP_MSI_INDEX_START) || (msi > XLP_MSI_INDEX_END)){
-		return -EINVAL;
-	}
+	uint64_t mask;
 
-	cpumask_and(&m, mask, cpu_online_mask);
-	if (cpumask_equal(&m, cpu_online_mask)){
-		return nlm_irq_set_affinity(XLP_PCIE_LINK_IRQ(XLP_MSI_TO_CTRL_FN(msi)), &m);
-	} else {
-		printk(KERN_WARNING "MSI cpu affinity change not supported\n");
-		return -EINVAL;
-	}
-}
-
-static void nlm_msi_shutdown(unsigned int msi)
-{
-	if ((msi < XLP_MSI_INDEX_START) || (msi > XLP_MSI_INDEX_END)){
-		return;
-	}
-	return nlm_irq_shutdown(XLP_PCIE_LINK_IRQ(XLP_MSI_TO_CTRL_FN(msi)));
-}
-static void nlm_msi_end(unsigned int msi)
-{
-	if ((msi < XLP_MSI_INDEX_START) || (msi > XLP_MSI_INDEX_END)){
-		return;
-	}
-	return nlm_irq_end(XLP_PCIE_LINK_IRQ(XLP_MSI_TO_CTRL_FN(msi)));
+	mask = read_64bit_cp0_eimr();
+	mask |= (1ULL << bit);
+	write_64bit_cp0_eimr(mask);
 }
 
-
-static void nlm_msi_ack(unsigned int msi)
+static void xlp_cpu_enable(struct irq_data *d)
 {
-	if ((msi < XLP_MSI_INDEX_START) || (msi > XLP_MSI_INDEX_END)){
-		return;
-	}
-	return nlm_irq_ack(XLP_PCIE_LINK_IRQ(XLP_MSI_TO_CTRL_FN(msi)));
+	set_cp0_eimr_bit(d->irq);
 }
 
-/*
- * Masks just one MSI
- * @msi : the MSI to mask
- */
-
-static u32 nlm_msi_change_mask(unsigned int msi, int val)
+static void xlp_cpu_disable(struct irq_data *d)
 {
-	unsigned long flags;
-	int bit, fn;
-	u32 mask;
-
-	fn = XLP_MSI_TO_CTRL_FN(msi);
-	bit = msi - XLP_MSI_IRQ_START(fn);
-	spin_lock_irqsave(&xlp_pic_lock, flags);
-	mask = xlp_msi_set_mask(fn, bit, val);
-	if (val == 0) {
-		if (mask == 0) { /* This was the last bit to clear */
-			__nlm_irq_mask(XLP_PCIE_LINK_IRQ(XLP_MSI_TO_CTRL_FN(msi)));
-		}
-		if ((mask & (mask - 1)) == 0) {	/* Just set the only bit*/
-			__nlm_irq_unmask(XLP_PCIE_LINK_IRQ(XLP_MSI_TO_CTRL_FN(msi)));
-		}
-	}
-	spin_unlock_irqrestore(&xlp_pic_lock, flags);
-	return mask;
+	clear_cp0_eimr_bit(d->irq);
 }
 
-static void nlm_msi_mask(unsigned int msi)
+static void xlp_cpu_ack(struct irq_data *d)
 {
-	if ((msi < XLP_MSI_INDEX_START) || (msi > XLP_MSI_INDEX_END)){
-		return ;
-	}
-	nlm_msi_change_mask(msi, 0);
-	return;
-}
+	uint64_t mask = (1ULL << d->irq);
 
-/*
- * Unmask just one MSI
- * If required, unmask the corresponding IRT as well
- */
-static void nlm_msi_unmask(unsigned int msi)
-{
-	if ((msi < XLP_MSI_INDEX_START) || (msi > XLP_MSI_INDEX_END)){
-		return ;
-	}
-	nlm_msi_change_mask(msi, 1);
-	return;
+	write_64bit_cp0_eirr(mask);
 }
 
-/*
- * MSI hook-up routines for Netlogic Boards;
- * Arch-dependent implementation called
- * from generic msi.c routines.
- */
-
-struct irq_chip nlm_msi_pic = {
-	.name = "XLP-PIC-MSI",
-	.irq_startup = nlm_msi_startup,
-	.irq_shutdown = nlm_msi_shutdown,
-	.irq_ack = nlm_msi_ack,
-	// .end = nlm_msi_end, /* deprecated */
-	.irq_mask = nlm_msi_mask,
-	.irq_unmask = nlm_msi_unmask,
-	.irq_set_affinity = nlm_msi_set_affinity
+struct irq_chip xlp_cpu_intr_timer = {
+	.name           = "XLP-CPU",
+	.irq_enable     = xlp_cpu_enable,
+	.irq_disable    = xlp_cpu_disable,
 };
 
-/*
- * These functions would find out the controller function using the
- * passed parameter and use nlm_irq_* function to operate on that IRT
- */
+struct irq_chip xlp_cpu_intr_ipi = {
+	.name           = "XLP-CPU",
+	.irq_enable     = xlp_cpu_enable,
+	.irq_disable    = xlp_cpu_disable,
+	.irq_ack	= xlp_cpu_ack,
+};
 
-static int nlm_msix_set_affinity(unsigned int msix, const struct cpumask *mask)
+static void __init init_xlp_cpu_irqs(void)
 {
-	if ((msix < XLP_MSIX_INDEX_START) || (msix > XLP_MSIX_INDEX_END)){
-		return -EINVAL;
+	int i;
+	for (i = 0; i < XLP_IRQ_RESERVED_MAX; i++) {
+		switch (i) {
+#ifdef CONFIG_SMP
+		case XLP_IRQ_IPI_SMP_FUNCTION:
+		case XLP_IRQ_IPI_SMP_RESCHEDULE:
+			xlp_irq_mask |= (1ULL << i);
+			irq_set_chip_and_handler(i, &xlp_cpu_intr_ipi,
+						 handle_percpu_irq);
+			break;
+#endif /* CONFIG_SMP */
+		case XLP_IRQ_TIMER:
+			xlp_irq_mask |= (1ULL << i);
+			irq_set_chip_and_handler(i, &xlp_cpu_intr_timer,
+						 handle_percpu_irq);
+			break;
+		default:
+			break;
+		}
 	}
-	return nlm_irq_set_affinity(XLP_PCIE_MSIX_IRQ(msix - XLP_MSIX_INDEX_START), mask);
 }
 
-static void nlm_msix_end(unsigned int msix)
+static void xlp_pic_irt_setup(int irt, int rvec)
 {
-	if ((msix < XLP_MSIX_INDEX_START) || (msix > XLP_MSIX_INDEX_END)){
-		return;
-	}
-	return nlm_irq_end(XLP_PCIE_MSIX_IRQ(msix - XLP_MSIX_INDEX_START));
-}
+	int cpu;
+	int cpugroup;
+	uint32_t thread_mask;
 
-static void nlm_msix_ack(unsigned int msix)
-{
-	if ((msix < XLP_MSIX_INDEX_START) || (msix > XLP_MSIX_INDEX_END)){
-		return;
-	}
-	return nlm_irq_ack(XLP_PCIE_MSIX_IRQ(msix - XLP_MSIX_INDEX_START));
-}
+	BUG_ON((irt < 0) || (irt >= XLP_PIC_NUM_IRTS));
 
-/*
- * Masks just one MSIX
- * @msix : the MSIX to mask
- */
-static void nlm_msix_mask(unsigned int msix)
-{
-	unsigned long flags;
-	if ((msix < XLP_MSIX_INDEX_START) || (msix > XLP_MSIX_INDEX_END)){
-		return ;
-	}
-	spin_lock_irqsave(&xlp_pic_lock, flags);
-	__nlm_irq_mask(XLP_PCIE_MSIX_IRQ(msix - XLP_MSIX_INDEX_START));
-	mask_msi_irq(msix); /* Disable MSI-X -- please note */
-	spin_unlock_irqrestore(&xlp_pic_lock, flags);
-	return;
-}
+	cpu = hard_smp_processor_id();
+	cpugroup = cpu >> 16;
+	thread_mask = 1 << (cpu & 0xffff);
 
-/*
- * Unmask just one MSIX
- * If required, unmask the corresponding IRT as well
- */
-static void nlm_msix_unmask(unsigned int msix)
-{
-	unsigned long flags;
-	if ((msix < XLP_MSIX_INDEX_START) || (msix > XLP_MSIX_INDEX_END)){
-		return ;
-	}
-	spin_lock_irqsave(&xlp_pic_lock, flags);
-	__nlm_irq_mask(XLP_PCIE_MSIX_IRQ(msix - XLP_MSIX_INDEX_START));
-	unmask_msi_irq(msix); /* Enable MSI-X -- please note */
-	spin_unlock_irqrestore(&xlp_pic_lock, flags);
-	return;
-}
+	nlm_hal_pic_write_irt(irt,
+			      0 /* en:0=disable */,
+			      0 /* nmi:0=maskable */,
+			      1 /* sch:1=local */,
+			      rvec /* vec */,
+			      1 /* dt:1=ID */,
+			      cpugroup /* db */,
+			      thread_mask /* dte */);
 
-static unsigned int nlm_msix_startup(unsigned int msix)
-{
-	if ((msix < XLP_MSIX_INDEX_START) || (msix > XLP_MSIX_INDEX_END)){
-		printk(KERN_WARNING "Invalid msix #%d\n", msix);
-		return -EINVAL;
-	}
-	nlm_msix_unmask(msix);
-	return nlm_irq_startup(XLP_PCIE_MSIX_IRQ(msix - XLP_MSIX_INDEX_START));
+	nlm_hal_ack_pic(irt);
+	pr_info("(%s) cpu=%3d irt=%3d rvec=%2d\n", __func__, cpu, irt, rvec);
 }
 
-static void nlm_msix_shutdown(unsigned int msix)
+static void __init init_xlp_irqs(void)
 {
-	if ((msix < XLP_MSIX_INDEX_START) || (msix > XLP_MSIX_INDEX_END)){
-		return;
-	}
-	nlm_msix_mask(msix);
-	return nlm_irq_shutdown(XLP_PCIE_MSIX_IRQ(msix - XLP_MSIX_INDEX_START));
-}
-
-/*
- * MSI-X hook-up routines for Netlogic Boards;
- * Arch-dependent implementation called
- * from generic msi.c routines.
- */
+	int irt;
+	int irq;
+	int rvec;
 
-struct irq_chip nlm_msix_pic = {
-	.name = "XLP-PIC-MSIX",
-	.irq_startup = nlm_msix_startup,
-	.irq_shutdown = nlm_msix_shutdown,
-	.irq_ack = nlm_msix_ack,
-	// .end = nlm_msix_end, /* deprecated */
-	.irq_mask = nlm_msix_mask,
-	.irq_unmask = nlm_msix_unmask,
-	.irq_set_affinity = nlm_msix_set_affinity
-};
+	xlp_irq_mask = 0;
+	init_xlp_irq_map();
 
+	init_xlp_cpu_irqs();
 
-/*
- * Composes MSI/MSI-X messages
- */
-static int xlp_msi_compose_msg(struct pci_dev *pdev, struct msi_desc *desc,
-		unsigned int irq, struct msi_msg *msg)
-{
-	u8 offset;
-	int fn = xlp_ctrl_fn_from_dev(pdev);
+	for (irq = XLP_PIC_IRQ_BASE; irq < NR_IRQS; irq++) {
+		irt = xlp_pic_irq_to_irt(irq);
+		if ((irt < 0) || (irt >= XLP_PIC_NUM_IRTS))
+			continue;
 
-	if (fn < 0) return -EINVAL;
-	if (desc->msi_attrib.is_msix) {
-		if (irq < XLP_MSIX_INDEX_START) {	/* enforce minimum */
-			dev_err(&pdev->dev, "Invalid irq %d", irq);
-			return -EINVAL;
-		}
-		offset = irq - XLP_MSIX_INDEX_START;
-		msg->address_hi = (virt_to_phys(xlp_msix_addr_start(fn)) >> 32);
-		msg->address_lo = (virt_to_phys(xlp_msix_addr_start(fn)) & 0xffffffff);
-		//dev_dbg(&pdev->dev, "MSI-X hi = %#x, lo = %#x, data = %#x\n", msg->address_hi, msg->address_lo, offset);
-	} else {
-		if (irq < XLP_MSI_IRQ_OFFSET) {	/* enforce minimum */
-			return -EINVAL;
+		rvec = xlp_irq_to_rvec_map[irq];
+		if (rvec == -1) {
+			xlp_pic_irt_setup(irt, 0);
+			continue;
 		}
-		offset = irq - (XLP_MSI_IRQ_START(xlp_ctrl_fn_from_dev(pdev)));
-		msg->address_hi = (virt_to_phys(xlp_msi_addr_start(fn)) >> 32) & 0xffffffff;
-		msg->address_lo = (virt_to_phys(xlp_msi_addr_start(fn)) & 0xffffffff);
-	}
-	msg->data = offset;
-	return 0;
-}
 
-/*
- * Returns the bitmask of currently used MSI-X on controller fn
- *
- * Must call with lock held
- * @fn : controller number
- */
-u32 __xlp_msix_bitmask(int fn)
-{
-	int idx = 0, ret = 0;
-
-	while (idx < XLP_MSIX_PER_SLOT) {
-		if (irq_map[XLP_MSIX_IRQ_START(fn) + idx].usage > 0) {
-			ret |= (1ULL << idx);
-		}
-		idx++;
+		xlp_irq_mask |= (1ULL << rvec);
+		xlp_pic_irt_setup(irt, rvec);
+		irq_set_chip_and_handler(irq, &xlp_pic_intr,
+					 handle_level_irq);
 	}
-	return ret;
 }
 
-/*
- * Back end of disable_msi()/ disable_msix()
- */
-void arch_teardown_msi_irq(unsigned int msi)
+void __init arch_init_irq(void)
 {
-	unsigned long flags;
-	int bit, fn;
-	u32 mask;
+	/* Initialize the irq descriptors */
+	init_xlp_irqs();
 
-	switch (msi) {
-	case XLP_MSI_INDEX_START ... XLP_MSI_INDEX_END:
-		fn = XLP_MSI_TO_CTRL_FN(msi);
-		bit = msi - XLP_MSI_IRQ_START(fn);
-		spin_lock_irqsave(&xlp_pic_lock, flags);
-		if (irq_map[msi].usage > 0) {
-			irq_map[msi].usage--;
-		}
-		mask = xlp_msi_set_mask(fn, bit, 0);/*set bit=0 & get the mask*/
-		if (mask == 0) { /* This was the last bit to clear */
-			if (irq_map[msi].usage != 0) {
-				printk("Fatal Error. Interrupt usage mismatch(%d), expected 0\n", irq_map[msi].usage);
-			}
-			__nlm_irq_mask(XLP_PCIE_LINK_IRQ(XLP_MSI_TO_CTRL_FN(msi)));
-			xlp_msi_disable(fn, 0xf);/* disable MSI; enable INTx */
-			xlp_intx_enable(fn);
-		}
-		spin_unlock_irqrestore(&xlp_pic_lock, flags);
-		return;
-	case XLP_MSIX_INDEX_START ... XLP_MSIX_INDEX_END:
-		fn = XLP_MSIX_TO_CTRL_FN(msi);
-		bit = msi - XLP_MSIX_IRQ_START(fn);
-		nlm_msix_shutdown(msi);
-		spin_lock_irqsave(&xlp_pic_lock, flags);
-		if (irq_map[msi].usage > 0) {
-			irq_map[msi].usage--;
-			msix_vec[fn].bitmap &= ~(1ULL << bit);
-			msix_vec[fn].count--;
-		}
-		if (!__xlp_msix_bitmask(fn)) {
-			xlp_msix_disable(fn);
-			xlp_intx_enable(fn);
-		}
-		spin_unlock_irqrestore(&xlp_pic_lock, flags);
-		return;
-	default:
-		return;	/* Do not proceed if !(msi || msix) */
-	}
+	write_64bit_cp0_eimr(xlp_irq_mask);
 }
 
-#endif		// CONFIG_PCI_MSI
-
-static int xlp_perf_irq(void)
+void __cpuinit nlm_smp_irq_init(void)
 {
-	return IRQ_HANDLED;
+	write_64bit_cp0_eimr(xlp_irq_mask);
 }
 
-/*
- * Entry function for interrupts
- */
 asmlinkage void plat_irq_dispatch(void)
 {
-	volatile u64 eirr;
-	volatile u64 eimr;
-	volatile u64 bitmap;
-	struct pt_regs *pt_regs = current_thread_info()->regs;
-	int rvec = 0, idx = 0, base_irq, irq, fn __maybe_unused;
-	unsigned long flags;
+	uint64_t eirr;
+	int rvec;
+	int irq;
 
 	eirr = read_64bit_cp0_eirr();
-	eimr = read_64bit_cp0_eimr();
-	eirr &= eimr;
 	if (eirr & (1ULL << XLP_IRQ_TIMER)) {
-		nlm_common_timer_interrupt(pt_regs, XLP_IRQ_TIMER);
-		return;
+		do_IRQ(XLP_IRQ_TIMER);
+		eirr &= ~(1ULL << XLP_IRQ_TIMER);
 	}
-	/* Loop till all bits of eirr is cleared */
-	while (eirr) {
 
-	rvec = __ilog2_u64(eirr);
-	if (rvec == -1) {
-		return;
-	}
-	eirr &= ~(1ULL << rvec);
+	while (eirr) {
+		rvec = __ilog2_u64(eirr);
 
-	if (rvec < XLP_IRQ_RESERVED_MAX) {
-		irq = rvec;
-		do_nlm_common_IRQ(irq, pt_regs);
-		return;
-	} else {
-		/* We need to loop through all possible irqs for an rvec */
-		base_irq = irqbase_from_rvec(rvec);
-		if(base_irq < 0) {
+		irq = xlp_rvec_to_irq_map[rvec];
+		if (irq == -1) {
+			spurious_interrupt();
 			return;
 		}
-		spin_lock_irqsave(&xlp_pic_lock, flags);
-		bitmap = rvec_map[rvec].bitmap;
-		spin_unlock_irqrestore(&xlp_pic_lock, flags);
-		switch(base_irq) {
-#if defined CONFIG_PCI_MSI
-		/* These are not MSIs, but IRT #s */
-		case XLP_PCIE_LINK_IRQ(0) ... XLP_PCIE_LINK_IRQ(3):
-			/* Here fn # of controller is easily calculated
-			 * Check the IRT table : 0 -> 78, 1-> 79 ..etc */
-			fn = base_irq - XLP_PCIE_LINK_IRQ(0);
-			if (is_msi_set(fn) != 0) { /* this is an MSI */
-				/* find vectors set */
-				bitmap = calc_msi_vector_offset(fn);
-				/* recalculate base_irq for MSI */
-				base_irq = XLP_MSI_IRQ_START(fn);
-				/* now handle it as any other interrupt */
-			}
-			break;
-		case XLP_PCIE_MSIX_IRQ(0) ... XLP_PCIE_MSIX_IRQ(31):
-			fn = XLP_MSIX_TO_CTRL_FN(base_irq - XLP_PCIE_MSIX_IRQ(0));/* This _is_ correct because of((x >>3) &3) */
-			/* this is an MSI/MSI-X. Find vectors set */
-			bitmap = xlp_msix_status_clear(fn);
-			/* recalculate base_irq for MSI */
-			base_irq = XLP_MSIX_IRQ_START(fn);
-			/* now handle it as any other interrupt */
-			break;
-#endif
-		default:
-			break;
-		}
-		while (bitmap) {
-			/* now that we have bitmap, serve all of them */
-			idx = ffs(bitmap) - 1;	/* man ffs */
-			bitmap &= ~(1 << idx);
-			irq = base_irq + idx;
-			do_IRQ(irq);
-		}
-		write_64bit_cp0_eirr(1ULL << rvec);
-	}
 
-	}	// End of while (eirr)...
-	return;
-}
-
-int nlm_xlp_request_irq(int irq)
-{
-	if((irq < XLP_IRQ_RESERVED_MAX) && (irq >= 0)) {
-		return -ENODEV;
-	} else if(irq >= XLP_IRQ_MAX) {
-		return -EFAULT;
-	}
-	return(irq + XLP_IRQ_RESERVED_MAX);
-}
-EXPORT_SYMBOL(nlm_xlp_request_irq);
-
-#if defined CONFIG_PCI_MSI
-#ifdef arch_setup_msi_irqs
-/*
- * Arch specific setup functions and helpers
- */
-static int __xlp_alloc_msi(int base, int max_base, int req, int *max_avail)
-{
-	int ret;
-
-	if ((base + req) > max_base) {
-		*max_avail = -1;
-		return -ENOSPC;
-	}
-	if (irq_map[base].usage == 0) { /* allocate this vector */
-		*max_avail += *max_avail;
-		irq_map[base].usage = 1;
-		if (req == 1) {
-			return 0;	/* success */
-		}
-		ret = __xlp_alloc_msi(base + 1, max_base, req - 1, max_avail);
-		if (ret != 0) {
-			irq_map[base].usage = 0; /* clear before return fail */
-		}
-		return ret;
-	} else {	/* usage > 0 */
-		return -ENOSPC;	/* max_avail is already set */
-	}
-}
-/*
- * Reserves max of req MSIs
- * @base : base offset to start searching for irq entries for this device
- * @idx	: index to start searching from base
- * @req : number of irqs requested
- * @min : Max number of MSIs that can be allocated at the moment. This value
- * can be modified if an iteration finds a value greater than this.
- */
-static int __xlp_reserve_nvec(int base, int req, int *max)
-{
-	int ret, idx = 0;
-	*max = 0;
-	while (idx < XLP_MSI_PER_SLOT) {
-		ret = __xlp_alloc_msi(base + idx, base + XLP_MSI_PER_SLOT, req, max);
-		if (ret == 0) {
-			return 0;
-		}
-		idx++;
-	}
-	return ret;
-}
-
-/*
- * Backend function that sets up MSI.
- * Called from arch_setup_msi_irqs()
- *
- * On XLP, we can have more than one MSI per device. But no device requests it
- * because of possible x86 influence (one MSI per device limitation)
- */
-int xlp_setup_msi_irq(struct pci_dev *dev, struct msi_desc *desc, int nvec)
-{
-	struct msi_msg msg;
-	int ret, max;
-	int base_msi = XLP_MSI_IRQ_START(xlp_ctrl_fn_from_dev(dev));
-	unsigned long flags;
-	int fn;
-
-
-	fn = xlp_ctrl_fn_from_dev(dev);
-	if (base_msi < XLP_MSI_IRQ_OFFSET) {
-		return -EFAULT;
-	}
-	if ((nvec & (nvec - 1))) {	/* test if multiple of 2 */
-		return -EINVAL;
-	}
-	if (nvec > XLP_MSI_PER_SLOT) {
-		return -EINVAL;
-	}
-	spin_lock_irqsave(&xlp_pic_lock, flags);
-	ret = __xlp_reserve_nvec(base_msi, nvec, &max);
-	spin_unlock_irqrestore(&xlp_pic_lock, flags);
-	if (ret < 0) {
-		return max;
-	}
-	irq_set_msi_desc(base_msi, desc);
-	ret = xlp_msi_compose_msg(dev, desc, base_msi, &msg);
-	if (ret < 0) {
-		return ret;
-	}
-	write_msi_msg(base_msi, &msg);
-	xlp_msix_disable(fn);
-	xlp_intx_disable(fn);
-	xlp_msi_enable(dev, 1);	// XXX Enable MSI for bit 0
-	return 0;
-}
-
-/*
- * MSI-X setup functions
- */
-
-/*
- * Allocates a single MSI-X vector for msi_desc entry
- * @dev		: pci device
- * @desc	: msi_descriptor for this msi entry
- * @nvec	: outstanding number of interrupts required for this device
- */
-int xlp_setup_msix_irq(struct pci_dev *dev, struct msi_desc *desc, int nvec)
-{
-	__label__ fail_nospc;
-	struct msi_msg msg;
-	int ret, idx, base_msix, fn = xlp_ctrl_fn_from_dev(dev);
-	unsigned long flags;
-
-	if (fn < 0) {
-		return -EFAULT;
-	}
-	base_msix = XLP_MSIX_IRQ_START(fn);
-	if (base_msix < XLP_MSIX_IRQ_OFFSET) {
-		return -EFAULT;
-	}
-
-	spin_lock_irqsave(&xlp_pic_lock, flags);
-	if ((XLP_MSIX_PER_SLOT - msix_vec[fn].count) < nvec) {
-		dev_err(&dev->dev, "Not enough vectors(%#x)to allocate(%#x)\n",
-			(XLP_MSIX_PER_SLOT - msix_vec[fn].count), nvec);
-		ret = -ENOSPC;
-		goto fail_nospc;
-	}
-	for (idx = 0; idx < XLP_MSIX_PER_SLOT; idx++) {
-		if (irq_map[base_msix + idx].usage != 0) {
-			continue;
-		}
-		/* We hit an unused entry */
-		irq_map[base_msix + idx].usage = 1;
-		msix_vec[fn].bitmap |= (1ULL << idx);
-		msix_vec[fn].count++;
-		break;
-	}
-	spin_unlock_irqrestore(&xlp_pic_lock, flags);
-	if (idx == XLP_MSIX_PER_SLOT) {
-		return -ENOSPC;
-	}
-	irq_set_msi_desc(base_msix + idx, desc);
-	ret = xlp_msi_compose_msg(dev, desc, base_msix + idx, &msg);
-	if (ret < 0) {
-		return ret;
-	}
-	write_msi_msg(base_msix + idx, &msg);
-	xlp_intx_disable(fn);
-	xlp_msi_disable(fn, 0xf);
-	xlp_msix_enable(dev);	/* enable msix */
-	return 0;
-
-fail_nospc:
-	spin_unlock_irqrestore(&xlp_pic_lock, flags);
-	return ret;
-}
-
-int arch_setup_msi_irqs(struct pci_dev *dev, int nvec, int type)
-{
-	struct msi_desc *entry;
-	int ret;
-
-	if ((type == PCI_CAP_ID_MSI) && (nvec > XLP_MSI_PER_SLOT)){
-		return -EINVAL;
-	}
-	if ((type == PCI_CAP_ID_MSIX) && (nvec > XLP_MSIX_PER_SLOT)){
-		return -EINVAL;
-	}
-	list_for_each_entry(entry, &dev->msi_list, list) {
-		if (type == PCI_CAP_ID_MSI) {
-			ret = xlp_setup_msi_irq(dev, entry, nvec);
-		} else {
-			ret = xlp_setup_msix_irq(dev, entry, nvec--);
-		}
-		if (ret < 0) {
-			return ret;
-		}
-	}
-	return 0;
-}
-EXPORT_SYMBOL(arch_setup_msi_irqs);
-#endif
-#endif
-
-#define PIC_IRQ_BASE XLP_IRQ_RESERVED_MAX
-
-static void clear_cp0_eimr_bit(unsigned int bit)
-{
-	u64 mask;
-
-	mask = read_64bit_cp0_eimr();
-	mask &= ~(1ULL << bit);
-	write_64bit_cp0_eimr(mask);
-}
-
-static void set_cp0_eimr_bit(unsigned int bit)
-{
-	u64 mask;
-
-	mask = read_64bit_cp0_eimr();
-	mask |= (1ULL << bit);
-	write_64bit_cp0_eimr(mask);
-}
-
-static void xlp_cpu_enable(struct irq_data *d)
-{
-	set_cp0_eimr_bit(d->irq);
-}
-
-static void xlp_cpu_disable(struct irq_data *d)
-{
-	clear_cp0_eimr_bit(d->irq);
-}
-
-static void xlp_cpu_mask(struct irq_data *d)
-{
-	clear_cp0_eimr_bit(d->irq);
-}
-
-static void xlp_cpu_unmask(struct irq_data *d)
-{
-	set_cp0_eimr_bit(d->irq);
-}
-
-static void xlp_cpu_ack(struct irq_data *d)
-{
-	clear_cp0_eimr_bit(d->irq);
-}
-
-static void xlp_cpu_eoi(struct irq_data *d)
-{
-	set_cp0_eimr_bit(d->irq);
-}
-
-/*
- * Chip definition for CPU originated interrupts(timer, msg) and
- * IPIs
- */
-struct irq_chip nlm_cpu_intr = {
-	.name           = "XLP-CPU-INTR",
-	.irq_enable     = xlp_cpu_enable,
-	.irq_disable    = xlp_cpu_disable,
-	.irq_mask	= xlp_cpu_mask,
-	.irq_unmask	= xlp_cpu_unmask,
-	.irq_ack        = xlp_cpu_ack,
-	.irq_eoi	= xlp_cpu_eoi,
-};
-
-static inline void irq_desc_set_chip(struct irq_desc *desc, struct irq_chip *chip)
-{
-	irq_desc_get_irq_data(desc)->chip = chip;
-}
-
-void __init init_nlm_common_irqs(void)
-{
-	int i;
-	u64	mask = 0;
+		do_IRQ(irq);
 
-	for (i = 0; i < XLP_IRQ_MAX; i++) {	// IRQ : 0 - 167
-		if (i >= PIC_IRQ_BASE)
-			irq_set_chip_and_handler(i, &nlm_irq_pic,
-						 handle_level_irq);
-		else
-			irq_set_chip_and_handler(i, &nlm_cpu_intr,
-						 handle_percpu_irq);
+		eirr &= ~(1ULL << rvec);
 	}
-#ifdef CONFIG_PCI_MSI
-	for (i = XLP_MSI_INDEX_START; i <= XLP_MSI_INDEX_END; i++) {
-		irq_set_chip(i, &nlm_msi_pic);
-	}
-	for (i = XLP_MSIX_INDEX_START; i <= XLP_MSIX_INDEX_END; i++) {
-		irq_set_chip(i, &nlm_msix_pic);
-	}
-#endif
-
-#ifdef CONFIG_REMOTE_DEBUG	/* REMOVE on XLP TODO */
-	irq_desc_set_chip(&irq_desc[XLP_IRQ_REMOTE_DEBUG], &nlm_common_rsvd_pic);
-	irq_desc[XLP_IRQ_REMOTE_DEBUG].chip = &nlm_common_rsvd_pic;
-	irq_desc[XLP_IRQ_REMOTE_DEBUG].action = nlm_common_rsvd_action;
-	// xlp_irq_mask |= (1ULL << XLP_IRQ_REMOTE_DEBUG);
-#endif
-#ifdef CONFIG_SMP
-
-#ifdef CONFIG_NLMCOMMON_IP_FLOW_AFFINITY	/* REMOVE on XLP TODO */
-	/* PR: New IPI added here for netrx balancing */
-	irq_desc_set_chip(&irq_desc[XLP_IRQ_IPI_NETRX], &nlm_common_rsvd_pic);
-	irq_desc[XLP_IRQ_IPI_NETRX].action = &nlm_common_rsvd_action;
-	//xlp_irq_mask |= (1ULL << XLP_IRQ_IPI_NETRX);
-#endif				/* CONFIG_NLMCOMMON_IP_FLOW_AFFINITY */
-
-#endif
-
-	/* msgring interrupt */
-	irq_desc_set_chip(&irq_desc[XLP_IRQ_MSGRING], &nlm_common_rsvd_pic);
-	irq_desc[XLP_IRQ_MSGRING].action = &nlm_common_rsvd_action;
-
-	mask = (
-			(1ULL << XLP_IRQ_TIMER) |
-			(1ULL << 10) |	/* timer */
-			(1ULL << 49) |	/* msg_idx */
-			(0x3ULL << 48) |	/* msg_idx */
-			(0xfULL << 32) |	/* pci msix */
-			(0xfULL << 41) |	/* pci and msi */
-			(0x1ULL << 58) |	/* nae */
-			(0x3ULL << 24) |	/* usb */
-			(0x3ULL	<< 17) |	/* uart */
-			(0xfULL << 13) |	/* gpio */
-#ifdef CONFIG_SMP
-			(1ULL << XLP_IRQ_IPI_SMP_FUNCTION) |
-			(1ULL << XLP_IRQ_IPI_SMP_RESCHEDULE) |
-#endif
-#ifdef CONFIG_OPROFILE
-			(1ULL << XLP_IRQ_IPI_OPROFILE) |
-#endif
-#ifdef CONFIG_KGDB
-			(1ULL << XLP_IRQ_IPI_SMP_KGDB) |
-#endif
-			(1ULL << XLP_IRQ_MSGRING) |
-			(0xfULL << 20)		/* nor, nand, spi and mmc */
-	       );
-	/* set interrupt mask for non-zero cpus */
-	mask |= read_64bit_cp0_eimr();
-	xlp_irq_mask = mask;
 }
 
 
-void __init arch_init_irq(void)
-{
-	extern int (*perf_irq)(void);
-
-	perf_irq = xlp_perf_irq;
-
-#ifdef CONFIG_KGDB
-	if (kgdb_early_setup)
-		return;
-#endif
-
-	/* Initialize the irq descriptors */
-	init_nlm_common_irqs();
-
-	write_64bit_cp0_eimr(xlp_irq_mask);
-
-}
diff --git a/arch/mips/netlogic/xlp/on_chip.c b/arch/mips/netlogic/xlp/on_chip.c
index 6ecf165..e466674 100644
--- a/arch/mips/netlogic/xlp/on_chip.c
+++ b/arch/mips/netlogic/xlp/on_chip.c
@@ -556,36 +556,6 @@ void xlp_set_cpumask(const struct cpumask *m, int irt)
 	return;
 }
 
-
-/*********************************************************************
- *  pic_init
- *  
- ********************************************************************/
-static void pic_init(void)
-{
-	int irt = 0;
-	int cpu;
-	int cpugroup;
-	uint32_t thread_mask;
-
-	cpu = hard_smp_processor_id();
-	cpugroup = cpu >> 16;
-	thread_mask = (1 << (cpu & 0xffff));
-
-	for (irt = 0; irt < XLP_IRT_NUM; irt++) {
-		/* Use local scheduling
-		 * Invalidate all IRTs, by default */
-		nlm_hal_pic_write_irt(irt, 0 /* en:0=disable */, 0 /* nmi */,
-				      1 /* sch:1=local */, 0 /* vec */,
-				      1 /* dt:1=ID */,
-				      cpugroup /* db */,
-				      thread_mask /* dte */);
-	}
-
-	/* On XLP, MSGRING config register is per hw-thread */
-	enable_msgconfig_int();
-}
-
 atomic_t nlm_common_counters[NR_CPUS][NLM_MAX_COUNTERS] __cacheline_aligned;
 
 /*********************************************************************
@@ -655,12 +625,13 @@ void on_chip_init(void)
 
 	nlm_hal_init();
 
-	pic_init(); 
-
 	for (i = 0; i < NR_CPUS; i++)
 	for (j = 0; j < NLM_MAX_COUNTERS; j++)
 			atomic_set(&nlm_common_counters[i][j], 0);
 
+	/* On XLP, MSGRING config register is per hw-thread */
+	enable_msgconfig_int();
+
 	/*enable vc interrupts*/
 	nlm_enable_vc_intr();
 }
diff --git a/arch/mips/netlogic/xlp/platform.c b/arch/mips/netlogic/xlp/platform.c
index f0502be..fd2f427 100644
--- a/arch/mips/netlogic/xlp/platform.c
+++ b/arch/mips/netlogic/xlp/platform.c
@@ -95,7 +95,7 @@ static void xlp_init_uart(int port_id)
         xlp_uart_port[port_id].mapbase       = DEFAULT_NETLOGIC_IO_BASE 
 						+ NETLOGIC_IO_UART_0_OFFSET + port_id * XLP_UART_PORTIO_OFFSET;
         xlp_uart_port[port_id].membase       = (void __iomem *)xlp_uart_port[port_id].mapbase;
-        xlp_uart_port[port_id].irq           = XLP_UART_IRQ(port_id);
+        xlp_uart_port[port_id].irq           = XLP_PIC_IRQ_UART(port_id);
 
         xlp_uart_port[port_id].uartclk       = UART_CLK;
         xlp_uart_port[port_id].iotype        = UPIO_NLM;
@@ -219,7 +219,7 @@ static int xlp_find_pci_dev(void)
 						pres[0].end	= mmio;
 						pres[0].flags	= IORESOURCE_MEM;
 						irt = (nlm_hal_read_32bit_reg(mmio, DEV_IRT_INFO) & 0xFFFF);
-						irq = xlp_irt_to_irq(irt);
+						irq = xlp_pic_irt_to_irq(irt);
 
 						pres[1].start = irq;
 						pres[1].end = irq;
diff --git a/arch/mips/netlogic/xlp/smp.c b/arch/mips/netlogic/xlp/smp.c
index ad70f94..79e3bfa 100644
--- a/arch/mips/netlogic/xlp/smp.c
+++ b/arch/mips/netlogic/xlp/smp.c
@@ -272,14 +272,12 @@ int wakeup_secondary_cpus(void)
 
 static irqreturn_t smp_resched_ipi_handler(int irq, void *dev_id)
 {
-	write_64bit_cp0_eirr(1ULL << irq);
 	scheduler_ipi();
 	return IRQ_HANDLED;
 }
 
 static irqreturn_t smp_function_ipi_handler(int irq, void *dev_id)
 {
-	write_64bit_cp0_eirr(1ULL << irq);
 	smp_call_function_interrupt();
 	return IRQ_HANDLED;
 }
diff --git a/drivers/usb/host/ehci-pci.c b/drivers/usb/host/ehci-pci.c
index 2b2eb91..4cd8a41 100644
--- a/drivers/usb/host/ehci-pci.c
+++ b/drivers/usb/host/ehci-pci.c
@@ -48,7 +48,6 @@ static const char hcd_name[] = "ehci-pci";
 #include <asm/netlogic/xlp_hal_pic.h>
 #include <asm/netlogic/xlp.h>
 #include <asm/netlogic/xlp_usb.h>
-extern int nlm_xlp_request_irq(int irq);
 volatile uint64_t *ehci_regs;
 
 static void xlp_usb_hw_start(int ctrl_no)
@@ -70,7 +69,7 @@ int xlp_ehci_hcd_pci_probe(struct pci_dev *dev, const struct pci_device_id *id)
 	ctrl_no = dev->devfn & 0xF;
 
 	irt = usb_reg_read(0, ctrl_no, 0x3D) & 0xFFFF;
-	irq = nlm_xlp_request_irq(irt);
+	irq = xlp_pic_irt_to_irq(irt);
 
 	if (!irq) {
 		printk("Found HC with no IRQ.  Check BIOS/PCI %s setup!\n", 
-- 
1.8.4.93.g57e4c17

