From 436dd40ecd440bdb1a1cea1ca2a207420b6486d4 Mon Sep 17 00:00:00 2001
From: Sreenidhi BR <sreenidhibr@netlogicmicro.com>
Date: Fri, 17 Jun 2011 16:30:59 +0530
Subject: [PATCH 227/565] Raid: DTRE-Added memset()

Raid: DTRE-Added memset() and improvements to memcpy() .

Based on Broadcom SDK 2.3.

Signed-off-by: Sreenidhi BR <sreenidhibr@netlogicmicro.com>
Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 drivers/dma/nlm_adma.c | 108 ++++++++++++++++++++++++++++++++++++++++++++-----
 drivers/dma/nlm_adma.h |   4 +-
 2 files changed, 102 insertions(+), 10 deletions(-)

diff --git a/drivers/dma/nlm_adma.c b/drivers/dma/nlm_adma.c
index 8f45296..a70c04f 100644
--- a/drivers/dma/nlm_adma.c
+++ b/drivers/dma/nlm_adma.c
@@ -51,6 +51,7 @@ uint64_t nlm_dtre_debug = 0;
 extern void nlm_hal_dtr_init();
 
 struct nlm_adma_device nlm_adma_raid_device;
+struct page * nlm_dtre_null_page;
 
 static inline int nlm_adma_get_max_xor(void)
 {
@@ -266,6 +267,8 @@ static void __process_completed_tx(struct nlm_adma_chan * chan)
 		}
 	}
 }
+
+
 static dma_cookie_t nlm_tx_submit (struct dma_async_tx_descriptor *tx)
 {
 	enum dma_transaction_type optype;
@@ -313,10 +316,41 @@ static dma_cookie_t nlm_tx_submit (struct dma_async_tx_descriptor *tx)
 	chan->pending_idx = (chan->pending_idx + 1) & DTRE_MAX_TX_Q_MASK;
 
 	spin_unlock_bh(&chan->lock);
-	/* Handle the DMA_MEMCPY case first and return.
+
+	/* handle size restrictions first, in case of 
+	   DMA_MEMSET and DMA_MEMCPY */
+
+	if (optype == DMA_MEMSET)
+	{
+		if ((nlm_tx->len > PAGE_SIZE) || (nlm_tx->memset_val != 0))
+		{
+			memset((phys_to_virt(nlm_tx->hw_desc.dst)), nlm_tx->memset_val, nlm_tx->len);
+			nlm_tx->done_flag = 1;
+			wmb();
+			tasklet_schedule(&chan->irq_tasklet);
+			return cookie;
+		}
+	}
+
+	if (optype == DMA_MEMCPY)
+	{
+		if (nlm_tx->len > DTRE_MAX_MEMCPY_SIZE)
+		{
+			memcpy((phys_to_virt(nlm_tx->hw_desc.dst)), (phys_to_virt(nlm_tx->hw_desc.src)), nlm_tx->len);
+			nlm_tx->done_flag = 1;
+			wmb();
+			tasklet_schedule(&chan->irq_tasklet);
+
+			return cookie;
+		}
+	}
+
+
+	/* Handle the DMA_MEMSET and DMA_MEMCPY cases first and return.
 	   All RAID messages are handled later 
 	   */
-	if (optype == DMA_MEMCPY)
+
+	if ((optype == DMA_MEMCPY) || (optype == DMA_MEMSET))
 	{
 		transfer_msg[0] = 
 			(1ULL << 63)
@@ -608,7 +642,7 @@ nlm_adma_prep_dma_memcpy(struct dma_chan *chan, dma_addr_t dma_dest,
 	}
 
 	desc->optype = DMA_MEMCPY;
-	desc->len = (unsigned int)len;
+	desc->len = (unsigned long)len;
 	desc->int_en = flags & DMA_PREP_INTERRUPT;
 	desc->async_tx.flags = flags;
 	desc->async_tx.cookie = -EBUSY;
@@ -619,6 +653,50 @@ nlm_adma_prep_dma_memcpy(struct dma_chan *chan, dma_addr_t dma_dest,
 	desc->hw_desc.src = dma_src;
 	desc->hw_desc.dst = dma_dest;
 	desc->pg = NULL;
+	desc->memset_val = 0;
+
+	ent = desc->hw_desc.entries;
+
+	for (i=0; i<DTRE_RAID_MAX_ENTRIES; i++)
+		desc->hw_desc.entries[i] = 0ULL;
+
+	return &desc->async_tx;
+}
+
+
+static struct dma_async_tx_descriptor *
+nlm_adma_prep_dma_memset(struct dma_chan *chan, dma_addr_t dma_dest,
+		int value, size_t len, unsigned long flags)
+{
+	int i;
+	struct nlm_tx_desc *desc;
+	uint64_t * ent;
+	struct nlm_adma_chan *nlm_chan = to_nlm_adma_chan(chan);
+
+	if (nlm_dtre_debug) {
+		printk("prep_dma_memset len 0x%lx.\n", len);
+	}
+
+	desc = alloc_tx_desc(nlm_chan);
+	if (!desc)
+	{
+		printk("DTRE Error: cannot allocate desc\n");
+		return (NULL);
+	}
+
+	desc->optype = DMA_MEMSET;
+	desc->len = (unsigned long)len;
+	desc->int_en = flags & DMA_PREP_INTERRUPT;
+	desc->async_tx.flags = flags;
+	desc->async_tx.cookie = -EBUSY;
+	desc->done_flag = 0;
+	desc->xor_check_result = 0;
+	desc->entries_count = 0;
+	desc->hw_desc.src_cnt = 0;
+	desc->hw_desc.src = (unsigned long)page_to_phys(nlm_dtre_null_page);
+	desc->hw_desc.dst = dma_dest;
+	desc->pg = NULL;
+	desc->memset_val = value;
 
 	ent = desc->hw_desc.entries;
 
@@ -664,7 +742,7 @@ nlm_adma_prep_dma_xor(struct dma_chan *chan, dma_addr_t dma_dest,
 	ent = desc->hw_desc.entries;
 
 	desc->optype = DMA_XOR;
-	desc->len = (unsigned int)len;
+	desc->len = (unsigned long)len;
 	desc->int_en = flags & DMA_PREP_INTERRUPT;
 	desc->async_tx.flags = flags;
 	desc->async_tx.cookie = -EBUSY;
@@ -675,6 +753,7 @@ nlm_adma_prep_dma_xor(struct dma_chan *chan, dma_addr_t dma_dest,
 	desc->hw_desc.src = 0;
 	desc->hw_desc.dst = 0;
 	desc->pg = NULL;
+	desc->memset_val = 0;
 
 	for (i=0; i<DTRE_RAID_MAX_ENTRIES; i++)
 		desc->hw_desc.entries[i] = 0ULL;
@@ -776,7 +855,7 @@ nlm_adma_prep_dma_xor_val(struct dma_chan *chan, dma_addr_t *dma_src,
 	ent = desc->hw_desc.entries;
 
 	desc->optype = DMA_XOR_VAL;
-	desc->len = (unsigned int)len;
+	desc->len = (unsigned long)len;
 	desc->int_en = flags & DMA_PREP_INTERRUPT;
 	desc->async_tx.flags = flags;
 	desc->async_tx.cookie = -EBUSY;
@@ -787,6 +866,7 @@ nlm_adma_prep_dma_xor_val(struct dma_chan *chan, dma_addr_t *dma_src,
 	desc->hw_desc.src = 0;
 	desc->hw_desc.dst = 0;
 	desc->pg = NULL;
+	desc->memset_val = 0;
 
 	for (i=0; i<DTRE_RAID_MAX_ENTRIES; i++)
 		desc->hw_desc.entries[i] = 0ULL;
@@ -907,7 +987,7 @@ nlm_adma_prep_dma_pq(struct dma_chan *chan, dma_addr_t *dst, dma_addr_t *src,
 	ent = desc->hw_desc.entries;
 
 	desc->optype = DMA_PQ;
-	desc->len = (unsigned int)len;
+	desc->len = (unsigned long)len;
 	desc->int_en = flags & DMA_PREP_INTERRUPT;
 	desc->async_tx.flags = flags;
 	desc->async_tx.cookie = -EBUSY;
@@ -918,6 +998,7 @@ nlm_adma_prep_dma_pq(struct dma_chan *chan, dma_addr_t *dst, dma_addr_t *src,
 	desc->hw_desc.src = 0;
 	desc->hw_desc.dst = 0;
 	desc->pg = NULL;
+	desc->memset_val = 0;
 
 	if ((flags & DMA_PREP_PQ_DISABLE_P) || (flags & DMA_PREP_PQ_DISABLE_Q))
 	{
@@ -1096,7 +1177,7 @@ nlm_adma_prep_dma_pq_val(struct dma_chan *chan, dma_addr_t *pq, dma_addr_t *src,
 	ent = desc->hw_desc.entries;
 
 	desc->optype = DMA_PQ_VAL;
-	desc->len = (unsigned int)len;
+	desc->len = (unsigned long)len;
 	desc->int_en = flags & DMA_PREP_INTERRUPT;
 	desc->async_tx.flags = flags;
 	desc->async_tx.cookie = -EBUSY;
@@ -1107,6 +1188,7 @@ nlm_adma_prep_dma_pq_val(struct dma_chan *chan, dma_addr_t *pq, dma_addr_t *src,
 	desc->hw_desc.src = 0;
 	desc->hw_desc.dst = 0;
 	desc->pg = NULL;
+	desc->memset_val = 0;
 
 	if ((flags & DMA_PREP_PQ_DISABLE_P) || (flags & DMA_PREP_PQ_DISABLE_Q))
 	{
@@ -1314,7 +1396,7 @@ static int __devinit nlm_adma_probe(struct platform_device *pdev)
 	dma_cap_set(DMA_PQ_VAL, dma_dev->cap_mask);
 
 	dma_cap_set(DMA_MEMCPY, dma_dev->cap_mask);
-	// dma_cap_set(DMA_MEMSET, dma_dev->cap_mask);
+	dma_cap_set(DMA_MEMSET, dma_dev->cap_mask);
 	// dma_cap_set(DMA_INTERRUPT, dma_dev->cap_mask);
 
 	dma_dev->device_alloc_chan_resources = nlm_adma_alloc_chan_resources;
@@ -1323,7 +1405,7 @@ static int __devinit nlm_adma_probe(struct platform_device *pdev)
 	dma_dev->device_issue_pending = nlm_adma_issue_pending;
 
 	dma_dev->device_prep_dma_memcpy = nlm_adma_prep_dma_memcpy;
-	// dma_dev->device_prep_dma_memset = nlm_adma_prep_dma_memset;
+	dma_dev->device_prep_dma_memset = nlm_adma_prep_dma_memset;
 	// dma_dev->device_prep_dma_interrupt = nlm_adma_prep_dma_interrupt;
 
 	dma_dev->max_xor = nlm_adma_get_max_xor();
@@ -1336,6 +1418,14 @@ static int __devinit nlm_adma_probe(struct platform_device *pdev)
 
 	dma_dev->dev = &pdev->dev;
 	dma_async_device_register(dma_dev);
+
+	nlm_dtre_null_page = alloc_page(GFP_KERNEL);
+	if (nlm_dtre_null_page == NULL) {
+		printk("DTRE error: page allocation failed.\n");
+		return 0;
+	}
+	memset((void *)(phys_to_virt(page_to_phys(nlm_dtre_null_page))), 0, PAGE_SIZE);
+
 	printk("NLM ASYNC Device Registered\n");
 
 	/* hal init stuff */
diff --git a/drivers/dma/nlm_adma.h b/drivers/dma/nlm_adma.h
index 88f09aa..615b207 100644
--- a/drivers/dma/nlm_adma.h
+++ b/drivers/dma/nlm_adma.h
@@ -42,6 +42,7 @@ THE POSSIBILITY OF SUCH DAMAGE.
 #define DTRE_RAID_MAX_DEVICES		24
 #define DTRE_RAID_MAX_SRC		16
 #define DTRE_NLM_Q_POLY			0x1d
+#define DTRE_MAX_MEMCPY_SIZE		((1*1024*1024) - 1) /* 1 MB */
 
 #define DTRE_NUM_VC		4
 #define DTRE_MIN_VC		264
@@ -66,7 +67,7 @@ struct nlm_tx_desc {
 	struct nlm_tx_desc * next;
 	struct dma_async_tx_descriptor async_tx;
 	enum dma_transaction_type optype;
-	unsigned int len;
+	unsigned long len;
 	int     int_en;
 	struct nlm_hw_desc hw_desc;
 	unsigned int entries_count;
@@ -74,6 +75,7 @@ struct nlm_tx_desc {
 	u32 *xor_check_result;
 	void    *chan; /* backpointer to channel */
 	struct page * pg;
+	int memset_val;
 };
 
 struct nlm_adma_chan {
-- 
1.8.4.93.g57e4c17

