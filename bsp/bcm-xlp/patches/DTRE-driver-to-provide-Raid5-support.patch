From 3ccbdc80065e46354276656cfa75d4f366b8a7ee Mon Sep 17 00:00:00 2001
From: Sreenidhi BR <sreenidhibr@netlogicmicro.com>
Date: Fri, 27 May 2011 16:17:02 +0530
Subject: [PATCH 220/565] DTRE driver to provide Raid5 support.

DTRE driver to provide Raid5 support.

Based on Broadcom SDK 2.3.

Signed-off-by: Sreenidhi BR <sreenidhibr@netlogicmicro.com>
Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 arch/mips/netlogic/xlp/on_chip.c |   3 +-
 drivers/dma/Kconfig              |   6 +
 drivers/dma/Makefile             |   2 +
 drivers/dma/nlm_adma.c           | 958 +++++++++++++++++++++++++++++++++++++++
 drivers/dma/nlm_adma.h           | 113 +++++
 5 files changed, 1081 insertions(+), 1 deletion(-)
 create mode 100644 drivers/dma/nlm_adma.c
 create mode 100644 drivers/dma/nlm_adma.h

diff --git a/arch/mips/netlogic/xlp/on_chip.c b/arch/mips/netlogic/xlp/on_chip.c
index 9dd3cb0..11d51be 100644
--- a/arch/mips/netlogic/xlp/on_chip.c
+++ b/arch/mips/netlogic/xlp/on_chip.c
@@ -84,7 +84,8 @@ static uint16_t vc_to_handle_map[MAX_VC] = {
 	[258 ... 259] = XLP_MSG_HANDLE_PCIE1,
 	[260 ... 261] = XLP_MSG_HANDLE_PCIE2,
 	[262 ... 263] = XLP_MSG_HANDLE_PCIE3,
-	[264 ... 271] = XLP_MSG_HANDLE_GDX,
+	[264 ... 267] = XLP_MSG_HANDLE_DTRE,
+	[268 ... 271] = XLP_MSG_HANDLE_GDX,
 	[272 ... 280] = XLP_MSG_HANDLE_RSA_ECC,
 	[281 ... 296] = XLP_MSG_HANDLE_CRYPTO,
 	[297 ... 304] = XLP_MSG_HANDLE_CMP,
diff --git a/drivers/dma/Kconfig b/drivers/dma/Kconfig
index cd2e04b..bb7e3df 100644
--- a/drivers/dma/Kconfig
+++ b/drivers/dma/Kconfig
@@ -71,6 +71,12 @@ config INTEL_IOATDMA
 
 	  If unsure, say N.
 
+config NLM_ADMA
+	bool "NLM_ADMA support"
+	select DMA_ENGINE
+	help
+		Enable support for NLM ADMA RAID
+
 config INTEL_IOP_ADMA
 	tristate "Intel IOP ADMA support"
 	depends on ARCH_IOP32X || ARCH_IOP33X || ARCH_IOP13XX
diff --git a/drivers/dma/Makefile b/drivers/dma/Makefile
index e958a48..78d6220 100644
--- a/drivers/dma/Makefile
+++ b/drivers/dma/Makefile
@@ -1,5 +1,6 @@
 ccflags-$(CONFIG_DMADEVICES_DEBUG)  := -DDEBUG
 ccflags-$(CONFIG_DMADEVICES_VDEBUG) += -DVERBOSE_DEBUG
+EXTRA_CFLAGS := $(CFLAGS) -DNLM_HAL_LINUX_KERNEL
 
 obj-$(CONFIG_DMA_ENGINE) += dmaengine.o
 obj-$(CONFIG_DMA_VIRTUAL_CHANNELS) += virt-dma.o
@@ -38,5 +39,6 @@ obj-$(CONFIG_DMA_SA11X0) += sa11x0-dma.o
 obj-$(CONFIG_MMP_TDMA) += mmp_tdma.o
 obj-$(CONFIG_DMA_OMAP) += omap-dma.o
 obj-$(CONFIG_MMP_PDMA) += mmp_pdma.o
+obj-$(CONFIG_NLM_ADMA) += nlm_adma.o
 obj-$(CONFIG_KEYSTONE_DMA) += keystone-pktdma.o
 obj-$(CONFIG_KEYSTONE_UDMA) += keystone-udma.o
diff --git a/drivers/dma/nlm_adma.c b/drivers/dma/nlm_adma.c
new file mode 100644
index 0000000..ab4a1bd
--- /dev/null
+++ b/drivers/dma/nlm_adma.c
@@ -0,0 +1,958 @@
+/***********************************************************************
+Copyright 2003-2011 Netlogic Microsystems (“Netlogic”). All rights
+reserved.
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are
+met:
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+*****************************#NETL_2#********************************/
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/async_tx.h>
+#include <linux/delay.h>
+#include <linux/dma-mapping.h>
+#include <linux/spinlock.h>
+#include <linux/interrupt.h>
+#include <linux/dmaengine.h>
+#include <linux/timer.h>
+
+#include <asm/netlogic/mips-exts.h>
+#include <asm/netlogic/msgring.h>
+
+#include "nlm_adma.h"
+
+#include <linux/types.h>
+#include <linux/kernel.h>
+#include <asm/netlogic/hal/nlm_hal_fmn.h>
+
+
+uint64_t nlm_dtre_debug = 0;
+
+#define shift_lower_bits(x, bitshift, numofbits) \
+	(((unsigned long long)(x) & ((1ULL << (numofbits)) - 1)) << (bitshift))
+
+extern void nlm_hal_dtr_init();
+
+struct nlm_adma_device nlm_adma_raid_device;
+
+static inline int nlm_adma_get_max_xor(void)
+{
+	/*
+	   * The final device in a RAID-5 list may be read to check the 
+	   * syndrome, or written with the computed syndrome, according to the 
+	   * WriteSyndrome bit
+	   */
+	return (DTRE_MAX_LIST_LENGTH-1);
+}
+
+static inline int nlm_adma_get_max_pq(void)
+{
+	/*
+	   * A RAID-6 list behaves similarly, except that the P syndrome 
+	   * is read from or written to the second to final device, and the 
+	   * Q syndrome is read from or written to the final device
+	   */
+	return (DTRE_MAX_LIST_LENGTH-2);
+}
+
+static __inline__
+uint64_t gen_dtr_raid_msg_format_0 (const uint32_t src_cnt,
+		const uint64_t* src_addr)
+{
+	return 1ULL << 63
+		| shift_lower_bits (DTRE_RAID_MESSAGE_TYPE, 60, 3)
+		| shift_lower_bits ((DTRE_RAID_ENTRY_SIZE * src_cnt), 40, 12)
+		| shift_lower_bits (virt_to_phys ((volatile void *)src_addr), 0, 40);
+}
+
+static __inline__
+uint64_t gen_dtr_raid_msg_format_1 (const uint32_t raid_type,
+		const uint32_t operation,
+		const uint32_t disks,
+		const uint32_t freeback_msg_dest_id)
+{
+	return 0ULL << 63
+		| shift_lower_bits (1, 56, 1) /* Inform Source */
+		| shift_lower_bits (freeback_msg_dest_id, 44, 12)
+		| shift_lower_bits (raid_type, 10, 2)
+		| shift_lower_bits (operation, 9, 1)
+		| shift_lower_bits (disks, 0, 4);
+}
+
+static __inline__
+uint64_t gen_dtr_raid_msg_format_2 (const void * ret_entry)
+{
+	return 0ULL << 63
+		| shift_lower_bits ((unsigned long)ret_entry, 0, 63);
+}
+
+
+static void nlm_dtre_msgring_handler(uint32_t vc, uint32_t src_id,
+		uint32_t size, uint32_t code,
+		uint64_t msg0, uint64_t msg1,
+		uint64_t msg2, uint64_t msg3, 
+		void* data)
+{       
+
+	struct nlm_tx_desc *desc;
+	struct nlm_adma_chan *nlm_chan ;
+
+	uint64_t addr = msg1;
+	// addr |= (1ULL << 63);
+	addr |= NLH_XKSEG;
+
+	desc = (struct nlm_tx_desc *)addr;
+
+	if (desc == NULL)
+	{
+		printk("NULL msg in nlm_dtre_msgring_handler..\n");
+		return;
+	}
+
+	if (nlm_dtre_debug) {
+		printk("DTRE recv msg: vc %d sender 0x%x, size 0x%x, data0 0x%llx data1 0x%llx optype %d.\n",
+				vc, src_id, size, msg0, msg1, desc->optype);
+	}
+
+	if ((desc->xor_check_result) != NULL)
+		*(desc->xor_check_result) = 0;
+
+	/* save the DMA_XOR_VAL and DMA_PQ_VAL operation result */
+	/* update if check failed */
+	if ((desc->optype == DMA_XOR_VAL) | (desc->optype == DMA_PQ_VAL))
+	{
+		if ( ((msg0>>42) & 0x3) == 0x1)
+			*(desc->xor_check_result) = (1<<SUM_CHECK_P);
+	}
+
+	if (desc->optype == DMA_PQ_VAL)
+	{
+		if ( ((msg0>>44) & 0x3) == 0x1) 
+			(*(desc->xor_check_result)) |= (1<<SUM_CHECK_Q);
+	}
+
+	nlm_chan = (struct nlm_adma_chan *)(desc->chan);
+
+	desc->done_flag = 1;
+
+	tasklet_schedule(&nlm_chan->irq_tasklet);
+}               
+
+static dma_cookie_t
+nlm_desc_assign_cookie(struct nlm_adma_chan *chan,
+		struct nlm_tx_desc *desc)
+{
+	dma_cookie_t cookie = chan->common.cookie;
+	cookie++;
+	if (cookie < 0)
+		cookie = 1;
+	chan->common.cookie = desc->async_tx.cookie = cookie;
+	return cookie;
+}
+
+static dma_cookie_t nlm_tx_submit (struct dma_async_tx_descriptor *tx)
+{
+	enum dma_transaction_type optype;
+	int rc, vc_id;
+	uint32_t disks, freeback_msg_dest_id;
+	dma_cookie_t cookie;
+	int i;
+
+	enum nlm_raid_type raid_type;
+	enum nlm_write_syndrome operation;
+	enum nlm_message_type msgtype;
+
+	struct nlm_tx_desc *nlm_tx ;
+	struct nlm_adma_chan *chan ;
+
+	uint32_t msgstatus1;
+
+	uint64_t raid_list_msg[DTRE_RAID_LIST_MSG_SIZE] = { 0ULL };
+	uint64_t transfer_msg[DTRE_TRANSFER_MSG_SIZE] = { 0ULL };
+
+	uint64_t pop_data[4];
+	uint32_t pop_vc, pop_src, pop_size, pop_code;
+
+	disks = 0;
+	nlm_tx = tx_to_nlm_adma_tx(tx);
+	chan = (struct nlm_adma_chan *)(nlm_tx->chan);
+
+	spin_lock_bh(&chan->lock);
+	cookie = nlm_desc_assign_cookie(chan, nlm_tx);
+
+	optype = nlm_tx->optype;
+	freeback_msg_dest_id = (netlogic_cpu_id()*16)+(netlogic_thr_id()*4);
+
+	/* Handle the DMA_MEMCPY case first and return.
+	   All RAID messages are handled later 
+	   */
+	if (optype == DMA_MEMCPY)
+	{
+		transfer_msg[0] = 
+			(1ULL << 63)
+			| shift_lower_bits(0, 60, 3)
+			| shift_lower_bits(8*(nlm_tx->len), 40, 20)
+			| shift_lower_bits (virt_to_phys ((volatile void *) nlm_tx->hw_desc.src), 0, 40);
+
+		transfer_msg[1] = 
+			(0ULL << 63)
+			| shift_lower_bits (1, 56, 1) /* Inform Source */
+			| shift_lower_bits (freeback_msg_dest_id, 44, 12);
+
+		transfer_msg[2] = 
+			(0ULL << 63)
+			| shift_lower_bits (1, 40, 1) /* perform transfer */
+			| shift_lower_bits (virt_to_phys ((volatile void *) nlm_tx->hw_desc.dst), 0, 40);
+
+		transfer_msg[3] = gen_dtr_raid_msg_format_2(nlm_tx);
+
+		vc_id = DTRE_MIN_VC + 1;
+		msgtype = p2d;
+
+		rc = xlp_message_send(vc_id, msgtype, 0, transfer_msg);
+		if (rc != 0) {
+			printk("Error:unable to send DTRE Transf msg %d\n",rc);
+		}
+		else
+		{
+#if 0
+			printk("*sent msg CPY 0x%llx 0x%llx 0x%llx 0x%llx to %d optype %d cookie %d.\n",
+					transfer_msg[0],
+					transfer_msg[1],
+					transfer_msg[2],
+					transfer_msg[3],
+					vc_id, optype, cookie);
+#endif
+		}
+
+		return cookie;
+	}
+
+	if (optype == DMA_XOR)
+	{
+		raid_type = raid5;  	// raid5
+		operation = writeop;  	// WRITE operation
+		msgtype = p2p;    	// P2P msg type
+		disks = (nlm_tx->hw_desc.src_cnt) + 1;
+	}
+
+	if (optype == DMA_XOR_VAL)
+	{
+		raid_type = raid5;  	// raid5
+		operation = readop;  	// READ operation
+		msgtype = p2p;    	// P2P msg type
+		disks = (nlm_tx->hw_desc.src_cnt);
+	}
+
+	if (optype == DMA_PQ)
+	{
+		raid_type = raid6;  	// raid6
+		operation = writeop;  	// WRITE operation
+		msgtype = p2p;    	// P2P msg type
+		disks = (nlm_tx->hw_desc.src_cnt) + 2;
+	}
+
+	if (optype == DMA_PQ_VAL)
+	{
+		raid_type = raid6;  	// raid6
+		operation = readop;  	// READ operation
+		msgtype = p2p;    	// P2P msg type
+		disks = (nlm_tx->hw_desc.src_cnt) + 2;
+	}
+
+	vc_id = DTRE_MIN_VC;
+
+	raid_list_msg [0] = gen_dtr_raid_msg_format_0((nlm_tx->entries_count), nlm_tx->hw_desc.entries);
+	raid_list_msg [1] = gen_dtr_raid_msg_format_1(raid_type, operation, disks, freeback_msg_dest_id);
+	raid_list_msg [2] = gen_dtr_raid_msg_format_2(nlm_tx);
+
+	for (i=0; i<16; i++)
+	{
+		rc = xlp_message_send(vc_id, msgtype, 0, raid_list_msg);
+		if (rc == 0)
+			break;
+
+		/* pop out messages from vc, if any */
+		pop_vc = freeback_msg_dest_id;
+
+		if ((nlm_hal_recv_msg2(pop_vc, &pop_src, &pop_size, &pop_code, &pop_data[0], &pop_data[1])) == 0)
+		{
+			if (nlm_dtre_debug)
+				printk("POP msg found in vc.\n");
+			nlm_dtre_msgring_handler(pop_vc, pop_src, pop_size, pop_code, pop_data[0], pop_data[1], pop_data[2], pop_data[3], NULL);
+		}
+	}
+
+	if (rc != 0) {
+		msgstatus1 = xlp_read_status1();
+		printk("Error: unable to send DTRE RAID msg: rc:%d, msgstatus1: 0x%08x.\n", rc, msgstatus1);
+		return -ENODEV;
+	}
+	else
+	{
+		if (nlm_dtre_debug) {
+			printk("sent msg RAID 0x%llx 0x%llx 0x%llx to %d optype %d cookie %d.\n",
+					raid_list_msg[0],
+					raid_list_msg[1],
+					raid_list_msg[2],
+					vc_id, optype, cookie);
+		}
+	}
+
+	if ((chan->tx_queue[chan->pending_idx]) != NULL)
+	{
+		/* TODO: error action TBD */
+		printk("DTRE Error: tx seems to be full, overwriting.\n");
+	}
+
+	chan->tx_queue[chan->pending_idx] = nlm_tx;
+
+	if (nlm_dtre_debug)
+		printk("pending_idx: %d.\n", chan->pending_idx);
+
+	chan->pending_idx = (chan->pending_idx + 1) & DTRE_MAX_TX_Q_MASK;
+
+	spin_unlock_bh(&chan->lock);
+
+	return cookie;
+
+}
+
+
+/* returns the actual number of allocated descriptors */
+static int nlm_adma_alloc_chan_resources(struct dma_chan *chan)
+{
+	struct nlm_adma_chan * nlm_chan = to_nlm_adma_chan(chan);
+
+	return (nlm_chan->num_tx_desc);
+}
+
+
+/* Called with chan->lock held */
+static void free_tx_desc(struct nlm_tx_desc *desc)
+{
+	struct nlm_adma_chan *chan = (struct nlm_adma_chan *)desc->chan;
+
+	/* return the descriptor back to the channel's pool */
+	desc->next = chan->desc_pool_head.next;
+	chan->desc_pool_head.next = desc;
+}
+
+
+static struct nlm_tx_desc *alloc_tx_desc(struct nlm_adma_chan *chan)
+{
+	struct nlm_tx_desc *ptr;
+
+	spin_lock_bh(&chan->lock);
+	ptr = chan->desc_pool_head.next;
+	if(ptr != NULL) {
+		chan->desc_pool_head.next = ptr->next;
+		ptr->next = NULL;
+		chan->alloc_desc++;
+		spin_unlock_bh(&chan->lock);
+		return ptr;
+	} else {
+		/* dynamic allocation */
+		ptr = kzalloc(sizeof(struct nlm_tx_desc), GFP_KERNEL);
+		if(!ptr) {
+			spin_unlock_bh(&chan->lock);
+			panic("OUT OF TX descriptors");
+		}
+		dma_async_tx_descriptor_init(&ptr->async_tx, &chan->common);
+		ptr->async_tx.tx_submit = nlm_tx_submit;
+		ptr->chan = (void *)chan;
+		chan->alloc_desc++;
+		spin_unlock_bh(&chan->lock);
+		return ptr;
+	}
+
+}
+
+
+/* Called with chan->lock held */
+static void __process_completed_tx(struct nlm_adma_chan * chan)
+{
+	struct nlm_tx_desc *ptr;
+	dma_cookie_t cookie = 0;
+	int i, loop;
+
+	if (chan->process_idx == chan->pending_idx)
+	{
+		/* nothing to process */
+
+		for (i=0; i<DTRE_MAX_TX_Q_LEN; i++)
+		{
+			if (chan->tx_queue[i] != NULL)
+			{
+				printk("DTRE index %d found, expected to be empty\n", i);
+			}
+		}
+
+		return;
+	}
+
+	loop = 0;
+	while (1)
+	{
+		ptr = chan->tx_queue[chan->process_idx];
+
+		if ((ptr->async_tx.cookie > 0) && (ptr->done_flag == 1))
+		{
+			if (nlm_dtre_debug)
+				printk("* process_idx: %d.\n", chan->process_idx);
+			cookie = ptr->async_tx.cookie;
+			ptr->async_tx.cookie = 0;
+			ptr->done_flag = 2;
+
+			if (ptr->async_tx.callback)
+			{
+				ptr->async_tx.callback(ptr->async_tx.callback_param);
+			}
+
+			chan->completed_cookie = cookie;
+
+			dma_run_dependencies(&ptr->async_tx);
+		}
+
+		if (!async_tx_test_ack(&ptr->async_tx))
+		{
+			tasklet_schedule(&chan->irq_tasklet);
+			break;
+		}
+
+		if (ptr->done_flag == 2)
+		{
+			if (nlm_dtre_debug)
+				printk("pro_idx: %d.\n", chan->process_idx);
+			ptr->done_flag = 0;
+			chan->tx_queue[chan->process_idx] = NULL;
+			chan->process_idx = (chan->process_idx + 1) & DTRE_MAX_TX_Q_MASK;
+			free_tx_desc(ptr);
+		}
+
+		if (chan->process_idx == chan->pending_idx)
+		{
+			for (i=0; i<DTRE_MAX_TX_Q_LEN; i++)
+			{
+				if (chan->tx_queue[i] != NULL)
+				{
+					printk("DTRE idx %d found, expected to be empty\n", i);
+				}
+			}
+			break;
+		}
+
+		/* exit clause */
+		if (loop++ > 1000)
+		{
+			loop = 0;
+			tasklet_schedule(&chan->irq_tasklet);
+			break;
+		}
+	}
+}
+
+static void process_completed_tx(struct nlm_adma_chan * nlm_chan)
+{
+	spin_lock_bh(&nlm_chan->lock);
+	__process_completed_tx(nlm_chan);
+	spin_unlock_bh(&nlm_chan->lock);
+}
+
+static void nlm_adma_tasklet(unsigned long data)
+{
+	struct nlm_adma_chan *nlm_chan = (struct nlm_adma_chan *) data;
+
+	spin_lock(&nlm_chan->lock);
+	__process_completed_tx(nlm_chan);
+	spin_unlock(&nlm_chan->lock);
+}
+
+static void nlm_adma_free_chan_resources(struct dma_chan *chan)
+{
+	struct nlm_adma_chan * nlm_chan = to_nlm_adma_chan(chan);
+	process_completed_tx(nlm_chan);
+}
+
+
+/**
+ * nlm_adma_is_complete - poll the status of an ADMA transaction
+ * @chan: ADMA channel handle
+ * @cookie: ADMA transaction identifier
+ */
+static enum dma_status nlm_adma_is_complete(struct dma_chan *chan,
+		dma_cookie_t cookie, 
+		dma_cookie_t *done, 
+		dma_cookie_t *used)
+{
+	struct nlm_adma_chan *nlm_chan = to_nlm_adma_chan(chan);
+	dma_cookie_t last_used;
+	dma_cookie_t last_complete;
+	enum dma_status ret;
+
+	rmb();
+	last_used = chan->cookie;
+	last_complete = nlm_chan->completed_cookie;
+
+	if (done)
+		*done = last_complete;
+
+	if (used)
+		*used = last_used;
+
+	ret = dma_async_is_complete(cookie, last_complete, last_used);
+	if (ret == DMA_SUCCESS)
+		return ret;
+
+	process_completed_tx(nlm_chan);
+
+	rmb();
+	last_used = chan->cookie;
+	last_complete = nlm_chan->completed_cookie;
+
+	if (done)
+		*done = last_complete;
+
+	if (used)
+		*used = last_used;
+
+	ret = dma_async_is_complete(cookie, last_complete, last_used);
+
+	return ret;
+
+}
+
+static void nlm_adma_issue_pending(struct dma_chan *chan)
+{
+	struct nlm_adma_chan * nlm_chan = to_nlm_adma_chan(chan);
+
+	process_completed_tx(nlm_chan);
+}
+
+
+
+static struct dma_async_tx_descriptor *
+nlm_adma_prep_dma_xor(struct dma_chan *chan, dma_addr_t dma_dest,
+		dma_addr_t *dma_src, unsigned int src_cnt, size_t len,
+		unsigned long flags)
+{
+	int i, p_device_id, disks;
+	struct nlm_tx_desc *desc;
+	struct nlm_adma_chan *nlm_chan = to_nlm_adma_chan(chan);
+	uint64_t * ent;
+
+	if (nlm_dtre_debug) {
+		printk("dma_xor src_cnt %d len 0x%lx.\n", src_cnt, len);
+	}
+
+	disks = src_cnt + 1;
+	p_device_id = src_cnt + 1;
+
+	if (disks > DTRE_RAID_MAX_DEVICES)
+	{
+		printk("DTRE Error: device count %d exceeds max %d\n",
+				disks, DTRE_RAID_MAX_DEVICES);
+		return (NULL);
+	}
+
+	desc = alloc_tx_desc(nlm_chan);
+
+	if (!desc)
+	{
+		printk("DTRE Error: cannot allocate desc\n");
+		return (NULL);
+	}
+
+	ent = desc->hw_desc.entries;
+
+	desc->optype = DMA_XOR;
+	desc->len = (unsigned int)len;
+	desc->int_en = flags & DMA_PREP_INTERRUPT;
+	desc->async_tx.flags = flags;
+	desc->async_tx.cookie = -EBUSY;
+	desc->done_flag = 0;
+	desc->xor_check_result = 0;
+	desc->entries_count = 0;
+	desc->hw_desc.src_cnt = src_cnt;
+	desc->hw_desc.src = 0;
+	desc->hw_desc.dst = 0;
+
+	for (i=0; i<DTRE_RAID_MAX_ENTRIES; i++)
+		desc->hw_desc.entries[i] = 0ULL;
+
+	/* fill the coefficients entry */
+	for (i=0; i<disks; i++)
+	{
+		/* set the Coefficient bit */
+		ent[i] = (1ULL << 63);
+
+		/* PCoefficient */
+		ent[i] |= shift_lower_bits (1, 0, 8);
+
+		/* QCoefficient */
+		ent[i] |= shift_lower_bits ((1 << i), 8, 8);
+
+		if ((i+1) == p_device_id)
+		{
+			/* fill for dest also , this is the p device id*/
+			ent[i] |= shift_lower_bits (1, 40, 2);
+
+			/* PCoefficient */
+			ent[i] |= shift_lower_bits (1, 0, 8);
+			ent[i] |= shift_lower_bits (0, 8, 8);
+
+		}
+		desc->entries_count++;
+	}
+	// printk("* check 1, entries %d.\n", desc->entries_count);
+
+	/* fill the segments entry */
+	for (i=0; i<disks; i++)
+	{
+		/* segment entry bit */
+		ent[disks + i] = (0ULL << 63);
+
+		/* SOD */
+		ent[disks + i] |= shift_lower_bits (1, 62, 1);
+
+		/* EOD */
+		ent[disks + i] |= shift_lower_bits (1, 61, 1);
+
+		/* segment length */
+		ent[disks + i] |= shift_lower_bits(desc->len, 40, 20);
+
+		/* segment address */
+		if (i < src_cnt)
+			ent[disks + i] |= shift_lower_bits(virt_to_phys ((volatile void *) dma_src[i]), 0, 40);
+
+		if ((i+1) == p_device_id)
+			ent[disks + i] |= shift_lower_bits(virt_to_phys ((volatile void *) dma_dest), 0, 40);
+
+		desc->entries_count++;
+	}
+
+	// printk("* check 2, entries %d.\n", desc->entries_count);
+
+	return &desc->async_tx;
+}
+
+
+static struct dma_async_tx_descriptor *
+nlm_adma_prep_dma_xor_val(struct dma_chan *chan, dma_addr_t *dma_src,
+		unsigned int src_cnt, size_t len, 
+		enum sum_check_flags *result, unsigned long flags)
+{
+	int i, p_device_id, disks;
+	struct nlm_tx_desc *desc;
+	struct nlm_adma_chan *nlm_chan = to_nlm_adma_chan(chan);
+	uint64_t * ent;
+
+	if (nlm_dtre_debug) {
+		printk("* fn dma_xor_val src_cnt %d len 0x%lx.\n", src_cnt, len);
+	}
+
+	/* assume last dev is p_device_id */
+	disks = src_cnt;
+	p_device_id = src_cnt;
+
+	if (disks > DTRE_RAID_MAX_DEVICES)
+	{
+		printk("DTRE Error: device count %d exceeds max %d\n",
+				disks, DTRE_RAID_MAX_DEVICES);
+		return (NULL);
+	}
+
+	desc = alloc_tx_desc(nlm_chan);
+
+	if (!desc)
+	{
+		printk("DTRE Error: cannot allocate desc\n");
+		return (NULL);
+	}
+
+	ent = desc->hw_desc.entries;
+
+	desc->optype = DMA_XOR_VAL;
+	desc->len = (unsigned int)len;
+	desc->int_en = flags & DMA_PREP_INTERRUPT;
+	desc->async_tx.flags = flags;
+	desc->async_tx.cookie = -EBUSY;
+	desc->done_flag = 0;
+	desc->xor_check_result = result;
+	desc->entries_count = 0;
+	desc->hw_desc.src_cnt = src_cnt;
+	desc->hw_desc.src = 0;
+	desc->hw_desc.dst = 0;
+
+	for (i=0; i<DTRE_RAID_MAX_ENTRIES; i++)
+		desc->hw_desc.entries[i] = 0ULL;
+
+	/* fill the coefficients entry */
+	for (i=0; i<disks; i++)
+	{
+		/* set the Coefficient bit */
+		ent[i] = (1ULL << 63);
+
+		/* PCoefficient */
+		ent[i] |= shift_lower_bits (1, 0, 8);
+
+		/* QCoefficient */
+		ent[i] |= shift_lower_bits ((1 << i), 8, 8);
+
+		if ((i+1) == p_device_id)
+		{
+			/* fill for dest also , this is the p device id*/
+			ent[i] |= shift_lower_bits (1, 40, 2);
+
+			/* PCoefficient */
+			ent[i] |= shift_lower_bits (1, 0, 8);
+			ent[i] |= shift_lower_bits (0, 8, 8);
+
+		}
+		desc->entries_count++;
+	}
+	// printk("* xor_val check 1, entries %d.\n", desc->entries_count);
+
+	/* fill the segments entry */
+	for (i=0; i<disks; i++)
+	{
+		/* segment entry bit */
+		ent[disks + i] = (0ULL << 63);
+
+		/* SOD */
+		ent[disks + i] |= shift_lower_bits (1, 62, 1);
+
+		/* EOD */
+		ent[disks + i] |= shift_lower_bits (1, 61, 1);
+
+		/* segment length */
+		ent[disks + i] |= shift_lower_bits(desc->len, 40, 20);
+
+		/* segment address */
+		if (i < src_cnt)
+			ent[disks + i] |= shift_lower_bits(virt_to_phys ((volatile void *) dma_src[i]), 0, 40);
+
+		desc->entries_count++;
+	}
+
+	// printk("* check 2, entries %d.\n", desc->entries_count);
+
+	return &desc->async_tx;
+}
+
+
+
+static void free_initial_tx_desc_pool(struct nlm_adma_chan *nlm_chan)
+{
+	struct nlm_tx_desc *desc, *next;
+
+	desc = nlm_chan->desc_pool_head.next;
+	while(desc) {
+		next = desc->next;
+		kfree(desc);
+		desc = next;
+	}
+	nlm_chan->desc_pool_head.next = NULL;
+}
+
+static int alloc_initial_tx_desc_pool (struct nlm_adma_chan *nlm_chan, int count)
+{
+	int i;
+	struct nlm_tx_desc *ptr, *head;
+
+	head = &nlm_chan->desc_pool_head;
+	head->next = NULL;
+
+	for(i=0; i < count; i++) {
+		ptr = kzalloc(sizeof(struct nlm_tx_desc), GFP_KERNEL);
+		if(!ptr) {
+			break;
+		}
+
+		dma_async_tx_descriptor_init(&ptr->async_tx, &nlm_chan->common);
+		ptr->async_tx.tx_submit = nlm_tx_submit;
+
+		/* insert at the head */
+		ptr->next = head->next;
+		head->next = ptr;
+		ptr->chan = (void *)nlm_chan;
+	}
+
+	printk("Channel %d: Allocated %d tx descriptors\n", nlm_chan->chan, count);
+	nlm_chan->num_tx_desc = count;
+
+	return 0;
+
+}
+
+static int __devinit nlm_adma_probe(struct platform_device *pdev)
+{
+	struct nlm_adma_device *adev;
+	struct dma_device *dma_dev;
+	struct nlm_adma_chan *nlm_chan;
+	int i, err;
+	uint64_t base;
+	uint32_t value;
+
+	/*
+	adev = kzalloc(sizeof(*adev), GFP_KERNEL);
+	if (!adev)
+		return -ENOMEM;
+		*/
+
+	adev = &nlm_adma_raid_device;
+
+	//dma_dev = &(adev->common);
+	dma_dev = &nlm_adma_raid_device.common;
+
+	if (register_xlp_msgring_handler(XLP_MSG_HANDLE_DTRE, nlm_dtre_msgring_handler, NULL)){
+		printk("Error: NLM-ADMA unable to register for msgring handler\n");
+		return -1;
+	}
+
+	INIT_LIST_HEAD(&dma_dev->channels);
+
+	/* channel init and allocation */
+	nlm_chan = kzalloc(sizeof(*nlm_chan), GFP_KERNEL);
+	if (!nlm_chan) {
+		kfree(adev);
+		return -ENOMEM;
+	}
+
+	nlm_chan->device = adev;
+	spin_lock_init(&nlm_chan->lock);
+	nlm_chan->common.device = dma_dev;
+	list_add_tail(&nlm_chan->common.device_node, &dma_dev->channels);
+
+	err = alloc_initial_tx_desc_pool(nlm_chan, DTRE_MAX_TX_DESC_PER_CHAN);
+
+	if(err != 0) {
+		free_initial_tx_desc_pool(nlm_chan);
+		kfree(adev);
+		kfree(nlm_chan);
+		return -ENOMEM;
+	}
+
+	for (i=0; i<DTRE_MAX_TX_Q_LEN; i++)
+		nlm_chan->tx_queue[i] = NULL;
+
+	tasklet_init(&nlm_chan->irq_tasklet, nlm_adma_tasklet, 
+			(unsigned long) nlm_chan);
+
+	/* set base routines */
+
+	dma_cap_set(DMA_XOR, dma_dev->cap_mask);
+	dma_cap_set(DMA_XOR_VAL, dma_dev->cap_mask);
+	// dma_cap_set(DMA_PQ, dma_dev->cap_mask);
+	// dma_cap_set(DMA_PQ_VAL, dma_dev->cap_mask);
+
+	// dma_cap_set(DMA_INTERRUPT, dma_dev->cap_mask);
+	// dma_cap_set(DMA_MEMCPY, dma_dev->cap_mask);
+	// dma_cap_set(DMA_MEMSET, dma_dev->cap_mask);
+
+	dma_dev->device_alloc_chan_resources = nlm_adma_alloc_chan_resources;
+	dma_dev->device_free_chan_resources = nlm_adma_free_chan_resources;
+	dma_dev->device_is_tx_complete = nlm_adma_is_complete;
+	dma_dev->device_issue_pending = nlm_adma_issue_pending;
+
+	// dma_dev->device_prep_dma_interrupt = nlm_adma_prep_dma_interrupt;
+	// dma_dev->device_prep_dma_memcpy = nlm_adma_prep_dma_memcpy;
+	// dma_dev->device_prep_dma_memset = nlm_adma_prep_dma_memset;
+
+	dma_dev->max_xor = nlm_adma_get_max_xor();
+	dma_dev->device_prep_dma_xor = nlm_adma_prep_dma_xor;
+	dma_dev->device_prep_dma_xor_val = nlm_adma_prep_dma_xor_val;
+
+	// dma_set_maxpq(dma_dev, nlm_adma_get_max_pq(), 0);
+	// dma_dev->device_prep_dma_pq = nlm_adma_prep_dma_pq;
+	// dma_dev->device_prep_dma_pq_val = nlm_adma_prep_dma_pq_val;
+
+	dma_dev->dev = &pdev->dev;
+	dma_async_device_register(dma_dev);
+	printk("NLM ASYNC Device Registered\n");
+
+	/* hal init stuff */
+	nlm_hal_dtr_init();
+
+	base = nlm_hal_get_dev_base (0 /*node*/, 0 /*B*/, 5 /*D*/, 0 /*F*/);
+	value = nlm_hal_read_32bit_reg (base, 0x40);
+	nlm_hal_write_32bit_reg (base, 0x40, value | (0x10));  /*maybe 0x30? */
+
+	/* Configure credits to non-n0c0 cores */
+	nlm_hal_fmn_init(0x10000000, 0x02000000, 50);
+
+	return 0;
+}
+
+
+static int __devexit nlm_adma_remove(struct platform_device *dev)
+{
+	return 0;
+}
+
+
+MODULE_ALIAS("platform:nlm-adma");
+
+static struct platform_driver nlm_adma_driver = {
+	.probe		= nlm_adma_probe,
+	.remove		= nlm_adma_remove,
+	.driver		= {
+		.owner	= THIS_MODULE,
+		.name	= "NLM-ADMA",
+	},
+};
+
+static int __init nlm_adma_init (void)
+{
+	return platform_driver_register(&nlm_adma_driver);
+}
+
+static void __exit nlm_adma_exit (void)
+{
+	platform_driver_unregister(&nlm_adma_driver);
+	return;
+}
+
+module_init(nlm_adma_init);
+module_exit(nlm_adma_exit);
+
+
+static __init int nlm_add_dma_dev(void)
+{
+	struct platform_device *pd;
+	int ret;
+
+	pd = platform_device_alloc("NLM-ADMA", -1);
+	if (!pd)
+		return -ENOMEM;
+
+	ret = platform_device_add(pd);
+	if (ret)
+		platform_device_put(pd);
+
+	return ret;
+}
+device_initcall(nlm_add_dma_dev);
+
+
+MODULE_AUTHOR("Netlogic");
+MODULE_DESCRIPTION("Netlogic ADMA Engine Driver");
+MODULE_LICENSE("GPL");
diff --git a/drivers/dma/nlm_adma.h b/drivers/dma/nlm_adma.h
new file mode 100644
index 0000000..e702cf3
--- /dev/null
+++ b/drivers/dma/nlm_adma.h
@@ -0,0 +1,113 @@
+/***********************************************************************
+Copyright 2003-2011 Netlogic Microsystems (“Netlogic”). All rights
+reserved.
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are
+met:
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+*****************************#NETL_2#********************************/
+
+#ifndef __NLM_ADMA_H__
+
+#define __NLM_ADMA_H__
+
+#include <linux/interrupt.h>
+#include <linux/dmaengine.h>
+
+#define DTRE_MAX_TX_DESC_PER_CHAN 	4096
+#define DTRE_MAX_TX_Q_LEN		(DTRE_MAX_TX_DESC_PER_CHAN *2)
+#define DTRE_MAX_TX_Q_MASK		(DTRE_MAX_TX_Q_LEN - 1)
+#define DTRE_MAX_LIST_LENGTH 		8
+#define DTRE_RAID_LIST_MSG_SIZE 	3
+#define DTRE_TRANSFER_MSG_SIZE		4
+#define DTRE_RAID_MESSAGE_TYPE		6
+#define DTRE_RAID_ENTRY_SIZE		8
+#define DTRE_RAID_MAX_ENTRIES		384
+#define DTRE_RAID_MAX_DEVICES		24
+#define DTRE_RAID_MAX_SRC		16
+
+#define DTRE_NUM_VC		4
+#define DTRE_MIN_VC		264
+#define DTRE_MAX_VC		267
+
+
+
+struct nlm_adma_device {
+	struct platform_device *pdev;
+	struct dma_device common;
+	spinlock_t lock;
+};
+
+struct nlm_hw_desc {
+	uint64_t entries [DTRE_RAID_MAX_ENTRIES] __attribute__((__aligned__(64)));
+	uint64_t src;
+	uint64_t dst;
+	uint32_t src_cnt;
+};
+
+struct nlm_tx_desc {
+	struct nlm_tx_desc * next;
+	struct dma_async_tx_descriptor async_tx;
+	enum dma_transaction_type optype;
+	unsigned int len;
+	int     int_en;
+	struct nlm_hw_desc hw_desc;
+	unsigned int entries_count;
+	int done_flag;
+	u32 *xor_check_result;
+	void    *chan; /* backpointer to channel */
+};
+
+struct nlm_adma_chan {
+	int chan; /* channel number */
+	dma_cookie_t completed_cookie;
+	spinlock_t lock; /* protects the descriptor slot pool */
+	struct nlm_adma_device *device;
+	struct dma_chan common;
+	struct nlm_tx_desc desc_pool_head; /* Free TX descriptors are linked here */
+	int num_tx_desc;
+	uint64_t alloc_desc;
+	struct tasklet_struct irq_tasklet;
+
+	/* transactions are queued here */
+	struct nlm_tx_desc * tx_queue[DTRE_MAX_TX_Q_LEN];
+	int pending_idx, process_idx;
+};
+
+enum nlm_raid_type {
+	raid5 = 0x1,
+	raid6 = 0x2
+};
+
+enum nlm_write_syndrome {
+	readop = 0,
+	writeop = 1
+};
+
+enum nlm_message_type {
+	p2p = 3,
+	p2d = 4
+};
+
+#define to_nlm_adma_chan(chan) container_of(chan, struct nlm_adma_chan, common)
+#define to_nlm_adma_device(dev) \
+	container_of(dev, struct nlm_adma_device, common)
+#define tx_to_nlm_adma_tx(tx) container_of(tx, struct nlm_tx_desc, async_tx)
+
+#endif
-- 
1.8.4.93.g57e4c17

