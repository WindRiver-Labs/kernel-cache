From edb4e98b585b0020975db7905863016138b54e7b Mon Sep 17 00:00:00 2001
From: Yanjiang Jin <yanjiang.jin@windriver.com>
Date: Fri, 11 Oct 2013 15:47:33 +0800
Subject: [PATCH] bcm-xlp: update HUGETLB for XLP HPW

The below error happens because of we didn't set or clear bit 61 which tells XLP
hardware page walker it is a hpage before.
Add set/clear steps when we apply/free the HUGEPAGE now.

CPU 6 Unable to handle kernel paging request at virtual address c02a0000e035c7e8,
 epc == ffffffffc116475c, ra == ffffffffc1164970
Oops[#1]:
CPU: 6 PID: 2003 Comm: hugemmap01 Not tainted 3.10.10-rt3-WR6.0.0.0_preempt-rt #1
task: c0000000f5dd07f8 ti: c0000000f56e0000 task.ti: c0000000f56e0000
$ 0   : 0000000000000000 ffffffffc1164970 002a00000035c7e8 c0000000fda307c0
$ 4   : 000600000007aed8 ffffffffffffffef ffffffffffffffff 0000000000000000
$ 8   : 0000000000000000 0000000000000029 0000000031202020 0000000020312020
$12   : c0000000f56efd00 000000001000001e 0000000020206361 000000006c6c2073
$16   : c02a0000e035c7e8 c0000000f3f119e0 000000002b000000 0000000000000000
$20   : 0000000000800000 c0000000f5db0000 c0000000fda00008 ffffffffc198ab80
$24   : 0000000000000009 00000000369482f0
$28   : c0000000f56e0000 c0000000f56efd08 0000000000000001 ffffffffc1164970
Hi    : 0000000000000000
Lo    : ffffffffcccccccd
epc   : ffffffffc116475c hugetlb_fault+0x394/0x8d0
    Not tainted
ra    : ffffffffc1164970 hugetlb_fault+0x5a8/0x8d0
Status: 7000b8e3    KX SX UX KERNEL EXL IE
Cause : 00800008ait |
BadVA : c02a0000e035c7e8
PrId  : 000c1201 (XLP208 Rev A1)
Modules linked in: loop
Process hugemmap01 (pid: 2003, threadinfo=c0000000f56e0000, task=c0000000f5dd07f8, tls=00000000368ba060)
Stack : c0000000f3c7d200 0000000000000029 000000002b000000 0000000000000003
      0000000000000029 000000002b000000 c0000000f3c7d200 c0000000f5dd07f8
      0000000000000001 c0000000f3c7d2a0 0000000000030002 fffffffffffffff7
      c0000000f3f119e0 ffffffffc1057c4c c000000000030002 c0000000f3c7d2a0
      000000002b000000 ffffffffc1138020 0000000000000000 0000000000000001
      0000000000100000 0000000000000001 c0000000f3f65500 0000000000ffffff
      0000000000000001 ffffffffc10a8190 c0000000f5dd07f8 ffffffffc10a9f70
      c0000000f5dd07f8 c0000000f3f65500 c0000000f56efe58 0000000000000000
      0000000000000000 0000000000800000 0000000000420000 0000000000420000
      0000000000420000 0000000000409ef8 000000000041c510 0000000000420000
      ...
Call Trace:
[<ffffffffc116475c>] hugetlb_fault+0x394/0x8d0
[<ffffffffc1057c4c>] do_page_fault+0x13c/0x3b0
[<ffffffffc103f1e4>] resume_userspace_check+0x0/0x10

Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 arch/mips/include/asm/hugetlb.h | 60 ++++++++++++++++++++++++++++++++---------
 arch/mips/mm/hugetlbpage.c      | 45 ++++++++++++++++++++++++++-----
 2 files changed, 87 insertions(+), 18 deletions(-)

diff --git a/arch/mips/include/asm/hugetlb.h b/arch/mips/include/asm/hugetlb.h
index fe0d15d..a44c38e 100644
--- a/arch/mips/include/asm/hugetlb.h
+++ b/arch/mips/include/asm/hugetlb.h
@@ -51,21 +51,53 @@ static inline void hugetlb_free_pgd_range(struct mmu_gather *tlb,
 	free_pgd_range(tlb, addr, end, floor, ceiling);
 }
 
+/**
+ * Fill the pte value to all pte's covered by the same huge page.
+ */
 static inline void set_huge_pte_at(struct mm_struct *mm, unsigned long addr,
 				   pte_t *ptep, pte_t pte)
 {
-	set_pte_at(mm, addr, ptep, pte);
+	unsigned long i;
+	unsigned long htlb_entries = 1 << HUGETLB_PAGE_ORDER;
+	pte_t entry2;
+
+	entry2 =  __pte(pte_val(pte) + (HPAGE_SIZE >> 1));
+
+	/* for hardware page walker, bit 61 tells hpw it is a hpage */
+	pte = __pte(pte_val(pte)  | (1ULL << 61));
+	entry2 = __pte(pte_val(entry2) | (1ULL << 61));
+
+	addr &= HPAGE_MASK;
+	for (i = 0; i < htlb_entries; i += 2) {
+		ptep = huge_pte_offset(mm, addr);
+		set_pte_at(mm, addr, ptep, pte);
+		addr += PAGE_SIZE;
+
+		ptep = huge_pte_offset(mm, addr);
+		set_pte_at(mm, addr, ptep, entry2);
+		addr += PAGE_SIZE;
+	}
 }
 
 static inline pte_t huge_ptep_get_and_clear(struct mm_struct *mm,
 					    unsigned long addr, pte_t *ptep)
 {
-	pte_t clear;
-	pte_t pte = *ptep;
+	pte_t entry;
+	unsigned long i;
+	unsigned long htlb_entries = 1 << HUGETLB_PAGE_ORDER;
+
+	entry = *ptep;
 
-	pte_val(clear) = (unsigned long)invalid_pte_table;
-	set_pte_at(mm, addr, ptep, clear);
-	return pte;
+	/* clear bit 61 before giving back to the upper level function */
+	entry = __pte(pte_val(entry) & ~(1ULL << 61));
+
+	addr &= HPAGE_MASK;
+	for (i = 0; i < htlb_entries; i++) {
+		ptep = huge_pte_offset(mm, addr);
+		pte_clear(mm, addr, ptep);
+		addr += PAGE_SIZE;
+	}
+	return entry;
 }
 
 static inline void huge_ptep_clear_flush(struct vm_area_struct *vma,
@@ -74,6 +106,15 @@ static inline void huge_ptep_clear_flush(struct vm_area_struct *vma,
 	flush_tlb_page(vma, addr & huge_page_mask(hstate_vma(vma)));
 }
 
+static inline pte_t huge_ptep_get(pte_t *ptep)
+{
+	/* Get the pte value for the even entry */
+	unsigned long pte = pte_val(*ptep) & ~(HPAGE_SIZE >> 1);
+
+	/* for XLP hpw, clear bit 61 which indicates hpw is a hpage */
+	return __pte(pte & ~(1ULL << 61));
+}
+
 static inline int huge_pte_none(pte_t pte)
 {
 	unsigned long val = pte_val(pte) & ~_PAGE_GLOBAL;
@@ -88,7 +129,7 @@ static inline pte_t huge_pte_wrprotect(pte_t pte)
 static inline void huge_ptep_set_wrprotect(struct mm_struct *mm,
 					   unsigned long addr, pte_t *ptep)
 {
-	ptep_set_wrprotect(mm, addr, ptep);
+	set_huge_pte_at(mm, addr, ptep, pte_wrprotect(huge_ptep_get(ptep)));
 }
 
 static inline int huge_ptep_set_access_flags(struct vm_area_struct *vma,
@@ -109,11 +150,6 @@ static inline int huge_ptep_set_access_flags(struct vm_area_struct *vma,
 	return changed;
 }
 
-static inline pte_t huge_ptep_get(pte_t *ptep)
-{
-	return *ptep;
-}
-
 static inline int arch_prepare_hugepage(struct page *page)
 {
 	return 0;
diff --git a/arch/mips/mm/hugetlbpage.c b/arch/mips/mm/hugetlbpage.c
index a7fee0d..c0a39cc 100644
--- a/arch/mips/mm/hugetlbpage.c
+++ b/arch/mips/mm/hugetlbpage.c
@@ -22,18 +22,47 @@
 #include <asm/tlb.h>
 #include <asm/tlbflush.h>
 
-pte_t *huge_pte_alloc(struct mm_struct *mm, unsigned long addr,
-		      unsigned long sz)
+pte_t *huge_pte_alloc_single(struct mm_struct *mm, unsigned long addr)
 {
 	pgd_t *pgd;
 	pud_t *pud;
+	pmd_t *pmd;
 	pte_t *pte = NULL;
 
 	pgd = pgd_offset(mm, addr);
 	pud = pud_alloc(mm, pgd, addr);
-	if (pud)
-		pte = (pte_t *)pmd_alloc(mm, pud, addr);
+	if (pud) {
+		pmd = (pmd_t *)pmd_alloc(mm, pud, addr);
+		if (pmd)
+			pte = pte_alloc_map(mm, NULL, pmd, addr);
+	}
+
+	return pte;
+}
+
+/**
+ * Given any address, we need to allocate page table entries
+ * for all pte's covered by the same huge page. This is needed if
+ * any address referencing the huge page faults and the tlb refill handler
+ * can refill the tlb entry with correct value.
+ *
+ * Return any valid pte pointer is fine as later on we still have
+ * "addr" to identify the correct huge page.
+ */
+pte_t *huge_pte_alloc(struct mm_struct *mm, unsigned long addr,
+		      unsigned long sz)
+{
+	pte_t *pte = NULL;
+	unsigned long i = 0;
+	unsigned long htlb_entries = 1 << HUGETLB_PAGE_ORDER;
 
+	addr &= HPAGE_MASK;
+	for (i = 0; i < htlb_entries; i++) {
+		pte = huge_pte_alloc_single(mm, addr);
+		if (!pte)
+			return NULL;
+		addr += PAGE_SIZE;
+	}
 	return pte;
 }
 
@@ -42,14 +71,18 @@ pte_t *huge_pte_offset(struct mm_struct *mm, unsigned long addr)
 	pgd_t *pgd;
 	pud_t *pud;
 	pmd_t *pmd = NULL;
+	pte_t *pte = NULL;
 
 	pgd = pgd_offset(mm, addr);
 	if (pgd_present(*pgd)) {
 		pud = pud_offset(pgd, addr);
-		if (pud_present(*pud))
+		if (pud_present(*pud)) {
 			pmd = pmd_offset(pud, addr);
+			if (pmd_present(*pmd))
+				pte = pte_offset_map(pmd, addr);
+		}
 	}
-	return (pte_t *) pmd;
+	return pte;
 }
 
 int huge_pmd_unshare(struct mm_struct *mm, unsigned long *addr, pte_t *ptep)
-- 
1.8.4.93.g57e4c17

