From 5ea3d5b595a95dbbf33a1a62194f85887cd358fb Mon Sep 17 00:00:00 2001
From: Yonghong Song <ysong@netlogicmicro.com>
Date: Fri, 20 Jan 2012 23:11:03 -0800
Subject: [PATCH 535/565] bcm-xlp: support more than 16GB memory

Support more than 16GB memory:

  o guarded with CONFIG_NLM_16G_MEM_SUPPORT

  o changes to map memory above a certain size in kernel page table

    Prior to this commit, all avaialable physical memory was wired.
    Since only 32 tlb entries can be wired, memory above 16GB
    (well really 15.75GB) must be mapped into kernel page table
    (swapper_pg_dir).

    However, this is trickly, since refill exception could be
    triggered in an exception context. Hence do_page_fault() has
    to patched with the appropriate functionality.

    Besides, the refill handler touches per_cpu memory in non-exception
    context and a host of kernel structures (such as vma_struct,
    task_struct, thread_info, mm_struct etc) in exception context.
    Hence those structures must come from wired area as recursive
    refill exceptions cannot be handled). Towards that end, appropriate
    flags (SLAB_CACHE_DMA) have to be passed to the slab allocator
    while creation and allocation of above objects.

    As part of above changes, the size of DMA region was increased to
    3GB from the earlier 2GB.

Based on Broadcom SDK 2.3.

Signed-off-by: Yonghong Song <ysong@netlogicmicro.com>
Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 arch/mips/Kconfig                            |   6 +
 arch/mips/include/asm/dma.h                  |   2 +-
 arch/mips/include/asm/mach-netlogic/mmu.h    |  25 ++++
 arch/mips/include/asm/mach-netlogic/mmzone.h |   5 +-
 arch/mips/include/asm/pgalloc.h              |  16 +++
 arch/mips/kernel/setup.c                     |   2 +-
 arch/mips/kernel/vmlinux.lds.S               |   2 +-
 arch/mips/mm/fault.c                         |  54 ++++++++
 arch/mips/mm/init.c                          |  40 ++++++
 arch/mips/netlogic/common/memory.c           | 194 +++++++++++++++++++++++++++
 arch/mips/netlogic/xlp/numa.c                |  21 +++
 fs/exec.c                                    |   4 +
 include/linux/gfp.h                          |   5 +
 kernel/fork.c                                |  28 ++++
 mm/memory.c                                  |  15 +++
 mm/mmap.c                                    |  20 +++
 mm/page_alloc.c                              |  20 +++
 mm/percpu-vm.c                               |   8 ++
 18 files changed, 463 insertions(+), 4 deletions(-)

diff --git a/arch/mips/Kconfig b/arch/mips/Kconfig
index e379c0f..707117c 100644
--- a/arch/mips/Kconfig
+++ b/arch/mips/Kconfig
@@ -1774,6 +1774,12 @@ config 64BIT
 	help
 	  Select this option if you want to build a 64-bit kernel.
 
+config NLM_16G_MEM_SUPPORT
+	bool "more than 16GB memory support"
+	depends on 64BIT
+	help
+	 Select this option if you want the kernel to support more than 16GB memory
+
 endchoice
 
 config KVM_GUEST
diff --git a/arch/mips/include/asm/dma.h b/arch/mips/include/asm/dma.h
index 4289d2c..57bb45f 100644
--- a/arch/mips/include/asm/dma.h
+++ b/arch/mips/include/asm/dma.h
@@ -88,7 +88,7 @@
 #define MAX_DMA_ADDRESS		PAGE_OFFSET
 #else
 #if defined(CONFIG_NLM_XLP) && defined(CONFIG_64BIT)
-#define MAX_DMA_ADDRESS		(PAGE_OFFSET + 0x80000000)
+#define MAX_DMA_ADDRESS		(PAGE_OFFSET + 0xc0000000)
 #else
 #define MAX_DMA_ADDRESS		(PAGE_OFFSET + 0x01000000)
 #endif
diff --git a/arch/mips/include/asm/mach-netlogic/mmu.h b/arch/mips/include/asm/mach-netlogic/mmu.h
index ec69f2c..2377955 100644
--- a/arch/mips/include/asm/mach-netlogic/mmu.h
+++ b/arch/mips/include/asm/mach-netlogic/mmu.h
@@ -17,9 +17,22 @@
 #define SMALLEST_TLBPAGE_SZ (4UL << 10)
 #define LARGEST_TLBPAGE_SZ  (256UL << 20)
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+#define MAX_WIRED_PFN PFN_UP(4UL << 30)
+#endif
+
 #define TRUE 1
 #define FALSE 0
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+typedef struct
+{
+	unsigned long entryHi;
+	unsigned long entrylo0;
+	unsigned long entrylo1;
+	int wired;
+} tlb_entry_t;
+#else
 typedef struct
 {
 	unsigned long vaddr;
@@ -30,6 +43,7 @@ typedef struct
 	uint32_t attr1;
 	int wired;
 } tlb_info_t;
+#endif
 
 #ifdef CONFIG_MAPPED_KERNEL
 extern unsigned long __vmalloc_start;
@@ -37,7 +51,10 @@ extern unsigned long __vmalloc_start;
 extern unsigned long long nlm_common_tlb_stats[];
 
 extern void mmu_init(void);
+
+#ifndef CONFIG_NLM_16G_MEM_SUPPORT
 extern void setup_tlb(tlb_info_t *tlb);
+#endif
 
 /*
  * the following needs an used argument to confirm to the 
@@ -58,5 +75,13 @@ extern unsigned long recalculate_max_low_pfn(unsigned long max_low_pfn);
 extern void __init nlm_numa_bootmem_init(unsigned long);
 #endif
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+extern int map_kernel_addrspace(unsigned long vaddr, unsigned long paddr,
+				unsigned long max_pfn);
+
+#define KERNEL_PAGE_ATTR \
+	(_CACHE_CACHABLE_COW |_PAGE_DIRTY |  _PAGE_VALID | _PAGE_GLOBAL)
+#endif
+
 #endif /* __ASSEMBLY__ */
 #endif
diff --git a/arch/mips/include/asm/mach-netlogic/mmzone.h b/arch/mips/include/asm/mach-netlogic/mmzone.h
index 42b7c42..47db26e 100644
--- a/arch/mips/include/asm/mach-netlogic/mmzone.h
+++ b/arch/mips/include/asm/mach-netlogic/mmzone.h
@@ -23,8 +23,11 @@ struct nlm_node_mem_frag {
 #define NLM_MAX_MEM_FRAGS_PER_NODE 16
 struct nlm_node_mem_info {
 	struct nlm_node_mem_frag mem[NLM_MAX_MEM_FRAGS_PER_NODE];
-	unsigned long free_addr; /* for node_data */
 	int frags;
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+	/* node 0: minimum start pfn about DMA region; other nodes: minimum start pfn */
+	int min_start_pfn;
+#endif
 };
 
 struct nlm_cpu_info {
diff --git a/arch/mips/include/asm/pgalloc.h b/arch/mips/include/asm/pgalloc.h
index 881d18b..a94f8a2 100644
--- a/arch/mips/include/asm/pgalloc.h
+++ b/arch/mips/include/asm/pgalloc.h
@@ -48,7 +48,11 @@ static inline pgd_t *pgd_alloc(struct mm_struct *mm)
 {
 	pgd_t *ret, *init;
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+	ret = (pgd_t *) __get_free_pages(GFP_DMA|GFP_KERNEL, PGD_ORDER);
+#else
 	ret = (pgd_t *) __get_free_pages(GFP_KERNEL, PGD_ORDER);
+#endif
 	if (ret) {
 		init = pgd_offset(&init_mm, 0UL);
 		pgd_init((unsigned long)ret);
@@ -69,7 +73,11 @@ static inline pte_t *pte_alloc_one_kernel(struct mm_struct *mm,
 {
 	pte_t *pte;
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+	pte = (pte_t *) __get_free_pages(GFP_DMA|GFP_KERNEL|__GFP_REPEAT|__GFP_ZERO, PTE_ORDER);
+#else
 	pte = (pte_t *) __get_free_pages(GFP_KERNEL|__GFP_REPEAT|__GFP_ZERO, PTE_ORDER);
+#endif
 
 	return pte;
 }
@@ -79,7 +87,11 @@ static inline struct page *pte_alloc_one(struct mm_struct *mm,
 {
 	struct page *pte;
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+	pte = alloc_pages(GFP_DMA | GFP_KERNEL | __GFP_REPEAT, PTE_ORDER);
+#else
 	pte = alloc_pages(GFP_KERNEL | __GFP_REPEAT, PTE_ORDER);
+#endif
 	if (pte) {
 		clear_highpage(pte);
 		pgtable_page_ctor(pte);
@@ -110,7 +122,11 @@ static inline pmd_t *pmd_alloc_one(struct mm_struct *mm, unsigned long address)
 {
 	pmd_t *pmd;
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+	pmd = (pmd_t *) __get_free_pages(GFP_DMA|GFP_KERNEL|__GFP_REPEAT, PMD_ORDER);
+#else
 	pmd = (pmd_t *) __get_free_pages(GFP_KERNEL|__GFP_REPEAT, PMD_ORDER);
+#endif
 	if (pmd)
 		pmd_init((unsigned long)pmd, (unsigned long)invalid_pte_table);
 	return pmd;
diff --git a/arch/mips/kernel/setup.c b/arch/mips/kernel/setup.c
index 0659dc3..2384ce9 100644
--- a/arch/mips/kernel/setup.c
+++ b/arch/mips/kernel/setup.c
@@ -589,7 +589,7 @@ static void __init arch_mem_init(char **cmdline_p)
 	}
     
 	bootmem_init();
-#if defined(CONFIG_NLM_XLP) && !defined(CONFIG_NUMA)
+#if defined(CONFIG_NLM_XLP) && !defined(CONFIG_NUMA) && !defined(CONFIG_NLM_16G_MEM_SUPPORT)
 	setup_mapped_kernel_tlbs(FALSE, TRUE);
 #endif
 	device_tree_init();
diff --git a/arch/mips/kernel/vmlinux.lds.S b/arch/mips/kernel/vmlinux.lds.S
index 0fd90af..a3854ee 100644
--- a/arch/mips/kernel/vmlinux.lds.S
+++ b/arch/mips/kernel/vmlinux.lds.S
@@ -78,7 +78,7 @@ SECTIONS
 	.data : {	/* Data */
 		. = . + DATAOFFSET;		/* for CONFIG_MAPPED_KERNEL */
 
-		INIT_TASK_DATA(THREAD_SIZE)
+		INIT_TASK_DATA(PAGE_SIZE)
 		NOSAVE_DATA
 		CACHELINE_ALIGNED_DATA(1 << CONFIG_MIPS_L1_CACHE_SHIFT)
 		READ_MOSTLY_DATA(1 << CONFIG_MIPS_L1_CACHE_SHIFT)
diff --git a/arch/mips/mm/fault.c b/arch/mips/mm/fault.c
index 88ffcdd..e3b7394 100644
--- a/arch/mips/mm/fault.c
+++ b/arch/mips/mm/fault.c
@@ -27,6 +27,49 @@
 #include <asm/highmem.h>		/* For VMALLOC_END */
 #include <linux/kdebug.h>
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+#define ENTER_CRITICAL(flags) local_irq_save(flags)
+#define EXIT_CRITICAL(flags) local_irq_restore(flags)
+
+extern void dump_pgtable(pgd_t *pgd);
+extern void print_pgtable(unsigned long, unsigned long);
+
+extern unsigned long NONWIRED_START, NONWIRED_END;
+
+static void update_kernel_tlb(unsigned long address)
+{
+	unsigned long flags;
+	pgd_t *pgdp;
+	pud_t *pudp;
+	pmd_t *pmdp;
+	pte_t *ptep;
+	int pid;
+	unsigned long config6_flags;
+
+	ENTER_CRITICAL(flags);
+	disable_pgwalker(config6_flags);
+
+	pid = read_c0_entryhi() & ASID_MASK;
+	address &= (PAGE_MASK << 1);
+	write_c0_entryhi(address | pid);
+	pgdp = pgd_offset_k(address);
+	mtc0_tlbw_hazard();
+	pudp = pud_offset(pgdp, address);
+	pmdp = pmd_offset(pudp, address);
+	{
+		ptep = pte_offset_map(pmdp, address);
+
+		write_c0_entrylo0(pte_to_entrylo(pte_val(*ptep++)));
+		write_c0_entrylo1(pte_to_entrylo(pte_val(*ptep)));
+		mtc0_tlbw_hazard();
+		tlb_write_random();
+	}
+	tlbw_use_hazard();
+	enable_pgwalker(config6_flags);
+	EXIT_CRITICAL(flags);
+}
+#endif
+
 /*
  * This routine handles page faults.  It determines the address,
  * and the problem, and then passes it off to one of the appropriate
@@ -78,6 +121,11 @@ asmlinkage void __kprobes do_page_fault(struct pt_regs *regs, unsigned long writ
 # define VMALLOC_FAULT_TARGET vmalloc_fault
 #endif
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+	if (unlikely(address >= NONWIRED_START && address < NONWIRED_END))
+		goto refill_kernel_tlb;
+#endif
+
 	if (unlikely(address >= VMALLOC_START && address <= VMALLOC_END))
 		goto VMALLOC_FAULT_TARGET;
 #ifdef MODULE_START
@@ -310,4 +358,10 @@ vmalloc_fault:
 		return;
 	}
 #endif
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+refill_kernel_tlb:
+	update_kernel_tlb(address);
+	current->thread.cp0_baduaddr = address;
+	return;
+#endif
 }
diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 1f21449..440a9da 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -88,7 +88,11 @@ void setup_zero_pages(void)
 	else
 		order = 0;
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+	empty_zero_page = __get_free_pages(GFP_DMA | GFP_KERNEL | __GFP_ZERO, order);
+#else
 	empty_zero_page = __get_free_pages(GFP_KERNEL | __GFP_ZERO, order);
+#endif
 	if (!empty_zero_page)
 		panic("Oh boy, that early out of memory?");
 
@@ -326,6 +330,10 @@ void __init paging_init(void)
 
 	pagetable_init();
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+	setup_mapped_kernel_tlbs(FALSE, TRUE);
+#endif
+
 #ifdef CONFIG_HIGHMEM
 	kmap_init();
 #endif
@@ -455,6 +463,38 @@ void __init_refok free_initmem(void)
 			__pa_symbol(&__init_end));
 }
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+#include <asm/mach-netlogic/mmu.h>
+
+int __init map_kernel_addrspace(unsigned long addr, unsigned long pfn,
+				unsigned long max_pfn)
+{
+	pgd_t *pgd;
+	unsigned long end;
+	unsigned long next;
+	pgprot_t prot;
+	int err;
+
+	printk("(%s): addr = 0x%lx, pfn = 0x%lx, max_pfn = 0x%lx\n", __func__,
+		addr, pfn, max_pfn);
+
+	end = addr + ((max_pfn - pfn) << PAGE_SHIFT);
+
+	prot = __pgprot(KERNEL_PAGE_ATTR);
+	pfn -= addr >> PAGE_SHIFT;
+	pgd = pgd_offset(&init_mm, addr);
+	do {
+		next = pgd_addr_end(addr, end);
+		err = remap_pud_range(&init_mm, pgd, addr, next,
+		pfn + (addr >> PAGE_SHIFT), prot);
+		if (err)
+			break;
+	} while (pgd++, addr = next, addr != end);
+
+	return err;
+}
+#endif /* CONFIG_NLM_16G_MEM_SUPPORT */
+
 #ifndef CONFIG_MIPS_PGD_C0_CONTEXT
 unsigned long pgd_current[NR_CPUS];
 #endif
diff --git a/arch/mips/netlogic/common/memory.c b/arch/mips/netlogic/common/memory.c
index cac5856..385629a 100644
--- a/arch/mips/netlogic/common/memory.c
+++ b/arch/mips/netlogic/common/memory.c
@@ -35,6 +35,12 @@
 #include <asm/page.h>
 #include <asm/mach-netlogic/mmu.h>
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+#include <linux/bootmem.h>
+#include <linux/gfp.h>
+#include <asm/pgtable.h>
+#endif
+
 /*
  * the following structures and definitions are internal to this
  * file and hence not defined in a header file
@@ -81,6 +87,30 @@ static uint32_t tlb_mask(uint32_t size)
 	return mipstlbs[i].mask;
 }
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+#define page_entrylo(paddr, attr) ((((paddr) >> 12) << 6) | (attr))
+
+/*
+ * External Function / APIs
+ */
+
+static void setup_tlb(tlb_entry_t *tlb, unsigned long pagesize)
+{
+	write_c0_pagemask(tlb_mask(pagesize) << 13);
+	write_c0_entryhi(tlb->entryHi & ~0x1fff);
+	write_c0_entrylo0(tlb->entrylo0);
+	write_c0_entrylo1(tlb->entrylo1);
+
+	if (tlb->wired) {
+		write_c0_index(read_c0_wired());
+		tlb_write_indexed();
+		write_c0_wired(read_c0_wired() + 1);
+	}
+	else {
+		tlb_write_random();
+	}
+}
+#else
 #define entrylo(paddr, attr) \
 	((((paddr & 0xffffffffffULL) >> 12) << 6) | (attr))
 
@@ -105,6 +135,7 @@ void setup_tlb(tlb_info_t *tlb)
 		tlb_write_random();
 	}
 }
+#endif
 
 #ifdef CONFIG_MAPPED_KERNEL
 
@@ -126,6 +157,167 @@ EXPORT_SYMBOL(__vmalloc_start);
 static volatile int max_low_pfn_set = 0;
 extern unsigned long max_low_pfn;
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+
+#ifdef CONFIG_64BIT
+#define KERNEL_SEG_START	XKSEG
+#else
+#define KERNEL_SEG_START	KSEG2
+#endif
+
+#define MIN(x,y)  ((x) < (y) ? (x) : (y))
+
+#undef alloc_bootmem_low
+
+static unsigned long
+__init alloc_bootmem_low(gfp_t gfp_mask, unsigned int order)
+{
+	return (unsigned long)__alloc_bootmem_low((1 << order) * PAGE_SIZE, SMP_CACHE_BYTES, 0);
+}
+
+unsigned long NONWIRED_START = ~0x0;
+unsigned long NONWIRED_END = ~0x0;
+
+void setup_mapped_kernel_tlbs(int firstpage, int primary_cpu)
+{
+	tlb_entry_t tlb;
+	unsigned long max_wired_size;
+	unsigned long pagesize;
+	unsigned long vaddr, paddr;
+	unsigned short attr;
+
+	pagesize = LARGEST_TLBPAGE_SZ; /* we set up the largest pages */
+
+	/*
+	 * In NetLogic's Linux kernel, the second 256MB of physical
+	 * address space is reserved for device configuration and
+	 * is not mapped to DRAM (to imply memory as opposed to IO
+	 * device space). Hence the attribute of the second part of
+	 * the first wired entry is invalid, while the both part of
+	 * other wired entries are symmetric. We handle the above
+	 * difference through the following unseemly if condition
+	 */
+	if (firstpage) {
+		tlb.entryHi = KERNEL_SEG_START;
+
+		attr = (KERNEL_PAGE_ATTR >> ENTRYLO_PFN_SHIFT);
+		tlb.entrylo0 = page_entrylo(0, attr); /* we start at pfn = 0 */
+
+		/* 256MB - 512 MB is IO config (invalid dram) */
+		attr = (_PAGE_GLOBAL >> ENTRYLO_PFN_SHIFT);
+		tlb.entrylo1 = page_entrylo(0, attr);
+
+		tlb.wired = TRUE;
+		setup_tlb(&tlb, pagesize);
+	}
+	else {
+		int retval;
+		/*
+		 * the primary cpu reads the memory map and records
+		 * the highest page frame number. Secondary cpus
+		 * must wait till the variable max_low_pfn is set
+		 */
+		if (!primary_cpu)
+			while (!max_low_pfn_set)
+				;
+		vaddr = KERNEL_SEG_START + 2 * LARGEST_TLBPAGE_SZ;
+		max_wired_size = PFN_PHYS(MIN(max_low_pfn, MAX_WIRED_PFN));
+		attr = (KERNEL_PAGE_ATTR >> ENTRYLO_PFN_SHIFT);
+
+		/*
+		 * the following loop assumes that pagesize is set to
+		 * 256 MB. change the logic if the pagesize ever changes
+		 */
+		paddr = 2 * LARGEST_TLBPAGE_SZ;
+		for (; paddr < max_wired_size;
+ 				paddr += 2 * pagesize, vaddr += 2 * pagesize) {
+#ifdef DEBUG
+			printk("(wired entry): vaddr = 0x%lx, paddr = 0x%lx\n", vaddr, paddr);
+#endif
+/* Skip 3 - 3.5GB range (PCI device space) */
+			if (paddr == PCIDEV_ADDRSPACE_START)
+				continue;
+			tlb.entryHi = vaddr;
+			tlb.entrylo0 = page_entrylo(paddr, attr);
+			tlb.entrylo1 = page_entrylo(paddr + pagesize, attr);
+			tlb.wired = TRUE;
+			setup_tlb(&tlb, pagesize);
+		}
+
+#ifdef CONFIG_NUMA
+		/* For NUMA, for each node, we have to wire the minimum physical page for that node.
+		 * NUMA uses that piece of memory immediately to keep its internal data structure.
+		 *
+		 * For node 0: the first chunk of memory above the MAX_DMA_ADDRESS needs to be wired
+		 * as the kernel will allocate space in that area before the exception handlers etc.
+		 * are setup.
+		 *
+		 * For other nodes, the first chunk of memory available on that node needs to be wired.
+		 */
+		for (node = 0; node < NLM_MAX_CPU_NODE; node ++) {
+			paddr = PFN_PHYS(node_mem_info[node].min_start_pfn);
+			if (paddr == 0)
+				continue;
+
+			/* for node 0, wire extra lines only if it is not wired yet */
+			if (node == 0 && node_mem_info[0].min_start_pfn < MAX_WIRED_PFN)
+				continue;
+
+			/* make sure the paddr is aligned also */
+			if (paddr % ( 2 * LARGEST_TLBPAGE_SZ))
+				paddr -= (paddr % (2 * LARGEST_TLBPAGE_SZ));
+
+			vaddr = KERNEL_SEG_START + paddr;
+			tlb.entryHi = vaddr;
+			tlb.entrylo0 = page_entrylo(paddr, attr);
+			tlb.entrylo1 = page_entrylo(paddr + pagesize, attr);
+			tlb.wired = TRUE;
+			setup_tlb(&tlb, pagesize);
+		}
+#endif
+	}
+}
+
+void __init setup_mapped_kernel_pgtable(void)
+{
+	unsigned long vaddr = KERNEL_SEG_START + PFN_PHYS(MIN(max_low_pfn, MAX_WIRED_PFN));
+	int retval;
+
+	if (max_low_pfn > MAX_WIRED_PFN) {
+		__get_free_pages = alloc_bootmem_low;
+		retval = map_kernel_addrspace(vaddr, MAX_WIRED_PFN, max_low_pfn);
+		if (retval != 0)
+			panic("unable to map kernel addrspace\n");
+		NONWIRED_START = vaddr;
+		NONWIRED_END = vaddr + (PFN_PHYS(max_low_pfn - MAX_WIRED_PFN));
+		__get_free_pages = ____get_free_pages;
+	}
+#ifdef CONFIG_64BIT
+	__vmalloc_start = KERNEL_SEG_START + (1UL << PGDIR_SHIFT);
+#else
+	__vmalloc_start = vaddr;
+#endif
+	return;
+}
+
+#define FLOOR(addr, alignment) ((addr) & ~((alignment) - 1)) /* alignment must be power of 2 */
+
+unsigned long recalculate_max_low_pfn(unsigned long max_low_pfn)
+{
+	/*
+	 * truncate max_low_pfn to 512MB boundary as largest tlb
+	 * pages are used to minimize the number of wired entries
+	 */
+	if ((max_low_pfn > PFN_DOWN(LARGEST_TLBPAGE_SZ << 1)) && (max_low_pfn <= MAX_WIRED_PFN))
+		max_low_pfn = PFN_DOWN(FLOOR(PFN_PHYS(max_low_pfn), LARGEST_TLBPAGE_SZ << 1));
+	max_low_pfn_set = TRUE;
+	__sync();
+
+	return max_low_pfn;
+}
+
+#else
+
 #ifdef CONFIG_64BIT
 #define TLB_VADDR	XKSEG	
 #else
@@ -197,6 +389,8 @@ unsigned long recalculate_max_low_pfn(unsigned long max_low_pfn)
 	return max_low_pfn;
 }
 
+#endif
+
 #else
 
 void setup_mapped_kernel_tlbs(int index, int secondary_cpu) { }
diff --git a/arch/mips/netlogic/xlp/numa.c b/arch/mips/netlogic/xlp/numa.c
index c8492b8..381974d 100644
--- a/arch/mips/netlogic/xlp/numa.c
+++ b/arch/mips/netlogic/xlp/numa.c
@@ -198,6 +198,27 @@ void __init nlm_numa_bootmem_init(unsigned long reserved_end)
 		node_mem_info[node].mem[seg].end_pfn = end;
 		node_mem_info[node].frags++;
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+		if (node == 0) {
+			/* 40 bit physical address, min_start_pfn is the minimum above dma region */
+			unsigned long max_dma_pfn = (MAX_DMA_ADDRESS & 0xffffffffffULL) >> PAGE_SHIFT;
+			if (seg == 0) {
+				if (start >= max_dma_pfn)
+					node_mem_info[node].min_start_pfn = start;
+			} else if (start > max_dma_pfn) {
+				if (node_mem_info[node].min_start_pfn == 0)
+					node_mem_info[node].min_start_pfn = start;
+				else if (start < node_mem_info[node].min_start_pfn)
+					node_mem_info[node].min_start_pfn = start;
+			}
+		} else {
+			if (seg == 0)
+				node_mem_info[node].min_start_pfn = start;
+			else if (start < node_mem_info[node].min_start_pfn)
+				node_mem_info[node].min_start_pfn = start;
+		}
+#endif
+
 		if (end > max_low_pfn)
 			max_low_pfn = end;
 		if (start < min_low_pfn)
diff --git a/fs/exec.c b/fs/exec.c
index 1f44670..8b002ec 100644
--- a/fs/exec.c
+++ b/fs/exec.c
@@ -249,7 +249,11 @@ static int __bprm_mm_init(struct linux_binprm *bprm)
 	struct vm_area_struct *vma = NULL;
 	struct mm_struct *mm = bprm->mm;
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+	bprm->vma = vma = kmem_cache_zalloc(vm_area_cachep, GFP_KERNEL | GFP_DMA);
+#else
 	bprm->vma = vma = kmem_cache_zalloc(vm_area_cachep, GFP_KERNEL);
+#endif
 	if (!vma)
 		return -ENOMEM;
 
diff --git a/include/linux/gfp.h b/include/linux/gfp.h
index 0f615eb..df37685 100644
--- a/include/linux/gfp.h
+++ b/include/linux/gfp.h
@@ -348,7 +348,12 @@ extern struct page *alloc_pages_vma(gfp_t gfp_mask, int order,
 #define alloc_page_vma_node(gfp_mask, vma, addr, node)		\
 	alloc_pages_vma(gfp_mask, 0, vma, addr, node)
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+extern unsigned long (*__get_free_pages)(gfp_t gfp_mask, unsigned int order);
+extern unsigned long ____get_free_pages(gfp_t gfp_mask, unsigned int order);
+#else
 extern unsigned long __get_free_pages(gfp_t gfp_mask, unsigned int order);
+#endif
 extern unsigned long get_zeroed_page(gfp_t gfp_mask);
 
 void *alloc_pages_exact(size_t size, gfp_t gfp_mask);
diff --git a/kernel/fork.c b/kernel/fork.c
index 93edb8b..f6f82e9 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -122,10 +122,17 @@ void __weak arch_release_task_struct(struct task_struct *tsk)
 #ifndef CONFIG_ARCH_TASK_STRUCT_ALLOCATOR
 static struct kmem_cache *task_struct_cachep;
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+static inline struct task_struct *alloc_task_struct_node(int node)
+{
+	return kmem_cache_alloc_node(task_struct_cachep, GFP_KERNEL | GFP_DMA, node);
+}
+#else /* CONFIG_NLM_16G_MEM_SUPPORT */
 static inline struct task_struct *alloc_task_struct_node(int node)
 {
 	return kmem_cache_alloc_node(task_struct_cachep, GFP_KERNEL, node);
 }
+#endif /* CONFIG_NLM_16G_MEM_SUPPORT */
 
 static inline void free_task_struct(struct task_struct *tsk)
 {
@@ -256,10 +263,16 @@ void __init fork_init(unsigned long mempages)
 #define ARCH_MIN_TASKALIGN	L1_CACHE_BYTES
 #endif
 	/* create a slab on which task_structs can be allocated */
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+	task_struct_cachep =
+		kmem_cache_create("task_struct", sizeof(struct task_struct),
+			ARCH_MIN_TASKALIGN, SLAB_PANIC | SLAB_NOTRACK | SLAB_CACHE_DMA, NULL);
+#else
 	task_struct_cachep =
 		kmem_cache_create("task_struct", sizeof(struct task_struct),
 			ARCH_MIN_TASKALIGN, SLAB_PANIC | SLAB_NOTRACK, NULL);
 #endif
+#endif
 
 	/* do the arch specific task caches init */
 	arch_task_cache_init();
@@ -397,7 +410,11 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 				goto fail_nomem;
 			charge = len;
 		}
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+		tmp = kmem_cache_alloc(vm_area_cachep, GFP_KERNEL | GFP_DMA);
+#else
 		tmp = kmem_cache_alloc(vm_area_cachep, GFP_KERNEL);
+#endif
 		if (!tmp)
 			goto fail_nomem;
 		*tmp = *mpnt;
@@ -505,7 +522,11 @@ static inline void mm_free_pgd(struct mm_struct *mm)
 
 __cacheline_aligned_in_smp DEFINE_SPINLOCK(mmlist_lock);
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+#define allocate_mm()	(kmem_cache_alloc(mm_cachep, GFP_KERNEL | GFP_DMA))
+#else
 #define allocate_mm()	(kmem_cache_alloc(mm_cachep, GFP_KERNEL))
+#endif
 #define free_mm(mm)	(kmem_cache_free(mm_cachep, (mm)))
 
 static unsigned long default_dump_filter = MMF_DUMP_FILTER_DEFAULT;
@@ -1728,10 +1749,17 @@ void __init proc_caches_init(void)
 	 * maximum number of CPU's we can ever have.  The cpumask_allocation
 	 * is at the end of the structure, exactly for that reason.
 	 */
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+	mm_cachep = kmem_cache_create("mm_struct",
+			sizeof(struct mm_struct), ARCH_MIN_MMSTRUCT_ALIGN,
+			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_NOTRACK|SLAB_CACHE_DMA, NULL);
+	vm_area_cachep = KMEM_CACHE(vm_area_struct, SLAB_PANIC|SLAB_CACHE_DMA);
+#else
 	mm_cachep = kmem_cache_create("mm_struct",
 			sizeof(struct mm_struct), ARCH_MIN_MMSTRUCT_ALIGN,
 			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_NOTRACK, NULL);
 	vm_area_cachep = KMEM_CACHE(vm_area_struct, SLAB_PANIC);
+#endif
 	mmap_init();
 	nsproxy_cache_init();
 }
diff --git a/mm/memory.c b/mm/memory.c
index 265c29b..078886d 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -2268,9 +2268,19 @@ static int remap_pte_range(struct mm_struct *mm, pmd_t *pmd,
 			unsigned long pfn, pgprot_t prot)
 {
 	pte_t *pte;
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+	spinlock_t *uninitialized_var(ptl);
+#else
 	spinlock_t *ptl;
+#endif
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+	pte = (mm == &init_mm) ?
+		pte_alloc_kernel(pmd, addr) :
+		pte_alloc_map_lock(mm, pmd, addr, &ptl);
+#else
 	pte = pte_alloc_map_lock(mm, pmd, addr, &ptl);
+#endif
 	if (!pte)
 		return -ENOMEM;
 	arch_enter_lazy_mmu_mode();
@@ -2280,7 +2290,12 @@ static int remap_pte_range(struct mm_struct *mm, pmd_t *pmd,
 		pfn++;
 	} while (pte++, addr += PAGE_SIZE, addr != end);
 	arch_leave_lazy_mmu_mode();
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+	if (mm != &init_mm)
+		pte_unmap_unlock(pte - 1, ptl);
+#else
 	pte_unmap_unlock(pte - 1, ptl);
+#endif
 	return 0;
 }
 
diff --git a/mm/mmap.c b/mm/mmap.c
index d7bae85..b5c55a0 100644
--- a/mm/mmap.c
+++ b/mm/mmap.c
@@ -1532,7 +1532,11 @@ munmap_back:
 	 * specific mapper. the address has already been validated, but
 	 * not unmapped, but the maps are removed from the list.
 	 */
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+	vma = kmem_cache_zalloc(vm_area_cachep, GFP_KERNEL | GFP_DMA);
+#else
 	vma = kmem_cache_zalloc(vm_area_cachep, GFP_KERNEL);
+#endif
 	if (!vma) {
 		error = -ENOMEM;
 		goto unacct_error;
@@ -2418,7 +2422,11 @@ static int __split_vma(struct mm_struct * mm, struct vm_area_struct * vma,
 					~(huge_page_mask(hstate_vma(vma)))))
 		return -EINVAL;
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+	new = kmem_cache_alloc(vm_area_cachep, GFP_KERNEL | GFP_DMA);
+#else
 	new = kmem_cache_alloc(vm_area_cachep, GFP_KERNEL);
+#endif
 	if (!new)
 		goto out_err;
 
@@ -2678,7 +2686,11 @@ static unsigned long do_brk(unsigned long addr, unsigned long len)
 	/*
 	 * create a vma struct for an anonymous mapping
 	 */
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+	vma = kmem_cache_zalloc(vm_area_cachep, GFP_KERNEL | GFP_DMA);
+#else
 	vma = kmem_cache_zalloc(vm_area_cachep, GFP_KERNEL);
+#endif
 	if (!vma) {
 		vm_unacct_memory(len >> PAGE_SHIFT);
 		return -ENOMEM;
@@ -2853,7 +2865,11 @@ struct vm_area_struct *copy_vma(struct vm_area_struct **vmap,
 		}
 		*need_rmap_locks = (new_vma->vm_pgoff <= vma->vm_pgoff);
 	} else {
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+		new_vma = kmem_cache_alloc(vm_area_cachep, GFP_KERNEL | GFP_DMA);
+#else
 		new_vma = kmem_cache_alloc(vm_area_cachep, GFP_KERNEL);
+#endif
 		if (new_vma) {
 			*new_vma = *vma;
 			new_vma->vm_start = addr;
@@ -2957,7 +2973,11 @@ int install_special_mapping(struct mm_struct *mm,
 	int ret;
 	struct vm_area_struct *vma;
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+	vma = kmem_cache_zalloc(vm_area_cachep, GFP_KERNEL | GFP_DMA);
+#else
 	vma = kmem_cache_zalloc(vm_area_cachep, GFP_KERNEL);
+#endif
 	if (unlikely(vma == NULL))
 		return -ENOMEM;
 
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 2ee0fd3..b19205d 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -2683,6 +2683,25 @@ EXPORT_SYMBOL(__alloc_pages_nodemask);
 /*
  * Common helper functions.
  */
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+unsigned long ____get_free_pages(gfp_t gfp_mask, unsigned int order)
+{
+	struct page *page;
+
+	/*
+	 * __get_free_pages() returns a 32-bit address, which cannot represent
+	 * a highmem page
+	 */
+	VM_BUG_ON((gfp_mask & __GFP_HIGHMEM) != 0);
+
+	page = alloc_pages(gfp_mask, order);
+	if (!page)
+		return 0;
+	return (unsigned long) page_address(page);
+}
+unsigned long (*__get_free_pages)(gfp_t gfp_mask, unsigned int order)
+	= ____get_free_pages;
+#else
 unsigned long __get_free_pages(gfp_t gfp_mask, unsigned int order)
 {
 	struct page *page;
@@ -2698,6 +2717,7 @@ unsigned long __get_free_pages(gfp_t gfp_mask, unsigned int order)
 		return 0;
 	return (unsigned long) page_address(page);
 }
+#endif
 EXPORT_SYMBOL(__get_free_pages);
 
 unsigned long get_zeroed_page(gfp_t gfp_mask)
diff --git a/mm/percpu-vm.c b/mm/percpu-vm.c
index 3707c71..82a4bfe 100644
--- a/mm/percpu-vm.c
+++ b/mm/percpu-vm.c
@@ -107,10 +107,18 @@ static int pcpu_alloc_pages(struct pcpu_chunk *chunk,
 			    struct page **pages, unsigned long *populated,
 			    int page_start, int page_end)
 {
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+	gfp_t gfp = GFP_KERNEL | __GFP_HIGHMEM | __GFP_COLD;
+#else
 	const gfp_t gfp = GFP_KERNEL | __GFP_HIGHMEM | __GFP_COLD;
+#endif
 	unsigned int cpu;
 	int i;
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+	gfp |= GFP_DMA;
+#endif
+
 	for_each_possible_cpu(cpu) {
 		for (i = page_start; i < page_end; i++) {
 			struct page **pagep = &pages[pcpu_page_idx(cpu, i)];
-- 
1.8.4.93.g57e4c17

