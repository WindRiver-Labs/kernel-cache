From 098469de4db889c6a153a28343207dd249488abc Mon Sep 17 00:00:00 2001
From: Om Narasimhan <onarasimhan@netlogicmicro.com>
Date: Fri, 27 May 2011 12:01:52 -0700
Subject: [PATCH 238/565] This commit implements the following changes:

1. IRQ numbers are changed. Now they are a function of IRT index
2. HAL support is minimized. Some parts still depend on HAL, but majority
of this dependency is removed.
3. Implements interrupt spraying to all CPUs.
4. Implements user changeable CPU affinity for all interrupts except
reserved ones and MSI

Notes :
Tested with sky2 (dlink) cards for IntX and MSI

Tested with e1000e (Intel) cards for IntX, MSI and MSI-X

Tested on 8x2 and 4x4 pcie lane configs

Many of the h/w specific registers and offsets are hard coded for the
time being, will be removed later.

Based on Broadcom SDK 2.3.

Signed-off-by: Om Narasimhan <onarasimhan@netlogicmicro.com>
Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 arch/mips/include/asm/netlogic/interrupt.h   |    7 +-
 arch/mips/include/asm/netlogic/xlp_hal_pic.h |  214 ++++
 arch/mips/include/asm/netlogic/xlp_irq.h     |  277 +++++
 arch/mips/kernel/nlm_fs_handler.S            |    2 +-
 arch/mips/netlogic/xlp/Makefile              |    2 +-
 arch/mips/netlogic/xlp/irq.c                 | 1473 +++++++++++++++++++++-----
 arch/mips/netlogic/xlp/on_chip.c             |   25 +-
 arch/mips/netlogic/xlp/platform.c            |   44 +-
 arch/mips/netlogic/xlp/setup.c               |   13 +-
 arch/mips/netlogic/xlp/smp.c                 |   10 +-
 arch/mips/netlogic/xlp/time.c                |   28 +-
 arch/mips/netlogic/xlp/xlp_gpio.c            |    2 +-
 arch/mips/netlogic/xlp/xlp_hal_pic.c         |   79 ++
 arch/mips/pci/pci-xlp.c                      |  607 ++++++++++-
 drivers/usb/host/ehci-pci.c                  |    6 +-
 15 files changed, 2397 insertions(+), 392 deletions(-)
 create mode 100644 arch/mips/include/asm/netlogic/xlp_hal_pic.h
 create mode 100644 arch/mips/include/asm/netlogic/xlp_irq.h
 create mode 100644 arch/mips/netlogic/xlp/xlp_hal_pic.c

diff --git a/arch/mips/include/asm/netlogic/interrupt.h b/arch/mips/include/asm/netlogic/interrupt.h
index 84d27d4..5fa0d53 100644
--- a/arch/mips/include/asm/netlogic/interrupt.h
+++ b/arch/mips/include/asm/netlogic/interrupt.h
@@ -26,10 +26,13 @@ THE POSSIBILITY OF SUCH DAMAGE.
 #ifndef _ASM_NLM_INTERRUPT_H
 #define _ASM_NLM_INTERRUPT_H
 
+#ifdef CONFIG_NLM_XLP
+#include <asm/netlogic/xlp_irq.h>
+#else
 #include <asm/netlogic/pic.h>
 
 /* Defines for the IRQ numbers */
-
+#define NR_IRQS			256
 #define IRQ_DUMMY_UART           2
 #define IRQ_IPI_SMP_FUNCTION     3
 #define IRQ_IPI_SMP_RESCHEDULE   4
@@ -52,6 +55,6 @@ THE POSSIBILITY OF SUCH DAMAGE.
 
 #define SMP_CALL_KGDB_HOOK 	8
 #define SMP_OPROFILE_IPI        16
-
+#endif		// CONFIG_NLM_XLP
 
 #endif
diff --git a/arch/mips/include/asm/netlogic/xlp_hal_pic.h b/arch/mips/include/asm/netlogic/xlp_hal_pic.h
new file mode 100644
index 0000000..1507708
--- /dev/null
+++ b/arch/mips/include/asm/netlogic/xlp_hal_pic.h
@@ -0,0 +1,214 @@
+/***********************************************************************
+Copyright 2003-2010 Netlogic Microsystems (“Netlogic”). All rights
+reserved.
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are
+met:
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+*****************************#NETL_2#********************************/
+
+#ifndef _NLM_HAL_PIC_H
+#define _NLM_HAL_PIC_H
+
+#include "nlm_hal.h"
+/*
+ *     Register Offsets
+ */
+#define PIC_CTRL             0x00
+#define PIC_BYTESWAP         0x01
+#define PIC_STATUS           0x02
+#define PIC_INT_TIMEOUT      0x03
+#define PIC_ICI0_INT_TIMEOUT 0x04
+#define PIC_ICI1_INT_TIMEOUT 0x05
+#define PIC_ICI2_INT_TIMEOUT 0x06
+#define PIC_IPI_CTL          0x07
+#define PIC_INT_ACK          0x08
+#define PIC_INT_PENDING0     0x09
+#define PIC_INT_PENDING1     0x0a
+#define PIC_INT_PENDING2     0x0b
+
+#define PIC_WD0_MAX_VAL      0x0c
+#define PIC_WD0_COUNT        0x0d
+#define PIC_WD0_MASK_0       0x0e
+#define PIC_WD0_MASK_1       0x0f
+#define PIC_WD0_HEARBEATCMD  0x10
+#define PIC_WD0_HEARBEAT_0   0x11
+#define PIC_WD0_HEARBEAT_1   0x12
+#define PIC_SYS_TIMER_0_COUNTER   0x22
+
+#define PIC_INT_THR_ENABLE_0_N01   0x2a
+#define PIC_INT_THR_ENABLE_0_N23   0x2b
+#define PIC_INT_THR_ENABLE_N01(id) (PIC_INT_THR_ENABLE_0_N12 + ((id) * 2))
+#define PIC_INT_THR_ENABLE_N23(id) (PIC_INT_THR_ENABLE_0_N12 + ((id) * 2))
+
+#define PIC_IRT_0   0x3a
+#define PIC_IRT(id) (PIC_IRT_0 + (id))
+#define PIC_CLOCK_TIMER     7
+#define ASM_XLP_IO_PIC_OFFSET        0xffffffffb8004100 /* TODO: This will change in to function */
+#define C_XLP_IO_PIC_OFFSET        0xffffffffb8004100ULL /* TODO: This will change in to function */
+#define XLP_IO_PIC_OFFSET        C_XLP_IO_PIC_OFFSET
+
+#ifndef __ASSEMBLY__
+#define __nlm_hal_set_irq_to_cpu	__nlm_hal_set_irt_to_cpu
+void __nlm_hal_set_irt_to_cpu(int, int);
+void __nlm_hal_release_irq(int);
+int __nlm_hal_request_irq(int, int);
+#define CPUIDBITS01(X) ((X) & 0x3)
+#define CPUIDBIT2(X) ((X >> 2) & 0x1)
+
+#if 0
+static inline int nlm_hal_irt_to_irq(int irt_num)
+{
+	return __nlm_hal_find_irt_from_irq(irt_num);
+}
+
+static inline int nlm_hal_irq_to_irt(int irq_num)
+{
+	int irt = __nlm_hal_find_irt_from_irq(irq_num); // same function
+	return irt;
+}
+#endif
+#define PIC_IRQ_IS_EDGE_TRIGGERED(irq) 0 // XLP interrupts are level triggered
+#define NODE_OFFSET(node) ((node) << 18)
+#define CPU_TO_NODE(cpu) ((cpu) >> 5)
+
+static __inline__ int nlm_hal_cpu_id(void)
+{
+	int cpu;
+
+	__asm__ __volatile__ (
+		".set push\n"
+		".set noreorder\n"
+		".set mips32\n"
+		"mfc0 %0, $15, 1\n"
+		"andi %0, %0, 0x3ff\n"
+		".set pop\n"
+		: "=r"(cpu)
+		);
+
+	return cpu;
+}
+
+typedef volatile unsigned long long pic_reg_t;
+
+static __inline__ pic_reg_t* nlm_hal_pic_offset(void)
+{
+	uint32_t cpu = nlm_hal_cpu_id();
+
+	return ((pic_reg_t *) (XLP_IO_PIC_OFFSET + NODE_OFFSET(CPU_TO_NODE(cpu))));
+}
+
+static __inline__ void nlm_hal_write_pic_reg(pic_reg_t *base,
+		unsigned int offset, unsigned long long value)
+{
+	base[offset] = value;
+}
+
+static __inline__ unsigned long long nlm_hal_read_pic_reg(pic_reg_t *base,
+		unsigned int offset)
+{
+	return ((base)[offset]);
+}
+
+static __inline__ void nlm_hal_pic_send_ipi(int nmi, int vec, int node, int cpu)
+{
+	pic_reg_t *mmio = nlm_hal_pic_offset();
+
+	unsigned long long ipi = (nmi << 31) | (vec << 20) | (node << 17) | (1 << (cpu & 0xf));
+	if (cpu > 15) {
+		ipi |= 0x10000; // Setting bit 16 to select cpus 16-31
+	}
+	nlm_hal_write_pic_reg(mmio, PIC_IPI_CTL, ipi);
+}
+
+static __inline__ unsigned long long nlm_hal_pic_read_control(void)
+{
+	pic_reg_t *mmio = nlm_hal_pic_offset();
+
+	return nlm_hal_read_pic_reg(mmio, PIC_CTRL);
+}
+
+static __inline__ void nlm_hal_pic_write_control(unsigned long long control)
+{
+	pic_reg_t *mmio = nlm_hal_pic_offset();
+
+	nlm_hal_write_pic_reg(mmio, PIC_CTRL, control);
+}
+
+static __inline__ void nlm_hal_pic_update_control(unsigned long long control)
+{
+	pic_reg_t *mmio = nlm_hal_pic_offset();
+
+	nlm_hal_write_pic_reg(mmio, PIC_CTRL,
+			(control | nlm_hal_read_pic_reg(mmio, PIC_CTRL)));
+}
+
+static __inline__ void nlm_hal_ack_pic(int irt_num)
+{
+	pic_reg_t *mmio = nlm_hal_pic_offset();
+
+	nlm_hal_write_pic_reg(mmio, PIC_INT_ACK, irt_num);
+
+	/* Ack the Status register for Watchdog & System timers */
+	if (irt_num < 12) {
+		nlm_hal_write_pic_reg(mmio, PIC_STATUS, (1 << irt_num));
+	}
+}
+
+static __inline__ unsigned long long nlm_hal_pic_read_irt(int irt_num)
+{
+	pic_reg_t *mmio = nlm_hal_pic_offset();
+
+	return nlm_hal_read_pic_reg(mmio, PIC_IRT(irt_num));
+}
+
+static __inline__ void nlm_hal_pic_write_irt(int irt_num, int en, int nmi, int sch, int vec, int dt, int db, int dte)
+{
+	pic_reg_t *mmio = nlm_hal_pic_offset();
+
+	unsigned long long val = (((en & 0x1) << 31) | ((nmi & 0x1) << 29) | ((sch & 0x1) << 28) |
+				  ((vec & 0x3f) << 20) | ((dt & 0x1 ) << 19) | ((db & 0x7) << 16) |
+				  (dte & 0xffff));
+
+	nlm_hal_write_pic_reg(mmio, PIC_IRT(irt_num), val);
+}
+
+static __inline__ void nlm_hal_pic_write_irt_direct(int irt_num, int en, int nmi, int sch, int vec, int cpu)
+{
+	nlm_hal_pic_write_irt(irt_num, en, nmi, sch, vec, 1, CPUIDBIT2(cpu), CPUIDBITS01(cpu));
+	/* Does not support multi node support yet */
+}
+
+static __inline__ unsigned long long nlm_hal_pic_read_timer(int timer)
+{
+	pic_reg_t *mmio = nlm_hal_pic_offset();
+
+	return nlm_hal_read_pic_reg(mmio, PIC_SYS_TIMER_COUNTER(timer));
+}
+
+static __inline__ void nlm_hal_pic_write_timer(int timer, pic_reg_t value)
+{
+	pic_reg_t *mmio = nlm_hal_pic_offset();
+
+	nlm_hal_write_pic_reg(mmio, PIC_SYS_TIMER_COUNTER(timer), value);
+}
+
+#endif /* __ASSEMBLY__ */
+
+#endif /* _NLM_HAL_PIC_H */
diff --git a/arch/mips/include/asm/netlogic/xlp_irq.h b/arch/mips/include/asm/netlogic/xlp_irq.h
new file mode 100644
index 0000000..3c67269
--- /dev/null
+++ b/arch/mips/include/asm/netlogic/xlp_irq.h
@@ -0,0 +1,277 @@
+/***********************************************************************
+Copyright 2003-2010 Netlogic Microsystems (“Netlogic”). All rights
+reserved.
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are
+met:
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+*****************************#NETL_2#********************************/
+
+#ifndef _ASM_NLM_XLP_IRQ_H
+#define _ASM_NLM_XLP_IRQ_H
+
+#include <asm/netlogic/pic.h>
+
+/* Defines for the IRQ numbers */
+/* We define NR_IRQs to be 254, but IRT entries are 160 in size
+ * Effectively, we cannot use anything more than 159 */
+#define NR_IRQS			256
+/* Maximum IRQ vector numbers supported by MIPS */
+#define XLP_EIRR_SIZE		64
+#define XLP_IRT_NUM	160
+#define XLP_IRQ_MAX	168	/* 0-7 are reserved + 160 IRT entries */
+
+/* The following interrupt assignments (0-7) are special.
+ * I need to find out what governs these assignments
+ * XXX
+ */
+#define fdebug(fmt,arg...)\
+	printk(KERN_WARNING "%s:%d " fmt, __FILE__, __LINE__, ##arg)
+
+#define XLP_IRQ_DUMMY_UART           2
+#define XLP_IRQ_IPI_SMP_FUNCTION     3
+#define XLP_IRQ_IPI_SMP_RESCHEDULE   4
+#define XLP_IRQ_REMOTE_DEBUG         5
+#define XLP_IRQ_MSGRING              6
+#define XLP_IRQ_TIMER                7
+#define XLP_IRQ_RESERVED_MAX		8
+
+#define XLP_IRQ_IPI_SMP_KGDB	     50
+#define XLP_IRQ_IPI_OPROFILE         51
+
+#ifdef CONFIG_NLMCOMMON_IP_FLOW_AFFINITY
+#define XLP_IRQ_IPI_NETRX		49
+#define SMP_NETRX_IPI			32
+#endif /* CONFIG_NLMCOMMON_IP_FLOW_AFFINITY */
+
+/* if you want some common #defines, please do it here */
+#define NLM_IRQ_DUMMY_UART		XLP_IRQ_DUMMY_UART
+#define NLM_IRQ_IPI_SMP_FUNCTION	XLP_IRQ_IPI_SMP_FUNCTION
+#define NLM_IRQ_IPI_SMP_RESCHEDULE	XLP_IRQ_IPI_SMP_RESCHEDULE
+#define NLM_IRQ_REMOTE_DEBUG		XLP_IRQ_REMOTE_DEBUG
+#define NLM_IRQ_MSGRING			XLP_IRQ_MSGRING
+#define NLM_IRQ_TIMER			XLP_IRQ_TIMER
+#define NLM_IRQ_IPI_SMP_KGDB		XLP_IRQ_IPI_SMP_KGDB
+#define NLM_IRQ_IPI_OPROFILE		XLP_IRQ_IPI_OPROFILE
+
+/* These are flags required for SMP
+ * Not XLP specifc -- possibly mips specific.
+ * Need to move out XXX
+ */
+#define SMP_CALL_KGDB_HOOK	8
+#define SMP_OPROFILE_IPI        16
+
+#define TIMER_CYCLES_MAXVAL        0xffffffffffffffffULL
+
+/*
+ *    IRT Map
+ */
+
+#define arch_setup_msi_irqs	arch_setup_msi_irqs /* defines arch. specific msi setup function */
+#define xlp_irq_to_irt(x)	((x) - XLP_IRQ_RESERVED_MAX)
+#define xlp_irt_to_irq(x)	((x) + XLP_IRQ_RESERVED_MAX)
+
+#define XLP_WD_BASE			(0 + XLP_IRQ_RESERVED_MAX)
+#define XLP_WD_IRQ_IRQ(x) (XLP_WD_BASE + (x))
+
+#define XLP_WD_NMI_IRT_OFFSET		(2 + XLP_IRQ_RESERVED_MAX)
+#define XLP_WD_NMI_IRQ(x) (XLP_WD_NMI_IRT_OFFSET + (x))
+
+#define XLP_TIMER_IRT_OFFSET		(4 + XLP_IRQ_RESERVED_MAX)
+#define XLP_TIMER_IRQ(x)	(XLP_TIMER_IRT_OFFSET + (x))
+
+#define XLP_MSGQ_IRT_OFFSET			(12 + XLP_IRQ_RESERVED_MAX)
+#define XLP_MSGQ_IRQ(x)	(XLP_MSGQ_IRT_OFFSET + (x))
+
+#define XLP_MSG_IRT_OFFSET			(44 + XLP_IRQ_RESERVED_MAX)
+#define XLP_MSG_IRQ(x)	(XLP_MSG_IRT_OFFSET + (x))
+
+#define XLP_PCIE_MSIX_IRT_OFFSET		(46 + XLP_IRQ_RESERVED_MAX)
+#define XLP_PCIE_MSIX_IRQ(x)	(XLP_PCIE_MSIX_IRT_OFFSET + (x))
+
+#define XLP_PCIE_LINK_IRT_OFFSET		(78 + XLP_IRQ_RESERVED_MAX)
+#define XLP_PCIE_LINK_IRQ(x)	(XLP_PCIE_LINK_IRT_OFFSET + (x))
+
+#define XLP_NAE_IRT_OFFSET			(82 + XLP_IRQ_RESERVED_MAX)
+#define XLP_XLP_NAE_IRQ(x)	(XLP_XLP_NAE_IRT_OFFSET + (x))
+
+#define XLP_POE_IRT_OFFSET			(114 + XLP_IRQ_RESERVED_MAX)
+#define XLP_POE_IRQ(x)	(XLP_POE_IRT_OFFSET + (x))
+
+#define XLP_USB_IRT_OFFSET			(115 + XLP_IRQ_RESERVED_MAX)
+#define XLP_USB_IRQ(x)	(XLP_USB_IRT_OFFSET + (x))
+
+#define XLP_DTR_IRT_OFFSET			(121 + XLP_IRQ_RESERVED_MAX)
+#define XLP_DTR_IRQ(x)	(XLP_DTR_IRT_OFFSET + (x))
+
+#define XLP_SAE_IRT_OFFSET			(122 + XLP_IRQ_RESERVED_MAX)
+#define XLP_SAE_IRQ(x)	(XLP_SAE_IRT_OFFSET + (x))
+
+#define XLP_RSA_IRT_OFFSET			(123 + XLP_IRQ_RESERVED_MAX)
+#define XLP_RSA_IRQ(x)	(XLP_RSA_IRT_OFFSET + (x))
+
+#define XLP_COMP_IRT_OFFSET			(124 + XLP_IRQ_RESERVED_MAX)
+#define XLP_COMP_IRQ(x)	(XLP_COMP_IRT_OFFSET + (x))
+
+#define XLP_FLASH_IRT_OFFSET		(128 + XLP_IRQ_RESERVED_MAX)
+#define XLP_FLASH_IRQ(x)	(XLP_FLASH_IRT_OFFSET + (x))
+
+#define	XLP_ICI_IRT_OFFSET			(131 + XLP_IRQ_RESERVED_MAX)
+#define XLP_ICI_IRQ(x)	(XLP_ICI_IRT_OFFSET + (x))
+
+#define	XLP_KBP_IRT_OFFSET			(132 + XLP_IRQ_RESERVED_MAX)
+#define XLP_KBP_IRQ(x)	(XLP_KBP_IRT_OFFSET + (x))
+
+#define XLP_UART_IRT_OFFSET			(133 + XLP_IRQ_RESERVED_MAX)
+#define XLP_UART_IRQ(x)	(XLP_UART_IRT_OFFSET + (x))
+
+#define XLP_I2C_IRT_OFFSET			(135 + XLP_IRQ_RESERVED_MAX)
+#define XLP_I2C_IRQ(x)	(XLP_I2C_IRT_OFFSET + (x))
+
+#define XLP_SM_IRT_OFFSET			(137 + XLP_IRQ_RESERVED_MAX)
+#define XLP_SM_IRQ(x)	(XLP_SM_IRT_OFFSET + (x))
+
+#define	XLP_JTAG_IRT_OFFSET			(139 + XLP_IRQ_RESERVED_MAX)
+#define XLP_JTAG_IRQ(x)	(XLP_JTAG_IRT_OFFSET + (x))
+
+#define XLP_PIC_IRT_OFFSET			(140 + XLP_IRQ_RESERVED_MAX)
+#define XLP_PIC_IRQ(x)	(XLP_PIC_IRT_OFFSET + (x))
+
+#define XLP_MIOCB_IRT_OFFSET		(141 + XLP_IRQ_RESERVED_MAX)
+#define XLP_MIOCB_IRQ(x)	(XLP_MIOCB_IRT_OFFSET + (x))
+
+#define XLP_TCU_IRT_OFFSET			(142 + XLP_IRQ_RESERVED_MAX)
+#define XLP_TCU_IRQ(x)	(XLP_TCU_IRT_OFFSET + (x))
+
+#define XLP_GCU_IRT_OFFSET			(143 + XLP_IRQ_RESERVED_MAX)
+#define XLP_GCU_IRQ(x)	(XLP_GCU_IRT_OFFSET + (x))
+
+#define XLP_DRAM_IRT_OFFSET			(144 + XLP_IRQ_RESERVED_MAX)
+#define XLP_DRAM_IRQ(x)	(XLP_DRAM_IRT_OFFSET + (x))
+
+#define XLP_GPIO_IRT_OFFSET			(146 + XLP_IRQ_RESERVED_MAX)
+#define XLP_GPIO_IRQ(x)	(XLP_GPIO_IRT_OFFSET + (x))
+
+#define XLP_NOR_IRT_OFFSET			(150 + XLP_IRQ_RESERVED_MAX)
+#define XLP_NOR_IRQ(x)	(XLP_NOR_IRT_OFFSET + (x))
+
+#define XLP_NAND_IRT_OFFSET			(151 + XLP_IRQ_RESERVED_MAX)
+#define XLP_NAND_IRQ(x)	(XLP_NAND_IRT_OFFSET + (x))
+
+#define XLP_SPI_IRT_OFFSET			(152 + XLP_IRQ_RESERVED_MAX)
+#define XLP_SPI_IRQ(x)	(XLP_SPI_IRT_OFFSET + (x))
+
+#define XLP_MMC_IRT_OFFSET			(153 + XLP_IRQ_RESERVED_MAX)
+#define XLP_MMC_IRQ(x)	(XLP_MMC_IRT_OFFSET + (x))
+
+/* The following are the values supported per slot. A slot can have a device or
+ * a bridge, but only this much MSI/MSI-X can be alloted on that slot
+ * This is a kludge to keep NR_IRQS == 256 and can be expanded later
+ * We are using IRQ 192 - 255 for MSI/MSI-X
+ * */
+#define CONFIG_PCI_MSI_XLP	1
+#define CONFIG_PCI_MSIX_XLP	1
+
+#define XLP_PIC_IRTREG_START 0xB4
+#define XLP_ITE_ENTRIES		8
+#ifdef CONFIG_XLP_MSI_ADDRESSES
+#define XLP_MSI_ADDR		0xFEE00000
+#endif
+
+#define XLP_MSI_ADDR_SIZE	0x00002000
+#define XLP_MSIX_ADDR_SIZE	0x00008000
+
+#define XLP_MAX_SLOTS		4	/* Only 4 slots now */
+#define XLP_PCIE_CTRL_DEVFN(node, ctr)	PCI_DEVFN((node + 1), ctr)
+#ifdef CONFIG_PCI_MSI_XLP
+#define XLP_MSI_MM_CAP		3	/* Multiple message capability */
+#define XLP_MSI_PER_SLOT	(1 << XLP_MSI_MM_CAP)
+#define XLP_MSI_IRQ_OFFSET	192	/* Note IRQ not IRT */
+#define XLP_MSI_IRQ_START(fn)	(XLP_MSI_IRQ_OFFSET + (fn) * XLP_MSI_PER_SLOT)
+#define XLP_MSI_INDEX_START XLP_MSI_IRQ_START(0)
+#define XLP_MSI_INDEX_END (XLP_MSI_IRQ_START(XLP_MAX_SLOTS) - 1)// 31 vectors
+#define XLP_MSI_TO_CTRL_FN(msi) (((msi) >> 3) & 3)
+#endif
+
+#ifdef CONFIG_PCI_MSIX_XLP
+#define XLP_MSIX_PER_SLOT	8
+#define XLP_MSIX_IRQ_OFFSET	(XLP_MSI_IRQ_OFFSET + XLP_MAX_SLOTS * XLP_MSIX_PER_SLOT) /* 192 + (4 * 8) => 224 */
+#define XLP_MSIX_TO_CTRL_FN(msix) (((msix) >> 3) & 3)
+#define XLP_MSIX_IRQ_START(fn)	(XLP_MSIX_IRQ_OFFSET + (fn) * XLP_MSIX_PER_SLOT)
+#define XLP_MSIX_INDEX_START XLP_MSIX_IRQ_START(0)
+#define XLP_MSIX_INDEX_END (XLP_MSIX_IRQ_START(XLP_MAX_SLOTS) - 1)// 31 vectors
+
+#endif
+/*
+ *     Register Offsets
+ */
+#define PIC_CTRL             0x00
+#define PIC_BYTESWAP         0x01
+#define PIC_STATUS           0x02
+#define PIC_INT_TIMEOUT      0x03
+#define PIC_ICI0_INT_TIMEOUT 0x04
+#define PIC_ICI1_INT_TIMEOUT 0x05
+#define PIC_ICI2_INT_TIMEOUT 0x06
+#define PIC_IPI_CTL          0x07
+#define PIC_INT_ACK          0x08
+#define PIC_INT_PENDING0     0x09
+#define PIC_INT_PENDING1     0x0a
+#define PIC_INT_PENDING2     0x0b
+
+#define PIC_WD0_MAX_VAL      0x0c
+#define PIC_WD0_COUNT        0x0d
+#define PIC_WD0_MASK_0       0x0e
+#define PIC_WD0_MASK_1       0x0f
+#define PIC_WD0_HEARBEATCMD  0x10
+#define PIC_WD0_HEARBEAT_0   0x11
+#define PIC_WD0_HEARBEAT_1   0x12
+
+#define PIC_WD_MAX_VAL(id)    (PIC_WD0_MAX_VAL + ((id) ? 7 : 0))
+#define PIC_WD_COUNT(id)      (PIC_WD0_COUNT + ((id) ? 7 : 0))
+#define PIC_WD_MASK_0(id)     (PIC_WD0_MASK_0 + ((id) ? 7 : 0))
+#define PIC_WD_MASK_1(id)     (PIC_WD0_MASK_1 + ((id) ? 7 : 0))
+#define PIC_WD_HEARBEAT_0(id) (PIC_WD0_HEARTBEAT_0 + ((id) ? 7 : 0))
+#define PIC_WD_HEARBEAT_1(id) (PIC_WD0_HEARTBEAT_1 + ((id) ? 7 : 0))
+
+#define PIC_SYS_TIMER_0_MAX_VAL   0x1a
+#define PIC_SYS_TIMER_MAX_VAL(id) (PIC_SYS_TIMER_0_MAX_VAL + (id))
+
+#define PIC_SYS_TIMER_0_COUNTER   0x22
+#define PIC_SYS_TIMER_COUNTER(id) (PIC_SYS_TIMER_0_COUNTER + (id))
+
+#define PIC_TIMER_0_MAXVAL   PIC_SYS_TIMER_0_MAX_VAL
+#define PIC_TIMER_0_COUNTER  PIC_SYS_TIMER_0_COUNTER
+#define PIC_TIMER_7_MAXVAL   PIC_SYS_TIMER_MAX_VAL(7)
+#define PIC_TIMER_7_COUNTER  PIC_SYS_TIMER_COUNTER(7)
+#define PIC_TIMER_6_MAXVAL   PIC_SYS_TIMER_MAX_VAL(6)
+#define PIC_TIMER_6_COUNTER  PIC_SYS_TIMER_COUNTER(6)
+
+#define PIC_INT_THR_ENABLE_0_N01   0x2a
+#define PIC_INT_THR_ENABLE_0_N23   0x2b
+#define PIC_INT_THR_ENABLE_N01(id) (PIC_INT_THR_ENABLE_0_N12 + ((id) * 2))
+#define PIC_INT_THR_ENABLE_N23(id) (PIC_INT_THR_ENABLE_0_N12 + ((id) * 2))
+
+#define PIC_IRT_0   0x3a
+#define PIC_IRT(id) (PIC_IRT_0 + (id))
+
+
+#define ASM_XLP_IO_PIC_OFFSET        0xffffffffb8004100 /* TODO: This will change in to function */
+#define C_XLP_IO_PIC_OFFSET        0xffffffffb8004100ULL /* TODO: This will change in to function */
+
+#endif
diff --git a/arch/mips/kernel/nlm_fs_handler.S b/arch/mips/kernel/nlm_fs_handler.S
index 563dea94..c6b3bdf 100644
--- a/arch/mips/kernel/nlm_fs_handler.S
+++ b/arch/mips/kernel/nlm_fs_handler.S
@@ -407,7 +407,7 @@ END(nlm_fs_processorId)
 NESTED(nlm_fs_read_timer, PT_SIZE, sp)
 
 #if defined(CONFIG_NLM_XLP)
-#include <asm/netlogic/hal/nlm_hal_pic.h>
+#include <asm/netlogic/xlp_hal_pic.h>
         MFC0    k0, CP0_PRID, 1
         andi    k0, k0, 0x3ff
         srl     k0, k0, 5  /* grab node id */
diff --git a/arch/mips/netlogic/xlp/Makefile b/arch/mips/netlogic/xlp/Makefile
index de03bde..c3f3a2f 100644
--- a/arch/mips/netlogic/xlp/Makefile
+++ b/arch/mips/netlogic/xlp/Makefile
@@ -3,7 +3,7 @@ EXTRA_CFLAGS := $(CFLAGS) -DNLM_HAL_LINUX_KERNEL -Iarch/mips/include/asm/netlogi
 
 obj-y                    	= setup.o nmi.o
 obj-y 				+= irq.o time.o on_chip.o mmu.o
-obj-$(CONFIG_NLM_XLP) 		+= platform.o board.o  xlp_gpio.o
+obj-$(CONFIG_NLM_XLP) 		+= platform.o board.o  xlp_gpio.o xlp_hal_pic.o
 obj-$(CONFIG_SMP)       	+= smp.o
 
 obj-$(CONFIG_KGDB)      += nmi.o
diff --git a/arch/mips/netlogic/xlp/irq.c b/arch/mips/netlogic/xlp/irq.c
index b529955..0c1a244 100644
--- a/arch/mips/netlogic/xlp/irq.c
+++ b/arch/mips/netlogic/xlp/irq.c
@@ -39,223 +39,682 @@ THE POSSIBILITY OF SUCH DAMAGE.
 #include <asm/kgdb.h>
 #include <asm/mipsregs.h>
 
+#include <asm/netlogic/xlp_hal_pic.h>
 #include <asm/netlogic/xlp.h>
+#include <asm/netlogic/xlp_irq.h>
 #include <asm/netlogic/msidef.h>
 #include <asm/netlogic/mips-exts.h>
 #include <asm/netlogic/pic.h>
 #include <asm/netlogic/debug.h>
 #include <asm/thread_info.h>
-#include <linux/irq.h>
-#include <asm/netlogic/hal/nlm_hal_pic.h>
+
 /*
- * These are the routines that handle all the low level interrupt stuff. 
+ * These are the routines that handle all the low level interrupt stuff.
  * Actions handled here are: initialization of the interrupt map, requesting of
  * interrupt lines by handlers, dispatching if interrupts to handlers, probing
- * for interrupt lines 
+ * for interrupt lines
  */
 
 /* Externs */
-extern void nlm_common_timer_interrupt(struct pt_regs *regs, int irq);
-extern void nlm_xlp_msgring_int_handler(int irq, struct pt_regs *regs);
+extern void nlm_common_timer_interrupt(struct pt_regs *, int);
+extern void nlm_xlp_msgring_int_handler(int , struct pt_regs *);
+extern int xlp_msi_status_clear(struct pci_dev *, int);
+extern int xlp_msi_enable(struct pci_dev *, u32);
+extern int xlp_msi_base_vector(struct pci_dev *);
+extern int is_msi_set(int);
+extern int calc_msi_vector_offset(int);
+extern void xlp_msi_disable(int, int);
+extern u32 xlp_msi_set_mask(int, int, int);
+extern u64 xlp_msix_addr_start(int);
+extern u64 xlp_msi_addr_start(int);
+extern int xlp_ctrl_fn_from_dev(const struct pci_dev *);
+extern u32 xlp_msix_status_clear(int);
+extern u32 xlp_msix_set_mask(int, int, int);
+extern int xlp_msix_enable(struct pci_dev *);
+extern void xlp_msix_disable(int);
+extern const struct cpumask *xlp_closest_match_cpumask(struct cpumask *);
+extern void xlp_set_cpumask(const struct cpumask *m, int irt);
+void mask_msi_irq(unsigned int);
+void unmask_msi_irq(unsigned int);
+extern void xlp_intx_enable(int);
+extern void xlp_intx_disable(int);
+
+/* own variables */
+static volatile uint64_t xlp_irq_mask;
+spinlock_t xlp_pic_lock = SPIN_LOCK_UNLOCKED;
+EXPORT_SYMBOL(xlp_pic_lock);
+/*
+ * This bitmap keeps track of the MSI vectors allocated from
+ * XLP_MSIX_IRQ_START(x)
+ */
+struct msix_alloc_bitmap {
+	u64 bitmap;	/* Can be any data structure to keep bits */
+	u32 count;	/* #of bits set at any point of time */
+};
+static struct msix_alloc_bitmap msix_vec[XLP_MAX_SLOTS];
 
-extern void *ht_config_base;
-extern int link0, link1;
-struct pic_tmask pic_tmask[PIC_NUM_IRTS];
+// #define pr_err fdebug
 
-uint64_t nlm_xlp_irq_mask;
-spinlock_t nlm_common_pic_lock = SPIN_LOCK_UNLOCKED;
+/*
+ * rvec_map is meant to map IRT numbers (index of IRT) to RVEC numbers.
+ * RVEC is just a number for s/w; which is the bit that is set in EIRR
+ * Refer PRM : 9.3
+ *
+ * irq_map : {<IERR RVEC>, <#of s/w vector multiplexed on this RVEC> }
+ * Some RVECs are reserved : so use only 9 through 63
+ * Any irt index can be derived from irq using the macros
+ * xlp_irt_to_irq() or xlp_irq_to_irt()
+ */
+struct irq_map_elem {
+	int rvec;
+	int usage;
+};
 
-static unsigned int pic_startup(unsigned int irq)
+static struct irq_map_elem irq_map[XLP_IRQ_MAX] = {
+	{0, 0},	/* Dummy			:	0 */
+	{0, 0},	/* Dummy			:	1 */
+	{0, 0},	/* Dummy			:	2 */
+	{0, 0},	/* Dummy			:	3 */
+	{0, 0},	/* Dummy			:	4 */
+	{0, 0},	/* Dummy			:	5 */
+	{0, 0},	/* Dummy			:	6 */
+	{0, 0},	/* Dummy			:	7 */
+        {9, 0}, /*XLP_WD_IDX(0)			:	8 */
+        {9, 0}, /*XLP_WD_IDX(1)			:	9 */
+        {19, 0}, /*XLP_WD_NMI_IDX(0)		:	10 */
+        {19, 0}, /*XLP_WD_NMI_IDX(1)		:	11 */
+        {10, 0}, /*XLP_TIMER_IDX(0)		:	12 */
+        {10, 0}, /*XLP_TIMER_IDX(1)		:	13 */
+        {10, 0}, /*XLP_TIMER_IDX(2)		:	14 */
+        {10, 0}, /*XLP_TIMER_IDX(3)		:	15 */
+        {10, 0}, /*XLP_TIMER_IDX(4)		:	16 */
+        {10, 0}, /*XLP_TIMER_IDX(5)		:	17 */
+        {10, 0}, /*XLP_TIMER_IDX(6)		:	18 */
+        {10, 0}, /*XLP_TIMER_IDX(7)		:	19 */
+        {59, 0}, /*XLP_MSGQ_IDX(0)		:	20 */
+        {59, 0}, /*XLP_MSGQ_IDX(1)		:	21 */
+        {59, 0}, /*XLP_MSGQ_IDX(2)		:	22 */
+        {59, 0}, /*XLP_MSGQ_IDX(3)		:	23 */
+        {59, 0}, /*XLP_MSGQ_IDX(4)		:	24 */
+        {59, 0}, /*XLP_MSGQ_IDX(5)		:	25 */
+        {59, 0}, /*XLP_MSGQ_IDX(6)		:	26 */
+        {59, 0}, /*XLP_MSGQ_IDX(7)		:	27 */
+        {59, 0}, /*XLP_MSGQ_IDX(8)		:	28 */
+        {59, 0}, /*XLP_MSGQ_IDX(9)		:	29 */
+        {59, 0}, /*XLP_MSGQ_IDX(10)		:	30 */
+        {59, 0}, /*XLP_MSGQ_IDX(11)		:	31 */
+        {59, 0}, /*XLP_MSGQ_IDX(12)		:	32 */
+        {59, 0}, /*XLP_MSGQ_IDX(13)		:	33 */
+        {59, 0}, /*XLP_MSGQ_IDX(14)		:	34 */
+        {59, 0}, /*XLP_MSGQ_IDX(15)		:	35 */
+        {59, 0}, /*XLP_MSGQ_IDX(16)		:	36 */
+        {59, 0}, /*XLP_MSGQ_IDX(17)		:	37 */
+        {59, 0}, /*XLP_MSGQ_IDX(18)		:	38 */
+        {59, 0}, /*XLP_MSGQ_IDX(19)		:	39 */
+        {59, 0}, /*XLP_MSGQ_IDX(20)		:	40 */
+        {59, 0}, /*XLP_MSGQ_IDX(21)		:	41 */
+        {59, 0}, /*XLP_MSGQ_IDX(22)		:	42 */
+        {59, 0}, /*XLP_MSGQ_IDX(23)		:	43 */
+        {59, 0}, /*XLP_MSGQ_IDX(24)		:	44 */
+        {59, 0}, /*XLP_MSGQ_IDX(25)		:	45 */
+        {59, 0}, /*XLP_MSGQ_IDX(26)		:	46 */
+        {59, 0}, /*XLP_MSGQ_IDX(27)		:	47 */
+        {59, 0}, /*XLP_MSGQ_IDX(28)		:	48 */
+        {59, 0}, /*XLP_MSGQ_IDX(29)		:	49 */
+        {59, 0}, /*XLP_MSGQ_IDX(30)		:	50 */
+        {59, 0}, /*XLP_MSGQ_IDX(31)		:	51 */
+        {49, 0}, /*XLP_MSG_IDX(0)		:	52 */
+        {48, 0}, /*XLP_MSG_IDX(1)		:	53 */
+        {32, 0}, /*XLP_PCIE_MSIX_IDX(0)		:	54 */
+        {32, 0}, /*XLP_PCIE_MSIX_IDX(1)		:	55 */
+        {32, 0}, /*XLP_PCIE_MSIX_IDX(2)		:	56 */
+        {32, 0}, /*XLP_PCIE_MSIX_IDX(3)		:	57 */
+        {32, 0}, /*XLP_PCIE_MSIX_IDX(4)		:	58 */
+        {32, 0}, /*XLP_PCIE_MSIX_IDX(5)		:	59 */
+        {32, 0}, /*XLP_PCIE_MSIX_IDX(6)		:	60 */
+        {32, 0}, /*XLP_PCIE_MSIX_IDX(7)		:	61 */
+        {33, 0}, /*XLP_PCIE_MSIX_IDX(8)		:	62 */
+        {33, 0}, /*XLP_PCIE_MSIX_IDX(9)		:	63 */
+        {33, 0}, /*XLP_PCIE_MSIX_IDX(10)	:	64 */
+        {33, 0}, /*XLP_PCIE_MSIX_IDX(11)	:	65 */
+        {33, 0}, /*XLP_PCIE_MSIX_IDX(12)	:	66 */
+        {33, 0}, /*XLP_PCIE_MSIX_IDX(13)	:	67 */
+        {33, 0}, /*XLP_PCIE_MSIX_IDX(14)	:	68 */
+        {33, 0}, /*XLP_PCIE_MSIX_IDX(15)	:	69 */
+        {34, 0}, /*XLP_PCIE_MSIX_IDX(16)	:	70 */
+        {34, 0}, /*XLP_PCIE_MSIX_IDX(17)	:	71 */
+        {34, 0}, /*XLP_PCIE_MSIX_IDX(18)	:	72 */
+        {34, 0}, /*XLP_PCIE_MSIX_IDX(19)	:	73 */
+        {34, 0}, /*XLP_PCIE_MSIX_IDX(20)	:	74 */
+        {34, 0}, /*XLP_PCIE_MSIX_IDX(21)	:	75 */
+        {34, 0}, /*XLP_PCIE_MSIX_IDX(22)	:	76 */
+        {34, 0}, /*XLP_PCIE_MSIX_IDX(23)	:	77 */
+        {35, 0}, /*XLP_PCIE_MSIX_IDX(24)	:	78 */
+        {35, 0}, /*XLP_PCIE_MSIX_IDX(25)	:	79 */
+        {35, 0}, /*XLP_PCIE_MSIX_IDX(26)	:	80 */
+        {35, 0}, /*XLP_PCIE_MSIX_IDX(27)	:	81 */
+        {35, 0}, /*XLP_PCIE_MSIX_IDX(28)	:	82 */
+        {35, 0}, /*XLP_PCIE_MSIX_IDX(29)	:	83 */
+        {35, 0}, /*XLP_PCIE_MSIX_IDX(30)	:	84 */
+        {35, 0}, /*XLP_PCIE_MSIX_IDX(31)	:	85 */
+        {41, 0}, /*XLP_PCIE_LINK_IRQ(0)		:	86 */
+        {42, 0}, /*XLP_PCIE_LINK_IRQ(1)		:	87 */
+        {43, 0}, /*XLP_PCIE_LINK_IRQ(2)		:	88 */
+        {44, 0}, /*XLP_PCIE_LINK_IRQ(3)		:	89 */
+        {58, 0}, /*XLP_NAE_IDX(0)		:	90 */
+        {58, 0}, /*XLP_NAE_IDX(1)		:	91 */
+        {58, 0}, /*XLP_NAE_IDX(2)		:	92 */
+        {58, 0}, /*XLP_NAE_IDX(3)		:	93 */
+        {58, 0}, /*XLP_NAE_IDX(4)		:	94 */
+        {58, 0}, /*XLP_NAE_IDX(5)		:	95 */
+        {58, 0}, /*XLP_NAE_IDX(6)		:	96 */
+        {58, 0}, /*XLP_NAE_IDX(7)		:	97 */
+        {58, 0}, /*XLP_NAE_IDX(8)		:	98 */
+        {58, 0}, /*XLP_NAE_IDX(9)		:	99 */
+        {58, 0}, /*XLP_NAE_IDX(10)		:	100 */
+        {58, 0}, /*XLP_NAE_IDX(11)		:	101 */
+        {58, 0}, /*XLP_NAE_IDX(12)		:	102 */
+        {58, 0}, /*XLP_NAE_IDX(13)		:	103 */
+        {58, 0}, /*XLP_NAE_IDX(14)		:	104 */
+        {58, 0}, /*XLP_NAE_IDX(15)		:	105 */
+        {58, 0}, /*XLP_NAE_IDX(16)		:	106 */
+        {58, 0}, /*XLP_NAE_IDX(17)		:	107 */
+        {58, 0}, /*XLP_NAE_IDX(18)		:	108 */
+        {58, 0}, /*XLP_NAE_IDX(19)		:	109 */
+        {58, 0}, /*XLP_NAE_IDX(20)		:	110 */
+        {58, 0}, /*XLP_NAE_IDX(21)		:	111 */
+        {58, 0}, /*XLP_NAE_IDX(22)		:	112 */
+        {58, 0}, /*XLP_NAE_IDX(23)		:	113 */
+        {58, 0}, /*XLP_NAE_IDX(24)		:	114 */
+        {58, 0}, /*XLP_NAE_IDX(25)		:	115 */
+        {58, 0}, /*XLP_NAE_IDX(26)		:	116 */
+        {58, 0}, /*XLP_NAE_IDX(27)		:	117 */
+        {58, 0}, /*XLP_NAE_IDX(28)		:	118 */
+        {58, 0}, /*XLP_NAE_IDX(29)		:	119 */
+        {58, 0}, /*XLP_NAE_IDX(30)		:	120 */
+        {58, 0}, /*XLP_NAE_IDX(31)		:	121 */
+        {60, 0}, /*XLP_POE_IDX			:	122 */
+        {24, 0}, /*XLP_USB_IDX(0)		:	123 */
+        {24, 0}, /*XLP_USB_IDX(1)		:	124 */
+        {24, 0}, /*XLP_USB_IDX(2)		:	125 */
+        {25, 0}, /*XLP_USB_IDX(3)		:	126 */
+        {25, 0}, /*XLP_USB_IDX(4)		:	127 */
+        {25, 0}, /*XLP_USB_IDX(5)		:	128 */
+        {61, 0}, /*XLP_GDX_IDX			:	129 */
+        {63, 0}, /*XLP_SEC_IDX			:	130 */
+        {62, 0}, /*XLP_RSA_IDX			:	131 */
+        {39, 0}, /*XLP_COMP_IDX(0)		:	132 */
+        {39, 0}, /*XLP_COMP_IDX(1)		:	133 */
+        {39, 0}, /*XLP_COMP_IDX(2)		:	134 */
+        {39, 0}, /*XLP_COMP_IDX(3)		:	135 */
+        {0, 0}, /*RESERVED_IDX			:	136 */
+        {37, 0}, /*XLP_ICC_IDX(0)		:	137  ICC - Inter Chip Coherency*/
+        {37, 0}, /*XLP_ICC_IDX(1)		:	138 */
+        {37, 0}, /*XLP_ICC_IDX(2)		:	139 */
+        {36, 0}, /*XLP_CAM_IDX			:	140 */
+        {17, 0}, /*XLP_UART_IDX(0)		:	141 */
+        {18, 0}, /*XLP_UART_IDX(0)		:	142 */
+        {11, 0}, /*XLP_I2C_IDX(0)		:	143 */
+        {11, 0}, /*XLP_I2C_IDX(0)		:	144 */
+        {12, 0}, /*XLP_SYS_IDX(0)		:	145 */
+        {12, 0}, /*XLP_SYS_IDX(1)		:	146 */
+        {55, 0}, /*XLP_JTAG_IDX			:	147 */
+        {50, 0}, /*XLP_PIC_IDX			:	148 */
+        {54, 0}, /*XLP_NBU_IDX			:	149 */
+        {53, 0}, /*XLP_TCU_IDX			:	150 */
+        {52, 0}, /*XLP_GCU_IDX			:	151  GBC - Global Coherency*/
+        {38, 0}, /*XLP_DMC_IDX			:	152 */	/* collision */
+        {38, 0}, /*XLP_DMC_IDX			:	153 */
+        {13, 0}, /*XLP_GPIO_IDX(0)		:	154 */
+        {14, 0}, /*XLP_GPIO_IDX(1)		:	155 */
+        {15, 0}, /*XLP_GPIO_IDX(2)		:	156 */
+        {16, 0}, /*XLP_GPIO_IDX(3)		:	157 */
+        {20, 0}, /*XLP_NOR_IDX			:	158 */
+        {21, 0}, /*XLP_NAND_IDX			:	159 */
+        {22, 0}, /*XLP_SPI_IDX			:	160 */
+        {23, 0}, /*XLP_MMC_IDX			:	161 */
+        {0, 0}, /*			    162 */
+        {0, 0}, /*                          163 */
+        {0, 0}, /*                          164 */
+        {0, 0}, /*                          165 */
+        {0, 0}, /*                          166 */
+        {0, 0}, /*                          167 */
+};
+
+/*
+ * When startup function is called on an IRQ, that IRT's rvec map's bitmap
+ * would be set. This serves as a quick reverse lookup at the time of dispatch
+ */
+struct rvec_map_elem {
+	/* irt = elem.irt + ffs(bitmap), where bitmap != 0 */
+	int irt;	/* The first IRT corresponding to this rvec */
+	volatile unsigned long bitmap;	/* bit set is the offset from irt */
+};
+static struct rvec_map_elem rvec_map[XLP_EIRR_SIZE] = {
+	{-1, 0},			/* 0 */
+	{-1, 0},			/* 1 */
+	{-1, 0},			/* 2 */
+	{-1, 0},			/* 3 */
+	{-1, 0},			/* 4 */
+	{-1, 0},			/* 5 */
+	{-1, 0},			/* 6 */
+	{-1, 0},			/* 7 */
+	{-1, 0},			/* 8 */
+	{0, 0},				/* 9  Watchdog timer */
+	{4, 0},				/* 10 PIC Timter */
+	{135, 0},			/* 11 */
+	{137, 0},			/* 12 */
+	{146, 0},			/* 13 */
+	{147, 0},			/* 14 */
+	{148, 0},			/* 15 */
+	{149, 0},			/* 16 */
+	{133, 0},			/* 17 */
+	{134, 0},			/* 18 */
+	{2, 0},				/* 19 , Watchdog NMI */
+	{150, 0},			/* 20 */
+	{151, 0},			/* 21 */
+	{152, 0},			/* 22 */
+	{153, 0},			/* 23 */
+	{115, 0},			/* 24 */
+	{118, 0},			/* 25 */
+	{-1, 0},			/* 26 */
+	{-1, 0},			/* 27 */
+	{-1, 0},			/* 28 */
+	{-1, 0},			/* 29 */
+	{-1, 0},			/* 30 */
+	{-1, 0},			/* 31 */
+	{46, 0},			/* 32  MSIX - FN(0)*/
+	{54, 0},			/* 33  MSIX - FN(1)*/
+	{62, 0},			/* 34  MSIX - FN(2)*/
+	{70, 0},			/* 35  MSIX - FN(3)*/
+	{132, 0},			/* 36 */
+	{129, 0},			/* 37 */
+	{144, 0},			/* 38 */
+	{124, 0},			/* 39 */
+	{-1, 0},			/* 40 */
+	{78, 0},			/* 41 */
+	{79, 0},			/* 42 */
+	{80, 0},			/* 43 */
+	{81, 0},			/* 44 */
+	{-1, 0},			/* 45 */
+	{-1, 0},			/* 46 */
+	{-1, 0},			/* 47 */
+	{45, 0},			/* 48 */
+	{44, 0},			/* 49 XLP_MSG_IDX*/
+	{140, 0},			/* 50 */
+	{-1, 0},			/* 51 */
+	{143, 0},			/* 52 */
+	{142, 0},			/* 53 */
+	{141, 0},			/* 54 */
+	{139, 0},			/* 55 */
+	{-1, 0},			/* 56 */
+	{-1, 0},			/* 57 */
+	{82, 0},			/* 58 */
+	{12, 0},			/* 59 , MSGQ*/
+	{114, 0},			/* 60 */
+	{121, 0},			/* 61 */
+	{123, 0},			/* 62 */
+	{122, 0},			/* 63 */
+};
+
+/*
+ * Set some eimr bits on each cpu
+ * This function will be called on each cpu by on_each_cpu()
+ * @bitmask	: bitmask to set in EIMR
+ */
+void xlp_set_eimr(void *param)
 {
-	pic_reg_t *mmio = nlm_hal_pic_offset();
-	unsigned long flags;
-	nlm_reg_t reg;
-	unsigned long irt;
+	u64 bitmask = (u64) param;
+	u64 eimr;
 
-	if(irq < 8) {
-		return 0;
-	}
-	irt = find_irt_from_irq(irq);
-	if (irt == -1)	{
-		printk("can't find irt for irq: %d\n",irq);
-		return -1;
-	}
+	eimr = read_64bit_cp0_eimr();
+	eimr |= bitmask;
+	write_64bit_cp0_eimr(eimr);
+	return;
+}
 
-	spin_lock_irqsave(&nlm_common_pic_lock, flags);
+/*
+ * Clear some eimr bits on each cpu
+ * This function will be called on each cpu by on_each_cpu()
+ * @bitmask	: bitmask to clear in EIMR
+ */
+void xlp_clear_eimr(void *param)
+{
+	u64 bitmask = (u64) param;
+	u64 eimr = read_64bit_cp0_eimr();
+	eimr &= ~bitmask;
+	write_64bit_cp0_eimr(eimr);
+	return;
+}
 
-	/* What happens if this irq was previously not ack'ed? 
-	 * Assume, that doesn't happen?
-	 */
-	reg = nlm_hal_read_pic_reg(mmio, PIC_IRT(irt));
-	/* By default all the interrupts are initialized as level senstive - fix for the PCMCIA flash */
-	nlm_hal_write_pic_reg(mmio, PIC_IRT(irt), reg|(1 << 28)|(1 << 31));
+void __xlp_setup_one_irq(u64 irq)
+{
+	int cpu = smp_processor_id();
+	__nlm_hal_set_irt_to_cpu(xlp_irq_to_irt(irq), cpu);
+}
 
-	spin_unlock_irqrestore(&nlm_common_pic_lock, flags);
+static inline int __irqbase_from_rvec(int rvec)
+{
+	int irt;
 
-	return 0;
+	irt = rvec_map[rvec].irt;
+	if (irt < 0) {
+		return -EINVAL;
+	}
+	return(irt + XLP_IRQ_RESERVED_MAX);
 }
 
-static void pic_unmask(unsigned int irq)
+
+static inline int irqbase_from_rvec(int rvec)
 {
-	pic_reg_t *mmio = nlm_hal_pic_offset();
+	int ret;
 	unsigned long flags;
-	nlm_reg_t reg;
-	unsigned long irt;
 
-	if(irq < 8) {
-		return;
+	spin_lock_irqsave(&xlp_pic_lock, flags);
+	ret = __irqbase_from_rvec(rvec);
+	spin_unlock_irqrestore(&xlp_pic_lock, flags);
+	return ret;
+}
+
+/*
+ * returns rvec from an IRQ entry
+ * An IRQ entry is (irt table index + reserved max)
+ *
+ * @irq : irq number
+ */
+int xlp_rvec_from_irq(int irq)
+{
+	if ((irq < XLP_IRQ_RESERVED_MAX) || (irq >= XLP_IRQ_MAX)) {
+		return -EINVAL;
 	}
-	irt = find_irt_from_irq(irq);
-	if(irt == -1)
-	{
-		printk("can't find irt for irq: %d\n",irq);
+	return(irq_map[irq].rvec);
+}
+EXPORT_SYMBOL(xlp_rvec_from_irq);
+
+/*
+ * Masks out one IRQ in the EIMR register
+ * Must be called with xlp_pic_lock held
+ * @irq : IRQ number
+ */
+static void __nlm_irq_mask(unsigned int irq)
+{
+	int rvec;
+	u64 mask;
+
+	rvec = xlp_rvec_from_irq(irq);
+	if (rvec < 0) {
 		return;
 	}
+	on_each_cpu(xlp_clear_eimr, (void *) (1ULL << rvec), 1);
+	return;
+}
+
+/*
+ * Interface function (unlocked version) to mask an IRQ
+ * Calls helper function after input tests and spin_lock holds
+ *
+ * @irq : IRQ number
+ */
+static void nlm_irq_mask(unsigned int irq)
+{
+	unsigned long flags;
 
-	spin_lock_irqsave(&nlm_common_pic_lock, flags);
+	if((irq < XLP_IRQ_RESERVED_MAX) && (irq >= 0)) {
+		return;
+	} else if(irq >= XLP_IRQ_MAX) {
+		pr_err("irq = %d. Invalid irq requested\n", irq);
+		return;
+	}
+	//spin_lock_irqsave(&xlp_pic_lock, flags);	// Remove XXX
+	__nlm_irq_mask(irq);
+	//spin_unlock_irqrestore(&xlp_pic_lock, flags);	// XXX remove
+	return;
+}
 
-	/* What happens if this irq was previously not ack'ed? 
-	 * Assume, that doesn't happen?
-	 */
-	reg = nlm_hal_read_pic_reg(mmio, PIC_IRT(irt));
-	/* By default all the interrupts are initialized as level senstive - fix for the PCMCIA flash */
-	nlm_hal_write_pic_reg(mmio, PIC_IRT(nlm_hal_irq_to_irt(irq)),
-		      reg | (1 << 28) | (1 << 31));
 
-	spin_unlock_irqrestore(&nlm_common_pic_lock, flags);
+/*
+ * Changes eimr bit value corresponding to IRT
+ * @irq : IRQ number
+ */
+static void __nlm_irq_unmask(int irq)
+{
+	volatile u64 mask;
+	int rvec = xlp_rvec_from_irq(irq);
 
+	if (rvec < 0) {
+		return;
+	}
+	on_each_cpu(xlp_set_eimr, (void *) (1ULL << rvec), 1);
 	return;
 }
 
-static void pic_ack(unsigned int irq)
+/*
+ * Interface function (unlocked version) to mask an IRQ
+ * Calls helper function after input tests and spin_lock holds
+ *
+ * @irq : IRQ number
+ */
+static void nlm_irq_unmask(unsigned int irq)
 {
 	unsigned long flags;
-	unsigned long irt;
-	/*uint64_t val;*/
-
 
-	if(irq < 8) {
+	if((irq < XLP_IRQ_RESERVED_MAX) && (irq >= 0)) {
+		return;
+	} else if(irq >= XLP_IRQ_MAX) {
+		pr_err("irq = %d. Invalid irq requested\n", irq);
 		return;
 	}
-	irt = find_irt_from_irq(irq);
-	if(irt == -1)
-	{
-		printk("can't find irt for irq: %d\n",irq);
+	//spin_lock_irqsave(&xlp_pic_lock, flags);
+	__nlm_irq_unmask(irq);
+	//spin_unlock_irqrestore(&xlp_pic_lock, flags);
+	return;
+}
+
+static void nlm_irq_ack(unsigned int irq)
+{
+	unsigned long flags;
+
+	if((irq < XLP_IRQ_RESERVED_MAX) && (irq >= 0)) {
+		return;
+	} else if(irq >= XLP_IRQ_MAX) {
+		pr_err("irq = %d. Invalid irq requested\n", irq);
 		return;
 	}
-	/* If edge triggered IRQ, ack it immediately, else when the device
-	 * interrupt condition is cleared, we may lose interrupts 
-	 */
-	if (PIC_IRQ_IS_EDGE_TRIGGERED(irq)) {
-		spin_lock_irqsave(&nlm_common_pic_lock, flags);
-		nlm_hal_ack_pic(irt);
-		spin_unlock_irqrestore(&nlm_common_pic_lock, flags);
+	/* If edge triggered, ack it ASAP. Handle the interrupt later */
+	if (PIC_IRQ_IS_EDGE_TRIGGERED(xlp_irq_to_irt(irq))) {
+		spin_lock_irqsave(&xlp_pic_lock, flags);
+		nlm_hal_ack_pic(xlp_irq_to_irt(irq));
+		spin_unlock_irqrestore(&xlp_pic_lock, flags);
 	}
 }
 
-static void pic_end(unsigned int irq)
+static void nlm_irq_end(unsigned int irq)
 {
 	unsigned long flags;
-	unsigned long irt;
-	/*uint64_t val;*/
 
-	if(irq < 8) {
+	if((irq < XLP_IRQ_RESERVED_MAX) && (irq >= 0)) {
 		return;
-	}
-	irt = find_irt_from_irq(irq);
-	if(irt == -1)
-	{
-		printk("can't find irt for irq: %d\n",irq);
+	} else if(irq >= XLP_IRQ_MAX) {
+		pr_err("irq = %d. Invalid irq requested\n", irq);
 		return;
 	}
 	/* If level triggered, ack it after the device condition is cleared */
-	if (!PIC_IRQ_IS_EDGE_TRIGGERED(irq)) {
-		spin_lock_irqsave(&nlm_common_pic_lock, flags);
-		nlm_hal_ack_pic(nlm_hal_irq_to_irt(irq));
-		spin_unlock_irqrestore(&nlm_common_pic_lock, flags);
+	if (!PIC_IRQ_IS_EDGE_TRIGGERED(xlp_irq_to_irt(irq))) {
+		spin_lock_irqsave(&xlp_pic_lock, flags);
+		nlm_hal_ack_pic(xlp_irq_to_irt(irq));
+		spin_unlock_irqrestore(&xlp_pic_lock, flags);
 	}
+	return;
 }
 
-static void pic_shutdown(unsigned int irq)
+/*
+ * Startup function for normal IRQ
+ * @irq: irq number
+ */
+static unsigned int nlm_irq_startup(unsigned int irq)
+{
+	__label__ __failure;
+	int ret = 0;
+	unsigned long flags;
+	int idx, rvec;
+	struct cpumask m, *n;
+
+	cpumask_clear(&m);
+	cpumask_set_cpu(0, &m);
+	if((irq < XLP_IRQ_RESERVED_MAX) && (irq >= 0)) {
+		return 0;
+	} else if(irq >= XLP_IRQ_MAX) {
+		return 0;
+	}
+	n = xlp_closest_match_cpumask(&m);
+	spin_lock_irqsave(&xlp_pic_lock, flags);
+	rvec = xlp_rvec_from_irq(irq);
+	if (irq_map[irq].usage == 0) {
+		/* Currently unused => not enabled. So, setup and enable */
+		xlp_set_cpumask(n, xlp_irq_to_irt(irq));
+		ret = __nlm_hal_request_irq(xlp_irq_to_irt(irq) , rvec);
+		if (ret != 0) {
+			printk(KERN_WARNING "Failed to setup IRQ %d\n", irq);
+			goto __failure;
+		}
+		idx = irq - __irqbase_from_rvec(rvec);
+		set_bit(idx, &(rvec_map[rvec].bitmap));
+		irq_map[irq].usage++;
+		/* At this point, make sure that each CPU has eimr bit
+		 * corresponding to this IRQ set. Later the driver can set
+		 * the cpu affinity of this interrupt */
+		__nlm_irq_unmask(irq);
+	} else if (irq_map[irq].usage > 0) {
+		/* already being used. No need to check mask
+		 * if masked, will be unmasked later
+		 */
+		irq_map[irq].usage++;
+		ret = 0;
+	} else {
+		pr_err("Error irq = %d, rvec = %d, usage count %d\n", irq,
+				irq_map[irq].rvec, irq_map[irq].usage);
+		ret = -EFAULT;
+	}
+__failure:
+	spin_unlock_irqrestore(&xlp_pic_lock, flags);
+	return ret;
+}
+/*
+ * IRQ shut down function
+ * Disables one IRQ
+ *
+ * @irq : irq to shut down
+ */
+static void nlm_irq_shutdown(unsigned int irq)
 {
-	pic_reg_t *mmio = nlm_hal_pic_offset();
 	unsigned long flags;
-	nlm_reg_t reg;
-	unsigned long irt;
+	int idx, rvec;
 
-	if(irq < 8) {
+	if((irq < XLP_IRQ_RESERVED_MAX) && (irq >= 0)) {
+		return;
+	} else if(irq >= XLP_IRQ_MAX) {
+		pr_err("irq = %d. Invalid irq requested\n", irq);
 		return;
 	}
-	irt = find_irt_from_irq(irq);
-	if(irt == -1)
-	{
-		printk("can't find irt for irq: %d\n",irq);
+	spin_lock_irqsave(&xlp_pic_lock, flags);
+	if (irq_map[irq].usage == 0) {
+		fdebug("irq = %d, usage = %d\n", irq, irq_map[irq].usage);
+		spin_unlock_irqrestore(&xlp_pic_lock, flags);
 		return;
+	} else if (irq_map[irq].usage > 0) {
+		irq_map[irq].usage--;
 	}
-
-	spin_lock_irqsave(&nlm_common_pic_lock, flags);
-
-	/* What happens if this irq is currently pending an ack? 
-	 * Assume, that doesn't happen?
-	 */
-	reg = nlm_hal_read_pic_reg(mmio, PIC_IRT(irt));
-	nlm_hal_write_pic_reg(mmio, PIC_IRT(irt), (reg & ~(1 << 31)));
-
-	spin_unlock_irqrestore(&nlm_common_pic_lock, flags);
+	if (irq_map[irq].usage == 0) {
+		fdebug("irq = %d, usage = %d\n", irq, irq_map[irq].usage);
+		rvec = xlp_rvec_from_irq(irq);
+		idx = irq - __irqbase_from_rvec(rvec);
+		clear_bit(idx, &(rvec_map[rvec].bitmap));
+		spin_unlock_irqrestore(&xlp_pic_lock, flags);
+		__nlm_irq_mask(irq); /* masks this IRQ */
+	} else {
+		spin_unlock_irqrestore(&xlp_pic_lock, flags);
+	}
+	return;
 }
 
 /*
  * Set affinity for the irq for chips
- * 
+ *
+ * When an interrupt is setup, its EIMR bit is set in all online cpus. That is,
+ * any cpu _can_ receive that interrupt. But it is the IRT entry that decides
+ * whether to send that interrupt (i.e, whether to set EIRR bit or not) to that
+ * particular CPU.
+ *
+ * This function sets up the IRT entries according to cpumask.
  */
-
-static int pic_set_affinity(unsigned int irq, const struct cpumask *mask)
+static int nlm_irq_set_affinity(unsigned int irq, const struct cpumask *mask)
 {
 	unsigned long flags;
 	int cpu;
+	const struct cpumask *m, n;
 
-	spin_lock_irqsave(&nlm_common_pic_lock, flags);
-	for_each_online_cpu(cpu){
-		if(cpumask_test_cpu(cpu, mask))
-		{
-			nlm_hal_set_irq_to_cpu(irq, cpu);
-		}
+	if((irq < XLP_IRQ_RESERVED_MAX) && (irq >= 0)) {
+		return 0;
+	} else if(irq >= XLP_IRQ_MAX) {
+		pr_err("irq = %d. Invalid irq requested\n", irq);
+		return -EINVAL;
 	}
-	spin_unlock_irqrestore(&nlm_common_pic_lock, flags);
-
+	cpumask_and(&n, mask, cpu_online_mask);	/* check include/linux/cpumask.h */
+	m = xlp_closest_match_cpumask(&n);
+	if (m == NULL) {
+		printk(KERN_WARNING "Could not find a match for specified cpumask\n");
+		return -EINVAL;
+	}
+	spin_lock_irqsave(&xlp_pic_lock, flags);
+	xlp_set_cpumask(m, xlp_irq_to_irt(irq));
+	spin_unlock_irqrestore(&xlp_pic_lock, flags);
 	return 0;
 }
 
-static struct irq_chip nlm_common_pic = {
+static struct irq_chip nlm_irq_pic = {
 	.name = "XLP-PIC",
-	.unmask = pic_unmask,
-	.mask = pic_shutdown,
-	.ack = pic_ack,
-	.end = pic_end,
-	.set_affinity = pic_set_affinity
+	.mask = nlm_irq_mask,
+	.unmask = nlm_irq_unmask,
+	.startup = nlm_irq_startup,
+	.mask = nlm_irq_shutdown,
+	.ack = nlm_irq_ack,
+	.end = nlm_irq_end,
+	.set_affinity = nlm_irq_set_affinity
 };
 
 static void rsvd_pic_handler_1_1(unsigned int irq)
 {
-	if(irq < PIC_IRQ_BASE)
+	if((irq < XLP_IRQ_RESERVED_MAX) && (irq >= 0)) {
+		return;
+	} else if(irq >= XLP_IRQ_MAX) {
+		pr_err("irq = %d. Invalid irq requested\n", irq);
 		return;
-  dbg_msg("Requesting a reserved irq (%d)??", irq);
-  return;
+	}
+	pr_err("Requesting a reserved irq (%d)??", irq);
+	return;
 }
 
 static void rsvd_pic_handler_1(unsigned int irq)
 {
-	if(irq < PIC_IRQ_BASE)
+	if((irq < XLP_IRQ_RESERVED_MAX) && (irq >= 0)) {
 		return;
-  dbg_msg("handler called for a reserved irq (%d)\n", irq);
+	} else if(irq >= XLP_IRQ_MAX) {
+		pr_err("irq = %d. Invalid irq requested\n", irq);
+		return;
+	}
+	pr_err("Requesting a reserved irq (%d)??", irq);
+	return;
 }
 
 static int rsvd_pic_handler_2(unsigned int irq, const struct cpumask *mask)
 {
-	if(irq < PIC_IRQ_BASE)
-		return -1;
-
-	dbg_msg("handler called for a reserved irq (%d)\n", irq);
+	if((irq < XLP_IRQ_RESERVED_MAX) && (irq >= 0)) {
+		return 0;
+	} else if(irq >= XLP_IRQ_MAX) {
+		pr_err("irq = %d. Invalid irq requested\n", irq);
+		return -EINVAL;
+	}
+	pr_err("Requesting a reserved irq (%d)??", irq);
 	return 0;
 }
 
@@ -270,10 +729,17 @@ struct irq_chip nlm_common_rsvd_pic = {
 
 static irqreturn_t nlm_common_rsvd_irq_handler(int irq, void *dev_id)
 {
-	if(irq == IRQ_TIMER) 
+	if((irq < XLP_IRQ_RESERVED_MAX) && (irq >= 0)) {
+		return 0;
+	} else if(irq >= XLP_IRQ_MAX) {
+		pr_err("irq = %d. Invalid irq requested\n", irq);
+		return -EINVAL;
+	}
+	if(irq == XLP_IRQ_TIMER) {
 		return IRQ_HANDLED;
-  dbg_msg("handler for reserved irq %d\n", irq);
-  return IRQ_NONE;
+	}
+	pr_err("Requesting a reserved irq (%d)??", irq);
+	return IRQ_NONE;
 }
 
 struct irqaction nlm_common_rsvd_action = {
@@ -284,207 +750,698 @@ struct irqaction nlm_common_rsvd_action = {
 	.next = 0
 };
 
-void __init init_nlm_common_irqs(void)
+void do_nlm_common_IRQ(unsigned int irq, struct pt_regs *regs)
 {
-	int i;
-
-	for (i = 0; i < NR_IRQS; i++) {
-		set_irq_chip(i, &nlm_common_pic);
+	if (irq == XLP_IRQ_IPI_SMP_FUNCTION || irq == XLP_IRQ_IPI_SMP_RESCHEDULE) {
+		nlm_common_ipi_handler(irq, regs);
+		return;
+	}
+	if (irq == XLP_IRQ_MSGRING) {
+		nlm_xlp_msgring_int_handler(irq, regs);
+	}
+	else if (irq == XLP_IRQ_IPI_SMP_KGDB) {
+		/* ignore now */
+	}
+	else if (irq == XLP_IRQ_IPI_OPROFILE) {
+		/* ignore now */
+	}
+	else {
+		do_IRQ(irq);
 	}
+}
 
-#ifdef CONFIG_REMOTE_DEBUG
-	irq_desc[IRQ_REMOTE_DEBUG].chip = &nlm_common_rsvd_pic;
-	irq_desc[IRQ_REMOTE_DEBUG].action = nlm_common_rsvd_action;
-	nlm_xlp_irq_mask |= (1ULL << IRQ_REMOTE_DEBUG);
+void __cpuinit nlm_smp_irq_init(void)
+{
+#ifdef XLP_MERGE_TODO
+	/* Set up kseg0 to be cachable coherent */
+	change_c0_config(CONF_CM_CMASK, CONF_CM_DEFAULT);
 #endif
+	/* set interrupt mask for non-zero cpus */
+	write_64bit_cp0_eimr(xlp_irq_mask | (1 << XLP_IRQ_TIMER));
+}
 
-#ifdef CONFIG_SMP
-	irq_desc[IRQ_IPI_SMP_FUNCTION].chip = &nlm_common_rsvd_pic;
-	irq_desc[IRQ_IPI_SMP_FUNCTION].action = &nlm_common_rsvd_action;
-
-	irq_desc[IRQ_IPI_SMP_RESCHEDULE].chip = &nlm_common_rsvd_pic;
-	irq_desc[IRQ_IPI_SMP_RESCHEDULE].action = &nlm_common_rsvd_action;
-
-#ifdef CONFIG_NLMCOMMON_IP_FLOW_AFFINITY
-	/* PR: New IPI added here for netrx balancing */
-	irq_desc[IRQ_IPI_NETRX].chip = &nlm_common_rsvd_pic;
-	irq_desc[IRQ_IPI_NETRX].action = &nlm_common_rsvd_action;
-	nlm_xlp_irq_mask |= (1ULL << IRQ_IPI_NETRX);
-#endif				/* CONFIG_NLMCOMMON_IP_FLOW_AFFINITY */
-
-	nlm_xlp_irq_mask |=
-	    ((1ULL << IRQ_IPI_SMP_FUNCTION) | (1ULL << IRQ_IPI_SMP_RESCHEDULE));
-#endif
+void destroy_irq(unsigned int irq)
+{
+    /* no-op */
+}
 
-	/* msgring interrupt */
-	irq_desc[IRQ_MSGRING].chip = &nlm_common_rsvd_pic;
-	irq_desc[IRQ_MSGRING].action = &nlm_common_rsvd_action;
-	nlm_xlp_irq_mask |= (1ULL << IRQ_MSGRING);
+#ifdef CONFIG_PCI_MSI_XLP
 
-	/* unmask all PIC related interrupts. If no handler is installed by the 
-	 * drivers, it'll just ack the interrupt and return 
-	 */
-	for (i = PIC_IRT_FIRST_IRQ; i <= PIC_IRT_LAST_IRQ(); i++)
-		nlm_xlp_irq_mask |= (1ULL << i);
+/*
+ * These bunch of functions would find out the controller function using the
+ * passed parameter and use nlm_irq_* function to operate on that IRT
+ */
+static unsigned int nlm_msi_startup(unsigned int msi)
+{
+	if ((msi < XLP_MSI_INDEX_START) || (msi > XLP_MSI_INDEX_END)){
+		return -EINVAL;
+	}
+	return nlm_irq_startup(XLP_PCIE_LINK_IRQ(XLP_MSI_TO_CTRL_FN(msi)));
+}
 
-#ifdef CONFIG_OPROFILE
-	nlm_xlp_irq_mask |= (1ULL << IRQ_IPI_OPROFILE);
-#endif
+static int nlm_msi_set_affinity(unsigned int msi, const struct cpumask *mask)
+{
+	struct cpumask m;
+	if ((msi < XLP_MSI_INDEX_START) || (msi > XLP_MSI_INDEX_END)){
+		return -EINVAL;
+	}
 
-#ifdef CONFIG_KGDB
-	nlm_xlp_irq_mask |= (1ULL << IRQ_IPI_SMP_KGDB);
-#endif
+	cpumask_and(&m, mask, cpu_online_mask);
+	if (cpumask_equal(&m, cpu_online_mask)){
+		return nlm_irq_set_affinity(XLP_PCIE_LINK_IRQ(XLP_MSI_TO_CTRL_FN(msi)), &m);
+	} else {
+		printk(KERN_WARNING "MSI cpu affinity change not supported\n");
+		return -EINVAL;
+	}
+}
 
-	nlm_xlp_irq_mask |= (1ULL << IRQ_TIMER);
+static void nlm_msi_shutdown(unsigned int msi)
+{
+	if ((msi < XLP_MSI_INDEX_START) || (msi > XLP_MSI_INDEX_END)){
+		return;
+	}
+	return nlm_irq_shutdown(XLP_PCIE_LINK_IRQ(XLP_MSI_TO_CTRL_FN(msi)));
+}
+static void nlm_msi_end(unsigned int msi)
+{
+	if ((msi < XLP_MSI_INDEX_START) || (msi > XLP_MSI_INDEX_END)){
+		return;
+	}
+	return nlm_irq_end(XLP_PCIE_LINK_IRQ(XLP_MSI_TO_CTRL_FN(msi)));
 }
 
-void do_nlm_common_IRQ(unsigned int irq, struct pt_regs *regs)
+
+static void nlm_msi_ack(unsigned int msi)
 {
-	if (irq == IRQ_IPI_SMP_FUNCTION || irq == IRQ_IPI_SMP_RESCHEDULE) {
-		nlm_common_ipi_handler(irq, regs);
+	if ((msi < XLP_MSI_INDEX_START) || (msi > XLP_MSI_INDEX_END)){
 		return;
 	}
-	if (irq == IRQ_MSGRING) {
-		nlm_xlp_msgring_int_handler(irq, regs);
+	return nlm_irq_ack(XLP_PCIE_LINK_IRQ(XLP_MSI_TO_CTRL_FN(msi)));
+}
+
+/*
+ * Masks just one MSI
+ * @msi : the MSI to mask
+ */
+
+static u32 nlm_msi_change_mask(unsigned int msi, int val)
+{
+	unsigned long flags;
+	int bit, fn;
+	u32 mask;
+
+	fn = XLP_MSI_TO_CTRL_FN(msi);
+	bit = msi - XLP_MSI_IRQ_START(fn);
+	spin_lock_irqsave(&xlp_pic_lock, flags);
+	mask = xlp_msi_set_mask(fn, bit, val);
+	if (val == 0) {
+		if (mask == 0) { /* This was the last bit to clear */
+			__nlm_irq_mask(XLP_PCIE_LINK_IRQ(XLP_MSI_TO_CTRL_FN(msi)));
+		}
+		if ((mask & (mask - 1)) == 0) {	/* Just set the only bit*/
+			__nlm_irq_unmask(XLP_PCIE_LINK_IRQ(XLP_MSI_TO_CTRL_FN(msi)));
+		}
 	}
-	else if (irq == IRQ_IPI_SMP_KGDB) {
-  	}
-	else if (irq == IRQ_IPI_OPROFILE) {
+	spin_unlock_irqrestore(&xlp_pic_lock, flags);
+	return mask;
+}
+
+static void nlm_msi_mask(unsigned int msi)
+{
+	if ((msi < XLP_MSI_INDEX_START) || (msi > XLP_MSI_INDEX_END)){
+		return ;
 	}
-  	else 
-		do_IRQ(irq);
+	nlm_msi_change_mask(msi, 0);
+	return;
 }
 
-void __cpuinit nlm_smp_irq_init(void)
+/*
+ * Unmask just one MSI
+ * If required, unmask the corresponding IRT as well
+ */
+static void nlm_msi_unmask(unsigned int msi)
 {
-#ifdef XLP_MERGE_TODO
-	/* Set up kseg0 to be cachable coherent */
-	change_c0_config(CONF_CM_CMASK, CONF_CM_DEFAULT);
-#endif
-	/* set interrupt mask for non-zero cpus */
-	write_64bit_cp0_eimr(nlm_xlp_irq_mask | (1 << IRQ_TIMER));
+	if ((msi < XLP_MSI_INDEX_START) || (msi > XLP_MSI_INDEX_END)){
+		return ;
+	}
+	nlm_msi_change_mask(msi, 1);
+	return;
 }
 
-/* 
+/*
  * MSI hook-up routines for Netlogic Boards;
  * Arch-dependent implementation called
  * from generic msi.c routines.
  */
 
-struct irq_chip nlm_common_pic_msi = {
-	.name = "Netlogic-PIC-MSI",
-	.startup = pic_startup,
-	.shutdown = pic_shutdown,
-	.ack = pic_ack,
-	.end = pic_end,
-	.set_affinity = pic_set_affinity
+struct irq_chip nlm_msi_pic = {
+	.name = "XLP-PIC-MSI",
+	.startup = nlm_msi_startup,
+	.shutdown = nlm_msi_shutdown,
+	.ack = nlm_msi_ack,
+	.end = nlm_msi_end,
+	.mask = nlm_msi_mask,
+	.unmask = nlm_msi_unmask,
+	.set_affinity = nlm_msi_set_affinity
 };
 
-void destroy_irq(unsigned int irq)
+/*
+ * These bunch of functions would find out the controller function using the
+ * passed parameter and use nlm_irq_* function to operate on that IRT
+ */
+
+static int nlm_msix_set_affinity(unsigned int msix, const struct cpumask *mask)
 {
-    /* no-op */
+	if ((msix < XLP_MSIX_INDEX_START) || (msix > XLP_MSIX_INDEX_END)){
+		return -EINVAL;
+	}
+	return nlm_irq_set_affinity(XLP_PCIE_MSIX_IRQ(msix - XLP_MSIX_INDEX_START), mask);
 }
 
-#ifdef CONFIG_PCI_MSI_XLR
+static void nlm_msix_end(unsigned int msix)
+{
+	if ((msix < XLP_MSIX_INDEX_START) || (msix > XLP_MSIX_INDEX_END)){
+		return;
+	}
+	return nlm_irq_end(XLP_PCIE_MSIX_IRQ(msix - XLP_MSIX_INDEX_START));
+}
 
-static int get_irq_vector(struct pci_dev *dev)
+static void nlm_msix_ack(unsigned int msix)
 {
-	return nlm_hal_irt_to_irq(PIC_IRT_PCIE_LINK_INDEX(0));
+	if ((msix < XLP_MSIX_INDEX_START) || (msix > XLP_MSIX_INDEX_END)){
+		return;
+	}
+	return nlm_irq_ack(XLP_PCIE_MSIX_IRQ(msix - XLP_MSIX_INDEX_START));
 }
 
-static int msi_compose_msg(struct pci_dev *pdev, unsigned int irq,
-			   struct msi_msg *msg)
+/*
+ * Masks just one MSIX
+ * @msix : the MSIX to mask
+ */
+static void nlm_msix_mask(unsigned int msix)
 {
+	unsigned long flags;
+	if ((msix < XLP_MSIX_INDEX_START) || (msix > XLP_MSIX_INDEX_END)){
+		return ;
+	}
+	spin_lock_irqsave(&xlp_pic_lock, flags);
+	__nlm_irq_mask(XLP_PCIE_MSIX_IRQ(msix - XLP_MSIX_INDEX_START));
+	mask_msi_irq(msix); /* Disable MSI-X -- please note */
+	spin_unlock_irqrestore(&xlp_pic_lock, flags);
+	return;
+}
 
-	unsigned dest;
+/*
+ * Unmask just one MSIX
+ * If required, unmask the corresponding IRT as well
+ */
+static void nlm_msix_unmask(unsigned int msix)
+{
+	unsigned long flags;
+	if ((msix < XLP_MSIX_INDEX_START) || (msix > XLP_MSIX_INDEX_END)){
+		return ;
+	}
+	spin_lock_irqsave(&xlp_pic_lock, flags);
+	__nlm_irq_mask(XLP_PCIE_MSIX_IRQ(msix - XLP_MSIX_INDEX_START));
+	unmask_msi_irq(msix); /* Enable MSI-X -- please note */
+	spin_unlock_irqrestore(&xlp_pic_lock, flags);
+	return;
+}
 
-	if (irq >= 0) {
-		dest = 0x00;
-		msg->address_hi = MSI_ADDR_BASE_HI;
-		msg->address_lo = MSI_ADDR_BASE_LO |
-		    MSI_ADDR_DEST_MODE_PHYSICAL |
-		    MSI_ADDR_REDIRECTION_CPU | MSI_ADDR_DEST_ID(dest);
-		msg->data = MSI_DATA_TRIGGER_EDGE |
-		    MSI_DATA_LEVEL_ASSERT |
-		    MSI_DATA_DELIVERY_FIXED | MSI_DATA_VECTOR(irq);
+static unsigned int nlm_msix_startup(unsigned int msix)
+{
+	if ((msix < XLP_MSIX_INDEX_START) || (msix > XLP_MSIX_INDEX_END)){
+		printk(KERN_WARNING "Invalid msix #%d\n", msix);
+		return -EINVAL;
 	}
-	return irq;
+	nlm_msix_unmask(msix);
+	return nlm_irq_startup(XLP_PCIE_MSIX_IRQ(msix - XLP_MSIX_INDEX_START));
 }
 
-void arch_teardown_msi_irq(unsigned int irq)
+static void nlm_msix_shutdown(unsigned int msix)
 {
-	destroy_irq(irq);
+	if ((msix < XLP_MSIX_INDEX_START) || (msix > XLP_MSIX_INDEX_END)){
+		return;
+	}
+	nlm_msix_mask(msix);
+	return nlm_irq_shutdown(XLP_PCIE_MSIX_IRQ(msix - XLP_MSIX_INDEX_START));
 }
 
-int arch_setup_msi_irq(struct pci_dev *dev, struct msi_desc *desc)
+/*
+ * MSI-X hook-up routines for Netlogic Boards;
+ * Arch-dependent implementation called
+ * from generic msi.c routines.
+ */
+
+struct irq_chip nlm_msix_pic = {
+	.name = "XLP-PIC-MSIX",
+	.startup = nlm_msix_startup,
+	.shutdown = nlm_msix_shutdown,
+	.ack = nlm_msix_ack,
+	.end = nlm_msix_end,
+	.mask = nlm_msix_mask,
+	.unmask = nlm_msix_unmask,
+	.set_affinity = nlm_msix_set_affinity
+};
+
+
+static int xlp_msi_compose_msg(struct pci_dev *pdev, struct msi_desc *desc,
+		unsigned int irq, struct msi_msg *msg)
 {
-	struct msi_msg msg;
-	int irq, ret;
+	u8 offset;
+	int fn = xlp_ctrl_fn_from_dev(pdev);
+
+	if (fn < 0) return -EINVAL;
+	if (desc->msi_attrib.is_msix) {
+		if (irq < XLP_MSIX_INDEX_START) {	/* enforce minimum */
+			fdebug("Invalid irq %d", irq);
+			return -EINVAL;
+		}
+		offset = irq - XLP_MSIX_INDEX_START;
+		msg->address_hi = (virt_to_phys(xlp_msix_addr_start(fn)) >> 32);
+		msg->address_lo = (virt_to_phys(xlp_msix_addr_start(fn)) & 0xffffffff);
+		dev_err(&pdev->dev, "MSI-X hi = %#x, lo = %#x, data = %#x\n", msg->address_hi, msg->address_lo, offset);
+	} else {
+		if (irq < XLP_MSI_IRQ_OFFSET) {	/* enforce minimum */
+			return -EINVAL;
+		}
+		offset = irq - (XLP_MSI_IRQ_START(xlp_ctrl_fn_from_dev(pdev)));
+		msg->address_hi = (virt_to_phys(xlp_msi_addr_start(fn)) >> 32) & 0xffffffff;
+		msg->address_lo = (virt_to_phys(xlp_msi_addr_start(fn)) & 0xffffffff);
+	}
+	msg->data = offset;
+	return 0;
+}
 
-	irq = get_irq_vector(dev);
-	if (irq < 0)
-		return irq;
-	set_irq_msi(irq, desc);
-	ret = msi_compose_msg(dev, irq, &msg);
-	if (ret < 0) {
-		destroy_irq(irq);
-		return ret;
+/*
+ * Returns the bitmask of currently used MSI-X on controller fn
+ *
+ * Must call with lock held
+ * @fn : controller number
+ */
+u32 __xlp_msix_bitmask(int fn)
+{
+	int idx = 0, ret = 0;
+
+	while (idx < XLP_MSIX_PER_SLOT) {
+		if (irq_map[XLP_MSIX_IRQ_START(fn) + idx].usage > 0) {
+			ret |= (1 << idx);
+		}
+		idx++;
 	}
-	write_msi_msg(irq, &msg);
-	irq_desc[irq].chip = &nlm_common_pic_msi;
-	nlm_xlp_irq_mask |= (1ULL << irq);
-	return irq;
+	return ret;
 }
-#endif
+
+void arch_teardown_msi_irq(unsigned int msi)
+{
+	unsigned long flags;
+	int bit, fn;
+	u32 mask;
+
+	switch (msi) {
+	case XLP_MSI_INDEX_START ... XLP_MSI_INDEX_END:
+		fn = XLP_MSI_TO_CTRL_FN(msi);
+		bit = msi - XLP_MSI_IRQ_START(fn);
+		spin_lock_irqsave(&xlp_pic_lock, flags);
+		if (irq_map[msi].usage > 0) {
+			irq_map[msi].usage--;
+		}
+		mask = xlp_msi_set_mask(fn, bit, 0);/*set bit=0 & get the mask*/
+		if (mask == 0) { /* This was the last bit to clear */
+			if (irq_map[msi].usage != 0) {
+				printk("Fatal Error. Interrupt usage mismatch(%d), expected 0\n", irq_map[msi].usage);
+			}
+			__nlm_irq_mask(XLP_PCIE_LINK_IRQ(XLP_MSI_TO_CTRL_FN(msi)));
+			xlp_msi_disable(fn, 0xf);/* disable MSI; enable INTx */
+			xlp_intx_enable(fn);
+		}
+		spin_unlock_irqrestore(&xlp_pic_lock, flags);
+		return;
+	case XLP_MSIX_INDEX_START ... XLP_MSIX_INDEX_END:
+		fn = XLP_MSIX_TO_CTRL_FN(msi);
+		bit = msi - XLP_MSIX_IRQ_START(fn);
+		nlm_msix_shutdown(msi);
+		spin_lock_irqsave(&xlp_pic_lock, flags);
+		if (irq_map[msi].usage > 0) {
+			irq_map[msi].usage--;
+			msix_vec[fn].bitmap &= ~(1 << bit);
+			msix_vec[fn].count--;
+		}
+		if (!__xlp_msix_bitmask(fn)) {
+			xlp_msix_disable(fn);
+			xlp_intx_enable(fn);
+		}
+		spin_unlock_irqrestore(&xlp_pic_lock, flags);
+		return;
+	default:
+		return;	/* Do not proceed if !(msi || msix) */
+	}
+}
+
+#endif		// CONFIG_PCI_MSI
 
 static int xlp_perf_irq(void)
 {
 	return IRQ_HANDLED;
 }
 
-void __init arch_init_irq(void)
+asmlinkage void plat_irq_dispatch(void)
 {
-	extern int (*perf_irq)(void);
+	volatile u64 eirr;
+	volatile u64 eimr;
+	volatile u64 bitmap;
+	struct pt_regs *pt_regs = current_thread_info()->regs;
+	int rvec = 0, idx = 0, base_irq, irq, fn;
+	unsigned long flags;
 
-	perf_irq = xlp_perf_irq;
+	eirr = read_64bit_cp0_eirr();
+	eimr = read_64bit_cp0_eimr();
+	eirr &= eimr;
+	if (eirr & (1 << XLP_IRQ_TIMER)) {
+		nlm_common_timer_interrupt(pt_regs, XLP_IRQ_TIMER);
+		return;
+	}
+	while (eirr) {		/* LOOP STARTS HERE */
 
-#ifdef CONFIG_KGDB
-	if (kgdb_early_setup)
+	rvec = __ilog2_u64(eirr);
+	if (rvec == -1) {
 		return;
-#endif
+	}
+	eirr &= ~(1ULL << rvec);
+	if (rvec != XLP_IRQ_MSGRING) {
+		write_64bit_cp0_eirr(1ULL << rvec);
+	}
+	if (rvec < XLP_IRQ_RESERVED_MAX) {
+		irq = rvec;
+		do_nlm_common_IRQ(irq, pt_regs);
+		if (rvec == XLP_IRQ_MSGRING) {
+			write_64bit_cp0_eirr(1ULL << rvec);
+		}
+		return;
+	} else {
+		/* We need to loop through all possible irqs for an rvec */
+		base_irq = irqbase_from_rvec(rvec);
+		if(base_irq < 0) {
+			return;
+		}
+		spin_lock_irqsave(&xlp_pic_lock, flags);
+		bitmap = rvec_map[rvec].bitmap;
+		spin_unlock_irqrestore(&xlp_pic_lock, flags);
+		switch(base_irq) {
+		/* These are not MSIs, but IRT #s */
+		case XLP_PCIE_LINK_IRQ(0) ... XLP_PCIE_LINK_IRQ(3):
+			/* Here fn # of controller is easily calculated
+			 * Check the IRT table : 0 -> 78, 1-> 79 ..etc */
+			fn = base_irq - XLP_PCIE_LINK_IRQ(0);
+			if (is_msi_set(fn) != 0) { /* this is an MSI */
+				/* find vectors set */
+				bitmap = calc_msi_vector_offset(fn);
+				/* recalculate base_irq for MSI */
+				base_irq = XLP_MSI_IRQ_START(fn);
+				/* now handle it as any other interrupt */
+			}
+			break;
+		case XLP_PCIE_MSIX_IRQ(0) ... XLP_PCIE_MSIX_IRQ(31):
+			fn = XLP_MSIX_TO_CTRL_FN(base_irq - XLP_PCIE_MSIX_IRQ(0));/* This _is_ correct because of((x >>3) &3) */
+			/* this is an MSI/MSI-X. Find vectors set */
+			bitmap = xlp_msix_status_clear(fn);
+			/* recalculate base_irq for MSI */
+			base_irq = XLP_MSIX_IRQ_START(fn);
+			/* now handle it as any other interrupt */
+			break;
+		default:
+			break;
+		}
+		while (bitmap) {
+			idx = ffs(bitmap) - 1;	/* man ffs */
+			bitmap &= ~(1 << idx);
+			irq = base_irq + idx;
+			do_nlm_common_IRQ(irq, pt_regs);
+		}
+		write_64bit_cp0_eirr(1ULL << rvec);
+	}
 
-	/* Initialize the irq descriptors */
-	init_nlm_common_irqs();
+	}	// End of while (eirr)...
+	return;
+}
+
+int nlm_xlp_request_irq(int irq)
+{
+	if((irq < XLP_IRQ_RESERVED_MAX) && (irq >= 0)) {
+		return -ENODEV;
+	} else if(irq >= XLP_IRQ_MAX) {
+		return -EFAULT;
+	}
+	return(irq + XLP_IRQ_RESERVED_MAX);
+}
+EXPORT_SYMBOL(nlm_xlp_request_irq);
+
+#ifdef arch_setup_msi_irqs
+/*
+ * Arch specific setup functions and helpers
+ */
 
-	write_64bit_cp0_eimr(nlm_xlp_irq_mask);
+static int __xlp_alloc_msi(int base, int max_base, int req, int *max_avail)
+{
+	int ret;
 
+	if ((base + req) > max_base) {
+		*max_avail = -1;
+		return -ENOSPC;
+	}
+	if (irq_map[base].usage == 0) { /* allocate this vector */
+		*max_avail += *max_avail;
+		irq_map[base].usage = 1;
+		if (req == 1) {
+			return 0;	/* success */
+		}
+		ret = __xlp_alloc_msi(base + 1, max_base, req - 1, max_avail);
+		if (ret != 0) {
+			irq_map[base].usage = 0; /* clear before return fail */
+		}
+		return ret;
+	} else {	/* usage > 0 */
+		return -ENOSPC;	/* max_avail is already set */
+	}
+}
+/*
+ * Reserves max of req MSIs
+ * @base : base offset to start searching for irq entries for this device
+ * @idx	: index to start searching from base
+ * @req : number of irqs requested
+ * @min : Max number of MSIs that can be allocated at the moment. This value
+ * can be modified if an iteration finds a value greater than this.
+ */
+static int __xlp_reserve_nvec(int base, int req, int *max)
+{
+	int ret, idx = 0;
+	*max = 0;
+	while (idx < XLP_MSI_PER_SLOT) {
+		ret = __xlp_alloc_msi(base + idx, base + XLP_MSI_PER_SLOT, req, max);
+		if (ret == 0) {
+			return 0;
+		}
+		idx++;
+	}
+	return ret;
 }
 
-asmlinkage void plat_irq_dispatch(void)
+int xlp_setup_msi_irq(struct pci_dev *dev, struct msi_desc *desc, int nvec)
 {
-	uint64_t eirr;
-	struct pt_regs *pt_regs = current_thread_info()->regs;
-	int i = 0;
+	struct msi_msg msg;
+	int ret, max;
+	int base_msi = XLP_MSI_IRQ_START(xlp_ctrl_fn_from_dev(dev));
+	unsigned long flags;
+	int fn;
 
-	eirr = read_64bit_cp0_eirr() & read_64bit_cp0_eimr();
 
-	if (eirr & (1 << IRQ_TIMER)) {
-		nlm_common_timer_interrupt(pt_regs, IRQ_TIMER);
-		return;
+	fn = xlp_ctrl_fn_from_dev(dev);
+	if (base_msi < XLP_MSI_IRQ_OFFSET) {
+		return -EFAULT;
 	}
+	if ((nvec & (nvec - 1))) {	/* test if multiple of 2 */
+		return -EINVAL;
+	}
+	if (nvec > XLP_MSI_PER_SLOT) {
+		return -EINVAL;
+	}
+	spin_lock_irqsave(&xlp_pic_lock, flags);
+	ret = __xlp_reserve_nvec(base_msi, nvec, &max);
+	spin_unlock_irqrestore(&xlp_pic_lock, flags);
+	if (ret < 0) {
+		return max;
+	}
+	set_irq_msi(base_msi, desc);
+	ret = xlp_msi_compose_msg(dev, desc, base_msi, &msg);
+	if (ret < 0) {
+		return ret;
+	}
+	write_msi_msg(base_msi, &msg);
+	xlp_msix_disable(fn);
+	xlp_intx_disable(fn);
+	xlp_msi_enable(dev, 1);	// XXX Enable MSI for bit 0
+	return 0;
+}
 
-	i = __ilog2_u64(eirr);
-	if (i == -1)
-		return;
+/*
+ * MSI-X setup functions
+ */
 
-	if (i  != IRQ_MSGRING)
-		write_64bit_cp0_eirr(1ULL << i);
+/*
+ * Allocates a single MSI-X vector for msi_desc entry
+ * @dev		: pci device
+ * @desc	: msi_descriptor for this msi entry
+ * @nvec	: outstanding number of interrupts required for this device
+ */
+int xlp_setup_msix_irq(struct pci_dev *dev, struct msi_desc *desc, int nvec)
+{
+	__label__ fail_nospc;
+	struct msi_msg msg;
+	int ret, idx, base_msix, fn = xlp_ctrl_fn_from_dev(dev);
+	unsigned long flags;
+
+	if (fn < 0) {
+		return -EFAULT;
+	}
+	base_msix = XLP_MSIX_IRQ_START(fn);
+	if (base_msix < XLP_MSIX_IRQ_OFFSET) {
+		return -EFAULT;
+	}
+
+	spin_lock_irqsave(&xlp_pic_lock, flags);
+	if ((XLP_MSIX_PER_SLOT - msix_vec[fn].count) < nvec) {
+		dev_err(&dev->dev, "Not enough vectors(%#x)to allocate(%#x)\n",
+			(XLP_MSIX_PER_SLOT - msix_vec[fn].count), nvec);
+		ret = -ENOSPC;
+		goto fail_nospc;
+	}
+	for (idx = 0; idx < XLP_MSIX_PER_SLOT; idx++) {
+		if (irq_map[base_msix + idx].usage != 0) {
+			continue;
+		}
+		/* We hit an unused entry */
+		irq_map[base_msix + idx].usage = 1;
+		msix_vec[fn].bitmap |= 1 << idx;
+		msix_vec[fn].count++;
+		break;
+	}
+	spin_unlock_irqrestore(&xlp_pic_lock, flags);
+	if (idx == XLP_MSIX_PER_SLOT) {
+		return -ENOSPC;
+	}
+	set_irq_msi(base_msix + idx, desc);
+	ret = xlp_msi_compose_msg(dev, desc, base_msix + idx, &msg);
+	if (ret < 0) {
+		return ret;
+	}
+	write_msi_msg(base_msix + idx, &msg);
+	xlp_intx_disable(fn);
+	xlp_msi_disable(fn, 0xf);
+	xlp_msix_enable(dev);	/* enable msix */
+	return 0;
 
-	do_nlm_common_IRQ(i, pt_regs);
+fail_nospc:
+	spin_unlock_irqrestore(&xlp_pic_lock, flags);
+	return ret;
+}
 
-	if (i  == IRQ_MSGRING)
-		write_64bit_cp0_eirr(1ULL << i);
+int arch_setup_msi_irqs(struct pci_dev *dev, int nvec, int type)
+{
+	struct msi_desc *entry;
+	int ret;
 
-	return;
+	if ((type == PCI_CAP_ID_MSI) && (nvec > XLP_MSI_PER_SLOT)){
+		return -EINVAL;
+	}
+	if ((type == PCI_CAP_ID_MSIX) && (nvec > XLP_MSIX_PER_SLOT)){
+		return -EINVAL;
+	}
+	list_for_each_entry(entry, &dev->msi_list, list) {
+		if (type == PCI_CAP_ID_MSI) {
+			ret = xlp_setup_msi_irq(dev, entry, nvec);
+		} else {
+			ret = xlp_setup_msix_irq(dev, entry, nvec--);
+		}
+		if (ret < 0) {
+			return ret;
+		}
+	}
+	return 0;
 }
+EXPORT_SYMBOL(arch_setup_msi_irqs);
+#endif
 
+void __init init_nlm_common_irqs(void)
+{
+	int i;
+
+	for (i = 0; i < XLP_IRQ_MAX; i++) {	// IRQ : 0 - 167
+		set_irq_chip(i, &nlm_irq_pic);
+	}
+#ifdef CONFIG_PCI_MSI_XLP
+	for (i = XLP_MSI_INDEX_START; i <= XLP_MSI_INDEX_END; i++) {
+		set_irq_chip(i, &nlm_msi_pic);
+	}
+	for (i = XLP_MSIX_INDEX_START; i <= XLP_MSIX_INDEX_END; i++) {
+		set_irq_chip(i, &nlm_msix_pic);
+	}
+#endif
+
+#ifdef CONFIG_REMOTE_DEBUG
+	irq_desc[XLP_IRQ_REMOTE_DEBUG].chip = &nlm_common_rsvd_pic;
+	irq_desc[XLP_IRQ_REMOTE_DEBUG].action = nlm_common_rsvd_action;
+	xlp_irq_mask |= (1ULL << XLP_IRQ_REMOTE_DEBUG);
+#endif
+
+#ifdef CONFIG_SMP
+	irq_desc[XLP_IRQ_IPI_SMP_FUNCTION].chip = &nlm_common_rsvd_pic;
+	irq_desc[XLP_IRQ_IPI_SMP_FUNCTION].action = &nlm_common_rsvd_action;
+
+	irq_desc[XLP_IRQ_IPI_SMP_RESCHEDULE].chip = &nlm_common_rsvd_pic;
+	irq_desc[XLP_IRQ_IPI_SMP_RESCHEDULE].action = &nlm_common_rsvd_action;
+
+#ifdef CONFIG_NLMCOMMON_IP_FLOW_AFFINITY
+	/* PR: New IPI added here for netrx balancing */
+	irq_desc[XLP_IRQ_IPI_NETRX].chip = &nlm_common_rsvd_pic;
+	irq_desc[XLP_IRQ_IPI_NETRX].action = &nlm_common_rsvd_action;
+	xlp_irq_mask |= (1ULL << XLP_IRQ_IPI_NETRX);
+#endif				/* CONFIG_NLMCOMMON_IP_FLOW_AFFINITY */
+
+	xlp_irq_mask |= ((1ULL << XLP_IRQ_IPI_SMP_FUNCTION) |
+			     (1ULL << XLP_IRQ_IPI_SMP_RESCHEDULE));
+#endif
+
+	/* msgring interrupt */
+	irq_desc[XLP_IRQ_MSGRING].chip = &nlm_common_rsvd_pic;
+	irq_desc[XLP_IRQ_MSGRING].action = &nlm_common_rsvd_action;
+	xlp_irq_mask |= (1ULL << XLP_IRQ_MSGRING);
+
+#ifdef CONFIG_OPROFILE
+	xlp_irq_mask |= (1ULL << XLP_IRQ_IPI_OPROFILE);
+#endif
+
+#ifdef CONFIG_KGDB
+	xlp_irq_mask |= (1ULL << XLP_IRQ_IPI_SMP_KGDB);
+#endif
+
+	xlp_irq_mask |= (1ULL << XLP_IRQ_TIMER);
+}
+
+
+void __init arch_init_irq(void)
+{
+	extern int (*perf_irq)(void);
+
+	perf_irq = xlp_perf_irq;
+
+#ifdef CONFIG_KGDB
+	if (kgdb_early_setup)
+		return;
+#endif
+
+	/* Initialize the irq descriptors */
+	init_nlm_common_irqs();
+
+	write_64bit_cp0_eimr(xlp_irq_mask);
+
+}
diff --git a/arch/mips/netlogic/xlp/on_chip.c b/arch/mips/netlogic/xlp/on_chip.c
index 1591743..ba9d53b 100644
--- a/arch/mips/netlogic/xlp/on_chip.c
+++ b/arch/mips/netlogic/xlp/on_chip.c
@@ -39,10 +39,11 @@ THE POSSIBILITY OF SUCH DAMAGE.
 #include <asm/netlogic/xlp.h>
 
 #include <asm/netlogic/hal/nlm_hal_fmn.h>
-#include <asm/netlogic/hal/nlm_hal_pic.h>
+#include <asm/netlogic/xlp_hal_pic.h>
 
 #define MAX_VC	4096
 
+extern int xlp_rvec_from_irt(int);
 unsigned long netlogic_io_base = (unsigned long)(DEFAULT_NETLOGIC_IO_BASE);
 EXPORT_SYMBOL(netlogic_io_base);
 
@@ -210,7 +211,6 @@ struct msgstn_handler msg_handler_map[XLP_MSG_HANDLE_MAX] = {
  ********************************************************************/
 void nlm_xlp_msgring_int_handler(unsigned int irq, struct pt_regs *regs)
 {
-	unsigned long mflags;
 	int vc = 0;
 	uint32_t size = 0, code = 0, src_id = 0, cycles = 0;
 	struct msgstn_handler *handler = 0;
@@ -223,7 +223,7 @@ void nlm_xlp_msgring_int_handler(unsigned int irq, struct pt_regs *regs)
 
 	msg0 = msg1 = msg2 = msg3 = 0;
 
-        if (irq == IRQ_MSGRING) {
+        if (irq == XLP_IRQ_MSGRING) {
                 /* normal message ring interrupt */
                 /* xlr_inc_counter(MSGRNG_INT);  */
                 nlm_cpu_stat_update_msgring_int();
@@ -289,7 +289,7 @@ static void msg_timer_handler(unsigned long data)
 	struct timer_list *timer = &per_cpu(msg_int_bkup_timer, cpu);
 
 
-	nlm_xlp_msgring_int_handler(IRQ_MSGRING, NULL);
+	nlm_xlp_msgring_int_handler(XLP_IRQ_MSGRING, NULL);
 
 	timer->expires = jiffies + (HZ/100);
 	add_timer(timer);
@@ -399,11 +399,9 @@ void nlm_nmi_cpus(unsigned int mask)
  ********************************************************************/
 void enable_msgconfig_int(void)
 {
-	uint32_t flags;
-
 	/* Need write interrupt vector to cp2 msgconfig register */
 	msgrng_access_enable(flags);
-	nlm_hal_set_fmn_interrupt(IRQ_MSGRING);
+	nlm_hal_set_fmn_interrupt(XLP_IRQ_MSGRING);
 	msgrng_access_disable(flags);
 }
 
@@ -412,6 +410,7 @@ void enable_msgconfig_int(void)
  *  pic_init
  *  
  ********************************************************************/
+extern int xlp_rvec_from_irq(int);
 static void pic_init(void)
 {
 	int i = 0;
@@ -419,19 +418,13 @@ static void pic_init(void)
 	uint32_t thread_mask;
 
 	vcpu = hard_smp_processor_id() & 0x1F;
-
 	thread_mask = (1 << vcpu);
-
-	for (i = 0; i < PIC_NUM_IRTS; i++) {
-
+	for (i = XLP_IRQ_RESERVED_MAX; i < XLP_IRT_NUM; i++) {
 		level = PIC_IRQ_IS_EDGE_TRIGGERED(i);
-
 		/* Use local scheduling and high polarity for all IRTs
-		 * Invalidate all IRTs, by default
-		 */
-		nlm_hal_pic_write_irt(i, 0, 0, 1, nlm_hal_irt_to_irq(i), 1, 0, thread_mask);
+		 * Invalidate all IRTs, by default */
+		nlm_hal_pic_write_irt(xlp_irq_to_irt(i), 0, 0, 1, xlp_rvec_from_irq(i), 1, 0, thread_mask);
 	}
-
 	/* On XLP, MSGRING config register is per hw-thread */
 	enable_msgconfig_int();
 }
diff --git a/arch/mips/netlogic/xlp/platform.c b/arch/mips/netlogic/xlp/platform.c
index c3db12e..2a87bb5 100644
--- a/arch/mips/netlogic/xlp/platform.c
+++ b/arch/mips/netlogic/xlp/platform.c
@@ -36,7 +36,7 @@ THE POSSIBILITY OF SUCH DAMAGE.
 
 #include <asm/time.h>
 #include <asm/netlogic/hal/nlm_hal_macros.h>
-#include <asm/netlogic/hal/nlm_hal_pic.h>
+#include <asm/netlogic/xlp_hal_pic.h>
 #include <asm/netlogic/xlp.h>
 #include <asm/netlogic/xlp_usb.h>
 
@@ -95,7 +95,7 @@ static void xlp_init_uart(int port_id)
         xlp_uart_port[port_id].mapbase       = DEFAULT_NETLOGIC_IO_BASE 
 						+ NETLOGIC_IO_UART_0_OFFSET + port_id * XLP_UART_PORTIO_OFFSET;
         xlp_uart_port[port_id].membase       = (void __iomem *)xlp_uart_port[port_id].mapbase;
-        xlp_uart_port[port_id].irq           = PIC_UART_0_IRQ + port_id;
+        xlp_uart_port[port_id].irq           = XLP_UART_IRQ(port_id);
 
         xlp_uart_port[port_id].uartclk       = UART_CLK;
         xlp_uart_port[port_id].iotype        = UPIO_NLM;
@@ -109,27 +109,21 @@ static void xlp_init_uart(int port_id)
 static void xlp_usb_hw_start(int ctrl_no)
 {
 	int val;
-	
-	/* Disable 64bit enable
- 	 */
-	val = usb_reg_read( 0, ctrl_no, XLP_USB_CTL0);
-	val &= ~USBEHCI64BITEN;
-	usb_reg_write(0, ctrl_no, XLP_USB_CTL0, val);
-
-	/* Reset USB phy 
-	 */
+	/* Reset USB phy */
 	val = usb_reg_read( 0, ctrl_no, XLP_USB_PHY0);
 
 	if(ctrl_no == 0)
 		val &= ~(USBPHYRESET | USBPHYPORTRESET0 | USBPHYPORTRESET1);
 	else if(ctrl_no == 3)
-		val &= ~(USBPHYPORTRESET0 | USBPHYPORTRESET1);
+		val &= ~(USBPHYRESET | USBPHYPORTRESET0 | USBPHYPORTRESET1);
 	usb_reg_write(0, ctrl_no, XLP_USB_PHY0, val);
-	
+
+	udelay(2000);
 	/* Bring usb controller out of reset
  	 */
 	val = usb_reg_read( 0, ctrl_no, XLP_USB_CTL0);
 	val &= ~(USBCONTROLLERRESET );
+	val |= 0x4;
 	usb_reg_write(0, ctrl_no, XLP_USB_CTL0, val);
 
 	return;
@@ -217,36 +211,30 @@ static int xlp_find_pci_dev(void)
 							xlp_init_uart(dev2drv_table[idx].id);
 						}
 
-						dev2drv_table[idx].id = dev2drv_table[idx].id + 1;	
+						dev2drv_table[idx].id = dev2drv_table[idx].id + 1;
 
 						pres[0].start	= mmio;
-						pres[0].end		= mmio;
+						pres[0].end	= mmio;
 						pres[0].flags	= IORESOURCE_MEM;
-
 						irt = (nlm_hal_read_32bit_reg(mmio, DEV_IRT_INFO) & 0xFFFF);
-					   	if(nlm_hal_is_shared_irt(irt))
-							irq = nlm_hal_request_shared_irq(irt);
-						else
-							irq = nlm_hal_irt_to_irq(irt);
+						printk(KERN_WARNING "XXXX IRT = %#x mmio = %#llx\n", irt, mmio);
+						irq = irt;	/* TODO */
 
-						pres[1].start	= irq;
-						pres[1].end		= irq;
-						pres[1].flags	= IORESOURCE_IRQ;
+						pres[1].start = irq;
+						pres[1].end = irq;
+						pres[1].flags = IORESOURCE_IRQ;
 
 						platform_device_add_resources(pplatdev, pres, 2);
-
 						pplatdev->dev.dma_mask	= &xlp_dev_dmamask;
-
 						pplatdev->dev.coherent_dma_mask = DMA_BIT_MASK(32);
-
-						platform_device_add( pplatdev);
+						platform_device_add(pplatdev);
 					}
 				}
 			}
 		}
 	}
 	kfree(pres);
-	return 0;	
+	return 0;
 }
 static int __init platform_devinit(void)
 {
diff --git a/arch/mips/netlogic/xlp/setup.c b/arch/mips/netlogic/xlp/setup.c
index 1aaabf4..22a6700 100644
--- a/arch/mips/netlogic/xlp/setup.c
+++ b/arch/mips/netlogic/xlp/setup.c
@@ -60,7 +60,7 @@
 #include <asm/netlogic/bootinfo.h>
 #include <asm/netlogic/cpumask.h>
 #include <asm/netlogic/hal/nlm_hal_macros.h>
-#include <asm/netlogic/hal/nlm_hal_pic.h>
+#include <asm/netlogic/xlp_hal_pic.h>
 #include <asm/netlogic/phnx_loader.h>
 #include "../boot/ops.h"
 
@@ -307,7 +307,7 @@ void prom_reconfigure_thr_resources(void)
 
 unsigned int __cpuinit get_c0_compare_int(void)
 {
-    return IRQ_TIMER;
+    return XLP_IRQ_TIMER;
 }
 
 unsigned long long storm_cpu_freq(void)  //TODO After fixing nlm_hal_cpu_freq(), remove this function
@@ -402,7 +402,7 @@ static void nlm_early_serial_setup(int uart_id)
 	switch(uart_id){
 		default:
 		case 0:
-			s.irq =  PIC_UART_0_IRQ;
+			s.irq = XLP_UART_IRQ(0);
 			s.membase = (unsigned char __iomem *)
 			(DEFAULT_NETLOGIC_IO_BASE + NETLOGIC_IO_UART_0_OFFSET);
 			s.mapbase = (DEFAULT_NETLOGIC_IO_BASE +
@@ -410,7 +410,7 @@ static void nlm_early_serial_setup(int uart_id)
 			s.line = 0;
 			break;
 		case 1:
-			s.irq =  PIC_UART_1_IRQ;
+			s.irq =  XLP_UART_IRQ(1);
 			s.membase = (unsigned char __iomem *)
 			(DEFAULT_NETLOGIC_IO_BASE + NETLOGIC_IO_UART_1_OFFSET);
 			s.mapbase = (DEFAULT_NETLOGIC_IO_BASE +
@@ -830,7 +830,10 @@ void prom_free_prom_memory(void)
 	/* nothing to free */
 }
 
-#define KSEG0 0xffffffff80000000
+#ifndef KSEG0
+#define KSEG0 0xffffffff80000000ULL
+#endif
+
 #define RING_BUFFER_BASE (511 << 20)
 #define RING_BUFFER_SIZE (8 << 10)
 static void outbyte_ring_buffer(char c)
diff --git a/arch/mips/netlogic/xlp/smp.c b/arch/mips/netlogic/xlp/smp.c
index d9d0083b..e6f802b 100644
--- a/arch/mips/netlogic/xlp/smp.c
+++ b/arch/mips/netlogic/xlp/smp.c
@@ -38,7 +38,7 @@ THE POSSIBILITY OF SUCH DAMAGE.
 #include <asm/netlogic/msgring.h>
 #include <asm/netlogic/mips-exts.h>
 #include <asm/netlogic/interrupt.h>
-#include <asm/netlogic/hal/nlm_hal_pic.h>
+#include <asm/netlogic/xlp_hal_pic.h>
 
 #include <asm/asm.h>
 #include <asm/mipsregs.h>
@@ -73,15 +73,15 @@ void nlm_send_ipi_single(int logical_cpu, unsigned int action)
 	cpu = cpu % 32;
 
         if (action & SMP_CALL_FUNCTION) {
-                ipi |= IRQ_IPI_SMP_FUNCTION;
+                ipi |= XLP_IRQ_IPI_SMP_FUNCTION;
 	} else if (action & SMP_RESCHEDULE_YOURSELF) {
-                ipi |= IRQ_IPI_SMP_RESCHEDULE;
+                ipi |= XLP_IRQ_IPI_SMP_RESCHEDULE;
 	} else if (action & SMP_CALL_KGDB_HOOK) {
-                ipi |= IRQ_IPI_SMP_KGDB;
+                ipi |= XLP_IRQ_IPI_SMP_KGDB;
 		/* for KGDB enable NMI also */
 		nmi = 1;
 	} else if (action & SMP_OPROFILE_IPI) {
-                ipi |= IRQ_IPI_OPROFILE;
+                ipi |= XLP_IRQ_IPI_OPROFILE;
         } else
 		return;
 
diff --git a/arch/mips/netlogic/xlp/time.c b/arch/mips/netlogic/xlp/time.c
index 038aa19..2a0bb6d 100644
--- a/arch/mips/netlogic/xlp/time.c
+++ b/arch/mips/netlogic/xlp/time.c
@@ -33,17 +33,12 @@ THE POSSIBILITY OF SUCH DAMAGE.
 #include <asm/time.h>
 #include <asm/cpu.h>
 #include <asm/cpu-features.h>
-#include <asm/perfctr.h>
 #include <linux/oprofile.h>
 
 #include <linux/proc_fs.h>
-#include <asm/netlogic/hal/nlm_hal_pic.h>
+#include <asm/netlogic/xlp_hal_pic.h>
 
-extern spinlock_t nlm_common_pic_lock;
-
-#if defined(CONFIG_PERFCTR) && defined(CONFIG_OPROFILE)
-#error "Cannot enable both VPERF and OProfile at the same time"
-#endif
+extern spinlock_t xlp_pic_lock;
 
 #ifndef CONFIG_NLMCOMMON_MAC
 void nlm_common_user_mac_update_time(void)
@@ -81,7 +76,7 @@ void nlm_common_timer_interrupt(struct pt_regs *regs, int irq)
 	nlm_hal_write_pic_reg(mmio, PIC_WD_HEARTBEAT_0(0), 1 << cpu_logical_map(cpu));
 #endif
 
-#if defined (CONFIG_OPROFILE) || defined (CONFIG_PERFCTR_INTERRUPT_SUPPORT)
+#if defined (CONFIG_OPROFILE)
 	int    perfctr_overflow = 0;
 #endif
 
@@ -90,12 +85,12 @@ void nlm_common_timer_interrupt(struct pt_regs *regs, int irq)
 	netlogic_write_reg(mmio, 0x0c, 1 << cpu_logical_map(cpu));
 #endif
 
-	if (irq != IRQ_TIMER) {
+	if (irq != XLP_IRQ_TIMER) {
 		printk("[%s]:cpu_%d: bad timer irq = %x\n", __FUNCTION__, cpu, irq);
 		BUG();
 	}
 
-#if defined (CONFIG_PERFCTR_INTERRUPT_SUPPORT) || defined (CONFIG_OPROFILE)
+#if defined (CONFIG_OPROFILE)
     perfctr_overflow = ((read_c0_cause() >> 26) & 0x1);
 
     if(perfctr_overflow == 0)
@@ -109,18 +104,11 @@ void nlm_common_timer_interrupt(struct pt_regs *regs, int irq)
         }
     }
 
-#if defined (CONFIG_PERFCTR_INTERRUPT_SUPPORT) || defined (CONFIG_OPROFILE)
-	if (perfctr_overflow) {
-#ifdef CONFIG_PERFCTR_INTERRUPT_SUPPORT
-		(*perfctr_ihandler) (instruction_pointer(regs));
-#endif
-    }
-#ifdef CONFIG_OPROFILE
+#if defined (CONFIG_OPROFILE)
 	if (perfctr_overflow) {
 			nlm_common_oprofile_int_handler (irq, NULL, regs);
 		}
 #endif
-#endif
 
 }
 
@@ -147,7 +135,7 @@ void nlm_common_timer_setup(void)
         pic_reg_t *mmio = nlm_hal_pic_offset();
         unsigned long flags = 0;
 
-        spin_lock_irqsave(&nlm_common_pic_lock, flags);
+        spin_lock_irqsave(&xlp_pic_lock, flags);
 
         /* Use PIC Timer 6 as a free running counter */
         nlm_hal_write_pic_reg(mmio, PIC_TIMER_6_MAXVAL, 0xffffffffffffffffULL);
@@ -155,7 +143,7 @@ void nlm_common_timer_setup(void)
 	/* enable the timer */
         nlm_hal_pic_update_control(1 << (10 + 6));
 
-        spin_unlock_irqrestore(&nlm_common_pic_lock, flags);
+        spin_unlock_irqrestore(&xlp_pic_lock, flags);
 
 }
 
diff --git a/arch/mips/netlogic/xlp/xlp_gpio.c b/arch/mips/netlogic/xlp/xlp_gpio.c
index 4ff1f8b..e37b66b 100644
--- a/arch/mips/netlogic/xlp/xlp_gpio.c
+++ b/arch/mips/netlogic/xlp/xlp_gpio.c
@@ -31,7 +31,7 @@
 #include <linux/platform_device.h>
 #include <asm/netlogic/xlp.h>
 #include <asm/netlogic/hal/nlm_hal.h>
-#include <asm/netlogic/hal/nlm_hal_pic.h>
+#include <asm/netlogic/xlp_hal_pic.h>
 #include <asm/netlogic/gpio.h>
 
 #define XLP_GPIO_MAX		41
diff --git a/arch/mips/netlogic/xlp/xlp_hal_pic.c b/arch/mips/netlogic/xlp/xlp_hal_pic.c
new file mode 100644
index 0000000..7c52470
--- /dev/null
+++ b/arch/mips/netlogic/xlp/xlp_hal_pic.c
@@ -0,0 +1,79 @@
+#include <asm/netlogic/xlp_hal_pic.h>
+
+/*
+ * __nlm_hal_request_irq
+ * This function will return the irt index for any given irq.
+ * Note :
+ *	We don't care if the irt is enabled or not, if there are
+ *	multiple assignments of the same irq or not.
+ *	must be called with irt_irq lock taken
+ *
+ * @irq : irq whose irt entry should be returned
+ *
+ * return : irt entry index
+ */
+int __nlm_hal_request_irq(int irt, int rvec)
+{
+        uint64_t  val;
+        val = nlm_hal_read_pic_reg(nlm_hal_pic_offset(), PIC_IRT(irt));
+        /* clear DB and DTE field */
+	val &= ~(0x3f << 20);
+	val |= ((rvec << 20) | (1 << 31));
+	nlm_hal_write_pic_reg(nlm_hal_pic_offset(), PIC_IRT(irt), val);
+        return 0;
+}
+
+/*
+ * __nlm_hal_release_irq
+ * Note :
+ *	must be called with irt_irq lock taken
+ *
+ * @irq : irq whose irt entry should be returned
+ *
+ * return : irt entry index
+ */
+void __nlm_hal_release_irq(int irt)
+{
+        uint64_t  val;
+
+        val = nlm_hal_read_pic_reg(nlm_hal_pic_offset(), PIC_IRT(irt));
+	val &= ~(1 << 31);
+	nlm_hal_write_pic_reg(nlm_hal_pic_offset(), PIC_IRT(irt), val);
+}
+
+/*
+ * __nlm_hal_set_irt_to_cpu
+ *
+ * Sets DT and DB in an IRT entry
+ */
+void __nlm_hal_set_irt_to_cpu(int irt, int cpu)
+{
+        uint64_t val;
+	uint node;
+	uint cpuid, threadid;
+	uint nodeid = 0;	/* TBD */
+	/* DT is set 1 ==> Destination thread is specificed in DB and
+	 * DTE fields.
+	 * DB : (18-17 : Node id)
+	 * DB : (16) : 1 ==> DTE selects cpu 0-15
+	 * DB : (16) : 0 ===> DTE selects cpu 16-31
+	 *
+	 * cpuid and thread id are found out from cpu# param as follows
+	 * threadid = (cpu & 0xf)
+	 * cpuid = (cpu >> 4)
+	 */
+	cpuid = (cpu >> 4) & 1;	/* DB group of CPU */
+	threadid = cpu & 0xf;	/* range 0 - 15 for threadid */
+        val = nlm_hal_read_pic_reg(nlm_hal_pic_offset(), PIC_IRT(irt));
+        val &= ~(0xfffff);	/* Clear DT, DB and DTE */
+        val |= ((1 << 19) | (nodeid << 17) | (cpuid << 16) | ( 1 << threadid));
+        nlm_hal_write_pic_reg(nlm_hal_pic_offset(), PIC_IRT(irt), val);
+}
+
+#ifdef NLM_HAL_LINUX_KERNEL
+#include <linux/kernel.h>
+#include <linux/module.h>
+EXPORT_SYMBOL(__nlm_hal_set_irt_to_cpu);
+EXPORT_SYMBOL(__nlm_hal_request_irq);
+EXPORT_SYMBOL(__nlm_hal_release_irq);
+#endif
diff --git a/arch/mips/pci/pci-xlp.c b/arch/mips/pci/pci-xlp.c
index e87374d..60bf390 100644
--- a/arch/mips/pci/pci-xlp.c
+++ b/arch/mips/pci/pci-xlp.c
@@ -32,19 +32,25 @@ THE POSSIBILITY OF SUCH DAMAGE.
 
 #include <asm/io.h>
 
-#include <asm/netlogic/interrupt.h>
+#include <asm/netlogic/xlp_irq.h>
 #include <asm/netlogic/pci.h>
 #include <asm/netlogic/io.h>
 #include <asm/netlogic/iomap.h>
 #include <asm/netlogic/sim.h>
 #define NLM_HAL_LINUX_KERNEL
-#include <asm/netlogic/hal/nlm_hal_pic.h>
+#include <asm/netlogic/xlp_hal_pic.h>
 
 extern int pci_probe_only;
-
 static void *pci_config_base;
 static void *pci_io_base;
 
+void xlp_intx_enable(int fn);
+void xlp_intx_disable(int fn);
+int xlp_msi_enable(struct pci_dev *dev, u32 bitmap);
+int xlp_msix_enable(struct pci_dev *dev);
+void xlp_msi_disable(int fn, u32 bitmap);
+void xlp_msix_disable(int fn);
+
 #define SWAP32(x)				\
         (((x) & 0xff000000) >> 24) |		\
         (((x) & 0x000000ff) << 24) |		\
@@ -52,15 +58,15 @@ static void *pci_io_base;
         (((x) & 0x00ff0000) >> 8)
 
 /*
- * Possible values are 44, 43, 42 or 41 for interrupts
+ * Possible values are no more hard coded.
  * For mapping of these values to IRT, refer
- * arch/mips/netlogic/common/nlm_hal.c
+ * arch/mips/netlogic/xlp/irq.c
  *
  * We have some unique problems here.
  * 1. Board could be configured in different lane widths. That means, the cards
- * could be controlled by different functions
+ * could be controlled by different functions of the controller on board
  * Eg. 2x8 config can have two cards (fn 0 and fn 2)
- *	4x4 config can also have two cards (under fn0 - fn 3)
+ *	4x4 config can also have two cards (under fn0 through fn 3)
  * So, it is important to figure out the lanes on which cards are placed.
  * First we read the lane config from POWER_ON_RESET_CFG
  * Then each line's LTSSM state would give the card presence
@@ -69,19 +75,24 @@ static void *pci_io_base;
  * XLP irq map is as follows
  *  \fn 0	1	2	3
  *plc\
- * 0	44	0	42	41
- * 1	44	43	42	0
- * 2	44	0	42	41
- * 3	44	43	42	41
+ * 0	86	0	88	89
+ * 1	86	87	88	0
+ * 2	86	0	88	89
+ * 3	86	87	88	89
+ * This map changes from processor to processor. check PRM or RTL
  */
 static int xlp_irq_map[4][4][3] = {
-	{{44, 0, 0}, {0, 0, 0}, {42, 0, 0}, {0, 0, 0}},
-	{{44, 0, 0}, {43, 0, 0}, {42, 0, 0}, {0, 0, 0}},
-	{{44, 0, 0}, {0, 0, 0}, {42, 0, 0}, {41, 0, 0}},
-	{{44, 0, 0}, {43, 0, 0}, {42, 0, 0}, {41, 0, 0}},
+	{{XLP_PCIE_LINK_IRQ(0), 0, 0}, {0, 0, 0},
+		{XLP_PCIE_LINK_IRQ(2), 0, 0}, {0, 0, 0}},
+	{{XLP_PCIE_LINK_IRQ(0), 0, 0}, {XLP_PCIE_LINK_IRQ(1), 0, 0},
+		{XLP_PCIE_LINK_IRQ(2), 0, 0}, {0, 0, 0}},
+	{{XLP_PCIE_LINK_IRQ(0), 0, 0}, {0, 0, 0},
+		{XLP_PCIE_LINK_IRQ(2), 0, 0}, {XLP_PCIE_LINK_IRQ(3), 0, 0}},
+	{{XLP_PCIE_LINK_IRQ(0), 0, 0}, {XLP_PCIE_LINK_IRQ(1), 0, 0},
+		{XLP_PCIE_LINK_IRQ(2), 0, 0}, {XLP_PCIE_LINK_IRQ(3), 0, 0}},
 };
 
-static void xlp_map_helper(int row, int fn)
+static int xlp_map_helper(int row, int fn)
 {
 	u64 xlp_pci_base;
 	u32 reg6, ltssm;
@@ -91,28 +102,137 @@ static void xlp_map_helper(int row, int fn)
 	if (ltssm != 0x00446000) {
 		printk(KERN_WARNING "LTSSM state is %#x. Fn %x link not up\n",
 				ltssm, fn);
-		return;
+		return -ENODEV;
 	}
 	reg6 = nlm_hal_read_32bit_reg(xlp_pci_base, 0x6);
 	xlp_irq_map[row][fn][1] = (reg6 >> 8) & 0xff;
 	xlp_irq_map[row][fn][2] = (reg6 >> 16) & 0xff;
-	printk(KERN_WARNING "Set sec = %x, sub = %x for fn = %#x\n",
-		xlp_irq_map[row][fn][1], xlp_irq_map[row][fn][2],fn);
-	return;
+	return 0;
 }
 
-static void pcie_controller_init_done(void)
+int xlp_ctrl_fn_from_dev(const struct pci_dev *dev)
+{
+	__label__ out;
+	int row = 0, fn = 0;
+
+	while (row < 4) {
+		fn = 0;
+		while (fn < 4) {
+			if ((dev->bus->number >= xlp_irq_map[row][fn][1]) &&
+				(dev->bus->number <= xlp_irq_map[row][fn][2])) {
+				goto out; /* No `break', note two loops */;
+			}
+			fn++;
+		}
+		row++;
+	}
+out:
+	if (fn >= 4) {
+		return -ENODEV;
+	}
+	return fn;
+}
+
+#ifndef CONFIG_XLP_MSI_ADDRESSES
+static u64 XLP_MSI_ADDR = 0;
+static u64 xlp_msix_addr_array[XLP_MAX_SLOTS];
+static u64 xlp_msi_addr_array[XLP_MAX_SLOTS];
+#else
+static u64 xlp_msi_addr_array[] = {
+	XLP_MSI_ADDR + (0 * XLP_MSI_ADDR_SIZE),
+	XLP_MSI_ADDR + (1 * XLP_MSI_ADDR_SIZE),
+	XLP_MSI_ADDR + (2 * XLP_MSI_ADDR_SIZE),
+	XLP_MSI_ADDR + (3 * XLP_MSI_ADDR_SIZE),
+};
+
+static u64 xlp_msix_addr_array[] = {
+	XLP_MSI_ADDR + (0 * XLP_MSIX_ADDR_SIZE),
+	XLP_MSI_ADDR + (1 * XLP_MSIX_ADDR_SIZE),
+	XLP_MSI_ADDR + (2 * XLP_MSIX_ADDR_SIZE),
+	XLP_MSI_ADDR + (3 * XLP_MSIX_ADDR_SIZE),
+};
+#endif
+
+u64 xlp_msix_addr_start(int fn)
+{
+#ifdef CONFIG_XLP_MSI_ADDRESSES
+	return xlp_msix_addr_array[fn];
+#else
+	if (XLP_MSI_ADDR == 0) {
+		return 0;
+	}
+	return (XLP_MSI_ADDR + (fn * XLP_MSIX_ADDR_SIZE));
+#endif
+}
+
+u64 xlp_msi_addr_start(int fn)
+{
+#ifdef CONFIG_XLP_MSI_ADDRESSES
+	return xlp_msi_addr_array[fn];
+#else
+	if (XLP_MSI_ADDR == 0) {
+		return 0;
+	}
+	return (XLP_MSI_ADDR + (fn * XLP_MSI_ADDR_SIZE));
+#endif
+}
+
+static void xlp_msi_controller_init(int fn)
+{
+	u64 xlp_pci_base;
+	u8 mmc;
+	u32 pci, msi;
+
+	xlp_pci_base = 0x18000000 + (0 << 20) + (1 << 15) + ( fn << 12);
+	if (XLP_MSI_ADDR == 0) {
+		printk(KERN_ERR "MSI/MSI-X CANNOT be programmed\n");
+		return;
+	}
+	msi = nlm_hal_read_32bit_reg(xlp_pci_base, 0x14);
+	mmc = (msi >> 17) & 0x7;
+	/* Initialize MSI Base register */
+	nlm_hal_write_32bit_reg(xlp_pci_base, 0x15, virt_to_phys(xlp_msi_addr_start(fn)) & 0xffffffff);
+	nlm_hal_write_32bit_reg(xlp_pci_base, 0x16, (virt_to_phys(xlp_msi_addr_start(fn)) >> 32) & 0xffffffff);
+	nlm_hal_write_32bit_reg(xlp_pci_base, 0x17, 0x0);
+	msi |= ((mmc << 10) | (1 << 16));
+	nlm_hal_write_32bit_reg(xlp_pci_base, 0x14, msi);
+	/* Initialize MSI-X Base and Address reg*/
+	nlm_hal_write_32bit_reg(xlp_pci_base, 0x24F, (virt_to_phys(xlp_msix_addr_start(fn)) >> 8));
+	nlm_hal_write_32bit_reg(xlp_pci_base, 0x250, (virt_to_phys((xlp_msix_addr_start(fn) + XLP_MSIX_ADDR_SIZE)) >> 8));
+}
+
+void xlp_pcie_controller_setup(int fn)
+{
+	xlp_msi_controller_init(fn);
+	xlp_msix_disable(fn);
+	xlp_msi_disable(fn, 0xf);
+	/* By default, leave INTX enabled */
+	xlp_intx_enable(fn);
+}
+
+u32 xlp_get_power_on_reset_cfg(void)
 {
 	u64 xlp_syscfg_base = 0x18000000 + (0 << 20) + (6 << 15) + ( 5 << 12);
+	return nlm_hal_read_32bit_reg(xlp_syscfg_base, 0x41);
+}
+
+static void pcie_controller_init_done(void)
+{
 	u32 plc, syscfg, mode, count = 0;
 
+#ifndef CONFIG_XLP_MSI_ADDRESSES
+	XLP_MSI_ADDR = (u64)__get_free_pages(GFP_KERNEL, get_order(0x1000000));
+	if (XLP_MSI_ADDR == 0) {
+		printk(KERN_ERR "Failed to get memory for MSI/MSI-X tables\n");
+	}
+#endif
 	if (!pci_probe_only){
 		printk(KERN_WARNING "PCIe bus IRQs configured incorrectly\n");
 		return;
 	}
+	syscfg = xlp_get_power_on_reset_cfg();
 	/* We don't manipulate pci_address space.
 	 * Get the link status from pcie lane config from 34.9.7.2 XLP PRM */
-	syscfg = nlm_hal_read_32bit_reg(xlp_syscfg_base, 0x41);
 	mode = (syscfg >> 19) & 0xf;
 	while (count < 4) {
 		printk(KERN_WARNING "Controller %d is in %s mode\n",
@@ -125,39 +245,75 @@ static void pcie_controller_init_done(void)
 		/* In each case find subordinate and primary numbers */
 	case 0:
 		/* controller 0 and 2 are active */
-		if (mode & 0x1)
-			xlp_map_helper(plc, 0);
-		if (mode & 0x4)
-			xlp_map_helper(plc, 2);
+		if (mode & 0x1){
+			if (xlp_map_helper(plc, 0) == 0) {
+				xlp_pcie_controller_setup(0);
+			}
+		}
+		if (mode & 0x4) {
+			if (xlp_map_helper(plc, 2) == 0) {
+				xlp_pcie_controller_setup(2);
+			}
+		}
 		break;
 	case 1:
-		if (mode & 0x1)
-			xlp_map_helper(plc, 0);
-		if (mode & 0x2)
-			xlp_map_helper(plc, 1);
-		if (mode & 0x4)
-			xlp_map_helper(plc, 2);
+		if (mode & 0x1){
+			if (xlp_map_helper(plc, 0) == 0) {
+				xlp_pcie_controller_setup(0);
+			}
+		}
+		if (mode & 0x2){
+			if (xlp_map_helper(plc, 1) == 0) {
+				xlp_pcie_controller_setup(1);
+			}
+		}
+		if (mode & 0x4){
+			if (xlp_map_helper(plc, 2) == 0) {
+				xlp_pcie_controller_setup(2);
+			}
+		}
 		break;
 	case 2:
-		if (mode & 0x1)
-			xlp_map_helper(plc, 0);
-		if (mode & 0x4)
-			xlp_map_helper(plc, 2);
-		if (mode & 0x8)
-			xlp_map_helper(plc, 3);
+		if (mode & 0x1){
+			if (xlp_map_helper(plc, 0) == 0) {
+				xlp_pcie_controller_setup(0);
+			}
+		}
+		if (mode & 0x4){
+			if (xlp_map_helper(plc, 2) == 0) {
+				xlp_pcie_controller_setup(2);
+			}
+		}
+		if (mode & 0x8){
+			if (xlp_map_helper(plc, 3) == 0) {
+				xlp_pcie_controller_setup(3);
+			}
+		}
 		break;
 	case 3:
-		if (mode & 0x1)
-			xlp_map_helper(plc, 0);
-		if (mode & 0x2)
-			xlp_map_helper(plc, 1);
-		if (mode & 0x4)
-			xlp_map_helper(plc, 2);
-		if (mode & 0x8)
-			xlp_map_helper(plc, 3);
+		if (mode & 0x1){
+			if (xlp_map_helper(plc, 0) == 0) {
+				xlp_pcie_controller_setup(0);
+			}
+		}
+		if (mode & 0x2){
+			if (xlp_map_helper(plc, 1) == 0) {
+				xlp_pcie_controller_setup(1);
+			}
+		}
+		if (mode & 0x4){
+			if (xlp_map_helper(plc, 2) == 0) {
+				xlp_pcie_controller_setup(2);
+			}
+		}
+		if (mode & 0x8){
+			if (xlp_map_helper(plc, 3) == 0) {
+				xlp_pcie_controller_setup(3);
+			}
+		}
 		break;
 	}
-	printk("[%s]: PCIE Controller initialization to be done\n", __FUNCTION__);
+	printk("[%s]: PCIE Controller initialization done\n", __FUNCTION__);
 	return;
 }
 
@@ -267,25 +423,243 @@ struct pci_controller xlp_controller = {
 	.mem_offset     = 0x00000000UL
 };
 
+/*
+ * Apparently this function is called for all pci controller functions
+ * viz. 0:1.0, 0:1.1, 0:1.2 and 0:1.3
+ * In fact, we need not assign them any interrupt.
+ */
 int __init pcibios_map_irq(const struct pci_dev *dev, u8 slot, u8 pin)
 {
-	int ret = 0;
 	int row = 0, fn = 0;
 
+	switch (dev->devfn) {
+	case XLP_PCIE_CTRL_DEVFN(0, 0) ... XLP_PCIE_CTRL_DEVFN(0, 3):
+		return 0;
+	default:
+		break;
+	}
+	row = (xlp_get_power_on_reset_cfg() >> 23) & 0x3;
+	fn = xlp_ctrl_fn_from_dev(dev);
+	dev_printk(KERN_WARNING, &dev->dev, "Assigning interrupt %#x\n", xlp_irq_map[row][fn][0]);
+	return xlp_irq_map[row][fn][0];
+}
+
+void xlp_intx_enable(int fn)
+{
+	u64 xlp_pci_base;
+	u32 pci;
+
+	xlp_pci_base = 0x18000000 + (0 << 20) + (1 << 15) + ( fn << 12);
+	pci = nlm_hal_read_32bit_reg(xlp_pci_base, 0x1);
+	pci &= ~(1 << 10);	/* Enable IntX assertion */
+	nlm_hal_write_32bit_reg(xlp_pci_base, 0x1, pci);
+	pci = nlm_hal_read_32bit_reg(xlp_pci_base, 0x261);
+	pci |= 0xf;	/* Enable INT A,B,C,D */
+	nlm_hal_write_32bit_reg(xlp_pci_base, 0x261, pci);
+	return;
+}
+
+void xlp_intx_disable(int fn)
+{
+	u64 xlp_pci_base;
+	u32 pci;
+
+	xlp_pci_base = 0x18000000 + (0 << 20) + (1 << 15) + ( fn << 12);
+	pci = nlm_hal_read_32bit_reg(xlp_pci_base, 0x1);
+	pci |= (1 << 10);
+	nlm_hal_write_32bit_reg(xlp_pci_base, 0x1, pci);
+	pci = nlm_hal_read_32bit_reg(xlp_pci_base, 0x261);
+	pci &= ~(0xf);
+	nlm_hal_write_32bit_reg(xlp_pci_base, 0x261, pci);
+	return;
+}
+
+/*
+ * Finds the slot on which this device is placed and enables corresponding
+ * MSI enable register on the controller
+ * @dev : pci device corresponding to this device
+ */
+int xlp_msi_enable(struct pci_dev *dev, u32 bitmap)
+{
+	int fn = 0;
+	u64 xlp_pci_base;
+	u32 msi_en;
+
+	fn = xlp_ctrl_fn_from_dev(dev);
+	xlp_pci_base = 0x18000000 + (0 << 20) + (1 << 15) + ( fn << 12);
+	/* First, set PCIe Int Enable register. __KEEP_THIS_ORDER__ */
+	msi_en = nlm_hal_read_32bit_reg(xlp_pci_base, 0x261);
+	if (!(msi_en & (1 << 9))) {
+		xlp_intx_disable(fn);
+		xlp_msix_disable(fn);
+		msi_en &= ~(0xf);
+		msi_en |= (1 << 9);	/* controls ONLY MSI, Not MSI-X */
+		/* Then, individually enable MSI bits */
+		nlm_hal_write_32bit_reg(xlp_pci_base, 0x261, msi_en);
+		msi_en = nlm_hal_read_32bit_reg(xlp_pci_base, 0x25B);
+		msi_en |= bitmap;
+		nlm_hal_write_32bit_reg(xlp_pci_base, 0x25B, msi_en);
+	}
+	return 0;
+}
+
+int xlp_msix_enable(struct pci_dev *dev)
+{
+	int fn = 0;
+	u64 xlp_pci_base;
+	u32 msix_ctrl;
+
+	fn = xlp_ctrl_fn_from_dev(dev);
+	xlp_pci_base = 0x18000000 + (0 << 20) + (1 << 15) + ( fn << 12);
+	msix_ctrl = nlm_hal_read_32bit_reg(xlp_pci_base, 0x2C);
+	if (!(msix_ctrl & 0x80000000)) {
+		/* disable MSI and intx first */
+		xlp_intx_disable(fn);
+		xlp_msi_disable(fn, 0xf);
+		msix_ctrl |= 0x80000000;	/* MSI-X enable */
+		nlm_hal_write_32bit_reg(xlp_pci_base, 0x2C, msix_ctrl);
+	}
+	//nlm_hal_write_32bit_reg(xlp_pci_base, 0xf, 0xFF);
+	return 0;
+}
+
+/*
+ * Disables MSI on controller function
+ */
+void xlp_msi_disable(int fn, u32 bitmap)
+{
+	u64 xlp_pci_base;
+	u32 msi_en;
+
+	xlp_pci_base = 0x18000000 + (0 << 20) + (1 << 15) + ( fn << 12);
+	/*set PCIe Int Enable register */
+	msi_en = nlm_hal_read_32bit_reg(xlp_pci_base, 0x261);
+	if (msi_en & (1 << 9)) {
+		msi_en &= ~(1 << 9);
+		msi_en |= (bitmap & 0xf);
+		nlm_hal_write_32bit_reg(xlp_pci_base, 0x261, msi_en);
+	}
+}
+
+void xlp_msix_disable(int fn)
+{
+	u64 xlp_pci_base;
+	u32 msix_ctrl;
+
+	xlp_pci_base = 0x18000000 + (0 << 20) + (1 << 15) + ( fn << 12);
+	msix_ctrl = nlm_hal_read_32bit_reg(xlp_pci_base, 0x2C);
+	msix_ctrl &= ~(0x80000000);	/* MSI-X disable */
+	nlm_hal_write_32bit_reg(xlp_pci_base, 0x2C, msix_ctrl);
+	//nlm_hal_write_32bit_reg(xlp_pci_base, 0xf, 0xFF);	/* TODO Get from dev */
+	return;
+}
+
+/*
+ * checks if msi is enabled for this controller
+ * @fn	: controller function number
+ */
+int is_msi_set(int fn)
+{
+	u64 xlp_pci_base;
+	u32 msi_en, status;
+
+	xlp_pci_base = 0x18000000 + (0 << 20) + (1 << 15) + ( fn << 12);
+	msi_en = nlm_hal_read_32bit_reg(xlp_pci_base, 0x261);
+	status = (msi_en >> 9) & 1 ;
+	return status;
+}
+
+
+u32 calc_msi_vector_offset(int fn)
+{
+	u64 xlp_pci_base;
+	u32 msi_en, msi_stat;
+
+	xlp_pci_base = 0x18000000 + (0 << 20) + (1 << 15) + ( fn << 12);
+	msi_en = nlm_hal_read_32bit_reg(xlp_pci_base, 0x25B);
+	msi_stat = nlm_hal_read_32bit_reg(xlp_pci_base, 0x25A);
+	nlm_hal_write_32bit_reg(xlp_pci_base, 0x25A, msi_stat);
+	msi_stat &= msi_en;
+	return msi_stat;
+}
+
+u32 xlp_msix_status_clear(int fn)
+{
+	u64 xlp_pci_base;
+	u32 msix_stat;
+	u32 mask = ((XLP_MSIX_PER_SLOT - 1) << (fn * XLP_MSIX_PER_SLOT));
+
+	xlp_pci_base = 0x18000000 + (0 << 20) + (1 << 15) + ( fn << 12);
+	msix_stat = nlm_hal_read_32bit_reg(xlp_pci_base, 0x25D);
+	//fdebug("mask = %#x, fn = %d, MSIX status = %#x\n", mask, fn, msix_stat);
+	msix_stat &= mask;
+	//fdebug("Masked MSIX status = %#x\n", msix_stat);
+	nlm_hal_write_32bit_reg(xlp_pci_base, 0x25D, msix_stat);
+	//fdebug("Stat cleared %#x\n", nlm_hal_read_32bit_reg(xlp_pci_base, 0x25D));
+	return (msix_stat >> (fn * XLP_MSIX_PER_SLOT));
+}
+
+#if 0
+/* required only if xlp_ctrl_fn_from_dev() is static */
+int xlp_msi_base_vector(struct pci_dev *dev)
+{
+	return(XLP_MSI_IRQ_START(xlp_ctrl_fn_from_dev(dev)));
+}
+
+
+int xlp_msix_base_vector(struct pci_dev *dev)
+{
+	return(XLP_MSIX_IRQ_START(xlp_ctrl_fn_from_dev(dev)));
+}
+
+#endif
+
+u32 xlp_msi_set_mask(int fn, int bit, int val)
+{
+	u64 xlp_pci_base;
+	u32 msi_en;
+
+	xlp_pci_base = 0x18000000 + (0 << 20) + (1 << 15) + ( fn << 12);
+	msi_en = nlm_hal_read_32bit_reg(xlp_pci_base, 0x25B);
+	if (val == 0) {	/* Make the bit 0 */
+		msi_en &= ~( 1 << bit);
+	} else {	/* Make the bit 1 */
+		msi_en |= ( 1 << bit);
+	}
+	nlm_hal_write_32bit_reg(xlp_pci_base, 0x25B, msi_en);
+	return msi_en;
+}
+
+/*
+ * Finds the slot on which this device is placed and clears the MSI status
+ * register on the controller
+ * @dev : pci device corresponding to this device
+ */
+int xlp_msi_status_clear(struct pci_dev *dev, int bit)
+{
+	__label__ out;
+	int row = 0, fn = 0;
+	u64 xlp_pci_base;
+	u32 msi_en;
+
 	while (row < 4) {
 		while (fn < 4) {
 			if ((dev->bus->number >= xlp_irq_map[row][fn][1]) &&
 				(dev->bus->number <= xlp_irq_map[row][fn][2])) {
-				ret = xlp_irq_map[row][fn][0];
-				goto out;
+				goto out; /* No `break', note two loops */
 			}
 			fn++;
 		}
 		row++;
 	}
 out:
-	dev_printk(KERN_WARNING, &dev->dev, "Assigning interrupt %#x\n", ret);
-	return ret;
+	if (fn >= 4) {
+		return -ENODEV;
+	}
+	xlp_pci_base = 0x18000000 + (0 << 20) + (1 << 15) + ( fn << 12);
+	msi_en = 1 << bit;
+	nlm_hal_write_32bit_reg(xlp_pci_base, 0x25B, msi_en);
+	return 0;
 }
 
 /* Do platform specific device initialization at pci_enable_device() time */
@@ -306,11 +680,140 @@ static int __init xlp_nopci_setup(char *str)
 }
 __setup("xlp_nopci", xlp_nopci_setup);
 
+/*
+ * Initializes PIC ITE entries PRM 9.5.6.26
+ * XLP restricts CPU affinity to 8 groups. They are,
+ * 0 =>	Only cpu0/thread0; mask = 1
+ * 1 => All CPUs/threads and nodes; mask = (~0 & online_cpu_mask) on all nodes
+ * 2 => cpu0-1 on all nodes. mask = 0x000000ff& online_cpu_mask  on all nodes
+ * 3 => cpu2-3 on all nodes; mask = 0x0000ff00 & online_cpu_mask on all nodes
+ * 4 => cpu4-5 on all nodes; mask = 0x00ff0000 & online_cpu_mask on all nodes
+ * 5 => cpu6-7 on all nodes; mask = 0xff000000 & online_cpu_mask on all nodes
+ * 6 => cpu0-15 on all nodes; mask = 0x0000ffff & online_cpu_mask on all nodes
+ * 7 => cpu15-31 on all nodes; mask = 0xffff0000 & online_cpu_mask on all nodes
+ */
+static struct cpumask xlp_ite_cpumask[XLP_ITE_ENTRIES];
+void xlp_pic_ite_init(void)
+{
+	int i;
+	struct cpumask m;
+	u64 xlp_pic_base = 0x18000000 + (0 << 20) + (0 << 15) + ( 4 << 12);
+	char buf[140];
+	u64 bitmask = 0;
+
+	printk(KERN_WARNING "Setting ITE entries only for 0-31 (Node 0) CPUs!\n");
+	cpumask_clear(&m);
+	/* We manipulate only NODE0 ITE entries here */
+	for (i = 0; i < XLP_ITE_ENTRIES; i++) {
+		cpumask_clear(&xlp_ite_cpumask[i]);
+	}
+	cpumask_set_cpu(cpumask_first(cpu_online_mask), &xlp_ite_cpumask[0]);
+
+	/* Set 0-31 cpus, if present in cpu_online mask */
+	for (i = cpumask_first(cpu_online_mask); i < 32; ) {
+		bitmask |= (1ULL << i);
+		i = cpumask_next(i, cpu_online_mask);
+	}
+	cpumask_scnprintf(buf, 140, cpu_online_mask); fdebug("cpu_online_mask -> %s\n", buf);
+	cpumask_copy(&xlp_ite_cpumask[1], cpu_online_mask);
+
+	/* Set 0-7 cpus */
+	for (i = 0; i < 8; i++) {
+		cpumask_set_cpu(i, &m);
+	}
+	/* logical and with cpuonline mask to get the actual mask */
+	cpumask_and(&xlp_ite_cpumask[2], &m, cpu_online_mask);
+	cpumask_shift_left(&xlp_ite_cpumask[3], &m, 8);
+	cpumask_and(&xlp_ite_cpumask[3], &xlp_ite_cpumask[3], cpu_online_mask);
+	cpumask_shift_left(&xlp_ite_cpumask[4], &m, 16);
+	cpumask_and(&xlp_ite_cpumask[4], &xlp_ite_cpumask[4], cpu_online_mask);
+	cpumask_shift_left(&xlp_ite_cpumask[5], &m, 24);
+	cpumask_and(&xlp_ite_cpumask[5], &xlp_ite_cpumask[5], cpu_online_mask);
+
+	cpumask_shift_left(&xlp_ite_cpumask[6], &m, 8);
+	cpumask_or(&xlp_ite_cpumask[6], &xlp_ite_cpumask[6], &m);
+	cpumask_shift_left(&xlp_ite_cpumask[7], &xlp_ite_cpumask[6], 16);
+	cpumask_and(&xlp_ite_cpumask[6], &xlp_ite_cpumask[6], cpu_online_mask);
+	cpumask_and(&xlp_ite_cpumask[7], &xlp_ite_cpumask[7], cpu_online_mask);
+
+
+	for (i = 0; i < XLP_ITE_ENTRIES; i++) {
+		cpumask_scnprintf(buf, 140, &xlp_ite_cpumask[i]);
+		printk(KERN_DEBUG "Supported CPUMASK (%d) -> %s\n", i, buf);
+	}
+
+	/* Right shift by 1 is required by HAL, _DO_NOT_REMOVE_ */
+	nlm_hal_write_64bit_reg(xlp_pic_base, 0x94 >> 1, (0x00000001 & bitmask));
+	nlm_hal_write_64bit_reg(xlp_pic_base, 0x98 >> 1, (0xffffffff & bitmask));
+	nlm_hal_write_64bit_reg(xlp_pic_base, 0x9C >> 1, (0x000000ff & bitmask));
+	nlm_hal_write_64bit_reg(xlp_pic_base, 0xA0 >> 1, (0x0000ff00 & bitmask));
+	nlm_hal_write_64bit_reg(xlp_pic_base, 0xA4 >> 1, (0x00ff0000 & bitmask));
+	nlm_hal_write_64bit_reg(xlp_pic_base, 0xA8 >> 1, (0xff000000 & bitmask));
+	nlm_hal_write_64bit_reg(xlp_pic_base, 0xAC >> 1, (0x0000ffff & bitmask));
+	nlm_hal_write_64bit_reg(xlp_pic_base, 0xB0 >> 1, (0xffff0000 & bitmask));
+	/* We don't populate redirection to other nodes now */
+
+}
+
+/*
+ * This function returns closest match cpumask among the supported bitmasks
+ * in XLP
+ * Logic is moot, need to improve it later.
+ * XXX
+ *
+ * @m	: user supplied cpumask
+ */
+const struct cpumask *xlp_closest_match_cpumask(struct cpumask *m)
+{
+	int i;
+
+	for (i = 0; i < XLP_ITE_ENTRIES; i++) {
+		if (cpumask_equal(m, &xlp_ite_cpumask[i])) {
+			return &xlp_ite_cpumask[i];
+		}
+	}
+	return NULL;
+}
+
+/*
+ * This function sets the cpumask for an interrupt vector
+ *
+ * m	: CPU mask resulting from xlp_closest_match_cpumask() call
+ */
+void xlp_set_cpumask(const struct cpumask *m, int irt)
+{
+	int i, pic_idx = XLP_PIC_IRTREG_START;
+	u64 xlp_pic_base = 0x18000000 + (0 << 20) + (0 << 15) + ( 4 << 12);
+	u64 val;
+	u32 offset = (XLP_PIC_IRTREG_START + (irt << 1) >> 1);	// Hal requires this nasty right shi(f)t
+
+	/* We set the following in IRT entry
+	 * 28 : clear to indicate global delivery
+	 * 19 : clear to indicate DB selects ITE
+	 * 16-18 : set to indicate ITE
+	 * 0-15 : Clear
+	 */
+	for (i = 0; i < XLP_ITE_ENTRIES; i++) {
+		if (m != &xlp_ite_cpumask[i]) {
+			continue;
+		}
+		val = nlm_hal_read_64bit_reg(xlp_pic_base, offset);
+		val &= ~((1 << 28) | (1 << 19) | (0x7 << 16) | 0xffff);
+		val |= (i << 16);
+		fdebug("Writing val = %#llx\n", val);
+		nlm_hal_write_64bit_reg(xlp_pic_base, offset, val);
+		return;
+	}
+	printk(KERN_WARNING "Failed to program IRT entry %d\n", irt);
+	return;
+}
+
 static int __init pcibios_init(void)
 {
 	unsigned long phys = 0;
 	unsigned long size = 0;
 
+	xlp_pic_ite_init();// Initialize even if xlp is passed nopci option
 	if (xlp_nopci) return 0;
 
 	/* Bootloader assigns PCI resources */
diff --git a/drivers/usb/host/ehci-pci.c b/drivers/usb/host/ehci-pci.c
index 25e8b30..69fdb45 100644
--- a/drivers/usb/host/ehci-pci.c
+++ b/drivers/usb/host/ehci-pci.c
@@ -45,10 +45,10 @@ static const char hcd_name[] = "ehci-pci";
 #if defined(CONFIG_NLM_XLP) && defined(CONFIG_USB)
 
 #include <asm/netlogic/hal/nlm_hal.h>
-#include <asm/netlogic/hal/nlm_hal_pic.h>
+#include <asm/netlogic/xlp_hal_pic.h>
 #include <asm/netlogic/xlp.h>
 #include <asm/netlogic/xlp_usb.h>
-
+extern int nlm_xlp_request_irq(int irq);
 volatile uint64_t *ehci_regs;
 
 static void xlp_usb_hw_start(int ctrl_no)
@@ -70,7 +70,7 @@ int xlp_ehci_hcd_pci_probe(struct pci_dev *dev, const struct pci_device_id *id)
 	ctrl_no = dev->devfn & 0xF;
 
 	irt = usb_reg_read(0, ctrl_no, 0x3D) & 0xFFFF;
-	irq = nlm_hal_request_shared_irq(irt);
+	irq = nlm_xlp_request_irq(irt);
 
 	if (!irq) {
 		printk("Found HC with no IRQ.  Check BIOS/PCI %s setup!\n", 
-- 
1.8.4.93.g57e4c17

