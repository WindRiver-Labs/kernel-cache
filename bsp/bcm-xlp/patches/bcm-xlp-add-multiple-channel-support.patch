From 0f5695145896c21b7417ab4d59638430ab695bd7 Mon Sep 17 00:00:00 2001
From: Sreenidhi BR <sreenidhibr@netlogicmicro.com>
Date: Thu, 18 Aug 2011 18:31:35 +0530
Subject: [PATCH 240/565] bcm-xlp: add multiple channel support

Add multiple channel support to DTRE/RAID driver, so that it
provides one-to-one mapping with hardware channels.

Based on Broadcom SDK 2.3.

Signed-off-by: Sreenidhi BR <sreenidhibr@netlogicmicro.com>
Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 drivers/dma/nlm_adma.c | 134 +++++++++++++++++++++++++++----------------------
 drivers/dma/nlm_adma.h |   3 +-
 2 files changed, 76 insertions(+), 61 deletions(-)

diff --git a/drivers/dma/nlm_adma.c b/drivers/dma/nlm_adma.c
index 98d942e..c7ef891 100644
--- a/drivers/dma/nlm_adma.c
+++ b/drivers/dma/nlm_adma.c
@@ -354,6 +354,7 @@ static void dtre_send_message(int vc_id, int msgtype, uint64_t * msg)
 	rc = 0;
 	i = 0;
 
+
 	while(1)
 	{
 		rc = xlp_message_send(vc_id, msgtype, 0, msg);
@@ -415,6 +416,9 @@ static dma_cookie_t nlm_tx_submit (struct dma_async_tx_descriptor *tx)
 	nlm_tx = tx_to_nlm_adma_tx(tx);
 	chan = (struct nlm_adma_chan *)(nlm_tx->chan);
 
+	/* update dest vc_id from channel number */
+	vc_id = DTRE_MIN_VC + chan->chan_num;
+
 	spin_lock_bh(&chan->lock);
 	cookie = nlm_desc_assign_cookie(chan, nlm_tx);
 
@@ -529,7 +533,6 @@ static dma_cookie_t nlm_tx_submit (struct dma_async_tx_descriptor *tx)
 			}
 
 			/* call the send msg function */
-			vc_id = DTRE_MIN_VC + 1;
 			msgtype = p2p;
 			dtre_send_message(vc_id, msgtype, raid_list_msg);
 
@@ -549,7 +552,6 @@ static dma_cookie_t nlm_tx_submit (struct dma_async_tx_descriptor *tx)
 		transfer_msg[2] = gen_dtr_xfer_msg_format_2(nlm_tx->hw_desc.dst);
 		transfer_msg[3] = gen_dtr_xfer_msg_format_3((void *)((unsigned long)nlm_tx>>1));
 
-		vc_id = DTRE_MIN_VC + 1;
 		msgtype = p2d;
 		dtre_send_message(vc_id, msgtype, transfer_msg);
 
@@ -588,7 +590,6 @@ static dma_cookie_t nlm_tx_submit (struct dma_async_tx_descriptor *tx)
 		disks = (nlm_tx->hw_desc.src_cnt) + 2;
 	}
 
-	vc_id = DTRE_MIN_VC;
 
 	raid_list_msg [0] = gen_dtr_raid_msg_format_0((nlm_tx->entries_count), nlm_tx->hw_desc.entries);
 	raid_list_msg [1] = gen_dtr_raid_msg_format_1(raid_type, operation, disks, freeback_msg_dest_id);
@@ -1419,7 +1420,7 @@ static int alloc_initial_tx_desc_pool (struct nlm_adma_chan *nlm_chan, int count
 		ptr->chan = (void *)nlm_chan;
 	}
 
-	printk("Channel %d: Allocated %d tx descriptors\n", nlm_chan->chan, count);
+	printk("Channel %d: Allocated %d tx descriptors\n", nlm_chan->chan_num, count);
 	nlm_chan->num_tx_desc = count;
 
 	return 0;
@@ -1431,85 +1432,98 @@ static int __devinit nlm_adma_probe(struct platform_device *pdev)
 	struct nlm_adma_device *adev;
 	struct dma_device *dma_dev;
 	struct nlm_adma_chan *nlm_chan;
-	int i, err;
+	int i, err, loop;
 	uint64_t base;
 	uint32_t value;
 
-	/*
-	adev = kzalloc(sizeof(*adev), GFP_KERNEL);
-	if (!adev)
-		return -ENOMEM;
-		*/
-
-	adev = &nlm_adma_raid_device;
-
-	//dma_dev = &(adev->common);
-	dma_dev = &nlm_adma_raid_device.common;
 
 	if (register_xlp_msgring_handler(XLP_MSG_HANDLE_DTRE, nlm_dtre_msgring_handler, NULL)){
 		printk("Error: NLM-ADMA unable to register for msgring handler\n");
 		return -1;
 	}
 
-	INIT_LIST_HEAD(&dma_dev->channels);
+	for (loop = 0; loop < DTRE_NUM_CHANNELS; loop++)
+	{
 
-	/* channel init and allocation */
-	nlm_chan = kzalloc(sizeof(*nlm_chan), GFP_KERNEL);
-	if (!nlm_chan) {
-		kfree(adev);
-		return -ENOMEM;
-	}
+		adev = kzalloc(sizeof(*adev), GFP_KERNEL);
+		if (!adev)
+			return -ENOMEM;
+		dma_dev = &(adev->common);
 
-	nlm_chan->device = adev;
-	spin_lock_init(&nlm_chan->lock);
-	nlm_chan->common.device = dma_dev;
-	list_add_tail(&nlm_chan->common.device_node, &dma_dev->channels);
+		/*
+		   adev = &nlm_adma_raid_device;
+		   dma_dev = &nlm_adma_raid_device.common;
+		 */
 
-	err = alloc_initial_tx_desc_pool(nlm_chan, DTRE_MAX_TX_DESC_PER_CHAN);
+		INIT_LIST_HEAD(&dma_dev->channels);
 
-	if(err != 0) {
-		free_initial_tx_desc_pool(nlm_chan);
-		kfree(adev);
-		kfree(nlm_chan);
-		return -ENOMEM;
-	}
+		/* channel init and allocation */
+		nlm_chan = kzalloc(sizeof(*nlm_chan), GFP_KERNEL);
+		if (!nlm_chan) {
+			kfree(adev);
+			return -ENOMEM;
+		}
+
+		nlm_chan->chan_num = loop;
+		nlm_chan->device = adev;
+		spin_lock_init(&nlm_chan->lock);
+		nlm_chan->common.device = dma_dev;
+		list_add_tail(&nlm_chan->common.device_node, &dma_dev->channels);
 
-	for (i=0; i<DTRE_MAX_TX_Q_LEN; i++)
-		nlm_chan->tx_queue[i] = NULL;
+		err = alloc_initial_tx_desc_pool(nlm_chan, DTRE_MAX_TX_DESC_PER_CHAN);
+
+		if(err != 0) {
+			free_initial_tx_desc_pool(nlm_chan);
+			kfree(adev);
+			kfree(nlm_chan);
+			return -ENOMEM;
+		}
 
-	tasklet_init(&nlm_chan->irq_tasklet, nlm_adma_tasklet, 
-			(unsigned long) nlm_chan);
+		for (i=0; i<DTRE_MAX_TX_Q_LEN; i++)
+			nlm_chan->tx_queue[i] = NULL;
 
-	/* set base routines */
+		tasklet_init(&nlm_chan->irq_tasklet, nlm_adma_tasklet, 
+				(unsigned long) nlm_chan);
 
-	dma_cap_set(DMA_XOR, dma_dev->cap_mask);
-	dma_cap_set(DMA_XOR_VAL, dma_dev->cap_mask);
-	dma_cap_set(DMA_PQ, dma_dev->cap_mask);
-	dma_cap_set(DMA_PQ_VAL, dma_dev->cap_mask);
+		/* set base routines */
 
-	dma_cap_set(DMA_MEMCPY, dma_dev->cap_mask);
-	dma_cap_set(DMA_MEMSET, dma_dev->cap_mask);
-	// dma_cap_set(DMA_INTERRUPT, dma_dev->cap_mask);
+		/* set RAID capabilities for chan 0 and 
+		   MEM capabilities for chan 1-3 */
+		if (loop == 0)
+		{
+			dma_cap_set(DMA_XOR, dma_dev->cap_mask);
+			dma_cap_set(DMA_XOR_VAL, dma_dev->cap_mask);
+			dma_cap_set(DMA_PQ, dma_dev->cap_mask);
+			dma_cap_set(DMA_PQ_VAL, dma_dev->cap_mask);
+
+			dma_dev->max_xor = nlm_adma_get_max_xor();
+			dma_dev->device_prep_dma_xor = nlm_adma_prep_dma_xor;
+			dma_dev->device_prep_dma_xor_val = nlm_adma_prep_dma_xor_val;
+			dma_set_maxpq(dma_dev, nlm_adma_get_max_pq(), 0);
+			dma_dev->device_prep_dma_pq = nlm_adma_prep_dma_pq;
+			dma_dev->device_prep_dma_pq_val = nlm_adma_prep_dma_pq_val;
+		}
+		else 
+		{
+			dma_cap_set(DMA_MEMCPY, dma_dev->cap_mask);
+			dma_cap_set(DMA_MEMSET, dma_dev->cap_mask);
+			dma_dev->device_prep_dma_memcpy = nlm_adma_prep_dma_memcpy;
+			dma_dev->device_prep_dma_memset = nlm_adma_prep_dma_memset;
+		}
 
-	dma_dev->device_alloc_chan_resources = nlm_adma_alloc_chan_resources;
-	dma_dev->device_free_chan_resources = nlm_adma_free_chan_resources;
-	dma_dev->device_is_tx_complete = nlm_adma_is_complete;
-	dma_dev->device_issue_pending = nlm_adma_issue_pending;
 
-	dma_dev->device_prep_dma_memcpy = nlm_adma_prep_dma_memcpy;
-	dma_dev->device_prep_dma_memset = nlm_adma_prep_dma_memset;
-	// dma_dev->device_prep_dma_interrupt = nlm_adma_prep_dma_interrupt;
+		dma_dev->device_alloc_chan_resources = nlm_adma_alloc_chan_resources;
+		dma_dev->device_free_chan_resources = nlm_adma_free_chan_resources;
+		dma_dev->device_is_tx_complete = nlm_adma_is_complete;
+		dma_dev->device_issue_pending = nlm_adma_issue_pending;
 
-	dma_dev->max_xor = nlm_adma_get_max_xor();
-	dma_dev->device_prep_dma_xor = nlm_adma_prep_dma_xor;
-	dma_dev->device_prep_dma_xor_val = nlm_adma_prep_dma_xor_val;
+		// dma_cap_set(DMA_INTERRUPT, dma_dev->cap_mask);
+		// dma_dev->device_prep_dma_interrupt = nlm_adma_prep_dma_interrupt;
 
-	dma_set_maxpq(dma_dev, nlm_adma_get_max_pq(), 0);
-	dma_dev->device_prep_dma_pq = nlm_adma_prep_dma_pq;
-	dma_dev->device_prep_dma_pq_val = nlm_adma_prep_dma_pq_val;
+		dma_dev->dev = &pdev->dev;
+		dma_async_device_register(dma_dev);
 
-	dma_dev->dev = &pdev->dev;
-	dma_async_device_register(dma_dev);
+	} /* end of loop DTRE_NUM_CHANNELS */
 
 	nlm_dtre_null_page = alloc_page(GFP_KERNEL);
 	if (nlm_dtre_null_page == NULL) {
diff --git a/drivers/dma/nlm_adma.h b/drivers/dma/nlm_adma.h
index 1375985..67f8bdb 100644
--- a/drivers/dma/nlm_adma.h
+++ b/drivers/dma/nlm_adma.h
@@ -50,6 +50,7 @@ THE POSSIBILITY OF SUCH DAMAGE.
 #define DTRE_NUM_VC		4
 #define DTRE_MIN_VC		264
 #define DTRE_MAX_VC		267
+#define DTRE_NUM_CHANNELS	4
 
 
 
@@ -85,7 +86,7 @@ struct nlm_tx_desc {
 };
 
 struct nlm_adma_chan {
-	int chan; /* channel number */
+	int chan_num; /* channel number */
 	dma_cookie_t completed_cookie;
 	spinlock_t lock; /* protects the descriptor slot pool */
 	struct nlm_adma_device *device;
-- 
1.8.4.93.g57e4c17

