From 5516d4756ad48377eb975451b63d4d2960625e59 Mon Sep 17 00:00:00 2001
From: Sreenidhi BR <sreenidhibr@netlogicmicro.com>
Date: Thu, 16 Jun 2011 14:19:47 +0530
Subject: [PATCH 226/565] RAID: Adding memcpy

RAID: Adding memcpy and improving tx_submit logic.

Based on Broadcom SDK 2.3.

Signed-off-by: Sreenidhi BR <sreenidhibr@netlogicmicro.com>
Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 drivers/dma/nlm_adma.c | 251 +++++++++++++++++++++++--------------------------
 1 file changed, 118 insertions(+), 133 deletions(-)

diff --git a/drivers/dma/nlm_adma.c b/drivers/dma/nlm_adma.c
index 65188b6..8f45296 100644
--- a/drivers/dma/nlm_adma.c
+++ b/drivers/dma/nlm_adma.c
@@ -43,7 +43,6 @@ THE POSSIBILITY OF SUCH DAMAGE.
 #include <linux/kernel.h>
 #include <asm/netlogic/hal/nlm_hal_fmn.h>
 
-
 uint64_t nlm_dtre_debug = 0;
 
 #define shift_lower_bits(x, bitshift, numofbits) \
@@ -131,12 +130,6 @@ static void nlm_dtre_msgring_handler(uint32_t vc, uint32_t src_id,
 				vc, src_id, size, msg0, msg1, desc->optype);
 	}
 
-#if 0
-	if (desc->optype == DMA_MEMCPY)
-		printk("DTRE recv msg: vc %d sender 0x%x, size 0x%x, data0 0x%llx data1 0x%llx optype %d.\n",
-				vc, src_id, size, msg0, msg1, desc->optype);
-#endif
-
 	if ((desc->xor_check_result) != NULL)
 		*(desc->xor_check_result) = 0;
 
@@ -174,6 +167,105 @@ nlm_desc_assign_cookie(struct nlm_adma_chan *chan,
 	return cookie;
 }
 
+/* Called with chan->lock held */
+static void free_tx_desc(struct nlm_tx_desc *desc)
+{
+	struct nlm_adma_chan *chan = (struct nlm_adma_chan *)desc->chan;
+
+	/* return the descriptor back to the channel's pool */
+	desc->next = chan->desc_pool_head.next;
+	chan->desc_pool_head.next = desc;
+}
+
+
+/* Called with chan->lock held */
+static void __process_completed_tx(struct nlm_adma_chan * chan)
+{
+	struct nlm_tx_desc *ptr;
+	dma_cookie_t cookie = 0;
+	int i;
+
+	if (chan->process_idx == chan->pending_idx)
+	{
+		/* nothing to process */
+
+		for (i=0; i<DTRE_MAX_TX_Q_LEN; i++)
+		{
+			if (chan->tx_queue[i] != NULL)
+			{
+				printk("DTRE index %d found, expected to be empty\n", i);
+			}
+		}
+
+		return;
+	}
+
+	while (1)
+	{
+		ptr = chan->tx_queue[chan->process_idx];
+
+		if ((ptr->async_tx.cookie > 0) && (ptr->done_flag == 1))
+		{
+			if (nlm_dtre_debug)
+				printk("* process_idx: %d.\n", chan->process_idx);
+			cookie = ptr->async_tx.cookie;
+			ptr->async_tx.cookie = 0;
+			ptr->done_flag = 2;
+			wmb();
+
+			if (ptr->async_tx.callback)
+			{
+				ptr->async_tx.callback(ptr->async_tx.callback_param);
+			}
+
+			chan->completed_cookie = cookie;
+
+			dma_run_dependencies(&ptr->async_tx);
+		}
+
+		if (!async_tx_test_ack(&ptr->async_tx))
+		{
+			tasklet_schedule(&chan->irq_tasklet);
+			break;
+		}
+
+		if (ptr->done_flag == 2)
+		{
+			if (nlm_dtre_debug)
+				printk("pro_idx: %d.\n", chan->process_idx);
+
+			ptr->done_flag = 0;
+			wmb();
+			chan->tx_queue[chan->process_idx] = NULL;
+			chan->process_idx = (chan->process_idx + 1) & DTRE_MAX_TX_Q_MASK;
+			if (ptr->pg)
+				put_page(ptr->pg);
+
+			free_tx_desc(ptr);
+		}
+
+		if (chan->process_idx == chan->pending_idx)
+		{
+			for (i=0; i<DTRE_MAX_TX_Q_LEN; i++)
+			{
+				if (chan->tx_queue[i] != NULL)
+				{
+					printk("DTRE idx %d found, expected to be empty\n", i);
+				}
+			}
+			break;
+		}
+
+		/* exit clause */
+		if ((chan->process_idx != chan->pending_idx) &&
+				(ptr != NULL) &&
+				(ptr->done_flag == 0))
+		{
+			tasklet_schedule(&chan->irq_tasklet);
+			break;
+		}
+	}
+}
 static dma_cookie_t nlm_tx_submit (struct dma_async_tx_descriptor *tx)
 {
 	enum dma_transaction_type optype;
@@ -207,6 +299,20 @@ static dma_cookie_t nlm_tx_submit (struct dma_async_tx_descriptor *tx)
 	optype = nlm_tx->optype;
 	freeback_msg_dest_id = (netlogic_cpu_id()*16)+(netlogic_thr_id()*4);
 
+	if ((chan->tx_queue[chan->pending_idx]) != NULL)
+	{
+		/* no space in tx array, try to cleanup by processing */
+		__process_completed_tx(chan);
+	}
+
+	chan->tx_queue[chan->pending_idx] = nlm_tx;
+
+	if (nlm_dtre_debug)
+		printk("pending_idx: %d.\n", chan->pending_idx);
+
+	chan->pending_idx = (chan->pending_idx + 1) & DTRE_MAX_TX_Q_MASK;
+
+	spin_unlock_bh(&chan->lock);
 	/* Handle the DMA_MEMCPY case first and return.
 	   All RAID messages are handled later 
 	   */
@@ -215,7 +321,7 @@ static dma_cookie_t nlm_tx_submit (struct dma_async_tx_descriptor *tx)
 		transfer_msg[0] = 
 			(1ULL << 63)
 			| shift_lower_bits(0, 60, 3)
-			| shift_lower_bits(8*(nlm_tx->len), 40, 20)
+			| shift_lower_bits((nlm_tx->len), 40, 20)
 			| shift_lower_bits ((volatile void *)nlm_tx->hw_desc.src, 0, 40);
 
 		transfer_msg[1] = 
@@ -254,14 +360,13 @@ static dma_cookie_t nlm_tx_submit (struct dma_async_tx_descriptor *tx)
 
 			if ((i%100) == 0) {
 				msgstatus1 = xlp_read_status1();
-				printk("DTRE:continuing retry, rc:0x%08x, msgstatus1: 0x%08x.\n", rc, msgstatus1);
+				// printk("DTRE:continuing retry, rc:0x%08x, msgstatus1: 0x%08x.\n", rc, msgstatus1);
 			}
 		}
 
 		if (rc != 0) {
 			msgstatus1 = xlp_read_status1();
 			printk("Error: unable to send DTRE Xfer msg: rc:%d, msgstatus1: 0x%08x.\n", rc, msgstatus1);
-			spin_unlock_bh(&chan->lock);
 			return -ENODEV;
 		}
 		else {
@@ -275,9 +380,7 @@ static dma_cookie_t nlm_tx_submit (struct dma_async_tx_descriptor *tx)
 			}
 		}
 
-		/* do the post msg sending operations */
-		goto post_msg_sent;
-
+		return cookie;
 	}
 
 	if (optype == DMA_XOR)
@@ -348,7 +451,6 @@ static dma_cookie_t nlm_tx_submit (struct dma_async_tx_descriptor *tx)
 		msgstatus1 = xlp_read_status1();
 		printk("Error: unable to send DTRE RAID msg: rc:%d, msgstatus1: 0x%08x.\n", rc, msgstatus1);
 
-		spin_unlock_bh(&chan->lock);
 		return -ENODEV;
 	}
 	else
@@ -362,22 +464,6 @@ static dma_cookie_t nlm_tx_submit (struct dma_async_tx_descriptor *tx)
 		}
 	}
 
-post_msg_sent:
-	if ((chan->tx_queue[chan->pending_idx]) != NULL)
-	{
-		/* TODO: error action TBD */
-		printk("DTRE Error: tx seems to be full, overwriting.\n");
-	}
-
-	chan->tx_queue[chan->pending_idx] = nlm_tx;
-
-	if (nlm_dtre_debug)
-		printk("pending_idx: %d.\n", chan->pending_idx);
-
-	chan->pending_idx = (chan->pending_idx + 1) & DTRE_MAX_TX_Q_MASK;
-
-	spin_unlock_bh(&chan->lock);
-
 	return cookie;
 
 }
@@ -392,16 +478,6 @@ static int nlm_adma_alloc_chan_resources(struct dma_chan *chan)
 }
 
 
-/* Called with chan->lock held */
-static void free_tx_desc(struct nlm_tx_desc *desc)
-{
-	struct nlm_adma_chan *chan = (struct nlm_adma_chan *)desc->chan;
-
-	/* return the descriptor back to the channel's pool */
-	desc->next = chan->desc_pool_head.next;
-	chan->desc_pool_head.next = desc;
-}
-
 
 static struct nlm_tx_desc *alloc_tx_desc(struct nlm_adma_chan *chan)
 {
@@ -433,95 +509,6 @@ static struct nlm_tx_desc *alloc_tx_desc(struct nlm_adma_chan *chan)
 }
 
 
-/* Called with chan->lock held */
-static void __process_completed_tx(struct nlm_adma_chan * chan)
-{
-	struct nlm_tx_desc *ptr;
-	dma_cookie_t cookie = 0;
-	int i;
-
-	if (chan->process_idx == chan->pending_idx)
-	{
-		/* nothing to process */
-
-		for (i=0; i<DTRE_MAX_TX_Q_LEN; i++)
-		{
-			if (chan->tx_queue[i] != NULL)
-			{
-				printk("DTRE index %d found, expected to be empty\n", i);
-			}
-		}
-
-		return;
-	}
-
-	while (1)
-	{
-		ptr = chan->tx_queue[chan->process_idx];
-
-		if ((ptr->async_tx.cookie > 0) && (ptr->done_flag == 1))
-		{
-			if (nlm_dtre_debug)
-				printk("* process_idx: %d.\n", chan->process_idx);
-			cookie = ptr->async_tx.cookie;
-			ptr->async_tx.cookie = 0;
-			ptr->done_flag = 2;
-			wmb();
-
-			if (ptr->async_tx.callback)
-			{
-				ptr->async_tx.callback(ptr->async_tx.callback_param);
-			}
-
-			chan->completed_cookie = cookie;
-
-			dma_run_dependencies(&ptr->async_tx);
-		}
-
-		if (!async_tx_test_ack(&ptr->async_tx))
-		{
-			tasklet_schedule(&chan->irq_tasklet);
-			break;
-		}
-
-		if (ptr->done_flag == 2)
-		{
-			if (nlm_dtre_debug)
-				printk("pro_idx: %d.\n", chan->process_idx);
-
-			ptr->done_flag = 0;
-			wmb();
-			chan->tx_queue[chan->process_idx] = NULL;
-			chan->process_idx = (chan->process_idx + 1) & DTRE_MAX_TX_Q_MASK;
-			if (ptr->pg)
-				put_page(ptr->pg);
-
-			free_tx_desc(ptr);
-		}
-
-		if (chan->process_idx == chan->pending_idx)
-		{
-			for (i=0; i<DTRE_MAX_TX_Q_LEN; i++)
-			{
-				if (chan->tx_queue[i] != NULL)
-				{
-					printk("DTRE idx %d found, expected to be empty\n", i);
-				}
-			}
-			break;
-		}
-
-		/* exit clause */
-		if ((chan->process_idx != chan->pending_idx) &&
-				(ptr != NULL) &&
-				(ptr->done_flag == 0))
-		{
-			tasklet_schedule(&chan->irq_tasklet);
-			break;
-		}
-	}
-}
-
 static void process_completed_tx(struct nlm_adma_chan * nlm_chan)
 {
 	spin_lock_bh(&nlm_chan->lock);
@@ -600,7 +587,6 @@ static void nlm_adma_issue_pending(struct dma_chan *chan)
 }
 
 
-#if 0
 static struct dma_async_tx_descriptor *
 nlm_adma_prep_dma_memcpy(struct dma_chan *chan, dma_addr_t dma_dest,
 		dma_addr_t dma_src, size_t len, unsigned long flags)
@@ -641,7 +627,6 @@ nlm_adma_prep_dma_memcpy(struct dma_chan *chan, dma_addr_t dma_dest,
 
 	return &desc->async_tx;
 }
-#endif
 
 
 static struct dma_async_tx_descriptor *
@@ -1328,7 +1313,7 @@ static int __devinit nlm_adma_probe(struct platform_device *pdev)
 	dma_cap_set(DMA_PQ, dma_dev->cap_mask);
 	dma_cap_set(DMA_PQ_VAL, dma_dev->cap_mask);
 
-	// dma_cap_set(DMA_MEMCPY, dma_dev->cap_mask);
+	dma_cap_set(DMA_MEMCPY, dma_dev->cap_mask);
 	// dma_cap_set(DMA_MEMSET, dma_dev->cap_mask);
 	// dma_cap_set(DMA_INTERRUPT, dma_dev->cap_mask);
 
@@ -1337,7 +1322,7 @@ static int __devinit nlm_adma_probe(struct platform_device *pdev)
 	dma_dev->device_is_tx_complete = nlm_adma_is_complete;
 	dma_dev->device_issue_pending = nlm_adma_issue_pending;
 
-	// dma_dev->device_prep_dma_memcpy = nlm_adma_prep_dma_memcpy;
+	dma_dev->device_prep_dma_memcpy = nlm_adma_prep_dma_memcpy;
 	// dma_dev->device_prep_dma_memset = nlm_adma_prep_dma_memset;
 	// dma_dev->device_prep_dma_interrupt = nlm_adma_prep_dma_interrupt;
 
-- 
1.8.4.93.g57e4c17

