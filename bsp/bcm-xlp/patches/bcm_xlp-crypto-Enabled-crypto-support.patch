From 1a051dc36f8c6de3e965620d31f498532fcfe070 Mon Sep 17 00:00:00 2001
From: Yanjiang Jin <yanjiang.jin@windriver.com>
Date: Tue, 7 May 2013 14:25:42 +0800
Subject: [PATCH 524/565] bcm_xlp: crypto: Enabled crypto support

crypto: Enabled crypto support for linux 3.0

Based on Broadcom SDK 2.3.

Signed-off-by: reshmic <reshmic@broadcom.com>
Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 drivers/crypto/Kconfig             |   11 +
 drivers/crypto/Makefile            |    1 +
 drivers/crypto/sae/Makefile        |   15 +
 drivers/crypto/sae/nlm_aead.c      | 2120 ++++++++++++++++++++++++++++++++++++
 drivers/crypto/sae/nlm_async.h     |   92 ++
 drivers/crypto/sae/nlm_auth.c      |  671 ++++++++++++
 drivers/crypto/sae/nlm_crypto.c    |  766 +++++++++++++
 drivers/crypto/sae/nlm_enc.c       |  525 +++++++++
 drivers/crypto/sae/nlmcrypto_ifc.h |   68 ++
 9 files changed, 4269 insertions(+)
 create mode 100755 drivers/crypto/sae/Makefile
 create mode 100755 drivers/crypto/sae/nlm_aead.c
 create mode 100644 drivers/crypto/sae/nlm_async.h
 create mode 100644 drivers/crypto/sae/nlm_auth.c
 create mode 100644 drivers/crypto/sae/nlm_crypto.c
 create mode 100755 drivers/crypto/sae/nlm_enc.c
 create mode 100644 drivers/crypto/sae/nlmcrypto_ifc.h

diff --git a/drivers/crypto/Kconfig b/drivers/crypto/Kconfig
index dffb855..80c25b7 100644
--- a/drivers/crypto/Kconfig
+++ b/drivers/crypto/Kconfig
@@ -386,4 +386,15 @@ config CRYPTO_DEV_ATMEL_SHA
 	  To compile this driver as a module, choose M here: the module
 	  will be called atmel-sha.
 
+config XLP_SAE
+        tristate "Broadcom xlp SAE driver"
+        default m
+        help
+          This driver supports broadcom's Security Acceleration Engine.
+          For more information on xlp SAE driver, please visit
+
+          <http://support.broadcom.com>
+
+	  To compile this driver as a module, choose M here. The module will be called sae.
+
 endif # CRYPTO_HW
diff --git a/drivers/crypto/Makefile b/drivers/crypto/Makefile
index 38ce13d..289e411 100644
--- a/drivers/crypto/Makefile
+++ b/drivers/crypto/Makefile
@@ -21,3 +21,4 @@ obj-$(CONFIG_CRYPTO_DEV_NX) += nx/
 obj-$(CONFIG_CRYPTO_DEV_ATMEL_AES) += atmel-aes.o
 obj-$(CONFIG_CRYPTO_DEV_ATMEL_TDES) += atmel-tdes.o
 obj-$(CONFIG_CRYPTO_DEV_ATMEL_SHA) += atmel-sha.o
+obj-$(CONFIG_XLP_SAE)	+= sae/
diff --git a/drivers/crypto/sae/Makefile b/drivers/crypto/sae/Makefile
new file mode 100755
index 0000000..52e01f7
--- /dev/null
+++ b/drivers/crypto/sae/Makefile
@@ -0,0 +1,15 @@
+
+################################################################################
+
+#
+# Makefile for xlp_sae security driver
+#
+
+#EXTRA_CFLAGS := -Werror
+EXTRA_CFLAGS := -DNLM_HAL_LINUX_KERNEL -Iarch/mips/include/asm/netlogic/hal
+EXTRA_CFLAGS += -Iarch/mips/netlogic/boot
+
+KBUILD_EXTRA_SYMBOLS := $(HAL_DIR)/mod/Module.symvers
+
+obj-$(CONFIG_XLP_SAE)	+= sae.o
+sae-objs 	:= nlm_enc.o nlm_crypto.o nlm_auth.o nlm_aead.o 
diff --git a/drivers/crypto/sae/nlm_aead.c b/drivers/crypto/sae/nlm_aead.c
new file mode 100755
index 0000000..ec3ccd03
--- /dev/null
+++ b/drivers/crypto/sae/nlm_aead.c
@@ -0,0 +1,2120 @@
+/***********************************************************************
+Copyright 2003-2010 Netlogic Microsystems (“Netlogic”). All rights
+reserved.
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are
+met:
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+*****************************#NETL_2#********************************/
+#include <linux/rtnetlink.h>
+#include <crypto/algapi.h>
+#include <crypto/aes.h>
+#include <crypto/des.h>
+#include <crypto/ctr.h>
+#include <crypto/sha.h>
+#include <crypto/aead.h>
+#include <crypto/authenc.h>
+
+#include <asm/netlogic/hal/nlm_hal_fmn.h>
+#include <asm/netlogic/msgring.h>
+#include "nlm_async.h"
+
+#undef NLM_CRYPTO_DEBUG
+#define Message(a, b...) //printk("[%s @ %d] "a"\n",__FUNCTION__,__LINE__, ##b)
+
+#define XLP_CRYPT_PRIORITY      310
+
+#define XCBC_DIGEST_SIZE        16
+#define MD5_DIGEST_SIZE         16
+#define MD5_BLOCK_SIZE          64
+
+#define GCM_RFC4106_IV_SIZE 8
+#define GCM_RFC4106_NONCE_SIZE 4
+#define GCM_RFC4106_DIGEST_SIZE 16
+
+#define CCM_RFC4309_NONCE_SIZE 3
+#define CCM_RFC4309_IV_SIZE 8
+#define CCM_RFC4309_DIGEST_SIZE 16
+
+/*
+ 						CTRL DESC MEMORY LAYOUT
+	 ------------------------------------------------------------------------------------
+	|  64 bytes	 | struct nlm_aead_ctx	  | 64bytes for   | struct nlm_aead_ctx     |
+	|  for alignment | 			  | for alignment | (used only for 3des)    |
+	 ------------------------------------------------------------------------------------
+*/
+
+struct nlm_aead_ctx
+{
+	struct nlm_crypto_pkt_ctrl ctrl;
+	uint8_t iv_buf[16];
+	uint32_t iv_len;
+	int cbc;
+	uint16_t stat;
+	struct crypto_aead  * fallback;
+};
+
+#define MAX_FRAGS		18	
+#define CTRL_DESC_SIZE		(sizeof(struct nlm_aead_ctx) + 128)
+#define DES3_CTRL_DESC_SIZE	(2*CTRL_DESC_SIZE + 2*64)	//Allocate 2 separate control desc for encryption and decryption
+#define CACHE_ALIGN		64
+#define IV_AEAD_PADDING         128
+#define TAG_LEN			64
+
+/*
+ 						PACKET DESC MEMORY LAYOUT
+	 ------------------------------------------------------------------------------------------------------
+	|  64 bytes	 | struct nlm_crypto_pkt_param +  | 64bytes for   | struct nlm_async_crypto | 64 bytes |
+	|  for alignment | 18 * (2*64)			  | for alignment |			    | for hash |
+	 ------------------------------------------------------------------------------------------------------
+ */
+#define PACKET_DESC_SIZE	(CACHE_ALIGN + sizeof(struct nlm_crypto_pkt_param) + (MAX_FRAGS*(2*8)) + 64 + sizeof(struct nlm_async_crypto) + 64 + IV_AEAD_PADDING + TAG_LEN)
+#define NLM_CRYPTO_PKT_PARAM_OFFSET(addr)	(((unsigned long)addr + CACHE_ALIGN ) & ~0x3fUL)
+#define NLM_ASYNC_PTR_PARAM_OFFSET(addr)	((unsigned long)addr + CACHE_ALIGN + sizeof(struct nlm_crypto_pkt_param) + (MAX_FRAGS*(2*8)) + 64 + ~0x3fUL) 
+#define NLM_HASH_OFFSET(addr)			((unsigned long)addr + (PACKET_DESC_SIZE - TAG_LEN))
+#define NLM_IV_OFFSET(addr)			((unsigned long)addr + (PACKET_DESC_SIZE - TAG_LEN - IV_AEAD_PADDING ))
+
+
+#define XLP_CRYPT_PRIORITY	310
+
+#define NETL_OP_ENCRYPT 1
+#define NETL_OP_DECRYPT 0
+
+#define PKT_DESC_OFF 64
+
+#ifdef NLM_CRYPTO_DEBUG
+extern void print_crypto_msg_desc(uint64_t entry1, uint64_t entry2, uint64_t entry3);
+extern void print_cntl_instr(uint64_t cntl_desc);
+extern void print_pkt_desc(struct nlm_crypto_pkt_param  *pkt_desc, int index);
+#endif
+extern struct nlm_crypto_stat crypto_stat[MAX_CPU];
+
+
+/*
+   All extern declaration goes here.
+ */
+extern uint32_t nlm_hal_send_msg3(uint32_t dst, uint32_t code, uint64_t data0, uint64_t data1, uint64_t data2);
+extern uint32_t nlm_hal_recv_msg2(uint32_t dst, uint32_t *src, uint32_t *size, uint32_t *code, uint64_t *data0, uint64_t *data1);
+extern int auth_mode_key_len[NLM_HASH_MAX][NLM_HASH_MODE_MAX];
+extern int cipher_mode_iv_len[NLM_CIPHER_MAX][NLM_CIPHER_MODE_MAX];
+extern int alloc_pkt_param( struct nlm_async_crypto * async,struct nlm_crypto_pkt_param ** pkt_param, int max_frags );
+
+
+
+static int no_of_alg_registered = 0;
+
+#ifdef NLM_CRYPTO_DEBUG
+static void print_buf(unsigned char *msg, unsigned char *buf, int len)
+{
+#define TMP_BUF		50
+	char tmp_buf[TMP_BUF + 1];
+	int i, index = 0;
+
+	printk("**********%s************\n",msg);
+	for(i=0; i<len; i++){
+		sprintf(&tmp_buf[index*2], "%02x", buf[i]);
+		index++;
+		if(index == (TMP_BUF/2)){
+			tmp_buf[index*2] = '\0';
+			printk("[%s]\n",tmp_buf);
+			index = 0;
+		}
+	}
+	if(index){
+		tmp_buf[index*2] = '\0';
+		printk("[%s]\n",tmp_buf);
+	}
+}
+#endif
+static struct nlm_aead_ctx *nlm_crypto_aead_ctx(struct crypto_aead *tfm)
+{
+
+	return (struct nlm_aead_ctx *)(((unsigned long)((uint8_t *)crypto_aead_ctx(tfm) + 64)) & ~(0x3fUL));
+}
+
+static struct nlm_aead_ctx *nlm_crypto_tfm_ctx(struct crypto_tfm *tfm)
+{
+	return (struct nlm_aead_ctx *)(((unsigned long)((uint8_t *)crypto_tfm_ctx(tfm) + 64)) & ~(0x3fUL));
+}
+
+
+
+static int aead_setauthsize(struct crypto_aead *tfm, unsigned int authsize)
+{
+	struct aead_tfm *crt = crypto_aead_crt(tfm);
+	crt->authsize = authsize;
+	return 0;
+}
+
+static void aead_session_cleanup(struct crypto_tfm *tfm)
+{
+}
+
+static int aead_cra_cbc_init(struct crypto_tfm *tfm)
+{
+	struct nlm_aead_ctx *nlm_ctx = (struct nlm_aead_ctx *)nlm_crypto_tfm_ctx(tfm);
+	nlm_ctx->cbc = 1;
+	tfm->crt_aead.reqsize = PACKET_DESC_SIZE; 
+	return 0;
+}
+
+static int aead_cra_init_ccm(struct crypto_tfm *tfm)
+{
+	struct nlm_aead_ctx *nlm_ctx = (struct nlm_aead_ctx *)nlm_crypto_tfm_ctx(tfm);
+	nlm_ctx->cbc = 0;
+	tfm->crt_aead.reqsize = PACKET_DESC_SIZE; 
+	nlm_ctx->fallback = crypto_alloc_aead("rfc4309(ccm(aes-generic))",CRYPTO_ALG_TYPE_AEAD ,0) ;
+	return 0;
+}
+
+static int aead_cra_init_gcm(struct crypto_tfm *tfm)
+{
+	struct nlm_aead_ctx *nlm_ctx = (struct nlm_aead_ctx *)nlm_crypto_tfm_ctx(tfm);
+	nlm_ctx->cbc = 0;
+	tfm->crt_aead.reqsize = PACKET_DESC_SIZE; 
+	nlm_ctx->fallback = crypto_alloc_aead("rfc4106(gcm(aes-generic))",CRYPTO_ALG_TYPE_AEAD ,0) ;
+	return 0;
+}
+
+static int aead_cra_init(struct crypto_tfm *tfm)
+{
+	struct nlm_aead_ctx *nlm_ctx = (struct nlm_aead_ctx *)nlm_crypto_tfm_ctx(tfm);
+	nlm_ctx->cbc = 0;
+	tfm->crt_aead.reqsize = PACKET_DESC_SIZE; 
+	return 0;
+}
+
+static int get_cipher_auth_keylen(const u8 *key, unsigned int keylen, int *cipher_keylen,
+			     int *auth_keylen)
+{
+	struct rtattr *rta = (struct rtattr *) key;
+	struct crypto_authenc_key_param *param;
+
+	if (!RTA_OK(rta, keylen)) {
+		goto badkey;
+	}
+
+	if (rta->rta_type != CRYPTO_AUTHENC_KEYA_PARAM) {
+		goto badkey;
+	}
+	if (RTA_PAYLOAD(rta) < sizeof (struct crypto_authenc_key_param)) {
+		goto badkey;
+	}
+
+	param = RTA_DATA(rta);
+	*cipher_keylen = be32_to_cpu(param->enckeylen);
+
+	key += RTA_ALIGN(rta->rta_len);
+	keylen -= RTA_ALIGN(rta->rta_len);
+
+	if (keylen < *cipher_keylen)
+		goto badkey;
+
+	*auth_keylen = keylen - *cipher_keylen;
+
+	return 0;
+badkey:
+	return -EINVAL;
+}
+
+static int get_cipher_aes_algid(unsigned int cipher_keylen)
+{
+
+	switch (cipher_keylen) {
+	case 16:
+		return NLM_CIPHER_AES128;
+		break;
+	case 24:
+		return NLM_CIPHER_AES192;
+		break;
+	case 32:
+		return NLM_CIPHER_AES256;
+		break;
+	default:
+		printk(KERN_WARNING "[%s]: Cannot handle keylen = %d\n",
+		       __FUNCTION__, cipher_keylen);
+		return -EINVAL;
+	}
+}
+
+
+static int get_auth_aes_algid(unsigned int hash_keylen)
+{
+
+	switch (hash_keylen) {
+	case 16:
+		return NLM_HASH_AES128;
+		break;
+	case 24:
+		return NLM_HASH_AES192;
+		break;
+	case 32:
+		return NLM_HASH_AES256;
+		break;
+	default:
+		printk(KERN_WARNING "[%s]: Cannot handle keylen = %d\n",
+		       __FUNCTION__, hash_keylen);
+	    return -EINVAL;
+	}
+}
+
+/*
+   All Setkey goes here.
+ */
+
+static int xlp_aes_cbc_setkey( struct crypto_aead *tfm, uint8_t *key, unsigned int keylen,
+				int hash, int mode,uint16_t h_stat )
+{ 
+	struct nlm_aead_ctx *ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+	struct nlm_crypto_pkt_ctrl *ctrl = &ctx->ctrl;
+	unsigned int cipher_keylen=0, auth_keylen=0;
+	int ret;
+	int cipher_alg;
+	uint8_t auth_key[128];
+	uint8_t *cipher_key;
+	struct rtattr *rta = (struct rtattr *)key;
+	int hmac = ((mode == NLM_HASH_MODE_XCBC) ? 0: 1);
+	if ((ret = get_cipher_auth_keylen(key, keylen, &cipher_keylen,
+					  &auth_keylen)) < 0) {
+		crypto_aead_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+		printk("ERR: Bad key len\n");
+		return ret;
+	}
+
+	cipher_alg = get_cipher_aes_algid(cipher_keylen);
+	ctx->stat = cipher_alg - 1;
+	ctx->stat = ctx->stat | (h_stat << 8);
+	if (cipher_alg < 0) {
+		crypto_aead_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+		printk("ERR: Bad aes key len\n");
+		return -EINVAL;
+	}
+
+	key += RTA_ALIGN(rta->rta_len);
+	cipher_key = key + auth_keylen;
+	memcpy(auth_key, key, auth_keylen);
+	if(auth_mode_key_len[hash][mode] > auth_keylen)
+		memset(auth_key + auth_keylen, 0,  auth_mode_key_len[hash][mode] - auth_keylen);
+
+	if(cipher_mode_iv_len[cipher_alg][NLM_CIPHER_MODE_CBC] > 0)
+		ctx->iv_len = cipher_mode_iv_len[cipher_alg][NLM_CIPHER_MODE_CBC];
+
+	auth_keylen = auth_mode_key_len[hash][mode];
+
+	#ifdef 	NLM_CRYPTO_DEBUG
+	printk("Initial authkeylen %d, cipherkeylen %d\n", cipher_keylen, auth_keylen);
+	print_buf("ENC_KEY:", cipher_key, cipher_keylen);
+	print_buf("AUTH_KEY:", auth_key, auth_keylen);
+	#endif
+
+	ret =  nlm_crypto_fill_pkt_ctrl(ctrl, hmac, hash, 
+			mode, cipher_alg, NLM_CIPHER_MODE_CBC, 0, cipher_key, 
+			cipher_keylen, auth_key, auth_keylen);
+
+	return ret;
+}
+
+static int xlp_aes_ctr_setkey( struct crypto_aead *tfm, u8 *key,
+					unsigned int keylen, int hash, int mode, uint16_t h_stat)
+{
+	struct nlm_aead_ctx *ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+	struct nlm_crypto_pkt_ctrl *ctrl = &ctx->ctrl;
+	unsigned int cipher_keylen=0, auth_keylen=0;
+	int ret;
+	int nonce_len = CTR_RFC3686_NONCE_SIZE;
+	int cipher_alg;
+	uint8_t auth_key[128];
+	uint8_t *cipher_key;
+	struct rtattr *rta = (struct rtattr *)key;
+	int hmac = ((mode == NLM_HASH_MODE_XCBC) ? 0: 1);
+
+	if ((ret = get_cipher_auth_keylen(key, keylen, &cipher_keylen,
+					  &auth_keylen)) < 0) {
+		crypto_aead_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+		printk("ERR: Bad key len\n");
+		return ret;
+	}
+
+	cipher_keylen -= nonce_len;
+
+	cipher_alg = get_cipher_aes_algid(cipher_keylen);
+	if (cipher_alg < 0) {
+		crypto_aead_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+		//sandip: check return val
+		printk("ERR: Bad aes key len\n");
+		return -EINVAL;
+	}
+	ctx->stat = cipher_alg - 1 + 3;
+	ctx->stat = ctx->stat | (h_stat << 8);
+	key += RTA_ALIGN(rta->rta_len);
+	memcpy(auth_key, key, auth_keylen);
+	cipher_key = key + auth_keylen;
+	memcpy(ctx->iv_buf,key+auth_keylen+cipher_keylen, nonce_len);
+	#ifdef 	NLM_CRYPTO_DEBUG
+	printk("Initial authkeylen %d, cipherkeylen %d\n", cipher_keylen, auth_keylen);
+	print_buf ( "key",key,128);
+	print_buf("NONCE:", &ctx->iv_buf[0] , nonce_len);
+	#endif
+
+	if ( auth_mode_key_len[hash][mode] > auth_keylen)
+		memset(auth_key + auth_keylen, 0,  auth_mode_key_len[hash][mode] - auth_keylen);
+	if(cipher_mode_iv_len[cipher_alg][NLM_CIPHER_MODE_CTR] > 0)
+		ctx->iv_len = cipher_mode_iv_len[cipher_alg][NLM_CIPHER_MODE_CTR];
+
+	auth_keylen = auth_mode_key_len[hash][mode];
+
+	#ifdef 	NLM_CRYPTO_DEBUG
+	print_buf("ENC_KEY:", cipher_key, cipher_keylen);
+	print_buf("AUTH_KEY:", auth_key, auth_keylen);
+	#endif
+
+	ret =  nlm_crypto_fill_pkt_ctrl(ctrl, hmac, hash,
+		mode, cipher_alg, NLM_CIPHER_MODE_CTR, 0, cipher_key,
+		cipher_keylen, auth_key, auth_keylen);
+
+	return ret;
+}
+static int  xlp_3des_setkey(struct crypto_aead *tfm, u8 *key, unsigned int keylen, int hash, int mode,uint16_t h_stat )
+{
+	struct nlm_aead_ctx *ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+	struct nlm_aead_ctx * nlm_ctx = (struct  nlm_aead_ctx *)(( unsigned long )(( uint8_t *)ctx + CTRL_DESC_SIZE + 64) & ~(0x3fUL));
+        unsigned int cipher_keylen=0, auth_keylen=0;
+	struct nlm_crypto_pkt_ctrl *ctrl = &ctx->ctrl;
+        int ret;
+	uint8_t auth_key[128];
+	uint64_t d_key[3] ;
+	int cipher_alg = NLM_CIPHER_3DES;
+	struct rtattr *rta = (struct rtattr *)key;
+	uint8_t *cipher_key;
+	int hmac = ((mode == NLM_HASH_MODE_XCBC) ? 0: 1);
+
+        if ((ret = get_cipher_auth_keylen(key, keylen, &cipher_keylen,
+                                          &auth_keylen)) < 0) {
+                crypto_aead_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+                printk("ERR: Bad key len\n");
+                return ret;
+        }
+	ctx->stat = TDES_CBC_STAT | h_stat << 8;;
+	key += RTA_ALIGN(rta->rta_len);
+	memcpy(auth_key, key, auth_keylen);
+	cipher_key = key + auth_keylen;
+	if ( auth_mode_key_len[hash][mode] > auth_keylen)
+		memset(auth_key + auth_keylen, 0,  auth_mode_key_len[hash][mode] - auth_keylen);
+	
+	if(cipher_mode_iv_len[cipher_alg][NLM_CIPHER_MODE_CBC] > 0) {
+		ctx->iv_len = cipher_mode_iv_len[cipher_alg][NLM_CIPHER_MODE_CBC];
+		nlm_ctx->iv_len = cipher_mode_iv_len[cipher_alg][NLM_CIPHER_MODE_CBC];
+	}
+	auth_keylen = auth_mode_key_len[hash][mode];
+
+	ret =  nlm_crypto_fill_pkt_ctrl(ctrl, hmac, hash,
+		mode, cipher_alg,NLM_CIPHER_MODE_CBC,0,cipher_key,
+		 cipher_keylen, auth_key, auth_keylen);
+	
+	nlm_ctx->stat = TDES_CBC_STAT | h_stat << 8;;
+	if( CHIP_SUPPORTS(DES3_KEY_SWAP) ) {
+		memcpy((uint8_t *)d_key,cipher_key,24);
+	}
+	else {
+		memcpy(d_key,&cipher_key[16],8);
+		memcpy(&d_key[1],&cipher_key[8],8);
+		memcpy(&d_key[2],&cipher_key[0],8);
+	}
+	ret =  nlm_crypto_fill_pkt_ctrl(&nlm_ctx->ctrl, hmac, hash,
+			mode, cipher_alg, NLM_CIPHER_MODE_CBC, 0, ( unsigned char * )d_key,
+			cipher_keylen, auth_key, auth_keylen);
+	#ifdef NLM_CRYPTO_DEBUG 
+	print_buf("ENC_KEY:", cipher_key, cipher_keylen);
+	print_buf("AUTH_KEY:", auth_key, auth_keylen);
+	print_buf("DECRY_KEY",(unsigned char * )&d_key[0],cipher_keylen);
+	#endif
+
+        return ret;
+}
+
+static int xlp_des_setkey( struct crypto_aead *tfm, uint8_t  *key, unsigned int keylen, int hash, int mode, uint16_t h_stat)
+{
+	struct nlm_aead_ctx *ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+        unsigned int cipher_keylen=0, auth_keylen=0;
+	struct nlm_crypto_pkt_ctrl *ctrl = &ctx->ctrl;
+        int ret;
+	uint8_t auth_key[128];
+	int cipher_alg = NLM_CIPHER_DES;
+	struct rtattr *rta = (struct rtattr *)key;
+	uint8_t *cipher_key;
+	int hmac = ((mode == NLM_HASH_MODE_XCBC) ? 0: 1);
+	ctx->stat = DES_CBC_STAT | h_stat << 8;
+	
+
+        if ((ret = get_cipher_auth_keylen(key, keylen, &cipher_keylen,
+                                          &auth_keylen)) < 0) {
+                crypto_aead_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+                printk("ERR: Bad key len\n");
+                return ret;
+        }
+	key += RTA_ALIGN(rta->rta_len);
+	memcpy(auth_key, key, auth_keylen);
+	cipher_key = key + auth_keylen;
+	if ( auth_mode_key_len[hash][mode] > auth_keylen)
+		memset(auth_key + auth_keylen, 0,  auth_mode_key_len[hash][mode] - auth_keylen);
+	
+	if(cipher_mode_iv_len[cipher_alg][NLM_CIPHER_MODE_CBC] > 0) {
+		ctx->iv_len = cipher_mode_iv_len[cipher_alg][NLM_CIPHER_MODE_CBC];
+	}
+	auth_keylen = auth_mode_key_len[hash][mode];
+
+	ret =  nlm_crypto_fill_pkt_ctrl(ctrl, hmac, hash,
+		mode, cipher_alg,NLM_CIPHER_MODE_CBC,0,cipher_key,
+		 cipher_keylen, auth_key, auth_keylen);
+	
+	#ifdef NLM_CRYPTO_DEBUG 
+	print_buf("ENC_KEY:", cipher_key, cipher_keylen);
+	print_buf("AUTH_KEY:", auth_key, auth_keylen);
+	#endif
+
+        return ret;
+
+
+}
+static int aead_gcm_rfc4106_setkey( struct crypto_aead *tfm, const u8 *key,
+						unsigned int keylen)
+{
+	struct nlm_aead_ctx *ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+	int cipher_alg;
+	unsigned int cipher_keylen=0;
+	int auth_alg;
+	int ret;
+	struct nlm_crypto_pkt_ctrl *ctrl = &ctx->ctrl;
+	if (keylen < GCM_RFC4106_NONCE_SIZE)
+                return -EINVAL;
+	cipher_keylen = keylen - GCM_RFC4106_NONCE_SIZE;
+	cipher_alg = get_cipher_aes_algid(cipher_keylen);
+	ctx->stat = cipher_alg - 1;
+	ctx->stat = ctx->stat | (GCM_STAT << 8);
+
+	if (cipher_alg < 0) {
+		crypto_aead_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+		//sandip: check return val
+		printk("ERR: Bad aes key len\n");
+		return -EINVAL;
+	}
+	auth_alg = get_auth_aes_algid(cipher_keylen);
+	if ( auth_alg < 0) {
+		crypto_aead_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+		printk("ERR: Bad aes key len\n");
+		return -EINVAL;
+	}
+	
+	ret = nlm_crypto_fill_pkt_ctrl(ctrl,0,auth_alg,NLM_HASH_MODE_GCM,cipher_alg,
+		NLM_CIPHER_MODE_GCM,0,(u8*)key,cipher_keylen,(u8*)key,cipher_keylen);
+	ctx->iv_len = cipher_mode_iv_len[cipher_alg][NLM_HASH_MODE_GCM];
+#ifdef	NLM_CRYPTO_DEBUG
+	print_buf("KEY\n",key,cipher_keylen);	
+#endif
+	
+	/*copy by nonce*/
+	memcpy(ctx->iv_buf, key + cipher_keylen, GCM_RFC4106_NONCE_SIZE);
+	crypto_aead_setkey(ctx->fallback, key, keylen);
+
+	return ret ;
+
+}
+
+static int aead_ccm_rfc4309_setkey(struct crypto_aead *tfm, const u8 *key, 
+				   unsigned int keylen)
+{
+	struct nlm_aead_ctx *ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+	unsigned int cipher_keylen=0;
+	int ret;
+	int nonce_len = CCM_RFC4309_NONCE_SIZE;
+	int cipher_alg;
+	int auth_alg;
+	struct nlm_crypto_pkt_ctrl *ctrl = &ctx->ctrl;
+	if (keylen < CCM_RFC4309_NONCE_SIZE)
+                return -EINVAL;
+
+	cipher_keylen = keylen - nonce_len;
+	cipher_alg = get_cipher_aes_algid(cipher_keylen);
+	ctx->stat = cipher_alg - 1;
+	ctx->stat = ctx->stat | (CCM_STAT << 8);
+
+	cipher_alg = get_cipher_aes_algid(cipher_keylen);
+	if (cipher_alg < 0) {
+		crypto_aead_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+		//sandip: check return val
+		printk("ERR: Bad aes key len\n");
+		return -EINVAL;
+	}
+	auth_alg = get_auth_aes_algid(cipher_keylen);
+	if ( auth_alg < 0) {
+		crypto_aead_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+		printk("ERR: Bad aes key len\n");
+		return -EINVAL;
+	}
+	
+
+	
+	ret = nlm_crypto_fill_pkt_ctrl(ctrl,0,auth_alg,NLM_HASH_MODE_CCM,cipher_alg,NLM_CIPHER_MODE_CCM,0,(u8*)key,cipher_keylen,(u8*)key,cipher_keylen);
+	ctx->iv_len = cipher_mode_iv_len[cipher_alg][NLM_HASH_MODE_GCM];
+#ifdef	NLM_CRYPTO_DEBUG
+	print_buf("KEY\n",key,cipher_keylen);	
+#endif
+	
+	/*copy by nonce*/
+	memcpy(ctx->iv_buf, key + cipher_keylen, nonce_len);
+	crypto_aead_setkey(ctx->fallback, key, keylen);
+	return ret;
+}
+
+static int xlp_aes_cbc_hmac_sha256_setkey( struct crypto_aead *tfm, const u8 *key, unsigned int keylen)
+{
+	return xlp_aes_cbc_setkey(tfm, (uint8_t *)key, keylen,NLM_HASH_SHA,NLM_HASH_MODE_SHA256,H_SHA256_STAT);
+
+}
+
+static int xlp_aes_cbc_hmac_sha1_setkey(struct crypto_aead *tfm, const u8 *key, unsigned int keylen)
+{
+	return xlp_aes_cbc_setkey(tfm, (uint8_t *)key, keylen,NLM_HASH_SHA,NLM_HASH_MODE_SHA1,H_SHA1_STAT);
+}
+
+static int xlp_aes_cbc_aes_xcbc_mac_setkey(struct crypto_aead *tfm, const u8 *key, unsigned int keylen)
+{
+	return xlp_aes_cbc_setkey(tfm, (uint8_t *)key, keylen,NLM_HASH_AES128,NLM_HASH_MODE_XCBC,AES128_XCBC_STAT);
+}
+
+static int xlp_aes_cbc_hmac_md5_setkey(struct crypto_aead *tfm, const u8 *key, unsigned int keylen)
+{
+	return xlp_aes_cbc_setkey(tfm, (uint8_t *)key, keylen,NLM_HASH_MD5,0,MD5_STAT);
+}
+
+static int xlp_3des_cbc_hmac_md5_setkey(struct crypto_aead *tfm, const u8 *key,
+                                        unsigned int keylen)
+{
+	return xlp_3des_setkey(tfm,(uint8_t *)key,keylen,NLM_HASH_MD5,0,MD5_STAT);
+
+}
+static int xlp_3des_cbc_hmac_sha256_setkey(struct crypto_aead *tfm, const u8 *key,
+                                        unsigned int keylen)
+{
+	return xlp_3des_setkey(tfm,(uint8_t *)key,keylen,NLM_HASH_SHA,NLM_HASH_MODE_SHA256,H_SHA256_STAT);
+        
+}
+static int xlp_3des_cbc_hmac_sha1_setkey(struct crypto_aead *tfm, const u8 *key,
+                                        unsigned int keylen)
+{
+	return xlp_3des_setkey(tfm,(uint8_t *)key,keylen,NLM_HASH_SHA,NLM_HASH_MODE_SHA1,H_SHA1_STAT);
+}
+static int xlp_3des_cbc_aes_xcbc_mac_setkey(struct crypto_aead *tfm, const u8 *key,
+                                        unsigned int keylen)
+{
+	return xlp_3des_setkey(tfm,(uint8_t *)key,keylen,NLM_HASH_AES128,NLM_HASH_MODE_XCBC,AES128_XCBC_STAT);
+}
+static int xlp_des_cbc_aes_xcbc_mac_setkey( struct crypto_aead *tfm, const u8 *key,
+					unsigned int keylen)
+{
+	return xlp_des_setkey(tfm,(uint8_t *)key,keylen,NLM_HASH_AES128,NLM_HASH_MODE_XCBC,AES128_XCBC_STAT);
+}
+static int xlp_des_cbc_hmac_sha1_setkey(struct crypto_aead *tfm, const u8 *key,
+						 unsigned int keylen)
+{
+	return xlp_des_setkey(tfm,(uint8_t *)key,keylen, NLM_HASH_SHA,NLM_HASH_MODE_SHA1,H_SHA1_STAT);
+
+}
+static int xlp_des_cbc_hmac_sha256_setkey(struct crypto_aead *tfm, const u8 *key,
+							unsigned int keylen)
+{
+	return xlp_des_setkey(tfm,(uint8_t *)key,keylen, NLM_HASH_SHA,NLM_HASH_MODE_SHA256,H_SHA256_STAT);
+
+}
+static int xlp_des_cbc_hmac_md5_setkey( struct crypto_aead *tfm, const u8 *key,
+						unsigned int keylen)
+{
+	return xlp_des_setkey(tfm,(uint8_t *)key,keylen,NLM_HASH_MD5,0,MD5_STAT);
+
+}
+
+static  int xlp_aes_ctr_hmac_sha256_setkey ( struct crypto_aead *tfm, const u8 *key,
+					unsigned int keylen)
+{
+	return xlp_aes_ctr_setkey(tfm, (uint8_t *)key, keylen,NLM_HASH_SHA,NLM_HASH_MODE_SHA256,H_SHA256_STAT);
+}
+static  int xlp_aes_ctr_hmac_sha1_setkey ( struct crypto_aead *tfm, const u8 *key,
+					unsigned int keylen)
+{
+	return xlp_aes_ctr_setkey(tfm, (uint8_t *)key, keylen,NLM_HASH_SHA,NLM_HASH_MODE_SHA1,H_SHA1_STAT);
+
+}
+static  int xlp_aes_ctr_aes_xcbc_mac_setkey ( struct crypto_aead *tfm, const u8 *key,
+					unsigned int keylen)
+{
+	return xlp_aes_ctr_setkey(tfm, (uint8_t *)key, keylen,NLM_HASH_AES128,NLM_HASH_MODE_XCBC,AES128_XCBC_STAT);
+}
+
+static int xlp_aes_ctr_hmac_md5_setkey(struct crypto_aead *tfm, const u8 *key,
+					unsigned int keylen)
+{
+	return xlp_aes_ctr_setkey(tfm, (uint8_t *)key, keylen,NLM_HASH_MD5,0,MD5_STAT);
+}
+//returns nr_aad_frags... -1 for error
+unsigned int fill_aead_aad(struct nlm_crypto_pkt_param *param, struct aead_request *req, unsigned int aad_len,int seg,int max_frags)
+{
+	struct scatterlist *sg;
+	struct scatter_walk walk;
+	int len;
+	uint8_t *virt;
+	int rv = 0;
+	
+	for (sg = req->assoc; aad_len > 0; sg = scatterwalk_sg_next(sg) ) {
+
+		len = min(aad_len, sg->length);
+		scatterwalk_start(&walk, sg);
+		//virt = scatterwalk_map(&walk, 1);
+		virt = page_address(scatterwalk_page(&walk)) + offset_in_page(walk.offset);
+		rv = nlm_crypto_fill_src_dst_seg(param, seg, max_frags, virt, len);
+		if ( rv <= seg ) 
+			return nlm_crypto_calc_rem_len(sg,aad_len) + max_frags;
+		else 
+			seg = rv;
+		aad_len -= len;
+	}
+	return seg;
+}
+		
+/*
+   Generic Encrypt / Decrypt Function
+ */
+//op is either encrypt or decrypt
+
+static void aead_request_callback(struct nlm_async_crypto *async, uint64_t msg1)
+{
+	struct crypto_async_request *base = (struct crypto_async_request *)async->args;
+	int err = 0;
+	int cpu = hard_smp_processor_id();
+	int enc = async->stat & 0xff;
+	int auth = (async->stat >> 8 ) & 0xff;
+
+	if (msg1 & 0x7ff80) {
+		printk("\n Error: entry1 is %llx",msg1);
+		err = -EIO;
+		base->complete(base, err);
+		return;
+	}
+	if (async->op){
+		scatterwalk_map_and_copy(async->hash_addr, async->dst, async->bytes, async->authsize, 1);
+	}else{
+		char authtag[64];
+		scatterwalk_map_and_copy(authtag, async->src, async->bytes-async->authsize, async->authsize, 0);
+		if(memcmp(authtag, async->hash_addr, async->authsize)){
+			err = -EBADMSG;
+		}
+	}
+	crypto_stat[cpu].enc[enc]++;
+	crypto_stat[cpu].auth[auth]++;	
+	crypto_stat[cpu].enc_tbytes[enc] += async->bytes;
+	crypto_stat[cpu].auth_tbytes[auth] += async->bytes;
+	if ( async->pkt_param)
+		kfree(async->pkt_param);
+	base->complete(base, err);
+	return;
+}
+
+static int aead_crypt_gcm(struct aead_request *req, unsigned int op)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_alg *alg = tfm->base.__crt_alg;
+	struct aead_alg *aead= &alg->cra_aead;
+	struct nlm_aead_ctx *ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+	struct nlm_crypto_pkt_param *param;
+	unsigned int cipher_off, auth_off, iv_off;
+	unsigned int auth_len, cipher_len;
+	int seg = 0,iv_size =16;
+	unsigned int hash_source;
+	uint64_t entry0, entry1, tx_id=0x12345678;
+	struct nlm_async_crypto *async =  (struct nlm_async_crypto *)NLM_ASYNC_PTR_PARAM_OFFSET(aead_request_ctx(req));
+	uint8_t *hash_addr;
+	int fb_vc; 
+	int err=0;
+	unsigned int authsize,maxauthsize;
+	struct nlm_crypto_pkt_ctrl *ctrl = &ctx->ctrl;
+	char * iv = (uint8_t *)NLM_IV_OFFSET(aead_request_ctx(req)); 
+	uint8_t *tmp_iv = iv;
+#ifdef CONFIG_32BIT
+	unsigned long msgrng_flags;
+#endif
+	unsigned int max_frags  = MAX_FRAGS;
+	int node_sae_base;
+	int node;
+
+	authsize = crypto_aead_authsize(tfm);
+	maxauthsize= aead->maxauthsize;
+	async->pkt_param = NULL;
+	async->src = req->src;
+	async->dst = req->dst;
+
+	if ( (op &&  (req->cryptlen == 0 )) || (!op && req->cryptlen <= aead->maxauthsize))
+	{
+		int ret =0;
+		ret = crypto_aead_setauthsize(ctx->fallback, authsize);
+		aead_request_set_tfm(req, ctx->fallback);
+		if ( op )
+		ret = crypto_aead_encrypt(req);
+		else 
+		ret = crypto_aead_decrypt(req);
+		aead_request_set_tfm(req,tfm);
+		
+		return ret;
+	}
+	
+	
+
+	/*Copy nonce*/
+	memcpy(tmp_iv,  ctx->iv_buf, GCM_RFC4106_NONCE_SIZE);
+	tmp_iv += GCM_RFC4106_NONCE_SIZE;
+
+	/*Copy IV*/
+	memcpy(tmp_iv, req->iv, GCM_RFC4106_IV_SIZE);
+	tmp_iv += GCM_RFC4106_IV_SIZE;
+
+	/*Set counter*/
+	*((uint32_t*)tmp_iv) = cpu_to_be32((uint32_t)1);
+
+	param = (struct nlm_crypto_pkt_param *)NLM_CRYPTO_PKT_PARAM_OFFSET(aead_request_ctx(req));
+	hash_addr = (uint8_t *)NLM_HASH_OFFSET(aead_request_ctx(req));
+
+	iv_off = 0;
+	auth_off = iv_size;
+	cipher_off = req->assoclen + iv_size;
+
+	//check if it should be aip->tag_len or can be taken from tfm
+	cipher_len = op ? req->cryptlen: req->cryptlen - authsize;
+	auth_len = req->assoclen + cipher_len; 
+	hash_source = (op) ? 0 : 1;
+
+	/*Setup NONCE_IV_CTR COMBO*/
+	seg = nlm_crypto_fill_src_dst_seg(param, seg, max_frags, iv, iv_size);
+
+	do { 
+	
+	/*Setup AAD - SPI/SEQ Number*/
+	seg = fill_aead_aad(param, req, req->assoclen,seg,max_frags);
+	seg = fill_src_dst_sg(req->src,req->dst, cipher_len, param, seg, max_frags,op);
+
+	if ( seg > max_frags) {
+		max_frags = seg;
+		seg = alloc_pkt_param(async,&param,max_frags);
+		if ( seg == -1 )
+			return -1;
+		seg =1 ;
+	}
+	}while(seg == 1);  
+
+	nlm_crypto_fill_cipher_auth_pkt_param(ctrl, param, op, hash_source, iv_off, 
+		iv_size, auth_off, auth_len, 0, cipher_off, cipher_len, hash_addr);
+	
+	fb_vc = crypto_get_fb_vc(&node);
+
+	entry0 = nlm_crypto_form_pkt_fmn_entry0(fb_vc, 0, 0, ctrl->cipherkeylen, virt_to_phys(ctrl));
+	entry1 = nlm_crypto_form_pkt_fmn_entry1(0, ctrl->hashkeylen, (seg + 2)<<4, virt_to_phys(param));
+
+
+#ifdef NLM_CRYPTO_DEBUG
+	printk("*****ctrl descriptor phys addr %#lx\n",virt_to_phys(ctrl));
+	print_crypto_msg_desc(entry0, entry1, tx_id);
+	print_cntl_instr(ctrl->desc0);
+	print_buf("ENC_KEY:", (uint8_t *)ctrl->key, ctrl->cipherkeylen);
+	print_buf("AUTH_KEY:", (uint8_t *)ctrl->key + ctrl->cipherkeylen, ctrl->hashkeylen);
+	print_pkt_desc(param,seg);
+	printk("iv_off: %d, cipher_off: %d, auth_off: %d\n",iv_off, cipher_off, auth_off);
+	printk("auth_len: %d, cipher_len: %d\n", auth_len, cipher_len);
+#endif
+
+	async->callback =  aead_request_callback;
+	async->args = (void *)&req->base; 
+	async->op  = op;
+	async->hash_addr = hash_addr;
+	async->authsize = authsize;
+	async->stat = ctx->stat; 
+	async->bytes = req->cryptlen; 
+	tx_id = (uint64_t)(unsigned long)async;
+
+	//construct pkt, send to engine and receive reply
+#ifdef CONFIG_32BIT
+	msgrng_access_enable(msgrng_flags);
+#endif
+	node_sae_base = (node << NODE_BASE_SHIFT_BIT) | crypto_vc_base;
+	err = nlm_hal_send_msg3(node_sae_base, 0 /*code */ , entry0, entry1, tx_id);
+#ifdef CONFIG_32BIT
+	msgrng_access_disable(msgrng_flags);
+#endif
+	if(err){
+		return -EAGAIN;
+	}
+
+
+	return -EINPROGRESS;
+
+}
+
+static int aead_crypt_ccm(struct aead_request *req, unsigned int op)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_alg *alg = tfm->base.__crt_alg;
+	struct aead_alg *aead= &alg->cra_aead;
+	struct nlm_aead_ctx *ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+	struct nlm_crypto_pkt_param *param;
+	unsigned int cipher_off, auth_off, iv_off;
+	unsigned int auth_len, cipher_len;
+	unsigned int hash_source;
+	uint64_t entry0, entry1, tx_id=0x12345678;
+	struct nlm_async_crypto *async =  (struct nlm_async_crypto *)NLM_ASYNC_PTR_PARAM_OFFSET(aead_request_ctx(req));
+	uint8_t *hash_addr;
+	int fb_vc; 
+	int err=0;
+	unsigned int authsize,maxauthsize;
+	struct nlm_crypto_pkt_ctrl *ctrl = &ctx->ctrl;
+	char * iv = (uint8_t *)NLM_IV_OFFSET(aead_request_ctx(req));
+	int iv_size = 16;
+	uint8_t *auth_iv = (uint8_t *)iv + iv_size;
+	unsigned int auth_iv_frag_len = iv_size; 
+	unsigned int extralen = 0, cipher_extralen =0;
+	int seg = 0;
+	uint8_t *tmp_iv = &iv[1];
+#ifdef CONFIG_32BIT
+	unsigned long msgrng_flags;
+#endif
+	unsigned int max_frags  = MAX_FRAGS;
+	int node_sae_base;
+	int node;
+
+	param = (struct nlm_crypto_pkt_param *)NLM_CRYPTO_PKT_PARAM_OFFSET(aead_request_ctx(req));
+	hash_addr = (uint8_t *)NLM_HASH_OFFSET(aead_request_ctx(req));
+
+	authsize = crypto_aead_authsize(tfm);
+	maxauthsize= aead->maxauthsize;
+	async->pkt_param = NULL;
+	async->src = req->src;
+	async->dst = req->dst;
+
+	if ( (op &&  (req->cryptlen == 0 )) || (!op && req->cryptlen <= aead->maxauthsize))
+	{
+		int ret =0;
+		ret = crypto_aead_setauthsize(ctx->fallback, authsize);
+		aead_request_set_tfm(req, ctx->fallback);
+		if ( op )
+		ret = crypto_aead_encrypt(req);
+		else 
+		ret = crypto_aead_decrypt(req);
+		aead_request_set_tfm(req,tfm);
+		
+		return ret;
+	}
+	
+		
+	/*Copy nonce*/
+	memcpy(tmp_iv,  &ctx->iv_buf, CCM_RFC4309_NONCE_SIZE);
+	tmp_iv += CCM_RFC4309_NONCE_SIZE;
+
+	/*Copy IV*/
+	memcpy(tmp_iv, req->iv, CCM_RFC4309_IV_SIZE);
+	tmp_iv += CCM_RFC4309_IV_SIZE;
+
+	/*Set counter*/
+	*((uint32_t*)tmp_iv) = (uint32_t)0;
+
+	memcpy(auth_iv,iv,iv_size);
+
+	/* Encryption iv  7            Reserved (always zero)
+	   6            Reserved (always zero)
+	   5 ... 3      Zero
+	   2 ... 0      L' ( L -1 ) ( Length of the counter ) */
+
+	iv[0] = 3;
+
+	iv_off = 0;
+	auth_off = iv_size;
+	cipher_len = op ? req->cryptlen: req->cryptlen - authsize;
+	hash_source = (op) ? 0 : 1;
+
+
+	/*Setup ENCRYPTION IV*/
+
+	/*	7            Reserved (always zero)
+		6            Adata
+		5 ... 3      M' ( (tag_len -2) /2)
+		2 ... 0      L' */
+
+	auth_iv[0] = ((req->assoclen?1 : 0 ) << 6 )| ((authsize - 2 )/2) << 3 | 3;
+
+	auth_iv_frag_len = iv_size; 
+	extralen = req->assoclen;
+	if ( req->assoclen ) {
+
+
+		if ( req->assoclen  < 65280) { 
+			*(short*)(auth_iv + 16) = cpu_to_be16((short)req->assoclen);
+			extralen += 2;
+			auth_iv_frag_len = 18;
+		}
+		else
+		{
+			*(short*)(auth_iv+ 16) = cpu_to_be16((short)0xfffe);
+			*(short*)(auth_iv + 16) = cpu_to_be16((short)0xfffe);
+			*(uint32_t*)(auth_iv + 18) = cpu_to_be32((uint32_t)req->assoclen);
+			*(uint32_t*)(auth_iv + 18) = cpu_to_be32((uint32_t)req->assoclen);
+			auth_iv_frag_len = 22;
+			extralen += 6;
+		}
+	}
+	/*Setup AUTH IV*/
+	*(uint32_t*)&auth_iv[12] |= cpu_to_be32((uint32_t )cipher_len);
+
+
+	//one for cipher iv one for auth iv
+	cipher_off = iv_size + iv_size + extralen;  
+
+	if ( req->assoclen ) {
+		/* AAD has to be block aligned */
+		extralen = extralen % 16;
+		if ( extralen ) {
+			extralen = AES_BLOCK_SIZE - extralen;
+			memset(auth_iv + 22,0,extralen);
+			cipher_off += extralen;
+		}
+	}
+
+	cipher_extralen = cipher_len % 16;
+
+	if ( cipher_extralen ) {
+		cipher_extralen = AES_BLOCK_SIZE - cipher_extralen;
+		memset(auth_iv + 38,0,cipher_extralen);
+	}
+	auth_len = cipher_off + cipher_len + cipher_extralen - auth_off; 
+
+	seg = nlm_crypto_fill_src_dst_seg(param, seg, max_frags, iv, iv_size);
+
+	do {
+	auth_iv = (uint8_t *)iv + iv_size;
+	seg = nlm_crypto_fill_src_dst_seg(param, seg,max_frags, auth_iv,auth_iv_frag_len);
+	if ( req->assoclen ) {
+		/*Setup AAD - SPI/SEQ Number*/
+		seg = fill_aead_aad(param, req, req->assoclen,seg,max_frags);
+		if ( extralen ) 
+			seg = nlm_crypto_fill_src_dst_seg(param,seg,max_frags, (auth_iv+22),extralen);
+		
+	}
+	seg = fill_src_dst_sg(req->src,req->dst, cipher_len, param,seg, max_frags,op);
+
+
+	if ( cipher_extralen ) {
+		if ( seg >= max_frags ){
+			seg+=  nlm_crypto_sae_num_seg_reqd(auth_iv+38, cipher_extralen);
+		}
+		else 
+			seg = nlm_crypto_fill_src_dst_seg(param,seg,max_frags, (auth_iv+38),cipher_extralen);
+	}
+	
+
+	if ( seg > max_frags) {
+		max_frags = seg;
+		seg = alloc_pkt_param(async,&param,max_frags);
+		if ( seg == -1 ) {
+			return -1;
+		}
+	}
+	}while(seg == 1);  
+
+	nlm_crypto_fill_cipher_auth_pkt_param(ctrl, param, op, hash_source, iv_off,
+			iv_size, auth_off, auth_len, 0, cipher_off, cipher_len, hash_addr);
+
+	fb_vc = crypto_get_fb_vc(&node);
+	entry0 = nlm_crypto_form_pkt_fmn_entry0(fb_vc, 0, 0, ctrl->cipherkeylen, virt_to_phys(ctrl))
+;
+	entry1 = nlm_crypto_form_pkt_fmn_entry1(0, ctrl->hashkeylen, (seg + 2)<<4, virt_to_phys(param));
+
+
+
+#ifdef NLM_CRYPTO_DEBUG
+	printk("*****ctrl descriptor phys addr %#lx\n",virt_to_phys(ctrl));
+	print_crypto_msg_desc(entry0, entry1, tx_id);
+	print_cntl_instr(ctrl->desc0);
+	print_buf("ENC_KEY:", (uint8_t *)ctrl->key, ctrl->cipherkeylen);
+	print_buf("AUTH_KEY:", (uint8_t *)ctrl->key + ctrl->cipherkeylen, ctrl->hashkeylen);
+	print_pkt_desc(param,seg);
+	printk("iv_off: %d, cipher_off: %d, auth_off: %d\n",iv_off, cipher_off, auth_off);
+	printk("auth_len: %d, cipher_len: %d\n", auth_len, cipher_len);
+#endif
+
+
+	async->callback =  aead_request_callback;
+	async->args = (void *)&req->base; 
+	async->op  = op;
+	async->hash_addr = hash_addr;
+	async->authsize = authsize;
+	async->stat = ctx->stat; 
+	async->bytes = req->cryptlen; 
+	tx_id = (uint64_t)(unsigned long)async;
+
+	//construct pkt, send to engine and receive reply
+#ifdef CONFIG_32BIT
+	msgrng_access_enable(msgrng_flags);
+#endif
+	node_sae_base = (node << NODE_BASE_SHIFT_BIT) | crypto_vc_base;
+	err = nlm_hal_send_msg3(node_sae_base, 0 /*code */ , entry0, entry1, tx_id);
+#ifdef CONFIG_32BIT
+	msgrng_access_disable(msgrng_flags);
+#endif
+	if(err){
+		return -EAGAIN;
+	}
+
+
+	return -EINPROGRESS;
+
+
+}
+static int aead_crypt_ctr(struct aead_request *req, unsigned int op)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_alg *alg = tfm->base.__crt_alg;
+	struct aead_alg *aead= &alg->cra_aead;
+	struct nlm_aead_ctx *ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+	struct nlm_crypto_pkt_param *param;
+	unsigned int cipher_off, iv_off;
+	unsigned int auth_len, cipher_len, auth_off;
+	int seg = 0,ivsize;
+	unsigned int hash_source;
+	uint64_t entry0, entry1;
+	uint64_t tx_id=0x12345678;
+	void  * addr = aead_request_ctx(req);
+	struct nlm_async_crypto *async =  (struct nlm_async_crypto *)NLM_ASYNC_PTR_PARAM_OFFSET(aead_request_ctx(req));
+	uint8_t *hash_addr;
+	int fb_vc; 
+	int err=0;
+	unsigned int authsize,maxauthsize;
+	uint8_t *iv = NULL; 
+	struct nlm_crypto_pkt_ctrl *ctrl = &ctx->ctrl;
+#ifdef CONFIG_32BIT
+	unsigned long msgrng_flags;
+#endif
+	unsigned int max_frags  = MAX_FRAGS;
+	int node_sae_base;
+	int node;
+
+	authsize = crypto_aead_authsize(tfm);
+	maxauthsize= aead->maxauthsize;
+	async->pkt_param = NULL;
+	async->src = req->src;
+	async->dst = req->dst;
+	param = (struct nlm_crypto_pkt_param *)NLM_CRYPTO_PKT_PARAM_OFFSET(addr);
+
+	iv = (uint8_t *)NLM_IV_OFFSET(addr);
+	if ( !op ) {
+		 uint8_t *tmp_iv = iv;
+		/*Copy nonce*/
+		memcpy(tmp_iv,  ctx->iv_buf, CTR_RFC3686_NONCE_SIZE);
+		tmp_iv += CTR_RFC3686_NONCE_SIZE;
+
+		memcpy(tmp_iv, req->iv, CTR_RFC3686_IV_SIZE);
+		tmp_iv += CTR_RFC3686_IV_SIZE;
+
+		/*Set counter*/
+		*((uint32_t *)tmp_iv) = cpu_to_be32((uint32_t)1);
+	}
+
+	hash_addr = (uint8_t *)NLM_HASH_OFFSET(addr);
+
+	ivsize = crypto_aead_ivsize(crypto_aead_reqtfm(req));
+
+	auth_off = CTR_RFC3686_BLOCK_SIZE;
+	cipher_off = auth_off + req->assoclen + ivsize;
+	iv_off = 0;
+
+	//check if it should be aip->tag_len or can be taken from tfm
+	cipher_len = op ? req->cryptlen:req->cryptlen - authsize;
+	auth_len = cipher_off - auth_off + cipher_len;
+	hash_source = op;
+	seg = nlm_crypto_fill_src_dst_seg(param, seg, max_frags, iv, CTR_RFC3686_BLOCK_SIZE);
+
+
+	do {
+	seg = fill_aead_aad(param, req, req->assoclen,seg,max_frags);
+
+	seg = 	nlm_crypto_fill_src_dst_seg(param, seg, max_frags, iv+CTR_RFC3686_NONCE_SIZE, ivsize);
+
+	seg = fill_src_dst_sg(req->src,req->dst, cipher_len, param, seg, max_frags,op);
+
+	if ( seg > max_frags) {
+		max_frags = seg;
+		seg = alloc_pkt_param(async,&param,max_frags);
+		if ( seg == -1 )
+			return -1;
+		seg = 1;
+	}
+	}while(seg == 1);  
+
+	nlm_crypto_fill_cipher_auth_pkt_param(ctrl, param, op, op, iv_off, 
+			CTR_RFC3686_BLOCK_SIZE, auth_off, auth_len, 0, cipher_off, cipher_len, hash_addr);
+	
+	fb_vc = crypto_get_fb_vc(&node);
+
+	entry0 = nlm_crypto_form_pkt_fmn_entry0(fb_vc, 0, 0, ctrl->cipherkeylen, virt_to_phys(ctrl));
+	entry1 = nlm_crypto_form_pkt_fmn_entry1(0, ctrl->hashkeylen, (seg + 2)<<4, virt_to_phys(param));
+
+
+#ifdef NLM_CRYPTO_DEBUG
+	printk("*****ctrl descriptor phys addr %#lx\n",virt_to_phys(ctrl));
+	print_crypto_msg_desc(entry0, entry1, tx_id);
+	print_pkt_desc(param,seg);
+	print_cntl_instr(ctrl->desc0);
+	print_buf("ENC_KEY:", (uint8_t *)ctrl->key, ctrl->cipherkeylen);
+	print_buf("AUTH_KEY:", (uint8_t *)ctrl->key + ctrl->cipherkeylen, ctrl->hashkeylen);
+	print_buf("IV ",iv,16);
+	printk("iv_off: %d, cipher_off: %d, auth_off: %d\n",iv_off, cipher_off, auth_off);
+	printk("auth_len: %d, cipher_len: %d\n", auth_len, cipher_len);
+#endif
+
+	async->callback =  aead_request_callback;
+	async->args = (void *)&req->base; 
+	async->op  = op;
+	async->hash_addr = hash_addr;
+	async->authsize = authsize;
+	async->stat = ctx->stat;
+	async->bytes = req->cryptlen;
+	tx_id = (uint64_t)(unsigned long)async;
+
+	//construct pkt, send to engine and receive reply
+#ifdef CONFIG_32BIT
+	msgrng_access_enable(msgrng_flags);
+#endif
+	node_sae_base = (node << NODE_BASE_SHIFT_BIT) | crypto_vc_base;
+	err = nlm_hal_send_msg3(node_sae_base, 0 /*code */ , entry0, entry1, tx_id);
+#ifdef CONFIG_32BIT
+	msgrng_access_disable(msgrng_flags);
+#endif
+	if(err){
+		return -EAGAIN;
+	}
+
+	return -EINPROGRESS;
+}
+
+static int aead_crypt_3des(struct aead_request *req, unsigned int op)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_alg *alg = tfm->base.__crt_alg;
+	struct aead_alg *aead= &alg->cra_aead;
+	struct nlm_aead_ctx *ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+	struct nlm_crypto_pkt_param *param;
+	unsigned int cipher_off, iv_off;
+	unsigned int auth_len, cipher_len, auth_off;
+	int seg =0,  ivsize;
+	unsigned int hash_source;
+	uint64_t entry0, entry1;
+	uint64_t tx_id=0x12345678;
+	struct nlm_async_crypto *async =  (struct nlm_async_crypto *)NLM_ASYNC_PTR_PARAM_OFFSET(aead_request_ctx(req));
+	uint8_t *hash_addr;
+	int fb_vc; 
+	int err=0;
+	unsigned int authsize,maxauthsize;
+	struct nlm_crypto_pkt_ctrl *ctrl = NULL;
+#ifdef CONFIG_32BIT
+	unsigned long msgrng_flags;
+#endif
+	unsigned int max_frags  = MAX_FRAGS;
+	int node_sae_base;
+	int node;
+
+	ctx = (struct  nlm_aead_ctx *)(( unsigned long )(( uint8_t *)ctx + CTRL_DESC_SIZE + 64) & ~(0x3fUL));
+	ctrl = &ctx->ctrl;
+	
+	authsize = crypto_aead_authsize(tfm);
+	maxauthsize= aead->maxauthsize;
+	async->pkt_param = NULL;
+	async->src = req->src;
+	async->dst = req->dst;
+
+	param = (struct nlm_crypto_pkt_param *)NLM_CRYPTO_PKT_PARAM_OFFSET(aead_request_ctx(req));
+	hash_addr = (uint8_t *)NLM_HASH_OFFSET(aead_request_ctx(req));
+
+	ivsize = crypto_aead_ivsize(crypto_aead_reqtfm(req));
+	auth_off = 0;
+	cipher_off = req->assoclen + ivsize;
+	iv_off = req->assoclen;
+	//check if it should be aip->tag_len or can be taken from tfm
+	cipher_len = op ? req->cryptlen:req->cryptlen - authsize;
+	auth_len = cipher_off + cipher_len;
+	hash_source = op;
+	do {
+
+	seg = fill_aead_aad(param, req, req->assoclen,seg,max_frags);
+
+	if (ivsize) 
+		seg = nlm_crypto_fill_src_dst_seg(param, seg, max_frags, req->iv, ivsize);
+
+	seg = fill_src_dst_sg(req->src,req->dst, cipher_len, param, seg, max_frags,op);
+
+	if ( seg > max_frags) {
+		max_frags = seg;
+		seg = alloc_pkt_param(async,&param,max_frags);
+		if ( seg == -1 )
+			return -1;
+		seg = 0;
+	}
+	}while(seg == 0);  
+
+	nlm_crypto_fill_cipher_auth_pkt_param(ctrl, param, op, op, iv_off, 
+			ivsize, auth_off, auth_len, 0, cipher_off, cipher_len, hash_addr);
+	
+	fb_vc = crypto_get_fb_vc(&node);
+
+	entry0 = nlm_crypto_form_pkt_fmn_entry0(fb_vc, 0, 0, ctrl->cipherkeylen, virt_to_phys(ctrl));
+	entry1 = nlm_crypto_form_pkt_fmn_entry1(0, ctrl->hashkeylen, (seg + 2)<<4, virt_to_phys(param));
+
+
+#ifdef NLM_CRYPTO_DEBUG
+	printk("*****ctrl descriptor phys addr %#lx\n",virt_to_phys(ctrl));
+	print_crypto_msg_desc(entry0, entry1, tx_id);
+	print_cntl_instr(ctrl->desc0);
+	print_buf("ENC_KEY:", (uint8_t *)ctrl->key, ctrl->cipherkeylen);
+	print_buf("AUTH_KEY:", (uint8_t *)ctrl->key + ctrl->cipherkeylen, ctrl->hashkeylen);
+	print_pkt_desc(param,seg);
+	printk("iv_off: %d, cipher_off: %d, auth_off: %d\n",iv_off, cipher_off, auth_off);
+	printk("auth_len: %d, cipher_len: %d\n", auth_len, cipher_len);
+#endif
+
+	async->callback =  aead_request_callback;
+	async->args = (void *)&req->base; 
+	async->op  = op;
+	async->hash_addr = hash_addr;
+	async->authsize = authsize;
+	async->stat = ctx->stat; 
+	async->bytes = req->cryptlen; 
+	tx_id = (uint64_t)(unsigned long)async;
+
+	//construct pkt, send to engine and receive reply
+#ifdef CONFIG_32BIT
+	msgrng_access_enable(msgrng_flags);
+#endif
+	node_sae_base = (node << NODE_BASE_SHIFT_BIT) | crypto_vc_base;
+	err = nlm_hal_send_msg3(node_sae_base, 0 /*code */ , entry0, entry1, tx_id);
+#ifdef CONFIG_32BIT
+	msgrng_access_disable(msgrng_flags);
+#endif
+	if(err){
+		return -EAGAIN;
+	}
+
+
+	return -EINPROGRESS;
+}
+static int aead_crypt(struct aead_request *req, unsigned int op)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_alg *alg = tfm->base.__crt_alg;
+	struct aead_alg *aead= &alg->cra_aead;
+	struct nlm_aead_ctx *ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+	struct nlm_crypto_pkt_param *param;
+	unsigned int cipher_off, iv_off;
+	unsigned int auth_len, cipher_len, auth_off;
+	int seg =0, ivsize;
+	unsigned int hash_source;
+	uint64_t entry0, entry1;
+	uint64_t tx_id=0x12345678;
+	struct nlm_async_crypto *async =  (struct nlm_async_crypto *)NLM_ASYNC_PTR_PARAM_OFFSET(aead_request_ctx(req));
+	uint8_t *hash_addr;
+	int fb_vc; 
+	int err=0;
+	unsigned int authsize,maxauthsize;
+	struct nlm_crypto_pkt_ctrl *ctrl = &ctx->ctrl;
+#ifdef CONFIG_32BIT
+	unsigned long msgrng_flags;
+#endif
+	unsigned int max_frags  = MAX_FRAGS;
+	int node_sae_base;
+	int node;
+
+	authsize = crypto_aead_authsize(tfm);
+	maxauthsize= aead->maxauthsize;
+	async->pkt_param = NULL;
+	async->src = req->src;
+	async->dst = req->dst;
+
+	param = (struct nlm_crypto_pkt_param *)NLM_CRYPTO_PKT_PARAM_OFFSET(aead_request_ctx(req));
+	hash_addr = (uint8_t *)NLM_HASH_OFFSET(aead_request_ctx(req));
+
+	ivsize = crypto_aead_ivsize(crypto_aead_reqtfm(req));
+	auth_off = 0;
+	cipher_off = req->assoclen + ivsize;
+	iv_off = req->assoclen;
+	//check if it should be aip->tag_len or can be taken from tfm
+	cipher_len = op ? req->cryptlen:req->cryptlen - authsize;
+	auth_len = cipher_off + cipher_len;
+	hash_source = op;
+
+	do { 
+
+	seg = fill_aead_aad(param, req, req->assoclen,seg,max_frags);
+
+	if (ivsize) 
+		seg = nlm_crypto_fill_src_dst_seg(param, seg, max_frags, req->iv, ivsize);
+
+	seg = fill_src_dst_sg(req->src,req->dst, cipher_len, param, seg, max_frags,op);
+
+	if ( seg > max_frags) {
+		max_frags = seg;
+		seg = alloc_pkt_param(async,&param,max_frags);
+		if ( seg == -1 )
+			return -1;
+		seg = 0;
+	}
+	}while(seg == 0);  
+
+	nlm_crypto_fill_cipher_auth_pkt_param(ctrl, param, op, op, iv_off, 
+			ivsize, auth_off, auth_len, 0, cipher_off, cipher_len, hash_addr);
+	
+	fb_vc = crypto_get_fb_vc(&node);
+
+	entry0 = nlm_crypto_form_pkt_fmn_entry0(fb_vc, 0, 0, ctrl->cipherkeylen, virt_to_phys(ctrl));
+	entry1 = nlm_crypto_form_pkt_fmn_entry1(0, ctrl->hashkeylen, (seg + 2)<<4, virt_to_phys(param));
+
+
+#ifdef NLM_CRYPTO_DEBUG
+	printk("*****ctrl descriptor phys addr %#lx\n",virt_to_phys(ctrl));
+	print_crypto_msg_desc(entry0, entry1, tx_id);
+	print_cntl_instr(ctrl->desc0);
+	print_buf("ENC_KEY:", (uint8_t *)ctrl->key, ctrl->cipherkeylen);
+	print_buf("AUTH_KEY:", (uint8_t *)ctrl->key + ctrl->cipherkeylen, ctrl->hashkeylen);
+	print_pkt_desc(param,seg);
+	printk("iv_off: %d, cipher_off: %d, auth_off: %d\n",iv_off, cipher_off, auth_off);
+	printk("auth_len: %d, cipher_len: %d\n", auth_len, cipher_len);
+#endif
+
+	async->callback =  aead_request_callback;
+	async->args = (void *)&req->base; 
+	async->op  = op;
+	async->hash_addr = hash_addr;
+	async->authsize = authsize;
+	async->stat = ctx->stat; 
+	async->bytes = req->cryptlen; 
+	tx_id = (uint64_t)(unsigned long)async;
+
+
+	//construct pkt, send to engine and receive reply
+#ifdef CONFIG_32BIT
+	msgrng_access_enable(msgrng_flags);
+#endif
+	node_sae_base = (node << NODE_BASE_SHIFT_BIT) | crypto_vc_base;
+	err = nlm_hal_send_msg3(node_sae_base, 0 /*code */ , entry0, entry1, tx_id);
+#ifdef CONFIG_32BIT
+	msgrng_access_disable(msgrng_flags);
+#endif
+	if(err){
+		return -EAGAIN;
+	}
+
+	return -EINPROGRESS;
+}
+
+
+/*
+ *  All Encrypt Functions goes here.
+ */
+
+static int 
+xlp_aes_cbc_encrypt(struct aead_request *req)
+{
+	return aead_crypt(req, NETL_OP_ENCRYPT);
+}
+
+static int
+xlp_3des_cbc_encrypt(struct aead_request *req)
+{
+	 return aead_crypt(req, NETL_OP_ENCRYPT);
+}
+static int 
+xlp_des_cbc_encrypt(struct aead_request *req)
+{
+	return aead_crypt(req, NETL_OP_ENCRYPT);
+}
+
+static int
+xlp_aes_gcm_encrypt(struct aead_request *req)
+{
+	return aead_crypt_gcm(req, NETL_OP_ENCRYPT);
+}
+
+static int
+xlp_aes_ccm_encrypt(struct aead_request *req)
+{
+	return aead_crypt_ccm(req, NETL_OP_ENCRYPT);
+}
+
+
+static int 
+xlp_aes_ctr_encrypt(struct aead_request *req)
+{
+	return aead_crypt_ctr(req, NETL_OP_ENCRYPT);
+}
+
+/*
+ *  All Decrypt Functions goes here.
+ */
+
+static int xlp_aes_cbc_decrypt(struct aead_request *req)
+{
+	return aead_crypt(req, NETL_OP_DECRYPT);
+}
+
+static int xlp_3des_cbc_decrypt( struct aead_request *req)
+{
+	return aead_crypt_3des(req, NETL_OP_DECRYPT);
+}
+static int xlp_des_cbc_decrypt(struct aead_request *req)
+{
+	return aead_crypt(req, NETL_OP_DECRYPT);
+}
+
+static int xlp_aes_gcm_decrypt(struct aead_request *req)
+{
+	return aead_crypt_gcm(req, NETL_OP_DECRYPT);
+}
+
+static int xlp_aes_ccm_decrypt(struct aead_request *req)
+{
+	return aead_crypt_ccm(req, NETL_OP_DECRYPT);
+}
+static int xlp_aes_ctr_decrypt(struct aead_request *req)
+{
+	return aead_crypt_ctr(req, NETL_OP_DECRYPT);
+} 
+/*
+ *  All Givencrypt Functions goes here.
+ */
+
+int xlp_aes_cbc_givencrypt(struct aead_givcrypt_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(&req->areq);
+	struct nlm_aead_ctx *nlm_ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+
+	//TODO: Get the IV from random pool
+	memcpy(req->giv, nlm_ctx->iv_buf, nlm_ctx->iv_len);
+	*(uint64_t *)req->giv += ccpu_to_be64(req->seq);
+	memcpy(req->areq.iv, req->giv, nlm_ctx->iv_len);
+
+	return xlp_aes_cbc_encrypt(&req->areq);
+}
+
+static int xlp_3des_cbc_givencrypt(struct aead_givcrypt_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(&req->areq);
+	struct nlm_aead_ctx *nlm_ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+
+	memcpy(req->giv, nlm_ctx->iv_buf, nlm_ctx->iv_len);
+	*(uint64_t *)req->giv += ccpu_to_be64(req->seq);
+	memcpy(req->areq.iv, req->giv, nlm_ctx->iv_len);
+
+	return xlp_3des_cbc_encrypt(&req->areq);
+
+}
+static int xlp_des_cbc_givencrypt(struct aead_givcrypt_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(&req->areq);
+	struct nlm_aead_ctx *nlm_ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+	
+	memcpy(req->giv, nlm_ctx->iv_buf, nlm_ctx->iv_len);
+	*(uint64_t *)req->giv += ccpu_to_be64(req->seq);
+	memcpy(req->areq.iv, req->giv, nlm_ctx->iv_len);
+
+	return xlp_des_cbc_encrypt(&req->areq);
+}
+
+
+static int xlp_aes_gcm_givencrypt(struct aead_givcrypt_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(&req->areq);
+	struct nlm_aead_ctx *nlm_ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+	struct aead_request *areq = &req->areq;
+	char * req_iv = areq->iv;
+	int ret;
+
+	memcpy(req->giv, nlm_ctx->iv_buf + GCM_RFC4106_NONCE_SIZE,nlm_ctx->iv_len);
+	*(uint64_t *)req->giv += ccpu_to_be64(req->seq);
+
+	areq->iv = req->giv;
+	ret = xlp_aes_gcm_encrypt(&req->areq);
+	areq->iv = req_iv;
+	return ret;
+
+}
+
+static int xlp_aes_ccm_givencrypt(struct aead_givcrypt_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(&req->areq);
+	struct nlm_aead_ctx *nlm_ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+	struct aead_request *areq = &req->areq;
+	char * req_iv = areq->iv;
+	int ret;
+
+	memcpy(req->giv, nlm_ctx->iv_buf + CCM_RFC4309_NONCE_SIZE, nlm_ctx->iv_len);
+	*(uint64_t *)req->giv += ccpu_to_be64(req->seq);
+
+	areq->iv = req->giv;
+	ret = xlp_aes_ccm_encrypt(&req->areq);
+	areq->iv = req_iv;
+	return ret;
+
+}
+
+int xlp_aes_ctr_givencrypt(struct aead_givcrypt_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(&req->areq);
+	struct nlm_aead_ctx *nlm_ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+	void *iv = (uint8_t *)NLM_IV_OFFSET(aead_request_ctx(&req->areq));
+	memcpy(req->giv, nlm_ctx->iv_buf+CTR_RFC3686_NONCE_SIZE, nlm_ctx->iv_len);
+	*(uint64_t *)req->giv += req->seq;
+
+	memcpy(iv,  nlm_ctx->iv_buf, CTR_RFC3686_NONCE_SIZE);
+	iv += CTR_RFC3686_NONCE_SIZE;
+
+	 /*Copy IV*/
+	memcpy(iv, req->giv, CTR_RFC3686_IV_SIZE);
+	iv += CTR_RFC3686_IV_SIZE;
+
+	/*Set counter*/
+	*((uint32_t *)iv) = cpu_to_be32((uint32_t)1);
+	
+	return xlp_aes_ctr_encrypt(&req->areq);
+}
+/* commented out to avoid the search time */
+static struct crypto_alg xlp_aes_cbc_hmac_sha256_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(hmac(sha256),cbc(aes))",
+	.cra_driver_name = "authenc-hmac-sha256-cbc-aes-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = AES_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC ,
+	.cra_ctxsize = CTRL_DESC_SIZE,
+	.cra_type = &crypto_aead_type,
+	.cra_init = aead_cra_cbc_init,
+	.cra_exit = aead_session_cleanup,
+	.cra_aead = {
+		     .setkey = xlp_aes_cbc_hmac_sha256_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_aes_cbc_encrypt,
+		     .decrypt = xlp_aes_cbc_decrypt,
+		     .givencrypt = xlp_aes_cbc_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = AES_BLOCK_SIZE,
+		     .maxauthsize = SHA256_DIGEST_SIZE,
+		     }
+};
+/* commented out to avoid the search time */
+static struct crypto_alg xlp_aes_cbc_hmac_sha1_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(hmac(sha1),cbc(aes))",
+	.cra_driver_name = "authenc-hmac-sha1-cbc-aes-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = AES_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC ,
+	.cra_ctxsize = CTRL_DESC_SIZE,
+	.cra_type = &crypto_aead_type,
+	.cra_init = aead_cra_cbc_init,
+	.cra_exit = aead_session_cleanup,
+	.cra_aead = {
+		     .setkey = xlp_aes_cbc_hmac_sha1_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_aes_cbc_encrypt,
+		     .decrypt = xlp_aes_cbc_decrypt,
+		     .givencrypt = xlp_aes_cbc_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = AES_BLOCK_SIZE,
+		     .maxauthsize = SHA1_DIGEST_SIZE,
+		     }
+};
+
+static struct crypto_alg xlp_aes_cbc_aes_xcbc_mac_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(xcbc(aes),cbc(aes))",
+	.cra_driver_name = "authenc-xcbc-mac-aes-cbc-aes-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = AES_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC ,
+	.cra_ctxsize = CTRL_DESC_SIZE,
+	.cra_type = &crypto_aead_type,
+	.cra_init = aead_cra_cbc_init,
+	.cra_exit = aead_session_cleanup,
+	.cra_aead = {
+		     .setkey = xlp_aes_cbc_aes_xcbc_mac_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_aes_cbc_encrypt,
+		     .decrypt = xlp_aes_cbc_decrypt,
+		     .givencrypt = xlp_aes_cbc_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = AES_BLOCK_SIZE,
+		     .maxauthsize = XCBC_DIGEST_SIZE,
+		     }
+};
+
+static struct crypto_alg xlp_aes_cbc_hmac_md5_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(hmac(md5),cbc(aes))",
+	.cra_driver_name = "authenc-hmac-md5-cbc-aes-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = AES_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC ,
+	.cra_ctxsize = CTRL_DESC_SIZE,
+	.cra_type = &crypto_aead_type,
+	.cra_init = aead_cra_cbc_init,
+	.cra_exit = aead_session_cleanup,
+	.cra_aead = {
+		     .setkey = xlp_aes_cbc_hmac_md5_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_aes_cbc_encrypt,
+		     .decrypt = xlp_aes_cbc_decrypt,
+		     .givencrypt = xlp_aes_cbc_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = AES_BLOCK_SIZE,
+		     .maxauthsize = MD5_DIGEST_SIZE,
+		     }
+};
+
+static struct crypto_alg xlp_3des_cbc_hmac_md5_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(hmac(md5),cbc(des3_ede))",
+	.cra_driver_name = "authenc-hmac-md5-cbc-des3-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+	.cra_ctxsize = DES3_CTRL_DESC_SIZE, 
+	.cra_type = &crypto_aead_type,
+	.cra_init = aead_cra_init,
+	.cra_exit = aead_session_cleanup,
+	.cra_aead = {
+		     .setkey = xlp_3des_cbc_hmac_md5_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_3des_cbc_encrypt,
+		     .decrypt = xlp_3des_cbc_decrypt,
+		     .givencrypt = xlp_3des_cbc_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = DES3_EDE_BLOCK_SIZE,
+		     .maxauthsize = MD5_DIGEST_SIZE,
+		     }
+};
+
+static struct crypto_alg xlp_3des_cbc_hmac_sha256_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(hmac(sha256),cbc(des3_ede))",
+	.cra_driver_name = "authenc-hmac-sha256-cbc-des3-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+	.cra_ctxsize = DES3_CTRL_DESC_SIZE,
+	.cra_type = &crypto_aead_type,
+	.cra_init = aead_cra_init,
+	.cra_exit = aead_session_cleanup,
+	.cra_aead = {
+		     .setkey = xlp_3des_cbc_hmac_sha256_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_3des_cbc_encrypt,
+		     .decrypt = xlp_3des_cbc_decrypt,
+		     .givencrypt = xlp_3des_cbc_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = DES3_EDE_BLOCK_SIZE,
+		     .maxauthsize = SHA256_DIGEST_SIZE,
+		     }
+};
+
+static struct crypto_alg xlp_3des_cbc_hmac_sha1_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(hmac(sha1),cbc(des3_ede))",
+	.cra_driver_name = "authenc-hmac-sha1-cbc-des3-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+	.cra_ctxsize = DES3_CTRL_DESC_SIZE,
+	.cra_type = &crypto_aead_type,
+	.cra_init = aead_cra_init,
+	.cra_exit = aead_session_cleanup,
+	.cra_aead = {
+		     .setkey = xlp_3des_cbc_hmac_sha1_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_3des_cbc_encrypt,
+		     .decrypt = xlp_3des_cbc_decrypt,
+		     .givencrypt = xlp_3des_cbc_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = DES3_EDE_BLOCK_SIZE,
+		     .maxauthsize = SHA1_DIGEST_SIZE,
+		     }
+};
+
+static struct crypto_alg xlp_3des_cbc_aes_xcbc_mac_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(xcbc(aes),cbc(des3_ede))",
+	.cra_driver_name = "authenc-aes-xcbc-cbc-des3-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+	.cra_ctxsize = DES3_CTRL_DESC_SIZE,
+	.cra_type = &crypto_aead_type,
+	.cra_exit = aead_session_cleanup,
+	.cra_init = aead_cra_init,
+	.cra_aead = {
+		     .setkey = xlp_3des_cbc_aes_xcbc_mac_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_3des_cbc_encrypt,
+		     .decrypt = xlp_3des_cbc_decrypt,
+		     .givencrypt = xlp_3des_cbc_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = DES3_EDE_BLOCK_SIZE,
+		     .maxauthsize = XCBC_DIGEST_SIZE,
+		     }
+};
+
+
+static struct crypto_alg xlp_des_cbc_aes_xcbc_mac_cipher_auth = {
+        /* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+        .cra_name = "authenc(xcbc(aes),cbc(des))",
+        .cra_driver_name = "authenc-xcbc-mac-aes-cbc-des-xlp",
+        .cra_priority = XLP_CRYPT_PRIORITY,
+        .cra_blocksize = DES_BLOCK_SIZE,
+        .cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+        .cra_ctxsize = CTRL_DESC_SIZE,
+        .cra_type = &crypto_aead_type,
+        .cra_init = aead_cra_cbc_init,
+        .cra_exit = aead_session_cleanup,
+        .cra_aead = {
+                     .setkey = xlp_des_cbc_aes_xcbc_mac_setkey,
+                     .setauthsize = aead_setauthsize,
+                     .encrypt = xlp_des_cbc_encrypt,
+                     .decrypt = xlp_des_cbc_decrypt,
+                     .givencrypt = xlp_des_cbc_givencrypt,
+                     .geniv = "<built-in>",
+                     .ivsize = DES_BLOCK_SIZE,
+                     .maxauthsize = MD5_DIGEST_SIZE,
+                     }
+};
+static struct crypto_alg xlp_des_cbc_hmac_md5_cipher_auth = {
+        /* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+        .cra_name = "authenc(hmac(md5),cbc(des))",
+        .cra_driver_name = "authenc-hmac-md5-cbc-des-xlp",
+        .cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_alignmask = 0,
+        .cra_blocksize = DES_BLOCK_SIZE,
+        .cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+        .cra_ctxsize = CTRL_DESC_SIZE,
+        .cra_type = &crypto_aead_type,
+        .cra_init = aead_cra_cbc_init,
+        .cra_exit = aead_session_cleanup,
+        .cra_aead = {
+                     .setkey = xlp_des_cbc_hmac_md5_setkey,
+                     .setauthsize = aead_setauthsize,
+                     .encrypt = xlp_des_cbc_encrypt,
+                     .decrypt = xlp_des_cbc_decrypt,
+                     .givencrypt = xlp_des_cbc_givencrypt,
+                     .geniv = "<built-in>",
+                     .ivsize = DES_BLOCK_SIZE,
+                     .maxauthsize = MD5_DIGEST_SIZE,
+                     }
+};
+
+static struct crypto_alg xlp_des_cbc_hmac_sha256_cipher_auth = {
+        /* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+        .cra_name = "authenc(hmac(sha256),cbc(des))",
+        .cra_driver_name = "authenc-hmac-sha256-cbc-des-xlp",
+        .cra_priority = XLP_CRYPT_PRIORITY,
+        .cra_blocksize = DES_BLOCK_SIZE,
+	.cra_alignmask = 0,
+        .cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+        .cra_ctxsize = CTRL_DESC_SIZE,
+        .cra_type = &crypto_aead_type,
+        .cra_init = aead_cra_cbc_init,
+        .cra_exit = aead_session_cleanup,
+        .cra_aead = {
+                     .setkey = xlp_des_cbc_hmac_sha256_setkey,
+                     .setauthsize = aead_setauthsize,
+                     .encrypt = xlp_des_cbc_encrypt,
+                     .decrypt = xlp_des_cbc_decrypt,
+                     .givencrypt = xlp_des_cbc_givencrypt,
+                     .geniv = "<built-in>",
+                     .ivsize = DES_BLOCK_SIZE,
+                     .maxauthsize = SHA256_DIGEST_SIZE,
+                     }
+};
+static struct crypto_alg xlp_des_cbc_hmac_sha1_cipher_auth = {
+        /* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+        .cra_name = "authenc(hmac(sha1),cbc(des))",
+        .cra_driver_name = "authenc-hmac-sha1-cbc-des-xlp",
+        .cra_priority = XLP_CRYPT_PRIORITY,
+        .cra_blocksize = DES_BLOCK_SIZE,
+	.cra_alignmask = 0,
+        .cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+        .cra_ctxsize = CTRL_DESC_SIZE,
+        .cra_type = &crypto_aead_type,
+        .cra_exit = aead_session_cleanup,
+        .cra_init = aead_cra_cbc_init,
+        .cra_aead = {
+                     .setkey = xlp_des_cbc_hmac_sha1_setkey,
+                     .setauthsize = aead_setauthsize,
+                     .encrypt = xlp_des_cbc_encrypt,
+                     .decrypt = xlp_des_cbc_decrypt,
+                     .givencrypt = xlp_des_cbc_givencrypt,
+                     .geniv = "<built-in>",
+                     .ivsize = DES_BLOCK_SIZE,
+                     .maxauthsize = SHA1_DIGEST_SIZE,
+                     }
+};
+
+static struct crypto_alg xlp_aes_gcm_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "rfc4106(gcm(aes))",
+	.cra_driver_name = "rfc4106-gcm-aes-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = AES_BLOCK_SIZE,
+	.cra_alignmask = 0,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+	.cra_type = &crypto_aead_type,
+	.cra_exit = aead_session_cleanup,
+        .cra_ctxsize = CTRL_DESC_SIZE,
+        .cra_init = aead_cra_init_gcm,
+	.cra_aead = {
+		.setkey = aead_gcm_rfc4106_setkey,
+		.setauthsize = aead_setauthsize,
+		.encrypt = xlp_aes_gcm_encrypt,
+		.decrypt = xlp_aes_gcm_decrypt,
+		.givencrypt = xlp_aes_gcm_givencrypt,
+		.geniv = "seqiv", 
+		.ivsize = GCM_RFC4106_IV_SIZE,
+		.maxauthsize = GCM_RFC4106_DIGEST_SIZE,
+	}
+};
+
+static struct crypto_alg xlp_aes_ccm_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "rfc4309(ccm(aes))",
+	.cra_driver_name = "rfc4309-ccm-aes-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = AES_BLOCK_SIZE,
+	.cra_alignmask = 0,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+	.cra_type = &crypto_aead_type,
+        .cra_ctxsize = CTRL_DESC_SIZE,
+        .cra_init = aead_cra_init_ccm,
+	.cra_aead = {
+		.setkey = aead_ccm_rfc4309_setkey,
+		.setauthsize = aead_setauthsize,
+		.encrypt = xlp_aes_ccm_encrypt,
+		.decrypt = xlp_aes_ccm_decrypt,
+		.givencrypt = xlp_aes_ccm_givencrypt,
+		.geniv = "seqiv", 
+		.ivsize = CCM_RFC4309_IV_SIZE,
+		.maxauthsize = CCM_RFC4309_DIGEST_SIZE,
+	}
+};
+
+static struct crypto_alg xlp_aes_ctr_hmac_sha1_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(hmac(sha1),rfc3686(ctr(aes)))",
+	.cra_driver_name = "authenc-hmac-sha1-ctr-aes-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = 1,
+	.cra_alignmask = 0,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC ,
+	.cra_ctxsize = CTRL_DESC_SIZE,
+	.cra_type = &crypto_aead_type,
+	.cra_init = aead_cra_init,
+	.cra_exit = aead_session_cleanup,
+	.cra_aead = {
+		     .setkey = xlp_aes_ctr_hmac_sha1_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_aes_ctr_encrypt,
+		     .decrypt = xlp_aes_ctr_decrypt,
+		     .givencrypt = xlp_aes_ctr_givencrypt,
+		     .geniv = "seqiv", 
+		     .ivsize = CTR_RFC3686_IV_SIZE,
+		     .maxauthsize = SHA1_DIGEST_SIZE,
+		     }
+};
+
+static struct crypto_alg xlp_aes_ctr_hmac_sha256_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(hmac(sha256),rfc3686(ctr(aes)))",
+	.cra_driver_name = "authenc-hmac-sha256-ctr-aes-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = 1,
+	.cra_alignmask = 0,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC ,
+	.cra_ctxsize = CTRL_DESC_SIZE,
+	.cra_type = &crypto_aead_type,
+	.cra_init = aead_cra_init,
+	.cra_exit = aead_session_cleanup,
+	.cra_aead = {
+		     .setkey = xlp_aes_ctr_hmac_sha256_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_aes_ctr_encrypt,
+		     .decrypt = xlp_aes_ctr_decrypt,
+		     .givencrypt = xlp_aes_ctr_givencrypt,
+		     .geniv = "seqiv", 
+		     .ivsize = CTR_RFC3686_IV_SIZE,
+		     .maxauthsize = SHA256_DIGEST_SIZE,
+		     }
+};
+static struct crypto_alg xlp_aes_ctr_aes_xcbc_mac_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(xcbc(aes),rfc3686(ctr(aes)))",
+	.cra_driver_name = "authenc-aes-xcbc-mac-ctr-aes-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = 1,
+	.cra_alignmask = 0,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC ,
+	.cra_ctxsize = CTRL_DESC_SIZE,
+	.cra_type = &crypto_aead_type,
+	.cra_init = aead_cra_init,
+	.cra_aead = {
+		     .setkey = xlp_aes_ctr_aes_xcbc_mac_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_aes_ctr_encrypt,
+		     .decrypt = xlp_aes_ctr_decrypt,
+		     .givencrypt = xlp_aes_ctr_givencrypt,
+		     .geniv = "seqiv", 
+		     .ivsize = CTR_RFC3686_IV_SIZE,
+		     .maxauthsize = XCBC_DIGEST_SIZE,
+		     }
+};
+
+static struct crypto_alg xlp_aes_ctr_hmac_md5_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(hmac(md5),rfc3686(ctr(aes)))",
+	.cra_driver_name = "authenc-hmac-sha1-ctr-aes-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = 1,
+	.cra_alignmask = 0,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC ,
+	.cra_ctxsize = CTRL_DESC_SIZE,
+	.cra_type = &crypto_aead_type,
+	.cra_init = aead_cra_init,
+	.cra_aead = {
+		     .setkey = xlp_aes_ctr_hmac_md5_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_aes_ctr_encrypt,
+		     .decrypt = xlp_aes_ctr_decrypt,
+		     .givencrypt = xlp_aes_ctr_givencrypt,
+		     .geniv = "seqiv", 
+		     .ivsize = CTR_RFC3686_IV_SIZE,
+		     .maxauthsize = MD5_DIGEST_SIZE,
+		     }
+};
+
+int xlp_aead_alg_init(void)
+{
+	int ret = 0;
+
+	if ((ret = crypto_register_alg(&xlp_aes_cbc_hmac_sha256_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+
+	if ((ret = crypto_register_alg(&xlp_aes_cbc_hmac_sha1_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+
+	if ((ret = crypto_register_alg(&xlp_aes_cbc_aes_xcbc_mac_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+
+	if ((ret = crypto_register_alg(&xlp_aes_cbc_hmac_md5_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+
+	if ((ret = crypto_register_alg(&xlp_3des_cbc_hmac_md5_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+
+	if ((ret = crypto_register_alg(&xlp_3des_cbc_hmac_sha256_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+
+	if ((ret = crypto_register_alg(&xlp_3des_cbc_hmac_sha1_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+
+	if ((ret = crypto_register_alg(&xlp_3des_cbc_aes_xcbc_mac_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+	if (( ret = crypto_register_alg(&xlp_des_cbc_hmac_md5_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+	if ( (ret = crypto_register_alg(&xlp_des_cbc_hmac_sha256_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+	if ((ret =  crypto_register_alg(&xlp_des_cbc_hmac_sha1_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+	if ((ret =  crypto_register_alg(&xlp_des_cbc_aes_xcbc_mac_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+	if((ret = crypto_register_alg(&xlp_aes_gcm_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+	if ((ret = crypto_register_alg(&xlp_aes_ccm_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+
+	if ((ret = crypto_register_alg(&xlp_aes_ctr_hmac_sha1_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+
+	if ((ret = crypto_register_alg(&xlp_aes_ctr_hmac_sha256_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+
+	if ((ret = crypto_register_alg(&xlp_aes_ctr_aes_xcbc_mac_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+
+	if ((ret = crypto_register_alg(&xlp_aes_ctr_hmac_md5_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+end:
+	return no_of_alg_registered;
+} 
+
+	void
+xlp_aead_alg_fini(void)
+{
+	crypto_unregister_alg(&xlp_aes_ctr_hmac_md5_cipher_auth);
+	crypto_unregister_alg(&xlp_aes_ctr_aes_xcbc_mac_cipher_auth);
+	crypto_unregister_alg(&xlp_aes_ctr_hmac_sha256_cipher_auth);
+	crypto_unregister_alg(&xlp_aes_ctr_hmac_sha1_cipher_auth);
+	crypto_unregister_alg(&xlp_aes_ccm_cipher_auth);
+	crypto_unregister_alg(&xlp_aes_gcm_cipher_auth);
+	crypto_unregister_alg(&xlp_des_cbc_aes_xcbc_mac_cipher_auth);
+	crypto_unregister_alg(&xlp_des_cbc_hmac_sha1_cipher_auth);
+	crypto_unregister_alg(&xlp_des_cbc_hmac_sha256_cipher_auth);
+	crypto_unregister_alg(&xlp_des_cbc_hmac_md5_cipher_auth);
+	crypto_unregister_alg(&xlp_3des_cbc_aes_xcbc_mac_cipher_auth);
+	crypto_unregister_alg(&xlp_3des_cbc_hmac_sha1_cipher_auth);
+	crypto_unregister_alg(&xlp_3des_cbc_hmac_sha256_cipher_auth);
+	crypto_unregister_alg(&xlp_3des_cbc_hmac_md5_cipher_auth);
+	crypto_unregister_alg(&xlp_aes_cbc_hmac_md5_cipher_auth);
+	crypto_unregister_alg(&xlp_aes_cbc_aes_xcbc_mac_cipher_auth);
+	crypto_unregister_alg(&xlp_aes_cbc_hmac_sha1_cipher_auth);
+	crypto_unregister_alg(&xlp_aes_cbc_hmac_sha256_cipher_auth);
+}
+
+EXPORT_SYMBOL(xlp_aead_alg_init);
+EXPORT_SYMBOL(xlp_aead_alg_fini);
diff --git a/drivers/crypto/sae/nlm_async.h b/drivers/crypto/sae/nlm_async.h
new file mode 100644
index 0000000..2dbbed0
--- /dev/null
+++ b/drivers/crypto/sae/nlm_async.h
@@ -0,0 +1,92 @@
+/***********************************************************************
+Copyright 2003-2010 Netlogic Microsystems (“Netlogic”). All rights
+reserved.
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are
+met:
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+*****************************#NETL_2#********************************/
+#ifndef __NLM_ASYNC_H
+#define __NLM_ASYNC_H
+
+#include <crypto/scatterwalk.h>
+#include "nlmcrypto.h"
+
+struct nlm_async_crypto;
+#define MAX_CPU 128
+#define NODE_ID_SHIFT_BIT 5
+#define NODE_BASE_SHIFT_BIT 10
+
+extern int crypto_get_fb_vc(int * node);
+
+struct nlm_async_crypto
+{
+	void (*callback) (struct nlm_async_crypto *args, uint64_t entry1);
+	void *args;
+	int op;
+	int authsize;
+	uint8_t *hash_addr;
+	uint8_t * pkt_param;
+	struct scatterlist * src;
+	struct scatterlist * dst;
+	uint16_t stat;
+	uint32_t bytes;
+};
+
+enum enc_stat {
+	DES_CBC_STAT = 0 ,
+	TDES_CBC_STAT ,
+	AES128_CBC_STAT ,
+	AES192_CBC_STAT ,
+	AES256_CBC_STAT, 
+	AES128_CTR_STAT,
+	AES192_CTR_STAT,
+	AES256_CTR_STAT,
+	ENC_MAX_STAT
+};
+enum {
+	MD5_STAT,
+	H_SHA1_STAT,
+	H_SHA256_STAT,
+	AES128_XCBC_STAT,
+	AES192_XCBC_STAT,
+	AES256_XCBC_STAT,
+	GCM_STAT,
+	CCM_STAT,
+	AUTH_MAX_STAT
+};
+
+struct nlm_crypto_stat
+{
+	uint64_t enc[ENC_MAX_STAT];
+	uint64_t enc_tbytes[ENC_MAX_STAT];
+	uint64_t auth[AUTH_MAX_STAT];
+	uint64_t auth_tbytes[AUTH_MAX_STAT];
+};
+
+extern int crypto_vc_base;
+extern int crypto_vc_limit;
+extern int nlm_crypto_sae_num_seg_reqd(void *data, unsigned int buflen);
+extern int nlm_crypto_calc_rem_len(struct scatterlist *sg, unsigned int cipher_len);
+extern int alloc_pkt_param( struct nlm_async_crypto * async,struct nlm_crypto_pkt_param ** pkt_param , int max_frags);
+
+extern int fill_src_dst_sg(struct scatterlist *src_sg, struct scatterlist *dst_sg, unsigned int cipher_len, 
+		struct nlm_crypto_pkt_param *param, 
+		int seg, unsigned int max_frags, int op);
+#endif
diff --git a/drivers/crypto/sae/nlm_auth.c b/drivers/crypto/sae/nlm_auth.c
new file mode 100644
index 0000000..e85b1f8
--- /dev/null
+++ b/drivers/crypto/sae/nlm_auth.c
@@ -0,0 +1,671 @@
+/***********************************************************************
+Copyright 2003-2010 Netlogic Microsystems (“Netlogic”). All rights
+reserved.
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are
+met:
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+*****************************#NETL_2#********************************/
+#include <linux/module.h>
+#include <crypto/sha.h>
+#include <nlm_hal_fmn.h>
+#include <crypto/aes.h>
+#include <crypto/internal/hash.h>
+#include "nlm_async.h"
+#include <asm/netlogic/msgring.h>
+
+#define XLP_AUTH_PRIORITY      300
+#define XLP_HMAC_PRIORITY      300
+
+#define XCBC_DIGEST_SIZE	16
+
+#define MD5_DIGEST_SIZE		16
+#define MD5_BLOCK_SIZE		64
+
+//#define AUTH_BUFFER_SIZE	(16 * 1024)
+void hex_dump(char * description,unsigned char *in, int num);
+extern int auth_mode_key_len[NLM_HASH_MAX][NLM_HASH_MODE_MAX];
+
+#define ASYNC_PTR_SIZE 128
+#define ASYNC_PTR_OFFSET (sizeof(struct auth_pkt_desc ) + (NLM_AUTH_MAX_FRAGS* 16))
+
+//#define SEC_DEBUG
+
+
+#ifdef SEC_DEBUG
+#ifdef __KERNEL__
+#define debug_print(fmt, args...) printk(fmt, ##args)
+#else				/* __KERNEL__ */
+#define debug_print(fmt, args...) printf(fmt, ##args)
+#endif				/* __KERNEL__ */
+#else				/* SEC_DEBUG */
+#define debug_print(fmt, args...)
+#endif				/* SEC_DEBUG */
+
+#define malloc(a) kmalloc(a, GFP_KERNEL)
+#define free kfree
+#define NLM_AUTH_MAX_FRAGS	(20)
+
+#define MAX_AUTH_DATA 64000
+
+
+struct nlm_auth_ctx
+{
+	struct nlm_crypto_pkt_ctrl ctrl;
+	uint16_t stat;
+	uint8_t hashed_key[128];
+	struct crypto_shash * fallback_tfm;
+	uint8_t data[MAX_AUTH_DATA];
+	/*Don't change the order of this strucutre*/
+};
+
+#define MAX_FRAGS               18
+#define CTRL_DESC_SIZE          (sizeof(struct nlm_auth_ctx) + 64)
+
+struct auth_pkt_desc
+{
+	uint16_t curr_index;
+	uint16_t total_len;
+	uint16_t stat;
+	uint16_t is_allocated;
+	int max_frags;
+	struct shash_desc * fallback;
+	uint8_t  * alloc_pkt_param;
+	struct nlm_crypto_pkt_param * pkt_param; /* maintain at the end */ 
+};
+
+#define PACKET_DESC_SIZE   (64+sizeof(struct auth_pkt_desc) + MAX_FRAGS*(2*8) ) /* should be less than PAGE_SIZE/8 */ 
+#define NLM_CRYPTO_PKT_PARAM_OFFSET(addr)       (((unsigned long)addr + 64) & ~0x3fUL)
+#define NLM_ASYNC_PTR_PARAM_OFFSET(addr)        (((unsigned long)(addr + 64 + sizeof(struct nlm_crypto_pkt_param) + MAX_FRAGS*(2*8)) + 64) & ~0x3fUL)
+/*
+   All extern declaration goes here.
+ */
+extern uint32_t nlm_hal_send_msg3(uint32_t dst, uint32_t code, uint64_t data0, uint64_t data1, uint64_t data2);
+extern uint32_t nlm_hal_recv_msg2(uint32_t dst, uint32_t *src, uint32_t *size, uint32_t *code, uint64_t *data0, uint64_t *data1);
+extern void print_cntl_instr(uint64_t cntl_desc);
+extern void print_crypto_msg_desc(uint64_t entry1, uint64_t entry2, uint64_t entry3);
+extern void print_pkt_desc(struct nlm_crypto_pkt_param * pkt_param, int index);
+extern struct nlm_crypto_stat crypto_stat[MAX_CPU];
+
+
+static inline void print_info(const char *func)
+{
+	extern void dump_stack(void);
+	printk("\n********[%s function called]**********\n",func);
+	dump_stack();
+	printk("\n*********[%s Dumpstack ends]***********\n\n\n",func);
+	return;
+}
+#ifdef NLM_CRYPTO_DEBUG
+static void print_buf(unsigned char *msg, unsigned char *buf, int len)
+{
+#define TMP_BUF		50
+	char tmp_buf[TMP_BUF + 1];
+	int i, index = 0;
+
+	printk("**********%s************\n",msg);
+	for(i=0; i<len; i++){
+		sprintf(&tmp_buf[index*2], "%02x", buf[i]);
+		index++;
+		if(index == (TMP_BUF/2)){
+			tmp_buf[index*2] = '\0';
+			printk("[%s]\n",tmp_buf);
+			index = 0;
+		}
+	}
+	if(index){
+		tmp_buf[index*2] = '\0';
+		printk("[%s]\n",tmp_buf);
+	}
+}
+#endif
+
+static struct nlm_auth_ctx *nlm_shash_auth_ctx(struct crypto_shash *shash)
+{
+	uint8_t *ctx = crypto_tfm_ctx(crypto_shash_tfm(shash));
+	ctx = (uint8_t *)(((unsigned long)ctx + 63) & ~(0x3fUL));
+	return (struct nlm_auth_ctx *)ctx;
+}
+
+static struct nlm_auth_ctx *pkt_ctrl_auth_ctx(struct shash_desc *desc)
+{
+	return nlm_shash_auth_ctx(desc->tfm);
+}
+
+static int
+xlp_cra_xcbc_init(struct crypto_tfm *tfm)
+{
+	struct crypto_shash * hash = __crypto_shash_cast(tfm);
+	struct nlm_auth_ctx * nlm_ctx = nlm_shash_auth_ctx(hash);
+	nlm_ctx->fallback_tfm = crypto_alloc_shash("xcbc(aes-generic)",CRYPTO_ALG_TYPE_SHASH ,0 );
+	return 0;
+}
+
+static int
+xlp_cra_hmac_sha1_init(struct crypto_tfm *tfm)
+{
+	struct crypto_shash * hash = __crypto_shash_cast(tfm);
+	struct nlm_auth_ctx * nlm_ctx = nlm_shash_auth_ctx(hash);
+	nlm_ctx->fallback_tfm = crypto_alloc_shash("hmac(sha1-generic)",CRYPTO_ALG_TYPE_SHASH ,0 );
+	return 0;
+}
+static int
+xlp_cra_hmac_sha256_init(struct crypto_tfm *tfm)
+{
+	struct crypto_shash * hash = __crypto_shash_cast(tfm);
+	struct nlm_auth_ctx * nlm_ctx = nlm_shash_auth_ctx(hash);
+	nlm_ctx->fallback_tfm = crypto_alloc_shash("hmac(sha256-generic)",CRYPTO_ALG_TYPE_SHASH ,0 );
+	return 0;
+}
+
+static int
+xlp_cra_md5_init(struct crypto_tfm *tfm)
+{
+	struct crypto_shash * hash = __crypto_shash_cast(tfm);
+	struct nlm_auth_ctx * nlm_ctx = nlm_shash_auth_ctx(hash);
+	nlm_ctx->fallback_tfm = crypto_alloc_shash("hmac(md5-generic)",CRYPTO_ALG_TYPE_SHASH ,0 );
+	
+	return 0;
+}
+static int
+xlp_auth_init(struct shash_desc *desc)
+{
+	struct auth_pkt_desc *auth_pkt_desc = (struct auth_pkt_desc * )shash_desc_ctx(desc);
+	auth_pkt_desc->curr_index = 0;
+	auth_pkt_desc->total_len = 0;
+	auth_pkt_desc->fallback = NULL;
+	auth_pkt_desc->pkt_param = ( struct nlm_crypto_pkt_param  * )NLM_CRYPTO_PKT_PARAM_OFFSET((unsigned long )auth_pkt_desc + sizeof(struct auth_pkt_desc ));
+	auth_pkt_desc->max_frags = MAX_FRAGS;
+	auth_pkt_desc->is_allocated = 0;
+	return 0;
+}
+static int
+xlp_auth_update(struct shash_desc *desc,
+		const uint8_t * data, unsigned int length)
+{
+	struct auth_pkt_desc *auth_pkt_desc = (struct auth_pkt_desc *)shash_desc_ctx(desc);
+	struct nlm_auth_ctx * nlm_ctx = nlm_shash_auth_ctx(desc->tfm);
+	int index = auth_pkt_desc->curr_index;
+	unsigned char * data_index = &nlm_ctx->data[auth_pkt_desc->total_len];
+	struct nlm_crypto_pkt_param  *pkt_param  = auth_pkt_desc->pkt_param;
+
+	auth_pkt_desc->total_len += length;
+	if  (auth_pkt_desc->total_len >= MAX_AUTH_DATA)  {
+		if (auth_pkt_desc->fallback == NULL ) {
+			auth_pkt_desc->fallback = kmalloc((sizeof(struct shash_desc ) + 
+				crypto_shash_descsize(nlm_ctx->fallback_tfm))
+									,GFP_KERNEL);
+			auth_pkt_desc->fallback->tfm = nlm_ctx->fallback_tfm;
+			auth_pkt_desc->fallback->flags = desc->flags & CRYPTO_TFM_REQ_MAY_SLEEP;
+			crypto_shash_init(auth_pkt_desc->fallback);
+			auth_pkt_desc->total_len -= length;
+			crypto_shash_update(auth_pkt_desc->fallback,nlm_ctx->data,auth_pkt_desc->total_len);
+			auth_pkt_desc->total_len += length;
+		}
+		crypto_shash_update(auth_pkt_desc->fallback,data,length);
+		
+		return 0;
+	}
+
+	memcpy(data_index,data,length);
+	if ( (auth_pkt_desc->curr_index + 1 ) > auth_pkt_desc->max_frags ) {
+		uint8_t * mem = kmalloc(sizeof (struct nlm_crypto_pkt_param) 
+					+ ((auth_pkt_desc->max_frags + MAX_FRAGS) * 2 * 8 ) + 64 , GFP_KERNEL);
+		auth_pkt_desc->alloc_pkt_param = mem;
+		mem = (uint8_t *)NLM_CRYPTO_PKT_PARAM_OFFSET((unsigned long )mem);
+		memcpy(mem,pkt_param,sizeof (struct nlm_crypto_pkt_param)+(auth_pkt_desc->max_frags * 2 * 8));
+		
+		if ( auth_pkt_desc->is_allocated )
+			kfree(auth_pkt_desc->alloc_pkt_param);
+		pkt_param = auth_pkt_desc->pkt_param = ( struct nlm_crypto_pkt_param  * )mem;
+		auth_pkt_desc->is_allocated = 0;
+		auth_pkt_desc->max_frags += MAX_FRAGS;
+	} 
+		
+	auth_pkt_desc->curr_index = nlm_crypto_fill_src_dst_seg(pkt_param, index , auth_pkt_desc->max_frags, data_index, length);
+
+	return 0;
+}
+static int
+crypto_get_sync_fb_vc(int * node)
+{
+    int cpu;
+    int node_id;
+    extern int ipsec_sync_vc;
+
+    cpu = hard_smp_processor_id();      //processor_id();
+    node_id = (cpu >> NODE_ID_SHIFT_BIT);
+    cpu = (node_id << NODE_BASE_SHIFT_BIT) | (((cpu & 0x1f) * 4) + ipsec_sync_vc);
+    *node = node_id;
+
+    return cpu;
+}
+
+static int
+xlp_auth_final(struct shash_desc *desc, uint8_t *out)
+{
+	struct auth_pkt_desc *auth_pkt_desc = (struct auth_pkt_desc *)shash_desc_ctx(desc);
+	int index = auth_pkt_desc->curr_index;
+	struct nlm_crypto_pkt_param  *pkt_param  = auth_pkt_desc->pkt_param;
+	struct nlm_auth_ctx  * auth_ctx   = pkt_ctrl_auth_ctx(desc);
+	int fb_vc ;
+	uint64_t entry0, entry1, tx_id=0x12345678ULL;
+	uint64_t  timeout = 0;
+	struct nlm_crypto_pkt_ctrl *ctrl = &auth_ctx->ctrl;
+	uint32_t size,code,src;
+	uint16_t stat = auth_ctx->stat;
+	int cpu = hard_smp_processor_id();
+        extern int ipsec_sync_vc;
+#ifdef CONFIG_32BIT
+	unsigned long msgrng_flags;
+#endif
+	int node_sae_base;
+	int node;
+
+	if ( (auth_pkt_desc->total_len == 0 ) ||  ( auth_pkt_desc->total_len > MAX_AUTH_DATA)) { 
+
+		if (auth_pkt_desc->fallback == NULL ) {
+			auth_pkt_desc->fallback = kmalloc((sizeof(struct shash_desc ) + 
+				crypto_shash_descsize(auth_ctx->fallback_tfm))
+									,GFP_KERNEL);
+			auth_pkt_desc->fallback->tfm = auth_ctx->fallback_tfm;
+			auth_pkt_desc->fallback->flags = desc->flags & CRYPTO_TFM_REQ_MAY_SLEEP;
+			crypto_shash_init(auth_pkt_desc->fallback);
+		}
+		crypto_shash_final(auth_pkt_desc->fallback,out);
+		return 0;
+	}
+	if ( auth_pkt_desc->fallback != NULL ) {
+		kfree(auth_pkt_desc->fallback);
+		auth_pkt_desc->fallback = NULL;
+	}
+		
+	nlm_crypto_fill_auth_pkt_param(ctrl,pkt_param,
+			0,auth_pkt_desc->total_len,0,out); 
+
+	preempt_disable();
+#ifdef CONFIG_32BIT
+	msgrng_access_enable(msgrng_flags);
+#endif
+
+	fb_vc = crypto_get_sync_fb_vc(&node);
+	entry0 = nlm_crypto_form_pkt_fmn_entry0(fb_vc, 0, 0, 0, virt_to_phys(ctrl));
+	entry1 = nlm_crypto_form_pkt_fmn_entry1(0, ctrl->hashkeylen, (32 + index * 16 ), virt_to_phys(pkt_param));
+	node_sae_base = (node << NODE_BASE_SHIFT_BIT) | crypto_vc_base;
+
+	while (nlm_hal_send_msg3(node_sae_base, 0 /*code */ , entry0, entry1, tx_id) != 0 );
+
+#ifdef NLM_CRYPTO_DEBUG
+	print_crypto_msg_desc(entry0, entry1, tx_id);
+	print_cntl_instr(ctrl->desc0);
+	print_pkt_desc(pkt_param,index);
+#endif
+
+	//construct pkt, send to engine and receive reply
+	timeout = 0;
+	do {
+		timeout++;
+		nlm_hal_recv_msg2(ipsec_sync_vc, &src, &size, &code, &entry0, &entry1);
+	} while(entry0 != tx_id && timeout < 0xffffffffULL) ;
+	
+
+	if (timeout >= 0xffffffffULL) {
+		printk("\nError: FreeBack message is not received");
+#ifdef CONFIG_32BIT
+		msgrng_access_disable(msgrng_flags);
+#endif
+		if ( auth_pkt_desc->is_allocated )
+			kfree(auth_pkt_desc->pkt_param);
+		preempt_enable();
+		return -EAGAIN;
+	}
+
+#ifdef NLM_CRYPTO_DEBUG
+	print_buf("AUTH:", out, 16);
+#endif
+#ifdef CONFIG_32BIT
+	msgrng_access_disable(msgrng_flags);
+#endif
+	preempt_enable();
+	if ( auth_pkt_desc->is_allocated )
+		kfree(auth_pkt_desc->pkt_param);
+	crypto_stat[cpu].auth[stat] ++;
+	crypto_stat[cpu].auth_tbytes[stat] += auth_pkt_desc->total_len + ctrl->taglen;
+ 	return 0;
+}
+static int xlp_auth_export(struct shash_desc *desc, void *out)
+{
+	struct auth_pkt_desc *auth_pkt_desc = (struct auth_pkt_desc *)shash_desc_ctx(desc);
+	if ( auth_pkt_desc->fallback == NULL )
+		return 0;
+	return crypto_shash_export(auth_pkt_desc->fallback,out);
+}
+
+static int xlp_auth_import(struct shash_desc *desc, const void *in)
+{
+	struct auth_pkt_desc *auth_pkt_desc = (struct auth_pkt_desc *)shash_desc_ctx(desc);
+	if ( auth_pkt_desc->fallback == NULL )
+		return 0;
+	return crypto_shash_import(auth_pkt_desc->fallback, in);
+}
+
+
+/*
+   All Setkey goes here.
+ */
+
+int hash_key(int alg, int mode, const uint8_t * key, unsigned int keylen, uint8_t * new_key)
+{
+
+	int fb_vc;
+	uint64_t entry0,entry1;
+	uint32_t size,code,src;
+        extern int ipsec_sync_vc;
+	uint64_t tx_id=0x12345678ULL;
+	uint64_t  timeout = 0;
+#ifdef CONFIG_32BIT
+	unsigned long msgrng_flags;
+#endif
+	int node_sae_base;
+	int node;
+	char * tmp = kmalloc(keylen + sizeof(struct nlm_crypto_pkt_ctrl) + sizeof( struct nlm_crypto_pkt_param ) 
+											+ 128, GFP_KERNEL);
+	struct nlm_crypto_pkt_ctrl * ctrl = (struct nlm_crypto_pkt_ctrl * ) ((((unsigned long)tmp + 63)) & ~(0x3fUL)); 
+	struct nlm_crypto_pkt_param * pkt_param = (struct nlm_crypto_pkt_param * )
+		(((unsigned long) ctrl + sizeof(struct nlm_crypto_pkt_ctrl) + 63) & ~(0x3fUL));
+	char *tmp_key = (char *)(((unsigned long) pkt_param + 
+			sizeof(struct nlm_crypto_pkt_param ) + 63)  & ~(0x3fUL));
+
+	memcpy(tmp_key,key,keylen);
+        nlm_crypto_fill_pkt_ctrl(ctrl,0,alg,mode,0,0,0,NULL,0,NULL,0);
+        nlm_crypto_fill_auth_pkt_param(ctrl,pkt_param,0,keylen,0,new_key);
+        nlm_crypto_fill_src_dst_seg(pkt_param,0,MAX_FRAGS,tmp_key,keylen);
+
+
+#ifdef CONFIG_32BIT
+	msgrng_access_enable(msgrng_flags);
+#endif
+        fb_vc = crypto_get_sync_fb_vc(&node);
+        entry0 = nlm_crypto_form_pkt_fmn_entry0(fb_vc, 0, 0, 0, virt_to_phys(ctrl));
+        entry1 = nlm_crypto_form_pkt_fmn_entry1(0, 0, (32 + 16 ), virt_to_phys(pkt_param));
+
+
+#ifdef NLM_CRYPTO_DEBUG
+       print_crypto_msg_desc(entry0, entry1, tx_id);
+       print_cntl_instr(ctrl->desc0);
+       print_pkt_desc(pkt_param,1);
+#endif
+
+        //construct pkt, send to engine and receive reply
+	node_sae_base = (node << NODE_BASE_SHIFT_BIT) | crypto_vc_base;
+
+	while (nlm_hal_send_msg3(node_sae_base, 0 /*code */ , entry0, entry1, tx_id) != 0 );
+        timeout = 0;
+        do {
+                timeout++;
+                nlm_hal_recv_msg2(ipsec_sync_vc, &src, &size, &code, &entry0, &entry1);
+        } while(entry0 != tx_id && timeout < 0xfffffUL) ;
+	kfree(tmp);
+#ifdef CONFIG_32BIT
+	msgrng_access_disable(msgrng_flags);
+#endif
+	return 0;
+
+	
+
+}
+
+static int xlp_auth_aes_xcbc_setkey(struct crypto_shash *tfm, const u8 * key, unsigned int keylen)
+{
+	struct nlm_auth_ctx * nlm_ctx = nlm_shash_auth_ctx(tfm);
+	uint32_t hash_alg = NLM_HASH_AES128;
+
+        switch (keylen) {
+        case 16:
+                hash_alg = NLM_HASH_AES128;
+		nlm_ctx->stat = AES128_XCBC_STAT;
+                break;
+        case 24:
+                hash_alg = NLM_HASH_AES192;
+		nlm_ctx->stat = AES192_XCBC_STAT;
+                break;
+        case 32:
+                hash_alg = NLM_HASH_AES256;
+		nlm_ctx->stat = AES256_XCBC_STAT;
+                break;
+        default:
+                printk(KERN_WARNING "[%s]: Cannot handle keylen = %d\n",
+                       __FUNCTION__, keylen);
+	}
+
+	/*setup ctrl descriptor*/
+	nlm_crypto_fill_pkt_ctrl(&nlm_ctx->ctrl,0,hash_alg,NLM_HASH_MODE_XCBC,
+		NLM_CIPHER_BYPASS,NLM_CIPHER_MODE_ECB,0,NULL,0,(unsigned char *)key,keylen);
+	
+	crypto_shash_setkey(nlm_ctx->fallback_tfm,key,keylen);
+
+	
+	return 0;
+	
+}
+
+
+static int
+xlp_auth_hmac_sha256_setkey(struct crypto_shash *tfm, const u8 * key, unsigned int keylen)
+{
+	struct nlm_auth_ctx * nlm_ctx = nlm_shash_auth_ctx(tfm);
+	struct nlm_crypto_pkt_ctrl * ctrl = &nlm_ctx->ctrl; 
+	
+	nlm_ctx->stat = H_SHA256_STAT;
+	if ( keylen > 64 ) {
+		hash_key(NLM_HASH_SHA,NLM_HASH_MODE_SHA256,key,keylen,&nlm_ctx->hashed_key[0]);
+		nlm_crypto_fill_pkt_ctrl(ctrl,1,NLM_HASH_SHA,NLM_HASH_MODE_SHA256,
+			NLM_CIPHER_BYPASS,NLM_CIPHER_MODE_ECB,0,NULL,0,(unsigned char*)nlm_ctx->hashed_key,64);
+		crypto_shash_setkey(nlm_ctx->fallback_tfm,nlm_ctx->hashed_key,64);
+		return 0;
+	}
+
+	/*setup ctrl descriptor*/
+	nlm_crypto_fill_pkt_ctrl(ctrl,1,NLM_HASH_SHA,NLM_HASH_MODE_SHA256,
+		NLM_CIPHER_BYPASS,NLM_CIPHER_MODE_ECB,0,NULL,0,(unsigned char*)key,keylen);
+	if ( ctrl->hashkeylen < auth_mode_key_len[NLM_HASH_SHA][NLM_HASH_MODE_SHA256]) 
+		ctrl->hashkeylen = auth_mode_key_len[NLM_HASH_SHA][NLM_HASH_MODE_SHA256];
+	crypto_shash_setkey(nlm_ctx->fallback_tfm,key,keylen);
+	return 0;
+	
+}
+
+
+static int
+xlp_auth_hmac_md5_setkey(struct crypto_shash *tfm, const u8 * key, unsigned int keylen)
+{
+	struct nlm_auth_ctx * nlm_ctx = nlm_shash_auth_ctx(tfm);
+	struct nlm_crypto_pkt_ctrl * ctrl = &nlm_ctx->ctrl; 
+
+	nlm_ctx->stat = MD5_STAT;
+	if ( keylen > 64 ) {
+		hash_key(NLM_HASH_MD5,NLM_HASH_MODE_SHA1,key,keylen,&nlm_ctx->hashed_key[0]);
+		nlm_crypto_fill_pkt_ctrl(ctrl,1,NLM_HASH_MD5,NLM_HASH_MODE_SHA1,
+			NLM_CIPHER_BYPASS,NLM_CIPHER_MODE_ECB,0,NULL,0,(unsigned char *)nlm_ctx->hashed_key,64);
+		crypto_shash_setkey(nlm_ctx->fallback_tfm,nlm_ctx->hashed_key,64);
+		return 0;
+	}
+
+	/*setup ctrl descriptor*/
+	nlm_crypto_fill_pkt_ctrl(ctrl,1,NLM_HASH_MD5,NLM_HASH_MODE_SHA1,
+		NLM_CIPHER_BYPASS,NLM_CIPHER_MODE_ECB,0,NULL,0,(unsigned char *)key,keylen);
+	if ( ctrl->hashkeylen < auth_mode_key_len[NLM_HASH_MD5][NLM_HASH_MODE_SHA1]) 
+		ctrl->hashkeylen = auth_mode_key_len[NLM_HASH_MD5][NLM_HASH_MODE_SHA1];
+	crypto_shash_setkey(nlm_ctx->fallback_tfm,key,keylen);
+	return 0;
+	
+}
+
+static int
+xlp_auth_hmac_sha1_setkey(struct crypto_shash *tfm, const u8 * key, unsigned int keylen)
+{
+	struct nlm_auth_ctx * nlm_ctx = nlm_shash_auth_ctx(tfm);
+	struct nlm_crypto_pkt_ctrl * ctrl = &nlm_ctx->ctrl; 
+
+	nlm_ctx->stat = H_SHA1_STAT;
+	if ( keylen > 64 ) {
+
+		hash_key(NLM_HASH_SHA,NLM_HASH_MODE_SHA1,key,keylen,&nlm_ctx->hashed_key[0]);
+		nlm_crypto_fill_pkt_ctrl(ctrl,1,NLM_HASH_SHA,NLM_HASH_MODE_SHA1,
+			NLM_CIPHER_BYPASS,NLM_CIPHER_MODE_ECB,0,NULL,0,(unsigned char *)nlm_ctx->hashed_key,64);
+		crypto_shash_setkey(nlm_ctx->fallback_tfm,nlm_ctx->hashed_key,64);
+		return 0;
+	}
+
+	/*setup ctrl descriptor*/
+	nlm_crypto_fill_pkt_ctrl(ctrl,1,NLM_HASH_SHA,NLM_HASH_MODE_SHA1,
+		NLM_CIPHER_BYPASS,NLM_CIPHER_MODE_ECB,0,NULL,0,(unsigned char *)key,keylen);
+	if ( ctrl->hashkeylen < auth_mode_key_len[NLM_HASH_SHA][NLM_HASH_MODE_SHA1]) 
+		ctrl->hashkeylen = auth_mode_key_len[NLM_HASH_SHA][NLM_HASH_MODE_SHA1];
+	crypto_shash_setkey(nlm_ctx->fallback_tfm,key,keylen);
+	return 0;
+	 
+	
+}
+
+static struct shash_alg xcbc_mac_alg = {
+	.digestsize = XCBC_DIGEST_SIZE,
+	.init = xlp_auth_init,
+	.update = xlp_auth_update,
+	.export = xlp_auth_export,
+	.import = xlp_auth_import,
+	.final = xlp_auth_final,
+	.setkey = xlp_auth_aes_xcbc_setkey,
+	.descsize = PACKET_DESC_SIZE,
+	.base = {
+		 .cra_name = "xcbc(aes)",
+		 .cra_driver_name = "xcbc-aes-xlp",
+		 .cra_priority = XLP_HMAC_PRIORITY,
+		 .cra_flags = CRYPTO_ALG_TYPE_SHASH,
+		 .cra_blocksize = AES_BLOCK_SIZE,
+		 .cra_module = THIS_MODULE,
+		 .cra_ctxsize = CTRL_DESC_SIZE, 
+		 .cra_init = xlp_cra_xcbc_init,
+		 }
+};
+
+static struct shash_alg sha256_hmac_alg = {
+	.digestsize = SHA256_DIGEST_SIZE,
+	.init = xlp_auth_init,
+	.update = xlp_auth_update,
+	.export = xlp_auth_export,
+	.import = xlp_auth_import,
+	.final = xlp_auth_final,
+	.setkey = xlp_auth_hmac_sha256_setkey,
+	.descsize = PACKET_DESC_SIZE,
+	.base = {
+		 .cra_name = "hmac(sha256)",
+		 .cra_driver_name = "hmac-sha256-xlp",
+		 .cra_priority = XLP_HMAC_PRIORITY,
+		 .cra_flags = CRYPTO_ALG_TYPE_SHASH,
+		 .cra_blocksize = SHA256_BLOCK_SIZE,
+		 .cra_module = THIS_MODULE,
+		 .cra_ctxsize = CTRL_DESC_SIZE, 
+		 .cra_init = xlp_cra_hmac_sha256_init,
+		 }
+};
+
+static struct shash_alg md5_hmac_alg = {
+	.digestsize = MD5_DIGEST_SIZE,
+	.init = xlp_auth_init,
+	.update = xlp_auth_update,
+	.final = xlp_auth_final,
+	.export = xlp_auth_export,
+	.import = xlp_auth_import,
+	.setkey = xlp_auth_hmac_md5_setkey,
+	.descsize = PACKET_DESC_SIZE,
+	.base = {
+		 .cra_name = "hmac(md5)",
+		 .cra_driver_name = "hmac-md5-xlp",
+		 .cra_priority = XLP_HMAC_PRIORITY,
+		 .cra_flags = CRYPTO_ALG_TYPE_SHASH,
+		 .cra_blocksize = MD5_BLOCK_SIZE,
+		 .cra_module = THIS_MODULE,
+		 .cra_ctxsize = CTRL_DESC_SIZE, 
+		 .cra_init = xlp_cra_md5_init,
+		 }
+};
+static struct shash_alg sha1_hmac_alg = {
+	.digestsize = SHA1_DIGEST_SIZE,
+	.init = xlp_auth_init,
+	.update = xlp_auth_update,
+	.final = xlp_auth_final,
+	.export = xlp_auth_export,
+	.import = xlp_auth_import,
+	.setkey = xlp_auth_hmac_sha1_setkey,
+	.descsize = PACKET_DESC_SIZE,
+	.base = {
+		 .cra_name = "hmac(sha1)",
+		 .cra_driver_name = "hmac-sha1-xlp",
+		 .cra_priority = XLP_HMAC_PRIORITY,
+		 .cra_flags = CRYPTO_ALG_TYPE_SHASH,
+		 .cra_blocksize = SHA1_BLOCK_SIZE,
+		 .cra_module = THIS_MODULE,
+		 .cra_ctxsize = CTRL_DESC_SIZE, 
+		 .cra_init = xlp_cra_hmac_sha1_init,
+		 }
+};
+
+int
+xlp_auth_alg_init(void)
+{
+	int rc = -ENODEV;
+	int no_of_alg_registered = 0;
+
+	rc = crypto_register_shash(&sha1_hmac_alg);
+	if (rc)
+		goto out;
+	no_of_alg_registered++;
+	rc = crypto_register_shash(&sha256_hmac_alg);
+	if (rc)
+		goto out;
+	no_of_alg_registered++;
+	rc = crypto_register_shash(&md5_hmac_alg);
+	if (rc)
+		goto out;
+	no_of_alg_registered++;
+	rc = crypto_register_shash(&xcbc_mac_alg);
+	if (rc)
+		goto out;
+	no_of_alg_registered++; 
+	//printk("Some of the FIPS test failed as the maximum key length supported is 64 bytes.\n");
+
+	printk(KERN_NOTICE "Using XLP hardware for SHA/MD5 algorithms.\n");
+out:
+
+	return 0;
+
+}
+
+void
+xlp_auth_alg_fini(void)
+{
+	crypto_unregister_shash(&xcbc_mac_alg);
+	crypto_unregister_shash(&md5_hmac_alg);
+	crypto_unregister_shash(&sha256_hmac_alg);
+	crypto_unregister_shash(&sha1_hmac_alg);
+}
+
+EXPORT_SYMBOL(xlp_auth_alg_init);
+EXPORT_SYMBOL(xlp_auth_alg_fini);
diff --git a/drivers/crypto/sae/nlm_crypto.c b/drivers/crypto/sae/nlm_crypto.c
new file mode 100644
index 0000000..ff659a8
--- /dev/null
+++ b/drivers/crypto/sae/nlm_crypto.c
@@ -0,0 +1,766 @@
+/***********************************************************************
+  Copyright 2003-2010 Netlogic Microsystems (“Netlogic”). All rights
+  reserved.
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions are
+met:
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+ *****************************#NETL_2#********************************/
+#include <linux/module.h>
+#include <asm/netlogic/xlp.h>
+#include <asm/netlogic/msgring.h>
+#include <linux/proc_fs.h>
+#include <asm/netlogic/proc.h>
+
+#include <asm/netlogic/hal/nlm_hal_fmn.h>
+#include <asm/netlogic/hal/nlm_hal_macros.h>
+#include <asm/netlogic/hal/nlm_hal_xlp_dev.h>
+#include <linux/crypto.h>
+#include "nlm_async.h"
+
+
+#ifdef TRACING
+#define TRACE_TEXT(str) printk(str);
+#define TRACE_RET printk(")")
+#else				/* !TRACING */
+#define TRACE_TEXT(str) ((void) 0)
+#define TRACE_RET ((void) 0)
+#endif				/* TRACING */
+#undef NLM_CRYPTO_DEBUG
+#define NETL_OP_ENCRYPT 1
+#define NETL_OP_DECRYPT 0
+
+
+/**
+ * @defgroup crypto Crypto API
+ * @brief Description about the crypto apis
+ */
+
+#define printf(a, b...) printk(KERN_ERR a, ##b)
+
+#define xtract_bits(x, bitpos, numofbits) ((x) >> (bitpos) & ((1ULL << (numofbits)) - 1))
+
+
+extern struct proc_dir_entry *nlm_root_proc;
+extern int xlp_aead_alg_init(void);
+extern void xlp_aead_alg_fini(void);
+extern int xlp_crypt_alg_init(void);
+extern void xlp_crypt_alg_fini(void);
+extern int xlp_auth_alg_init(void);
+extern void xlp_auth_alg_fini(void);
+static void xlp_sae_cleanup(void);
+
+static int xlp_sae_major;
+static int xlp_sae_open(struct inode *, struct file *);
+static int xlp_sae_release(struct inode *, struct file *);
+
+struct nlm_crypto_stat crypto_stat[MAX_CPU];
+int crypto_vc_base;
+int crypto_vc_limit;
+int chip_feature = 0 ;
+
+
+
+/*
+ * is the following table needed for all modes?
+Cipher            keylen           iv_len
+*/
+
+//-1 indicates variable length IV
+// In case of AES/Camelia cipher and CBC-MAC auth, IV is not needed.
+// In case of AES/Camelia cipher and XCBC-MAC auth, IV is needed only for 
+//CBC, CFB, OFB and CTR modes..
+int cipher_mode_iv_len[NLM_CIPHER_MAX][NLM_CIPHER_MODE_MAX] = {
+/*		       ECB  CBC   CFB   OFB   CTR  AESF8    GCM  CCM    8   9  LRW   XTS */
+/* BYPASS */       {   0,    0,    0,    0,    0,    0,      0,    0,   0,  0,  0,    0,},
+/* DES */          {   0,    8,    0,    0,    0,    0,      0,    0,   0,  0,  0,    0,},
+/* 3DES */         {   0,    8,    0,    0,    0,    0,      0,    0,   0,  0,  0,    0,},
+/* AES128 */       {   0,    16,   16,   16,   8,   16,     8,   8,   0,  0,  16,   16,},
+/* AES192 */       {   0,    16,   16,   16,   8,   16,     8,   8,   0,  0,  16,   16,},
+/* AES256 */       {   0,    16,   16,   16,   8,   16,     8,   8,   0,  0,  16,   16,},
+/* ARC4 */         {   0,    0,    0,    0,    0,    0,      0,    0,   0,  0,  0,    0,},
+/* KASUMI F8 */    {   0,    0,    0,    0,    0,    0,      0,    0,   0,  0,  0,    0,},
+/* SNOW3G F8 */    {   0,    0,    0,    0,    0,    0,      0,    0,   0,  0,  0,    0,},
+/* CAMELLIA128 */  {   0,    16,   16,   16,   16,   16,     -1,   0,   0,  0,  16,   16,},
+/* CAMELLIA192 */  {   0,    16,   16,   16,   16,   16,     -1,   0,   0,  0,  16,   16,}, 
+/* CAMELLIA256 */  {   0,    16,   16,   16,   16,   16      -1,   0,   0,  0,  16,   16,},
+};
+
+int auth_mode_key_len[NLM_HASH_MAX][NLM_HASH_MODE_MAX] = {
+/*	               SHA1 SHA224 SHA256 SHA384 SHA512  CMAC  XCBC CBC_MAC CCM  GCM*/
+/* BYPASS */		{0,    0,     0,     0,     0,     0,   0,    0,     0,    0, },
+/* MD5 */		{64,   64,    64,   64,    64,    64,  64,   64,    64,   64, },
+/* SHA */		{64,   64,    64,   128,   128,    0,   0,    0,     0,    0, },
+/* 3 */			{0,    0,     0,     0,     0,     0,   0,    0,     0,    0, },
+/* AES128 */		{0,    0,     0,     0,     0,    16,  16,   16,    16,   16, },
+/* AES192 */		{0,    0,     0,     0,     0,    24,  24,   24,    24,   24, },
+/* AES256 */		{0,    0,     0,     0,     0,    32,  32,   32,    32,   32, },
+/* KASUMI_F9 */		{16,  16,    16,    16,    16,    16,  16,   16,    16,   16, },
+/* SNOW3G_F9 */		{16,  16,    16,    16,    16,    16,  16,   16,    16,   16, }, //sandip -> verify
+/* CAMELLIA128 */	{0,    0,     0,     0,     0,    16,  16,   16,    16,   16, },
+/* CAMELLIA192 */	{0,    0,     0,     0,     0,    24,  24,   24,    24,   24, },
+/* CAMELLIA256 */	{0,    0,     0,     0,     0,    32,  32,   32,    32,   32, },
+/* GHASH */		{0,    0,     0,     0,     0,    32,  32,   32,    32,   32, }, //todo:
+};
+
+#define NLM_CRYPTO_MAX_STR_LEN 200
+#ifdef NLM_CRYPTO_DEBUG
+static char str_cipher_alg[NLM_CIPHER_MAX + 1][NLM_CRYPTO_MAX_STR_LEN] = {
+"bypass",       // NLM_CIPHER_BYPASS
+"des",          // NLM_CIPHER_DES
+"3des",         // NLM_CIPHER_3DES
+"aes 128",      // NLM_CIPHER_AES128
+"aes 192",      // NLM_CIPHER_AES192
+"aes 256",      // NLM_CIPHER_AES256
+"arc4",         // NLM_CIPHER_ARC4
+"Kasumi f8",    // NLM_CIPHER_KASUMI_F8
+"snow3g f8",    // NLM_CIPHER_SNOW3G_F8
+"camellia 128", // NLM_CIPHER_CAMELLIA128
+"camelia 192",  // NLM_CIPHER_CAMELLIA192
+"camelia 256",  // NLM_CIPHER_CAMELLIA256
+"undefined",  // > max
+};
+static char str_cipher_mode[NLM_CIPHER_MODE_MAX+ 1][NLM_CRYPTO_MAX_STR_LEN] = {
+"ecb",          // NLM_CIPHER_MODE_ECB
+"cbc",          // NLM_CIPHER_MODE_CBC
+"cfb",          // NLM_CIPHER_MODE_CFB
+"ofb",          // NLM_CIPHER_MODE_OFB
+"ctr",          // NLM_CIPHER_MODE_CTR
+"aes f8",       // NLM_CIPHER_MODE_AES_F8
+"gcm",          // NLM_CIPHER_MODE_GCM
+"ccm",          // NLM_CIPHER_MODE_CCM
+"undefined",    // NLM_CIPHER_MODE_UNDEFINED1
+"undefined",    // NLM_CIPHER_MODE_UNDEFINED2
+"lrw",          // NLM_CIPHER_MODE_LRW
+"xts",          // NLM_CIPHER_MODE_XTS
+"undefined", // > max
+};
+static char str_auth_alg[NLM_HASH_MAX + 1][NLM_CRYPTO_MAX_STR_LEN] = {
+"bypass",       // NLM_AUTH_BYPASS
+"md5",          // NLM_AUTH_MD5
+"sha",          // NLM_AUTH_SHA
+"invalid",       // NLM_AUTH_UNDEFINED
+"aes 128",      // NLM_AUTH_AES128
+"aes 192",      // NLM_AUTH_AES192
+"aes 256",      // NLM_AUTH_AES256
+"kasumi f9",    // NLM_AUTH_KASUMI_F9
+"snow3g f9",    // NLM_AUTH_SNOW3G_F9
+"camellia 128", // NLM_AUTH_CAMELLIA128
+"camellia 192", // NLM_AUTH_CAMELLIA192
+"camellia 256", // NLM_AUTH_CAMELLIA256
+"ghash",        // NLM_AUTH_GHASH
+"undefined",    // > max
+};
+static char str_auth_mode[NLM_HASH_MODE_MAX + 1][NLM_CRYPTO_MAX_STR_LEN] = {
+"sha1",         // NLM_AUTH_MODE_SHA1
+"sha 224",      // NLM_AUTH_MODE_SHA224
+"sha 256",      // NLM_AUTH_MODE_SHA256
+"sha 384",      // NLM_AUTH_MODE_SHA384
+"sha 512",      // NLM_AUTH_MODE_SHA512
+"cmac",         // NLM_AUTH_MODE_CMAC
+"xcbc",         // NLM_AUTH_MODE_XCBC
+"cbc mac",      // NLM_AUTH_MODE_CBC_MAC
+"undefined", // > max
+};
+void hex_dump(char * description,unsigned char *in, int num)
+{
+        int i, j;
+        char buf[50];
+        char *buf_ptr;
+        printk("%s\n",description);
+
+        for (i = 0; i < num; i+= 16) {
+                if (i + 16 > num) {
+                        buf_ptr = buf;
+                        sprintf(buf_ptr, "    ");
+                        buf_ptr += 4;
+                        for (j = 0 ; j < num - i ; j++) {
+                                sprintf(buf_ptr, "%02x ", in[j + i]);
+                                buf_ptr += 3;
+                        }
+                        *buf_ptr = '\0';
+                        printk("%s\n",buf);
+                        break;
+                }
+                printk("    %02x %02x %02x %02x %02x %02x %02x %02x %02x %02x %02x %02x %02x %02x %02x %02x\n",
+                        in[i + 0 ],
+                        in[i + 1 ],
+                        in[i + 2 ],
+                        in[i + 3 ],
+                        in[i + 4 ],
+                        in[i + 5 ],
+                        in[i + 6 ],
+                        in[i + 7 ],
+                        in[i + 8 ],
+                        in[i + 9 ],
+                        in[i + 10],
+                        in[i + 11],
+                        in[i + 12],
+                        in[i + 13],
+                        in[i + 14],
+                        in[i + 15]
+              );
+        }
+}
+
+char *nlm_crypto_cipher_alg_get_name(unsigned int cipher_alg)
+{
+	if (cipher_alg >= NLM_CIPHER_MAX)
+		return str_cipher_alg[NLM_CIPHER_MAX];
+	else 
+		return str_cipher_alg[cipher_alg];
+}
+
+char *nlm_crypto_cipher_mode_get_name(unsigned int cipher_mode)
+{
+	if (cipher_mode >= NLM_CIPHER_MODE_MAX)
+		return str_cipher_mode[NLM_CIPHER_MODE_MAX];
+	else 
+		return str_cipher_mode[cipher_mode];
+}
+
+char *nlm_crypto_auth_alg_get_name(unsigned int auth_alg)
+{
+	if (auth_alg >= NLM_HASH_MAX)
+		return str_auth_alg[NLM_HASH_MAX];
+	else 
+		return str_auth_alg[auth_alg];
+}
+
+char *nlm_crypto_auth_mode_get_name(unsigned int auth_mode)
+{
+	if (auth_mode >= NLM_HASH_MODE_MAX)
+		return str_auth_mode[NLM_HASH_MODE_MAX];
+	else 
+		return str_auth_mode[auth_mode];
+}
+
+void print_crypto_msg_desc(uint64_t entry1, uint64_t entry2, uint64_t entry3)
+{
+        printk("Security Message Descriptor 0: 0x%llx\n", entry1);
+        printk("Security Message Descriptor 1: 0x%llx\n", entry2);
+        printk("Security Message Descriptor 2: 0x%llx\n", entry3);
+
+
+        printk("Free descriptor response destination : 0x%llx  \n", xtract_bits(entry1, 48, 16));
+        printk("Use designer freeback : 0x%llx  \n", xtract_bits(entry1, 45, 1));
+        printk("cipher key length (in dwords) : 0x%llx  \n", xtract_bits(entry1, 40, 5));
+        printf("Control desc cacheline addr : 0x%llx  \n", xtract_bits(entry1, 0, 34));
+        if (xtract_bits(entry1, 45, 1)) {
+                printf("Designer freeback length (actual len - 1): 0x%llx  \n", xtract_bits(entry1, 46, 2));
+        }
+
+
+        printf("Arc4 load state : 0x%llx  \n", xtract_bits(entry2, 63, 1));
+        printf("Hash key length (in dwords) : 0x%llx  \n", xtract_bits(entry2, 56, 5));
+        printf("Pkt desc length (in multiple of 16 bytes - 1): 0x%llx  \n", xtract_bits(entry2, 43, 12));
+        printf("Pkt desc cacheline addr : 0x%llx  \n", xtract_bits(entry2, 0, 34));
+
+        printf("Software Scratch Pad : 0x%llx  \n", xtract_bits(entry3, 0, 34));
+}
+void print_cntl_instr(uint64_t cntl_desc)
+{
+	unsigned int tmp;
+	char *x;
+	char s[NLM_CRYPTO_MAX_STR_LEN];
+	cntl_desc = ccpu_to_be64(cntl_desc);
+
+	printf("control description: 0x%016llx\n", (unsigned long long)cntl_desc);
+	printf("HMac = 0x%llx  \n", xtract_bits(cntl_desc, 61, 1));
+//	printk("Pad Hash = 0x%llx  \n", xtract_bits(cntl_desc, 62, 1));
+	/* Check cipher, hash type and mode b4 printing */
+	tmp = xtract_bits(cntl_desc, 52, 8);
+	x = nlm_crypto_auth_alg_get_name(tmp);
+	strncpy(s, x, NLM_CRYPTO_MAX_STR_LEN);
+	printf("Hash Type = 0x%llx(%s)  \n", xtract_bits(cntl_desc, 52, 8), s);
+	tmp = xtract_bits(cntl_desc, 43, 8);
+	x = nlm_crypto_auth_mode_get_name(tmp);
+	strncpy(s, x, NLM_CRYPTO_MAX_STR_LEN);
+	printf("Hash Mode = 0x%llx(%s)  \n", xtract_bits(cntl_desc, 43, 8), s);
+	tmp = xtract_bits(cntl_desc, 34, 8);
+	x = nlm_crypto_cipher_alg_get_name(tmp);
+	strncpy(s, x, NLM_CRYPTO_MAX_STR_LEN);
+	printf("Cipher Type = 0x%llx(%s)  \n", xtract_bits(cntl_desc, 34, 8), s);
+	tmp = xtract_bits(cntl_desc, 25, 8);
+	x = nlm_crypto_cipher_mode_get_name(tmp);
+	strncpy(s, x, NLM_CRYPTO_MAX_STR_LEN);
+	printf("Cipher Mode = 0x%llx(%s)  \n", xtract_bits(cntl_desc, 25, 8), s);
+
+
+	if (xtract_bits(cntl_desc, 34, 8) == NLM_CIPHER_ARC4) {
+		printf("Arc4 cipher key byte count= 0x%llx  \n", xtract_bits(cntl_desc, 18, 5));
+		printf("Arc4 key init = 0x%llx  \n", xtract_bits(cntl_desc, 17, 1));
+	}
+
+}
+struct pkt_desc_src_dst {
+	uint64_t pkt_desc4;
+	uint64_t pkt_desc5;
+};
+
+struct designer_desc{
+	uint64_t desc0;
+	uint64_t desc1;
+	uint64_t desc2;
+	uint64_t desc3;
+};
+
+
+void print_pkt_desc(struct nlm_crypto_pkt_param  *pkt_desc1, int index)
+{
+	int i;
+	unsigned long  phys;
+	void * virt;
+	
+	struct nlm_crypto_pkt_param * pkt_desc = kmalloc(sizeof(struct nlm_crypto_pkt_param) + 256 ,GFP_KERNEL);;
+	pkt_desc->desc0 = ccpu_to_be64(pkt_desc1->desc0); 
+	pkt_desc->desc1 = ccpu_to_be64(pkt_desc1->desc1); 
+	pkt_desc->desc2 = ccpu_to_be64(pkt_desc1->desc2); 
+	pkt_desc->desc3 = ccpu_to_be64(pkt_desc1->desc3); 
+	
+
+	printf("Packet desc address = %p\n",pkt_desc);
+	printf("Packet Descriptor 0: 0x%016llx\n", (unsigned long long)pkt_desc->desc0);
+	printf("Packet Descriptor 1: 0x%016llx\n", (unsigned long long)pkt_desc->desc1);
+	printf("Packet Descriptor 2: 0x%016llx\n", (unsigned long long)pkt_desc->desc2);
+	printf("Packet Descriptor 3: 0x%016llx\n", (unsigned long long)pkt_desc->desc3);
+
+	printf("\nPacket Descriptor 0\n");
+	printf("TLS protocol = 0x%llx  \n", xtract_bits(pkt_desc->desc0, 63, 1));
+	printf("Hash source(0-plain, 1-encrypted text) = 0x%llx  \n", xtract_bits(pkt_desc->desc0, 62, 1));
+	printf("Hash output l3 alloc = 0x%llx  \n", xtract_bits(pkt_desc->desc0, 60, 1));
+	printf("Encrypt(1)/Decrypt(0)= 0x%llx  \n", xtract_bits(pkt_desc->desc0, 59, 1));
+	printf("IV length = 0x%llx  \n", xtract_bits(pkt_desc->desc0, 41, 16));
+	printf("Hash Dest addr = 0x%llx \n", xtract_bits(pkt_desc->desc0, 0, 39));
+
+	printf("\nPacket Descriptor 1\n");
+	printf("Cipher length = 0x%llx \n", xtract_bits(pkt_desc->desc1, 32, 32));
+	printf("Hash length = 0x%llx  \n", xtract_bits(pkt_desc->desc1, 0, 32));
+	printf("IV Offset = 0x%llx  \n", xtract_bits(pkt_desc->desc2, 45, 17));
+
+	printf("\nPacket Descriptor 2\n");
+	printf("Cipher bit count = 0x%llx  \n", xtract_bits(pkt_desc->desc2, 42, 3));
+	printf("Cipher Offset = 0x%llx  \n", xtract_bits(pkt_desc->desc2, 22, 16));
+	printf("Hash bit count = 0x%llx  \n", xtract_bits(pkt_desc->desc2, 19, 3));
+	printf("Hash clobber = 0x%llx  \n", xtract_bits(pkt_desc->desc2, 18, 1));
+	printf("Hash Offset = 0x%llx  \n", xtract_bits(pkt_desc->desc2, 0, 16));
+
+
+	printf("\nPacket Descriptor 3\n");
+	printf("designer fb dest id = 0x%llx  \n", xtract_bits(pkt_desc->desc3, 48, 16));
+	printf("tag length = 0x%llx  \n", xtract_bits(pkt_desc->desc3, 11, 16));
+
+	printf("arc4 sbox l3 alloc = 0x%llx  \n", xtract_bits(pkt_desc->desc3, 8, 1));
+	printf("arc4 save box = 0x%llx  \n", xtract_bits(pkt_desc->desc3, 6, 1));
+	printf("hmac ext pad key = 0x%llx  \n", xtract_bits(pkt_desc->desc3, 5, 1));
+	int len;
+
+        for (i=0; i < index; i++) {
+		pkt_desc->segment[i][0] = ccpu_to_be64(pkt_desc1->segment[i][0]); 
+		pkt_desc->segment[i][1] = ccpu_to_be64(pkt_desc1->segment[i][1]);
+		printf("Packet Descriptor frag src %d: 0x%016llx\n", i, (unsigned long long)pkt_desc->segment[i][0]);
+		printf("Packet Descriptor frag dst %d: 0x%016llx\n", i, (unsigned long long)pkt_desc->segment[i][1]);
+
+		phys = xtract_bits(pkt_desc->segment[i][0], 0,40);
+		virt = phys_to_virt(phys);
+		len = xtract_bits(pkt_desc->segment[i][0], 48, 16); 
+		len = (len > 64 ) ? 64 : len;
+		hex_dump("src",virt,len);
+
+
+                printf("frag src length = 0x%llx  \n", xtract_bits(pkt_desc->segment[i][0], 48, 16));
+                printf("frag src = 0x%llx \n", xtract_bits(pkt_desc->segment[i][0], 0, 40));
+
+                printf("frag dest length = 0x%llx \n", xtract_bits(pkt_desc->segment[i][1], 48, 16));
+                printf("cipher output l3 alloc = 0x%llx \n", xtract_bits(pkt_desc->segment[i][1], 46, 1));
+                printf("cipher output write clobber = 0x%llx \n", xtract_bits(pkt_desc->segment[i][1], 41, 1));
+                printf("frag dest = 0x%llx \n", xtract_bits(pkt_desc->segment[i][1], 0, 40));
+        }
+}
+#endif
+
+int alloc_pkt_param( struct nlm_async_crypto * async,struct nlm_crypto_pkt_param ** pkt_param , int max_frags)
+{
+	uint8_t * new_pkt_param = NULL;
+	async->pkt_param = kmalloc((sizeof (struct nlm_crypto_pkt_param) +( max_frags * 2 * 8 )+ 64),GFP_KERNEL);
+
+	if ( !async->pkt_param) {
+		return -1;
+	}
+
+	new_pkt_param = (uint8_t * )(((unsigned long)async->pkt_param + 64) & ~0x3fUL);
+
+	memcpy(new_pkt_param,*pkt_param,sizeof(struct nlm_crypto_pkt_param));
+	*pkt_param = (struct nlm_crypto_pkt_param *)new_pkt_param;
+
+	
+	return 1;
+}
+
+int nlm_crypto_sae_num_seg_reqd(void *data, unsigned int buflen)
+{
+	return nlm_crypto_num_segs_reqd(buflen);
+}
+
+int nlm_crypto_calc_rem_len(struct scatterlist *sg, unsigned int cipher_len)
+{
+	int len,seg = 0;
+	for (;cipher_len > 0;sg = scatterwalk_sg_next(sg)){
+		len = min(cipher_len, sg->length);
+		seg += nlm_crypto_sae_num_seg_reqd(NULL,len);
+		cipher_len -= len;
+	}
+	return seg;
+}
+
+int fill_src_dst_sg(struct scatterlist *src_sg, struct scatterlist *dst_sg, unsigned int cipher_len, 
+		struct nlm_crypto_pkt_param *param, 
+		int seg, unsigned int max_frags, int op)
+{
+	struct scatterlist *sg;
+	struct scatter_walk walk;
+	unsigned int len = 0;
+	uint8_t *virt = NULL;
+	int rv = 0;
+	int i;
+
+	if (src_sg == dst_sg ) {
+		for( sg = src_sg; sg != NULL ; sg = scatterwalk_sg_next(sg)) {
+			len = min(cipher_len, sg->length);
+			scatterwalk_start(&walk, sg);
+			virt = page_address(scatterwalk_page(&walk)) + offset_in_page(walk.offset);
+			if ( cipher_len > 0 ) {
+				rv = nlm_crypto_fill_src_dst_seg(param, seg, max_frags, virt, len);
+				if ( rv < seg ) {
+					return nlm_crypto_calc_rem_len(sg,cipher_len) + max_frags; 
+				}
+				else
+					seg = rv;
+			}
+			cipher_len -= len;
+		}
+		return seg;
+	}
+	else {
+		int nr_src_frags = 0;
+		int nr_dst_frags = 0;
+		int index = seg;
+		int nbytes = cipher_len;
+		for (sg = src_sg,seg;cipher_len > 0; sg = scatterwalk_sg_next(sg)) {
+			len = min(cipher_len, sg->length);
+			scatterwalk_start(&walk, sg);
+			virt = page_address(scatterwalk_page(&walk)) + offset_in_page(walk.offset);
+			rv = nlm_crypto_fill_src_seg(param,seg,max_frags,virt,len);
+			if ( rv <= seg )
+				return nlm_crypto_calc_rem_len(sg,cipher_len) + max_frags;
+			else
+				seg = rv;
+			cipher_len -= len;
+		}
+		nr_src_frags = seg;
+		cipher_len = nbytes;
+		for (sg = dst_sg,seg = index ;cipher_len > 0; sg = scatterwalk_sg_next(sg)) {
+			len = min(cipher_len, sg->length);
+			scatterwalk_start(&walk, sg);
+			virt = page_address(scatterwalk_page(&walk)) + offset_in_page(walk.offset);
+			rv = nlm_crypto_fill_dst_seg(param,seg,max_frags,virt,len);
+			if ( rv <= seg )
+				return nlm_crypto_calc_rem_len(sg,cipher_len) + max_frags;
+			else
+				seg = rv;
+			cipher_len -= len;
+		}
+
+		nr_dst_frags = seg;
+
+		if ((nr_src_frags > nr_dst_frags) && (nr_src_frags < max_frags)) {
+			for (i = 0; i < nr_src_frags - nr_dst_frags; i++)
+				param->segment[index + nr_dst_frags + i][1] = 0ULL;
+			seg = nr_src_frags;
+		}
+		else  { 
+			if ((nr_src_frags < nr_dst_frags) && (nr_dst_frags < max_frags )){
+				for (i = 0; i < nr_dst_frags - nr_src_frags; i++)
+					param->segment[index + nr_src_frags + i][0] = 0ULL;
+			}
+			seg = nr_dst_frags;
+		}
+		return seg;
+	}
+
+}
+
+    static void
+reset_crypto_stats(void)
+{
+    int i, j;
+    for (i = 0; i < MAX_CPU; i++) {
+	for (j = 0; j < ENC_MAX_STAT; j++) {
+		crypto_stat[i].enc[j] = 0;
+		crypto_stat[i].enc_tbytes[j] = 0;
+	}
+	for (j = 0; j < AUTH_MAX_STAT; j++) {
+		crypto_stat[i].auth[j] = 0;
+		crypto_stat[i].auth_tbytes[j] = 0;
+	}
+		
+    }
+
+}
+
+int
+crypto_get_fb_vc(int * node)
+{
+    int cpu;
+    int node_id = 0;
+    extern int ipsec_async_vc;
+
+
+    cpu = hard_smp_processor_id();	//processor_id();
+    node_id = (cpu >> NODE_ID_SHIFT_BIT);
+    cpu = (node_id << NODE_BASE_SHIFT_BIT) | (((cpu & 0x1f) * 4) + ipsec_async_vc);
+    *node = node_id;
+
+    return cpu;
+}
+
+static const struct file_operations xlp_sae_fops = {
+    .owner = THIS_MODULE,
+    .open = xlp_sae_open,
+    .release = xlp_sae_release,
+};
+
+/* Note that nobody ever sets xlp_sae_busy... */
+    static int
+xlp_sae_open(struct inode *inode, struct file *file)
+{
+    TRACE_TEXT("(xlp_sae_open");
+    return 0;
+}
+
+    static int
+xlp_sae_release(struct inode *inode, struct file *file)
+{
+    TRACE_TEXT("(xlp_sae_release");
+
+    return 0;
+}
+
+    static void
+nlm_xlp_sae_msgring_handler(uint32_t vc, uint32_t src_id,
+	uint32_t size, uint32_t code,
+	uint64_t msg0, uint64_t msg1,
+	uint64_t msg2, uint64_t msg3, void *data)
+{
+	struct nlm_async_crypto *async = (struct nlm_async_crypto *)(unsigned long )msg0;
+	if(async)	
+		async->callback(async, msg1);
+}
+
+static int
+nlm_crypto_read_stats_proc(char *page, char **start, off_t off, int count,
+                       int *eof, void *data)
+{
+        int len = 0;
+	int i,j;
+	off_t begin = 0;
+	uint64_t enc_tp[ENC_MAX_STAT];
+	uint64_t auth_tp[AUTH_MAX_STAT];
+	uint64_t enc_tb[ENC_MAX_STAT];
+	uint64_t auth_tb[AUTH_MAX_STAT];
+
+	len += sprintf(page + len, "\t\tPkt\t\tTotal Bytes\n");
+
+	if (!proc_pos_check(&begin, &len, off, count))
+		goto out;
+	for(j =0 ;j <= ENC_MAX_STAT ; j++) {
+		enc_tp[j] = 0;	
+		enc_tb[j] = 0;
+		for(i = 0; i < MAX_CPU; i++)  {
+			enc_tp[j] = enc_tp[j] + crypto_stat[i].enc[j];
+			enc_tb[j] = enc_tb[j] + crypto_stat[i].enc_tbytes[j];
+		}
+			
+	}
+
+	len += sprintf(page + len,"DES-CBC\t\t%lld\t\t%lld\nTDES-CBC\t%lld\t\t%lld\nAES128-CBC\t%lld\t\t%lld\n",
+			enc_tp[DES_CBC_STAT],enc_tb[DES_CBC_STAT],enc_tp[TDES_CBC_STAT],enc_tb[TDES_CBC_STAT],
+			enc_tp[AES128_CBC_STAT],enc_tb[AES128_CBC_STAT]);
+	if (!proc_pos_check(&begin, &len, off, count))
+		goto out;
+
+	len += sprintf(page + len,"AES192-CBC\t%lld\t\t%lld\nAES256-CBC\t%lld\t\t%lld\nAES128-CTR\t%lld\t\t%lld\n",
+		enc_tp[AES192_CBC_STAT],enc_tb[AES192_CBC_STAT],enc_tp[AES256_CBC_STAT],enc_tb[AES256_CBC_STAT],
+		enc_tp[AES128_CTR_STAT],enc_tb[AES128_CTR_STAT]);
+	if (!proc_pos_check(&begin, &len, off, count))
+		goto out;
+
+	len += sprintf(page + len,"AES192-CTR\t%lld\t\t%lld\nAES256-CTR\t%lld\t\t%lld\n",
+		enc_tp[AES192_CTR_STAT],enc_tb[AES192_CTR_STAT],enc_tp[AES256_CTR_STAT],enc_tb[AES256_CTR_STAT]);
+	if (!proc_pos_check(&begin, &len, off, count))
+		goto out;
+
+	for(j =0 ;j < AUTH_MAX_STAT; j++) {
+		auth_tp[j] = 0;
+		auth_tb[j] = 0;
+		for(i = 0; i < MAX_CPU; i++) { 
+			auth_tp[j] +=  crypto_stat[i].auth[j];
+			auth_tb[j] += crypto_stat[i].auth_tbytes[j];
+		}
+	}
+
+	len  += sprintf(page + len,"MD5\t\t%lld\t\t%lld\nH-SHA1\t\t%lld\t\t%lld\nH-SHA256\t%lld\t\t%lld\n",
+				    auth_tp[MD5_STAT],auth_tb[MD5_STAT],auth_tp[H_SHA1_STAT],auth_tb[H_SHA1_STAT],auth_tp[H_SHA256_STAT],auth_tb[H_SHA256_STAT]);
+	if (!proc_pos_check(&begin, &len, off, count))
+		 goto out;
+
+	len  += sprintf(page + len,"AES128-XCBC\t%lld\t\t%lld\nAES198-XCBC\t%lld\t\t%lld\nAES256-XCBC\t%lld\t\t%lld\n",
+		auth_tp[AES128_XCBC_STAT],auth_tb[AES128_XCBC_STAT],auth_tp[AES192_XCBC_STAT],auth_tb[AES192_XCBC_STAT],auth_tp[AES256_XCBC_STAT],auth_tb[AES256_XCBC_STAT]);
+
+	if (!proc_pos_check(&begin, &len, off, count))
+		 goto out;
+
+	len  += sprintf(page + len,"GCM\t\t%lld\t\t%lld\nCCM\t\t%lld\t\t%lld\n",auth_tp[GCM_STAT],auth_tb[GCM_STAT],auth_tp[CCM_STAT],auth_tb[CCM_STAT]);
+
+	if (!proc_pos_check(&begin, &len, off, count))
+		 goto out;
+
+        *eof = 1;
+
+      out:
+        *start = page + (off - begin);
+        len -= (off - begin);
+        if (len > count)
+                len = count;
+        if (len < 0)
+                len = 0;
+
+        return len;
+
+}
+int nlm_crypto_clear_stat_proc(struct file *file, const char __user *buffer, 
+		unsigned long count, void *data)
+{
+	char buf[16];
+	unsigned long val;
+
+	copy_from_user(buf, buffer, count);
+	val =  simple_strtol(buf, NULL, 0);	
+
+	if ( val  == 1)
+    		reset_crypto_stats();
+
+	return 1;
+
+}
+
+int
+nlm_crypto_init(void)
+{
+    int ret = 0;
+    struct proc_dir_entry *entry = NULL;
+    struct proc_dir_entry *clear_entry  = NULL;
+
+    entry = create_proc_read_entry("crypto_stats", 0, nlm_root_proc,
+		    nlm_crypto_read_stats_proc,
+		    0);
+
+    if(entry == NULL) {
+	    printk("%s:%d failed creating proc stats entry.\n",
+			    __FUNCTION__, __LINE__);
+	    ret = -EINVAL;
+    }
+
+    clear_entry = create_proc_entry("clear_crypto_stats", S_IFREG|S_IRUGO|S_IWUSR,nlm_root_proc);
+
+    if ( clear_entry != NULL ) {
+	   clear_entry->write_proc = nlm_crypto_clear_stat_proc;
+    }
+
+   nlm_hal_get_crypto_vc_nums(&crypto_vc_base, &crypto_vc_limit); 
+
+    if (register_xlp_msgring_handler
+		    (XLP_MSG_HANDLE_CRYPTO, nlm_xlp_sae_msgring_handler, NULL)) {
+	    panic("can't register msgring handler for TX_STN_GMAC0");
+    }
+    reset_crypto_stats();
+
+
+
+    return ret;
+}
+
+static void  init_sae(void)
+{
+	if( is_nlm_xlp2xx())
+		chip_feature = (ZUC | DES3_KEY_SWAP); 
+	else
+		chip_feature = 0x0;
+}
+
+    static int __init
+xlp_sae_init(void)
+{
+    extern int ipsec_sync_vc;
+    extern int ipsec_async_vc;
+    printk(KERN_ERR ",\n XLP SAE/Crypto Initialization \n");
+
+    xlp_sae_major = register_chrdev(0, "NLM_XLP_SAE", &xlp_sae_fops);
+    if (xlp_sae_major < 0) {
+	printk(KERN_ERR "XLP_SAE - cannot register device\n");
+	return xlp_sae_major;
+    }
+    //  printk (KERN_ERR ",XLP SAE MAJOR %d\n", xlp_sae_major);
+    if ( (ipsec_async_vc == -1) && (ipsec_sync_vc == -1) )  {
+	printk(KERN_ERR "XLP_SAE - cannot be loaeded,Please set ipsec-async-vc and ipsec-sync-vc in the dts file\n");
+    	return -1;
+    }
+    nlm_crypto_init();
+    init_sae();
+    if(ipsec_async_vc != -1){
+    	xlp_crypt_alg_init();
+    	xlp_aead_alg_init();
+    }else{
+	printk(KERN_ERR "Cannot perform aead/enc operation, Please set ipsec-async-vc in the dts file\n");
+    }
+    if(ipsec_sync_vc != -1){
+    	xlp_auth_alg_init();
+    }else{
+	printk(KERN_ERR "Cannot perform auth operation, Please exclude ipsec_sync_vc from the node-vc-mask in dts\n");	
+	return 0;
+    }
+
+    return 0;
+}
+
+    static void __exit
+xlp_sae_cleanup(void)
+{
+    xlp_crypt_alg_fini();
+    xlp_auth_alg_fini();
+    xlp_aead_alg_fini();
+    unregister_chrdev(xlp_sae_major, "NLM_XLP_SAE");
+}
+
+module_init(xlp_sae_init);
+module_exit(xlp_sae_cleanup);
+MODULE_DESCRIPTION("XLP Hardware crypto support for AES/DES/3DES/SHA/MD5 .");
+MODULE_LICENSE("GPL");
+MODULE_VERSION("0.1");
+MODULE_AUTHOR("Alok Agrawat");
diff --git a/drivers/crypto/sae/nlm_enc.c b/drivers/crypto/sae/nlm_enc.c
new file mode 100755
index 0000000..45c423a
--- /dev/null
+++ b/drivers/crypto/sae/nlm_enc.c
@@ -0,0 +1,525 @@
+/***********************************************************************
+Copyright 2003-2010 Netlogic Microsystems (“Netlogic”). All rights
+reserved.
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are
+met:
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+*****************************#NETL_2#********************************/
+#include <linux/module.h>
+#include <linux/crypto.h>
+#include <crypto/aes.h>
+#include <crypto/des.h>
+#include <crypto/ctr.h>
+#include <nlm_hal_fmn.h>
+#include <asm/netlogic/hal/nlm_hal.h>
+#include <asm/netlogic/msgring.h>
+
+#include "nlm_async.h"
+#undef NLM_CRYPTO_DEBUG
+
+
+#define XLP_CRYPT_PRIORITY	300
+
+
+struct nlm_enc_ctx {
+	struct nlm_crypto_pkt_ctrl ctrl; 
+	uint16_t stat;
+	char nonce[4];
+};
+/* mem utilisation of CTX_SIZE */
+#define MAX_FRAGS               18 
+#define CTRL_DESC_SIZE          (sizeof(struct nlm_enc_ctx) + 64)
+#define DES3_CTRL_DESC_SIZE     (2*CTRL_DESC_SIZE + 2*64)
+
+
+/* mem utilisation of req mem */
+
+#define PACKET_DESC_SIZE        (64 + sizeof(struct nlm_crypto_pkt_param) + (MAX_FRAGS*16) + 64 + sizeof(struct nlm_async_crypto) + 64)
+#define NLM_CRYPTO_PKT_PARAM_OFFSET(addr)       (((unsigned long)addr + 64) & ~0x3fUL)
+#define NLM_ASYNC_PTR_PARAM_OFFSET(addr)        (((unsigned long)addr + 64 + sizeof(struct nlm_crypto_pkt_param) + (MAX_FRAGS*16) + 64) & ~0x3fUL)
+#define NLM_IV_OFFSET(addr)			((unsigned long)addr + (PACKET_DESC_SIZE - 64))
+
+
+static int no_of_alg_registered = 0;
+
+
+
+extern uint32_t nlm_hal_send_msg3(uint32_t dst, uint32_t code, uint64_t data0, uint64_t data1, uint64_t data2);
+extern __inline__ uint32_t nlm_hal_recv_msg2(uint32_t dst, uint32_t *src, uint32_t *size, uint32_t *code, uint64_t *data0, uint64_t *data1);
+
+#ifdef NLM_CRYPTO_DEBUG
+extern void print_cntl_instr(uint64_t cntl_desc);
+extern void print_crypto_msg_desc(uint64_t entry1, uint64_t entry2, uint64_t entry3);
+extern void print_pkt_desc(struct nlm_crypto_pkt_param * pkt_param, int index);
+#endif
+
+extern int cipher_mode_iv_len[NLM_CIPHER_MAX][NLM_CIPHER_MODE_MAX];
+extern struct nlm_crypto_stat crypto_stat[MAX_CPU];
+
+
+
+
+static int enc_cra_init(struct crypto_tfm *tfm)
+{ 
+	tfm->crt_ablkcipher.reqsize = PACKET_DESC_SIZE; //reqsize of 512 bytes for packet desc
+	return 0;
+}
+
+static struct nlm_enc_ctx * nlm_crypto_ablkcipher_ctx(struct crypto_ablkcipher *tfm)
+{
+	return (struct  nlm_enc_ctx *)(((unsigned long)((uint8_t *)crypto_ablkcipher_ctx(tfm) + 63 )) & ~(0x3fUL));
+} 
+
+static struct nlm_enc_ctx * nlm_crypto_ablkcipher_ctx_2(struct crypto_ablkcipher *tfm)
+{
+	return (struct  nlm_enc_ctx *)(( unsigned long )(( uint8_t *)nlm_crypto_ablkcipher_ctx(tfm) + CTRL_DESC_SIZE + 63) & ~(0x3fUL));
+}
+
+static int
+xlp_setkey(struct crypto_ablkcipher *tfm, const u8 * in_key, unsigned int len, uint32_t cipher_alg, uint32_t cipher_mode,uint16_t stat)
+{
+	struct nlm_enc_ctx * nlm_ctx = nlm_crypto_ablkcipher_ctx(tfm); 
+	
+	nlm_crypto_fill_pkt_ctrl(&nlm_ctx->ctrl,0,0,0,cipher_alg,cipher_mode,0,(unsigned char*)in_key,len,0,0);
+	crypto_ablkcipher_crt(tfm)->ivsize = cipher_mode_iv_len[cipher_alg][ cipher_mode];
+	nlm_ctx->stat = stat;
+
+	return 0;
+
+}
+
+static int
+xlp_setkey_des3(struct crypto_ablkcipher *tfm, const u8 * in_key, unsigned int len, uint32_t cipher_alg, uint32_t cipher_mode, uint16_t stat)
+{
+	struct nlm_enc_ctx * nlm_ctx = nlm_crypto_ablkcipher_ctx_2(tfm); 
+	uint64_t key[3] ;
+	if( CHIP_SUPPORTS(DES3_KEY_SWAP) ) {
+		memcpy((uint8_t *)key,in_key,len);
+	}
+	else {
+		memcpy(key,&in_key[16],8);
+		memcpy(&key[1],&in_key[8],8);
+		memcpy(&key[2],&in_key[0],8);
+	}
+
+	nlm_crypto_fill_pkt_ctrl(&nlm_ctx->ctrl,0,0,0,cipher_alg,cipher_mode,0,(unsigned char*)&key[0],len,0,0);
+	crypto_ablkcipher_crt(tfm)->ivsize = cipher_mode_iv_len[cipher_alg][ cipher_mode];
+	nlm_ctx->stat = stat;
+
+	return 0;
+
+}
+
+static int
+xlp_des3_setkey(struct crypto_ablkcipher *tfm, const u8 * in_key, unsigned int len)
+{
+	uint32_t cipher_alg;
+	u32 flags = 0;
+
+	switch (len) {
+	case DES3_EDE_KEY_SIZE:
+	        cipher_alg = NLM_CIPHER_3DES;
+		break;
+	default:
+		printk(KERN_WARNING "[%s]: Cannot handle keylen = %d\n",
+		       __FUNCTION__, len);
+		flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
+		crypto_ablkcipher_set_flags(tfm,flags);
+		return -EINVAL;
+	}
+	xlp_setkey(tfm, in_key, len, cipher_alg, NLM_CIPHER_MODE_CBC,TDES_CBC_STAT);
+	return xlp_setkey_des3(tfm, in_key, len, cipher_alg, NLM_CIPHER_MODE_CBC, TDES_CBC_STAT);
+}
+
+static int
+xlp_des_setkey(struct crypto_ablkcipher *tfm, const u8 * in_key, unsigned int len)
+{
+	uint32_t cipher_alg;
+	u32 flags = 0;
+
+	switch (len) {
+	case DES_KEY_SIZE:
+	        cipher_alg = NLM_CIPHER_DES;
+		break;
+	default:
+		printk(KERN_WARNING "[%s]: Cannot handle keylen = %d\n",
+		       __FUNCTION__, len);
+		flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
+		crypto_ablkcipher_set_flags(tfm, flags);
+		return -EINVAL;
+	}
+	return xlp_setkey(tfm, in_key, len, cipher_alg,NLM_CIPHER_MODE_CBC,DES_CBC_STAT);
+}
+
+static int
+xlp_aes_setkey(struct crypto_ablkcipher *tfm, const u8 *in_key, 
+		unsigned int len, uint32_t mode)
+{
+	uint32_t cipher_alg;
+	uint16_t stat;
+	u32 flags = 0;
+
+	switch (len) {
+	case 16:
+	        cipher_alg = NLM_CIPHER_AES128;
+		stat = AES128_CBC_STAT;
+		break;
+	case 24:
+		cipher_alg = NLM_CIPHER_AES192;
+		stat = AES192_CBC_STAT;
+		break;
+	case 32:
+		cipher_alg = NLM_CIPHER_AES256;
+		stat = AES256_CBC_STAT;
+		break;
+	default:
+		printk(KERN_WARNING "[%s]: Cannot handle keylen = %d\n",
+		       __FUNCTION__, len);
+		flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
+		crypto_ablkcipher_set_flags(tfm,flags);
+		return -EINVAL;
+	}
+	if ( mode == NLM_CIPHER_MODE_CTR )
+		stat += 3;
+
+	return xlp_setkey(tfm, in_key, len, cipher_alg, mode, stat);
+}
+
+static int xlp_cbc_aes_setkey(struct crypto_ablkcipher *tfm, const u8 *key,
+					unsigned int keylen)
+{
+	return xlp_aes_setkey(tfm,key,keylen,NLM_CIPHER_MODE_CBC);
+}
+
+static int xlp_ctr_rfc3686_setkey(struct crypto_ablkcipher *tfm, const u8 *key,
+                                 unsigned int keylen)
+{
+
+        int err;
+	struct nlm_enc_ctx  *nlm_ctx = ( struct nlm_enc_ctx  * )nlm_crypto_ablkcipher_ctx(tfm);
+	unsigned char  *nonce = &(nlm_ctx->nonce[0]); 
+
+        if (keylen < CTR_RFC3686_NONCE_SIZE)
+                return -EINVAL;
+        memcpy(nonce, key + (keylen - CTR_RFC3686_NONCE_SIZE),
+               CTR_RFC3686_NONCE_SIZE);
+        keylen -= CTR_RFC3686_NONCE_SIZE;
+        err = xlp_aes_setkey(tfm, key, keylen, NLM_CIPHER_MODE_CTR );
+
+        return err;
+}
+
+void enc_request_callback(struct nlm_async_crypto *async, uint64_t msg1 )
+{
+	struct crypto_async_request * base = (struct crypto_async_request *)async->args; 
+	int err =0;
+	int cpu = hard_smp_processor_id();
+	int stat = async->stat;
+
+	if (msg1 & 0x7ff80) {
+		printk("\n Error: entry1 is %llx",msg1);
+		err = -EIO;
+		base->complete(base, err);
+		return;
+	}
+	crypto_stat[cpu].enc[stat]++;
+	crypto_stat[cpu].enc_tbytes[stat]+= async->bytes;
+	if ( async->pkt_param)
+		kfree(async->pkt_param);
+	
+	base->complete(base, err);
+}
+
+static int
+xlp_crypt(struct ablkcipher_request *req, unsigned int enc, int iv_size, struct nlm_crypto_pkt_ctrl *ctrl, uint16_t stat)
+{
+	int seg = 0;
+	uint64_t msg0, msg1;
+	int pktdescsize = 0;
+	int fb_vc;
+#ifdef CONFIG_32BIT
+	unsigned long msgrng_flags;
+#endif
+	int node_sae_base;
+	int node ;
+	unsigned int max_frags= MAX_FRAGS;
+	unsigned int cipher_len = req->nbytes;
+	int ret = -EINPROGRESS; 
+	int try = 0;
+	struct nlm_crypto_pkt_param * pkt_param = (struct nlm_crypto_pkt_param *) NLM_CRYPTO_PKT_PARAM_OFFSET(ablkcipher_request_ctx(req));
+	struct nlm_async_crypto * async =  (struct nlm_async_crypto *)NLM_ASYNC_PTR_PARAM_OFFSET(ablkcipher_request_ctx(req));;
+
+	async->pkt_param = NULL;
+
+	nlm_crypto_fill_cipher_pkt_param(ctrl, pkt_param, enc,0,iv_size,iv_size ,req->nbytes); 
+
+	nlm_crypto_fill_src_dst_seg(pkt_param,seg,MAX_FRAGS,(unsigned char *)req->info,iv_size);
+	seg++;
+
+	do {
+		cipher_len = req->nbytes;
+		seg = fill_src_dst_sg(req->src,req->dst,cipher_len,pkt_param,seg,max_frags,0);
+
+		if ( seg > max_frags ) {
+			max_frags = seg; 
+			seg = alloc_pkt_param(async,&pkt_param,max_frags);
+		}
+	}while(seg == 1 ); 
+
+	pktdescsize = 32 + seg * 16;
+
+	preempt_disable();
+	fb_vc = crypto_get_fb_vc(&node); 
+
+	msg0 = nlm_crypto_form_pkt_fmn_entry0(fb_vc, 0, 0, ctrl->cipherkeylen,
+			virt_to_phys(ctrl));
+	msg1 = nlm_crypto_form_pkt_fmn_entry1(0,ctrl->hashkeylen, pktdescsize,
+				virt_to_phys(pkt_param));
+#ifdef NLM_CRYPTO_DEBUG
+	print_crypto_msg_desc(msg0,msg1,0xdeadbeef);
+	print_pkt_desc(pkt_param,seg);
+	print_cntl_instr(ctrl->desc0);
+#endif
+	async->callback = &enc_request_callback;
+	async->args = &req->base;
+	async->stat = stat; 
+	async->bytes = req->nbytes; 
+	mb();
+#ifdef CONFIG_32BIT
+	msgrng_access_enable(msgrng_flags);
+#endif
+	node_sae_base = (node << NODE_BASE_SHIFT_BIT) | crypto_vc_base;
+	while( nlm_hal_send_msg3(node_sae_base, 0 /*code */ , msg0, msg1, (unsigned long )async) != 0 ){
+		if ( try++ > 16) {
+			ret = -EAGAIN;
+			break;
+		}
+	}
+#ifdef CONFIG_32BIT
+	msgrng_access_disable(msgrng_flags);
+#endif
+	preempt_enable();
+	return ret;
+}
+
+static int
+xlp_3des_cbc_decrypt( struct ablkcipher_request *req)
+{
+      	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);
+	struct nlm_enc_ctx * nlm_ctx = nlm_crypto_ablkcipher_ctx_2(tfm); 
+	return xlp_crypt(req, 0, DES3_EDE_BLOCK_SIZE,&nlm_ctx->ctrl,nlm_ctx->stat);
+}
+
+static int
+xlp_3des_cbc_encrypt( struct ablkcipher_request *req )
+{
+      	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);
+	struct nlm_enc_ctx *nlm_ctx = nlm_crypto_ablkcipher_ctx(tfm); 
+	return xlp_crypt(req, 1, DES3_EDE_BLOCK_SIZE,&nlm_ctx->ctrl,nlm_ctx->stat);
+}
+
+static int
+xlp_cbc_decrypt( struct ablkcipher_request *req )
+{
+	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);
+	int iv_size = crypto_ablkcipher_ivsize(tfm);
+	struct nlm_enc_ctx *nlm_ctx = nlm_crypto_ablkcipher_ctx(tfm); 
+	return xlp_crypt(req, 0, iv_size,&nlm_ctx->ctrl,nlm_ctx->stat);
+}
+
+static int
+xlp_cbc_encrypt( struct ablkcipher_request *req )
+{
+	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);
+	int iv_size = crypto_ablkcipher_ivsize(tfm);
+	struct nlm_enc_ctx *nlm_ctx = nlm_crypto_ablkcipher_ctx(tfm); 
+	return xlp_crypt(req, 1, iv_size,&nlm_ctx->ctrl, nlm_ctx->stat);
+}
+
+static int crypto_rfc3686_crypt(struct ablkcipher_request *req,
+		unsigned int enc )
+{
+	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);
+	struct nlm_enc_ctx  *nlm_ctx = ( struct nlm_enc_ctx  * )nlm_crypto_ablkcipher_ctx(tfm);
+	u8 *iv = (uint8_t *)NLM_IV_OFFSET(ablkcipher_request_ctx(req));
+	int ret = 0;
+
+	/*  uniqueness is maintained by the req->info */
+	u8 *info = req->info;
+	unsigned char * nonce = &(nlm_ctx->nonce[0]); 
+
+	memcpy(iv, nonce, CTR_RFC3686_NONCE_SIZE);
+	memcpy(iv + CTR_RFC3686_NONCE_SIZE, info, CTR_RFC3686_IV_SIZE);
+
+	*(__be32 *)(iv + CTR_RFC3686_NONCE_SIZE + CTR_RFC3686_IV_SIZE) =
+				cpu_to_be32(1);
+	req->info = iv;
+	ret = xlp_crypt(req, enc, 16,&nlm_ctx->ctrl, nlm_ctx->stat);
+	req->info = info;
+	return ret;
+}
+
+static int
+xlp_ctr_rfc3686_decrypt( struct ablkcipher_request *req)
+{
+	return crypto_rfc3686_crypt(req, 0);
+}
+
+static int
+xlp_ctr_rfc3686_encrypt(struct ablkcipher_request *req)
+{
+	return crypto_rfc3686_crypt(req,1);
+}
+
+static struct crypto_alg xlp_cbc_aes_alg = {
+	.cra_name = "cbc(aes)",
+	.cra_driver_name = "cbc-aes-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER|CRYPTO_ALG_ASYNC,
+	.cra_blocksize = AES_BLOCK_SIZE,
+	.cra_ctxsize = CTRL_DESC_SIZE,
+	.cra_alignmask = 15,
+	.cra_type = &crypto_ablkcipher_type,
+	.cra_module = THIS_MODULE,
+	.cra_init = enc_cra_init,
+	.cra_list = LIST_HEAD_INIT(xlp_cbc_aes_alg.cra_list),
+	.cra_u = {
+		.ablkcipher = {
+			.min_keysize = AES_MIN_KEY_SIZE,
+			.max_keysize = AES_MAX_KEY_SIZE,
+			.setkey = xlp_cbc_aes_setkey,
+			.encrypt = xlp_cbc_encrypt,
+			.decrypt = xlp_cbc_decrypt,
+			.ivsize = AES_BLOCK_SIZE,
+		}
+	}
+};
+
+static struct crypto_alg xlp_cbc_des_alg = {
+	.cra_name = "cbc(des)",
+	.cra_driver_name = "cbc-des-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER | CRYPTO_ALG_ASYNC,
+	.cra_blocksize = DES_BLOCK_SIZE,
+	.cra_ctxsize = CTRL_DESC_SIZE,
+	.cra_alignmask = 15,
+	.cra_type = &crypto_ablkcipher_type,
+	.cra_module = THIS_MODULE,
+	.cra_init = enc_cra_init,
+	.cra_list = LIST_HEAD_INIT(xlp_cbc_des_alg.cra_list),
+	.cra_u = {
+		.ablkcipher = {
+			.min_keysize = DES_KEY_SIZE,
+			.max_keysize = DES_KEY_SIZE,
+			.setkey = xlp_des_setkey,
+			.encrypt = xlp_cbc_encrypt,
+			.decrypt = xlp_cbc_decrypt,
+			.ivsize = DES_BLOCK_SIZE,
+		}
+	}
+};
+
+static struct crypto_alg xlp_cbc_des3_alg = {
+	.cra_name = "cbc(des3_ede)",
+	.cra_driver_name = "cbc-des3-ede-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER|CRYPTO_ALG_ASYNC,
+	.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+	.cra_ctxsize = DES3_CTRL_DESC_SIZE,
+	.cra_alignmask = 15,
+	.cra_type = &crypto_ablkcipher_type,
+	.cra_module = THIS_MODULE,
+	.cra_init = enc_cra_init,
+	.cra_list = LIST_HEAD_INIT(xlp_cbc_des3_alg.cra_list),
+	.cra_u = {
+		.ablkcipher = {
+			.min_keysize = DES3_EDE_KEY_SIZE,
+			.max_keysize = DES3_EDE_KEY_SIZE,
+			.setkey = xlp_des3_setkey,
+			.encrypt = xlp_3des_cbc_encrypt,
+			.decrypt = xlp_3des_cbc_decrypt,
+			.ivsize = DES3_EDE_BLOCK_SIZE,
+		}
+	}
+};
+
+static struct crypto_alg xlp_ctr_aes_alg = {
+	.cra_name = "rfc3686(ctr(aes))",
+	.cra_driver_name = "rfc3686-ctr-aes-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER|CRYPTO_ALG_ASYNC,
+	.cra_blocksize = 1,
+	.cra_ctxsize = CTRL_DESC_SIZE,
+	.cra_alignmask = 0,
+	.cra_type = &crypto_ablkcipher_type,
+	.cra_module = THIS_MODULE,
+	.cra_init = enc_cra_init,
+	.cra_list = LIST_HEAD_INIT(xlp_ctr_aes_alg.cra_list),
+	.cra_u = {
+		.ablkcipher = {
+			.min_keysize = AES_MIN_KEY_SIZE,
+			.max_keysize = AES_MAX_KEY_SIZE,
+			.setkey = xlp_ctr_rfc3686_setkey,
+			.encrypt = xlp_ctr_rfc3686_encrypt,
+			.decrypt = xlp_ctr_rfc3686_decrypt,
+			.ivsize = CTR_RFC3686_IV_SIZE,
+			.geniv = "seqiv",
+		}
+	}
+};
+
+int xlp_crypt_alg_init(void)
+{
+	int ret = 0;
+	ret = crypto_register_alg(&xlp_cbc_des3_alg);
+	if (ret) {
+		goto end;
+	}
+	no_of_alg_registered++;
+	ret = crypto_register_alg(&xlp_cbc_des_alg);
+	if (ret) {
+		goto end;
+	}
+	no_of_alg_registered++;
+	ret = crypto_register_alg(&xlp_cbc_aes_alg);
+	if (ret) {
+		goto end;
+	}
+	no_of_alg_registered++;
+	ret = crypto_register_alg(&xlp_ctr_aes_alg);
+	if (ret) {
+		goto end;
+	}
+	no_of_alg_registered++;
+	printk(KERN_NOTICE "Using XLP hardware for AES, DES, 3DES algorithms.\n");
+end:
+	return 0;
+}
+
+void
+xlp_crypt_alg_fini(void) {
+	crypto_unregister_alg(&xlp_cbc_des3_alg);
+	crypto_unregister_alg(&xlp_cbc_des_alg);
+	crypto_unregister_alg(&xlp_cbc_aes_alg);
+	crypto_unregister_alg(&xlp_ctr_aes_alg);
+}
+
+EXPORT_SYMBOL(xlp_crypt_alg_init);
+EXPORT_SYMBOL(xlp_crypt_alg_fini);
diff --git a/drivers/crypto/sae/nlmcrypto_ifc.h b/drivers/crypto/sae/nlmcrypto_ifc.h
new file mode 100644
index 0000000..e5dd44d
--- /dev/null
+++ b/drivers/crypto/sae/nlmcrypto_ifc.h
@@ -0,0 +1,68 @@
+/*********************************************************************
+
+  Copyright 2003-2010 Netlogic Microsystem, Inc. ("Netlogic"). All rights
+  reserved.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+  1. Redistributions of source code must retain the above copyright
+  notice, this list of conditions and the following disclaimer.
+  2. Redistributions in binary form must reproduce the above copyright
+  notice, this list of conditions and the following disclaimer in
+  the documentation and/or other materials provided with the
+  distribution.
+
+  THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems, Inc. ``AS IS'' AND
+  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL RMI OR CONTRIBUTORS BE LIABLE
+  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+  INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+  CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+  THE POSSIBILITY OF SUCH DAMAGE.
+
+  *****************************#NLM_2#**********************************/
+#ifndef _NLM_CRYPTO_IFC_H
+#define _NLM_CRYPTO_IFC_H
+extern void *linuxu_shvaddr;
+extern unsigned long long linuxu_shoff ;
+
+static inline unsigned long long crypto_virt_to_phys(void *vaddr)
+{
+	return virt_to_phys(vaddr);
+}
+
+static inline void *crypto_phys_to_virt(unsigned long long paddr)
+{
+	return phys_to_virt(paddr);
+}
+
+static inline int crypto_fill_pkt_seg_paddr_len(void *vaddr, unsigned int inlen, 
+	       struct nlm_crypto_pkt_seg_desc *segs, unsigned int s_seg, unsigned int max_segs,
+	       int fillsrc, int filldst, unsigned long long sinitval, unsigned long long dinitval)
+{
+	unsigned int remlen = inlen, sg = 0, len;
+	for(; remlen > 0;) {
+		if ( sg >= max_segs ) 
+			return -1;
+			
+		len = remlen > NLM_CRYPTO_MAX_SEG_LEN ? NLM_CRYPTO_MAX_SEG_LEN : remlen;
+		if(fillsrc)
+			segs[sg].src = ccpu_to_be64((virt_to_phys(vaddr) | 
+					((unsigned long long)(len - 1) << NLM_CRYPTO_SEGS_LEN_OFF) | sinitval));
+		if(filldst)
+			segs[sg].dst = ccpu_to_be64((virt_to_phys(vaddr) | 
+					((unsigned long long)(len - 1) << NLM_CRYPTO_SEGS_LEN_OFF) | dinitval));
+		remlen -= len;
+		vaddr += len;
+		sg++;
+	}
+	return sg;
+}
+
+#endif
-- 
1.8.4.93.g57e4c17

