From 87fa4d15cb5d556b90db430462e814c97bf94eba Mon Sep 17 00:00:00 2001
From: Yonghong Song <ysong@netlogicmicro.com>
Date: Fri, 20 Jan 2012 23:11:03 -0800
Subject: [PATCH 749/762] Support more than 16GB memory

  o guarded with CONFIG_NLM_16G_MEM_SUPPORT

  o changes to map memory above a certain size in kernel page table

    Prior to this commit, all avaialable physical memory was wired.
    Since only 32 tlb entries can be wired, memory above 16GB
    (well really 15.75GB) must be mapped into kernel page table
    (swapper_pg_dir).

    However, this is trickly, since refill exception could be
    triggered in an exception context. Hence do_page_fault() has
    to patched with the appropriate functionality.

    Besides, the refill handler touches per_cpu memory in non-exception
    context and a host of kernel structures (such as vma_struct,
    task_struct, thread_info, mm_struct etc) in exception context.
    Hence those structures must come from wired area as recursive
    refill exceptions cannot be handled). Towards that end, appropriate
    flags (SLAB_CACHE_DMA) have to be passed to the slab allocator
    while creation and allocation of above objects.

    As part of above changes, the size of DMA region was increased to
    3GB from the earlier 2GB.

Based on Broadcom SDK 2.3.

Signed-off-by: Yonghong Song <ysong@netlogicmicro.com>
Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 arch/mips/include/asm/dma.h               |    2 +-
 arch/mips/include/asm/mach-netlogic/mmu.h |   25 +++++
 arch/mips/include/asm/pgalloc.h           |   16 +++
 arch/mips/include/asm/thread_info.h       |   14 ++-
 arch/mips/kernel/setup.c                  |    2 +-
 arch/mips/kernel/vmlinux.lds.S            |    2 +-
 arch/mips/mm/fault.c                      |   11 ++
 arch/mips/mm/init.c                       |    8 ++
 arch/mips/netlogic/common/memory.c        |  156 +++++++++++++++++++++++++++++
 fs/exec.c                                 |    4 +
 include/linux/gfp.h                       |    5 +
 kernel/fork.c                             |   22 ++++
 mm/memory.c                               |   15 +++
 mm/mmap.c                                 |   12 ++
 mm/page_alloc.c                           |    8 ++
 15 files changed, 297 insertions(+), 5 deletions(-)

diff --git a/arch/mips/include/asm/dma.h b/arch/mips/include/asm/dma.h
index 4289d2c..57bb45f 100644
--- a/arch/mips/include/asm/dma.h
+++ b/arch/mips/include/asm/dma.h
@@ -88,7 +88,7 @@
 #define MAX_DMA_ADDRESS		PAGE_OFFSET
 #else
 #if defined(CONFIG_NLM_XLP) && defined(CONFIG_64BIT)
-#define MAX_DMA_ADDRESS		(PAGE_OFFSET + 0x80000000)
+#define MAX_DMA_ADDRESS		(PAGE_OFFSET + 0xc0000000)
 #else
 #define MAX_DMA_ADDRESS		(PAGE_OFFSET + 0x01000000)
 #endif
diff --git a/arch/mips/include/asm/mach-netlogic/mmu.h b/arch/mips/include/asm/mach-netlogic/mmu.h
index ec69f2c..2377955 100644
--- a/arch/mips/include/asm/mach-netlogic/mmu.h
+++ b/arch/mips/include/asm/mach-netlogic/mmu.h
@@ -17,9 +17,22 @@
 #define SMALLEST_TLBPAGE_SZ (4UL << 10)
 #define LARGEST_TLBPAGE_SZ  (256UL << 20)
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+#define MAX_WIRED_PFN PFN_UP(4UL << 30)
+#endif
+
 #define TRUE 1
 #define FALSE 0
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+typedef struct
+{
+	unsigned long entryHi;
+	unsigned long entrylo0;
+	unsigned long entrylo1;
+	int wired;
+} tlb_entry_t;
+#else
 typedef struct
 {
 	unsigned long vaddr;
@@ -30,6 +43,7 @@ typedef struct
 	uint32_t attr1;
 	int wired;
 } tlb_info_t;
+#endif
 
 #ifdef CONFIG_MAPPED_KERNEL
 extern unsigned long __vmalloc_start;
@@ -37,7 +51,10 @@ extern unsigned long __vmalloc_start;
 extern unsigned long long nlm_common_tlb_stats[];
 
 extern void mmu_init(void);
+
+#ifndef CONFIG_NLM_16G_MEM_SUPPORT
 extern void setup_tlb(tlb_info_t *tlb);
+#endif
 
 /*
  * the following needs an used argument to confirm to the 
@@ -58,5 +75,13 @@ extern unsigned long recalculate_max_low_pfn(unsigned long max_low_pfn);
 extern void __init nlm_numa_bootmem_init(unsigned long);
 #endif
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+extern int map_kernel_addrspace(unsigned long vaddr, unsigned long paddr,
+				unsigned long max_pfn);
+
+#define KERNEL_PAGE_ATTR \
+	(_CACHE_CACHABLE_COW |_PAGE_DIRTY |  _PAGE_VALID | _PAGE_GLOBAL)
+#endif
+
 #endif /* __ASSEMBLY__ */
 #endif
diff --git a/arch/mips/include/asm/pgalloc.h b/arch/mips/include/asm/pgalloc.h
index 881d18b..a94f8a2 100644
--- a/arch/mips/include/asm/pgalloc.h
+++ b/arch/mips/include/asm/pgalloc.h
@@ -48,7 +48,11 @@ static inline pgd_t *pgd_alloc(struct mm_struct *mm)
 {
 	pgd_t *ret, *init;
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+	ret = (pgd_t *) __get_free_pages(GFP_DMA|GFP_KERNEL, PGD_ORDER);
+#else
 	ret = (pgd_t *) __get_free_pages(GFP_KERNEL, PGD_ORDER);
+#endif
 	if (ret) {
 		init = pgd_offset(&init_mm, 0UL);
 		pgd_init((unsigned long)ret);
@@ -69,7 +73,11 @@ static inline pte_t *pte_alloc_one_kernel(struct mm_struct *mm,
 {
 	pte_t *pte;
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+	pte = (pte_t *) __get_free_pages(GFP_DMA|GFP_KERNEL|__GFP_REPEAT|__GFP_ZERO, PTE_ORDER);
+#else
 	pte = (pte_t *) __get_free_pages(GFP_KERNEL|__GFP_REPEAT|__GFP_ZERO, PTE_ORDER);
+#endif
 
 	return pte;
 }
@@ -79,7 +87,11 @@ static inline struct page *pte_alloc_one(struct mm_struct *mm,
 {
 	struct page *pte;
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+	pte = alloc_pages(GFP_DMA | GFP_KERNEL | __GFP_REPEAT, PTE_ORDER);
+#else
 	pte = alloc_pages(GFP_KERNEL | __GFP_REPEAT, PTE_ORDER);
+#endif
 	if (pte) {
 		clear_highpage(pte);
 		pgtable_page_ctor(pte);
@@ -110,7 +122,11 @@ static inline pmd_t *pmd_alloc_one(struct mm_struct *mm, unsigned long address)
 {
 	pmd_t *pmd;
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+	pmd = (pmd_t *) __get_free_pages(GFP_DMA|GFP_KERNEL|__GFP_REPEAT, PMD_ORDER);
+#else
 	pmd = (pmd_t *) __get_free_pages(GFP_KERNEL|__GFP_REPEAT, PMD_ORDER);
+#endif
 	if (pmd)
 		pmd_init((unsigned long)pmd, (unsigned long)invalid_pte_table);
 	return pmd;
diff --git a/arch/mips/include/asm/thread_info.h b/arch/mips/include/asm/thread_info.h
index 9778e32..348f525 100644
--- a/arch/mips/include/asm/thread_info.h
+++ b/arch/mips/include/asm/thread_info.h
@@ -60,8 +60,6 @@ struct thread_info {
 register struct thread_info *__current_thread_info __asm__("$28");
 #define current_thread_info()  __current_thread_info
 
-#endif /* !__ASSEMBLY__ */
-
 /* thread information allocation */
 #if defined(CONFIG_PAGE_SIZE_4KB) && defined(CONFIG_32BIT)
 #define THREAD_SIZE_ORDER (1)
@@ -90,15 +88,27 @@ register struct thread_info *__current_thread_info __asm__("$28");
 #define __HAVE_ARCH_THREAD_INFO_ALLOCATOR
 
 #ifdef CONFIG_DEBUG_STACK_USAGE
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+#define alloc_thread_info_node(tsk, node) \
+		kzalloc_node(THREAD_SIZE, GFP_KERNEL | GFP_DMA, node)
+#else /* CONFIG_NLM_16G_MEM_SUPPORT */
 #define alloc_thread_info_node(tsk, node) \
 		kzalloc_node(THREAD_SIZE, GFP_KERNEL, node)
+#endif /* CONFIG_NLM_16G_MEM_SUPPORT */
 #else
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+#define alloc_thread_info_node(tsk, node) \
+		kmalloc_node(THREAD_SIZE, GFP_KERNEL | GFP_DMA, node)
+#else /* CONFIG_NLM_16G_MEM_SUPPORT */
 #define alloc_thread_info_node(tsk, node) \
 		kmalloc_node(THREAD_SIZE, GFP_KERNEL, node)
+#endif /* CONFIG_NLM_16G_MEM_SUPPORT */
 #endif
 
 #define free_thread_info(info) kfree(info)
 
+#endif /* !__ASSEMBLY__ */
+
 #define PREEMPT_ACTIVE		0x10000000
 
 /*
diff --git a/arch/mips/kernel/setup.c b/arch/mips/kernel/setup.c
index 0659dc3..2384ce9 100644
--- a/arch/mips/kernel/setup.c
+++ b/arch/mips/kernel/setup.c
@@ -589,7 +589,7 @@ static void __init arch_mem_init(char **cmdline_p)
 	}
     
 	bootmem_init();
-#if defined(CONFIG_NLM_XLP) && !defined(CONFIG_NUMA)
+#if defined(CONFIG_NLM_XLP) && !defined(CONFIG_NUMA) && !defined(CONFIG_NLM_16G_MEM_SUPPORT)
 	setup_mapped_kernel_tlbs(FALSE, TRUE);
 #endif
 	device_tree_init();
diff --git a/arch/mips/kernel/vmlinux.lds.S b/arch/mips/kernel/vmlinux.lds.S
index c0c051c..cbbf050 100644
--- a/arch/mips/kernel/vmlinux.lds.S
+++ b/arch/mips/kernel/vmlinux.lds.S
@@ -78,7 +78,7 @@ SECTIONS
 	.data : {	/* Data */
 		. = . + DATAOFFSET;		/* for CONFIG_MAPPED_KERNEL */
 
-		INIT_TASK_DATA(THREAD_SIZE)
+		INIT_TASK_DATA(PAGE_SIZE)
 		NOSAVE_DATA
 		CACHELINE_ALIGNED_DATA(1 << CONFIG_MIPS_L1_CACHE_SHIFT)
 		READ_MOSTLY_DATA(1 << CONFIG_MIPS_L1_CACHE_SHIFT)
diff --git a/arch/mips/mm/fault.c b/arch/mips/mm/fault.c
index daba703..b11f39e 100644
--- a/arch/mips/mm/fault.c
+++ b/arch/mips/mm/fault.c
@@ -88,6 +88,11 @@ asmlinkage void __kprobes do_page_fault(struct pt_regs *regs, unsigned long writ
 # define VMALLOC_FAULT_TARGET vmalloc_fault
 #endif
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+	if (unlikely(address >= NONWIRED_START && address < NONWIRED_END))
+		goto refill_kernel_tlb;
+#endif
+
 	if (unlikely(address >= VMALLOC_START && address <= VMALLOC_END))
 		goto VMALLOC_FAULT_TARGET;
 #ifdef MODULE_START
@@ -323,4 +328,10 @@ vmalloc_fault:
 		return;
 	}
 #endif
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+refill_kernel_tlb:
+	update_kernel_tlb(address);
+	current->thread.cp0_baduaddr = address;
+	return;
+#endif
 }
diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 1a85ba9..f3143b6 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -88,7 +88,11 @@ unsigned long setup_zero_pages(void)
 	else
 		order = 0;
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+	empty_zero_page = __get_free_pages(GFP_DMA | GFP_KERNEL | __GFP_ZERO, order);
+#else
 	empty_zero_page = __get_free_pages(GFP_KERNEL | __GFP_ZERO, order);
+#endif
 	if (!empty_zero_page)
 		panic("Oh boy, that early out of memory?");
 
@@ -331,6 +335,10 @@ void __init paging_init(void)
 
 	pagetable_init();
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+	setup_mapped_kernel_tlbs(FALSE, TRUE);
+#endif
+
 #ifdef CONFIG_HIGHMEM
 	kmap_init();
 #endif
diff --git a/arch/mips/netlogic/common/memory.c b/arch/mips/netlogic/common/memory.c
index cac5856..2aa5f2c 100644
--- a/arch/mips/netlogic/common/memory.c
+++ b/arch/mips/netlogic/common/memory.c
@@ -35,6 +35,11 @@
 #include <asm/page.h>
 #include <asm/mach-netlogic/mmu.h>
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+#include <linux/bootmem.h>
+#include <asm/pgtable.h>
+#endif
+
 /*
  * the following structures and definitions are internal to this
  * file and hence not defined in a header file
@@ -81,6 +86,30 @@ static uint32_t tlb_mask(uint32_t size)
 	return mipstlbs[i].mask;
 }
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+#define page_entrylo(paddr, attr) ((((paddr) >> 12) << 6) | (attr))
+
+/*
+ * External Function / APIs
+ */
+
+static void setup_tlb(tlb_entry_t *tlb, unsigned long pagesize)
+{
+	write_c0_pagemask(tlb_mask(pagesize) << 13);
+	write_c0_entryhi(tlb->entryHi & ~0x1fff);
+	write_c0_entrylo0(tlb->entrylo0);
+	write_c0_entrylo1(tlb->entrylo1);
+
+	if (tlb->wired) {
+		write_c0_index(read_c0_wired());
+		tlb_write_indexed();
+		write_c0_wired(read_c0_wired() + 1);
+	}
+	else {
+		tlb_write_random();
+	}
+}
+#else
 #define entrylo(paddr, attr) \
 	((((paddr & 0xffffffffffULL) >> 12) << 6) | (attr))
 
@@ -105,6 +134,7 @@ void setup_tlb(tlb_info_t *tlb)
 		tlb_write_random();
 	}
 }
+#endif
 
 #ifdef CONFIG_MAPPED_KERNEL
 
@@ -126,6 +156,130 @@ EXPORT_SYMBOL(__vmalloc_start);
 static volatile int max_low_pfn_set = 0;
 extern unsigned long max_low_pfn;
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+
+#ifdef CONFIG_64BIT
+#define KERNEL_SEG_START	XKSEG
+#else
+#define KERNEL_SEG_START	KSEG2
+#endif
+
+#define MIN(x,y)  ((x) < (y) ? (x) : (y))
+
+#undef alloc_bootmem_low
+
+static unsigned long
+alloc_bootmem_low(gfp_t gfp_mask, unsigned int order)
+{
+	return (unsigned long)__alloc_bootmem_low((1 << order) * PAGE_SIZE, SMP_CACHE_BYTES, 0);
+}
+
+unsigned long NONWIRED_START = ~0x0;
+unsigned long NONWIRED_END = ~0x0;
+
+void setup_mapped_kernel_tlbs(int firstpage, int primary_cpu)
+{
+	tlb_entry_t tlb;
+	unsigned long max_wired_size;
+	unsigned long pagesize;
+	unsigned long vaddr, paddr;
+	unsigned short attr;
+
+	pagesize = LARGEST_TLBPAGE_SZ; /* we set up the largest pages */
+
+	/*
+	 * In NetLogic's Linux kernel, the second 256MB of physical
+	 * address space is reserved for device configuration and
+	 * is not mapped to DRAM (to imply memory as opposed to IO
+	 * device space). Hence the attribute of the second part of
+	 * the first wired entry is invalid, while the both part of
+	 * other wired entries are symmetric. We handle the above
+	 * difference through the following unseemly if condition
+	 */
+	if (firstpage) {
+		tlb.entryHi = KERNEL_SEG_START;
+
+		attr = (KERNEL_PAGE_ATTR >> ENTRYLO_PFN_SHIFT);
+		tlb.entrylo0 = page_entrylo(0, attr); /* we start at pfn = 0 */
+
+		/* 256MB - 512 MB is IO config (invalid dram) */
+		attr = (_PAGE_GLOBAL >> ENTRYLO_PFN_SHIFT);
+		tlb.entrylo1 = page_entrylo(0, attr);
+
+		tlb.wired = TRUE;
+		setup_tlb(&tlb, pagesize);
+	}
+	else {
+		int retval;
+		/*
+		 * the primary cpu reads the memory map and records
+		 * the highest page frame number. Secondary cpus
+		 * must wait till the variable max_low_pfn is set
+		 */
+		if (!primary_cpu)
+			while (!max_low_pfn_set)
+				;
+		vaddr = KERNEL_SEG_START + 2 * LARGEST_TLBPAGE_SZ;
+		max_wired_size = PFN_PHYS(MIN(max_low_pfn, MAX_WIRED_PFN));
+		attr = (KERNEL_PAGE_ATTR >> ENTRYLO_PFN_SHIFT);
+
+		/*
+		 * the following loop assumes that pagesize is set to
+		 * 256 MB. change the logic if the pagesize ever changes
+		 */
+		paddr = 2 * LARGEST_TLBPAGE_SZ;
+		for (; paddr < max_wired_size;
+ 				paddr += 2 * pagesize, vaddr += 2 * pagesize) {
+#ifdef DEBUG
+			printk("(wired entry): vaddr = 0x%lx, paddr = 0x%lx\n", vaddr, paddr);
+#endif
+/* Skip 3 - 3.5GB range (PCI device space) */
+			if (paddr == PCIDEV_ADDRSPACE_START)
+				continue;
+			tlb.entryHi = vaddr;
+			tlb.entrylo0 = page_entrylo(paddr, attr);
+			tlb.entrylo1 = page_entrylo(paddr + pagesize, attr);
+			tlb.wired = TRUE;
+			setup_tlb(&tlb, pagesize);
+		}
+
+		if (primary_cpu) {
+			if (max_low_pfn > MAX_WIRED_PFN) {
+				__get_free_pages = alloc_bootmem_low;
+				retval = map_kernel_addrspace(vaddr, MAX_WIRED_PFN, max_low_pfn);
+				if (retval != 0)
+					panic("unable to map kernel addrspace\n");
+				NONWIRED_START = vaddr;
+				NONWIRED_END = vaddr + (PFN_PHYS(max_low_pfn - MAX_WIRED_PFN));
+				__get_free_pages = ____get_free_pages;
+			}
+#ifdef CONFIG_64BIT
+			__vmalloc_start = KERNEL_SEG_START + (1UL << PGDIR_SHIFT);
+#else
+			__vmalloc_start = vaddr;
+#endif
+		}
+	}
+}
+
+#define FLOOR(addr, alignment) ((addr) & ~((alignment) - 1)) /* alignment must be power of 2 */
+
+unsigned long recalculate_max_low_pfn(unsigned long max_low_pfn)
+{
+	/*
+	 * truncate max_low_pfn to 512MB boundary as largest tlb
+	 * pages are used to minimize the number of wired entries
+	 */
+	if ((max_low_pfn > PFN_DOWN(LARGEST_TLBPAGE_SZ << 1)) && (max_low_pfn <= MAX_WIRED_PFN))
+		max_low_pfn = PFN_DOWN(FLOOR(PFN_PHYS(max_low_pfn), LARGEST_TLBPAGE_SZ << 1));
+	max_low_pfn_set = TRUE;
+	__sync();
+
+	return max_low_pfn;
+}
+
+#else
+
 #ifdef CONFIG_64BIT
 #define TLB_VADDR	XKSEG	
 #else
@@ -197,6 +351,8 @@ unsigned long recalculate_max_low_pfn(unsigned long max_low_pfn)
 	return max_low_pfn;
 }
 
+#endif
+
 #else
 
 void setup_mapped_kernel_tlbs(int index, int secondary_cpu) { }
diff --git a/fs/exec.c b/fs/exec.c
index 9c5157a..e7c945b 100644
--- a/fs/exec.c
+++ b/fs/exec.c
@@ -266,7 +266,11 @@ static int __bprm_mm_init(struct linux_binprm *bprm)
 	struct vm_area_struct *vma = NULL;
 	struct mm_struct *mm = bprm->mm;
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+	bprm->vma = vma = kmem_cache_zalloc(vm_area_cachep, GFP_KERNEL | GFP_DMA);
+#else
 	bprm->vma = vma = kmem_cache_zalloc(vm_area_cachep, GFP_KERNEL);
+#endif
 	if (!vma)
 		return -ENOMEM;
 
diff --git a/include/linux/gfp.h b/include/linux/gfp.h
index 1e49be4..ac3277b 100644
--- a/include/linux/gfp.h
+++ b/include/linux/gfp.h
@@ -343,7 +343,12 @@ extern struct page *alloc_pages_vma(gfp_t gfp_mask, int order,
 #define alloc_page_vma_node(gfp_mask, vma, addr, node)		\
 	alloc_pages_vma(gfp_mask, 0, vma, addr, node)
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+extern unsigned long (*__get_free_pages)(gfp_t gfp_mask, unsigned int order);
+extern unsigned long ____get_free_pages(gfp_t gfp_mask, unsigned int order);
+#else
 extern unsigned long __get_free_pages(gfp_t gfp_mask, unsigned int order);
+#endif
 extern unsigned long get_zeroed_page(gfp_t gfp_mask);
 
 void *alloc_pages_exact(size_t size, gfp_t gfp_mask);
diff --git a/kernel/fork.c b/kernel/fork.c
index 48f107d..eb91a80 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -116,8 +116,13 @@ int nr_processes(void)
 }
 
 #ifndef __HAVE_ARCH_TASK_STRUCT_ALLOCATOR
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+# define alloc_task_struct_node(node)		\
+		kmem_cache_alloc_node(task_struct_cachep, GFP_KERNEL | GFP_DMA, node)
+#else /* CONFIG_NLM_16G_MEM_SUPPORT */
 # define alloc_task_struct_node(node)		\
 		kmem_cache_alloc_node(task_struct_cachep, GFP_KERNEL, node)
+#endif /* CONFIG_NLM_16G_MEM_SUPPORT */
 # define free_task_struct(tsk)			\
 		kmem_cache_free(task_struct_cachep, (tsk))
 static struct kmem_cache *task_struct_cachep;
@@ -223,10 +228,16 @@ void __init fork_init(unsigned long mempages)
 #define ARCH_MIN_TASKALIGN	L1_CACHE_BYTES
 #endif
 	/* create a slab on which task_structs can be allocated */
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+	task_struct_cachep =
+		kmem_cache_create("task_struct", sizeof(struct task_struct),
+			ARCH_MIN_TASKALIGN, SLAB_PANIC | SLAB_NOTRACK | SLAB_CACHE_DMA, NULL);
+#else
 	task_struct_cachep =
 		kmem_cache_create("task_struct", sizeof(struct task_struct),
 			ARCH_MIN_TASKALIGN, SLAB_PANIC | SLAB_NOTRACK, NULL);
 #endif
+#endif
 
 	/* do the arch specific task caches init */
 	arch_task_cache_init();
@@ -467,7 +478,11 @@ static inline void mm_free_pgd(struct mm_struct *mm)
 
 __cacheline_aligned_in_smp DEFINE_SPINLOCK(mmlist_lock);
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+#define allocate_mm()	(kmem_cache_alloc(mm_cachep, GFP_KERNEL | GFP_DMA))
+#else
 #define allocate_mm()	(kmem_cache_alloc(mm_cachep, GFP_KERNEL))
+#endif
 #define free_mm(mm)	(kmem_cache_free(mm_cachep, (mm)))
 
 static unsigned long default_dump_filter = MMF_DUMP_FILTER_DEFAULT;
@@ -1697,10 +1712,17 @@ void __init proc_caches_init(void)
 	 * maximum number of CPU's we can ever have.  The cpumask_allocation
 	 * is at the end of the structure, exactly for that reason.
 	 */
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+	mm_cachep = kmem_cache_create("mm_struct",
+			sizeof(struct mm_struct), ARCH_MIN_MMSTRUCT_ALIGN,
+			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_NOTRACK|SLAB_CACHE_DMA, NULL);
+	vm_area_cachep = KMEM_CACHE(vm_area_struct, SLAB_PANIC|SLAB_CACHE_DMA);
+#else
 	mm_cachep = kmem_cache_create("mm_struct",
 			sizeof(struct mm_struct), ARCH_MIN_MMSTRUCT_ALIGN,
 			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_NOTRACK, NULL);
 	vm_area_cachep = KMEM_CACHE(vm_area_struct, SLAB_PANIC);
+#endif
 	mmap_init();
 	nsproxy_cache_init();
 }
diff --git a/mm/memory.c b/mm/memory.c
index 0c4347e..fc4bd89 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -2212,9 +2212,19 @@ static int remap_pte_range(struct mm_struct *mm, pmd_t *pmd,
 			unsigned long pfn, pgprot_t prot)
 {
 	pte_t *pte;
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+	spinlock_t *uninitialized_var(ptl);
+#else
 	spinlock_t *ptl;
+#endif
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+	pte = (mm == &init_mm) ?
+		pte_alloc_kernel(pmd, addr) :
+		pte_alloc_map_lock(mm, pmd, addr, &ptl);
+#else
 	pte = pte_alloc_map_lock(mm, pmd, addr, &ptl);
+#endif
 	if (!pte)
 		return -ENOMEM;
 	arch_enter_lazy_mmu_mode();
@@ -2224,7 +2234,12 @@ static int remap_pte_range(struct mm_struct *mm, pmd_t *pmd,
 		pfn++;
 	} while (pte++, addr += PAGE_SIZE, addr != end);
 	arch_leave_lazy_mmu_mode();
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+	if (mm != &init_mm)
+		pte_unmap_unlock(pte - 1, ptl);
+#else
 	pte_unmap_unlock(pte - 1, ptl);
+#endif
 	return 0;
 }
 
diff --git a/mm/mmap.c b/mm/mmap.c
index 353883d..2a527bd 100644
--- a/mm/mmap.c
+++ b/mm/mmap.c
@@ -1309,7 +1309,11 @@ munmap_back:
 	 * specific mapper. the address has already been validated, but
 	 * not unmapped, but the maps are removed from the list.
 	 */
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+	vma = kmem_cache_zalloc(vm_area_cachep, GFP_KERNEL | GFP_DMA);
+#else
 	vma = kmem_cache_zalloc(vm_area_cachep, GFP_KERNEL);
+#endif
 	if (!vma) {
 		error = -ENOMEM;
 		goto unacct_error;
@@ -2429,7 +2433,11 @@ struct vm_area_struct *copy_vma(struct vm_area_struct **vmap,
 		} else
 			anon_vma_moveto_tail(new_vma);
 	} else {
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+		new_vma = kmem_cache_alloc(vm_area_cachep, GFP_KERNEL | GFP_DMA);
+#else
 		new_vma = kmem_cache_alloc(vm_area_cachep, GFP_KERNEL);
+#endif
 		if (new_vma) {
 			*new_vma = *vma;
 			pol = mpol_dup(vma_policy(vma));
@@ -2534,7 +2542,11 @@ int install_special_mapping(struct mm_struct *mm,
 	int ret;
 	struct vm_area_struct *vma;
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+	vma = kmem_cache_zalloc(vm_area_cachep, GFP_KERNEL | GFP_DMA);
+#else
 	vma = kmem_cache_zalloc(vm_area_cachep, GFP_KERNEL);
+#endif
 	if (unlikely(vma == NULL))
 		return -ENOMEM;
 
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 0a55b3c..81db8b2 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -2541,7 +2541,11 @@ EXPORT_SYMBOL(__alloc_pages_nodemask);
 /*
  * Common helper functions.
  */
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+unsigned long ___get_free_pages(gfp_t gfp_mask, unsigned int order)
+#else
 unsigned long __get_free_pages(gfp_t gfp_mask, unsigned int order)
+#endif
 {
 	struct page *page;
 
@@ -2556,6 +2560,10 @@ unsigned long __get_free_pages(gfp_t gfp_mask, unsigned int order)
 		return 0;
 	return (unsigned long) page_address(page);
 }
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+unsigned long (*__get_free_pages)(gfp_t gfp_mask, unsigned int order)
+	= ____get_free_pages;
+#endif
 EXPORT_SYMBOL(__get_free_pages);
 
 unsigned long get_zeroed_page(gfp_t gfp_mask)
-- 
1.7.0.4

