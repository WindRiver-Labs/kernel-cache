From 0550dd06cda6c2c820eb19353659e7fa30801bf4 Mon Sep 17 00:00:00 2001
From: henry shao <hshao@netlogicmicro.com>
Date: Fri, 21 May 2010 16:26:57 -0700
Subject: [PATCH 030/762] add config_net.c irq.c mmu.c nmi.S on_chip.c platform.c setup.c smpboot.S smp.c time.c to netlogic/xlr directory

Based on Broadcom SDK 2.3.

Signed-off-by: henry shao <hshao@netlogicmicro.com>
Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 arch/mips/netlogic/xlr/config_net.c |  555 +++++++++++
 arch/mips/netlogic/xlr/irq.c        |  652 ++++++++++++
 arch/mips/netlogic/xlr/nmi.S        |  105 ++
 arch/mips/netlogic/xlr/on_chip.c    |  817 +++++++++++++++
 arch/mips/netlogic/xlr/platform.c   |  160 ++--
 arch/mips/netlogic/xlr/setup.c      | 1854 ++++++++++++++++++++++++++++++++---
 arch/mips/netlogic/xlr/smp.c        |  239 +++++
 arch/mips/netlogic/xlr/smpboot.S    |   68 ++
 arch/mips/netlogic/xlr/time.c       |  224 +++++
 9 files changed, 4457 insertions(+), 217 deletions(-)
 create mode 100644 arch/mips/netlogic/xlr/config_net.c
 create mode 100644 arch/mips/netlogic/xlr/irq.c
 create mode 100644 arch/mips/netlogic/xlr/nmi.S
 create mode 100644 arch/mips/netlogic/xlr/on_chip.c
 create mode 100644 arch/mips/netlogic/xlr/smp.c
 create mode 100644 arch/mips/netlogic/xlr/smpboot.S
 create mode 100644 arch/mips/netlogic/xlr/time.c

diff --git a/arch/mips/netlogic/xlr/config_net.c b/arch/mips/netlogic/xlr/config_net.c
new file mode 100644
index 0000000..037faca
--- /dev/null
+++ b/arch/mips/netlogic/xlr/config_net.c
@@ -0,0 +1,555 @@
+/***********************************************************************
+Copyright 2003-2010 Netlogic Microsystems (“Netlogic”). All rights
+reserved.
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are
+met:
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+*****************************#NETL_2#********************************/
+
+/*
+ * Setup code for Netlogic's XLR-based boards
+ */
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/interrupt.h>
+#include <linux/fs.h>
+#include <linux/types.h>
+#include <linux/string.h>
+#include <linux/socket.h>
+#include <linux/errno.h>
+#include <asm/netlogic/sim.h>
+#include <asm/netlogic/config_net.h>
+#include <asm/netlogic/xlr_mac.h>
+#include <asm/netlogic/gpio.h>
+
+struct net_device_cfg xlr_net_dev_cfg;
+extern unsigned long netlogic_io_base;
+/*
+extern uint32_t dev_tree_en;
+extern void nlm_dev_config_net(void);
+*/
+static uint32_t gmac_offsets[] = { NETLOGIC_IO_GMAC_0_OFFSET, NETLOGIC_IO_GMAC_1_OFFSET, 
+			NETLOGIC_IO_GMAC_2_OFFSET, NETLOGIC_IO_GMAC_3_OFFSET,
+			NETLOGIC_IO_GMAC_4_OFFSET, NETLOGIC_IO_GMAC_5_OFFSET,
+			NETLOGIC_IO_GMAC_6_OFFSET, NETLOGIC_IO_GMAC_7_OFFSET };
+
+#if !defined(XLP_SIM)
+static uint32_t gmac_irqs[] = { PIC_GMAC_0_IRQ, PIC_GMAC_1_IRQ, 
+			PIC_GMAC_2_IRQ, PIC_GMAC_3_IRQ,
+			PIC_GMAC_4_IRQ, PIC_GMAC_5_IRQ,
+			PIC_GMAC_6_IRQ, PIC_GMAC_7_IRQ };
+
+static uint32_t xgmac_offsets[] = { NETLOGIC_IO_XGMAC_0_OFFSET, NETLOGIC_IO_XGMAC_1_OFFSET };
+static uint32_t spi4_offsets[] = { NETLOGIC_IO_SPI4_0_OFFSET, NETLOGIC_IO_SPI4_1_OFFSET };
+static uint32_t xgs_irqs[] = { PIC_XGS_0_IRQ, PIC_XGS_1_IRQ };
+#endif /* XLP_SIM */
+
+#define MAX_NUM_DESC		512
+#define NLM_BASE(x) (netlogic_io_base + x)
+
+extern int xlr_loader_support;
+extern int xlr_loader_sharedcore;
+extern int xlr_loader_own_gmac;
+/*
+   This functions returns:
+   True (1): if block is in XAUI mode
+   False(0): if block is in GMAC mode
+*/
+
+int xlsb0_in_xaui(int block)
+{
+    unsigned int gpio_xaui = 0;
+    nlm_reg_t *gpio_mmio =
+                    (unsigned int *)(netlogic_io_base + NETLOGIC_IO_GPIO_OFFSET);
+    if (xlr_board_atx_xi() || xlr_board_atx_xii()) {
+        gpio_xaui = ((netlogic_read_reg(gpio_mmio,21) >> 24) & 0x3);
+        switch(gpio_xaui){
+            case 0x1:
+                return block==0?1:0;
+            case 0x2:
+                 return block==1?1:0;
+            case 0x3:
+                 return 1;
+            default:
+                return 0;
+        }
+    }
+    return 0;
+}
+
+static int sgmii_daughter_card_present(int block)
+{
+	unsigned long cpld_base = (unsigned long)(NETLOGIC_CPLD_OFFSET);
+	unsigned char *mmio = (unsigned char*)cpld_base;
+	unsigned char value = mmio[0x0d];
+	value = value & 0x03;
+
+	switch (block)
+	{
+		case 0:
+			if ((value == 0x0) || (value == 0x1))
+				return 1;
+			break;
+		case 1:
+			if ((value == 0x0) || (value == 0x2))
+				return 1;
+			break;
+		default:
+			return 0;
+	}
+	return 0;
+}
+
+/* This arrray is indexed with the processor id 8bits */
+char nlm_chip_gmac_count[256];
+void init_gmac_ports(void)
+{
+	int processor_id;
+
+    processor_id = ((read_c0_prid() & 0xff00) >> 8);
+
+    /* Currently handle for XLS B0 parts... */
+    switch(processor_id) {
+        case CHIP_PROCESSOR_ID_XLS_616_B0:
+        case CHIP_PROCESSOR_ID_XLS_608_B0:
+        case CHIP_PROCESSOR_ID_XLS_416_B0:
+        case CHIP_PROCESSOR_ID_XLS_412_B0:
+        case CHIP_PROCESSOR_ID_XLS_408_B0:
+        case CHIP_PROCESSOR_ID_XLS_404_B0:
+            nlm_chip_gmac_count[processor_id] = 8;
+            break;
+
+        default:
+            break;
+    }
+}
+int xlr_is_mac_active(int instance, int type, int *mode)
+{
+    uint32_t *gpio_base = (uint32_t *)(DEFAULT_NETLOGIC_IO_BASE +
+                                    NETLOGIC_IO_GPIO_OFFSET);    
+	int processor_id;
+    int xaui_board = 0;
+
+
+    if(xlr_board_atx_xi() || xlr_board_atx_xii())
+        xaui_board = 1;
+
+	*mode = PHY_MODE_RGMII;
+
+	/* On XLS xgmac is not available */
+	if (is_xls()) {
+		if(type == TYPE_XGMAC || type == TYPE_SPI4)
+                return 0;
+
+		processor_id = ((read_c0_prid() & 0xff00) >> 8);
+
+		*mode = PHY_MODE_SGMII;
+
+		if(instance == 0) {
+			/* Lite board does not have rgmii ifc */
+			if(xlr_board_atx_viii())
+				*mode = PHY_MODE_SGMII;
+			/* atx-xi/xii boards: SGMII mode for gmac 0 only if 
+			   daughter card for gmac block-0 is present.
+			   */
+			else if((xlr_board_atx_xi() || xlr_board_atx_xii()) &&
+					(!sgmii_daughter_card_present(0)))
+				*mode = PHY_MODE_RGMII;
+			else
+				*mode = PHY_MODE_RGMII | PHY_MODE_SELECTABLE;
+		}
+
+		if(is_xls_b0() && xaui_board){
+			if(instance < 4){
+                /* If port is not in XAUI mode, this board does not have SGMII.
+                   Only port 0 is in RGMII mode
+                   */
+				if(xlsb0_in_xaui(0)) {
+                    *mode = PHY_MODE_XAUI; /* else mode is set above */
+                    printk("Port %d is in XAUI mode\n", instance);
+                    if(instance == 0)
+                        return 1;
+                    else
+                        return 0;
+                } else
+				{
+					/* return TRUE if (instance == 0) OR
+					   if daughter card for block0 is 
+					   present 
+					 */
+					if ((instance == 0) || 
+						sgmii_daughter_card_present(0))
+						return 1;
+					else
+						return 0;
+				}
+			}
+			else if(instance < 8){
+                /* If port is not in XAUI mode, this board does not have SGMII*/
+				if(xlsb0_in_xaui(1)) {
+                    *mode = PHY_MODE_XAUI;
+                    printk("Port %d is in XAUI mode\n", instance);
+                    if(instance == 4)
+                        return 1;
+                    else
+                        return 0;
+                } else 
+				{
+					/* return TRUE only if daughter card 
+					   for block 1 is present */
+					if (sgmii_daughter_card_present(1))
+						return 1;
+					else
+						return 0;
+				}
+			}else
+				return 0;
+		}
+
+		/* all XLS parts have gmac0, gmac1 */
+		if (instance < 2)
+				return 1;
+
+		if (processor_id <= CHIP_PROCESSOR_ID_XLS_104) {
+			/* all XLS parts with processor_id <=104, have gmac3 */
+			if (instance < 3) 	
+				return 1;
+		}
+		if (processor_id <= CHIP_PROCESSOR_ID_XLS_204) {
+			/* all XLS parts with processor_id <=204, have gmac4 */
+			if (instance < 4) 	
+				return 1;
+		}
+
+		if ((processor_id >= CHIP_PROCESSOR_ID_XLS_608) &&
+		    (processor_id < CHIP_PROCESSOR_ID_XLS_208)) {
+			if (instance < 6) 
+				return 1;
+
+			if(((gpio_base[NETLOGIC_GPIO_FUSE_BANK_REG] & (1<<28)) == 0)  &&
+    	            ((gpio_base[NETLOGIC_GPIO_FUSE_BANK_REG] & (1<<29)) ==  0)){
+				/*Below bits are set when ports are disabled.
+				28 - GMAC7
+				29 - GMAC6
+				30 - GMAC5
+				31 - GMAC4
+				*/
+				/*We found an XLS-408 with 8 gmacs*/
+				if (instance < 8) 
+					return 1;
+			}
+		}
+		if (processor_id == CHIP_PROCESSOR_ID_XLS_608) {
+			if (instance == 6 || instance == 7) 
+				return 1;
+		}
+
+        if(nlm_chip_gmac_count[processor_id] &&
+                (instance < nlm_chip_gmac_count[processor_id]))
+            return 1;
+
+		/* should never come here */
+		return 0;
+	}
+	
+	if (type == TYPE_GMAC) {
+		/* On XLR gmac4 to gmac7 are unavailable */
+		if(instance >= 4)
+			return 0;
+
+		/* On ATX-II, gmac 0 and gmac 1 are not available */
+		if (xlr_board_atx_ii() && !xlr_board_atx_ii_b()) {
+			if(instance < 2)
+				return 0;
+		}
+
+		/* On ATX-IV-B and ATX-V, gmac 3 is not available */
+		if ((xlr_board_atx_v() || xlr_board_atx_iv_b())) {
+			if(instance > 2)
+				return 0;
+		}
+
+		return 1;
+
+	} else if(type == TYPE_XGMAC) {
+		/* On ATX-II and ATX IIB 2 xgmac is  available */
+		if (xlr_board_atx_ii() || xlr_board_atx_ii_b())
+			return 1;
+		return 0;
+
+	}  else if(type == TYPE_SPI4) {
+		if(xlr_board_atx_i())
+			return 1;
+		return 0;
+	}
+	
+	return 0;
+}
+
+int xlr_get_phy_info(int instance, int mode, unsigned long *mii_addr, 
+					unsigned long *pcs_addr, unsigned long *serdes_addr)
+{
+	uint32_t phy_addr;
+
+	*pcs_addr = 0x0;
+	*serdes_addr = 0x0;
+	*mii_addr = NLM_BASE(gmac_offsets[0]);
+
+	if(is_xls()) {
+		if(instance < NETLOGIC_GMAC_PORTS_PER_CTRL) {
+			*pcs_addr = NLM_BASE(gmac_offsets[0]);
+		} else  {
+			*pcs_addr = NLM_BASE(gmac_offsets[NETLOGIC_GMAC_PORTS_PER_CTRL]);
+		}
+		*serdes_addr = NLM_BASE(gmac_offsets[0]);
+
+		if(mode & PHY_MODE_RGMII) {
+			phy_addr = 0 + instance;
+			/*only atx-vi has rgmii-0 linked to sgmii-4 offset*/
+			if(xlr_board_atx_vi())
+				*mii_addr = NLM_BASE(gmac_offsets[NETLOGIC_GMAC_PORTS_PER_CTRL]);		
+		} else
+			phy_addr = 0x10 + instance;
+
+		/* boards 11 / 12 may have SGMII ports due to daughter
+		   cards.  In this case, the phy for block-0 is same as 
+		   gmac[0], but phy for block-1 is taken from gmac[4].
+		   Hence, update phy values if gmac instance >= 4.
+		   */
+		if ((xlr_board_atx_xi() || xlr_board_atx_xii()) &&
+				(instance >= NETLOGIC_GMAC_PORTS_PER_CTRL))
+		{
+			*mii_addr = NLM_BASE(gmac_offsets[NETLOGIC_GMAC_PORTS_PER_CTRL]);
+			phy_addr -= NETLOGIC_GMAC_PORTS_PER_CTRL;
+		}
+	} else {
+		if (xlr_board_atx_ii() && !xlr_board_atx_ii_b()) {
+			if(instance < 2)
+					phy_addr =  0;
+			phy_addr = instance - 2;
+		}
+		phy_addr = 0 + instance;
+	}
+	return phy_addr;
+}
+
+void config_net_init(void)
+{
+#if !defined(XLP_SIM)
+	struct net_device_cfg *net_dev = &xlr_net_dev_cfg;
+	int i, mode, gmac_pblock = 0;
+	int num_desc = MAX_NUM_DESC;
+
+    init_gmac_ports();
+	for(i = 0; i < NETLOGIC_MAX_GMACS; i++)  {
+		/* general config for gmac */
+		net_dev->gmac_port[i].instance = i;
+		net_dev->gmac_port[i].irqno = gmac_irqs[i];
+		net_dev->gmac_port[i].config_pde = 1;
+		/* chip specific config for gmac */
+		if(xlr_is_mac_active(i, TYPE_GMAC, &mode) == 1) {
+			net_dev->gmac_port[i].mmio_addr = NLM_BASE(gmac_offsets[i]);
+			net_dev->gmac_port[i].cfg_flag 	= PORT_OWN_LINUX;
+			
+			if(xlr_board_atx_vii() && i==4)
+				/*atx-vii board workaround for mdio-1*/
+				net_dev->gmac_port[i].cfg_flag 	= PORT_OWN_LINUX & ~(NLM_INT_ATTACH);
+
+			if(i >= gmac_pblock) {
+				net_dev->gmac_port[i].num_desc = num_desc;
+				if(is_xls()) {
+					if(i < NETLOGIC_GMAC_PORTS_PER_CTRL) {
+						net_dev->gmac_port[i].bucket = &xls_bucket_sizes.bucket[MSGRNG_STNID_GMAC0];
+						net_dev->gmac_port[i].credit = &xls_cc_table_gmac0;
+					} else {
+						net_dev->gmac_port[i].bucket = &xls_bucket_sizes.bucket[MSGRNG_STNID_GMAC1];
+						net_dev->gmac_port[i].credit = &xls_cc_table_gmac1;
+					}
+				} else {
+					net_dev->gmac_port[i].bucket = &bucket_sizes.bucket[MSGRNG_STNID_GMAC];
+					net_dev->gmac_port[i].credit = &cc_table_gmac;
+				}
+				gmac_pblock += NETLOGIC_GMAC_PORTS_PER_CTRL;
+			}
+ 
+			net_dev->gmac_port[i].phy_mode = mode;
+
+			net_dev->gmac_port[i].phy_addr = xlr_get_phy_info(i, 
+				net_dev->gmac_port[i].phy_mode, 
+				&net_dev->gmac_port[i].mii_addr, 
+				&net_dev->gmac_port[i].pcs_addr, 
+				&net_dev->gmac_port[i].serdes_addr);
+			
+		}
+	}
+
+	/* general config for xgmac */
+	for(i = 0; i < NETLOGIC_MAX_XGMACS; i++)  {
+		net_dev->xgs_port[i].instance = i;
+		net_dev->xgs_port[i].irqno = xgs_irqs[i];
+		net_dev->xgs_port[i].config_pde = 1;
+
+		if(xlr_is_mac_active(i, TYPE_XGMAC, &mode) == 1) {
+			net_dev->xgs_port[i].mmio_addr = NLM_BASE(xgmac_offsets[i]);
+			net_dev->xgs_port[i].cfg_flag 	= PORT_OWN_LINUX;
+			net_dev->xgs_port[i].num_desc = num_desc;
+			net_dev->xgs_type[i] = TYPE_XGMAC;
+
+		} else 	if(xlr_is_mac_active(i, TYPE_SPI4, &mode) == 1) {
+			net_dev->xgs_port[i].mmio_addr = NLM_BASE(spi4_offsets[i]);
+			net_dev->xgs_port[i].cfg_flag 	= PORT_OWN_LINUX;
+			net_dev->xgs_port[i].num_desc = num_desc;
+			net_dev->xgs_type[i] = TYPE_SPI4;
+		}
+		/* as descriptors are discontinues we need to pass the 
+           full list */
+		net_dev->xgs_port[i].bucket = &bucket_sizes.bucket[0];
+		if(i == 0)
+			net_dev->xgs_port[i].credit = &cc_table_xgs_0;
+		else
+			net_dev->xgs_port[i].credit = &cc_table_xgs_1;
+	}
+
+	/* Modify the basic configurations with the options 
+			supported in Linux */
+	/* Loader support */
+	if(xlr_loader_support) {
+		if(xlr_loader_sharedcore) {
+			/* GMAC, XGMAC and SPI4 are owned by apps */
+			for(i = 0; i < NETLOGIC_MAX_XGMACS; i++)  {
+				net_dev->xgs_port[i].cfg_flag     = 0;
+			}
+
+			/* GMAC will be owned by linux if xlr_loader_own_gmac = 1*/
+			for(i = 0; i < NETLOGIC_MAX_GMACS; i++)  {
+				if(!xlr_loader_own_gmac) {
+					net_dev->gmac_port[i].cfg_flag     = 0;
+				} else if(!(is_xls())) {
+					net_dev->gmac_port[i].bucket = &shared_bucket_sizes.bucket[MSGRNG_STNID_GMAC];
+					net_dev->gmac_port[i].credit = &shared_cc_table_gmac;
+				}
+			}
+		}
+	}
+
+	/* usermac support */
+	if(xlr_hybrid_user_mac()) {
+		for(i = 0; i < NETLOGIC_MAX_GMACS; i++)  {
+			if(net_dev->gmac_port[i].mmio_addr == 0)
+				continue;
+			net_dev->gmac_port[i].cfg_flag     = NLM_PORT_INIT;
+		}
+	}
+	if(xlr_hybrid_user_mac() || xlr_hybrid_user_mac_xgmac()) {
+		for(i = 0; i < NETLOGIC_MAX_XGMACS; i++)  {
+			if(net_dev->xgs_port[i].mmio_addr == 0)
+				continue;
+			net_dev->xgs_port[i].cfg_flag = NLM_PORT_INIT;
+		}
+	}
+
+	if(xlr_hybrid_rmios_ipsec()) {
+		/* port should be enabled by rmios apps
+          after configuring the descriptors */
+		for(i = 0; i < NETLOGIC_MAX_GMACS; i++)  {
+			if(net_dev->gmac_port[i].mmio_addr == 0)
+				continue;
+		//	net_dev->gmac_port[i].cfg_flag     = NLM_PORT_INIT | NLM_PORT_ATTACH;
+			net_dev->gmac_port[i].cfg_flag     = NLM_PORT_ATTACH;
+		}
+		for(i = 0; i < NETLOGIC_MAX_XGMACS; i++)  {
+			net_dev->xgs_port[i].cfg_flag     = 0;
+		}
+	}
+
+	if(xlr_hybrid_rmios_tcpip_stack()) {
+		/* port should be enabled by rmios apps
+          after configuring the descriptors */
+		for(i = 0; i < NETLOGIC_MAX_GMACS; i++)  {
+			if(net_dev->gmac_port[i].mmio_addr == 0)
+				continue;
+			net_dev->gmac_port[i].cfg_flag     = NLM_PORT_ATTACH;
+		}
+		for(i = 0; i < NETLOGIC_MAX_XGMACS; i++)  {
+			net_dev->xgs_port[i].cfg_flag     = 0;
+		}
+	}
+
+	/* dev_tree_en */
+/*
+	if(dev_tree_en) {
+		nlm_dev_config_net();
+	}
+*/
+
+	return;
+#endif /* XLP_SIM */
+}
+
+static int __init xlr_mac_desc_setup(char *str)
+{
+	int desc = simple_strtoul(str, 0, 10);
+	struct net_device_cfg *net_dev = &xlr_net_dev_cfg;
+	int i;
+
+	printk("[%s]: str = \"%s\", desc=%d\n", __FUNCTION__, str, desc);
+	if(desc == 0)
+		return 1;
+
+	for(i = 0; i < NETLOGIC_MAX_GMACS; i++) {
+		if(net_dev->gmac_port[i].num_desc != 0)
+			net_dev->gmac_port[i].num_desc = desc;
+	}
+
+	for(i = 0; i < NETLOGIC_MAX_XGMACS; i++)  {
+		if(net_dev->xgs_port[i].num_desc != 0)
+			net_dev->xgs_port[i].num_desc = desc;
+	}
+
+	return 1;
+}
+
+__setup("xlr_mac_desc=", xlr_mac_desc_setup);
+
+static int __init xls_gmac0_sgmii_setup(char *str)
+{
+	struct net_device_cfg *net_dev = &xlr_net_dev_cfg;
+	
+	if (is_xls()) {
+		if(net_dev->gmac_port[0].phy_mode & PHY_MODE_SELECTABLE) {
+			net_dev->gmac_port[0].phy_mode = 
+				(net_dev->gmac_port[0].phy_mode & PHY_MODE_SELECTABLE) | PHY_MODE_SGMII;
+			net_dev->gmac_port[0].phy_addr = xlr_get_phy_info(0, 
+				net_dev->gmac_port[0].phy_mode, 
+				&net_dev->gmac_port[0].mii_addr, 
+				&net_dev->gmac_port[0].pcs_addr, 
+				&net_dev->gmac_port[0].serdes_addr);
+
+			printk("[%s]: *********************************************\n", __FUNCTION__);
+			printk("[%s]: Enabling SGMII mode for gmac0\n", __FUNCTION__);
+			printk("[%s]: *********************************************\n", __FUNCTION__);
+		}
+	}
+
+	return 1;
+}
+__setup("xls_gmac0_sgmii=", xls_gmac0_sgmii_setup);
+
+
diff --git a/arch/mips/netlogic/xlr/irq.c b/arch/mips/netlogic/xlr/irq.c
new file mode 100644
index 0000000..0603208
--- /dev/null
+++ b/arch/mips/netlogic/xlr/irq.c
@@ -0,0 +1,652 @@
+/***********************************************************************
+Copyright 2003-2010 Netlogic Microsystems (“Netlogic”). All rights
+reserved.
+Redistribution and use in source and binary forms, with or without
+modification, are peNLMtted provided that the following conditions are
+met:
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+*****************************#NETL_2#********************************/
+
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/linkage.h>
+#include <linux/interrupt.h>
+#include <linux/spinlock.h>
+#include <linux/mm.h>
+#include <linux/slab.h>
+#include <linux/pci.h>
+#include <linux/msi.h>
+#include <asm/errno.h>
+#include <asm/signal.h>
+#include <asm/system.h>
+#include <asm/ptrace.h>
+#include <asm/kgdb.h>
+#include <asm/mipsregs.h>
+
+#include <asm/netlogic/sim.h>
+#include <asm/netlogic/nlm_srio.h>
+#include <asm/netlogic/msidef.h>
+#include <asm/netlogic/mips-exts.h>
+#include <asm/netlogic/pic.h>
+#include <asm/netlogic/debug.h>
+#include <asm/thread_info.h>
+#include <linux/irq.h>
+#include <asm/netlogic/hal/nlm_hal_pic.h>
+/*
+ * These are the routines that handle all the low level interrupt stuff. 
+ * Actions handled here are: initialization of the interrupt map, requesting of
+ * interrupt lines by handlers, dispatching if interrupts to handlers, probing
+ * for interrupt lines 
+ */
+
+/* Externs */
+extern void xlr_timer_interrupt(struct pt_regs *regs, int irq);
+
+#ifdef XLP_MERGE_TODO
+extern void xlr_smp_time_init(void);
+#endif
+
+extern void *ht_config_base;
+extern int link0, link1;
+struct pic_tmask pic_tmask[PIC_NUM_IRTS];
+
+__u64 phnx_irq_mask;
+spinlock_t phnx_pic_lock = SPIN_LOCK_UNLOCKED;
+
+static unsigned int pic_startup(unsigned int irq)
+{
+	nlm_reg_t *mmio = netlogic_io_mmio(NETLOGIC_IO_PIC_OFFSET);
+	unsigned long flags;
+	nlm_reg_t reg;
+/* 	uint32_t thread_mask = (1 << cpu_logical_map(0)); */
+
+/* 	printk("[%s]: IN irq=%d\n", __func__, irq); */
+
+	if (!PIC_IRQ_IS_IRT(irq))
+		return EINVAL;
+
+	spin_lock_irqsave(&phnx_pic_lock, flags);
+
+	/* What happens if this irq was previously not ack'ed? 
+	 * Assume, that doesn't happen?
+	 */
+	reg = netlogic_read_reg(mmio, PIC_IRT_1_BASE + irq - PIC_IRQ_BASE);
+	/* netlogic_write_reg(mmio, PIC_IRT_1_BASE + irq - PIC_IRQ_BASE, reg | (1<<31)); */
+	/* By default all the interrupts are initialized as level senstive - fix for the PCMCIA flash */
+	netlogic_write_reg(mmio, PIC_IRT_1_BASE + irq - PIC_IRQ_BASE,
+			  reg | (1 << 6) | (1 << 30) | (1 << 31));
+	printk("%s: Writing IRT reg %d with IRQ %d\n", __FUNCTION__, PIC_IRT_1_BASE+irq-PIC_IRQ_BASE, irq);
+
+	spin_unlock_irqrestore(&phnx_pic_lock, flags);
+
+	return 0;
+}
+
+static void pic_unmask(unsigned int irq)
+{
+	nlm_reg_t *mmio = netlogic_io_mmio(NETLOGIC_IO_PIC_OFFSET);
+	unsigned long flags;
+	nlm_reg_t reg;
+/* 	uint32_t thread_mask = (1 << cpu_logical_map(0)); */
+
+/* 	printk("%s.%d: IN irq=%d\n", __func__, __LINE__, irq); */
+
+	if (!PIC_IRQ_IS_IRT(irq))
+		return;
+
+	spin_lock_irqsave(&phnx_pic_lock, flags);
+
+	/* What happens if this irq was previously not ack'ed? 
+	 * Assume, that doesn't happen?
+	 */
+	reg = netlogic_read_reg(mmio, PIC_IRT_1_BASE + irq - PIC_IRQ_BASE);
+	/* netlogic_write_reg(mmio, PIC_IRT_1_BASE + irq - PIC_IRQ_BASE, reg | (1<<31)); */
+	/* By default all the interrupts are initialized as level senstive - fix for the PCMCIA flash */
+	netlogic_write_reg(mmio, PIC_IRT_1_BASE + irq - PIC_IRQ_BASE,
+			  reg | (1 << 6) | (1 << 30) | (1 << 31));
+/* 	printk("%s: Writing IRT reg %d with IRQ %d\n", __FUNCTION__, PIC_IRT_1_BASE+irq-PIC_IRQ_BASE, irq); */
+
+	spin_unlock_irqrestore(&phnx_pic_lock, flags);
+
+	return;
+}
+
+static void pic_ack(unsigned int irq)
+{
+	unsigned long flags;
+	nlm_reg_t *pci_mmio = netlogic_io_mmio(NETLOGIC_IO_PCIX_OFFSET);
+	nlm_reg_t *ht_mmio = netlogic_io_mmio(NETLOGIC_IO_HT_OFFSET);
+	nlm_reg_t *mmio = netlogic_io_mmio(NETLOGIC_IO_PIC_OFFSET);
+
+	/* XLS PCIE : the Little Endian region */
+	nlm_reg_t *pcie_mmio_le = NULL;
+
+	unsigned long i;
+	nlm_reg_t reg;
+
+	//dbg_msg("IN irq=%d\n", irq);
+
+	if (is_xls()) {
+		pcie_mmio_le = netlogic_io_mmio(NETLOGIC_IO_PCIE_1_OFFSET);
+	}
+
+	if (!PIC_IRQ_IS_IRT(irq))
+		return;
+
+	/* Interrupt (level sensitive ) acknowledge method for the PCMCIA flash */
+
+	if (irq == 21) {
+		reg = *(unsigned char *)(unsigned long)(0xffffffffBD0001f7ULL);
+		reg = *(unsigned int *)(unsigned long)(0xffffffffBEF19180ULL);
+		for (i = 0; i < 0x100; i++) ;
+		*(unsigned int *)(unsigned long)(0xffffffffBEF19180ULL) = reg;
+		for (i = 0; i < 0x1000; i++) ;
+		reg = *(unsigned int *)(unsigned long)(0xffffffffBEF19180ULL);
+	}
+
+	/* Deal with All PCI-Interrupts.. Brigde ACK */
+	if ((irq == 24) && (!is_xls()))
+		netlogic_read_reg(pci_mmio, (0x140 >> 2));
+
+	if (irq == 23) {
+
+		/* HyperTransport: Clear INT Status */
+		netlogic_read_reg(ht_mmio, (0x700 >> 2));
+
+		/* 
+		 *  ---------------------------------------------------------
+		 *  Generating EOI.
+		 *  Clear Interrupts by directly writing to PLX's CFG Space. 
+		 *  1. setup the off value in register 0xB8
+		 *     (Interrupt Discovery Configuration, bits 23-16). 
+		 *  2. clear the interrupt by setting the IRR bit 
+		 *     (bit 63) in reg 0xBC (IRDR). 
+		 *  ---------------------------------------------------------
+		 *  If more devices are added to HT, we have to use the EOI 
+		 *  broadcast.
+		 *  ---------------------------------------------------------
+		 *  NOTE: Send EOI for all interrupts (INT A, B, C and D).
+		 *  Bridge Cards, if plugged into the slot, may re-route 
+		 *  interrupts. E.g: Intel Bridge 31154 eval board re-routes 
+		 *  INTA of the endpoint to INTC of PLX.
+		 *  ---------------------------------------------------------
+		 */
+
+		/* Generate EOI for INTA */
+		*(volatile uint32_t *)(ht_config_base + 0x008b8) = 0x08c01180;
+		*(volatile uint32_t *)(ht_config_base + 0x008bc) = 0x00000080;
+
+		/* Generate EOI for INTB */
+		*(volatile uint32_t *)(ht_config_base + 0x008b8) = 0x08c01380;
+		*(volatile uint32_t *)(ht_config_base + 0x008bc) = 0x00000080;
+
+		/* Generate EOI for INTC */
+		*(volatile uint32_t *)(ht_config_base + 0x008b8) = 0x08c01580;
+		*(volatile uint32_t *)(ht_config_base + 0x008bc) = 0x00000080;
+
+		/* Generate EOI for INTD */
+		*(volatile uint32_t *)(ht_config_base + 0x008b8) = 0x08c01780;
+		*(volatile uint32_t *)(ht_config_base + 0x008bc) = 0x00000080;
+	}
+
+	/* Ack the PCIE Block MSI Status Register(s) */
+	if (is_xls() && !is_xlsb0_srio()) {
+		if (irq == 34) {
+			/*Link0 */
+			netlogic_write_reg(pcie_mmio_le, (0x90 >> 2),
+					  0xffffffff);
+		}
+		if (irq == 35) {
+			/*Link1 */
+			netlogic_write_reg(pcie_mmio_le, (0x94 >> 2),
+					  0xffffffff);
+		}
+		if ((is_xls2xx() && irq == 31) || (is_xls_b0() && irq == 36)) {
+			/*Link2 */
+			netlogic_write_reg(pcie_mmio_le, (0x190 >> 2),
+					  0xffffffff);
+		}
+		if ((is_xls2xx() && irq == 32) || (is_xls_b0() && irq == 37)) {
+			/*Link3 */
+			netlogic_write_reg(pcie_mmio_le, (0x194 >> 2),
+					  0xffffffff);
+		}
+	}
+
+	/* If edge triggered IRQ, ack it immediately, else when the device
+	 * interrupt condition is cleared, we may lose interrupts 
+	 */
+	if (PIC_IRQ_IS_EDGE_TRIGGERED(irq)) {
+		spin_lock_irqsave(&phnx_pic_lock, flags);
+		netlogic_write_reg(mmio, PIC_INT_ACK,
+				  (1 << (irq - PIC_IRQ_BASE)));
+		spin_unlock_irqrestore(&phnx_pic_lock, flags);
+	}
+}
+
+static void pic_end(unsigned int irq)
+{
+	unsigned long flags;
+	nlm_reg_t *mmio = netlogic_io_mmio(NETLOGIC_IO_PIC_OFFSET);
+
+	//dbg_msg("IN irq=%d\n", irq);
+
+	if (!PIC_IRQ_IS_IRT(irq))
+		return;
+
+	/* If level triggered, ack it after the device condition is cleared */
+	if (!PIC_IRQ_IS_EDGE_TRIGGERED(irq)) {
+
+		spin_lock_irqsave(&phnx_pic_lock, flags);
+
+		netlogic_write_reg(mmio, PIC_INT_ACK,
+				  (1 << (irq - PIC_IRQ_BASE)));
+		spin_unlock_irqrestore(&phnx_pic_lock, flags);
+	}
+}
+
+static void pic_shutdown(unsigned int irq)
+{
+	nlm_reg_t *mmio = netlogic_io_mmio(NETLOGIC_IO_PIC_OFFSET);
+	unsigned long flags;
+	nlm_reg_t reg;
+
+	//dbg_msg("IN irq=%d\n", irq);
+
+	if (!PIC_IRQ_IS_IRT(irq))
+		return;
+
+	spin_lock_irqsave(&phnx_pic_lock, flags);
+
+	/* What happens if this irq is currently pending an ack? 
+	 * Assume, that doesn't happen?
+	 */
+	reg = netlogic_read_reg(mmio, PIC_IRT_1_BASE + irq - PIC_IRQ_BASE);
+	netlogic_write_reg(mmio, PIC_IRT_1_BASE + irq - PIC_IRQ_BASE,
+			  (reg & ~(1 << 31)));
+
+	spin_unlock_irqrestore(&phnx_pic_lock, flags);
+}
+
+static int pic_set_affinity(unsigned int irq, const struct cpumask *mask)
+{
+	nlm_reg_t *mmio = netlogic_io_mmio(NETLOGIC_IO_PIC_OFFSET);
+	unsigned long flags;
+
+	//dbg_msg("IN irq=%d, mask=%lx\n", irq, mask);
+
+	if (!PIC_IRQ_IS_IRT(irq))
+		return -1;
+
+	spin_lock_irqsave(&phnx_pic_lock, flags);
+
+
+	netlogic_write_reg(mmio, PIC_IRT_0_BASE + irq - PIC_IRQ_BASE,
+			  (uint32_t) (mask->bits[0]));
+	spin_unlock_irqrestore(&phnx_pic_lock, flags);
+
+	return 0;
+}
+
+static struct irq_chip phnx_pic = {
+	.name = "Phoenix-PIC",
+	.unmask = pic_unmask,
+	.mask = pic_shutdown,
+	.ack = pic_ack,
+	.end = pic_end,
+	.set_affinity = pic_set_affinity
+};
+
+static void rsvd_pic_handler_1_1(unsigned int irq)
+{
+	if(irq < PIC_IRQ_BASE)
+		return;
+  dbg_msg("Requesting a reserved irq (%d)??", irq);
+  return;
+}
+
+static void rsvd_pic_handler_1(unsigned int irq)
+{
+	if(irq < PIC_IRQ_BASE)
+		return;
+  dbg_msg("handler called for a reserved irq (%d)\n", irq);
+}
+
+static int rsvd_pic_handler_2(unsigned int irq, const struct cpumask *mask)
+{
+	if(irq < PIC_IRQ_BASE)
+		return -1;
+  dbg_msg("handler called for a reserved irq (%d)\n", irq);
+  return 0;
+}
+
+struct irq_chip phnx_rsvd_pic_irq_timer = {
+  .name     =          "Count-Compare",
+  .unmask	=          rsvd_pic_handler_1_1,
+  .mask		=          rsvd_pic_handler_1,
+  .ack          =          rsvd_pic_handler_1,
+  .end          =          rsvd_pic_handler_1,
+  .set_affinity =          rsvd_pic_handler_2
+};
+
+struct irq_chip phnx_rsvd_pic = {
+	.name = "Phoenix-RSVD-PIC",
+	.unmask = rsvd_pic_handler_1_1,
+	.mask = rsvd_pic_handler_1,
+	.ack = rsvd_pic_handler_1,
+	.end = rsvd_pic_handler_1,
+	.set_affinity = rsvd_pic_handler_2
+};
+
+static irqreturn_t phnx_rsvd_irq_handler(int irq, void *dev_id)
+{
+	if(irq == IRQ_TIMER) 
+		return IRQ_HANDLED;
+  dbg_msg("handler for reserved irq %d\n", irq);
+  return IRQ_NONE;
+}
+
+struct irqaction phnx_rsvd_action = {
+	.handler = phnx_rsvd_irq_handler,
+	.flags = 0,
+	//.mask = 0,
+	.name = "phnx_rsvd_action",
+	.dev_id = 0,
+	.next = 0
+};
+
+void __init init_xlr_irqs(void)
+{
+	int i;
+
+	for (i = 0; i < NR_IRQS; i++) {
+		set_irq_chip(i, &phnx_pic);
+	}
+
+#ifdef CONFIG_REMOTE_DEBUG
+	irq_desc[IRQ_REMOTE_DEBUG].chip = &phnx_rsvd_pic;
+	irq_desc[IRQ_REMOTE_DEBUG].action = phnx_rsvd_action;
+	phnx_irq_mask |= (1ULL << IRQ_REMOTE_DEBUG);
+#endif
+
+#ifdef CONFIG_SMP
+	irq_desc[IRQ_IPI_SMP_FUNCTION].chip = &phnx_rsvd_pic;
+	irq_desc[IRQ_IPI_SMP_FUNCTION].action = &phnx_rsvd_action;
+
+	irq_desc[IRQ_IPI_SMP_RESCHEDULE].chip = &phnx_rsvd_pic;
+	irq_desc[IRQ_IPI_SMP_RESCHEDULE].action = &phnx_rsvd_action;
+
+#ifdef CONFIG_PHOENIX_IP_FLOW_AFFINITY
+	/* PR: New IPI added here for netrx balancing */
+	irq_desc[IRQ_IPI_NETRX].chip = &phnx_rsvd_pic;
+	irq_desc[IRQ_IPI_NETRX].action = &phnx_rsvd_action;
+	phnx_irq_mask |= (1ULL << IRQ_IPI_NETRX);
+#endif				/* CONFIG_PHOENIX_IP_FLOW_AFFINITY */
+
+	phnx_irq_mask |=
+	    ((1ULL << IRQ_IPI_SMP_FUNCTION) | (1ULL << IRQ_IPI_SMP_RESCHEDULE));
+#endif
+
+	/* msgring interrupt */
+	irq_desc[IRQ_MSGRING].chip = &phnx_rsvd_pic;
+	irq_desc[IRQ_MSGRING].action = &phnx_rsvd_action;
+	phnx_irq_mask |= (1ULL << IRQ_MSGRING);
+
+	/* unmask all PIC related interrupts. If no handler is installed by the 
+	 * drivers, it'll just ack the interrupt and return 
+	 */
+	for (i = PIC_IRT_FIRST_IRQ; i <= PIC_IRT_LAST_IRQ(); i++)
+		phnx_irq_mask |= (1ULL << i);
+
+#ifdef CONFIG_OPROFILE
+	phnx_irq_mask |= (1ULL << IRQ_IPI_OPROFILE);
+#endif
+
+#ifdef CONFIG_KGDB
+	phnx_irq_mask |= (1ULL << IRQ_IPI_SMP_KGDB);
+#endif
+
+	phnx_irq_mask |= (1ULL << IRQ_TIMER);
+}
+
+#ifdef CONFIG_KGDB
+extern irqreturn_t xlr_kgdb_ipi_handler(int irq, struct pt_regs *regs);
+#endif
+#ifdef CONFIG_OPROFILE
+extern void xlr_oprofile_int_handler(int irq, void *dev_id,
+					 struct pt_regs *regs);
+#endif
+void do_phnx_IRQ(unsigned int irq, struct pt_regs *regs)
+{
+#ifdef CONFIG_SMP
+
+#ifdef CONFIG_PHOENIX_IP_FLOW_AFFINITY
+	if (irq == IRQ_IPI_SMP_FUNCTION || irq == IRQ_IPI_SMP_RESCHEDULE
+	    || irq == IRQ_IPI_NETRX) {
+#else
+	if (irq == IRQ_IPI_SMP_FUNCTION || irq == IRQ_IPI_SMP_RESCHEDULE) {
+#endif				/* CONFIG_PHOENIX_IP_FLOW_AFFINITY */
+		nlm_common_ipi_handler(irq, regs);
+		return;
+	}
+#endif
+
+	if (irq == IRQ_MSGRING) nlm_common_msgring_int_handler(irq, regs);
+
+#ifdef CONFIG_KGDB
+	else if (irq == IRQ_IPI_SMP_KGDB) {
+#if 0
+	  xlr_kgdb_ipi_handler(irq, regs);
+#endif
+  }
+#endif
+#ifdef CONFIG_OPROFILE
+	else if (irq == IRQ_IPI_OPROFILE) {
+		if (xlr_thr_id() != 0)
+			xlr_oprofile_int_handler(irq, NULL, regs);
+	}
+#endif
+  else do_IRQ(irq);
+}
+
+void __cpuinit nlm_smp_irq_init(void)
+{
+	/* Set up kseg0 to be cachable coherent */
+//hsxlr	change_c0_config(CONF_CM_CMASK, CONF_CM_DEFAULT);
+
+	/* set interrupt mask for non-zero cpus */
+	write_64bit_cp0_eimr(phnx_irq_mask | (1 << IRQ_TIMER));
+#ifdef XLP_MERGE_TODO
+	xlr_smp_time_init();
+#endif
+}
+
+/* 
+ * MSI hook-up routines for NLM Boards;
+ * Arch-dependent implementation called
+ * from generic msi.c routines.
+ */
+
+struct irq_chip phnx_pic_msi = {
+	.name = "Phoenix-PIC-MSI",
+	.startup = pic_startup,
+	.shutdown = pic_shutdown,
+	.ack = pic_ack,
+	.end = pic_end,
+	.set_affinity = pic_set_affinity
+};
+
+void destroy_irq(unsigned int irq)
+{
+    /* no-op */
+}
+
+#ifdef CONFIG_PCI_MSI_XLR
+
+static int get_irq_vector(struct pci_dev *dev)
+{
+
+    int irq = 0;
+
+	if (is_xls() && !is_xls2xx() && !is_xls_b0()) {
+		/* Currently, PCIE bridges not supported */
+		if (link0) {
+			if (dev->bus->number == 1)
+				irq = PIC_PCIE_LINK0_IRQ;
+			else
+				irq = PIC_PCIE_LINK1_IRQ;
+		} else if (link1) {
+			if (dev->bus->number == 1)
+				irq = PIC_PCIE_LINK1_IRQ;
+		}
+	} else if (is_xls2xx() || is_xls_b0()) {
+		switch (dev->bus->self->devfn) {
+		case 0x0:
+			irq = PIC_PCIE_LINK0_IRQ;
+			break;
+		case 0x8:
+			irq = PIC_PCIE_LINK1_IRQ;
+			break;
+		case 0x10:
+			if (is_xls_b0())
+				irq = PIC_PCIE_XLSB0_LINK2_IRQ;
+			else
+				irq = PIC_PCIE_LINK2_IRQ;
+			break;
+		case 0x18:
+			if (is_xls_b0())
+				irq = PIC_PCIE_XLSB0_LINK3_IRQ;
+			else
+				irq = PIC_PCIE_LINK3_IRQ;
+			break;
+		default:
+			break;
+		}
+	} else {
+		irq = PIC_HYPER_IRQ;
+	}
+
+	return irq;
+}
+
+static int msi_compose_msg(struct pci_dev *pdev, unsigned int irq,
+			   struct msi_msg *msg)
+{
+
+	unsigned dest;
+
+	if (irq >= 0) {
+		dest = 0x00;
+		msg->address_hi = MSI_ADDR_BASE_HI;
+		msg->address_lo = MSI_ADDR_BASE_LO |
+		    MSI_ADDR_DEST_MODE_PHYSICAL |
+		    MSI_ADDR_REDIRECTION_CPU | MSI_ADDR_DEST_ID(dest);
+		msg->data = MSI_DATA_TRIGGER_EDGE |
+		    MSI_DATA_LEVEL_ASSERT |
+		    MSI_DATA_DELIVERY_FIXED | MSI_DATA_VECTOR(irq);
+	}
+	return irq;
+}
+
+void arch_teardown_msi_irq(unsigned int irq)
+{
+	destroy_irq(irq);
+}
+
+int arch_setup_msi_irq(struct pci_dev *dev, struct msi_desc *desc)
+{
+	struct msi_msg msg;
+	int irq, ret;
+
+	irq = get_irq_vector(dev);
+	if (irq < 0)
+		return irq;
+	set_irq_msi(irq, desc);
+	ret = msi_compose_msg(dev, irq, &msg);
+	if (ret < 0) {
+		destroy_irq(irq);
+		return ret;
+	}
+	write_msi_msg(irq, &msg);
+	irq_desc[irq].chip = &phnx_pic_msi;
+	phnx_irq_mask |= (1ULL << irq);
+	return irq;
+}
+#endif
+
+
+void __init arch_init_irq(void)
+{
+
+#ifdef CONFIG_KGDB
+	if (kgdb_early_setup)
+		return;
+#endif
+
+	/* TODO:
+	 * Initialize the irq registers on the PIC to route
+	 * interrupts to appropriate pins
+	 */
+
+	/* Initialize the irq descriptors */
+	init_xlr_irqs();
+
+	write_64bit_cp0_eimr(phnx_irq_mask);
+
+}
+
+asmlinkage void plat_irq_dispatch(void)
+{
+	uint64_t eirr;
+	struct pt_regs *pt_regs = current_thread_info()->regs;
+	int i = 0;
+	eirr = read_64bit_cp0_eirr() & read_64bit_cp0_eimr();
+
+	if (!eirr)
+		return;
+
+	if (eirr & (1 << IRQ_TIMER)) {
+		xlr_timer_interrupt(pt_regs, IRQ_TIMER);
+		return;
+	}
+	/*TODO use dcltz: optimize below code */
+	for (i = 63; i != -1; i--) {
+		if (eirr & (1ULL << i))
+			break;
+	}
+	if (i == -1) {
+		printk("no interrupt !!\n");
+		return;
+	}
+	/*ack eirr */
+	write_64bit_cp0_eirr(1ULL << i);
+	do_phnx_IRQ(i, pt_regs);
+	return;
+}
+
+
+void pic_setup_threadmask(unsigned int irt, uint32_t mask)
+{
+	nlm_reg_t *mmio = netlogic_io_mmio(NETLOGIC_IO_PIC_OFFSET);
+	pic_tmask[irt].mask = mask;
+	pic_tmask[irt].set = 1;
+	pic_tmask[irt].valid = 1;
+ 	netlogic_write_reg(mmio, PIC_IRT_0_BASE + irt, mask);
+	return;
+}
diff --git a/arch/mips/netlogic/xlr/nmi.S b/arch/mips/netlogic/xlr/nmi.S
new file mode 100644
index 0000000..26d2fa7
--- /dev/null
+++ b/arch/mips/netlogic/xlr/nmi.S
@@ -0,0 +1,105 @@
+/***********************************************************************
+Copyright 2003-2010 Netlogic Microsystems (�~@~\Netlogic�~@~]). All rights
+reserved.
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are
+met:
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+*****************************#NETL_2#********************************/
+
+#include <linux/init.h>
+
+#include <asm/asm.h>
+#include <asm/asmmacro.h>
+#include <asm/cacheops.h>
+#include <asm/irqflags.h>
+#include <asm/regdef.h>
+#include <asm/fpregdef.h>
+#include <asm/mipsregs.h>
+#include <asm/stackframe.h>
+#include <asm/war.h>
+#include <asm/page.h>
+#include <asm/netlogic/mips-exts.h>
+#include <asm/netlogic/interrupt.h>
+
+
+NESTED(nlm_except_vec_nmi, 0, sp)
+	.set push
+	.set noat
+	.set mips64
+	.set noreorder
+	MTC0	k0, OS_KGDB_SCRATCH_REG6
+	nop
+	nop
+	PTR_LA	k0, nlm_nmi_handler
+	jr       k0
+	nop
+	.set pop
+END(nlm_except_vec_nmi)
+
+
+	/* This nmi handler is currently only for taking oprofile samples
+	   on non-zero cpus
+	   */
+NESTED(nlm_nmi_handler, PT_SIZE,  sp)
+	.set	push
+	.set	noat
+	.set noreorder
+	.set 	mips64
+
+	/* Save K0 and K1 first */
+	/* K0 is already saved in nlm_except_vec_nmi */
+	MTC0	k1, OS_KGDB_SCRATCH_REG7
+	
+	/* Clear the  NMI and BEV bits */
+	MFC0	k0, CP0_STATUS
+	li 	k1, 0xffb7ffff
+	and	k0, k0, k1
+	MTC0	k0, CP0_STATUS
+
+	SAVE_ALL
+	CLI
+	TRACE_IRQS_OFF
+
+	li	a0, IRQ_IPI_SMP_KGDB
+	move	a1, sp
+	/* jal	do_nlm_common_IRQ */
+	/* nop */
+	jal	nlm_kgdb_call_nmi_hook
+	nop
+
+	RESTORE_ALL
+
+	/*
+	MFC0 	k0, $15, 1
+	andi	k0, 0x1f
+	sll	k0, 2
+	la	k1, nlm_cpus_in_nmi
+ 	PTR_ADDU	k1, k0
+	sw	zero, 0(k1)
+	*/
+
+	MFC0	k0, OS_KGDB_SCRATCH_REG6
+	MFC0	k1, OS_KGDB_SCRATCH_REG7
+	
+	.set mips3
+	eret
+
+	.set pop
+END(nlm_nmi_handler)
diff --git a/arch/mips/netlogic/xlr/on_chip.c b/arch/mips/netlogic/xlr/on_chip.c
new file mode 100644
index 0000000..505ae9b
--- /dev/null
+++ b/arch/mips/netlogic/xlr/on_chip.c
@@ -0,0 +1,817 @@
+/***********************************************************************
+Copyright 2003-2010 Netlogic Microsystems (“Netlogic”). All rights
+reserved.
+Redistribution and use in source and binary forms, with or without
+modification, are peNLMtted provided that the following conditions are
+met:
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+*****************************#NETL_2#********************************/
+
+#include <linux/init.h>
+#include <linux/smp.h>
+#include <linux/interrupt.h>
+#include <linux/spinlock.h>
+#include <linux/module.h>
+#include <linux/timer.h>
+
+#include <asm/netlogic/msgring.h>
+#include <asm/netlogic/iomap.h>
+#include <asm/netlogic/mips-exts.h>
+#include <asm/netlogic/debug.h>
+#include <asm/netlogic/xlr_user_mac.h>
+#include <asm/netlogic/sim.h>
+
+#ifdef CONFIG_NLM_XLP
+#include <asm/netlogic/hal/nlm_hal.h>
+#include <asm/netlogic/hal/nlm_hal_pic.h>
+#endif
+
+unsigned long netlogic_io_base = (unsigned long)(DEFAULT_NETLOGIC_IO_BASE);
+EXPORT_SYMBOL(netlogic_io_base);
+extern int xlr_loader_support;
+extern int xlr_loader_sharedcore;
+extern int xlr_loader_own_gmac;
+extern int xlr_loader_own_dma;
+int msgring_timer_irq;
+
+#define MSGRNG_CC_INIT_CPU_DEST(conf, dest,cpu) \
+do { \
+     msgrng_write_cc(MSGRNG_CC_##dest##_REG, conf##cc_table_cpu_##cpu.counters[dest][0], 0 ); \
+     msgrng_write_cc(MSGRNG_CC_##dest##_REG, conf##cc_table_cpu_##cpu.counters[dest][1], 1 ); \
+     msgrng_write_cc(MSGRNG_CC_##dest##_REG, conf##cc_table_cpu_##cpu.counters[dest][2], 2 ); \
+     msgrng_write_cc(MSGRNG_CC_##dest##_REG, conf##cc_table_cpu_##cpu.counters[dest][3], 3 ); \
+     msgrng_write_cc(MSGRNG_CC_##dest##_REG, conf##cc_table_cpu_##cpu.counters[dest][4], 4 ); \
+     msgrng_write_cc(MSGRNG_CC_##dest##_REG, conf##cc_table_cpu_##cpu.counters[dest][5], 5 ); \
+     msgrng_write_cc(MSGRNG_CC_##dest##_REG, conf##cc_table_cpu_##cpu.counters[dest][6], 6 ); \
+     msgrng_write_cc(MSGRNG_CC_##dest##_REG, conf##cc_table_cpu_##cpu.counters[dest][7], 7 ); \
+} while(0)
+
+/* Initialized CC for cpu 0 to send to all buckets at 0-7 cpus */
+#define MSGRNG_CC_INIT_CPU(conf, cpu) \
+do { \
+  MSGRNG_CC_INIT_CPU_DEST(conf,0,cpu); \
+  MSGRNG_CC_INIT_CPU_DEST(conf,1,cpu); \
+  MSGRNG_CC_INIT_CPU_DEST(conf,2,cpu); \
+  MSGRNG_CC_INIT_CPU_DEST(conf,3,cpu); \
+  MSGRNG_CC_INIT_CPU_DEST(conf,4,cpu); \
+  MSGRNG_CC_INIT_CPU_DEST(conf,5,cpu); \
+  MSGRNG_CC_INIT_CPU_DEST(conf,6,cpu); \
+  MSGRNG_CC_INIT_CPU_DEST(conf,7,cpu); \
+  MSGRNG_CC_INIT_CPU_DEST(conf,8,cpu); \
+  MSGRNG_CC_INIT_CPU_DEST(conf,9,cpu); \
+  MSGRNG_CC_INIT_CPU_DEST(conf,10,cpu); \
+  MSGRNG_CC_INIT_CPU_DEST(conf,11,cpu); \
+  MSGRNG_CC_INIT_CPU_DEST(conf,12,cpu); \
+  MSGRNG_CC_INIT_CPU_DEST(conf,13,cpu); \
+  MSGRNG_CC_INIT_CPU_DEST(conf,14,cpu); \
+  MSGRNG_CC_INIT_CPU_DEST(conf,15,cpu); \
+} while (0)
+
+#define MSGRNG_BUCKETSIZE_INIT_CPU(conf, base) \
+do { \
+  msgrng_write_bucksize(0, conf##bucket_sizes.bucket[base+0]);         \
+  msgrng_write_bucksize(1, conf##bucket_sizes.bucket[base+1]);         \
+  msgrng_write_bucksize(2, conf##bucket_sizes.bucket[base+2]);  \
+  msgrng_write_bucksize(3, conf##bucket_sizes.bucket[base+3]);  \
+  msgrng_write_bucksize(4, conf##bucket_sizes.bucket[base+4]);  \
+  msgrng_write_bucksize(5, conf##bucket_sizes.bucket[base+5]);  \
+  msgrng_write_bucksize(6, conf##bucket_sizes.bucket[base+6]);  \
+  msgrng_write_bucksize(7, conf##bucket_sizes.bucket[base+7]);  \
+} while(0)
+
+#define XLR_MSG_TBL
+#define XLS_MSG_TBL  xls_
+#define SHARED_XLR_MSG_TBL shared_
+
+#define X_MSGRNG_BUCKETSIZE_INIT_CPU(x,y) MSGRNG_BUCKETSIZE_INIT_CPU(x,y)
+
+__u32  pop_bucket_mask[NR_CORES];
+__u32  pop_bucket_start[NR_CORES];
+__u32  pop_bucket_end[NR_CORES];
+__u32 cpu_to_bktmask[NR_CPUS];
+__u32 cpu_to_frstid[NR_CPUS];
+
+uint32_t hard_cpu_online_map = 0;
+uint32_t msgring_global_thread_mask = 0;
+
+/* make this a read/write spinlock */
+spinlock_t msgrng_lock;
+static nlm_common_atomic_t msgring_registered;
+
+int msgring_int_type;
+int msgring_int_en;
+int msgring_watermark_count;
+static __u32 msgring_thread_mask;
+
+extern int nlm_dev_own_bucket_list_get(int *start, int *end, int *mask);
+extern struct irq_chip nlm_common_rsvd_pic;
+extern struct irqaction nlm_common_rsvd_action;
+
+#ifdef CONFIG_PHOENIX_MSGRING_NAPI
+extern int nlm_msgring_napi;
+extern int xlr_napi_ready;
+extern void xlr_napi_rx_schedule(void);
+#endif				/* CONFIG_PHOENIX_MSGRING_NAPI */
+
+struct tx_stn tx_stns[MAX_TX_STNS];
+
+int rxstn_to_txstn_map[128] = {
+	[0 ... 7] = TX_STN_CPU_0,
+	[8 ... 15] = TX_STN_CPU_1,
+	[16 ... 23] = TX_STN_CPU_2,
+	[24 ... 31] = TX_STN_CPU_3,
+	[32 ... 39] = TX_STN_CPU_4,
+	[40 ... 47] = TX_STN_CPU_5,
+	[48 ... 55] = TX_STN_CPU_6,
+	[56 ... 63] = TX_STN_CPU_7,
+	[64 ... 95] = TX_STN_INVALID,
+	[96 ... 103] = TX_STN_GMAC,
+	[104 ... 107] = TX_STN_DMA,
+	[108 ... 111] = TX_STN_INVALID,
+	[112 ... 113] = TX_STN_XGS_0,
+	[114 ... 115] = TX_STN_XGS_1,
+	[116 ... 119] = TX_STN_INVALID,
+	[120 ... 127] = TX_STN_SEC
+};
+
+int xls_rxstn_to_txstn_map[128] = {
+        [0 ... 7] = TX_STN_CPU_0,
+        [8 ... 15] = TX_STN_CPU_1,
+	[16 ... 23] = TX_STN_CPU_2,
+	[24 ... 31] = TX_STN_CPU_3,
+        [32 ... 80] = TX_STN_INVALID,
+	[80 ... 87] = TX_STN_GMAC1,
+	[96 ... 103] = TX_STN_GMAC0,
+	[104 ... 107] = TX_STN_DMA,
+	[108 ... 111] = TX_STN_CMP,
+	[112 ... 115] = TX_STN_INVALID,
+	[116 ... 119] = TX_STN_PCIE,
+	[120 ... 121] = TX_STN_SEC,
+	[122 ... 127] = TX_STN_INVALID,
+};
+
+void dummy_handler(int bucket, int size, int code, int tx_stid,
+		   struct msgrng_msg *msg, void *dev_id)
+{
+	printk("[%s]: No Handler for message from stn_id=%d, bucket=%d, "
+	       "size=%d, msg0=%llx, dropping message\n",
+	       __FUNCTION__, tx_stid, bucket, size,
+	       (unsigned long long)msg->msg0);
+}
+
+struct tx_stn_handler tx_stn_handler_map[128] = {
+	[0 ... 127] = {dummy_handler, NULL},
+};
+
+void nlm_common_msgring_cpu_init(void)
+{
+	int id;
+	unsigned long flags;
+	int shared_msgring = 0;
+	static int only_once = 0;
+
+	id = cpu_logical_map(get_cpu());
+
+	msgring_int_en = 1;
+
+	if (xlr_loader_support && xlr_loader_sharedcore) {
+		/* if support for loading apps on same core as Linux is enabled */
+		if (xlr_loader_own_gmac || xlr_loader_own_dma) {
+			/* pop should only the buckets matching with the thread
+			   on which linux is loaded */
+			shared_msgring = 1;
+			msgring_int_en = 0;
+			pop_bucket_start[id >> 2] = 0;
+			pop_bucket_mask[id >> 2] |= (1 << (id % 4));
+			if (pop_bucket_end[id >> 2] < (id % 4) + 1)
+				pop_bucket_end[id >> 2] = (id % 4) + 1;
+		} else if (xlr_hybrid_rmios_ipsec()) {
+			/* rmios will always send to the bucket 0 */
+			pop_bucket_start[id >> 2] = 0;
+			pop_bucket_mask[id >> 2] = 1;
+			pop_bucket_end[id >> 2] = 1;
+			put_cpu();
+			return;
+		} else {
+			/* all the stations are owned by apps, 
+			   linux should not poll for any bucket */
+			pop_bucket_start[id >> 2] = 0;
+			pop_bucket_mask[id >> 2] = 0;
+			pop_bucket_end[id >> 2] = 0;
+		}
+	} else if (xlr_hybrid_user_mac() || xlr_hybrid_user_mac_xgmac()) {
+		/* msgring interrupt should be disabled */
+		msgring_int_type = 0x0;
+
+		pop_bucket_start[id >> 2] = 0;
+		pop_bucket_end[id >> 2] = 4;
+		pop_bucket_mask[id >> 2] = 0xf;
+	} else {
+		/* all the stations are owned by linux */
+		pop_bucket_start[id >> 2] = 0;
+		pop_bucket_end[id >> 2] = 8;
+		pop_bucket_mask[id >> 2] = 0xff;
+	}
+
+	/* if not thead 0 */
+	if ((id & 0x03) != 0) {
+		put_cpu();
+		return;
+	}
+
+	prom_dbg_msg("Initializing message ring for cpu_%d\n", id);
+
+	msgrng_flags_save(flags);
+
+	/* Message Stations are shared among all threads in a cpu core
+	 * Assume, thread 0 on all cores are always active when more than
+	 * 1 thread is active in a core
+	 */
+	if (is_xls()) {
+		if (id == 0) {
+			X_MSGRNG_BUCKETSIZE_INIT_CPU(XLS_MSG_TBL, 0);
+			MSGRNG_CC_INIT_CPU(XLS_MSG_TBL, 0);
+		} else if (id == 4) {
+			X_MSGRNG_BUCKETSIZE_INIT_CPU(XLS_MSG_TBL, 8);
+			MSGRNG_CC_INIT_CPU(XLS_MSG_TBL, 1);
+		} else if (id == 8) {
+			X_MSGRNG_BUCKETSIZE_INIT_CPU(XLS_MSG_TBL, 16);
+			MSGRNG_CC_INIT_CPU(XLS_MSG_TBL, 2);
+		} else if (id == 12) {
+			X_MSGRNG_BUCKETSIZE_INIT_CPU(XLS_MSG_TBL, 24);
+			MSGRNG_CC_INIT_CPU(XLS_MSG_TBL, 3);
+		}
+	} else {
+		if (shared_msgring) {
+			if (id == 0) {
+				X_MSGRNG_BUCKETSIZE_INIT_CPU(SHARED_XLR_MSG_TBL,
+							     0);
+				MSGRNG_CC_INIT_CPU(SHARED_XLR_MSG_TBL, 0);
+			} else if (id == 4) {
+				X_MSGRNG_BUCKETSIZE_INIT_CPU(SHARED_XLR_MSG_TBL,
+							     8);
+				MSGRNG_CC_INIT_CPU(SHARED_XLR_MSG_TBL, 1);
+			} else if (id == 8) {
+				X_MSGRNG_BUCKETSIZE_INIT_CPU(SHARED_XLR_MSG_TBL,
+							     16);
+				MSGRNG_CC_INIT_CPU(SHARED_XLR_MSG_TBL, 2);
+			} else if (id == 12) {
+				X_MSGRNG_BUCKETSIZE_INIT_CPU(SHARED_XLR_MSG_TBL,
+							     24);
+				MSGRNG_CC_INIT_CPU(SHARED_XLR_MSG_TBL, 3);
+			} else if (id == 16) {
+				X_MSGRNG_BUCKETSIZE_INIT_CPU(SHARED_XLR_MSG_TBL,
+							     32);
+				MSGRNG_CC_INIT_CPU(SHARED_XLR_MSG_TBL, 4);
+			} else if (id == 20) {
+				X_MSGRNG_BUCKETSIZE_INIT_CPU(SHARED_XLR_MSG_TBL,
+							     40);
+				MSGRNG_CC_INIT_CPU(SHARED_XLR_MSG_TBL, 5);
+			} else if (id == 24) {
+				X_MSGRNG_BUCKETSIZE_INIT_CPU(SHARED_XLR_MSG_TBL,
+							     48);
+				MSGRNG_CC_INIT_CPU(SHARED_XLR_MSG_TBL, 6);
+			} else if (id == 28) {
+				X_MSGRNG_BUCKETSIZE_INIT_CPU(SHARED_XLR_MSG_TBL,
+							     56);
+				MSGRNG_CC_INIT_CPU(SHARED_XLR_MSG_TBL, 7);
+			}
+		} else {
+			if (id == 0) {
+				X_MSGRNG_BUCKETSIZE_INIT_CPU(XLR_MSG_TBL, 0);
+				MSGRNG_CC_INIT_CPU(XLR_MSG_TBL, 0);
+			} else if (id == 4) {
+				X_MSGRNG_BUCKETSIZE_INIT_CPU(XLR_MSG_TBL, 8);
+				MSGRNG_CC_INIT_CPU(XLR_MSG_TBL, 1);
+			} else if (id == 8) {
+				X_MSGRNG_BUCKETSIZE_INIT_CPU(XLR_MSG_TBL, 16);
+				MSGRNG_CC_INIT_CPU(XLR_MSG_TBL, 2);
+			} else if (id == 12) {
+				X_MSGRNG_BUCKETSIZE_INIT_CPU(XLR_MSG_TBL, 24);
+				MSGRNG_CC_INIT_CPU(XLR_MSG_TBL, 3);
+			} else if (id == 16) {
+				X_MSGRNG_BUCKETSIZE_INIT_CPU(XLR_MSG_TBL, 32);
+				MSGRNG_CC_INIT_CPU(XLR_MSG_TBL, 4);
+			} else if (id == 20) {
+				X_MSGRNG_BUCKETSIZE_INIT_CPU(XLR_MSG_TBL, 40);
+				MSGRNG_CC_INIT_CPU(XLR_MSG_TBL, 5);
+			} else if (id == 24) {
+				X_MSGRNG_BUCKETSIZE_INIT_CPU(XLR_MSG_TBL, 48);
+				MSGRNG_CC_INIT_CPU(XLR_MSG_TBL, 6);
+			} else if (id == 28) {
+				X_MSGRNG_BUCKETSIZE_INIT_CPU(XLR_MSG_TBL, 56);
+				MSGRNG_CC_INIT_CPU(XLR_MSG_TBL, 7);
+			}
+		}
+	}
+	msgrng_flags_restore(flags);
+	put_cpu();
+}
+
+void nlm_common_msgring_config(void)
+{
+
+#ifdef CONFIG_PHOENIX_MSGRING_NAPI
+	/* If we use NAPI then we enable queue non-empty interrupt */
+	msgring_int_type = nlm_msgring_napi ? 0x01 : 0x02;
+#else
+	msgring_int_type = 0x02;
+#endif				/* CONFIG_PHOENIX_MSGRING_NAPI */
+
+	msgring_watermark_count = 1;
+	msgring_thread_mask = 0x0f;
+
+/* 	printk("[%s]: int_type = 0x%x, pop_num_buckets=%d, pop_bucket_mask=%x" */
+/* 	       "watermark_count=%d, thread_mask=%x\n", __FUNCTION__, */
+/* 	       msgring_int_type, msgring_pop_num_buckets, msgring_pop_bucket_mask, */
+/* 	       msgring_watermark_count, msgring_thread_mask); */
+}
+
+void nlm_common_derive_cpu_to_bkt_map(void)
+{
+	int cpus_per_core[NR_CORES];
+	int stns_per_core[NR_CORES];
+	int num_cpus, cpus, cpu_off, from, i;
+	int bucket_mask[NR_CPUS_PER_CORE];
+	int fr_bucket[NR_CPUS_PER_CORE];
+	int core, bkt_idx, bkt_mask;
+
+#define GET_NEXT_SET_BIT_U8(val, rv) { \
+    if(val < ( 1 << rv)) \
+        rv = 0; \
+    for(i = rv; val != 0 && i <= 7; i++) { \
+        if(val & (1 << i)) { \
+            rv = i; \
+            break; \
+        } \
+    }  \
+    if( i >= 8) \
+        rv = 0; \
+}
+
+	memset(cpus_per_core, 0, sizeof(cpus_per_core));
+	memset(stns_per_core, 0, sizeof(stns_per_core));
+
+	for (i = 0; i < NR_CPUS; i++) {
+		if (!(hard_cpu_online_map & (1 << i)))
+			continue;
+		core = i / NR_CPUS_PER_CORE;
+		cpus_per_core[core]++;
+	}
+	for (core = 0; core < NR_CORES; core++) {
+		for (i = 0; i < NR_STNS_PER_CORE; i++) {
+			if (!(pop_bucket_mask[core] & (1 << i)))
+				continue;
+			stns_per_core[core]++;
+		}
+	}
+
+	for (core = 0; core < NR_CORES; core++) {
+		int filled_all = 0;
+		int rv = 0;
+		uint8_t fr_bucket_map = 0;
+		memset(bucket_mask, 0, sizeof(bucket_mask));
+		memset(fr_bucket, 0xff, sizeof(fr_bucket));
+
+        num_cpus = cpus_per_core[core];
+        if(num_cpus == 0)
+            continue;
+        for(cpus = 0, bkt_idx = 0, bkt_mask = pop_bucket_mask[core];
+                bkt_mask; bkt_mask = bkt_mask >> 1, bkt_idx++) {
+            if(!(bkt_mask & 0x01))
+                continue;
+            bucket_mask[cpus] |=  (1 << bkt_idx);
+			
+			if(((int)fr_bucket[cpus] != -1) && (fr_bucket[cpus] < NR_CPUS_PER_CORE))
+                fr_bucket_map &= (~(1 << fr_bucket[cpus]));
+            fr_bucket_map |= (1 << bkt_idx);
+			fr_bucket[cpus] = bkt_idx;
+
+			if((cpus + 1) == num_cpus)
+            	filled_all = 1;
+
+            cpus = (cpus + 1) % num_cpus;
+        }
+
+        /* fill the non filled cpus */
+		if(filled_all == 0) {
+	        for(from = 0; cpus < num_cpus; cpus++, from++) {
+    	        bucket_mask[cpus] = bucket_mask[from];
+       	 	}
+		}
+        cpu_off = core * NR_CPUS_PER_CORE;
+        for(from = 0, cpus = cpu_off;
+                cpus < cpu_off + NR_CPUS_PER_CORE; cpus++) {
+            if(!(hard_cpu_online_map & (1 << cpus)))
+                continue;
+            cpu_to_bktmask[cpus] = bucket_mask[from];
+			GET_NEXT_SET_BIT_U8(fr_bucket_map, rv);
+            cpu_to_frstid[cpus] = rv + (core * NR_STNS_PER_CORE);
+            from++;
+			rv++;
+        }
+    }
+#if 0
+	for (i = 0; i < NR_CPUS; i++)
+		printk("%d: bktmask=0x%x frstid=%d\n",
+		       i, cpu_to_bktmask[i], cpu_to_frstid[i]);
+#endif
+
+	return;
+}
+
+static int __init xlr_msgring_watermark_setup(char *str)
+{
+	if (*str == '=')
+		str++;
+
+	msgring_watermark_count = (int)simple_strtoul(str, NULL, 10);
+
+	return 1;
+}
+
+static int __init xlr_msgring_thread_mask_setup(char *str)
+{
+	if (*str == '=')
+		str++;
+
+	msgring_thread_mask = simple_strtoul(str, NULL, 16);
+	msgring_thread_mask &= 0x0f;
+
+	return 1;
+}
+
+static int __init xlr_complete_msgring_thread_mask_setup(char *str)
+{
+	if (*str == '=')
+		str++;
+	msgring_global_thread_mask = simple_strtoul(str, NULL, 16);
+	msgring_global_thread_mask &= 0xffffffff;
+	return 1;
+}
+
+__setup("xlr_msgring_watermark=", xlr_msgring_watermark_setup);
+__setup("xlr_msgring_thread_mask=", xlr_msgring_thread_mask_setup);
+__setup("xlr_complete_msgring_thread_mask=",
+	xlr_complete_msgring_thread_mask_setup);
+
+extern void nlm_cpu_stat_update_msgring_int(void);
+extern void nlm_cpu_stat_update_msgring_cycles(__u32 cycles);
+extern void nlm_cpu_stat_update_msgring_pic_int(void);
+
+void msgring_process_rx_msgs(int start_bucket, int end_bucket,
+			     __u32 pop_bucket_mask)
+{
+	unsigned int bucket_empty_bm = 0;
+	int bucket = 0;
+	int size = 0, code = 0, rx_stid = 0;
+	struct msgrng_msg msg;
+	struct tx_stn_handler *handler = 0;
+	unsigned int status = 0;
+
+#ifdef CONFIG_PHOENIX_MSGRING_NAPI
+	if (xlr_napi_ready && in_irq()) {
+		xlr_napi_rx_schedule();
+		return;
+	}
+#endif				/* CONFIG_PHOENIX_MSGRING_NAPI */
+
+	/* First Drain all the high priority messages */
+	for (;;) {
+
+		bucket_empty_bm =
+		    (msgrng_read_status() >> 24) & pop_bucket_mask;
+
+		/* all buckets empty, break */
+		if (bucket_empty_bm == pop_bucket_mask)
+			break;
+
+		for (bucket = start_bucket; bucket < end_bucket; bucket++) {
+
+			if ((bucket_empty_bm & (1 << bucket)) ||	/* empty */
+			    !((1 << bucket) & pop_bucket_mask))	/* not in mask */
+				continue;
+
+			status =
+			    message_receive(bucket, &size, &code, &rx_stid,
+					    &msg);
+			if (status)
+				continue;
+
+			handler = &tx_stn_handler_map[rx_stid];
+			/* Handler is always present. If not actual, atleast 
+			 * dummy_handler
+			 */
+			(handler->action) (bucket, size, code, rx_stid, &msg,
+					   handler->dev_id);
+		}
+	}
+}
+
+#if !defined(CONFIG_NLMCOMMON_MAC) && !defined(CONFIG_NLM_XLP)
+//__u64 xlr_cp2_exceptions[32];
+//struct user_mac_kernal_data user_mac_krnl_data;
+//struct xlr_user_mac_config xlr_user_mac;
+void nlm_cpu_stat_update_msgring_int(void) { }
+void nlm_cpu_stat_update_msgring_cycles(__u32 cycles) { }
+void nlm_cpu_stat_update_msgring_pic_int(void) { }
+#endif /* CONFIG_PHOENIX_MAC */
+
+__u32 msgrng_msg_cycles = 0;
+void nlm_common_msgring_int_handler(unsigned int irq, struct pt_regs *regs)
+{
+	unsigned long mflags;
+	int core;
+	__u32 cycles = 0;
+
+	if (irq == IRQ_MSGRING) {
+		/* normal message ring interrupt */
+		xlr_inc_counter(MSGRNG_INT);
+		nlm_cpu_stat_update_msgring_int();
+	} else {
+		nlm_cpu_stat_update_msgring_pic_int();
+	}
+
+	irq_enter();
+
+	//dbg_msg("IN irq=%d\n", irq);
+
+	/* TODO: not necessary to disable preemption */
+	msgrng_flags_save(mflags);
+
+	cycles = read_c0_count();
+
+	core = cpu_logical_map(smp_processor_id()) >> 2;
+	msgring_process_rx_msgs(pop_bucket_start[core], pop_bucket_end[core], pop_bucket_mask[core]);
+
+	nlm_cpu_stat_update_msgring_cycles(read_c0_count() - cycles);
+
+	msgrng_flags_restore(mflags);
+
+	//dbg_msg("OUT irq=%d\n", irq);
+
+	/* Call the msg callback */
+	irq_exit();
+}
+
+static void enable_msgring_int(void *info)
+{
+	unsigned long flags = 0, mflags = 0;
+	unsigned int th_mask;
+	unsigned int core;
+	msgrng_access_save(&msgrng_lock, flags, mflags);
+
+	core = hard_smp_processor_id() & ~(0x3);
+	th_mask = (msgring_global_thread_mask >> core) & 0x0f;
+
+#if 0
+	printk
+	    ("[%s:%d] cpu_%d cpu_online_map=0x%04x msgring_global_mask=0x%08x "
+	     "th_mask=0x%02x intype=%d wm=%d\n", __FUNCTION__, __LINE__,
+	     hard_smp_processor_id(), hard_cpu_online_map,
+	     msgring_global_thread_mask, th_mask, msgring_int_type,
+	     msgring_watermark_count);
+#endif
+
+	/* enable the message ring interrupts */
+	msgrng_write_config((msgring_watermark_count << 24) |
+			    (IRQ_MSGRING << 16)
+			    | (th_mask << 8) | msgring_int_type);
+	msgrng_access_restore(&msgrng_lock, flags, mflags);
+}
+
+static void msgring_bkp_timer(unsigned long data)
+{
+	unsigned long flags;
+	struct timer_list *timer = (struct timer_list *)data;
+	local_irq_save(flags);
+	nlm_common_msgring_int_handler(-1,NULL);
+	local_irq_restore(flags);
+	mod_timer(timer, timer->expires+2);
+}
+
+static void enable_msgring_timer(void *data)
+{
+	struct timer_list *timer;
+	timer = kmalloc(sizeof(struct timer_list), GFP_KERNEL);
+	setup_timer(timer, msgring_bkp_timer, (unsigned long)timer);
+	timer->expires = jiffies + 2;
+	add_timer(timer);
+}
+
+extern spinlock_t nlm_common_pic_lock;
+int register_msgring_handler(int major,
+			     void (*action) (int, int, int, int,
+					     struct msgrng_msg *, void *),
+			     void *dev_id)
+{
+	struct tx_stn_handler *handler = 0;
+	int ret = 1;
+	int i,j,tx_stid;
+	unsigned long flags = 0;
+	cpumask_t timer_cpu_mask;
+
+	if (major >= MAX_TX_STNS || action == NULL) {
+		printk(KERN_ALERT "%s:%d  Invalid parameter: major=%d, "
+		       "MAX_TX_STN=%d action=%p",
+		       __FUNCTION__, __LINE__, major, MAX_TX_STNS, action);
+		return ret;
+	}
+
+	/* Check if the message station is valid, if not return error */
+	spin_lock_irqsave(&msgrng_lock, flags);
+
+	for (i = 0; i < 128; i++) {
+		if (is_xls())
+			tx_stid = xls_rxstn_to_txstn_map[i];
+		else
+			tx_stid = rxstn_to_txstn_map[i];
+		if (tx_stid == major) {
+			tx_stn_handler_map[i].action = action;
+			tx_stn_handler_map[i].dev_id = dev_id;
+		}
+	}
+
+	handler = &tx_stns[major].handler;
+
+	// dbg_msg("major=%d, action=%p, dev_id=%p\n", major, action, dev_id);
+	handler->action = action;
+	handler->dev_id = dev_id;
+
+	ret = 0;
+	spin_unlock_irqrestore(&msgrng_lock, flags);
+
+	if (!ret && nlm_common_test_and_set(&msgring_registered)) {
+		int i=0;
+
+		hard_cpu_online_map = 0;
+		for (i = 0; i < NR_CPUS; i++) {
+			if (cpu_isset(i, cpu_online_map))
+				hard_cpu_online_map |=
+				    (1 << cpu_logical_map(i));
+		}
+
+		/* derive the cpu to bucket map */
+		nlm_common_derive_cpu_to_bkt_map();
+
+
+		/* Configure PIC to deliver msgring interrupt for timeouts */
+		if (msgring_global_thread_mask == 0) {
+			for (i = 0; i < NR_CORES; i++) {
+				msgring_global_thread_mask |=
+				    (msgring_thread_mask << (i << 2));
+			}
+		}
+
+		msgring_global_thread_mask &= hard_cpu_online_map;
+
+		/* configure the msgring interrupt on all cpus */
+		if (msgring_int_en)
+			on_each_cpu(enable_msgring_int, 0, 1);
+
+/* 		printk("[%s]: cpu_online_map = %lx, hard_cpu_online_map=%x, " */
+/* 		       "msgring_global_thread_mask=%x\n", */
+/* 		       __FUNCTION__,  */
+/* 		       (unsigned long)cpu_online_map,  */
+/* 		       hard_cpu_online_map,  */
+/* 		       msgring_global_thread_mask); */
+
+		/* Schedule a messagering backup timer at every 2 jiffies on one 
+		   therad per core 
+		 */
+
+		cpus_clear(timer_cpu_mask);
+		for(i = 0; i < NR_CORES; i++) {
+			int core_mask;			
+			int phys_id, logical_id;
+			if(hard_cpu_online_map & (0xf<<(i*NR_CPUS_PER_CORE))){
+				core_mask = (hard_cpu_online_map>>(i*NR_CPUS_PER_CORE)) & 0xf;
+				for(j=0; j<NR_CPUS_PER_CORE; j++){
+					if(core_mask & (1<<j))
+						break;
+				}
+				phys_id = (i*NR_CPUS_PER_CORE) + j;
+				logical_id = cpu_number_map(phys_id);
+				cpu_set(logical_id, timer_cpu_mask);
+			}
+		}
+		preempt_disable();
+		smp_call_function_many(&timer_cpu_mask, enable_msgring_timer, NULL, 1);
+		preempt_enable();
+		if(cpu_isset(cpu_number_map(hard_smp_processor_id()),timer_cpu_mask))
+			enable_msgring_timer(NULL);
+	}
+
+	return ret;
+}
+
+EXPORT_SYMBOL(register_msgring_handler);
+
+static void pic_init(void)
+{
+	nlm_reg_t *mmio = netlogic_io_mmio(NETLOGIC_IO_PIC_OFFSET);
+	int i = 0;
+	int level;
+	uint32_t thread_mask = (1 << hard_smp_processor_id());
+
+	for (i = 0; i < PIC_NUM_IRTS; i++) {
+
+		level = PIC_IRQ_IS_EDGE_TRIGGERED(i);
+
+		/* Bind all PIC irqs to boot cpu */
+		netlogic_write_reg(mmio, PIC_IRT_0_BASE + i, thread_mask);
+
+		/* Use local scheduling and high polarity for all IRTs
+		 * Invalidate all IRTs, by default
+		 */
+		netlogic_write_reg(mmio, PIC_IRT_1_BASE + i,
+				  (level << 30) | (1 << 6) | (PIC_IRQ_BASE +
+							      i));
+	}
+}
+
+atomic_t nlm_common_counters[NR_CPUS][NLM_MAX_COUNTERS] __cacheline_aligned;
+
+static void nlm_usb_init (void)
+{
+	nlm_reg_t * gpio_mmio = netlogic_io_mmio(NETLOGIC_IO_GPIO_OFFSET);
+	nlm_reg_t * usb_mmio  = netlogic_io_mmio(NETLOGIC_IO_USB_1_OFFSET);
+
+   /* The NLM-Specific USB Block */
+   netlogic_write_reg(usb_mmio, 49, 0x10000000); //Clear Rogue Phy INTs
+   netlogic_write_reg(usb_mmio, 50, 0x1f000000);
+
+	if (is_xls1xx()) {
+		/* Enabling only 1 USB Port */
+		if (xlr_board_atx_viii()) {
+			/* LTE board has usb port #1 */
+			netlogic_write_reg(usb_mmio,  1, 0x05000500);
+		}
+		else {
+			/* enable usb port #0 */
+			netlogic_write_reg(usb_mmio,  1, 0x03000500);
+		}
+	}
+	else {
+   	netlogic_write_reg(usb_mmio,  1, 0x07000500);
+	}
+
+   {
+      volatile unsigned int value = gpio_mmio[21];
+      if ((value >> 22) & 0x01) {
+         printk("Detected USB Host mode..\n");
+         netlogic_write_reg(usb_mmio,  0, 0x02000000);
+      }
+      else {
+         printk("Detected USB Device mode..\n");
+         netlogic_write_reg(usb_mmio,  0, 0x01000000);
+      }
+   }
+}
+
+void on_chip_init(void)
+{
+	int i = 0, j = 0;
+
+	cpu_logical_map(0)  = hard_smp_processor_id();
+
+	/* Set netlogic_io_base to the run time value */
+	spin_lock_init(&msgrng_lock);
+
+	msgring_registered.value = 0;
+
+#if defined(CONFIG_NLM_XLP)
+	nlm_hal_init();
+#endif
+
+	nlm_common_msgring_config();
+
+	pic_init(); 
+
+	nlm_common_msgring_cpu_init();
+
+
+
+	for (i = 0; i < NR_CPUS; i++)
+		for (j = 0; j < NLM_MAX_COUNTERS; j++)
+			atomic_set(&nlm_common_counters[i][j], 0);
+
+	if (is_xls())
+		nlm_usb_init();
+}
diff --git a/arch/mips/netlogic/xlr/platform.c b/arch/mips/netlogic/xlr/platform.c
index eab64b4..8616902 100644
--- a/arch/mips/netlogic/xlr/platform.c
+++ b/arch/mips/netlogic/xlr/platform.c
@@ -1,99 +1,109 @@
+/***********************************************************************
+Copyright 2003-2010 Netlogic Microsystems Inc. (“Netlogic”).
+This is a derived work from software originally provided by the external
+entity identified below. The licensing terms and warranties specified in
+the header of the original work apply to this derived work.
+
+*****************************#NETL_1#********************************/
+
 /*
- * Copyright 2011, Netlogic Microsystems.
  * Copyright 2004, Matt Porter <mporter@kernel.crashing.org>
  *
  * This file is licensed under the terms of the GNU General Public
  * License version 2.  This program is licensed "as is" without any
  * warranty of any kind, whether express or implied.
  */
-
 #include <linux/device.h>
 #include <linux/platform_device.h>
 #include <linux/kernel.h>
 #include <linux/init.h>
 #include <linux/resource.h>
-#include <linux/serial_8250.h>
-#include <linux/serial_reg.h>
-
-#include <asm/netlogic/haldefs.h>
-#include <asm/netlogic/xlr/iomap.h>
-#include <asm/netlogic/xlr/pic.h>
-#include <asm/netlogic/xlr/xlr.h>
-
-unsigned int nlm_xlr_uart_in(struct uart_port *p, int offset)
-{
-	uint64_t uartbase;
-	unsigned int value;
-
-	/* sign extend to 64 bits, if needed */
-	uartbase = (uint64_t)(long)p->membase;
-	value = nlm_read_reg(uartbase, offset);
-
-	/* See XLR/XLS errata */
-	if (offset == UART_MSR)
-		value ^= 0xF0;
-	else if (offset == UART_MCR)
-		value ^= 0x3;
-
-	return value;
-}
-
-void nlm_xlr_uart_out(struct uart_port *p, int offset, int value)
-{
-	uint64_t uartbase;
+#include <asm/netlogic/nlm_srio.h>
 
-	/* sign extend to 64 bits, if needed */
-	uartbase = (uint64_t)(long)p->membase;
+static u64 xls_usb_dmamask = ~(u32) 0;
 
-	/* See XLR/XLS errata */
-	if (offset == UART_MSR)
-		value ^= 0xF0;
-	else if (offset == UART_MCR)
-		value ^= 0x3;
-
-	nlm_write_reg(uartbase, offset, value);
-}
-
-#define PORT(_irq)					\
-	{						\
-		.irq		= _irq,			\
-		.regshift	= 2,			\
-		.iotype		= UPIO_MEM32,		\
-		.flags		= (UPF_SKIP_TEST |	\
-			 UPF_FIXED_TYPE | UPF_BOOT_AUTOCONF),\
-		.uartclk	= PIC_CLKS_PER_SEC,	\
-		.type		= PORT_16550A,		\
-		.serial_in	= nlm_xlr_uart_in,	\
-		.serial_out	= nlm_xlr_uart_out,	\
-	}
+static struct platform_device xls_usb_ehci_device = {
+	.name = "ehci-xls",
+	.id = 0,
+	.num_resources = 2,
+	.dev = {
+		.dma_mask = &xls_usb_dmamask,
+		.coherent_dma_mask = 0xffffffff,
+		},
+	.resource = (struct resource[]){
+					{
+					 .start = 0x1EF24000,
+					 .end = (0x1EF24000 + 0x400 - 0x01),
+					 .flags = IORESOURCE_MEM,
+					 },
+					{
+					 .start = 39,
+					 .end = 39,
+					 .flags = IORESOURCE_IRQ,
+					 },
+					},
+};
 
-static struct plat_serial8250_port xlr_uart_data[] = {
-	PORT(PIC_UART_0_IRQ),
-	PORT(PIC_UART_1_IRQ),
-	{},
+static struct platform_device xls_usb_ohci_device_0 = {
+	.name = "ohci-xls-0",
+	.id = 1,
+	.num_resources = 2,
+	.dev = {
+		.dma_mask = &xls_usb_dmamask,
+		.coherent_dma_mask = 0xffffffff,
+		},
+	.resource = (struct resource[]){
+					{
+					 .start = 0x1EF24400,
+					 .end = (0x1EF24400 + 0x400 - 0x01),
+					 .flags = IORESOURCE_MEM,
+					 },
+					{
+					 .start = 39,
+					 .end = 39,
+					 .flags = IORESOURCE_IRQ,
+					 },
+					},
 };
 
-static struct platform_device uart_device = {
-	.name		= "serial8250",
-	.id		= PLAT8250_DEV_PLATFORM,
+static struct platform_device xls_usb_ohci_device_1 = {
+	.name = "ohci-xls-1",
+	.id = 2,
+	.num_resources = 2,
 	.dev = {
-		.platform_data = xlr_uart_data,
-	},
+		.dma_mask = &xls_usb_dmamask,
+		.coherent_dma_mask = 0xffffffff,
+		},
+	.resource = (struct resource[]){
+					{
+					 .start = 0x1EF24800,
+					 .end = (0x1EF24800 + 0x400 - 0x01),
+					 .flags = IORESOURCE_MEM,
+					 },
+					{
+					 .start = 39,
+					 .end = 39,
+					 .flags = IORESOURCE_IRQ,
+					 },
+					},
 };
 
-static int __init nlm_uart_init(void)
-{
-	unsigned long uartbase;
+static struct platform_device *xls_platform_devices[] __initdata = {
+	&xls_usb_ehci_device,
+	&xls_usb_ohci_device_0,
+	&xls_usb_ohci_device_1,
+};
 
-	uartbase = (unsigned long)nlm_mmio_base(NETLOGIC_IO_UART_0_OFFSET);
-	xlr_uart_data[0].membase = (void __iomem *)uartbase;
-	xlr_uart_data[0].mapbase = CPHYSADDR(uartbase);
+int xls_platform_init(void)
+{
+    return platform_add_devices(xls_platform_devices, ARRAY_SIZE(xls_platform_devices));
+}
 
-	uartbase = (unsigned long)nlm_mmio_base(NETLOGIC_IO_UART_1_OFFSET);
-	xlr_uart_data[1].membase = (void __iomem *)uartbase;
-	xlr_uart_data[1].mapbase = CPHYSADDR(uartbase);
+arch_initcall(xls_platform_init);
 
-	return platform_device_register(&uart_device);
+#ifdef CONFIG_RAPIDIO
+void platform_rio_init(void)
+{
+	nlm_rio_setup();
 }
-
-arch_initcall(nlm_uart_init);
+#endif				/* CONFIG_RAPIDIO */
diff --git a/arch/mips/netlogic/xlr/setup.c b/arch/mips/netlogic/xlr/setup.c
index c9d066d..fc61b3e 100644
--- a/arch/mips/netlogic/xlr/setup.c
+++ b/arch/mips/netlogic/xlr/setup.c
@@ -1,201 +1,1771 @@
+/***********************************************************************
+Copyright 2003-2010 Netlogic Microsystems (Netlogic). All rights
+reserved.
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are
+met:
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+*****************************#NETL_2#********************************/
+
 /*
- * Copyright 2003-2011 NetLogic Microsystems, Inc. (NetLogic). All rights
- * reserved.
- *
- * This software is available to you under a choice of one of two
- * licenses.  You may choose to be licensed under the terms of the GNU
- * General Public License (GPL) Version 2, available from the file
- * COPYING in the main directory of this source tree, or the NetLogic
- * license below:
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in
- *    the documentation and/or other materials provided with the
- *    distribution.
- *
- * THIS SOFTWARE IS PROVIDED BY NETLOGIC ``AS IS'' AND ANY EXPRESS OR
- * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
- * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
- * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
- * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
- * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
- * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
- * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ * Setup code for Netlogic's XLR-based boards
  */
 
-#include <linux/kernel.h>
-#include <linux/serial_8250.h>
+#include <linux/spinlock.h>
+#include <linux/mm.h>
+#include <linux/bootmem.h>
+#include <linux/init.h>
 #include <linux/pm.h>
 
+#include <asm/irq.h>
+#include <asm/io.h>
+#include <asm/bootinfo.h>
+#include <asm/addrspace.h>
 #include <asm/reboot.h>
 #include <asm/time.h>
-#include <asm/bootinfo.h>
+#include <linux/interrupt.h>
+#include <asm/atomic.h>
+#include <asm/cacheflush.h>
 
-#include <asm/netlogic/interrupt.h>
-#include <asm/netlogic/psb-bootinfo.h>
-#include <asm/netlogic/haldefs.h>
-#include <asm/netlogic/common.h>
+#include <asm/netlogic/sim.h>
+#include <asm/mipsregs.h>
+#include <asm/netlogic/mips-exts.h>
+#include <asm/netlogic/iomap.h>
+#include <asm/netlogic/debug.h>
+#include <asm/netlogic/xlr_user_mac.h>
+#include <asm/netlogic/msgring.h>
 
-#include <asm/netlogic/xlr/xlr.h>
-#include <asm/netlogic/xlr/iomap.h>
-#include <asm/netlogic/xlr/pic.h>
-#include <asm/netlogic/xlr/gpio.h>
+#include <asm/netlogic/nlm_common_loader.h>
+#include <user/netlogic/nlm_common_loader.h>
+#include <asm/netlogic/nlm_pcix_gen_dev.h>
+#include <asm/netlogic/memory-exclusion.h>
 
-uint64_t nlm_io_base = DEFAULT_NETLOGIC_IO_BASE;
-uint64_t nlm_pic_base;
-struct psb_info nlm_prom_info;
+#include <linux/serial.h>
+#include <linux/serial_core.h>
+#include <linux/module.h>
+#include <linux/proc_fs.h>
+#include <asm/mach-netlogic/mmu.h>
 
-unsigned long nlm_common_ebase = 0x0;
+#ifdef NLM_BRIDGE_WKAROUND
+#include <asm/netlogic/nlm_rw_lock.h>
+#include <asm/netlogic/global_shmem.h>
+nlm_rwlock_t *nlm_bridge_lock;
+EXPORT_SYMBOL(nlm_bridge_lock);
+int nlm_enable_br_wrkaround = 0;
+EXPORT_SYMBOL(nlm_enable_br_wrkaround);
+#endif
+
+#ifdef CONFIG_NLM_XLP
+#include <asm/netlogic/hal/nlm_hal_macros.h>
+#include <asm/netlogic/hal/nlm_hal_pic.h>
+#endif
+
+/* Certain macros for this file
+ */
+
+#define TRUE 					1
+#define FALSE 					0
+
+#define LOADER_UBOOT			1
+#define LOADER_OTHER			2
+
+#define GPIO_SWRESET_REG 		8
+
+#define DEFAULT_LINUX_CPU_MASK 	0x1
+#define DEFAULT_LOADER_MASK 	~DEFAULT_LINUX_CPU_MASK
+
+#define PER_CPU_THREAD_SIZE 	(THREAD_SIZE >> 2)
+#define TOTAL_THREAD_SIZE       (PER_CPU_THREAD_SIZE * (NR_CPUS - 1))
+
+#define BOOT_LOADER_REGION_SZ 	0x04000000
+#define LOADER_KSEG_END 		0x10000000
+
+/* used by the default memory map
+ */
+#define DEF_PHYMEM_START_ADDR 	0x100000
+#define DEF_PHYMEM_SIZE 		0x0ff00000
+
+#define LOADER_KSEG_DEFAULTS nlm_common_loader_kseg_start = NLM_LOADER_KSEG0_START;\
+		nlm_common_loader_kseg_size = NLM_LOADER_KSEG0_SIZE;
+
+#define LOADER_KUSEG_DEFAULTS   \
+		memset(kuseg_mem_map, 0, (sizeof(struct kuseg_mem_info) * 4));	\
+		use_kuseg_defaults(map);
+
+extern char _end;
+
+/* by default, do not assume u-boot */
+int loader_used = LOADER_OTHER; 
+
+/* Struct for temp. allocation
+ * of sp/gp for secondary CPUs 
+ */
+struct xlr_stack_pages {
+	unsigned long stack[(TOTAL_THREAD_SIZE)/sizeof(long)];
+};
+
+struct xlr_stack_pages xlr_stack_pages_temp
+__attribute__((__section__(".data.init_task"),
+	       __aligned__(THREAD_SIZE)));
+
+extern void prom_pre_boot_secondary_cpus(void *);
+
+struct proc_dir_entry *nlm_root_proc;
+EXPORT_SYMBOL(nlm_root_proc);
+
+unsigned long long nlm_common_tlb_stats[32] __cacheline_aligned;
+
+spinlock_t atomic_lock = SPIN_LOCK_UNLOCKED;
+
+__u8 xlr_base_mac_addr[6];
+volatile nlm_common_loader_shared_struct_t *nlm_common_loader_sh_mem = NULL;
+/* used for command line parsing */
+uint32_t nlm_common_loader_kseg_start, nlm_common_loader_kseg_size;
+uint32_t nlm_common_loader_mask;
+/* Size of the shared memory b/w Linux userapp and rmios apps */
+uint32_t nlm_common_app_sh_mem_sz;
+unsigned long  nlm_common_app_shmem_start;
+static int index = 0;
+static char *hybrid_str = NULL;
+
+/* xls chip family variables */
+int chip_is_xls6xx = 0;
+int chip_is_xls4xx = 0;
+int chip_is_xls2xx = 0;
+int chip_is_xls1xx = 0;
+int chip_is_xls = 0;
+int chip_is_xls_b0 = 0;
+int chip_is_xls6xx_b0 = 0;
+int chip_is_xls4xx_b0 = 0;
+EXPORT_SYMBOL(chip_is_xls6xx);
+EXPORT_SYMBOL(chip_is_xls4xx);
+EXPORT_SYMBOL(chip_is_xls2xx);
+EXPORT_SYMBOL(chip_is_xls1xx);
+EXPORT_SYMBOL(chip_is_xls);
+EXPORT_SYMBOL(chip_is_xls_b0);
+EXPORT_SYMBOL(chip_is_xls6xx_b0);
+EXPORT_SYMBOL(chip_is_xls4xx_b0);
 
-/* default to uniprocessor */
-uint32_t nlm_coremask = 1, nlm_cpumask  = 1;
-int  nlm_threads_per_core = 1;
+int xlp_with_mac_driver = 0;
+EXPORT_SYMBOL(xlp_with_mac_driver);
 
-static void __init nlm_early_serial_setup(void)
+/* Environment Variables
+ */
+struct environment xlr_bldr_env ;
+
+__u32 xlr_board_major_version = NLM_XLR_BOARD_ARIZONA_I;
+__u32 xlr_board_minor_version = 0;
+
+struct kuseg_mem_info kuseg_mem_map[MAX_NUM_KUSEG_BLOCKS];
+
+void *nlm_common_psb_shm = 0;
+unsigned long nlm_common_psb_shm_size = 0;
+static int dyna_exc_index=0;
+extern unsigned long _text[];
+
+#ifdef CONFIG_NLMCOMMON_GLOBAL_TLB_SPLIT_ASID
+unsigned long nlm_asid_mask = 0x3f;
+unsigned int nlm_shtlb = 1; /* by default shared TLB is enabled */
+#endif
+
+extern struct psb_info *nlm_boot_info;
+static struct psb_info *prom_info = NULL;
+struct psb_info prom_info_copy; /* Bootloader prom_info is saved here */
+static struct psb_info default_prom_info = {
+	.boot_level              = 2,
+	.io_base                 = DEFAULT_NETLOGIC_IO_BASE,
+	.output_device           = 2,
+	.nlm_cpu_online_map          = 0x01,
+	.magic_dword             = (((__u64)0x900dbeef << 32)|PSB_INFO_VERSION),
+	.size                    = sizeof(struct psb_info),
+	.mac_addr                = 0x000102030405ULL,
+	.cpu_frequency           = 1200000000,
+	.board_version           = 1,
+	.board_major_version     = 1,
+	.board_minor_version     = 0,
+};
+
+static struct physmap_info {
+	int type;
+	char *name;
+} psb_physmap_info[] =
 {
-	struct uart_port s;
-	unsigned long uart_base;
+	{ 0x01 , "Memory" },
+	{ 0x02 , " *** HOLE ***" },
+	{ 0x03 , "Exception Vectors" },
+	{ 0x04 , "Bootloader 0" },
+	{ 0x05 , "NMI Memory" },
+	{ 0x10 , "PCI ECFG Space" },
+	{ 0x11 , "PCIX IO Space"    },
+	{ 0x12 , "PCIX CFG Space"   },
+	{ 0x13 , "PCIX Memory Space"},
+	{ 0x14 , "HT IO Space"      },
+	{ 0x15 , "HT CFG Space" },
+	{ 0x16 , "HT Memory Space" },
+	{ 0x17 , "SRAM (QDR) Space" },
+	{ 0x18 , "Flash Region(Re-mapped)" },
+	{ 0x19 , "PCIE IO Space"    },
+	{ 0x1A , "PCIE CFG Space"   },
+	{ 0x1B , "PCIE Memory Space"},
+	{ 0xff , "Unknown type" }
+};
 
-	uart_base = (unsigned long)nlm_mmio_base(NETLOGIC_IO_UART_0_OFFSET);
-	memset(&s, 0, sizeof(s));
-	s.flags		= ASYNC_BOOT_AUTOCONF | ASYNC_SKIP_TEST;
-	s.iotype	= UPIO_MEM32;
-	s.regshift	= 2;
-	s.irq		= PIC_UART_0_IRQ;
-	s.uartclk	= PIC_CLKS_PER_SEC;
-	s.serial_in	= nlm_xlr_uart_in;
-	s.serial_out	= nlm_xlr_uart_out;
-	s.mapbase	= uart_base;
-	s.membase	= (unsigned char __iomem *)uart_base;
-	early_serial_setup(&s);
-}
-
-static void nlm_linux_exit(void)
-{
-	uint64_t gpiobase;
-
-	gpiobase = nlm_mmio_base(NETLOGIC_IO_GPIO_OFFSET);
-	/* trigger a chip reset by writing 1 to GPIO_SWRESET_REG */
-	nlm_write_reg(gpiobase, NETLOGIC_GPIO_SWRESET_REG, 1);
-	for ( ; ; )
+struct boot_mem_map boot_physaddr_info;
+
+/* Maintain in ascending order of 
+ * the starting physical addresses 
+ */
+static struct boot_mem_map_exclude_region dynamic_exclude_regions[] = {
+	[0] = {0, 0}, /* PCI Shared Mem Or RMIOS Lib Memory*/
+	[1] = {0, 0}, /* PCI Shared Mem Or RMIOS Lib Memory*/
+	[2] = {0, 0}, /* Loader KSEG0 region */
+	[3] = {0, 0}, /* Loader KUSEG region Block 1*/
+	[4] = {0, 0}, /* Loader KUSEG region Block 2 or Hybrid Mode exclusion*/
+	[5] = {0, 0}, /* Loader KUSEG region Block 3 or Hybrid Mode exclusion */
+	[6] = {0, 0}, /* Loader KUSEG region Block 4 or Hybrid Mode exclusion */
+	[7] = {0, 0}, /* Hybrid Mode exclusion*/
+	[8] = {0, 0}, /* END of the list - MUST be the last entry always */
+};
+
+static char *get_psb_physmap_name(int type)
+{
+	int i = 0;
+	int tsize = sizeof(psb_physmap_info) / sizeof(struct physmap_info);
+
+	for (i = 0; i < tsize; i++)	{
+		if ( (psb_physmap_info[i].type == type ) ||
+		    (psb_physmap_info[i].type == 0xff ) )
+			return psb_physmap_info[i].name;
+	}
+	return ("Unknown type");
+}
+
+/* Return value
+ * 	1 ==> IO (or not found)
+ *  0 ==> mem 
+ */
+int nlm_common_get_pgprot(unsigned long address)
+{
+	int i;
+	__u64 start=0, end=0;
+	char *name = NULL;
+
+	for (i = 0; i < boot_physaddr_info.nr_map; i++) {
+		start = boot_physaddr_info.map[i].addr;
+		end = boot_physaddr_info.map[i].addr + boot_physaddr_info.map[i].size;
+		if ((address >= start) && (address < end)) {
+			name = get_psb_physmap_name(boot_physaddr_info.map[i].type);
+			if (!(strcmp(name, "Memory"))) {
+				return 0;
+			} else {
+				return 1;
+			}
+		}
+	}
+	return 1;
+}
+
+int valid_mmap_nlm_common_addr_range(unsigned long pfn)
+{
+	int i;
+	__u64 end=0;
+	for (i = 0; i < boot_physaddr_info.nr_map; i++) {
+		end = boot_physaddr_info.map[i].addr + boot_physaddr_info.map[i].size;
+		end = end >> PAGE_SHIFT;
+		if (pfn <= (unsigned long)end)
+			return 1;
+	}
+	return 0;
+}
+
+static int is_valid_prominfo(struct psb_info *info)
+{
+	if (!prom_info) 
+		return -1;
+  
+	if ((prom_info->magic_dword & 0xffffffffULL) != 0x900dbeef) 
+		return -1;
+
+	if ((prom_info->magic_dword >> 32) != PSB_INFO_VERSION) 
+		return -1;
+
+	return 0;
+}
+
+#if defined(CONFIG_NLM_XLP_SIM)
+const char *DEFAULT_CONSOLE_BOOT_PARAMS = "boot_noi2c mem=255m@1m mem=512m@512m console=ttyS0,115200 ";
+#else
+const char *DEFAULT_CONSOLE_BOOT_PARAMS = "mem=255m@1m mem=512m@512m console=ttyS0,115200 ";
+#endif
+const char *DEFAULT_INITRD_BOOT_PARAMS = "rdinit=/sbin/init ";
+
+const char *get_system_type(void)
+{
+#ifdef CONFIG_NLM_XLP
+	return "Netlogic XLP SIM";
+#else
+	if ( is_xls() )
+		return "Netlogic XLS";
+	return "Netlogic XLR";
+#endif
+}
+
+#ifdef CONFIG_SMP
+atomic_t cpus_rebooted = ATOMIC_INIT(0);
+#endif
+
+static void ptr_linux_exit(void)
+{
+       nlm_reg_t *mmio;
+
+#ifdef CONFIG_NLM_XLP
+	write_32bit_cfg_reg((uint32_t *)xlp_sys_base, 0x40 | CHIP_RESET, 1);
+#else
+	/* trigger a chip reset 
+	 */
+	mmio = netlogic_io_mmio(NETLOGIC_IO_GPIO_OFFSET);
+	netlogic_write_reg(mmio, GPIO_SWRESET_REG, 1);
+#endif
+	for ( ; ; ) 
 		cpu_wait();
 }
 
+void __init bus_error_init(void)
+{
+}
+
+void prom_reconfigure_thr_resources(void)
+{
+	unsigned int mmu_setup=0;
+	int i=0, count=0, dis_contig=0;
+	int value = 0;
+
+	__u32 online_map, thr_mask; 
+
+#ifdef CONFIG_NLMCOMMON_GLOBAL_TLB_SPLIT_ASID
+	uint32_t map;
+#endif
+
+	if (loader_used == LOADER_UBOOT) {
+		/* currently, only 1Core
+		 * 1Thread supported in u-boot
+		 */
+		online_map = 0x01;
+	}
+	else {
+		online_map = prom_info->nlm_cpu_online_map;
+	}
+	
+	thr_mask = online_map >> (netlogic_cpu_id()<<2);
+
+#ifdef CONFIG_NLMCOMMON_GLOBAL_TLB_SPLIT_ASID
+	/* netlogic kernel configures this 
+	 */
+
+	if (nlm_shtlb && (nlm_asid_mask == 0x3f)) {
+		/* Global TLB will work only if all 
+		 * the enabled cores have all their
+		 * threads owned by Linux. 
+		 */
+		map = online_map;
+		for (i = 0; i < NR_CPUS; i += 4) {
+			if ((map & 0xf) && ((map & 0xf) != 0xf)) {
+				nlm_asid_mask = 0xff;
+				nlm_shtlb = 0;
+				printk("Disabling Shared TLB mode\n");
+				break;
+			}
+			map >>= 4;
+		}
+		if ((nlm_asid_mask == 0x3f) && (netlogic_thr_id() == 0)) {
+			mmu_setup = read_32bit_nlm_ctrl_reg(4, 0);
+			mmu_setup = mmu_setup | 0x1;
+			write_32bit_nlm_ctrl_reg(4, 0, mmu_setup);
+
+			printk("CPU %d: Enabled Shared TLB mode \n", 
+					netlogic_cpu_id());
+			return;
+		}
+	}
+	
+	return;
+
+#endif /* CONFIG_NLMCOMMON_GLOBAL_TLB_SPLIT_ASID */
+
+
+	 if (netlogic_thr_id() == 0) { 
+
+		for (i=0;i<4;i++) {
+			if (thr_mask & (1<<i)) {
+				if (i != count)
+					dis_contig = 1;
+				count++;
+			}
+		}
+
+		switch(count) {
+			case 1: value = 0x00; break;
+			case 2: value = 0x02; break;
+			default:
+					value = 0x03; break;
+		}
+
+		if (dis_contig)
+			value = 0x3; 
+
+		mmu_setup = read_32bit_nlm_ctrl_reg(4, 0);
+		mmu_setup = mmu_setup & ~0x06;
+		mmu_setup |= (value << 1);
+
+		write_32bit_nlm_ctrl_reg(4, 0, mmu_setup);
+	} 
+}
+
+int xlr_hybrid;
+int xlr_loader_support=0;
+int xlr_loader_sharedcore=0;
+int xlr_loader_own_gmac=0;
+int xlr_loader_own_dma=0;
+
+uint32_t xlr_linux_cpu_mask;
+int xlr_console_pci_con_dev = 0;
+int xlr_console_pci_con_baud = 0;
+int xlr_boot_over_nfs = 0;
+
+unsigned long nlm_common_ebase = 0x0;
+
+//#if !defined(CONFIG_NLMCOMMON_MAC)
+//struct user_mac_data *user_mac;
+struct xlr_user_mac_config xlr_user_mac;
+//#endif
+
+static inline void init_default_macaddr(void)
+{
+	xlr_base_mac_addr[0] = 0x00;
+	xlr_base_mac_addr[1] = 0x01;
+	xlr_base_mac_addr[2] = 0x02;
+	xlr_base_mac_addr[3] = 0x03;
+	xlr_base_mac_addr[4] = 0x04;
+	xlr_base_mac_addr[5] = 0x05;
+}
+
+static void setup_default_configuration(void)
+{
+	xlr_hybrid = XLR_HYBRID_NONE;
+	xlr_user_mac.l4_extract = 0;
+	xlr_user_mac.fast_syscall = 1;
+	xlr_loader_support = 0;
+	xlr_loader_sharedcore = 0;
+	xlr_loader_own_gmac = 0;
+	xlr_loader_own_dma = 0;
+	xlr_linux_cpu_mask = DEFAULT_LINUX_CPU_MASK; 
+	nlm_common_loader_kseg_start = 0;
+	for ( index = 0 ; index < MAX_NUM_KUSEG_BLOCKS ; index++) {
+		kuseg_mem_map[index].start_addr = 0;
+		kuseg_mem_map[index].size = 0;
+	}
+	nlm_common_loader_kseg_size = 0;
+	nlm_common_loader_mask = DEFAULT_LOADER_MASK;
+
+	nlm_common_psb_shm = 0;
+
+	init_default_macaddr();
+}
+
+void exclude_hybrid_mem_region(void)
+{
+	if (xlr_loader_support){
+		return;
+	}
+	dynamic_exclude_regions[dyna_exc_index].start = 1<<20;
+	dynamic_exclude_regions[dyna_exc_index].end = 
+		(unsigned long long)(((unsigned long)&_text) & 0x1fffffffUL);
+	dyna_exc_index++;
+}
+
+#ifndef CONFIG_MAPPED_KERNEL
+static void xlr_early_hybrid_setup(char *str)
+{
+
+	hybrid_str = str;
+
+
+    if ((strcmp(str, "=rmios_ipsec") == 0)||
+		 (strcmp(str, "rmios_ipsec") == 0)) {
+        exclude_hybrid_mem_region();
+	}
+	else if ((strcmp(str, "=rmios_tcpip_stack") == 0)||
+		 (strcmp(str, "rmios_tcpip_stack") == 0)) {
+		exclude_hybrid_mem_region();
+	}
+}
+#endif
+
+static int xlr_hybrid_setup(char *str)
+{
+	uint32_t loader_reg;
+	uint64_t kernel_start;
+
+	if ((strcmp(str, "=user_mac_xgmac") == 0)||
+	    (strcmp(str, "user_mac_xgmac") == 0)) {
+		if (xlr_board_atx_ii()) {
+			xlr_hybrid = XLR_HYBRID_USER_MAC_XGMAC;
+			printk("Configured for Hybrid mode with USER_MAC_XGMAC\n");
+		} else if ( xlr_board_atx_i()) {
+			xlr_hybrid = XLR_HYBRID_USER_MAC_SPI4;
+			printk("Configured for Hybrid mode with USER_MAC_SPI4\n");
+		}
+		else {
+			printk("user_mac_xgmac hybrid mode is available only on ATX-II\n");
+		}
+	}
+	else if ((strcmp(str, "=user_mac") == 0)||(strcmp(str, "user_mac") == 0)) {
+		xlr_hybrid = XLR_HYBRID_USER_MAC;
+		printk("Configured for Hybrid mode with USER_MAC\n");
+	}
+	else if ((strcmp(str, "=rmios_ipsec") == 0) ||
+			 (strcmp(str, "rmios_ipsec") == 0)) {
+		xlr_hybrid = XLR_HYBRID_RMIOS_IPSEC;
+		printk("Configured for Hybrid mode with RMIOS IPSEC\n");
+	}
+	else if ((strcmp(str, "=rmios_tcpip_stack") == 0)||
+			 (strcmp(str, "rmios_tcpip_stack") == 0)) {
+		xlr_hybrid = XLR_HYBRID_RMIOS_TCPIP_STACK;
+		kernel_start = (uint64_t)(((unsigned long)&_text) & 0x1fffffffUL);
+		if (kernel_start < NLM_RMIOS_TCPIP_END) {
+			panic("Build kernel with loadaddress above %#x\n",
+			      NLM_RMIOS_TCPIP_END);
+		}
+		printk("Configured for Hybrid mode with RMIOS_TCPIP_STACK\n");
+	}
+	else {
+		xlr_hybrid = XLR_HYBRID_NONE;
+		printk("Configured for Hybrid mode with None\n");
+	}
+
+	/* usermac/xgmac cannot work with shared core  */
+	if (xlr_hybrid_user_mac() || xlr_hybrid_user_mac_xgmac()) {
+		if (xlr_loader_support && xlr_loader_sharedcore) {
+			printk("Disabling USER_MAC support:Cannot be enabled with shared_core option\n");
+			xlr_hybrid = XLR_HYBRID_NONE;
+		}
+	}
+
+	if ((xlr_hybrid != XLR_HYBRID_NONE) && (xlr_loader_support)) {
+		/* Don't allow loader feature if hybrid 
+		 * app and loader are using same memory 
+		 */
+		loader_reg = nlm_common_loader_kseg_start + nlm_common_loader_kseg_size;
+
+        if (xlr_hybrid == XLR_HYBRID_RMIOS_IPSEC) {
+            if (((NLM_RMIOS_IPSEC_START >= nlm_common_loader_kseg_start) &&
+	    		(NLM_RMIOS_IPSEC_END< loader_reg)) ||
+	           	  ((nlm_common_loader_kseg_start >= NLM_RMIOS_IPSEC_START) &&
+    		   (nlm_common_loader_kseg_start < NLM_RMIOS_IPSEC_END)))	{
+	    		xlr_loader_support=0;
+		    	printk("Disabling Loader support as hybrid mode is selected\n");
+    			printk("Use different memory range for loader KSEG region if hybrid mode needs to be enabled.\n");
+	    	}
+        }else if (xlr_hybrid == XLR_HYBRID_RMIOS_TCPIP_STACK) {
+            if (((NLM_RMIOS_TCPIP_START >= nlm_common_loader_kseg_start) &&
+			    (NLM_RMIOS_TCPIP_END< loader_reg)) ||
+			   ((nlm_common_loader_kseg_start >= NLM_RMIOS_TCPIP_START) &&
+			    (nlm_common_loader_kseg_start < NLM_RMIOS_TCPIP_END)))	{
+				xlr_loader_support=0;
+				printk("Disabling Loader support as hybrid mode is selected\n");
+				printk("Use different memory range for loader KSEG region if hybrid mode needs to be enabled.\n");
+			}
+		}
+	}
+
+//	}
+			
+	
+	return 1;
+}
+
+
+unsigned int __cpuinit get_c0_compare_int(void)
+{
+    return IRQ_TIMER;
+}
+
+void plat_time_init(void)
+{
+    extern void nlm_common_timer_setup(void);
+
+	if (loader_used == LOADER_UBOOT) {
+		/* this is currently hardcoded from
+		 * the bootloader value, to be fixed
+		 */
+    	mips_hpt_frequency = (unsigned int)0x5f5e1000;
+	}
+	else {
+    	mips_hpt_frequency = (unsigned int)prom_info->cpu_frequency;
+	}
+
+    printk("mips_hpt_frequency = %u\n", mips_hpt_frequency);
+
+    nlm_common_timer_setup();
+}
+
+#ifdef CONFIG_NLM_COMMON
+int avail_mem_above_4G;
+int force_usb __initdata = 0;
+static int __init xls_force_usb(char *p)
+{
+    force_usb = 1;
+        return 0;
+}
+early_param("forceusb", xls_force_usb);
+
+
+/* This routine is useful when USB is desired on
+ * 64-Bit Linux with DRAM mapped >4G. On such systems,
+ * since the XLS USB controller is 32-bit, USB is
+ * disabled. Use command line option 'forceusb' to
+ * enable it; This adjusts the mapped available mem
+ * to a max of till 0xFFFFFFFF.
+ */
+static void __init tweak_avail_dram_map(void) {
+
+    int j=0;
+    int nrmap_ctr = (boot_mem_map.nr_map - 1);
+
+    avail_mem_above_4G = 0;
+
+    for (j=nrmap_ctr; j>=0; j--) {
+        if ((boot_mem_map.map[j].addr + boot_mem_map.map[j].size)
+                > 0x100000000ULL) {
+            avail_mem_above_4G++;
+#ifdef CONFIG_64BIT
+            if (force_usb) {
+                printk(KERN_WARNING "[USB]:Re-adjusting Available DRAM map\n");
+                if (boot_mem_map.map[j].addr > 0x100000000ULL) {
+                    boot_mem_map.nr_map--;
+                }
+                else {
+                    /* Reclaim whatever we can... */
+                    boot_mem_map.map[j].size =
+                        0x100000000ULL - boot_mem_map.map[j].addr;
+                }
+            }
+#endif
+        }
+    }
+}
+#endif
+
+
 void __init plat_mem_setup(void)
 {
-	panic_timeout	= 5;
-	_machine_restart = (void (*)(char *))nlm_linux_exit;
-	_machine_halt	= nlm_linux_exit;
-	pm_power_off	= nlm_linux_exit;
+	extern int panic_timeout;
+  
+	panic_timeout = 5;  
+  
+	_machine_restart = (void (*)(char *))ptr_linux_exit;
+	_machine_halt    = ptr_linux_exit;
+	pm_power_off 	 = ptr_linux_exit;
+
+	tweak_avail_dram_map();
+
+	return;
+}  
+
+#ifdef CONFIG_MAPPED_KERNEL
+#define secondary_cpus_bootup_func \
+	((unsigned long)prom_pre_boot_secondary_cpus - \
+	 (unsigned long)LOADADDR + (unsigned long)PHYSADDR)
+#else
+#define secondary_cpus_bootup_func prom_pre_boot_secondary_cpus
+#endif
+
+/* arg 	- arg passed by user
+ * name - pointer to the start of name=value string
+ * base - conversion base 
+ * res 	- converted number is stored here 
+ * Note: -
+ *	returned value is a 32 bit number always
+ * Returns 0 on success, -1 otherwise
+ */
+
+static int get_name_value(char *arg, char *name, int base, uint32_t *res)
+{
+	char *ptr;
+
+	if ((ptr = strstr(arg, name)) == NULL)
+		return -1;
+
+	if (!strcmp("app_sh_mem_sz=", name)) {
+
+		printk("WARNING: \"app_sh_mem_sz\"  option  is  deprecated\n");
+		printk("WARNING: Use ./userapp shmem option to reserve app "
+				 "shared memory\n");
+		return -1;
+	}
+			
+	ptr = strrchr(ptr, '=');
+	dprintk("ptr after strrchr = %s\n", ptr);
+	ptr++;
+	*res = (uint32_t)simple_strtol(ptr, (char **)NULL, base);
+	return 0;
+
 }
 
-const char *get_system_type(void)
+struct nlm_common_name_value_struct {
+	char *name;
+	uint32_t *val;
+};
+
+static struct nlm_common_name_value_struct nlm_common_name_value_args[] = {
+	{"linux_cpu_mask=", &xlr_linux_cpu_mask },
+    {"kseg0_start=", &nlm_common_loader_kseg_start}, 
+	{"kseg0_size=", &nlm_common_loader_kseg_size},
+	{"app_sh_mem_sz=", &nlm_common_app_sh_mem_sz},
+	{NULL, NULL}
+};
+
+void parse_kuseg_mem_args(char *p)
 {
-	return "Netlogic XLR/XLS Series";
+	static int count = 0;
+	uint64_t start = 0, size= 0;
+
+	if (count == MAX_NUM_KUSEG_BLOCKS) {
+		printk("Max # of kuseg blocks allowed : %d! Ignoring (%s)\n",
+				MAX_NUM_KUSEG_BLOCKS, p);
+		return;
+	}
+
+	p = p + strlen("kumem=");
+	size = memparse(p, &p);
+
+	if ((size == 0) || ((size & (((uint64_t)2 << 20) - 1)) == (1 << 20 )))
+		return;
+
+	kuseg_mem_map[count].size = size;
+
+	if (*p == '@') {
+		start = memparse(p + 1, &p);
+		
+		/* start Addr should be the multiple of 2M 
+		 */
+		if (((size & (((uint64_t)2 << 20) - 1)) == (1 << 20 ) ) ) 
+			return;
+	}
+	
+	kuseg_mem_map[count].start_addr = start;
+    count++;
 }
 
-unsigned int nlm_get_cpu_frequency(void)
+//void prom_parse_args(int argc, char *argv[])
+static void parse_cmdline_args(int argc, char *argv[])  
 {
-	return (unsigned int)nlm_prom_info.cpu_frequency;
+	int i, j;
+	int ret;
+	char *tmp = NULL;
+
+	/* Check if loader support needs to be enabled 
+	 */
+	for (i=1; i<argc; i++) {
+
+		if (argv && argv[i]) {
+
+			if (strcmp(argv[i], "nlm_no_shtlb") == 0) {
+
+#ifdef CONFIG_NLMCOMMON_GLOBAL_TLB_SPLIT_ASID
+				nlm_shtlb = 0;
+				nlm_asid_mask = 0xff;
+				printk("Disabling Shared TLB Support\n");
+#endif
+			} else if (strcmp(argv[i], "xlr_loader") == 0) {
+
+				xlr_loader_support = 1;
+
+#ifdef CONFIG_NLMCOMMON_GLOBAL_TLB_SPLIT_ASID
+				nlm_shtlb = 0;
+				nlm_asid_mask = 0xff;
+#endif
+				printk("Enabling XLR Linux Loader support\n");
+
+			} else if (strcmp(argv[i], "shared_core") == 0) {
+
+				xlr_loader_sharedcore = 1;
+				printk("Linux/RMIOS apps can run on same core\n");
+
+			} else if (strstr(argv[i],"kumem=") != NULL) {
+				parse_kuseg_mem_args(argv[i]);
+
+			} else if (strcmp(argv[i],"console=/dev/pci_co0") == 0) {
+				xlr_console_pci_con_dev = 1;
+
+			} else if (strcmp(argv[i],"console=pci_co,38400") == 0) {
+				xlr_console_pci_con_baud = 1;
+
+			} else if (strcmp(argv[i],"root=/dev/nfs") == 0) {
+				xlr_boot_over_nfs = 1;
+
+			} else if (strcmp(argv[i], "own_gmac") == 0) {
+				xlr_loader_own_gmac = 1;
+				printk("Linux will own gmac ports\n");
+
+			} else if (strncmp(argv[i],"xlr_hybrid=",strlen("xlr_hybrid=")) == 0) {
+				tmp = argv[i]+strlen("xlr_hybrid=");
+
+			} else if (strcmp(argv[i], "xlp_with_mac_driver=1") == 0) {
+				xlp_with_mac_driver = 1;
+			} else if (strcmp(argv[i], "loader=uboot") == 0) {
+				loader_used = LOADER_UBOOT;
+			}
+			else {
+				j = 0;
+				while(nlm_common_name_value_args[j].name != NULL) {
+					
+					ret = get_name_value(argv[i], 
+							nlm_common_name_value_args[j].name, 16, 
+							nlm_common_name_value_args[j].val);
+					
+					if (ret == 0)
+						break;
+					j++;
+				}
+			}
+		}
+	}
+
+#ifdef CONFIG_MAPPED_KERNEL
+	exclude_hybrid_mem_region();
+#else
+	if (tmp) {
+		xlr_early_hybrid_setup(tmp);
+	}
+#endif
 }
 
-void __init prom_free_prom_memory(void)
+void check_cpu_mask(void) 
 {
-	/* Nothing yet */
+	uint32_t tmask,i;
+
+	if (!xlr_linux_cpu_mask)
+		xlr_linux_cpu_mask = 0x1;
+	
+	/* trim to what is available 
+	 */
+	xlr_linux_cpu_mask &= prom_info->nlm_cpu_online_map;
+	xlr_linux_cpu_mask |= (1U<<hard_smp_processor_id());
+
+	/* Exclude CPUs that boot linux from the loader CPU mask 
+	 */
+	nlm_common_loader_mask = ~xlr_linux_cpu_mask;
+	nlm_common_loader_mask &= prom_info->nlm_cpu_online_map;
+	
+	/* Loader should not run on the same core, 
+	 * unless "sharedcore" option is enabled 
+	 */
+	
+	if (xlr_loader_sharedcore == 0) {
+		tmask = 0xf;
+		for (i=0; i < 8; i++) {
+			if (tmask & xlr_linux_cpu_mask)
+				nlm_common_loader_mask &= ~tmask;
+			tmask = tmask << 4;
+		}
+	}
+
+	if (nlm_common_loader_mask == 0) {
+		xlr_loader_support = 0;
+		printk("Disabling loader support as loader mask is 0\n");
+		return;
+	}
+
+	printk("Using 0x%08x as linux cpu mask\n", xlr_linux_cpu_mask);
+	printk("Using 0x%08x as loader cpu mask\n", nlm_common_loader_mask);
+}
+
+void check_kseg_args(void)
+{
+	if ((nlm_common_loader_kseg_start == 0) ||(nlm_common_loader_kseg_size == 0)) {
+		/* no args passed 
+		 */
+		LOADER_KSEG_DEFAULTS;
+		printk("No KSEG args passed. Using defaults\n");
+		return;
+	}
+
+	dprintk("Checking kseg start %x with _end %p\n", 
+			nlm_common_loader_kseg_start, &_end);
+
+	if (((nlm_common_loader_kseg_start | CKSEG0) < (unsigned long)&_end) || 
+			(nlm_common_loader_kseg_start >= LOADER_KSEG_END)) {
+
+		printk("Invalid KSEG args passed. Using defaults\n");
+		printk("Start cannot overlap with image or bootloader region\n");
+		LOADER_KSEG_DEFAULTS;
+		return;
+	}
+
+	if (nlm_common_loader_kseg_start & ((2 << 20) - 1)) {
+		/* Start not aligned at 2MB boundary
+		 */
+		printk("Invalid KSEG args passed. Using defaults\n");
+		printk("Start address not aligned at 2MB boundry\n");
+		LOADER_KSEG_DEFAULTS;
+		return;
+	}
+
+	if ((nlm_common_loader_kseg_start + nlm_common_loader_kseg_size) > 
+			LOADER_KSEG_END) {
+
+		printk("Invalid KSEG args passed. Using defaults\n");
+		printk("Bootloader region cannot be used\n");
+		LOADER_KSEG_DEFAULTS;
+		return;
+	}
+
+	printk("Using 0x%08x as KSEG0 load start and 0x%08x as size\n",
+			nlm_common_loader_kseg_start, nlm_common_loader_kseg_size);
+
+	return;
 }
 
-static void __init build_arcs_cmdline(int *argv)
+static void use_kuseg_defaults(struct boot_mem_map *map)
 {
-	int i, remain, len;
-	char *arg;
+	int i=0;
 
-	remain = sizeof(arcs_cmdline) - 1;
-	arcs_cmdline[0] = '\0';
-	for (i = 0; argv[i] != 0; i++) {
-		arg = (char *)(long)argv[i];
-		len = strlen(arg);
-		if (len + 1 > remain)
+	uint64_t start = NLM_LOADER_KUSEG_PHYS_START;
+	uint64_t size = NLM_LOADER_KUSEG_PHYS_SIZE;
+	
+	for (i=0; i<map->nr_map; i++) {
+		if (map->map[i].type != BOOT_MEM_RAM) continue;
+		if (map->map[i].addr >= start)
 			break;
-		strcat(arcs_cmdline, arg);
-		strcat(arcs_cmdline, " ");
-		remain -=  len + 1;
-	}
-
-	/* Add the default options here */
-	if ((strstr(arcs_cmdline, "console=")) == NULL) {
-		arg = "console=ttyS0,38400 ";
-		len = strlen(arg);
-		if (len > remain)
-			goto fail;
-		strcat(arcs_cmdline, arg);
-		remain -= len;
-	}
-#ifdef CONFIG_BLK_DEV_INITRD
-	if ((strstr(arcs_cmdline, "rdinit=")) == NULL) {
-		arg = "rdinit=/sbin/init ";
-		len = strlen(arg);
-		if (len > remain)
-			goto fail;
-		strcat(arcs_cmdline, arg);
-		remain -= len;
+	}
+
+	if (i == map->nr_map) {
+		/* found no memory
+		 */
+		printk("\n[--------# WARNING #--------]");
+		printk("\nNo Loader KUSEG Region Found!\n");
+		return;
+	}
+
+	start = map->map[i].addr;
+	size = map->map[i].size > size ? size : map->map[i].size;
+	kuseg_mem_map[0].start_addr = start;
+	kuseg_mem_map[0].size = size;
+
+	printk("\nUsing Kuseg Region %#llx@%#llx\n",
+			(unsigned long long)kuseg_mem_map[0].size,
+			(unsigned long long)kuseg_mem_map[0].start_addr);
+	return;
+}
+
+void check_kuseg_args(struct boot_mem_map *map)
+{
+	int i,j;
+	uint64_t end1, end2;
+
+	for (j = 0; j < MAX_NUM_KUSEG_BLOCKS; j++) {
+		/* if size is 0 ignore the entry 
+		 */
+		if ( kuseg_mem_map[j].size == 0)
+			continue;
+
+		if ( kuseg_mem_map[j].start_addr < (512 << 20))  { 
+			/* cannot be < 512MB 
+			 */
+			printk("Kuseg start should be > 512MB. Using defaults for start addr %llx\n", 
+					(unsigned long long)kuseg_mem_map[j].start_addr);
+
+			LOADER_KUSEG_DEFAULTS;
+			return;
+		}
+
+		end1 = kuseg_mem_map[j].start_addr + kuseg_mem_map[j].size;
+
+		for (i=0; i < map->nr_map; i++) {
+			if (map->map[i].type != BOOT_MEM_RAM) continue;
+			end2 = map->map[i].addr +  map->map[i].size;
+			if (( kuseg_mem_map[j].start_addr >= map->map[i].addr) &&
+			   (end1 <= end2)) break;
+		}
+
+		if (i == map->nr_map) {
+			printk("Invalid KUSEG range passed. Using defaults\n");
+			LOADER_KUSEG_DEFAULTS;
+			return;
+		}
+
+		printk("Using 0x%llx as KUSEG start and 0x%llx as KUSEG size\n",
+		       (unsigned long long)kuseg_mem_map[j].start_addr,
+		       (unsigned long long)kuseg_mem_map[j].size);
+	}
+	/* if no input is given, use default 
+	 */
+	if ( kuseg_mem_map[0].start_addr == 0 )	{
+		LOADER_KUSEG_DEFAULTS;
+	}
+
+	return;
+}
+
+uint32_t align_shared_mem(uint32_t shared_mem)
+{
+	if (shared_mem <= (2<<20))
+		return (2<<20);
+	if (shared_mem <= (8 << 20))
+		return (8<<20);
+	if (shared_mem <= (32 << 20))
+		return (32<<20);
+	if (shared_mem <= (128 << 20))
+		return (128<<20);
+	if (shared_mem <= (512 << 20))
+		return (512<<20);
+	return (2<<20);
+}
+
+void prom_validate_loader_args(struct boot_mem_map *map)
+{
+	if (!xlr_loader_support) 
+		return;
+	check_cpu_mask();
+	check_kseg_args();
+	check_kuseg_args(map);
+	
+}
+
+/* Maintain in ascending order of 
+ * the starting physical addresses 
+ */
+static struct boot_mem_map_exclude_region _exclude_regions[2][MAX_EXCLUDE + 2];
+
+static struct boot_mem_map_exclude_region *exclude_regions = _exclude_regions[1];
+
+static struct boot_mem_map_exclude_region static_exclude_regions[] = {
+	[0] = { 0,0},
+};
+
+void prom_exclude_kseg(void)
+{
+   	dynamic_exclude_regions[dyna_exc_index].start = (unsigned long long)
+		1<<20;
+   	dynamic_exclude_regions[dyna_exc_index].end = (unsigned long long)
+                (((unsigned long)&_text) & 0x1fffffffUL);
+   	dyna_exc_index++;
+
+	dynamic_exclude_regions[dyna_exc_index].start = (unsigned long long)
+		nlm_common_loader_kseg_start;
+	dynamic_exclude_regions[dyna_exc_index].end = (unsigned long long)
+		(nlm_common_loader_kseg_start + nlm_common_loader_kseg_size);
+	dyna_exc_index++;
+}
+
+void sort_kuseg_region(void)
+{
+	int i,j;
+	uint64_t temp_addr;
+	uint64_t temp_size;
+
+	for (i = 1; i < 4; i++) {
+		for ( j = 0; j < i; j++) {
+			if ( kuseg_mem_map[i].start_addr <  kuseg_mem_map[j].start_addr ) {
+				temp_addr = kuseg_mem_map[j].start_addr;
+				temp_size = kuseg_mem_map[j].size;
+				kuseg_mem_map[j].start_addr = kuseg_mem_map[i].start_addr;
+				kuseg_mem_map[j].size = kuseg_mem_map[i].size;
+				kuseg_mem_map[i].start_addr = temp_addr;
+				kuseg_mem_map[i].size = temp_size;
+			}
+		}
+	}
+}
+
+void check_kuseg_region_overlap(void)
+{
+	int i,max;
+	uint64_t end1, end2;
+
+	max = MAX_NUM_KUSEG_BLOCKS - 1;
+
+	sort_kuseg_region();
+	for ( i = 0 ;i < max; i++) {
+		end1 = kuseg_mem_map[i].start_addr + kuseg_mem_map[i].size ;
+		if ( ( kuseg_mem_map[i+1].start_addr <= end1 )&& ( kuseg_mem_map[i].start_addr != 0 ) ) {
+			end2 = kuseg_mem_map[i+1].start_addr + kuseg_mem_map[i+1].size;
+			if ( end2 >  end1 )
+				kuseg_mem_map[i].size = end2 - kuseg_mem_map[i].start_addr;
+			kuseg_mem_map[i+1].start_addr = 0;
+			kuseg_mem_map[i+1].size = 0;
+			sort_kuseg_region();
+		}
+	}
+}
+
+void prom_exclude_kuseg(void)
+{
+	int i = 0;
+	check_kuseg_region_overlap();
+
+	for (i = 0 ; i < MAX_NUM_KUSEG_BLOCKS ; i++) {
+		if ( (kuseg_mem_map[i].start_addr != 0 ) && (kuseg_mem_map[i].size != 0) ) {
+			dynamic_exclude_regions[dyna_exc_index].start = kuseg_mem_map[i].start_addr;
+			dynamic_exclude_regions[dyna_exc_index].end = kuseg_mem_map[i].start_addr + kuseg_mem_map[i].size;
+			dyna_exc_index++;
+		}
+	}
+}
+
+void prom_exclude_pci_shmem(void)
+{
+	dynamic_exclude_regions[dyna_exc_index].start =
+		NLM_PCIX_SHARED_MEM_START;
+	dynamic_exclude_regions[dyna_exc_index].end = NLM_PCIX_SHARED_MEM_END; 
+	dyna_exc_index++;
+	printk("Excluding PCI Shared Memory\n");
+}
+
+void sort_dynamic_exclude_region(void)
+{
+	int i=0;
+	int j=0;
+	int max=0;
+	struct boot_mem_map_exclude_region *list = dynamic_exclude_regions;
+
+	uint64_t start = 0;
+	uint64_t end = 0;
+
+	while (list[max].start != 0)
+		max++;
+
+	for (i = 0; i < max; i++) {
+		for (j = i; j < max; j++) {
+			if (list[i].start > list[j].start) {
+				start = list[i].start;
+				end = list[i].end;
+				list[i].start = list[j].start;
+				list[i].end = list[j].end;
+				list[j].start = start;
+				list[j].end = end;
+			}
+		}
+	}
+}
+
+static int merge_exclude_regions(struct boot_mem_map_exclude_region *,
+                                 struct boot_mem_map_exclude_region *);
+
+void prom_update_exclude_region(void)
+{
+	int i;
+
+#ifdef CONFIG_NLMCOMMON_PCIX_GEN_DRIVER
+	if (xlr_get_pci_mode() == XLR_PCI_DEV_MODE) {
+		prom_exclude_pci_shmem();
 	}
 #endif
+
+	if (xlr_loader_support) {
+		prom_exclude_kseg();
+		prom_exclude_kuseg();
+	}
+    
+	sort_dynamic_exclude_region();
+	
+	exclude_regions = _exclude_regions[0];
+	
+	/*
+	 * we assume that all exclude regions are sorted
+	 * to start with.
+	 */
+	merge_exclude_regions(exclude_regions, static_exclude_regions);
+	merge_exclude_regions(exclude_regions, dynamic_exclude_regions);
+	
+	dprintk("Final exclude regions ----->\n");
+	for (i = 0; exclude_regions[i].start; i++) {
+		dprintk("%d: Start 0x%llx End 0x%llx\n", i, 
+			exclude_regions[i].start,
+			exclude_regions[i].end);
+	}
+}
+
+struct boot_mem_map prom_map;
+int use_default_phymem = FALSE;
+
+void read_prom_memory(void)
+{
+	struct boot_mem_map *map;
+
+	if (loader_used == LOADER_UBOOT)
+		goto set_default_mmap;
+		
+	/* sanity check prom_info 
+	 * and it's mem_map fields 
+	 */
+	if (!prom_info || (!prom_info->psb_mem_map && !prom_info->avail_mem_map)) 
+		goto set_default_mmap;
+
+	/* copy the mem_map from bootloader */
+	if (sizeof(*prom_info) <= prom_info->size && prom_info->avail_mem_map)
+		map = (struct boot_mem_map *) ((unsigned long)prom_info->avail_mem_map);	
+	else
+		map = (struct boot_mem_map *)((unsigned long)prom_info->psb_mem_map);
+	
+	if (!(map->nr_map > 0 && map->nr_map <= 32))
+		goto set_default_mmap;
+
+	memcpy (&prom_map, map,	sizeof(struct boot_mem_map));
+	
+	return;
+
+set_default_mmap:
+	/* We just set a global flag
+	 */
+	use_default_phymem = TRUE;
 	return;
-fail:
-	panic("Cannot add %s, command line too big!", arg);
 }
 
 static void prom_add_memory(void)
 {
-	struct nlm_boot_mem_map *bootm;
-	u64 start, size;
-	u64 pref_backup = 512;  /* avoid pref walking beyond end */
+	int i = 0, j = 0;
+	__u64 start = 0, end = 0, exc_start = 0, exc_end = 0;
+	__u64 pref_backup = 512;
+
+	if (use_default_phymem)
+		goto use_default;
+
+	prom_validate_loader_args(&prom_map);
+	
+	prom_update_exclude_region();
+	
+	/* TODO: Need to remove this hack
+	 */
+	if (prom_map.map[0].size == 0x0c000000)
+		prom_map.map[0].size = 0x0ff00000;
+	
+	for (i = 0; i < prom_map.nr_map; i++) {
+		start = prom_map.map[i].addr;
+		end = prom_map.map[i].addr + prom_map.map[i].size;
+
+		for (j = 0; j < MAX_EXCLUDE; j++) {
+			exc_start = exclude_regions[j].start;
+			exc_end = exclude_regions[j].end;
+			
+			if ((exc_start == 0) && (exc_end == 0)) /* Empty slot */
+				continue;
+
+			if (exc_start >= start && exc_start < end) {
+				if (exc_start == start) { /* Continuous exclude */
+					start = exc_end;
+					continue;
+				}
+				if (prom_map.map[i].type == BOOT_MEM_RAM) {
+
+					/*
+					 * memcpy/__copy_user prefetch, which
+					 * will cause a bus error for
+					 * KSEG/KUSEG addrs not backed by RAM.
+					 * Hence, reserve some padding for the
+					 * prefetch distance.
+				 	*/
+					if (exc_start-start > pref_backup) {
+						add_memory_region(start,
+									  exc_start-start-pref_backup, 
+									  (long)prom_map.map[i].type);
+					}
+					start = exc_end;
+				}
+			} 
+			else if ((exc_start < start) && (exc_end > start)) {
+				/* Overlapping excludes 
+				 */
+				start = exc_end;
+			}
+		}
+		if (start != end)
+			if (prom_map.map[i].type == BOOT_MEM_RAM) {
+				if (end-start > pref_backup)
+					add_memory_region(start, end-start-pref_backup, (long)prom_map.map[i].type);
+			}
+	}
+	
+	return;
+	
+ use_default:
+	printk("Using Default Physical Mem Map\n"); 
+	/* 255m@1m 
+	 */
+	add_memory_region (DEF_PHYMEM_START_ADDR, 
+			DEF_PHYMEM_SIZE-pref_backup, (long)BOOT_MEM_RAM);
+	xlr_loader_support = 0;
+}
+
+static void psb_print_physmap(void)
+{
+	struct boot_mem_map *physaddr_map = 
+		(struct boot_mem_map *)((unsigned long)prom_info->psb_physaddr_map);
+	char *name;
+	int i = 0;
+	int max;
+
+	if (physaddr_map == NULL)
+		return;
+
+	max = physaddr_map->nr_map;
+
+	prom_dbg_msg("Physical Address Map\n");
+	for (i = 0 ; i <max ; i++) {
+		name = get_psb_physmap_name(physaddr_map->map[i].type);		
+		if ( i == max-1) {
+			prom_dbg_msg("\t%010llx --> %010llx ( %s )\n",
+				     (unsigned long long)physaddr_map->map[i].addr,
+				     (unsigned long long)(physaddr_map->map[i].addr + physaddr_map->map[i].size),
+				     name);
+		}
+		else {
+			prom_dbg_msg("\t%010llx --> %010llx ( %s )\n",
+				     (unsigned long long)physaddr_map->map[i].addr,
+				     (unsigned long long)(physaddr_map->map[i].addr +
+							  physaddr_map->map[i].size -1),
+				     name);
+		}
+	}
+}
+
+static void save_physaddr_info(void)
+{
+	struct boot_mem_map *physaddr_map = 
+		(struct boot_mem_map *)((unsigned long)prom_info->psb_physaddr_map);
+
+	if (physaddr_map == NULL)
+		return;
+
+	memcpy(&boot_physaddr_info,  physaddr_map, sizeof(struct boot_mem_map));
+	return;
+}
+
+/* disable dedicated interrupt vector for virtual mips mode */
+void disable_divec(void)
+{
+    int i;
+    for (i = 0; i < NR_CPUS; i++)
+        cpu_data[i].options &= ~MIPS_CPU_DIVEC;
+
+    return;
+}
+
+extern void (*board_nmi_handler_setup)(void );
+
+void __init nlm_nmi_setup (void)
+{
+	/* setup nmi handler only if KGDB is enabled */
+#ifdef CONFIG_KGDB
+	void *base;
+	extern char nlm_except_vec_nmi;
+
+	printk("Setting up NMI Handler \n");
+	base = (void *)(unsigned long)0xffffffffbfc00000ULL;
+	memcpy(base, &nlm_except_vec_nmi, 0x80);
+#endif
+}
+
+/* setup early serial port driver */
+#ifdef CONFIG_SERIAL_8250
+
+#ifdef CONFIG_NLM_XLR
+#define UART_CLK 66666666
+#else
+#define UART_CLK 133333333
+#endif
+
+static void nlm_early_serial_setup(void)
+{
+	struct uart_port s;
+	extern int __init early_serial_setup(struct uart_port *port);
+
+	memset(&s, 0, sizeof(s));
+
+	s.flags = ASYNC_BOOT_AUTOCONF | ASYNC_SKIP_TEST;
+	/* XLP_MERGE_TODO */
+	s.iotype = UPIO_MEM;
+	/* registers are 4 bytes wide */
+	s.regshift = 2;
+	/* hardware int 4 - the serial int, is CPU int 6
+	 but poll for now */
+	s.irq =  PIC_UART_0_IRQ;
+	s.uartclk = UART_CLK;
+	s.membase = (unsigned char __iomem *)(DEFAULT_NETLOGIC_IO_BASE+NETLOGIC_IO_UART_0_OFFSET);
+	s.mapbase = (DEFAULT_NETLOGIC_IO_BASE+NETLOGIC_IO_UART_0_OFFSET);
+
+	if (early_serial_setup(&s) != 0) {
+		printk(KERN_ERR "Serial setup failed!\n");
+	}
+
+
+}
+#else
+static void nlm_early_serial_setup(void) {}
+#endif
+
+extern struct plat_smp_ops nlm_smp_ops;
+/*
+ * prom_init is called just after the cpu type is detenetlogicned, from setup_arch()
+ */
+
+static int read_cmdline_args(int *argc, char *n_argv[], char *n_envp[])
+{
+	char **argv, **envp;
 	int i;
+	int32_t *t_argv;
+	int32_t *t_envp;
 
-	bootm = (void *)(long)nlm_prom_info.psb_mem_map;
-	for (i = 0; i < bootm->nr_map; i++) {
-		if (bootm->map[i].type != BOOT_MEM_RAM)
-			continue;
-		start = bootm->map[i].addr;
-		size   = bootm->map[i].size;
+	*argc = (int)fw_arg0;
+	argv = (char **)(unsigned long)(int)fw_arg1;
+	envp = (char **)(unsigned long)(int)fw_arg2;
+		
+	for (i = 0, t_argv = (int32_t *)argv; i < *argc; i++, t_argv++)
+		n_argv[i] = (char *)(unsigned long)(*t_argv);
 
-		/* Work around for using bootloader mem */
-		if (i == 0 && start == 0 && size == 0x0c000000)
-			size = 0x0ff00000;
+		if (envp != NULL) {
 
-		add_memory_region(start, size - pref_backup, BOOT_MEM_RAM);
+			for (i = 0, t_envp = (int32_t *)envp; *t_envp; i++) {
+				n_envp[i] = (char *)(unsigned long)(*t_envp);
+				t_envp++;
+		}
 	}
+
+	return 0;
+}
+
+static int read_prominfo(void)
+{
+	struct psb_info *t_prom_info;
+
+	prom_info = &prom_info_copy;
+
+	t_prom_info = (struct psb_info *)(unsigned long)(int)fw_arg3;
+	memcpy((void *)prom_info, (void *)t_prom_info, sizeof(struct psb_info));
+	
+	return is_valid_prominfo(prom_info);
+}
+
+static void process_prominfo(void (*(*wakeup))(void *, void *, __u32))
+{
+#ifdef NLM_BRIDGE_WKAROUND
+	if (prom_info->global_shmem_size == GLOBAL_SHMEM_SIZE) {
+		nlm_bridge_lock = (nlm_rwlock_t *)(unsigned long)
+			(prom_info->global_shmem_addr + BRIDGE_WKAROUND_AREA_OFFSET);
+		nlm_enable_br_wrkaround = 1;
+		printk("Enabling Bridge Workaround \n");
+	}
+#endif
+
+	/* Copy Environment variable */
+	if (prom_info->bldr_envp) {
+		memcpy(&xlr_bldr_env, (void *)(unsigned long)prom_info->bldr_envp, 
+			   sizeof(xlr_bldr_env));
+	}
+
+	xlr_board_major_version = prom_info->board_major_version;
+	xlr_board_minor_version = prom_info->board_minor_version;
+
+	psb_print_physmap();
+
+	save_physaddr_info();
+
+	*wakeup = ((void (*)(void *, void *, __u32))(unsigned long)(prom_info->wakeup));
+
+	/* update the nlm_common  mac addr */
+	xlr_base_mac_addr[0] = (prom_info->mac_addr >> 40) & 0xff;
+	xlr_base_mac_addr[1] = (prom_info->mac_addr >> 32) & 0xff;
+	xlr_base_mac_addr[2] = (prom_info->mac_addr >> 24) & 0xff;
+	xlr_base_mac_addr[3] = (prom_info->mac_addr >> 16) & 0xff;
+	xlr_base_mac_addr[4] = (prom_info->mac_addr >> 8) & 0xff;
+	xlr_base_mac_addr[5] = (prom_info->mac_addr >> 0) & 0xff;
+
+#ifdef DEBUG
+	printk("MAC ADDR BASE: %02x:%02x:%02x:%02x:%02x:%02x\n",
+	       xlr_base_mac_addr[0], xlr_base_mac_addr[1], xlr_base_mac_addr[2],
+	       xlr_base_mac_addr[3], xlr_base_mac_addr[4], xlr_base_mac_addr[5]);
+#endif
+
+	if (xlr_loader_support) {
+		if (xlr_loader_sharedcore && 
+		    (xlr_board_atx_iii() || xlr_board_atx_v())) {
+			xlr_loader_own_dma = 1;
+		}
+/*		
+ *		prom_init_xlr_loader_setup(prom_info);  
+ */
+	}
+}
+
+static int wakeup_secondary_cpus(void (*wakeup)(void *, void *, __u32), struct psb_info *prom_info)
+{
+	__u32 wakeup_mask;
+
+	if (xlr_loader_support) {
+		wakeup_mask = xlr_linux_cpu_mask | nlm_common_loader_mask;
+		if (wakeup != NULL)
+			wakeup((void *)secondary_cpus_bootup_func, 0, wakeup_mask);
+	} 
+	else {
+		if (wakeup != NULL) {
+			wakeup((void *)secondary_cpus_bootup_func, 0, 
+				   (__u32)prom_info->nlm_cpu_online_map & (~smp_boot.online_map));
+#if defined(CONFIG_NLM_XLP_SIM)
+			unsigned int wait_count = 0;
+			
+			while (smp_boot.online_map != prom_info->nlm_cpu_online_map) {
+				if ((wait_count++ % 1000000) == 0) {
+					printk("[%s%d]: Master cpu waiting for slave cpus to wakeup from bootloader (%x != %llx)\n",
+					       __FUNCTION__, __LINE__, smp_boot.online_map, 
+						   (unsigned long long) prom_info->nlm_cpu_online_map);
+				}
+			}
+			printk("[%s@%d]: woke up prom_info->nlm_cpu_online_map=%016llx\n", __FILE__, __LINE__, 
+				   (unsigned long long) prom_info->nlm_cpu_online_map);
+#endif
+		}
+	}
+
+	return 0;
+}
+
+static int build_arcs_cmdline(char *arcs_cmdline, int argc, char *argv[])
+{
+	int i;
+
+	for (i = 1; i < argc; i++) {
+		if (argv[i]) {
+			strcat(arcs_cmdline, argv[i]);
+			strcat(arcs_cmdline, " ");
+		}
+	}
+	strcat(arcs_cmdline, " ");
+
+
+#ifdef CONFIG_NLMCOMMON_CONSOLE_OVER_PCI
+	if (!(xlr_board_atx_iii() || xlr_board_atx_v()) ||
+		!(xlr_console_pci_con_baud && 
+		  xlr_console_pci_con_dev))
+		strcat(arcs_cmdline, DEFAULT_CONSOLE_BOOT_PARAMS);
+#else
+	if ((strstr(arcs_cmdline, "console=")) == NULL)
+		strcat(arcs_cmdline, DEFAULT_CONSOLE_BOOT_PARAMS);
+#endif
+	strcat(arcs_cmdline, " ");
+
+#ifdef CONFIG_ROOT_NFS
+	if (!xlr_boot_over_nfs)
+		strcat(arcs_cmdline, DEFAULT_INITRD_BOOT_PARAMS);
+#else
+	strcat(arcs_cmdline, DEFAULT_INITRD_BOOT_PARAMS);
+#endif
+	strcat(arcs_cmdline, " ");
+
+	return 0;
 }
 
 void __init prom_init(void)
 {
-	int *argv, *envp;		/* passed as 32 bit ptrs */
-	struct psb_info *prom_infop;
+	int argc, retval;
+	
+	char *n_argv[NLM_MAX_ARGS] = {NULL};
+	char *n_envp[NLM_MAX_ENVS] = {NULL};
+	void (*wakeup)(void *, void *, __u32) = NULL;
 
-	/* truncate to 32 bit and sign extend all args */
-	argv = (int *)(long)(int)fw_arg1;
-	envp = (int *)(long)(int)fw_arg2;
-	prom_infop = (struct psb_info *)(long)(int)fw_arg3;
+	setup_mapped_kernel_tlbs(TRUE, TRUE);
 
-	nlm_prom_info = *prom_infop;
-	nlm_pic_base = nlm_mmio_base(NETLOGIC_IO_PIC_OFFSET);
+	setup_default_configuration();
+	read_cmdline_args(&argc, n_argv, n_envp);
+	parse_cmdline_args(argc, n_argv);
+
+	build_arcs_cmdline(arcs_cmdline, argc, n_argv);
+
+	if (loader_used == LOADER_UBOOT)
+		goto setup_memory;
+
+	retval = read_prominfo();
+	if (retval != 0) {
+		prom_info = &default_prom_info;
+		goto rest_prom_int;
+	}
+
+	process_prominfo(&wakeup);
+
+setup_memory:
+	read_prom_memory();
 
-	nlm_early_serial_setup();
-	build_arcs_cmdline(argv);
 	nlm_common_ebase = read_c0_ebase() & (~((1 << 12) - 1));
+
 	prom_add_memory();
 
-#ifdef CONFIG_SMP
-	nlm_wakeup_secondary_cpus(nlm_prom_info.online_cpu_map);
+	smp_boot.online_map = (1 << hard_smp_processor_id());
+
+	wakeup_secondary_cpus(wakeup, prom_info);
+
+rest_prom_int:
+
+	if (hybrid_str != NULL)
+		xlr_hybrid_setup(hybrid_str);
+/*
+	config_net_init();
+*/
+	board_nmi_handler_setup = nlm_nmi_setup;
+
+	on_chip_init();
+
+	prom_reconfigure_thr_resources();
+
+	/* setup early serial port driver */
+	nlm_early_serial_setup();
+
 	register_smp_ops(&nlm_smp_ops);
+}
+
+void prom_free_prom_memory(void)
+{
+	/* nothing to free */
+}
+
+void read_cp0_regs(void)
+{
+	printk("[%s]: count = 0x%x, compare = 0x%x\n"
+	       "status = 0x%x, cause = 0x%x\n"
+	       "eimr = 0x%llx, eirr = 0x%llx\n",
+	       __FUNCTION__, 
+	       read_c0_count(),
+	       read_c0_compare(),
+	       read_c0_status(),
+	       read_c0_cause(),
+	       (unsigned long long)read_64bit_cp0_eimr(),
+	       (unsigned long long)read_64bit_cp0_eirr()
+		);
+}
+
+struct boot_mem_map_entry *psb_get_physaddr_base_address(unsigned long type)
+{
+	struct boot_mem_map *physaddr_map = 
+		(struct boot_mem_map *)((unsigned long)prom_info->psb_physaddr_map);
+
+	int i = 0;
+	int max = physaddr_map->nr_map;
+
+	for (i=0 ; i <max ; i++) {
+		if (physaddr_map->map[i].type == type)
+			return (physaddr_map->map);
+	}
+	return NULL;
+}
+
+void static add_region(struct boot_mem_map_exclude_region *x, int *k,
+		       uint64_t start, uint64_t end)
+{
+	if (*k > MAX_EXCLUDE) {
+		printk("No of exclude regions = %d; Cannot add more\n", MAX_EXCLUDE);
+		return;
+	}
+
+	if (start < x[*k-1].end) {
+		return;
+	}
+
+	x[*k].start = start;
+	x[*k].end = end;
+	++*k;
+}
+
+static int merge_exclude_regions(struct boot_mem_map_exclude_region *x,
+				 struct boot_mem_map_exclude_region *y)
+{
+	static int _index = 0;
+	int i, j, k;
+
+	i = j = 0;
+	k = 1;
+
+	while (x[i].start != 0 && y[j].start != 0) {
+		if (x[i].start < y[j].start) {
+			add_region(_exclude_regions[_index], &k, x[i].start, x[i].end);
+			++i;
+		}
+		else {
+			add_region(_exclude_regions[_index], &k, y[j].start, y[j].end);
+			++j;
+		}
+	}
+
+	if (x[i].start == 0) {
+		while (y[j].start) {
+			add_region(_exclude_regions[_index], &k, y[j].start, y[j].end);
+			++j;
+		}
+	}
+	else if (y[j].start == 0) {
+		while (x[i].start) {
+			add_region(_exclude_regions[_index], &k, x[i].start, x[i].end);
+			++i;
+		}
+	}
+
+	exclude_regions = &_exclude_regions[_index][1];
+	_index = _index ? 0 : 1;
+
+	return 0;
+}
+
+static void NS16550_putc(char c)
+{
+	nlm_reg_t *mmio = 
+		netlogic_io_mmio(NETLOGIC_IO_UART_0_OFFSET);
+
+	while (netlogic_read_reg(mmio, 0x5) == 0);
+	netlogic_write_reg(mmio, 0x0, c);
+}
+
+#ifdef CONFIG_EARLY_PRINTK
+void prom_putchar(char c)
+{
+	void (*putchar)(char);
+
+	if (loader_used == LOADER_UBOOT) {
+		/* u-boot */
+		putchar = ((void (*)(char c))(unsigned long)(&NS16550_putc));
+	}
+	else {
+		/* netlboot */
+		putchar = ((void (*)(char c))(unsigned long)(prom_info->uart_putchar));
+	}
+	putchar(c);
+}
 #endif
+
+static int __init nlm_proc_setup(void)
+{
+	nlm_root_proc = proc_mkdir("netlogic", 0);	
+	if (!nlm_root_proc)
+		return -ENOMEM;
+	
+	return 0;
 }
+rootfs_initcall(nlm_proc_setup);
+
+#ifdef CONFIG_NLM_XLP
+/*placeholder for smp setup function*/
+
+#endif //CONFIG_NLM_XLP
diff --git a/arch/mips/netlogic/xlr/smp.c b/arch/mips/netlogic/xlr/smp.c
new file mode 100644
index 0000000..38eb157
--- /dev/null
+++ b/arch/mips/netlogic/xlr/smp.c
@@ -0,0 +1,239 @@
+/***********************************************************************
+Copyright 2003-2010 Netlogic Microsystems (“Netlogic”). All rights
+reserved.
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are
+met:
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+*****************************#NETL_2#********************************/
+
+#include <linux/kernel.h>
+#include <linux/delay.h>
+#include <linux/init.h>
+#include <linux/smp.h>
+
+#include <asm/mipsregs.h>
+#include <asm/mmu_context.h>
+#include <asm/atomic.h>
+//#include <asm/cpumask.h>
+
+#include <asm/netlogic/sim.h>
+#include <asm/netlogic/msgring.h>
+#include <asm/netlogic/mips-exts.h>
+#include <asm/netlogic/xlr_user_mac.h>
+
+#include <asm/asm.h>
+#include <asm/mipsregs.h>
+#include <asm/processor.h>
+#include <asm/netlogic/nlm_common_loader.h>
+#include <user/netlogic/nlm_common_loader.h>
+
+#include <asm/mach-netlogic/mmu.h>
+
+extern int xlr_loader_support;
+extern volatile cpumask_t cpu_callin_map;
+extern void __init phoenix_smp_init(void); 
+extern void phoenix_smp_finish(void);
+
+extern void smp_tune_scheduling (void);
+extern void ptr_smp_boot(unsigned long, unsigned long, unsigned long);
+struct smp_boot_info smp_boot;
+extern void prom_reconfigure_thr_resources(void);
+extern uint32_t nlm_common_loader_mask;
+extern unsigned long nlm_common_ebase;
+
+
+int phys_proc_id[NR_CPUS]; /* cpuid+thrid of each logical CPU */
+cpumask_t phys_cpu_present_map;
+extern void asmlinkage smp_bootstrap(void);
+extern void core_send_ipi(int cpu, unsigned int action);
+
+void nlm_send_ipi_single(int cpu, unsigned int action)
+{
+    core_send_ipi(cpu, action);
+}
+
+void nlm_send_ipi_mask(const struct cpumask * mask, unsigned int action)
+{
+    int cpu;
+    for_each_cpu(cpu, mask){
+        core_send_ipi(cpu, action);
+    }
+}
+
+/*
+ * Code to run on secondary just after probing the CPU
+ */
+static void __cpuinit nlm_init_secondary(void)
+{
+    extern void nlm_smp_irq_init(void);
+
+    nlm_smp_irq_init();
+    /* Time init for this cpu is done in mips_clockevent_init() */
+}
+
+void nlm_smp_finish(void)
+{
+#if !defined(CONFIG_NLM_XLP)
+    nlm_common_msgring_cpu_init();
+#endif
+}
+
+void nlm_cpus_done(void)
+{
+}
+
+/* Boot all other cpus in the system, initialize them, and
+   bring them into the boot fn */
+void nlm_boot_secondary(int logical_cpu, struct task_struct *idle)
+{
+	unsigned long gp = (unsigned long)task_thread_info(idle);
+	unsigned long sp = (unsigned long)__KSTK_TOS(idle);
+	int cpu = cpu_logical_map(logical_cpu);
+
+/* 	printk("[%s]: (PROM): waking up phys cpu# %d: address of boot_info=%p (addressof(ready)=%p)\n", */
+/* 	       __FUNCTION__, cpu, &(smp_boot.boot_info[cpu]), &((smp_boot.boot_info[cpu]).ready)); */
+  
+	smp_boot.boot_info[cpu].sp = sp;
+	smp_boot.boot_info[cpu].gp = gp;
+	smp_boot.boot_info[cpu].fn = (unsigned long)&smp_bootstrap;  
+	/* barrier */
+	__sync();
+	smp_boot.boot_info[cpu].ready = 1;
+  
+/* 	printk("[%s]: (PROM): sent a wakeup message to cpu %d\n", __FUNCTION__, cpu); */
+}
+
+extern void ptr_smp_boot(unsigned long, unsigned long, unsigned long);
+struct smp_boot_info smp_boot;
+extern void prom_reconfigure_thr_resources(void);
+extern uint32_t nlm_common_loader_mask;
+extern unsigned long nlm_common_ebase;
+
+unsigned int fast_syscall_cpumask_phy = 0x1;
+
+void __init nlm_smp_setup(void)
+{
+	int num_cpus = 1;
+	__u32 boot_cpu_online_map = 0, boot_cpu = 0x0;
+
+
+	extern __u32 ipi_3_counter_tx[NR_CPUS][NR_CPUS];
+	extern __u32 ipi_3_counter_rx[NR_CPUS];
+	int i=0, j=0;
+
+	boot_cpu = hard_smp_processor_id();
+
+	cpus_clear(phys_cpu_present_map);
+	/*	cpu_set(0, phys_cpu_present_map);
+	__cpu_number_map[0] = 0;
+	__cpu_logical_map[0] = 0;  
+	dev_tree_en fix , and also not required for the existing case also */
+
+	cpus_clear(cpu_possible_map);
+	/* cpu_set(0, cpu_possible_map); */
+
+	/* Initialize the ipi debug stat variables */
+	for(i=0;i<NR_CPUS;i++) {
+		for(j=0;j<NR_CPUS;j++)
+			ipi_3_counter_tx[i][j] = 0;
+  
+		ipi_3_counter_rx[i] = 0;
+	}
+
+	if(xlr_loader_support) {
+		smp_boot.online_map &= ~nlm_common_loader_mask;
+	}
+
+	boot_cpu_online_map = smp_boot.online_map;
+	printk("(PROM) CPU present map: %x\n", boot_cpu_online_map);
+
+	/* 0th entry in the logical_map should be the bootcpu and all
+       others proceeds after that */
+	/* Fill the entries for boot cpu */
+	boot_cpu_online_map &= (~(1 << boot_cpu));
+	cpu_set(boot_cpu, phys_cpu_present_map);
+	__cpu_number_map[boot_cpu] = 0;
+	__cpu_logical_map[0] = boot_cpu;
+	cpu_set(0, cpu_possible_map);
+
+	for(i = 0;i<NR_CPUS;i++) {
+		if (boot_cpu_online_map & (1<<i)) {
+			cpu_set(i, phys_cpu_present_map);
+			__cpu_number_map[i] = num_cpus;
+			__cpu_logical_map[num_cpus] = i;
+			cpu_set(num_cpus, cpu_possible_map);
+			++num_cpus;
+		}
+	}
+
+
+	fast_syscall_cpumask_phy = (unsigned int)phys_cpu_present_map.bits[0];
+
+	printk("Phys CPU present map: %lx, possible map %lx\n", 
+	       (unsigned long)phys_cpu_present_map.bits[0], 
+	       (unsigned long)cpu_possible_map.bits[0]);
+
+	printk("Detected %i Slave CPU(s)\n", num_cpus);
+}
+
+void nlm_prepare_cpus(unsigned int max_cpus)
+{
+}
+
+
+struct plat_smp_ops nlm_smp_ops = {
+    .send_ipi_single    = nlm_send_ipi_single,
+    .send_ipi_mask      = nlm_send_ipi_mask,
+    .init_secondary     = nlm_init_secondary,
+    .smp_finish     = nlm_smp_finish,
+    .cpus_done      = nlm_cpus_done,
+    .boot_secondary     = nlm_boot_secondary,
+    .smp_setup      = nlm_smp_setup,
+    .prepare_cpus       = nlm_prepare_cpus,
+};
+
+
+
+void prom_boot_cpus_secondary(void *args)
+{
+	int cpu = hard_smp_processor_id();
+  
+	write_c0_ebase((uint32_t)nlm_common_ebase);
+	atomic_add((1<<cpu), (atomic_t *)&smp_boot.online_map);
+	for(;;) {
+		if (smp_boot.boot_info[cpu].ready) break;
+	}
+	__sync();
+
+	prom_reconfigure_thr_resources();
+
+        setup_mapped_kernel_tlbs(TRUE, FALSE);
+        setup_mapped_kernel_tlbs(FALSE, FALSE);
+
+	ptr_smp_boot(smp_boot.boot_info[cpu].fn, smp_boot.boot_info[cpu].sp, 
+		     smp_boot.boot_info[cpu].gp);
+}
+
+
+
+#ifdef CONFIG_NLM_XLP
+/* place holder for boot multiple cpu function */
+
+#endif //CONFIG_NLM_XLP
diff --git a/arch/mips/netlogic/xlr/smpboot.S b/arch/mips/netlogic/xlr/smpboot.S
new file mode 100644
index 0000000..78b9ec2
--- /dev/null
+++ b/arch/mips/netlogic/xlr/smpboot.S
@@ -0,0 +1,68 @@
+/***********************************************************************
+Copyright 2003-2010 Netlogic Microsystems (“Netlogic”). All rights
+reserved.
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are
+met:
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+*****************************#NETL_2#********************************/
+
+#include <asm/asm.h>
+#include <asm/asm-offsets.h>
+#include <asm/regdef.h>
+#include <asm/mipsregs.h>
+
+#include <asm/mach-netlogic/kernel-entry-init.h>
+
+NESTED(ptr_smp_boot, 16, sp)
+
+	move	sp, a1
+	move	gp, a2
+	jal	a0
+	nop
+	
+END(ptr_smp_boot)
+	
+/* Don't jump to linux function from Bootloader stack. Change it 
+ * here. Kernel might allocate bootloader memory before all the CPUs are 
+ * brought up (eg: Inode cache region) and we better don't overwrite this 
+ * memory
+ */
+NESTED(prom_pre_boot_secondary_cpus, 16, sp)
+	SET_MIPS64
+	MAPPED_KERNEL_SETUP_TLB
+	mfc0 t0, $15, 1 #read ebase
+	andi t0, 0x1f #t0 has the processor_id()
+	PTR_LA	t1, xlr_stack_pages_temp
+	li   t2, _THREAD_SIZE
+	srl  t2, 2
+	mul  t3, t2, t0
+	nop
+	nop
+	nop
+	nop
+	nop
+	nop
+	PTR_ADDU  gp, t1, t3
+	PTR_ADDU       sp, gp, t2
+	PTR_ADDI       sp, sp, -32
+	PTR_LA t0, prom_boot_cpus_secondary
+	jr t0
+	nop
+END(prom_pre_boot_secondary_cpus)
diff --git a/arch/mips/netlogic/xlr/time.c b/arch/mips/netlogic/xlr/time.c
new file mode 100644
index 0000000..975a8d9
--- /dev/null
+++ b/arch/mips/netlogic/xlr/time.c
@@ -0,0 +1,224 @@
+/***********************************************************************
+Copyright 2003-2010 Netlogic Microsystems (“Netlogic”). All rights
+reserved.
+Redistribution and use in source and binary forms, with or without
+modification, are peNLMtted provided that the following conditions are
+met:
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+*****************************#NETL_2#********************************/
+
+#include <linux/interrupt.h>
+#include <linux/sched.h>
+#include <linux/spinlock.h>
+
+#include <asm/irq.h>
+#include <asm/ptrace.h>
+#include <asm/addrspace.h>
+#include <asm/time.h>
+#include <asm/cpu.h>
+#include <asm/cpu-features.h>
+#include <asm/perfctr.h>
+#include <linux/oprofile.h>
+
+#include <linux/proc_fs.h>
+
+extern spinlock_t phnx_pic_lock;
+
+#if defined(CONFIG_PERFCTR) && defined(CONFIG_OPROFILE)
+#error "Cannot enable both VPERF and OProfile at the same time"
+#endif
+
+#ifndef CONFIG_NLMCOMMON_MAC
+void nlm_common_user_mac_update_time(void)
+{
+}
+void nlm_common_user_mac_update_ktime(void)
+{
+}
+#else
+extern void nlm_common_user_mac_update_time(void);
+extern void nlm_common_user_mac_update_ktime(void);
+#endif
+ 
+extern struct irq_chip phnx_rsvd_pic;
+extern struct irqaction phnx_rsvd_action;
+
+void save_epc(unsigned long *epc)
+{
+	__asm__ __volatile__(".set push\n"
+			     ".set noreorder\n"
+			     "mfc0 %0, $14\n" ".set pop\n":"=r"(*epc));
+}
+
+#ifdef CONFIG_OPROFILE
+extern void xlr_oprofile_int_handler(int irq, void *dev_id,
+					 struct pt_regs *regs);
+#endif
+void xlr_timer_interrupt(struct pt_regs *regs, int irq)
+{
+	int cpu = smp_processor_id();
+
+#ifdef CONFIG_NLM_WATCHDOG
+        nlm_reg_t *mmio = netlogic_io_mmio(NETLOGIC_IO_PIC_OFFSET);
+
+	/* ack the watchdog */
+	netlogic_write_reg(mmio, 0x0c, 1 << cpu_logical_map(cpu));
+#endif
+
+#if defined (CONFIG_OPROFILE) || defined (CONFIG_PERFCTR_INTERRUPT_SUPPORT)
+    int cntr0, cntr1;
+    uint32_t ctrl0, ctrl1;
+	int    perfctr_overflow = 0;
+#endif
+
+	if (irq != IRQ_TIMER) {
+		printk("[%s]:cpu_%d: bad timer irq = %x\n", __FUNCTION__, cpu, irq);
+		BUG();
+	}
+
+#if defined (CONFIG_PERFCTR_INTERRUPT_SUPPORT) || defined (CONFIG_OPROFILE)
+    ctrl0 = __read_32bit_c0_register($25, 0);
+    ctrl1 = __read_32bit_c0_register($25, 2);
+    cntr0 = __read_32bit_c0_register($25, 1);
+    cntr1 = __read_32bit_c0_register($25, 3);
+
+    /* if interrupts are enabled for perf events, check if any counter has
+       overflowed. Then we know for sure that this is a perf event
+       */
+    if((ctrl0 & 0x10) || (ctrl1 & 0x10))
+            if((cntr0 < 0) || (cntr1 < 0))
+                perfctr_overflow = 1;
+    if(perfctr_overflow == 0)
+#endif
+    {
+        do_IRQ(irq);
+
+        if (cpu == 0) {
+            nlm_common_user_mac_update_time();
+	    nlm_common_user_mac_update_ktime();
+        }
+    }
+
+#if defined (CONFIG_PERFCTR_INTERRUPT_SUPPORT) || defined (CONFIG_OPROFILE)
+	if (perfctr_overflow) {
+#ifdef CONFIG_PERFCTR_INTERRUPT_SUPPORT
+		(*perfctr_ihandler) (instruction_pointer(regs));
+#endif
+    }
+#ifdef CONFIG_OPROFILE
+	if (perfctr_overflow) {
+		if(xlr_thr_id() == 0) {
+			xlr_oprofile_int_handler (irq, NULL, regs);
+		}
+    }
+#endif
+#endif
+
+}
+
+/* PIC clock at 66Mhz takes more than 60 secs to come to 0 from max. So 32bit 
+   counter is sufficient
+   */
+#define PIC_FREE_RUNNING_TIMER_MAX_VAL 0xffffffff
+cycle_t xlr_hpt_read(void)
+{
+	uint32_t counter = 0;
+	nlm_reg_t *mmio = netlogic_io_mmio(NETLOGIC_IO_PIC_OFFSET);
+	counter = netlogic_read_reg(mmio, PIC_TIMER_6_COUNTER_0);
+	return (cycle_t)(PIC_FREE_RUNNING_TIMER_MAX_VAL - counter);
+}
+EXPORT_SYMBOL(xlr_hpt_read);
+
+int read_current_timer(unsigned long *timer_val)
+{
+	*timer_val = xlr_hpt_read();
+	return 0;
+}
+
+void nlm_common_timer_setup(void)
+{
+        nlm_reg_t *mmio = netlogic_io_mmio(NETLOGIC_IO_PIC_OFFSET);
+        unsigned long flags = 0;
+
+        spin_lock_irqsave(&phnx_pic_lock, flags);
+
+        /* Use PIC Timer 6 as a free running counter */
+        netlogic_write_reg(mmio, PIC_TIMER_6_MAXVAL_0, 0xffffffff);
+        netlogic_write_reg(mmio, PIC_TIMER_6_MAXVAL_1, 0xffffffff);
+        /* we Don't need interrupts */
+        netlogic_write_reg(mmio, PIC_IRT_0_TIMER_6, 0);
+        netlogic_write_reg(mmio, PIC_IRT_1_TIMER_6,
+                          (1 << 31) | (0 << 30) | (1 << 6) | (PIC_TIMER_6_IRQ));
+        pic_update_control(1 << (8 + 6));
+
+        spin_unlock_irqrestore(&phnx_pic_lock, flags);
+
+        //do_gettimeoffset = xlr_gettimeoffset;
+
+}
+
+static int nlm_timer_proc_read(char *page, char **start, off_t off, int count,
+			       int *eof, void *data)
+{
+	int len = 0;
+
+	preempt_disable();
+	len += sprintf(page + len, "cpu = %d, eimr = 0x%016llx, status = 0x%x\n", 
+				   smp_processor_id(), 
+                   (unsigned long long)read_64bit_cp0_eimr(), read_c0_status());
+	preempt_enable();
+	*eof = 1;
+
+	return len;
+}
+
+extern struct proc_dir_entry *nlm_root_proc;
+struct proc_dir_entry *main_entry;
+struct proc_dir_entry *sub_entry;
+
+static int init_pic_timer_procfs(void)
+{
+	main_entry = proc_mkdir("nlm_timer", nlm_root_proc);
+	if (!main_entry) {
+		printk(KERN_ERR "unable to create /proc/nlm_timer\n");
+		return -ENOMEM;
+	}
+
+	sub_entry = create_proc_entry("debug", 0644, main_entry);
+
+	if (!sub_entry) {
+		remove_proc_entry("nlm_timer", nlm_root_proc);
+		return -ENOMEM;
+	}
+
+	sub_entry->read_proc = nlm_timer_proc_read;
+
+	printk("created NLM_timer proc fs entry\n");
+
+	return 0;
+}
+
+static void exit_pic_timer_procfs(void)
+{
+	remove_proc_entry("debug", main_entry);
+	remove_proc_entry("nlm_timer", nlm_root_proc);
+}
+
+module_init(init_pic_timer_procfs);
+module_exit(exit_pic_timer_procfs);
-- 
1.7.0.4

