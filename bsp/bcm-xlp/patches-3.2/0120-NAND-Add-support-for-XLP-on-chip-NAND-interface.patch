From bd729462d2bf2b7b84a82e29d11ea3efe1377223 Mon Sep 17 00:00:00 2001
From: Jayachandran C <jchandra@broadcom.com>
Date: Sat, 12 Apr 2014 22:53:48 +0530
Subject: NAND: Add support for XLP on-chip NAND interface

Add driver for XLP on-chip NAND interface.
[Based on SDK 3.2]
Signed-off-by: Quanyang Wang <quanyang.wang@windriver.com>
Signed-off-by: Nam Ninh <nam.ninh@windriver.com>

diff --git a/drivers/mtd/nand/Kconfig b/drivers/mtd/nand/Kconfig
index 50543f1..a682764 100644
--- a/drivers/mtd/nand/Kconfig
+++ b/drivers/mtd/nand/Kconfig
@@ -536,6 +536,17 @@ config MTD_NAND_FSMC
 	  Enables support for NAND Flash chips on the ST Microelectronics
 	  Flexible Static Memory Controller (FSMC)
 
+config MTD_NAND_XLP
+	tristate "Support for NAND on Netlogic XLP SoC"
+	depends on CPU_XLP
+	help
+	  Enables support for NAND Flash driver on Netlogic XLP SoCs.
+
+	  If you have Netlogic XLP 8xx, 3xx or 2xx boards,
+	  say yes here.
+
+	  If unsure, say N.
+
 config MTD_NAND_XWAY
 	tristate "Support for NAND on Lantiq XWAY SoC"
 	depends on LANTIQ && SOC_TYPE_XWAY
diff --git a/drivers/mtd/nand/Makefile b/drivers/mtd/nand/Makefile
index bb81891..022b399 100644
--- a/drivers/mtd/nand/Makefile
+++ b/drivers/mtd/nand/Makefile
@@ -48,6 +48,7 @@ obj-$(CONFIG_MTD_NAND_MPC5121_NFC)	+= mpc5121_nfc.o
 obj-$(CONFIG_MTD_NAND_RICOH)		+= r852.o
 obj-$(CONFIG_MTD_NAND_JZ4740)		+= jz4740_nand.o
 obj-$(CONFIG_MTD_NAND_GPMI_NAND)	+= gpmi-nand/
+obj-$(CONFIG_MTD_NAND_XLP)		+= xlp_nand.o
 obj-$(CONFIG_MTD_NAND_XWAY)		+= xway_nand.o
 obj-$(CONFIG_MTD_NAND_BCM47XXNFLASH)	+= bcm47xxnflash/
 
diff --git a/drivers/mtd/nand/xlp_nand.c b/drivers/mtd/nand/xlp_nand.c
new file mode 100644
index 0000000..d9eb1db
--- /dev/null
+++ b/drivers/mtd/nand/xlp_nand.c
@@ -0,0 +1,875 @@
+/*
+ * Copyright (c) 2003-2013 Broadcom Corporation
+ * All Rights Reserved
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the Broadcom
+ * license below:
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in
+ *    the documentation and/or other materials provided with the
+ *    distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY BROADCOM ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL BROADCOM OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
+ * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
+ * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
+ * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/slab.h>
+#include <linux/mtd/mtd.h>
+#include <linux/mtd/nand.h>
+#include <linux/mtd/partitions.h>
+#include <linux/dma-mapping.h>
+#include <linux/of_mtd.h>
+#include <linux/of_irq.h>
+#include <linux/interrupt.h>
+
+#define NAND_CMD		0x40
+#define NAND_CTRL		0x41
+#define NAND_STATUS		0x42
+#define NAND_INTMASK		0x43
+#define NAND_INT_STATUS		0x44
+#define	NAND_ECC_CTRL		0x45
+#define NAND_ECC_OFFSET		0x46
+#define NAND_ADDR0_L		0x47
+#define NAND_ADDR0_H		0x49
+#define NAND_ADDR1_L		0x48
+#define NAND_ADDR1_H		0x4A
+#define NAND_SPARE_SIZE		0x4C
+#define NAND_DMA_ADDR_L		0x59
+#define NAND_DMA_CNT		0x5A
+#define NAND_DMA_CTRL		0x5B
+#define NAND_MEMCTRL		0x60
+#define NAND_DATA_SIZE		0x61
+#define NAND_READ_STATUS	0x62
+#define NAND_TIME_SEQ0		0x63
+#define NAND_TIMINGS_ASYN	0x64
+#define NAND_TIMINGS_SYN	0x65
+#define NAND_FIFO_DATA		0x66
+#define NAND_TIME_MODE		0x67
+#define NAND_DMA_ADDR_H		0x68
+#define NAND_FIFO_INIT		0x6C
+#define NAND_GENERIC_SEQ	0x6D
+#define NAND_FIFO_STATE		0x6E
+#define NAND_TIME_SEQ1		0x6F
+#define NAND_SYSCTRL		0x80
+#define NAND_RYBYSEL		0x81
+				/*CMD 3		CMD 2		CMD 1	  SEQ */
+#define NAND_RESET_CMD		((0x0 << 24) | (0x0 << 16) | (0xFF << 8) | 0x0)
+#define NAND_READ_PARAMETER_CMD	((0x0 << 24) | (0x0 << 16) | (0xEC << 8) | 0x22)
+#define NAND_READ_ID_CMD	((0x0 << 24) | (0x0 << 16) | (0x90 << 8) | 0x21)
+#define NAND_READ_PAGE_CMD	((0x0 << 24) | (0x30 << 16) | (0x0 << 8) | 0x2a)
+#define NAND_ERASE_BLOCK_CMD	((0x0 << 24) | (0xD0 << 16) | (0x60 << 8) | 0xe)
+#define NAND_PAGE_PROGRAM_CMD	((0x0 << 24) | (0x10 << 16) | (0x80 << 8) | 0xc)
+#define NAND_READ_STATUS_CMD	((0x0 << 24) | (0x0 << 16) | (0x70 << 8) | 0x24)
+
+#define NAND_CMD_DMA_FLAG		(1 << 6)
+#define NAND_CMD_ADDR1_FLAG		(1 << 7)
+#define NAND_CTRL_GINTR_EN		(1 << 4)
+#define NAND_CTRL_X16_FLAG		(1 << 12)
+#define NAND_CTRL_CUSTOM_XFER_FLAG	(1 << 11)
+
+#define NAND_CTRL_PAGE_SIZE(size)	(size << 8)
+#define NAND_CTRL_BLOCK_SIZE(size)	(size << 6)
+#define NAND_CTRL_ADDR_CYCLE(cyc)	(cyc << 0)
+#define NAND_CTRL_ECC_EN(en)		(en << 5)
+
+/*Sync mode WE High->RE Low*/
+#define NAND_TIME_SEQ0_TWHR(x)		(x << 24)
+/*ASync mode RE High->WE Low*/
+#define NAND_TIME_SEQ0_TRHW(x)		(x << 16)
+/*Async ALE->Data start*/
+#define NAND_TIME_SEQ0_TADL(x)		(x << 8)
+/*Chance column setup*/
+#define NAND_TIME_SEQ0_TCCS(x)		(x << 0)
+/*TRR time peroid*/
+#define NAND_TIME_SEQ1_TRR(x)		(x << 9)
+/*Busy time peroid for async->sync*/
+#define NAND_TIME_SEQ1_TWB(x)		(x << 0)
+/*RE/WE high hold time*/
+#define NAND_TIME_ASYN_TRWH(x)		(x << 4)
+/*RE/WE pulse width*/
+#define NAND_TIME_ASYN_TRWP(x)		(x << 0)
+
+#define BUF_SIZE	(16 * 1024)
+#define NAND_DEV_CS	1
+
+static u64 xlp_dev_dma_mask = DMA_BIT_MASK(64);
+
+/* The oobsize is the area visible to software, and software will read/write
+ * in this region. If hardware ecc is enabled, since our implementation of
+ * write_page does not calculate hardware ECC, the hardware ECC area should
+ * not be overwritten by software.
+ *
+ * In this particular case, let us force the oobsize to be 0xc. The hardware
+ * will use the area in from spare area offset 0xc to the end of spare area.
+ *
+ * Why 0xc? This is the minimum space the hardware ECC will not occupy based
+ * on the calculation in onfi_init.
+ */
+
+#define XLP_HWECC_OOBSIZE       0xc
+
+struct xlp_nand_data {
+	struct nand_chip	chip;
+	struct mtd_info		mtd;
+	struct completion	cmd_complete;
+	void __iomem		*io_base;
+	int			hwecc;
+};
+
+struct nand_state {
+	int col_cyc;
+	int row_cyc;
+	int page_size;
+	int block_size;
+	int pages_per_block;
+	int spare_size;
+	int cs;
+	int buf_ptr;
+	u8  *buf;
+	u32 last_cmd;
+	dma_addr_t buf_dma;
+};
+
+struct nand_info {
+	int node;
+	struct nand_state *nand_state;
+};
+
+static inline int xlp_nand_read_reg(struct xlp_nand_data *data,
+		int regidx)
+{
+	return readl(data->io_base + (regidx << 2));
+}
+
+static inline void xlp_nand_write_reg(struct xlp_nand_data *data,
+		int regidx, u32 val)
+{
+	writel(val, data->io_base + (regidx << 2));
+}
+
+static int xlp_dma_wait(struct xlp_nand_data *data, int cs)
+{
+	int timeout = 0xffffff;
+	while ((!(xlp_nand_read_reg(data, NAND_STATUS) & (1 << cs))) ||
+		(!(xlp_nand_read_reg(data, NAND_DMA_CTRL) & 0x01))) {
+
+		timeout--;
+		if (timeout == 0) {
+			pr_info("%d DMA timed out NAND_STATUS:%x\n", cs,
+					xlp_nand_read_reg(data, NAND_STATUS));
+			return -1;
+		}
+	}
+	return 0;
+}
+
+static void xlp_onfi_init(struct nand_chip *chip)
+{
+	struct nand_info *info = (struct nand_info *)chip->priv;
+	struct nand_state *state = (struct nand_state *)info->nand_state;
+	u8 *param_ptr = state->buf;
+	u32 val, spare_bytes_per_512;
+	int page_val;
+	int block_val;
+	int addr_cyc, addr_val;
+	int ecc_bytes;
+	int ecc_bits;
+	int ecc_val;
+	int ecc_offset;
+	int i;
+
+	struct xlp_nand_data *data = container_of(chip,
+					struct xlp_nand_data, chip);
+
+	state->page_size = ((param_ptr[80] << 0) |
+			    (param_ptr[81] << 8) |
+			    (param_ptr[82] << 16) |
+			    (param_ptr[83] << 24));
+
+	switch (state->page_size) {
+	case 256:
+		page_val = 0;
+		break;
+	case 512:
+		page_val = 1;
+		break;
+	case 1024:
+		page_val = 2;
+		break;
+	case 2048:
+		page_val = 3;
+		break;
+	case 4096:
+		page_val = 4;
+		break;
+	case 8192:
+		page_val = 5;
+		break;
+	case 16384:
+		page_val = 6;
+		break;
+	default:
+		page_val = 7;
+	}
+
+	state->pages_per_block = ((param_ptr[92] << 0) |
+				  (param_ptr[93] << 8) |
+				  (param_ptr[94] << 16) |
+				  (param_ptr[95] << 24));
+
+	state->block_size = state->pages_per_block * state->page_size;
+
+	switch (state->pages_per_block) {
+	case 32:
+		block_val = 0;
+		break;
+	case 64:
+		block_val = 1;
+		break;
+	case 128:
+		block_val = 2;
+		break;
+	case 256:
+		block_val = 3;
+		break;
+	default:
+		block_val = -1;
+	}
+
+	addr_cyc = param_ptr[101];
+	state->row_cyc = (addr_cyc & 0xf);
+	state->col_cyc = ((addr_cyc >> 4) & 0xf);
+	addr_val = state->row_cyc + state->col_cyc;
+
+	state->spare_size = ((unsigned int)(param_ptr[84] << 0) |
+				(unsigned int)(param_ptr[85] << 8));
+
+	spare_bytes_per_512 = state->spare_size/(state->page_size/512);
+
+	if (spare_bytes_per_512 <= 4) {
+		ecc_bytes = 0;
+		ecc_bits  = 0;
+		ecc_val   = 0;
+	} else if (spare_bytes_per_512 <= 8) {
+		ecc_bytes = 4;
+		ecc_bits  = 2;
+		ecc_val   = 0;
+	} else if (spare_bytes_per_512 <= 16) {
+		ecc_bytes = 13;
+		ecc_bits  = 8;
+		ecc_val   = 3;
+	} else if (spare_bytes_per_512 <= 24) {
+		ecc_bytes = 20;
+		ecc_bits  = 12;
+		ecc_val   = 5;
+	} else {
+		ecc_bytes = 23;
+		ecc_bits  = 14;
+		ecc_val   = 6;
+	}
+	ecc_offset = state->spare_size - ((state->page_size/512) * ecc_bytes);
+
+	if (data->hwecc) {
+		xlp_nand_write_reg(data, NAND_ECC_CTRL, ecc_val << 5);
+		xlp_nand_write_reg(data, NAND_ECC_OFFSET,
+				state->page_size + ecc_offset);
+
+		if (ecc_offset < XLP_HWECC_OOBSIZE)
+			pr_info("%s: OOBSIZE is small for nand!\n", __func__);
+
+		xlp_nand_write_reg(data, NAND_SPARE_SIZE, XLP_HWECC_OOBSIZE);
+
+		val = xlp_nand_read_reg(data, NAND_CTRL);
+		val |= NAND_CTRL_ECC_EN(1);
+
+		chip->ecc.size = 512;
+		chip->ecc.strength = 2;
+		chip->ecc.bytes = ecc_bytes;
+		chip->ecc.steps	= state->page_size / 512;
+		chip->ecc.total	= chip->ecc.steps * chip->ecc.bytes;
+		chip->ecc.layout = kzalloc(sizeof(struct nand_ecclayout),
+					GFP_KERNEL);
+		if (!chip->ecc.layout)
+			return;
+		chip->ecc.layout->eccbytes = ecc_bytes;
+
+		for (i = 0; i < ecc_bytes; i++)
+			chip->ecc.layout->eccpos[i] = ecc_offset + i;
+
+		chip->ecc.layout->oobfree[0].offset = 2;
+		chip->ecc.layout->oobfree[0].length = XLP_HWECC_OOBSIZE - 2;
+	} else {
+		xlp_nand_write_reg(data, NAND_SPARE_SIZE, state->spare_size);
+		val = xlp_nand_read_reg(data, NAND_CTRL);
+		val &= ~NAND_CTRL_ECC_EN(1);
+	}
+
+	val |= NAND_CTRL_PAGE_SIZE(page_val) |
+		NAND_CTRL_BLOCK_SIZE(block_val)	|
+		NAND_CTRL_ADDR_CYCLE(addr_val);
+	xlp_nand_write_reg(data, NAND_CTRL, val);
+}
+
+static void xlp_nand_send_cmd(struct xlp_nand_data *data,
+			struct nand_state *state,
+			unsigned int cmd,
+			int page_addr, int column, int len)
+{
+	unsigned long val;
+
+	xlp_nand_write_reg(data, NAND_DATA_SIZE, len);
+	xlp_nand_write_reg(data, NAND_DMA_CNT, len);
+
+	val = (page_addr >> (32 - (state->col_cyc * 8)));
+	xlp_nand_write_reg(data, NAND_ADDR0_H, val);
+
+	val = ((page_addr << (state->col_cyc * 8)) | column);
+	xlp_nand_write_reg(data, NAND_ADDR0_L, val);
+
+	val = state->buf_dma + state->buf_ptr;
+	xlp_nand_write_reg(data, NAND_DMA_ADDR_L, val);
+	xlp_nand_write_reg(data, NAND_DMA_ADDR_H, (val >> 32));
+
+	if ((cmd == NAND_READ_PAGE_CMD) ||
+	    (cmd == NAND_READ_ID_CMD) ||
+	    (cmd == NAND_READ_PARAMETER_CMD)) {
+
+		xlp_nand_write_reg(data, NAND_DMA_CTRL,
+				(1 << 7) | (1 << 6) | (5 << 2));
+	} else
+		xlp_nand_write_reg(data, NAND_DMA_CTRL,
+				(1 << 7) | (0 << 6) | (5 << 2));
+}
+
+static void xlp_send_cmd(struct mtd_info *mtd,
+		unsigned int cmd,
+		int column,
+		int page_addr,
+		int len)
+{
+	struct nand_chip *chip = (struct nand_chip *)mtd->priv;
+	struct nand_info *info = (struct nand_info *)chip->priv;
+	struct nand_state *state = (struct nand_state *)info->nand_state;
+	unsigned long val;
+	int column2, len2;
+	int timeout;
+
+	struct xlp_nand_data *data = container_of(mtd,
+					struct xlp_nand_data, mtd);
+
+	/* Enable CEIE interrupt */
+	xlp_nand_write_reg(data, NAND_INTMASK, 0x2);
+	xlp_nand_write_reg(data, NAND_INT_STATUS, 0x0);
+
+	/* The hardware ECC will be generated if the size is mtd->writesize.
+	 * So if the write data is more than mtd->writesize,
+	 * let us break it into two.
+	 * hardware ECC will be disabled for the second part.
+	 */
+	if ((column + len) > mtd->writesize) {
+		if (mtd->writesize > 0) {
+			column2 = mtd->writesize;
+			len2 = column + len - mtd->writesize;
+			len = mtd->writesize - column;
+		} else {
+			column2 = column;
+			len2 = len;
+			len = 0;
+		}
+	} else {
+		column2 = 0;
+		len2 = 0;
+	}
+
+	if (len > 0) {
+		xlp_nand_send_cmd(data, state, cmd, page_addr, column, len);
+		xlp_nand_write_reg(data, NAND_CMD, cmd | NAND_CMD_DMA_FLAG);
+		xlp_dma_wait(data, state->cs);
+	}
+
+	if (len2 > 0) {
+		xlp_nand_send_cmd(data, state, cmd, page_addr, column, len2);
+
+		val = xlp_nand_read_reg(data, NAND_CTRL);
+		xlp_nand_write_reg(data, NAND_CTRL,
+				(val & ~NAND_CTRL_ECC_EN(1)));
+		xlp_nand_write_reg(data, NAND_CMD, cmd | NAND_CMD_DMA_FLAG);
+		xlp_dma_wait(data, state->cs);
+		if (data->hwecc) {
+			val = xlp_nand_read_reg(data, NAND_CTRL);
+			xlp_nand_write_reg(data, NAND_CTRL,
+					(val | NAND_CTRL_ECC_EN(1)));
+		}
+	}
+
+	if (cmd == NAND_CMD_PAGEPROG || cmd == NAND_CMD_READ0 ||
+			cmd == NAND_CMD_READOOB) {
+		timeout = wait_for_completion_timeout(&data->cmd_complete,
+				msecs_to_jiffies(1000));
+		if (timeout)
+			pr_err("xfer timedout!!!\n");
+	}
+
+	state->last_cmd = cmd;
+}
+
+static void xlp_nand_cmdfunc(struct mtd_info *mtd,
+		unsigned int command,
+		int column,
+		int page_addr)
+{
+	struct nand_chip *chip = (struct nand_chip *)mtd->priv;
+	struct nand_info *info = (struct nand_info *)chip->priv;
+	struct nand_state *state = (struct nand_state *)info->nand_state;
+	static int column_prog;
+	static int page_prog;
+	int len = 0, status;
+	uint32_t val;
+
+	struct xlp_nand_data *data = container_of(mtd,
+					struct xlp_nand_data, mtd);
+
+	init_completion(&data->cmd_complete);
+
+	if (state->cs < 0)
+		return;
+
+	switch (command) {
+	/*
+	 * READ0 - read in first  256 bytes
+	 * READ1 - read in second 256 bytes
+	 */
+	case NAND_CMD_READ1:
+		column += 256;
+	case NAND_CMD_READ0:
+		state->buf_ptr = 0;
+		xlp_send_cmd(mtd,
+				NAND_READ_PAGE_CMD,
+				column,
+				page_addr,
+				mtd->writesize);
+		state->buf_ptr += mtd->writesize;
+		xlp_send_cmd(mtd,
+				NAND_READ_PAGE_CMD,
+				(mtd->writesize + column),
+				page_addr,
+				(mtd->oobsize - column));
+		state->buf_ptr = 0;
+		break;
+		/* READOOB reads only the OOB because no ECC is performed. */
+	case NAND_CMD_READOOB:
+		state->buf_ptr = 0;
+		xlp_send_cmd(mtd,
+				NAND_READ_PAGE_CMD,
+				(mtd->writesize + column),
+				page_addr,
+				(mtd->oobsize - column));
+		state->buf_ptr = 0;
+		break;
+		/* READID must read all 5 possible bytes while CEB is active */
+	case NAND_CMD_READID:
+		state->buf_ptr = 0;
+		xlp_send_cmd(mtd, NAND_READ_ID_CMD, column, 0, 8);
+		state->buf_ptr = 0;
+		break;
+	case NAND_CMD_PARAM:
+		state->buf_ptr = 0;
+		xlp_send_cmd(mtd, NAND_READ_PARAMETER_CMD, 0, 0, 1024);
+		xlp_onfi_init(chip);
+		break;
+		/* ERASE1 stores the block and page address */
+	case NAND_CMD_ERASE1:
+		val = (page_addr >> (32 - (state->col_cyc*8)));
+		xlp_nand_write_reg(data, NAND_ADDR1_L, val);
+		val = ((page_addr << (state->col_cyc * 8)));
+		xlp_nand_write_reg(data, NAND_ADDR0_L, val);
+		break;
+		/* ERASE2 uses the block and page address from ERASE1 */
+	case NAND_CMD_ERASE2:
+		xlp_nand_write_reg(data, NAND_CMD, NAND_ERASE_BLOCK_CMD);
+		state->last_cmd = NAND_ERASE_BLOCK_CMD;
+		status = chip->waitfunc(mtd, chip);
+		if (status & 0x01)
+			pr_debug("%s: error status = 0x%08x\n",
+					__func__, status);
+		break;
+		/* SEQIN sets up the addr buffer and all registers except
+		 * the length */
+	case NAND_CMD_SEQIN:
+		column_prog  = column;
+		page_prog = page_addr;
+		state->buf_ptr = 0;
+		break;
+		/* PAGEPROG reuses all of the setup from SEQIN and adds
+		 * the length */
+	case NAND_CMD_PAGEPROG:
+		len = state->buf_ptr;
+		state->buf_ptr = 0;
+		xlp_send_cmd(mtd,
+				NAND_PAGE_PROGRAM_CMD,
+				column_prog,
+				page_prog,
+				len);
+		status = chip->waitfunc(mtd, chip);
+		if (status & 0x01)
+			pr_debug("%s: error status = 0x%08x\n",
+					__func__, status);
+		break;
+
+	case NAND_CMD_STATUS:
+		xlp_nand_write_reg(data, NAND_CMD, NAND_READ_STATUS_CMD);
+		state->last_cmd = NAND_READ_STATUS_CMD;
+		break;
+		/* RESET command */
+	case NAND_CMD_RESET:
+		xlp_nand_write_reg(data, NAND_CMD, NAND_RESET_CMD);
+		state->last_cmd = NAND_RESET_CMD;
+		status = chip->waitfunc(mtd, chip);
+		if (status & 0x01)
+			pr_debug("%s: error status = 0x%08x\n",
+					__func__, status);
+		break;
+
+	default:
+		pr_info("%s: unsupported command 0x%x\n", __func__, command);
+	}
+}
+
+static void xlp_select_chip(struct mtd_info *mtd, int dev)
+{
+	struct nand_chip *chip = (struct nand_chip *)mtd->priv;
+	struct nand_info *info = (struct nand_info *)chip->priv;
+	struct nand_state *state = (struct nand_state *)info->nand_state;
+
+	struct xlp_nand_data *data = container_of(mtd,
+			struct xlp_nand_data, mtd);
+
+	if ((dev >= 0) && (dev < 8)) {
+		xlp_nand_write_reg(data, NAND_MEMCTRL, state->cs);
+	}
+}
+
+static uint8_t xlp_nand_read_byte(struct mtd_info *mtd)
+{
+	struct nand_chip *chip = (struct nand_chip *)mtd->priv;
+	struct nand_info *info = (struct nand_info *)chip->priv;
+	struct nand_state *state = (struct nand_state *)info->nand_state;
+
+	struct xlp_nand_data *data = container_of(mtd,
+					struct xlp_nand_data, mtd);
+	uint32_t status;
+
+	if (state->cs < 0)
+		return 0;
+
+	if (state->last_cmd == NAND_READ_STATUS_CMD) {
+		status = xlp_nand_read_reg(data, NAND_READ_STATUS);
+		return status;
+	} else {
+		status = state->buf[state->buf_ptr];
+		state->buf_ptr = (state->buf_ptr + 1) % BUF_SIZE;
+		return status;
+	}
+}
+
+static void xlp_nand_read_buf(struct mtd_info *mtd, uint8_t *buf, int len)
+{
+	struct nand_chip *chip = (struct nand_chip *)mtd->priv;
+	struct nand_info *info = (struct nand_info *)chip->priv;
+	struct nand_state *state = (struct nand_state *)info->nand_state;
+	int i;
+
+	if (state->cs < 0)
+		return;
+
+	for (i = 0; i < len; i++) {
+		buf[i] = state->buf[state->buf_ptr];
+		state->buf_ptr = (state->buf_ptr + 1) % BUF_SIZE;
+	}
+}
+
+static void xlp_nand_write_buf(struct mtd_info *mtd, const u8 *buf, int len)
+{
+	struct nand_chip *chip = (struct nand_chip *)mtd->priv;
+	struct nand_info *info = (struct nand_info *)chip->priv;
+	struct nand_state *state = (struct nand_state *)info->nand_state;
+	int i = 0;
+
+	if (state->cs < 0)
+		return;
+
+	while (len > 0) {
+		state->buf[state->buf_ptr] = buf[i++];
+		len--;
+		state->buf_ptr = (state->buf_ptr + 1) % BUF_SIZE;
+	}
+}
+
+static int xlp_nand_read_page(struct mtd_info *mtd, struct nand_chip *chip,
+		uint8_t *buf, int oob, int page)
+{
+	struct nand_info *info = (struct nand_info *)chip->priv;
+	struct nand_state *state = (struct nand_state *)info->nand_state;
+
+	if (state->cs < 0)
+		return -1;
+
+	if (oob)
+		xlp_nand_read_buf(mtd, chip->oob_poi, mtd->oobsize);
+	else
+		xlp_nand_read_buf(mtd, buf, mtd->writesize);
+
+	return 0;
+}
+
+static int xlp_nand_write_page(struct mtd_info *mtd, struct nand_chip *chip,
+			const uint8_t *buf, int oob)
+{
+	struct nand_info *info = (struct nand_info *)chip->priv;
+	struct nand_state *state = (struct nand_state *)info->nand_state;
+
+	if (state->cs < 0)
+		return 0;
+
+	if (oob)
+		xlp_nand_write_buf(mtd, chip->oob_poi, mtd->oobsize);
+	else
+		xlp_nand_write_buf(mtd, buf, mtd->writesize);
+
+	return 0;
+}
+
+static irqreturn_t xlp_nand_interrupt(int irq, void *dev_id)
+{
+	struct xlp_nand_data *data = dev_id;
+	int stat;
+
+	stat = xlp_nand_read_reg(data, NAND_INT_STATUS);
+	/* Clear all interrupts and Disable the interrupt */
+	xlp_nand_write_reg(data, NAND_INT_STATUS, 0x0);
+	xlp_nand_write_reg(data, NAND_INTMASK, 0x0);
+
+	complete(&data->cmd_complete);
+
+	return IRQ_HANDLED;
+}
+
+static int of_xlp_nand_devices(void __iomem *io_base,
+		struct platform_device *pdev,
+		struct device_node *child, int irq, int count)
+{
+	struct xlp_nand_data *data = NULL;
+	struct nand_info *info = NULL;
+	struct nand_state *state = NULL;
+	struct mtd_part_parser_data ppdata;
+	const __be32 *prop;
+	uint32_t val;
+	int ecc_mode, ret = 0, len, chip_select;
+
+	if (count > 0) {
+		chip_select = 1;
+	} else {
+		prop = of_get_property(child, "reg", &len);
+		if (!prop || len < sizeof(*prop)) {
+			dev_err(&pdev->dev, "No 'reg' property\n");
+			return -ENXIO;
+		}
+		chip_select = be32_to_cpup(prop);
+	}
+
+	data = devm_kzalloc(&pdev->dev,
+			sizeof(struct xlp_nand_data), GFP_KERNEL);
+	if (!data) {
+		dev_err(&pdev->dev, "devm_kzalloc failed!!\n");
+		return -ENOMEM;
+	}
+
+	info = devm_kzalloc(&pdev->dev, sizeof(struct nand_info), GFP_KERNEL);
+	if (!info)
+		return -ENOMEM;
+
+	info->nand_state = devm_kzalloc(&pdev->dev,
+				sizeof(struct nand_state), GFP_KERNEL);
+	if (!info->nand_state)
+		return -ENOMEM;
+
+	data->io_base = io_base;
+
+	pdev->dev.dma_mask = &xlp_dev_dma_mask;
+	pdev->dev.coherent_dma_mask = DMA_BIT_MASK(64);
+
+	state		= info->nand_state;
+	state->last_cmd	= 0;
+	state->cs	= chip_select;
+	state->buf_ptr	= 0;
+	state->buf	= dmam_alloc_coherent(&pdev->dev, BUF_SIZE,
+			&(state->buf_dma), GFP_KERNEL);
+	if (!state->buf) {
+		pr_info("%s: dma_alloc_coherent failed!!\n", __func__);
+		return -ENXIO;
+	}
+
+	data->chip.priv	= (void *)info;
+
+	data->mtd.priv	= &data->chip;
+	data->mtd.owner	= THIS_MODULE;
+	data->mtd.name	= dev_name(&pdev->dev);
+
+	data->chip.chip_delay	= 20;
+	data->chip.IO_ADDR_R	= data->io_base;
+	data->chip.IO_ADDR_W	= data->io_base;
+	data->chip.read_byte	= xlp_nand_read_byte;
+	data->chip.read_buf	= xlp_nand_read_buf;
+	data->chip.write_buf	= xlp_nand_write_buf;
+	data->chip.cmdfunc	= xlp_nand_cmdfunc;
+	data->chip.select_chip	= xlp_select_chip;
+	data->chip.options	= NAND_SKIP_BBTSCAN;
+
+	xlp_nand_write_reg(data, NAND_CTRL, NAND_CTRL_CUSTOM_XFER_FLAG |
+			NAND_CTRL_GINTR_EN);
+	val = (NAND_TIME_SEQ0_TWHR(7) |
+			NAND_TIME_SEQ0_TRHW(7) |
+			NAND_TIME_SEQ0_TADL(7) |
+			NAND_TIME_SEQ0_TCCS(7));
+	xlp_nand_write_reg(data, NAND_TIME_SEQ0, val);
+
+	val = NAND_TIME_ASYN_TRWH(8) | NAND_TIME_ASYN_TRWP(8);
+	xlp_nand_write_reg(data, NAND_TIMINGS_ASYN, val);
+
+	ecc_mode = of_get_nand_ecc_mode(pdev->dev.of_node);
+	data->chip.ecc.mode = ecc_mode < 0 ? NAND_ECC_SOFT : ecc_mode;
+	data->hwecc = ecc_mode == NAND_ECC_HW ? 1 : 0;
+
+	if (data->hwecc)
+		data->mtd.oobsize = XLP_HWECC_OOBSIZE;
+	else
+		data->mtd.oobsize = 64;
+
+	data->chip.ecc.read_page  = xlp_nand_read_page;
+	data->chip.ecc.write_page = xlp_nand_write_page;
+
+	ret = devm_request_irq(&pdev->dev, irq, xlp_nand_interrupt, IRQF_SHARED,
+			pdev->name, data);
+	if (ret) {
+		dev_err(&pdev->dev, "request_irq failed!!\n");
+		return ret;
+	}
+
+	platform_set_drvdata(pdev, data);
+
+	if (nand_scan(&data->mtd, 1))
+		return -ENXIO;
+
+	ppdata.of_node = child;
+	ret = mtd_device_parse_register(&data->mtd, NULL, &ppdata, NULL, 0);
+	if (!ret)
+		return ret;
+
+	nand_release(&data->mtd);
+
+	return ret;
+}
+/*
+ * Probe for the NAND device.
+ */
+static int xlp_nand_probe(struct platform_device *pdev)
+{
+	struct device_node *child;
+	struct resource *res;
+	void __iomem *io_base;
+	const __be32 *prop;
+	int irq, ret = 0, count = 0;
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!res) {
+		pr_info("couldn't get resource !!\n");
+		return -ENOMEM;
+	}
+
+	prop = of_get_property(pdev->dev.of_node, "interrupts", NULL);
+	if (prop == NULL) {
+		dev_err(&pdev->dev, "No \"interrupts\" property!\n");
+		return -ENXIO;
+	}
+	irq = be32_to_cpu(*prop);
+	if (!irq) {
+		dev_err(&pdev->dev, "no irq!\n");
+		return -ENXIO;
+	}
+
+	io_base = devm_request_and_ioremap(&pdev->dev, res);
+	if (!io_base) {
+		dev_err(&pdev->dev, "ioremap failed!!\n");
+		return -ENOMEM;
+	}
+
+	for_each_child_of_node(pdev->dev.of_node, child) {
+		/* Needs to be removed with updated FDT */
+		if (!strcmp(child->name, "partition")) {
+			count++;
+		} else {
+			ret = of_xlp_nand_devices(io_base, pdev, child,
+					irq, count);
+			if (ret < 0)
+				return ret;
+		}
+
+		continue;
+	}
+	/* Needs to be removed  with updated FDT */
+	if (count > 0) {
+		dev_err(&pdev->dev, "Old FDT, DTS file needs to be updated!\n");
+		child = pdev->dev.of_node;
+		ret = of_xlp_nand_devices(io_base, pdev, child, irq, count);
+		if (ret < 0)
+			return ret;
+	}
+	return 0;
+}
+
+static int xlp_nand_remove(struct platform_device *pdev)
+{
+	struct xlp_nand_data *data = platform_get_drvdata(pdev);
+	nand_release(&data->mtd);
+	kfree(data);
+	return 0;
+
+}
+static const struct of_device_id xlp_nand_dt[] = {
+	{ .compatible = "netlogic,xlp-nand" },
+};
+
+static struct platform_driver xlp_nand_driver = {
+	.probe	= xlp_nand_probe,
+	.remove	= xlp_nand_remove,
+	.driver	= {
+		.name		= "xlp-nand",
+		.owner		= THIS_MODULE,
+		.of_match_table = xlp_nand_dt,
+	},
+};
+module_platform_driver(xlp_nand_driver);
+
+MODULE_AUTHOR("Kamlakant Patel <kamlakant.patel@broadcom.com>");
+MODULE_DESCRIPTION("Netlogic XLP NAND Flash Controller driver");
+MODULE_LICENSE("GPL v2");
-- 
1.7.1

