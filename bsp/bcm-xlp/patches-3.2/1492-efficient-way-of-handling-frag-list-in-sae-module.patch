From c3566901a06a73727c51316cfc299b85c9d80f14 Mon Sep 17 00:00:00 2001
From: reshmic <reshmic@broadcom.com>
Date: Fri, 16 Mar 2012 17:32:34 +0530
Subject: efficient way of handling frag list in sae module

[Based on SDK 3.2]
Signed-off-by: Quanyang Wang <quanyang.wang@windriver.com>
Signed-off-by: Nam Ninh <nam.ninh@windriver.com>

diff --git a/drivers/crypto/sae/nlm_aead.c b/drivers/crypto/sae/nlm_aead.c
index a7f80a0..ffd6606 100755
--- a/drivers/crypto/sae/nlm_aead.c
+++ b/drivers/crypto/sae/nlm_aead.c
@@ -30,10 +30,8 @@ THE POSSIBILITY OF SUCH DAMAGE.
 #include <crypto/sha.h>
 #include <crypto/aead.h>
 #include <crypto/authenc.h>
-#include <crypto/scatterwalk.h>
 
 #include <asm/netlogic/hal/nlm_hal_fmn.h>
-#include "nlmcrypto.h"
 #include <asm/netlogic/msgring.h>
 #include "nlm_async.h"
 
@@ -72,8 +70,8 @@ struct nlm_aead_ctx
 	struct crypto_aead  * fallback;
 };
 
-#define MAX_FRAGS		0xfff	
-#define CTRL_DESC_SIZE		(sizeof(struct nlm_aead_ctx) + 64)
+#define MAX_FRAGS		18	
+#define CTRL_DESC_SIZE		(sizeof(struct nlm_aead_ctx) + 128)
 #define DES3_CTRL_DESC_SIZE	(2*CTRL_DESC_SIZE + 2*64)	//Allocate 2 separate control desc for encryption and decryption
 #define CACHE_ALIGN		64
 #define IV_AEAD_PADDING         128
@@ -86,10 +84,9 @@ struct nlm_aead_ctx
 	|  for alignment | 18 * (2*64)			  | for alignment |			    | for hash |
 	 ------------------------------------------------------------------------------------------------------
  */
-
-#define PACKET_DESC_SIZE	(CACHE_ALIGN + sizeof(struct nlm_crypto_pkt_param) + MAX_FRAGS*(2*CACHE_ALIGN) + CACHE_ALIGN + sizeof(struct nlm_async_crypto) + IV_AEAD_PADDING + TAG_LEN)
+#define PACKET_DESC_SIZE	(CACHE_ALIGN + sizeof(struct nlm_crypto_pkt_param) + (MAX_FRAGS*(2*8)) + 64 + sizeof(struct nlm_async_crypto) + 64 + IV_AEAD_PADDING + TAG_LEN)
 #define NLM_CRYPTO_PKT_PARAM_OFFSET(addr)	(((unsigned long)addr + CACHE_ALIGN ) & ~0x3fUL)
-#define NLM_ASYNC_PTR_PARAM_OFFSET(addr)	(((unsigned long)(addr + CACHE_ALIGN + sizeof(struct nlm_crypto_pkt_param) + MAX_FRAGS*(2*CACHE_ALIGN)) + CACHE_ALIGN) & ~0x3fUL)
+#define NLM_ASYNC_PTR_PARAM_OFFSET(addr)	((unsigned long)addr + CACHE_ALIGN + sizeof(struct nlm_crypto_pkt_param) + (MAX_FRAGS*(2*8)) + 64 + ~0x3fUL) 
 #define NLM_HASH_OFFSET(addr)			((unsigned long)addr + (PACKET_DESC_SIZE - TAG_LEN))
 #define NLM_IV_OFFSET(addr)			((unsigned long)addr + (PACKET_DESC_SIZE - TAG_LEN - IV_AEAD_PADDING ))
 
@@ -116,6 +113,8 @@ extern uint32_t nlm_hal_send_msg3(uint32_t dst, uint32_t code, uint64_t data0, u
 extern uint32_t nlm_hal_recv_msg2(uint32_t dst, uint32_t *src, uint32_t *size, uint32_t *code, uint64_t *data0, uint64_t *data1);
 extern int auth_mode_key_len[NLM_HASH_MAX][NLM_HASH_MODE_MAX];
 extern int cipher_mode_iv_len[NLM_CIPHER_MAX][NLM_CIPHER_MODE_MAX];
+extern int alloc_pkt_param( struct nlm_async_crypto * async,struct nlm_crypto_pkt_param ** pkt_param, int max_frags );
+
 
 
 static int no_of_alg_registered = 0;
@@ -144,12 +143,13 @@ static void print_buf(unsigned char *msg, unsigned char *buf, int len)
 
 static struct nlm_aead_ctx *nlm_crypto_aead_ctx(struct crypto_aead *tfm)
 {
-	return (struct nlm_aead_ctx *)(((unsigned long)((uint8_t *)crypto_aead_ctx(tfm) + 63)) & ~(0x3fUL));
+
+	return (struct nlm_aead_ctx *)(((unsigned long)((uint8_t *)crypto_aead_ctx(tfm) + 64)) & ~(0x3fUL));
 }
 
 static struct nlm_aead_ctx *nlm_crypto_tfm_ctx(struct crypto_tfm *tfm)
 {
-	return (struct nlm_aead_ctx *)(((unsigned long)((uint8_t *)crypto_tfm_ctx(tfm) + 63)) & ~(0x3fUL));
+	return (struct nlm_aead_ctx *)(((unsigned long)((uint8_t *)crypto_tfm_ctx(tfm) + 64)) & ~(0x3fUL));
 }
 
 
@@ -392,7 +392,7 @@ static int xlp_aes_ctr_setkey( struct crypto_aead *tfm, u8 *key,
 static int  xlp_3des_setkey(struct crypto_aead *tfm, u8 *key, unsigned int keylen, int hash, int mode,uint16_t h_stat )
 {
 	struct nlm_aead_ctx *ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
-	struct nlm_aead_ctx * nlm_ctx = (struct  nlm_aead_ctx *)(( unsigned long )(( uint8_t *)ctx + CTRL_DESC_SIZE + 63) & ~(0x3fUL));
+	struct nlm_aead_ctx * nlm_ctx = (struct  nlm_aead_ctx *)(( unsigned long )(( uint8_t *)ctx + CTRL_DESC_SIZE + 64) & ~(0x3fUL));
         unsigned int cipher_keylen=0, auth_keylen=0;
 	struct nlm_crypto_pkt_ctrl *ctrl = &ctx->ctrl;
         int ret;
@@ -665,92 +665,30 @@ static int xlp_aes_ctr_hmac_md5_setkey(struct crypto_aead *tfm, const u8 *key,
 	return xlp_aes_ctr_setkey(tfm, (uint8_t *)key, keylen,NLM_HASH_MD5,0,MD5_STAT);
 }
 //returns nr_aad_frags... -1 for error
-int fill_aead_aad(struct nlm_crypto_pkt_param *param, struct aead_request *req, unsigned int aad_len,int seg)
+unsigned int fill_aead_aad(struct nlm_crypto_pkt_param *param, struct aead_request *req, unsigned int aad_len,int seg,int max_frags)
 {
 	struct scatterlist *sg;
 	struct scatter_walk walk;
 	int len;
 	uint8_t *virt;
+	int rv = 0;
 	
-	for (sg = req->assoc; aad_len > 0; sg = scatterwalk_sg_next(sg), 
-			seg++) {
+	for (sg = req->assoc; aad_len > 0; sg = scatterwalk_sg_next(sg) ) {
 
 		len = min(aad_len, sg->length);
 		scatterwalk_start(&walk, sg);
 		//virt = scatterwalk_map(&walk, 1);
 		virt = page_address(scatterwalk_page(&walk)) + offset_in_page(walk.offset);
-		nlm_crypto_fill_src_seg(param, seg, MAX_FRAGS, virt, len);
-		nlm_crypto_fill_dst_seg(param, seg, MAX_FRAGS, virt, len);
+		rv = nlm_crypto_fill_src_dst_seg(param, seg, max_frags, virt, len);
+		if ( rv <= seg ) 
+			return nlm_crypto_calc_rem_len(sg,aad_len) + max_frags;
+		else 
+			seg = rv;
 		aad_len -= len;
 	}
 	return seg;
 }
-
-int fill_aead_crypt(struct aead_request *req, unsigned int cipher_len, 
-		struct nlm_crypto_pkt_param *param, unsigned char **actual_tag, int op, int seg)
-{
-	struct scatterlist *sg;
-	struct scatter_walk walk;
-	unsigned int len = 0;
-	uint8_t *virt = NULL;
-	int nr_src_frags = 0;
-	int nr_dst_frags = 0;
-	int passed_len, i;
-	int index = 0;
-
-	if (req->src == req->dst) {
-		for (sg = req->src; cipher_len > 0; sg = scatterwalk_sg_next(sg), index++) {
-			len = min(cipher_len, sg->length);
-			scatterwalk_start(&walk, sg);
-			virt = page_address(scatterwalk_page(&walk)) + offset_in_page(walk.offset);
-			nlm_crypto_fill_src_seg(param, seg + index, MAX_FRAGS, virt, len);
-			nlm_crypto_fill_dst_seg(param, seg + index, MAX_FRAGS, virt, len);
-			cipher_len -= len;
-		}
-		*actual_tag = virt + len;
-		return index;
-	}
-	passed_len = cipher_len;
-
-	for (sg = req->src, index = 0; cipher_len > 0; sg = scatterwalk_sg_next(sg),
-	     nr_src_frags++, index++) {
-		len = min(cipher_len, sg->length);
-		scatterwalk_start(&walk, sg);
-		virt = page_address(scatterwalk_page(&walk)) + offset_in_page(walk.offset);
-		nlm_crypto_fill_src_seg(param, seg + index, MAX_FRAGS, virt, len);
-		cipher_len -= len;
-	}
-
-	if (op == NETL_OP_ENCRYPT)
-		*actual_tag = virt + len;
-
-	cipher_len = passed_len;
-
-	for (sg = req->dst, index=0; cipher_len > 0; sg = scatterwalk_sg_next(sg),
-	     nr_dst_frags++, index++) {
-		len = min(cipher_len, sg->length);
-		scatterwalk_start(&walk, sg);
-		virt = page_address(scatterwalk_page(&walk)) + offset_in_page(walk.offset);
-		nlm_crypto_fill_dst_seg(param, seg + index, MAX_FRAGS, virt, len);
-		cipher_len -= len;
-	}
-
-	if (op == NETL_OP_DECRYPT)
-		*actual_tag = virt + len;
-
-	if (nr_src_frags > nr_dst_frags) {
-		for (i = 0; i < nr_src_frags - nr_dst_frags; i++)
-			param->segment[seg + nr_dst_frags + i][1] = 0ULL;
-		return nr_src_frags;
-	}else{
-		if (nr_src_frags < nr_dst_frags) {
-			for (i = 0; i < nr_dst_frags - nr_src_frags; i++)
-				param->segment[seg + nr_src_frags + i][0] = 0ULL;
-		}
-		return nr_dst_frags;
-	}
-}
-
+		
 /*
    Generic Encrypt / Decrypt Function
  */
@@ -771,9 +709,11 @@ static void aead_request_callback(struct nlm_async_crypto *async, uint64_t msg1)
 		return;
 	}
 	if (async->op){
-		memcpy(async->actual_tag, async->hash_addr, async->authsize);
+		scatterwalk_map_and_copy(async->hash_addr, async->dst, async->bytes, async->authsize, 1);
 	}else{
-		if(memcmp(async->actual_tag, async->hash_addr, async->authsize)){
+		char authtag[64];
+		scatterwalk_map_and_copy(authtag, async->src, async->bytes-async->authsize, async->authsize, 0);
+		if(memcmp(authtag, async->hash_addr, async->authsize)){
 			err = -EBADMSG;
 		}
 	}
@@ -781,6 +721,8 @@ static void aead_request_callback(struct nlm_async_crypto *async, uint64_t msg1)
 	crypto_stat[cpu].auth[auth]++;	
 	crypto_stat[cpu].enc_tbytes[enc] += async->bytes;
 	crypto_stat[cpu].auth_tbytes[auth] += async->bytes;
+	if ( async->pkt_param)
+		kfree(async->pkt_param);
 	base->complete(base, err);
 	return;
 }
@@ -795,23 +737,25 @@ static int aead_crypt_gcm(struct aead_request *req, unsigned int op)
 	struct nlm_crypto_pkt_param *param;
 	unsigned int cipher_off, auth_off, iv_off;
 	unsigned int auth_len, cipher_len;
-	unsigned char *actual_tag;
 	int seg = 0,iv_size =16;
-	unsigned int hash_source, nr_aad_frags;
+	unsigned int hash_source;
 	uint64_t entry0, entry1, tx_id=0x12345678;
-	struct nlm_async_crypto *async = NULL;
+	struct nlm_async_crypto *async =  (struct nlm_async_crypto *)NLM_ASYNC_PTR_PARAM_OFFSET(aead_request_ctx(req));
 	uint8_t *hash_addr;
 	int fb_vc; 
 	int err=0;
-	int nr_enc_frags;
 	unsigned int authsize,maxauthsize;
 	struct nlm_crypto_pkt_ctrl *ctrl = &ctx->ctrl;
 	char * iv = (uint8_t *)NLM_IV_OFFSET(aead_request_ctx(req)); 
 	uint8_t *tmp_iv = iv;
 	unsigned long msgrng_flags;
+	unsigned int max_frags  = MAX_FRAGS;
 
-	authsize = crypto_aead_crt(crt->base)->authsize;
+	authsize = crypto_aead_authsize(tfm);
 	maxauthsize= aead->maxauthsize;
+	async->pkt_param = NULL;
+	async->src = req->src;
+	async->dst = req->dst;
 
 	if ( (op &&  (req->cryptlen == 0 )) || (!op && req->cryptlen <= aead->maxauthsize))
 	{
@@ -853,28 +797,23 @@ static int aead_crypt_gcm(struct aead_request *req, unsigned int op)
 	hash_source = (op) ? 0 : 1;
 
 	/*Setup NONCE_IV_CTR COMBO*/
-	nlm_crypto_fill_src_seg(param, seg, MAX_FRAGS, iv, iv_size);
-	nlm_crypto_fill_dst_seg(param, seg, MAX_FRAGS, iv, iv_size);
-	seg++;
+	seg = nlm_crypto_fill_src_dst_seg(param, seg, max_frags, iv, iv_size);
+
+	do { 
 	
 	/*Setup AAD - SPI/SEQ Number*/
-	nr_aad_frags = fill_aead_aad(param, req, req->assoclen,seg);
-	if (nr_aad_frags == 0)
-		return -1;
-	seg = nr_aad_frags;
+	seg = fill_aead_aad(param, req, req->assoclen,seg,max_frags);
+	seg = fill_src_dst_sg(req->src,req->dst, cipher_len, param, seg, max_frags,op);
 
-	nr_enc_frags = fill_aead_crypt(req, cipher_len, param, &actual_tag, op, seg);
-
-	if (nr_enc_frags == -1)
-		return -1;
-
-	seg += nr_enc_frags;
-
-	if ( seg > MAX_FRAGS) {
-		printk("fragments exceeded 0xfff. Cannot handle this\n");
-		return 0;
+	if ( seg > max_frags) {
+		max_frags = seg;
+		seg = alloc_pkt_param(async,&param,max_frags);
+		if ( seg == -1 )
+			return -1;
+		seg =1 ;
 	}
-		
+	}while(seg == 1);  
+
 	nlm_crypto_fill_cipher_auth_pkt_param(ctrl, param, op, hash_source, iv_off, 
 		iv_size, auth_off, auth_len, 0, cipher_off, cipher_len, hash_addr);
 	
@@ -895,16 +834,14 @@ static int aead_crypt_gcm(struct aead_request *req, unsigned int op)
 	printk("auth_len: %d, cipher_len: %d\n", auth_len, cipher_len);
 #endif
 
-	async = (struct nlm_async_crypto *)NLM_ASYNC_PTR_PARAM_OFFSET(aead_request_ctx(req));
 	async->callback =  aead_request_callback;
 	async->args = (void *)&req->base; 
 	async->op  = op;
-	async->actual_tag = actual_tag;
 	async->hash_addr = hash_addr;
 	async->authsize = authsize;
 	async->stat = ctx->stat; 
 	async->bytes = req->cryptlen; 
-	tx_id = (uint64_t)async;
+	tx_id = (uint64_t)(unsigned long)async;
 
 	//construct pkt, send to engine and receive reply
 	msgrng_access_enable(msgrng_flags);
@@ -930,10 +867,9 @@ static int aead_crypt_ccm(struct aead_request *req, unsigned int op)
 	struct nlm_crypto_pkt_param *param;
 	unsigned int cipher_off, auth_off, iv_off;
 	unsigned int auth_len, cipher_len;
-	unsigned char *actual_tag;
-	unsigned int hash_source, nr_aad_frags;
+	unsigned int hash_source;
 	uint64_t entry0, entry1, tx_id=0x12345678;
-	struct nlm_async_crypto *async = NULL;
+	struct nlm_async_crypto *async =  (struct nlm_async_crypto *)NLM_ASYNC_PTR_PARAM_OFFSET(aead_request_ctx(req));
 	uint8_t *hash_addr;
 	int fb_vc; 
 	int err=0;
@@ -943,16 +879,19 @@ static int aead_crypt_ccm(struct aead_request *req, unsigned int op)
 	int iv_size = 16;
 	uint8_t *auth_iv = (uint8_t *)iv + iv_size;
 	unsigned int auth_iv_frag_len = iv_size; 
-	unsigned int extralen = req->assoclen;
+	unsigned int extralen = 0, cipher_extralen =0;
 	int seg = 0;
-	int nr_enc_frags;
 	uint8_t *tmp_iv = &iv[1];
 	unsigned long msgrng_flags;
+	unsigned int max_frags  = MAX_FRAGS;
 	param = (struct nlm_crypto_pkt_param *)NLM_CRYPTO_PKT_PARAM_OFFSET(aead_request_ctx(req));
 	hash_addr = (uint8_t *)NLM_HASH_OFFSET(aead_request_ctx(req));
 
-	authsize = crypto_aead_crt(crt->base)->authsize;
+	authsize = crypto_aead_authsize(tfm);
 	maxauthsize= aead->maxauthsize;
+	async->pkt_param = NULL;
+	async->src = req->src;
+	async->dst = req->dst;
 
 	if ( (op &&  (req->cryptlen == 0 )) || (!op && req->cryptlen <= aead->maxauthsize))
 	{
@@ -969,7 +908,6 @@ static int aead_crypt_ccm(struct aead_request *req, unsigned int op)
 	}
 	
 		
-
 	/*Copy nonce*/
 	memcpy(tmp_iv,  &ctx->iv_buf, CCM_RFC4309_NONCE_SIZE);
 	tmp_iv += CCM_RFC4309_NONCE_SIZE;
@@ -990,7 +928,6 @@ static int aead_crypt_ccm(struct aead_request *req, unsigned int op)
 
 	iv[0] = 3;
 
-	//check if it should be aip->tag_len or can be taken from tfm
 	iv_off = 0;
 	auth_off = iv_size;
 	cipher_len = op ? req->cryptlen: req->cryptlen - authsize;
@@ -998,9 +935,6 @@ static int aead_crypt_ccm(struct aead_request *req, unsigned int op)
 
 
 	/*Setup ENCRYPTION IV*/
-	nlm_crypto_fill_src_seg(param, seg, MAX_FRAGS, iv, iv_size);
-	nlm_crypto_fill_dst_seg(param, seg, MAX_FRAGS, iv, iv_size);
-	seg++;
 
 	/*	7            Reserved (always zero)
 		6            Adata
@@ -1008,6 +942,9 @@ static int aead_crypt_ccm(struct aead_request *req, unsigned int op)
 		2 ... 0      L' */
 
 	auth_iv[0] = ((req->assoclen?1 : 0 ) << 6 )| ((authsize - 2 )/2) << 3 | 3;
+
+	auth_iv_frag_len = iv_size; 
+	extralen = req->assoclen;
 	if ( req->assoclen ) {
 
 
@@ -1029,52 +966,60 @@ static int aead_crypt_ccm(struct aead_request *req, unsigned int op)
 	/*Setup AUTH IV*/
 	*(uint32_t*)&auth_iv[12] |= (uint32_t )cipher_len;
 
-	nlm_crypto_fill_src_seg(param, seg,MAX_FRAGS, auth_iv,auth_iv_frag_len);
-	nlm_crypto_fill_dst_seg(param, seg,MAX_FRAGS, auth_iv,auth_iv_frag_len);
-	seg++;
 
 	//one for cipher iv one for auth iv
 	cipher_off = iv_size + iv_size + extralen;  
 
 	if ( req->assoclen ) {
-		/*Setup AAD - SPI/SEQ Number*/
-		nr_aad_frags = fill_aead_aad(param, req, req->assoclen,seg);
-		if (nr_aad_frags == -1)
-			return -1;
-
-		seg = nr_aad_frags;
-
 		/* AAD has to be block aligned */
 		extralen = extralen % 16;
 		if ( extralen ) {
 			extralen = AES_BLOCK_SIZE - extralen;
 			memset(auth_iv + 22,0,extralen);
-			nlm_crypto_fill_src_seg(param,seg,MAX_FRAGS, (auth_iv+22),extralen);
-			nlm_crypto_fill_dst_seg(param,seg,MAX_FRAGS, (auth_iv+22),extralen);
-			seg++;
 			cipher_off += extralen;
 		}
 	}
 
-	nr_enc_frags = fill_aead_crypt(req, cipher_len, param, &actual_tag, op, seg);
+	cipher_extralen = cipher_len % 16;
+
+	if ( cipher_extralen ) {
+		cipher_extralen = AES_BLOCK_SIZE - cipher_extralen;
+		memset(auth_iv + 38,0,cipher_extralen);
+	}
+	auth_len = cipher_off + cipher_len + cipher_extralen - auth_off; 
+
+	seg = nlm_crypto_fill_src_dst_seg(param, seg, max_frags, iv, iv_size);
 
-	seg += nr_enc_frags;
+	do {
+	auth_iv = (uint8_t *)iv + iv_size;
+	seg = nlm_crypto_fill_src_dst_seg(param, seg,max_frags, auth_iv,auth_iv_frag_len);
+	if ( req->assoclen ) {
+		/*Setup AAD - SPI/SEQ Number*/
+		seg = fill_aead_aad(param, req, req->assoclen,seg,max_frags);
+		if ( extralen ) 
+			seg = nlm_crypto_fill_src_dst_seg(param,seg,max_frags, (auth_iv+22),extralen);
+		
+	}
+	seg = fill_src_dst_sg(req->src,req->dst, cipher_len, param,seg, max_frags,op);
 
-	extralen = cipher_len % 16;
 
-	if ( extralen ) {
-		extralen = AES_BLOCK_SIZE - extralen;
-		memset(auth_iv + 38,0,extralen);
-		nlm_crypto_fill_src_seg(param,seg,MAX_FRAGS, (auth_iv+38),extralen);
-		nlm_crypto_fill_dst_seg(param,seg,MAX_FRAGS, (auth_iv+38),extralen);
-		seg++;
+	if ( cipher_extralen ) {
+		if ( seg >= max_frags ){
+			seg+=  nlm_crypto_sae_num_seg_reqd(auth_iv+38, cipher_extralen);
+		}
+		else 
+			seg = nlm_crypto_fill_src_dst_seg(param,seg,max_frags, (auth_iv+38),cipher_extralen);
 	}
-	auth_len = cipher_off + cipher_len - 16 + extralen; 
 	
-	if ( seg > MAX_FRAGS) {
-		printk("fragments exceeded 0xfff. Cannot handle this\n");
-		return 0;
+
+	if ( seg > max_frags) {
+		max_frags = seg;
+		seg = alloc_pkt_param(async,&param,max_frags);
+		if ( seg == -1 ) {
+			return -1;
+		}
 	}
+	}while(seg == 1);  
 
 	nlm_crypto_fill_cipher_auth_pkt_param(ctrl, param, op, hash_source, iv_off,
 			iv_size, auth_off, auth_len, 0, cipher_off, cipher_len, hash_addr);
@@ -1085,6 +1030,7 @@ static int aead_crypt_ccm(struct aead_request *req, unsigned int op)
 	entry1 = nlm_crypto_form_pkt_fmn_entry1(0, ctrl->hashkeylen, (seg + 2)<<4, virt_to_phys(param));
 
 
+
 #ifdef NLM_CRYPTO_DEBUG
 	printk("*****ctrl descriptor phys addr %#lx\n",virt_to_phys(ctrl));
 	print_crypto_msg_desc(entry0, entry1, tx_id);
@@ -1097,16 +1043,14 @@ static int aead_crypt_ccm(struct aead_request *req, unsigned int op)
 #endif
 
 
-	async = (struct nlm_async_crypto *)NLM_ASYNC_PTR_PARAM_OFFSET(aead_request_ctx(req));
 	async->callback =  aead_request_callback;
 	async->args = (void *)&req->base; 
 	async->op  = op;
-	async->actual_tag = actual_tag;
 	async->hash_addr = hash_addr;
 	async->authsize = authsize;
 	async->stat = ctx->stat; 
 	async->bytes = req->cryptlen; 
-	tx_id = (uint64_t)async;
+	tx_id = (uint64_t)(unsigned long)async;
 
 	//construct pkt, send to engine and receive reply
 	msgrng_access_enable(msgrng_flags);
@@ -1132,24 +1076,29 @@ static int aead_crypt_ctr(struct aead_request *req, unsigned int op)
 	struct nlm_crypto_pkt_param *param;
 	unsigned int cipher_off, iv_off;
 	unsigned int auth_len, cipher_len, auth_off;
-	unsigned char *actual_tag;
-	int seg = 0, nr_enc_frags, ivsize;
+	int seg = 0,ivsize;
 	unsigned int hash_source;
 	uint64_t entry0, entry1;
 	uint64_t tx_id=0x12345678;
 	void  * addr = aead_request_ctx(req);
-	struct nlm_async_crypto *async = NULL;
+	struct nlm_async_crypto *async =  (struct nlm_async_crypto *)NLM_ASYNC_PTR_PARAM_OFFSET(aead_request_ctx(req));
 	uint8_t *hash_addr;
 	int fb_vc; 
 	int err=0;
 	unsigned int authsize,maxauthsize;
-	uint8_t *iv = (uint8_t *)NLM_IV_OFFSET(addr); 
+	uint8_t *iv = NULL; 
 	struct nlm_crypto_pkt_ctrl *ctrl = &ctx->ctrl;
 	unsigned long msgrng_flags;
+	unsigned int max_frags  = MAX_FRAGS;
 
-	authsize = crypto_aead_crt(crt->base)->authsize;
+	authsize = crypto_aead_authsize(tfm);
 	maxauthsize= aead->maxauthsize;
+	async->pkt_param = NULL;
+	async->src = req->src;
+	async->dst = req->dst;
+	param = (struct nlm_crypto_pkt_param *)NLM_CRYPTO_PKT_PARAM_OFFSET(addr);
 
+	iv = (uint8_t *)NLM_IV_OFFSET(addr);
 	if ( !op ) {
 		 uint8_t *tmp_iv = iv;
 		/*Copy nonce*/
@@ -1163,7 +1112,6 @@ static int aead_crypt_ctr(struct aead_request *req, unsigned int op)
 		*((uint32_t *)tmp_iv) = (uint32_t)1;
 	}
 
-	param = (struct nlm_crypto_pkt_param *)NLM_CRYPTO_PKT_PARAM_OFFSET(addr);
 	hash_addr = (uint8_t *)NLM_HASH_OFFSET(addr);
 
 	ivsize = crypto_aead_ivsize(crypto_aead_reqtfm(req));
@@ -1176,31 +1124,24 @@ static int aead_crypt_ctr(struct aead_request *req, unsigned int op)
 	cipher_len = op ? req->cryptlen:req->cryptlen - authsize;
 	auth_len = cipher_off - auth_off + cipher_len;
 	hash_source = op;
-	nlm_crypto_fill_src_seg(param, seg, MAX_FRAGS, iv, CTR_RFC3686_BLOCK_SIZE);
-	nlm_crypto_fill_dst_seg(param, seg, MAX_FRAGS, iv, CTR_RFC3686_BLOCK_SIZE);
-	seg++;
-
-	seg = fill_aead_aad(param, req, req->assoclen,seg);
-	if (seg == 0)
-		return -1;
-
-	if (ivsize) {
-		nlm_crypto_fill_src_seg(param, seg, MAX_FRAGS, iv+CTR_RFC3686_NONCE_SIZE, ivsize);
-		nlm_crypto_fill_dst_seg(param, seg, MAX_FRAGS, iv+CTR_RFC3686_NONCE_SIZE, ivsize);
-		seg++;
-	}
+	seg = nlm_crypto_fill_src_dst_seg(param, seg, max_frags, iv, CTR_RFC3686_BLOCK_SIZE);
 
-	nr_enc_frags = fill_aead_crypt(req, cipher_len, param, &actual_tag, op, seg);
 
-	if (nr_enc_frags == -1)
-		return -1;
+	do {
+	seg = fill_aead_aad(param, req, req->assoclen,seg,max_frags);
 
-	seg += nr_enc_frags;
+	seg = 	nlm_crypto_fill_src_dst_seg(param, seg, max_frags, iv+CTR_RFC3686_NONCE_SIZE, ivsize);
 
-	if ( seg > MAX_FRAGS) {
-		printk("fragments exceeded 0xfff. Cannot handle this\n");
-		return 0;
+	seg = fill_src_dst_sg(req->src,req->dst, cipher_len, param, seg, max_frags,op);
+
+	if ( seg > max_frags) {
+		max_frags = seg;
+		seg = alloc_pkt_param(async,&param,max_frags);
+		if ( seg == -1 )
+			return -1;
+		seg = 1;
 	}
+	}while(seg == 1);  
 
 	nlm_crypto_fill_cipher_auth_pkt_param(ctrl, param, op, op, iv_off, 
 			CTR_RFC3686_BLOCK_SIZE, auth_off, auth_len, 0, cipher_off, cipher_len, hash_addr);
@@ -1214,25 +1155,23 @@ static int aead_crypt_ctr(struct aead_request *req, unsigned int op)
 #ifdef NLM_CRYPTO_DEBUG
 	printk("*****ctrl descriptor phys addr %#lx\n",virt_to_phys(ctrl));
 	print_crypto_msg_desc(entry0, entry1, tx_id);
+	print_pkt_desc(param,seg);
 	print_cntl_instr(ctrl->desc0);
 	print_buf("ENC_KEY:", (uint8_t *)ctrl->key, ctrl->cipherkeylen);
 	print_buf("AUTH_KEY:", (uint8_t *)ctrl->key + ctrl->cipherkeylen, ctrl->hashkeylen);
-	print_pkt_desc(param,seg);
 	print_buf("IV ",iv,16);
 	printk("iv_off: %d, cipher_off: %d, auth_off: %d\n",iv_off, cipher_off, auth_off);
 	printk("auth_len: %d, cipher_len: %d\n", auth_len, cipher_len);
 #endif
 
-	async = (struct nlm_async_crypto *)NLM_ASYNC_PTR_PARAM_OFFSET(addr);
 	async->callback =  aead_request_callback;
 	async->args = (void *)&req->base; 
 	async->op  = op;
-	async->actual_tag = actual_tag;
 	async->hash_addr = hash_addr;
 	async->authsize = authsize;
 	async->stat = ctx->stat;
 	async->bytes = req->cryptlen;
-	tx_id = (uint64_t)async;
+	tx_id = (uint64_t)(unsigned long)async;
 
 	//construct pkt, send to engine and receive reply
 	msgrng_access_enable(msgrng_flags);
@@ -1255,25 +1194,26 @@ static int aead_crypt_3des(struct aead_request *req, unsigned int op)
 	struct nlm_crypto_pkt_param *param;
 	unsigned int cipher_off, iv_off;
 	unsigned int auth_len, cipher_len, auth_off;
-	unsigned char *actual_tag;
-	int seg =0, nr_enc_frags, ivsize;
-	unsigned int hash_source, nr_aad_frags;
+	int seg =0,  ivsize;
+	unsigned int hash_source;
 	uint64_t entry0, entry1;
 	uint64_t tx_id=0x12345678;
-	struct nlm_async_crypto *async = NULL;
+	struct nlm_async_crypto *async =  (struct nlm_async_crypto *)NLM_ASYNC_PTR_PARAM_OFFSET(aead_request_ctx(req));
 	uint8_t *hash_addr;
 	int fb_vc; 
 	int err=0;
 	unsigned int authsize,maxauthsize;
-	uint8_t *new_iv_ptr_lo = NULL;
-	uint8_t *new_iv_ptr_hi = NULL;
 	struct nlm_crypto_pkt_ctrl *ctrl = NULL;
 	unsigned long msgrng_flags;
-	ctx = (struct  nlm_aead_ctx *)(( unsigned long )(( uint8_t *)ctx + CTRL_DESC_SIZE + 63) & ~(0x3fUL));
+	unsigned int max_frags  = MAX_FRAGS;
+	ctx = (struct  nlm_aead_ctx *)(( unsigned long )(( uint8_t *)ctx + CTRL_DESC_SIZE + 64) & ~(0x3fUL));
 	ctrl = &ctx->ctrl;
 	
-	authsize = crypto_aead_crt(crt->base)->authsize;
+	authsize = crypto_aead_authsize(tfm);
 	maxauthsize= aead->maxauthsize;
+	async->pkt_param = NULL;
+	async->src = req->src;
+	async->dst = req->dst;
 
 	param = (struct nlm_crypto_pkt_param *)NLM_CRYPTO_PKT_PARAM_OFFSET(aead_request_ctx(req));
 	hash_addr = (uint8_t *)NLM_HASH_OFFSET(aead_request_ctx(req));
@@ -1286,51 +1226,23 @@ static int aead_crypt_3des(struct aead_request *req, unsigned int op)
 	cipher_len = op ? req->cryptlen:req->cryptlen - authsize;
 	auth_len = cipher_off + cipher_len;
 	hash_source = op;
+	do {
 
-	nr_aad_frags = fill_aead_aad(param, req, req->assoclen,seg);
-	if (nr_aad_frags == 0)
-		return -1;
-	seg = nr_aad_frags;
+	seg = fill_aead_aad(param, req, req->assoclen,seg,max_frags);
 
-	if (ivsize) {
-		nlm_crypto_fill_src_seg(param, seg, MAX_FRAGS, req->iv, ivsize);
-		nlm_crypto_fill_dst_seg(param, seg, MAX_FRAGS, req->iv, ivsize);
-		seg++;
-	}
+	if (ivsize) 
+		seg = nlm_crypto_fill_src_dst_seg(param, seg, max_frags, req->iv, ivsize);
 
-	nr_enc_frags = fill_aead_crypt(req, cipher_len, param, &actual_tag, op, seg);
-
-	if (nr_enc_frags == -1)
-		return -1;
-
-	if(ctx->cbc){
-		uint32_t tmp_len;
-		uint8_t *tmp_virt;
-
-		tmp_len = (param->segment[seg][1] >> 48) + 1;
-		if(tmp_len >= 16){
-			tmp_virt = phys_to_virt((param->segment[seg][1]) & 0xffffffffffULL);
-			new_iv_ptr_hi = tmp_virt;
-			new_iv_ptr_lo = new_iv_ptr_hi + 8;;
-		}else if(tmp_len >=8){
-			tmp_virt = phys_to_virt((param->segment[seg][1]) & 0xffffffffffULL);
-			new_iv_ptr_lo = tmp_virt;
-			/*goto next frag*/
-			if(nr_enc_frags > 1){
-				tmp_virt = phys_to_virt((param->segment[seg + 1][1]) & 0xffffffffffULL);
-				tmp_len = (uint32_t)(param->segment[seg + 1][1] >> 48) + 1;
-				if(tmp_len >= 8)
-					new_iv_ptr_hi = tmp_virt + 8;
-			}
-		}
-	}
+	seg = fill_src_dst_sg(req->src,req->dst, cipher_len, param, seg, max_frags,op);
 
-	seg += nr_enc_frags;
-
-	if ( seg > MAX_FRAGS) {
-		printk("fragments exceeded 0xfff. Cannot handle this\n");
-		return 0;
+	if ( seg > max_frags) {
+		max_frags = seg;
+		seg = alloc_pkt_param(async,&param,max_frags);
+		if ( seg == -1 )
+			return -1;
+		seg = 0;
 	}
+	}while(seg == 0);  
 
 	nlm_crypto_fill_cipher_auth_pkt_param(ctrl, param, op, op, iv_off, 
 			ivsize, auth_off, auth_len, 0, cipher_off, cipher_len, hash_addr);
@@ -1352,16 +1264,14 @@ static int aead_crypt_3des(struct aead_request *req, unsigned int op)
 	printk("auth_len: %d, cipher_len: %d\n", auth_len, cipher_len);
 #endif
 
-	async = (struct nlm_async_crypto *)NLM_ASYNC_PTR_PARAM_OFFSET(aead_request_ctx(req));
 	async->callback =  aead_request_callback;
 	async->args = (void *)&req->base; 
 	async->op  = op;
-	async->actual_tag = actual_tag;
 	async->hash_addr = hash_addr;
 	async->authsize = authsize;
 	async->stat = ctx->stat; 
 	async->bytes = req->cryptlen; 
-	tx_id = (uint64_t)async;
+	tx_id = (uint64_t)(unsigned long)async;
 
 	//construct pkt, send to engine and receive reply
 	msgrng_access_enable(msgrng_flags);
@@ -1385,23 +1295,24 @@ static int aead_crypt(struct aead_request *req, unsigned int op)
 	struct nlm_crypto_pkt_param *param;
 	unsigned int cipher_off, iv_off;
 	unsigned int auth_len, cipher_len, auth_off;
-	unsigned char *actual_tag;
-	int seg =0, nr_enc_frags, ivsize;
-	unsigned int hash_source, nr_aad_frags;
+	int seg =0, ivsize;
+	unsigned int hash_source;
 	uint64_t entry0, entry1;
 	uint64_t tx_id=0x12345678;
-	struct nlm_async_crypto *async = NULL;
+	struct nlm_async_crypto *async =  (struct nlm_async_crypto *)NLM_ASYNC_PTR_PARAM_OFFSET(aead_request_ctx(req));
 	uint8_t *hash_addr;
 	int fb_vc; 
 	int err=0;
 	unsigned int authsize,maxauthsize;
-	uint8_t *new_iv_ptr_lo = NULL;
-	uint8_t *new_iv_ptr_hi = NULL;
 	struct nlm_crypto_pkt_ctrl *ctrl = &ctx->ctrl;
 	unsigned long msgrng_flags;
+	unsigned int max_frags  = MAX_FRAGS;
 
-	authsize = crypto_aead_crt(crt->base)->authsize;
+	authsize = crypto_aead_authsize(tfm);
 	maxauthsize= aead->maxauthsize;
+	async->pkt_param = NULL;
+	async->src = req->src;
+	async->dst = req->dst;
 
 	param = (struct nlm_crypto_pkt_param *)NLM_CRYPTO_PKT_PARAM_OFFSET(aead_request_ctx(req));
 	hash_addr = (uint8_t *)NLM_HASH_OFFSET(aead_request_ctx(req));
@@ -1415,50 +1326,23 @@ static int aead_crypt(struct aead_request *req, unsigned int op)
 	auth_len = cipher_off + cipher_len;
 	hash_source = op;
 
-	nr_aad_frags = fill_aead_aad(param, req, req->assoclen,seg);
-	if (nr_aad_frags == 0)
-		return -1;
-	seg = nr_aad_frags;
+	do { 
 
-	if (ivsize) {
-		nlm_crypto_fill_src_seg(param, seg, MAX_FRAGS, req->iv, ivsize);
-		nlm_crypto_fill_dst_seg(param, seg, MAX_FRAGS, req->iv, ivsize);
-		seg++;
-	}
+	seg = fill_aead_aad(param, req, req->assoclen,seg,max_frags);
 
-	nr_enc_frags = fill_aead_crypt(req, cipher_len, param, &actual_tag, op, seg);
-
-	if (nr_enc_frags == -1)
-		return -1;
-
-	if(ctx->cbc){
-		uint32_t tmp_len;
-		uint8_t *tmp_virt;
-
-		tmp_len = (param->segment[seg][1] >> 48) + 1;
-		if(tmp_len >= 16){
-			tmp_virt = phys_to_virt((param->segment[seg][1]) & 0xffffffffffULL);
-			new_iv_ptr_hi = tmp_virt;
-			new_iv_ptr_lo = new_iv_ptr_hi + 8;;
-		}else if(tmp_len >=8){
-			tmp_virt = phys_to_virt((param->segment[seg][1]) & 0xffffffffffULL);
-			new_iv_ptr_lo = tmp_virt;
-			/*goto next frag*/
-			if(nr_enc_frags > 1){
-				tmp_virt = phys_to_virt((param->segment[seg + 1][1]) & 0xffffffffffULL);
-				tmp_len = (uint32_t)(param->segment[seg + 1][1] >> 48) + 1;
-				if(tmp_len >= 8)
-					new_iv_ptr_hi = tmp_virt + 8;
-			}
-		}
-	}
+	if (ivsize) 
+		seg = nlm_crypto_fill_src_dst_seg(param, seg, max_frags, req->iv, ivsize);
 
-	seg += nr_enc_frags;
+	seg = fill_src_dst_sg(req->src,req->dst, cipher_len, param, seg, max_frags,op);
 
-	if ( seg > MAX_FRAGS) {
-		printk("fragments exceeded 0xfff. Cannot handle this\n");
-		return 0;
+	if ( seg > max_frags) {
+		max_frags = seg;
+		seg = alloc_pkt_param(async,&param,max_frags);
+		if ( seg == -1 )
+			return -1;
+		seg = 0;
 	}
+	}while(seg == 0);  
 
 	nlm_crypto_fill_cipher_auth_pkt_param(ctrl, param, op, op, iv_off, 
 			ivsize, auth_off, auth_len, 0, cipher_off, cipher_len, hash_addr);
@@ -1480,16 +1364,14 @@ static int aead_crypt(struct aead_request *req, unsigned int op)
 	printk("auth_len: %d, cipher_len: %d\n", auth_len, cipher_len);
 #endif
 
-	async = (struct nlm_async_crypto *)NLM_ASYNC_PTR_PARAM_OFFSET(aead_request_ctx(req));
 	async->callback =  aead_request_callback;
 	async->args = (void *)&req->base; 
 	async->op  = op;
-	async->actual_tag = actual_tag;
 	async->hash_addr = hash_addr;
 	async->authsize = authsize;
 	async->stat = ctx->stat; 
 	async->bytes = req->cryptlen; 
-	tx_id = (uint64_t)async;
+	tx_id = (uint64_t)(unsigned long)async;
 
 
 	//construct pkt, send to engine and receive reply
@@ -1887,6 +1769,7 @@ static struct crypto_alg xlp_des_cbc_hmac_md5_cipher_auth = {
         .cra_name = "authenc(hmac(md5),cbc(des))",
         .cra_driver_name = "authenc-hmac-md5-cbc-des-xlp",
         .cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_alignmask = 0,
         .cra_blocksize = DES_BLOCK_SIZE,
         .cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
         .cra_ctxsize = CTRL_DESC_SIZE,
@@ -1911,6 +1794,7 @@ static struct crypto_alg xlp_des_cbc_hmac_sha256_cipher_auth = {
         .cra_driver_name = "authenc-hmac-sha256-cbc-des-xlp",
         .cra_priority = XLP_CRYPT_PRIORITY,
         .cra_blocksize = DES_BLOCK_SIZE,
+	.cra_alignmask = 0,
         .cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
         .cra_ctxsize = CTRL_DESC_SIZE,
         .cra_type = &crypto_aead_type,
@@ -1933,6 +1817,7 @@ static struct crypto_alg xlp_des_cbc_hmac_sha1_cipher_auth = {
         .cra_driver_name = "authenc-hmac-sha1-cbc-des-xlp",
         .cra_priority = XLP_CRYPT_PRIORITY,
         .cra_blocksize = DES_BLOCK_SIZE,
+	.cra_alignmask = 0,
         .cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
         .cra_ctxsize = CTRL_DESC_SIZE,
         .cra_type = &crypto_aead_type,
@@ -1956,6 +1841,7 @@ static struct crypto_alg xlp_aes_gcm_cipher_auth = {
 	.cra_driver_name = "rfc4106-gcm-aes-xlp",
 	.cra_priority = XLP_CRYPT_PRIORITY,
 	.cra_blocksize = AES_BLOCK_SIZE,
+	.cra_alignmask = 0,
 	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
 	.cra_type = &crypto_aead_type,
 	.cra_exit = aead_session_cleanup,
@@ -1967,7 +1853,7 @@ static struct crypto_alg xlp_aes_gcm_cipher_auth = {
 		.encrypt = xlp_aes_gcm_encrypt,
 		.decrypt = xlp_aes_gcm_decrypt,
 		.givencrypt = xlp_aes_gcm_givencrypt,
-		.geniv = "<built-in>",
+		.geniv = "seqiv", 
 		.ivsize = GCM_RFC4106_IV_SIZE,
 		.maxauthsize = GCM_RFC4106_DIGEST_SIZE,
 	}
@@ -1979,6 +1865,7 @@ static struct crypto_alg xlp_aes_ccm_cipher_auth = {
 	.cra_driver_name = "rfc4309-ccm-aes-xlp",
 	.cra_priority = XLP_CRYPT_PRIORITY,
 	.cra_blocksize = AES_BLOCK_SIZE,
+	.cra_alignmask = 0,
 	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
 	.cra_type = &crypto_aead_type,
         .cra_ctxsize = CTRL_DESC_SIZE,
@@ -1989,7 +1876,7 @@ static struct crypto_alg xlp_aes_ccm_cipher_auth = {
 		.encrypt = xlp_aes_ccm_encrypt,
 		.decrypt = xlp_aes_ccm_decrypt,
 		.givencrypt = xlp_aes_ccm_givencrypt,
-		.geniv = "<built-in>",
+		.geniv = "seqiv", 
 		.ivsize = CCM_RFC4309_IV_SIZE,
 		.maxauthsize = CCM_RFC4309_DIGEST_SIZE,
 	}
diff --git a/drivers/crypto/sae/nlm_async.h b/drivers/crypto/sae/nlm_async.h
index 139a2db..3c022ff 100644
--- a/drivers/crypto/sae/nlm_async.h
+++ b/drivers/crypto/sae/nlm_async.h
@@ -24,6 +24,10 @@ THE POSSIBILITY OF SUCH DAMAGE.
 *****************************#NETL_2#********************************/
 #ifndef __NLM_ASYNC_H
 #define __NLM_ASYNC_H
+
+#include <crypto/scatterwalk.h>
+#include "nlmcrypto.h"
+
 struct nlm_async_crypto;
 #define MAX_CPU 32
 extern int crypto_get_fb_vc(void);
@@ -34,8 +38,10 @@ struct nlm_async_crypto
 	void *args;
 	int op;
 	int authsize;
-	uint8_t *actual_tag;
 	uint8_t *hash_addr;
+	uint8_t * pkt_param;
+	struct scatterlist * src;
+	struct scatterlist * dst;
 	uint16_t stat;
 	uint32_t bytes;
 };
@@ -73,5 +79,11 @@ struct nlm_crypto_stat
 
 extern int crypto_vc_base;
 extern int crypto_vc_limit;
+extern int nlm_crypto_sae_num_seg_reqd(void *data, unsigned int buflen);
+extern int nlm_crypto_calc_rem_len(struct scatterlist *sg, unsigned int cipher_len);
+extern int alloc_pkt_param( struct nlm_async_crypto * async,struct nlm_crypto_pkt_param ** pkt_param , int max_frags);
 
+extern int fill_src_dst_sg(struct scatterlist *src_sg, struct scatterlist *dst_sg, unsigned int cipher_len, 
+		struct nlm_crypto_pkt_param *param, 
+		int seg, unsigned int max_frags, int op);
 #endif
diff --git a/drivers/crypto/sae/nlm_auth.c b/drivers/crypto/sae/nlm_auth.c
index 474dbb5..bf21fea 100644
--- a/drivers/crypto/sae/nlm_auth.c
+++ b/drivers/crypto/sae/nlm_auth.c
@@ -26,7 +26,6 @@ THE POSSIBILITY OF SUCH DAMAGE.
 #include <nlm_hal_fmn.h>
 #include <crypto/aes.h>
 #include <crypto/internal/hash.h>
-#include "nlmcrypto.h"
 #include "nlm_async.h"
 #include <asm/netlogic/msgring.h>
 
@@ -83,13 +82,16 @@ struct auth_pkt_desc
 	uint16_t curr_index;
 	uint16_t total_len;
 	uint16_t stat;
+	uint16_t is_allocated;
+	int max_frags;
 	struct shash_desc * fallback;
+	uint8_t  * alloc_pkt_param;
 	struct nlm_crypto_pkt_param * pkt_param; /* maintain at the end */ 
 };
 
-#define PACKET_DESC_SIZE   (64+sizeof(struct auth_pkt_desc) + MAX_FRAGS*(2*64) ) /* should be less than PAGE_SIZE/8 */ 
+#define PACKET_DESC_SIZE   (64+sizeof(struct auth_pkt_desc) + MAX_FRAGS*(2*8) ) /* should be less than PAGE_SIZE/8 */ 
 #define NLM_CRYPTO_PKT_PARAM_OFFSET(addr)       (((unsigned long)addr + 64) & ~0x3fUL)
-#define NLM_ASYNC_PTR_PARAM_OFFSET(addr)        (((unsigned long)(addr + 64 + sizeof(struct nlm_crypto_pkt_param) + MAX_FRAGS*(2*64)) + 64) & ~0x3fUL)
+#define NLM_ASYNC_PTR_PARAM_OFFSET(addr)        (((unsigned long)(addr + 64 + sizeof(struct nlm_crypto_pkt_param) + MAX_FRAGS*(2*8)) + 64) & ~0x3fUL)
 /*
    All extern declaration goes here.
  */
@@ -187,7 +189,9 @@ xlp_auth_init(struct shash_desc *desc)
 	auth_pkt_desc->curr_index = 0;
 	auth_pkt_desc->total_len = 0;
 	auth_pkt_desc->fallback = NULL;
-	auth_pkt_desc->pkt_param = NLM_CRYPTO_PKT_PARAM_OFFSET((unsigned long )auth_pkt_desc + sizeof(struct auth_pkt_desc ));
+	auth_pkt_desc->pkt_param = ( struct nlm_crypto_pkt_param  * )NLM_CRYPTO_PKT_PARAM_OFFSET((unsigned long )auth_pkt_desc + sizeof(struct auth_pkt_desc ));
+	auth_pkt_desc->max_frags = MAX_FRAGS;
+	auth_pkt_desc->is_allocated = 0;
 	return 0;
 }
 static int
@@ -198,10 +202,10 @@ xlp_auth_update(struct shash_desc *desc,
 	struct nlm_auth_ctx * nlm_ctx = nlm_shash_auth_ctx(desc->tfm);
 	int index = auth_pkt_desc->curr_index;
 	unsigned char * data_index = &nlm_ctx->data[auth_pkt_desc->total_len];
-	struct nlm_crypto_pkt_param  *pkt_param  = &(auth_pkt_desc->pkt_param);
+	struct nlm_crypto_pkt_param  *pkt_param  = auth_pkt_desc->pkt_param;
 
 	auth_pkt_desc->total_len += length;
-	if ( (auth_pkt_desc->total_len >= MAX_AUTH_DATA) || (index >= MAX_FRAGS)) {
+	if  (auth_pkt_desc->total_len >= MAX_AUTH_DATA)  {
 		if (auth_pkt_desc->fallback == NULL ) {
 			auth_pkt_desc->fallback = kmalloc((sizeof(struct shash_desc ) + 
 				crypto_shash_descsize(nlm_ctx->fallback_tfm))
@@ -219,8 +223,21 @@ xlp_auth_update(struct shash_desc *desc,
 	}
 
 	memcpy(data_index,data,length);
-	nlm_crypto_fill_src_seg(pkt_param, index, MAX_FRAGS, data_index, length);
-	auth_pkt_desc->curr_index = nlm_crypto_fill_dst_seg(pkt_param, index , MAX_FRAGS, data_index, length);
+	if ( (auth_pkt_desc->curr_index + 1 ) > auth_pkt_desc->max_frags ) {
+		uint8_t * mem = kmalloc(sizeof (struct nlm_crypto_pkt_param) 
+					+ ((auth_pkt_desc->max_frags + MAX_FRAGS) * 2 * 8 ) + 64 , GFP_KERNEL);
+		auth_pkt_desc->alloc_pkt_param = mem;
+		mem = (uint8_t *)NLM_CRYPTO_PKT_PARAM_OFFSET((unsigned long )mem);
+		memcpy(mem,pkt_param,sizeof (struct nlm_crypto_pkt_param)+(auth_pkt_desc->max_frags * 2 * 8));
+		
+		if ( auth_pkt_desc->is_allocated )
+			kfree(auth_pkt_desc->alloc_pkt_param);
+		pkt_param = auth_pkt_desc->pkt_param = ( struct nlm_crypto_pkt_param  * )mem;
+		auth_pkt_desc->is_allocated = 0;
+		auth_pkt_desc->max_frags += MAX_FRAGS;
+	} 
+		
+	auth_pkt_desc->curr_index = nlm_crypto_fill_src_dst_seg(pkt_param, index , auth_pkt_desc->max_frags, data_index, length);
 
 	return 0;
 }
@@ -241,7 +258,7 @@ xlp_auth_final(struct shash_desc *desc, uint8_t *out)
 {
 	struct auth_pkt_desc *auth_pkt_desc = (struct auth_pkt_desc *)shash_desc_ctx(desc);
 	int index = auth_pkt_desc->curr_index;
-	struct nlm_crypto_pkt_param  *pkt_param  = &(auth_pkt_desc->pkt_param);
+	struct nlm_crypto_pkt_param  *pkt_param  = auth_pkt_desc->pkt_param;
 	struct nlm_auth_ctx  * auth_ctx   = pkt_ctrl_auth_ctx(desc);
 	int fb_vc ;
 	uint64_t entry0, entry1, tx_id=0x12345678ULL;
@@ -253,8 +270,8 @@ xlp_auth_final(struct shash_desc *desc, uint8_t *out)
         extern int ipsec_sync_vc;
 	unsigned long msgrng_flags;
 
-	if ( (auth_pkt_desc->total_len == 0 ) ||  ( auth_pkt_desc->total_len > MAX_AUTH_DATA) 
-						|| (index >=  MAX_FRAGS)){
+	if ( (auth_pkt_desc->total_len == 0 ) ||  ( auth_pkt_desc->total_len > MAX_AUTH_DATA)) { 
+
 		if (auth_pkt_desc->fallback == NULL ) {
 			auth_pkt_desc->fallback = kmalloc((sizeof(struct shash_desc ) + 
 				crypto_shash_descsize(auth_ctx->fallback_tfm))
@@ -263,7 +280,6 @@ xlp_auth_final(struct shash_desc *desc, uint8_t *out)
 			auth_pkt_desc->fallback->flags = desc->flags & CRYPTO_TFM_REQ_MAY_SLEEP;
 			crypto_shash_init(auth_pkt_desc->fallback);
 		}
-		
 		crypto_shash_final(auth_pkt_desc->fallback,out);
 		return 0;
 	}
@@ -301,6 +317,8 @@ xlp_auth_final(struct shash_desc *desc, uint8_t *out)
 	if (timeout >= 0xffffffffULL) {
 		printk("\nError: FreeBack message is not received");
 		msgrng_access_disable(msgrng_flags);
+		if ( auth_pkt_desc->is_allocated )
+			kfree(auth_pkt_desc->pkt_param);
 		preempt_enable();
 		return -EIO;
 	}
@@ -309,6 +327,8 @@ xlp_auth_final(struct shash_desc *desc, uint8_t *out)
 #endif
 	msgrng_access_disable(msgrng_flags);
 	preempt_enable();
+	if ( auth_pkt_desc->is_allocated )
+		kfree(auth_pkt_desc->pkt_param);
 	crypto_stat[cpu].auth[stat] ++;
 	crypto_stat[cpu].auth_tbytes[stat] += auth_pkt_desc->total_len + ctrl->taglen;
  	return 0;
@@ -355,8 +375,7 @@ int hash_key(int alg, int mode, const uint8_t * key, unsigned int keylen, uint8_
 	memcpy(tmp_key,key,keylen);
         nlm_crypto_fill_pkt_ctrl(ctrl,0,alg,mode,0,0,0,NULL,0,NULL,0);
         nlm_crypto_fill_auth_pkt_param(ctrl,pkt_param,0,keylen,0,new_key);
-        nlm_crypto_fill_src_seg(pkt_param,0,MAX_FRAGS,tmp_key,keylen);
-        nlm_crypto_fill_dst_seg(pkt_param,0,MAX_FRAGS,tmp_key,keylen);
+        nlm_crypto_fill_src_dst_seg(pkt_param,0,MAX_FRAGS,tmp_key,keylen);
 
 
 	msgrng_access_enable(msgrng_flags);
@@ -372,7 +391,7 @@ int hash_key(int alg, int mode, const uint8_t * key, unsigned int keylen, uint8_
 #endif
 
         //construct pkt, send to engine and receive reply
-        xlp_message_send_block_fast_3(0, crypto_vc_base, entry0, entry1, tx_id);
+	nlm_hal_send_msg3(crypto_vc_base, 0 /*code */ , entry0, entry1, tx_id);
         timeout = 0;
         do {
                 timeout++;
diff --git a/drivers/crypto/sae/nlm_crypto.c b/drivers/crypto/sae/nlm_crypto.c
index bad3e07..144bfc8 100644
--- a/drivers/crypto/sae/nlm_crypto.c
+++ b/drivers/crypto/sae/nlm_crypto.c
@@ -31,7 +31,6 @@ THE POSSIBILITY OF SUCH DAMAGE.
 #include <asm/netlogic/hal/nlm_hal_macros.h>
 #include <linux/crypto.h>
 #include "nlm_async.h"
-#include "nlmcrypto.h"
 
 
 #ifdef TRACING
@@ -42,7 +41,8 @@ THE POSSIBILITY OF SUCH DAMAGE.
 #define TRACE_RET ((void) 0)
 #endif				/* TRACING */
 #undef NLM_CRYPTO_DEBUG
-#define NLM_CRYPTO_DEBUG
+#define NETL_OP_ENCRYPT 1
+#define NETL_OP_DECRYPT 0
 
 
 /**
@@ -321,6 +321,10 @@ struct designer_desc{
 
 void print_pkt_desc(struct nlm_crypto_pkt_param  *pkt_desc, int index)
 {
+	int i;
+	unsigned long  phys;
+	void * virt;
+
 	printf("Packet desc address = %p\n",pkt_desc);
 	printf("Packet Descriptor 0: 0x%016llx\n", (unsigned long long)pkt_desc->desc0);
 	printf("Packet Descriptor 1: 0x%016llx\n", (unsigned long long)pkt_desc->desc1);
@@ -355,18 +359,17 @@ void print_pkt_desc(struct nlm_crypto_pkt_param  *pkt_desc, int index)
 	printf("arc4 sbox l3 alloc = 0x%llx  \n", xtract_bits(pkt_desc->desc3, 8, 1));
 	printf("arc4 save box = 0x%llx  \n", xtract_bits(pkt_desc->desc3, 6, 1));
 	printf("hmac ext pad key = 0x%llx  \n", xtract_bits(pkt_desc->desc3, 5, 1));
-
-        int i;
-	unsigned long  phys;
-	void * virt;
-	
+	int len;
 
         for (i=0; i < index; i++) {
-                printf("Packet Descriptor frag src %d: 0x%016llx\n", i, (unsigned long long)pkt_desc->segment[i][0]);
-                printf("Packet Descriptor frag dst %d: 0x%016llx\n", i, (unsigned long long)pkt_desc->segment[i][1]);
+		printf("Packet Descriptor frag src %d: 0x%016llx\n", i, (unsigned long long)pkt_desc->segment[i][0]);
+		printf("Packet Descriptor frag dst %d: 0x%016llx\n", i, (unsigned long long)pkt_desc->segment[i][1]);
+
 		phys = xtract_bits(pkt_desc->segment[i][0], 0,40);
 		virt = phys_to_virt(phys);
-		hex_dump("src \n",virt, 30);
+		len = xtract_bits(pkt_desc->segment[i][0], 48, 16); 
+		len = (len > 32 ) ? 32 : len;
+		hex_dump("src",virt,len);
 		printk("virtual is %p and phys is %lx\n",virt,phys);
 
 
@@ -380,6 +383,118 @@ void print_pkt_desc(struct nlm_crypto_pkt_param  *pkt_desc, int index)
         }
 }
 #endif
+
+int alloc_pkt_param( struct nlm_async_crypto * async,struct nlm_crypto_pkt_param ** pkt_param , int max_frags)
+{
+	uint8_t * new_pkt_param = NULL;
+	async->pkt_param = kmalloc((sizeof (struct nlm_crypto_pkt_param) +( max_frags * 2 * 8 )+ 64),GFP_KERNEL);
+
+	if ( !async->pkt_param) {
+		return -1;
+	}
+
+	new_pkt_param = (uint8_t * )(((unsigned long)async->pkt_param + 64) & ~0x3fUL);
+
+	memcpy(new_pkt_param,*pkt_param,sizeof(struct nlm_crypto_pkt_param));
+	*pkt_param = (struct nlm_crypto_pkt_param *)new_pkt_param;
+
+	
+	return 1;
+}
+
+int nlm_crypto_sae_num_seg_reqd(void *data, unsigned int buflen)
+{
+	return nlm_crypto_num_segs_reqd(buflen);
+}
+
+int nlm_crypto_calc_rem_len(struct scatterlist *sg, unsigned int cipher_len)
+{
+	int len,seg = 0;
+	for (;cipher_len > 0;sg = scatterwalk_sg_next(sg)){
+		len = min(cipher_len, sg->length);
+		seg += nlm_crypto_sae_num_seg_reqd(NULL,len);
+		cipher_len -= len;
+	}
+	return seg;
+}
+
+int fill_src_dst_sg(struct scatterlist *src_sg, struct scatterlist *dst_sg, unsigned int cipher_len, 
+		struct nlm_crypto_pkt_param *param, 
+		int seg, unsigned int max_frags, int op)
+{
+	struct scatterlist *sg;
+	struct scatter_walk walk;
+	unsigned int len = 0;
+	uint8_t *virt = NULL;
+	int rv = 0;
+	int i;
+
+	if (src_sg == dst_sg ) {
+		for( sg = src_sg; sg != NULL ; sg = scatterwalk_sg_next(sg)) {
+			len = min(cipher_len, sg->length);
+			scatterwalk_start(&walk, sg);
+			virt = page_address(scatterwalk_page(&walk)) + offset_in_page(walk.offset);
+			if ( cipher_len > 0 ) {
+				rv = nlm_crypto_fill_src_dst_seg(param, seg, max_frags, virt, len);
+				if ( rv < seg ) {
+					return nlm_crypto_calc_rem_len(sg,cipher_len) + max_frags; 
+				}
+				else
+					seg = rv;
+			}
+			cipher_len -= len;
+		}
+		return seg;
+	}
+	else {
+		int nr_src_frags = 0;
+		int nr_dst_frags = 0;
+		int index = seg;
+		int nbytes = cipher_len;
+		for (sg = src_sg,seg;cipher_len > 0; sg = scatterwalk_sg_next(sg)) {
+			len = min(cipher_len, sg->length);
+			scatterwalk_start(&walk, sg);
+			virt = page_address(scatterwalk_page(&walk)) + offset_in_page(walk.offset);
+			rv = nlm_crypto_fill_src_seg(param,seg,max_frags,virt,len);
+			if ( rv <= seg )
+				return nlm_crypto_calc_rem_len(sg,cipher_len) + max_frags;
+			else
+				seg = rv;
+			cipher_len -= len;
+		}
+		nr_src_frags = seg;
+		cipher_len = nbytes;
+		for (sg = dst_sg,seg = index ;cipher_len > 0; sg = scatterwalk_sg_next(sg)) {
+			len = min(cipher_len, sg->length);
+			scatterwalk_start(&walk, sg);
+			virt = page_address(scatterwalk_page(&walk)) + offset_in_page(walk.offset);
+			rv = nlm_crypto_fill_dst_seg(param,seg,max_frags,virt,len);
+			if ( rv <= seg )
+				return nlm_crypto_calc_rem_len(sg,cipher_len) + max_frags;
+			else
+				seg = rv;
+			cipher_len -= len;
+		}
+
+		nr_dst_frags = seg;
+
+		if ((nr_src_frags > nr_dst_frags) && (nr_src_frags < max_frags)) {
+			for (i = 0; i < nr_src_frags - nr_dst_frags; i++)
+				param->segment[index + nr_dst_frags + i][1] = 0ULL;
+			seg = nr_src_frags;
+		}
+		else  { 
+			if ((nr_src_frags < nr_dst_frags) && (nr_dst_frags < max_frags )){
+				for (i = 0; i < nr_dst_frags - nr_src_frags; i++)
+					param->segment[index + nr_src_frags + i][0] = 0ULL;
+			}
+			seg = nr_dst_frags;
+		}
+		return seg;
+	}
+
+}
+
     static void
 reset_crypto_stats(void)
 {
@@ -450,7 +565,6 @@ nlm_crypto_read_stats_proc(char *page, char **start, off_t off, int count,
 {
         int len = 0;
 	int i,j;
-	uint64_t  cnt;
 	off_t begin = 0;
 	uint64_t enc_tp[ENC_MAX_STAT];
 	uint64_t auth_tp[AUTH_MAX_STAT];
diff --git a/drivers/crypto/sae/nlm_enc.c b/drivers/crypto/sae/nlm_enc.c
index 1f17c73..cef9945 100755
--- a/drivers/crypto/sae/nlm_enc.c
+++ b/drivers/crypto/sae/nlm_enc.c
@@ -22,12 +22,10 @@ CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
 THE POSSIBILITY OF SUCH DAMAGE.
 *****************************#NETL_2#********************************/
-#include <crypto/scatterwalk.h>
 #include <linux/crypto.h>
 #include <crypto/aes.h>
 #include <crypto/des.h>
 #include <crypto/ctr.h>
-#include "nlmcrypto.h"
 #include <nlm_hal_fmn.h>
 #include <asm/netlogic/hal/nlm_hal.h>
 #include <asm/netlogic/msgring.h>
@@ -45,16 +43,16 @@ struct nlm_enc_ctx {
 	char nonce[4];
 };
 /* mem utilisation of CTX_SIZE */
-#define MAX_FRAGS               0xfff 
+#define MAX_FRAGS               18 
 #define CTRL_DESC_SIZE          (sizeof(struct nlm_enc_ctx) + 64)
 #define DES3_CTRL_DESC_SIZE     (2*CTRL_DESC_SIZE + 2*64)
 
 
 /* mem utilisation of req mem */
 
-#define PACKET_DESC_SIZE        (64 + sizeof(struct nlm_crypto_pkt_param) + MAX_FRAGS*(2*64) + 64 + sizeof(struct nlm_async_crypto) + 64)
+#define PACKET_DESC_SIZE        (64 + sizeof(struct nlm_crypto_pkt_param) + (MAX_FRAGS*16) + 64 + sizeof(struct nlm_async_crypto) + 64)
 #define NLM_CRYPTO_PKT_PARAM_OFFSET(addr)       (((unsigned long)addr + 64) & ~0x3fUL)
-#define NLM_ASYNC_PTR_PARAM_OFFSET(addr)        (((unsigned long)(addr + 64 + sizeof(struct nlm_crypto_pkt_param) + MAX_FRAGS*(2*64)) + 64) & ~0x3fUL)
+#define NLM_ASYNC_PTR_PARAM_OFFSET(addr)        (((unsigned long)addr + 64 + sizeof(struct nlm_crypto_pkt_param) + (MAX_FRAGS*16) + 64) & ~0x3fUL)
 #define NLM_IV_OFFSET(addr)			((unsigned long)addr + (PACKET_DESC_SIZE - 64))
 
 
@@ -64,6 +62,7 @@ static int no_of_alg_registered = 0;
 
 extern uint32_t nlm_hal_send_msg3(uint32_t dst, uint32_t code, uint64_t data0, uint64_t data1, uint64_t data2);
 extern __inline__ uint32_t nlm_hal_recv_msg2(uint32_t dst, uint32_t *src, uint32_t *size, uint32_t *code, uint64_t *data0, uint64_t *data1);
+
 #ifdef NLM_CRYPTO_DEBUG
 extern void print_cntl_instr(uint64_t cntl_desc);
 extern void print_crypto_msg_desc(uint64_t entry1, uint64_t entry2, uint64_t entry3);
@@ -238,6 +237,8 @@ void enc_request_callback(struct nlm_async_crypto *async, uint64_t msg1 )
 	}
 	crypto_stat[cpu].enc[stat]++;
 	crypto_stat[cpu].enc_tbytes[stat]+= async->bytes;
+	if ( async->pkt_param)
+		kfree(async->pkt_param);
 	
 	base->complete(base, err);
 }
@@ -246,77 +247,30 @@ static int
 xlp_crypt(struct ablkcipher_request *req, unsigned int enc, int iv_size, struct nlm_crypto_pkt_ctrl *ctrl, uint16_t stat)
 {
 	int seg = 0;
-	int i;
 	uint64_t msg0, msg1;
-	struct scatterlist *sg;
-	struct scatter_walk walk;
-	uint8_t *virt;
-	int len;
 	int pktdescsize = 0;
 	int fb_vc;
 	unsigned long msgrng_flags;
-	
+	unsigned int max_frags= MAX_FRAGS;
 	unsigned int cipher_len = req->nbytes;
 	struct nlm_crypto_pkt_param * pkt_param = (struct nlm_crypto_pkt_param *) NLM_CRYPTO_PKT_PARAM_OFFSET(ablkcipher_request_ctx(req));
 	struct nlm_async_crypto * async =  (struct nlm_async_crypto *)NLM_ASYNC_PTR_PARAM_OFFSET(ablkcipher_request_ctx(req));;
+	async->pkt_param = NULL;
 
 	nlm_crypto_fill_cipher_pkt_param(ctrl, pkt_param, enc,0,iv_size,iv_size ,req->nbytes); 
 
-	nlm_crypto_fill_src_seg(pkt_param,seg,MAX_FRAGS,(unsigned char *)req->info,iv_size);
-	nlm_crypto_fill_dst_seg(pkt_param,seg,MAX_FRAGS,(unsigned char *)req->info,iv_size);
+	nlm_crypto_fill_src_dst_seg(pkt_param,seg,MAX_FRAGS,(unsigned char *)req->info,iv_size);
 	seg++;
 
-	if ( req->src == req->dst) {
-		for (sg = req->src; cipher_len > 0; sg = scatterwalk_sg_next(sg)) {
-			len = min(cipher_len, sg->length);
-			scatterwalk_start(&walk, sg);
-			virt = page_address(scatterwalk_page(&walk)) + offset_in_page(walk.offset);
-			nlm_crypto_fill_src_seg(pkt_param,seg,MAX_FRAGS,virt,len);
-			seg = nlm_crypto_fill_dst_seg(pkt_param,seg,MAX_FRAGS,virt,len);
-			if(seg == MAX_FRAGS) {
-				printk("fragments exceeded 0xfff. Cannot handle the packet\n");
-				return 0;
-			}
-				
-			cipher_len -= len;
-		}
-	}
-	else {
-		int nr_src_frags = 0;
-		int nr_dst_frags = 0;
-		int index = 0;
-		for (sg = req->src,index = seg;cipher_len > 0; sg = scatterwalk_sg_next(sg)) {
-			len = min(cipher_len, sg->length);
-			scatterwalk_start(&walk, sg);
-			virt = page_address(scatterwalk_page(&walk)) + offset_in_page(walk.offset);
-			index = nlm_crypto_fill_src_seg(pkt_param,index,MAX_FRAGS,virt,len);
-			cipher_len -= len;
-		}
-		nr_src_frags = index;
+	do {
 		cipher_len = req->nbytes;
-		for (sg = req->dst, index = seg ;cipher_len > 0; sg = scatterwalk_sg_next(sg)) {
-			len = min(cipher_len, sg->length);
-			scatterwalk_start(&walk, sg);
-			virt = page_address(scatterwalk_page(&walk)) + offset_in_page(walk.offset);
-			index = nlm_crypto_fill_dst_seg(pkt_param,index,MAX_FRAGS,virt,len);
-			cipher_len -= len;
-		}
-		nr_dst_frags = index;
+		seg = fill_src_dst_sg(req->src,req->dst,cipher_len,pkt_param,seg,max_frags,0);
 
-		if (nr_src_frags > nr_dst_frags) {
-			for (i = 0; i < nr_src_frags - nr_dst_frags; i++)
-				pkt_param->segment[seg + nr_dst_frags + i][1] = 0ULL;
-			seg = nr_src_frags;
+		if ( seg > max_frags ) {
+			max_frags = seg; 
+			seg = alloc_pkt_param(async,&pkt_param,max_frags);
 		}
-		else  { 
-			if (nr_src_frags < nr_dst_frags) {
-				for (i = 0; i < nr_dst_frags - nr_src_frags; i++)
-					pkt_param->segment[seg + nr_src_frags + i][0] = 0ULL;
-			}
-			seg = nr_dst_frags;
-		}
-		
-	}
+	}while(seg == 1 ); 
 
 	pktdescsize = 32 + seg * 16;
 
@@ -324,7 +278,7 @@ xlp_crypt(struct ablkcipher_request *req, unsigned int enc, int iv_size, struct
 	fb_vc = crypto_get_fb_vc(); 
 
 	msg0 = nlm_crypto_form_pkt_fmn_entry0(fb_vc, 0, 0, ctrl->cipherkeylen,
-								virt_to_phys(ctrl));
+			virt_to_phys(ctrl));
 	msg1 = nlm_crypto_form_pkt_fmn_entry1(0,ctrl->hashkeylen, pktdescsize,
 				virt_to_phys(pkt_param));
 #ifdef NLM_CRYPTO_DEBUG
diff --git a/drivers/crypto/sae/nlmcrypto_ifc.h b/drivers/crypto/sae/nlmcrypto_ifc.h
index 885f81b..843c547 100644
--- a/drivers/crypto/sae/nlmcrypto_ifc.h
+++ b/drivers/crypto/sae/nlmcrypto_ifc.h
@@ -29,7 +29,6 @@
   *****************************#NLM_2#**********************************/
 #ifndef _NLM_CRYPTO_IFC_H
 #define _NLM_CRYPTO_IFC_H
-
 extern void *linuxu_shvaddr;
 extern unsigned long long linuxu_shoff ;
 
@@ -49,6 +48,9 @@ static inline int crypto_fill_pkt_seg_paddr_len(void *vaddr, unsigned int inlen,
 {
 	unsigned int remlen = inlen, sg = 0, len;
 	for(; remlen > 0;) {
+		if ( sg >= max_segs ) 
+			return -1;
+			
 		len = remlen > NLM_CRYPTO_MAX_SEG_LEN ? NLM_CRYPTO_MAX_SEG_LEN : remlen;
 		if(fillsrc)
 			segs[sg].src = ccpu_to_be64(virt_to_phys(vaddr) | 
-- 
1.7.1

