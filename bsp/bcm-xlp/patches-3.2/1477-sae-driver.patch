From 968bea95b4a8e8033826197727f2373e5ed3e5de Mon Sep 17 00:00:00 2001
From: Alok Agrawat <aagrawat@netlogicmicro.com>
Date: Tue, 5 Apr 2011 14:27:50 +0530
Subject: sae driver

[Based on SDK 3.2]
Signed-off-by: Quanyang Wang <quanyang.wang@windriver.com>
Signed-off-by: Nam Ninh <nam.ninh@windriver.com>

diff --git a/drivers/crypto/sae/nlm_aead.c b/drivers/crypto/sae/nlm_aead.c
new file mode 100755
index 0000000..ed471c6
--- /dev/null
+++ b/drivers/crypto/sae/nlm_aead.c
@@ -0,0 +1,3091 @@
+/***********************************************************************
+Copyright 2003-2010 Netlogic Microsystems (“Netlogic”). All rights
+reserved.
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are
+met:
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+*****************************#NETL_2#********************************/
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/pci.h>
+#include <linux/pci_ids.h>
+#include <linux/crypto.h>
+#include <linux/spinlock.h>
+#include <crypto/algapi.h>
+#include <crypto/aes.h>
+#include <crypto/des.h>
+#include <linux/rtnetlink.h>
+#include <crypto/ctr.h>
+#include <crypto/sha.h>
+#include <crypto/aead.h>
+#include <crypto/authenc.h>
+#include <crypto/scatterwalk.h>
+#include <linux/hardirq.h>
+
+#include <asm/io.h>
+#include <asm/delay.h>
+#include <asm/netlogic/msgring.h>
+#include "nlm_crypto_api.h"
+#include "nlm_crypto.h"
+#include "nlm_crypto_data.h"
+
+#define AES_CTR_IV_SIZE		8
+#define XLP_CRYPT_PRIORITY	310
+
+#define XCBC_DIGEST_SIZE	16
+#define MD5_DIGEST_SIZE		16
+#define MD5_BLOCK_SIZE		64
+
+//#define SEC_DEBUG 1
+
+#ifdef SEC_DEBUG
+#ifdef __KERNEL__
+#define debug_print(fmt, args...) printk(fmt, ##args)
+#else				/* __KERNEL__ */
+#define debug_print(fmt, args...) printf(fmt, ##args)
+#endif				/* __KERNEL__ */
+#else				/* SEC_DEBUG */
+#define debug_print(fmt, args...)
+#endif				/* SEC_DEBUG */
+
+#define malloc(a) kmalloc(a, GFP_ATOMIC)
+#define free kfree
+
+struct crypto_rfc4106_ctx {
+        struct crypto_aead *child;
+        u8 nonce[4];
+};
+
+struct crypto_rfc3686_ctx {
+	struct crypto_aead *child;
+	u8 nonce[CTR_RFC3686_NONCE_SIZE];
+};
+
+struct crypto_rfc4309_ctx {
+        struct crypto_aead *child;
+        u8 nonce[3];
+};
+
+unsigned char rfc3686_nonce[4], rfc4106_nonce[4], rfc4309_nonce[3];
+	
+static int
+aead_setauthsize(struct crypto_aead *authenc, unsigned int authsize)
+{
+	struct crypto_session *session = crypto_aead_ctx(authenc);
+
+	session->aip = (unsigned char*)session + sizeof(struct crypto_session);
+//printk(KERN_ERR "\n aead_setauthsize authsize= %d", authsize);
+	session->aip->tag_len = authsize;
+
+	return 0;
+}
+
+
+static int
+aead_setkey(struct crypto_aead *tfm, const u8 * key, unsigned int keylen, uint32_t key_copy)
+{
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	unsigned int authkeylen = 0;
+	unsigned int enckeylen = 0;
+	u32 *flags = 0;
+	unsigned char *temp;
+
+	temp = (unsigned char*)session + sizeof(struct crypto_session);
+	session->aip = temp;
+	temp = temp + sizeof (struct crypto_auth_init_param);
+	session->cip = temp;
+	temp = temp + sizeof(struct crypto_cipher_init_param);
+//printk(KERN_ERR "\n Inside aead_setkey");
+
+	enckeylen = keylen;
+
+//printk(KERN_ERR "\n aead_setkey enckeylen = %d", enckeylen);
+	if(key_copy)
+		authkeylen = keylen;
+
+//printk(KERN_ERR "\n aead_setkey authkeylen = %d", authkeylen);
+	if (enckeylen) {
+#if 0
+		session->cip = (struct crypto_cipher_init_param *)
+		    malloc(sizeof (struct crypto_cipher_init_param));
+#endif
+//		session->cip = temp;
+//		temp = temp + sizeof (struct crypto_cipher_init_param);
+		if (session->cip == NULL) {
+			printk(KERN_ERR
+			       "\nError: malloc failed for session->cip. Returning from %s",
+			       __FUNCTION__);
+			return -EINVAL;
+		}
+		memset(session->cip, 0,
+		       sizeof (struct crypto_cipher_init_param));
+
+		switch (enckeylen) {
+			case 16:
+				session->cip->cipher_alg = NLM_CIPHER_AES128;
+				break;
+			case 24:
+				session->cip->cipher_alg = NLM_CIPHER_AES192;
+				break;
+			case 32:
+				session->cip->cipher_alg = NLM_CIPHER_AES256;
+				break;
+			default:
+				printk(KERN_WARNING "[%s]: Cannot handle keylen = %d\n",
+						__FUNCTION__, enckeylen);
+				*flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
+				return -EINVAL;
+		}
+#if 0
+		session->cip->cipher_key = malloc(sizeof (struct crypto_iovec));
+#endif
+		session->cip->cipher_key = temp;
+		temp = temp + sizeof (struct crypto_iovec);
+		if (session->cip->cipher_key == NULL) {
+			printk(KERN_ERR
+			       "\nError: malloc failed for session->cip->cipher_key Returning from %s",
+			       __FUNCTION__);
+			return -EINVAL;
+		}
+#if 0
+		session->cip->cipher_key->buf = malloc(enckeylen + 1);
+#endif
+		session->cip->cipher_key->buf = temp;
+		temp = temp + sizeof (enckeylen + 1);
+		if (session->cip->cipher_key->buf == NULL) {
+			printk(KERN_ERR
+			       "\nError: malloc failed for session->cip->cipher_key->buf. Returning from %s",
+			       __FUNCTION__);
+			return -EINVAL;
+		}
+		memcpy(session->cip->cipher_key->buf, key, enckeylen);
+		session->cip->cipher_key->iov_len = enckeylen;
+
+		session->cip->arc4_cipher_key_len = 0;
+		session->cip->arc4_key_init = 0;
+		session->cip->cfb_mask = 0;
+	}
+
+	if (authkeylen) {
+#if 0
+		session->aip = (struct crypto_auth_init_param *)
+	    	malloc(sizeof (struct crypto_auth_init_param));
+#endif
+//		session->aip = temp;
+//		temp = temp + sizeof (struct crypto_auth_init_param);
+		if (session->aip == NULL) {
+			printk(KERN_ERR
+		       	"\nError: malloc failed for session->aip. Returning from %s",
+		       	__FUNCTION__);
+			return -EINVAL;
+		}
+
+	//	memset(session->aip, 0, sizeof (struct crypto_auth_init_param));
+#if 0
+		session->aip->auth_key = malloc(sizeof (struct crypto_iovec));
+#endif
+		session->aip->auth_key = temp;
+		temp = temp + sizeof (struct crypto_iovec);
+		if (session->aip->auth_key == NULL) {
+			printk(KERN_ERR
+			       "\nError: malloc failed for session->aip->auth_key. Returning from %s",
+			       __FUNCTION__);
+			return -EINVAL;
+		}
+#if 0
+		session->aip->auth_key->buf = malloc(authkeylen + 1);
+#endif
+		session->aip->auth_key->buf = temp;
+		temp = temp + sizeof (authkeylen + 1);
+		if (session->aip->auth_key->buf == NULL) {
+			printk(KERN_ERR
+			       "\nError: malloc failed for session->aip->auth_key->buf. Returning from %s",
+			       __FUNCTION__);
+			return -EINVAL;
+		}
+		memcpy(session->aip->auth_key->buf, key, authkeylen);
+		session->aip->auth_key->iov_len = authkeylen;
+		//session->aip->hmac = 1;
+	}
+
+//printk(KERN_ERR "\n returning aead_setkey");
+	return 0;
+
+      badkey:
+	crypto_aead_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+	return -EINVAL;
+}
+
+void dump_buffer(unsigned char *buf, unsigned int len, unsigned char *msg)
+{
+	int k = 0;
+	unsigned char *temp_buf[len*2+1];
+	printk("\n %s", msg);
+	for(k = 0; k < len; k++)
+        	printk("%.2x",buf[k]);
+
+}
+void dump_session(struct crypto_session *session)
+{
+	printk("\n session = %x",(unsigned long)session);
+	printk("\n session->cntrl_desc = %x session->pkt_dec",(unsigned long)session->cntrl_desc, (unsigned long)session->pkt_desc);
+	printk("\n session->aip = %x session->cip = %x\n",(unsigned long)session->aip, (unsigned long)session->cip);
+	printk("\n session->cip->cipher_mode = %d session->cip->cipher_alg = %d",session->cip->cipher_mode, session->cip->cipher_alg);
+	dump_buffer(session->cip->cipher_key->buf, session->cip->cipher_key->iov_len, "Cipher_key:");
+	printk("\n session->cip->cipher_key->iov_len = %d session->cip->arc4_cipher_key_len = %d session->cip->arc4_key_init= %d session->cip->cfb_mask = %d\n",session->cip->cipher_key->iov_len,session->cip->arc4_cipher_key_len, session->cip->arc4_key_init, session->cip->cfb_mask);
+	printk("\n session->aip->auth_mode = %d session->aip->auth_alg = %d",session->aip->auth_mode, session->aip->auth_alg);
+	dump_buffer(session->aip->auth_key->buf, session->aip->auth_key->iov_len, "Auth_key:");
+	printk("\n session->aip->auth_key->iov_len = %d session->aip->tag_len = %d session->aip->hmac = %d",session->aip->auth_key->iov_len, session->aip->tag_len, session->aip->hmac);
+
+}
+static int
+authenc_setkey(struct crypto_aead *tfm, const u8 * key, unsigned int keylen, uint32_t cipher_alg)
+{
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	struct rtattr *rta = (struct rtattr *) key;
+	struct crypto_authenc_key_param *param;
+	unsigned int authkeylen;
+	unsigned int enckeylen;
+	unsigned char *temp;
+	u32 *flags = 0;
+
+	//memset(session, 0, sizeof (struct crypto_session));
+	temp = (unsigned char*)session + sizeof(struct crypto_session);
+	session->aip = temp;
+	temp = temp + sizeof (struct crypto_auth_init_param);
+	session->cip = temp;
+	temp = temp + sizeof(struct crypto_cipher_init_param);
+
+//printk(KERN_ERR "\n inside authenc_setkey");
+//printk(KERN_ERR "\n session = %x aip= %x cip = %x",(unsigned int)session,(unsigned int) session->aip,(unsigned int) session->cip);
+	if (!RTA_OK(rta, keylen)) {
+		goto badkey;
+	}
+
+	if (rta->rta_type != CRYPTO_AUTHENC_KEYA_PARAM) {
+		goto badkey;
+	}
+	if (RTA_PAYLOAD(rta) < sizeof (*param)) {
+		goto badkey;
+	}
+
+	param = RTA_DATA(rta);
+	enckeylen = be32_to_cpu(param->enckeylen);
+
+//printk(KERN_ERR "\n inside authenc_setkey enckeylen = %d",enckeylen);
+	key += RTA_ALIGN(rta->rta_len);
+	keylen -= RTA_ALIGN(rta->rta_len);
+
+	if (keylen < enckeylen)
+		goto badkey;
+
+	authkeylen = keylen - enckeylen;
+
+	if (enckeylen) {
+#if 0
+		session->cip = (struct crypto_cipher_init_param *)
+		    malloc(sizeof (struct crypto_cipher_init_param));
+#endif
+	//	session->cip = temp;
+	//	temp = temp + sizeof(struct crypto_cipher_init_param);
+
+		if (session->cip == NULL) {
+			printk(KERN_ERR
+			       "\nError: malloc failed for session->cip. Returning from %s",
+			       __FUNCTION__);
+			return -EINVAL;
+		}
+		memset(session->cip, 0,
+		       sizeof (struct crypto_cipher_init_param));
+//printk(KERN_ERR, "\n cipher_alg = %d",cipher_alg);
+		switch(cipher_alg)
+		{
+			case NLM_CIPHER_AES128:
+				switch (enckeylen) {
+					case 16:
+						session->cip->cipher_alg = NLM_CIPHER_AES128;
+						break;
+					case 24:
+						session->cip->cipher_alg = NLM_CIPHER_AES192;
+						break;
+					case 32:
+						session->cip->cipher_alg = NLM_CIPHER_AES256;
+						break;
+					default:
+						printk(KERN_WARNING "[%s]: Cannot handle keylen = %d\n",
+								__FUNCTION__, enckeylen);
+						*flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
+						return -EINVAL;
+				}
+				break;
+			case NLM_CIPHER_DES:
+				switch (enckeylen) {
+					case DES_KEY_SIZE:
+						session->cip->cipher_alg = NLM_CIPHER_DES;
+						break;
+					default:
+						printk(KERN_WARNING "[%s]: Cannot handle keylen = %d\n",
+								__FUNCTION__, enckeylen);
+						*flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
+						return -EINVAL;
+				}
+
+				break;
+			case NLM_CIPHER_3DES:
+				switch (enckeylen) {
+					case DES3_EDE_KEY_SIZE:
+						session->cip->cipher_alg = NLM_CIPHER_3DES;
+						break;
+					default:
+						printk(KERN_WARNING "[%s]: Cannot handle keylen = %d\n",
+								__FUNCTION__, enckeylen);
+						*flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
+						return -EINVAL;
+				}
+
+				break;
+		}
+#if 0
+		session->cip->cipher_key = malloc(sizeof (struct crypto_iovec));
+#endif
+		session->cip->cipher_key = temp;
+		temp = temp + sizeof (struct crypto_iovec);
+		if (session->cip->cipher_key == NULL) {
+			printk(KERN_ERR
+			       "\nError: malloc failed for session->cip->cipher_key Returning from %s",
+			       __FUNCTION__);
+			return -EINVAL;
+		}
+#if 0
+		session->cip->cipher_key->buf = malloc(enckeylen + 1);
+#endif
+		session->cip->cipher_key->buf = temp;
+		temp = temp + (enckeylen + 1);
+		if (session->cip->cipher_key->buf == NULL) {
+			printk(KERN_ERR
+			       "\nError: malloc failed for session->cip->cipher_key->buf. Returning from %s",
+			       __FUNCTION__);
+			return -EINVAL;
+		}
+		memcpy(session->cip->cipher_key->buf, key + authkeylen,
+		       enckeylen);
+		session->cip->cipher_key->iov_len = enckeylen;
+
+		session->cip->arc4_cipher_key_len = 0;
+		session->cip->arc4_key_init = 0;
+		session->cip->cfb_mask = 0;
+	}
+
+	if (authkeylen) {
+#if 0
+		session->aip = (struct crypto_auth_init_param *)
+	    		malloc(sizeof (struct crypto_auth_init_param));
+#endif
+	//	session->aip = temp;
+	//	temp = temp + sizeof (struct crypto_auth_init_param);
+		if (session->aip == NULL) {
+			printk(KERN_ERR
+			       "\nError: malloc failed for session->aip. Returning from %s",
+		       		__FUNCTION__);
+			return -EINVAL;
+		}
+
+//printk(KERN_ERR "\n inside authenc_setkey authkeylen = %d",authkeylen);
+	//	memset(session->aip, 0, sizeof (struct crypto_auth_init_param));
+#if 0
+		session->aip->auth_key = malloc(sizeof (struct crypto_iovec));
+#endif
+		session->aip->auth_key = temp;
+		temp = temp + sizeof (struct crypto_iovec);
+		if (session->aip->auth_key == NULL) {
+			printk(KERN_ERR
+			       "\nError: malloc failed for session->aip->auth_key. Returning from %s",
+			       __FUNCTION__);
+			return -EINVAL;
+		}
+#if 0
+		session->aip->auth_key->buf = malloc(authkeylen + 1);
+#endif
+		session->aip->auth_key->buf = temp;
+		temp = temp + (authkeylen + 1);
+		if (session->aip->auth_key->buf == NULL) {
+			printk(KERN_ERR
+			       "\nError: malloc failed for session->aip->auth_key->buf. Returning from %s",
+			       __FUNCTION__);
+			return -EINVAL;
+		}
+		memcpy(session->aip->auth_key->buf, key, authkeylen);
+		session->aip->auth_key->iov_len = authkeylen;
+		//session->aip->hmac = 1;
+	}
+
+//printk(KERN_ERR "\n returning authenc_setkey cipher_alg = %d auth_alg = %d", session->cip->cipher_alg, session->aip->auth_alg);
+//dump_session(session);
+	return 0;
+
+      badkey:
+	crypto_aead_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+	return -EINVAL;
+}
+
+static int
+aead_aes_setkey(struct crypto_aead *tfm, const u8 * key, unsigned int keylen)
+{
+	uint32_t cipher_alg;
+	
+//printk(KERN_ERR "\n aead_aes_setkey");
+	cipher_alg = NLM_CIPHER_AES128;
+	return authenc_setkey(tfm, key, keylen, cipher_alg);
+}
+
+static int
+aead_des_setkey(struct crypto_aead *tfm, const u8 * key, unsigned int keylen)
+{
+        uint32_t cipher_alg;
+
+//printk(KERN_ERR "\n aead_des_setkey");
+        cipher_alg = NLM_CIPHER_DES;
+        return authenc_setkey(tfm, key, keylen, cipher_alg);
+}
+
+static int
+aead_des3_setkey(struct crypto_aead *tfm, const u8 * key, unsigned int keylen)
+{
+        uint32_t cipher_alg;
+
+//printk(KERN_ERR "\n aead_des3_setkey");
+        cipher_alg = NLM_CIPHER_3DES;
+        return authenc_setkey(tfm, key, keylen, cipher_alg);
+}
+
+static int xlp_ctr_rfc3686_setkey(struct crypto_tfm *parent, const u8 *key,
+                                 unsigned int keylen)
+{
+	int err;
+	uint32_t cipher_alg;
+
+        if (keylen < CTR_RFC3686_NONCE_SIZE)
+                return -EINVAL;
+
+        memcpy(rfc3686_nonce, key + (keylen - CTR_RFC3686_NONCE_SIZE),
+               CTR_RFC3686_NONCE_SIZE);
+
+        keylen -= CTR_RFC3686_NONCE_SIZE;
+	cipher_alg = NLM_CIPHER_AES128;
+        err = authenc_setkey(parent, key, keylen, cipher_alg);
+#if 0
+        struct crypto_rfc3686_ctx *ctx = (crypto_aead_ctx(parent) + sizeof (struct crypto_session) + sizeof(struct crypto_param));
+        int err;
+	uint32_t cipher_alg;
+
+        /* the nonce is stored in bytes at end of key */
+        if (keylen < CTR_RFC3686_NONCE_SIZE)
+                return -EINVAL;
+
+        memcpy(ctx->nonce, key + (keylen - CTR_RFC3686_NONCE_SIZE),
+               CTR_RFC3686_NONCE_SIZE);
+
+        keylen -= CTR_RFC3686_NONCE_SIZE;
+
+	cipher_alg = NLM_CIPHER_AES128;
+        err = authenc_setkey(parent, key, keylen, cipher_alg);
+#endif
+        return err;
+}
+
+static int aead_gcm_rfc4106_setkey(struct crypto_aead *parent, const u8 *key,
+                                 unsigned int keylen)
+{
+	int err;
+
+        if (keylen < 4)
+                return -EINVAL;
+
+        keylen -= 4;
+        memcpy(rfc4106_nonce, key + keylen, 4);
+
+        err = aead_setkey(parent, key, keylen, 0);
+
+#if 0
+        struct crypto_rfc4106_ctx *ctx = (crypto_aead_ctx(parent) + sizeof (struct crypto_session) + sizeof(struct crypto_param));
+        struct crypto_aead *child = ctx->child;
+        int err;
+	uint32_t cipher_alg;
+
+        if (keylen < 4)
+                return -EINVAL;
+
+        keylen -= 4;
+        memcpy(ctx->nonce, key + keylen, 4);
+
+        crypto_aead_clear_flags(child, CRYPTO_TFM_REQ_MASK);
+        crypto_aead_set_flags(child, crypto_aead_get_flags(parent) &
+                                     CRYPTO_TFM_REQ_MASK);
+        err = aead_setkey(parent, key, keylen, 0);
+        crypto_aead_set_flags(parent, crypto_aead_get_flags(child) &
+                                      CRYPTO_TFM_RES_MASK);
+#endif
+        return err;
+}
+
+static int aead_ccm_rfc4309_setkey(struct crypto_aead *parent, const u8 *key,
+                                 unsigned int keylen)
+{
+	int err;
+
+        if (keylen < 3)
+                return -EINVAL;
+
+        keylen -= 3;
+        memcpy(rfc4309_nonce, key + keylen, 3);
+
+        err = aead_setkey(parent, key, keylen, 1);/* 1 for copy same key ffor both cipher_key and auth_key*/
+#if 0
+        struct crypto_rfc4309_ctx *ctx = (crypto_aead_ctx(parent) + sizeof (struct crypto_session) + sizeof(struct crypto_param));
+	struct crypto_aead *child = ctx->child;
+        int err;
+	uint32_t cipher_alg;
+
+        if (keylen < 3)
+                return -EINVAL;
+
+        keylen -= 3;
+        memcpy(ctx->nonce, key + keylen, 3);
+
+        crypto_aead_clear_flags(child, CRYPTO_TFM_REQ_MASK);
+        crypto_aead_set_flags(child, crypto_aead_get_flags(parent) &
+                                     CRYPTO_TFM_REQ_MASK);
+        err = aead_setkey(child, key, keylen, 1);/* 1 for copy same key ffor both cipher_key and auth_key*/
+        crypto_aead_set_flags(parent, crypto_aead_get_flags(child) &
+                                      CRYPTO_TFM_RES_MASK);
+#endif
+        return err;
+
+}
+#if 0
+static int
+aead_crypt(struct aead_request *req, unsigned int enc, uint32_t tag_len)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	struct crypto_param *sd = NULL; 
+	unsigned int ivsize = 0, len = 0, sg_len = 0;
+	unsigned int cipher_offset = 0, auth_len = 0, cipher_len =
+	    0, asooc_data_len = 0, num_frags = 0;
+	struct scatterlist *src, *dst, *sg;
+	int nbytes, err = 0, ret = 0, i, j, k;
+
+dump_session(session);
+
+	sd = malloc(sizeof (struct crypto_param));
+	if (!sd) {
+		printk(KERN_ERR
+		       "\nError: malloc failed for sd. Returning from %s",
+		       __FUNCTION__);
+		return -EINVAL;	//NLM_ERR_INVALID_PARAM;
+	}
+
+	memset(sd, 0, sizeof (struct crypto_param));
+printk(KERN_ERR "\n aead_encrypt req->src->length = %d",req->src->length);
+	src = req->src;
+	dst = req->dst;
+	sd->iv = NULL;
+	sd->iv_offset = 0;
+	sd->auth_offset = 0;
+	sd->cipher_offset = 0;
+
+	ret = crypto_setup_cipher_auth_session(session);
+	if (ret) {
+		printk(KERN_ERR
+		       "\nError: Error in session setup ret = %x. Returning from %s",
+		       ret, __FUNCTION__);
+		return -EINVAL;
+	}
+
+	if(enc)
+		nbytes = cipher_len = req->cryptlen;
+	else
+		nbytes = cipher_len = req->cryptlen - session->aip->tag_len;
+	
+	if (session->cip) {
+		ivsize = crypto_aead_ivsize(crypto_aead_reqtfm(req));
+		cipher_offset = req->assoclen + ivsize;
+
+		sd->iv_len =
+		    cipher_mode_iv_len[session->cip->cipher_alg][session->cip->
+								 cipher_mode];
+		if (ivsize != sd->iv_len) {
+			printk(KERN_ERR
+			       "\n Error: Incorrect IV Length %d. Returning from %s",
+			       ivsize, __FUNCTION__);
+			return -EINVAL;
+		}
+		sd->iv_offset = req->assoclen;
+		sd->cipher_len = cipher_len;
+		sd->cipher_offset = cipher_offset;
+		sd->cipher_key_len = session->cip->cipher_key->iov_len;
+	}
+	sd->nr_frags = 1;
+
+printk(KERN_ERR "\n  aead_encrypt req->assoclen = %d req->cryptlen= %d ivsize = %d",req->assoclen, req->cryptlen, ivsize);
+	//if (session->aip->hmac)
+	if(session->aip->auth_key)
+		sd->auth_key_len = auth_mode_key_len[session->aip->auth_alg][session->aip->auth_mode];
+
+	asooc_data_len = req->assoclen;
+	auth_len = req->assoclen + ivsize + cipher_len;
+	sd->enc = enc;
+	sd->hash_source = enc;
+	sd->src_len = auth_len;
+	sd->auth_len = auth_len;
+printk(KERN_ERR "\n Session->aip->tag_len = %d",session->aip->tag_len);
+	//sd->tag_len = (session->aip->tag_len)?session->aip->tag_len:tag_len;
+	sd->tag_len = tag_len;
+
+	num_frags = 3;		/* 1 extra for IV */
+
+printk(KERN_ERR "\n aead_encrypt num_frags = %di nbytes = %d", num_frags, nbytes);
+	sd->src = malloc(sizeof (struct crypto_iovec) * num_frags);
+	sd->dst = malloc(sizeof (struct crypto_iovec) * num_frags);
+	if (sd->src == NULL || sd->dst == NULL) {
+		printk(KERN_ERR
+		       "\nError: malloc failed for sd->dst/src. Returning from %s",
+		       __FUNCTION__);
+		return -EINVAL;
+	}
+	i = 0;
+
+	sd->src[0].buf = (unsigned char*)sg_virt(&req->assoc[0]);
+	sd->src[0].iov_len = req->assoclen;
+	sd->src[1].buf = (unsigned char*)req->iv;
+	sd->src[1].iov_len = ivsize;
+	sd->src[2].buf = (unsigned char*)sg_virt(&req->src[0]);
+	sd->src[2].iov_len = cipher_len;
+
+	sd->dst[0].buf = (unsigned char*)malloc(req->assoclen);//sg_virt(&req->assoc[0]);
+	sd->dst[0].iov_len = req->assoclen;
+	sd->dst[1].buf = (unsigned char*)malloc(ivsize);//req->iv;
+	sd->dst[1].iov_len = ivsize;
+	sd->dst[2].buf = (unsigned char*)malloc(cipher_len);//sg_virt(&req->dst[0]);
+	sd->dst[2].iov_len = cipher_len;
+
+	sd->nr_frags = 3;
+
+	if (session->aip) {
+		if(session->aip->tag_len < tag_len)
+		{
+printk(KERN_ERR "\n sd->tag_len < tag_len");
+			sd->hash_dst_address = (unsigned char *)malloc(tag_len+1);
+		}
+	}
+dump_buffer(sd->src[2].buf, sd->src[2].iov_len+session->aip->tag_len, "input text with digest:");
+	ret = crypto_cipher_auth_op(session, sd);
+dump_buffer(sd->hash_dst_address, tag_len, "outdigest:");
+	if (ret) {
+		printk(KERN_ERR
+		       "\nError: Error in cipher auth operation ret = %x. Returning from %s",
+		       ret, __FUNCTION__);
+		return -EAGAIN;
+	}
+	if(session->aip->tag_len < tag_len)
+	{
+		unsigned char *temp_dst = sg_virt(&req->dst[0]);
+		memcpy(temp_dst, sd->dst[2].buf, sd->cipher_len);
+		if(enc)
+		{
+			memcpy((unsigned char*)&temp_dst[sd->cipher_len], sd->hash_dst_address, session->aip->tag_len);
+		//	memcpy((unsigned char *) &sd->dst[2].buf[sd->dst[2].iov_len], sd->hash_dst_address, session->aip->tag_len);
+			
+		}
+		else
+		{
+			//if(memcmp((unsigned char *) &sd->dst[2].buf[sd->dst[2].iov_len], sd->hash_dst_address, session->aip->tag_len))
+			if(memcmp((unsigned char *) &sd->src[2].buf[sd->src[2].iov_len], sd->hash_dst_address, session->aip->tag_len))
+			{
+				printk(KERN_ERR "\n Digest do not match");
+				err = -EBADMSG;
+			}
+			else
+				printk(KERN_ERR "\n Digest Matched");
+		}
+		dump_buffer(temp_dst, req->dst->length, "out  text with digest:");
+		free(sd->hash_dst_address);
+	}
+	crypto_cleanup_session(session);
+	free(sd->dst[0].buf);
+	free(sd->dst[1].buf);
+	free(sd->dst[2].buf);
+	if (sd->src)
+		free(sd->src);
+	if (sd->dst)
+		free(sd->dst);
+	if(sd)
+		free(sd);
+printk(KERN_ERR "\n aead_encrypt end");
+dump_session(session);
+ 	return 0;
+}
+#endif
+#if 1
+static int
+aead_crypt(struct aead_request *req, unsigned int enc, uint32_t tag_len)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	struct crypto_param *sd = NULL; 
+	unsigned int ivsize = 0, len = 0, sg_len = 0;
+	unsigned int cipher_offset = 0, auth_len = 0, cipher_len =
+	    0, asooc_data_len = 0, num_frags = 0;
+	struct scatterlist *src, *dst, *sg;
+	int nbytes, err = 0, ret = 0, i, j, k;
+
+
+	sd = malloc(sizeof (struct crypto_param));
+	if (!sd) {
+		printk(KERN_ERR
+		       "\nError: malloc failed for sd. Returning from %s",
+		       __FUNCTION__);
+		return -EINVAL;	//NLM_ERR_INVALID_PARAM;
+	}
+
+	memset(sd, 0, sizeof (struct crypto_param));
+//printk(KERN_ERR "\n aead_encrypt req->src->length = %d",req->src->length);
+	src = req->src;
+	dst = req->dst;
+	sd->iv = NULL;
+	sd->iv_offset = 0;
+	sd->auth_offset = 0;
+	sd->cipher_offset = 0;
+
+	ret = crypto_setup_cipher_auth_session(session);
+	if (ret) {
+		printk(KERN_ERR
+		       "\nError: Error in session setup ret = %x. Returning from %s",
+		       ret, __FUNCTION__);
+		return -EINVAL;
+	}
+
+	if(enc)
+		nbytes = cipher_len = req->cryptlen;
+	else
+		nbytes = cipher_len = req->cryptlen - session->aip->tag_len;
+	
+	if (session->cip) {
+		ivsize = crypto_aead_ivsize(crypto_aead_reqtfm(req));
+		cipher_offset = req->assoclen + ivsize;
+
+		sd->iv_len =
+		    cipher_mode_iv_len[session->cip->cipher_alg][session->cip->
+								 cipher_mode];
+		if (ivsize != sd->iv_len) {
+			printk(KERN_ERR
+			       "\n Error: Incorrect IV Length %d. Returning from %s",
+			       ivsize, __FUNCTION__);
+			return -EINVAL;
+		}
+		sd->iv_offset = req->assoclen;
+		sd->cipher_len = cipher_len;
+		sd->cipher_offset = cipher_offset;
+		sd->cipher_key_len = session->cip->cipher_key->iov_len;
+	}
+	sd->nr_frags = 1;
+
+//printk(KERN_ERR "\n  aead_encrypt req->assoclen = %d req->cryptlen= %d ivsize = %d",req->assoclen, req->cryptlen, ivsize);
+	//if (session->aip->hmac)
+	if(session->aip->auth_key)
+		sd->auth_key_len = auth_mode_key_len[session->aip->auth_alg][session->aip->auth_mode];
+
+	asooc_data_len = req->assoclen;
+	auth_len = req->assoclen + ivsize + cipher_len;
+	sd->enc = enc;
+	sd->hash_source = enc;
+	sd->src_len = auth_len;
+	sd->auth_len = auth_len;
+//printk(KERN_ERR "\n Session->aip->tag_len = %d",session->aip->tag_len);
+	//sd->tag_len = (session->aip->tag_len)?session->aip->tag_len:tag_len;
+	sd->tag_len = tag_len;
+
+	j = asooc_data_len;
+	num_frags = 0;
+	for (sg = req->assoc; j > 0; sg = scatterwalk_sg_next(sg), i++) {
+		len = min(asooc_data_len, sg->length);
+		j -= len;
+		num_frags++;
+	}
+
+	j = nbytes;
+	for (sg = src; j > 0; sg = scatterwalk_sg_next(sg), i++) {
+		len = min(nbytes, sg->length);
+		j -= len;
+		num_frags++;
+	}
+
+	num_frags++;		/* 1 extra for IV */
+//printk(KERN_ERR "\n aead_encrypt num_frags = %di nbytes = %d", num_frags, nbytes);
+	sd->src = malloc(sizeof (struct crypto_iovec) * num_frags);
+	sd->dst = malloc(sizeof (struct crypto_iovec) * num_frags);
+	if (sd->src == NULL || sd->dst == NULL) {
+		printk(KERN_ERR
+		       "\nError: malloc failed for sd->dst/src. Returning from %s",
+		       __FUNCTION__);
+		return -EINVAL;
+	}
+	i = 0;
+	for (sg = req->assoc; asooc_data_len > 0;
+	     sg = scatterwalk_sg_next(sg), i++) {
+		struct scatter_walk walk;
+
+		len = min(asooc_data_len, sg->length);
+		scatterwalk_start(&walk, sg);
+		sd->src[i].buf = scatterwalk_map(&walk, 1);
+		sd->src[i].iov_len = len;	//sg->length;
+
+//printk(KERN_ERR "\n aead_encrypt assoc_data sg->length = %d",sg->length);
+		sd->dst[i].buf = sd->src[i].buf;
+		sd->dst[i].iov_len = sd->src[i].iov_len;
+		asooc_data_len -= sd->src[i].iov_len;
+	}
+
+	if (session->cip) {
+		sd->src[i].iov_len = ivsize;
+		sd->src[i].buf = (unsigned char *) req->iv;
+		sd->dst[i].iov_len = ivsize;
+		sd->dst[i].buf = (unsigned char *) req->iv;
+		i++;
+	}
+	k = i;
+	j = nbytes;
+	for (sg = src; nbytes > 0; sg = scatterwalk_sg_next(sg), i++) {
+		struct scatter_walk walk;
+
+		len = min(nbytes, sg->length);
+		scatterwalk_start(&walk, sg);
+//printk(KERN_ERR "\n aead_encrypt src_data sg->length = %d, nbytes = %d",sg->length, nbytes);
+		sd->src[i].buf = scatterwalk_map(&walk, 1);
+		sd->src[i].iov_len = len;	//sg->length;
+		nbytes -= sd->src[i].iov_len;
+	}
+
+	i = k;
+	nbytes = j;
+	for (sg = dst; nbytes > 0; sg = scatterwalk_sg_next(sg), i++) {
+		struct scatter_walk walk;
+		sg_len = sg->length;
+		len = min(nbytes, sg->length);
+		scatterwalk_start(&walk, sg);
+//printk(KERN_ERR "\n aead_encrypt dst_data sg->length = %d, nbytes = %d",sg->length, nbytes);
+		sd->dst[i].buf = scatterwalk_map(&walk, 1);
+		sd->dst[i].iov_len = len;	//sg_dma_len(sg);
+		nbytes -= sd->dst[i].iov_len;
+	}
+	sd->nr_frags = i;
+//printk(KERN_ERR "\n aead_crypt len = %d, sg_len = %d",len, sg_len);
+	if (len < sg_len)
+		i--;
+
+	if (session->aip) {
+		if(session->aip->tag_len < tag_len)
+		{
+//printk(KERN_ERR "\n sd->tag_len < tag_len");
+			sd->hash_dst_address = (unsigned char *)malloc(tag_len+1);
+		}
+		else
+			sd->hash_dst_address = (unsigned char *) &sd->dst[i].buf[sd->dst[i].iov_len];	// malloc(sd->tag_len+1);
+			//  sd->dst[i].iov_len += sd->tag_len;
+	}
+//dump_buffer(sd->src[i].buf, sd->src[i].iov_len, "plaintext:");
+//dump_buffer(sd->dst[i].buf, sd->dst[i].iov_len+session->aip->tag_len, "input text with digest:");
+	ret = crypto_cipher_auth_op(session, sd);
+//dump_buffer(sd->dst[i].buf, sd->dst[i].iov_len, "ciphertext:");
+//dump_buffer(sd->hash_dst_address, tag_len, "outdigest:");
+	if (ret) {
+		printk(KERN_ERR
+		       "\nError: Error in cipher auth operation ret = %x. Returning from %s",
+		       ret, __FUNCTION__);
+		return -EAGAIN;
+	}
+	if(session->aip->tag_len < tag_len)
+	{
+		if(enc)
+			memcpy((unsigned char *) &sd->dst[i].buf[sd->dst[i].iov_len], sd->hash_dst_address, session->aip->tag_len);
+		else
+		{
+			if(memcmp((unsigned char *) &sd->dst[i].buf[sd->dst[i].iov_len], sd->hash_dst_address, session->aip->tag_len))
+			{
+		//		printk(KERN_ERR "\n Digest do not match");
+				err = -EBADMSG;
+			}
+		/*	else
+				printk(KERN_ERR "\n Digest Matched");*/
+		}
+//		dump_buffer(sd->dst[i].buf, sd->dst[i].iov_len+session->aip->tag_len, "text with digest:");
+		free(sd->hash_dst_address);
+	}
+	crypto_cleanup_session(session);
+	if (sd->src)
+		free(sd->src);
+	if (sd->dst)
+		free(sd->dst);
+	if(sd)
+		free(sd);
+//printk(KERN_ERR "\n aead_encrypt end");
+ 	return 0;
+}
+#endif
+
+static int
+aead_encrypt(struct aead_request *req, unsigned int enc, uint32_t tag_len)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	struct crypto_param *sd = NULL; 
+	unsigned int ivsize = 0, len = 0, sg_len = 0;
+	unsigned int cipher_offset = 0, auth_len = 0, cipher_len =
+	    0, asooc_data_len = 0, num_frags = 0;
+	struct scatterlist *src, *dst, *sg;
+	int nbytes, err = 0, ret = 0, i, j, k;
+
+
+	sd = malloc(sizeof (struct crypto_param));
+	if (!sd) {
+		printk(KERN_ERR
+		       "\nError: malloc failed for sd. Returning from %s",
+		       __FUNCTION__);
+		return -EINVAL;	//NLM_ERR_INVALID_PARAM;
+	}
+
+	memset(sd, 0, sizeof (struct crypto_param));
+printk(KERN_ERR "\n aead_encrypt req->src->length = %d",req->src->length);
+	src = req->src;
+	dst = req->dst;
+	sd->iv = NULL;
+	sd->iv_offset = 0;
+	sd->auth_offset = 0;
+	sd->cipher_offset = 0;
+
+	ret = crypto_setup_cipher_auth_session(session);
+	if (ret) {
+		printk(KERN_ERR
+		       "\nError: Error in session setup ret = %x. Returning from %s",
+		       ret, __FUNCTION__);
+		return -EINVAL;
+	}
+
+	nbytes = cipher_len = req->cryptlen;
+	if (session->cip) {
+		ivsize = crypto_aead_ivsize(crypto_aead_reqtfm(req));
+		cipher_offset = req->assoclen + ivsize;
+
+		sd->iv_len =
+		    cipher_mode_iv_len[session->cip->cipher_alg][session->cip->
+								 cipher_mode];
+		if (ivsize != sd->iv_len) {
+			printk(KERN_ERR
+			       "\n Error: Incorrect IV Length %d. Returning from %s",
+			       ivsize, __FUNCTION__);
+			return -EINVAL;
+		}
+		sd->iv_offset = req->assoclen;
+		sd->cipher_len = cipher_len;
+		sd->cipher_offset = cipher_offset;
+		sd->cipher_key_len = session->cip->cipher_key->iov_len;
+	}
+	sd->nr_frags = 1;
+
+printk(KERN_ERR "\n  aead_encrypt req->assoclen = %d req->cryptlen= %d ivsize = %d",req->assoclen, req->cryptlen, ivsize);
+	//if (session->aip->hmac)
+	if(session->aip->auth_key)
+		sd->auth_key_len = session->aip->auth_key->iov_len;
+
+	asooc_data_len = req->assoclen;
+	auth_len = req->assoclen + ivsize + cipher_len;
+	sd->enc = enc;
+	sd->hash_source = enc;
+	sd->src_len = auth_len;
+	sd->auth_len = auth_len;
+printk(KERN_ERR "\n Session->aip->tag_len = %d",session->aip->tag_len);
+	//sd->tag_len = (session->aip->tag_len)?session->aip->tag_len:tag_len;
+	sd->tag_len = tag_len;
+
+	j = asooc_data_len;
+	num_frags = 0;
+	for (sg = req->assoc; j > 0; sg = scatterwalk_sg_next(sg), i++) {
+		len = min(asooc_data_len, sg->length);
+		j -= len;
+		num_frags++;
+	}
+
+	j = nbytes;
+	for (sg = src; j > 0; sg = scatterwalk_sg_next(sg), i++) {
+		len = min(nbytes, sg->length);
+		j -= len;
+		num_frags++;
+	}
+
+	num_frags++;		/* 1 extra for IV */
+printk(KERN_ERR "\n aead_encrypt num_frags = %di nbytes = %d", num_frags, nbytes);
+	sd->src = malloc(sizeof (struct crypto_iovec) * num_frags);
+	sd->dst = malloc(sizeof (struct crypto_iovec) * num_frags);
+	if (sd->src == NULL || sd->dst == NULL) {
+		printk(KERN_ERR
+		       "\nError: malloc failed for sd->dst/src. Returning from %s",
+		       __FUNCTION__);
+		return -EINVAL;
+	}
+	i = 0;
+	for (sg = req->assoc; asooc_data_len > 0;
+	     sg = scatterwalk_sg_next(sg), i++) {
+		struct scatter_walk walk;
+
+		len = min(asooc_data_len, sg->length);
+		scatterwalk_start(&walk, sg);
+		sd->src[i].buf = scatterwalk_map(&walk, 1);
+		sd->src[i].iov_len = len;	//sg->length;
+
+printk(KERN_ERR "\n aead_encrypt assoc_data sg->length = %d",sg->length);
+		sd->dst[i].buf = sd->src[i].buf;
+		sd->dst[i].iov_len = sd->src[i].iov_len;
+		asooc_data_len -= sd->src[i].iov_len;
+	}
+
+	if (session->cip) {
+		sd->src[i].iov_len = ivsize;
+		sd->src[i].buf = (unsigned char *) req->iv;
+		sd->dst[i].iov_len = ivsize;
+		sd->dst[i].buf = (unsigned char *) req->iv;
+		i++;
+	}
+	k = i;
+	j = nbytes;
+	for (sg = src; nbytes > 0; sg = scatterwalk_sg_next(sg), i++) {
+		struct scatter_walk walk;
+
+		len = min(nbytes, sg->length);
+		scatterwalk_start(&walk, sg);
+printk(KERN_ERR "\n aead_encrypt src_data sg->length = %d, nbytes = %d",sg->length, nbytes);
+		sd->src[i].buf = scatterwalk_map(&walk, 1);
+		sd->src[i].iov_len = len;	//sg->length;
+		nbytes -= sd->src[i].iov_len;
+	}
+
+	i = k;
+	nbytes = j;
+	for (sg = dst; nbytes > 0; sg = scatterwalk_sg_next(sg), i++) {
+		struct scatter_walk walk;
+		sg_len = sg->length;
+		len = min(nbytes, sg->length);
+		scatterwalk_start(&walk, sg);
+printk(KERN_ERR "\n aead_encrypt dst_data sg->length = %d, nbytes = %d",sg->length, nbytes);
+		sd->dst[i].buf = scatterwalk_map(&walk, 1);
+		sd->dst[i].iov_len = len;	//sg_dma_len(sg);
+		nbytes -= sd->dst[i].iov_len;
+	}
+	sd->nr_frags = i;
+printk(KERN_ERR "\n aead_crypt len = %d, sg_len = %d",len, sg_len);
+	if (len < sg_len)
+		i--;
+
+	if (session->aip) {
+		if(session->aip->tag_len < tag_len)
+		{
+printk(KERN_ERR "\n sd->tag_len < tag_len");
+			sd->hash_dst_address = (unsigned char *)malloc(tag_len+1);
+		}
+		else
+			sd->hash_dst_address = (unsigned char *) &sd->dst[i].buf[sd->dst[i].iov_len];	// malloc(sd->tag_len+1);
+			//  sd->dst[i].iov_len += sd->tag_len;
+	}
+//dump_buffer(sd->src[i].buf, sd->src[i].iov_len, "plaintext:");
+	ret = crypto_cipher_auth_op(session, sd);
+//dump_buffer(sd->dst[i].buf, sd->dst[i].iov_len, "ciphertext:");
+//dump_buffer(sd->hash_dst_address, tag_len, "outdigest:");
+	if (ret) {
+		printk(KERN_ERR
+		       "\nError: Error in cipher auth operation ret = %x. Returning from %s",
+		       ret, __FUNCTION__);
+		return -EAGAIN;
+	}
+	if(session->aip->tag_len < tag_len)
+	{
+		memcpy((unsigned char *) &sd->dst[i].buf[sd->dst[i].iov_len], sd->hash_dst_address, session->aip->tag_len);
+		dump_buffer(sd->dst[i].buf, sd->dst[i].iov_len+session->aip->tag_len, "text with digest:");
+		free(sd->hash_dst_address);
+	}
+	crypto_cleanup_session(session);
+	if (sd->src)
+		free(sd->src);
+	if (sd->dst)
+		free(sd->dst);
+	if(sd)
+		free(sd);
+printk(KERN_ERR "\n aead_encrypt end");
+	return err;
+}
+#if 0
+static int
+aead_decrypt(struct aead_request *req, unsigned int enc, uint32_t tag_len)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	struct crypto_param *sd; 
+	unsigned int ivsize = 0, len = 0, sg_len = 0;
+	unsigned int cipher_offset = 0, auth_len = 0, cipher_len =
+	    0, asooc_data_len = 0, num_frags = 0;
+	struct scatterlist *src, *dst, *sg;
+	int nbytes, err = 0, ret = 0, i, j, k;
+	unsigned char *temp;
+printk(KERN_ERR "\n session = %x aip= %x cip = %x cipher_algo = %d cipher_mode = %d session->aip->auth_alg= %d",(unsigned int)session,(unsigned int) session->aip,(unsigned int) session->cip, session->cip->cipher_alg, session->cip->cipher_mode, session->aip->auth_alg);
+	sd = malloc(sizeof (struct crypto_param));
+	if (!sd) {
+		printk(KERN_ERR
+		       "\nError: malloc failed for sd. Returning from %s",
+		       __FUNCTION__);
+		return -EINVAL;	//NLM_ERR_INVALID_PARAM;
+	}
+
+	memset(sd, 0, sizeof (struct crypto_param));
+printk(KERN_ERR "\n aead_decrypt req->src->length = %d",req->src->length);
+	src = req->src;
+	dst = req->dst;
+	sd->iv = NULL;
+	sd->iv_offset = 0;
+	sd->auth_offset = 0;
+	sd->cipher_offset = 0;
+
+	ret = crypto_setup_cipher_auth_session(session);
+	if (ret) {
+		printk(KERN_ERR
+		       "\nError: Error in session setup ret = %x. Returning from %s",
+		       ret, __FUNCTION__);
+		return -EINVAL;
+	}
+
+	nbytes = cipher_len = req->cryptlen - session->aip->tag_len;
+	if (session->cip) {
+		ivsize = crypto_aead_ivsize(crypto_aead_reqtfm(req));
+		cipher_offset = req->assoclen + ivsize;
+
+		sd->iv_len =
+		    cipher_mode_iv_len[session->cip->cipher_alg][session->cip->
+								 cipher_mode];
+		if (ivsize != sd->iv_len) {
+			printk(KERN_ERR
+			       "\n Error: Incorrect IV Length %d. Returning from %s",
+			       ivsize, __FUNCTION__);
+			return -EINVAL;
+		}
+		sd->iv_offset = req->assoclen;
+		sd->cipher_len = cipher_len;
+		sd->cipher_offset = cipher_offset;
+		sd->cipher_key_len = session->cip->cipher_key->iov_len;
+	}
+	sd->nr_frags = 1;
+
+printk(KERN_ERR "\n  aead_decrypt req->assoclen = %d req->cryptlen= %d ivsize = %d",req->assoclen, req->cryptlen, ivsize);
+	//if (session->aip->hmac)
+	if(session->aip->auth_key)
+		sd->auth_key_len = session->aip->auth_key->iov_len;
+
+	asooc_data_len = req->assoclen;
+	auth_len = req->assoclen + ivsize + cipher_len;
+	sd->enc = enc;
+	sd->hash_source = enc;
+	sd->src_len = auth_len;
+	sd->auth_len = auth_len;
+printk(KERN_ERR "\n eead_decrypt session->aip->tag_len = %d",session->aip->tag_len);
+printk(KERN_ERR "\n  aead_decrypt auth_offset = %d iv_offset = %d cipher_offset = %d auth_len = %d cipher_len = %d",sd->auth_offset,sd->iv_offset,sd->cipher_offset, sd->cipher_len ,sd->auth_len);
+	//sd->tag_len = (session->aip->tag_len)?session->aip->tag_len:tag_len;
+	sd->tag_len = tag_len;
+
+	j = asooc_data_len;
+	num_frags = 0;
+	for (sg = req->assoc; j > 0; sg = scatterwalk_sg_next(sg), i++) {
+		len = min(asooc_data_len, sg->length);
+		j -= len;
+		num_frags++;
+	}
+
+	j = nbytes;
+	for (sg = src; j > 0; sg = scatterwalk_sg_next(sg), i++) {
+		len = min(nbytes, sg->length);
+		j -= len;
+		num_frags++;
+	}
+
+	num_frags++;		/* 1 extra for IV */
+//printk(KERN_ERR "\n aead_decrypt num_frags = %di nbytes = %d", num_frags, nbytes);
+	//sd->src = malloc(sizeof (struct crypto_iovec) * num_frags);
+	//sd->dst = malloc(sizeof (struct crypto_iovec) * num_frags);
+
+	sd->src = malloc(sizeof (struct crypto_iovec) );
+	sd->dst = malloc(sizeof (struct crypto_iovec) );
+
+	sd->src[0].buf = malloc(sd->src_len);
+	memset(sd->src[0].buf, 0, sd->src_len);
+	sd->src[0].iov_len = 0;
+
+	sd->dst[0].buf = malloc(sd->src_len);
+	memset(sd->dst[0].buf, 0, sd->src_len);
+	sd->dst[0].iov_len = sd->src_len;
+
+	if (sd->src == NULL || sd->dst == NULL) {
+		printk(KERN_ERR
+		       "\nError: malloc failed for sd->dst/src. Returning from %s",
+		       __FUNCTION__);
+		return -EINVAL;
+	}
+	i = 0;
+	for (sg = req->assoc; asooc_data_len > 0;
+	     sg = scatterwalk_sg_next(sg), i++) {
+		struct scatter_walk walk;
+
+		len = min(asooc_data_len, sg->length);
+		scatterwalk_start(&walk, sg);
+		temp = scatterwalk_map(&walk, 1);
+		memcpy(sd->src[0].buf, temp, len);
+		sd->src[0].iov_len += len;
+//		sd->src[i].buf = scatterwalk_map(&walk, 1);
+//		sd->src[i].iov_len = len;	//sg->length;
+
+//printk(KERN_ERR "\n aead_decrypt assoc_data sg->length = %d",sg->length);
+//		sd->dst[i].buf = sd->src[i].buf;
+//		sd->dst[i].buf = malloc(len);
+//		memset(sd->dst[i].buf, 0, len);
+//		sd->dst[i].iov_len = sd->src[i].iov_len;
+//		asooc_data_len -= sd->src[i].iov_len;
+		asooc_data_len -=len;
+	}
+
+	if (session->cip) {
+		memcpy(sd->src[0].buf + sd->src[0].iov_len, req->iv, ivsize);
+		sd->src[0].iov_len += ivsize;
+//		sd->src[i].iov_len = ivsize;
+//		sd->src[i].buf = (unsigned char *) req->iv;
+//		sd->dst[i].iov_len = ivsize;
+		//sd->dst[i].buf = (unsigned char *) req->iv;
+//		sd->dst[i].buf = malloc(ivsize);
+//		memset(sd->dst[i].buf, 0, ivsize);
+		i++;
+	}
+	k = i;
+	j = nbytes;
+	for (sg = src; nbytes > 0; sg = scatterwalk_sg_next(sg), i++) {
+		struct scatter_walk walk;
+		len = min(nbytes, sg->length);
+		scatterwalk_start(&walk, sg);
+//printk(KERN_ERR "\n aead_decrypt src_data sg->length = %d, nbytes = %d",sg->length, nbytes);
+	//	sd->src[i].buf = scatterwalk_map(&walk, 1);
+	//	sd->src[i].iov_len = len;	//sg->length;
+		temp = scatterwalk_map(&walk, 1);
+		memcpy(sd->src[0].buf+sd->src[0].iov_len, temp, len);
+		sd->src[0].iov_len +=len;
+		nbytes -= len;
+	}
+
+	i = k;
+	nbytes = j;
+	for (sg = dst; nbytes > 0; sg = scatterwalk_sg_next(sg), i++) {
+		struct scatter_walk walk;
+		sg_len = sg->length;
+		len = min(nbytes, sg->length);
+		scatterwalk_start(&walk, sg);
+//printk(KERN_ERR "\n aead_decrypt dst_data sg->length = %d, nbytes = %d",sg->length, nbytes);
+		//sd->dst[i].buf = scatterwalk_map(&walk, 1);
+		//sd->dst[i].buf = malloc(len);
+		//memset(sd->dst[i].buf, 0, len);
+		//sd->dst[i].iov_len = len;	//sg_dma_len(sg);
+		temp = scatterwalk_map(&walk, 1);
+		nbytes -= len;
+	}
+	sd->nr_frags = 1;
+//printk(KERN_ERR "\n aead_decrypt len = %d, sg_len = %d",len, sg_len);
+	if (len < sg_len)
+		i--;
+
+i = 0;
+	if (session->aip) {
+		if(session->aip->tag_len < tag_len)
+		{
+//printk(KERN_ERR "\n sd->tag_len < tag_len");
+			sd->hash_dst_address = (unsigned char *)malloc(tag_len+1);
+		}
+		else
+			sd->hash_dst_address = (unsigned char *) &sd->dst[i].buf[sd->dst[i].iov_len];	// malloc(sd->tag_len+1);
+			//  sd->dst[i].iov_len += sd->tag_len;
+	}
+printk(KERN_ERR "\n sd->src[i].iov_len = %d, sd->dst[i].iov_len = %d sd->cipher_offset= %d sd->cipher_len = %d\n",sd->src[i].iov_len, sd->dst[i].iov_len, sd->cipher_offset, sd->cipher_len);
+
+/*{int k = 0;
+for(k = 0; k < session->aip->tag_len; k++)
+	printk(KERN_ERR "%x",sd->src[i].buf[sd->src[i].iov_len+k]);
+}*/
+dump_buffer(sd->src[i].buf, sd->src[i].iov_len, "plaintext:");
+dump_buffer(sd->dst[i].buf, sd->dst[i].iov_len, "dst buffer:");
+dump_buffer((unsigned char *) &temp[len], session->aip->tag_len, "input_digest:");
+	ret = crypto_cipher_auth_op(session, sd);
+dump_buffer(sd->hash_dst_address, session->aip->tag_len, "output_digest:");
+dump_buffer(sd->dst[i].buf, sd->dst[i].iov_len, "cipher_text::");
+
+	if (ret) {
+		printk(KERN_ERR
+		       "\nError: Error in cipher auth operation ret = %x. Returning from %s",
+		       ret, __FUNCTION__);
+		return -EAGAIN;
+	}
+	if(session->aip->tag_len < tag_len)
+	{
+printk(KERN_ERR "\n Copy digest to sg");
+
+		if(memcmp((unsigned char *) &temp[len], sd->hash_dst_address, session->aip->tag_len))
+		{
+			return -EBADMSG;
+		}
+		else
+		{
+			printk(KERN_ERR "\n DIGEST MATCHED");
+			{
+			struct scatterlist *sg_temp_src = req->dst;
+			unsigned char *sg_temp = sg_virt(&sg_temp_src[0]);
+dump_buffer(sg_temp, sg_temp_src->length, "source virt:");
+
+			memcpy(sg_temp,(unsigned char*)(&sd->dst[0].buf[sd->cipher_offset]), sd->cipher_len);
+dump_buffer(sg_temp, sg_temp_src->length, "source virt after copy:");
+
+			}
+		}	
+		//memcpy((unsigned char *) &sd->dst[i].buf[sd->dst[i].iov_len], sd->hash_dst_address, session->aip->tag_len);
+		free(sd->hash_dst_address);
+	}
+	crypto_cleanup_session(session);
+	if (sd->src)
+		free(sd->src);
+	if (sd->dst)
+		free(sd->dst);
+	free(sd->src[0].buf);
+	free(sd->dst[0].buf);
+	if(sd)
+		free(sd);
+
+printk(KERN_ERR "\n aead_decrypt end");
+	return err;
+}
+#endif
+#if 1
+static int
+aead_decrypt(struct aead_request *req, unsigned int enc, uint32_t tag_len)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	struct crypto_param *sd = NULL; 
+	unsigned int ivsize = 0, len = 0, sg_len = 0;
+	unsigned int cipher_offset = 0, auth_len = 0, cipher_len =
+	    0, asooc_data_len = 0, num_frags = 0;
+	struct scatterlist *src, *dst, *sg;
+	int nbytes, err = 0, ret = 0, i, j, k;
+
+printk(KERN_ERR "\n session = %x aip= %x cip = %x",(unsigned int)session,(unsigned int) session->aip,(unsigned int) session->cip);
+	sd = malloc(sizeof (struct crypto_param));
+	if (!sd) {
+		printk(KERN_ERR
+		       "\nError: malloc failed for sd. Returning from %s",
+		       __FUNCTION__);
+		return -EINVAL;	//NLM_ERR_INVALID_PARAM;
+	}
+
+	memset(sd, 0, sizeof (struct crypto_param));
+printk(KERN_ERR "\n aead_decrypt req->src->length = %d",req->src->length);
+	src = req->src;
+	dst = req->dst;
+	sd->iv = NULL;
+	sd->iv_offset = 0;
+	sd->auth_offset = 0;
+	sd->cipher_offset = 0;
+
+	ret = crypto_setup_cipher_auth_session(session);
+	if (ret) {
+		printk(KERN_ERR
+		       "\nError: Error in session setup ret = %x. Returning from %s",
+		       ret, __FUNCTION__);
+		return -EINVAL;
+	}
+
+	nbytes = cipher_len = req->cryptlen - session->aip->tag_len;
+	if (session->cip) {
+		ivsize = crypto_aead_ivsize(crypto_aead_reqtfm(req));
+		cipher_offset = req->assoclen + ivsize;
+
+		sd->iv_len =
+		    cipher_mode_iv_len[session->cip->cipher_alg][session->cip->
+								 cipher_mode];
+		if (ivsize != sd->iv_len) {
+			printk(KERN_ERR
+			       "\n Error: Incorrect IV Length %d. Returning from %s",
+			       ivsize, __FUNCTION__);
+			return -EINVAL;
+		}
+		sd->iv_offset = req->assoclen;
+		sd->cipher_len = cipher_len;
+		sd->cipher_offset = cipher_offset;
+		sd->cipher_key_len = session->cip->cipher_key->iov_len;
+	}
+	sd->nr_frags = 1;
+
+printk(KERN_ERR "\n  aead_decrypt req->assoclen = %d req->cryptlen= %d ivsize = %d",req->assoclen, req->cryptlen, ivsize);
+	//if (session->aip->hmac)
+	if(session->aip->auth_key)
+		sd->auth_key_len = session->aip->auth_key->iov_len;
+
+	asooc_data_len = req->assoclen;
+	auth_len = req->assoclen + ivsize + cipher_len;
+	sd->enc = enc;
+	sd->hash_source = enc;
+	sd->src_len = auth_len;
+	sd->auth_len = auth_len;
+printk(KERN_ERR "\n aead_decrypt session->aip->tag_len = %d",session->aip->tag_len);
+printk(KERN_ERR "\n  aead_decrypt auth_offset = %d iv_offset = %d cipher_offset = %d auth_len = %d cipher_len = %d",sd->auth_offset,sd->iv_offset,sd->cipher_offset, sd->auth_len, sd->cipher_len);
+	//sd->tag_len = (session->aip->tag_len)?session->aip->tag_len:tag_len;
+	sd->tag_len = tag_len;
+
+	j = asooc_data_len;
+	num_frags = 0;
+	for (sg = req->assoc; j > 0; sg = scatterwalk_sg_next(sg), i++) {
+		len = min(asooc_data_len, sg->length);
+		j -= len;
+		num_frags++;
+	}
+
+	j = nbytes;
+	for (sg = src; j > 0; sg = scatterwalk_sg_next(sg), i++) {
+		len = min(nbytes, sg->length);
+		j -= len;
+		num_frags++;
+	}
+
+	num_frags++;		/* 1 extra for IV */
+printk(KERN_ERR "\n aead_decrypt num_frags = %di nbytes = %d", num_frags, nbytes);
+	sd->src = malloc(sizeof (struct crypto_iovec) * num_frags);
+	sd->dst = malloc(sizeof (struct crypto_iovec) * num_frags);
+
+	if (sd->src == NULL || sd->dst == NULL) {
+		printk(KERN_ERR
+		       "\nError: malloc failed for sd->dst/src. Returning from %s",
+		       __FUNCTION__);
+		return -EINVAL;
+	}
+	i = 0;
+	for (sg = req->assoc; asooc_data_len > 0;
+	     sg = scatterwalk_sg_next(sg), i++) {
+		struct scatter_walk walk;
+
+		len = min(asooc_data_len, sg->length);
+		scatterwalk_start(&walk, sg);
+		sd->src[i].buf = scatterwalk_map(&walk, 1);
+		sd->src[i].iov_len = len;	//sg->length;
+
+printk(KERN_ERR "\n aead_decrypt assoc_data sg->length = %d",sg->length);
+		sd->dst[i].buf = sd->src[i].buf;
+		sd->dst[i].iov_len = len;
+		asooc_data_len -= len;
+	}
+
+	if (session->cip) {
+		sd->src[i].iov_len = ivsize;
+		sd->src[i].buf = (unsigned char *) req->iv;
+		sd->dst[i].iov_len = ivsize;
+		sd->dst[i].buf = (unsigned char *) req->iv;
+		i++;
+	}
+	k = i;
+	j = nbytes;
+	for (sg = src; nbytes > 0; sg = scatterwalk_sg_next(sg), i++) {
+		struct scatter_walk walk;
+
+		len = min(nbytes, sg->length);
+		scatterwalk_start(&walk, sg);
+printk(KERN_ERR "\n aead_decrypt src_data sg->length = %d, nbytes = %d",sg->length, nbytes);
+		sd->src[i].buf = scatterwalk_map(&walk, 1);
+		sd->src[i].iov_len = len;	//sg->length;
+		nbytes -= sd->src[i].iov_len;
+	}
+
+	i = k;
+	nbytes = j;
+	for (sg = dst; nbytes > 0; sg = scatterwalk_sg_next(sg), i++) {
+		struct scatter_walk walk;
+		sg_len = sg->length;
+		len = min(nbytes, sg->length);
+		scatterwalk_start(&walk, sg);
+printk(KERN_ERR "\n aead_decrypt dst_data sg->length = %d, nbytes = %d",sg->length, nbytes);
+		sd->dst[i].buf = scatterwalk_map(&walk, 1);
+		sd->dst[i].iov_len = len;	//sg_dma_len(sg);
+		nbytes -= sd->dst[i].iov_len;
+	}
+	sd->nr_frags = i;
+printk(KERN_ERR "\n aead_decrypt len = %d, sg_len = %d",len, sg_len);
+	if (len < sg_len)
+		i--;
+
+	if (session->aip)
+	{
+		if(session->aip->tag_len < tag_len)
+		{
+printk(KERN_ERR "\n sd->tag_len < tag_len");
+			sd->hash_dst_address = (unsigned char *)malloc(tag_len+1);
+		}
+		else
+			sd->hash_dst_address = (unsigned char *) &sd->dst[i].buf[sd->dst[i].iov_len];	// malloc(sd->tag_len+1);
+			//  sd->dst[i].iov_len += sd->tag_len;
+	}
+
+//dump_buffer((unsigned char *)sd, sizeof(struct crypto_param), "sd_buf");
+//dump_buffer((unsigned char *)session, sizeof(struct crypto_session), "session");
+dump_buffer(sd->src[i].buf, sd->src[i].iov_len, "plaintext:");
+//dump_buffer(sd->dst[i].buf, sd->dst[i].iov_len, "dst buffer:");
+dump_buffer(sd->src[0].buf, sd->src[0].iov_len, "Assoc data:");
+dump_buffer(sd->src[1].buf, sd->src[1].iov_len, "IV:");
+dump_buffer((unsigned char *) &sd->src[i].buf[sd->src[i].iov_len], session->aip->tag_len, "input_digest:");
+	ret = crypto_cipher_auth_op(session, sd);
+dump_buffer(sd->hash_dst_address, session->aip->tag_len, "output_digest:");
+dump_buffer(sd->dst[0].buf, sd->dst[0].iov_len, "Assoc data:");
+dump_buffer(sd->dst[1].buf, sd->dst[1].iov_len, "IV:");
+//dump_buffer(sd->dst[i].buf, sd->dst[i].iov_len+session->aip->tag_len, "cipher_text+ digest:");
+	if (ret) {
+		printk(KERN_ERR
+		       "\nError: Error in cipher auth operation ret = %x. Returning from %s",
+		       ret, __FUNCTION__);
+		return -EAGAIN;
+	}
+	if(session->aip->tag_len < tag_len)
+	{
+printk(KERN_ERR "\n Copy digest to sg");
+
+		if(0/*memcmp((unsigned char *) &sd->src[i].buf[sd->src[i].iov_len], sd->hash_dst_address, session->aip->tag_len)*/)
+		{
+			free(sd->hash_dst_address);
+			return -EBADMSG;
+		}
+		else
+			printk(KERN_ERR "\n DIGEST MATCHED");
+		free(sd->hash_dst_address);
+	}
+	crypto_cleanup_session(session);
+	if (sd->src)
+		free(sd->src);
+	if (sd->dst)
+		free(sd->dst);
+	if(sd)
+		free(sd);
+
+printk(KERN_ERR "\n aead_decrypt end");
+	return err;
+}
+#endif
+
+
+static int
+xlp_aes_cbc_hmac_sha1_encrypt(struct aead_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	uint32_t tag_len = 20;
+
+//printk(KERN_ERR "\n xlp_aes_cbc_hmac_sha1_encrypt");
+	if (session->cip)
+		session->cip->cipher_mode = NLM_CIPHER_MODE_CBC;
+	if (session->aip) {
+		session->aip->auth_alg = NLM_AUTH_SHA;
+		session->aip->auth_mode = NLM_AUTH_MODE_SHA1;
+		session->aip->hmac = 1;
+	}
+	return aead_crypt(req, 1, tag_len);
+}
+
+static int
+xlp_aes_cbc_hmac_sha1_decrypt(struct aead_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	uint32_t tag_len = 20;
+
+//printk(KERN_ERR "\n xlp_aes_cbc_hmac_sha1_decrypt");
+	if (session->cip)
+		session->cip->cipher_mode = NLM_CIPHER_MODE_CBC;
+	if (session->aip) {
+		session->aip->auth_alg = NLM_AUTH_SHA;
+		session->aip->auth_mode = NLM_AUTH_MODE_SHA1;
+		session->aip->hmac = 1;
+	}
+	//return aead_decrypt(req, 0, tag_len);
+	return aead_crypt(req, 0, tag_len);
+}
+
+
+static int
+xlp_aes_cbc_hmac_md5_encrypt(struct aead_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	uint32_t tag_len = 16;
+
+	if (session->cip)
+		session->cip->cipher_mode = NLM_CIPHER_MODE_CBC;
+	if (session->aip) {
+		session->aip->auth_alg = NLM_AUTH_MD5;
+		session->aip->hmac = 1;
+	}
+	return aead_crypt(req, 1, tag_len);
+}
+
+static int
+xlp_aes_cbc_hmac_md5_decrypt(struct aead_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	uint32_t tag_len = 16;
+
+	session->cip->cipher_mode = NLM_CIPHER_MODE_CBC;
+	session->aip->auth_alg = NLM_AUTH_MD5;
+	session->aip->hmac = 1;
+
+	return aead_crypt(req, 0, tag_len);
+}
+static int
+xlp_aes_cbc_hmac_sha256_encrypt(struct aead_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	uint32_t tag_len = 32;
+
+	if (session->cip)
+		session->cip->cipher_mode = NLM_CIPHER_MODE_CBC;
+	if (session->aip) {
+		session->aip->auth_alg = NLM_AUTH_SHA;
+		session->aip->auth_mode = NLM_AUTH_MODE_SHA256;
+		session->aip->hmac = 1;
+	}
+	return aead_crypt(req, 1, tag_len);
+}
+
+static int
+xlp_aes_cbc_hmac_sha256_decrypt(struct aead_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	uint32_t tag_len = 32;
+
+	session->cip->cipher_mode = NLM_CIPHER_MODE_CBC;
+	session->aip->auth_alg = NLM_AUTH_SHA;
+	session->aip->auth_mode = NLM_AUTH_MODE_SHA256;
+	session->aip->hmac = 1;
+
+	return aead_crypt(req, 0, tag_len);
+}
+
+static int
+xlp_aes_cbc_aes_xcbc_mac_encrypt(struct aead_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	uint32_t tag_len = 16;
+
+	if (session->cip)
+		session->cip->cipher_mode = NLM_CIPHER_MODE_CBC;
+	if (session->aip) {
+		switch(session->cip->cipher_alg)
+		{
+			case NLM_CIPHER_AES128:
+				session->aip->auth_alg = NLM_AUTH_AES128;
+				break;
+			case NLM_CIPHER_AES192:
+				session->aip->auth_alg = NLM_AUTH_AES192;
+				break;
+			case NLM_CIPHER_AES256:
+				session->aip->auth_alg = NLM_AUTH_AES256;
+				break;
+		}
+		session->aip->auth_mode = NLM_AUTH_MODE_XCBC;
+	}
+	return aead_crypt(req, 1, tag_len);
+}
+
+static int
+xlp_aes_cbc_aes_xcbc_mac_decrypt(struct aead_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	uint32_t tag_len = 16;
+
+	session->cip->cipher_mode = NLM_CIPHER_MODE_CBC;
+	switch(session->cip->cipher_alg)
+	{
+		case NLM_CIPHER_AES128:
+			session->aip->auth_alg = NLM_AUTH_AES128;
+			break;
+		case NLM_CIPHER_AES192:
+			session->aip->auth_alg = NLM_AUTH_AES192;
+			break;
+		case NLM_CIPHER_AES256:
+			session->aip->auth_alg = NLM_AUTH_AES256;
+			break;
+	}
+	session->aip->auth_mode = NLM_AUTH_MODE_XCBC;
+
+	return aead_crypt(req, 0, tag_len);
+}
+
+static int crypto_rfc3686_crypt(struct aead_request *req, uint32_t enc, uint32_t tag_len)
+{
+	unsigned char  iv[16];
+        unsigned char *org_iv = req->iv;
+        int err;
+
+        memcpy((unsigned char*)iv, rfc3686_nonce, CTR_RFC3686_NONCE_SIZE);
+        memcpy((unsigned char*)iv + CTR_RFC3686_NONCE_SIZE, req->iv, CTR_RFC3686_IV_SIZE);
+
+        *(__be32 *)((unsigned char*)iv + CTR_RFC3686_NONCE_SIZE + CTR_RFC3686_IV_SIZE) =
+                cpu_to_be32(1);
+
+        req->iv = (unsigned char*)&iv[0];
+        err = aead_crypt(req, enc, tag_len);//xlp_crypt(desc, dst, src, nbytes, enc, algo);
+        req->iv = org_iv;
+		
+	return err;	
+#if 0
+        struct aead_request *subreq = aead_request_ctx(req);
+        struct crypto_aead *aead = crypto_aead_reqtfm(req);
+        struct crypto_rfc4106_ctx *ctx = (crypto_aead_ctx(aead) + sizeof (struct crypto_session) + sizeof(struct crypto_param));
+        struct crypto_aead *child = ctx->child;
+        u8 *iv = PTR_ALIGN((u8 *)(subreq + 1) + crypto_aead_reqsize(child),
+                           crypto_aead_alignmask(child) + 1);
+
+	/* set up counter block */
+        memcpy(iv, ctx->nonce, CTR_RFC3686_NONCE_SIZE);
+        memcpy(iv + CTR_RFC3686_NONCE_SIZE, req->iv, CTR_RFC3686_IV_SIZE);
+
+        /* initialize counter portion of counter block */
+        *(__be32 *)(iv + CTR_RFC3686_NONCE_SIZE + CTR_RFC3686_IV_SIZE) =
+                cpu_to_be32(1);
+
+        aead_request_set_tfm(subreq, child);
+        aead_request_set_callback(subreq, req->base.flags, req->base.complete,
+                                  req->base.data);
+        aead_request_set_crypt(subreq, req->src, req->dst, req->cryptlen, iv);
+        aead_request_set_assoc(subreq, req->assoc, req->assoclen);
+        return subreq;
+#endif
+}
+
+
+static int crypto_rfc4106_crypt(struct aead_request *req, uint32_t enc, uint32_t tag_len)
+{
+	unsigned char  iv[16];
+        unsigned char *org_iv = req->iv;
+        int err;
+
+        memcpy((unsigned char*)iv, rfc4106_nonce, 4);
+        memcpy((unsigned char*)iv + 4, req->iv, 8);
+
+        req->iv = (unsigned char*)&iv[0];
+        err = aead_crypt(req, enc, tag_len);//xlp_crypt(desc, dst, src, nbytes, enc, algo);
+        req->iv = org_iv;
+
+        return err;
+
+#if 0
+        struct aead_request *subreq = aead_request_ctx(req);
+        struct crypto_aead *aead = crypto_aead_reqtfm(req);
+	struct crypto_rfc4106_ctx *ctx = (crypto_aead_ctx(aead) + sizeof (struct crypto_session) + sizeof(struct crypto_param));
+        struct crypto_aead *child = ctx->child;
+        u8 *iv = PTR_ALIGN((u8 *)(subreq + 1) + crypto_aead_reqsize(child),
+                           crypto_aead_alignmask(child) + 1);
+
+        memcpy(iv, ctx->nonce, 4);
+        memcpy(iv + 4, req->iv, 8);
+
+        aead_request_set_tfm(subreq, child);
+        aead_request_set_callback(subreq, req->base.flags, req->base.complete,
+                                  req->base.data);
+        aead_request_set_crypt(subreq, req->src, req->dst, req->cryptlen, iv);
+        aead_request_set_assoc(subreq, req->assoc, req->assoclen);
+
+        return subreq;
+#endif
+}
+
+static int crypto_rfc4309_crypt(struct aead_request *req, uint32_t enc, uint32_t tag_len)
+{
+	unsigned char  iv[16];
+        unsigned char *org_iv = req->iv;
+        int err;
+
+	memset((unsigned char*)iv, 0, 16);
+        /* L' */
+        iv[0] = 4;
+
+        memcpy((unsigned char*)iv, rfc4309_nonce, 3);
+        memcpy((unsigned char*)iv + 4, req->iv, 8);
+
+        req->iv = (unsigned char*)&iv[0];
+        err = aead_crypt(req, enc, tag_len);
+        req->iv = org_iv;
+
+        return err;
+#if 0
+        struct aead_request *subreq = aead_request_ctx(req);
+        struct crypto_aead *aead = crypto_aead_reqtfm(req);
+        struct crypto_rfc4309_ctx *ctx = (crypto_aead_ctx(aead) + sizeof (struct crypto_session) + sizeof(struct crypto_param));
+        struct crypto_aead *child = ctx->child;
+        u8 *iv = PTR_ALIGN((u8 *)(subreq + 1) + crypto_aead_reqsize(child),
+                           crypto_aead_alignmask(child) + 1);
+
+        /* L' */
+        iv[0] = 3;
+
+        memcpy(iv + 1, ctx->nonce, 3);
+        memcpy(iv + 4, req->iv, 8);
+
+        aead_request_set_tfm(subreq, child);
+        aead_request_set_callback(subreq, req->base.flags, req->base.complete,
+                                  req->base.data);
+        aead_request_set_crypt(subreq, req->src, req->dst, req->cryptlen, iv);
+        aead_request_set_assoc(subreq, req->assoc, req->assoclen);
+
+        return subreq;
+#endif
+}
+
+static int
+xlp_aes_ctr_hmac_sha1_encrypt(struct aead_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	uint32_t tag_len = 20;
+
+	if (session->cip)
+		session->cip->cipher_mode = NLM_CIPHER_MODE_CTR;
+	if (session->aip) {
+		session->aip->auth_alg = NLM_AUTH_SHA;
+		session->aip->auth_mode = NLM_AUTH_MODE_SHA1;
+		session->aip->hmac = 1;
+	}
+	//req = crypto_rfc3686_crypt(req);
+	//return aead_crypt(req, 1, tag_len);
+	return crypto_rfc3686_crypt(req, 1, tag_len);
+}
+
+static int
+xlp_aes_ctr_hmac_sha1_decrypt(struct aead_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	uint32_t tag_len = 20;
+
+	session->cip->cipher_mode = NLM_CIPHER_MODE_CTR;
+	session->aip->auth_alg = NLM_AUTH_SHA;
+	session->aip->auth_mode = NLM_AUTH_MODE_SHA1;
+	session->aip->hmac = 1;
+
+	//req = crypto_rfc3686_crypt(req);
+	//return aead_crypt(req, 0, tag_len);
+	return crypto_rfc3686_crypt(req, 0, tag_len);
+}
+
+static int
+xlp_aes_ctr_hmac_md5_encrypt(struct aead_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	uint32_t tag_len = 16;
+
+	if (session->cip)
+		session->cip->cipher_mode = NLM_CIPHER_MODE_CTR;
+	if (session->aip) {
+		session->aip->auth_alg = NLM_AUTH_MD5;
+		session->aip->hmac = 1;
+	}
+	//req = crypto_rfc3686_crypt(req);
+	//return aead_crypt(req, 1, tag_len);
+	return crypto_rfc3686_crypt(req, 1, tag_len);
+}
+
+static int
+xlp_aes_ctr_hmac_md5_decrypt(struct aead_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	uint32_t tag_len = 16;
+
+	session->cip->cipher_mode = NLM_CIPHER_MODE_CTR;
+	session->aip->auth_alg = NLM_AUTH_MD5;
+	session->aip->hmac = 1;
+
+	return crypto_rfc3686_crypt(req, 0, tag_len);
+	//req = crypto_rfc3686_crypt(req);
+	//return aead_crypt(req, 0, tag_len);
+}
+static int
+xlp_aes_ctr_hmac_sha256_encrypt(struct aead_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	uint32_t tag_len = 32;
+
+	if (session->cip)
+		session->cip->cipher_mode = NLM_CIPHER_MODE_CTR;
+	if (session->aip) {
+		session->aip->auth_alg = NLM_AUTH_SHA;
+		session->aip->auth_mode = NLM_AUTH_MODE_SHA256;
+		session->aip->hmac = 1;
+	}
+	return crypto_rfc3686_crypt(req, 1, tag_len);
+	//req = crypto_rfc3686_crypt(req);
+	//return aead_crypt(req, 1, tag_len);
+}
+
+static int
+xlp_aes_ctr_hmac_sha256_decrypt(struct aead_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	uint32_t tag_len = 32;
+
+	session->cip->cipher_mode = NLM_CIPHER_MODE_CTR;
+	session->aip->auth_alg = NLM_AUTH_SHA;
+	session->aip->auth_mode = NLM_AUTH_MODE_SHA256;
+	session->aip->hmac = 1;
+
+	return crypto_rfc3686_crypt(req, 0, tag_len);
+	//req = crypto_rfc3686_crypt(req);
+	//return aead_crypt(req, 0, tag_len);
+}
+
+
+static int
+xlp_aes_ctr_aes_xcbc_mac_encrypt(struct aead_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	uint32_t tag_len = 16;
+
+	if (session->cip)
+		session->cip->cipher_mode = NLM_CIPHER_MODE_CTR;
+	if (session->aip) {
+		switch(session->cip->cipher_alg)
+		{
+			case NLM_CIPHER_AES128:
+				session->aip->auth_alg = NLM_AUTH_AES128;
+				break;
+			case NLM_CIPHER_AES192:
+				session->aip->auth_alg = NLM_AUTH_AES192;
+				break;
+			case NLM_CIPHER_AES256:
+				session->aip->auth_alg = NLM_AUTH_AES256;
+				break;
+		}
+		session->aip->auth_mode = NLM_AUTH_MODE_XCBC;
+	}
+	return crypto_rfc3686_crypt(req, 1, tag_len);
+	//req = crypto_rfc3686_crypt(req);
+	//return aead_crypt(req, 1, tag_len);
+}
+
+static int
+xlp_aes_ctr_aes_xcbc_mac_decrypt(struct aead_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	uint32_t tag_len = 16;
+
+	session->cip->cipher_mode = NLM_CIPHER_MODE_CTR;
+	switch(session->cip->cipher_alg)
+	{
+		case NLM_CIPHER_AES128:
+			session->aip->auth_alg = NLM_AUTH_AES128;
+			break;
+		case NLM_CIPHER_AES192:
+			session->aip->auth_alg = NLM_AUTH_AES192;
+			break;
+		case NLM_CIPHER_AES256:
+			session->aip->auth_alg = NLM_AUTH_AES256;
+			break;
+	}
+	session->aip->auth_mode = NLM_AUTH_MODE_XCBC;
+
+	return crypto_rfc3686_crypt(req, 0, tag_len);
+	//req = crypto_rfc3686_crypt(req);
+	//return aead_crypt(req, 0, tag_len);
+}
+
+xlp_aes_gcm_encrypt(struct aead_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	uint32_t tag_len = 16;
+
+	if (session->cip)
+		session->cip->cipher_mode = NLM_CIPHER_MODE_GCM;
+	if (session->aip) {
+		switch(session->cip->cipher_alg)
+		{
+			case NLM_CIPHER_AES128:
+				session->aip->auth_alg = NLM_AUTH_AES128;
+				break;
+			case NLM_CIPHER_AES192:
+				session->aip->auth_alg = NLM_AUTH_AES192;
+				break;
+			case NLM_CIPHER_AES256:
+				session->aip->auth_alg = NLM_AUTH_AES256;
+				break;
+		}
+		session->aip->auth_mode = NLM_AUTH_MODE_GCM;
+	}
+	//req = crypto_rfc4106_crypt(req);
+	//return aead_crypt(req, 1, tag_len);
+	return crypto_rfc4106_crypt(req, 1, tag_len);
+}
+
+static int
+xlp_aes_gcm_decrypt(struct aead_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	uint32_t tag_len = 16;
+
+	session->cip->cipher_mode = NLM_CIPHER_MODE_GCM;
+	switch(session->cip->cipher_alg)
+	{
+		case NLM_CIPHER_AES128:
+			session->aip->auth_alg = NLM_AUTH_AES128;
+			break;
+		case NLM_CIPHER_AES192:
+			session->aip->auth_alg = NLM_AUTH_AES192;
+			break;
+		case NLM_CIPHER_AES256:
+			session->aip->auth_alg = NLM_AUTH_AES256;
+			break;
+	}
+	session->aip->auth_mode = NLM_AUTH_MODE_GCM;
+
+	//req = crypto_rfc4106_crypt(req);
+	//return aead_crypt(req, 0, tag_len);
+	return crypto_rfc4106_crypt(req, 0, tag_len);
+}
+
+xlp_aes_ccm_encrypt(struct aead_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	uint32_t tag_len = 16;
+
+	if (session->cip)
+		session->cip->cipher_mode = NLM_CIPHER_MODE_CCM;
+	if (session->aip) {
+		switch(session->cip->cipher_alg)
+		{
+			case NLM_CIPHER_AES128:
+				session->aip->auth_alg = NLM_AUTH_AES128;
+				break;
+			case NLM_CIPHER_AES192:
+				session->aip->auth_alg = NLM_AUTH_AES192;
+				break;
+			case NLM_CIPHER_AES256:
+				session->aip->auth_alg = NLM_AUTH_AES256;
+				break;
+		}
+		session->aip->auth_mode = NLM_AUTH_MODE_CCM;
+	}
+	//req = crypto_rfc4309_crypt(req);
+	//return aead_crypt(req, 1, tag_len);
+	return crypto_rfc4309_crypt(req, 1, tag_len);
+}
+
+static int
+xlp_aes_ccm_decrypt(struct aead_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	uint32_t tag_len = 16;
+
+	session->cip->cipher_mode = NLM_CIPHER_MODE_CCM;
+	switch(session->cip->cipher_alg)
+	{
+		case NLM_CIPHER_AES128:
+			session->aip->auth_alg = NLM_AUTH_AES128;
+			break;
+		case NLM_CIPHER_AES192:
+			session->aip->auth_alg = NLM_AUTH_AES192;
+			break;
+		case NLM_CIPHER_AES256:
+			session->aip->auth_alg = NLM_AUTH_AES256;
+			break;
+	}
+	session->aip->auth_mode = NLM_AUTH_MODE_CCM;
+
+	//req = crypto_rfc4309_crypt(req);
+	//return aead_crypt(req, 0, tag_len);
+	return crypto_rfc4309_crypt(req, 0, tag_len);
+}
+
+static int
+xlp_des3_cbc_hmac_sha1_encrypt(struct aead_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	uint32_t tag_len = 20;
+
+	if (session->cip)
+		session->cip->cipher_mode = NLM_CIPHER_MODE_CBC;
+	if (session->aip) {
+		session->aip->auth_alg = NLM_AUTH_SHA;
+		session->aip->auth_mode = NLM_AUTH_MODE_SHA1;
+		session->aip->hmac = 1;
+	}
+	return aead_crypt(req, 1, tag_len);
+}
+
+static int
+xlp_des3_cbc_hmac_sha1_decrypt(struct aead_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	uint32_t tag_len = 20;
+
+	session->cip->cipher_mode = NLM_CIPHER_MODE_CBC;
+	session->aip->auth_alg = NLM_AUTH_SHA;
+	session->aip->auth_mode = NLM_AUTH_MODE_SHA1;
+	session->aip->hmac = 1;
+
+	return aead_crypt(req, 0, tag_len);
+}
+
+static int
+xlp_des3_cbc_hmac_md5_encrypt(struct aead_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	uint32_t tag_len = 16;
+
+	if (session->cip)
+		session->cip->cipher_mode = NLM_CIPHER_MODE_CBC;
+	if (session->aip) {
+		session->aip->auth_alg = NLM_AUTH_MD5;
+		session->aip->hmac = 1;
+	}
+	return aead_crypt(req, 1, tag_len);
+}
+
+static int
+xlp_des3_cbc_hmac_md5_decrypt(struct aead_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	uint32_t tag_len = 16;
+
+	session->cip->cipher_mode = NLM_CIPHER_MODE_CBC;
+	session->aip->auth_alg = NLM_AUTH_MD5;
+	session->aip->hmac = 1;
+
+	return aead_crypt(req, 0, tag_len);
+}
+
+static int
+xlp_des3_cbc_hmac_sha256_encrypt(struct aead_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	uint32_t tag_len = 32;
+
+	if (session->cip)
+		session->cip->cipher_mode = NLM_CIPHER_MODE_CBC;
+	if (session->aip) {
+		session->aip->auth_alg = NLM_AUTH_SHA;
+		session->aip->auth_mode = NLM_AUTH_MODE_SHA256;
+		session->aip->hmac = 1;
+	}
+	return aead_crypt(req, 1, tag_len);
+}
+
+static int
+xlp_des3_cbc_hmac_sha256_decrypt(struct aead_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	uint32_t tag_len = 32;
+
+	session->cip->cipher_mode = NLM_CIPHER_MODE_CBC;
+	session->aip->auth_alg = NLM_AUTH_SHA;
+	session->aip->auth_mode = NLM_AUTH_MODE_SHA256;
+	session->aip->hmac = 1;
+
+	return aead_crypt(req, 0, tag_len);
+}
+
+
+static int
+xlp_des3_cbc_aes_xcbc_mac_encrypt(struct aead_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	uint32_t tag_len = 16;
+
+	if (session->cip)
+		session->cip->cipher_mode = NLM_CIPHER_MODE_CBC;
+	if (session->aip) {
+		switch(session->cip->cipher_alg)
+		{
+			case NLM_CIPHER_AES128:
+				session->aip->auth_alg = NLM_AUTH_AES128;
+				break;
+			case NLM_CIPHER_AES192:
+				session->aip->auth_alg = NLM_AUTH_AES192;
+				break;
+			case NLM_CIPHER_AES256:
+				session->aip->auth_alg = NLM_AUTH_AES256;
+				break;
+		}
+		session->aip->auth_mode = NLM_AUTH_MODE_XCBC;
+	}
+	return aead_crypt(req, 1, tag_len);
+}
+
+static int
+xlp_des3_cbc_aes_xcbc_mac_decrypt(struct aead_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	uint32_t tag_len = 16;
+
+	session->cip->cipher_mode = NLM_CIPHER_MODE_CBC;
+	switch(session->cip->cipher_alg)
+	{
+		case NLM_CIPHER_AES128:
+			session->aip->auth_alg = NLM_AUTH_AES128;
+			break;
+		case NLM_CIPHER_AES192:
+			session->aip->auth_alg = NLM_AUTH_AES192;
+			break;
+		case NLM_CIPHER_AES256:
+			session->aip->auth_alg = NLM_AUTH_AES256;
+			break;
+	}
+	session->aip->auth_mode = NLM_AUTH_MODE_XCBC;
+
+	return aead_crypt(req, 0, tag_len);
+}
+static int
+xlp_des_cbc_hmac_sha1_encrypt(struct aead_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	uint32_t tag_len = 20;
+
+	if (session->cip)
+		session->cip->cipher_mode = NLM_CIPHER_MODE_CBC;
+	if (session->aip) {
+		session->aip->auth_alg = NLM_AUTH_SHA;
+		session->aip->auth_mode = NLM_AUTH_MODE_SHA1;
+		session->aip->hmac = 1;
+	}
+	return aead_crypt(req, 1, tag_len);
+}
+
+static int
+xlp_des_cbc_hmac_sha1_decrypt(struct aead_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	uint32_t tag_len = 20;
+
+	session->cip->cipher_mode = NLM_CIPHER_MODE_CBC;
+	session->aip->auth_alg = NLM_AUTH_SHA;
+	session->aip->auth_mode = NLM_AUTH_MODE_SHA1;
+	session->aip->hmac = 1;
+
+	return aead_crypt(req, 0, tag_len);
+}
+
+static int
+xlp_des_cbc_hmac_md5_encrypt(struct aead_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	uint32_t tag_len = 16;
+
+	if (session->cip)
+		session->cip->cipher_mode = NLM_CIPHER_MODE_CBC;
+	if (session->aip) {
+		session->aip->auth_alg = NLM_AUTH_MD5;
+		session->aip->hmac = 1;
+	}
+	return aead_crypt(req, 1, tag_len);
+}
+
+static int
+xlp_des_cbc_hmac_md5_decrypt(struct aead_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	uint32_t tag_len = 16;
+
+	session->cip->cipher_mode = NLM_CIPHER_MODE_CBC;
+	session->aip->auth_alg = NLM_AUTH_MD5;
+	session->aip->hmac = 1;
+
+	return aead_crypt(req, 0, tag_len);
+}
+static int
+xlp_des_cbc_hmac_sha256_encrypt(struct aead_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	uint32_t tag_len = 32;
+
+	if (session->cip)
+		session->cip->cipher_mode = NLM_CIPHER_MODE_CBC;
+	if (session->aip) {
+		session->aip->auth_alg = NLM_AUTH_SHA;
+		session->aip->auth_mode = NLM_AUTH_MODE_SHA256;
+		session->aip->hmac = 1;
+	}
+	return aead_crypt(req, 1, tag_len);
+}
+
+static int
+xlp_des_cbc_hmac_sha256_decrypt(struct aead_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	uint32_t tag_len = 32;
+
+	session->cip->cipher_mode = NLM_CIPHER_MODE_CBC;
+	session->aip->auth_alg = NLM_AUTH_SHA;
+	session->aip->auth_mode = NLM_AUTH_MODE_SHA256;
+	session->aip->hmac = 1;
+
+	return aead_crypt(req, 0, tag_len);
+}
+
+static int
+xlp_des_cbc_aes_xcbc_mac_encrypt(struct aead_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	uint32_t tag_len = 16;
+
+	if (session->cip)
+		session->cip->cipher_mode = NLM_CIPHER_MODE_CBC;
+	if (session->aip) {
+		switch(session->cip->cipher_alg)
+		{
+			case NLM_CIPHER_AES128:
+				session->aip->auth_alg = NLM_AUTH_AES128;
+				break;
+			case NLM_CIPHER_AES192:
+				session->aip->auth_alg = NLM_AUTH_AES192;
+				break;
+			case NLM_CIPHER_AES256:
+				session->aip->auth_alg = NLM_AUTH_AES256;
+				break;
+		}
+		session->aip->auth_mode = NLM_AUTH_MODE_XCBC;
+	}
+	return aead_crypt(req, 1, tag_len);
+}
+
+static int
+xlp_des_cbc_aes_xcbc_mac_decrypt(struct aead_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_session *session = crypto_aead_ctx(tfm);
+	uint32_t tag_len = 16;
+
+	session->cip->cipher_mode = NLM_CIPHER_MODE_CBC;
+	switch(session->cip->cipher_alg)
+	{
+		case NLM_CIPHER_AES128:
+			session->aip->auth_alg = NLM_AUTH_AES128;
+			break;
+		case NLM_CIPHER_AES192:
+			session->aip->auth_alg = NLM_AUTH_AES192;
+			break;
+		case NLM_CIPHER_AES256:
+			session->aip->auth_alg = NLM_AUTH_AES256;
+			break;
+	}
+	session->aip->auth_mode = NLM_AUTH_MODE_XCBC;
+
+	return aead_crypt(req, 0, tag_len);
+}
+
+
+static int
+aead_givencrypt(struct aead_givcrypt_request *req)
+{
+	return 0;
+}
+
+static int
+xlp_des_cbc_hmac_sha1_givencrypt(struct aead_givcrypt_request *req)
+{
+	struct crypto_aead *geniv = aead_givcrypt_reqtfm(req);
+        struct seqiv_ctx *ctx = crypto_aead_ctx(geniv);
+	struct aead_request *areq = &req->areq;
+
+	unsigned int ivsize;
+	u8 *info;
+	ivsize = crypto_aead_ivsize(geniv);
+	info = areq->iv;
+	//seqiv_geniv(ctx, info, req->seq, ivsize);
+	memcpy(req->giv, areq->iv, ivsize);
+//printk(KERN_ERR "\n aead_givencrypt");
+	return xlp_des_cbc_hmac_sha1_encrypt(&req->areq);
+}
+
+static int
+xlp_des_cbc_hmac_sha256_givencrypt(struct aead_givcrypt_request *req)
+{
+	struct crypto_aead *geniv = aead_givcrypt_reqtfm(req);
+        struct seqiv_ctx *ctx = crypto_aead_ctx(geniv);
+	struct aead_request *areq = &req->areq;
+
+	unsigned int ivsize;
+	u8 *info;
+	ivsize = crypto_aead_ivsize(geniv);
+	info = areq->iv;
+	//seqiv_geniv(ctx, info, req->seq, ivsize);
+	memcpy(req->giv, areq->iv, ivsize);
+//printk(KERN_ERR "\n aead_givencrypt");
+	return xlp_des_cbc_hmac_sha256_encrypt(&req->areq);
+}
+
+static int
+xlp_des_cbc_hmac_md5_givencrypt(struct aead_givcrypt_request *req)
+{
+	struct crypto_aead *geniv = aead_givcrypt_reqtfm(req);
+        struct seqiv_ctx *ctx = crypto_aead_ctx(geniv);
+	struct aead_request *areq = &req->areq;
+
+	unsigned int ivsize;
+	u8 *info;
+	ivsize = crypto_aead_ivsize(geniv);
+	info = areq->iv;
+	//seqiv_geniv(ctx, info, req->seq, ivsize);
+	memcpy(req->giv, areq->iv, ivsize);
+//printk(KERN_ERR "\n aead_givencrypt");
+	return xlp_des_cbc_hmac_md5_encrypt(&req->areq);
+}
+
+static int
+xlp_des3_cbc_hmac_sha1_givencrypt(struct aead_givcrypt_request *req)
+{
+	struct crypto_aead *geniv = aead_givcrypt_reqtfm(req);
+        struct seqiv_ctx *ctx = crypto_aead_ctx(geniv);
+	struct aead_request *areq = &req->areq;
+
+	unsigned int ivsize;
+	u8 *info;
+	ivsize = crypto_aead_ivsize(geniv);
+	info = areq->iv;
+	//seqiv_geniv(ctx, info, req->seq, ivsize);
+	memcpy(req->giv, areq->iv, ivsize);
+//printk(KERN_ERR "\n aead_givencrypt");
+	return xlp_des3_cbc_hmac_sha1_encrypt(&req->areq);
+}
+
+static int
+xlp_des3_cbc_hmac_sha256_givencrypt(struct aead_givcrypt_request *req)
+{
+	struct crypto_aead *geniv = aead_givcrypt_reqtfm(req);
+        struct seqiv_ctx *ctx = crypto_aead_ctx(geniv);
+	struct aead_request *areq = &req->areq;
+
+	unsigned int ivsize;
+	u8 *info;
+	ivsize = crypto_aead_ivsize(geniv);
+	info = areq->iv;
+	//seqiv_geniv(ctx, info, req->seq, ivsize);
+	memcpy(req->giv, areq->iv, ivsize);
+//printk(KERN_ERR "\n aead_givencrypt");
+	return xlp_des3_cbc_hmac_sha256_encrypt(&req->areq);
+}
+
+static int
+xlp_des3_cbc_hmac_md5_givencrypt(struct aead_givcrypt_request *req)
+{
+	struct crypto_aead *geniv = aead_givcrypt_reqtfm(req);
+        struct seqiv_ctx *ctx = crypto_aead_ctx(geniv);
+	struct aead_request *areq = &req->areq;
+
+	unsigned int ivsize;
+	u8 *info;
+	ivsize = crypto_aead_ivsize(geniv);
+	info = areq->iv;
+	//seqiv_geniv(ctx, info, req->seq, ivsize);
+	memcpy(req->giv, areq->iv, ivsize);
+//printk(KERN_ERR "\n aead_givencrypt");
+	return xlp_des3_cbc_hmac_md5_encrypt(&req->areq);
+}
+
+static int
+xlp_aes_cbc_hmac_sha1_givencrypt(struct aead_givcrypt_request *req)
+{
+	struct crypto_aead *geniv = aead_givcrypt_reqtfm(req);
+        struct seqiv_ctx *ctx = crypto_aead_ctx(geniv);
+	struct aead_request *areq = &req->areq;
+
+	unsigned int ivsize;
+	u8 *info;
+	ivsize = crypto_aead_ivsize(geniv);
+	info = areq->iv;
+	//seqiv_geniv(ctx, info, req->seq, ivsize);
+	memcpy(req->giv, areq->iv, ivsize);
+//printk(KERN_ERR "\n aead_givencrypt");
+	return xlp_aes_cbc_hmac_sha1_encrypt(&req->areq);
+}
+
+static int
+xlp_aes_cbc_hmac_sha256_givencrypt(struct aead_givcrypt_request *req)
+{
+	struct crypto_aead *geniv = aead_givcrypt_reqtfm(req);
+        struct seqiv_ctx *ctx = crypto_aead_ctx(geniv);
+	struct aead_request *areq = &req->areq;
+
+	unsigned int ivsize;
+	u8 *info;
+	ivsize = crypto_aead_ivsize(geniv);
+	info = areq->iv;
+	//seqiv_geniv(ctx, info, req->seq, ivsize);
+	memcpy(req->giv, areq->iv, ivsize);
+//printk(KERN_ERR "\n aead_givencrypt");
+	return xlp_aes_cbc_hmac_sha256_encrypt(&req->areq);
+}
+
+static int
+xlp_aes_cbc_hmac_md5_givencrypt(struct aead_givcrypt_request *req)
+{
+	struct crypto_aead *geniv = aead_givcrypt_reqtfm(req);
+        struct seqiv_ctx *ctx = crypto_aead_ctx(geniv);
+	struct aead_request *areq = &req->areq;
+
+	unsigned int ivsize;
+	u8 *info;
+	ivsize = crypto_aead_ivsize(geniv);
+	info = areq->iv;
+	//seqiv_geniv(ctx, info, req->seq, ivsize);
+	memcpy(req->giv, areq->iv, ivsize);
+//printk(KERN_ERR "\n aead_givencrypt");
+	return xlp_aes_cbc_hmac_md5_encrypt(&req->areq);
+}
+
+static struct crypto_alg xlp_aes_gcm_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "rfc4106(gcm(aes))",
+	.cra_driver_name = "rfc4106-gcm-aes-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = AES_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+	.cra_type = &crypto_aead_type,
+	.cra_aead = {
+		     .setkey = aead_gcm_rfc4106_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_aes_gcm_encrypt,
+		     .decrypt = xlp_aes_gcm_decrypt,
+		     .givencrypt = aead_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = AES_BLOCK_SIZE,
+		     .maxauthsize = SHA1_DIGEST_SIZE,
+		     }
+};
+
+static struct crypto_alg xlp_aes_ccm_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "rfc4309(ccm(aes))",
+	.cra_driver_name = "rfc4309-ccm-aes-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = AES_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+	.cra_type = &crypto_aead_type,
+	.cra_aead = {
+		     .setkey = aead_ccm_rfc4309_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_aes_ccm_encrypt,
+		     .decrypt = xlp_aes_ccm_decrypt,
+		     .givencrypt = aead_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = AES_BLOCK_SIZE,
+		     .maxauthsize = SHA1_DIGEST_SIZE,
+		     }
+};
+
+
+static struct crypto_alg xlp_aes_cbc_hmac_sha1_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(hmac(sha1),cbc(aes))",
+	.cra_driver_name = "authenc-hmac-sha1-cbc-aes-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = AES_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC ,
+	.cra_ctxsize = (sizeof (struct crypto_session) + 128),
+	.cra_type = &crypto_aead_type,
+	.cra_aead = {
+		     .setkey = aead_aes_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_aes_cbc_hmac_sha1_encrypt,
+		     .decrypt = xlp_aes_cbc_hmac_sha1_decrypt,
+		     .givencrypt = xlp_aes_cbc_hmac_sha1_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = AES_BLOCK_SIZE,
+		     .maxauthsize = SHA1_DIGEST_SIZE,
+		     }
+};
+
+
+static struct crypto_alg xlp_aes_cbc_hmac_sha256_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(hmac(sha256),cbc(aes))",
+	.cra_driver_name = "authenc-hmac-sha256-cbc-aes-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = AES_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+	.cra_ctxsize = (sizeof (struct crypto_session) + 128),
+	.cra_type = &crypto_aead_type,
+	.cra_aead = {
+		     .setkey = aead_aes_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_aes_cbc_hmac_sha256_encrypt,
+		     .decrypt = xlp_aes_cbc_hmac_sha256_decrypt,
+		     .givencrypt = xlp_aes_cbc_hmac_sha256_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = AES_BLOCK_SIZE,
+		     .maxauthsize = SHA256_DIGEST_SIZE,
+		     }
+};
+
+static struct crypto_alg xlp_aes_cbc_hmac_md5_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(hmac(md5),cbc(aes))",
+	.cra_driver_name = "authenc-hmac-md5-cbc-aes-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = AES_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+	.cra_ctxsize = (sizeof (struct crypto_session) + 128),
+	.cra_type = &crypto_aead_type,
+	.cra_aead = {
+		     .setkey = aead_aes_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_aes_cbc_hmac_md5_encrypt,
+		     .decrypt = xlp_aes_cbc_hmac_md5_decrypt,
+		     .givencrypt = xlp_aes_cbc_hmac_md5_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = AES_BLOCK_SIZE,
+		     .maxauthsize = MD5_DIGEST_SIZE,
+		     }
+};
+
+static struct crypto_alg xlp_aes_cbc_hmac_xcbc_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(hmac(xcbc),cbc(aes))",
+	.cra_driver_name = "authenc-hmac-sha1-cbc-aes-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = AES_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+	.cra_type = &crypto_aead_type,
+	.cra_aead = {
+		     .setkey = aead_aes_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_aes_cbc_aes_xcbc_mac_encrypt,
+		     .decrypt = xlp_aes_cbc_aes_xcbc_mac_decrypt,
+		     .givencrypt = aead_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = AES_BLOCK_SIZE,
+		     .maxauthsize = XCBC_DIGEST_SIZE,
+		     }
+};
+static struct crypto_alg xlp_aes_ctr_hmac_sha1_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(hmac(sha1),ctr(aes))",
+	.cra_driver_name = "authenc-hmac-sha1-ctr-aes-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = AES_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+	.cra_type = &crypto_aead_type,
+	.cra_aead = {
+		     .setkey = xlp_ctr_rfc3686_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_aes_ctr_hmac_sha1_encrypt,
+		     .decrypt = xlp_aes_ctr_hmac_sha1_decrypt,
+		     .givencrypt = aead_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = AES_BLOCK_SIZE,
+		     .maxauthsize = SHA1_DIGEST_SIZE,
+		     }
+};
+
+
+static struct crypto_alg xlp_aes_ctr_hmac_sha256_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(hmac(sha256),ctr(aes))",
+	.cra_driver_name = "authenc-hmac-sha256-ctr-aes-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = AES_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+	.cra_type = &crypto_aead_type,
+	.cra_aead = {
+		     .setkey = xlp_ctr_rfc3686_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_aes_ctr_hmac_sha256_encrypt,
+		     .decrypt = xlp_aes_ctr_hmac_sha256_decrypt,
+		     .givencrypt = aead_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = AES_BLOCK_SIZE,
+		     .maxauthsize = SHA256_DIGEST_SIZE,
+		     }
+};
+
+static struct crypto_alg xlp_aes_ctr_hmac_md5_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(hmac(md5),ctr(aes))",
+	.cra_driver_name = "authenc-hmac-md5-ctr-aes-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = AES_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+	.cra_type = &crypto_aead_type,
+	.cra_aead = {
+		     .setkey = xlp_ctr_rfc3686_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_aes_ctr_hmac_md5_encrypt,
+		     .decrypt = xlp_aes_ctr_hmac_md5_decrypt,
+		     .givencrypt = aead_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = AES_BLOCK_SIZE,
+		     .maxauthsize = MD5_DIGEST_SIZE,
+		     }
+};
+
+static struct crypto_alg xlp_aes_ctr_hmac_xcbc_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(hmac(xcbc),ctr(aes))",
+	.cra_driver_name = "authenc-hmac-sha1-ctr-aes-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = AES_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+	.cra_type = &crypto_aead_type,
+	.cra_aead = {
+		     .setkey = xlp_ctr_rfc3686_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_aes_ctr_aes_xcbc_mac_encrypt,
+		     .decrypt = xlp_aes_ctr_aes_xcbc_mac_decrypt,
+		     .givencrypt = aead_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = AES_BLOCK_SIZE,
+		     .maxauthsize = XCBC_DIGEST_SIZE,
+		     }
+};
+
+
+static struct crypto_alg xlp_des_hmac_sha1_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(hmac(sha1),cbc(des))",
+	.cra_driver_name = "authenc-hmac-sha1-cbc-des-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = DES_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+	.cra_ctxsize = (sizeof (struct crypto_session) + 128),
+	.cra_type = &crypto_aead_type,
+	.cra_aead = {
+		     .setkey = aead_des_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_des_cbc_hmac_sha1_encrypt,
+		     .decrypt = xlp_des_cbc_hmac_sha1_decrypt,
+		     .givencrypt = xlp_des_cbc_hmac_sha1_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = DES_BLOCK_SIZE,
+		     .maxauthsize = SHA1_DIGEST_SIZE,
+		     }
+};
+static struct crypto_alg xlp_des_hmac_sha256_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(hmac(sha256),cbc(des))",
+	.cra_driver_name = "authenc-hmac-sha256-cbc-des-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = DES_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+	.cra_ctxsize = (sizeof (struct crypto_session) + 128),
+	.cra_type = &crypto_aead_type,
+	.cra_aead = {
+		     .setkey = aead_des_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_des_cbc_hmac_sha256_encrypt,
+		     .decrypt = xlp_des_cbc_hmac_sha256_decrypt,
+		     .givencrypt = xlp_des_cbc_hmac_sha256_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = DES_BLOCK_SIZE,
+		     .maxauthsize = SHA256_DIGEST_SIZE,
+		     }
+};
+static struct crypto_alg xlp_des_hmac_md5_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(hmac(md5),cbc(des))",
+	.cra_driver_name = "authenc-hmac-md5-cbc-des-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = DES_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+	.cra_ctxsize = (sizeof (struct crypto_session) + 128),
+	.cra_type = &crypto_aead_type,
+	.cra_aead = {
+		     .setkey = aead_des_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_des_cbc_hmac_md5_encrypt,
+		     .decrypt = xlp_des_cbc_hmac_md5_decrypt,
+		     .givencrypt = xlp_des_cbc_hmac_md5_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = DES_BLOCK_SIZE,
+		     .maxauthsize = MD5_DIGEST_SIZE,
+		     }
+};
+static struct crypto_alg xlp_des_hmac_xcbc_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(hmac(xcbc),cbc(des))",
+	.cra_driver_name = "authenc-hmac-xcbc-cbc-des-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = DES_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+	.cra_type = &crypto_aead_type,
+	.cra_aead = {
+		     .setkey = aead_des_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_des_cbc_aes_xcbc_mac_encrypt,
+		     .decrypt = xlp_des_cbc_aes_xcbc_mac_decrypt,
+		     .givencrypt = aead_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = DES_BLOCK_SIZE,
+		     .maxauthsize = XCBC_DIGEST_SIZE,
+		     }
+};
+static struct crypto_alg xlp_des3_hmac_sha1_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(hmac(sha1),cbc(des3_ede))",
+	.cra_driver_name = "authenc-hmac-sha1-cbc-des3-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+	.cra_ctxsize = (sizeof (struct crypto_session) + 128),
+	.cra_type = &crypto_aead_type,
+	.cra_aead = {
+		     .setkey = aead_des3_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_des3_cbc_hmac_sha1_encrypt,
+		     .decrypt = xlp_des3_cbc_hmac_sha1_decrypt,
+		     .givencrypt = xlp_des3_cbc_hmac_sha1_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = DES3_EDE_BLOCK_SIZE,
+		     .maxauthsize = SHA1_DIGEST_SIZE,
+		     }
+};
+static struct crypto_alg xlp_des3_hmac_sha256_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(hmac(sha256),cbc(des3_ede))",
+	.cra_driver_name = "authenc-hmac-sha256-cbc-des3-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+	.cra_ctxsize = (sizeof (struct crypto_session) + 128),
+	.cra_type = &crypto_aead_type,
+	.cra_aead = {
+		     .setkey = aead_des3_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_des3_cbc_hmac_sha256_encrypt,
+		     .decrypt = xlp_des3_cbc_hmac_sha256_decrypt,
+		     .givencrypt = xlp_des3_cbc_hmac_sha256_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = DES3_EDE_BLOCK_SIZE,
+		     .maxauthsize = SHA256_DIGEST_SIZE,
+		     }
+};
+static struct crypto_alg xlp_des3_hmac_md5_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(hmac(md5),cbc(des3_ede))",
+	.cra_driver_name = "authenc-hmac-md5-cbc-des3-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+	.cra_ctxsize = (sizeof (struct crypto_session) + 128),
+	.cra_type = &crypto_aead_type,
+	.cra_aead = {
+		     .setkey = aead_des3_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_des3_cbc_hmac_md5_encrypt,
+		     .decrypt = xlp_des3_cbc_hmac_md5_decrypt,
+		     .givencrypt = xlp_des3_cbc_hmac_md5_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = DES3_EDE_BLOCK_SIZE,
+		     .maxauthsize = MD5_DIGEST_SIZE,
+		     }
+};
+static struct crypto_alg xlp_des3_hmac_xcbc_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(hmac(xcbc),cbc(des3_ede))",
+	.cra_driver_name = "authenc-hmac-xcbc-cbc-des3-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+	.cra_type = &crypto_aead_type,
+	.cra_aead = {
+		     .setkey = aead_des3_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_des3_cbc_aes_xcbc_mac_encrypt,
+		     .decrypt = xlp_des3_cbc_aes_xcbc_mac_decrypt,
+		     .givencrypt = aead_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = DES3_EDE_BLOCK_SIZE,
+		     .maxauthsize = XCBC_DIGEST_SIZE,
+		     }
+};
+
+int
+xlp_aead_alg_init(void)
+{
+	int ret = 0;
+
+	if ((ret = crypto_register_alg(&xlp_aes_gcm_cipher_auth)))
+		goto err1;
+	if ((ret = crypto_register_alg(&xlp_aes_ccm_cipher_auth)))
+		goto err2;
+	if ((ret = crypto_register_alg(&xlp_aes_cbc_hmac_sha1_cipher_auth)))
+		goto err3;
+	if ((ret = crypto_register_alg(&xlp_aes_cbc_hmac_sha256_cipher_auth)))
+		goto err4;
+	if ((ret = crypto_register_alg(&xlp_aes_cbc_hmac_md5_cipher_auth)))
+		goto err5;
+	if ((ret = crypto_register_alg(&xlp_aes_cbc_hmac_xcbc_cipher_auth)))
+		goto err6;
+	if ((ret = crypto_register_alg(&xlp_aes_ctr_hmac_sha1_cipher_auth)))
+		goto err7;
+	if ((ret = crypto_register_alg(&xlp_aes_ctr_hmac_sha256_cipher_auth)))
+		goto err8;
+	if ((ret = crypto_register_alg(&xlp_aes_ctr_hmac_md5_cipher_auth)))
+		goto err9;
+	if ((ret = crypto_register_alg(&xlp_aes_ctr_hmac_xcbc_cipher_auth)))
+		goto err10;
+	if ((ret = crypto_register_alg(&xlp_des_hmac_sha1_cipher_auth)))
+		goto err11;
+	if ((ret = crypto_register_alg(&xlp_des_hmac_sha256_cipher_auth)))
+		goto err12;
+	if ((ret = crypto_register_alg(&xlp_des_hmac_md5_cipher_auth)))
+		goto err13;
+	if ((ret = crypto_register_alg(&xlp_des_hmac_xcbc_cipher_auth)))
+		goto err14;
+	if ((ret = crypto_register_alg(&xlp_des3_hmac_sha1_cipher_auth)))
+		goto err15;
+	if ((ret = crypto_register_alg(&xlp_des3_hmac_sha256_cipher_auth)))
+		goto err16;
+	if ((ret = crypto_register_alg(&xlp_des3_hmac_md5_cipher_auth)))
+		goto err17;
+	if ((ret = crypto_register_alg(&xlp_des3_hmac_xcbc_cipher_auth)))
+		goto err18;
+
+	printk(KERN_NOTICE
+	       "Using XLP hardware for AEAD AES/DES/3DES algorithm.\n");
+	return 0;
+
+      err1:
+	crypto_unregister_alg(&xlp_aes_gcm_cipher_auth);
+		goto error_out;
+      err2:
+	crypto_unregister_alg(&xlp_aes_ccm_cipher_auth);
+		goto error_out;
+      err3:
+	crypto_unregister_alg(&xlp_aes_cbc_hmac_sha1_cipher_auth);
+		goto error_out;
+      err4:
+	crypto_unregister_alg(&xlp_aes_cbc_hmac_sha256_cipher_auth);
+		goto error_out;
+      err5:
+	crypto_unregister_alg(&xlp_aes_cbc_hmac_md5_cipher_auth);
+		goto error_out;
+      err6:
+	crypto_unregister_alg(&xlp_aes_cbc_hmac_xcbc_cipher_auth);
+		goto error_out;
+      err7:
+	crypto_unregister_alg(&xlp_aes_ctr_hmac_sha1_cipher_auth);
+		goto error_out;
+      err8:
+	crypto_unregister_alg(&xlp_aes_ctr_hmac_sha256_cipher_auth);
+		goto error_out;
+      err9:
+	crypto_unregister_alg(&xlp_aes_ctr_hmac_md5_cipher_auth);
+		goto error_out;
+      err10:
+	crypto_unregister_alg(&xlp_aes_ctr_hmac_xcbc_cipher_auth);
+		goto error_out;
+      err11:
+	crypto_unregister_alg(&xlp_des_hmac_sha1_cipher_auth);
+		goto error_out;
+      err12:
+	crypto_unregister_alg(&xlp_des_hmac_sha256_cipher_auth);
+		goto error_out;
+      err13:
+	crypto_unregister_alg(&xlp_des_hmac_md5_cipher_auth);
+		goto error_out;
+      err14:
+	crypto_unregister_alg(&xlp_des_hmac_xcbc_cipher_auth);
+		goto error_out;
+      err15:
+	crypto_unregister_alg(&xlp_des3_hmac_sha1_cipher_auth);
+		goto error_out;
+      err16:
+	crypto_unregister_alg(&xlp_des3_hmac_sha256_cipher_auth);
+		goto error_out;
+      err17:
+	crypto_unregister_alg(&xlp_des3_hmac_md5_cipher_auth);
+		goto error_out;
+      err18:
+	crypto_unregister_alg(&xlp_des3_hmac_xcbc_cipher_auth);
+      error_out:
+	printk(KERN_ERR
+	       "\nError: XLP hardware AEAD AES/DES/3DES initialization failed.");
+	return ret;
+}
+
+void
+xlp_aead_alg_fini(void)
+{
+	crypto_unregister_alg(&xlp_aes_gcm_cipher_auth);
+	crypto_unregister_alg(&xlp_aes_ccm_cipher_auth);
+	crypto_unregister_alg(&xlp_aes_cbc_hmac_sha1_cipher_auth);
+	crypto_unregister_alg(&xlp_aes_cbc_hmac_sha256_cipher_auth);
+	crypto_unregister_alg(&xlp_aes_cbc_hmac_md5_cipher_auth);
+	crypto_unregister_alg(&xlp_aes_cbc_hmac_xcbc_cipher_auth);
+	crypto_unregister_alg(&xlp_aes_ctr_hmac_sha1_cipher_auth);
+	crypto_unregister_alg(&xlp_aes_ctr_hmac_sha256_cipher_auth);
+	crypto_unregister_alg(&xlp_aes_ctr_hmac_md5_cipher_auth);
+	crypto_unregister_alg(&xlp_aes_ctr_hmac_xcbc_cipher_auth);
+	crypto_unregister_alg(&xlp_des_hmac_sha1_cipher_auth);
+	crypto_unregister_alg(&xlp_des_hmac_sha256_cipher_auth);
+	crypto_unregister_alg(&xlp_des_hmac_md5_cipher_auth);
+	crypto_unregister_alg(&xlp_des_hmac_xcbc_cipher_auth);
+	crypto_unregister_alg(&xlp_des3_hmac_sha1_cipher_auth);
+	crypto_unregister_alg(&xlp_des3_hmac_sha256_cipher_auth);
+	crypto_unregister_alg(&xlp_des3_hmac_md5_cipher_auth);
+	crypto_unregister_alg(&xlp_des3_hmac_xcbc_cipher_auth);
+}
+
+EXPORT_SYMBOL(xlp_aead_alg_init);
+EXPORT_SYMBOL(xlp_aead_alg_fini);
diff --git a/drivers/crypto/sae/nlm_auth.c b/drivers/crypto/sae/nlm_auth.c
new file mode 100644
index 0000000..5ff1d29
--- /dev/null
+++ b/drivers/crypto/sae/nlm_auth.c
@@ -0,0 +1,710 @@
+/***********************************************************************
+Copyright 2003-2010 Netlogic Microsystems (“Netlogic”). All rights
+reserved.
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are
+met:
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+*****************************#NETL_2#********************************/
+#include <crypto/algapi.h>
+#include <crypto/sha.h>
+#include <linux/err.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/errno.h>
+#include <linux/cryptohash.h>
+#include <linux/interrupt.h>
+#include <linux/kernel.h>
+#include <linux/scatterlist.h>
+#include <crypto/internal/hash.h>
+#include <crypto/sha.h>
+#include <crypto/aes.h>
+#include "nlm_crypto_api.h"
+#include "nlm_crypto.h"
+#include "nlm_crypto_data.h"
+
+#define XLP_AUTH_PRIORITY      300
+#define XLP_HMAC_PRIORITY      300
+
+#define XCBC_DIGEST_SIZE	16
+
+#define MD5_DIGEST_SIZE		16
+#define MD5_BLOCK_SIZE		64
+
+#define AUTH_BUFFER_SIZE	(16 * 1024)
+
+//#define SEC_DEBUG
+
+#ifdef SEC_DEBUG
+#ifdef __KERNEL__
+#define debug_print(fmt, args...) printk(fmt, ##args)
+#else				/* __KERNEL__ */
+#define debug_print(fmt, args...) printf(fmt, ##args)
+#endif				/* __KERNEL__ */
+#else				/* SEC_DEBUG */
+#define debug_print(fmt, args...)
+#endif				/* SEC_DEBUG */
+
+#define malloc(a) kmalloc(a, GFP_KERNEL)
+#define free kfree
+
+static int
+xlp_auth_init(struct shash_desc *desc)
+{
+	struct crypto_session *session = shash_desc_ctx(desc);
+	struct crypto_param *sd =
+	    (struct crypto_param *) (session + sizeof (struct crypto_session));
+
+//printk(KERN_ERR "\n ah: xlp_auth_init"); 
+	memset(session, 0, sizeof (struct crypto_session));
+
+	session->aip = (struct crypto_auth_init_param *)
+	    malloc(sizeof (struct crypto_auth_init_param));
+	memset(session->aip, 0, sizeof (struct crypto_auth_init_param));
+
+	memset(sd, 0, sizeof (struct crypto_param));
+
+	sd->src = (struct crypto_iovec *) malloc(sizeof (struct crypto_iovec));
+	sd->dst = malloc(sizeof (struct crypto_iovec));
+	if ((sd->src == NULL) || (sd->dst == NULL)) {
+		printk(KERN_ERR
+		       "\nError: Cannot malloc sd->src/dst. Returning from %s",
+		       __FUNCTION__);
+		return -EINVAL;
+	}
+	sd->src->buf = (unsigned char *) malloc(AUTH_BUFFER_SIZE);
+	sd->dst->buf = (unsigned char *) malloc(AUTH_BUFFER_SIZE);
+	if ((sd->src->buf == NULL) || (sd->src->buf == NULL)) {
+		printk(KERN_ERR
+		       "\nError: Cannot malloc sd->src/dst->buf. Returning from %s",
+		       __FUNCTION__);
+		return -EINVAL;
+	}
+	sd->src->iov_len = 0;
+	sd->dst->iov_len = 0;
+	sd->src_len = 0;
+	return 0;
+}
+
+static int
+xlp_auth_update(struct shash_desc *desc,
+		const uint8_t * data, unsigned int length)
+{
+	struct crypto_session *session = shash_desc_ctx(desc);
+	struct crypto_param *sd =
+	    (struct crypto_param *) (session + sizeof (struct crypto_session));
+
+//printk(KERN_ERR "\n ah: xlp_auth_update"); 
+	if (sd->src == NULL) {
+		printk(KERN_ERR "\nError: sd->src is NULL. Retunrning from %s",
+		       __FUNCTION__);
+		return -EINVAL;
+	}
+	if ((sd->src->iov_len + length) > AUTH_BUFFER_SIZE) {
+		printk(KERN_ERR
+		       "\nError: Data length is more than the allocated buffer size. Returning from %s",
+		       __FUNCTION__);
+		return -EINVAL;
+	}
+	memcpy(sd->src->buf + sd->src->iov_len, data, length);
+	sd->src->iov_len += length;
+	sd->dst->iov_len += length;
+	sd->src_len += length;
+/*
+{
+int i =0;
+printk(KERN_ERR "\nsd->src->iov_len = %d sd->src->buf:",sd->src->iov_len);
+for(i=0; i < sd->src->iov_len; i++)
+	printk(KERN_ERR "%x",sd->src->buf[i]);
+}*/
+	return 0;
+}
+
+static int
+xlp_auth_final(struct shash_desc *desc, uint8_t * out)
+{
+	struct crypto_session *session = shash_desc_ctx(desc);
+	struct crypto_param *sd =
+	    (struct crypto_param *) (session + sizeof (struct crypto_session));
+	int ret = 0;
+
+//printk(KERN_ERR "\n ah: xlp_auth_final"); 
+	ret = crypto_setup_cipher_auth_session(session);
+	if (ret) {
+		printk(KERN_ERR
+		       "\nError: Error in session setup ret = %x. Returning from %s",
+		       ret, __FUNCTION__);
+		return -EINVAL;
+	}
+
+	sd->auth_offset = 0;
+	sd->hash_dst_address = out;
+	sd->nr_frags = 1;
+	sd->auth_len = sd->src_len;
+
+	//if (session->aip->hmac)
+	if(session->aip->auth_key)
+		sd->auth_key_len = auth_mode_key_len[session->aip->auth_alg][session->aip->auth_mode];
+
+	ret = crypto_cipher_auth_op(session, sd);
+	if (ret) {
+		printk(KERN_ERR
+		       "\nError: Error in cipher auth operation ret = %x. Returning from %s",
+		       ret, __FUNCTION__);
+		return -EAGAIN;
+	}
+	if (session->aip) {
+#if 0
+		if (session->aip->auth_key) {
+			if (session->aip->auth_key->buf)
+				free(session->aip->auth_key->buf);
+			//         free(session->aip->auth_key);
+		}
+#endif
+		free(session->aip);
+	}
+
+	crypto_cleanup_session(session);
+
+	if (sd->src) {
+		if (sd->src->buf)
+			free(sd->src->buf);
+		free(sd->src);
+	}
+	if (sd->dst) {
+		if (sd->dst->buf)
+			free(sd->dst->buf);
+		free(sd->dst);
+	}
+/*{
+int i =0;
+printk(KERN_ERR "\nsd->tag_len = %d out:",sd->tag_len);
+for(i=0; i < sd->tag_len; i++)
+        printk(KERN_ERR "%x",out[i]);
+}*/
+	return 0;
+}
+
+static int
+xlp_sha1_final(struct shash_desc *desc, uint8_t * out)
+{
+	struct crypto_session *session = shash_desc_ctx(desc);
+	struct crypto_param *sd =
+	    (struct crypto_param *) (session + sizeof (struct crypto_session));
+//printk(KERN_ERR "\n ah: xlp_sha1_final"); 
+	session->aip->auth_alg = NLM_AUTH_SHA;
+	session->aip->auth_mode = NLM_AUTH_MODE_SHA1;
+	sd->tag_len = 20;
+
+	return xlp_auth_final(desc, out);
+}
+
+static int
+xlp_sha256_final(struct shash_desc *desc, uint8_t * out)
+{
+	struct crypto_session *session = shash_desc_ctx(desc);
+	struct crypto_param *sd =
+	    (struct crypto_param *) (session + sizeof (struct crypto_session));
+
+//printk(KERN_ERR "\n ah: xlp_sha256_final"); 
+	session->aip->auth_alg = NLM_AUTH_SHA;
+	session->aip->auth_mode = NLM_AUTH_MODE_SHA256;
+	sd->tag_len = 32;
+	return xlp_auth_final(desc, out);
+}
+
+static int
+xlp_sha384_final(struct shash_desc *desc, uint8_t * out)
+{
+	struct crypto_session *session = shash_desc_ctx(desc);
+	struct crypto_param *sd =
+	    (struct crypto_param *) (session + sizeof (struct crypto_session));
+
+//printk(KERN_ERR "\n ah: xlp_sha384_final"); 
+	session->aip->auth_alg = NLM_AUTH_SHA;
+	session->aip->auth_mode = NLM_AUTH_MODE_SHA384;
+	sd->tag_len = 48;
+	return xlp_auth_final(desc, out);
+}
+
+static int
+xlp_sha512_final(struct shash_desc *desc, uint8_t * out)
+{
+	struct crypto_session *session = shash_desc_ctx(desc);
+	struct crypto_param *sd =
+	    (struct crypto_param *) (session + sizeof (struct crypto_session));
+
+//printk(KERN_ERR "\n ah: xlp_sha512_final"); 
+	session->aip->auth_alg = NLM_AUTH_SHA;
+	session->aip->auth_mode = NLM_AUTH_MODE_SHA512;
+	sd->tag_len = 64;
+
+	return xlp_auth_final(desc, out);
+}
+
+static int
+xlp_md5_final(struct shash_desc *desc, uint8_t * out)
+{
+	struct crypto_session *session = shash_desc_ctx(desc);
+	struct crypto_param *sd =
+	    (struct crypto_param *) (session + sizeof (struct crypto_session));
+
+//printk(KERN_ERR "\n ah: xlp_md5_final"); 
+	session->aip->auth_alg = NLM_AUTH_MD5;
+	session->aip->auth_mode = 0;
+	sd->tag_len = 16;
+	return xlp_auth_final(desc, out);
+}
+
+static void
+copy_key(struct shash_desc *desc)
+{
+	struct crypto_shash *tfm = desc->tfm;
+	struct crypto_iovec *auth_key =
+	    (struct crypto_iovec *) tfm->base.__crt_ctx;
+	struct crypto_session *session = shash_desc_ctx(desc);
+
+//printk(KERN_ERR "\n ah: copy_key"); 
+	session->aip->auth_key = auth_key;
+	//session->aip->hmac = 1;
+}
+
+
+static int
+xlp_hmac_sha1_final(struct shash_desc *desc, uint8_t * out)
+{
+	struct crypto_session *session = shash_desc_ctx(desc);
+	struct crypto_param *sd =
+	    (struct crypto_param *) (session + sizeof (struct crypto_session));
+//printk(KERN_ERR "\n xlp_hmac_sha1_final");
+	session->aip->auth_alg = NLM_AUTH_SHA;
+	session->aip->auth_mode = NLM_AUTH_MODE_SHA1;
+	copy_key(desc);
+	session->aip->hmac = 1;
+	sd->tag_len = 20;
+	return xlp_auth_final(desc, out);
+}
+
+
+static int
+xlp_hmac_sha256_final(struct shash_desc *desc, uint8_t * out)
+{
+	struct crypto_session *session = shash_desc_ctx(desc);
+	struct crypto_param *sd =
+	    (struct crypto_param *) (session + sizeof (struct crypto_session));
+
+//printk(KERN_ERR "\n xlp_hmac_sha256_final");
+	session->aip->auth_alg = NLM_AUTH_SHA;
+	session->aip->auth_mode = NLM_AUTH_MODE_SHA256;
+	copy_key(desc);
+	session->aip->hmac = 1;
+	sd->tag_len = 32;
+	return xlp_auth_final(desc, out);
+}
+
+static int
+xlp_hmac_sha384_final(struct shash_desc *desc, uint8_t * out)
+{
+	struct crypto_session *session = shash_desc_ctx(desc);
+	struct crypto_param *sd =
+	    (struct crypto_param *) (session + sizeof (struct crypto_session));
+
+//printk(KERN_ERR "\n xlp_hmac_sha384_final");
+	session->aip->auth_alg = NLM_AUTH_SHA;
+	session->aip->auth_mode = NLM_AUTH_MODE_SHA384;
+	copy_key(desc);
+	session->aip->hmac = 1;
+	sd->tag_len = 48;
+	return xlp_auth_final(desc, out);
+}
+
+static int
+xlp_hmac_sha512_final(struct shash_desc *desc, uint8_t * out)
+{
+	struct crypto_session *session = shash_desc_ctx(desc);
+	struct crypto_param *sd =
+	    (struct crypto_param *) (session + sizeof (struct crypto_session));
+
+//printk(KERN_ERR "\n xlp_hmac_sha512_final");
+	session->aip->auth_alg = NLM_AUTH_SHA;
+	session->aip->auth_mode = NLM_AUTH_MODE_SHA512;
+	copy_key(desc);
+	session->aip->hmac = 1;
+	sd->tag_len = 64;
+
+	return xlp_auth_final(desc, out);
+}
+
+
+static int
+xlp_hmac_md5_final(struct shash_desc *desc, uint8_t * out)
+{
+	struct crypto_session *session = shash_desc_ctx(desc);
+	struct crypto_param *sd =
+	    (struct crypto_param *) (session + sizeof (struct crypto_session));
+
+//printk(KERN_ERR "\n xlp_hmac_mad5_final");
+	session->aip->auth_alg = NLM_AUTH_MD5;
+	session->aip->auth_mode = 0;
+	copy_key(desc);
+	session->aip->hmac = 1;
+	sd->tag_len = 16;
+	return xlp_auth_final(desc, out);
+}
+
+static int
+xlp_aes_xcbc_mac_final(struct shash_desc *desc, uint8_t * out)
+{
+	struct crypto_session *session = shash_desc_ctx(desc);
+	struct crypto_param *sd =
+	    (struct crypto_param *) (session + sizeof (struct crypto_session));
+
+//printk(KERN_ERR "\n xlp_aes_xcbc_mac_final");
+	session->aip->auth_alg = NLM_AUTH_AES128;
+	session->aip->auth_mode = NLM_AUTH_MODE_XCBC;
+	copy_key(desc);
+	//session->aip->hmac = 1;
+	sd->tag_len = XCBC_DIGEST_SIZE;
+	return xlp_auth_final(desc, out);
+}
+
+static int
+xlp_auth_setkey(struct crypto_shash *tfm, const u8 * key, unsigned int keylen)
+{
+	struct crypto_iovec *auth_key =
+	    (struct crypto_iovec *) tfm->base.__crt_ctx;
+
+//printk(KERN_ERR "\n ah: xlp_auth_setkey keylen = %d",keylen); 
+
+	auth_key->buf = (auth_key + sizeof(struct crypto_iovec));
+#if 0	
+	auth_key->buf = malloc(keylen + 1);
+#endif
+	memcpy(auth_key->buf, key, keylen);
+	auth_key->iov_len = keylen;
+	return 0;
+}
+
+static struct shash_alg sha512_alg = {
+	.digestsize = SHA512_DIGEST_SIZE,
+	.init = xlp_auth_init,
+	.update = xlp_auth_update,
+	.final = xlp_sha512_final,
+	.descsize = sizeof (struct crypto_session),
+	.base = {
+		 .cra_name = "sha512",
+		 .cra_driver_name = "sha512-xlp",
+		 .cra_priority = XLP_AUTH_PRIORITY,
+		 .cra_flags = CRYPTO_ALG_TYPE_SHASH,
+		 .cra_blocksize = SHA512_BLOCK_SIZE,
+		 .cra_module = THIS_MODULE,
+		 }
+};
+
+static struct shash_alg sha384_alg = {
+	.digestsize = SHA384_DIGEST_SIZE,
+	.init = xlp_auth_init,
+	.update = xlp_auth_update,
+	.final = xlp_sha384_final,
+	.descsize = sizeof (struct crypto_session),
+	.base = {
+		 .cra_name = "sha384",
+		 .cra_driver_name = "sha384-xlp",
+		 .cra_priority = XLP_AUTH_PRIORITY,
+		 .cra_flags = CRYPTO_ALG_TYPE_SHASH,
+		 .cra_blocksize = SHA384_BLOCK_SIZE,
+		 .cra_module = THIS_MODULE,
+		 }
+};
+
+static struct shash_alg sha256_alg = {
+	.digestsize = SHA256_DIGEST_SIZE,
+	.init = xlp_auth_init,
+	.update = xlp_auth_update,
+	.final = xlp_sha256_final,
+	.descsize = sizeof (struct crypto_session),
+	.base = {
+		 .cra_name = "sha256",
+		 .cra_driver_name = "sha256-xlp",
+		 .cra_priority = XLP_AUTH_PRIORITY,
+		 .cra_flags = CRYPTO_ALG_TYPE_SHASH,
+		 .cra_blocksize = SHA256_BLOCK_SIZE,
+		 .cra_module = THIS_MODULE,
+		 }
+};
+
+static struct shash_alg sha1_alg = {
+	.digestsize = SHA1_DIGEST_SIZE,
+	.init = xlp_auth_init,
+	.update = xlp_auth_update,
+	.final = xlp_sha1_final,
+	.descsize = sizeof (struct crypto_session),
+	.base = {
+		 .cra_name = "sha1",
+		 .cra_driver_name = "sha1-xlp",
+		 .cra_priority = XLP_AUTH_PRIORITY,
+		 .cra_flags = CRYPTO_ALG_TYPE_SHASH,
+		 .cra_blocksize = SHA1_BLOCK_SIZE,
+		 .cra_module = THIS_MODULE,
+		 . cra_ctxsize =
+                 (sizeof (struct crypto_session) +
+                  sizeof (struct crypto_param))
+		 }
+};
+
+static struct shash_alg md5_alg = {
+	.digestsize = MD5_DIGEST_SIZE,
+	.init = xlp_auth_init,
+	.update = xlp_auth_update,
+	.final = xlp_md5_final,
+	.descsize = sizeof (struct crypto_session),
+	.base = {
+		 .cra_name = "md5",
+		 .cra_driver_name = "md5-xlp",
+		 .cra_priority = XLP_AUTH_PRIORITY,
+		 .cra_flags = CRYPTO_ALG_TYPE_SHASH,
+		 .cra_blocksize = MD5_BLOCK_SIZE,
+		 .cra_module = THIS_MODULE,
+		 }
+};
+
+static struct shash_alg sha512_hmac_alg = {
+	.digestsize = SHA512_DIGEST_SIZE,
+	.init = xlp_auth_init,
+	.update = xlp_auth_update,
+	.final = xlp_hmac_sha512_final,
+	.descsize = sizeof (struct crypto_session),
+	.setkey = xlp_auth_setkey,
+	.base = {
+		 .cra_name = "hmac(sha512)",
+		 .cra_driver_name = "hmac-sha512-xlp",
+		 .cra_priority = XLP_HMAC_PRIORITY,
+		 .cra_flags = CRYPTO_ALG_TYPE_SHASH,
+		 .cra_blocksize = SHA512_BLOCK_SIZE,
+		 .cra_module = THIS_MODULE,
+		 .cra_ctxsize =
+		 (sizeof (struct crypto_session) +
+		  sizeof (struct crypto_param)),
+		 }
+};
+
+static struct shash_alg sha384_hmac_alg = {
+	.digestsize = SHA384_DIGEST_SIZE,
+	.init = xlp_auth_init,
+	.update = xlp_auth_update,
+	.final = xlp_hmac_sha384_final,
+	.descsize = sizeof (struct crypto_session),
+	.setkey = xlp_auth_setkey,
+	.base = {
+		 .cra_name = "hmac(sha384)",
+		 .cra_driver_name = "hmac-sha384-xlp",
+		 .cra_priority = XLP_HMAC_PRIORITY,
+		 .cra_flags = CRYPTO_ALG_TYPE_SHASH,
+		 .cra_blocksize = SHA384_BLOCK_SIZE,
+		 .cra_module = THIS_MODULE,
+		 .cra_ctxsize =
+		 (sizeof (struct crypto_session) +
+		  sizeof (struct crypto_param)),
+		 }
+};
+
+static struct shash_alg sha256_hmac_alg = {
+	.digestsize = SHA256_DIGEST_SIZE,
+	.init = xlp_auth_init,
+	.update = xlp_auth_update,
+	.final = xlp_hmac_sha256_final,
+	.descsize = sizeof (struct crypto_session),
+	.setkey = xlp_auth_setkey,
+	.base = {
+		 .cra_name = "hmac(sha256)",
+		 .cra_driver_name = "hmac-sha256-xlp",
+		 .cra_priority = XLP_HMAC_PRIORITY,
+		 .cra_flags = CRYPTO_ALG_TYPE_SHASH,
+		 .cra_blocksize = SHA256_BLOCK_SIZE,
+		 .cra_module = THIS_MODULE,
+		 .cra_ctxsize =
+		 (sizeof (struct crypto_session) +
+		  sizeof (struct crypto_param)),
+		 }
+};
+
+static struct shash_alg sha1_hmac_alg = {
+	.digestsize = SHA1_DIGEST_SIZE,
+	.init = xlp_auth_init,
+	.update = xlp_auth_update,
+	.final = xlp_hmac_sha1_final,
+	.descsize = sizeof (struct crypto_session),
+	.setkey = xlp_auth_setkey,
+	.base = {
+		 .cra_name = "hmac(sha1)",
+		 .cra_driver_name = "hmac-sha1-xlp",
+		 .cra_priority = XLP_HMAC_PRIORITY,
+		 .cra_flags = CRYPTO_ALG_TYPE_SHASH,
+		 .cra_blocksize = SHA1_BLOCK_SIZE,
+		 .cra_module = THIS_MODULE,
+		 .cra_ctxsize =
+		 (sizeof (struct crypto_session) +
+		  sizeof (struct crypto_param)),
+		 }
+};
+
+static struct shash_alg md5_hmac_alg = {
+	.digestsize = MD5_DIGEST_SIZE,
+	.init = xlp_auth_init,
+	.update = xlp_auth_update,
+	.final = xlp_hmac_md5_final,
+	.setkey = xlp_auth_setkey,
+	.descsize = sizeof (struct crypto_session),
+	.base = {
+		 .cra_name = "hmac(md5)",
+		 .cra_driver_name = "hmac-md5-xlp",
+		 .cra_priority = XLP_HMAC_PRIORITY,
+		 .cra_flags = CRYPTO_ALG_TYPE_SHASH,
+		 .cra_blocksize = MD5_BLOCK_SIZE,
+		 .cra_module = THIS_MODULE,
+		 .cra_ctxsize =
+		 (sizeof (struct crypto_session) +
+		  sizeof (struct crypto_param)),
+		 }
+};
+
+static struct shash_alg xcbc_mac_alg = {
+	.digestsize = XCBC_DIGEST_SIZE,
+	.init = xlp_auth_init,
+	.update = xlp_auth_update,
+	.final = xlp_aes_xcbc_mac_final,
+	.setkey = xlp_auth_setkey,
+	.descsize = sizeof (struct crypto_session),
+	.base = {
+		 .cra_name = "xcbc(aes)",
+		 .cra_driver_name = "xcbc-aes-xlp",
+		 .cra_priority = XLP_HMAC_PRIORITY,
+		 .cra_flags = CRYPTO_ALG_TYPE_SHASH,
+		 .cra_blocksize = AES_BLOCK_SIZE,
+		 .cra_module = THIS_MODULE,
+		 .cra_ctxsize =
+		 (sizeof (struct crypto_session) +
+		  sizeof (struct crypto_param)),
+		 }
+};
+
+int
+xlp_auth_alg_init(void)
+{
+	int rc = -ENODEV;
+
+	rc = crypto_register_shash(&sha1_alg);
+	if (rc)
+		goto out;
+#if 0
+	rc = crypto_register_shash(&sha256_alg);
+	if (rc)
+		goto out_unreg1;
+
+	rc = crypto_register_shash(&sha384_alg);
+	if (rc)
+		goto out_unreg2;
+
+	rc = crypto_register_shash(&sha512_alg);
+	if (rc)
+		goto out_unreg3;
+
+	rc = crypto_register_shash(&md5_alg);
+	if (rc)
+		goto out_unreg4;
+#endif
+	rc = crypto_register_shash(&sha1_hmac_alg);
+	if (rc)
+		goto out_unreg5;
+
+#if 0
+	rc = crypto_register_shash(&sha256_hmac_alg);
+	if (rc)
+		goto out_unreg6;
+
+	rc = crypto_register_shash(&sha384_hmac_alg);
+	if (rc)
+		goto out_unreg7;
+
+	rc = crypto_register_shash(&sha512_hmac_alg);
+	if (rc)
+		goto out_unreg8;
+#endif
+	rc = crypto_register_shash(&md5_hmac_alg);
+	if (rc)
+		goto out_unreg9;
+#if 0
+	rc = crypto_register_shash(&xcbc_mac_alg);
+	if (rc)
+		goto out_unreg10;
+#endif
+	//printk("Some of the FIPS test failed as the maximum key length supported is 64 bytes.\n");
+
+	printk(KERN_NOTICE "Using XLP hardware for SHA/MD5 algorithms.\n");
+
+	return 0;
+/*      out_unreg10:
+	crypto_unregister_shash(&xcbc_mac_alg);*/
+      out_unreg9:
+	crypto_unregister_shash(&md5_hmac_alg);
+      /*out_unreg8:
+	crypto_unregister_shash(&sha512_hmac_alg);
+      out_unreg7:
+	crypto_unregister_shash(&sha384_hmac_alg);
+      out_unreg6:
+	crypto_unregister_shash(&sha256_hmac_alg);*/
+      out_unreg5:
+	crypto_unregister_shash(&sha1_hmac_alg);
+    /*  out_unreg4:
+	crypto_unregister_shash(&md5_alg);
+      out_unreg3:
+	crypto_unregister_shash(&sha512_alg);
+      out_unreg2:
+	crypto_unregister_shash(&sha384_alg);
+      out_unreg1:
+	crypto_unregister_shash(&sha256_alg);*/
+      out:
+	crypto_unregister_shash(&sha1_alg);
+	printk(KERN_ERR "\nError: XLP SHA/MD5 initialization failed.");
+	return rc;
+
+}
+
+void
+xlp_auth_alg_fini(void)
+{
+	crypto_unregister_shash(&sha1_alg);
+#if 0
+	crypto_unregister_shash(&sha256_alg);
+	crypto_unregister_shash(&sha384_alg);
+	crypto_unregister_shash(&sha512_alg);
+	crypto_unregister_shash(&md5_alg);
+#endif
+	crypto_unregister_shash(&sha1_hmac_alg);
+	crypto_unregister_shash(&md5_hmac_alg);
+#if 0
+	crypto_unregister_shash(&sha256_hmac_alg);
+	crypto_unregister_shash(&sha384_hmac_alg);
+	crypto_unregister_shash(&sha512_hmac_alg);
+	crypto_unregister_shash(&xcbc_mac_alg);
+#endif
+}
+
+EXPORT_SYMBOL(xlp_auth_alg_init);
+EXPORT_SYMBOL(xlp_auth_alg_fini);
diff --git a/drivers/crypto/sae/nlm_auth.c_org b/drivers/crypto/sae/nlm_auth.c_org
new file mode 100644
index 0000000..c8f187a
--- /dev/null
+++ b/drivers/crypto/sae/nlm_auth.c_org
@@ -0,0 +1,761 @@
+/***********************************************************************
+Copyright 2003-2010 Netlogic Microsystems (“Netlogic”). All rights
+reserved.
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are
+met:
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+*****************************#NETL_2#********************************/
+#include <crypto/algapi.h>
+#include <crypto/sha.h>
+#include <linux/err.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/errno.h>
+#include <linux/cryptohash.h>
+#include <linux/interrupt.h>
+#include <linux/kernel.h>
+#include <linux/scatterlist.h>
+#include <crypto/internal/hash.h>
+#include <crypto/sha.h>
+#include "nlm_crypto_api.h"
+#include "nlm_crypto.h"
+//#include "nlm_crypto_data.h"
+
+#define XLP_AUTH_PRIORITY      300
+#define XLP_HMAC_PRIORITY      300
+
+#define MD5_DIGEST_SIZE		16
+#define MD5_BLOCK_SIZE		64
+
+#define AUTH_BUFFER_SIZE	(16 * 1024)
+
+//#define SEC_DEBUG
+
+#ifdef SEC_DEBUG
+#ifdef __KERNEL__
+#define debug_print(fmt, args...) printk(fmt, ##args)
+#else				/* __KERNEL__ */
+#define debug_print(fmt, args...) printf(fmt, ##args)
+#endif				/* __KERNEL__ */
+#else				/* SEC_DEBUG */
+#define debug_print(fmt, args...)
+#endif				/* SEC_DEBUG */
+
+#define malloc(a) kmalloc(a, GFP_KERNEL)
+#define free kfree
+
+static int
+xlp_auth_init(struct shash_desc *desc)
+{
+	struct crypto_session *session = shash_desc_ctx(desc);
+	struct crypto_param *sd =
+	    (struct crypto_param *) (session + sizeof (struct crypto_session));
+
+	memset(session, 0, sizeof (struct crypto_session));
+
+	session->aip = (struct crypto_auth_init_param *)
+	    malloc(sizeof (struct crypto_auth_init_param));
+	memset(session->aip, 0, sizeof (struct crypto_auth_init_param));
+
+	memset(sd, 0, sizeof (struct crypto_param));
+
+	sd->src = (struct crypto_iovec *) malloc(sizeof (struct crypto_iovec));
+	sd->dst = malloc(sizeof (struct crypto_iovec));
+	if ((sd->src == NULL) || (sd->dst == NULL)) {
+		printk(KERN_ERR
+		       "\nError: Cannot malloc sd->src/dst. Returning from %s",
+		       __FUNCTION__);
+		return -EINVAL;
+	}
+	sd->src->buf = (unsigned char *) malloc(AUTH_BUFFER_SIZE);
+	sd->dst->buf = (unsigned char *) malloc(AUTH_BUFFER_SIZE);
+	if ((sd->src->buf == NULL) || (sd->src->buf == NULL)) {
+		printk(KERN_ERR
+		       "\nError: Cannot malloc sd->src/dst->buf. Returning from %s",
+		       __FUNCTION__);
+		return -EINVAL;
+	}
+	sd->src->iov_len = 0;
+	sd->dst->iov_len = 0;
+	sd->src_len = 0;
+	return 0;
+}
+
+static int
+xlp_auth_update(struct shash_desc *desc,
+		const uint8_t * data, unsigned int length)
+{
+	struct crypto_session *session = shash_desc_ctx(desc);
+	struct crypto_param *sd =
+	    (struct crypto_param *) (session + sizeof (struct crypto_session));
+
+	if (sd->src == NULL) {
+		printk(KERN_ERR "\nError: sd->src is NULL. Retunrning from %s",
+		       __FUNCTION__);
+		return -EINVAL;
+	}
+	if ((sd->src->iov_len + length) > AUTH_BUFFER_SIZE) {
+		printk(KERN_ERR
+		       "\nError: Data length is more than the allocated buffer size. Returning from %s",
+		       __FUNCTION__);
+		return -EINVAL;
+	}
+	memcpy(sd->src->buf + sd->src->iov_len, data, length);
+	sd->src->iov_len += length;
+	sd->dst->iov_len += length;
+	sd->src_len += length;
+	return 0;
+}
+
+static int
+xlp_auth_final(struct shash_desc *desc, uint8_t * out)
+{
+	struct crypto_session *session = shash_desc_ctx(desc);
+	struct crypto_param *sd =
+	    (struct crypto_param *) (session + sizeof (struct crypto_session));
+	int ret = 0;
+
+	ret = crypto_setup_cipher_auth_session(session);
+	if (ret) {
+		printk(KERN_ERR
+		       "\nError: Error in session setup ret = %x. Returning from %s",
+		       ret, __FUNCTION__);
+		return -EINVAL;
+	}
+
+	sd->auth_offset = 0;
+	sd->hash_dst_address = out;
+	sd->nr_frags = 1;
+	sd->auth_len = sd->src_len;
+
+	if (session->aip->hmac)
+		sd->auth_key_len = session->aip->auth_key->iov_len;
+
+	ret = crypto_cipher_auth_op(session, sd);
+	if (ret) {
+		printk(KERN_ERR
+		       "\nError: Error in cipher auth operation ret = %x. Returning from %s",
+		       ret, __FUNCTION__);
+		return -EAGAIN;
+	}
+
+	if (session->aip) {
+		if (session->aip->auth_key) {
+			if (session->aip->auth_key->buf)
+				free(session->aip->auth_key->buf);
+			//         free(session->aip->auth_key);
+		}
+		free(session->aip);
+	}
+
+	crypto_cleanup_session(session);
+
+	if (sd->src) {
+		if (sd->src->buf)
+			free(sd->src->buf);
+		free(sd->src);
+	}
+	if (sd->dst) {
+		if (sd->dst->buf)
+			free(sd->dst->buf);
+		free(sd->dst);
+	}
+	return 0;
+}
+
+static int
+xlp_sha1_final(struct shash_desc *desc, uint8_t * out)
+{
+	struct crypto_session *session = shash_desc_ctx(desc);
+	struct crypto_param *sd =
+	    (struct crypto_param *) (session + sizeof (struct crypto_session));
+	session->aip->auth_alg = NLM_AUTH_SHA;
+	session->aip->auth_mode = NLM_AUTH_MODE_SHA1;
+	sd->tag_len = 20;
+
+	return xlp_auth_final(desc, out);
+}
+
+static int
+xlp_sha256_final(struct shash_desc *desc, uint8_t * out)
+{
+	struct crypto_session *session = shash_desc_ctx(desc);
+	struct crypto_param *sd =
+	    (struct crypto_param *) (session + sizeof (struct crypto_session));
+
+	session->aip->auth_alg = NLM_AUTH_SHA;
+	session->aip->auth_mode = NLM_AUTH_MODE_SHA256;
+	sd->tag_len = 32;
+	return xlp_auth_final(desc, out);
+}
+
+static int
+xlp_sha384_final(struct shash_desc *desc, uint8_t * out)
+{
+	struct crypto_session *session = shash_desc_ctx(desc);
+	struct crypto_param *sd =
+	    (struct crypto_param *) (session + sizeof (struct crypto_session));
+
+	session->aip->auth_alg = NLM_AUTH_SHA;
+	session->aip->auth_mode = NLM_AUTH_MODE_SHA384;
+	sd->tag_len = 48;
+	return xlp_auth_final(desc, out);
+}
+
+static int
+xlp_sha512_final(struct shash_desc *desc, uint8_t * out)
+{
+	struct crypto_session *session = shash_desc_ctx(desc);
+	struct crypto_param *sd =
+	    (struct crypto_param *) (session + sizeof (struct crypto_session));
+
+	session->aip->auth_alg = NLM_AUTH_SHA;
+	session->aip->auth_mode = NLM_AUTH_MODE_SHA512;
+	sd->tag_len = 64;
+
+	return xlp_auth_final(desc, out);
+}
+
+static int
+xlp_md5_final(struct shash_desc *desc, uint8_t * out)
+{
+	struct crypto_session *session = shash_desc_ctx(desc);
+	struct crypto_param *sd =
+	    (struct crypto_param *) (session + sizeof (struct crypto_session));
+
+	session->aip->auth_alg = NLM_AUTH_MD5;
+	session->aip->auth_mode = 0;
+	sd->tag_len = 16;
+	return xlp_auth_final(desc, out);
+}
+
+static void
+copy_key(struct shash_desc *desc)
+{
+	struct crypto_shash *tfm = desc->tfm;
+	struct crypto_iovec *auth_key =
+	    (struct crypto_iovec *) tfm->base.__crt_ctx;
+	struct crypto_session *session = shash_desc_ctx(desc);
+
+	session->aip->auth_key = auth_key;
+	session->aip->hmac = 1;
+}
+
+static int
+xlp_hmac_sha1_96_final(struct shash_desc *desc, uint8_t * out)
+{
+	uint8_t temp_out[20];
+	int ret = 0;
+	struct crypto_session *session = shash_desc_ctx(desc);
+	struct crypto_param *sd =
+	    (struct crypto_param *) (session + sizeof (struct crypto_session));
+
+	session->aip->auth_alg = NLM_AUTH_SHA;
+	session->aip->auth_mode = NLM_AUTH_MODE_SHA1;
+	copy_key(desc);
+	session->aip->hmac = 1;
+	sd->tag_len = 20;
+
+	
+	ret = xlp_auth_final(desc, temp_out);
+	memcpy(out, (unsigned char*)temp_out, 12);
+
+	return ret;
+}
+
+static int
+xlp_hmac_sha1_final(struct shash_desc *desc, uint8_t * out)
+{
+	struct crypto_session *session = shash_desc_ctx(desc);
+	struct crypto_param *sd =
+	    (struct crypto_param *) (session + sizeof (struct crypto_session));
+
+	session->aip->auth_alg = NLM_AUTH_SHA;
+	session->aip->auth_mode = NLM_AUTH_MODE_SHA1;
+	copy_key(desc);
+	session->aip->hmac = 1;
+	sd->tag_len = 20;
+	return xlp_auth_final(desc, out);
+}
+
+static int
+xlp_hmac_sha256_128_final(struct shash_desc *desc, uint8_t * out)
+{
+	uint8_t temp_out[32];
+	int ret = 0;
+	struct crypto_session *session = shash_desc_ctx(desc);
+	struct crypto_param *sd =
+	    (struct crypto_param *) (session + sizeof (struct crypto_session));
+
+	session->aip->auth_alg = NLM_AUTH_SHA;
+	session->aip->auth_mode = NLM_AUTH_MODE_SHA256;
+	copy_key(desc);
+	session->aip->hmac = 1;
+	sd->tag_len = 32;
+
+	ret = xlp_auth_final(desc, temp_out);
+	memcpy(out, (unsigned char*)temp_out, 16);
+
+	return ret;
+}
+
+static int
+xlp_hmac_sha256_final(struct shash_desc *desc, uint8_t * out)
+{
+	struct crypto_session *session = shash_desc_ctx(desc);
+	struct crypto_param *sd =
+	    (struct crypto_param *) (session + sizeof (struct crypto_session));
+
+	session->aip->auth_alg = NLM_AUTH_SHA;
+	session->aip->auth_mode = NLM_AUTH_MODE_SHA256;
+	copy_key(desc);
+	session->aip->hmac = 1;
+	sd->tag_len = 32;
+	return xlp_auth_final(desc, out);
+}
+
+static int
+xlp_hmac_sha384_final(struct shash_desc *desc, uint8_t * out)
+{
+	struct crypto_session *session = shash_desc_ctx(desc);
+	struct crypto_param *sd =
+	    (struct crypto_param *) (session + sizeof (struct crypto_session));
+
+	session->aip->auth_alg = NLM_AUTH_SHA;
+	session->aip->auth_mode = NLM_AUTH_MODE_SHA384;
+	copy_key(desc);
+	session->aip->hmac = 1;
+	sd->tag_len = 48;
+	return xlp_auth_final(desc, out);
+}
+
+static int
+xlp_hmac_sha512_final(struct shash_desc *desc, uint8_t * out)
+{
+	struct crypto_session *session = shash_desc_ctx(desc);
+	struct crypto_param *sd =
+	    (struct crypto_param *) (session + sizeof (struct crypto_session));
+
+	session->aip->auth_alg = NLM_AUTH_SHA;
+	session->aip->auth_mode = NLM_AUTH_MODE_SHA512;
+	copy_key(desc);
+	session->aip->hmac = 1;
+	sd->tag_len = 64;
+
+	return xlp_auth_final(desc, out);
+}
+
+static int
+xlp_hmac_md5_96_final(struct shash_desc *desc, uint8_t * out)
+{
+	uint8_t temp_out[16];
+	int ret = 0;
+	struct crypto_session *session = shash_desc_ctx(desc);
+	struct crypto_param *sd =
+	    (struct crypto_param *) (session + sizeof (struct crypto_session));
+
+	session->aip->auth_alg = NLM_AUTH_MD5;
+	session->aip->auth_mode = 0;
+	copy_key(desc);
+	session->aip->hmac = 1;
+	sd->tag_len = 16;
+	ret = xlp_auth_final(desc, out);
+	memcpy(out, (unsigned char*)temp_out, 12);
+
+	return ret;
+}
+
+static int
+xlp_hmac_md5_final(struct shash_desc *desc, uint8_t * out)
+{
+	struct crypto_session *session = shash_desc_ctx(desc);
+	struct crypto_param *sd =
+	    (struct crypto_param *) (session + sizeof (struct crypto_session));
+
+	session->aip->auth_alg = NLM_AUTH_MD5;
+	session->aip->auth_mode = 0;
+	copy_key(desc);
+	session->aip->hmac = 1;
+	sd->tag_len = 16;
+	return xlp_auth_final(desc, out);
+}
+
+static int
+xlp_auth_setkey(struct crypto_shash *tfm, const u8 * key, unsigned int keylen)
+{
+	struct crypto_iovec *auth_key =
+	    (struct crypto_iovec *) tfm->base.__crt_ctx;
+
+	auth_key->buf = malloc(keylen + 1);
+	memcpy(auth_key->buf, key, keylen);
+	auth_key->iov_len = keylen;
+
+	return 0;
+}
+
+static struct shash_alg sha512_alg = {
+	.digestsize = SHA512_DIGEST_SIZE,
+	.init = xlp_auth_init,
+	.update = xlp_auth_update,
+	.final = xlp_sha512_final,
+	.descsize = sizeof (struct crypto_session),
+	.base = {
+		 .cra_name = "sha512",
+		 .cra_driver_name = "sha512-xlp",
+		 .cra_priority = XLP_AUTH_PRIORITY,
+		 .cra_flags = CRYPTO_ALG_TYPE_SHASH,
+		 .cra_blocksize = SHA512_BLOCK_SIZE,
+		 .cra_module = THIS_MODULE,
+		 }
+};
+
+static struct shash_alg sha384_alg = {
+	.digestsize = SHA384_DIGEST_SIZE,
+	.init = xlp_auth_init,
+	.update = xlp_auth_update,
+	.final = xlp_sha384_final,
+	.descsize = sizeof (struct crypto_session),
+	.base = {
+		 .cra_name = "sha384",
+		 .cra_driver_name = "sha384-xlp",
+		 .cra_priority = XLP_AUTH_PRIORITY,
+		 .cra_flags = CRYPTO_ALG_TYPE_SHASH,
+		 .cra_blocksize = SHA384_BLOCK_SIZE,
+		 .cra_module = THIS_MODULE,
+		 }
+};
+
+static struct shash_alg sha256_alg = {
+	.digestsize = SHA256_DIGEST_SIZE,
+	.init = xlp_auth_init,
+	.update = xlp_auth_update,
+	.final = xlp_sha256_final,
+	.descsize = sizeof (struct crypto_session),
+	.base = {
+		 .cra_name = "sha256",
+		 .cra_driver_name = "sha256-xlp",
+		 .cra_priority = XLP_AUTH_PRIORITY,
+		 .cra_flags = CRYPTO_ALG_TYPE_SHASH,
+		 .cra_blocksize = SHA256_BLOCK_SIZE,
+		 .cra_module = THIS_MODULE,
+		 }
+};
+
+static struct shash_alg sha1_alg = {
+	.digestsize = SHA1_DIGEST_SIZE,
+	.init = xlp_auth_init,
+	.update = xlp_auth_update,
+	.final = xlp_sha1_final,
+	.descsize = sizeof (struct crypto_session),
+	.base = {
+		 .cra_name = "sha1",
+		 .cra_driver_name = "sha1-xlp",
+		 .cra_priority = XLP_AUTH_PRIORITY,
+		 .cra_flags = CRYPTO_ALG_TYPE_SHASH,
+		 .cra_blocksize = SHA1_BLOCK_SIZE,
+		 .cra_module = THIS_MODULE,
+		 }
+};
+
+static struct shash_alg md5_alg = {
+	.digestsize = MD5_DIGEST_SIZE,
+	.init = xlp_auth_init,
+	.update = xlp_auth_update,
+	.final = xlp_md5_final,
+	.descsize = sizeof (struct crypto_session),
+	.base = {
+		 .cra_name = "md5",
+		 .cra_driver_name = "md5-xlp",
+		 .cra_priority = XLP_AUTH_PRIORITY,
+		 .cra_flags = CRYPTO_ALG_TYPE_SHASH,
+		 .cra_blocksize = MD5_BLOCK_SIZE,
+		 .cra_module = THIS_MODULE,
+		 }
+};
+
+static struct shash_alg sha512_hmac_alg = {
+	.digestsize = SHA512_DIGEST_SIZE,
+	.init = xlp_auth_init,
+	.update = xlp_auth_update,
+	.final = xlp_hmac_sha512_final,
+	.descsize = sizeof (struct crypto_session),
+	.setkey = xlp_auth_setkey,
+	.base = {
+		 .cra_name = "hmac(sha512)",
+		 .cra_driver_name = "hmac-sha512-xlp",
+		 .cra_priority = XLP_HMAC_PRIORITY,
+		 .cra_flags = CRYPTO_ALG_TYPE_SHASH,
+		 .cra_blocksize = SHA512_BLOCK_SIZE,
+		 .cra_module = THIS_MODULE,
+		 .cra_ctxsize =
+		 (sizeof (struct crypto_session) +
+		  sizeof (struct crypto_param)),
+		 }
+};
+
+static struct shash_alg sha384_hmac_alg = {
+	.digestsize = SHA384_DIGEST_SIZE,
+	.init = xlp_auth_init,
+	.update = xlp_auth_update,
+	.final = xlp_hmac_sha384_final,
+	.descsize = sizeof (struct crypto_session),
+	.setkey = xlp_auth_setkey,
+	.base = {
+		 .cra_name = "hmac(sha384)",
+		 .cra_driver_name = "hmac-sha384-xlp",
+		 .cra_priority = XLP_HMAC_PRIORITY,
+		 .cra_flags = CRYPTO_ALG_TYPE_SHASH,
+		 .cra_blocksize = SHA384_BLOCK_SIZE,
+		 .cra_module = THIS_MODULE,
+		 .cra_ctxsize =
+		 (sizeof (struct crypto_session) +
+		  sizeof (struct crypto_param)),
+		 }
+};
+
+static struct shash_alg sha256_hmac_alg = {
+	.digestsize = SHA256_DIGEST_SIZE,
+	.init = xlp_auth_init,
+	.update = xlp_auth_update,
+	.final = xlp_hmac_sha256_final,
+	.descsize = sizeof (struct crypto_session),
+	.setkey = xlp_auth_setkey,
+	.base = {
+		 .cra_name = "hmac(sha256)",
+		 .cra_driver_name = "hmac-sha256-xlp",
+		 .cra_priority = XLP_HMAC_PRIORITY,
+		 .cra_flags = CRYPTO_ALG_TYPE_SHASH,
+		 .cra_blocksize = SHA256_BLOCK_SIZE,
+		 .cra_module = THIS_MODULE,
+		 .cra_ctxsize =
+		 (sizeof (struct crypto_session) +
+		  sizeof (struct crypto_param)),
+		 }
+};
+
+static struct shash_alg sha1_hmac_alg = {
+	.digestsize = SHA1_DIGEST_SIZE,
+	.init = xlp_auth_init,
+	.update = xlp_auth_update,
+	.final = xlp_hmac_sha1_final,
+	.descsize = sizeof (struct crypto_session),
+	.setkey = xlp_auth_setkey,
+	.base = {
+		 .cra_name = "hmac(sha1)",
+		 .cra_driver_name = "hmac-sha1-xlp",
+		 .cra_priority = XLP_HMAC_PRIORITY,
+		 .cra_flags = CRYPTO_ALG_TYPE_SHASH,
+		 .cra_blocksize = SHA1_BLOCK_SIZE,
+		 .cra_module = THIS_MODULE,
+		 .cra_ctxsize =
+		 (sizeof (struct crypto_session) +
+		  sizeof (struct crypto_param)),
+		 }
+};
+
+static struct shash_alg md5_hmac_alg = {
+	.digestsize = MD5_DIGEST_SIZE,
+	.init = xlp_auth_init,
+	.update = xlp_auth_update,
+	.final = xlp_hmac_md5_final,
+	.setkey = xlp_auth_setkey,
+	.descsize = sizeof (struct crypto_session),
+	.base = {
+		 .cra_name = "hmac(md5)",
+		 .cra_driver_name = "hmac-md5-xlp",
+		 .cra_priority = XLP_HMAC_PRIORITY,
+		 .cra_flags = CRYPTO_ALG_TYPE_SHASH,
+		 .cra_blocksize = MD5_BLOCK_SIZE,
+		 .cra_module = THIS_MODULE,
+		 .cra_ctxsize =
+		 (sizeof (struct crypto_session) +
+		  sizeof (struct crypto_param)),
+		 }
+};
+
+static struct shash_alg sha256_128_hmac_alg = {
+	.digestsize = SHA256_DIGEST_SIZE,
+	.init = xlp_auth_init,
+	.update = xlp_auth_update,
+	.final = xlp_hmac_sha256_128_final,
+	.descsize = sizeof (struct crypto_session),
+	.setkey = xlp_auth_setkey,
+	.base = {
+		 .cra_name = "hmac(sha256-128)",
+		 .cra_driver_name = "hmac-sha256-128-xlp",
+		 .cra_priority = XLP_HMAC_PRIORITY,
+		 .cra_flags = CRYPTO_ALG_TYPE_SHASH,
+		 .cra_blocksize = SHA256_BLOCK_SIZE,
+		 .cra_module = THIS_MODULE,
+		 .cra_ctxsize =
+		 (sizeof (struct crypto_session) +
+		  sizeof (struct crypto_param)),
+		 }
+};
+
+static struct shash_alg sha1_96_hmac_alg = {
+	.digestsize = SHA1_DIGEST_SIZE,
+	.init = xlp_auth_init,
+	.update = xlp_auth_update,
+	.final = xlp_hmac_sha1_96_final,
+	.descsize = sizeof (struct crypto_session),
+	.setkey = xlp_auth_setkey,
+	.base = {
+		 .cra_name = "hmac(sha1-96)",
+		 .cra_driver_name = "hmac-sha1-96-xlp",
+		 .cra_priority = XLP_HMAC_PRIORITY,
+		 .cra_flags = CRYPTO_ALG_TYPE_SHASH,
+		 .cra_blocksize = SHA1_BLOCK_SIZE,
+		 .cra_module = THIS_MODULE,
+		 .cra_ctxsize =
+		 (sizeof (struct crypto_session) +
+		  sizeof (struct crypto_param)),
+		 }
+};
+
+static struct shash_alg md5_96_hmac_alg = {
+	.digestsize = MD5_DIGEST_SIZE,
+	.init = xlp_auth_init,
+	.update = xlp_auth_update,
+	.final = xlp_hmac_md5_96_final,
+	.setkey = xlp_auth_setkey,
+	.descsize = sizeof (struct crypto_session),
+	.base = {
+		 .cra_name = "hmac(md5-96)",
+		 .cra_driver_name = "hmac-md5-96-xlp",
+		 .cra_priority = XLP_HMAC_PRIORITY,
+		 .cra_flags = CRYPTO_ALG_TYPE_SHASH,
+		 .cra_blocksize = MD5_BLOCK_SIZE,
+		 .cra_module = THIS_MODULE,
+		 .cra_ctxsize =
+		 (sizeof (struct crypto_session) +
+		  sizeof (struct crypto_param)),
+		 }
+};
+
+int
+xlp_auth_alg_init(void)
+{
+	int rc = -ENODEV;
+
+	rc = crypto_register_shash(&sha1_alg);
+	if (rc)
+		goto out;
+	rc = crypto_register_shash(&sha256_alg);
+	if (rc)
+		goto out_unreg1;
+
+	rc = crypto_register_shash(&sha384_alg);
+	if (rc)
+		goto out_unreg2;
+
+	rc = crypto_register_shash(&sha512_alg);
+	if (rc)
+		goto out_unreg3;
+
+	rc = crypto_register_shash(&md5_alg);
+	if (rc)
+		goto out_unreg4;
+
+	rc = crypto_register_shash(&sha1_hmac_alg);
+	if (rc)
+		goto out_unreg5;
+
+	rc = crypto_register_shash(&sha256_hmac_alg);
+	if (rc)
+		goto out_unreg6;
+
+	rc = crypto_register_shash(&sha384_hmac_alg);
+	if (rc)
+		goto out_unreg7;
+
+	rc = crypto_register_shash(&sha512_hmac_alg);
+	if (rc)
+		goto out_unreg8;
+	rc = crypto_register_shash(&md5_hmac_alg);
+	if (rc)
+		goto out_unreg9;
+	rc = crypto_register_shash(&sha256_128_hmac_alg);
+	if (rc)
+		goto out_unreg10;
+
+	rc = crypto_register_shash(&sha1_96_hmac_alg);
+	if (rc)
+		goto out_unreg11;
+	rc = crypto_register_shash(&md5_96_hmac_alg);
+	if (rc)
+		goto out_unreg12;
+	//printk("Some of the FIPS test failed as the maximum key length supported is 64 bytes.\n");
+
+	printk(KERN_NOTICE "Using XLP hardware for SHA/MD5 algorithms.\n");
+
+	return 0;
+
+      out_unreg12:
+	crypto_unregister_shash(&md5_96_hmac_alg);
+      out_unreg11:
+	crypto_unregister_shash(&sha256_128_hmac_alg);
+      out_unreg10:
+	crypto_unregister_shash(&sha1_96_hmac_alg);
+      out_unreg9:
+	crypto_unregister_shash(&md5_hmac_alg);
+      out_unreg8:
+	crypto_unregister_shash(&sha512_hmac_alg);
+      out_unreg7:
+	crypto_unregister_shash(&sha384_hmac_alg);
+      out_unreg6:
+	crypto_unregister_shash(&sha256_hmac_alg);
+      out_unreg5:
+	crypto_unregister_shash(&sha1_hmac_alg);
+      out_unreg4:
+	crypto_unregister_shash(&md5_alg);
+      out_unreg3:
+	crypto_unregister_shash(&sha512_alg);
+      out_unreg2:
+	crypto_unregister_shash(&sha384_alg);
+      out_unreg1:
+	crypto_unregister_shash(&sha256_alg);
+      out:
+	crypto_unregister_shash(&sha1_alg);
+	printk(KERN_ERR "\nError: XLP SHA/MD5 initialization failed.");
+	return rc;
+
+}
+
+void
+xlp_auth_alg_fini(void)
+{
+	crypto_unregister_shash(&sha1_alg);
+	crypto_unregister_shash(&sha256_alg);
+	crypto_unregister_shash(&sha384_alg);
+	crypto_unregister_shash(&sha512_alg);
+	crypto_unregister_shash(&md5_alg);
+	crypto_unregister_shash(&sha1_hmac_alg);
+	crypto_unregister_shash(&sha256_hmac_alg);
+	crypto_unregister_shash(&sha384_hmac_alg);
+	crypto_unregister_shash(&sha512_hmac_alg);
+	crypto_unregister_shash(&md5_hmac_alg);
+	crypto_unregister_shash(&sha1_96_hmac_alg);
+	crypto_unregister_shash(&sha256_128_hmac_alg);
+	crypto_unregister_shash(&md5_96_hmac_alg);
+}
+
+EXPORT_SYMBOL(xlp_auth_alg_init);
+EXPORT_SYMBOL(xlp_auth_alg_fini);
diff --git a/drivers/crypto/sae/nlm_crypto.c b/drivers/crypto/sae/nlm_crypto.c
new file mode 100644
index 0000000..3f8460a
--- /dev/null
+++ b/drivers/crypto/sae/nlm_crypto.c
@@ -0,0 +1,1022 @@
+/***********************************************************************
+Copyright 2003-2010 Netlogic Microsystems (“Netlogic”). All rights
+reserved.
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are
+met:
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+*****************************#NETL_2#********************************/
+
+//#define DEBUG 
+
+#include <linux/module.h>
+
+//#define KERNEL
+#include <linux/types.h>
+#include <linux/fs.h>
+#include <linux/mm.h>
+#include <linux/errno.h>	/* for -EBUSY */
+#include <linux/ioport.h>	/* for request_region */
+#include <linux/delay.h>	/* for loops_per_jiffy */
+#include <linux/sched.h>
+#include <linux/smp_lock.h>	/* cycle_kernel_lock() */
+#include <asm/io.h>		/* for inb_p, outb_p, inb, outb, etc. */
+#include <asm/uaccess.h>	/* for get_user, etc. */
+#include <linux/wait.h>		/* for wait_queue */
+#include <linux/init.h>		/* for __init, module_{init,exit} */
+#include <linux/poll.h>		/* for POLLIN, etc. */
+#include <asm/netlogic/msgring.h>
+
+#include <asm/netlogic/hal/nlm_hal_fmn.h>
+#include <asm/netlogic/hal/nlm_hal_macros.h>
+#include "nlm_crypto_api.h"
+#include <asm/netlogic/hal/nlm_hal_sae.h>
+//#include "nlm_crypto_drv.h"
+#include "nlm_crypto.h"
+#include "nlm_crypto_data.h"
+
+#define XLP_POLLING 1
+#ifdef TRACING
+#define TRACE_TEXT(str) printk(str);
+#define TRACE_RET printk(")")
+#else				/* !TRACING */
+#define TRACE_TEXT(str) ((void) 0)
+#define TRACE_RET ((void) 0)
+#endif				/* TRACING */
+
+//#define dbg printk(KERN_ERR "\n %s %d",__FUNCTION__, __LINE__)
+#define dbg 
+
+static int xlp_sae_major;
+static int xlp_sae_open(struct inode *, struct file *);
+static int xlp_sae_release(struct inode *, struct file *);
+static int nlm_fb_vc;
+static int volatile crypto_status;
+static uint64_t nlm_tx_id, nlm_err_msg;
+wait_queue_head_t wait;
+
+#define NLM_CRYPTO_OP_IN_PROGRESS 0
+#define NLM_CRYPTO_OP_DONE	  1
+
+/**
+* @file_name crypto.c
+*/
+
+/**
+* @defgroup crypto Crypto API
+* @brief Description about the crypto apis
+*/
+
+//#define printf(a, b...) printk(KERN_ERR a, ##b)
+#define printf(a, b...)
+#define malloc(a) kmalloc(a, GFP_ATOMIC)
+#define free kfree
+
+#define shift_lower_bits(x, bitshift, numofbits) (((unsigned long long)(x) & ((1ULL << (numofbits)) - 1)) << (bitshift))
+#define xtract_bits(x, bitpos, numofbits) ((x) >> (bitpos) & ((1ULL << (numofbits)) - 1))
+
+static void *get_cache_aligned_mem(size_t size);
+static void free_cache_aligned_mem(void *addr);
+
+#define VC_MODE_ROUND_ROBIN 1
+#define NUM_VC 16
+
+extern int xlp_aead_alg_init();
+extern void xlp_aead_alg_fini();
+extern int xlp_crypt_alg_init();
+extern void xlp_crypt_alg_fini();
+extern int xlp_auth_alg_init();
+extern void xlp_auth_alg_fini();
+/********************* H/W Counters  *******************************/
+
+#if 0
+struct crypto_hw_cntrs {
+	uint64_t ingress_msg_cntr;
+	uint64_t egress_msg_cntr;
+	uint64_t egress_err_msg_cntr;
+	uint64_t dma_rd_bytes[NUM_VC];
+	uint64_t dma_wr_bytes[NUM_VC];
+	uint64_t cipher_bytes[NUM_VC];
+	uint64_t hash_bytes[NUM_VC];
+	uint64_t op_cntr_0[NUM_VC];
+	uint64_t op_cntr_1[NUM_VC];
+	uint64_t op_cntr_2[NUM_VC];
+	uint64_t op_cntr_3[NUM_VC];
+};
+
+static void
+nlm_crypto_print_hw_cntrs(struct crypto_hw_cntrs *hw_cnt)
+{
+	int i;
+
+	printf("***************** h/w counter dump start *****************n");
+	printf("Ingress msg cntr: 0x%llx\n", (unsigned long long)
+	       hw_cnt->ingress_msg_cntr);
+	printf("Egress msg cntr: 0x%llx\n", (unsigned long long)
+	       hw_cnt->egress_msg_cntr);
+	printf("Egress error msg cntr: 0x%llx\n", (unsigned long long)
+	       hw_cnt->egress_err_msg_cntr);
+
+	for (i = 0; i < NUM_VC; i++) {
+		printf("================ VC %d counter ===============\n", i);
+
+		printf("DMA read bytes: 0x%llx\n", (unsigned long long)
+		       hw_cnt->dma_rd_bytes[i]);
+		printf("DMA write bytes: 0x%llx\n", (unsigned long long)
+		       hw_cnt->dma_wr_bytes[i]);
+		printf("Cipher bytes: 0x%llx\n", (unsigned long long)
+		       hw_cnt->cipher_bytes[i]);
+		printf("Hash bytes: 0x%llx\n", (unsigned long long)
+		       hw_cnt->hash_bytes[i]);
+		printf("Operation counter 0: 0x%llx\n", (unsigned long long)
+		       hw_cnt->op_cntr_0[i]);
+		printf("Operation counter 1: 0x%llx\n", (unsigned long long)
+		       hw_cnt->op_cntr_1[i]);
+		printf("Operation counter 2: 0x%llx\n", (unsigned long long)
+		       hw_cnt->op_cntr_2[i]);
+		printf("Operation counter 3: 0x%llx\n", (unsigned long long)
+		       hw_cnt->op_cntr_3[i]);
+	}
+	printf("***************** h/w counter dump end *****************n");
+}
+
+static uint64_t
+nlm_crypto_read_reg_lo_hi(int regnum_lo, int regnum_hi)
+{
+	return ((unsigned long long) read_crypto_reg(regnum_lo) |
+		((unsigned long long) read_crypto_reg(regnum_hi) << 32));
+}
+
+static void
+nlm_crypto_read_hw_counters(struct crypto_hw_cntrs *hw_cnt)
+{
+	int i;
+
+	hw_cnt->ingress_msg_cntr =
+	    nlm_crypto_read_reg_lo_hi(NLM_CRYPTO_REG_ING_MSG_CNTR_LOW,
+				      NLM_CRYPTO_REG_ING_MSG_CNTR_HIGH);
+
+	hw_cnt->egress_msg_cntr =
+	    nlm_crypto_read_reg_lo_hi(NLM_CRYPTO_REG_EGR_MSG_CNTR_LOW,
+				      NLM_CRYPTO_REG_EGR_MSG_CNTR_HIGH);
+
+	hw_cnt->egress_err_msg_cntr =
+	    nlm_crypto_read_reg_lo_hi(NLM_CRYPTO_REG_EGR_ERR_MSG_CNTR_LOW,
+				      NLM_CRYPTO_REG_EGR_ERR_MSG_CNTR_HIGH);
+
+	for (i = 0; i < NUM_VC; i++) {
+		hw_cnt->dma_rd_bytes[i] =
+		    nlm_crypto_read_reg_lo_hi(NLM_CRYPTO_REG_DMA_RD_LOW_BYTES
+					      (i),
+					      NLM_CRYPTO_REG_DMA_RD_HIGH_BYTES
+					      (i));
+
+		hw_cnt->dma_wr_bytes[i] =
+		    nlm_crypto_read_reg_lo_hi(NLM_CRYPTO_REG_DMA_WR_LOW_BYTES
+					      (i),
+					      NLM_CRYPTO_REG_DMA_WR_HIGH_BYTES
+					      (i));
+
+		hw_cnt->cipher_bytes[i] =
+		    nlm_crypto_read_reg_lo_hi(NLM_CRYPTO_REG_CIPHER_LOW_BYTES
+					      (i),
+					      NLM_CRYPTO_REG_CIPHER_HIGH_BYTES
+					      (i));
+
+		hw_cnt->hash_bytes[i] =
+		    nlm_crypto_read_reg_lo_hi(NLM_CRYPTO_REG_HASH_LOW_BYTES(i),
+					      NLM_CRYPTO_REG_HASH_HIGH_BYTES
+					      (i));
+
+		hw_cnt->op_cntr_0[i] =
+		    nlm_crypto_read_reg_lo_hi(NLM_CRYPTO_REG_OP_CNTR_0_LOW_BYTES
+					      (i),
+					      NLM_CRYPTO_REG_OP_CNTR_0_HIGH_BYTES
+					      (i));
+
+		hw_cnt->op_cntr_1[i] =
+		    nlm_crypto_read_reg_lo_hi(NLM_CRYPTO_REG_OP_CNTR_1_LOW_BYTES
+					      (i),
+					      NLM_CRYPTO_REG_OP_CNTR_1_HIGH_BYTES
+					      (i));
+
+		hw_cnt->op_cntr_2[i] =
+		    nlm_crypto_read_reg_lo_hi(NLM_CRYPTO_REG_OP_CNTR_2_LOW_BYTES
+					      (i),
+					      NLM_CRYPTO_REG_OP_CNTR_2_HIGH_BYTES
+					      (i));
+
+		hw_cnt->op_cntr_3[i] =
+		    nlm_crypto_read_reg_lo_hi(NLM_CRYPTO_REG_OP_CNTR_3_LOW_BYTES
+					      (i),
+					      NLM_CRYPTO_REG_OP_CNTR_3_HIGH_BYTES
+					      (i));
+
+	}
+}
+
+/********************* S/W Counters  *******************************/
+uint64_t crypto_cntr_crypt_ops[NLM_CIPHER_MAX][NLM_CIPHER_MODE_MAX];
+uint64_t crypto_cntr_crypt_bytes[NLM_CIPHER_MAX][NLM_CIPHER_MODE_MAX];
+uint64_t crypto_cntr_auth_ops[NLM_AUTH_MAX][NLM_AUTH_MODE_MAX];
+uint64_t crypto_cntr_auth_bytes[NLM_AUTH_MAX][NLM_AUTH_MODE_MAX];
+
+static void
+print_crypto_stats(void)
+{
+	// Take care of invalid modes...
+}
+
+static void
+update_crypto_stats(int cipher_alg, int cipher_mode, int cipher_len,
+		    int auth_alg, int auth_mode, int auth_len)
+{
+	if (cipher_alg) {
+		crypto_cntr_crypt_ops[cipher_alg][cipher_mode]++;
+		crypto_cntr_crypt_bytes[cipher_alg][cipher_mode] += cipher_len;
+	}
+	if (auth_alg) {
+		crypto_cntr_auth_ops[auth_alg][auth_mode]++;
+		crypto_cntr_auth_ops[auth_alg][auth_mode] += auth_len;
+	}
+}
+
+static void
+update_session_stats(struct nlm_crypto_op *cop)
+{
+	update_crypto_stats(cop->session->cipher_alg,
+			    cop->session->cipher_mode,
+			    cop->cipher_len,
+			    cop->session->auth_alg,
+			    cop->session->auth_mode, cop->auth_len);
+}
+
+static void
+reset_crypto_stats(void)
+{
+	int i, j;
+
+	for (i = 0; i < NLM_CIPHER_MAX; i++) {
+		for (j = 0; j < NLM_CIPHER_MODE_MAX; j++) {
+			crypto_cntr_crypt_ops[i][j] = 0;
+			crypto_cntr_crypt_bytes[i][j] = 0;
+		}
+	}
+
+	for (i = 0; i < NLM_AUTH_MAX; i++) {
+		for (j = 0; j < NLM_AUTH_MODE_MAX; j++) {
+			crypto_cntr_auth_ops[i][j] = 0;
+			crypto_cntr_auth_ops[i][j] = 0;
+		}
+	}
+}
+#endif
+
+struct dev_data {
+	void *crypto_vc;
+	int crypto_fb_vc_base;
+};
+
+static struct dev_data dd;
+
+static void
+init_crypto_vc(void)
+{
+	dd.crypto_fb_vc_base = 1;
+}
+
+static int
+crypto_get_vc(void)
+{
+	return NLM_CRYPTO_VC_BASE;
+}
+
+static int
+crypto_get_fb_vc(void)
+{
+	int vc, cpu;
+
+	vc = dd.crypto_fb_vc_base;
+
+	cpu = hard_smp_processor_id();	//processor_id();
+	cpu = cpu * 4 + vc;
+
+	return cpu;
+}
+
+/****************** Validate input, fill fields, alloc mem***************************/
+
+//no checking for overflow
+int
+nlm_crypto_cipher_alg_has_mode(unsigned int cipher_alg)
+{
+dbg;
+	if (cipher_alg >= NLM_CIPHER_MAX)
+		return 0;
+	return cipher_alg_needs_mode[cipher_alg];
+}
+
+//no checking for overflow
+int
+nlm_crypto_auth_alg_has_mode(unsigned int auth_alg)
+{
+dbg;
+	if (auth_alg >= NLM_AUTH_MAX)
+		return 0;
+	return auth_alg_needs_mode[auth_alg];
+}
+
+static int
+validate_cipher_alg_mode(struct crypto_session *session)
+{
+dbg;
+	if (session->cip) {
+dbg;
+		if (session->cip->cipher_alg != NLM_CIPHER_BYPASS) {
+dbg;
+			if (session->cip->cipher_alg >= NLM_CIPHER_MAX)
+				return NLM_ERR_CIPHER_ALG_INVALID;
+dbg;
+
+			if (nlm_crypto_cipher_alg_has_mode
+			    (session->cip->cipher_alg)) {
+dbg;
+				if ((session->cip->cipher_mode >=
+				     NLM_CIPHER_MODE_MAX)
+				    ||
+				    !(cipher_mode_valid
+				      [session->cip->cipher_alg]
+				      [session->cip->cipher_mode]))
+
+					return NLM_ERR_CIPHER_ALG_MODE_INVALID;
+			}
+		}
+	}
+	return 0;
+}
+
+static int
+validate_auth_alg_mode(struct crypto_session *session)
+{
+dbg;
+	if (session->aip) {
+dbg;
+		if (session->aip->auth_alg != NLM_AUTH_BYPASS) {
+dbg;
+			if (session->aip->auth_alg >= NLM_AUTH_MAX)
+				return NLM_ERR_AUTH_ALG_INVALID;
+dbg;
+			if (nlm_crypto_auth_alg_has_mode
+			    (session->aip->auth_alg)) {
+dbg;
+				if ((session->aip->auth_mode >=
+				     NLM_AUTH_MODE_MAX)
+				    || !(auth_mode_valid[session->aip->auth_alg]
+					 [session->aip->auth_mode]))
+
+					return NLM_ERR_AUTH_ALG_MODE_INVALID;
+			}
+		}
+	}
+	return 0;
+}
+
+static int
+validate_alg_mode_key_size(struct crypto_session *session)
+{
+	int ret = 0;
+dbg;
+	if (session->cip) {
+dbg;
+		ret = validate_cipher_alg_mode(session);
+dbg;
+		if (!ret && session->cip->cipher_key) {
+dbg;
+			if (session->cip->cipher_key->iov_len !=
+			    cipher_mode_key_len[session->cip->
+						cipher_alg][session->
+							    cip->cipher_mode])
+				return NLM_ERR_CIPHER_KEY_LEN_INVALID;
+		}
+dbg;
+	}
+	if (!ret && session->aip) {
+dbg;
+		ret = validate_auth_alg_mode(session);
+dbg;
+		if (!ret && session->aip->auth_key) {
+dbg;
+			if (session->aip->auth_key->iov_len >
+			    auth_mode_key_len[session->aip->auth_alg][session->
+								      aip->auth_mode])
+				return NLM_ERR_AUTH_KEY_LEN_INVALID;
+		}
+	}
+	return ret;
+}
+
+#define L3_CACHELINE_SIZE 64
+#define L3_CACHELINE_MASK 0x3fULL
+#define NUM_64BIT_LOCATIONS 16
+#define NUM_128BIT_LOCATIONS 8
+#define NUM_256BIT_LOCATIONS 4
+
+static int
+crypto_fb_msg_print_err(uint64_t msg)
+{
+	uint32_t err, ret = 0;
+	err = (uint32_t) msg;
+
+	if (err & NLM_CRYPTO_FB_ERR_MASK) {
+		ret |= NLM_CRYPTO_FB_ERR_MASK printf("        Force bypass \n");
+	}
+	if (err & NLM_CRYPTO_FB_DATA_ERR_SW_DESC) {
+		ret |= NLM_CRYPTO_FB_DATA_ERR_SW_DESC;
+		printf("        SW descriptor \n");
+	}
+	if (err & NLM_CRYPTO_FB_DATA_ERR_INSUFFICIENT_DATA_TO_AUTH) {
+		ret |= NLM_CRYPTO_FB_DATA_ERR_INSUFFICIENT_DATA_TO_AUTH;
+		printf("        Insufficient data to authenticate \n");
+	}
+	if (err & NLM_CRYPTO_FB_DATA_ERR_INSUFFICIENT_DATA_TO_CIPHER) {
+		ret |= NLM_CRYPTO_FB_DATA_ERR_INSUFFICIENT_DATA_TO_CIPHER;
+		printf("        Insufficient data to cipher \n");
+	}
+	if (err & NLM_CRYPTO_FB_DATA_ERR_NO_PKT_DESC) {
+		ret |= NLM_CRYPTO_FB_DATA_ERR_NO_PKT_DESC;
+		printf("        No pkt descriptor \n");
+	}
+	if (err & NLM_CRYPTO_FB_DATA_ERR_ECC_SRC_DST) {
+		ret |= NLM_CRYPTO_FB_DATA_ERR_ECC_SRC_DST;
+		printf("        ECC error in src or dst\n");
+	}
+	if (err & NLM_CRYPTO_FB_DATA_ERR_ECC_DESC_0123) {
+		ret |= NLM_CRYPTO_FB_DATA_ERR_ECC_DESC_0123;
+		printf("        ECC error in descriptor 0123\n");
+	}
+	if (err & NLM_CRYPTO_FB_DATA_ERR_ECC_AUTH_KEY) {
+		ret |= NLM_CRYPTO_FB_DATA_ERR_ECC_AUTH_KEY;
+		printf("        ECC error in auth key\n");
+	}
+	if (err & NLM_CRYPTO_FB_DATA_ERR_ECC_CIPHER_KEY) {
+		ret |= NLM_CRYPTO_FB_DATA_ERR_ECC_CIPHER_KEY;
+		printf("        ECC error in cipher key\n");
+	}
+	if (err & NLM_CRYPTO_FB_DATA_ERR_ECC_CNTL_DESC) {
+		ret |= NLM_CRYPTO_FB_DATA_ERR_ECC_CNTL_DESC;
+		printf("        ECC error in CNTL descriptor\n");
+	}
+	if (err & NLM_CRYPTO_FB_DATA_ERR_ECC_DATA) {
+		ret |= NLM_CRYPTO_FB_DATA_ERR_ECC_DATA;
+		printf("        ECC error in data\n");
+	}
+	if (err & NLM_CRYPTO_FB_DATA_ERR_ECC_DESIGNER_FB) {
+		ret |= NLM_CRYPTO_FB_DATA_ERR_ECC_DESIGNER_FB;
+		printf("        ECC error in designed freeback descriptor\n");
+	}
+	return ret;
+}
+
+static int
+create_session(struct crypto_session *session)
+{
+	void *tmp_cntrl_desc = NULL;
+	struct nlm_crypto_cipher_init_param cip, *tmp_cip = NULL;
+	struct nlm_crypto_auth_init_param aip, *tmp_aip = NULL;
+	unsigned int key_len;
+
+dbg;
+	memset(&cip, 0, sizeof (struct nlm_crypto_cipher_init_param));
+dbg;
+	memset(&aip, 0, sizeof (struct nlm_crypto_auth_init_param));
+dbg;
+
+	if (!session->flag)
+		session->cntrl_desc =
+		    get_cache_aligned_mem(NLM_CRYPTO_SIZEOF_CNTRL_DESC);
+
+dbg;
+	tmp_cntrl_desc = session->cntrl_desc;
+	if (tmp_cntrl_desc == NULL) {
+		printk(KERN_ERR "\nError: session->cntrl_desc is NULL");
+		return NLM_ERR_INVALID_PARAM;
+	}
+dbg;
+	if (session->cip) {
+		cip.cipher_alg = session->cip->cipher_alg;
+		cip.cipher_mode = session->cip->cipher_mode;
+
+dbg;
+		if (session->cip->cipher_key) {
+			cip.cipher_key =
+			    (struct nlm_iovec *) session->cip->cipher_key;
+		}
+		cip.arc4_cipher_key_len = session->cip->arc4_cipher_key_len;
+		cip.arc4_key_init = session->cip->arc4_key_init;
+		cip.cfb_mask = session->cip->cfb_mask;
+		tmp_cip = &cip;
+	}
+dbg;
+	if (session->aip) {
+		aip.auth_alg = session->aip->auth_alg;
+		aip.auth_mode = session->aip->auth_mode;
+		if (session->aip->auth_key) {
+dbg;
+			aip.auth_key =
+			    (struct nlm_iovec *)
+			    malloc(sizeof (struct nlm_iovec));
+			if (aip.auth_key == NULL) {
+				printk(KERN_ERR
+				       "\n Malloc failed for aip.auth_key");
+				return NLM_ERR_NO_MEM;
+			}
+
+dbg;
+			key_len =
+			    auth_mode_key_len[session->aip->auth_alg][session->
+								      aip->
+								      auth_mode];
+//			key_len = session->aip->auth_key->iov_len;
+//printk(KERN_ERR "\n key_len = %d", key_len);
+			aip.auth_key->buf =
+			    (unsigned char *) malloc(key_len + 1);
+dbg;
+			if (aip.auth_key->buf == NULL) {
+				printk(KERN_ERR
+				       "\n Malloc failed for aip.auth_key->buf");
+				return NLM_ERR_NO_MEM;
+			}
+dbg;
+			memset(aip.auth_key->buf, 0, key_len);
+			memcpy(aip.auth_key->buf, session->aip->auth_key->buf,
+			       session->aip->auth_key->iov_len);
+dbg;
+			aip.auth_key->iov_len = key_len;
+			//session->aip->auth_key->iov_len = key_len;
+			aip.hmac = session->aip->hmac;
+		}
+
+		tmp_aip = &aip;
+	}
+dbg;
+	nlm_hal_crypto_preprocess_request((void *) tmp_cntrl_desc, tmp_cip,
+					  tmp_aip);
+	if (session->aip->auth_key) {
+		free(aip.auth_key);
+		free(aip.auth_key->buf);
+	}
+
+	return 0;
+
+}
+
+/**
+ * @brief This API sets up session for performing cipher and authentication
+ * operation.
+ *
+ * @param[in] session_cntx User allocates memory for session context and passes
+ * the pointer to this structure. This structure is internally filled in by
+ * driver with the provided parameters. Also, validation checks are run on the
+ * input parameters for correctness. Application passes this data structure to
+ * subsequent cipher and auth apis.
+ * @param[in] cipher_alg cipher algorithm
+ * @param[in] cipher_mode cipher mode
+ * @param[in] cipher_key cipher key
+ * @param[in] additional_cipher_info additional cipher information. Valid for
+ * ARC4 and other ciphers which need additional information.
+ * @param[in] auth_alg authentication algorithm
+ * @param[in] auth_mode authentication mode
+ * @param[in] auth_key authentication key. NULL in case of MD5 and SHA without
+ * hmac.
+ * @param[in] tag_len length of the tag which is to be returned on tag
+ * computation.
+ *
+ * @return Return value of 0 indicates success. 
+ * Any other error code in case of failure.
+ *
+ * @sa (see also)
+ *
+ * @ingroup crypto
+ */
+int
+crypto_setup_cipher_auth_session(struct crypto_session *session)
+{
+	int ret = 0;
+dbg;
+	if (!session)
+		return NLM_ERR_INVALID_PARAM;
+
+dbg;
+	if (session->flag
+	    && (session->cntrl_desc == NULL || session->pkt_desc == NULL)) {
+		printk(KERN_ERR
+		       "\nError: session->cntrl_desc/pkt_desc is NULL");
+		return NLM_ERR_INVALID_PARAM;
+	}
+dbg;
+	if (session->aip) {
+		if (session->aip->hmac
+		    && (session->aip->auth_key == NULL
+			|| session->aip->auth_alg == NLM_AUTH_BYPASS)) {
+			crypto_cleanup_session(session);
+			return NLM_ERR_INVALID_PARAM;
+		}
+	}
+dbg;
+	ret = validate_alg_mode_key_size(session);
+	if (ret) {
+dbg;
+		crypto_cleanup_session(session);
+		return ret;
+	}
+dbg;
+	return create_session(session);
+}
+
+/**
+ * @brief This API resets key for an already configured session.
+ *
+ * @param[in] session Specifies the session which is setup by calling
+ * one of the session setup apis..
+ *
+ * @param[in] cipher_key New cipher key. For auth only sessions , this parameter
+ * is ignored.
+ * @param[in] auth_key New auth key. For cipher only sessions , this parameter
+ * is ignored.
+ *
+ * @return Return value of 0 indicates success. 
+ * Any other error code in case of failure.
+ *
+ * @sa (see also)
+ *
+ * @ingroup crypto
+ */
+int
+crypto_session_reset_key(struct crypto_session *session, unsigned char
+			 *cipher_key, unsigned char *auth_key)
+{
+	if (cipher_key && session->cip)
+		memcpy(session->cip->cipher_key->buf, cipher_key,
+		       session->cip->cipher_key->iov_len);
+	if (auth_key && session->aip)
+		memcpy(session->aip->auth_key->buf, auth_key,
+		       session->aip->auth_key->iov_len);
+	return create_session(session);
+
+	return 0;
+}
+
+/**
+ * @brief This API cleans up internal driver memory associated with a session.
+ *
+ * @param[in] session Specifies the session which is setup by calling
+ * one of the session setup apis..
+ *
+ * @return Return value of 0 indicates success. 
+ * Any other error code in case of failure.
+ *
+ * @sa (see also)
+ *
+ * @ingroup crypto
+ */
+int
+crypto_cleanup_session(struct crypto_session *session)
+{
+	if (!session)
+		return NLM_ERR_INVALID_PARAM;
+
+	if (session->flag == 0) {
+		if (session->cntrl_desc)
+		{
+			free_cache_aligned_mem(session->cntrl_desc);
+			session->cntrl_desc = NULL;
+		}
+		if (session->pkt_desc)
+		{
+			free_cache_aligned_mem(session->pkt_desc);
+			session->pkt_desc = NULL;
+		}
+	}
+	return 0;
+}
+
+/******************** cipher auth api ****************************/
+/*
+// Only sessions are supported
+User allocates cop, fills in the required fields and calls the API. 
+In this API, first field of cop points to session which is already
+established.
+Return value of 0 indicates success. NLM_ERR_IN_PROGRESS in case the validation 
+is successful and async operation is not yet completed.
+Any other error code in case of failure.
+*/
+
+uint64_t err_msg = 0;
+int
+crypto_cipher_auth_op(struct crypto_session *session, struct crypto_param *cop)
+{
+	int ret = 0, mflags, i;
+	struct crypto_session *sess = session;
+	struct nlm_crypto_param cprm;
+
+//printk(KERN_ERR "\n crypto_cipher_auth_op"); 
+	if (!cop)
+		return NLM_ERR_CRYPTO_PARAM_NULL;
+
+	if (!sess)
+		return NLM_ERR_SESSION_NOT_SETUP;
+
+	memset(&cprm, 0, sizeof (struct nlm_crypto_param));
+	
+	if (!session->flag)
+		session->pkt_desc = get_cache_aligned_mem(NLM_CRYPTO_SIZEOF_PKT_DESC(cop->nr_frags));
+
+	cprm.enc = cop->enc;
+
+	/* cprm.src_phy = virt_to_phys (cop->src->buf);
+	   cprm.dst_phy = virt_to_phys (cop->dst->buf); */
+	cprm.src_len = cop->src_len;
+
+	cprm.src_phy =
+	    (struct nlm_iovec *) malloc(sizeof (struct nlm_iovec) *
+					cop->nr_frags);
+	cprm.dst_phy =
+	    (struct nlm_iovec *) malloc(sizeof (struct nlm_iovec) *
+					cop->nr_frags);
+	if (cprm.src_phy == NULL || cprm.dst_phy == NULL) {
+		printk(KERN_ERR "\n Mallof failed for src_phy/dst_phy");
+		return NLM_ERR_NO_MEM;
+	}
+
+	i = 0;
+	while (i < cop->nr_frags) {
+		cprm.src_phy[i].buf =
+		    (unsigned char *) virt_to_phys(cop->src[i].buf);
+		cprm.dst_phy[i].buf =
+		    (unsigned char *) virt_to_phys(cop->dst[i].buf);
+		cprm.src_phy[i].iov_len = cop->src[i].iov_len;
+		cprm.dst_phy[i].iov_len = cop->dst[i].iov_len;
+
+		i++;
+	}
+
+	if (session->cip) {
+		if (cop->iv_len !=
+		    cipher_mode_iv_len[session->cip->cipher_alg][session->
+								 cip->cipher_mode])
+			return NLM_ERR_CIPHER_IV_LEN_INVALID;
+	}
+
+	cprm.cipher_len = cop->cipher_len;
+	cprm.iv_len = cop->iv_len;
+	cprm.iv = cop->iv;
+
+	cprm.iv_offset = cop->iv_offset;
+	cprm.cipher_offset = cop->cipher_offset;
+	cprm.cipher_bit_count = cop->cipher_bit_count;
+
+	cprm.hash_bit_count = cop->hash_bit_count;
+	cprm.tls_proto = cop->tls_proto;
+	cprm.arc4_save_l3_alloc = cop->arc4_save_l3_alloc;
+	cprm.arc4_save_state = cop->arc4_save_state;
+	cprm.arc4_load_state = cop->arc4_load_state;
+	cprm.hmac_external_pad_key = cop->hmac_external_pad_key;
+	cprm.hash_clobber = cop->hash_clobber;
+	cprm.cipher_clobber = cop->cipher_clobber;
+	cprm.hash_output_l3_alloc = cop->hash_output_l3_alloc;
+	cprm.cipher_output_l3_alloc = cop->cipher_output_l3_alloc;
+	cprm.hash_source = cop->hash_source;
+	cprm.send_designer_fb = cop->send_designer_fb;
+
+	cprm.auth_len = cop->auth_len;
+	cprm.auth_offset = cop->auth_offset;
+	cprm.tag_len = cop->tag_len * 8;
+
+	cprm.cipher_key_len = cop->cipher_key_len;
+	cprm.auth_key_len = cop->auth_key_len;
+	cprm.hash_dst_address_phy = virt_to_phys(cop->hash_dst_address);
+	cprm.nr_frags = cop->nr_frags;
+
+	cprm.designer_freeback_id = cop->designer_freeback_id;
+	cprm.designer_freeback_len = cop->designer_freeback_len;
+	cprm.designer_fb[0] = cop->designer_fb[0];
+	cprm.designer_fb[1] = cop->designer_fb[1];
+	cprm.designer_fb[2] = cop->designer_fb[2];
+	cprm.designer_fb[3] = cop->designer_fb[3];
+
+	msgrng_access_enable(mflags);
+
+	nlm_fb_vc = crypto_get_fb_vc();
+	nlm_tx_id = (uint64_t) cop;
+//printk(KERN_ERR "\n crypto_cipher_auth_op 1"); 
+	ret = nlm_hal_crypto_send_request(crypto_get_vc(), nlm_fb_vc,
+					  (void *) session->cntrl_desc,
+					  virt_to_phys(session->cntrl_desc),
+					  (void *) session->pkt_desc,
+					  virt_to_phys(session->pkt_desc),
+					  &cprm, nlm_tx_id);
+
+	crypto_status = NLM_CRYPTO_OP_IN_PROGRESS;
+//	msgrng_access_disable(mflags);
+//#ifdef XLP_POLLING
+//	msgrng_access_enable(mflags);
+	{
+//printk(KERN_ERR "\n crypto_cipher_auth_op 2"); 
+		//uint64_t err_msg = 0;
+		i = 0;
+		//nlm_err_msg = 0;
+		while (!nlm_err_msg && nlm_tx_id != nlm_hal_crypto_receive_response(nlm_fb_vc, &nlm_err_msg)
+                       && i < 10000)
+                	i++;
+		if (i < 10000 || nlm_err_msg)
+                        ret = crypto_fb_msg_print_err(nlm_err_msg);
+                else
+		{
+			ret = NLM_ERR_FREEBACK_NOT_RECEIVED;
+                        printf("\n Error _ FreeBack message is not received");
+		}
+		nlm_err_msg = 0;
+	}
+//printk(KERN_ERR "\n crypto_cipher_auth_op 3"); 
+	msgrng_access_disable(mflags);
+//#else
+
+#if 0
+	{
+		init_waitqueue_head (&wait);
+		i = 0;
+		while (crypto_status != NLM_CRYPTO_OP_DONE  && i < 10000) {
+			i++;
+			//udelay(1);
+			wait_event_interruptible_timeout(wait, 0, 1);
+		}
+	}
+	if (!ret) {
+		if (crypto_status == NLM_CRYPTO_OP_DONE)
+			ret = crypto_fb_msg_print_err(nlm_err_msg);
+		else {
+			ret = NLM_ERR_FREEBACK_NOT_RECEIVED;
+			printk(KERN_ERR "\nError: Freeback messgae not received");
+		}
+	}
+#endif
+//#endif
+	free(cprm.src_phy);
+	free(cprm.dst_phy);
+
+	return ret;
+}
+
+static void *
+get_cache_aligned_mem(size_t size)
+{
+	void *addr, **tmp;
+	unsigned long loc;
+	addr = malloc(size + L3_CACHELINE_SIZE * 2);
+
+	if (addr == NULL) {
+		printk(KERN_ERR
+		       "\n Malloc failed for cache aligned memory allocation");
+		return NULL;	//NLM_ERR_NO_MEM;
+	}
+	loc = (unsigned long) addr;
+	loc = (loc + L3_CACHELINE_SIZE - 1) & ~L3_CACHELINE_MASK;
+
+	tmp = (void **) loc;
+
+	*tmp = addr;
+
+	loc += L3_CACHELINE_SIZE;
+
+	addr = (void *) loc;
+
+	return addr;
+}
+
+static void
+free_cache_aligned_mem(void *addr)
+{
+	unsigned long loc;
+
+	loc = (unsigned long) addr;
+
+	loc -= 64;
+
+	addr = (void *) loc;
+
+	addr = (void *) *(unsigned long *) addr;
+
+	free(addr);
+}
+
+static const struct file_operations xlp_sae_fops = {
+	.owner = THIS_MODULE,
+	.open = xlp_sae_open,
+	.release = xlp_sae_release,
+};
+
+/* Note that nobody ever sets xlp_sae_busy... */
+static int
+xlp_sae_open(struct inode *inode, struct file *file)
+{
+	TRACE_TEXT("(xlp_sae_open");
+	return 0;
+}
+
+static int
+xlp_sae_release(struct inode *inode, struct file *file)
+{
+	TRACE_TEXT("(xlp_sae_release");
+
+	return 0;
+}
+
+static void
+nlm_xlp_sae_msgring_handler(uint32_t vc, uint32_t src_id,
+			    uint32_t size, uint32_t code,
+			    uint64_t msg0, uint64_t msg1,
+			    uint64_t msg2, uint64_t msg3, void *data)
+{
+
+	if (vc == nlm_fb_vc && size == 2) {
+		if (nlm_tx_id ==
+		    nlm_hal_crypto_process_response(vc, code, src_id, msg0,
+						    msg1, &nlm_err_msg)) {
+			crypto_status = NLM_CRYPTO_OP_DONE;
+			nlm_err_msg = 1 | nlm_err_msg;
+printk(KERN_ERR "\n nlm_xlp_sae_msgring_handler nlm_err_msg = %llx", nlm_err_msg);
+	//		wake_up(&wait);
+		}
+	}
+}
+
+int
+nlm_crypto_init(void)
+{
+	init_crypto_vc();
+#if 0
+	if (register_xlp_msgring_handler
+	    (XLP_MSG_HANDLE_CRYPTO, nlm_xlp_sae_msgring_handler, NULL)) {
+		panic("can't register msgring handler for TX_STN_GMAC0");
+	}
+#endif
+	return 0;
+}
+
+static int __init
+xlp_sae_init(void)
+{
+	printk(KERN_ERR ",\n XLP SAE/Crypto Initialization \n");
+
+	xlp_sae_major = register_chrdev(0, "NLM_XLP_SAE", &xlp_sae_fops);
+	if (xlp_sae_major < 0) {
+		printk(KERN_ERR "XLP_SAE - cannot register device\n");
+		return xlp_sae_major;
+	}
+//  printk (KERN_ERR ",XLP SAE MAJOR %d\n", xlp_sae_major);
+
+	nlm_crypto_init();
+	xlp_crypt_alg_init();
+	xlp_auth_alg_init();
+	xlp_aead_alg_init();
+
+	return 0;
+}
+
+static void __exit
+xlp_sae_cleanup(void)
+{
+	xlp_crypt_alg_fini();
+	xlp_aead_alg_fini();
+	xlp_auth_alg_fini();
+	unregister_chrdev(xlp_sae_major, "XLP_SAE");
+}
+
+EXPORT_SYMBOL(nlm_crypto_init);
+EXPORT_SYMBOL(crypto_cleanup_session);
+EXPORT_SYMBOL(crypto_session_reset_key);
+EXPORT_SYMBOL(crypto_setup_cipher_auth_session);
+EXPORT_SYMBOL(crypto_cipher_auth_op);
+
+module_init(xlp_sae_init);
+module_exit(xlp_sae_cleanup);
+MODULE_DESCRIPTION("XLP Hardware crypto support for AES/DES/3DES/SHA/MD5 .");
+MODULE_LICENSE("GPL");
+MODULE_VERSION("0.1");
+MODULE_AUTHOR("Alok Agrawat");
diff --git a/drivers/crypto/sae/nlm_crypto.h b/drivers/crypto/sae/nlm_crypto.h
new file mode 100644
index 0000000..085411a
--- /dev/null
+++ b/drivers/crypto/sae/nlm_crypto.h
@@ -0,0 +1,95 @@
+/***********************************************************************
+Copyright 2003-2010 Netlogic Microsystems (“Netlogic”). All rights
+reserved.
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are
+met:
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+*****************************#NETL_2#********************************/
+#ifndef _NLM_CRYPTO_H_
+#define _NLM_CRYPTO_H_
+
+#define NLM_ENCRYPT 1
+#define NLM_DECRYPT 0
+
+/* Explicitly assigned numbers for readability */
+typedef enum cipher_algo {
+	NLM_CIPHER_BYPASS = 0,
+	NLM_CIPHER_DES = 1,
+	NLM_CIPHER_3DES = 2,
+	NLM_CIPHER_AES128 = 3,
+	NLM_CIPHER_AES192 = 4,
+	NLM_CIPHER_AES256 = 5,
+	NLM_CIPHER_ARC4 = 6,
+	NLM_CIPHER_KASUMI_F8 = 7,
+	NLM_CIPHER_SNOW3G_F8 = 8,
+	NLM_CIPHER_CAMELLIA128 = 9,
+	NLM_CIPHER_CAMELLIA192 = 0xA,
+	NLM_CIPHER_CAMELLIA256 = 0xB,
+	NLM_CIPHER_MAX = 0xC,
+} cipher_algo_t;
+
+typedef enum cipher_mode {
+	NLM_CIPHER_MODE_ECB = 0,
+	NLM_CIPHER_MODE_CBC = 1,
+	NLM_CIPHER_MODE_CFB = 2,
+	NLM_CIPHER_MODE_OFB = 3,
+	NLM_CIPHER_MODE_CTR = 4,
+	NLM_CIPHER_MODE_AES_F8 = 5,
+	NLM_CIPHER_MODE_GCM = 6,
+	NLM_CIPHER_MODE_CCM = 7,
+	NLM_CIPHER_MODE_UNDEFINED1 = 8,
+	NLM_CIPHER_MODE_UNDEFINED2 = 9,
+	NLM_CIPHER_MODE_LRW = 0xA,
+	NLM_CIPHER_MODE_XTS = 0xB,
+	NLM_CIPHER_MODE_MAX = 0xC,
+} cipher_mode_t;;
+
+typedef enum auth_algo {
+	NLM_AUTH_BYPASS = 0,
+	NLM_AUTH_MD5 = 1,
+	NLM_AUTH_SHA = 2,
+	NLM_AUTH_UNDEFINED = 3,
+	NLM_AUTH_AES128 = 4,
+	NLM_AUTH_AES192 = 5,
+	NLM_AUTH_AES256 = 6,
+	NLM_AUTH_KASUMI_F9 = 7,
+	NLM_AUTH_SNOW3G_F9 = 8,
+	NLM_AUTH_CAMELLIA128 = 9,
+	NLM_AUTH_CAMELLIA192 = 0xA,
+	NLM_AUTH_CAMELLIA256 = 0xB,
+	NLM_AUTH_GHASH = 0xC,
+	NLM_AUTH_MAX = 0xD
+} auth_algo_t;
+
+typedef enum auth_mode {
+	NLM_AUTH_MODE_SHA1 = 0,	/* Only SHA */
+	NLM_AUTH_MODE_SHA224 = 1,	/* Only SHA */
+	NLM_AUTH_MODE_SHA256 = 2,	/* Only SHA */
+	NLM_AUTH_MODE_SHA384 = 3,	/* Only SHA */
+	NLM_AUTH_MODE_SHA512 = 4,	/* Only SHA */
+	NLM_AUTH_MODE_CMAC = 5,	/* AES and Camellia */
+	NLM_AUTH_MODE_XCBC = 6,	/* AES and Camellia */
+	NLM_AUTH_MODE_CBC_MAC = 7,	/* AES and Camellia */
+	NLM_AUTH_MODE_CCM = 8,	/* AES */
+	NLM_AUTH_MODE_GCM = 9,	/* AES */
+	NLM_AUTH_MODE_MAX = 0xA,
+} auth_mode_t;
+
+#endif
diff --git a/drivers/crypto/sae/nlm_crypto_api.h b/drivers/crypto/sae/nlm_crypto_api.h
new file mode 100644
index 0000000..144661e
--- /dev/null
+++ b/drivers/crypto/sae/nlm_crypto_api.h
@@ -0,0 +1,230 @@
+/***********************************************************************
+Copyright 2003-2010 Netlogic Microsystems (“Netlogic”). All rights
+reserved.
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are
+met:
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+*****************************#NETL_2#********************************/
+/**
+* @file_name nlm_crypto_api.h
+*/
+
+/**
+* @defgroup crypto Crypto API
+* @brief Description about the crypto apis supported in NETOS.
+*/
+
+/**
+* If user wish to allocate memory for the control and packet descriptors then following macros
+* guides for the memory requirement of control and packet descriptors.
+*/
+#define NLM_MAX_CIPHER_KEY_LEN		(32)
+#define NLM_MAX_AUTH_KEY_LEN		(128)
+#define NLM_MAX_ARC4_STATE_SIZE		(264)
+#define CRYPTO_SIZEOF_CNTRL_DESC	(8 + NLM_MAX_CIPHER_KEY_LEN + NLM_MAX_AUTH_KEY_LEN + NLM_MAX_ARC4_STATE_SIZE)
+
+#define CRYPTO_SIZEOF_PKT_DESC(nr_frags)	(32 + (nr_frags * 2)*8 + 4 /* Designed freeback*/)
+
+#define NLM_CRYPTO_FB_ERR_MASK 0x27bf80
+
+#define NLM_CRYPTO_FB_DATA_ERR_FORCE_BYPASS 		   (1 << 21)
+#define NLM_CRYPTO_FB_DATA_ERR_SW_DESC 			   (1 << 18)
+#define NLM_CRYPTO_FB_DATA_ERR_INSUFFICIENT_DATA_TO_AUTH   (1 << 17)
+#define NLM_CRYPTO_FB_DATA_ERR_INSUFFICIENT_DATA_TO_CIPHER (1 << 16)
+#define NLM_CRYPTO_FB_DATA_ERR_NO_PKT_DESC 		   (1 << 15)
+#define NLM_CRYPTO_FB_DATA_ERR_ECC_SRC_DST 		   (1 << 13)
+#define NLM_CRYPTO_FB_DATA_ERR_ECC_DESC_0123          	   (1 << 12)
+#define NLM_CRYPTO_FB_DATA_ERR_ECC_AUTH_KEY 		   (1 << 11)
+#define NLM_CRYPTO_FB_DATA_ERR_ECC_CIPHER_KEY 		   (1 << 10)
+#define NLM_CRYPTO_FB_DATA_ERR_ECC_CNTL_DESC 		   (1 << 9)
+#define NLM_CRYPTO_FB_DATA_ERR_ECC_DATA 		   (1 << 8)
+#define NLM_CRYPTO_FB_DATA_ERR_ECC_DESIGNER_FB 		   (1 << 7)
+
+#define NLM_ERR_BASE				0xCAE0000
+#define NLM_ERR_CIPHER_ALG_INVALID 		(NLM_ERR_BASE | 1)
+#define NLM_ERR_CIPHER_ALG_MODE_INVALID 	(NLM_ERR_BASE | 2)
+#define NLM_ERR_AUTH_ALG_INVALID 		(NLM_ERR_BASE | 3)
+#define NLM_ERR_AUTH_ALG_MODE_INVALID 		(NLM_ERR_BASE | 4)
+#define NLM_ERR_CIPHER_IV_LEN_INVALID 		(NLM_ERR_BASE | 5)
+#define NLM_ERR_CIPHER_KEY_LEN_INVALID 		(NLM_ERR_BASE | 6)
+#define NLM_ERR_CIPHER_KEY_NULL 		(NLM_ERR_BASE | 7)
+#define NLM_ERR_AUTH_KEY_NULL 			(NLM_ERR_BASE | 8)
+#define NLM_ERR_AUTH_DST_NULL 			(NLM_ERR_BASE | 9)
+#define NLM_ERR_TAG_LEN 			(NLM_ERR_BASE | 10)
+#define NLM_ERR_SESSION_NOT_SETUP 		(NLM_ERR_BASE | 11)
+#define NLM_ERR_CRYPTO_PARAM_NULL		(NLM_ERR_BASE | 12)
+#define NLM_ERR_INVALID_PARAM 			(NLM_ERR_BASE | 13)
+#define NLM_ERR_AUTH_LEN_LESS_THAN_CIPHER_LEN 	(NLM_ERR_BASE | 14)
+#define NLM_ERR_CALLBACK_NULL 			(NLM_ERR_BASE | 15)
+#define NLM_ERR_NO_MEM 				(NLM_ERR_BASE | 16)
+#define NLM_ERR_UNSPECIFIED 			(NLM_ERR_BASE | 17)
+#define	NLM_ERR_SEND_TIMEOUT 			(NLM_ERR_BASE | 18)
+#define NLM_ERR_IN_PROGRESS 			(NLM_ERR_BASE | 19)
+#define NLM_ERR_AUTH_KEY_LEN_INVALID 		(NLM_ERR_BASE | 20)
+
+#define NLM_ERR_FREEBACK_NOT_RECEIVED           (NLM_ERR_BASE | 256)
+
+struct crypto_iovec {
+	unsigned char *buf;
+	uint32_t iov_len;
+};
+
+struct crypto_cipher_init_param {
+	uint32_t cipher_alg;
+	uint32_t cipher_mode;
+	struct crypto_iovec *cipher_key;
+	uint8_t arc4_cipher_key_len;
+	uint8_t arc4_key_init;
+	unsigned char cfb_mask;
+};
+
+struct crypto_auth_init_param {
+	uint32_t auth_alg;
+	uint32_t auth_mode;
+	uint32_t tag_len;
+	struct crypto_iovec *auth_key;
+	uint8_t hmac;
+};
+
+struct crypto_session {
+	void *cntrl_desc;
+	void *pkt_desc;
+	uint32_t flag;		/* This will indicate the owner of the memory */
+	struct crypto_cipher_init_param *cip;
+	struct crypto_auth_init_param *aip;
+
+};
+
+struct crypto_param {
+	uint32_t enc;
+	struct crypto_iovec *src;	/* 1 or more fragments of the source data */
+	struct crypto_iovec *dst;
+	uint32_t src_len;
+	uint32_t nr_frags;
+
+	uint32_t cipher_len;
+	uint32_t iv_len;
+	unsigned char *iv;
+
+	uint32_t iv_offset;
+	uint32_t cipher_offset;
+	uint8_t cipher_bit_count:3;
+
+	uint8_t hash_bit_count:3;
+	uint8_t tls_proto:1;
+	uint8_t arc4_save_l3_alloc:1;
+	uint8_t arc4_save_state:1;
+	uint8_t arc4_load_state:1;
+	uint8_t hmac_external_pad_key:1;
+	uint8_t hash_clobber:1;
+	uint8_t cipher_clobber:1;
+	uint8_t hash_output_l3_alloc:1;
+	uint8_t cipher_output_l3_alloc:1;
+	uint8_t hash_source:1;
+	uint8_t send_designer_fb:1;
+
+	uint32_t auth_len;
+	uint32_t auth_offset;
+	uint32_t tag_len;
+
+	uint32_t cipher_key_len;
+	uint32_t auth_key_len;
+	unsigned char *hash_dst_address;
+
+	uint32_t designer_freeback_id;
+	uint32_t designer_freeback_len;
+	uint64_t designer_fb[4];
+};
+
+/**
+ * @brief This API sets up session for performing cipher and authentication
+ *         operation. It can be used for both cipher and authentication operation in one pass.
+ * 	   It can also be used for either cipher or authentication.
+ *
+ * @param[in] session  User allocates memory for session context and passes
+ *                     the structure pointer. It contains two memory pointer for control and packet descriptors.
+ *                     If user wants to own the memory then flag parameter in the session structure needs to be set.
+ *                     If flag is not set then driver will allocate the memory for control and packet descriptors.
+		       It contains two structure to take input for cipher and auth operation separately.
+ *
+ * @return Return value of 0 indicates success.
+ * Returns error code, in case of failure.
+ *
+ * @ingroup crypto
+ *
+ */
+
+extern int crypto_setup_cipher_auth_session(struct crypto_session *session);
+
+/**
+ * @brief This API performs cipher and/or authentication operation. It will use preallocated session for algorithm type and mode of operation.
+ *	  Authentication output will be written in the hash_dst_address and cipher output will be written in dst of the cprm structure.
+ *        Source data can be fragmented and cprm->nr_frags represents number of fragments.
+ * @param[in] session  Specifies the session which is setup by calling
+ *                     crypto_setup_cipher_auth_session. 
+ * @param[in] cprm     This structure contains input parameters for  cipher and authentication. 
+ * 
+ * @param[out] cprm->hash_dst_address For authentication output.
+ * @param[out] cprm->dst For cipher output.
+ *
+ * @return Return value of 0 indicates success.
+ * Returns error code, in case of failure.
+ *
+ * @ingroup crypto
+ *
+ */
+extern int crypto_cipher_auth_op(struct crypto_session *session,
+				 struct crypto_param *cprm);
+
+/**
+ * @brief This API resets key for an already configured session.
+ *
+ * @param[in] session    Specifies the session which is setup by calling
+ *                       crypto_setup_cipher_auth_session.
+ *
+ * @param[in] cipher_key New cipher key. For auth only sessions, this parameter
+ *                       is ignored.
+ * @param[in] auth_key   New auth key. For cipher only sessions, this parameter
+ *                       is ignored.
+ *
+ * @return Return value of 0 indicates success.
+ * Returns error code, in case of failure.
+ *
+ * @ingroup crypto
+ */
+
+extern int crypto_session_reset_key(struct crypto_session *session,
+				    unsigned char *cipher_key, unsigned
+				    char *auth_key);
+
+/**
+ * @brief This API cleans up internal driver memory associated with a session.
+ *
+ * @param[in] session Specifies the session which is setup by calling
+ *                     crypto_setup_cipher_auth_session. 
+ *
+ * @return Return value of 0 indicates success.
+ * Returns error code, in case of failure.
+ *
+ * @ingroup crypto
+ */
+extern int crypto_cleanup_session(struct crypto_session *session);
+
+extern int nlm_crypto_init(void);
diff --git a/drivers/crypto/sae/nlm_crypto_data.h b/drivers/crypto/sae/nlm_crypto_data.h
new file mode 100755
index 0000000..a57d59b
--- /dev/null
+++ b/drivers/crypto/sae/nlm_crypto_data.h
@@ -0,0 +1,158 @@
+/***********************************************************************
+Copyright 2003-2010 Netlogic Microsystems (“Netlogic”). All rights
+reserved.
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are
+met:
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+*****************************#NETL_2#********************************/
+#ifndef NLM_CRYPTO_DATA_H
+#define NLM_CRYPTO_DATA_H
+
+#define NLM_CRYPTO_VC_BASE 281
+/*
+ * is the following table needed for all modes?
+Cipher            keylen           iv_len
+*/
+
+//Ciphers ====> NLM_CIPHER_AES128	
+static int cipher_mode_key_len[NLM_CIPHER_MAX][NLM_CIPHER_MODE_MAX] = {
+/*		      ECB    CBC   CFB   OFB   CTR   AESF8    GCM   CCM    8   9   LRW   XTS */
+/* BYPASS */       {   0,     0,    0,    0,    0,     0,      0,    0,    0,  0,   0,    0,},
+/* DES */          {   8,     8,    0,    0,    0,     0,      0,    0,    0,  0,   0,    0,},
+/* 3DES */         {   24,    24,   0,    0,    0,     0,      0,    0,    0,  0,   0,    0,},
+/* AES128 */       {   16,    16,   16,   16,   16,    16,     16,   16,   0,  0,   32,   32,},
+/* AES192 */       {   24,    24,   24,   24,   24,    24,     24,   24,   0,  0,   40,   48,},
+/* AES256 */	   {   32,    32,   32,   32,   32,    32,     32,   32,   0,  0,   48,   64,},
+/* There is no mode associated with ARC4 ... probably ignored... */
+/* ARC4 */         {   32,    32,   32,   32,   32,    32,     32,   32,   0,  0, 32,   32,}, 
+/* SBOX for ARC4 comes in the next cacheline... So, if SBOX is used, few bytes
+ * of random padding is done to the key to make the SBOX cacheline aligned*/
+/* There is no mode associated with Kasumi F8... probably ignored... */
+/* KASUMI F8 */    {   16,    16,   16,   16,   16,    16,     16,   16,   0,  0,  16,   16,},
+/* There is no mode associated with Snow3G F8... probably ignored... */
+/* SNOW3G F8 */    {   16,    16,   16,   16,   16,    16,     16,   16,   0,  0,  16,   16,},
+/* CAMELLIA128*/   {   16,    16,   16,   16,   16,    16,     16,   16,   0,  0,  32,   32,},
+/* CAMELLIA192*/   {   24,    24,   24,   24,   24,    24,     24,   24,   0,  0,  40,   48,},
+/* CAMELLIA256*/   {   32,    32,   32,   32,   32,    32,     32,   32,   0,  0,  48,   64,},
+};
+
+//-1 indicates variable length IV
+// In case of AES/Camelia cipher and CBC-MAC auth, IV is not needed.
+// In case of AES/Camelia cipher and XCBC-MAC auth, IV is needed only for 
+//CBC, CFB, OFB and CTR modes..
+static int cipher_mode_iv_len[NLM_CIPHER_MAX][NLM_CIPHER_MODE_MAX] = {
+/*		       ECB  CBC   CFB   OFB   CTR  AESF8    GCM  CCM    8   9  LRW   XTS */
+/* BYPASS */       {   0,    0,    0,    0,    0,    0,      0,    0,   0,  0,  0,    0,},
+/* DES */          {   0,    8,    0,    0,    0,    0,      0,    0,   0,  0,  0,    0,},
+/* 3DES */         {   0,    8,    0,    0,    0,    0,      0,    0,   0,  0,  0,    0,},
+/* AES128 */       {   0,    16,   16,   16,   16,   16,     16,   16,   0,  0,  16,   16,},
+/* AES192 */       {   0,    16,   16,   16,   16,   16,     16,   16,   0,  0,  16,   16,},
+/* AES256 */       {   0,    16,   16,   16,   16,   16,     16,   16,   0,  0,  16,   16,},
+/* ARC4 */         {   0,    0,    0,    0,    0,    0,      0,    0,   0,  0,  0,    0,},
+/* KASUMI F8 */    {   0,    0,    0,    0,    0,    0,      0,    0,   0,  0,  0,    0,},
+/* SNOW3G F8 */    {   0,    0,    0,    0,    0,    0,      0,    0,   0,  0,  0,    0,},
+/* CAMELLIA128 */  {   0,    16,   16,   16,   16,   16,     -1,   0,   0,  0,  16,   16,},
+/* CAMELLIA192 */  {   0,    16,   16,   16,   16,   16,     -1,   0,   0,  0,  16,   16,}, 
+/* CAMELLIA256 */  {   0,    16,   16,   16,   16,   16      -1,   0,   0,  0,  16,   16,},
+};
+
+
+static int cipher_alg_needs_mode[NLM_CIPHER_MAX] = {
+0, //NLM_CIPHER_BYPASS
+1, //NLM_CIPHER_DES
+1, //NLM_CIPHER_3DES
+1, //NLM_CIPHER_AES128
+1, //NLM_CIPHER_AES192
+1, //NLM_CIPHER_AES256
+0, //NLM_CIPHER_ARC4
+0, //NLM_CIPHER_KASUMI_F8
+0, //NLM_CIPHER_SNOW3G_F8 
+1, //NLM_CIPHER_CAMELLIA128
+1, //NLM_CIPHER_CAMELLIA192
+1, //NLM_CIPHER_CAMELLIA256
+};
+
+static int cipher_mode_valid[NLM_CIPHER_MAX][NLM_CIPHER_MODE_MAX] = {
+	/* Mode numbers 8 and 9 are undefined.. LRW comes after that */
+/*		       ECB  CBC  CFB  OFB  CTR  AESF8  GCM  CCM  8   9  LRW  XTS */
+/* BYPASS */       {   0,    0,   0,   0,   0,    0,    0,   0,  0,  0,  0,   0,},
+/* DES */          {   1,    1,   0,   0,   0,    0,    0,   0,  0,  0,  0,   0,},
+/* 3DES */         {   1,    1,   0,   0,   0,    0,    0,   0,  0,  0,  0,   0,},
+/* AES128 */       {   1,    1,   1,   1,   1,    1,    1,   1,  0,  0,  1,   1,},
+/* AES192 */       {   1,    1,   1,   1,   1,    1,    1,   1,  0,  0,  1,   1,},
+/* AES256 */       {   1,    1,   1,   1,   1,    1,    1,   1,  0,  0,  1,   1,},
+/* ARC4 */         {   0,    0,   0,   0,   0,    0,    0,   0,  0,  0,  0,   0,},
+/* KASUMI F8 */    {   0,    0,   0,   0,   0,    0,    0,   0,  0,  0,  0,   0,},
+/* SNOW3G F8 */    {   0,    0,   0,   0,   0,    0,    0,   0,  0,  0,  0,   0,},
+/* CAMELLIA128*/   {   1,    1,   0,   0,   1,    0,    0,   1,  0,  0,  0,   0,},
+/* CAMELLIA192*/   {   1,    1,   0,   0,   1,    0,    0,   1,  0,  0,  0,   0,},
+/* CAMELLIA256*/   {   1,    1,   0,   0,   1,    0,    0,   1,  0,  0,  0,   0,},
+};
+
+static int auth_alg_needs_mode[NLM_AUTH_MAX] = {
+0,	// NLM_AUTH_BYPASS
+0,	// NLM_AUTH_MD5
+1,	// NLM_AUTH_SHA
+0,	// NLM_AUTH_UNDEFINED
+1,	// NLM_AUTH_AES128
+1,	// NLM_AUTH_AES192
+1,	// NLM_AUTH_AES256
+0,	// NLM_AUTH_KASUMI_F9
+0,	// NLM_AUTH_SNOW3G_F9
+1,	// NLM_AUTH_CAMELLIA128
+1,	// NLM_AUTH_CAMELLIA192
+1,	// NLM_AUTH_CAMELLIA256
+};
+
+static int auth_mode_key_len[NLM_AUTH_MAX][NLM_AUTH_MODE_MAX] = {
+/*	               SHA1 SHA224 SHA256 SHA384 SHA512  CMAC  XCBC CBC_MAC CCM  GCM*/
+/* BYPASS */		{0,    0,     0,     0,     0,     0,   0,    0,     0,    0, },
+/* MD5 */		{64,   64,    64,   64,    64,    64,  64,   64,    64,   64, },
+/* SHA */		{64,   64,    64,   128,   128,    0,   0,    0,     0,    0, },
+/* 3 */			{0,    0,     0,     0,     0,     0,   0,    0,     0,    0, },
+/* AES128 */		{0,    0,     0,     0,     0,    16,  16,   16,    16,   16, },
+/* AES192 */		{0,    0,     0,     0,     0,    24,  24,   24,    24,   24, },
+/* AES256 */		{0,    0,     0,     0,     0,    32,  32,   32,    32,   32, },
+/* KASUMI_F9 */		{16,  16,    16,    16,    16,    16,  16,   16,    16,   16, },
+/* SNOW3G_F9 */		{16,  16,    16,    16,    16,    16,  16,   16,    16,   16, }, //sandip -> verify
+/* CAMELLIA128 */	{0,    0,     0,     0,     0,    16,  16,   16,    16,   16, },
+/* CAMELLIA192 */	{0,    0,     0,     0,     0,    24,  24,   24,    24,   24, },
+/* CAMELLIA256 */	{0,    0,     0,     0,     0,    32,  32,   32,    32,   32, },
+/* GHASH */		{0,    0,     0,     0,     0,    32,  32,   32,    32,   32, }, //todo:
+};
+
+static int auth_mode_valid[NLM_AUTH_MAX][NLM_AUTH_MODE_MAX] = {
+/*	               SHA1 SHA224 SHA256 SHA384 SHA512  CMAC  XCBC CBC_MAC CCM GCM */
+/* BYPASS */		{0,   0,     0,     0,     0,     0,   0,    0,      0,  0,  },
+/* MD5 */		{1,   1,     1,     1,     1,     1,   1,    1,      1,  1,  }, /* dont care */
+/* SHA */		{1,   1,     1,     1,     1,     0,   0,    0,      0,  0,  },
+/* 3 */			{0,   0,     0,     0,     0,     0,   0,    0,      0,  0,  },
+/* AES128 */		{0,   0,     0,     0,     0,     1,   1,    1,      1,  1,  },
+/* AES192 */		{0,   0,     0,     0,     0,     1,   1,    1,      1,  1,  },
+/* AES256 */		{0,   0,     0,     0,     0,     1,   1,    1,      1,  1,  },
+/* KASUMI_F9 */		{1,   1,     1,     1,     1,     1,   1,    1,      1,  1,  }, /* dont care */
+/* SNOW3G_F9 */		{1,   1,     1,     1,     1,     1,   1,    1,      1,  1,  }, /* dont care */
+/* CAMELLIA128 */	{0,   0,     0,     0,     0,     1,   1,    1,      1,  1,  },
+/* CAMELLIA192 */	{0,   0,     0,     0,     0,     1,   1,    1,      1,  1,  },
+/* CAMELLIA256 */	{0,   0,     0,     0,     0,     1,   1,    1,      1,  1,  },
+/* GHASH */		{1,   1,     1,     1,     1,     1,   1,    1,      1,  1,  }, //todo
+};
+
+#endif
diff --git a/drivers/crypto/sae/nlm_enc.c b/drivers/crypto/sae/nlm_enc.c
new file mode 100755
index 0000000..a3550c8
--- /dev/null
+++ b/drivers/crypto/sae/nlm_enc.c
@@ -0,0 +1,644 @@
+/***********************************************************************
+Copyright 2003-2010 Netlogic Microsystems (“Netlogic”). All rights
+reserved.
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are
+met:
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+*****************************#NETL_2#********************************/
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/pci.h>
+#include <linux/pci_ids.h>
+#include <linux/crypto.h>
+#include <linux/spinlock.h>
+#include <crypto/algapi.h>
+#include <crypto/aes.h>
+#include <crypto/des.h>
+#include <crypto/ctr.h>
+#include <crypto/scatterwalk.h>
+#include <linux/hardirq.h>
+
+#include <asm/io.h>
+#include <asm/delay.h>
+#include <asm/netlogic/msgring.h>
+#include "nlm_crypto_api.h"
+#include "nlm_crypto.h"
+#include "nlm_crypto_data.h"
+
+#define AES_CTR_IV_SIZE		8
+#define XLP_CRYPT_PRIORITY	300
+
+//#define SEC_DEBUG
+
+#ifdef SEC_DEBUG
+#ifdef __KERNEL__
+#define debug_print(fmt, args...) printk(fmt, ##args)
+#else				/* __KERNEL__ */
+#define debug_print(fmt, args...) printf(fmt, ##args)
+#endif				/* __KERNEL__ */
+#else				/* SEC_DEBUG */
+#define debug_print(fmt, args...)
+#endif				/* SEC_DEBUG */
+
+#define malloc(a) kmalloc(a, GFP_KERNEL)
+#define free kfree
+
+/* CRYPTO-API Functions */
+
+struct crypto_rfc3686_ctx {
+        struct crypto_blkcipher *child;
+        u8 nonce[CTR_RFC3686_NONCE_SIZE];
+};
+
+unsigned char nonce[4];
+
+static int
+xlp_setkey(struct crypto_tfm *tfm, const u8 * in_key, unsigned int len, uint32_t cipher_alg)
+{
+	struct crypto_session *session = crypto_tfm_ctx(tfm);
+
+//printk(KERN_ERR "\n enc xlp_setkey session = %x",(unsigned int)session);
+	memset(session, 0, sizeof (struct crypto_session));
+	session->aip = NULL;
+#if 0
+	session->cip =
+	    (struct crypto_cipher_init_param *)
+	    malloc(sizeof (struct crypto_cipher_init_param));
+#endif
+	session->cip = ((unsigned char*)session + sizeof(struct crypto_session));
+//printk(KERN_ERR "\n enc xlp_setkey session->cip = %x",(unsigned int)session->cip);
+	if (session->cip == NULL) {
+		printk(KERN_ERR
+		       "\nError: session->cip is NULL. Returning from %s",
+		       __FUNCTION__);
+		return -EINVAL;
+	}
+	memset(session->cip, 0, sizeof (struct crypto_cipher_init_param));
+
+	session->cip->cipher_alg = cipher_alg;
+#if 0
+	session->cip->cipher_key = malloc(sizeof (struct crypto_iovec));
+#endif
+	session->cip->cipher_key = ((unsigned char*)session->cip + sizeof(struct crypto_cipher_init_param));
+//printk(KERN_ERR "\n enc xlp_setkey session->cip->cipher_key = %x",(unsigned int)session->cip->cipher_key);
+	if (session->cip->cipher_key == NULL) {
+		printk(KERN_ERR
+		       "\nError: malloc failed for session->cip->cipher_key. Returning from %s",
+		       __FUNCTION__);
+		return -EINVAL;
+	}
+#if 0
+	session->cip->cipher_key->buf = malloc(len + 1);
+#endif
+	session->cip->cipher_key->buf = ((unsigned char*)session->cip->cipher_key + sizeof(struct crypto_iovec));
+//printk(KERN_ERR "\n enc xlp_setkey session->cip->cipher_key->buf = %x",(unsigned int)session->cip->cipher_key->buf);
+	if (session->cip->cipher_key->buf == NULL) {
+		printk(KERN_ERR
+		       "\nError: malloc failed for session->cip->cipher_key->buf. Returning from %s",
+		       __FUNCTION__);
+		return -EINVAL;
+	}
+	memcpy(session->cip->cipher_key->buf, in_key, len);
+	session->cip->cipher_key->iov_len = len;
+
+	session->cip->arc4_cipher_key_len = 0;
+	session->cip->arc4_key_init = 0;
+	session->cip->cfb_mask = 0;
+
+//printk(KERN_ERR "\n enc: xlp_setkey session = %x session->cip = %x ession->cip->cipher_alg=%d",(unsigned int)session, (unsigned int)session->cip, session->cip->cipher_alg);
+	return 0;
+}
+
+static int
+xlp_des3_setkey(struct crypto_tfm *tfm, const u8 * in_key, unsigned int len)
+{
+	uint32_t cipher_alg;
+	u32 *flags = &tfm->crt_flags;
+
+//printk(KERN_ERR "\n enc xlp_des3_setkey");
+	switch (len) {
+	case DES3_EDE_KEY_SIZE:
+	        cipher_alg = NLM_CIPHER_3DES;
+		break;
+	default:
+		printk(KERN_WARNING "[%s]: Cannot handle keylen = %d\n",
+		       __FUNCTION__, len);
+		*flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
+		return -EINVAL;
+	}
+	return xlp_setkey(tfm, in_key, len, cipher_alg);
+}
+
+static int
+xlp_des_setkey(struct crypto_tfm *tfm, const u8 * in_key, unsigned int len)
+{
+	uint32_t cipher_alg;
+	u32 *flags = &tfm->crt_flags;
+
+//printk(KERN_ERR "\n enc xlp_des_setkey");
+	switch (len) {
+	case DES_KEY_SIZE:
+	        cipher_alg = NLM_CIPHER_DES;
+		break;
+	default:
+		printk(KERN_WARNING "[%s]: Cannot handle keylen = %d\n",
+		       __FUNCTION__, len);
+		*flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
+		return -EINVAL;
+	}
+	return xlp_setkey(tfm, in_key, len, cipher_alg);
+}
+
+static int
+xlp_aes_setkey(struct crypto_tfm *tfm, const u8 * in_key, unsigned int len)
+{
+	uint32_t cipher_alg;
+	u32 *flags = &tfm->crt_flags;
+
+//printk(KERN_ERR "\n enc xlp_aes_setkey");
+	switch (len) {
+	case 16:
+	        cipher_alg = NLM_CIPHER_AES128;
+		break;
+	case 24:
+		cipher_alg = NLM_CIPHER_AES192;
+		break;
+	case 32:
+		cipher_alg = NLM_CIPHER_AES256;
+		break;
+	default:
+		printk(KERN_WARNING "[%s]: Cannot handle keylen = %d\n",
+		       __FUNCTION__, len);
+		*flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
+		return -EINVAL;
+	}
+	return xlp_setkey(tfm, in_key, len, cipher_alg);
+}
+
+static int xlp_ctr_rfc3686_setkey(struct crypto_tfm *parent, const u8 *key,
+                                 unsigned int keylen)
+{
+
+        int err;
+
+        if (keylen < CTR_RFC3686_NONCE_SIZE)
+                return -EINVAL;
+        memcpy(nonce, key + (keylen - CTR_RFC3686_NONCE_SIZE),
+               CTR_RFC3686_NONCE_SIZE);
+printk(KERN_ERR "nonce = %x",*(unsigned char*)nonce);
+        keylen -= CTR_RFC3686_NONCE_SIZE;
+        err = xlp_aes_setkey(parent, key, keylen);
+
+printk(KERN_ERR "\n returning");
+#if 0
+        struct crypto_rfc3686_ctx *ctx = crypto_tfm_ctx(parent);
+	struct crypto_blkcipher *child = ctx->child;
+        int err, i;
+
+        /* the nonce is stored in bytes at end of key */
+        if (keylen < CTR_RFC3686_NONCE_SIZE)
+                return -EINVAL;
+printk(KERN_ERR "\n key :");
+for(i =0; i < keylen; i++)
+printk(KERN_ERR "%.2x",key[i]);
+
+printk(KERN_ERR "\nCTR_RFC3686_NONCE_SIZE = %d",CTR_RFC3686_NONCE_SIZE);
+
+        memcpy(ctx->nonce, key + (keylen - CTR_RFC3686_NONCE_SIZE),
+               CTR_RFC3686_NONCE_SIZE);
+printk(KERN_ERR "ctx->nonce = %x",*(unsigned char*)ctx->nonce);
+
+        keylen -= CTR_RFC3686_NONCE_SIZE;
+
+printk(KERN_ERR "\n 111111111");
+	crypto_blkcipher_clear_flags(child, CRYPTO_TFM_REQ_MASK);
+printk(KERN_ERR "\n 222222222");
+        crypto_blkcipher_set_flags(child, crypto_tfm_get_flags(parent) &
+                                          CRYPTO_TFM_REQ_MASK);
+printk(KERN_ERR "\n 333333333");
+        err = xlp_aes_setkey(child, key, keylen);
+printk(KERN_ERR "\n 444444444");
+        crypto_tfm_set_flags(parent, crypto_blkcipher_get_flags(child) &
+                                     CRYPTO_TFM_RES_MASK);
+
+printk(KERN_ERR "\n returning");
+#endif
+
+        return err;
+}
+
+
+static int
+xlp_crypt(struct blkcipher_desc *desc,
+	      struct scatterlist *dst, struct scatterlist *src,
+	      unsigned int nbytes, unsigned int enc, uint32_t mode)
+{
+	struct crypto_session *session = crypto_blkcipher_ctx(desc->tfm);
+	struct crypto_param *sd;
+	struct blkcipher_walk walk;
+	int err, ret, i, j;
+	struct scatterlist *sg_src, *sg_dst;
+
+//printk(KERN_ERR "\n enc: xlp_crypt session = %x session->cip = %x ession->cip->cipher_alg=%d",(unsigned int)session, (unsigned int)session->cip, session->cip->cipher_alg);
+	sd = malloc(sizeof (struct crypto_param));
+	if (!sd) {
+		printk(KERN_ERR
+		       "\nError: malloc failed for sd. Returning from %s",
+		       __FUNCTION__);
+		return -EINVAL;	//NLM_ERR_INVALID_PARAM;
+	}
+
+	memset(sd, 0, sizeof (struct crypto_param));
+	sd->iv = NULL;
+
+	session->cip->cipher_mode = mode;
+
+	ret = crypto_setup_cipher_auth_session(session);
+	if (ret) {
+		printk(KERN_ERR
+		       "\nError: Error in session setup ret = %x. Returning from %s",
+		       ret, __FUNCTION__);
+		return -EINVAL;	//NLM_ERR_INVALID_PARAM;
+	}
+
+//printk(KERN_ERR "\n enc: xlp_crypt 1"); 
+	sd->iv_len =
+	    cipher_mode_iv_len[session->cip->cipher_alg][session->cip->
+							 cipher_mode];
+
+	sd->nr_frags = 1;
+
+	sd->enc = enc;
+	sd->cipher_len = nbytes;
+	sd->src_len = sd->iv_len + nbytes;
+	sd->cipher_key_len = session->cip->cipher_key->iov_len;
+	sd->auth_len = 0;
+
+	sd->iv_offset = 0;
+	sd->auth_offset = 0;
+	sd->cipher_offset = 0;
+#if 0
+	j = nbytes;
+	for (i = 0, sg_src = src, sg_dst = dst; j > 0;
+	     sg_src = scatterwalk_sg_next(sg_src), sg_dst =
+	     scatterwalk_sg_next(sg_dst), i++) {
+		unsigned len = min(nbytes, sg_src->length);
+		j -= len;
+
+	}
+#endif
+	sd->src = malloc(sizeof (struct crypto_iovec) * (i + 1));	/* Extra 1 for IV */
+	sd->dst = malloc(sizeof (struct crypto_iovec) * (i + 1));
+	if (sd->src == NULL || sd->dst == NULL) {
+		printk(KERN_ERR
+		       "\nError: malloc failed for sd->dst/src. Returning from %s",
+		       __FUNCTION__);
+		return -EINVAL;
+	}
+
+	sd->cipher_offset = sd->iv_len;
+
+	blkcipher_walk_init(&walk, dst, src, nbytes);
+	err = blkcipher_walk_virt(desc, &walk);
+
+	i = 0;
+	sd->src[i].iov_len = sd->iv_len;
+	sd->src[i].buf = (unsigned char *) walk.iv;
+
+	sd->dst[i].iov_len = sd->iv_len;
+	sd->dst[i].buf = (unsigned char *) walk.iv;
+//	sd->dst[i].buf = (unsigned char *) malloc(sd->iv_len);
+//printk(KERN_ERR "\n enc: xlp_crypt src = %x, dst = %x",(unsigned long)src, (unsigned long) dst);
+	i++;
+	for (sg_src = src, sg_dst = dst; nbytes > 0;
+	     sg_src = scatterwalk_sg_next(sg_src), sg_dst =
+	     scatterwalk_sg_next(sg_dst), i++) {
+		struct scatter_walk walk_src, walk_dst;
+		unsigned len = min(nbytes, sg_src->length);
+
+//printk(KERN_ERR "\n enc: xlp_crypt i = %d sg_src->length = %d, sg_dst->length = %d, nbytes = %d",i,sg_src->length, sg_dst->length, nbytes); 
+		scatterwalk_start(&walk_src, sg_src);
+		sd->src[i].buf = scatterwalk_map(&walk_src, 1);
+		sd->src[i].iov_len = len;
+
+		scatterwalk_start(&walk_dst, sg_dst);
+		sd->dst[i].buf = scatterwalk_map(&walk_dst, 1);
+		sd->dst[i].iov_len = len;
+
+//printk(KERN_ERR "\n enc: xlp_crypt sd->src[i].buf = %x, sd->dst[i].buf = %x ",(unsigned long)sd->src[i].buf, (unsigned long)sd->dst[i].buf); 
+		nbytes -= len;
+	}
+	sd->nr_frags = i;
+
+/*{int k = 0;
+i--;
+printk(KERN_ERR "\n plaintext:");
+for(k = 0; k < sd->src[i].iov_len; k++)
+        printk(KERN_ERR "%x",sd->src[i].buf[k]);
+}*/
+//printk(KERN_ERR "\n enc: xlp_crypt num_frags = %d", i); 
+//printk(KERN_ERR "\n enc: xlp_crypt 2"); 
+//printk(KERN_ERR "\n111111111111 enc: xlp_crypt session = %x session->cip = %x ession->cip->cipher_alg=%d",(unsigned int)session, (unsigned int)session->cip, session->cip->cipher_alg);
+	ret = crypto_cipher_auth_op(session, sd);
+/*printk(KERN_ERR "\n222222222222 enc: xlp_crypt session = %x session->cip = %x ession->cip->cipher_alg=%d",(unsigned int)session, (unsigned int)session->cip, session->cip->cipher_alg);
+{int k = 0;
+printk(KERN_ERR "\n cipher_text:");
+for(k = 0; k < sd->dst[i].iov_len; k++)
+	printk(KERN_ERR "%x",sd->dst[i].buf[k]);
+
+}*/
+	if (ret) {
+		printk(KERN_ERR
+		       "\nError: Error in cipher auth operation ret = %x. Returning from %s",
+		       ret, __FUNCTION__);
+		return -EAGAIN;
+	}
+
+#if 0
+	if (session->cip) {
+		if (session->cip->cipher_key) {
+			if (session->cip->cipher_key->buf)
+				free(session->cip->cipher_key->buf);
+			free(session->cip->cipher_key);
+		}
+		free(session->cip);
+	}
+#endif
+//printk(KERN_ERR "\n enc: xlp_crypt 3"); 
+	crypto_cleanup_session(session);
+	if (sd->src)
+		free(sd->src);
+	if (sd->dst)
+		free(sd->dst);
+	if (sd)
+		free(sd);
+
+//printk(KERN_ERR "\n enc: xlp_crypt 4"); 
+	return 0;
+}
+
+static int crypto_rfc3686_crypt(struct blkcipher_desc *desc,
+                                struct scatterlist *dst,
+                                struct scatterlist *src, unsigned int nbytes, unsigned int enc, unsigned int algo)
+{
+
+        unsigned char  iv[16];
+        unsigned char *info = desc->info;
+        int err;
+
+        memcpy((unsigned char*)iv, nonce, CTR_RFC3686_NONCE_SIZE);
+        memcpy((unsigned char*)iv + CTR_RFC3686_NONCE_SIZE, info, CTR_RFC3686_IV_SIZE);
+
+        *(__be32 *)((unsigned char*)iv + CTR_RFC3686_NONCE_SIZE + CTR_RFC3686_IV_SIZE) =
+                cpu_to_be32(1);
+
+        desc->info = (unsigned char*)&iv[0];
+        err = xlp_crypt(desc, dst, src, nbytes, enc, algo);
+        desc->info = info;
+#if 0
+        struct crypto_blkcipher *tfm = desc->tfm;
+        struct crypto_rfc3686_ctx *ctx = crypto_blkcipher_ctx(tfm);
+	struct crypto_blkcipher *child = ctx->child;
+        unsigned long alignmask = crypto_blkcipher_alignmask(tfm);
+        u8 ivblk[CTR_RFC3686_BLOCK_SIZE + alignmask];
+        u8 *iv = PTR_ALIGN(ivblk + 0, alignmask + 1);
+        u8 *info = desc->info;
+        int err;
+
+        /* set up counter block */
+        memcpy(iv, ctx->nonce, CTR_RFC3686_NONCE_SIZE);
+        memcpy(iv + CTR_RFC3686_NONCE_SIZE, info, CTR_RFC3686_IV_SIZE);
+
+        /* initialize counter portion of counter block */
+        *(__be32 *)(iv + CTR_RFC3686_NONCE_SIZE + CTR_RFC3686_IV_SIZE) =
+                cpu_to_be32(1);
+
+	desc->tfm = child;
+        desc->info = iv;
+        err = xlp_crypt(desc, dst, src, nbytes, enc, algo);
+        desc->tfm = tfm;
+        desc->info = info;
+#endif
+        return err;
+}
+
+static int
+xlp_ctr_rfc3686_decrypt(struct blkcipher_desc *desc,
+                struct scatterlist *dst, struct scatterlist *src,
+                unsigned int nbytes)
+{
+	return crypto_rfc3686_crypt(desc, dst, src, nbytes, 0, NLM_CIPHER_MODE_CTR);
+}
+
+static int
+xlp_ctr_rfc3686_encrypt(struct blkcipher_desc *desc,
+                struct scatterlist *dst, struct scatterlist *src,
+                unsigned int nbytes)
+{
+        return crypto_rfc3686_crypt(desc, dst, src, nbytes, 1, NLM_CIPHER_MODE_CTR);
+}
+
+static int
+xlp_ctr_decrypt(struct blkcipher_desc *desc,
+		struct scatterlist *dst, struct scatterlist *src,
+		unsigned int nbytes)
+{
+printk(KERN_ERR "\n enc: xlp_ctr_decrypt"); 
+	return xlp_crypt(desc, dst, src, nbytes, 0, NLM_CIPHER_MODE_CTR);
+}
+
+static int
+xlp_ctr_encrypt(struct blkcipher_desc *desc,
+		struct scatterlist *dst, struct scatterlist *src,
+		unsigned int nbytes)
+{
+printk(KERN_ERR "\n enc: xlp_ctr_encrypt"); 
+	return xlp_crypt(desc, dst, src, nbytes, 1, NLM_CIPHER_MODE_CTR);
+}
+
+static int
+xlp_cbc_decrypt(struct blkcipher_desc *desc,
+		struct scatterlist *dst, struct scatterlist *src,
+		unsigned int nbytes)
+{
+//printk(KERN_ERR "\n enc: xlp_cbc_decrypt"); 
+//return 0;
+	return xlp_crypt(desc, dst, src, nbytes, 0, NLM_CIPHER_MODE_CBC);
+}
+
+static int
+xlp_cbc_encrypt(struct blkcipher_desc *desc,
+		struct scatterlist *dst, struct scatterlist *src,
+		unsigned int nbytes)
+{
+//printk(KERN_ERR "\n enc: xlp_cbc_encrypt   -------------3");
+//return 0; 
+	return xlp_crypt(desc, dst, src, nbytes, 1, NLM_CIPHER_MODE_CBC);
+}
+
+static struct crypto_alg xlp_ctr_aes_alg = {
+	.cra_name = "rfc3686(ctr(aes))",
+	.cra_driver_name = "rfc3686-ctr-aes-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_flags = CRYPTO_ALG_TYPE_BLKCIPHER,
+	.cra_blocksize = AES_BLOCK_SIZE,
+	.cra_ctxsize = (sizeof (struct crypto_session) + sizeof(struct crypto_param)),
+	.cra_alignmask = 15,
+	.cra_type = &crypto_blkcipher_type,
+	.cra_module = THIS_MODULE,
+	.cra_list = LIST_HEAD_INIT(xlp_ctr_aes_alg.cra_list),
+	.cra_u = {
+		  .blkcipher = {
+				.min_keysize = AES_MIN_KEY_SIZE,
+				.max_keysize = AES_MAX_KEY_SIZE,
+				.setkey = xlp_ctr_rfc3686_setkey,
+				.encrypt = xlp_ctr_rfc3686_encrypt,
+				.decrypt = xlp_ctr_rfc3686_decrypt,
+				.ivsize = AES_BLOCK_SIZE,
+				}
+		  }
+};
+
+static struct crypto_alg xlp_cbc_aes_alg = {
+	.cra_name = "cbc(aes)",
+	.cra_driver_name = "cbc-aes-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_flags = CRYPTO_ALG_TYPE_BLKCIPHER,
+	.cra_blocksize = AES_BLOCK_SIZE,
+	.cra_ctxsize = (sizeof (struct crypto_session) + 128/* sizeof(struct crypto_param)*/),
+	.cra_alignmask = 15,
+	.cra_type = &crypto_blkcipher_type,
+	.cra_module = THIS_MODULE,
+	.cra_list = LIST_HEAD_INIT(xlp_cbc_aes_alg.cra_list),
+	.cra_u = {
+		  .blkcipher = {
+				.min_keysize = AES_MIN_KEY_SIZE,
+				.max_keysize = AES_MAX_KEY_SIZE,
+				.setkey = xlp_aes_setkey,
+				.encrypt = xlp_cbc_encrypt,
+				.decrypt = xlp_cbc_decrypt,
+				.ivsize = AES_BLOCK_SIZE,
+				}
+		  }
+};
+
+static struct crypto_alg xlp_cbc_des_alg = {
+	.cra_name = "cbc(des)",
+	.cra_driver_name = "cbc-des-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_flags = CRYPTO_ALG_TYPE_BLKCIPHER,
+	.cra_blocksize = DES_BLOCK_SIZE,
+	.cra_ctxsize = (sizeof (struct crypto_session) + 128/* sizeof(struct crypto_param)*/),
+	.cra_alignmask = 15,
+	.cra_type = &crypto_blkcipher_type,
+	.cra_module = THIS_MODULE,
+	.cra_list = LIST_HEAD_INIT(xlp_cbc_des_alg.cra_list),
+	.cra_u = {
+		  .blkcipher = {
+				.min_keysize = DES_KEY_SIZE,
+				.max_keysize = DES_KEY_SIZE,
+				.setkey = xlp_des_setkey,
+				.encrypt = xlp_cbc_encrypt,
+				.decrypt = xlp_cbc_decrypt,
+				.ivsize = DES_BLOCK_SIZE,
+				}
+		  }
+};
+
+static struct crypto_alg xlp_cbc_des3_alg = {
+	.cra_name = "cbc(des3_ede)",
+	.cra_driver_name = "cbc-des3-ede-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_flags = CRYPTO_ALG_TYPE_BLKCIPHER,
+	.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+	.cra_ctxsize = (sizeof (struct crypto_session) + 128/* sizeof(struct crypto_param)*/),
+	.cra_alignmask = 15,
+	.cra_type = &crypto_blkcipher_type,
+	.cra_module = THIS_MODULE,
+	.cra_list = LIST_HEAD_INIT(xlp_cbc_des3_alg.cra_list),
+	.cra_u = {
+		  .blkcipher = {
+				.min_keysize = DES3_EDE_KEY_SIZE,
+				.max_keysize = DES3_EDE_KEY_SIZE,
+				.setkey = xlp_des3_setkey,
+				.encrypt = xlp_cbc_encrypt,
+				.decrypt = xlp_cbc_decrypt,
+				.ivsize = DES3_EDE_BLOCK_SIZE,
+				}
+		  }
+};
+
+int
+xlp_crypt_alg_init(void)
+{
+	int ret = 0;
+	ret = crypto_register_alg(&xlp_cbc_des3_alg);
+	if (ret) {
+		printk(KERN_ERR "\n Modlue not registred");
+		goto err4;
+	}
+	ret = crypto_register_alg(&xlp_cbc_des_alg);
+	if (ret) {
+		printk(KERN_ERR "\n Modlue not registred");
+		goto err3;
+	}
+#if 0
+	ret = crypto_register_alg(&xlp_ctr_aes_alg);
+	if (ret) {
+		printk(KERN_ERR "\n Modlue not registred");
+		goto err2;
+	}
+#endif
+	ret = crypto_register_alg(&xlp_cbc_aes_alg);
+	if (ret) {
+		printk(KERN_ERR "\n Modlue not registred");
+		goto err1;
+	}
+	printk(KERN_NOTICE "Using XLP hardware for AES-Crypto algorithm.\n");
+	return 0;
+      err4:
+	crypto_unregister_alg(&xlp_cbc_des3_alg);
+      err3:
+	crypto_unregister_alg(&xlp_cbc_des_alg);
+#if 0
+      err2:
+	crypto_unregister_alg(&xlp_ctr_aes_alg);
+#endif
+      err1:
+	crypto_unregister_alg(&xlp_cbc_aes_alg);
+
+	printk(KERN_ERR
+	       "\nError: XLP hardware AES/DES/3DES initialization failed.");
+	return 0;
+}
+
+void
+xlp_crypt_alg_fini(void)
+{
+	crypto_unregister_alg(&xlp_cbc_des3_alg);
+	crypto_unregister_alg(&xlp_cbc_des_alg);
+#if 0
+	crypto_unregister_alg(&xlp_ctr_aes_alg);
+#endif
+	crypto_unregister_alg(&xlp_cbc_aes_alg);
+}
+
+EXPORT_SYMBOL(xlp_crypt_alg_init);
+EXPORT_SYMBOL(xlp_crypt_alg_fini);
-- 
1.7.1

