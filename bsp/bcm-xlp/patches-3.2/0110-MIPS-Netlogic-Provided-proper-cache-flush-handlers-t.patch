From cdd198ae536bbe3d18edabb1b0b70b14fd3e5644 Mon Sep 17 00:00:00 2001
From: Ashok Kumar <ashoks@broadcom.com>
Date: Thu, 18 Apr 2013 17:51:45 +0530
Subject: MIPS: Netlogic: Provided proper cache flush handlers to protect from aliases

XLR cache is PIPT whereas XLP is VIPT and suffers from aliases in
specific config.
XLP I has erratas with L1D cache operations and hence using
internal debug registers to flush the entire cache.
XLP II processors support cache operations and uses the same.

Conflicts:
	arch/mips/mm/c-netlogic.c
[Folded in later changes]
[Based on SDK 3.2]
Signed-off-by: Quanyang Wang <quanyang.wang@windriver.com>
Signed-off-by: Nam Ninh <nam.ninh@windriver.com>

diff --git a/arch/mips/mm/c-netlogic.c b/arch/mips/mm/c-netlogic.c
index c4ba549..3fd3052 100644
--- a/arch/mips/mm/c-netlogic.c
+++ b/arch/mips/mm/c-netlogic.c
@@ -33,8 +33,12 @@
 #include <linux/kallsyms.h>
 #include <linux/mm.h>
 #include <linux/module.h>
+#ifdef CONFIG_CPU_XLP
+#include <asm/netlogic/xlp-hal/xlp.h>
+#include <asm/netlogic/xlp-hal/cpucontrol.h>
+#endif
 
-#ifdef CPU_XLR
+#ifdef CONFIG_CPU_XLR
 static inline void sync_istream(void)
 {
 	__asm__ __volatile__ (
@@ -62,7 +66,14 @@ static inline void cacheop_sync_istream(void)
 	cacheop_hazard();
 	sync_istream();
 }
-#else
+
+static inline void nlm_flush_l1_dcache(void)
+{
+	blast_dcache32();
+	cacheop_hazard();
+}
+
+#else /* !CONFIG_CPU_XLR */
 static inline void sync_istream(void)
 {
 	instruction_hazard();
@@ -77,7 +88,225 @@ static inline void cacheop_sync_istream(void)
 {
 	instruction_hazard();
 }
-#endif
+
+#endif /* CONFIG_CPU_XLR */
+
+static inline void nlm_flush_l1_icache(void)
+{
+	blast_icache32();
+	cacheop_sync_istream();
+}
+
+static inline void nlm_flush_l1_icache_page(void *addr)
+{
+	blast_icache32_page((unsigned long)addr);
+	cacheop_sync_istream();
+}
+
+#ifdef CONFIG_CPU_XLP
+/* XLP  errata E28_CPU */
+static inline void nlm_flush_l1_dcache_line(uint32_t line)
+{
+	__asm__ __volatile__ (
+		"       .set push\n"
+		"       .set noat\n"
+		"       .set noreorder\n"
+		"       li $8, "STR(LSU_DEBUG_DATA0)"\n"
+		"       mtcr $0, $8\n"
+		"       li $9, "STR(LSU_DEBUG_ADDR)"\n"
+		"       ori %0, %0, 0x1\n"
+		"       mtcr %0, $9\n"
+		"1:\n"
+		"       mfcr $8, $9\n"
+		"       andi $8, $8, 0x1\n"
+		"       bnez $8, 1b\n"
+		"       nop\n"
+		"       .set pop\n"
+		: : "r"(line) : "$8" , "$9");
+}
+
+static inline void nlm_flush_l1_dcache_hack(void)
+{
+	uint32_t index, line, max;
+	uint32_t cpu = read_c0_ebase() & 0x7f;
+	uint32_t thread = cpu & 0x3;
+
+	max = (thread + 1) * current_cpu_data.dcache.sets;
+	index = thread * current_cpu_data.dcache.sets;
+
+	for (; index < max; index++) {
+
+		line = (index << 5) | (1<<1);
+		nlm_flush_l1_dcache_line(line);
+		line = (1 << 2) | (index << 5) | (1<<1);
+		nlm_flush_l1_dcache_line(line);
+
+		line = (index << 5) | (1<<1) | (0x1 << 14);
+		nlm_flush_l1_dcache_line(line);
+		line = (1 << 2) | (index << 5) | (1<<1) | (0x1 << 14);
+		nlm_flush_l1_dcache_line(line);
+	}
+}
+
+static inline void nlm_flush_l1_dcache(void)
+{
+	if (cpu_is_xlpii())
+		blast_dcache32();
+	else
+		nlm_flush_l1_dcache_hack();
+
+	cacheop_hazard();
+}
+
+static inline void nlm_flush_l1_dcache_page(void *addr)
+{
+	if (cpu_is_xlpii())
+		blast_dcache32_page((unsigned long)addr);
+	else
+		nlm_flush_l1_dcache_hack();
+
+	cacheop_hazard();
+}
+
+static inline void nlm_flush_l1_dcache_range(unsigned long vaddr, int size)
+{
+	if (cpu_is_xlpii())
+		blast_dcache_range(vaddr, vaddr + size);
+	else
+		nlm_flush_l1_dcache_hack();
+
+	cacheop_hazard();
+}
+
+struct flush_cache_page_args {
+	struct vm_area_struct *vma;
+	unsigned long addr;
+	unsigned long pfn;
+};
+
+static inline int has_valid_asid(const struct mm_struct *mm)
+{
+	return cpu_context(smp_processor_id(), mm);
+}
+
+static void local_nlm_flush_cache_mm(void *args)
+{
+	struct mm_struct *mm = args;
+
+	if (!has_valid_asid(mm))
+		return;
+
+	nlm_flush_l1_dcache();
+}
+
+static void nlm_flush_cache_mm(struct mm_struct *mm)
+{
+	if (!cpu_has_dc_aliases)
+		return;
+
+	on_each_cpu(local_nlm_flush_cache_mm, mm, 1);
+}
+
+static inline void local_nlm_flush_cache_range(void *args)
+{
+	struct vm_area_struct *vma = args;
+	int exec = vma->vm_flags & VM_EXEC;
+
+	if (!(has_valid_asid(vma->vm_mm)))
+		return;
+
+	nlm_flush_l1_dcache();
+
+	if (exec)
+		nlm_flush_l1_icache();
+
+}
+
+static void nlm_flush_cache_range(struct vm_area_struct *vma,
+					unsigned long start, unsigned long end)
+{
+	int exec = vma->vm_flags & VM_EXEC;
+
+	if (cpu_has_dc_aliases || (exec && !cpu_has_ic_fills_f_dc))
+		on_each_cpu(local_nlm_flush_cache_range, vma, 1);
+}
+
+static inline void local_nlm_flush_cache_page(void *args)
+{
+	struct flush_cache_page_args *fcp_args = args;
+	unsigned long addr = fcp_args->addr;
+	struct vm_area_struct *vma = fcp_args->vma;
+	int exec = vma->vm_flags & VM_EXEC;
+	struct mm_struct *mm = vma->vm_mm;
+
+	if (!has_valid_asid(mm))
+		return;
+
+	addr &= PAGE_MASK;
+
+	nlm_flush_l1_dcache_page((void *)addr);
+
+	if (exec)
+		nlm_flush_l1_icache_page((void *)addr);
+}
+
+static void nlm_flush_cache_page(struct vm_area_struct *vma,
+				unsigned long addr, unsigned long pfn)
+{
+	struct flush_cache_page_args args;
+
+	args.vma = vma;
+	args.addr = addr;
+	args.pfn = pfn;
+
+	on_each_cpu(local_nlm_flush_cache_page, &args, 1);
+}
+
+static void local_nlm_flush_dcache_page(void *addr)
+{
+	nlm_flush_l1_dcache_page(addr);
+}
+
+static void nlm_flush_dcache_page(unsigned long addr)
+{
+	on_each_cpu(local_nlm_flush_dcache_page, (void *)addr, 1);
+}
+
+static void nlm_flush_cache_vmap(void)
+{
+
+	nlm_flush_l1_dcache();
+}
+
+static void nlm_flush_cache_vunmap(void)
+{
+	nlm_flush_l1_dcache();
+}
+
+struct flush_kernel_vmap_range_args {
+	unsigned long vaddr;
+	int     size;
+};
+
+static inline void local_nlm_flush_cache_vmap_range(void *args)
+{
+	struct flush_kernel_vmap_range_args *vmra = args;
+	unsigned long vaddr = vmra->vaddr;
+	int size = vmra->size;
+
+	nlm_flush_l1_dcache_range(vaddr, size);
+}
+
+static void nlm_flush_cache_vmap_range(unsigned long vaddr, int size)
+{
+	struct flush_kernel_vmap_range_args args;
+
+	args.vaddr = (unsigned long) vaddr;
+	args.size = size;
+
+	on_each_cpu(local_nlm_flush_cache_vmap_range, &args, 1);
+}
+#endif /* CONFIG_CPU_XLP */
 
 /*
  * These routines support Generic Kernel cache flush requirements
@@ -110,7 +339,6 @@ void nlm_flush_icache_range(unsigned long start, unsigned long end)
 {
 	struct flush_icache_range_args args;
 
-	WARN_ON((end - start) > PAGE_SIZE);
 	args.start = start;
 	args.end = end;
 	on_each_cpu(nlm_flush_icache_range_ipi, &args, 1);
@@ -130,53 +358,14 @@ static void nlm_flush_cache_sigtramp(unsigned long addr)
 	on_each_cpu(nlm_flush_cache_sigtramp_ipi, (void *)addr, 1);
 }
 
-/*
- * These routines support MIPS specific cache flush requirements.
- * These are called only during bootup or special system calls
- */
-
-static void nlm_local_flush_icache(void)
-{
-	int i = 0;
-	unsigned long base = CKSEG0;
-	unsigned int lines;
-
-	lines = current_cpu_data.icache.ways *
-				current_cpu_data.icache.sets;
-
-	/* Index Invalidate all the lines and the ways */
-	for (i = 0; i < lines; i++) {
-		flush_icache_line(base);
-		base += cpu_icache_line_size();
-	}
-	cacheop_sync_istream();
-}
-
-static void nlm_local_flush_dcache(void)
-{
-	int i = 0;
-	unsigned long base = CKSEG0;
-	unsigned int lines;
-
-	lines = current_cpu_data.dcache.ways * current_cpu_data.dcache.sets;
-
-	/* Index Invalidate all the lines and the ways */
-	for (i = 0; i < lines; i++) {
-		flush_dcache_line(base);
-		base += current_cpu_data.dcache.linesz;
-	}
-	cacheop_hazard();
-}
-
 static void nlm_flush_l1_caches_ipi(void *info)
 {
-	nlm_local_flush_dcache();
-	nlm_local_flush_icache();
+	nlm_flush_l1_dcache();
+	nlm_flush_l1_icache();
 }
 
 static void nlm_flush_l1_caches(void)
 {
-	pr_err("CACHE FLUSH: flushing L1 caches on all cpus!\n");
 	on_each_cpu(nlm_flush_l1_caches_ipi, (void *)NULL, 1);
 }
 
@@ -201,6 +390,7 @@ static __cpuinit void probe_l1_cache(void)
 	icache_size = c->icache.sets *
 		c->icache.ways * c->icache.linesz;
 	c->icache.waybit = ffs(icache_size/c->icache.ways) - 1;
+	c->icache.waysize = icache_size / c->icache.ways;
 
 	c->dcache.flags = 0;
 
@@ -215,13 +405,27 @@ static __cpuinit void probe_l1_cache(void)
 	dcache_size = c->dcache.sets *
 		c->dcache.ways * c->dcache.linesz;
 	c->dcache.waybit = ffs(dcache_size/c->dcache.ways) - 1;
+	c->dcache.waysize = dcache_size / c->dcache.ways;
+
+#ifdef CONFIG_CPU_XLR
+	c->dcache.flags |= MIPS_CACHE_PINDEX;
+#else
+	if (c->dcache.waysize > PAGE_SIZE)
+		c->dcache.flags |= MIPS_CACHE_ALIASES;
+	if (c->icache.waysize > PAGE_SIZE)
+		c->icache.flags |= MIPS_CACHE_ALIASES;
+#endif
 
 	if (smp_processor_id() == 0) {
 		pr_info("Primary instruction cache %dkB, %d-way, linesize"
 			" %d bytes.\n", icache_size >> 10, c->icache.ways,
 			c->icache.linesz);
-		pr_info("Primary data cache %dkB %d-way, linesize %d bytes.\n",
-			dcache_size >> 10, c->dcache.ways, c->dcache.linesz);
+		pr_info("Primary data cache %dkB %d-way, %s, %s, linesize %d bytes.\n",
+			dcache_size >> 10, c->dcache.ways,
+			(c->dcache.flags & MIPS_CACHE_PINDEX) ? "PIPT" : "VIPT",
+			(c->dcache.flags & MIPS_CACHE_ALIASES) ?
+			"cache aliases" : "no aliases",
+			c->dcache.linesz);
 	}
 }
 
@@ -245,24 +449,21 @@ void __cpuinit nlm_cache_init(void)
 	/* update cpu_data */
 	probe_l1_cache();
 	if (smp_processor_id() != 0) {
-		nlm_local_flush_icache();
+		nlm_flush_l1_icache();
 		coherency_setup();
 		return;
 	}
 
+	shm_align_mask = max_t(unsigned long,
+				current_cpu_data.dcache.waysize - 1,
+				PAGE_SIZE - 1);
+
 	/*
 	 * When does this function get called? Looks like MIPS has some syscalls
 	 * to flush the caches.
 	 */
 	__flush_cache_all = nlm_flush_l1_caches;
 
-	/* flush_cache_all: makes all kernel data coherent.
-	 * This gets called just before changing or removing
-	 * a mapping in the page-table-mapped kernel segment (kmap).
-	 * Physical Cache -> do nothing
-	 */
-	flush_cache_all = nlm_noflush;
-
 	/* flush_icache_range: makes the range of addresses coherent w.r.t
 	 * I-cache and D-cache. This gets called after the instructions are
 	 * written to memory. All addresses are valid kernel or mapped
@@ -276,15 +477,58 @@ void __cpuinit nlm_cache_init(void)
 	 * to be changed. These are closely related to TLB coherency
 	 * (flush_tlb_{mm, range, page})
 	 */
-	flush_cache_mm = (void (*)(struct mm_struct *))nlm_noflush;
-	flush_cache_range = (void *) nlm_noflush;
-	flush_cache_page = (void *) nlm_noflush;
-
-	/*
-	 * flush_icache_page: flush_dcache_page + update_mmu_cache takes care
-	 * of this
-	 */
-	flush_data_cache_page = (void *) nlm_noflush;
+#ifdef CONFIG_CPU_XLP
+	if (cpu_has_dc_aliases) {
+		flush_cache_mm = nlm_flush_cache_mm;
+		flush_cache_range = nlm_flush_cache_range;
+		flush_cache_page = nlm_flush_cache_page;
+
+		/*
+		 * flush_icache_page: flush_dcache_page + update_mmu_cache
+		 * takes care of this
+		 */
+		flush_data_cache_page = nlm_flush_dcache_page;
+
+		/* flush_cache_all: makes all kernel data coherent.
+		 * This gets called just before changing or removing
+		 * a mapping in the page-table-mapped kernel segment (kmap).
+		 * Physical Cache -> do nothing
+		 */
+		flush_cache_all = nlm_flush_l1_caches;
+
+		local_flush_data_cache_page	= local_nlm_flush_dcache_page;
+
+		__flush_cache_vmap = nlm_flush_cache_vmap;
+		__flush_cache_vunmap = nlm_flush_cache_vunmap;
+
+		__flush_kernel_vmap_range = nlm_flush_cache_vmap_range;
+	} else
+#endif
+	{
+		flush_cache_mm = (void (*)(struct mm_struct *))nlm_noflush;
+		flush_cache_range = (void *) nlm_noflush;
+		flush_cache_page = (void *) nlm_noflush;
+
+		/*
+		 * flush_icache_page: flush_dcache_page + update_mmu_cache
+		 * takes care of this
+		 */
+		flush_data_cache_page = (void *) nlm_noflush;
+
+		/* flush_cache_all: makes all kernel data coherent.
+		 * This gets called just before changing or removing
+		 * a mapping in the page-table-mapped kernel segment (kmap).
+		 * Physical Cache -> do nothing
+		 */
+		flush_cache_all = nlm_noflush;
+
+		local_flush_data_cache_page	= (void *)nlm_noflush;
+
+		__flush_cache_vmap = (void *)nlm_noflush;
+		__flush_cache_vunmap = (void *)nlm_noflush;
+
+		__flush_kernel_vmap_range = (void *)nlm_noflush;
+	}
 
 	/*
 	 * flush_cache_sigtramp: flush the single I-cache line with the proper
@@ -299,17 +543,13 @@ void __cpuinit nlm_cache_init(void)
 	flush_icache_all = (void *)nlm_noflush;
 
 	local_flush_icache_range = nlm_local_flush_icache_range;
-	local_flush_data_cache_page	= (void *)nlm_noflush;
-
-	__flush_cache_vmap = (void *)nlm_noflush;
-	__flush_cache_vunmap = (void *)nlm_noflush;
 
 	/* memcpy((void *)(nlm_common_ebase + 0x100), &except_vec2_generic, 0x80); */
 
 	build_clear_page();
 	build_copy_page();
 
-	nlm_local_flush_icache();
+	nlm_flush_l1_icache();
 	coherency_setup();
 	board_cache_error_setup = cache_error_setup;
 }
-- 
1.7.1

