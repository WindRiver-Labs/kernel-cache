From 2078882d788c42d8ef59403919ed88bcba407b50 Mon Sep 17 00:00:00 2001
From: Hareesh R <hareeshr@broadcom.com>
Date: Thu, 29 Mar 2012 19:31:49 +0530
Subject: Multi node support for nae-perf driver. NAE ownership and fifo ownership support in nae-perf driver

[Based on SDK 3.2]
Signed-off-by: Quanyang Wang <quanyang.wang@windriver.com>
Signed-off-by: Nam Ninh <nam.ninh@windriver.com>

diff --git a/drivers/misc/netlogic/nae-perf/init_nae.c b/drivers/misc/netlogic/nae-perf/init_nae.c
index 6dad687..c224ffd 100644
--- a/drivers/misc/netlogic/nae-perf/init_nae.c
+++ b/drivers/misc/netlogic/nae-perf/init_nae.c
@@ -2,31 +2,337 @@
 #include <linux/kernel.h>
 #include <linux/mm.h>
 #include <linux/delay.h>
-
+#include <linux/skbuff.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/inet_lro.h>
 #include <asm/netlogic/msgring.h>
 #include <asm/netlogic/cpumask.h>
-
 #include <asm/netlogic/hal/nlm_hal_fmn.h>
 #include <asm/netlogic/hal/nlm_hal_nae.h>
 #include <asm/netlogic/hal/nlm_hal_xlp_dev.h>
 #include <ops.h>
-
+#include <asm/netlogic/xlp.h>
 #include "net_common.h"
+#include "xlp_nae.h"
+
+unsigned int cpu_2_normal_frfifo[NLM_MAX_NODES][NLM_NCPUS_PER_NODE];
+unsigned int cpu_2_jumbo_frfifo[NLM_MAX_NODES][NLM_NCPUS_PER_NODE];
+static unsigned int lnx_normal_mask;
+static unsigned int lnx_jumbo_mask;
+extern int num_descs_perq; 	
+
+struct nlm_nae_linux_shinfo {
+	int valid;
+	int rxvc;
+	int domid;
+	int mode;
+	int jumbo_enabled;
+	int node;
+	/* logical cpu to physical cpu map */
+	unsigned int lcpu_2_pcpu_map[NLM_NAE_SH_LCPU_TO_MAP_SZ];
+	/* cpu to freein fifo map */
+	unsigned int cpu_2_freeinfifo_map[NLM_NAE_SH_LCPU_TO_MAP_SZ];
+	unsigned int cpu_2_jumbo_freeinfifo_map[NLM_NAE_SH_LCPU_TO_MAP_SZ];
+};
+
+struct nlm_nae_linux_shinfo lnx_shinfo[NLM_NAE_MAX_SHARED_DOMS + 1]; //1 extra for owned domain
+
+
+/* As there is a port level fifo checkup done in NAE h/w, we need to fill up the port
+ fifos ( 0, 4, 8, 12 & 16) with some dummy entries if it is not owned by linux. 
+ If these owned by an app, these dummy entries need to be cleared by the app before reinitializing it
+ */
+static int init_dummy_entries_for_port_fifos(int node, nlm_nae_config_ptr nae_cfg, int jumbo_enabled)
+{
+	static unsigned long long msg;
+	int __attribute__ ((unused)) mflags, code;
+	int rv = 0, vc_index, i, j, ret;
+
+	if(!nae_cfg->dummy_pktdata_addr)
+		return 0;
+
+	msg = (unsigned long long)nae_cfg->dummy_pktdata_addr & 0xffffffffffULL;
+
+	msgrng_access_enable(mflags);
+	for(i = 0; i < nae_cfg->frin_total_queue; i += 4) {
+		/* nothing to do, if it is owned by linux */
+		if((1 << i) & nae_cfg->freein_fifo_dom_mask) 
+			continue;
 
-extern int rely_on_firmware_config;
+		vc_index = i + nae_cfg->frin_queue_base;
 
-int initialize_nae(uint32_t cm0, uint32_t cm1, uint32_t cm2, uint32_t cm3)
+		for(j = 0; j < 4; j++) {
+			if ( (ret = nlm_hal_send_msg1(vc_index, code, msg)) & 0x7) {
+				print_fmn_send_error(__func__, ret);
+				printk("Unable to send configured free desc, check freein carving (qid=%d)\n", vc_index);
+				rv = -1;
+				goto err;
+			}
+		}
+	}
+err:
+	msgrng_access_disable(mflags);
+	return rv;
+}
+
+
+static int nlm_initialize_vfbid(int node, nlm_nae_config_ptr nae_cfg)
+{
+	int cpu =0, tblidx, i = 0;
+	uint32_t vfbid_tbl[128];
+	int start = nae_cfg->vfbtbl_sw_offset;
+	int end = start + nae_cfg->vfbtbl_sw_nentries;
+	int frin_q_base = nlm_node_cfg.nae_cfg[0]->frin_queue_base;
+	
+	/* For s/w replenishment, each nodes tx completes can send to his own node cpus only */
+	for (tblidx = start; tblidx < end ; tblidx++, cpu++) {
+		vfbid_tbl[tblidx] = (cpu * 4) + nae_cfg->fb_vc + (node * 1024);
+	}
+	nlm_config_vfbid_table(node, start, end - start, &vfbid_tbl[start]);
+	/* For h/w replenishment, each node fills up 20 entries for all other nodes
+	starting from node0's queue-id. Software should offset the hw-offset + rx-node id
+	to get the actual index 
+	 */
+	start = nae_cfg->vfbtbl_hw_offset;
+	end = start + nae_cfg->vfbtbl_hw_nentries;
+	for(tblidx = start; tblidx < end; tblidx++, i++) {
+		if(i >= NLM_NAE_MAX_FREEIN_FIFOS_PER_NODE) {
+			i = 0;
+			frin_q_base = 1024 + frin_q_base;
+		}
+		vfbid_tbl[tblidx] = frin_q_base + i;
+	}
+	nlm_config_vfbid_table(node, start, end - start, &vfbid_tbl[start]);
+
+	/* NULL FBID Should map to cpu0 to detect NAE send message errors*/
+	vfbid_tbl[127] = 0;
+	nlm_config_vfbid_table(node, 127, 1, &vfbid_tbl[127]);
+	return 0;
+}
+
+static void dump_lnx_shinfo(int node)
+{
+	int i, pos, bitoff;
+	return;
+	printk("%s(node %d) in, valid %d rxvc %d domid %d jumbo-en %d mode %d node %d\n", 
+			__FUNCTION__, node,
+			lnx_shinfo[0].valid, lnx_shinfo[0].rxvc, lnx_shinfo[0].domid,
+			lnx_shinfo[0].jumbo_enabled, lnx_shinfo[0].mode,  lnx_shinfo[0].node);
+	for(i = 0; i < NLM_NCPUS_PER_NODE; i++) {
+		pos = i / NLM_NAE_SH_LCPU_TO_MAP_SZ;
+		bitoff = (i % NLM_NAE_SH_LCPU_TO_MAP_NVALS_PER_ENTRY) * 
+			NLM_NAE_SH_LCPU_TO_MAP_SNG_VAL_SZ;
+		printk(" node %d lcpu %d pcpu %d rxfifo %d jumbo-rxfifo %d \n", 
+				node, i, (lnx_shinfo[0].lcpu_2_pcpu_map[pos] >> bitoff) & 0x1f,
+				(lnx_shinfo[0].cpu_2_freeinfifo_map[pos] >> bitoff) & 0x1f,
+				(lnx_shinfo[0].cpu_2_jumbo_freeinfifo_map[pos] >> bitoff) & 0x1f
+				);
+	}
+	for(i = 0; i < NLM_NCPUS_PER_NODE; i++) {
+		printk(" node %d cpu %d cpu2nor-fr %d cpu2jum-fr %d\n", node, i, cpu_2_normal_frfifo[node][i],
+				cpu_2_jumbo_frfifo[node][i]);
+	}
+}
+         
+
+int initialize_nae(unsigned int *phys_cpu_map, int mode, int *jumbo_enabled)
 {
 	int dom_id = 0;
 	int node = 0;
 	unsigned long __attribute__ ((unused)) mflags;
+	int i,pos, bitoff;
+	int rv = -1;
+	nlm_nae_config_ptr nae_cfg;
 
 	msgrng_access_enable(mflags);
+
 	nlm_hal_init_nae(fdt, dom_id);
 
-	printk("Overriding HAL POE configuration based on current active cpumask\n");
-	nlm_hal_init_poe_distvec(node, 0, cm0, cm1, cm2, cm3, (1 << nlm_node_cfg.nae_cfg[node]->rx_vc));	//FIXME for multinode
+	for(node = 0; node < NLM_MAX_NODES; node++) {
+		nae_cfg = nlm_node_cfg.nae_cfg[node];
+		if (nae_cfg == NULL) 
+			continue;
+		
+		/* This driver works only with freein-fifo shared mode */
+		if(!nae_cfg->freein_fifo_shared) {
+			printk("%s, Error, Driver works only with freein fifo shared mode\n", __FUNCTION__);
+			goto err;
+		}
+		
 
+		for(i = 0; i <= NLM_NAE_MAX_SHARED_DOMS; i++) {
+			lnx_shinfo[i].valid = nae_cfg->shinfo[i].valid;
+			lnx_shinfo[i].rxvc = nae_cfg->shinfo[i].rxvc;
+			lnx_shinfo[i].domid = nae_cfg->shinfo[i].domid;
+			memcpy(&lnx_shinfo[i].lcpu_2_pcpu_map, nae_cfg->shinfo[i].lcpu_2_pcpu_map, 
+					sizeof(nae_cfg->shinfo[i].lcpu_2_pcpu_map));
+			memcpy(&lnx_shinfo[i].cpu_2_freeinfifo_map, nae_cfg->shinfo[i].cpu_2_freeinfifo_map, 
+					sizeof(nae_cfg->shinfo[i].cpu_2_freeinfifo_map));
+		}
+
+		lnx_normal_mask = nae_cfg->freein_fifo_dom_mask;
+
+		/* if jumbo enabled , we use half of the linux owned freein fifos for jumbo skbs */
+		if(*jumbo_enabled) {
+			int mine = 1;
+			for(i = 0; i < nae_cfg->frin_total_queue; i++) {
+				if((1 << i) & nae_cfg->freein_fifo_dom_mask) {
+					if(mine) {
+						mine = 0;
+						continue;
+					}
+					lnx_normal_mask &= (~(1 << i));
+					lnx_jumbo_mask |= (1 << i);
+					mine = 1;
+				}
+			}
+			if(lnx_jumbo_mask) {
+				nlm_hal_derive_cpu_to_freein_fifo_map(node, phys_cpu_map[node],
+					       	lnx_normal_mask, cpu_2_normal_frfifo[node]);
+				nlm_hal_derive_cpu_to_freein_fifo_map(node, phys_cpu_map[node],
+					       	lnx_jumbo_mask, cpu_2_jumbo_frfifo[node]);
+				memset(lnx_shinfo[0].cpu_2_freeinfifo_map, 0, 
+						sizeof(lnx_shinfo[0].cpu_2_freeinfifo_map));
+				for(i = 0; i < NLM_NCPUS_PER_NODE; i++) {
+					pos = i / NLM_NAE_SH_LCPU_TO_MAP_SZ;
+					bitoff = (i % NLM_NAE_SH_LCPU_TO_MAP_NVALS_PER_ENTRY) * 
+						NLM_NAE_SH_LCPU_TO_MAP_SNG_VAL_SZ;
+					lnx_shinfo[0].cpu_2_freeinfifo_map[pos] |= (cpu_2_normal_frfifo[node][i] << bitoff);
+					lnx_shinfo[0].cpu_2_jumbo_freeinfifo_map[pos] |= (cpu_2_jumbo_frfifo[node][i] << bitoff);
+				}
+			} else {
+				printk("Disabling Jumbo because of unavailability of freein-fifo\n");
+				*jumbo_enabled = 0;
+			}
+		} 
+		if(*jumbo_enabled == 0) {
+			for(i = 0; i < NLM_NCPUS_PER_NODE; i++) {
+				pos = i / NLM_NAE_SH_LCPU_TO_MAP_SZ;
+				bitoff = (i % NLM_NAE_SH_LCPU_TO_MAP_NVALS_PER_ENTRY) *
+					NLM_NAE_SH_LCPU_TO_MAP_SNG_VAL_SZ;
+				cpu_2_normal_frfifo[node][i] = (lnx_shinfo[0].cpu_2_freeinfifo_map[pos] >> bitoff) & 0x1f;
+			}
+		}
+
+		lnx_shinfo[0].mode = mode;
+		lnx_shinfo[0].jumbo_enabled = *jumbo_enabled;
+		lnx_shinfo[0].node = node;
+		if(nae_cfg->owned) {
+			nlm_hal_write_ucore_shared_mem(node, (unsigned int *)lnx_shinfo, sizeof(lnx_shinfo)/sizeof(uint32_t));
+			nlm_hal_restart_ucore(node, fdt);
+		}
+
+		dump_lnx_shinfo(node);
+	}
+
+	for(node = 0; node < NLM_MAX_NODES; node++) {
+		nae_cfg = nlm_node_cfg.nae_cfg[node];
+		if (nae_cfg == NULL) 
+			continue;
+		/* initialize my vfbid table */
+		if(!(nae_cfg->flags & VFBID_FROM_FDT))
+			nlm_initialize_vfbid(node, nae_cfg);
+		
+		if(nae_cfg->owned == 0)
+			continue;
+
+		/* Update RX_CONFIG for desc size */
+		if(*jumbo_enabled)
+			nlm_hal_init_ingress (node, (ETH_JUMBO_DATA_LEN+ETH_HLEN+ETH_FCS_LEN+SMP_CACHE_BYTES) & ~(SMP_CACHE_BYTES - 1));
+		else
+			nlm_hal_init_ingress (node, (ETH_DATA_LEN+ETH_HLEN+ETH_FCS_LEN+SMP_CACHE_BYTES) & ~(SMP_CACHE_BYTES - 1));
+
+		if(init_dummy_entries_for_port_fifos(node, nae_cfg, *jumbo_enabled) != 0)
+			goto err;
+
+	}
+	rv = 0;
+err:
 	msgrng_access_disable(mflags);
-	return 0;
+	return rv;
+}
+
+static int nlm_replenish_per_cpu_buffer(int node, nlm_nae_config_ptr nae_cfg, int qindex, int bufcnt)
+{
+	int i, port;
+	int vc_index = 0;
+	int __attribute__ ((unused)) mflags, code;
+	struct xlp_msg msg;
+	struct sk_buff * skb;
+	int ret = 0;
+	int size = NLM_RX_ETH_BUF_SIZE;
+
+	if((1 << qindex) & lnx_jumbo_mask)
+		size = NLM_RX_JUMBO_BUF_SIZE;
+
+	/* For queue index 16 and 17, we still use  the port level descriptor info */
+	if(qindex >= 16) {
+		for(port = 0; port < nae_cfg->num_ports; port++) {
+			if(nae_cfg->ports[port].hw_port_id == qindex)
+				bufcnt = nae_cfg->ports[port].num_free_desc;
+	 	}
+	}
+
+	for(i = 0; i < bufcnt; i++)
+	{
+		vc_index = qindex + nae_cfg->frin_queue_base;
+		skb = nlm_xlp_alloc_skb_atomic(size);
+		if(!skb)
+		{
+			printk("[%s] alloc skb failed\n",__FUNCTION__);
+			break;
+		}
+		/* Store skb in back_ptr */
+		mac_put_skb_back_ptr(skb);
+		code = 0;
+
+		/* Send the free Rx desc to the MAC */
+		msgrng_access_enable(mflags);
+		msg.entry[0] = (unsigned long long)virt_to_bus(skb->data) & 0xffffffffffULL;
+		msg.entry[1]= msg.entry[2] = msg.entry[3] = 0;
+		/* Send the packet to nae rx  */
+		__sync();
+
+		if ( (ret = nlm_hal_send_msg1(vc_index, code, msg.entry[0])) & 0x7)
+		{
+			print_fmn_send_error(__func__, ret);
+			printk("Unable to send configured free desc, check freein carving (qid=%d)\n", vc_index);
+			/* free the buffer and return! */
+			dev_kfree_skb_any(skb);
+
+			msgrng_access_disable(mflags);
+			ret = -EBUSY;
+			break;
+		}
+		msgrng_access_disable(mflags);
+	}
+	printk("Send %d descriptors for queue %d(vc %d) of length %d\n", bufcnt, qindex, vc_index, size);
+	return ret;
+}
+
+
+int replenish_freein_fifos(void)
+{
+	int node, i, rv;
+	nlm_nae_config_ptr nae_cfg;
+	int max_descs_pqueue, num_descs;
+
+	for(node = 0; node < NLM_MAX_NODES; node++) {
+		nae_cfg = nlm_node_cfg.nae_cfg[node];
+		if (nae_cfg == NULL) 
+			continue;
+
+		/* configure the descs */
+		max_descs_pqueue = nae_cfg->freein_fifo_onchip_num_descs +  nae_cfg->freein_fifo_spill_num_descs;
+		num_descs = num_descs_perq <= max_descs_pqueue ? num_descs_perq : max_descs_pqueue;
+
+		for(i = 0; i < nae_cfg->frin_total_queue; i++) {
+			if(((1 << i) & lnx_normal_mask) || ((1 << i) & lnx_jumbo_mask)) 
+				rv = nlm_replenish_per_cpu_buffer(node, nae_cfg, i, num_descs);
+		}
+		if(rv != 0)
+			break;
+	}
+	return rv;
 }
diff --git a/drivers/misc/netlogic/nae-perf/net_common.h b/drivers/misc/netlogic/nae-perf/net_common.h
index 9cecbcf..d14131f 100644
--- a/drivers/misc/netlogic/nae-perf/net_common.h
+++ b/drivers/misc/netlogic/nae-perf/net_common.h
@@ -15,6 +15,81 @@
 
 #define MAX_DEST_QID            50
 
+
+#if 1
+#include <asm/atomic.h>
+
+#define STATS_SET(x,v)  //atomic64_set((atomic64_t *)&(x), (v))
+#define STATS_ADD(x,v)  //atomic64_add((long)(v), (atomic64_t *)&(x))
+#define STATS_INC(x)    //atomic64_inc((atomic64_t *)&(x))
+#define STATS_READ(x)   (x)//atomic64_read((atomic64_t *)&(x))
+#else
+#define STATS_SET(x,v)  do { (x) = (v); } while (0)
+#define STATS_ADD(x,v)  do { (x) += (v); } while (0)
+#define STATS_INC(x)    do { (x) += 1; } while (0)
+#define STATS_READ(x)   (x)
+#endif
+
+#define XLP_SOC_MAC_DRIVER "XLP Mac Driver"
+
+/* On-Chip NAE PCI Header */
+#undef PCI_NETL_VENDOR
+#define PCI_NETL_VENDOR			0xfecc
+#define PCI_DEVID_BASE			0
+#define PCI_DEVID_OFF_NET		0
+
+#define MAX_GMAC_PORT               	18
+#define XLP_SGMII_RCV_CONTEXT_NUM	8
+
+
+#define  DUMP_PKT(str, x, y)	if (debug == 2)  {	\
+	int i;      					\
+        printk(" %s \n", str);                  	\
+        for(i = 0; i < y; i++)				\
+        {						\
+                printk("%02x ", (x)[i]);		\
+                if( i % 16 == 15)			\
+                        printk("\n");			\
+        }						\
+	printk("\n"); }
+
+
+#define NUM_XLP8XX_MGMT_PORTS	2
+
+#define MAX_TSO_SKB_PEND_REQS	200
+#define MAX_PACKET_SZ_PER_MSG	16384
+#define P2P_EXTRA_DESCS		((PAGE_SIZE / MAX_PACKET_SZ_PER_MSG) + 4)
+#define P2P_SKB_OFF		(MAX_SKB_FRAGS + P2P_EXTRA_DESCS - 1)
+#define tso_dbg(fmt, args...)	//printk(fmt, ##args);
+#define RX_UNCLASSIFIED_PKT 	(1<<5)
+#define RX_IP_CSUM_VALID 	(1<<3)
+#define RX_TCP_CSUM_VALID 	(1<<2)
+
+struct p2p_desc_mem {
+	void *mem;
+	uint64_t dsize;
+	uint64_t pad[6];
+};
+
+enum msc_opcodes { IP_CHKSUM = 1,
+	TCP_CHKSUM,
+	UDP_CHKSUM,
+	SCTP_CRC,
+	FCOE_CRC,
+	IP_TCP_CHKSUM,
+	TSO_IP_TCP_CHKSUM,
+	IP_UDP_CHKSUM,
+	IP_CHKSUM_SCTP_CRC
+};
+
+#define ETHER_FRAME_MIN_LEN	64
+
+/* Use index of 8 as the offset because of n64 abi and 64B cacheline size */
+#define CPU_INDEX(cpu) ((cpu) * 8)
+
+
+#define napi_dbg(fmt, args...) { }
+
 typedef struct fmn_credit_struct {
    unsigned int   s_qid;
    unsigned int   d_qid;
@@ -173,7 +248,9 @@ struct nae_config {
 	struct nae_port ports[18];
 };
 
-extern int initialize_nae(uint32_t cm0, uint32_t cm1, uint32_t cm2, uint32_t cm3);
+extern int initialize_nae(unsigned int *phys_cpu_map, int mode, int *jumbo_enabled);
 extern void nlm_xlp_msgring_int_handler(unsigned int irq, struct pt_regs *regs);
+extern int replenish_freein_fifos(void);
+
 
 #endif
diff --git a/drivers/misc/netlogic/nae-perf/xlp_nae.c b/drivers/misc/netlogic/nae-perf/xlp_nae.c
index c017b10..20268a1 100755
--- a/drivers/misc/netlogic/nae-perf/xlp_nae.c
+++ b/drivers/misc/netlogic/nae-perf/xlp_nae.c
@@ -61,51 +61,11 @@
 #include <asm/netlogic/hal/nlm_hal_nae.h>
 #include <asm/netlogic/xlp.h>
 #include <asm/netlogic/xlp_irq.h>
+#include <asm/netlogic/hal/nlm_eeprom.h>
 
 #include "net_common.h"
 #include "xlp_nae.h"
 
-#if 1
-#include <asm/atomic.h>
-
-#define STATS_SET(x,v)  //atomic64_set((atomic64_t *)&(x), (v))
-#define STATS_ADD(x,v)  //atomic64_add((long)(v), (atomic64_t *)&(x))
-#define STATS_INC(x)    //atomic64_inc((atomic64_t *)&(x))
-#define STATS_READ(x)   (x)//atomic64_read((atomic64_t *)&(x))
-#else
-#define STATS_SET(x,v)  do { (x) = (v); } while (0)
-#define STATS_ADD(x,v)  do { (x) += (v); } while (0)
-#define STATS_INC(x)    do { (x) += 1; } while (0)
-#define STATS_READ(x)   (x)
-#endif
-
-#define XLP_SOC_MAC_DRIVER "XLP Mac Driver"
-
-/* On-Chip NAE PCI Header */
-#define PCI_NETL_VENDOR			0xfecc
-#define PCI_DEVID_BASE			0
-#define PCI_DEVID_OFF_NET		0
-
-#define MAX_NET_INF             	1
-#define MAX_GMAC_PORT               	18
-#define XLP_SGMII_RCV_CONTEXT_NUM	8
-
-/* FMN send failure errors */
-#define MSG_DST_FC_FAIL                 0x01
-#define MSG_INFLIGHT_MSG_EX             0x02
-#define MSG_TXQ_FULL                    0x04
-
-#define  DUMP_PKT(str, x, y)	if (debug == 2)  {	\
-	int i;      					\
-        printk(" %s \n", str);                  	\
-        for(i = 0; i < y; i++)				\
-        {						\
-                printk("%02x ", (x)[i]);		\
-                if( i % 16 == 15)			\
-                        printk("\n");			\
-        }						\
-	printk("\n"); }
-
 #define NLM_TCP_MODE	1
 #define NLM_RT_MODE	2
 
@@ -115,9 +75,8 @@
 /*Enable sanity checks while receiving or transmitting buffer */
 #undef ENABLE_SANITY_CHECKS
 
-#define NUM_XLP8XX_MGMT_PORTS	2
 
-static int debug = 0;
+int debug = 0;
 /* Module Parameters */
 
 static int perf_mode= NLM_TCP_MODE;
@@ -129,7 +88,7 @@ module_param(enable_lro, int, 0);
 static int enable_napi =  0;
 module_param(enable_napi, int, 0);
 
-static int num_descs_perq = 500;
+int num_descs_perq = 500;
 module_param(num_descs_perq, int, 0);
 
 static int enable_jumbo = 0;
@@ -140,93 +99,39 @@ module_param(enable_jumbo, int, 0);
  * Below parameters are set during FDT file parsing
  */
 
-static uint32_t frin_queue_base = 1000;
-static uint32_t frin_total_queue = 18;
-static uint32_t nae_rx_vc = 0;
-static uint32_t nae_fb_vc = 0;
+extern uint32_t nae_rx_vc;
+extern uint32_t nae_fb_vc;
 static uint32_t napi_vc_mask;
-static uint32_t num_cpu_share_freein = 2;
-static uint32_t jumbo_freein_offset = 0;
 /***************************************************************/
 
-unsigned char eth_hw_addr[18][6] = {
-	{0x00,0x01,0x02,0x03,0x04,0x05},
-	{0x00,0x01,0x02,0x03,0x04,0x06},
-	{0x00,0x01,0x02,0x03,0x04,0x07},
-	{0x00,0x01,0x02,0x03,0x04,0x08},
-	{0x00,0x01,0x02,0x03,0x04,0x09},
-	{0x00,0x01,0x02,0x03,0x04,0x0A},
-	{0x00,0x01,0x02,0x03,0x04,0x0B},
-	{0x00,0x01,0x02,0x03,0x04,0x0C},
-	{0x00,0x01,0x02,0x03,0x04,0x0D},
-	{0x00,0x01,0x02,0x03,0x04,0x0E},
-	{0x00,0x01,0x02,0x03,0x04,0x0F},
-	{0x00,0x01,0x02,0x03,0x04,0x10},
-	{0x00,0x01,0x02,0x03,0x04,0x11},
-	{0x00,0x01,0x02,0x03,0x04,0x12},
-	{0x00,0x01,0x02,0x03,0x04,0x13},
-	{0x00,0x01,0x02,0x03,0x04,0x14},
-	{0x00,0x01,0x02,0x03,0x04,0x15},
-	{0x00,0x01,0x02,0x03,0x04,0x16}
-};
-
-/* Use index of 8 as the offset because of n64 abi and 64B cacheline size */
-#define LAST_RCVD_INDEX(cpu) ((cpu) * 8)
-
-uint64_t nlm_mode[NR_CPUS*8] ____cacheline_aligned;
-struct sk_buff *last_rcvd_skb[NR_CPUS * 8] ____cacheline_aligned;
-uint32_t last_rcvd_len[NR_CPUS * 8] ____cacheline_aligned;
-uint64_t last_rcvd_skb_phys[NR_CPUS * 8] ____cacheline_aligned;
-uint64_t receive_count[NR_CPUS * 8] __cacheline_aligned;
-uint64_t fast_replenish_count[NR_CPUS * 8] __cacheline_aligned;
-uint64_t slow_replenish_count[NR_CPUS * 8] __cacheline_aligned;
-uint64_t err_replenish_count[NR_CPUS * 8] __cacheline_aligned;
-//make this array of 24 ports to keep it cacheline aligned.
-struct net_device *per_cpu_netdev[NR_CPUS][24] __cacheline_aligned;
-
-#define ETHER_FRAME_MIN_LEN	64
+unsigned char eth_hw_addr[NLM_MAX_NODES][MAX_GMAC_PORT][6];
+static unsigned int phys_cpu_map[NLM_MAX_NODES];
+extern uint32_t cpu_2_normal_frfifo[NLM_MAX_NODES][NLM_NCPUS_PER_NODE];
+extern uint32_t cpu_2_jumbo_frfifo[NLM_MAX_NODES][NLM_NCPUS_PER_NODE];
+
+static uint64_t nlm_mode[NR_CPUS*8] ____cacheline_aligned;
+static struct sk_buff *last_rcvd_skb[NR_CPUS * 8] ____cacheline_aligned;
+static uint32_t last_rcvd_len[NR_CPUS * 8] ____cacheline_aligned;
+static uint32_t last_rcvd_node[NR_CPUS * 8] ____cacheline_aligned;
+static uint64_t last_rcvd_skb_phys[NR_CPUS * 8] ____cacheline_aligned;
+static uint64_t receive_count[NR_CPUS * 8] __cacheline_aligned;
+static uint64_t fast_replenish_count[NR_CPUS * 8] __cacheline_aligned;
+static uint64_t slow_replenish_count[NR_CPUS * 8] __cacheline_aligned;
+static uint64_t err_replenish_count[NR_CPUS * 8] __cacheline_aligned;
+static struct net_device *per_cpu_netdev[NLM_MAX_NODES][NR_CPUS][24] __cacheline_aligned;
 static struct pci_device_id soc_pci_table[] __devinitdata = {
         {PCI_NETL_VENDOR, PCI_DEVID_BASE + PCI_DEVID_OFF_NET,
          PCI_ANY_ID, PCI_ANY_ID, 0},
         {}
 };
-
-#define MAX_TSO_SKB_PEND_REQS	200
-#define MAX_PACKET_SZ_PER_MSG	16384
-#define P2P_EXTRA_DESCS		((PAGE_SIZE / MAX_PACKET_SZ_PER_MSG) + 4)
-#define P2P_SKB_OFF		(MAX_SKB_FRAGS + P2P_EXTRA_DESCS - 1)
-#define tso_dbg(fmt, args...)	//printk(fmt, ##args);
-#define RX_UNCLASSIFIED_PKT 	(1<<5)
-#define RX_IP_CSUM_VALID 	(1<<3)
-#define RX_TCP_CSUM_VALID 	(1<<2)
-
 static uint64_t dbg_tcp_rx_cons[NR_CPUS * 8] __cacheline_aligned;
 static uint64_t p2p_dynamic_alloc_cnt[NR_CPUS * 8] __cacheline_aligned;
-#define CPU_INDEX(x) (x * 8)
-
-struct p2p_desc_mem {
-	void *mem;
-	uint64_t dsize;
-	uint64_t pad[6];
-};
-struct p2p_desc_mem p2p_desc_mem[NR_CPUS] __cacheline_aligned;
-enum msc_opcodes { IP_CHKSUM = 1,
-	TCP_CHKSUM,
-	UDP_CHKSUM,
-	SCTP_CRC,
-	FCOE_CRC,
-	IP_TCP_CHKSUM,
-	TSO_IP_TCP_CHKSUM,
-	IP_UDP_CHKSUM,
-	IP_CHKSUM_SCTP_CRC
-};
-
-
-#define napi_dbg(fmt, args...) { }
+static struct p2p_desc_mem p2p_desc_mem[NR_CPUS] __cacheline_aligned;
+static struct net_device_stats *nlm_xlp_mac_get_stats(struct net_device *dev);
+static struct net_device *dev_mac[NLM_MAX_NODES][MAX_GMAC_PORT];
 
 extern void xlp_set_ethtool_ops(struct net_device *netdev);
 extern void xlp_get_mac_stats(struct net_device* dev, struct net_device_stats* stats);
-spinlock_t  nlm_xlp_nae_lock;
 static void nlm_xlp_nae_init(void);
 static int xlp_mac_proc_read(char *page, char **start, off_t off,int count, int *eof, void *data);
 static int  nlm_xlp_nae_open (struct net_device *dev);
@@ -238,19 +143,16 @@ static int  nlm_xlp_nae_change_mtu(struct net_device *dev, int new_mtu);
 static void  nlm_xlp_nae_tx_timeout (struct net_device *dev);
 static void xlp_mac_setup_hwaddr(struct dev_data *priv);
 static int nlm_xlp_nae_set_hwaddr(struct net_device *dev, void *p);
-
+extern void nlm_xlp_mac_set_enable(struct dev_data *priv, int flag);
+extern struct proc_dir_entry *nlm_root_proc;
+extern struct eeprom_data * get_nlm_eeprom(void);
 #ifdef  ENABLE_NAE_PIC_INT
 static irqreturn_t nlm_xlp_nae_int_handler(int irq, void * dev_id);
 #endif
 
-static void nlm_xlp_mac_timer(unsigned long data);
-static struct net_device_stats *nlm_xlp_mac_get_stats(struct net_device *dev);
-
-static struct net_device *dev_mac[MAX_GMAC_PORT];
-
-extern struct proc_dir_entry *nlm_root_proc;
+#define Message(fmt, args...) { }
+//#define Message(fmt, args...) printk(fmt, ##args)
 
-extern void nlm_xlp_mac_set_enable(struct dev_data *priv, int flag);
 
 static unsigned short  nlm_select_queue(struct net_device *dev, struct sk_buff *skb)
 {
@@ -281,26 +183,52 @@ static __inline__ void cpu_halt(void)
 		);
 }
 
-static __inline__ void print_fmn_send_error(const char *str, uint32_t send_result)
+static void gen_mac_address(void)
 {
-	if (debug < 1) return;
+	struct eeprom_data *nlm_eeprom=NULL;
+	unsigned char mac_base[6],temp,buf_write[2],buf0_read[2],buf1_read[2];
+	int if_mac_set=0,mac0_set=0, mac1_set=0;
+	int i,j;
+	buf_write[0]= MAC_MAGIC_BYTE0;
+	buf_write[1]= MAC_MAGIC_BYTE1;
+
+	memset(mac_base, '0', 6);
+	nlm_eeprom = get_nlm_eeprom();
+
+	eeprom_get_magic_bytes(nlm_eeprom,buf0_read,0);/* signature*/
+	eeprom_get_magic_bytes(nlm_eeprom,buf1_read,1);
 
-	if(send_result & MSG_DST_FC_FAIL)
+	if((buf0_read[0]==buf_write[0]) && (buf0_read[1]==buf_write[1]))/*match the signature*/
 	{
-		printk("[%s] Msg Destination flow control credit fail(send_result=%08x)\n",
-		       str, send_result);
+		mac0_set=1;
+		eeprom_get_mac_addr(nlm_eeprom, mac_base,0);/* get the mac address*/
 	}
-	else if (send_result & MSG_INFLIGHT_MSG_EX) {
-		printk("[%s] MSG_INFLIGHT_MSG_EX(send_result=%08x)\n", __func__, send_result);
+	else if((buf1_read[0]==buf_write[0]) && (buf1_read[1]==buf_write[1]))
+	{
+		mac1_set=1;
+		eeprom_get_mac_addr(nlm_eeprom, mac_base,1);/* get the mac address*/
 	}
-	else if (send_result & MSG_TXQ_FULL) {
-		printk("[%s] TX message Q full(send_result=%08x)\n", __func__, send_result);
+
+	for(temp=0;temp<6;temp++)
+	{
+		if(mac_base[temp]!=0)
+		{
+			if_mac_set=1;
+		}
 	}
-	else {
-		printk("[%s] Unknown send error type(send_result=%08x)\n", __func__, send_result);
+	if( ((mac0_set | mac1_set) && if_mac_set) == 0){
+		random_ether_addr(mac_base);
+	}
+	for(i=0 ; i<NLM_MAX_NODES; i++){ /*poppulate the eth_hw_add array according to the get mac address*/
+		for(j=0;j<18;j++){
+			memcpy(eth_hw_addr[i][j], mac_base, 6);
+			mac_base[5] += 1;
+		}
 	}
 }
 
+
+
 static __inline__ struct sk_buff *mac_get_skb_back_ptr(uint64_t addr)
 {
         uint64_t *back_ptr = (uint64_t *)(addr - SKB_BACK_PTR_SIZE);
@@ -310,16 +238,6 @@ static __inline__ struct sk_buff *mac_get_skb_back_ptr(uint64_t addr)
         return (struct sk_buff *)(*back_ptr);
 }
 
-static __inline__ void mac_put_skb_back_ptr(struct sk_buff *skb)
-{
-        uint64_t *back_ptr = (uint64_t *)skb->data;
-
-        /* this function should be used only for newly allocated packets. It assumes
-         * the first cacheline is for the back pointer related book keeping info
-         */
-        skb_reserve(skb, SKB_BACK_PTR_SIZE);
-        *back_ptr = (uint64_t)skb;
-}
 
 #define CACHELINE_ALIGNED_ADDR(addr) (((unsigned long)(addr)) & ~(CACHELINE_SIZE-1))
 
@@ -336,53 +254,6 @@ static __inline__ void *cacheline_aligned_kmalloc(int size, int gfp_mask)
 						    CACHELINE_SIZE));
         return buf;
 }
-#if 0
-/**********************************************************************
- * nlm_xlp_alloc_skb -  64 bits cache aligned skb buffer allocate
- * return - skb buffer address
- *
- **********************************************************************/
-static __inline__ struct sk_buff *nlm_xlp_alloc_skb(void)
-{
-        int offset = 0;
-        struct sk_buff *skb = __dev_alloc_skb(NLM_RX_ETH_BUF_SIZE, GFP_KERNEL);
-
-        if (!skb) {
-                return NULL;
-        }
-        /* align the data to the next cache line */
-        offset = ((unsigned long)skb->data + CACHELINE_SIZE) &
-                ~(CACHELINE_SIZE - 1);
-        skb_reserve(skb, (offset - (unsigned long)skb->data));
-#ifdef CONFIG_NLM_NET_OPTS
-        skb->netl_skb = skb;
-#endif
-        return skb;
-}
-#endif
-/**********************************************************************
- * nlm_xlp_alloc_skb_atomic -  Atomically allocates 64 bits cache aligned skb buffer
- * return - skb buffer address
- *
- **********************************************************************/
-static __inline__ struct sk_buff *nlm_xlp_alloc_skb_atomic(int size)
-{
-        int offset = 0;
-        struct sk_buff *skb = __dev_alloc_skb(size, GFP_ATOMIC);
-
-        if (!skb) {
-                return NULL;
-        }
-
-        /* align the data to the next cache line */
-        offset = ((unsigned long)skb->data + CACHELINE_SIZE) &
-                ~(CACHELINE_SIZE - 1);
-        skb_reserve(skb, (offset - (unsigned long)skb->data));
-#ifdef CONFIG_NLM_NET_OPTS
-        skb->netl_skb = skb;
-#endif
-        return skb;
-}
 
 /*********************************************************************
   * set tso enable features in the dev list
@@ -518,7 +389,6 @@ static __inline__ uint64_t nae_tso_desc1(
 
 }
 
-#define FLOW_TABLE3_CFG 0x86
 #define NUM_VC_PER_THREAD 4
 #define NUM_CPU_VC	  128
 #define RX_PARSER_EN 	1
@@ -530,9 +400,6 @@ static void nlm_enable_l3_l4_parser(int node)
 	int port = 0, i, ipchk = 1;
 	uint32_t val = 0;
 	uint32_t naereg;
-	int dstvc, cpu, num_cpus = 0;
-       	int max_key_size = 40; /* Ip dst, src, tcp src and dst port(12), reminining padded to zero */
-	int crcpoly = 0xffff;//0xbaad; //0xbb3d;
 
 	//enabling hardware parser
 	naereg = nlm_hal_read_nae_reg(node, RX_CONFIG);
@@ -554,35 +421,6 @@ static void nlm_enable_l3_l4_parser(int node)
 	val = ((0 << 21) | (2 << 17) | (2 << 11) | (2 << 7)); /* extract source and dst port*/
 	nlm_hal_write_nae_reg(node, L4_CTABLE_0_1, val);
 
-	/* Configure flow table 1 to the num cpus as the modular */
-	for(i = 0; i < NR_CPUS; i++) {
-		if(cpu_isset(i, cpu_present_map))
-			num_cpus++;
-	}
-	for(i = 0; i < 20; i++) {
-		nlm_hal_write_nae_reg(node, 0x84, i);
-		nlm_hal_write_nae_reg(node, 0x84, (num_cpus << 20)| i);
-	}
-
-
-	/* Using hash based distribution, Configure the flow table to send packets to the nae_rx_vc of
-	 each online cpu */
-	dstvc = nae_rx_vc;
-	for(i = 0; i < 512;) {
-		cpu = dstvc / NUM_VC_PER_THREAD;
-		if(cpu_isset(cpu, cpu_present_map)) {
-			nlm_hal_write_nae_reg(node, FLOW_TABLE3_CFG, 0 << 31 | i);
-			nlm_hal_write_nae_reg(node, FLOW_TABLE3_CFG, dstvc << 12 | i);
-			i++;
-		}
-		dstvc += NUM_VC_PER_THREAD;
-		if(dstvc >= NUM_CPU_VC) /* max vcs in a core */
-			dstvc = nae_rx_vc;
-	}
-	val = nlm_hal_read_nae_reg(node, 0x2f);
-	val &= (~0x3fffff) ;
-	nlm_hal_write_nae_reg(node, 0x2f, (max_key_size << 16) | crcpoly | val);
-
 }
 
 static int lro_get_skb_hdr(struct sk_buff *skb, void **iphdr, void **tcph,
@@ -602,33 +440,6 @@ static int lro_get_skb_hdr(struct sk_buff *skb, void **iphdr, void **tcph,
 	return 0;
 }
 
-void dump_parser_config(int node)
-{
-	int i;
-	uint32_t val;
-
-	val = nlm_hal_read_nae_reg(node, 0x2f);
-	printk("Crc config %x crc %x keysize %d Flowidx %d Dstidx %d\n", val,
-			(val & 0xffff), (val >> 16) & 0x3f, (val >> 22) & 0x01, (val >> 23) & 0x01);
-
-	printk("Flow table 1 cfg\n");
-	for(i = 0; i < 20; i++) {
-		nlm_hal_write_nae_reg(node, 0x84, 1 << 31 | i);
-		val = nlm_hal_read_nae_reg(node, 0x84);
-		printk("reg %x val %x index %d (%x:%x)\n",
-				0x84, val, i, (val >> 8) & 0x1ff, (val >> 20) & 0x1ff);
-	}
-
-	printk("Flow table 3 cfg\n");
-	for(i = 0; i < 50; i++) {
-		nlm_hal_write_nae_reg(node, 0x86, 1 << 31 | i);
-		val = nlm_hal_read_nae_reg(node, 0x86);
-		printk("reg %x val %x index %d (%x:%x)\n",
-				0x86, val, i, (val >> 12) & 0x1ff, (val >> 22) & 0x1ff);
-	}
-}
-
-
 void lro_init(struct net_device *dev)
 {
 	struct dev_data* priv;
@@ -658,7 +469,6 @@ void lro_init(struct net_device *dev)
 	if(!done) {
 		done = 1;
 		nlm_enable_l3_l4_parser(priv->node);
-		//dump_parser_config(priv->node);
 	}
 }
 
@@ -680,11 +490,42 @@ static void dump_skbuff (struct sk_buff *skb)
 }
 #endif
 
-static int mac_refill_frin_skb(uint64_t paddr, int qid)
+#ifdef CONFIG_NLM_NET_OPTS
+/* Get the hardware replenishment queue id */
+static int get_hw_frfifo_queue_id(int rxnode, nlm_nae_config_ptr nae_cfg, int cpu, unsigned int truesize)
+{
+	int qid;
+	int node_cpu = cpu % NLM_NCPUS_PER_NODE;
+
+	qid = cpu_2_normal_frfifo[rxnode][node_cpu];
+	if (enable_jumbo)
+	{
+		if(truesize > NLM_RX_JUMBO_BUF_SIZE) 
+			qid = cpu_2_jumbo_frfifo[rxnode][node_cpu];
+	}
+	/* all the nodes vfbtable should be filled with starting node of 0 to ending node 
+	 with 20 entries each */
+	return nae_cfg->vfbtbl_hw_offset + (rxnode * NLM_NAE_MAX_FREEIN_FIFOS_PER_NODE) + qid;
+}
+#endif
+
+static int mac_refill_frin_skb(int node, int cpu, uint64_t paddr, unsigned int bufsize)
 {
-       int ret, code;
+	int ret, code, qid;
+	nlm_nae_config_ptr nae_cfg;
+	int node_cpu = cpu % NLM_NCPUS_PER_NODE;
+
+	qid = (bufsize >= NLM_RX_JUMBO_BUF_SIZE) ? cpu_2_jumbo_frfifo[node][node_cpu] : cpu_2_normal_frfifo[node][node_cpu];
+	
+	nae_cfg = nlm_node_cfg.nae_cfg[node];
+	if(nae_cfg == NULL) {
+		printk("%s Error, Invalid node id %d\n", __FUNCTION__, node);
+		return -1;
+	}
+	Message("%s in cpu %d bufsize %d node %d qid %d qbase %d\n", __FUNCTION__, cpu, bufsize, node, qid,  nae_cfg->frin_queue_base);
 
 	ret = 0;
+	qid = nae_cfg->frin_queue_base + qid;
 
 	/* Assumption: SKB is all set to go */
 	/* Send the free Rx desc to the MAC */
@@ -699,42 +540,31 @@ static int mac_refill_frin_skb(uint64_t paddr, int qid)
 	return ret;
 }
 
-static int mac_refill_frin_one_buffer(struct net_device *dev, int cpu, int queue_id)
+static int mac_refill_frin_one_buffer(struct net_device *dev, int cpu, unsigned int truesize)
 {
-	struct dev_data* priv;
-	struct net_device *ndev;
+	struct dev_data* priv = netdev_priv(dev);
 	struct sk_buff * skb;
-	int buf_size;
+	int buf_size = NLM_RX_ETH_BUF_SIZE;
 
 	if (enable_jumbo)
 	{
-		if ((queue_id -  frin_queue_base) < jumbo_freein_offset)
-			buf_size = NLM_RX_ETH_BUF_SIZE;
-		else
+		if(truesize > NLM_RX_JUMBO_BUF_SIZE) 
 			buf_size = NLM_RX_JUMBO_BUF_SIZE; 
 	}
-	else
-		buf_size = NLM_RX_ETH_BUF_SIZE;
 
 	skb = nlm_xlp_alloc_skb_atomic(buf_size);
 	if(!skb)
-	  {
-	    printk("[%s] alloc skb failed\n",__FUNCTION__);
-	    panic("panic...");
-	    return -ENOMEM;
-	  }
-
-	ndev = (struct net_device *)dev;
-	priv = netdev_priv(ndev);
-	skb->dev = ndev;
+	{
+		printk("[%s] alloc skb failed\n",__FUNCTION__);
+		panic("panic...");
+		return -ENOMEM;
+	}
 
-#ifdef CONFIG_NLM_NET_OPTS
-	skb->queue_id = queue_id;
-#endif
+	skb->dev = dev;
 
 	mac_put_skb_back_ptr(skb);
-	
-	return mac_refill_frin_skb((unsigned long long)virt_to_bus(skb->data), queue_id);
+
+	return mac_refill_frin_skb(priv->node, cpu, (unsigned long long)virt_to_bus(skb->data), buf_size);
 }
 
 static int nae_proc_read(char *page, char **start, off_t off,
@@ -746,24 +576,24 @@ static int nae_proc_read(char *page, char **start, off_t off,
 
 	for(i=0; i<32; i++){
 		printk("cpu%d, recv %ld fast_repl %ld, slow_repl %ld, err_repl %ld tcprxcons %lld p2pdalloc %lld\n",i,
-			(unsigned long)receive_count[LAST_RCVD_INDEX(i)],
-			(unsigned long)fast_replenish_count[LAST_RCVD_INDEX(i)],
-			(unsigned long)slow_replenish_count[LAST_RCVD_INDEX(i)],
-			(unsigned long)err_replenish_count[LAST_RCVD_INDEX(i)],
+			(unsigned long)receive_count[CPU_INDEX(i)],
+			(unsigned long)fast_replenish_count[CPU_INDEX(i)],
+			(unsigned long)slow_replenish_count[CPU_INDEX(i)],
+			(unsigned long)err_replenish_count[CPU_INDEX(i)],
 			dbg_tcp_rx_cons[CPU_INDEX(i)],
 			p2p_dynamic_alloc_cnt[CPU_INDEX(i)]);
 
-		total_err += err_replenish_count[LAST_RCVD_INDEX(i)];
-		total_fast += fast_replenish_count[LAST_RCVD_INDEX(i)];
-		total_slow += slow_replenish_count[LAST_RCVD_INDEX(i)];
-		total_recv += receive_count[LAST_RCVD_INDEX(i)];
+		total_err += err_replenish_count[CPU_INDEX(i)];
+		total_fast += fast_replenish_count[CPU_INDEX(i)];
+		total_slow += slow_replenish_count[CPU_INDEX(i)];
+		total_recv += receive_count[CPU_INDEX(i)];
 
 		dbg_tcp_rx_cons[CPU_INDEX(i)] = 0;
 		p2p_dynamic_alloc_cnt[CPU_INDEX(i)] = 0;
-		slow_replenish_count[LAST_RCVD_INDEX(i)] = 0;
-		fast_replenish_count[LAST_RCVD_INDEX(i)] = 0;
-		err_replenish_count[LAST_RCVD_INDEX(i)] = 0;
-		receive_count[LAST_RCVD_INDEX(i)] = 0;
+		slow_replenish_count[CPU_INDEX(i)] = 0;
+		fast_replenish_count[CPU_INDEX(i)] = 0;
+		err_replenish_count[CPU_INDEX(i)] = 0;
+		receive_count[CPU_INDEX(i)] = 0;
 	}
 	/*check how many hash are empty...*/
 	printk("TOTAL_FAST_REPL %ld, TOTAL_SLOW_REPL %ld, TOTAL_ERR_REPL %ld TOTAL_RECV %ld\n",
@@ -807,6 +637,7 @@ xlp_poll_upper(int cpu)
 			if(status) break;
 			__sync();
 
+			Message("poll upper cpu %d src_id %d size %d\n", cpu, src_id, size);
 			/* Process Transmit Complete, addr is the skb pointer */
 			addr = msg0 & 0xffffffffffULL;
 
@@ -820,8 +651,9 @@ xlp_poll_upper(int cpu)
 			context = (msg0 >> 40) & 0x3fff;
 			node = (src_id >> 10) & 0x3;
 			port = *(cntx2port[node] + context);
+
 #ifdef TSO_ENABLED
-			if(nlm_mode[LAST_RCVD_INDEX(cpu)] == NLM_TCP_MODE){
+			if(nlm_mode[CPU_INDEX(cpu)] == NLM_TCP_MODE){
 				p2pfbdesc = bus_to_virt(addr);
 				skb = (struct sk_buff *)(p2pfbdesc[P2P_SKB_OFF]);
 				free_p2p_desc_mem(cpu, p2pfbdesc);
@@ -862,8 +694,7 @@ static int xlp_poll_lower(int budget, int cpu)
 	struct dev_data *priv = NULL;
 	uint64_t vaddr;
 	struct sk_buff* skb;
-	uint32_t src_id, size, code;
-	uint32_t last_rcvd_queue_id = -1;
+	uint32_t src_id, size, code, truesize;
 
 #ifdef CONFIG_INET_LRO
 	int lro_flush_priv_cnt = 0, i;
@@ -894,20 +725,21 @@ static int xlp_poll_lower(int budget, int cpu)
 		addr	= msg1 & 0xffffffffc0ULL;
 		len	= (msg1 >> 40) & 0x3fff;
 		context = (msg1 >> 54) & 0x3ff;
+		node = (src_id >> 10) & 0x3;
 
+		Message("poll lower cpu %d src_id %d size %d len %d context %d node %d err %d\n", 
+				cpu, src_id, size, len, context, node, err);
 		if (err) {
 
 			vaddr = (uint64_t)bus_to_virt(addr);
 			skb = mac_get_skb_back_ptr(vaddr);
-#ifdef CONFIG_NLM_NET_OPTS
-			mac_refill_frin_skb(addr, skb->queue_id);
-#endif
+			mac_refill_frin_skb(node, cpu, addr, skb->truesize);
 			/*
 			Commenting as priv is not available here
 			STATS_INC(priv->stats.rx_errors);
 			STATS_INC(priv->stats.rx_dropped);
 			*/
-			err_replenish_count[LAST_RCVD_INDEX(cpu)]++;
+			err_replenish_count[CPU_INDEX(cpu)]++;
 			continue;
 		}
 
@@ -915,7 +747,6 @@ static int xlp_poll_lower(int budget, int cpu)
 			printk("Dropping firmware RX packet (addr=%llx)!\n", addr);
 			continue;
 		}
-		node = (src_id >> 10) & 0x3;
 		port = *(cntx2port[node] + context);
 
 #ifdef ENABLE_SANITY_CHECKS
@@ -926,7 +757,7 @@ static int xlp_poll_lower(int budget, int cpu)
 			continue;
 		}
 #endif
-		pdev = per_cpu_netdev[cpu][port];
+		pdev = per_cpu_netdev[node][cpu][port];
 #ifdef ENABLE_SANITY_CHECKS
 		if(!pdev) {
 			printk("[%s]: [rx] wrong port=%d(context=%d)? pdev = NULL!\n", __func__, port, context);
@@ -940,16 +771,14 @@ static int xlp_poll_lower(int budget, int cpu)
 		len = len  - ETH_FCS_LEN;
 
 		skb = mac_get_skb_back_ptr(vaddr);
-#ifdef CONFIG_NLM_NET_OPTS
-		//printk ("cpu %d Freein_fifo %d len %d skb->len %d \n", cpu, skb->queue_id, len, skb->len);
-#endif
+
 #ifdef ENABLE_SANITY_CHECKS
 		if (!skb) {
 			STATS_INC(priv->stats.rx_dropped);
 			printk("[%s] Null skb? addr=%llx, vaddr=%llx, dropping it and losing one buffer!\n",
 					__func__, addr, vaddr);
 			STATS_INC(priv->stats.rx_dropped);
-			err_replenish_count[LAST_RCVD_INDEX(cpu)]++;
+			err_replenish_count[CPU_INDEX(cpu)]++;
 			continue;
 		}
 #endif
@@ -958,13 +787,16 @@ static int xlp_poll_lower(int budget, int cpu)
 		skb_put(skb, len);
 		skb->protocol = eth_type_trans(skb, pdev);
 
+		/* We use jumbo rx buffers if the ethernet type is not ip, see perf_nae ucore file */
+		truesize = skb->truesize;
+		if(skb->protocol != htons(ETH_P_IP))
+			truesize = NLM_RX_JUMBO_BUF_SIZE + sizeof(struct sk_buff);
+
 		/* Pass the packet to Network stack */
-		last_rcvd_skb[LAST_RCVD_INDEX(cpu)] = skb;
-		last_rcvd_skb_phys[LAST_RCVD_INDEX(cpu)] = addr;
-		last_rcvd_len[LAST_RCVD_INDEX(cpu)] = len;
-#ifdef CONFIG_NLM_NET_OPTS
-		last_rcvd_queue_id  = skb->queue_id;
-#endif
+		last_rcvd_skb[CPU_INDEX(cpu)] = skb;
+		last_rcvd_skb_phys[CPU_INDEX(cpu)] = addr;
+		last_rcvd_len[CPU_INDEX(cpu)] = len;
+		last_rcvd_node[CPU_INDEX(cpu)] = node;
 
 #ifdef CONFIG_INET_LRO
 		if((skb->dev->features & NETIF_F_LRO) &&
@@ -987,14 +819,14 @@ static int xlp_poll_lower(int budget, int cpu)
 		STATS_ADD(priv->stats.rx_bytes, len);
 		STATS_INC(priv->stats.rx_packets);
 		//priv->cpu_stats[cpu].rx_packets++;
-		receive_count[LAST_RCVD_INDEX(cpu)]++;
+		receive_count[CPU_INDEX(cpu)]++;
 
-		if (last_rcvd_skb[LAST_RCVD_INDEX(cpu)]) {
+		if (last_rcvd_skb[CPU_INDEX(cpu)]) {
 			//printk("[%s@%d]: Unwanted buffer allocation in driver data path!\n", __FILE__, __LINE__);
-			slow_replenish_count[LAST_RCVD_INDEX(cpu)]++;
-			mac_refill_frin_one_buffer(pdev, cpu, last_rcvd_queue_id);
-			last_rcvd_skb[LAST_RCVD_INDEX(cpu)] = NULL;
-			last_rcvd_len[LAST_RCVD_INDEX(cpu)] = 0;
+			slow_replenish_count[CPU_INDEX(cpu)]++;
+			mac_refill_frin_one_buffer(pdev, cpu, truesize);
+			last_rcvd_skb[CPU_INDEX(cpu)] = NULL;
+			last_rcvd_len[CPU_INDEX(cpu)] = 0;
 		}
 	}
 
@@ -1015,8 +847,6 @@ static int xlp_nae_napi_poll(struct napi_struct *napi, int budget)
 	int cpu = hard_smp_processor_id();
 
 	napi_dbg("%s in budget %d\n", __FUNCTION__, budget);
-	if(perf_mode == NLM_RT_MODE)
-		budget = 300000;
 
 	xlp_poll_upper(cpu);
 	rx_pkts = xlp_poll_lower(budget, cpu);
@@ -1055,9 +885,11 @@ static int xlp_nae_poll(void *buf)
 
 void nlm_spawn_kthread(void)
 {
-    unsigned int i = 0, nr_cpus = 32;
+    unsigned int i = 0, nr_cpus;
     char buf[20];
-    static struct task_struct *task[32];
+    static struct task_struct *task[NR_CPUS];
+
+    nr_cpus = nlm_node_cfg.num_nodes * NLM_NCPUS_PER_NODE;	
     /*Spawn kthread*/
     for(i=0; i<nr_cpus; i++){
 	if(!cpu_isset(i, cpu_present_map))
@@ -1078,89 +910,6 @@ void nlm_spawn_kthread(void)
 
 }
 
-static void nlm_update_ucore_shared_memory(int node)
-{
-	uint32_t data[35] = {0};
-	int i = 0;
-	int j = 0;
-
-	while(i < 32){
-		for(j=0; j < NR_CPUS && i < 32; j++){
-			if(!cpu_isset(j, cpu_present_map))
-				continue;
-			data[i] = j*4 + nae_rx_vc;
-			i++;
-		}
-	}
-	if(perf_mode == NLM_TCP_MODE)
-		data[32] = NLM_TCP_MODE;
-	else
-		data[32] = NLM_RT_MODE;
-
-	data[33] = enable_jumbo;
-
-	/* jumbo_freein_offset */
-	if (is_nlm_xlp3xx())
-		data[34] = 4;
-	else
-		data[34] = 8;
-
-	nlm_hal_write_ucore_shared_mem(0, data, sizeof(data)/sizeof(uint32_t));
-}
-
-static void nlm_replenish_per_cpu_buffer(int qindex, int bufcnt)
-{
-	int i;
-	int vc_index = 0;
-	int __attribute__ ((unused)) mflags, code;
-	struct xlp_msg msg;
-	struct sk_buff * skb;
-	int ret = 0;
-	int size = 0;
-
-	if (!enable_jumbo || (qindex < jumbo_freein_offset) || (qindex >= (jumbo_freein_offset * 2)))
-		size = NLM_RX_ETH_BUF_SIZE;
-	else
-		size = NLM_RX_JUMBO_BUF_SIZE;
-
-	for(i = 0; i < bufcnt; i++)
-	{
-		vc_index = qindex + frin_queue_base;
-		skb = nlm_xlp_alloc_skb_atomic(size);
-		if(!skb)
-		{
-			printk("[%s] alloc skb failed\n",__FUNCTION__);
-			break;
-		}
-#ifdef CONFIG_NLM_NET_OPTS
-		skb->queue_id = vc_index;
-#endif
-		/* Store skb in back_ptr */
-		mac_put_skb_back_ptr(skb);
-		code = 0;
-
-		/* Send the free Rx desc to the MAC */
-		msgrng_access_enable(mflags);
-		msg.entry[0] = (unsigned long long)virt_to_bus(skb->data) & 0xffffffffffULL;
-		msg.entry[1]= msg.entry[2] = msg.entry[3] = 0;
-		/* Send the packet to nae rx  */
-		__sync();
-
-		if ( (ret = nlm_hal_send_msg1(vc_index, code, msg.entry[0])) & 0x7)
-		{
-			print_fmn_send_error(__func__, ret);
-			printk("Unable to send configured free desc, check freein carving (qid=%d)\n", vc_index);
-			/* free the buffer and return! */
-			dev_kfree_skb_any(skb);
-
-			msgrng_access_disable(mflags);
-			ret = -EBUSY;
-			break;
-		}
-		msgrng_access_disable(mflags);
-	}
-	printk("Send %d descriptors for queue %d(vc %d) of length %d\n", bufcnt, qindex, vc_index, size);
-}
 
 /*
  * Setup XLP NAPI subsystem
@@ -1215,25 +964,6 @@ static int nlm_xlp_napi_setup(void)
 	return 0;
 }
 
-static int nlm_initialize_vfbid(void)
-{
-	int cpu =0, tblidx, i = 0;
-	uint32_t vfbid_tbl[128];
-
-	/* freeback bucket vc based on hard cpu id */
-	for (tblidx = 0; tblidx < 32 ; tblidx++, cpu++) {
-		vfbid_tbl[tblidx] = (cpu * 4) + nae_fb_vc;
-	}
-	/* from 32 points to hard replenish */
-	for(tblidx = 32; tblidx < (32 + frin_total_queue); tblidx++, i++) {
-		vfbid_tbl[tblidx] = frin_queue_base + i;
-	}
-	/* NULL FBID Should map to cpu0 to detect NAE send message errors*/
-	vfbid_tbl[127] = 0;
-	nlm_config_vfbid_table(0, 0, 128, vfbid_tbl);
-	return 0;
-}
-
 /**********************************************************************
  * nlm_xlp_nae_init -  xlp_nae device driver init function
  * @dev  -  this is per device based function
@@ -1244,10 +974,9 @@ static void nlm_xlp_nae_init(void)
 {
 	struct net_device *dev = NULL;
 	struct dev_data *priv;
-	int i, node = 0;
+	int i, node = 0, maxnae;
 	struct proc_dir_entry *entry;
 	int cpu = 0;
-	int vc = 0;
 	unsigned char *mode_str[3] = {"INVALID","TCP_PERF","ROUTE_PERF"};
 	nlm_nae_config_ptr nae_cfg;
 
@@ -1267,133 +996,107 @@ static void nlm_xlp_nae_init(void)
 	}
 	
 	for(i=0; i<NR_CPUS; i++)
-		nlm_mode[LAST_RCVD_INDEX(i)] = perf_mode;
+		nlm_mode[CPU_INDEX(i)] = perf_mode;
 
 	/*Disable interrupts for VC - 0-127*/
 	for(i=0; i<NR_CPUS; i++){
-		if(!cpu_isset(i, cpu_present_map))
-			continue;
-		for(vc=0; vc<4; vc++)
-			nlm_hal_disable_vc_intr((i/32), ((i*4 + vc) & 0x7f));
+	        if(!cpu_isset(cpu, phys_cpu_present_map))
+                        continue;
+		phys_cpu_map[i / NLM_NCPUS_PER_NODE] |= (1 << (i % NLM_NCPUS_PER_NODE));
 	}
 
-	/* This function has to be called before initialize_nae */
-	nlm_update_ucore_shared_memory(node);
-
 	if(perf_mode == NLM_TCP_MODE)
 		p2p_desc_mem_init();
 
-	if (initialize_nae(cpumask_to_uint32(&cpu_present_map), 0, 0, 0))
-		return;
+	gen_mac_address();
 
-	nae_cfg = nlm_node_cfg.nae_cfg[node];		//FIXME for multinode
-	if (nae_cfg == NULL) {
-		printk("Node %d NAE configuration is NULL \n",node);
+	if (initialize_nae(phys_cpu_map, perf_mode, &enable_jumbo))
 		return;
-	}
-	nae_fb_vc = nae_cfg->fb_vc;
-	nae_rx_vc = nae_cfg->rx_vc;
 
-	if(nae_cfg->frin_queue_base != 0)
-		frin_queue_base = nae_cfg->frin_queue_base;
-	if(nae_cfg->frin_total_queue != 0)
-		frin_total_queue = nae_cfg->frin_total_queue;
-
-	/* Update RX_CONFIG for desc size */
-	if(enable_jumbo)
-	{
-		nlm_hal_init_ingress (node, (ETH_JUMBO_DATA_LEN+ETH_HLEN+ETH_FCS_LEN+SMP_CACHE_BYTES) & ~(SMP_CACHE_BYTES - 1));
-		num_cpu_share_freein = 4;
-		if (is_nlm_xlp8xx())
-			jumbo_freein_offset = (frin_total_queue - NUM_XLP8XX_MGMT_PORTS)/2;
-		else
-			jumbo_freein_offset = frin_total_queue/2;
-	}
-	else
-		nlm_hal_init_ingress (node, (ETH_DATA_LEN+ETH_HLEN+ETH_FCS_LEN+SMP_CACHE_BYTES) & ~(SMP_CACHE_BYTES - 1));
-
-	printk("nae_cfg frin_queue_base %d, frin_total_queue %d\n",nae_cfg->frin_queue_base, nae_cfg->frin_total_queue);
-	printk("jumbo_freein_offset %d, num_cpu_share_freein %d\n",jumbo_freein_offset, num_cpu_share_freein);
-
-	nlm_initialize_vfbid();
-
-	for(i = 0; i < nae_cfg->num_ports; i++)
-	{
-		/* Register only valid ports which are management */
-		if (!nae_cfg->ports[i].valid)
+	maxnae = nlm_node_cfg.num_nodes;	
+	for(node = 0; node < maxnae; node++) {
+		nae_cfg = nlm_node_cfg.nae_cfg[node];
+		if (nae_cfg == NULL)
 			continue;
 
-		dev = alloc_etherdev_mq(sizeof(struct dev_data), 32);
-		if(!dev)
-			return;
+		for(i = 0; i < nae_cfg->num_ports; i++)
+		{
+			/* Register only valid ports which are management */
+			if (!nae_cfg->ports[i].valid)
+				continue;
 
-		ether_setup(dev);
-		dev->tx_queue_len = 0;	/* routing gives good performance with tx_queue_len = 0; */
+			dev = alloc_etherdev_mq(sizeof(struct dev_data), maxnae * NLM_NCPUS_PER_NODE);
+			if(!dev)
+				return;
 
-		priv = netdev_priv(dev);
-		spin_lock_init(&priv->lock);
-		priv->dev 	= dev;
-		dev->netdev_ops = &nlm_xlp_nae_ops;
+			ether_setup(dev);
+			dev->tx_queue_len = 0;	/* routing gives good performance with tx_queue_len = 0; */
+
+			priv = netdev_priv(dev);
+			spin_lock_init(&priv->lock);
+			priv->dev 	= dev;
+			dev->netdev_ops = &nlm_xlp_nae_ops;
+
+			/* set ethtool_ops which is inside xlp_ethtool.c file*/
+			xlp_set_ethtool_ops(dev);
+
+			dev->dev_addr 	= eth_hw_addr[node][i];
+			priv->port	= i;
+			priv->hw_port_id = nae_cfg->ports[i].hw_port_id;
+
+			priv->inited	= 0;
+			priv->node 	= node;
+			priv->block	= nae_cfg->ports[i].hw_port_id / 4;
+			priv->type	= nae_cfg->ports[i].iftype;
+
+			switch(nae_cfg->ports[i].iftype) {
+				case SGMII_IF:
+					priv->index = nae_cfg->ports[i].hw_port_id & 0x3;
+					priv->phy.addr = nae_cfg->ports[i].hw_port_id;
+					break;
+				case XAUI_IF:
+					nlm_hal_write_mac_reg(priv->node, (nae_cfg->ports[i].hw_port_id / 4),
+							XGMAC, XAUI_MAX_FRAME_LEN , 0x01800600);
+					priv->index = XGMAC;
+					break;
+				case INTERLAKEN_IF:
+					priv->index = INTERLAKEN;
+					priv->phy.addr = nae_cfg->ports[i].ext_phy_addr;
+					if (nae_cfg->ports[i].hw_port_id == 0) {
+						if (dev_alloc_name(dev, "ilk0-%d") < 0)
+							printk("alloc name failed \n");
+					}
+					else {
+						if (dev_alloc_name(dev, "ilk8-%d") < 0)
+							printk("alloc name failed \n");
+					}
+					break;
+				default:
+					priv->index=0;
+					break;
+			}
+			//nlm_print("port%d hw %d block %d index %d type %d \n",i, nae_cfg->ports[i].hw_port_id,
+			//							priv->block, priv->index, priv->type);
+			priv->nae_tx_qid	= nae_cfg->ports[i].txq;
+			priv->nae_rx_qid 	= nae_cfg->ports[i].rxq;
+			dev->features 		|= NETIF_F_LLTX;
 
-		/* set ethtool_ops which is inside xlp_ethtool.c file*/
-		xlp_set_ethtool_ops(dev);
+			register_netdev(dev);
 
-		dev->dev_addr 	= eth_hw_addr[i];
-		priv->port	= i;
-		priv->hw_port_id = nae_cfg->ports[i].hw_port_id;
+			dev_mac[node][i] = dev;
+			xlp_mac_setup_hwaddr(priv);
 
-		priv->inited	= 0;
-		priv->node 	= node;
-		priv->block	= nae_cfg->ports[i].hw_port_id / 4;
-		priv->type	= nae_cfg->ports[i].iftype;
+			for(cpu = 0; cpu<NR_CPUS; cpu++){
+				per_cpu_netdev[node][cpu][i] = dev;
+			}
 
-		switch(nae_cfg->ports[i].iftype) {
-			case SGMII_IF:
-				priv->index = nae_cfg->ports[i].hw_port_id & 0x3;
-				priv->phy.addr = nae_cfg->ports[i].hw_port_id;
-				break;
-			case XAUI_IF:
-				nlm_hal_write_mac_reg(priv->node, (nae_cfg->ports[i].hw_port_id / 4),
-						XGMAC, XAUI_MAX_FRAME_LEN , 0x01800600);
-				priv->index = XGMAC;
-				break;
-			case INTERLAKEN_IF:
-				priv->index = INTERLAKEN;
-				priv->phy.addr = nae_cfg->ports[i].ext_phy_addr;
-				if (nae_cfg->ports[i].hw_port_id == 0) {
-					if (dev_alloc_name(dev, "ilk0-%d") < 0)
-						printk("alloc name failed \n");
-				}
-				else {
-					if (dev_alloc_name(dev, "ilk8-%d") < 0)
-						printk("alloc name failed \n");
-				}
-				break;
-			default:
-				priv->index=0;
-				break;
-		}
-		//nlm_print("port%d hw %d block %d index %d type %d \n",i, nae_cfg->ports[i].hw_port_id,
-		//							priv->block, priv->index, priv->type);
-		priv->nae_tx_qid	= nae_cfg->ports[i].txq;
-		priv->nae_rx_qid 	= nae_cfg->ports[i].rxq;
-		dev->features 		|= NETIF_F_LLTX;
-
-		register_netdev(dev);
-
-		dev_mac[i] = dev;
-		xlp_mac_setup_hwaddr(priv);
-		for(cpu = 0; cpu<NR_CPUS; cpu++){
-			per_cpu_netdev[cpu][i] = dev;
 		}
-
 	}
 
 	entry = create_proc_read_entry("mac_stats", 0 /* def mode */ ,
 				       nlm_root_proc /* parent */ ,
 				       xlp_mac_proc_read /* proc read function */ ,
-				       0	/* no client data */
-		);
+				       0	/* no client data */);
 	if (!entry) {
 		printk("[%s]: Unable to create proc read entry for xlp_mac!\n",
 		       __FUNCTION__);
@@ -1409,31 +1112,10 @@ static void nlm_xlp_nae_init(void)
 		nlm_spawn_kthread();
 	}
 
-	/* configure the descs */
-	if(nae_cfg->freein_fifo_shared) {
-		int max_descs_pqueue, num_descs;
-		max_descs_pqueue = nae_cfg->freein_fifo_onchip_num_descs +  nae_cfg->freein_fifo_spill_num_descs;
-		num_descs = num_descs_perq <= max_descs_pqueue ? num_descs_perq : max_descs_pqueue;
-
-		for(i = 0; i < frin_total_queue; i++) {
-			if(i < 16)
-				nlm_replenish_per_cpu_buffer(i, num_descs);
-		}
-		/* configure the mgmt port, for mgmt ports take it from the port config */
-		for(i = 0; i < nae_cfg->num_ports; i++) {
-			if(!nae_cfg->ports[i].mgmt)
-				continue;
-			nlm_replenish_per_cpu_buffer(nae_cfg->ports[i].rxq - frin_queue_base,
-					nae_cfg->ports[i].num_free_desc);
-		}
-
-	} else {
-		for(i = 0; i < nae_cfg->num_ports; i++)
-		{
-			nlm_replenish_per_cpu_buffer(nae_cfg->ports[i].rxq - frin_queue_base,
-					nae_cfg->ports[i].num_free_desc);
-		}
+	if(replenish_freein_fifos() != 0) {
+		printk("Replenishmemt of freein fifos failed\n");
 	}
+	return;
 }
 
 /**********************************************************************
@@ -1446,6 +1128,7 @@ static int  nlm_xlp_nae_open (struct net_device *dev)
 	struct dev_data *priv = netdev_priv(dev);
 	int i;
 	int ret = 0;
+	nlm_nae_config_ptr nae_cfg = nlm_node_cfg.nae_cfg[priv->node];
 
 	if(perf_mode == NLM_TCP_MODE) {
 #ifdef TSO_ENABLED
@@ -1456,7 +1139,8 @@ static int  nlm_xlp_nae_open (struct net_device *dev)
 
 	if (priv->inited) {
 		spin_lock_irq(&priv->lock);
-		nlm_xlp_mac_set_enable(priv, 1);
+		if(nae_cfg->owned)
+			nlm_xlp_mac_set_enable(priv, 1);
 		netif_tx_wake_all_queues(dev);
 		spin_unlock_irq(&priv->lock);
 		return 0;
@@ -1474,16 +1158,7 @@ static int  nlm_xlp_nae_open (struct net_device *dev)
 	}
 #endif
 
-	/* set timer to test rx routine */
-	init_timer(&priv->link_timer);
-	priv->link_timer.expires = jiffies + HZ ; /* First timer after 1 sec */
-	priv->link_timer.data    = (unsigned long) priv->port;
-	priv->link_timer.function = &nlm_xlp_mac_timer;
-	priv->phy_oldlinkstat = -1;
-
 	netif_tx_start_all_queues(dev);
-	if(!(perf_mode == NLM_TCP_MODE || perf_mode == NLM_RT_MODE))
-		add_timer(&priv->link_timer);
 
 	STATS_SET(priv->stats.tx_packets, 0);
 	STATS_SET(priv->stats.tx_errors, 0);
@@ -1504,8 +1179,11 @@ static int  nlm_xlp_nae_open (struct net_device *dev)
 		priv->cpu_stats[i].interrupts	= 0;
 
 	}
+
 	priv->inited = 1;
-	nlm_xlp_mac_set_enable(priv, 1);
+
+	if(nae_cfg->owned)
+		nlm_xlp_mac_set_enable(priv, 1);
 
 	return ret;
 }
@@ -1518,12 +1196,13 @@ static int  nlm_xlp_nae_open (struct net_device *dev)
 static int  nlm_xlp_nae_stop (struct net_device *dev)
 {
 	struct dev_data *priv = netdev_priv(dev);
+	nlm_nae_config_ptr nae_cfg = nlm_node_cfg.nae_cfg[priv->node];
 
 	spin_lock_irq(&priv->lock);
-	nlm_xlp_mac_set_enable(priv, 0);
-	priv->inited = 0;
-	del_timer_sync(&priv->link_timer);
 
+	if(nae_cfg->owned)
+		nlm_xlp_mac_set_enable(priv, 0);
+	priv->inited = 0;
 	netif_tx_stop_all_queues(dev);
 
 	spin_unlock_irq(&priv->lock);
@@ -1589,7 +1268,8 @@ static inline int tso_xmit_skb(struct sk_buff *skb, struct net_device *dev)
 	//unsigned int mflags;
 	uint64_t *p2pdesc = NULL;
 	int cpu = hard_smp_processor_id();
-	int  ret, retry_cnt = 0;
+	int  ret, retry_cnt = 0, qid;
+	nlm_nae_config_ptr nae_cfg = nlm_node_cfg.nae_cfg[priv->node];
 
 	p2pdesc = alloc_p2p_desc_mem(cpu);
 	if(p2pdesc == NULL) {
@@ -1657,7 +1337,9 @@ static inline int tso_xmit_skb(struct sk_buff *skb, struct net_device *dev)
 	}
 
 	create_last_p2p_desc(p2pdesc, skb, idx);
-	msg = nae_tx_desc(P2P, 0, cpu, idx, virt_to_bus(p2pdesc));
+
+	qid = nae_cfg->vfbtbl_sw_offset + (cpu % NLM_NCPUS_PER_NODE);
+	msg = nae_tx_desc(P2P, 0, qid, idx, virt_to_bus(p2pdesc));
 
 	tso_dbg("msg0 %llx p2pdesc0 %llx p2pdesc1 %llx p2pdesc2 %llx idx %d\n",
 			msg, p2pdesc[0], p2pdesc[1], p2pdesc[2], idx);
@@ -1686,6 +1368,7 @@ retry_send:
 
 	return NETDEV_TX_OK;
 out_unlock:
+
 	dev_kfree_skb_any(skb);
 	if(p2pdesc)
 		free_p2p_desc_mem(cpu, p2pdesc);
@@ -1706,7 +1389,9 @@ static int nlm_xlp_nae_start_xmit(struct sk_buff *skb, struct net_device *dev)
 	uint64_t msg0, msg1;
 	int retry_count = 128;
 	volatile int hw_repl = 0;
-	int  offset;
+	int  offset, qid;
+	nlm_nae_config_ptr nae_cfg = nlm_node_cfg.nae_cfg[priv->node];
+
 
 #ifdef ENABLE_SANITY_CHECKS
 	if(!skb)
@@ -1721,35 +1406,36 @@ static int nlm_xlp_nae_start_xmit(struct sk_buff *skb, struct net_device *dev)
 	}
 #endif
 #ifdef TSO_ENABLED
-	if(nlm_mode[LAST_RCVD_INDEX(cpu)] == NLM_TCP_MODE){
+	if(nlm_mode[CPU_INDEX(cpu)] == NLM_TCP_MODE){
 		return tso_xmit_skb(skb, dev);
 	}
 #endif
 
 #ifdef CONFIG_NLM_NET_OPTS
-	if(skb->netl_skb && (last_rcvd_skb[LAST_RCVD_INDEX(cpu)] == skb->netl_skb)
-		&& !skb_shared(skb) && (last_rcvd_len[LAST_RCVD_INDEX(cpu)] == skb->len)
-		&& !skb_cloned(skb))
-#endif
+	if(skb->netl_skb && (last_rcvd_skb[CPU_INDEX(cpu)] == skb->netl_skb)
+		&& !skb_shared(skb) && (last_rcvd_len[CPU_INDEX(cpu)] == skb->len)
+		&& !skb_cloned(skb) && nae_cfg->vfbtbl_hw_nentries)
 	{
-		last_rcvd_skb[LAST_RCVD_INDEX(cpu)] = NULL;
-		last_rcvd_len[LAST_RCVD_INDEX(cpu)] = 0;
+		last_rcvd_skb[CPU_INDEX(cpu)] = NULL;
+		last_rcvd_len[CPU_INDEX(cpu)] = 0;
 
-		/* Do h/w replenishment and 4 CPUs share a FIFO */
-#ifdef CONFIG_NLM_NET_OPTS
-		msg0 = nae_tx_desc(P2D_NEOP, 0, skb->queue_id - frin_queue_base + 32,
-				0, last_rcvd_skb_phys[LAST_RCVD_INDEX(cpu)]);
-#endif
+		qid = get_hw_frfifo_queue_id(last_rcvd_node[CPU_INDEX(cpu)], nae_cfg, cpu, skb->truesize);
+		msg0 = nae_tx_desc(P2D_NEOP, 0, qid,
+				0, last_rcvd_skb_phys[CPU_INDEX(cpu)]);
 		hw_repl = 1;
-		//printk ("%s skb->len %d \n", __func__, skb->len);
 
-		fast_replenish_count[LAST_RCVD_INDEX(cpu)]++;
-	}
-#ifdef CONFIG_NLM_NET_OPTS
-	else {
-		msg0 = nae_tx_desc(P2D_NEOP, 0, cpu, 0, virt_to_bus(skb));
+		Message("Inside fast replensh cpu %d len %d qid %d\n", cpu, skb->len, qid);
+
+		fast_replenish_count[CPU_INDEX(cpu)]++;
 	}
+	else 
 #endif
+	{
+		qid = nae_cfg->vfbtbl_sw_offset + (cpu % NLM_NCPUS_PER_NODE);
+		msg0 = nae_tx_desc(P2D_NEOP, 0, qid, 0, virt_to_bus(skb));
+
+		Message("Inside slow replensh cpu %d len %d qid %d\n", cpu, skb->len, qid);
+	}
 	msg1 = nae_tx_desc(P2D_EOP, 0, NULL_VFBID, skb->len,
 		       virt_to_bus(skb->data));
 	if(hw_repl) {
@@ -1855,9 +1541,7 @@ static int nlm_xlp_nae_change_mtu(struct net_device *dev, int new_mtu)
 	struct dev_data *priv = netdev_priv(dev);
 	unsigned long flags;
 	unsigned long local_mtu;
-#if 0
-	uint32_t rx_config = 0, node = 0;
-#endif
+	nlm_nae_config_ptr nae_cfg = nlm_node_cfg.nae_cfg[priv->node];
 
 	if (enable_jumbo && (new_mtu > ETH_JUMBO_DATA_LEN || new_mtu < ETH_ZLEN)) {
 		printk ("MTU should be between %d and %d\n", ETH_ZLEN, ETH_JUMBO_DATA_LEN);
@@ -1874,14 +1558,9 @@ static int nlm_xlp_nae_change_mtu(struct net_device *dev, int new_mtu)
 	local_mtu = (new_mtu+ETH_HLEN+ETH_FCS_LEN+SMP_CACHE_BYTES) & ~(SMP_CACHE_BYTES - 1);
 	if (netif_running(dev))
 	{
-#if 0
-		/* Disable RX enable bit in RX_CONFIG */
-		rx_config = nlm_hal_read_nae_reg(node, RX_CONFIG);
-		rx_config &= 0xfffffffe;
-		nlm_hal_write_nae_reg(node, RX_CONFIG, rx_config);
-#endif
 		netif_tx_stop_all_queues (dev);
-		nlm_xlp_mac_set_enable(priv, 0); /* Disable MAC TX/RX */
+		if(nae_cfg->owned)
+			nlm_xlp_mac_set_enable(priv, 0); /* Disable MAC TX/RX */
 	}
 
 	if(priv->type==SGMII_IF){
@@ -1902,14 +1581,9 @@ static int nlm_xlp_nae_change_mtu(struct net_device *dev, int new_mtu)
 
 	if (netif_running(dev))
 	{
-#if 0
-		/* Enable RX enable bit in RX_CONFIG */
-		rx_config = nlm_hal_read_nae_reg(node, RX_CONFIG);
-		rx_config |= 0x1;
-		nlm_hal_write_nae_reg(node, RX_CONFIG, rx_config);
-#endif
 		netif_tx_start_all_queues (dev);
-		nlm_xlp_mac_set_enable(priv, 1);
+		if(nae_cfg->owned)
+			nlm_xlp_mac_set_enable(priv, 1);
 	}
 
 	spin_unlock_irqrestore(&priv->lock, flags);
@@ -2026,38 +1700,40 @@ static int xlp_mac_proc_read(char *page, char **start, off_t off,
 {
 	int len = 0;
 	off_t begin = 0;
-	int i = 0, cpu = 0;
+	int i = 0, cpu = 0, node;
 	struct net_device *dev = 0;
 	struct dev_data *priv = 0;
 
-	for (i = 0; i < MAX_GMAC_PORT; i++) {
+	for(node = 0; node < NLM_MAX_NODES; node++) {
+		for (i = 0; i < MAX_GMAC_PORT; i++) {
 
-		dev = dev_mac[i];
+			dev = dev_mac[node][i];
 
-		if(dev == 0) continue;
+			if(dev == 0) continue;
 
-		priv = netdev_priv(dev);
+			priv = netdev_priv(dev);
 
-		len += sprintf(page + len, "=============== port@%d ==================\n", i);
+			len += sprintf(page + len, "=============== port@%d ==================\n", i);
 
-		len += sprintf(page + len,
-			       "per port@%d: %lu(rxp) %lu(rxb) %lu(txp) %lu(txb)\n",
-			       i,
-			       STATS_READ(priv->stats.rx_packets),
-			       STATS_READ(priv->stats.rx_bytes),
-			       STATS_READ(priv->stats.tx_packets),
-			       STATS_READ(priv->stats.tx_bytes));
+			len += sprintf(page + len,
+					"per port@%d: %lu(rxp) %lu(rxb) %lu(txp) %lu(txb)\n",
+					i,
+					STATS_READ(priv->stats.rx_packets),
+					STATS_READ(priv->stats.rx_bytes),
+					STATS_READ(priv->stats.tx_packets),
+					STATS_READ(priv->stats.tx_bytes));
 
-		for (cpu = 0; cpu < NR_CPUS ; cpu++) {
-			unsigned long tx = priv->cpu_stats[cpu].tx_packets;
-			unsigned long txc = priv->cpu_stats[cpu].txc_packets;
-			unsigned long rx = priv->cpu_stats[cpu].rx_packets;
-			unsigned long ints = priv->cpu_stats[cpu].interrupts;
+			for (cpu = 0; cpu < NR_CPUS ; cpu++) {
+				unsigned long tx = priv->cpu_stats[cpu].tx_packets;
+				unsigned long txc = priv->cpu_stats[cpu].txc_packets;
+				unsigned long rx = priv->cpu_stats[cpu].rx_packets;
+				unsigned long ints = priv->cpu_stats[cpu].interrupts;
 
-			if (!tx && !txc && !rx && !ints) continue;
+				if (!tx && !txc && !rx && !ints) continue;
 
-			len += sprintf(page + len, "per cpu@%d: %lu(txp) %lu(txcp) %lu(rxp) %lu(int)\n",
-				       cpu, tx, txc, rx, ints);
+				len += sprintf(page + len, "per cpu@%d: %lu(txp) %lu(txcp) %lu(rxp) %lu(int)\n",
+						cpu, tx, txc, rx, ints);
+			}
 		}
 	}
 
@@ -2073,44 +1749,7 @@ static int xlp_mac_proc_read(char *page, char **start, off_t off,
 	return len;
 }
 
-/**********************************************************************
- * nlm_xlp_mac_timer - interrupt handler routine
- * @data - parameter passed in when timer interrupt handler is called.
- **********************************************************************/
-static void nlm_xlp_mac_timer(unsigned long data)
-{
-	unsigned port = data;
-        struct net_device *dev = (struct net_device *)dev_mac[port];
-        struct dev_data *priv = netdev_priv(dev);
-        int next_tick = HZ / 1000; /* 1ms */
-
-	/* printk("[%s] A0 Workaround, forcing FMN int handling \n",__func__); */
-	if (priv->inited)
-	{
-		uint32_t cpumask = cpumask_to_uint32(&cpu_present_map); /* doesn't handle non-n0 nodes */
-		uint32_t cpumask_lo;
-		uint32_t cpumask_hi;
-		int cpu = hard_smp_processor_id();
-
-		cpumask = cpumask & ~(1 << cpu);
-		cpumask_hi = cpumask >> 16;;
-		cpumask_lo = cpumask & 0xffff;
-
-		/* Send IRQ_MSGRING vector in an IPI to all cpus but the current one */
-		if (cpumask_lo)
-			nlh_pic_w64r(cpu/NLM_MAX_CPU_PER_NODE, XLP_PIC_IPI_CTL, (XLP_IRQ_MSGRING_RVEC << 20) | cpumask_lo );
 
-		if (cpumask_hi)
-			nlh_pic_w64r(cpu/NLM_MAX_CPU_PER_NODE, XLP_PIC_IPI_CTL, (XLP_IRQ_MSGRING_RVEC << 20) | (1 << 16)
-					      | (cpumask_hi));
-
-		/* Run IPI handler on this cpu too */
-		nlm_xlp_msgring_int_handler(XLP_IRQ_MSGRING_RVEC, NULL);
-	}
-
-        priv->link_timer.expires = jiffies + next_tick;
-        add_timer(&priv->link_timer);
-}
 
 static int __devinit nlm_xlp_nae_pci_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
 {
@@ -2129,16 +1768,19 @@ static void nlm_xlp_nae_remove(void)
 	int i;
 	struct net_device *dev = 0;
         struct dev_data *priv = 0;
+	int node = 0;
 
-	for (i = 0; i < MAX_GMAC_PORT; i++)
-	{
-		dev = dev_mac[i];
+	for(node = 0; node < NLM_MAX_NODES; node++) {
+		for (i = 0; i < MAX_GMAC_PORT; i++)
+		{
+			dev = dev_mac[node][i];
 
-		if (dev == 0) continue;
+			if (dev == 0) continue;
 
-		priv = netdev_priv(dev);
-		unregister_netdev(dev);
-		free_netdev(dev);
+			priv = netdev_priv(dev);
+			unregister_netdev(dev);
+			free_netdev(dev);
+		}
 	}
 
 	remove_proc_entry("mac_stats", nlm_root_proc /* parent dir*/ );
diff --git a/drivers/misc/netlogic/nae-perf/xlp_nae.h b/drivers/misc/netlogic/nae-perf/xlp_nae.h
index 6009ff5..f0e2b32 100644
--- a/drivers/misc/netlogic/nae-perf/xlp_nae.h
+++ b/drivers/misc/netlogic/nae-perf/xlp_nae.h
@@ -109,5 +109,65 @@ static inline void prefetch_local(const void *addr)
         : "i" (Pref_StoreStreamed), "r" (addr));
 }
 
+/**********************************************************************
+ * nlm_xlp_alloc_skb_atomic -  Atomically allocates 64 bits cache aligned skb buffer
+ * return - skb buffer address
+ *
+ **********************************************************************/
+static __inline__ struct sk_buff *nlm_xlp_alloc_skb_atomic(int size)
+{
+        int offset = 0;
+        struct sk_buff *skb = __dev_alloc_skb(size, GFP_ATOMIC);
+
+        if (!skb) {
+                return NULL;
+        }
+
+        /* align the data to the next cache line */
+        offset = ((unsigned long)skb->data + CACHELINE_SIZE) &
+                ~(CACHELINE_SIZE - 1);
+        skb_reserve(skb, (offset - (unsigned long)skb->data));
+#ifdef CONFIG_NLM_NET_OPTS
+        skb->netl_skb = skb;
+#endif
+        return skb;
+}
+
+static __inline__ void mac_put_skb_back_ptr(struct sk_buff *skb)
+{
+        uint64_t *back_ptr = (uint64_t *)skb->data;
+
+        /* this function should be used only for newly allocated packets. It assumes
+         * the first cacheline is for the back pointer related book keeping info
+         */
+        skb_reserve(skb, SKB_BACK_PTR_SIZE);
+        *back_ptr = (uint64_t)skb;
+}
+
+/* FMN send failure errors */
+#define MSG_DST_FC_FAIL                 0x01
+#define MSG_INFLIGHT_MSG_EX             0x02
+#define MSG_TXQ_FULL                    0x04
+
+static __inline__ void print_fmn_send_error(const char *str, uint32_t send_result)
+{
+	extern int debug;
+	if (debug < 1) return;
+
+	if(send_result & MSG_DST_FC_FAIL)
+	{
+		printk("[%s] Msg Destination flow control credit fail(send_result=%08x)\n",
+		       str, send_result);
+	}
+	else if (send_result & MSG_INFLIGHT_MSG_EX) {
+		printk("[%s] MSG_INFLIGHT_MSG_EX(send_result=%08x)\n", __func__, send_result);
+	}
+	else if (send_result & MSG_TXQ_FULL) {
+		printk("[%s] TX message Q full(send_result=%08x)\n", __func__, send_result);
+	}
+	else {
+		printk("[%s] Unknown send error type(send_result=%08x)\n", __func__, send_result);
+	}
+}
 
 #endif
-- 
1.7.1

