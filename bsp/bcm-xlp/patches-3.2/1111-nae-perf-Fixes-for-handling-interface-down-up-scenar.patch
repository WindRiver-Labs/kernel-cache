From 36261c024d5444514786b5a646ca4c5d4eb0abae Mon Sep 17 00:00:00 2001
From: Rahul Jain <rahulj@broadcom.com>
Date: Fri, 2 Mar 2012 15:01:37 +0530
Subject: nae-perf: Fixes for handling interface down up scenario

[Based on SDK 3.2]
Signed-off-by: Quanyang Wang <quanyang.wang@windriver.com>
Signed-off-by: Nam Ninh <nam.ninh@windriver.com>

diff --git a/drivers/misc/netlogic/nae-perf/xlp_nae.c b/drivers/misc/netlogic/nae-perf/xlp_nae.c
index 0eff5cb..ab2f61a 100755
--- a/drivers/misc/netlogic/nae-perf/xlp_nae.c
+++ b/drivers/misc/netlogic/nae-perf/xlp_nae.c
@@ -175,6 +175,7 @@ unsigned char eth_hw_addr[18][6] = {
 
 uint64_t nlm_mode[NR_CPUS*8] ____cacheline_aligned;
 struct sk_buff *last_rcvd_skb[NR_CPUS * 8] ____cacheline_aligned;
+uint32_t last_rcvd_len[NR_CPUS * 8] ____cacheline_aligned;
 uint64_t last_rcvd_skb_phys[NR_CPUS * 8] ____cacheline_aligned;
 uint64_t receive_count[NR_CPUS * 8] __cacheline_aligned;
 uint64_t fast_replenish_count[NR_CPUS * 8] __cacheline_aligned;
@@ -236,6 +237,7 @@ static int  nlm_xlp_nae_ioctl (struct net_device *dev, struct ifreq *rq, int cmd
 static int  nlm_xlp_nae_change_mtu(struct net_device *dev, int new_mtu);
 static void  nlm_xlp_nae_tx_timeout (struct net_device *dev);
 static void xlp_mac_setup_hwaddr(struct dev_data *priv);
+static int nlm_xlp_nae_set_hwaddr(struct net_device *dev, void *p);
 
 #ifdef  ENABLE_NAE_PIC_INT
 static irqreturn_t nlm_xlp_nae_int_handler(int irq, void * dev_id);
@@ -256,16 +258,16 @@ static unsigned short  nlm_select_queue(struct net_device *dev, struct sk_buff *
 }
 
 static const struct net_device_ops nlm_xlp_nae_ops = {
-	.ndo_open	= nlm_xlp_nae_open,
-	.ndo_stop	= nlm_xlp_nae_stop,
-	.ndo_start_xmit	= nlm_xlp_nae_start_xmit,
-	.ndo_set_multicast_list	= nlm_xlp_set_multicast_list,
-	.ndo_do_ioctl	= nlm_xlp_nae_ioctl,
-	.ndo_tx_timeout = nlm_xlp_nae_tx_timeout,
-	.ndo_change_mtu	= nlm_xlp_nae_change_mtu,
-	.ndo_set_mac_address	= eth_mac_addr,
-	.ndo_get_stats = nlm_xlp_mac_get_stats,
-	.ndo_select_queue	= nlm_select_queue,
+	.ndo_open			= nlm_xlp_nae_open,
+	.ndo_stop			= nlm_xlp_nae_stop,
+	.ndo_start_xmit			= nlm_xlp_nae_start_xmit,
+	.ndo_set_multicast_list		= nlm_xlp_set_multicast_list,
+	.ndo_do_ioctl			= nlm_xlp_nae_ioctl,
+	.ndo_tx_timeout 		= nlm_xlp_nae_tx_timeout,
+	.ndo_change_mtu			= nlm_xlp_nae_change_mtu,
+	.ndo_set_mac_address		= nlm_xlp_nae_set_hwaddr,
+	.ndo_get_stats 			= nlm_xlp_mac_get_stats,
+	.ndo_select_queue		= nlm_select_queue,
 };
 
 static __inline__ void cpu_halt(void)
@@ -371,6 +373,7 @@ static __inline__ struct sk_buff *nlm_xlp_alloc_skb_atomic(int size)
         if (!skb) {
                 return NULL;
         }
+
         /* align the data to the next cache line */
         offset = ((unsigned long)skb->data + CACHELINE_SIZE) &
                 ~(CACHELINE_SIZE - 1);
@@ -534,7 +537,7 @@ static void nlm_enable_l3_l4_parser(int node)
 	//enabling hardware parser
 	naereg = nlm_hal_read_nae_reg(node, RX_CONFIG);
 	nlm_hal_write_nae_reg(node, RX_CONFIG, (naereg | RX_PARSER_EN << 12 | RX_PPAD_EN << 13 | RX_PPAD_SZ << 22));
-	printk("Enabling parser, reg content = %x\n", nlm_hal_read_nae_reg(node, RX_CONFIG));
+	//printk("Enabling parser, reg content = %x\n", nlm_hal_read_nae_reg(node, RX_CONFIG));
 
 	/* enabling extraction of data */
 	for(i=0; i<16;i++)
@@ -660,21 +663,13 @@ void lro_init(struct net_device *dev)
 }
 
 #if 0
-static void dump_skb_info(struct sk_buff *skb, uint64_t msg1)
+static void dump_skbuff (struct sk_buff *skb)
 {
-	struct tcphdr *tcp = (void *)(skb->data + 20);
 	int cpu = hard_smp_processor_id();
 	char buf[512];
 	int blen = 0, i, len = 64;
 	unsigned char *data = skb->data;
 
-	if(tcp->syn) {
-	//	printk("cpu %04d ipsrc %04x ipdst %04x tcpsrc %08d tcpdst %08d\n",
-	//			cpu, iph->saddr, iph->daddr, tcp->source, tcp->dest);
-		dbg_tcp_rx_cons[CPU_INDEX(cpu)]++;
-	}
-	return;
-
 	for(i = 0; i < len;) {
 		if(i != 0 && (i % 16 == 0))
 			blen += sprintf(&buf[blen], "\n");
@@ -704,22 +699,22 @@ static int mac_refill_frin_skb(uint64_t paddr, int qid)
 	return ret;
 }
 
-static int mac_refill_frin_one_buffer(struct net_device *dev, int cpu, int len)
+static int mac_refill_frin_one_buffer(struct net_device *dev, int cpu, int queue_id)
 {
 	struct dev_data* priv;
 	struct net_device *ndev;
 	struct sk_buff * skb;
-	int freein_fifo;
 	int buf_size;
 
-	if (!enable_jumbo || ((len - ETH_HLEN) <= ETH_DATA_LEN))
-		buf_size = NLM_RX_ETH_BUF_SIZE;
+	if (enable_jumbo)
+	{
+		if ((queue_id -  frin_queue_base) < jumbo_freein_offset)
+			buf_size = NLM_RX_ETH_BUF_SIZE;
+		else
+			buf_size = NLM_RX_JUMBO_BUF_SIZE; 
+	}
 	else
-		buf_size = NLM_RX_JUMBO_BUF_SIZE;
-
-	freein_fifo = frin_queue_base + (cpu/num_cpu_share_freein);
-	if (enable_jumbo && ((len - ETH_HLEN) <= ETH_DATA_LEN) )
-		freein_fifo += jumbo_freein_offset;
+		buf_size = NLM_RX_ETH_BUF_SIZE;
 
 	skb = nlm_xlp_alloc_skb_atomic(buf_size);
 	if(!skb)
@@ -734,12 +729,12 @@ static int mac_refill_frin_one_buffer(struct net_device *dev, int cpu, int len)
 	skb->dev = ndev;
 
 #ifdef CONFIG_NLM_NET_OPTS
-	skb->queue_id = freein_fifo;
+	skb->queue_id = queue_id;
 #endif
 
 	mac_put_skb_back_ptr(skb);
-
-	return mac_refill_frin_skb((unsigned long long)virt_to_bus(skb->data), freein_fifo);
+	
+	return mac_refill_frin_skb((unsigned long long)virt_to_bus(skb->data), queue_id);
 }
 
 static int nae_proc_read(char *page, char **start, off_t off,
@@ -791,18 +786,6 @@ static inline void nlm_enable_msgring_intr(void)
 
 }
 
-#if 0
-static void dump_skbuff (struct sk_buff* skb)
-{
-	printk ("len %d\n", skb->len);
-	printk ("data_len %d\n", skb->data_len);
-	printk ("mac_len %d\n", skb->mac_len);
-	printk ("hdr_len %d\n", skb->hdr_len);
-	printk ("truesize %d\n", skb->truesize);
-	printk ("protocol %#x\n", skb->protocol);
-}
-#endif
-
 /*
  * NAE poll function on upper four buckets
 */
@@ -863,10 +846,9 @@ xlp_poll_upper(int cpu)
 	} /* closing while (1) */
 }
 
-
 /*
  * NAE poll function on lower four buckets
-*/
+ */
 static int xlp_poll_lower(int budget, int cpu)
 {
 	int err ;
@@ -881,6 +863,7 @@ static int xlp_poll_lower(int budget, int cpu)
 	uint64_t vaddr;
 	struct sk_buff* skb;
 	uint32_t src_id, size, code;
+	uint32_t last_rcvd_queue_id = -1;
 
 #ifdef CONFIG_INET_LRO
 	int lro_flush_priv_cnt = 0, i;
@@ -960,23 +943,24 @@ static int xlp_poll_lower(int budget, int cpu)
 		if (!skb) {
 			STATS_INC(priv->stats.rx_dropped);
 			printk("[%s] Null skb? addr=%llx, vaddr=%llx, dropping it and losing one buffer!\n",
-						__func__, addr, vaddr);
+					__func__, addr, vaddr);
 			STATS_INC(priv->stats.rx_dropped);
 			err_replenish_count[LAST_RCVD_INDEX(cpu)]++;
 			continue;
 		}
 #endif
 
+		skb->dev = pdev;
 		skb_put(skb, len);
-
 		skb->protocol = eth_type_trans(skb, pdev);
-		//skb->dev->last_rx = jiffies;
 
-		//dump_skbuff (skb);
-		//printk ("skb->len%d \n", skb->len);
 		/* Pass the packet to Network stack */
 		last_rcvd_skb[LAST_RCVD_INDEX(cpu)] = skb;
 		last_rcvd_skb_phys[LAST_RCVD_INDEX(cpu)] = addr;
+		last_rcvd_len[LAST_RCVD_INDEX(cpu)] = len;
+#ifdef CONFIG_NLM_NET_OPTS
+		last_rcvd_queue_id  = skb->queue_id;
+#endif
 
 #ifdef CONFIG_INET_LRO
 		if((skb->dev->features & NETIF_F_LRO) &&
@@ -1002,23 +986,24 @@ static int xlp_poll_lower(int budget, int cpu)
 		receive_count[LAST_RCVD_INDEX(cpu)]++;
 
 		if (last_rcvd_skb[LAST_RCVD_INDEX(cpu)]) {
-		  //printk("[%s@%d]: Unwanted buffer allocation in driver data path!\n", __FILE__, __LINE__);
+			//printk("[%s@%d]: Unwanted buffer allocation in driver data path!\n", __FILE__, __LINE__);
 			slow_replenish_count[LAST_RCVD_INDEX(cpu)]++;
-			mac_refill_frin_one_buffer(pdev, cpu, len);
+			mac_refill_frin_one_buffer(pdev, cpu, last_rcvd_queue_id);
 			last_rcvd_skb[LAST_RCVD_INDEX(cpu)] = NULL;
+			last_rcvd_len[LAST_RCVD_INDEX(cpu)] = 0;
 		}
 	}
 
 #ifdef CONFIG_INET_LRO
 	for(i = 0; i < lro_flush_priv_cnt; i++)
-                lro_flush_all(&lro_flush_priv[i]->lro_mgr[cpu]);
+		lro_flush_all(&lro_flush_priv[i]->lro_mgr[cpu]);
 #endif
 	return no_rx_pkt_rcvd;
 }
 
 /*
  * Main NAE poll loop
-*/
+ */
 
 static int xlp_nae_napi_poll(struct napi_struct *napi, int budget)
 {
@@ -1235,7 +1220,7 @@ static int nlm_initialize_vfbid(void)
 	for (tblidx = 0; tblidx < 32 ; tblidx++, cpu++) {
 		vfbid_tbl[tblidx] = (cpu * 4) + nae_fb_vc;
 	}
-	/* from 32 points to  hard replenish */
+	/* from 32 points to hard replenish */
 	for(tblidx = 32; tblidx < (32 + frin_total_queue); tblidx++, i++) {
 		vfbid_tbl[tblidx] = frin_queue_base + i;
 	}
@@ -1349,7 +1334,7 @@ static void nlm_xlp_nae_init(void)
 		/* set ethtool_ops which is inside xlp_ethtool.c file*/
 		xlp_set_ethtool_ops(dev);
 
-		dev->dev_addr = eth_hw_addr[i];
+		dev->dev_addr 	= eth_hw_addr[i];
 		priv->port	= i;
 		priv->hw_port_id = nae_cfg->ports[i].hw_port_id;
 
@@ -1707,7 +1692,8 @@ static int nlm_xlp_nae_start_xmit(struct sk_buff *skb, struct net_device *dev)
 	int cpu = hard_smp_processor_id(), ret = 0;
 	uint64_t msg0, msg1;
 	int retry_count = 128;
-	int hw_repl = 0, offset;
+	volatile int hw_repl = 0;
+	int  offset;
 
 #ifdef ENABLE_SANITY_CHECKS
 	if(!skb)
@@ -1728,12 +1714,14 @@ static int nlm_xlp_nae_start_xmit(struct sk_buff *skb, struct net_device *dev)
 #endif
 
 #ifdef CONFIG_NLM_NET_OPTS
-	if(skb->netl_skb && (last_rcvd_skb[LAST_RCVD_INDEX(cpu)] == skb->netl_skb) && !skb_shared(skb))
-#else
-	if((last_rcvd_skb[LAST_RCVD_INDEX(cpu)] == skb) && !skb_shared(skb))
+	if(skb->netl_skb && (last_rcvd_skb[LAST_RCVD_INDEX(cpu)] == skb->netl_skb)
+		&& !skb_shared(skb) && (last_rcvd_len[LAST_RCVD_INDEX(cpu)] == skb->len)
+		&& !skb_cloned(skb))
 #endif
 	{
 		last_rcvd_skb[LAST_RCVD_INDEX(cpu)] = NULL;
+		last_rcvd_len[LAST_RCVD_INDEX(cpu)] = 0;
+
 		/* Do h/w replenishment and 4 CPUs share a FIFO */
 #ifdef CONFIG_NLM_NET_OPTS
 		msg0 = nae_tx_desc(P2D_NEOP, 0, skb->queue_id - frin_queue_base + 32,
@@ -1744,9 +1732,11 @@ static int nlm_xlp_nae_start_xmit(struct sk_buff *skb, struct net_device *dev)
 
 		fast_replenish_count[LAST_RCVD_INDEX(cpu)]++;
 	}
+#ifdef CONFIG_NLM_NET_OPTS
 	else {
 		msg0 = nae_tx_desc(P2D_NEOP, 0, cpu, 0, virt_to_bus(skb));
 	}
+#endif
 	msg1 = nae_tx_desc(P2D_EOP, 0, NULL_VFBID, skb->len,
 		       virt_to_bus(skb->data));
 	if(hw_repl) {
@@ -1764,7 +1754,7 @@ static int nlm_xlp_nae_start_xmit(struct sk_buff *skb, struct net_device *dev)
 
 		/*this buffer already has backptr...
 		mac_put_skb_back_ptr(skb); */
-		skb_reserve(skb, CACHELINE_SIZE);
+		skb_reserve(skb, SKB_BACK_PTR_SIZE);
 	}
 
 
@@ -1852,6 +1842,9 @@ static int nlm_xlp_nae_change_mtu(struct net_device *dev, int new_mtu)
 	struct dev_data *priv = netdev_priv(dev);
 	unsigned long flags;
 	unsigned long local_mtu;
+#if 0
+	uint32_t rx_config = 0, node = 0;
+#endif
 
 	if (enable_jumbo && (new_mtu > ETH_JUMBO_DATA_LEN || new_mtu < ETH_ZLEN)) {
 		printk ("MTU should be between %d and %d\n", ETH_ZLEN, ETH_JUMBO_DATA_LEN);
@@ -1870,6 +1863,17 @@ static int nlm_xlp_nae_change_mtu(struct net_device *dev, int new_mtu)
 	spin_lock_irqsave(&priv->lock, flags);
 
 	local_mtu = (new_mtu+ETH_HLEN+ETH_FCS_LEN+SMP_CACHE_BYTES) & ~(SMP_CACHE_BYTES - 1);
+	if (netif_running(dev))
+	{
+#if 0
+		/* Disable RX enable bit in RX_CONFIG */
+		rx_config = nlm_hal_read_nae_reg(node, RX_CONFIG);
+		rx_config &= 0xfffffffe;
+		nlm_hal_write_nae_reg(node, RX_CONFIG, rx_config);
+#endif
+		netif_tx_stop_all_queues (dev);
+		nlm_xlp_mac_set_enable(priv, 0); /* Disable MAC TX/RX */
+	}
 
 	if(priv->type==SGMII_IF){
 		nlm_hal_set_sgmii_framesize(priv->node, priv->block, priv->index, local_mtu);
@@ -1880,12 +1884,15 @@ static int nlm_xlp_nae_change_mtu(struct net_device *dev, int new_mtu)
 
 	dev->mtu = new_mtu;
 
-	if (netif_running(dev)) {
-		/* Disable MAC TX/RX */
-		nlm_xlp_mac_set_enable(priv, 0);
-
-		/* Flush RX FR IN */
-		/* Flush TX IN */
+	if (netif_running(dev))
+	{
+#if 0
+		/* Enable RX enable bit in RX_CONFIG */
+		rx_config = nlm_hal_read_nae_reg(node, RX_CONFIG);
+		rx_config |= 0x1;
+		nlm_hal_write_nae_reg(node, RX_CONFIG, rx_config);
+#endif
+		netif_tx_start_all_queues (dev);
 		nlm_xlp_mac_set_enable(priv, 1);
 	}
 
@@ -1933,6 +1940,33 @@ static void  nlm_xlp_nae_tx_timeout (struct net_device *dev)
 	return;
 }
 
+static int nlm_xlp_nae_set_hwaddr(struct net_device *dev, void *p)
+{
+	struct sockaddr *addr = (struct sockaddr *)p;
+	struct dev_data *priv = netdev_priv(dev);
+	int rc = 0;
+
+	rc = eth_mac_addr(dev, p);
+
+	if (rc)
+		return rc;
+
+	if (priv->type == SGMII_IF)
+	{
+	  nlm_hal_write_mac_reg(priv->node,priv->block, priv->index, MAC_ADDR0_LO,
+				(addr->sa_data[5] << 24) |
+				(addr->sa_data[4] << 16) |
+				(addr->sa_data[3] << 8) |
+				(addr->sa_data[2]));
+
+	  nlm_hal_write_mac_reg(priv->node,priv->block, priv->index, MAC_ADDR0_HI,
+				(addr->sa_data[1] << 24) |
+				(addr->sa_data[0] << 16));
+	}
+
+	return rc;
+}
+
 #ifdef ENABLE_NAE_PIC_INT
 /**********************************************************************
  * nlm_xlp_nae_int_handler -  interrupt handler
-- 
1.7.1

