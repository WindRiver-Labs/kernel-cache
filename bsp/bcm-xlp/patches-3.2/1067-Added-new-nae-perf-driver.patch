From 4421e903088b7c458784458888d35da70dcaaf8e Mon Sep 17 00:00:00 2001
From: Mehul <vmehul@netlogicmicro.com>
Date: Fri, 1 Apr 2011 18:27:53 +0530
Subject: Added new nae-perf driver.

[Based on SDK 3.2]
Signed-off-by: Quanyang Wang <quanyang.wang@windriver.com>
Signed-off-by: Nam Ninh <nam.ninh@windriver.com>

diff --git a/drivers/misc/netlogic/nae-perf/Makefile b/drivers/misc/netlogic/nae-perf/Makefile
new file mode 100644
index 0000000..3f7b127
--- /dev/null
+++ b/drivers/misc/netlogic/nae-perf/Makefile
@@ -0,0 +1,13 @@
+
+################################################################################
+
+#
+# Makefile for xlp_nae network driver
+#
+
+#EXTRA_CFLAGS := -Werror
+EXTRA_CFLAGS := -DNLM_HAL_LINUX_KERNEL -Iarch/mips/include/asm/netlogic/hal
+EXTRA_CFLAGS += -Iarch/mips/netlogic/boot
+
+obj-m 		+= nae-perf.o
+nae-perf-objs 	:= xlp_nae.o init_nae.o xlp_hw.o
diff --git a/drivers/misc/netlogic/nae-perf/init_nae.c b/drivers/misc/netlogic/nae-perf/init_nae.c
new file mode 100644
index 0000000..db35027
--- /dev/null
+++ b/drivers/misc/netlogic/nae-perf/init_nae.c
@@ -0,0 +1,54 @@
+#include <linux/types.h>
+#include <linux/kernel.h>
+#include <linux/mm.h>
+#include <linux/delay.h>
+
+#include <asm/netlogic/msgring.h>
+#include <asm/netlogic/cpumask.h>
+
+#include <asm/netlogic/hal/nlm_hal_fmn.h>
+#include <asm/netlogic/hal/nlm_hal_nae.h>
+#include <asm/netlogic/hal/nlm_hal_xlp_dev.h>
+#include <ops.h>
+
+#include "net_common.h"
+
+extern int rely_on_firmware_config;
+
+static void config_fmn(void)
+{
+	unsigned long mflags = 0;
+	struct cpumask cpumask;
+
+	/* bind cpu to n0c0t0 and drain all leftover firmware messages */
+	sched_bindto_save_affinity(0, &cpumask);
+
+	/* Configure FMN again but only cpu credits */
+	msgrng_access_enable(mflags);
+
+//	nlm_xlp_msgring_int_handler(IRQ_MSGRING, NULL);
+
+	/* Configure credits to non-n0c0 cores */
+	nlm_hal_fmn_init(0x10000000, 0x02000000, 50);
+
+	msgrng_access_disable(mflags);
+
+	sched_bindto_restore_affinity(&cpumask);
+}
+
+int initialize_nae(uint32_t cm0, uint32_t cm1, uint32_t cm2, uint32_t cm3)
+{
+	int dom_id = 0;
+	unsigned long mflags;
+
+	config_fmn();
+
+	msgrng_access_enable(mflags);
+	nlm_hal_init_nae(fdt, dom_id);
+
+	printk("Overriding HAL POE configuration based on current active cpumask\n");
+	nlm_hal_init_poe_distvec(0, cm0, cm1, cm2, cm3, (1 << nae_cfg.rx_vc));
+
+	msgrng_access_disable(mflags);
+	return 0;
+}
diff --git a/drivers/misc/netlogic/nae-perf/net_common.h b/drivers/misc/netlogic/nae-perf/net_common.h
new file mode 100644
index 0000000..0d0e8e8
--- /dev/null
+++ b/drivers/misc/netlogic/nae-perf/net_common.h
@@ -0,0 +1,185 @@
+#ifndef NET_COMMON_H
+#define NET_COMMON_H
+
+#include <nlm_hal_nae.h>
+
+#define MAX_FMN_CODE            -1
+#define FMN_CREDIT_DEFAULT      8
+#define FMN_POE_CREDIT_DEFAULT      9
+#define MAX_FMN_ARRAY               50
+#define SUCCESS                 0
+#define FAIL                    -1
+#define CPU0_VC                 0
+
+#define CPU_Q_ID(cpu, vid) (cpu)
+
+#define MAX_DEST_QID            50
+
+typedef struct fmn_credit_struct {
+   unsigned int   s_qid;
+   unsigned int   d_qid;
+   unsigned int   flag;
+   #define SET_UP_QUEUE         0x1
+   #define SET_UP_CREDITS       0x2
+   #define SET_UP_MULTI_DEST    0x4
+   #define SET_UP_MULTI_SRC     0x8
+   unsigned int   q_len;
+#define FMN_QLEN_USE_DEFAULT      0
+   unsigned int   credit;
+} fmn_credit_type;
+
+extern int init_gmac(unsigned int inf);
+extern int init_tx_if_credit( /*uint32_t*/__u32 credit_val, unsigned int if_bmask);
+extern int init_ucore(uint32_t ucore_mask, int if_num);
+extern void init_ingress(void);
+extern void init_egress(void);
+extern int fmn_init(const fmn_credit_type *credit);
+extern void *xlp_init_buffer( size_t size,
+			      size_t pbase ,
+			      uint64_t *vaddr_base);
+
+extern void *init_nae_free_pool(int num_queue,
+				unsigned char *pktmem ,
+				int num_bytes,
+				int num_desc);
+extern void print_netreg(void);
+
+#define DBG        1
+#ifdef DBG
+    #define log_dbg     printk
+    #define log_pkt     printk
+#else
+    #define log_dbg(...)
+    #define log_pkt(...)
+//    #define log_err(...)
+#endif
+#define log_err
+#define log_info   printk
+
+#ifdef DBG
+static __inline__ void press_key_to_continue(void) {
+	log_dbg("press <enter> to continue...\n");
+/*	getchar();*/
+}
+#else
+#define press_key_to_continue()
+#endif
+
+enum NAE_REG_CMD {
+	CMD_READ = 0,
+	CMD_WRITE
+};
+
+#define NAE_RX_ENABLE 0x1
+#define NAE_TX_ENABLE 0x1
+
+struct xlp_msg {
+	uint64_t entry[4];
+};
+
+static __inline__ void msg_print(uint32_t size, uint32_t code, uint32_t dest, struct xlp_msg *msg) {
+	int i;
+	log_dbg("  size = %u\n"
+	       "  code = %u (0x%x)\n"
+	       "  dest = %u (0x%x)\n",
+	       size, code, code, dest, dest);
+	for (i = 0; i < size && size <= 4; ++i) {
+		log_dbg("  msg.entry%d = 0x%016llx\n",
+		       i, msg->entry[i]);
+	}
+}
+
+static __inline__ void poe_print(uint64_t msg0) {
+	log_dbg("POE nextfid  = %llu (0x%llx)\n"
+	       "    nextdist = %llu (0x%llx)\n"
+	       "    nextdest = %llu (0x%llx)\n"
+	       "    msgaddr  = 0x%llx\n"
+	       "    fid      = %llu (0x%llx)\n",
+	       (msg0 >> 48) & 0xffff, (msg0 >> 48) & 0xffff,
+	       (msg0 >> 44) & 0xf, (msg0 >> 44) & 0xf,
+	       (msg0 >> 32) & 0xfff, (msg0 >> 32) & 0xfff,
+	       (msg0 >> 16) & 0xffff,
+	       (msg0) & 0xffff, (msg0) & 0xffff);
+}
+
+static __inline__ void rx_print(uint64_t msg0) {
+	log_dbg("RX  context = %llu\n"
+	       "    length  = %llu (0x%llx)\n"
+	       "    address = 0x%010llx\n"
+	       "    unclass = %llu\n"
+	       "    err     = %llu\n"
+	       "    IPcksm  = %llu\n"
+	       "    TCPcksm = %llu\n"
+	       "    prepad  = %llu\n"
+	       "    p2p     = %llu\n",
+	       (msg0 >> 54) & 0x3ff,
+	       (msg0 >> 40) & 0x3fff, (msg0 >> 40) & 0x3fff,
+	       (msg0) & 0xffffffffc0ULL,
+	       (msg0 >> 5) & 0x1,
+	       (msg0 >> 4) & 0x1,
+	       (msg0 >> 3) & 0x1,
+	       (msg0 >> 2) & 0x1,
+	       (msg0 >> 1) & 0x1,
+	       (msg0) & 0x1);
+}
+
+static __inline__ void buf_print(unsigned char *buf, unsigned long len) {
+	unsigned long i;
+	for (i = 0; i < len; ++i) {
+		log_dbg(" %02x", buf[i]);
+		if (i % 8 == 7) log_dbg(" ");
+		if (i % 32 == 31) log_dbg("\n");
+	}
+	log_dbg("\n");
+}
+
+#define CRC_LEN 4
+#define BYTE_OFFSET 2
+
+#define NULL_VFBID 127
+
+static __inline__ uint64_t nae_tx_desc(unsigned int type,
+	unsigned int rdex, unsigned int fbid, unsigned int len, uint64_t addr) {
+	return ((uint64_t)(type & 0x3) << 62) |
+	       ((uint64_t)(rdex & 0x1) << 61) |
+	       ((uint64_t)(fbid & 0x7f) << 54) |
+	       ((uint64_t)(len & 0x3fff) << 40) |
+	       (addr&0xffffffffffULL);
+}
+
+static __inline__ void tx_print(uint64_t msg0) {
+	log_dbg("TX  type    = %llu\n"
+	       "    rdex    = %llu\n"
+	       "    vfbid   = %llu\n"
+	       "    length  = %llu (0x%llx)\n"
+	       "    address = 0x%010llx\n",
+	       ((msg0 >> 62) & 0x3),
+	       ((msg0 >> 61) & 0x1),
+	       ((msg0 >> 54) & 0x7f),
+	       ((msg0 >> 40) & 0x3fff), ((msg0 >> 40) & 0x3fff),
+	       (msg0) & 0xffffffffffULL);
+}
+
+extern int debug;
+extern void *fdt;
+
+struct nae_port {
+	int  valid;
+	int  mgmt;
+        int  num_free_desc;
+        int  txq_range[2];
+        int  rxq;
+        int  hw_port_id;
+};
+
+struct nae_config {
+	int fb_vc;
+        int rx_vc;
+	int num_ports;
+	struct nae_port ports[18];
+};
+
+extern int initialize_nae(uint32_t cm0, uint32_t cm1, uint32_t cm2, uint32_t cm3);
+extern void nlm_xlp_msgring_int_handler(unsigned int irq, struct pt_regs *regs);
+
+#endif
diff --git a/drivers/misc/netlogic/nae-perf/reg.h b/drivers/misc/netlogic/nae-perf/reg.h
new file mode 100644
index 0000000..d97fa53
--- /dev/null
+++ b/drivers/misc/netlogic/nae-perf/reg.h
@@ -0,0 +1,40 @@
+#ifndef REG_H
+#define REG_H
+
+// Reg info
+#define INF_MAC_CONFIG1                     0
+#define INF_MAC_CONFIG2                     1
+#define INF_NETWK_INF_CTRL_REG              0x7f
+
+#define NETIOR_MISC_REG1_ADDR       0x39
+/* BAR address          */
+
+#define NAE_BAR_ADDRESS             0
+#define NETIOR_SOFTRESET            0x3
+    //NETWORK INF CTRL REG(non of the values match PRM)
+#define SOFTRESET(x)                        (x<<11)
+#define STATS_EN(x)                         (x<<16)
+#define TX_EN(x)                            (x<<2)
+#define SPEED(x)                            (x&0x3)
+    //MAC_CONFIG1
+#define INF_SOFTRESET(x)                    (x<< 31)
+#define INF_LOOP_BACK(x)                    (x<< 8)
+#define INF_RX_ENABLE(x)                    (x<< 2)
+#define INF_TX_ENABLE(x)                    (0x1)
+    //MAC_CONFIG2
+
+#define INF_PREMBL_LEN(x)                   ((x & 0xf)<<12)
+#define INF_IFMODE(x)                       ((x & 0x3) << 8)
+#define INF_LENCHK(x)                       (((x & 0x1)) << 4)
+#define INF_PADCRCEN(x)                     ((x&0x1)<<2)
+#define INF_PADCRC(x)                       ((x&0x1)<<1)
+#define INF_FULLDUP(x)                      (x&0x1)
+#define NETIOR_MISC_REG2_ADDR               (0x3a)
+#define NAE_REG_ADDRS(r)              (NAE_BAR_ADDRESS&(0xffffff00000)|0x7<<13| ((r &0x3ff)<<2))
+#define NAE_REG_TX_CONFIG              0x11
+#define NAE_REG_TXIORCRDT_INIT         0x59
+#define TXINITIORCR(x)                 (x & 0x7ffff) << 8
+
+
+
+#endif
diff --git a/drivers/misc/netlogic/nae-perf/xlp_hw.c b/drivers/misc/netlogic/nae-perf/xlp_hw.c
new file mode 100755
index 0000000..405ce7f
--- /dev/null
+++ b/drivers/misc/netlogic/nae-perf/xlp_hw.c
@@ -0,0 +1,487 @@
+/*********************************************************************
+
+  Copyright 2003-2010 Netlogic Microsystem, Inc. ("Netlogic"). All rights
+  reserved.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+  1. Redistributions of source code must retain the above copyright
+  notice, this list of conditions and the following disclaimer.
+  2. Redistributions in binary form must reproduce the above copyright
+  notice, this list of conditions and the following disclaimer in
+  the documentation and/or other materials provided with the
+  distribution.
+
+  THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems, Inc. ``AS IS'' AND
+  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL RMI OR CONTRIBUTORS BE LIABLE
+  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+  INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+  CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+  THE POSSIBILITY OF SUCH DAMAGE.
+
+  *****************************#NLM_2#**********************************/
+
+#include <linux/types.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/init.h>
+
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/mii.h>
+
+#include <asm/netlogic/xlr_mac.h>
+#include <asm/netlogic/hal/nlm_hal_nae.h>
+#include "xlp_nae.h"
+
+
+#define NLM_NUM_REG_DUMP 9 /* Register 0xa0 to 0xa8 */
+#define NLM_ETHTOOL_REG_LEN (NLM_NUM_REG_DUMP * 4)
+#define PHY_STATUS_RETRIES 25000
+
+#define DRV_NAME	"xlp_nae"
+#define DRV_VERSION     "0.1"
+
+static void nlm_xlp_mac_mii_write(struct dev_data *priv, int regidx, uint16_t regval);
+static unsigned int nlm_xlp_mac_mii_read(struct dev_data *priv, int regidx);
+void nlm_xlp_mac_set_enable(struct dev_data *priv, int flag);
+static int xlp_enable_autoneg(struct net_device *dev, u32 adv);
+static int xlp_set_link_speed(struct net_device *dev, int speed, int duplex);
+
+static int xlp_get_settings(struct net_device *dev, struct ethtool_cmd *cmd)
+{
+	struct dev_data *priv = netdev_priv(dev);
+	int mii_status;
+
+	if (priv->type != SGMII_IF) {
+		cmd->supported = SUPPORTED_FIBRE|SUPPORTED_10000baseT_Full;
+		cmd->advertising = SUPPORTED_FIBRE|SUPPORTED_10000baseT_Full;
+		cmd->speed = SPEED_10000;
+		cmd->port = PORT_FIBRE;
+		cmd->duplex = DUPLEX_FULL;
+		cmd->phy_address = priv->port;
+		cmd->autoneg = AUTONEG_DISABLE;
+		cmd->maxtxpkt = 0;
+		cmd->maxrxpkt = 0;
+
+	}else{
+
+		cmd->supported = SUPPORTED_10baseT_Full |
+			SUPPORTED_10baseT_Half |
+			SUPPORTED_100baseT_Full | SUPPORTED_100baseT_Half |
+			SUPPORTED_1000baseT_Full | SUPPORTED_MII |
+			SUPPORTED_Autoneg | SUPPORTED_TP;
+
+		cmd->advertising = priv->advertising;
+
+		mii_status = nlm_xlp_mac_mii_read(priv, MII_NCONFIG);
+		priv->speed = (mii_status >> 3) & 0x03;
+
+		cmd->speed = (priv->speed == xlp_mac_speed_1000) ? SPEED_1000 :
+		(priv->speed == xlp_mac_speed_100) ? SPEED_100: SPEED_10;
+
+		cmd->duplex = (mii_status >> 5) & 0x1;
+		cmd->port = PORT_TP;
+		cmd->phy_address = priv->port;
+		cmd->transceiver = XCVR_INTERNAL;
+		cmd->autoneg = (~(mii_status >> 14)) & 0x1;
+		cmd->maxtxpkt = 0;
+		cmd->maxrxpkt = 0;
+	}
+
+	return 0;
+}
+static int xlp_enable_autoneg(struct net_device *dev, u32 adv)
+{
+	struct dev_data *priv = netdev_priv(dev);
+	int mii_status;
+	u32 adv1, adv2;
+	unsigned long flags;
+
+	spin_lock_irqsave(&priv->lock, flags);
+	nlm_xlp_mac_set_enable(priv, 0);
+	/* advertising for 10/100 Mbps */
+	adv1 = nlm_xlp_mac_mii_read(priv, MII_ADVERTISE);
+	adv1 &= ~(ADVERTISE_ALL | ADVERTISE_100BASE4);
+	/* advertising for 1000 Mbps */
+	adv2 = nlm_xlp_mac_mii_read(priv, 0x9);
+	adv2 &= ~(0x300);
+
+	if(adv & ADVERTISED_10baseT_Half)
+		adv1 |= ADVERTISE_10HALF;
+	if(adv & ADVERTISED_10baseT_Full)
+		adv1 |= ADVERTISE_10FULL;
+	if(adv & ADVERTISED_100baseT_Full)
+		adv1 |= ADVERTISE_100FULL;
+	if(adv & ADVERTISED_100baseT_Half)
+		adv1 |= ADVERTISE_100HALF;
+
+	if(adv & ADVERTISED_1000baseT_Full)
+		adv2 |= 0x200;
+	if(adv & ADVERTISED_1000baseT_Half)
+		adv2 |= 0x100;
+
+	/* Set the advertising parameters */
+	nlm_xlp_mac_mii_write(priv, MII_ADVERTISE, adv1);
+	nlm_xlp_mac_mii_write(priv, 0x9, adv2);
+
+	priv->advertising = adv1 | adv2;
+
+	mii_status = nlm_xlp_mac_mii_read(priv, MII_BMCR);
+	/* enable autoneg and force restart autoneg */
+	mii_status |= (BMCR_ANENABLE | BMCR_ANRESTART);
+	nlm_xlp_mac_mii_write(priv, MII_BMCR, mii_status);
+
+	nlm_xlp_mac_set_enable(priv, 1);
+	spin_unlock_irqrestore(&priv->lock, flags);
+
+	return 0;
+}
+
+static int xlp_set_link_speed(struct net_device *dev, int speed, int duplex)
+{
+	u32 adv;
+	int ret =0;
+
+	switch(speed) {
+		case SPEED_10:
+			if ( duplex == DUPLEX_FULL )
+				adv = ADVERTISED_10baseT_Full;
+			else
+				adv = ADVERTISED_10baseT_Half;
+			break;
+		case SPEED_100:
+			if ( duplex == DUPLEX_FULL )
+				adv = ADVERTISED_100baseT_Full;
+			else
+				adv = ADVERTISED_100baseT_Half;
+			break;
+		case SPEED_1000:
+			if ( duplex == DUPLEX_FULL )
+				adv = ADVERTISED_1000baseT_Full;
+			else
+				adv = ADVERTISED_1000baseT_Half;
+			break;
+		default:
+			ret = -EINVAL;
+			return ret;
+	}
+	ret = xlp_enable_autoneg( dev,adv);
+	return ret;
+
+}
+
+static int xlp_set_settings(struct net_device *dev, struct ethtool_cmd *cmd)
+{
+	int ret;
+	struct dev_data *priv = netdev_priv(dev);
+
+	if (priv->type != SGMII_IF) {
+		return -EIO;
+	}
+	if (cmd->autoneg == AUTONEG_ENABLE) {
+		ret = xlp_enable_autoneg(dev, cmd->advertising);
+	}else {
+		ret = xlp_set_link_speed(dev, cmd->speed, cmd->duplex);
+	}
+	return ret;
+}
+
+static void xlp_get_drvinfo(struct net_device *dev,
+				struct ethtool_drvinfo *info)
+{
+	strcpy(info->driver, DRV_NAME);
+	strcpy(info->version, DRV_VERSION);
+}
+
+static int xlp_get_regs_len(struct net_device *dev)
+{
+	return NLM_ETHTOOL_REG_LEN;
+}
+static void xlp_get_regs(struct net_device *dev,
+				struct ethtool_regs *regs, void *p)
+{
+	u32 *data = (u32 *)p;
+	int i;
+	struct dev_data *priv = netdev_priv(dev);
+	unsigned long flags;
+
+	memset((void *)data, 0, NLM_ETHTOOL_REG_LEN);
+
+	spin_lock_irqsave(&priv->lock, flags);
+	for(i=0; i <= NLM_NUM_REG_DUMP; i++)
+		*(data + i) = nlm_hal_read_mac_reg(priv->block, priv->index,  R_TX_CONTROL + i);
+	spin_unlock_irqrestore(&priv->lock, flags);
+}
+static u32 xlp_get_msglevel(struct net_device *dev)
+{
+	return 0; //mac_debug;
+}
+static void xlp_set_msglevel(struct net_device *dev, u32 value)
+{
+//	mac_debug = value;
+}
+
+static int xlp_nway_reset(struct net_device *dev)
+{
+	struct dev_data *priv = netdev_priv(dev);
+	int mii_status;
+	unsigned long flags;
+	int ret = -EINVAL;
+
+	if (priv->type != SGMII_IF)
+		return -EIO;
+
+	spin_lock_irqsave(&priv->lock, flags);
+	mii_status = nlm_xlp_mac_mii_read(priv, MII_BMCR);
+	if(mii_status & BMCR_ANENABLE)
+	{
+		nlm_xlp_mac_mii_write(priv,
+				MII_BMCR, BMCR_ANRESTART | mii_status);
+		ret = 0;
+	}
+	spin_unlock_irqrestore(&priv->lock, flags);
+	return ret;
+}
+
+static u32 xlp_get_link(struct net_device *dev)
+{
+	struct dev_data *priv = netdev_priv(dev);
+	int mii_status;
+	unsigned long flags;
+	if (priv->type != SGMII_IF)
+		return -EIO;
+
+	spin_lock_irqsave(&priv->lock, flags);
+	mii_status = nlm_xlp_mac_mii_read(priv, MII_BMSR);
+
+	spin_unlock_irqrestore(&priv->lock, flags);
+
+	if(mii_status & BMSR_LSTATUS)
+		return 1;
+	return 0;
+}
+#define NLM_STATS_KEY_LEN  \
+		(sizeof(struct net_device_stats) / sizeof(unsigned long))
+static struct {
+	        const char string[ETH_GSTRING_LEN];
+} xlp_ethtool_stats_keys[NLM_STATS_KEY_LEN] = {
+	{ "rx_packets" },
+	{ "tx_packets" },
+	{ "rx_bytes" },
+	{ "tx_bytes" },
+	{ "rx_errors" },
+	{ "tx_errors" },
+	{ "rx_dropped" },
+	{ "tx_dropped" },
+	{ "multicast" },
+	{ "collisions" },
+	{ "rx_length_errors" },
+	{ "rx_over_errors" },
+	{ "rx_crc_errors" },
+	{ "rx_frame_errors" },
+	{ "rx_fifo_errors" },
+	{ "rx_missed_errors" },
+	{ "tx_aborted_errors" },
+	{ "tx_carrier_errors" },
+	{ "tx_fifo_errors" },
+	{ "tx_heartbeat_errors" },
+	{ "tx_window_errors" },
+	{ "rx_compressed" },
+	{ "tx_compressed" }
+};
+static int xlp_get_stats_count (struct net_device *dev)
+{
+	return NLM_STATS_KEY_LEN;
+}
+
+static void xlp_get_strings (struct net_device *dev, u32 stringset, u8 *buf)
+{
+	switch (stringset) {
+	case ETH_SS_STATS:
+		memcpy(buf, &xlp_ethtool_stats_keys,
+				sizeof(xlp_ethtool_stats_keys));
+		break;
+	default:
+		printk(KERN_WARNING "%s: Invalid stringset %d\n",
+				__func__, stringset);
+		break;
+	}
+}
+
+
+/**********************************************************************
+ * xlp_get_mac_stats -  collect stats info from Mac stats register
+ * @dev   -  this is per device based function
+ * @stats -  net device stats structure
+ **********************************************************************/
+void xlp_get_mac_stats(struct net_device *dev, struct net_device_stats *stats)
+{
+	struct dev_data *priv = netdev_priv(dev);
+
+	if (priv->type != SGMII_IF)
+		return;
+
+	stats->tx_errors = nlm_hal_read_mac_reg( priv->block, priv->index, TX_FCS_ERROR_COUNTER);
+	stats->rx_dropped = nlm_hal_read_mac_reg( priv->block, priv->index, RX_DROP_PACKET_COUNTER);
+	stats->tx_dropped = nlm_hal_read_mac_reg( priv->block, priv->index, TX_DROP_FRAME_COUNTER);
+	stats->multicast = nlm_hal_read_mac_reg( priv->block, priv->index, RX_MULTICAST_PACKET_COUNTER);
+	stats->collisions = nlm_hal_read_mac_reg( priv->block, priv->index, TX_TOTAL_COLLISION_COUNTER);
+	stats->rx_length_errors = nlm_hal_read_mac_reg( priv->block, priv->index, RX_FRAME_LENGTH_ERROR_COUNTER);
+	stats->rx_over_errors = nlm_hal_read_mac_reg( priv->block, priv->index, RX_DROP_PACKET_COUNTER);
+	stats->rx_crc_errors = nlm_hal_read_mac_reg( priv->block, priv->index, RX_FCS_ERROR_COUNTER);
+	stats->rx_frame_errors = nlm_hal_read_mac_reg( priv->block, priv->index, RX_ALIGNMENT_ERROR_COUNTER);
+	stats->rx_fifo_errors = nlm_hal_read_mac_reg( priv->block, priv->index,RX_DROP_PACKET_COUNTER);
+	stats->rx_missed_errors = nlm_hal_read_mac_reg( priv->block, priv->index,RX_CARRIER_SENSE_ERROR_COUNTER);
+	stats->rx_errors = (stats->rx_over_errors + stats->rx_crc_errors + stats->rx_frame_errors + stats->rx_fifo_errors +stats->rx_missed_errors);
+	stats->tx_aborted_errors = nlm_hal_read_mac_reg( priv->block, priv->index, TX_EXCESSIVE_COLLISION_PACKET_COUNTER);
+	/*
+	stats->tx_carrier_errors = nlm_hal_read_mac_reg( priv->block, priv->index, TX_DROP_FRAME_COUNTER);
+	stats->tx_fifo_errors = nlm_hal_read_mac_reg( priv->block, priv->index, TX_DROP_FRAME_COUNTER);
+	*/
+	return;
+}
+
+/**********************************************************************
+ * xlp_get_ethtool_stats -  part of ethtool_ops member function
+ * @dev   -  this is per device based function
+ * @stats -  net device stats structure
+ **********************************************************************/
+static void xlp_get_ethtool_stats (struct net_device *dev,
+			struct ethtool_stats *estats, u64 *stats)
+{
+	int i;
+	struct dev_data *priv = netdev_priv(dev);
+	unsigned long flags;
+	unsigned long *tmp_stats;
+
+	spin_lock_irqsave(&priv->lock, flags);
+
+	xlp_get_mac_stats(dev, &priv->stats);
+
+
+	spin_unlock_irqrestore(&priv->lock, flags);
+
+	tmp_stats = (unsigned long *)&priv->stats;
+	for(i=0; i < NLM_STATS_KEY_LEN; i++) {
+		*stats = (u64)*tmp_stats;
+		stats++;
+		tmp_stats++;
+	}
+}
+
+/**********************************************************************
+ *  nlm_xlp_mac_mii_read - Read mac mii phy register
+ *
+ *  Input parameters:
+ *  	   priv - priv structure
+ *  	   phyaddr - PHY's address
+ *  	   regidx = index of register to read
+ *
+ *  Return value:
+ *  	   value read (16 bits), or 0xffffffff if an error occurred.
+ ********************************************************************* */
+static unsigned int nlm_xlp_mac_mii_read(struct dev_data *priv, int regidx)
+{
+        return nlm_hal_mdio_read(NLM_HAL_EXT_MDIO, 0, BLOCK_7, LANE_CFG, priv->phy.addr, regidx);
+}
+
+/**********************************************************************
+ *  nlm_xlp_mac_mii_write -Write mac mii PHY register.
+ *
+ *  Input parameters:
+ *  	   priv - priv structure
+ *  	   regidx - register within the PHY
+ *  	   regval - data to write to register
+ *
+ *  Return value:
+ *  	   nothing
+ ********************************************************************* */
+static void nlm_xlp_mac_mii_write(struct dev_data *priv, int regidx, uint16_t regval)
+{
+	nlm_hal_mdio_write(NLM_HAL_EXT_MDIO, 0, BLOCK_7, LANE_CFG, priv->phy.addr, regidx, regval);
+	return;
+}
+
+static struct ethtool_ops xlp_ethtool_ops= {
+        .get_settings           = xlp_get_settings,
+        .set_settings           = xlp_set_settings,
+        .get_drvinfo            = xlp_get_drvinfo,
+        .get_regs_len           = xlp_get_regs_len,
+        .get_regs               = xlp_get_regs,
+        .get_msglevel           = xlp_get_msglevel,
+        .set_msglevel           = xlp_set_msglevel,
+        .nway_reset             = xlp_nway_reset,
+        .get_link               = xlp_get_link,
+        .get_strings            = xlp_get_strings,
+        .get_stats_count        = xlp_get_stats_count,
+        .get_ethtool_stats      = xlp_get_ethtool_stats,
+};
+
+void xlp_set_ethtool_ops(struct net_device *netdev)
+{
+	SET_ETHTOOL_OPS(netdev, &xlp_ethtool_ops);
+}
+
+
+/**********************************************************************
+ **********************************************************************/
+void nlm_xlp_mac_set_enable(struct dev_data *priv, int flag)
+{
+	int inf;
+	uint32_t speed = 0, duplex = 0, ifmode = 0;
+	uint32_t netwk_inf = 0, mac_cfg2 = 0;
+
+
+	if ((priv->type != SGMII_IF) && (priv->type != XAUI_IF))
+		return;
+	switch(priv->type) {
+		case SGMII_IF:
+			inf = (priv->block * 4) + priv->index;
+			break;
+		case XAUI_IF:
+		case INTERLAKEN_IF:
+			inf = priv->block;
+			break;
+		default:
+			return;
+	}
+
+	if (flag) {
+		if (priv->type == SGMII_IF) {
+			if (nlm_hal_get_phy_status(inf, &speed, &duplex)) {
+				//nlm_print("mac set enable speed %d duplex %d\n",speed, duplex);
+				ifmode = ((speed == 2) ? 2: 1);
+				nlm_hal_mac_disable(inf, priv->type);
+			        netwk_inf  = read_gmac_reg(inf, NETWK_INF_CTRL_REG);
+				netwk_inf &= (~(0x3));
+				write_gmac_reg(inf , NETWK_INF_CTRL_REG, netwk_inf | speed);
+				mac_cfg2 = read_gmac_reg(inf, MAC_CONF2);
+				mac_cfg2 &= (~((0x3 << 8) | 1));
+				write_gmac_reg(inf , MAC_CONF2,
+					              mac_cfg2 | (ifmode << 8) | duplex);
+			}
+		}
+		nlm_hal_mac_enable(inf, priv->type);
+	} else {
+		nlm_hal_mac_disable(inf, priv->type);
+	}
+}
+
+int nlm_xlp_link_up(struct dev_data *priv, int phy)
+{
+        uint16_t extstatus;
+
+	if (priv->type != SGMII_IF)
+                return -EIO;
+        nlm_hal_mdio_write(NLM_HAL_EXT_MDIO, 0, BLOCK_7, LANE_CFG, priv->phy.addr, 22, 0);
+        extstatus = nlm_hal_mdio_read(NLM_HAL_EXT_MDIO, 0, BLOCK_7, LANE_CFG, priv->phy.addr, 17);
+        return ((extstatus & 0x0400) ? 1 : 0 );
+}
diff --git a/drivers/misc/netlogic/nae-perf/xlp_nae.c b/drivers/misc/netlogic/nae-perf/xlp_nae.c
new file mode 100755
index 0000000..8641988
--- /dev/null
+++ b/drivers/misc/netlogic/nae-perf/xlp_nae.c
@@ -0,0 +1,2106 @@
+/********************************************************************
+
+  Copyright 2003-2010 Netlogic Microsystem, Inc. ("Netlogic"). All rights
+  reserved.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+  1. Redistributions of source code must retain the above copyright
+  notice, this list of conditions and the following disclaimer.
+  2. Redistributions in binary form must reproduce the above copyright
+  notice, this list of conditions and the following disclaimer in
+  the documentation and/or other materials provided with the
+  distribution.
+
+  THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems, Inc. ``AS IS'' AND
+  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL RMI OR CONTRIBUTORS BE LIABLE
+  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+  INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+  CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+  THE POSSIBILITY OF SUCH DAMAGE.
+
+  *****************************#NLM_2#**********************************/
+
+#include <linux/types.h>
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/interrupt.h>
+
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/fs.h>
+#include <linux/errno.h>
+#include <linux/proc_fs.h>
+#include <linux/fcntl.h>
+#include <linux/mman.h>
+#include <linux/mm.h>
+#include <linux/pci.h>
+#include <linux/kthread.h>
+
+#include <net/ip.h>
+
+#include <asm/current.h>
+#include <asm/system.h>
+#include <asm/uaccess.h>
+#include <asm/netlogic/msgring.h>
+#include <asm/netlogic/cpumask.h>
+
+#include <asm/netlogic/hal/nlm_hal_fmn.h>
+#include <asm/netlogic/hal/nlm_hal_nae.h>
+#include <asm/netlogic/hal/nlm_hal_pic.h>
+
+#include "net_common.h"
+#include "xlp_nae.h"
+
+#if 1
+#include <asm/atomic.h>
+
+#define STATS_SET(x,v)  //atomic64_set((atomic64_t *)&(x), (v))
+#define STATS_ADD(x,v)  //atomic64_add((long)(v), (atomic64_t *)&(x))
+#define STATS_INC(x)    //atomic64_inc((atomic64_t *)&(x))
+#define STATS_READ(x)   (x)//atomic64_read((atomic64_t *)&(x))
+#else
+#define STATS_SET(x,v)  do { (x) = (v); } while (0)
+#define STATS_ADD(x,v)  do { (x) += (v); } while (0)
+#define STATS_INC(x)    do { (x) += 1; } while (0)
+#define STATS_READ(x)   (x)
+#endif
+
+#define XLP_SOC_MAC_DRIVER "XLP Mac Driver"
+
+/* On-Chip NAE PCI Header */
+#define PCI_NETL_VENDOR			0xfecc
+#define PCI_DEVID_BASE			0
+#define PCI_DEVID_OFF_NET		0
+
+#define MAX_NET_INF             	1
+#define MAX_GMAC_PORT               	18
+#define XLP_SGMII_RCV_CONTEXT_NUM	8
+
+/* FMN send failure errors */
+#define MSG_DST_FC_FAIL                 0x01
+#define MSG_INFLIGHT_MSG_EX             0x02
+#define MSG_TXQ_FULL                    0x04
+
+#define ETH_MTU_SIZE		 	1536
+#define MIN_ETH_FRAME_SIZE		64
+
+#define  DUMP_PKT(str, x, y)	if (debug == 2)  {	\
+	int i;      				\
+        printk(" %s \n", str);                  \
+        for(i = 0; i < y; i++)			\
+        {					\
+                printk("%02x ", (x)[i]);		\
+                if( i % 16 == 15)		\
+                        printk("\n");		\
+        }					\
+	printk("\n"); }
+
+#define NLM_TCP_MODE	1
+#define NLM_RT_MODE	2
+/*calculate number of instructions/clocks per packet*/
+#undef ENABLE_DEBUG_STATISTICS
+
+/*Calculate per cpu statistics - fast_path_replenish, total_packet_rcvd etc*/
+#undef ENABLE_PER_CPU_COUNTER
+
+/*Enable sanity checks while receiving or transmitting buffer */
+#undef ENABLE_SANITY_CHECKS
+
+
+/* Module Parameters */
+int debug = 0;
+module_param(debug, int, 0);
+
+static int drop_uboot_pkt = 1;
+module_param(drop_uboot_pkt, int, 0);
+static unsigned long stats_uboot_pkts;
+
+extern int naecfg_hack;
+module_param(naecfg_hack, int, 0);
+
+static int perf_mode= NLM_TCP_MODE;
+module_param(perf_mode, int, 0);
+
+/***************************************************************
+ *
+ * Below parameters are set during FDT file parsing
+ */
+int frin_desc_thres = 24;
+module_param(frin_desc_thres, int, 0);
+
+static uint32_t nae_rx_vc = 0;
+static uint32_t nae_fb_vc = 0;
+/***************************************************************/
+
+unsigned char eth_hw_addr[18][6] = {
+	{0x00,0x01,0x02,0x03,0x04,0x05},
+	{0x00,0x01,0x02,0x03,0x04,0x06},
+	{0x00,0x01,0x02,0x03,0x04,0x07},
+	{0x00,0x01,0x02,0x03,0x04,0x08},
+	{0x00,0x01,0x02,0x03,0x04,0x09},
+	{0x00,0x01,0x02,0x03,0x04,0x0A},
+	{0x00,0x01,0x02,0x03,0x04,0x0B},
+	{0x00,0x01,0x02,0x03,0x04,0x0C},
+	{0x00,0x01,0x02,0x03,0x04,0x0D},
+	{0x00,0x01,0x02,0x03,0x04,0x0E},
+	{0x00,0x01,0x02,0x03,0x04,0x0F},
+	{0x00,0x01,0x02,0x03,0x04,0x10},
+	{0x00,0x01,0x02,0x03,0x04,0x11},
+	{0x00,0x01,0x02,0x03,0x04,0x12},
+	{0x00,0x01,0x02,0x03,0x04,0x13},
+	{0x00,0x01,0x02,0x03,0x04,0x14},
+	{0x00,0x01,0x02,0x03,0x04,0x15},
+	{0x00,0x01,0x02,0x03,0x04,0x16}
+};
+
+/* Use index of 8 as the offset because of n64 abi and 64B cacheline size */
+#define LAST_RCVD_INDEX(cpu) ((cpu) * 8)
+
+uint64_t nlm_mode[NR_CPUS*8] ____cacheline_aligned;
+struct sk_buff *last_rcvd_skb[NR_CPUS * 8] ____cacheline_aligned;
+uint64_t last_rcvd_skb_phys[NR_CPUS * 8] ____cacheline_aligned;
+uint64_t fast_replenish_count[NR_CPUS * 8] __cacheline_aligned;
+uint64_t slow_replenish_count[NR_CPUS * 8] __cacheline_aligned;
+uint64_t err_replenish_count[NR_CPUS * 8] __cacheline_aligned;
+#ifdef ENABLE_DEBUG_STATISTICS
+volatile uint64_t skbuff_clocks[NR_CPUS * 8] __cacheline_aligned;
+volatile uint64_t skbuff_event[NR_CPUS * 8] __cacheline_aligned;
+#endif
+//make this array of 24 ports to keep it cacheline aligned.
+struct net_device *per_cpu_netdev[NR_CPUS][24] __cacheline_aligned;
+uint64_t total_cycles[NR_CPUS * 8] __cacheline_aligned;
+uint64_t total_rx_pkts[NR_CPUS * 8] __cacheline_aligned;
+
+#define ETHER_FRAME_MIN_LEN	64
+static struct pci_device_id soc_pci_table[] __devinitdata = {
+        {PCI_NETL_VENDOR, PCI_DEVID_BASE + PCI_DEVID_OFF_NET,
+         PCI_ANY_ID, PCI_ANY_ID, 0},
+        {}
+};
+
+
+#define MAX_TSO_SKB_PEND_REQS 	50
+#define MAX_PACKET_SZ_PER_MSG	16384
+#define P2P_EXTRA_DESCS	      	((PAGE_SIZE / MAX_PACKET_SZ_PER_MSG) + 4)
+#define P2P_SKB_OFF	      	(MAX_SKB_FRAGS + P2P_EXTRA_DESCS - 1)
+#define tso_dbg(fmt, args...) //printk(fmt, ##args);
+static uint64_t p2p_dynamic_alloc_cnt;
+
+struct p2p_desc_mem {
+	void *mem;
+	uint64_t dsize;
+	uint64_t pad[6];
+};
+struct p2p_desc_mem p2p_desc_mem[NR_CPUS] __cacheline_aligned;
+enum msc_opcodes { IP_CHKSUM = 1,
+	TCP_CHKSUM,
+	UDP_CHKSUM,
+	SCTP_CRC,
+	FCOE_CRC,
+	IP_TCP_CHKSUM,
+	TSO_IP_TCP_CHKSUM,
+	IP_UDP_CHKSUM,
+	IP_CHKSUM_SCTP_CRC
+};
+
+
+
+extern void xlp_set_ethtool_ops(struct net_device *netdev);
+extern void xlp_get_mac_stats(struct net_device* dev, struct net_device_stats* stats);
+spinlock_t  nlm_xlp_nae_lock;
+static void nlm_xlp_nae_init(void);
+static int xlp_mac_proc_read(char *page, char **start, off_t off,int count, int *eof, void *data);
+static int  nlm_xlp_nae_open (struct net_device *dev);
+static int  nlm_xlp_nae_stop (struct net_device *dev);
+static int  nlm_xlp_nae_start_xmit (struct sk_buff *skb, struct net_device *dev);
+static void  nlm_xlp_set_multicast_list (struct net_device *dev);
+static int  nlm_xlp_nae_ioctl (struct net_device *dev, struct ifreq *rq, int cmd);
+static int  nlm_xlp_nae_change_mtu(struct net_device *dev, int new_mtu);
+static void  nlm_xlp_nae_tx_timeout (struct net_device *dev);
+static void xlp_mac_setup_hwaddr(struct dev_data *priv);
+
+#ifdef  ENABLE_NAE_PIC_INT
+static irqreturn_t nlm_xlp_nae_int_handler(int irq, void * dev_id);
+#endif
+
+static void nlm_xlp_nae_msgring_handler(uint32_t vc, uint32_t src_id,
+					uint32_t size, uint32_t code,
+					uint64_t msg0, uint64_t msg1,
+					uint64_t msg2, uint64_t msg3, void* data);
+
+static void nlm_xlp_mac_timer(unsigned long data);
+static struct net_device_stats *nlm_xlp_mac_get_stats(struct net_device *dev);
+
+static struct net_device *dev_mac[MAX_GMAC_PORT];
+
+extern struct proc_dir_entry *nlm_root_proc;
+static struct tasklet_struct mac_refill_task[MAX_GMAC_PORT];
+static int mac_refill_frin_desc(unsigned long dev);
+
+extern void nlm_xlp_mac_set_enable(struct dev_data *priv, int flag);
+
+static unsigned short  nlm_select_queue(struct net_device *dev, struct sk_buff *skb)
+{
+	        return (unsigned short)smp_processor_id();
+}
+
+static const struct net_device_ops nlm_xlp_nae_ops = {
+	.ndo_open	= nlm_xlp_nae_open,
+	.ndo_stop	= nlm_xlp_nae_stop,
+	.ndo_start_xmit	= nlm_xlp_nae_start_xmit,
+	.ndo_set_multicast_list	= nlm_xlp_set_multicast_list,
+	.ndo_do_ioctl	= nlm_xlp_nae_ioctl,
+	.ndo_tx_timeout = nlm_xlp_nae_tx_timeout,
+	.ndo_change_mtu	= nlm_xlp_nae_change_mtu,
+	.ndo_set_mac_address	= eth_mac_addr,
+	.ndo_get_stats = nlm_xlp_mac_get_stats,
+	.ndo_select_queue	= nlm_select_queue,
+};
+
+static __inline__ void cpu_halt(void)
+{
+	__asm__ volatile (".set push\n"
+			  ".set noreorder\n"
+			  "   wait\n"
+			  "1: b    1b\n"
+			  "   nop\n"
+			  ".set pop\n"
+		);
+}
+
+static __inline__ void print_fmn_send_error(const char *str, uint32_t send_result)
+{
+	if (debug < 1) return;
+
+	if(send_result & MSG_DST_FC_FAIL)
+	{
+		printk("[%s] Msg Destination flow control credit fail(send_result=%08x)\n",
+		       str, send_result);
+	}
+	else if (send_result & MSG_INFLIGHT_MSG_EX) {
+		printk("[%s] MSG_INFLIGHT_MSG_EX(send_result=%08x)\n", __func__, send_result);
+	}
+	else if (send_result & MSG_TXQ_FULL) {
+		printk("[%s] TX message Q full(send_result=%08x)\n", __func__, send_result);
+	}
+	else {
+		printk("[%s] Unknown send error type(send_result=%08x)\n", __func__, send_result);
+	}
+}
+
+static __inline__ struct sk_buff *mac_get_skb_back_ptr(uint64_t addr)
+{
+        uint64_t *back_ptr = (uint64_t *)(addr - CACHELINE_SIZE);
+        /* this function should be used only for newly allocated packets. It assumes
+         * the first cacheline is for the back pointer related book keeping info
+         */
+        return (struct sk_buff *)(*back_ptr);
+}
+
+static __inline__ void mac_put_skb_back_ptr(struct sk_buff *skb)
+{
+        uint64_t *back_ptr = (uint64_t *)skb->data;
+
+        /* this function should be used only for newly allocated packets. It assumes
+         * the first cacheline is for the back pointer related book keeping info
+         */
+        skb_reserve(skb, CACHELINE_SIZE);
+        *back_ptr = (uint64_t)skb;
+}
+
+#define CACHELINE_ALIGNED_ADDR(addr) (((unsigned long)(addr)) & ~(CACHELINE_SIZE-1))
+
+/**********************************************************************
+ * cacheline_aligned_kmalloc -  64 bits cache aligned kmalloc
+ * return -  buffer address
+ *
+ **********************************************************************/
+static __inline__ void *cacheline_aligned_kmalloc(int size, int gfp_mask)
+{
+        void *buf = kmalloc(size + CACHELINE_SIZE, gfp_mask);
+        if (buf)
+                buf =(void*)(CACHELINE_ALIGNED_ADDR((unsigned long)buf +
+						    CACHELINE_SIZE));
+        return buf;
+}
+
+/**********************************************************************
+ * nlm_xlp_alloc_skb -  64 bits cache aligned skb buffer allocate
+ * return - skb buffer address
+ *
+ **********************************************************************/
+static __inline__ struct sk_buff *nlm_xlp_alloc_skb(void)
+{
+        int offset = 0;
+        struct sk_buff *skb = __dev_alloc_skb(NLM_RX_BUF_SIZE, GFP_KERNEL);
+
+        if (!skb) {
+                return NULL;
+        }
+        /* align the data to the next cache line */
+        offset = ((unsigned long)skb->data + CACHELINE_SIZE) &
+                ~(CACHELINE_SIZE - 1);
+        skb_reserve(skb, (offset - (unsigned long)skb->data));
+
+        return skb;
+}
+
+/**********************************************************************
+ * nlm_xlp_alloc_skb_atomic -  Atomically allocates 64 bits cache aligned skb buffer
+ * return - skb buffer address
+ *
+ **********************************************************************/
+static __inline__ struct sk_buff *nlm_xlp_alloc_skb_atomic(void)
+{
+        int offset = 0;
+        struct sk_buff *skb = __dev_alloc_skb(NLM_RX_BUF_SIZE, GFP_ATOMIC);
+
+        if (!skb) {
+                return NULL;
+        }
+        /* align the data to the next cache line */
+        offset = ((unsigned long)skb->data + CACHELINE_SIZE) &
+                ~(CACHELINE_SIZE - 1);
+        skb_reserve(skb, (offset - (unsigned long)skb->data));
+
+        return skb;
+}
+
+
+/**********************************************************************
+ * nlm_xlp_free_skb -  change msg into skb buffer address, free it
+ * @msg - freeback msg that sent to cpu vc
+ *
+ **********************************************************************/
+static inline void nlm_xlp_free_skb(struct xlp_msg *msg)
+{
+	struct sk_buff *skb;
+	struct dev_data *priv;
+	int cpu = hard_smp_processor_id();
+	unsigned long tmp;
+
+	tmp = (unsigned long)(msg->entry[0] & 0xffffffffffULL);
+	skb = (struct sk_buff *)bus_to_virt(tmp);
+
+	if(!skb)
+		return;
+	/* Tx Complete */
+
+	/* release the skb and update statistics outside the spinlock */
+	priv = netdev_priv(skb->dev);
+	STATS_INC(priv->stats.tx_packets);
+	STATS_ADD(priv->stats.tx_bytes, skb->len);
+	priv->cpu_stats[cpu].txc_packets++;
+
+
+	netif_tx_wake_all_queues(skb->dev);
+	/* nlm_netif_queue_tx_complete(skb->dev);*/
+
+	dev_kfree_skb_any(skb);
+}
+
+/*********************************************************************
+  * set tso enable features in the dev list
+ **********************************************************************/
+static __inline__ int tso_enable(struct net_device *dev, u32 data)
+{
+	int rv;
+	rv = ethtool_op_set_tso(dev, data);
+	if(rv == 0)
+		rv = ethtool_op_set_tx_csum(dev, data);
+	if(rv == 0)
+		rv = ethtool_op_set_sg(dev, data);
+	dev->features |= NETIF_F_FRAGLIST | NETIF_F_HIGHDMA;
+	return rv;
+}
+
+static int p2p_desc_mem_init(void)
+{
+	int cpu, cnt;
+	int dsize, tsize;
+	void *buf;
+	/* MAX_SKB_FRAGS + 4.  Out of 4, 2 will be used for skb and freeback storage */
+	dsize = ((((MAX_SKB_FRAGS + P2P_EXTRA_DESCS) * sizeof(uint64_t)) + CACHELINE_SIZE - 1) & (~((CACHELINE_SIZE)-1)));
+	tsize = dsize * MAX_TSO_SKB_PEND_REQS;
+
+	printk("%s in, dsize %d tsize %d \n", __FUNCTION__, dsize, tsize);
+
+	for(cpu = 0; cpu < NR_CPUS; cpu++) {
+		buf = cacheline_aligned_kmalloc(tsize, GFP_KERNEL);
+		if (!buf)
+			return -ENOMEM;
+		p2p_desc_mem[cpu].mem = buf;
+		for(cnt = 1; cnt < MAX_TSO_SKB_PEND_REQS; cnt++) {
+			*(unsigned long *)buf = (unsigned long)(buf + dsize);
+			buf += dsize;
+			*(unsigned long *)buf = 0;
+		}
+		p2p_desc_mem[cpu].dsize = dsize;
+	}
+	return 0;
+}
+
+static inline void *alloc_p2p_desc_mem(int cpu)
+{
+	void *buf;
+	buf = p2p_desc_mem[cpu].mem;
+	if(buf) {
+		p2p_desc_mem[cpu].mem = (void *)*(unsigned long *)(buf);
+	} else {
+		buf = cacheline_aligned_kmalloc(p2p_desc_mem[cpu].dsize, GFP_KERNEL);
+		p2p_dynamic_alloc_cnt++;
+	}
+	return buf;
+}
+
+static inline void free_p2p_desc_mem(int cpu, void *buf)
+{
+	*(unsigned long *)buf = (unsigned long)p2p_desc_mem[cpu].mem;
+	p2p_desc_mem[cpu].mem = buf;
+
+}
+
+static inline int create_p2p_desc(uint64_t paddr, uint64_t len, uint64_t *p2pmsg, int idx)
+{
+	int plen;
+	do {
+		plen = len >= MAX_PACKET_SZ_PER_MSG ? (MAX_PACKET_SZ_PER_MSG - 64): len;
+		p2pmsg[idx] = nae_tx_desc(P2D_NEOP, 0, NULL_VFBID, plen, paddr);
+		len -= plen;
+		paddr += plen;
+		idx++;
+
+	} while(len > 0);
+	return idx;
+}
+
+static inline void create_last_p2p_desc(uint64_t *p2pmsg, struct sk_buff *skb, int idx)
+{
+	p2pmsg[idx - 1] |= ((uint64_t)P2D_EOP << 62);
+	p2pmsg[P2P_SKB_OFF] = (uint64_t)skb;
+}
+
+uint16_t tcp_pseuodo_chksum(uint16_t *ipsrc)
+{
+	uint32_t sum = 0;
+	sum += ipsrc[0];
+	sum += ipsrc[1];
+	sum += ipsrc[2];
+	sum += ipsrc[3];
+	sum += 6;
+	while(sum >> 16)
+		sum = (sum & 0xffff)  + (sum >> 16);
+	//      sum = ~sum;
+	return (uint16_t)sum;
+}
+
+static __inline__ uint64_t nae_tso_desc0(
+		unsigned int type,
+		unsigned int subtype,
+		unsigned int opcode,
+		unsigned int l3hdroff,
+		unsigned int l4hdroff,
+		unsigned int l3chksumoff,
+		unsigned int pseudohdrchksum,
+		unsigned int l4chksumoff,
+		unsigned int pyldoff)
+{
+
+	return ((uint64_t)(type & 0x3) << 62) |
+		((uint64_t)(subtype & 3) << 60) |
+		((uint64_t)(opcode & 0xf) << 56) |
+		((uint64_t)(l3hdroff & 0x3f) << 43) |
+		((uint64_t)(l4hdroff & 0x7f) << 36) |
+		((uint64_t)(l3chksumoff & 0x1f) << 31) |
+		((uint64_t)(pseudohdrchksum & 0xffff) << 15) |
+		((uint64_t)(l4chksumoff & 0x7f) << 8) |
+		((uint64_t)(pyldoff & 0xff));
+}
+
+static __inline__ uint64_t nae_tso_desc1(
+		unsigned int type,
+		unsigned int subtype,
+		unsigned int poly,
+		unsigned int mss,
+		unsigned int crcstopoff,
+		unsigned int crcinsoff)
+{
+	return ((uint64_t)(type & 0x3) << 62) |
+		((uint64_t)(subtype & 3) << 60) |
+		((uint64_t)(poly & 0x3) << 48) |
+		((uint64_t)(mss & 0xffff) << 32) |
+		((uint64_t)(crcstopoff & 0xffff) << 16) |
+		((uint64_t)(crcinsoff & 0xffff));
+
+}
+
+
+
+
+/**********************************************************************
+ * mac_refill_frin_desc -  refill rx freein buffer for a device
+ * @dev -  this is per device based function
+ *
+ **********************************************************************/
+static int mac_refill_frin_desc(unsigned long dev)
+{
+	struct dev_data* priv;
+	struct net_device *ndev;
+        int ret, mflags, i, code,limit;
+        struct xlp_msg msg;
+	struct sk_buff * skb;
+
+	ndev = (struct net_device *) dev;
+	priv = netdev_priv(ndev);
+	ret = 0;
+
+	atomic64_inc(&priv->num_replenishes);
+
+	limit = atomic64_read(&priv->frin_to_be_sent);
+
+	for(i = 0; i < limit; i++)
+	{
+		skb = nlm_xlp_alloc_skb_atomic();
+		if(!skb)
+		{
+			printk("[%s] alloc skb failed\n",__FUNCTION__);
+
+			ret = -ENOMEM;
+			break;
+		}
+
+		skb->dev = ndev;
+
+		/* Send the free Rx desc to the MAC */
+		mac_put_skb_back_ptr(skb);
+		code = 0;
+
+		msgrng_access_enable(mflags);
+		msg.entry[0] = (unsigned long long)virt_to_bus(skb->data) & 0xffffffffffULL;
+		msg.entry[1]= msg.entry[2] = msg.entry[3] = 0;
+		/* Send the packet to nae rx  */
+		__sync();
+
+		if ( (ret = nlm_hal_send_msg1(priv->nae_rx_qid, code, msg.entry[0])) & 0x7)
+		{
+			print_fmn_send_error(__func__, ret);
+			printk("Unable to send configured free desc, check freein carving (qid=%d)\n", priv->nae_rx_qid);
+
+			/* free the buffer and return! */
+			dev_kfree_skb_any(skb);
+
+			msgrng_access_disable(mflags);
+
+			ret = -EBUSY;
+			break;
+                }
+		msgrng_access_disable(mflags);
+
+		atomic64_dec(&priv->frin_to_be_sent);
+
+		atomic64_inc(&priv->total_frin_sent);
+	}
+
+        return ret;
+}
+
+static int mac_refill_frin_skb(uint64_t paddr, int qid)
+{
+       int ret, code;
+
+	ret = 0;
+
+	/* Assumption: SKB is all set to go */
+
+	/* Send the free Rx desc to the MAC */
+	code = 0;
+
+	/* Send the packet to nae rx  */
+//	__sync();
+	for(;;) {
+	  ret = nlm_hal_send_msg1(qid, code, (paddr & 0xffffffffffULL) );
+	  if (!ret) break;
+	}
+
+	return ret;
+}
+
+static int mac_refill_frin_one_buffer(struct net_device *dev, int cpu)
+{
+	struct dev_data* priv;
+	struct net_device *ndev;
+	struct sk_buff * skb;
+
+	skb = nlm_xlp_alloc_skb_atomic();
+	if(!skb)
+	  {
+	    printk("[%s] alloc skb failed\n",__FUNCTION__);
+	    panic("panic...");
+	    return -ENOMEM;
+	  }
+
+	ndev = (struct net_device *)dev;
+	priv = netdev_priv(ndev);
+	skb->dev = ndev;
+	mac_put_skb_back_ptr(skb);
+	if(nlm_mode[LAST_RCVD_INDEX(cpu)] == NLM_RT_MODE)
+		return mac_refill_frin_skb((unsigned long long)virt_to_bus(skb->data), 1000 + (cpu/2));
+	else
+		return mac_refill_frin_skb((unsigned long long)virt_to_bus(skb->data), priv->nae_rx_qid);
+}
+
+static int nae_proc_read(char *page, char **start, off_t off,
+			     int count, int *eof, void *data)
+{
+	int len = 0;
+	int i = 0;
+	uint64_t total_err = 0, total_fast = 0, total_slow = 0;
+#ifdef ENABLE_DEBUG_STATISTICS
+	uint64_t clocks = 0, instrs = 0;
+#endif
+	for(i=0; i<32; i++){
+		printk("cpu%d, fast_repl %ld, slow_repl %ld, err_repl %ld\n",i, (unsigned long)fast_replenish_count[LAST_RCVD_INDEX(i)], (unsigned long)slow_replenish_count[LAST_RCVD_INDEX(i)], (unsigned long)err_replenish_count[LAST_RCVD_INDEX(i)]);
+		total_err += err_replenish_count[LAST_RCVD_INDEX(i)];
+		total_fast += fast_replenish_count[LAST_RCVD_INDEX(i)];
+		total_slow += slow_replenish_count[LAST_RCVD_INDEX(i)];
+#ifdef ENABLE_DEBUG_STATISTICS
+		clocks += skbuff_clocks[LAST_RCVD_INDEX(i)];
+		instrs += skbuff_event[LAST_RCVD_INDEX(i)];
+		printk("%d %lld %lld\n",i, (unsigned long long)skbuff_clocks[LAST_RCVD_INDEX(i)], skbuff_event[LAST_RCVD_INDEX(i)]);
+		skbuff_clocks[LAST_RCVD_INDEX(i)] = 0;
+		skbuff_event[LAST_RCVD_INDEX(i)] = 0 ;
+#endif
+	}
+	/*check how many hash are empty...*/
+	printk("TOTAL_FAST_REPL %ld, TOTAL_SLOW_REPL %ld, TOTAL_ERR_REPL %ld\n",(unsigned long)total_fast, (unsigned long)total_slow, (unsigned long)total_err);
+
+	*eof = 1;
+	return len;
+}
+/*
+ * NAE poll function on upper four buckets
+*/
+static void
+xlp_poll_upper(int cpu)
+{
+	unsigned int status;
+	uint64_t msg0 = 0, addr;
+	uint32_t src_id, size, code, context, port;
+	struct sk_buff* skb;
+	struct dev_data *priv;
+	uint64_t *p2pfbdesc;
+
+	while (1) {
+			status = xlp_message_receive_1(nae_fb_vc, &src_id, &size, &code, &msg0);
+			if(status) break;
+			__sync();
+
+			/* Process Transmit Complete, addr is the skb pointer */
+			addr = msg0 & 0xffffffffffULL;
+
+			if (drop_uboot_pkt) {
+				if ( (addr >= (192<<20)) && (addr < (256 << 20)) ){
+					printk("Dropping firmware TXC packet (addr=%llx)!\n", addr);
+					//stats_uboot_pkts++;
+					return;
+				}
+			}
+
+			/* context field is currently unused */
+			context = (msg0 >> 40) & 0x3fff;
+			port = cntx2port[context];
+			if(nlm_mode[LAST_RCVD_INDEX(cpu)] == NLM_TCP_MODE){
+				p2pfbdesc = bus_to_virt(addr);
+				skb = (struct sk_buff *)(p2pfbdesc[P2P_SKB_OFF]);
+				free_p2p_desc_mem(cpu, p2pfbdesc);
+			}
+			else{
+				skb = (struct sk_buff *)bus_to_virt(addr);
+			}
+			if(skb)
+			{
+				priv = netdev_priv(skb->dev);
+				if (debug) {
+					printk("[%s][TXC] addr=%llx, skb=%p, context=%d, port=%d\n",
+								__func__, addr, skb, context, port);
+				}
+				dev_kfree_skb_any(skb);
+				//priv->cpu_stats[cpu].txc_packets++;
+			}
+			else {
+				printk("[%s]: [txc] Null skb? paddr = %llx (halting cpu!)\n", __func__, addr);
+				cpu_halt();
+			}
+	} /* closing while (1) */
+}
+
+/*
+ * NAE poll function on lower four buckets
+*/
+static int
+xlp_poll_lower(int budget, int cpu)
+{
+	int err ;
+	int status;
+	uint64_t msg1;
+	int no_rx_pkt_rcvd = 0;
+	uint64_t addr;
+	uint32_t len, context;
+	int port;
+	struct net_device *pdev;
+	struct dev_data *priv = NULL;
+	uint64_t vaddr;
+	struct sk_buff* skb;
+	uint64_t msg0;
+	uint32_t src_id, size, code;
+#ifdef ENABLE_DEBUG_STATISTICS
+	uint64_t stamp1, stamp2;
+	uint64_t count1, count2;
+#endif
+	while (budget--) {
+
+		status = xlp_message_receive_2(nae_rx_vc, &src_id, &size, &code, &msg0, &msg1);
+		if (status)	continue;
+//		__sync();
+#ifdef ENABLE_PER_CPU_COUNTER
+		no_rx_pkt_rcvd++;
+#endif
+#ifdef ENABLE_SANITY_CHECKS
+		if(size != 2) {
+			printk("Unexpected single entry packet in poll_lower\n");
+			continue;
+		}
+#endif
+
+		err = (msg1 >> 4) & 0x1;
+
+		/* Rx packet */
+		addr	= msg1 & 0xffffffffc0ULL;
+		len	= (msg1 >> 40) & 0x3fff;
+		context = (msg1 >> 54) & 0x3ff;
+
+		if (err) {
+			if(nlm_mode[LAST_RCVD_INDEX(cpu)] == NLM_RT_MODE){
+				mac_refill_frin_skb(addr, 1000 + (cpu/2));
+			}
+			else{
+				int qid = 1000 + context;
+				mac_refill_frin_skb(addr, qid);
+			}
+			STATS_INC(priv->stats.rx_errors);
+			STATS_INC(priv->stats.rx_dropped);
+			err_replenish_count[LAST_RCVD_INDEX(cpu)]++;
+			continue;
+		}
+
+		if ( (addr >= (192<<20)) && (addr < (256 << 20))){
+			printk("Dropping firmware RX packet (addr=%llx)!\n", addr);
+			//stats_uboot_pkts++;
+			continue;
+		}
+
+#if 0
+		if(context == 0){
+			port = 0;
+		}else if(context == 4){
+			port = 1;
+		}else{
+			port = cntx2port[context];
+		}
+#endif
+		if(context < 4)
+			port = context;
+		else
+			port = cntx2port[context];
+//		port = cntx2port[context];
+#ifdef ENABLE_SANITY_CHECKS
+		if(port >= MAX_GMAC_PORT)
+		{
+			printk("[%s]: bad port=%d, context=%d\n", __func__, port, context);
+			/*TODO: Where to replenish this packet ???? Context is out of range!*/
+			continue;
+		}
+#endif
+		pdev = per_cpu_netdev[cpu][port];
+#ifdef ENABLE_SANITY_CHECKS
+		if(!pdev) {
+			printk("[%s]: [rx] wrong port=%d(context=%d)? pdev = NULL!\n", __func__, port, context);
+			continue;
+		}
+#endif
+		priv = netdev_priv(pdev);
+
+		vaddr = (uint64_t)bus_to_virt(addr);
+
+		len = len  - MAC_CRC_LEN;
+
+		skb = mac_get_skb_back_ptr(vaddr);
+#ifdef ENABLE_SANITY_CHECKS
+		if (!skb) {
+			STATS_INC(priv->stats.rx_dropped);
+			printk("[%s] Null skb? addr=%llx, vaddr=%llx, dropping it and losing one buffer!\n",
+						__func__, addr, vaddr);
+			STATS_INC(priv->stats.rx_dropped);
+			err_replenish_count[LAST_RCVD_INDEX(cpu)]++;
+			continue;
+		}
+#endif
+#if 0
+		prefetch_local(skb);
+		prefetch_local((void *)((unsigned long)skb + 1 * SMP_CACHE_BYTES));
+		prefetch_local((void *)((unsigned long)skb + 2 * SMP_CACHE_BYTES));
+		prefetch_local((void *)((unsigned long)skb + 3 * SMP_CACHE_BYTES));
+		prefetch_local(skb->data);
+#endif
+
+//		skb->dev = dev_mac[port];
+
+		skb_put(skb, len);
+
+		skb->protocol = eth_type_trans(skb, pdev);
+		//skb->dev->last_rx = jiffies;
+//		skb->ip_summed = CHECKSUM_UNNECESSARY;
+
+		/* Pass the packet to Network stack */
+		last_rcvd_skb[LAST_RCVD_INDEX(cpu)] = skb;
+		last_rcvd_skb_phys[LAST_RCVD_INDEX(cpu)] = addr;
+
+//		start_counter = read_dmfur_cycles();
+		{
+#ifdef DRV_LOOPBACK
+		  if(port > 3)
+		    netif_receive_skb(skb);
+		  else
+		    nlm_xlp_nae_start_xmit(skb, skb->dev);
+#else
+#ifdef ENABLE_DEBUG_STATISTICS
+		  stamp1 = read_dmfur_cycles();
+		  count1 = read_c0_perfcntr0();
+#endif
+		  netif_receive_skb(skb);
+#ifdef ENABLE_DEBUG_STATISTICS
+		  stamp2 = read_dmfur_cycles();
+		  count2 = read_c0_perfcntr0();
+
+		  if(skbuff_clocks[LAST_RCVD_INDEX(cpu)]){
+			  skbuff_clocks[LAST_RCVD_INDEX(cpu)] = ((stamp2 - stamp1) + skbuff_clocks[LAST_RCVD_INDEX(cpu)])/2;
+			  skbuff_event[LAST_RCVD_INDEX(cpu)] = ((count2 - count1) + skbuff_event[LAST_RCVD_INDEX(cpu)])/2;
+		  }
+		  else{
+			  skbuff_clocks[LAST_RCVD_INDEX(cpu)] = stamp2 - stamp1;
+			  skbuff_event[LAST_RCVD_INDEX(cpu)] = count2 - count1;
+		  }
+#endif
+#endif
+		}
+//		stop_counter = read_dmfur_cycles();
+		//total_cycles[LAST_RCVD_INDEX(cpu)] += (stop_counter - start_counter);
+
+		/* Update Stats */
+		STATS_ADD(priv->stats.rx_bytes, len);
+		STATS_INC(priv->stats.rx_packets);
+		//priv->cpu_stats[cpu].rx_packets++;
+#if 0
+		if (atomic64_inc_return(&priv->frin_to_be_sent) > frin_desc_thres);
+		{
+			tasklet_schedule(&mac_refill_task[port]);
+			//mac_refill_frin_desc((unsigned long) skb->dev) ;
+		}
+
+		if((no_rx_pkt_rcvd + 1) == budget)
+				break;
+#else
+		if (last_rcvd_skb[LAST_RCVD_INDEX(cpu)]) {
+		  //printk("[%s@%d]: Unwanted buffer allocation in driver data path!\n", __FILE__, __LINE__);
+			slow_replenish_count[LAST_RCVD_INDEX(cpu)]++;
+			mac_refill_frin_one_buffer(dev_mac[port], cpu);
+			last_rcvd_skb[LAST_RCVD_INDEX(cpu)] = NULL;
+		}
+#endif
+	}
+#ifdef ENABLE_PER_CPU_COUNTER
+	total_rx_pkts[LAST_RCVD_INDEX(cpu)] += no_rx_pkt_rcvd;
+#endif
+
+	return no_rx_pkt_rcvd;
+}
+
+/*
+ * Main NAE poll loop
+*/
+static int xlp_nae_poll(void *buf)
+{
+	//unsigned int count=0;
+	int rx_pkts = 0;
+	int cpu = hard_smp_processor_id();
+	int budget = 300;
+
+	if(perf_mode == NLM_RT_MODE)
+		budget = 300000;
+#ifdef ENABLE_DEBUG_STATISTICS
+	/*start perf counter to count number of instructions..*/
+	write_c0_perfctrl0(24<<5|0xf);
+#endif
+	while (1) {
+#ifdef ENABLE_PER_CPU_COUNTER
+        uint64_t pkts = 0;
+	total_cycles[LAST_RCVD_INDEX(cpu)] = 0;
+	total_rx_pkts[LAST_RCVD_INDEX(cpu)] = 0;
+#endif
+
+	local_bh_disable();
+	xlp_poll_upper(cpu);
+	rx_pkts = xlp_poll_lower(budget, cpu);
+	local_bh_enable();
+
+#ifdef ENABLE_PER_CPU_COUNTER
+	pkts = total_rx_pkts[LAST_RCVD_INDEX(cpu)];
+#endif
+	/* if (pkts) { */
+	/*   printk("[cpu@%d]/cycles: total_cycles = %llu, pkts = %llu, cycles/pkt = %llu\n", */
+	/* 	 cpu, total_cycles, pkts, total_cycles[LAST_RCVD_INDEX(cpu)]/pkts); */
+	/* } */
+	//count++;
+	//if(count == 1000) {
+		schedule();
+	//	count=0;
+	//}
+	}
+	return 0;
+}
+
+void nlm_spawn_kthread(void)
+{
+    unsigned int i = 0, nr_cpus = 32;
+    char buf[20];
+    static struct task_struct *task[32];
+    /*Spawn kthread*/
+    for(i=0; i<nr_cpus; i++){
+	if(!cpu_isset(i, cpu_online_map))
+		continue;
+        sprintf(buf,"nae_task_%d",i);
+        task[i] = kthread_create(xlp_nae_poll, (void *)(long)i, (void *)&buf);
+        if(!task[i])
+            break;
+    }
+    if(i == nr_cpus){
+        for(i=0; i<nr_cpus; i++){
+	    if(!cpu_isset(i, cpu_online_map))
+		    continue;
+            kthread_bind(task[i], i);
+            wake_up_process(task[i]);
+        }
+    }
+
+}
+
+#if 0
+static void nlm_enable_parser(void)
+{
+	int l2proto = 1; //ethernet
+	int port = 0, i, ipchk = 1;
+	uint32_t val = 0;
+	uint32_t naereg = nlm_hal_read_nae_reg(RX_CONFIG);
+
+	nlm_hal_write_nae_reg(RX_CONFIG, (naereg | 1 << 12 | 1 << 13 | 3 << 22)); //enabling hardware parser
+	printk("Enabling parser reg.. %x\n", nlm_hal_read_nae_reg(RX_CONFIG));
+
+	/* enabling extraction of data */
+	for(i=0; i<16;i++)
+		nlm_hal_write_nae_reg(L2_TYPE_0 + i, l2proto);
+
+	nlm_hal_write_nae_reg(L3_CTABLE_MASK_0, port | 0 << 5 | 1 << 6); // l2proto and ethtype included
+
+	val = ((0 << 26) | (9 << 20) | (ipchk << 18) | (1 << 16) | (0x800));
+	nlm_hal_write_nae_reg(L3_CTABLE_0_0, val);
+	val =   (12 << 26) | (4 << 21) | (16 << 15) | (4 << 10); /* extract sourceip and dstip */
+	nlm_hal_write_nae_reg(L3_CTABLE_0_1, val);
+
+//	nlm_hal_write_nae_reg(L4_CTABLE_0_0, 1 << 17 | 0x6); /* ip_proto = tcp */
+//	val = ((0 << 21) | (2 << 17) | (2 << 11) | (2 << 7)); /* extract source and dst port*/
+//	nlm_hal_write_nae_reg(L4_CTABLE_0_1, val);
+}
+#endif
+
+static void nlm_update_ucore_shared_memory(void)
+{
+	uint32_t data[33] = {0};
+	int i = 0;
+	int j = 0;
+
+	while(i < 32){
+		for(j=0; j<NR_CPUS && i<32; j++){
+			if(!cpu_isset(j, cpu_online_map))
+				continue;
+			data[i] = j*4 + nae_cfg.rx_vc;
+			i++;
+		}
+	}
+	if(perf_mode == NLM_TCP_MODE)
+		data[32] = NLM_TCP_MODE;
+	else
+		data[32] = NLM_RT_MODE;
+	nlm_hal_write_ucore_shared_mem(data, sizeof(data)/sizeof(uint32_t));
+}
+
+
+static void nlm_replenish_per_cpu_buffer(void)
+{
+	int i;
+	int vc_index = 0;
+        int mflags, code;
+        struct xlp_msg msg;
+	struct sk_buff * skb;
+	int ret = 0;
+
+	/*Using 16 rx_free_in_fifo (1 per 2 cpu)
+	  Allocate 16 buffers per queue.*/
+	for(i = 0; i < 18*16; i++)
+	{
+		vc_index = (i/16) + 1000;
+		skb = nlm_xlp_alloc_skb_atomic();
+		if(!skb)
+		{
+			printk("[%s] alloc skb failed\n",__FUNCTION__);
+			break;
+		}
+		/* Send the free Rx desc to the MAC */
+		mac_put_skb_back_ptr(skb);
+		code = 0;
+
+		msgrng_access_enable(mflags);
+		msg.entry[0] = (unsigned long long)virt_to_bus(skb->data) & 0xffffffffffULL;
+		msg.entry[1]= msg.entry[2] = msg.entry[3] = 0;
+		/* Send the packet to nae rx  */
+		__sync();
+
+		if ( (ret = nlm_hal_send_msg1(vc_index, code, msg.entry[0])) & 0x7)
+		{
+			print_fmn_send_error(__func__, ret);
+			printk("Unable to send configured free desc, check freein carving (qid=%d)\n", vc_index);
+			/* free the buffer and return! */
+			dev_kfree_skb_any(skb);
+
+			msgrng_access_disable(mflags);
+			ret = -EBUSY;
+			break;
+		}
+		msgrng_access_disable(mflags);
+	}
+}
+//#endif
+
+
+
+/**********************************************************************
+ * nlm_xlp_nae_init -  xlp_nae device driver init function
+ * @dev  -  this is per device based function
+ *
+ **********************************************************************/
+
+static void nlm_xlp_nae_init(void)
+{
+	struct net_device *dev = NULL;
+	struct dev_data *priv;
+	int i;
+	struct proc_dir_entry *entry;
+	int cpu = 0;
+	int vc = 0;
+	unsigned char *mode_str[3] = {"INVALID","TCP_PERF","ROUTE_PERF"};
+
+	if(!(perf_mode == NLM_TCP_MODE || perf_mode == NLM_RT_MODE)){
+		printk("Invalid perf mode passed -- Using TCP_PERF mode\n");
+		perf_mode = NLM_TCP_MODE;
+	}
+
+
+	printk("======= Module Parameters =========\n");
+	printk("debug = %d, frin_desc_thres=%d naecfg_hack=%d drop_uboot_pkt=%d, perf_mode=%s\n",
+	       debug, frin_desc_thres, naecfg_hack, drop_uboot_pkt, mode_str[perf_mode]);
+
+	for(i=0; i<NR_CPUS; i++)
+		nlm_mode[LAST_RCVD_INDEX(i)] = perf_mode;
+
+	/*Disable interrupts for VC - 0-127*/
+	for(vc=0; vc<128; vc++)
+		nlm_hal_disable_vc_intr(vc);
+
+	nlm_update_ucore_shared_memory();
+
+	if(perf_mode == NLM_TCP_MODE)
+		p2p_desc_mem_init();
+
+	if (initialize_nae(cpumask_to_uint32(&cpu_present_map), 0, 0, 0))
+		return;
+
+	nae_fb_vc = nae_cfg.fb_vc;
+	nae_rx_vc = nae_cfg.rx_vc;
+
+	for(i = 0; i < nae_cfg.num_ports; i++)
+	{
+		/* Register only valid ports which are management */
+		if (!nae_cfg.ports[i].valid)
+			continue;
+
+		dev = alloc_etherdev_mq(sizeof(struct dev_data), 32);
+		if(!dev)
+			return;
+
+		ether_setup(dev);
+		dev->tx_queue_len = 0;	/* routing gives good performance with tx_queue_len = 0; */
+
+		priv = netdev_priv(dev);
+		spin_lock_init(&priv->lock);
+		priv->dev 	= dev;
+		dev->netdev_ops = &nlm_xlp_nae_ops;
+
+		/* set ethtool_ops which is inside xlp_ethtool.c file*/
+		xlp_set_ethtool_ops(dev);
+
+		/*netif_napi_add(dev, &priv->napi, nlm_xlp_napi_poll, 16);*/
+
+		dev->dev_addr = eth_hw_addr[i];
+		priv->port	= i;
+		priv->hw_port_id = nae_cfg.ports[i].hw_port_id;
+
+		atomic64_set(&priv->frin_to_be_sent, nae_cfg.ports[i].num_free_desc);
+		atomic64_set(&priv->num_replenishes, 0);
+		atomic64_set(&priv->total_frin_sent, 0);
+
+		priv->inited	= 0;
+		priv->block 	= nae_cfg.ports[i].hw_port_id / 4;
+		priv->type = nae_cfg.ports[i].iftype;
+		switch(nae_cfg.ports[i].iftype) {
+			case SGMII_IF:
+				priv->index = nae_cfg.ports[i].hw_port_id & 0x3;
+				priv->phy.addr = nae_cfg.ports[i].hw_port_id;
+				break;
+			case XAUI_IF:
+				priv->index = XGMAC;
+				break;
+			case INTERLAKEN_IF:
+				priv->index = INTERLAKEN;
+				break;
+			default:
+				priv->index=0;
+				break;
+		}
+		//nlm_print("port%d hw %d block %d index %d type %d \n",i, nae_cfg.ports[i].hw_port_id,
+		//							priv->block, priv->index, priv->type);
+		priv->nae_tx_qid 	= nae_cfg.ports[i].txq_range[0];
+		priv->nae_rx_qid 	= nae_cfg.ports[i].rxq;
+		dev->features |= NETIF_F_LLTX;
+
+		register_netdev(dev);
+
+		dev_mac[i] = dev;
+		xlp_mac_setup_hwaddr(priv);
+		for(cpu = 0; cpu<NR_CPUS; cpu++){
+			per_cpu_netdev[cpu][i] = dev;
+		}
+
+		tasklet_init(&mac_refill_task[priv->port],
+			     (void (*)(long unsigned int))mac_refill_frin_desc,
+			     (unsigned long)dev);
+	}
+
+	entry = create_proc_read_entry("mac_stats", 0 /* def mode */ ,
+				       nlm_root_proc /* parent */ ,
+				       xlp_mac_proc_read /* proc read function */ ,
+				       0	/* no client data */
+		);
+	if (!entry) {
+		printk("[%s]: Unable to create proc read entry for xlp_mac!\n",
+		       __FUNCTION__);
+	}
+	entry = create_proc_read_entry("nae_stat", 0, nlm_root_proc, nae_proc_read, 0);
+	if (!entry) {
+		printk("[%s]: Unable to create proc read entry for nae_proc!\n",
+		       __FUNCTION__);
+	}
+	/*spawn percpu kthread*/
+	nlm_spawn_kthread();
+//	nlm_enable_parser();
+	if(perf_mode == NLM_RT_MODE)
+		nlm_replenish_per_cpu_buffer();
+
+}
+
+/**********************************************************************
+ * nlm_xlp_nae_open -  called when bring up a device interface
+ * @dev  -  this is per device based function
+ *
+ **********************************************************************/
+static int  nlm_xlp_nae_open (struct net_device *dev)
+{
+	struct dev_data *priv = netdev_priv(dev);
+	int i;
+	int ret = 0;
+
+	if(perf_mode == NLM_TCP_MODE)
+		tso_enable(dev, 1);
+
+	if (priv->inited) {
+		spin_lock_irq(&priv->lock);
+		nlm_xlp_mac_set_enable(priv, 1);
+		netif_tx_wake_all_queues(dev);
+		spin_unlock_irq(&priv->lock);
+		return 0;
+	}
+
+#if 0
+	if(register_xlp_msgring_handler( XLP_MSG_HANDLE_NAE_0 , nlm_xlp_nae_msgring_handler, dev))
+	{
+		printk("Fatal error! Can't register msgring handler for TX_STN_GMAC0");
+		ret = -1;
+		goto out;
+	}
+#endif
+
+	if(perf_mode == NLM_TCP_MODE){
+		ret = mac_refill_frin_desc((unsigned long)dev);
+		if (ret) goto out;
+	}
+
+#ifdef ENABLE_NAE_PIC_INT
+	{
+		int port = priv->port;
+		irq  = irt_irq_table[PIC_IRT_NA_INDEX(port)][0];
+		if(request_irq( irq, nlm_xlp_nae_int_handler, IRQF_SHARED,dev->name, dev)){
+			ret = -EBUSY;
+			printk("can't get mac interrupt line (%d)\n",dev->irq);
+		}
+		dump_irt_entry(PIC_IRT_NA_INDEX(port));
+	}
+#endif
+
+	/* set timer to test rx routine */
+	init_timer(&priv->link_timer);
+	priv->link_timer.expires = jiffies + HZ ; /* First timer after 1 sec */
+	priv->link_timer.data    = (unsigned long) priv->port;
+	priv->link_timer.function = &nlm_xlp_mac_timer;
+	priv->phy_oldlinkstat = -1;
+
+	netif_tx_start_all_queues(dev);
+	if(!(perf_mode == NLM_TCP_MODE || perf_mode == NLM_RT_MODE))
+		add_timer(&priv->link_timer);
+
+	STATS_SET(priv->stats.tx_packets, 0);
+	STATS_SET(priv->stats.tx_errors, 0);
+	STATS_SET(priv->stats.tx_bytes, 0);
+	STATS_SET(priv->stats.tx_dropped, 0);
+	STATS_SET(priv->stats.rx_packets, 0);
+	STATS_SET(priv->stats.rx_errors, 0);
+	STATS_SET(priv->stats.rx_bytes, 0);
+	STATS_SET(priv->stats.rx_dropped, 0);
+	STATS_SET(priv->stats.multicast, 0);
+	STATS_SET(priv->stats.collisions, 0);
+
+	for(i = 0; i < NR_CPUS; i++)
+	{
+		priv->cpu_stats[i].tx_packets	= 0;
+		priv->cpu_stats[i].txc_packets	= 0;
+		priv->cpu_stats[i].rx_packets	= 0;
+		priv->cpu_stats[i].interrupts	= 0;
+
+	}
+	priv->inited = 1;
+	nlm_xlp_mac_set_enable(priv, 1);
+
+ out:
+	return ret;
+}
+
+/**********************************************************************
+ * nlm_xlp_nae_stop -  called when bring down the interface
+ * @dev  -  this is per device based function
+ *
+ **********************************************************************/
+static int  nlm_xlp_nae_stop (struct net_device *dev)
+{
+	struct dev_data *priv = netdev_priv(dev);
+
+	spin_lock_irq(&priv->lock);
+	nlm_xlp_mac_set_enable(priv, 0);
+	priv->inited = 0;
+	del_timer_sync(&priv->link_timer);
+
+	netif_tx_stop_all_queues(dev);
+
+//	napi_disable(&priv->napi);
+	spin_unlock_irq(&priv->lock);
+	return 0;
+}
+
+/*This macro resets first 164 (offsetof(struct sk_buff, tail))bytes of skb header.*/
+#define fast_reset_skbptrs(skb) \
+		*(uint64_t *)(unsigned long)((uint64_t *)skb + 0) = 0;\
+		*(uint64_t *)(unsigned long)((uint64_t *)skb + 1) = 0;\
+		*(uint64_t *)(unsigned long)((uint64_t *)skb + 2) = 0;\
+		*(uint64_t *)(unsigned long)((uint64_t *)skb + 3) = 0;\
+		*(uint64_t *)(unsigned long)((uint64_t *)skb + 4) = 0;\
+		*(uint64_t *)(unsigned long)((uint64_t *)skb + 5) = 0;\
+		*(uint64_t *)(unsigned long)((uint64_t *)skb + 6) = 0;\
+		*(uint64_t *)(unsigned long)((uint64_t *)skb + 7) = 0;\
+		*(uint64_t *)(unsigned long)((uint64_t *)skb + 8) = 0;\
+		*(uint64_t *)(unsigned long)((uint64_t *)skb + 9) = 0;\
+		*(uint64_t *)(unsigned long)((uint64_t *)skb + 10) = 0;\
+		*(uint64_t *)(unsigned long)((uint64_t *)skb + 11) = 0;\
+		*(uint64_t *)(unsigned long)((uint64_t *)skb + 12) = 0;\
+		*(uint64_t *)(unsigned long)((uint64_t *)skb + 13) = 0;\
+		*(uint64_t *)(unsigned long)((uint64_t *)skb + 14) = 0;\
+		*(uint64_t *)(unsigned long)((uint64_t *)skb + 15) = 0;\
+		*(uint64_t *)(unsigned long)((uint64_t *)skb + 16) = 0;\
+		*(uint64_t *)(unsigned long)((uint64_t *)skb + 17) = 0;\
+		*(uint64_t *)(unsigned long)((uint64_t *)skb + 18) = 0;\
+		*(uint64_t *)(unsigned long)((uint64_t *)skb + 19) = 0;\
+		*(uint32_t *)(unsigned long)((uint64_t *)skb + 20) = 0;\
+/*
+ * This helper macro resets SKB data pointers for reuse
+ * as free-in buffer
+*/
+#define skb_reset_ptrs(skb) \
+do { \
+	struct skb_shared_info *shinfo; \
+	\
+	shinfo = skb_shinfo(skb); \
+	\
+	\
+	/* Now reinitialize old skb, cut & paste from dev_alloc_skb */ \
+	/*memset(skb, 0, offsetof(struct sk_buff, tail));*/ \
+	fast_reset_skbptrs(skb);\
+	skb->data = skb->head;  \
+	skb_reset_tail_pointer(skb);\
+	\
+	atomic_set(&shinfo->dataref, 1); \
+	shinfo->nr_frags  = 0; \
+	shinfo->gso_size = 0; \
+	shinfo->gso_segs = 0; \
+	shinfo->gso_type = 0; \
+	shinfo->ip6_frag_id = 0; \
+	shinfo->frag_list = NULL; \
+} while (0)
+
+static inline int tso_xmit_skb(struct sk_buff *skb, struct net_device *dev)
+{
+	int mss  = 0, idx = 0, len, i ;
+	struct skb_shared_info *sp = skb_shinfo(skb);
+	struct iphdr *iph;
+	struct dev_data *priv = netdev_priv(dev);
+	uint64_t msg, mscmsg0, mscmsg1;
+	//unsigned int mflags;
+	uint64_t *p2pdesc;
+	int cpu = hard_smp_processor_id();
+	int  ret, retry_cnt = 0;
+
+	p2pdesc = alloc_p2p_desc_mem(cpu);
+	if(p2pdesc == NULL) {
+		dev_kfree_skb_any(skb);
+		goto out_unlock;
+	}
+	tso_dbg("%s in gso_size %d nrfrags %d len %d p2pdesc %llx skb %llx headlen %d\n", __FUNCTION__,
+			sp->gso_size, sp->nr_frags, skb->len, (uint64_t)p2pdesc, (uint64_t)skb, skb_headlen(skb));
+
+
+	if (((mss = sp->gso_size) != 0) || (skb->ip_summed == CHECKSUM_PARTIAL)) {
+		u32 iphdroff, tcphdroff, pyldoff, tcppcsum;
+
+		if (skb_header_cloned(skb) &&
+				pskb_expand_head(skb, 0, 0, GFP_ATOMIC)) {
+			dev_kfree_skb_any(skb);
+			free_p2p_desc_mem(cpu, p2pdesc);
+			goto out_unlock;
+		}
+
+		iph = ip_hdr(skb);
+		iphdroff = (char *)iph - (char *)skb->data;
+		tcphdroff = iphdroff + ip_hdrlen(skb);
+		pyldoff = iphdroff + ip_hdrlen(skb) + sizeof(struct tcphdr) + tcp_optlen(skb);
+
+		tso_dbg("iphdroff %d tcphdroff %d pyldoff %d\n", iphdroff, tcphdroff, pyldoff);
+		tcppcsum = tcp_pseuodo_chksum((uint16_t *)((char *)iph + 12));
+		tcp_hdr(skb)->check = 0;
+		if(mss) {
+			iph->check = 0;
+			iph->tot_len = 0;
+			mscmsg0 = nae_tso_desc0(MSC, 1, TSO_IP_TCP_CHKSUM,
+				iphdroff, tcphdroff, (iphdroff + 10),
+				tcppcsum, tcphdroff + 16, pyldoff);
+			mscmsg1 = nae_tso_desc1(MSC, 2, 0, mss, 0, 0);
+		} else {
+			mscmsg0 = nae_tso_desc0(MSC, 0, TCP_CHKSUM,
+				iphdroff, tcphdroff, (iphdroff + 10),
+				tcppcsum, tcphdroff + 16, pyldoff);
+		}
+
+	}
+
+	if((len = skb_headlen(skb)) != 0) {
+		idx = create_p2p_desc(virt_to_bus((char *)skb->data), len, p2pdesc, idx);
+	}
+
+	for (i = 0; i < sp->nr_frags; i++)  {
+		skb_frag_t *fp = &sp->frags[i];
+		tso_dbg("frags %d pageaddr %lx off %x size %d\n", i, (long)page_address(fp->page),
+				fp->page_offset, fp->size);
+		idx = create_p2p_desc(virt_to_bus(((char *)page_address(fp->page)) + fp->page_offset),
+				fp->size, p2pdesc, idx);
+	}
+
+	create_last_p2p_desc(p2pdesc, skb, idx);
+	msg = nae_tx_desc(P2P, 0, cpu, idx, virt_to_bus(p2pdesc));
+
+	tso_dbg("msg0 %llx p2pdesc0 %llx p2pdesc1 %llx p2pdesc2 %llx idx %d\n",
+			msg, p2pdesc[0], p2pdesc[1], p2pdesc[2], idx);
+
+	__sync();
+retry_send:
+	if(mss)
+		ret = nlm_hal_send_msg3(priv->nae_tx_qid, 0, mscmsg0, mscmsg1, msg);
+	else if(skb->ip_summed == CHECKSUM_PARTIAL)
+		ret = nlm_hal_send_msg2(priv->nae_tx_qid, 0, mscmsg0, msg);
+	else
+		ret = nlm_hal_send_msg1(priv->nae_tx_qid, 0, msg);
+	if(ret)	{
+		xlp_poll_upper(cpu);
+		retry_cnt++;
+		if(retry_cnt >= 128) {
+			dev_kfree_skb_any(skb);
+			free_p2p_desc_mem(cpu, p2pdesc);
+			goto out_unlock;
+		}
+		goto retry_send;
+	}
+
+	dev->trans_start = jiffies;
+	STATS_ADD(priv->stats.tx_bytes, skb->len);
+	STATS_ADD(priv->stats.tx_packets, idx);
+	priv->cpu_stats[cpu].tx_packets += idx;
+
+out_unlock:
+	return NETDEV_TX_OK;
+}
+
+
+
+/**********************************************************************
+ * nlm_xlp_nae_start_xmit -  transmit a packet from buffer
+ * @dev  -  this is per device based function
+ * @skb  -  data buffer to send
+ **********************************************************************/
+static int nlm_xlp_nae_start_xmit(struct sk_buff *skb, struct net_device *dev)
+{
+	struct dev_data *priv = netdev_priv(dev);
+	int cpu = hard_smp_processor_id(), ret = 0;
+	uint64_t msg0, msg1;
+	int retry_count = 128;
+	int hw_repl = 0, offset;
+
+#ifdef ENABLE_SANITY_CHECKS
+	if(!skb)
+	{
+		printk("[%s] skb is NULL\n",__FUNCTION__);
+		return -1;
+	}
+	if(skb->len == 0)
+	{
+		printk("[%s] skb empty packet\n",__FUNCTION__);
+		return -1;
+	}
+#endif
+	if(nlm_mode[LAST_RCVD_INDEX(cpu)] == NLM_TCP_MODE){
+		return tso_xmit_skb(skb, dev);
+	}
+
+	if((last_rcvd_skb[LAST_RCVD_INDEX(cpu)] == skb) && !skb_shared(skb)) {
+		last_rcvd_skb[LAST_RCVD_INDEX(cpu)] = NULL;
+		/* Do h/w replenishment */
+		msg0 = nae_tx_desc(P2D_NEOP, 0, (cpu/2) + 32,
+					   0, last_rcvd_skb_phys[LAST_RCVD_INDEX(cpu)]);
+		hw_repl = 1;
+#ifdef ENABLE_PER_CPU_COUNTER
+		fast_replenish_count[LAST_RCVD_INDEX(cpu)]++;
+#endif
+	}
+	else {
+		msg0 = nae_tx_desc(P2D_NEOP, 0, cpu, 0, virt_to_bus(skb));
+	}
+	msg1 = nae_tx_desc(P2D_EOP,
+			0,
+			NULL_VFBID,
+			skb->len,
+			virt_to_bus(skb->data));
+	if(hw_repl) {
+//      	uint64_t cycles = 0;
+
+//		cycles = read_dmfur_cycles();
+		/* reset the skb for next rx */
+#ifndef DRV_LOOPBACK
+		/* Leak no dsk entries! */
+		//dst_release((struct dst_entry *)skb->_skb_dst);
+		skb_dst_drop(skb);
+#endif
+		//total_cycles[LAST_RCVD_INDEX(hard_smp_processor_id())] += (read_dmfur_cycles() - cycles);
+
+		/* Reset all fields to 0, reset data pointers */
+		skb_reset_ptrs(skb);
+
+		offset = (((unsigned long)skb->data + CACHELINE_SIZE) & ~(CACHELINE_SIZE - 1));
+		skb_reserve(skb, (offset - (unsigned long)skb->data));
+
+		/*this buffer already has backptr...*/
+//		mac_put_skb_back_ptr(skb);
+		skb_reserve(skb, CACHELINE_SIZE);
+	}
+
+#ifdef ENABLE_SANITY_CHECKS
+	if (0) {
+//		printk("[%s]: tx_qid=%d, entry0=%llx, entry1=%llx\n", __func__,
+//		       priv->nae_tx_qid, msg.entry[0], msg.entry[1]);
+	}
+#endif
+
+retry_send:
+	ret = nlm_hal_send_msg2(priv->nae_tx_qid, 0, msg0, msg1);
+
+	if (ret)
+	{
+		retry_count--;
+		if(retry_count){
+			xlp_poll_upper(cpu);
+			goto retry_send;
+		}
+		//print_fmn_send_error(__func__, ret);
+		//printk("[%s] HACK ALERT! dropping packet(skb=%p)!\n", __func__, skb);
+		dev_kfree_skb_any(skb);
+		//goto retry_send;
+        }
+//	dev->trans_start = jiffies;
+
+	STATS_ADD(priv->stats.tx_bytes, skb->len);
+	STATS_INC(priv->stats.tx_packets);
+//	priv->cpu_stats[cpu].tx_packets++;
+
+	return NETDEV_TX_OK;
+}
+
+/**********************************************************************
+ * nlm_xlp_set_multicast_list
+ *
+ **********************************************************************/
+static void  nlm_xlp_set_multicast_list (struct net_device *dev)
+{
+	if (dev->flags & IFF_ALLMULTI) {
+		/*
+		 * Enable ALL multicasts.  Do this by inverting the
+		 * multicast enable bit.
+		 */
+		return;
+	}
+	return;
+}
+
+static void xlp_mac_setup_hwaddr(struct dev_data *priv)
+{
+        struct net_device *dev = priv->dev;
+
+        nlm_hal_write_mac_reg(priv->block, priv->index, MAC_ADDR0_LO, (dev->dev_addr[5] << 24) |
+				(dev->dev_addr[4] << 16) | (dev->dev_addr[3] << 8) | (dev->dev_addr[2]));
+
+	nlm_hal_write_mac_reg(priv->block, priv->index, MAC_ADDR0_HI, (dev->dev_addr[1] << 24) |
+				(dev->dev_addr[0] << 16));
+
+	nlm_hal_write_mac_reg(priv->block, priv->index, MAC_ADDR0_MASK_LO, 0xFFFFFFFF);
+	nlm_hal_write_mac_reg(priv->block, priv->index, MAC_ADDR0_MASK_HI, 0xFFFFFFFF);
+
+        nlm_hal_write_mac_reg(priv->block, priv->index, MAC_FILTER_CONFIG, (1 << MAC_FILTER_BCAST_EN_POS) |
+						 (1 << MAC_FILTER_MCAST_EN_POS) | (1 << MAC_FILTER_ADDR0_VALID_POS) );
+
+}
+
+
+/**********************************************************************
+ * nlm_xlp_nae_ioctl
+ *
+ **********************************************************************/
+static int  nlm_xlp_nae_ioctl (struct net_device *dev, struct ifreq *rq, int cmd)
+{
+	int rc = 0;
+	switch (cmd) {
+	default:
+		rc = -EOPNOTSUPP;
+		break;
+	}
+
+	return rc;
+}
+
+/**********************************************************************
+ * nlm_xlp_nae_change_mtu
+ *
+ **********************************************************************/
+static int nlm_xlp_nae_change_mtu(struct net_device *dev, int new_mtu)
+{
+	struct dev_data *priv = netdev_priv(dev);
+	unsigned long flags;
+
+	if ((new_mtu > ETH_MTU_SIZE) || (new_mtu < MIN_ETH_FRAME_SIZE)) {
+		return -EINVAL;
+	}
+
+	spin_lock_irqsave(&priv->lock, flags);
+
+	dev->mtu = new_mtu;
+
+	if (netif_running(dev)) {
+		/* Disable MAC TX/RX */
+		nlm_xlp_mac_set_enable(priv, 0);
+
+		/* Flush RX FR IN */
+		/* Flush TX IN */
+		nlm_xlp_mac_set_enable(priv, 1);
+	}
+
+	spin_unlock_irqrestore(&priv->lock, flags);
+	return 0;
+}
+
+/**********************************************************************
+ * nlm_xlp_mac_get_stats - wrap function for xlp_get_mac_stats
+ * @dev   -  this is per device based function
+ **********************************************************************/
+static struct net_device_stats *nlm_xlp_mac_get_stats(struct net_device *dev)
+{
+	struct dev_data *priv = netdev_priv(dev);
+	unsigned long flags;
+
+	spin_lock_irqsave(&priv->lock, flags);
+
+	xlp_get_mac_stats(dev, &priv->stats);
+
+	/* XXX update other stats here */
+	spin_unlock_irqrestore(&priv->lock, flags);
+
+	return &priv->stats;
+}
+
+/**********************************************************************
+ * nlm_xlp_nae_tx_timeout -  called when transmiter timeout
+ * @dev  -  this is per device based function
+ *
+ **********************************************************************/
+static void  nlm_xlp_nae_tx_timeout (struct net_device *dev)
+{
+	struct dev_data *priv = netdev_priv(dev);
+
+	spin_lock_irq(&priv->lock);
+
+	priv->stats.tx_errors++;
+
+	spin_unlock_irq(&priv->lock);
+
+	netif_tx_wake_all_queues(dev);
+
+	printk(KERN_WARNING "%s: Transmit timed out\n", dev->name);
+	return;
+}
+
+#ifdef ENABLE_NAE_PIC_INT
+/**********************************************************************
+ * nlm_xlp_nae_int_handler -  interrupt handler
+ * @irq     -  irq number
+ * @dev_id  -  this device
+ *
+ **********************************************************************/
+static irqreturn_t nlm_xlp_nae_int_handler(int irq, void *dev_id)
+{
+        struct net_device *dev;
+        struct dev_data *priv;
+	int i;
+	int cpu = 0;
+
+	cpu = hard_smp_processor_id();
+	priv->cpu_stats[cpu].interrupts++;
+
+	if(!dev_id)
+	{
+		printk("[%s]: NULL dev_id \n", __FUNCTION__ );
+		return IRQ_HANDLED;
+	}
+	dev = (struct net_device*)dev_id;
+	priv = netdev_priv(dev);
+
+	i = find_irt_from_irq(irq);
+
+
+	return IRQ_HANDLED;
+}
+#endif
+
+/**********************************************************************
+ * nlm_xlp_nae_msgring_handler -  message ring interrupt handler
+ * @vc-  virtual channel number
+ * @dev_id  -  this device
+ *
+ **********************************************************************/
+static void nlm_xlp_nae_msgring_handler(uint32_t vc, uint32_t src_id,
+					uint32_t size, uint32_t code,
+					uint64_t msg0, uint64_t msg1,
+					uint64_t msg2, uint64_t msg3, void* data)
+{
+        struct net_device *pdev;
+        struct dev_data *priv;
+	unsigned int len, port = 0, context;
+        uint64_t addr , vaddr;
+	struct sk_buff* skb;
+	int cpu = 0;
+
+	printk("[%s @ %d] MSGRING HANDLER CALLED!!!!!!!\n",__FUNCTION__,__LINE__);
+	cpu = hard_smp_processor_id();
+	vc = vc & 0x03;
+
+	if (debug) {
+		printk("[%s] src_id=%d vc = %d, size = %d, entry0=%llx entry1=%llx\n", __func__,
+		       src_id, vc, size, msg0, msg1);
+	}
+
+	if( vc == nae_fb_vc && size == 1)
+	{
+		/* Process Transmit Complete, addr is the skb pointer */
+		addr = msg0 & 0xffffffffffULL;
+
+		if (drop_uboot_pkt) {
+			if ( (addr >= (192<<20)) && (addr < (256 << 20)) ) {
+				printk("Dropping firmware TXC packet (addr=%llx)!\n", addr);
+				stats_uboot_pkts++;
+				return;
+			}
+		}
+
+		/* context field is currently unused */
+		context = (msg0 >> 40) & 0x3fff;
+		port = cntx2port[context];
+#ifdef DEBUG_CONTEXT_PORT_MAPPING
+                if (port == 0) printk("FB context %d port %d \n",context, port);
+#endif
+		skb = (struct sk_buff *)bus_to_virt(addr);
+		if(skb)
+		{
+			priv = netdev_priv(skb->dev);
+
+			if (debug) {
+				printk("[%s][TXC] addr=%llx, skb=%p, context=%d, port=%d\n",
+				       __func__, addr, skb, context, port);
+			}
+			dev_kfree_skb_any(skb);
+
+			priv->cpu_stats[cpu].txc_packets++;
+		}
+		else {
+			printk("[%s]: [txc] Null skb? paddr = %llx (halting cpu!)\n", __func__, addr);
+			cpu_halt();
+		}
+	}
+	else if(vc == nae_rx_vc && size == 2)
+	{
+		int bad_pkt = 0;
+		int err = (msg1 >> 4) & 0x1;
+		int ip_csum_valid = (msg1 >> 3) & 0x1;
+		int tcp_csum_valid = (msg1 >> 2) & 0x1;
+
+		/* Rx packet */
+		addr	= msg1 & 0xffffffffc0ULL;
+		len	= (msg1 >> 40) & 0x3fff;
+		context = (msg1 >> 54) & 0x3ff;
+
+#ifdef DEBUG_RXPKT_ADDR_NULL
+		if (addr == 0) {
+			printk("Rcvd pkt address NULL !!!\n");
+			printk("[%s] src_id=%d vc = %d, size = %d, entry0=%llx entry1=%llx\n", __func__,
+                       src_id, vc, size, msg0, msg1);
+			return;
+		}
+#endif
+		if (err) bad_pkt = 1;
+
+		if (drop_uboot_pkt) {
+			if ( (addr >= (192<<20)) && (addr < (256 << 20)) ) {
+				printk("Dropping firmware RX packet (addr=%llx)!\n", addr);
+				stats_uboot_pkts++;
+				return;
+			}
+		}
+
+		port = cntx2port[context];
+#ifdef DEBUG_CONTEXT_PORT_MAPPING
+		if (port == 0) printk("Rx context %d port %d \n",context, port);
+#endif
+		if(port >= MAX_GMAC_PORT)
+		{
+			printk("[%s]: bad port=%d, context=%d\n", __func__, port, context);
+			return;
+		}
+
+		pdev = (struct net_device*)dev_mac[port];
+		if(!pdev) {
+			printk("[%s]: [rx] wrong port=%d(context=%d)? pdev = NULL!\n", __func__, port, context);
+			return;
+		}
+		priv = netdev_priv(pdev);
+
+		vaddr = (uint64_t)bus_to_virt(addr);
+
+		if (debug) {
+			printk("[%s][RX] addr=%llx, len=%d, context=%d, port=%d, vaddr=%llx\n",
+			       __func__, addr, len, context, port, vaddr);
+		}
+
+		DUMP_PKT("RX Packet: ", (unsigned char *)vaddr, len);
+
+		len = len  - MAC_CRC_LEN;
+
+		skb = mac_get_skb_back_ptr(vaddr);
+		if (!skb) {
+			STATS_INC(priv->stats.rx_errors);
+			STATS_INC(priv->stats.rx_dropped);
+			printk("[%s] Null skb? addr=%llx, vaddr=%llx, drop it!\n",
+			       __func__, addr, vaddr);
+			cpu_halt();
+			return;
+		}
+
+		if (debug) {
+			struct iphdr *iph = (struct iphdr *)(vaddr + 14);
+			int net_pkt_len = iph->tot_len + 14;
+			int eth_proto = *(unsigned short *)(vaddr + 12);
+
+			if ((eth_proto == 0x800) && (net_pkt_len != len)) bad_pkt = 1;
+
+			if (bad_pkt) {
+				printk("[%s]: vaddr=%llx (len:%d/%d) (ip:proto=%d) (%d/%d/%d))\n",
+				       __func__, vaddr, net_pkt_len, len, iph->protocol,
+				       err, ip_csum_valid, tcp_csum_valid);
+			}
+		}
+
+		if (bad_pkt) {
+			STATS_INC(priv->stats.rx_errors);
+			STATS_INC(priv->stats.rx_dropped);
+
+			dev_kfree_skb_any(skb);
+			goto out;
+		}
+
+		skb_put(skb, len);
+		skb->dev = dev_mac[port];
+		skb->protocol = eth_type_trans(skb, dev_mac[port]);
+		skb->dev->last_rx = jiffies;
+
+		/* Pass the packet to Network stack */
+		netif_rx (skb);
+
+		/* Update Stats */
+		STATS_ADD(priv->stats.rx_bytes, len);
+		STATS_INC(priv->stats.rx_packets);
+		priv->cpu_stats[cpu].rx_packets++;
+
+	out:
+		if (atomic64_inc_return(&priv->frin_to_be_sent) > frin_desc_thres);
+		{
+			tasklet_schedule(&mac_refill_task[port]);
+			//mac_refill_frin_desc((unsigned long) skb->dev) ;
+		}
+	} else {
+		printk("[%s]: wrong vc=%d or size=%d?\n", __func__, vc, size);
+	}
+
+	return;
+}
+
+/**********************************************************************
+ * xlp_mac_proc_read -  proc file system read routine
+ * @page     -  buffer address
+ * @dev_id  -  this device
+ *
+ **********************************************************************/
+static int xlp_mac_proc_read(char *page, char **start, off_t off,
+			     int count, int *eof, void *data)
+{
+	int len = 0;
+	off_t begin = 0;
+	int i = 0, cpu = 0;
+	struct net_device *dev = 0;
+	struct dev_data *priv = 0;
+
+	len += sprintf(page + len, "uboot_pkts = %ld\n", stats_uboot_pkts);
+
+	for (i = 0; i < MAX_GMAC_PORT; i++) {
+
+		dev = dev_mac[i];
+
+		if(dev == 0) continue;
+
+		priv = netdev_priv(dev);
+
+		len += sprintf(page + len, "=============== port@%d ==================\n", i);
+
+		len += sprintf(page + len, "per port@%d: frin_to_be_sent = %ld num_replenishes = %ld frin_sent = %ld\n",
+			       i, atomic64_read(&priv->frin_to_be_sent),
+			       atomic64_read(&priv->num_replenishes),
+			       atomic64_read(&priv->total_frin_sent));
+
+		len += sprintf(page + len,
+			       "per port@%d: %lu(rxp) %lu(rxb) %lu(txp) %lu(txb)\n",
+			       i,
+			       STATS_READ(priv->stats.rx_packets),
+			       STATS_READ(priv->stats.rx_bytes),
+			       STATS_READ(priv->stats.tx_packets),
+			       STATS_READ(priv->stats.tx_bytes));
+
+		for (cpu = 0; cpu < NR_CPUS ; cpu++) {
+			unsigned long tx = priv->cpu_stats[cpu].tx_packets;
+			unsigned long txc = priv->cpu_stats[cpu].txc_packets;
+			unsigned long rx = priv->cpu_stats[cpu].rx_packets;
+			unsigned long ints = priv->cpu_stats[cpu].interrupts;
+
+			if (!tx && !txc && !rx && !ints) continue;
+
+			len += sprintf(page + len, "per cpu@%d: %lu(txp) %lu(txcp) %lu(rxp) %lu(int)\n",
+				       cpu, tx, txc, rx, ints);
+		}
+	}
+
+	*eof = 1;
+
+	*start = page + (off - begin);
+	len -= (off - begin);
+	if (len > count)
+		len = count;
+	if (len < 0)
+		len = 0;
+
+	return len;
+}
+
+/**********************************************************************
+ * nlm_xlp_mac_timer - interrupt handler routine
+ * @data - parameter passed in when timer interrupt handler is called.
+ **********************************************************************/
+static void nlm_xlp_mac_timer(unsigned long data)
+{
+	unsigned port = data;
+        struct net_device *dev = (struct net_device *)dev_mac[port];
+        struct dev_data *priv = netdev_priv(dev);
+        int next_tick = HZ / 1000; /* 1ms */
+
+	/* printk("[%s] A0 Workaround, forcing FMN int handling \n",__func__); */
+	if (priv->inited)
+	{
+		uint32_t cpumask = cpumask_to_uint32(&cpu_present_map); /* doesn't handle non-n0 nodes */
+		uint32_t cpumask_lo;
+		uint32_t cpumask_hi;
+
+		pic_reg_t *mmio = nlm_hal_pic_offset();
+		int cpu = hard_smp_processor_id();
+
+		cpumask = cpumask & ~(1 << cpu);
+		cpumask_hi = cpumask >> 16;;
+		cpumask_lo = cpumask & 0xffff;
+
+		/* Send IRQ_MSGRING vector in an IPI to all cpus but the current one */
+		if (cpumask_lo)
+			nlm_hal_write_pic_reg(mmio, PIC_IPI_CTL, (IRQ_MSGRING << 20) | cpumask_lo );
+
+		if (cpumask_hi)
+			nlm_hal_write_pic_reg(mmio, PIC_IPI_CTL, (IRQ_MSGRING << 20) | (1 << 16)
+					      | (cpumask_hi));
+
+		/* Run IPI handler on this cpu too */
+		nlm_xlp_msgring_int_handler(IRQ_MSGRING, NULL);
+	}
+
+        priv->link_timer.expires = jiffies + next_tick;
+        add_timer(&priv->link_timer);
+}
+
+static int __devinit nlm_xlp_nae_pci_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
+{
+        int result = 0;
+
+        result = pci_enable_device(pdev);
+	return result;
+}
+
+/**********************************************************************
+ * nlm_xlp_nae_remove - driver remove routine
+ * @pdev - pci device.
+ **********************************************************************/
+static void nlm_xlp_nae_remove(void)
+{
+	int i;
+	struct net_device *dev = 0;
+        struct dev_data *priv = 0;
+
+	for (i = 0; i < MAX_GMAC_PORT; i++)
+	{
+		dev = dev_mac[i];
+
+		if (dev == 0) continue;
+
+		priv = netdev_priv(dev);
+		unregister_netdev(dev);
+		free_netdev(dev);
+	}
+
+	remove_proc_entry("mac_stats", nlm_root_proc /* parent dir*/ );
+
+}
+
+static struct pci_driver soc_driver = {
+	.name             = XLP_SOC_MAC_DRIVER,
+	.id_table         = soc_pci_table,
+	.probe            = nlm_xlp_nae_pci_probe,
+	.remove		  = NULL,
+};
+
+static int __init nlm_xlp_mac_init(void)
+{
+#ifndef CONFIG_NLM_NET_OPTS
+	printk("Wrong config option. Recompile linux kernel using build_linux_kernel_no_preempt command\n");
+	return -1;
+#endif
+
+	nlm_xlp_nae_init();
+
+	return pci_register_driver(&soc_driver);
+}
+
+static void __exit nlm_xlp_mac_exit(void)
+{
+	/* unregister mac driver */
+
+
+	nlm_xlp_nae_remove();
+
+	pci_unregister_driver(&soc_driver);
+}
+
+module_init(nlm_xlp_mac_init);
+module_exit(nlm_xlp_mac_exit);
+
+MODULE_AUTHOR("Netlogic Microsystems");
+MODULE_DESCRIPTION("Netlogic XLP SoC Network driver ");
+MODULE_LICENSE("GPL");
+MODULE_VERSION("0.1");
diff --git a/drivers/misc/netlogic/nae-perf/xlp_nae.h b/drivers/misc/netlogic/nae-perf/xlp_nae.h
new file mode 100644
index 0000000..8e19ada
--- /dev/null
+++ b/drivers/misc/netlogic/nae-perf/xlp_nae.h
@@ -0,0 +1,93 @@
+#ifndef _XLP_NAE_H
+#define _XLP_NAE_H
+
+#define MAC_MAX_FRAME_SIZE      1600
+#define MAC_SKB_BACK_PTR_SIZE   SMP_CACHE_BYTES
+
+
+#define MAC_PREPAD		0
+#define BYTE_OFFSET             2
+#define NLM_RX_BUF_SIZE (MAC_MAX_FRAME_SIZE+BYTE_OFFSET+MAC_PREPAD+MAC_SKB_BACK_PTR_SIZE+SMP_CACHE_BYTES)
+#define MAC_CRC_LEN             4
+#define CACHELINE_SIZE          (1ULL << 6)
+#define CACHELINE_ALIGNED(addr) ( ((addr) + (CACHELINE_SIZE-1)) & ~(CACHELINE_SIZE-1) )
+#define PHYS_TO_VIRT(paddr) (uint64_t)((paddr) - (netlib_paddrb) + (netlib_vaddrb))
+#define VIRT_TO_PHYS(vaddr) (uint64_t)((vaddr) - (netlib_vaddrb) + (netlib_paddrb))
+extern  unsigned long long netlib_vaddrb;
+extern unsigned long long netlib_paddrb;
+#define PADDR_BASE 0x100000ULL
+#define PADDR_SIZE 0x200000
+#define INIT_VBASE( vbase, pbase) {netlib_vaddrb = vbase ; netlib_paddrb = pbase;}
+
+struct cpu_stat {
+        unsigned long tx_packets;
+        unsigned long txc_packets;
+        unsigned long rx_packets;
+        unsigned long interrupts;
+};
+
+
+typedef enum xlp_net_types { TYPE_XLP_GMAC = 0, TYPE_XLP_XGMAC, TYPE_XLP_XAUI, TYPE_XLP_INTERLAKEN, MAX_XLP_NET_TYPES }xlp_interface_t;
+
+typedef enum { xlp_mac_speed_10, xlp_mac_speed_100,
+               xlp_mac_speed_1000, xlp_mac_speed_rsvd
+} xlp_mac_speed_t;
+
+typedef enum { xlp_mac_duplex_auto, xlp_mac_duplex_half,
+               xlp_mac_duplex_full
+} xlp_mac_duplex_t;
+
+typedef enum { xlp_mac_fc_auto, xlp_mac_fc_disabled, xlp_mac_fc_frame,
+               xlp_mac_fc_collision, xlp_mac_fc_carrier
+} xlp_mac_fc_t;
+
+struct phy_info {
+        int addr;
+        int mode;
+        uint32_t *mii_addr;
+        uint32_t *pcs_addr;
+        uint32_t *serdes_addr;
+};
+
+struct dev_data
+{
+        struct net_device *dev;
+        struct net_device_stats stats;
+        struct cpu_stat cpu_stats[NR_CPUS];
+        struct timer_list link_timer;
+        struct napi_struct napi;
+        spinlock_t lock;
+        unsigned short port;
+	unsigned short inited;
+        unsigned short block;
+        unsigned short index;
+        unsigned short type;
+        struct sk_buff* skb;
+        int phy_oldlinkstat;
+        atomic64_t frin_to_be_sent;
+	atomic64_t num_replenishes;
+	atomic64_t total_frin_sent;
+        __u8 hwaddr[6];
+
+        xlp_mac_speed_t speed;  /* current speed */
+        xlp_mac_duplex_t duplex;        /* current duplex */
+        xlp_mac_fc_t flow_ctrl; /* current flow control setting */
+        int advertising;
+        struct phy_info phy;
+        int nae_rx_qid;
+        int nae_tx_qid;
+	int hw_port_id;
+};
+
+static inline void prefetch_local(const void *addr)
+{
+        __asm__ __volatile__(
+        "       .set    mips4           \n"
+        "       pref    %0, (%1)        \n"
+        "       .set    mips0           \n"
+        :
+        : "i" (Pref_StoreStreamed), "r" (addr));
+}
+
+
+#endif
-- 
1.7.1

