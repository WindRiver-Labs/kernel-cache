From 3febb056f73f74b371baf4169a6c2417c9ed7f34 Mon Sep 17 00:00:00 2001
From: reshmic <reshmic@netlogicmicro.com>
Date: Tue, 13 Dec 2011 13:03:19 +0530
Subject: TDES-CBC, DES-CBC,AES-CBC, H-MD5, H-SHA1, H-SHA256, AES-XCBC using the cryptolib functions

[Based on SDK 3.2]
Signed-off-by: Quanyang Wang <quanyang.wang@windriver.com>
Signed-off-by: Nam Ninh <nam.ninh@windriver.com>

diff --git a/drivers/crypto/sae/nlm_aead.c b/drivers/crypto/sae/nlm_aead.c
new file mode 100755
index 0000000..593d7e4
--- /dev/null
+++ b/drivers/crypto/sae/nlm_aead.c
@@ -0,0 +1,1285 @@
+/***********************************************************************
+Copyright 2003-2010 Netlogic Microsystems (“Netlogic”). All rights
+reserved.
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are
+met:
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+*****************************#NETL_2#********************************/
+#include <linux/rtnetlink.h>
+#include <crypto/algapi.h>
+#include <crypto/aes.h>
+#include <crypto/des.h>
+#include <crypto/sha.h>
+#include <crypto/aead.h>
+#include <crypto/authenc.h>
+#include <crypto/scatterwalk.h>
+
+#include <asm/netlogic/hal/nlm_hal_fmn.h>
+#include "nlmcrypto.h"
+#include <asm/netlogic/msgring.h>
+#include "nlm_async.h"
+
+#undef NLM_CRYPTO_DEBUG
+#define Message(a, b...) //printk("[%s @ %d] "a"\n",__FUNCTION__,__LINE__, ##b)
+
+#define AES_CTR_IV_SIZE         8
+#define XLP_CRYPT_PRIORITY      310
+
+#define XCBC_DIGEST_SIZE        16
+#define MD5_DIGEST_SIZE         16
+#define MD5_BLOCK_SIZE          64
+#define CTR_RFC3686_IV_SIZE 8
+
+
+/*
+ 						CTRL DESC MEMORY LAYOUT
+	 ------------------------------------------------------------------------------------
+	|  64 bytes	 | struct nlm_aead_ctx	  | 64bytes for   | struct nlm_aead_ctx     |
+	|  for alignment | 			  | for alignment | (used only for 3des)    |
+	 ------------------------------------------------------------------------------------
+*/
+
+struct nlm_aead_ctx
+{
+	struct nlm_crypto_pkt_ctrl ctrl;
+	uint8_t iv_buf[16];
+	uint32_t iv_len;
+	int cbc;
+	uint16_t stat;
+};
+
+#define MAX_FRAGS		18
+#define CTRL_DESC_SIZE		(sizeof(struct nlm_aead_ctx) + 128)
+#define DES3_CTRL_DESC_SIZE	(2*CTRL_DESC_SIZE + 2*64)	//Allocate 2 separate control desc for encryption and decryption
+
+/*
+ 						PACKET DESC MEMORY LAYOUT
+	 ------------------------------------------------------------------------------------------------------
+	|  64 bytes	 | struct nlm_crypto_pkt_param +  | 64bytes for   | struct nlm_async_crypto | 64 bytes |
+	|  for alignment | 18 * (2*64)			  | for alignment |			    | for hash |
+	 ------------------------------------------------------------------------------------------------------
+ */
+
+#define PACKET_DESC_SIZE	(64 + sizeof(struct nlm_crypto_pkt_param) + MAX_FRAGS*(2*64) + 64 + sizeof(struct nlm_async_crypto) + 128 )
+#define NLM_CRYPTO_PKT_PARAM_OFFSET(addr)	(((unsigned long)addr + 64) & ~0x3fULL)
+#define NLM_ASYNC_PTR_PARAM_OFFSET(addr)	(((unsigned long)(addr + 64 + sizeof(struct nlm_crypto_pkt_param) + MAX_FRAGS*(2*64)) + 64) & ~0x3fULL)
+#define NLM_HASH_OFFSET(addr)			((unsigned long)addr + (PACKET_DESC_SIZE - 64))
+#define NLM_IV_OFFSET(addr)			((unsigned long)addr + (PACKET_DESC_SIZE - 128))
+
+
+#define XLP_CRYPT_PRIORITY	310
+
+#define NETL_OP_ENCRYPT 1
+#define NETL_OP_DECRYPT 0
+
+#define PKT_DESC_OFF 64
+
+#ifdef NLM_CRYPTO_DEBUG
+extern void print_crypto_msg_desc(uint64_t entry1, uint64_t entry2, uint64_t entry3);
+extern void print_cntl_instr(uint64_t cntl_desc);
+extern void print_pkt_desc(struct nlm_crypto_pkt_param  *pkt_desc, int index);
+#endif
+extern struct nlm_crypto_stat crypto_stat[MAX_CPU];
+
+
+/*
+   All extern declaration goes here.
+ */
+extern uint32_t nlm_hal_send_msg3(uint32_t dst, uint32_t code, uint64_t data0, uint64_t data1, uint64_t data2);
+extern uint32_t nlm_hal_recv_msg2(uint32_t dst, uint32_t *src, uint32_t *size, uint32_t *code, uint64_t *data0, uint64_t *data1);
+extern int auth_mode_key_len[NLM_HASH_MAX][NLM_HASH_MODE_MAX];
+extern int cipher_mode_iv_len[NLM_CIPHER_MAX][NLM_CIPHER_MODE_MAX];
+
+static void print_buf(unsigned char *msg, unsigned char *buf, int len)
+{
+#define TMP_BUF		50
+	char tmp_buf[TMP_BUF + 1];
+	int i, index = 0;
+
+	printk("**********%s************\n",msg);
+	for(i=0; i<len; i++){
+		sprintf(&tmp_buf[index*2], "%02x", buf[i]);
+		index++;
+		if(index == (TMP_BUF/2)){
+			tmp_buf[index*2] = '\0';
+			printk("[%s]\n",tmp_buf);
+			index = 0;
+		}
+	}
+	if(index){
+		tmp_buf[index*2] = '\0';
+		printk("[%s]\n",tmp_buf);
+	}
+}
+
+static struct nlm_aead_ctx *nlm_crypto_aead_ctx(struct crypto_aead *tfm)
+{
+	return (struct nlm_aead_ctx *)(((unsigned long)((uint8_t *)crypto_aead_ctx(tfm) + 63)) & ~(0x3f));
+}
+
+static struct nlm_aead_ctx *nlm_crypto_tfm_ctx(struct crypto_tfm *tfm)
+{
+	return (struct nlm_aead_ctx *)(((unsigned long)((uint8_t *)crypto_tfm_ctx(tfm) + 63)) & ~(0x3f));
+}
+
+
+
+static int aead_setauthsize(struct crypto_aead *tfm, unsigned int authsize)
+{
+	struct aead_tfm *crt = crypto_aead_crt(tfm);
+	crt->authsize = authsize;
+	return 0;
+}
+
+static void aead_session_cleanup(struct crypto_tfm *tfm)
+{
+}
+
+static int aead_cra_cbc_init(struct crypto_tfm *tfm)
+{
+	struct nlm_aead_ctx *nlm_ctx = (struct nlm_aead_ctx *)nlm_crypto_tfm_ctx(tfm);
+	nlm_ctx->cbc = 1;
+	tfm->crt_aead.reqsize = PACKET_DESC_SIZE; 
+	return 0;
+}
+
+static int aead_cra_init(struct crypto_tfm *tfm)
+{
+	printk("[%s %d]\n",__FUNCTION__, __LINE__);
+	tfm->crt_aead.reqsize = PACKET_DESC_SIZE;
+	printk("[%s %d]\n",__FUNCTION__, __LINE__);
+	return 0;
+}
+
+static int get_cipher_auth_keylen(const u8 *key, unsigned int keylen, int *cipher_keylen,
+			     int *auth_keylen)
+{
+	struct rtattr *rta = (struct rtattr *) key;
+	struct crypto_authenc_key_param *param;
+
+	if (!RTA_OK(rta, keylen)) {
+		goto badkey;
+	}
+
+	if (rta->rta_type != CRYPTO_AUTHENC_KEYA_PARAM) {
+		goto badkey;
+	}
+	if (RTA_PAYLOAD(rta) < sizeof (struct crypto_authenc_key_param)) {
+		goto badkey;
+	}
+
+	param = RTA_DATA(rta);
+	*cipher_keylen = be32_to_cpu(param->enckeylen);
+
+	key += RTA_ALIGN(rta->rta_len);
+	keylen -= RTA_ALIGN(rta->rta_len);
+
+	if (keylen < *cipher_keylen)
+		goto badkey;
+
+	*auth_keylen = keylen - *cipher_keylen;
+
+	return 0;
+badkey:
+	return -EINVAL;
+}
+
+static int get_cipher_aes_algid(unsigned int cipher_keylen)
+{
+
+	switch (cipher_keylen) {
+	case 16:
+		return NLM_CIPHER_AES128;
+		break;
+	case 24:
+		return NLM_CIPHER_AES192;
+		break;
+	case 32:
+		return NLM_CIPHER_AES256;
+		break;
+	default:
+		printk(KERN_WARNING "[%s]: Cannot handle keylen = %d\n",
+		       __FUNCTION__, cipher_keylen);
+		return -EINVAL;
+	}
+}
+
+/*
+   All Setkey goes here.
+ */
+
+static int xlp_aes_cbc_setkey( struct crypto_aead *tfm, uint8_t *key, unsigned int keylen,
+				int hash, int mode,uint16_t h_stat )
+{ 
+	struct nlm_aead_ctx *ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+	struct nlm_crypto_pkt_ctrl *ctrl = &ctx->ctrl;
+	unsigned int cipher_keylen=0, auth_keylen=0;
+	int ret;
+	int cipher_alg;
+	uint8_t auth_key[128];
+	uint8_t *cipher_key;
+	struct rtattr *rta = (struct rtattr *)key;
+	int hmac = ((mode == NLM_HASH_MODE_XCBC) ? 0: 1);
+	if ((ret = get_cipher_auth_keylen(key, keylen, &cipher_keylen,
+					  &auth_keylen)) < 0) {
+		crypto_aead_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+		printk("ERR: Bad key len\n");
+		return ret;
+	}
+
+	cipher_alg = get_cipher_aes_algid(cipher_keylen);
+	ctx->stat = cipher_alg - 1;
+	ctx->stat = ctx->stat | (h_stat << 8);
+	if (cipher_alg < 0) {
+		crypto_aead_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+		printk("ERR: Bad aes key len\n");
+		return -EINVAL;
+	}
+
+	printk("Initial authkeylen %d, cipherkeylen %d\n", cipher_keylen, auth_keylen);
+	key += RTA_ALIGN(rta->rta_len);
+	cipher_key = key + auth_keylen;
+	memcpy(auth_key, key, auth_keylen);
+	if(auth_mode_key_len[hash][mode] > auth_keylen)
+		memset(auth_key + auth_keylen, 0,  auth_mode_key_len[hash][mode] - auth_keylen);
+
+	if(cipher_mode_iv_len[cipher_alg][NLM_CIPHER_MODE_CBC] > 0)
+		ctx->iv_len = cipher_mode_iv_len[cipher_alg][NLM_CIPHER_MODE_CBC];
+
+	auth_keylen = auth_mode_key_len[hash][mode];
+
+	print_buf("ENC_KEY:", cipher_key, cipher_keylen);
+	print_buf("AUTH_KEY:", auth_key, auth_keylen);
+
+	ret =  nlm_crypto_fill_pkt_ctrl(ctrl, hmac, hash, 
+			mode, cipher_alg, NLM_CIPHER_MODE_CBC, 0, cipher_key, 
+			cipher_keylen, auth_key, auth_keylen);
+
+	return ret;
+}
+
+static int  xlp_3des_setkey(struct crypto_aead *tfm, u8 *key, unsigned int keylen, int hash, int mode,uint16_t h_stat )
+{
+	struct nlm_aead_ctx *ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+	struct nlm_aead_ctx * nlm_ctx = (struct  nlm_aead_ctx *)(( unsigned long )(( uint8_t *)ctx + CTRL_DESC_SIZE + 63) & ~(0x3f));
+        unsigned int cipher_keylen=0, auth_keylen=0;
+	struct nlm_crypto_pkt_ctrl *ctrl = &ctx->ctrl;
+        int ret;
+	uint8_t auth_key[128];
+	uint64_t d_key[3] ;
+	int cipher_alg = NLM_CIPHER_3DES;
+	struct rtattr *rta = (struct rtattr *)key;
+	uint8_t *cipher_key;
+	int hmac = ((mode == NLM_HASH_MODE_XCBC) ? 0: 1);
+
+        if ((ret = get_cipher_auth_keylen(key, keylen, &cipher_keylen,
+                                          &auth_keylen)) < 0) {
+                crypto_aead_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+                printk("ERR: Bad key len\n");
+                return ret;
+        }
+	ctx->stat = TDES_CBC_STAT | h_stat << 8;;
+	key += RTA_ALIGN(rta->rta_len);
+	memcpy(auth_key, key, auth_keylen);
+	cipher_key = key + auth_keylen;
+	if ( auth_mode_key_len[hash][mode] > auth_keylen)
+		memset(auth_key + auth_keylen, 0,  auth_mode_key_len[hash][mode] - auth_keylen);
+	
+	if(cipher_mode_iv_len[cipher_alg][NLM_CIPHER_MODE_CBC] > 0) {
+		ctx->iv_len = cipher_mode_iv_len[cipher_alg][NLM_CIPHER_MODE_CBC];
+		nlm_ctx->iv_len = cipher_mode_iv_len[cipher_alg][NLM_CIPHER_MODE_CBC];
+	}
+	auth_keylen = auth_mode_key_len[hash][mode];
+
+	ret =  nlm_crypto_fill_pkt_ctrl(ctrl, hmac, hash,
+		mode, cipher_alg,NLM_CIPHER_MODE_CBC,0,cipher_key,
+		 cipher_keylen, auth_key, auth_keylen);
+	
+	memcpy(d_key,&cipher_key[16],8);
+        memcpy(&d_key[1],&cipher_key[8],8);
+        memcpy(&d_key[2],&cipher_key[0],8);
+	ret =  nlm_crypto_fill_pkt_ctrl(&nlm_ctx->ctrl, hmac, hash,
+		mode, cipher_alg, NLM_CIPHER_MODE_CBC, 0, ( unsigned char * )d_key,
+		cipher_keylen, auth_key, auth_keylen);
+	print_buf("ENC_KEY:", cipher_key, cipher_keylen);
+	print_buf("AUTH_KEY:", auth_key, auth_keylen);
+	print_buf("DECRY_KEY",(unsigned char * )&d_key[0],cipher_keylen);
+
+        return ret;
+}
+
+static int xlp_des_setkey( struct crypto_aead *tfm, uint8_t  *key, unsigned int keylen, int hash, int mode, uint16_t h_stat)
+{
+	struct nlm_aead_ctx *ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+        unsigned int cipher_keylen=0, auth_keylen=0;
+	struct nlm_crypto_pkt_ctrl *ctrl = &ctx->ctrl;
+        int ret;
+	uint8_t auth_key[128];
+	uint64_t d_key[3] ;
+	int cipher_alg = NLM_CIPHER_DES;
+	struct rtattr *rta = (struct rtattr *)key;
+	uint8_t *cipher_key;
+	int hmac = ((mode == NLM_HASH_MODE_XCBC) ? 0: 1);
+	ctx->stat = DES_CBC_STAT | h_stat << 8;
+	
+
+        if ((ret = get_cipher_auth_keylen(key, keylen, &cipher_keylen,
+                                          &auth_keylen)) < 0) {
+                crypto_aead_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+                printk("ERR: Bad key len\n");
+                return ret;
+        }
+	key += RTA_ALIGN(rta->rta_len);
+	memcpy(auth_key, key, auth_keylen);
+	cipher_key = key + auth_keylen;
+	if ( auth_mode_key_len[hash][mode] > auth_keylen)
+		memset(auth_key + auth_keylen, 0,  auth_mode_key_len[hash][mode] - auth_keylen);
+	
+	if(cipher_mode_iv_len[cipher_alg][NLM_CIPHER_MODE_CBC] > 0) {
+		ctx->iv_len = cipher_mode_iv_len[cipher_alg][NLM_CIPHER_MODE_CBC];
+	}
+	auth_keylen = auth_mode_key_len[hash][mode];
+
+	ret =  nlm_crypto_fill_pkt_ctrl(ctrl, hmac, hash,
+		mode, cipher_alg,NLM_CIPHER_MODE_CBC,0,cipher_key,
+		 cipher_keylen, auth_key, auth_keylen);
+	
+	print_buf("ENC_KEY:", cipher_key, cipher_keylen);
+	print_buf("AUTH_KEY:", auth_key, auth_keylen);
+	print_buf("DECRY_KEY",(unsigned char *)&d_key[0],cipher_keylen);
+
+        return ret;
+
+
+}
+static int xlp_aes_cbc_hmac_sha256_setkey( struct crypto_aead *tfm, const u8 *key, unsigned int keylen)
+{
+	return xlp_aes_cbc_setkey(tfm, (uint8_t *)key, keylen,NLM_HASH_SHA,NLM_HASH_MODE_SHA256,H_SHA256_STAT);
+
+}
+
+static int xlp_aes_cbc_hmac_sha1_setkey(struct crypto_aead *tfm, const u8 *key, unsigned int keylen)
+{
+	return xlp_aes_cbc_setkey(tfm, (uint8_t *)key, keylen,NLM_HASH_SHA,NLM_HASH_MODE_SHA1,H_SHA1_STAT);
+}
+
+static int xlp_aes_cbc_aes_xcbc_mac_setkey(struct crypto_aead *tfm, const u8 *key, unsigned int keylen)
+{
+	return xlp_aes_cbc_setkey(tfm, (uint8_t *)key, keylen,NLM_HASH_AES128,NLM_HASH_MODE_XCBC,AES128_XCBC_STAT);
+}
+
+static int xlp_aes_cbc_hmac_md5_setkey(struct crypto_aead *tfm, const u8 *key, unsigned int keylen)
+{
+	return xlp_aes_cbc_setkey(tfm, (uint8_t *)key, keylen,NLM_HASH_MD5,0,MD5_STAT);
+}
+
+static int xlp_3des_cbc_hmac_md5_setkey(struct crypto_aead *tfm, const u8 *key,
+                                        unsigned int keylen)
+{
+	return xlp_3des_setkey(tfm,(uint8_t *)key,keylen,NLM_HASH_MD5,0,MD5_STAT);
+
+}
+static int xlp_3des_cbc_hmac_sha256_setkey(struct crypto_aead *tfm, const u8 *key,
+                                        unsigned int keylen)
+{
+	return xlp_3des_setkey(tfm,(uint8_t *)key,keylen,NLM_HASH_SHA,NLM_HASH_MODE_SHA256,H_SHA256_STAT);
+        
+}
+static int xlp_3des_cbc_hmac_sha1_setkey(struct crypto_aead *tfm, const u8 *key,
+                                        unsigned int keylen)
+{
+	return xlp_3des_setkey(tfm,(uint8_t *)key,keylen,NLM_HASH_SHA,NLM_HASH_MODE_SHA1,H_SHA1_STAT);
+}
+static int xlp_3des_cbc_aes_xcbc_mac_setkey(struct crypto_aead *tfm, const u8 *key,
+                                        unsigned int keylen)
+{
+	return xlp_3des_setkey(tfm,(uint8_t *)key,keylen,NLM_HASH_AES128,NLM_HASH_MODE_XCBC,AES128_XCBC_STAT);
+}
+static int xlp_des_cbc_aes_xcbc_mac_setkey( struct crypto_aead *tfm, const u8 *key,
+					unsigned int keylen)
+{
+	return xlp_des_setkey(tfm,(uint8_t *)key,keylen,NLM_HASH_AES128,NLM_HASH_MODE_XCBC,AES128_XCBC_STAT);
+}
+static int xlp_des_cbc_hmac_sha1_setkey(struct crypto_aead *tfm, const u8 *key,
+						 unsigned int keylen)
+{
+	return xlp_des_setkey(tfm,(uint8_t *)key,keylen, NLM_HASH_SHA,NLM_HASH_MODE_SHA1,H_SHA1_STAT);
+
+}
+static int xlp_des_cbc_hmac_sha256_setkey(struct crypto_aead *tfm, const u8 *key,
+							unsigned int keylen)
+{
+	return xlp_des_setkey(tfm,(uint8_t *)key,keylen, NLM_HASH_SHA,NLM_HASH_MODE_SHA256,H_SHA256_STAT);
+
+}
+static int xlp_des_cbc_hmac_md5_setkey( struct crypto_aead *tfm, const u8 *key,
+						unsigned int keylen)
+{
+	return xlp_des_setkey(tfm,(uint8_t *)key,keylen,NLM_HASH_MD5,0,MD5_STAT);
+
+}
+
+//returns nr_aad_frags... -1 for error
+int fill_aead_aad(struct nlm_crypto_pkt_param *param, struct aead_request *req, unsigned int aad_len,int seg)
+{
+	struct scatterlist *sg;
+	struct scatter_walk walk;
+	int len;
+	uint8_t *virt;
+	
+	for (sg = req->assoc; aad_len > 0; sg = scatterwalk_sg_next(sg), 
+			seg++) {
+
+		len = min(aad_len, sg->length);
+		scatterwalk_start(&walk, sg);
+		//virt = scatterwalk_map(&walk, 1);
+		virt = page_address(scatterwalk_page(&walk)) + offset_in_page(walk.offset);
+		nlm_crypto_fill_src_seg(param, seg, virt, len);
+		nlm_crypto_fill_dst_seg(param, seg, virt, len);
+		aad_len -= len;
+	}
+	return seg;
+}
+
+int fill_aead_crypt(struct aead_request *req, unsigned int cipher_len, 
+		struct nlm_crypto_pkt_param *param, unsigned char **actual_tag, int op, int seg)
+{
+	struct scatterlist *sg;
+	struct scatter_walk walk;
+	unsigned int len = 0;
+	uint8_t *virt = NULL;
+	int nr_src_frags = 0;
+	int nr_dst_frags = 0;
+	int passed_len, i;
+	int index = 0;
+
+	if (req->src == req->dst) {
+		for (sg = req->src; cipher_len > 0; sg = scatterwalk_sg_next(sg), index++) {
+			len = min(cipher_len, sg->length);
+			scatterwalk_start(&walk, sg);
+			virt = page_address(scatterwalk_page(&walk)) + offset_in_page(walk.offset);
+			nlm_crypto_fill_src_seg(param, seg + index, virt, len);
+			nlm_crypto_fill_dst_seg(param, seg + index, virt, len);
+			cipher_len -= len;
+		}
+		*actual_tag = virt + len;
+		return index;
+	}
+	passed_len = cipher_len;
+
+	for (sg = req->src, index = 0; cipher_len > 0; sg = scatterwalk_sg_next(sg),
+	     nr_src_frags++, index++) {
+		len = min(cipher_len, sg->length);
+		scatterwalk_start(&walk, sg);
+		virt = page_address(scatterwalk_page(&walk)) + offset_in_page(walk.offset);
+		nlm_crypto_fill_src_seg(param, seg + index, virt, len);
+		cipher_len -= len;
+	}
+
+	if (op == NETL_OP_ENCRYPT)
+		*actual_tag = virt + len;
+
+	cipher_len = passed_len;
+
+	for (sg = req->dst, index=0; cipher_len > 0; sg = scatterwalk_sg_next(sg),
+	     nr_dst_frags++, index++) {
+		len = min(cipher_len, sg->length);
+		scatterwalk_start(&walk, sg);
+		virt = page_address(scatterwalk_page(&walk)) + offset_in_page(walk.offset);
+		nlm_crypto_fill_dst_seg(param, seg + index, virt, len);
+		cipher_len -= len;
+	}
+
+	if (op == NETL_OP_DECRYPT)
+		*actual_tag = virt + len;
+
+	if (nr_src_frags > nr_dst_frags) {
+		for (i = 0; i < nr_src_frags - nr_dst_frags; i++)
+			param->segment[seg + nr_dst_frags + i][1] = 0ULL;
+		return nr_src_frags;
+	}else{
+		if (nr_src_frags < nr_dst_frags) {
+			for (i = 0; i < nr_dst_frags - nr_src_frags; i++)
+				param->segment[seg + nr_src_frags + i][0] = 0ULL;
+		}
+		return nr_dst_frags;
+	}
+}
+
+/*
+   Generic Encrypt / Decrypt Function
+ */
+//op is either encrypt or decrypt
+
+#if 1
+static inline void ncd_block_fast_send(unsigned int dest)
+{
+	int success;
+        __asm__ volatile (".set push\n"
+                          ".set noreorder\n"
+                          ".set arch=xlp\n"
+                          "sync\n"
+                          "1: msgsnds %0, %1\n"
+			  "beqz %0, 1b\n"
+			  "nop\n"
+                          ".set pop\n"
+			  : "=&r"(success)
+			  : "r" (dest));
+
+        return ;
+}
+
+static inline int ncd_fast_recv_msg2(uint32_t vc, uint64_t *msg0, uint64_t *msg1)
+{
+	if (!xlp_receive(vc))
+		return -1;
+
+	*msg0 = xlp_load_rx_msg0();
+	*msg1 = xlp_load_rx_msg1();
+	return 0;
+}
+
+static inline void ncd_fast_send_msg3(uint32_t dst, uint64_t data0, uint64_t data1, uint64_t data2)
+{
+  unsigned int dest = 0;
+
+
+  xlp_load_tx_msg0(data0);
+  xlp_load_tx_msg1(data1);
+  xlp_load_tx_msg2(data2);
+
+  dest = ((2 << 16) | dst);
+
+#ifdef MSGRING_DUMP_MESSAGES
+  nlm_hal_dbg_msg("Sending msg<%llx, %llx, %llx> to dest = %x\n", 
+	  data0, data1, data2, dest);
+#endif
+	
+  ncd_block_fast_send(dest);
+
+  return;
+}
+#endif
+
+static void aead_request_callback(struct nlm_async_crypto *async, uint64_t msg1)
+{
+	struct crypto_async_request *base = (struct crypto_async_request *)async->args;
+	int err = 0;
+	int cpu = nlm_processor_id();
+	int enc = async->stat & 0xff;
+	int auth = (async->stat >> 8 ) & 0xff;
+
+	if (msg1 & 0x7ff80) {
+		printk("\n Error: entry1 is %llx",msg1);
+		err = -EIO;
+		base->complete(base, err);
+	}
+	if (async->op){
+		memcpy(async->actual_tag, async->hash_addr, async->authsize);
+	}else{
+		if(memcmp(async->actual_tag, async->hash_addr, async->authsize)){
+			err = -EBADMSG;
+			printk("TAG MISMATCH!!!\n");
+		}
+	}
+	crypto_stat[cpu].enc[enc]++;
+	crypto_stat[cpu].auth[auth]++;	
+	crypto_stat[cpu].enc_tbytes[enc] += async->bytes;
+	crypto_stat[cpu].auth_tbytes[auth] += async->bytes;
+	base->complete(base, err);
+	return;
+}
+
+
+static int aead_crypt_3des(struct aead_request *req, unsigned int op)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_alg *alg = tfm->base.__crt_alg;
+	struct aead_alg *aead= &alg->cra_aead;
+	struct nlm_aead_ctx *ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+	struct aead_tfm *crt = crypto_aead_crt(tfm);
+	struct nlm_crypto_pkt_param *param;
+	unsigned int cipher_off, iv_off;
+	unsigned int auth_len, cipher_len, auth_off;
+	unsigned char *actual_tag;
+	int seg =0, nr_enc_frags, ivsize;
+	unsigned int hash_source, nr_aad_frags;
+	uint64_t entry0, entry1;
+	uint64_t tx_id=0x12345678;
+	struct nlm_async_crypto *async = NULL;
+	uint8_t *hash_addr;
+	int fb_vc; 
+	int err=0;
+	unsigned int authsize,maxauthsize;
+	uint8_t *new_iv_ptr_lo = NULL;
+	uint8_t *new_iv_ptr_hi = NULL;
+	struct nlm_crypto_pkt_ctrl *ctrl = NULL;
+	ctx = (struct  nlm_aead_ctx *)(( unsigned long )(( uint8_t *)ctx + CTRL_DESC_SIZE + 63) & ~(0x3f));
+	ctrl = &ctx->ctrl;
+	
+	authsize = crypto_aead_crt(crt->base)->authsize;
+	maxauthsize= aead->maxauthsize;
+
+	param = (struct nlm_crypto_pkt_param *)NLM_CRYPTO_PKT_PARAM_OFFSET(aead_request_ctx(req));
+	hash_addr = (uint8_t *)NLM_HASH_OFFSET(aead_request_ctx(req));
+
+	ivsize = crypto_aead_ivsize(crypto_aead_reqtfm(req));
+	auth_off = 0;
+	cipher_off = req->assoclen + ivsize;
+	iv_off = req->assoclen;
+	//check if it should be aip->tag_len or can be taken from tfm
+	cipher_len = op ? req->cryptlen:req->cryptlen - authsize;
+	auth_len = cipher_off + cipher_len;
+	hash_source = op;
+
+	nr_aad_frags = fill_aead_aad(param, req, req->assoclen,seg);
+	if (nr_aad_frags == 0)
+		return -1;
+	seg = nr_aad_frags;
+
+	if (ivsize) {
+		nlm_crypto_fill_src_seg(param, seg, req->iv, ivsize);
+		nlm_crypto_fill_dst_seg(param, seg, req->iv, ivsize);
+		seg++;
+	}
+
+	nr_enc_frags = fill_aead_crypt(req, cipher_len, param, &actual_tag, op, seg);
+
+	if (nr_enc_frags == -1)
+		return -1;
+
+	if(ctx->cbc){
+		uint32_t tmp_len;
+		uint8_t *tmp_virt;
+
+		tmp_len = (param->segment[seg][1] >> 48) + 1;
+		if(tmp_len >= 16){
+			tmp_virt = phys_to_virt((param->segment[seg][1]) & 0xffffffffffULL);
+			new_iv_ptr_hi = tmp_virt;
+			new_iv_ptr_lo = new_iv_ptr_hi + 8;;
+		}else if(tmp_len >=8){
+			tmp_virt = phys_to_virt((param->segment[seg][1]) & 0xffffffffffULL);
+			new_iv_ptr_lo = tmp_virt;
+			/*goto next frag*/
+			if(nr_enc_frags > 1){
+				tmp_virt = phys_to_virt((param->segment[seg + 1][1]) & 0xffffffffffULL);
+				tmp_len = (uint32_t)(param->segment[seg + 1][1] >> 48) + 1;
+				if(tmp_len >= 8)
+					new_iv_ptr_hi = tmp_virt + 8;
+			}
+		}
+	}
+
+	seg += nr_enc_frags;
+
+	nlm_crypto_fill_cipher_auth_pkt_param(ctrl, param, op, op, iv_off, 
+			ivsize, auth_off, auth_len, 0, cipher_off, cipher_len, hash_addr);
+	
+	fb_vc = crypto_get_fb_vc();
+
+	entry0 = nlm_crypto_form_pkt_fmn_entry0(fb_vc, 0, 0, ctrl->cipherkeylen, virt_to_phys(ctrl));
+	entry1 = nlm_crypto_form_pkt_fmn_entry1(0, ctrl->hashkeylen, (seg + 2)<<4, virt_to_phys(param));
+
+
+#ifdef NLM_CRYPTO_DEBUG
+	printk("*****ctrl descriptor phys addr %#lx\n",virt_to_phys(ctrl));
+	print_crypto_msg_desc(entry0, entry1, tx_id);
+	print_cntl_instr(ctrl->desc0);
+	print_buf("ENC_KEY:", (uint8_t *)ctrl->key, ctrl->cipherkeylen);
+	print_buf("AUTH_KEY:", (uint8_t *)ctrl->key + ctrl->cipherkeylen, ctrl->hashkeylen);
+	print_pkt_desc(param,seg);
+	printk("iv_off: %d, cipher_off: %d, auth_off: %d\n",iv_off, cipher_off, auth_off);
+	printk("auth_len: %d, cipher_len: %d\n", auth_len, cipher_len);
+#endif
+
+	async = (struct nlm_async_crypto *)NLM_ASYNC_PTR_PARAM_OFFSET(aead_request_ctx(req));
+	async->callback =  aead_request_callback;
+	async->args = (void *)&req->base; 
+	async->op  = op;
+	async->actual_tag = actual_tag;
+	async->hash_addr = hash_addr;
+	async->authsize = authsize;
+	async->stat = ctx->stat; 
+	async->bytes = req->cryptlen; 
+	tx_id = (uint64_t)async;
+
+	//construct pkt, send to engine and receive reply
+	err = nlm_hal_send_msg3(NLM_CRYPTO_VC_BASE, 0 /*code */ , entry0, entry1, tx_id);
+	if(err){
+		printk("err\n");
+		return -EIO;
+	}
+
+
+	return -EINPROGRESS;
+}
+static int aead_crypt(struct aead_request *req, unsigned int op)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_alg *alg = tfm->base.__crt_alg;
+	struct aead_alg *aead= &alg->cra_aead;
+	struct aead_tfm *crt = crypto_aead_crt(tfm);
+	struct nlm_aead_ctx *ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+	struct nlm_crypto_pkt_param *param;
+	unsigned int cipher_off, iv_off;
+	unsigned int auth_len, cipher_len, auth_off;
+	unsigned char *actual_tag;
+	int seg =0, nr_enc_frags, ivsize;
+	unsigned int hash_source, nr_aad_frags;
+	uint64_t entry0, entry1;
+	uint64_t tx_id=0x12345678;
+	struct nlm_async_crypto *async = NULL;
+	uint8_t *hash_addr;
+	int fb_vc; 
+	int err=0;
+	unsigned int authsize,maxauthsize;
+	uint8_t *new_iv_ptr_lo = NULL;
+	uint8_t *new_iv_ptr_hi = NULL;
+	struct nlm_crypto_pkt_ctrl *ctrl = &ctx->ctrl;
+	
+	authsize = crypto_aead_crt(crt->base)->authsize;
+	maxauthsize= aead->maxauthsize;
+
+	param = (struct nlm_crypto_pkt_param *)NLM_CRYPTO_PKT_PARAM_OFFSET(aead_request_ctx(req));
+	hash_addr = (uint8_t *)NLM_HASH_OFFSET(aead_request_ctx(req));
+
+	ivsize = crypto_aead_ivsize(crypto_aead_reqtfm(req));
+	auth_off = 0;
+	cipher_off = req->assoclen + ivsize;
+	iv_off = req->assoclen;
+	//check if it should be aip->tag_len or can be taken from tfm
+	cipher_len = op ? req->cryptlen:req->cryptlen - authsize;
+	auth_len = cipher_off + cipher_len;
+	hash_source = op;
+
+	nr_aad_frags = fill_aead_aad(param, req, req->assoclen,seg);
+	if (nr_aad_frags == 0)
+		return -1;
+	seg = nr_aad_frags;
+
+	if (ivsize) {
+		nlm_crypto_fill_src_seg(param, seg, req->iv, ivsize);
+		nlm_crypto_fill_dst_seg(param, seg, req->iv, ivsize);
+		seg++;
+	}
+
+	nr_enc_frags = fill_aead_crypt(req, cipher_len, param, &actual_tag, op, seg);
+
+	if (nr_enc_frags == -1)
+		return -1;
+
+	if(ctx->cbc){
+		uint32_t tmp_len;
+		uint8_t *tmp_virt;
+
+		tmp_len = (param->segment[seg][1] >> 48) + 1;
+		if(tmp_len >= 16){
+			tmp_virt = phys_to_virt((param->segment[seg][1]) & 0xffffffffffULL);
+			new_iv_ptr_hi = tmp_virt;
+			new_iv_ptr_lo = new_iv_ptr_hi + 8;;
+		}else if(tmp_len >=8){
+			tmp_virt = phys_to_virt((param->segment[seg][1]) & 0xffffffffffULL);
+			new_iv_ptr_lo = tmp_virt;
+			/*goto next frag*/
+			if(nr_enc_frags > 1){
+				tmp_virt = phys_to_virt((param->segment[seg + 1][1]) & 0xffffffffffULL);
+				tmp_len = (uint32_t)(param->segment[seg + 1][1] >> 48) + 1;
+				if(tmp_len >= 8)
+					new_iv_ptr_hi = tmp_virt + 8;
+			}
+		}
+	}
+
+	seg += nr_enc_frags;
+
+	nlm_crypto_fill_cipher_auth_pkt_param(ctrl, param, op, op, iv_off, 
+			ivsize, auth_off, auth_len, 0, cipher_off, cipher_len, hash_addr);
+	
+	fb_vc = crypto_get_fb_vc();
+
+	entry0 = nlm_crypto_form_pkt_fmn_entry0(fb_vc, 0, 0, ctrl->cipherkeylen, virt_to_phys(ctrl));
+	entry1 = nlm_crypto_form_pkt_fmn_entry1(0, ctrl->hashkeylen, (seg + 2)<<4, virt_to_phys(param));
+
+
+#ifdef NLM_CRYPTO_DEBUG
+	printk("*****ctrl descriptor phys addr %#lx\n",virt_to_phys(ctrl));
+	print_crypto_msg_desc(entry0, entry1, tx_id);
+	print_cntl_instr(ctrl->desc0);
+	print_buf("ENC_KEY:", (uint8_t *)ctrl->key, ctrl->cipherkeylen);
+	print_buf("AUTH_KEY:", (uint8_t *)ctrl->key + ctrl->cipherkeylen, ctrl->hashkeylen);
+	print_pkt_desc(param,seg);
+	printk("iv_off: %d, cipher_off: %d, auth_off: %d\n",iv_off, cipher_off, auth_off);
+	printk("auth_len: %d, cipher_len: %d\n", auth_len, cipher_len);
+#endif
+
+	async = (struct nlm_async_crypto *)NLM_ASYNC_PTR_PARAM_OFFSET(aead_request_ctx(req));
+	async->callback =  aead_request_callback;
+	async->args = (void *)&req->base; 
+	async->op  = op;
+	async->actual_tag = actual_tag;
+	async->hash_addr = hash_addr;
+	async->authsize = authsize;
+	async->stat = ctx->stat; 
+	async->bytes = req->cryptlen; 
+	tx_id = (uint64_t)async;
+
+	//construct pkt, send to engine and receive reply
+	err = nlm_hal_send_msg3(NLM_CRYPTO_VC_BASE, 0 /*code */ , entry0, entry1, tx_id);
+	if(err){
+		printk("err\n");
+		return -EIO;
+	}
+
+
+	return -EINPROGRESS;
+}
+
+/*
+ *  All Encrypt Functions goes here.
+ */
+
+static int 
+xlp_aes_cbc_encrypt(struct aead_request *req)
+{
+	return aead_crypt(req, NETL_OP_ENCRYPT);
+}
+
+static int
+xlp_3des_cbc_encrypt(struct aead_request *req)
+{
+	 return aead_crypt(req, NETL_OP_ENCRYPT);
+}
+static int 
+xlp_des_cbc_encrypt(struct aead_request *req)
+{
+	return aead_crypt(req, NETL_OP_ENCRYPT);
+}
+
+
+/*
+ *  All Decrypt Functions goes here.
+ */
+
+static int xlp_aes_cbc_decrypt(struct aead_request *req)
+{
+	return aead_crypt(req, NETL_OP_DECRYPT);
+}
+
+static int xlp_3des_cbc_decrypt( struct aead_request *req)
+{
+	return aead_crypt_3des(req, NETL_OP_DECRYPT);
+}
+static int xlp_des_cbc_decrypt(struct aead_request *req)
+{
+	return aead_crypt(req, NETL_OP_DECRYPT);
+}
+/*
+ *  All Givencrypt Functions goes here.
+ */
+
+int xlp_aes_cbc_givencrypt(struct aead_givcrypt_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(&req->areq);
+	struct nlm_aead_ctx *nlm_ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+
+	//TODO: Get the IV from random pool
+	memcpy(req->giv, nlm_ctx->iv_buf, nlm_ctx->iv_len);
+	*(uint64_t *)req->giv += req->seq;
+	memcpy(req->areq.iv, req->giv, nlm_ctx->iv_len);
+
+	return xlp_aes_cbc_encrypt(&req->areq);
+}
+
+static int xlp_3des_cbc_givencrypt(struct aead_givcrypt_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(&req->areq);
+	struct nlm_aead_ctx *nlm_ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+
+	memcpy(req->giv, nlm_ctx->iv_buf, nlm_ctx->iv_len);
+	*(uint64_t *)req->giv += req->seq;
+	memcpy(req->areq.iv, req->giv, nlm_ctx->iv_len);
+
+	return xlp_3des_cbc_encrypt(&req->areq);
+
+}
+static int xlp_des_cbc_givencrypt(struct aead_givcrypt_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(&req->areq);
+	struct nlm_aead_ctx *nlm_ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+	
+	memcpy(req->giv, nlm_ctx->iv_buf, nlm_ctx->iv_len);
+	*(uint64_t *)req->giv += req->seq;
+	memcpy(req->areq.iv, req->giv, nlm_ctx->iv_len);
+
+	return xlp_des_cbc_encrypt(&req->areq);
+}
+
+
+/* commented out to avoid the search time */
+static struct crypto_alg xlp_aes_cbc_hmac_sha256_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(hmac(sha256),cbc(aes))",
+	.cra_driver_name = "authenc-hmac-sha256-cbc-aes-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = AES_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC ,
+	.cra_ctxsize = CTRL_DESC_SIZE,
+	.cra_type = &crypto_aead_type,
+	.cra_init = aead_cra_cbc_init,
+	.cra_exit = aead_session_cleanup,
+	.cra_aead = {
+		     .setkey = xlp_aes_cbc_hmac_sha256_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_aes_cbc_encrypt,
+		     .decrypt = xlp_aes_cbc_decrypt,
+		     .givencrypt = xlp_aes_cbc_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = AES_BLOCK_SIZE,
+		     .maxauthsize = SHA256_DIGEST_SIZE,
+		     }
+};
+/* commented out to avoid the search time */
+static struct crypto_alg xlp_aes_cbc_hmac_sha1_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(hmac(sha1),cbc(aes))",
+	.cra_driver_name = "authenc-hmac-sha1-cbc-aes-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = AES_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC ,
+	.cra_ctxsize = CTRL_DESC_SIZE,
+	.cra_type = &crypto_aead_type,
+	.cra_init = aead_cra_cbc_init,
+	.cra_exit = aead_session_cleanup,
+	.cra_aead = {
+		     .setkey = xlp_aes_cbc_hmac_sha1_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_aes_cbc_encrypt,
+		     .decrypt = xlp_aes_cbc_decrypt,
+		     .givencrypt = xlp_aes_cbc_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = AES_BLOCK_SIZE,
+		     .maxauthsize = SHA1_DIGEST_SIZE,
+		     }
+};
+
+static struct crypto_alg xlp_aes_cbc_aes_xcbc_mac_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(xcbc(aes),cbc(aes))",
+	.cra_driver_name = "authenc-xcbc-mac-aes-cbc-aes-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = AES_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC ,
+	.cra_ctxsize = CTRL_DESC_SIZE,
+	.cra_type = &crypto_aead_type,
+	.cra_init = aead_cra_cbc_init,
+	.cra_exit = aead_session_cleanup,
+	.cra_aead = {
+		     .setkey = xlp_aes_cbc_aes_xcbc_mac_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_aes_cbc_encrypt,
+		     .decrypt = xlp_aes_cbc_decrypt,
+		     .givencrypt = xlp_aes_cbc_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = AES_BLOCK_SIZE,
+		     .maxauthsize = XCBC_DIGEST_SIZE,
+		     }
+};
+
+static struct crypto_alg xlp_aes_cbc_hmac_md5_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(hmac(md5),cbc(aes))",
+	.cra_driver_name = "authenc-hmac-md5-cbc-aes-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = AES_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC ,
+	.cra_ctxsize = CTRL_DESC_SIZE,
+	.cra_type = &crypto_aead_type,
+	.cra_init = aead_cra_cbc_init,
+	.cra_exit = aead_session_cleanup,
+	.cra_aead = {
+		     .setkey = xlp_aes_cbc_hmac_md5_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_aes_cbc_encrypt,
+		     .decrypt = xlp_aes_cbc_decrypt,
+		     .givencrypt = xlp_aes_cbc_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = AES_BLOCK_SIZE,
+		     .maxauthsize = MD5_DIGEST_SIZE,
+		     }
+};
+
+static struct crypto_alg xlp_3des_cbc_hmac_md5_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(hmac(md5),cbc(des3_ede))",
+	.cra_driver_name = "authenc-hmac-md5-cbc-des3-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+	.cra_ctxsize = DES3_CTRL_DESC_SIZE, 
+	.cra_type = &crypto_aead_type,
+	.cra_init = aead_cra_init,
+	.cra_exit = aead_session_cleanup,
+	.cra_aead = {
+		     .setkey = xlp_3des_cbc_hmac_md5_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_3des_cbc_encrypt,
+		     .decrypt = xlp_3des_cbc_decrypt,
+		     .givencrypt = xlp_3des_cbc_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = DES3_EDE_BLOCK_SIZE,
+		     .maxauthsize = MD5_DIGEST_SIZE,
+		     }
+};
+
+static struct crypto_alg xlp_3des_cbc_hmac_sha256_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(hmac(sha256),cbc(des3_ede))",
+	.cra_driver_name = "authenc-hmac-sha256-cbc-des3-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+	.cra_ctxsize = DES3_CTRL_DESC_SIZE,
+	.cra_type = &crypto_aead_type,
+	.cra_init = aead_cra_init,
+	.cra_exit = aead_session_cleanup,
+	.cra_aead = {
+		     .setkey = xlp_3des_cbc_hmac_sha256_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_3des_cbc_encrypt,
+		     .decrypt = xlp_3des_cbc_decrypt,
+		     .givencrypt = xlp_3des_cbc_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = DES3_EDE_BLOCK_SIZE,
+		     .maxauthsize = SHA256_DIGEST_SIZE,
+		     }
+};
+
+static struct crypto_alg xlp_3des_cbc_hmac_sha1_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(hmac(sha1),cbc(des3_ede))",
+	.cra_driver_name = "authenc-hmac-sha1-cbc-des3-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+	.cra_ctxsize = DES3_CTRL_DESC_SIZE,
+	.cra_type = &crypto_aead_type,
+	.cra_init = aead_cra_init,
+	.cra_exit = aead_session_cleanup,
+	.cra_aead = {
+		     .setkey = xlp_3des_cbc_hmac_sha1_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_3des_cbc_encrypt,
+		     .decrypt = xlp_3des_cbc_decrypt,
+		     .givencrypt = xlp_3des_cbc_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = DES3_EDE_BLOCK_SIZE,
+		     .maxauthsize = SHA1_DIGEST_SIZE,
+		     }
+};
+
+static struct crypto_alg xlp_3des_cbc_aes_xcbc_mac_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(xcbc(aes),cbc(des3_ede))",
+	.cra_driver_name = "authenc-aes-xcbc-cbc-des3-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+	.cra_ctxsize = DES3_CTRL_DESC_SIZE,
+	.cra_type = &crypto_aead_type,
+	.cra_exit = aead_session_cleanup,
+	.cra_init = aead_cra_init,
+	.cra_aead = {
+		     .setkey = xlp_3des_cbc_aes_xcbc_mac_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_3des_cbc_encrypt,
+		     .decrypt = xlp_3des_cbc_decrypt,
+		     .givencrypt = xlp_3des_cbc_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = DES3_EDE_BLOCK_SIZE,
+		     .maxauthsize = XCBC_DIGEST_SIZE,
+		     }
+};
+
+
+static struct crypto_alg xlp_des_cbc_aes_xcbc_mac_cipher_auth = {
+        /* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+        .cra_name = "authenc(xcbc(aes),cbc(des))",
+        .cra_driver_name = "authenc-xcbc-mac-aes-cbc-des-xlp",
+        .cra_priority = XLP_CRYPT_PRIORITY,
+        .cra_blocksize = DES_BLOCK_SIZE,
+        .cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+        .cra_ctxsize = CTRL_DESC_SIZE,
+        .cra_type = &crypto_aead_type,
+        .cra_init = aead_cra_cbc_init,
+        .cra_exit = aead_session_cleanup,
+        .cra_aead = {
+                     .setkey = xlp_des_cbc_aes_xcbc_mac_setkey,
+                     .setauthsize = aead_setauthsize,
+                     .encrypt = xlp_des_cbc_encrypt,
+                     .decrypt = xlp_des_cbc_decrypt,
+                     .givencrypt = xlp_des_cbc_givencrypt,
+                     .geniv = "<built-in>",
+                     .ivsize = DES_BLOCK_SIZE,
+                     .maxauthsize = MD5_DIGEST_SIZE,
+                     }
+};
+static struct crypto_alg xlp_des_cbc_hmac_md5_cipher_auth = {
+        /* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+        .cra_name = "authenc(hmac(md5),cbc(des))",
+        .cra_driver_name = "authenc-hmac-md5-cbc-des-xlp",
+        .cra_priority = XLP_CRYPT_PRIORITY,
+        .cra_blocksize = DES_BLOCK_SIZE,
+        .cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+        .cra_ctxsize = CTRL_DESC_SIZE,
+        .cra_type = &crypto_aead_type,
+        .cra_init = aead_cra_cbc_init,
+        .cra_exit = aead_session_cleanup,
+        .cra_aead = {
+                     .setkey = xlp_des_cbc_hmac_md5_setkey,
+                     .setauthsize = aead_setauthsize,
+                     .encrypt = xlp_des_cbc_encrypt,
+                     .decrypt = xlp_des_cbc_decrypt,
+                     .givencrypt = xlp_des_cbc_givencrypt,
+                     .geniv = "<built-in>",
+                     .ivsize = DES_BLOCK_SIZE,
+                     .maxauthsize = MD5_DIGEST_SIZE,
+                     }
+};
+
+static struct crypto_alg xlp_des_cbc_hmac_sha256_cipher_auth = {
+        /* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+        .cra_name = "authenc(hmac(sha256),cbc(des))",
+        .cra_driver_name = "authenc-hmac-sha256-cbc-des-xlp",
+        .cra_priority = XLP_CRYPT_PRIORITY,
+        .cra_blocksize = DES_BLOCK_SIZE,
+        .cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+        .cra_ctxsize = CTRL_DESC_SIZE,
+        .cra_type = &crypto_aead_type,
+        .cra_init = aead_cra_cbc_init,
+        .cra_exit = aead_session_cleanup,
+        .cra_aead = {
+                     .setkey = xlp_des_cbc_hmac_sha256_setkey,
+                     .setauthsize = aead_setauthsize,
+                     .encrypt = xlp_des_cbc_encrypt,
+                     .decrypt = xlp_des_cbc_decrypt,
+                     .givencrypt = xlp_des_cbc_givencrypt,
+                     .geniv = "<built-in>",
+                     .ivsize = DES_BLOCK_SIZE,
+                     .maxauthsize = SHA256_DIGEST_SIZE,
+                     }
+};
+static struct crypto_alg xlp_des_cbc_hmac_sha1_cipher_auth = {
+        /* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+        .cra_name = "authenc(hmac(sha1),cbc(des))",
+        .cra_driver_name = "authenc-hmac-sha1-cbc-des-xlp",
+        .cra_priority = XLP_CRYPT_PRIORITY,
+        .cra_blocksize = DES_BLOCK_SIZE,
+        .cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+        .cra_ctxsize = CTRL_DESC_SIZE,
+        .cra_type = &crypto_aead_type,
+        .cra_exit = aead_session_cleanup,
+        .cra_init = aead_cra_cbc_init,
+        .cra_aead = {
+                     .setkey = xlp_des_cbc_hmac_sha1_setkey,
+                     .setauthsize = aead_setauthsize,
+                     .encrypt = xlp_des_cbc_encrypt,
+                     .decrypt = xlp_des_cbc_decrypt,
+                     .givencrypt = xlp_des_cbc_givencrypt,
+                     .geniv = "<built-in>",
+                     .ivsize = DES_BLOCK_SIZE,
+                     .maxauthsize = SHA1_DIGEST_SIZE,
+                     }
+};
+
+
+int xlp_aead_alg_init(void)
+{
+	int ret = 0;
+	int no_of_alg_registered = 0;
+	
+	if ((ret = crypto_register_alg(&xlp_aes_cbc_hmac_sha256_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+
+	if ((ret = crypto_register_alg(&xlp_aes_cbc_hmac_sha1_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+
+	if ((ret = crypto_register_alg(&xlp_aes_cbc_aes_xcbc_mac_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+
+	if ((ret = crypto_register_alg(&xlp_aes_cbc_hmac_md5_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+
+	if ((ret = crypto_register_alg(&xlp_3des_cbc_hmac_md5_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+
+	if ((ret = crypto_register_alg(&xlp_3des_cbc_hmac_sha256_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+
+	if ((ret = crypto_register_alg(&xlp_3des_cbc_hmac_sha1_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+
+	if ((ret = crypto_register_alg(&xlp_3des_cbc_aes_xcbc_mac_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+	if (( ret = crypto_register_alg(&xlp_des_cbc_hmac_md5_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+	if ( (ret = crypto_register_alg(&xlp_des_cbc_hmac_sha256_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+	if ((ret =  crypto_register_alg(&xlp_des_cbc_hmac_sha1_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+	if ((ret =  crypto_register_alg(&xlp_des_cbc_aes_xcbc_mac_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+
+end:
+	return no_of_alg_registered;
+} 
+
+void
+xlp_aead_alg_fini(void)
+{
+	
+	crypto_unregister_alg(&xlp_aes_cbc_hmac_sha1_cipher_auth);
+	crypto_unregister_alg(&xlp_aes_cbc_aes_xcbc_mac_cipher_auth);
+	crypto_unregister_alg(&xlp_aes_cbc_hmac_md5_cipher_auth);
+	crypto_unregister_alg(&xlp_3des_cbc_hmac_md5_cipher_auth);
+	crypto_unregister_alg(&xlp_3des_cbc_hmac_sha256_cipher_auth);
+	crypto_unregister_alg(&xlp_3des_cbc_hmac_sha1_cipher_auth);
+	crypto_unregister_alg(&xlp_3des_cbc_aes_xcbc_mac_cipher_auth);
+	crypto_unregister_alg(&xlp_des_cbc_hmac_md5_cipher_auth);
+	crypto_unregister_alg(&xlp_des_cbc_hmac_sha256_cipher_auth);
+	crypto_unregister_alg(&xlp_des_cbc_hmac_sha1_cipher_auth);
+	crypto_unregister_alg(&xlp_aes_cbc_hmac_sha256_cipher_auth);
+	crypto_unregister_alg(&xlp_des_cbc_aes_xcbc_mac_cipher_auth);
+	return;
+}
+
+EXPORT_SYMBOL(xlp_aead_alg_init);
+EXPORT_SYMBOL(xlp_aead_alg_fini);
diff --git a/drivers/crypto/sae/nlm_async.h b/drivers/crypto/sae/nlm_async.h
new file mode 100644
index 0000000..5ed501b
--- /dev/null
+++ b/drivers/crypto/sae/nlm_async.h
@@ -0,0 +1,85 @@
+/***********************************************************************
+Copyright 2003-2010 Netlogic Microsystems (“Netlogic”). All rights
+reserved.
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are
+met:
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+*****************************#NETL_2#********************************/
+#ifndef __NLM_ASYNC_H
+#define __NLM_ASYNC_H
+struct nlm_async_crypto;
+#define NLM_CRYPTO_VC_BASE 281 
+#define MAX_CPU 32
+extern int crypto_get_fb_vc(void);
+
+#define nlm_processor_id()                              \
+        ({ int __res;                                   \
+         __asm__ __volatile__(                          \
+                 ".set\tmips32\n\t"                     \
+                 "mfc0\t%0, $15, 1\n\t"                 \
+                 "andi\t%0, 0x1f\n\t"                   \
+                 ".set\tmips0\n\t"                      \
+                 : "=r" (__res));                       \
+         __res;                                         \
+         })
+
+
+
+
+struct nlm_async_crypto
+{
+	void (*callback) (struct nlm_async_crypto *args, uint64_t entry1);
+	void *args;
+	int op;
+	int authsize;
+	uint8_t *actual_tag;
+	uint8_t *hash_addr;
+	uint16_t stat;
+	uint32_t bytes;
+};
+
+enum enc_stat {
+	DES_CBC_STAT = 0 ,
+	TDES_CBC_STAT ,
+	AES128_CBC_STAT ,
+	AES192_CBC_STAT ,
+	AES256_CBC_STAT, 
+	ENC_MAX_STAT
+};
+enum {
+	MD5_STAT,
+	H_SHA1_STAT,
+	H_SHA256_STAT,
+	AES128_XCBC_STAT,
+	AES192_XCBC_STAT,
+	AES256_XCBC_STAT,
+	AUTH_MAX_STAT
+};
+
+struct nlm_crypto_stat
+{
+	uint64_t enc[ENC_MAX_STAT];
+	uint64_t enc_tbytes[ENC_MAX_STAT];
+	uint64_t auth[AUTH_MAX_STAT];
+	uint64_t auth_tbytes[AUTH_MAX_STAT];
+};
+
+
+#endif
diff --git a/drivers/crypto/sae/nlm_auth.c b/drivers/crypto/sae/nlm_auth.c
new file mode 100644
index 0000000..3234370
--- /dev/null
+++ b/drivers/crypto/sae/nlm_auth.c
@@ -0,0 +1,432 @@
+/***********************************************************************
+Copyright 2003-2010 Netlogic Microsystems (“Netlogic”). All rights
+reserved.
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are
+met:
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+*****************************#NETL_2#********************************/
+#include <crypto/sha.h>
+#include <nlm_hal_fmn.h>
+#include <crypto/aes.h>
+#include <crypto/internal/hash.h>
+#include "nlmcrypto.h"
+#include "nlm_async.h"
+
+#define XLP_AUTH_PRIORITY      300
+#define XLP_HMAC_PRIORITY      300
+
+#define XCBC_DIGEST_SIZE	16
+
+#define MD5_DIGEST_SIZE		16
+#define MD5_BLOCK_SIZE		64
+
+//#define AUTH_BUFFER_SIZE	(16 * 1024)
+void hex_dump(char * description,unsigned char *in, int num);
+extern int auth_mode_key_len[NLM_HASH_MAX][NLM_HASH_MODE_MAX];
+
+#define ASYNC_PTR_SIZE 128
+#define ASYNC_PTR_OFFSET (sizeof(struct auth_pkt_desc ) + (NLM_AUTH_MAX_FRAGS* 16))
+
+//#define SEC_DEBUG
+
+#ifdef SEC_DEBUG
+#ifdef __KERNEL__
+#define debug_print(fmt, args...) printk(fmt, ##args)
+#else				/* __KERNEL__ */
+#define debug_print(fmt, args...) printf(fmt, ##args)
+#endif				/* __KERNEL__ */
+#else				/* SEC_DEBUG */
+#define debug_print(fmt, args...)
+#endif				/* SEC_DEBUG */
+
+#define malloc(a) kmalloc(a, GFP_KERNEL)
+#define free kfree
+#define NLM_AUTH_MAX_FRAGS	(20)
+#define NLM_CRYPTO_SYNC_VC 3
+
+
+struct nlm_auth_ctx
+{
+	struct nlm_crypto_pkt_ctrl ctrl;
+	uint16_t stat;
+	/*Don't change the order of this strucutre*/
+};
+
+#define MAX_FRAGS               18
+#define CTRL_DESC_SIZE          (sizeof(struct nlm_auth_ctx) + 64)
+
+struct auth_pkt_desc
+{
+	uint32_t curr_index;
+	uint32_t total_len;
+	uint8_t pad[56];
+	//struct pkt_desc pkt_desc;
+	struct nlm_crypto_pkt_param pkt_param; 
+	uint16_t stat;
+};
+
+#define PACKET_DESC_SIZE   (64+sizeof(struct auth_pkt_desc) + MAX_FRAGS*(2*64) + 64 + sizeof(struct nlm_async_crypto) + 64)
+#define NLM_CRYPTO_PKT_PARAM_OFFSET(addr)       (((unsigned long)addr + 64) & ~0x3fULL)
+#define NLM_ASYNC_PTR_PARAM_OFFSET(addr)        (((unsigned long)(addr + 64 + sizeof(struct nlm_crypto_pkt_param) + MAX_FRAGS*(2*64)) + 64) & ~0x3fULL)
+/*
+   All extern declaration goes here.
+ */
+extern uint32_t nlm_hal_send_msg3(uint32_t dst, uint32_t code, uint64_t data0, uint64_t data1, uint64_t data2);
+extern uint32_t nlm_hal_recv_msg2(uint32_t dst, uint32_t *src, uint32_t *size, uint32_t *code, uint64_t *data0, uint64_t *data1);
+extern void print_cntl_instr(uint64_t cntl_desc);
+extern void print_crypto_msg_desc(uint64_t entry1, uint64_t entry2, uint64_t entry3);
+extern void print_pkt_desc(struct nlm_crypto_pkt_param * pkt_param, int index);
+extern struct nlm_crypto_stat crypto_stat[MAX_CPU];
+
+static inline void print_info(const char *func)
+{
+	extern void dump_stack(void);
+	printk("\n********[%s function called]**********\n",func);
+	dump_stack();
+	printk("\n*********[%s Dumpstack ends]***********\n\n\n",func);
+	return;
+}
+#ifdef NLM_CRYPTO_DEBUG
+static void print_buf(unsigned char *msg, unsigned char *buf, int len)
+{
+#define TMP_BUF		50
+	char tmp_buf[TMP_BUF + 1];
+	int i, index = 0;
+
+	printk("**********%s************\n",msg);
+	for(i=0; i<len; i++){
+		sprintf(&tmp_buf[index*2], "%02x", buf[i]);
+		index++;
+		if(index == (TMP_BUF/2)){
+			tmp_buf[index*2] = '\0';
+			printk("[%s]\n",tmp_buf);
+			index = 0;
+		}
+	}
+	if(index){
+		tmp_buf[index*2] = '\0';
+		printk("[%s]\n",tmp_buf);
+	}
+}
+#endif
+
+static struct nlm_auth_ctx *nlm_shash_auth_ctx(struct crypto_shash *shash)
+{
+	uint8_t *ctx = crypto_tfm_ctx(crypto_shash_tfm(shash));
+	ctx = (uint8_t *)(((unsigned long)ctx + 63) & ~(0x3f));
+	return (struct nlm_auth_ctx *)ctx;
+}
+
+static struct nlm_auth_ctx *pkt_ctrl_auth_ctx(struct shash_desc *desc)
+{
+	return nlm_shash_auth_ctx(desc->tfm);
+}
+static int
+xlp_auth_init(struct shash_desc *desc)
+{
+	struct auth_pkt_desc *auth_pkt_desc = (struct auth_pkt_desc * )NLM_CRYPTO_PKT_PARAM_OFFSET(shash_desc_ctx(desc));
+	//printk("[%s] [%p]\n",__FUNCTION__, pkt_desc);
+	//printk("[%s] [%p]\n",__FUNCTION__, &pkt_desc->pkt_param.desc0);
+	//printk("[%s] [%p]\n",__FUNCTION__, &pkt_desc->pkt_param);
+	auth_pkt_desc->curr_index = 0;
+	auth_pkt_desc->total_len = 0;
+	return 0;
+}
+
+static int
+xlp_auth_update(struct shash_desc *desc,
+		const uint8_t * data, unsigned int length)
+{
+	struct auth_pkt_desc *auth_pkt_desc = (struct auth_pkt_desc *)NLM_CRYPTO_PKT_PARAM_OFFSET(shash_desc_ctx(desc));
+	int index = auth_pkt_desc->curr_index;
+	struct nlm_crypto_pkt_param  *pkt_param  = &(auth_pkt_desc->pkt_param);
+	
+	nlm_crypto_fill_src_seg(pkt_param, index, (unsigned char*)data, length);
+	auth_pkt_desc->curr_index = nlm_crypto_fill_dst_seg(pkt_param, index , (unsigned char*)data, length);
+	auth_pkt_desc->total_len += length;
+
+	return 0;
+}
+static int
+crypto_get_sync_fb_vc(void)
+{
+    int cpu;
+    extern int sae_rx_sync_vc;
+
+    cpu = hard_smp_processor_id();      //processor_id();
+    cpu = cpu * 4 + sae_rx_sync_vc;
+
+    return cpu;
+}
+
+static int
+xlp_auth_final(struct shash_desc *desc, uint8_t *out)
+{
+	struct auth_pkt_desc *auth_pkt_desc = (struct auth_pkt_desc *)NLM_CRYPTO_PKT_PARAM_OFFSET(shash_desc_ctx(desc));
+	int index = auth_pkt_desc->curr_index;
+	struct nlm_crypto_pkt_param  *pkt_param  = &(auth_pkt_desc->pkt_param);
+	struct nlm_auth_ctx  * auth_ctx   = pkt_ctrl_auth_ctx(desc);
+	int fb_vc ;
+	uint64_t entry0, entry1, tx_id=0x12345678;
+	uint64_t  timeout = 0;
+	struct nlm_crypto_pkt_ctrl *ctrl = &auth_ctx->ctrl;
+	uint32_t size,code,src;
+	uint16_t stat = auth_ctx->stat;
+	int cpu = nlm_processor_id();
+
+	nlm_crypto_fill_auth_pkt_param(ctrl,pkt_param,
+			0,auth_pkt_desc->total_len,0,out); 
+
+	preempt_disable();
+
+	fb_vc = crypto_get_sync_fb_vc();
+	entry0 = nlm_crypto_form_pkt_fmn_entry0(fb_vc, 0, 0, 0, virt_to_phys(ctrl));
+	entry1 = nlm_crypto_form_pkt_fmn_entry1(0, ctrl->hashkeylen, (32 + index * 16 ), virt_to_phys(pkt_param));
+	
+
+#ifdef NLM_CRYPTO_DEBUG
+	print_crypto_msg_desc(entry0, entry1, tx_id);
+	print_cntl_instr(ctrl->desc0);
+	print_pkt_desc(pkt_param,index);
+#endif
+
+	//construct pkt, send to engine and receive reply
+	xlp_message_send_block_fast_3(0, NLM_CRYPTO_VC_BASE, entry0, entry1, tx_id);
+	timeout = 0;
+	do {
+		timeout++;
+		nlm_hal_recv_msg2(NLM_CRYPTO_SYNC_VC, &src, &size, &code, &entry0, &entry1);
+	} while(entry0 != tx_id && timeout < 0xffffffff) ;
+	
+
+
+	if (timeout >= 0xffffffff) {
+		printk("Error: FreeBack message is not received");
+		preempt_enable();
+		return -EIO;
+	}
+#ifdef NLM_CRYPTO_DEBUG
+	print_buf("AUTH:", out, 16);
+#endif
+	crypto_stat[cpu].auth[stat] ++;
+	crypto_stat[cpu].auth_tbytes[stat] += auth_pkt_desc->total_len + ctrl->taglen;
+	preempt_enable();
+ 	return 0;
+}
+
+/*
+   All Setkey goes here.
+ */
+
+static int xlp_auth_aes_xcbc_setkey(struct crypto_shash *tfm, const u8 * key, unsigned int keylen)
+{
+	struct nlm_auth_ctx * nlm_ctx = nlm_shash_auth_ctx(tfm);
+	uint32_t hash_alg = NLM_HASH_AES128;
+
+        switch (keylen) {
+        case 16:
+                hash_alg = NLM_HASH_AES128;
+		nlm_ctx->stat = AES128_XCBC_STAT;
+                break;
+        case 24:
+                hash_alg = NLM_HASH_AES192;
+		nlm_ctx->stat = AES192_XCBC_STAT;
+                break;
+        case 32:
+                hash_alg = NLM_HASH_AES256;
+		nlm_ctx->stat = AES256_XCBC_STAT;
+                break;
+        default:
+                printk(KERN_WARNING "[%s]: Cannot handle keylen = %d\n",
+                       __FUNCTION__, keylen);
+	}
+
+
+	/*setup ctrl descriptor*/
+	nlm_crypto_fill_pkt_ctrl(&nlm_ctx->ctrl,0,hash_alg,NLM_HASH_MODE_XCBC,
+		NLM_CIPHER_BYPASS,NLM_CIPHER_MODE_ECB,0,NULL,0,(unsigned char *)key,keylen);
+	return 0;
+	
+}
+
+
+static int
+xlp_auth_hmac_sha256_setkey(struct crypto_shash *tfm, const u8 * key, unsigned int keylen)
+{
+	struct nlm_auth_ctx * nlm_ctx = nlm_shash_auth_ctx(tfm);
+	struct nlm_crypto_pkt_ctrl * ctrl = &nlm_ctx->ctrl; 
+	nlm_ctx->stat = H_SHA256_STAT;
+
+	/*setup ctrl descriptor*/
+	nlm_crypto_fill_pkt_ctrl(ctrl,1,NLM_HASH_SHA,NLM_HASH_MODE_SHA256,
+		NLM_CIPHER_BYPASS,NLM_CIPHER_MODE_ECB,0,NULL,0,(unsigned char*)key,keylen);
+	if ( ctrl->hashkeylen < auth_mode_key_len[NLM_HASH_SHA][NLM_HASH_MODE_SHA256]) 
+		ctrl->hashkeylen = auth_mode_key_len[NLM_HASH_SHA][NLM_HASH_MODE_SHA256];
+	return 0;
+	
+}
+
+
+static int
+xlp_auth_hmac_md5_setkey(struct crypto_shash *tfm, const u8 * key, unsigned int keylen)
+{
+	struct nlm_auth_ctx * nlm_ctx = nlm_shash_auth_ctx(tfm);
+	struct nlm_crypto_pkt_ctrl * ctrl = &nlm_ctx->ctrl; 
+	nlm_ctx->stat = MD5_STAT;
+
+	/*setup ctrl descriptor*/
+	nlm_crypto_fill_pkt_ctrl(ctrl,1,NLM_HASH_MD5,NLM_HASH_MODE_SHA1,
+		NLM_CIPHER_BYPASS,NLM_CIPHER_MODE_ECB,0,NULL,0,(unsigned char *)key,keylen);
+	if ( ctrl->hashkeylen < auth_mode_key_len[NLM_HASH_MD5][NLM_HASH_MODE_SHA1]) 
+		ctrl->hashkeylen = auth_mode_key_len[NLM_HASH_MD5][NLM_HASH_MODE_SHA1];
+	return 0;
+	
+}
+
+static int
+xlp_auth_hmac_sha1_setkey(struct crypto_shash *tfm, const u8 * key, unsigned int keylen)
+{
+	struct nlm_auth_ctx * nlm_ctx = nlm_shash_auth_ctx(tfm);
+	struct nlm_crypto_pkt_ctrl * ctrl = &nlm_ctx->ctrl; 
+	nlm_ctx->stat = H_SHA1_STAT;
+
+	/*setup ctrl descriptor*/
+	nlm_crypto_fill_pkt_ctrl(ctrl,1,NLM_HASH_SHA,NLM_HASH_MODE_SHA1,
+		NLM_CIPHER_BYPASS,NLM_CIPHER_MODE_ECB,0,NULL,0,(unsigned char *)key,keylen);
+	if ( ctrl->hashkeylen < auth_mode_key_len[NLM_HASH_SHA][NLM_HASH_MODE_SHA1]) 
+		ctrl->hashkeylen = auth_mode_key_len[NLM_HASH_SHA][NLM_HASH_MODE_SHA1];
+	return 0;
+	 
+	
+}
+
+static struct shash_alg xcbc_mac_alg = {
+	.digestsize = XCBC_DIGEST_SIZE,
+	.init = xlp_auth_init,
+	.update = xlp_auth_update,
+	.final = xlp_auth_final,
+	.setkey = xlp_auth_aes_xcbc_setkey,
+	.descsize = PACKET_DESC_SIZE,
+	.base = {
+		 .cra_name = "xcbc(aes)",
+		 .cra_driver_name = "xcbc-aes-xlp",
+		 .cra_priority = XLP_HMAC_PRIORITY,
+		 .cra_flags = CRYPTO_ALG_TYPE_SHASH,
+		 .cra_blocksize = AES_BLOCK_SIZE,
+		 .cra_module = THIS_MODULE,
+		 .cra_ctxsize = CTRL_DESC_SIZE, 
+		 }
+};
+
+static struct shash_alg sha256_hmac_alg = {
+	.digestsize = SHA256_DIGEST_SIZE,
+	.init = xlp_auth_init,
+	.update = xlp_auth_update,
+	.final = xlp_auth_final,
+	.setkey = xlp_auth_hmac_sha256_setkey,
+	.descsize = PACKET_DESC_SIZE,
+	.base = {
+		 .cra_name = "hmac(sha256)",
+		 .cra_driver_name = "hmac-sha256-xlp",
+		 .cra_priority = XLP_HMAC_PRIORITY,
+		 .cra_flags = CRYPTO_ALG_TYPE_SHASH,
+		 .cra_blocksize = SHA256_BLOCK_SIZE,
+		 .cra_module = THIS_MODULE,
+		 .cra_ctxsize = CTRL_DESC_SIZE, 
+		 }
+};
+
+static struct shash_alg md5_hmac_alg = {
+	.digestsize = MD5_DIGEST_SIZE,
+	.init = xlp_auth_init,
+	.update = xlp_auth_update,
+	.final = xlp_auth_final,
+	.setkey = xlp_auth_hmac_md5_setkey,
+	.descsize = PACKET_DESC_SIZE,
+	.base = {
+		 .cra_name = "hmac(md5)",
+		 .cra_driver_name = "hmac-md5-xlp",
+		 .cra_priority = XLP_HMAC_PRIORITY,
+		 .cra_flags = CRYPTO_ALG_TYPE_SHASH,
+		 .cra_blocksize = MD5_BLOCK_SIZE,
+		 .cra_module = THIS_MODULE,
+		 .cra_ctxsize = CTRL_DESC_SIZE, 
+		 }
+};
+static struct shash_alg sha1_hmac_alg = {
+	.digestsize = SHA1_DIGEST_SIZE,
+	.init = xlp_auth_init,
+	.update = xlp_auth_update,
+	.final = xlp_auth_final,
+	.setkey = xlp_auth_hmac_sha1_setkey,
+	.descsize = PACKET_DESC_SIZE,
+	.base = {
+		 .cra_name = "hmac(sha1)",
+		 .cra_driver_name = "hmac-sha1-xlp",
+		 .cra_priority = XLP_HMAC_PRIORITY,
+		 .cra_flags = CRYPTO_ALG_TYPE_SHASH,
+		 .cra_blocksize = SHA1_BLOCK_SIZE,
+		 .cra_module = THIS_MODULE,
+		 .cra_ctxsize = CTRL_DESC_SIZE, 
+		 }
+};
+
+int
+xlp_auth_alg_init(void)
+{
+	int rc = -ENODEV;
+	int no_of_alg_registered = 0;
+
+	rc = crypto_register_shash(&sha1_hmac_alg);
+	if (rc)
+		goto out;
+	no_of_alg_registered++;
+	rc = crypto_register_shash(&sha256_hmac_alg);
+	if (rc)
+		goto out;
+	no_of_alg_registered++;
+	rc = crypto_register_shash(&md5_hmac_alg);
+	if (rc)
+		goto out;
+	no_of_alg_registered++;
+	rc = crypto_register_shash(&xcbc_mac_alg);
+	if (rc)
+		goto out;
+	no_of_alg_registered++;
+	//printk("Some of the FIPS test failed as the maximum key length supported is 64 bytes.\n");
+
+	printk(KERN_NOTICE "Using XLP hardware for SHA/MD5 algorithms.\n");
+out:
+
+	return 0;
+
+}
+
+void
+xlp_auth_alg_fini(void)
+{
+	crypto_unregister_shash(&sha1_hmac_alg);
+	crypto_unregister_shash(&md5_hmac_alg);
+	crypto_unregister_shash(&sha256_hmac_alg);
+	crypto_unregister_shash(&xcbc_mac_alg);
+}
+
+EXPORT_SYMBOL(xlp_auth_alg_init);
+EXPORT_SYMBOL(xlp_auth_alg_fini);
diff --git a/drivers/crypto/sae/nlm_crypto.c b/drivers/crypto/sae/nlm_crypto.c
new file mode 100644
index 0000000..f50eca6
--- /dev/null
+++ b/drivers/crypto/sae/nlm_crypto.c
@@ -0,0 +1,591 @@
+/***********************************************************************
+  Copyright 2003-2010 Netlogic Microsystems (“Netlogic”). All rights
+  reserved.
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions are
+met:
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+ *****************************#NETL_2#********************************/
+
+#include <asm/netlogic/msgring.h>
+#include <linux/proc_fs.h>
+#include <asm/netlogic/proc.h>
+
+#include <asm/netlogic/hal/nlm_hal_fmn.h>
+#include <asm/netlogic/hal/nlm_hal_macros.h>
+#include <linux/crypto.h>
+#include "nlm_async.h"
+#include "nlmcrypto.h"
+
+
+#define XLP_POLLING 1
+#ifdef TRACING
+#define TRACE_TEXT(str) printk(str);
+#define TRACE_RET printk(")")
+#else				/* !TRACING */
+#define TRACE_TEXT(str) ((void) 0)
+#define TRACE_RET ((void) 0)
+#endif				/* TRACING */
+#undef NLM_CRYPTO_DEBUG
+
+#define DRIVER_NAME "nlmsae"
+
+#define NLM_CRYPTO_OP_IN_PROGRESS 0
+#define NLM_CRYPTO_OP_DONE	  1
+
+/**
+ * @file_name crypto.c
+ */
+
+/**
+ * @defgroup crypto Crypto API
+ * @brief Description about the crypto apis
+ */
+
+#define printf(a, b...) printk(KERN_ERR a, ##b)
+//#define printf(a, b...)
+#define malloc(a) kmalloc(a, GFP_ATOMIC)
+#define free kfree
+
+#define xtract_bits(x, bitpos, numofbits) ((x) >> (bitpos) & ((1ULL << (numofbits)) - 1))
+
+#define VC_MODE_ROUND_ROBIN 1
+#define NUM_VC 16
+
+extern struct proc_dir_entry *nlm_root_proc;
+extern int xlp_aead_alg_init(void);
+extern void xlp_aead_alg_fini(void);
+extern int xlp_crypt_alg_init(void);
+extern void xlp_crypt_alg_fini(void);
+extern int xlp_auth_alg_init(void);
+extern void xlp_auth_alg_fini(void);
+static void xlp_sae_cleanup(void);
+
+static int xlp_sae_major;
+static int xlp_sae_open(struct inode *, struct file *);
+static int xlp_sae_release(struct inode *, struct file *);
+
+struct nlm_crypto_stat crypto_stat[MAX_CPU];
+
+
+/*
+ * is the following table needed for all modes?
+Cipher            keylen           iv_len
+*/
+
+//-1 indicates variable length IV
+// In case of AES/Camelia cipher and CBC-MAC auth, IV is not needed.
+// In case of AES/Camelia cipher and XCBC-MAC auth, IV is needed only for 
+//CBC, CFB, OFB and CTR modes..
+int cipher_mode_iv_len[NLM_CIPHER_MAX][NLM_CIPHER_MODE_MAX] = {
+/*		       ECB  CBC   CFB   OFB   CTR  AESF8    GCM  CCM    8   9  LRW   XTS */
+/* BYPASS */       {   0,    0,    0,    0,    0,    0,      0,    0,   0,  0,  0,    0,},
+/* DES */          {   0,    8,    0,    0,    0,    0,      0,    0,   0,  0,  0,    0,},
+/* 3DES */         {   0,    8,    0,    0,    0,    0,      0,    0,   0,  0,  0,    0,},
+/* AES128 */       {   0,    16,   16,   16,   8,   16,     8,   8,   0,  0,  16,   16,},
+/* AES192 */       {   0,    16,   16,   16,   8,   16,     8,   8,   0,  0,  16,   16,},
+/* AES256 */       {   0,    16,   16,   16,   8,   16,     8,   8,   0,  0,  16,   16,},
+/* ARC4 */         {   0,    0,    0,    0,    0,    0,      0,    0,   0,  0,  0,    0,},
+/* KASUMI F8 */    {   0,    0,    0,    0,    0,    0,      0,    0,   0,  0,  0,    0,},
+/* SNOW3G F8 */    {   0,    0,    0,    0,    0,    0,      0,    0,   0,  0,  0,    0,},
+/* CAMELLIA128 */  {   0,    16,   16,   16,   16,   16,     -1,   0,   0,  0,  16,   16,},
+/* CAMELLIA192 */  {   0,    16,   16,   16,   16,   16,     -1,   0,   0,  0,  16,   16,}, 
+/* CAMELLIA256 */  {   0,    16,   16,   16,   16,   16      -1,   0,   0,  0,  16,   16,},
+};
+
+int auth_mode_key_len[NLM_HASH_MAX][NLM_HASH_MODE_MAX] = {
+/*	               SHA1 SHA224 SHA256 SHA384 SHA512  CMAC  XCBC CBC_MAC CCM  GCM*/
+/* BYPASS */		{0,    0,     0,     0,     0,     0,   0,    0,     0,    0, },
+/* MD5 */		{64,   64,    64,   64,    64,    64,  64,   64,    64,   64, },
+/* SHA */		{64,   64,    64,   128,   128,    0,   0,    0,     0,    0, },
+/* 3 */			{0,    0,     0,     0,     0,     0,   0,    0,     0,    0, },
+/* AES128 */		{0,    0,     0,     0,     0,    16,  16,   16,    16,   16, },
+/* AES192 */		{0,    0,     0,     0,     0,    24,  24,   24,    24,   24, },
+/* AES256 */		{0,    0,     0,     0,     0,    32,  32,   32,    32,   32, },
+/* KASUMI_F9 */		{16,  16,    16,    16,    16,    16,  16,   16,    16,   16, },
+/* SNOW3G_F9 */		{16,  16,    16,    16,    16,    16,  16,   16,    16,   16, }, //sandip -> verify
+/* CAMELLIA128 */	{0,    0,     0,     0,     0,    16,  16,   16,    16,   16, },
+/* CAMELLIA192 */	{0,    0,     0,     0,     0,    24,  24,   24,    24,   24, },
+/* CAMELLIA256 */	{0,    0,     0,     0,     0,    32,  32,   32,    32,   32, },
+/* GHASH */		{0,    0,     0,     0,     0,    32,  32,   32,    32,   32, }, //todo:
+};
+
+#define MAX_KEY_LEN_IN_DW 20
+#define NLM_CRYPTO_MAX_STR_LEN 200
+static char str_cipher_alg[NLM_CIPHER_MAX + 1][NLM_CRYPTO_MAX_STR_LEN] = {
+"bypass",       // NLM_CIPHER_BYPASS
+"des",          // NLM_CIPHER_DES
+"3des",         // NLM_CIPHER_3DES
+"aes 128",      // NLM_CIPHER_AES128
+"aes 192",      // NLM_CIPHER_AES192
+"aes 256",      // NLM_CIPHER_AES256
+"arc4",         // NLM_CIPHER_ARC4
+"Kasumi f8",    // NLM_CIPHER_KASUMI_F8
+"snow3g f8",    // NLM_CIPHER_SNOW3G_F8
+"camellia 128", // NLM_CIPHER_CAMELLIA128
+"camelia 192",  // NLM_CIPHER_CAMELLIA192
+"camelia 256",  // NLM_CIPHER_CAMELLIA256
+"undefined",  // > max
+};
+static char str_cipher_mode[NLM_CIPHER_MODE_MAX+ 1][NLM_CRYPTO_MAX_STR_LEN] = {
+"ecb",          // NLM_CIPHER_MODE_ECB
+"cbc",          // NLM_CIPHER_MODE_CBC
+"cfb",          // NLM_CIPHER_MODE_CFB
+"ofb",          // NLM_CIPHER_MODE_OFB
+"ctr",          // NLM_CIPHER_MODE_CTR
+"aes f8",       // NLM_CIPHER_MODE_AES_F8
+"gcm",          // NLM_CIPHER_MODE_GCM
+"ccm",          // NLM_CIPHER_MODE_CCM
+"undefined",    // NLM_CIPHER_MODE_UNDEFINED1
+"undefined",    // NLM_CIPHER_MODE_UNDEFINED2
+"lrw",          // NLM_CIPHER_MODE_LRW
+"xts",          // NLM_CIPHER_MODE_XTS
+"undefined", // > max
+};
+static char str_auth_alg[NLM_HASH_MAX + 1][NLM_CRYPTO_MAX_STR_LEN] = {
+"bypass",       // NLM_AUTH_BYPASS
+"md5",          // NLM_AUTH_MD5
+"sha",          // NLM_AUTH_SHA
+"invalid",       // NLM_AUTH_UNDEFINED
+"aes 128",      // NLM_AUTH_AES128
+"aes 192",      // NLM_AUTH_AES192
+"aes 256",      // NLM_AUTH_AES256
+"kasumi f9",    // NLM_AUTH_KASUMI_F9
+"snow3g f9",    // NLM_AUTH_SNOW3G_F9
+"camellia 128", // NLM_AUTH_CAMELLIA128
+"camellia 192", // NLM_AUTH_CAMELLIA192
+"camellia 256", // NLM_AUTH_CAMELLIA256
+"ghash",        // NLM_AUTH_GHASH
+"undefined",    // > max
+};
+static char str_auth_mode[NLM_HASH_MODE_MAX + 1][NLM_CRYPTO_MAX_STR_LEN] = {
+"sha1",         // NLM_AUTH_MODE_SHA1
+"sha 224",      // NLM_AUTH_MODE_SHA224
+"sha 256",      // NLM_AUTH_MODE_SHA256
+"sha 384",      // NLM_AUTH_MODE_SHA384
+"sha 512",      // NLM_AUTH_MODE_SHA512
+"cmac",         // NLM_AUTH_MODE_CMAC
+"xcbc",         // NLM_AUTH_MODE_XCBC
+"cbc mac",      // NLM_AUTH_MODE_CBC_MAC
+"undefined", // > max
+};
+#ifdef NLM_CRYPTO_DEBUG
+void hex_dump(char * description,unsigned char *in, int num)
+{
+        int i, j;
+        char buf[50];
+        char *buf_ptr;
+        printk("%s\n",description);
+
+        for (i = 0; i < num; i+= 16) {
+                if (i + 16 > num) {
+                        buf_ptr = buf;
+                        sprintf(buf_ptr, "    ");
+                        buf_ptr += 4;
+                        for (j = 0 ; j < num - i ; j++) {
+                                sprintf(buf_ptr, "%02x ", in[j + i]);
+                                buf_ptr += 3;
+                        }
+                        *buf_ptr = '\0';
+                        printk("%s\n",buf);
+                        break;
+                }
+                printk("    %02x %02x %02x %02x %02x %02x %02x %02x %02x %02x %02x %02x %02x %02x %02x %02x\n",
+                        in[i + 0 ],
+                        in[i + 1 ],
+                        in[i + 2 ],
+                        in[i + 3 ],
+                        in[i + 4 ],
+                        in[i + 5 ],
+                        in[i + 6 ],
+                        in[i + 7 ],
+                        in[i + 8 ],
+                        in[i + 9 ],
+                        in[i + 10],
+                        in[i + 11],
+                        in[i + 12],
+                        in[i + 13],
+                        in[i + 14],
+                        in[i + 15]
+              );
+        }
+}
+
+char *nlm_crypto_cipher_alg_get_name(unsigned int cipher_alg)
+{
+	if (cipher_alg >= NLM_CIPHER_MAX)
+		return str_cipher_alg[NLM_CIPHER_MAX];
+	else 
+		return str_cipher_alg[cipher_alg];
+}
+
+char *nlm_crypto_cipher_mode_get_name(unsigned int cipher_mode)
+{
+	if (cipher_mode >= NLM_CIPHER_MODE_MAX)
+		return str_cipher_mode[NLM_CIPHER_MODE_MAX];
+	else 
+		return str_cipher_mode[cipher_mode];
+}
+
+char *nlm_crypto_auth_alg_get_name(unsigned int auth_alg)
+{
+	if (auth_alg >= NLM_HASH_MAX)
+		return str_auth_alg[NLM_HASH_MAX];
+	else 
+		return str_auth_alg[auth_alg];
+}
+
+char *nlm_crypto_auth_mode_get_name(unsigned int auth_mode)
+{
+	if (auth_mode >= NLM_HASH_MODE_MAX)
+		return str_auth_mode[NLM_HASH_MODE_MAX];
+	else 
+		return str_auth_mode[auth_mode];
+}
+
+void print_crypto_msg_desc(uint64_t entry1, uint64_t entry2, uint64_t entry3)
+{
+
+        printk("Security Message Descriptor 0: 0x%lx\n", entry1);
+        printk("Security Message Descriptor 1: 0x%lx\n", entry2);
+        printk("Security Message Descriptor 2: 0x%lx\n", entry3);
+
+
+        printk("Free descriptor response destination : 0x%llx  \n", xtract_bits(entry1, 48, 16));
+        printk("Use designer freeback : 0x%llx  \n", xtract_bits(entry1, 45, 1));
+        printk("cipher key length (in dwords) : 0x%llx  \n", xtract_bits(entry1, 40, 5));
+        printf("Control desc cacheline addr : 0x%llx  \n", xtract_bits(entry1, 0, 34));
+        if (xtract_bits(entry1, 45, 1)) {
+                printf("Designer freeback length (actual len - 1): 0x%llx  \n", xtract_bits(entry1, 46, 2));
+        }
+
+
+        printf("Arc4 load state : 0x%llx  \n", xtract_bits(entry2, 63, 1));
+        printf("Hash key length (in dwords) : 0x%llx  \n", xtract_bits(entry2, 56, 5));
+        printf("Pkt desc length (in multiple of 16 bytes - 1): 0x%llx  \n", xtract_bits(entry2, 43, 12));
+        printf("Pkt desc cacheline addr : 0x%llx  \n", xtract_bits(entry2, 0, 34));
+
+        printf("Software Scratch Pad : 0x%llx  \n", xtract_bits(entry3, 0, 34));
+}
+void print_cntl_instr(uint64_t cntl_desc)
+{
+	unsigned int tmp;
+	char *x;
+	char s[NLM_CRYPTO_MAX_STR_LEN];
+
+	printf("control description: 0x%016llx\n", (unsigned long long)cntl_desc);
+	printf("HMac = 0x%llx  \n", xtract_bits(cntl_desc, 61, 1));
+//	printk("Pad Hash = 0x%llx  \n", xtract_bits(cntl_desc, 62, 1));
+	/* Check cipher, hash type and mode b4 printing */
+	tmp = xtract_bits(cntl_desc, 52, 8);
+	x = nlm_crypto_auth_alg_get_name(tmp);
+	strncpy(s, x, NLM_CRYPTO_MAX_STR_LEN);
+	printf("Hash Type = 0x%llx(%s)  \n", xtract_bits(cntl_desc, 52, 8), s);
+	tmp = xtract_bits(cntl_desc, 43, 8);
+	x = nlm_crypto_auth_mode_get_name(tmp);
+	strncpy(s, x, NLM_CRYPTO_MAX_STR_LEN);
+	printf("Hash Mode = 0x%llx(%s)  \n", xtract_bits(cntl_desc, 43, 8), s);
+	tmp = xtract_bits(cntl_desc, 34, 8);
+	x = nlm_crypto_cipher_alg_get_name(tmp);
+	strncpy(s, x, NLM_CRYPTO_MAX_STR_LEN);
+	printf("Cipher Type = 0x%llx(%s)  \n", xtract_bits(cntl_desc, 34, 8), s);
+	tmp = xtract_bits(cntl_desc, 25, 8);
+	x = nlm_crypto_cipher_mode_get_name(tmp);
+	strncpy(s, x, NLM_CRYPTO_MAX_STR_LEN);
+	printf("Cipher Mode = 0x%llx(%s)  \n", xtract_bits(cntl_desc, 25, 8), s);
+
+
+	if (xtract_bits(cntl_desc, 34, 8) == NLM_CIPHER_ARC4) {
+		printf("Arc4 cipher key byte count= 0x%llx  \n", xtract_bits(cntl_desc, 18, 5));
+		printf("Arc4 key init = 0x%llx  \n", xtract_bits(cntl_desc, 17, 1));
+	}
+
+}
+struct pkt_desc_src_dst {
+	uint64_t pkt_desc4;
+	uint64_t pkt_desc5;
+};
+
+struct designer_desc{
+	uint64_t desc0;
+	uint64_t desc1;
+	uint64_t desc2;
+	uint64_t desc3;
+};
+
+
+void print_pkt_desc(struct nlm_crypto_pkt_param  *pkt_desc, int index)
+{
+	printf("Packet desc address = %p\n",pkt_desc);
+	printf("Packet Descriptor 0: 0x%016llx\n", (unsigned long long)pkt_desc->desc0);
+	printf("Packet Descriptor 1: 0x%016llx\n", (unsigned long long)pkt_desc->desc1);
+	printf("Packet Descriptor 2: 0x%016llx\n", (unsigned long long)pkt_desc->desc2);
+	printf("Packet Descriptor 3: 0x%016llx\n", (unsigned long long)pkt_desc->desc3);
+
+	printf("\nPacket Descriptor 0\n");
+	printf("TLS protocol = 0x%llx  \n", xtract_bits(pkt_desc->desc0, 63, 1));
+	printf("Hash source(0-plain, 1-encrypted text) = 0x%llx  \n", xtract_bits(pkt_desc->desc0, 62, 1));
+	printf("Hash output l3 alloc = 0x%llx  \n", xtract_bits(pkt_desc->desc0, 60, 1));
+	printf("Encrypt(1)/Decrypt(0)= 0x%llx  \n", xtract_bits(pkt_desc->desc0, 59, 1));
+	printf("IV length = 0x%llx  \n", xtract_bits(pkt_desc->desc0, 41, 16));
+	printf("Hash Dest addr = 0x%llx \n", xtract_bits(pkt_desc->desc0, 0, 39));
+
+	printf("\nPacket Descriptor 1\n");
+	printf("Cipher length = 0x%llx \n", xtract_bits(pkt_desc->desc1, 32, 32));
+	printf("Hash length = 0x%llx  \n", xtract_bits(pkt_desc->desc1, 0, 32));
+	printf("IV Offset = 0x%llx  \n", xtract_bits(pkt_desc->desc2, 45, 17));
+
+	printf("\nPacket Descriptor 2\n");
+	printf("Cipher bit count = 0x%llx  \n", xtract_bits(pkt_desc->desc2, 42, 3));
+	printf("Cipher Offset = 0x%llx  \n", xtract_bits(pkt_desc->desc2, 22, 16));
+	printf("Hash bit count = 0x%llx  \n", xtract_bits(pkt_desc->desc2, 19, 3));
+	printf("Hash clobber = 0x%llx  \n", xtract_bits(pkt_desc->desc2, 18, 1));
+	printf("Hash Offset = 0x%llx  \n", xtract_bits(pkt_desc->desc2, 0, 16));
+
+
+	printf("\nPacket Descriptor 3\n");
+	printf("designer fb dest id = 0x%llx  \n", xtract_bits(pkt_desc->desc3, 48, 16));
+	printf("tag length = 0x%llx  \n", xtract_bits(pkt_desc->desc3, 11, 16));
+
+	printf("arc4 sbox l3 alloc = 0x%llx  \n", xtract_bits(pkt_desc->desc3, 8, 1));
+	printf("arc4 save box = 0x%llx  \n", xtract_bits(pkt_desc->desc3, 6, 1));
+	printf("hmac ext pad key = 0x%llx  \n", xtract_bits(pkt_desc->desc3, 5, 1));
+
+        int i;
+	unsigned long  phys;
+	void * virt;
+	
+
+        for (i=0; i < index; i++) {
+                printf("Packet Descriptor frag src %d: 0x%016llx\n", i, (unsigned long long)pkt_desc->segment[i][0]);
+                printf("Packet Descriptor frag dst %d: 0x%016llx\n", i, (unsigned long long)pkt_desc->segment[i][1]);
+		phys = xtract_bits(pkt_desc->segment[i][0], 0,40);
+		virt = phys_to_virt(phys);
+		hex_dump("src \n",virt, 30);
+		printk("virtual is %p and phys is %lx\n",virt,phys);
+
+
+                printf("frag src length = 0x%llx  \n", xtract_bits(pkt_desc->segment[i][0], 48, 16));
+                printf("frag src = 0x%llx \n", xtract_bits(pkt_desc->segment[i][0], 0, 40));
+
+                printf("frag dest length = 0x%llx \n", xtract_bits(pkt_desc->segment[i][1], 48, 16));
+                printf("cipher output l3 alloc = 0x%llx \n", xtract_bits(pkt_desc->segment[i][1], 46, 1));
+                printf("cipher output write clobber = 0x%llx \n", xtract_bits(pkt_desc->segment[i][1], 41, 1));
+                printf("frag dest = 0x%llx \n", xtract_bits(pkt_desc->segment[i][1], 0, 40));
+        }
+}
+#endif
+    static void
+reset_crypto_stats(void)
+{
+    int i, j;
+    for (i = 0; i < MAX_CPU; i++) {
+	for (j = 0; j < ENC_MAX_STAT; j++) {
+		crypto_stat[i].enc[j] = 0;
+		crypto_stat[i].enc_tbytes[j] = 0;
+	}
+	for (j = 0; j < AUTH_MAX_STAT; j++) {
+		crypto_stat[i].auth[j] = 0;
+		crypto_stat[i].auth_tbytes[j] = 0;
+	}
+		
+    }
+
+}
+
+int
+crypto_get_fb_vc(void)
+{
+    int cpu;
+    extern int sae_rx_vc;
+
+    cpu = hard_smp_processor_id();	//processor_id();
+    cpu = cpu * 4 + sae_rx_vc;
+
+    return cpu;
+}
+
+static const struct file_operations xlp_sae_fops = {
+    .owner = THIS_MODULE,
+    .open = xlp_sae_open,
+    .release = xlp_sae_release,
+};
+
+/* Note that nobody ever sets xlp_sae_busy... */
+    static int
+xlp_sae_open(struct inode *inode, struct file *file)
+{
+    TRACE_TEXT("(xlp_sae_open");
+    return 0;
+}
+
+    static int
+xlp_sae_release(struct inode *inode, struct file *file)
+{
+    TRACE_TEXT("(xlp_sae_release");
+
+    return 0;
+}
+
+    static void
+nlm_xlp_sae_msgring_handler(uint32_t vc, uint32_t src_id,
+	uint32_t size, uint32_t code,
+	uint64_t msg0, uint64_t msg1,
+	uint64_t msg2, uint64_t msg3, void *data)
+{
+	struct nlm_async_crypto *async = (struct nlm_async_crypto *)(msg0);
+	if(async)	
+		async->callback(async, msg1);
+}
+
+static int
+nlm_crypto_read_stats_proc(char *page, char **start, off_t off, int count,
+                       int *eof, void *data)
+{
+        int len = 0;
+	int i,j;
+	uint64_t  cnt;
+	off_t begin = 0;
+	uint64_t enc_tp[ENC_MAX_STAT];
+	uint64_t auth_tp[AUTH_MAX_STAT];
+	uint64_t enc_tb[ENC_MAX_STAT];
+	uint64_t auth_tb[AUTH_MAX_STAT];
+
+	len += sprintf(page + len, "\t\tPkt\t\tTotal Bytes\n");
+
+	if (!proc_pos_check(&begin, &len, off, count))
+		goto out;
+	for(j =0 ;j <= ENC_MAX_STAT ; j++) {
+		enc_tp[j] = 0;	
+		enc_tb[j] = 0;
+		for(i = 0; i < MAX_CPU; i++)  {
+			enc_tp[j] = enc_tp[j] + crypto_stat[i].enc[j];
+			enc_tb[j] = enc_tb[j] + crypto_stat[i].enc_tbytes[j];
+		}
+			
+	}
+
+	len += sprintf(page + len,"DES-CBC\t\t%lld\t\t%lld\nTDES-CBC\t%lld\t\t%lld\nAES128-CBC\t%lld\t\t%lld\n",
+			enc_tp[DES_CBC_STAT],enc_tb[DES_CBC_STAT],enc_tp[TDES_CBC_STAT],enc_tb[TDES_CBC_STAT],
+			enc_tp[AES128_CBC_STAT],enc_tb[AES128_CBC_STAT]);
+	if (!proc_pos_check(&begin, &len, off, count))
+		goto out;
+
+	len += sprintf(page + len,"AES192-CBC\t%lld\t\t%lld\nAES256-CBC\t%lld\t\t%lld\n",
+		enc_tp[AES192_CBC_STAT],enc_tb[AES192_CBC_STAT],enc_tp[AES256_CBC_STAT],enc_tb[AES256_CBC_STAT]);
+	if (!proc_pos_check(&begin, &len, off, count))
+		goto out;
+
+	for(j =0 ;j < AUTH_MAX_STAT; j++) {
+		auth_tp[j] = 0;
+		auth_tb[j] = 0;
+		for(i = 0; i < MAX_CPU; i++) { 
+			auth_tp[j] +=  crypto_stat[i].auth[j];
+			auth_tb[j] += crypto_stat[i].auth_tbytes[j];
+		}
+	}
+
+	len  += sprintf(page + len,"MD5\t\t%lld\t\t%lld\nH-SHA1\t\t%lld\t\t%lld\nH-SHA256\t%lld\t\t%lld\n",
+				    auth_tp[MD5_STAT],auth_tb[MD5_STAT],auth_tp[H_SHA1_STAT],auth_tb[H_SHA1_STAT],auth_tp[H_SHA256_STAT],auth_tb[H_SHA256_STAT]);
+	if (!proc_pos_check(&begin, &len, off, count))
+		 goto out;
+
+	len  += sprintf(page + len,"AES128-XCBC\t%lld\t\t%lld\nAES198-XCBC\t%lld\t\t%lld\nAES256-XCBC\t%lld\t\t%lld\n",
+		auth_tp[AES128_XCBC_STAT],auth_tb[AES128_XCBC_STAT],auth_tp[AES192_XCBC_STAT],auth_tb[AES192_XCBC_STAT],auth_tp[AES256_XCBC_STAT],auth_tb[AES256_XCBC_STAT]);
+
+	if (!proc_pos_check(&begin, &len, off, count))
+		 goto out;
+
+        *eof = 1;
+
+      out:
+        *start = page + (off - begin);
+        len -= (off - begin);
+        if (len > count)
+                len = count;
+        if (len < 0)
+                len = 0;
+
+        return len;
+
+}
+
+int
+nlm_crypto_init(void)
+{
+    int ret = 0;
+    struct proc_dir_entry *entry = NULL;
+
+    entry = create_proc_read_entry("crypto_stats", 0, nlm_root_proc,
+		    nlm_crypto_read_stats_proc,
+		    0);
+
+    if(entry == NULL) {
+	    printk("%s:%d failed creating proc stats entry.\n",
+			    __FUNCTION__, __LINE__);
+	    ret = -EINVAL;
+    }
+
+    if (register_xlp_msgring_handler
+		    (XLP_MSG_HANDLE_CRYPTO, nlm_xlp_sae_msgring_handler, NULL)) {
+	    panic("can't register msgring handler for TX_STN_GMAC0");
+    }
+    reset_crypto_stats();
+
+    return ret;
+}
+
+    static int __init
+xlp_sae_init(void)
+{
+    extern int sae_rx_vc, sae_rx_sync_vc;
+    printk(KERN_ERR ",\n XLP SAE/Crypto Initialization \n");
+	printk("sae_rx_vc %d\n",sae_rx_vc);
+	printk("sae_rx_sync_vc %d\n",sae_rx_sync_vc);
+
+    xlp_sae_major = register_chrdev(0, "NLM_XLP_SAE", &xlp_sae_fops);
+    if (xlp_sae_major < 0) {
+	printk(KERN_ERR "XLP_SAE - cannot register device\n");
+	return xlp_sae_major;
+    }
+    //  printk (KERN_ERR ",XLP SAE MAJOR %d\n", xlp_sae_major);
+
+    nlm_crypto_init();
+    xlp_crypt_alg_init();
+    xlp_auth_alg_init();
+    xlp_aead_alg_init();
+
+    return 0;
+}
+
+    static void __exit
+xlp_sae_cleanup(void)
+{
+    xlp_crypt_alg_fini();
+    xlp_auth_alg_fini();
+    xlp_aead_alg_fini();
+    unregister_chrdev(xlp_sae_major, "NLM_XLP_SAE");
+}
+
+module_init(xlp_sae_init);
+module_exit(xlp_sae_cleanup);
+MODULE_DESCRIPTION("XLP Hardware crypto support for AES/DES/3DES/SHA/MD5 .");
+MODULE_LICENSE("GPL");
+MODULE_VERSION("0.1");
+MODULE_AUTHOR("Alok Agrawat");
diff --git a/drivers/crypto/sae/nlm_enc.c b/drivers/crypto/sae/nlm_enc.c
new file mode 100755
index 0000000..d90b013
--- /dev/null
+++ b/drivers/crypto/sae/nlm_enc.c
@@ -0,0 +1,457 @@
+/***********************************************************************
+Copyright 2003-2010 Netlogic Microsystems (“Netlogic”). All rights
+reserved.
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are
+met:
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+*****************************#NETL_2#********************************/
+#include <crypto/scatterwalk.h>
+/*#include <crypto/algapi.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+*/
+#include <linux/crypto.h>
+#include <crypto/aes.h>
+#include <crypto/des.h>
+#include <crypto/ctr.h>
+#include "nlmcrypto.h"
+#include <nlm_hal_fmn.h>
+
+/*#include <linux/pci.h>
+#include <linux/pci_ids.h>
+#include <asm/io.h>
+*/
+#include "nlm_async.h"
+#undef NLM_CRYPTO_DEBUG
+
+
+#define XLP_CRYPT_PRIORITY	300
+
+
+struct nlm_enc_ctx {
+	struct nlm_crypto_pkt_ctrl ctrl; 
+	uint16_t stat;
+};
+/* mem utilisation of CTX_SIZE */
+#define MAX_FRAGS               18
+#define CTRL_DESC_SIZE          (sizeof(struct nlm_enc_ctx) + 64)
+#define DES3_CTRL_DESC_SIZE     (2*CTRL_DESC_SIZE + 2*64)
+
+
+/* mem utilisation of req mem */
+
+#define PACKET_DESC_SIZE        (64 + sizeof(struct nlm_crypto_pkt_param) + MAX_FRAGS*(2*64) + 64 + sizeof(struct nlm_async_crypto) + 64)
+#define NLM_CRYPTO_PKT_PARAM_OFFSET(addr)       (((unsigned long)addr + 64) & ~0x3fULL)
+#define NLM_ASYNC_PTR_PARAM_OFFSET(addr)        (((unsigned long)(addr + 64 + sizeof(struct nlm_crypto_pkt_param) + MAX_FRAGS*(2*64)) + 64) & ~0x3fULL)
+#define NLM_IV_OFFSET(addr)			((unsigned long)addr + (PACKET_DESC_SIZE - 64))
+
+
+
+extern uint32_t nlm_hal_send_msg3(uint32_t dst, uint32_t code, uint64_t data0, uint64_t data1, uint64_t data2);
+extern __inline__ uint32_t nlm_hal_recv_msg2(uint32_t dst, uint32_t *src, uint32_t *size, uint32_t *code, uint64_t *data0, uint64_t *data1);
+#ifdef NLM_CRYPTO_DEBUG
+extern void print_cntl_instr(uint64_t cntl_desc);
+extern void print_crypto_msg_desc(uint64_t entry1, uint64_t entry2, uint64_t entry3);
+extern void print_pkt_desc(struct nlm_crypto_pkt_param * pkt_param, int index);
+#endif
+
+extern int cipher_mode_iv_len[NLM_CIPHER_MAX][NLM_CIPHER_MODE_MAX];
+extern struct nlm_crypto_stat crypto_stat[MAX_CPU];
+
+
+
+//extern void print_cntl_instr(uint64_t cntl_desc);
+static void enc_session_cleanup(struct crypto_tfm *tfm)
+{
+}
+
+static int enc_cra_init(struct crypto_tfm *tfm)
+{ 
+	tfm->crt_ablkcipher.reqsize = PACKET_DESC_SIZE; //reqsize of 512 bytes for packet desc
+	return 0;
+}
+
+static struct nlm_enc_ctx * nlm_crypto_ablkcipher_ctx(struct crypto_ablkcipher *tfm)
+{
+	return (struct  nlm_enc_ctx *)(((unsigned long)((uint8_t *)crypto_ablkcipher_ctx(tfm) + 63 )) & ~(0x3f));
+} 
+
+
+
+static int
+xlp_setkey(struct crypto_ablkcipher *tfm, const u8 * in_key, unsigned int len, uint32_t cipher_alg, uint32_t cipher_mode,uint16_t stat)
+{
+	struct nlm_enc_ctx * nlm_ctx = nlm_crypto_ablkcipher_ctx(tfm); 
+	
+	nlm_crypto_fill_pkt_ctrl(&nlm_ctx->ctrl,0,0,0,cipher_alg,cipher_mode,0,(unsigned char*)in_key,len,0,0);
+	crypto_ablkcipher_crt(tfm)->ivsize = cipher_mode_iv_len[cipher_alg][ cipher_mode];
+	nlm_ctx->stat = stat;
+
+	return 0;
+
+}
+static int
+xlp_setkey_des3(struct crypto_ablkcipher *tfm, const u8 * in_key, unsigned int len, uint32_t cipher_alg, uint32_t cipher_mode, uint16_t stat)
+{
+	struct nlm_enc_ctx * nlm_ctx = (struct  nlm_enc_ctx *)(( unsigned long )(( uint8_t *)nlm_crypto_ablkcipher_ctx(tfm) + CTRL_DESC_SIZE + 63) & ~(0x3f)); 
+	uint64_t key[3] ;
+	
+	memcpy(key,&in_key[16],8);
+        memcpy(&key[1],&in_key[8],8);
+        memcpy(&key[2],&in_key[0],8);
+
+
+	nlm_crypto_fill_pkt_ctrl(&nlm_ctx->ctrl,0,0,0,cipher_alg,cipher_mode,0,(unsigned char*)&key[0],len,0,0);
+	crypto_ablkcipher_crt(tfm)->ivsize = cipher_mode_iv_len[cipher_alg][ cipher_mode];
+	nlm_ctx->stat = stat;
+
+	return 0;
+
+}
+
+
+static int
+xlp_des3_setkey(struct crypto_ablkcipher *tfm, const u8 * in_key, unsigned int len)
+{
+	uint32_t cipher_alg;
+	u32 flags = 0;
+
+	switch (len) {
+	case DES3_EDE_KEY_SIZE:
+	        cipher_alg = NLM_CIPHER_3DES;
+		break;
+	default:
+		printk(KERN_WARNING "[%s]: Cannot handle keylen = %d\n",
+		       __FUNCTION__, len);
+		flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
+		crypto_ablkcipher_set_flags(tfm,flags);
+		return -EINVAL;
+	}
+	xlp_setkey(tfm, in_key, len, cipher_alg, NLM_CIPHER_MODE_CBC,TDES_CBC_STAT);
+	return xlp_setkey_des3(tfm, in_key, len, cipher_alg, NLM_CIPHER_MODE_CBC, TDES_CBC_STAT);
+}
+
+static int
+xlp_des_setkey(struct crypto_ablkcipher *tfm, const u8 * in_key, unsigned int len)
+{
+	uint32_t cipher_alg;
+	u32 flags = 0;
+
+	switch (len) {
+	case DES_KEY_SIZE:
+	        cipher_alg = NLM_CIPHER_DES;
+		break;
+	default:
+		printk(KERN_WARNING "[%s]: Cannot handle keylen = %d\n",
+		       __FUNCTION__, len);
+		flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
+		crypto_ablkcipher_set_flags(tfm, flags);
+		return -EINVAL;
+	}
+	return xlp_setkey(tfm, in_key, len, cipher_alg,NLM_CIPHER_MODE_CBC,DES_CBC_STAT);
+}
+
+static int
+xlp_aes_setkey(struct crypto_ablkcipher *tfm, const u8 *in_key, 
+		unsigned int len, uint32_t mode)
+{
+	uint32_t cipher_alg;
+	uint16_t stat;
+	u32 flags = 0;
+
+	switch (len) {
+	case 16:
+	        cipher_alg = NLM_CIPHER_AES128;
+		stat = AES128_CBC_STAT;
+		break;
+	case 24:
+		cipher_alg = NLM_CIPHER_AES192;
+		stat = AES192_CBC_STAT;
+		break;
+	case 32:
+		cipher_alg = NLM_CIPHER_AES256;
+		stat = AES256_CBC_STAT;
+		break;
+	default:
+		printk(KERN_WARNING "[%s]: Cannot handle keylen = %d\n",
+		       __FUNCTION__, len);
+		flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
+		crypto_ablkcipher_set_flags(tfm,flags);
+		return -EINVAL;
+	}
+	return xlp_setkey(tfm, in_key, len, cipher_alg, mode, stat);
+}
+
+static int xlp_cbc_aes_setkey(struct crypto_ablkcipher *tfm, const u8 *key,
+					unsigned int keylen)
+{
+	return xlp_aes_setkey(tfm,key,keylen,NLM_CIPHER_MODE_CBC);
+}
+
+void enc_request_callback(struct nlm_async_crypto *async, uint64_t msg1 )
+{
+	struct crypto_async_request * base = (struct crypto_async_request *)async->args; 
+	int err =0;
+	int cpu = nlm_processor_id();
+	int stat = async->stat;
+
+	if (msg1 & 0x7ff80) {
+		printk("\n Error: entry1 is %llx",msg1);
+		err = -EIO;
+		base->complete(base, err);
+	}
+	crypto_stat[cpu].enc[stat]++;
+	crypto_stat[cpu].enc_tbytes[stat]+= async->bytes;
+	base->complete(base, err);
+}
+static int
+xlp_crypt(struct ablkcipher_request *req, unsigned int enc, int iv_size, struct nlm_crypto_pkt_ctrl *ctrl, uint16_t stat)
+{
+
+	int seg = 0;
+	int i;
+	uint64_t msg0, msg1;
+	struct scatterlist *sg;
+	struct scatter_walk walk;
+	uint8_t *virt;
+	int len;
+	int pktdescsize = 0;
+	
+	unsigned int cipher_len = req->nbytes;
+	struct nlm_crypto_pkt_param * pkt_param = (struct nlm_crypto_pkt_param *) NLM_CRYPTO_PKT_PARAM_OFFSET(ablkcipher_request_ctx(req));
+	struct nlm_async_crypto * async =  (struct nlm_async_crypto *)NLM_ASYNC_PTR_PARAM_OFFSET(ablkcipher_request_ctx(req));;
+	int fb_vc;
+
+	nlm_crypto_fill_cipher_pkt_param(ctrl, pkt_param, enc,0,iv_size,iv_size ,req->nbytes); 
+
+	nlm_crypto_fill_src_seg(pkt_param,seg,(unsigned char *)req->info,iv_size);
+	nlm_crypto_fill_dst_seg(pkt_param,seg,(unsigned char *)req->info,iv_size);
+	seg++;
+
+	if ( req->src == req->dst) {
+		for (sg = req->src; cipher_len > 0; sg = scatterwalk_sg_next(sg)) {
+			len = min(cipher_len, sg->length);
+			scatterwalk_start(&walk, sg);
+			virt = page_address(scatterwalk_page(&walk)) + offset_in_page(walk.offset);
+			nlm_crypto_fill_src_seg(pkt_param,seg,virt,len);
+			seg = nlm_crypto_fill_dst_seg(pkt_param,seg,virt,len);
+			cipher_len -= len;
+		}
+	}
+	else {
+		int nr_src_frags = 0;
+		int nr_dst_frags = 0;
+		int index = 0;
+		for (sg = req->src,index = seg;cipher_len > 0; sg = scatterwalk_sg_next(sg)) {
+			len = min(cipher_len, sg->length);
+			scatterwalk_start(&walk, sg);
+			virt = page_address(scatterwalk_page(&walk)) + offset_in_page(walk.offset);
+			index = nlm_crypto_fill_src_seg(pkt_param,index,virt,len);
+			cipher_len -= len;
+		}
+		nr_src_frags = index;
+		cipher_len = req->nbytes;
+		for (sg = req->dst, index = seg ;cipher_len > 0; sg = scatterwalk_sg_next(sg)) {
+			len = min(cipher_len, sg->length);
+			scatterwalk_start(&walk, sg);
+			virt = page_address(scatterwalk_page(&walk)) + offset_in_page(walk.offset);
+			index = nlm_crypto_fill_dst_seg(pkt_param,index,virt,len);
+			cipher_len -= len;
+		}
+		nr_dst_frags = index;
+
+		if (nr_src_frags > nr_dst_frags) {
+			for (i = 0; i < nr_src_frags - nr_dst_frags; i++)
+				pkt_param->segment[seg + nr_dst_frags + i][1] = 0ULL;
+			seg = nr_src_frags;
+		}
+		else  { 
+			if (nr_src_frags < nr_dst_frags) {
+				for (i = 0; i < nr_dst_frags - nr_src_frags; i++)
+					pkt_param->segment[seg + nr_src_frags + i][0] = 0ULL;
+			}
+			seg = nr_dst_frags;
+		}
+		
+	}
+
+	pktdescsize = 32 + seg * 16;
+	fb_vc = crypto_get_fb_vc(); 
+
+	msg0 = nlm_crypto_form_pkt_fmn_entry0(fb_vc, 0, 0, ctrl->cipherkeylen,
+								virt_to_phys(ctrl));
+	msg1 = nlm_crypto_form_pkt_fmn_entry1(0,ctrl->hashkeylen, pktdescsize,
+				virt_to_phys(pkt_param));
+#ifdef NLM_CRYPTO_DEBUG
+	print_crypto_msg_desc(msg0,msg1,0xdeadbeef);
+	print_pkt_desc(pkt_param,seg);
+	print_cntl_instr(ctrl);
+#endif
+	async->callback = &enc_request_callback;
+	async->args = &req->base;
+	async->stat = stat; 
+	async->bytes = req->nbytes; 
+	while( nlm_hal_send_msg3(NLM_CRYPTO_VC_BASE, 0 /*code */ , msg0, msg1, (unsigned long )async) != 0 );
+	return -EINPROGRESS;
+
+}
+
+static int
+xlp_3des_cbc_decrypt( struct ablkcipher_request *req)
+{
+      	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);
+	struct nlm_enc_ctx * nlm_ctx = (struct  nlm_enc_ctx *)(( unsigned long )(( uint8_t *)nlm_crypto_ablkcipher_ctx(tfm) + CTRL_DESC_SIZE + 63) & ~(0x3f)); 
+	return xlp_crypt(req, 0, DES3_EDE_BLOCK_SIZE,&nlm_ctx->ctrl,nlm_ctx->stat);
+}
+
+static int
+xlp_3des_cbc_encrypt( struct ablkcipher_request *req )
+{
+      	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);
+	struct nlm_enc_ctx *nlm_ctx = nlm_crypto_ablkcipher_ctx(tfm); 
+	return xlp_crypt(req, 1, DES3_EDE_BLOCK_SIZE,&nlm_ctx->ctrl,nlm_ctx->stat);
+}
+
+static int
+xlp_cbc_decrypt( struct ablkcipher_request *req )
+{
+	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);
+	int iv_size = crypto_ablkcipher_ivsize(tfm);
+	struct nlm_enc_ctx *nlm_ctx = nlm_crypto_ablkcipher_ctx(tfm); 
+	return xlp_crypt(req, 0, iv_size,&nlm_ctx->ctrl,nlm_ctx->stat);
+}
+
+static int
+xlp_cbc_encrypt( struct ablkcipher_request *req )
+{
+	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);
+	int iv_size = crypto_ablkcipher_ivsize(tfm);
+	struct nlm_enc_ctx *nlm_ctx = nlm_crypto_ablkcipher_ctx(tfm); 
+	return xlp_crypt(req, 1, iv_size,&nlm_ctx->ctrl, nlm_ctx->stat);
+}
+
+static struct crypto_alg xlp_cbc_aes_alg = {
+	.cra_name = "cbc(aes)",
+	.cra_driver_name = "cbc-aes-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER,
+	.cra_blocksize = AES_BLOCK_SIZE,
+	.cra_ctxsize = CTRL_DESC_SIZE,
+	.cra_alignmask = 15,
+	.cra_type = &crypto_ablkcipher_type,
+	.cra_module = THIS_MODULE,
+	.cra_init = enc_cra_init,
+	.cra_exit = enc_session_cleanup,
+	.cra_list = LIST_HEAD_INIT(xlp_cbc_aes_alg.cra_list),
+	.cra_u = {
+		.ablkcipher = {
+			.min_keysize = AES_MIN_KEY_SIZE,
+			.max_keysize = AES_MAX_KEY_SIZE,
+			.setkey = xlp_cbc_aes_setkey,
+			.encrypt = xlp_cbc_encrypt,
+			.decrypt = xlp_cbc_decrypt,
+			.ivsize = AES_BLOCK_SIZE,
+		}
+	}
+};
+
+static struct crypto_alg xlp_cbc_des_alg = {
+	.cra_name = "cbc(des)",
+	.cra_driver_name = "cbc-des-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER,
+	.cra_blocksize = DES_BLOCK_SIZE,
+	.cra_ctxsize = CTRL_DESC_SIZE,
+	.cra_alignmask = 15,
+	.cra_type = &crypto_ablkcipher_type,
+	.cra_module = THIS_MODULE,
+	.cra_init = enc_cra_init,
+	.cra_exit = enc_session_cleanup,
+	.cra_list = LIST_HEAD_INIT(xlp_cbc_des_alg.cra_list),
+	.cra_u = {
+		.ablkcipher = {
+			.min_keysize = DES_KEY_SIZE,
+			.max_keysize = DES_KEY_SIZE,
+			.setkey = xlp_des_setkey,
+			.encrypt = xlp_cbc_encrypt,
+			.decrypt = xlp_cbc_decrypt,
+			.ivsize = DES_BLOCK_SIZE,
+		}
+	}
+};
+
+static struct crypto_alg xlp_cbc_des3_alg = {
+	.cra_name = "cbc(des3_ede)",
+	.cra_driver_name = "cbc-des3-ede-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER,
+	.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+	.cra_ctxsize = DES3_CTRL_DESC_SIZE,
+	.cra_alignmask = 15,
+	.cra_type = &crypto_ablkcipher_type,
+	.cra_module = THIS_MODULE,
+	.cra_init = enc_cra_init,
+	.cra_exit = enc_session_cleanup,
+	.cra_list = LIST_HEAD_INIT(xlp_cbc_des3_alg.cra_list),
+	.cra_u = {
+		.ablkcipher = {
+			.min_keysize = DES3_EDE_KEY_SIZE,
+			.max_keysize = DES3_EDE_KEY_SIZE,
+			.setkey = xlp_des3_setkey,
+			.encrypt = xlp_3des_cbc_encrypt,
+			.decrypt = xlp_3des_cbc_decrypt,
+			.ivsize = DES3_EDE_BLOCK_SIZE,
+		}
+	}
+};
+
+int xlp_crypt_alg_init(void)
+{
+	int ret = 0;
+	int no_of_alg_registered = 0;
+	ret = crypto_register_alg(&xlp_cbc_des3_alg);
+	if (ret) {
+		goto end;
+	}
+	no_of_alg_registered++;
+	ret = crypto_register_alg(&xlp_cbc_des_alg);
+	if (ret) {
+		goto end;
+	}
+	no_of_alg_registered++;
+	ret = crypto_register_alg(&xlp_cbc_aes_alg);
+	if (ret) {
+		goto end;
+	}
+	no_of_alg_registered++;
+	printk(KERN_NOTICE "Using XLP hardware for AES, DES, 3DES algorithms.\n");
+end:
+	return 0;
+}
+
+void
+xlp_crypt_alg_fini(void) {
+	crypto_unregister_alg(&xlp_cbc_des3_alg);
+	crypto_unregister_alg(&xlp_cbc_des_alg);
+	crypto_unregister_alg(&xlp_cbc_aes_alg);
+}
+
+EXPORT_SYMBOL(xlp_crypt_alg_init);
+EXPORT_SYMBOL(xlp_crypt_alg_fini);
diff --git a/drivers/crypto/sae/nlmcrypto_ifc.h b/drivers/crypto/sae/nlmcrypto_ifc.h
new file mode 100644
index 0000000..d0d89b8
--- /dev/null
+++ b/drivers/crypto/sae/nlmcrypto_ifc.h
@@ -0,0 +1,46 @@
+/*********************************************************************
+
+  Copyright 2003-2010 Netlogic Microsystem, Inc. ("Netlogic"). All rights
+  reserved.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+  1. Redistributions of source code must retain the above copyright
+  notice, this list of conditions and the following disclaimer.
+  2. Redistributions in binary form must reproduce the above copyright
+  notice, this list of conditions and the following disclaimer in
+  the documentation and/or other materials provided with the
+  distribution.
+
+  THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems, Inc. ``AS IS'' AND
+  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL RMI OR CONTRIBUTORS BE LIABLE
+  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+  INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+  CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+  THE POSSIBILITY OF SUCH DAMAGE.
+
+  *****************************#NLM_2#**********************************/
+#ifndef _NLM_CRYPTO_IFC_H
+#define _NLM_CRYPTO_IFC_H
+
+extern void *linuxu_shvaddr;
+extern unsigned long long linuxu_shoff ;
+
+static inline unsigned long long crypto_virt_to_phys(void *vaddr)
+{
+	return virt_to_phys(vaddr);
+}
+
+static inline void *crypto_phys_to_virt(unsigned long long paddr)
+{
+	return phys_to_virt(paddr);
+}
+
+#endif
-- 
1.7.1

