From 1f361211fe8c36e9e0b7009d2cdccbf525ba32a9 Mon Sep 17 00:00:00 2001
From: Vikas Gupta <vikas.gupta@broadcom.com>
Date: Mon, 10 Nov 2014 12:14:35 +0530
Subject: nae : Fix to handle low mem in NAE driver.

[Based on SDK 3.2]
Signed-off-by: Quanyang Wang <quanyang.wang@windriver.com>
Signed-off-by: Nam Ninh <nam.ninh@windriver.com>

diff --git a/drivers/net/ethernet/broadcom/nae/xlpge.h b/drivers/net/ethernet/broadcom/nae/xlpge.h
index 30e5244..da97546 100644
--- a/drivers/net/ethernet/broadcom/nae/xlpge.h
+++ b/drivers/net/ethernet/broadcom/nae/xlpge.h
@@ -339,6 +339,8 @@ struct dev_data
 #if (LINUX_VERSION_CODE < KERNEL_VERSION(3, 8, 0))
 	struct timecompare compare;
 #endif
+	atomic_t cpu_alloc_fail[NR_CPUS][NLM_NAE_MAX_FREEIN_FIFOS_PER_NODE];
+	struct delayed_work alloc_failed_work[NR_CPUS];
 };
 
 
@@ -348,6 +350,7 @@ struct p2p_desc_mem {
 	uint64_t pad[6];
 };
 
+
 extern void *fdt;
 
 static __inline__ void cpu_halt(void)
@@ -414,6 +417,24 @@ static __inline__ struct sk_buff *nlm_xlp_alloc_skb_atomic(int length, int node)
 	return skb;
 }
 
+static __inline__ struct sk_buff *nlm_xlp_alloc_skb(int length, int node)
+{
+	int offset = 0;
+	gfp_t gfp_mask = GFP_KERNEL;
+	struct sk_buff *skb = __alloc_skb(length + NET_SKB_PAD, gfp_mask, 0,
+						node);
+
+	if (!skb) 
+		return NULL;
+
+	skb_reserve(skb, NET_SKB_PAD);
+
+	/* align the data to the next cache line */
+	offset = ((ulong)skb->data + CACHELINE_SIZE) & ~(CACHELINE_SIZE - 1);
+	skb_reserve(skb, (offset - (ulong)skb->data));
+	return skb;
+}
+
 static __inline__ void mac_put_skb_back_ptr(struct sk_buff *skb, nae_t* naeptr)
 {
 	uint64_t *back_ptr = (uint64_t *)skb->data;
@@ -460,6 +481,14 @@ static __inline__ void dump_buffer(unsigned char *buf, uint32_t len,
 	nae_print(NAE_DBG_TRACE, "%s\n", out);
 }
 
+extern uint32_t cpu_2_normal_frfifo[NLM_MAX_NODES][MAX_NAE_PERNODE][NR_CPUS];
+extern uint32_t cpu_2_jumbo_frfifo[NLM_MAX_NODES][MAX_NAE_PERNODE][NR_CPUS];
+extern int phys_cpu_to_log_map[NLM_MAX_NODES][MAX_NAE_PERNODE][NR_CPUS];
+extern uint32_t lnx_frfifo_jumbo_mask[NLM_MAX_NODES][MAX_NAE_PERNODE];
+extern  uint32_t lnx_frfifo_normal_mask[NLM_MAX_NODES][MAX_NAE_PERNODE];
+extern int enable_jumbo;
+extern void fill_alloc_failed(struct work_struct *work);     
+
 static inline int mac_refill_frin_skb(nae_t* nae_cfg, int cpu, uint64_t paddr,
 			       uint32_t bufsize, int hw_port_id)
 {
@@ -472,10 +501,6 @@ static inline int mac_refill_frin_skb(nae_t* nae_cfg, int cpu, uint64_t paddr,
 	int node = nae_cfg->node;
 	int nae_id = nae_cfg->nae_id;
 	int node_cpu;
-	extern uint32_t cpu_2_normal_frfifo[NLM_MAX_NODES][MAX_NAE_PERNODE][NR_CPUS];
-	extern uint32_t cpu_2_jumbo_frfifo[NLM_MAX_NODES][MAX_NAE_PERNODE][NR_CPUS];
-	extern int phys_cpu_to_log_map[NLM_MAX_NODES][MAX_NAE_PERNODE][NR_CPUS];
-	extern int enable_jumbo;
 
 	node_cpu = phys_cpu_to_log_map[node][nae_id][cpu];
 
@@ -514,32 +539,6 @@ static inline int mac_refill_frin_skb(nae_t* nae_cfg, int cpu, uint64_t paddr,
 	return ret;
 }
 
-static inline int mac_refill_frin_one_buffer(struct net_device *dev, int cpu,
-				      uint32_t truesize)
-{
-	struct dev_data* priv = netdev_priv(dev);
-	struct sk_buff * skb;
-	int buf_size = NLM_RX_ETH_BUF_SIZE;
-	nae_t* nae_cfg = priv->nae;
-	extern int enable_jumbo;
-
-	if (enable_jumbo)
-		if(truesize > NLM_RX_JUMBO_BUF_SIZE)
-			buf_size = NLM_RX_JUMBO_BUF_SIZE;
-
-	skb = nlm_xlp_alloc_skb_atomic(buf_size, priv->node);
-	if (!skb) {
-		nae_print(NAE_DBG_ERROR, "[%s] alloc skb failed\n",__FUNCTION__);
-		panic("panic...");
-		return -ENOMEM;
-	}
-
-	skb->dev = dev;
-
-	mac_put_skb_back_ptr(skb, nae_cfg);
-
-	return mac_refill_frin_skb(nae_cfg, cpu, (uint64_t)virt_to_bus(skb->data), buf_size, priv->hw_port_id);
-}
 
 void nlm_xlp_nae_init(void);
 void nlm_xlp_nae_remove(void);
diff --git a/drivers/net/ethernet/broadcom/nae/xlpge_nae.c b/drivers/net/ethernet/broadcom/nae/xlpge_nae.c
index 4dc376e..d82ec2f 100644
--- a/drivers/net/ethernet/broadcom/nae/xlpge_nae.c
+++ b/drivers/net/ethernet/broadcom/nae/xlpge_nae.c
@@ -36,6 +36,7 @@
 #include <linux/proc_fs.h>
 #include <linux/timer.h>
 #include <linux/kthread.h>
+#include <asm/netlogic/xlp-hal/xlp.h>	
 //#include <nlm_xlp.h>
 //#include <nlm_msgring.h>
 //#include <nlm_hal_fmn.h>
@@ -94,9 +95,6 @@ struct dev_data *lro_flush_priv[NR_CPUS][20];
 int force_tso=0;
 module_param(force_tso, int, 0);
 
-static uint32_t lnx_frfifo_normal_mask[NLM_MAX_NODES][MAX_NAE_PERNODE];
-static uint32_t lnx_frfifo_jumbo_mask[NLM_MAX_NODES][MAX_NAE_PERNODE];
-
 int enable_jumbo = 0;
 module_param(enable_jumbo, int, 0);
 struct p2p_desc_mem p2p_desc_mem[NR_CPUS] __cacheline_aligned;
@@ -1044,7 +1042,6 @@ int get_hw_frfifo_queue_id(int rxnode, nae_t* nae_cfg,
 	 */
 	return nae_cfg->vfbtbl_hw_offset + qid;
 }
-
 /**********************************************************************
  * nlm_xlp_nae_open -  called when bring up a device interface
  * @dev  -  this is per device based function
@@ -1119,7 +1116,6 @@ static int  nlm_xlp_nae_open (struct net_device *dev)
 		else
 			priv->flow_ctrl=0;
 		//nlm_xlp_mac_set_enable(priv, 1);
-		//netsoc_disable_flow_control(priv->nae_port);
 
 	return ret;
 }
@@ -1549,8 +1545,10 @@ int nlm_per_port_nae_init(nae_t* nae_cfg, int port, int num_nae, char * const ma
     priv->compare.num_samples = 0;
     nlm_timecompare_update(&priv->compare,0);
 #endif
-	for (cpu = 0; cpu < NR_CPUS; cpu++)
+	for (cpu = 0; cpu < NR_CPUS; cpu++){
 		per_cpu_netdev[node][cpu][nae_cfg->nae_id][port] = dev;
+		INIT_DELAYED_WORK(&priv->alloc_failed_work[cpu], fill_alloc_failed);
+	}	
 
 	return 0;
 }
@@ -1586,7 +1584,6 @@ int get_num_cpus_per_node(void)
 }
 
 
-
 /**********************************************************************
  * nlm_xlp_nae_init -  xlp_nae device driver init function
  * @dev  -  this is per device based function
diff --git a/drivers/net/ethernet/broadcom/nae/xlpge_rx.c b/drivers/net/ethernet/broadcom/nae/xlpge_rx.c
index 2086aa4..c28312f 100644
--- a/drivers/net/ethernet/broadcom/nae/xlpge_rx.c
+++ b/drivers/net/ethernet/broadcom/nae/xlpge_rx.c
@@ -63,8 +63,100 @@ extern uint32_t last_rcvd_node[NR_CPUS * 8] ____cacheline_aligned;
 extern uint64_t last_rcvd_skb_phys[NR_CPUS * 8] ____cacheline_aligned;
 struct dev_data *last_rcvd_priv[NR_CPUS * 8] ____cacheline_aligned;
 
+void fill_alloc_failed(struct work_struct *work);	
 uint64_t receive_count[NR_CPUS * 8] __cacheline_aligned;
 //#define MACSEC_DEBUG	1
+//
+void fill_alloc_failed(struct work_struct *work)
+{
+	int cpu = hard_smp_processor_id(), node_cpu;
+	unsigned int frq_num, ret, abs_frq_num, buf_size;
+	struct sk_buff * skb;
+	int nodes = nlm_hal_get_num_nodes();
+	int nae_units = get_num_nae_pernode();
+	struct  dev_data *priv = container_of(work, struct dev_data, alloc_failed_work[cpu].work);
+	net_port_t *netport = priv->nae_port;	
+	nae_t* nae = netport->nae;
+	int node = nae->node;
+	int nae_no = nae->nae_id;
+
+
+	for(frq_num=0; frq_num<NLM_NAE_MAX_FREEIN_FIFOS_PER_NODE/*FIX ME in netlib macro should be defined as NLM_NAE_MAX_FREEIN_FIFOS_PER_NAE*/; frq_num++){
+		int bad_allocs = atomic_read(&priv->cpu_alloc_fail[cpu][frq_num]);
+		int bad_cnt = 0;
+		ulong __attribute__ ((unused)) mflags;
+		node_cpu = phys_cpu_to_log_map[node][nae_no][cpu];
+
+		while(bad_allocs !=0){
+			if (bad_allocs < 0) {
+				panic("BUG: bad alloc for cpu[%d] and freq = %d ", cpu, frq_num);	
+			}
+			abs_frq_num = frq_num + nae->frin_queue_base;
+			if(lnx_frfifo_jumbo_mask[node][nae_no] & (1<<frq_num)){
+				buf_size = NLM_RX_JUMBO_BUF_SIZE;
+			}else
+				buf_size = NLM_RX_ETH_BUF_SIZE;
+
+			skb = nlm_xlp_alloc_skb(buf_size, node);
+			if (!skb) {
+				printk(KERN_NOTICE "Low memory, not able to send free ins  count = %d\n",  atomic_read(&priv->cpu_alloc_fail[cpu][frq_num]));
+				/*reschedule in case of failure*/
+				schedule_delayed_work_on(cpu, &priv->alloc_failed_work[cpu], 2 * HZ);
+				atomic_sub(bad_cnt, &priv->cpu_alloc_fail[cpu][frq_num]);
+				return;
+			}
+			mac_put_skb_back_ptr(skb, nae);
+			msgrng_access_enable(mflags);
+			for (;;) {
+				ret = xlp_message_send_1(abs_frq_num, 0, ((uint64_t)virt_to_bus(skb->data) & 0xffffffffffULL));
+				if (!ret) break;
+			}
+			msgrng_access_disable(mflags);
+			bad_allocs--;
+			bad_cnt++;
+		}
+		atomic_sub(bad_cnt, &priv->cpu_alloc_fail[cpu][frq_num]);
+	}
+}
+
+static inline int mac_refill_frin_one_buffer(struct net_device *dev, int cpu,
+				      uint32_t truesize)
+{
+	struct dev_data* priv = netdev_priv(dev);
+	struct sk_buff * skb;
+	int buf_size = NLM_RX_ETH_BUF_SIZE, qid, node_cpu;
+	nae_t* nae_cfg = priv->nae;
+	int nae_id = nae_cfg->nae_id;
+	int node = nae_cfg->node;
+	int hw_port_id = priv->hw_port_id; 
+	node_cpu = phys_cpu_to_log_map[node][nae_id][cpu];
+	
+	if (enable_jumbo)
+		if(truesize > NLM_RX_JUMBO_BUF_SIZE)
+			buf_size = NLM_RX_JUMBO_BUF_SIZE;
+
+	if(nae_cfg->port_fifo_en)
+		qid = hw_port_id;
+	else
+		qid = (buf_size >= NLM_RX_JUMBO_BUF_SIZE) ?
+		cpu_2_jumbo_frfifo[node][nae_id][node_cpu] :
+		cpu_2_normal_frfifo[node][nae_id][node_cpu];
+
+	skb = nlm_xlp_alloc_skb_atomic(buf_size, priv->node);
+	if (!skb) {
+		atomic_inc(&priv->cpu_alloc_fail[cpu][qid]);
+		printk(KERN_NOTICE "%s: Low memory, packet dropped. free cnt = %d\n", dev->name,  atomic_read(&priv->cpu_alloc_fail[cpu][qid]));
+		schedule_delayed_work_on(cpu, &priv->alloc_failed_work[cpu], 0);
+		//schedule_delayed_work(&priv->alloc_failed_work[cpu], HZ);
+		return -ENOMEM;
+	}
+
+	//skb->dev = dev;
+
+	mac_put_skb_back_ptr(skb, nae_cfg);
+
+	return mac_refill_frin_skb(nae_cfg, cpu, (uint64_t)virt_to_bus(skb->data), buf_size, priv->hw_port_id);
+}
 
 inline void process_tx_complete(int cpu, uint32_t src_id, uint64_t msg0)
 {
-- 
1.7.1

