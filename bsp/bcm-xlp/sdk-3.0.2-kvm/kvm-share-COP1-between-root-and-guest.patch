From acc55049b63f823ca745b5ba961b58df79a3aee2 Mon Sep 17 00:00:00 2001
From: Ashok Kumar <ashoks@broadcom.com>
Date: Wed, 6 Nov 2013 04:38:15 -0800
Subject: [PATCH] kvm: share COP1 between root and guest.

Commit 71a0585076a52fde96d6ed7de07a3cd0e99a194e from Broadcom SDK 3.0.2

save/restore FPU registers to/from vm state in context switch
if guest context is present for the corresponding process involed
in the context switch

Signed-off-by: Ashok Kumar <ashoks@broadcom.com>
Signed-off-by: Nam Ninh <nam.ninh@windriver.com>

diff --git a/arch/mips/include/asm/asmmacro-64.h b/arch/mips/include/asm/asmmacro-64.h
index 01c06de..6c36f46 100644
--- a/arch/mips/include/asm/asmmacro-64.h
+++ b/arch/mips/include/asm/asmmacro-64.h
@@ -13,6 +13,104 @@
 #include <asm/fpregdef.h>
 #include <asm/mipsregs.h>
 
+#ifdef CONFIG_KVM
+	.macro	fpu_vm_save_16even thread tmp=t0
+	cfc1	\tmp, fcr31
+	sdc1	$f0,  THREAD_VM_FPR0(\thread)
+	sdc1	$f2,  THREAD_VM_FPR2(\thread)
+	sdc1	$f4,  THREAD_VM_FPR4(\thread)
+	sdc1	$f6,  THREAD_VM_FPR6(\thread)
+	sdc1	$f8,  THREAD_VM_FPR8(\thread)
+	sdc1	$f10, THREAD_VM_FPR10(\thread)
+	sdc1	$f12, THREAD_VM_FPR12(\thread)
+	sdc1	$f14, THREAD_VM_FPR14(\thread)
+	sdc1	$f16, THREAD_VM_FPR16(\thread)
+	sdc1	$f18, THREAD_VM_FPR18(\thread)
+	sdc1	$f20, THREAD_VM_FPR20(\thread)
+	sdc1	$f22, THREAD_VM_FPR22(\thread)
+	sdc1	$f24, THREAD_VM_FPR24(\thread)
+	sdc1	$f26, THREAD_VM_FPR26(\thread)
+	sdc1	$f28, THREAD_VM_FPR28(\thread)
+	sdc1	$f30, THREAD_VM_FPR30(\thread)
+	sw	\tmp, THREAD_VM_FCR31(\thread)
+	.endm
+
+	.macro	fpu_vm_save_16odd thread
+	sdc1	$f1,  THREAD_VM_FPR1(\thread)
+	sdc1	$f3,  THREAD_VM_FPR3(\thread)
+	sdc1	$f5,  THREAD_VM_FPR5(\thread)
+	sdc1	$f7,  THREAD_VM_FPR7(\thread)
+	sdc1	$f9,  THREAD_VM_FPR9(\thread)
+	sdc1	$f11, THREAD_VM_FPR11(\thread)
+	sdc1	$f13, THREAD_VM_FPR13(\thread)
+	sdc1	$f15, THREAD_VM_FPR15(\thread)
+	sdc1	$f17, THREAD_VM_FPR17(\thread)
+	sdc1	$f19, THREAD_VM_FPR19(\thread)
+	sdc1	$f21, THREAD_VM_FPR21(\thread)
+	sdc1	$f23, THREAD_VM_FPR23(\thread)
+	sdc1	$f25, THREAD_VM_FPR25(\thread)
+	sdc1	$f27, THREAD_VM_FPR27(\thread)
+	sdc1	$f29, THREAD_VM_FPR29(\thread)
+	sdc1	$f31, THREAD_VM_FPR31(\thread)
+	.endm
+
+	.macro	fpu_vm_save_double thread status tmp
+	sll	\tmp, \status, 5
+	bgez	\tmp, 2f
+	fpu_vm_save_16odd \thread
+2:
+	fpu_vm_save_16even \thread \tmp
+	.endm
+
+	.macro	fpu_vm_restore_16even thread tmp=t0
+	lw	\tmp, THREAD_VM_FCR31(\thread)
+	ldc1	$f0,  THREAD_VM_FPR0(\thread)
+	ldc1	$f2,  THREAD_VM_FPR2(\thread)
+	ldc1	$f4,  THREAD_VM_FPR4(\thread)
+	ldc1	$f6,  THREAD_VM_FPR6(\thread)
+	ldc1	$f8,  THREAD_VM_FPR8(\thread)
+	ldc1	$f10, THREAD_VM_FPR10(\thread)
+	ldc1	$f12, THREAD_VM_FPR12(\thread)
+	ldc1	$f14, THREAD_VM_FPR14(\thread)
+	ldc1	$f16, THREAD_VM_FPR16(\thread)
+	ldc1	$f18, THREAD_VM_FPR18(\thread)
+	ldc1	$f20, THREAD_VM_FPR20(\thread)
+	ldc1	$f22, THREAD_VM_FPR22(\thread)
+	ldc1	$f24, THREAD_VM_FPR24(\thread)
+	ldc1	$f26, THREAD_VM_FPR26(\thread)
+	ldc1	$f28, THREAD_VM_FPR28(\thread)
+	ldc1	$f30, THREAD_VM_FPR30(\thread)
+	ctc1	\tmp, fcr31
+	.endm
+
+	.macro	fpu_vm_restore_16odd thread
+	ldc1	$f1,  THREAD_VM_FPR1(\thread)
+	ldc1	$f3,  THREAD_VM_FPR3(\thread)
+	ldc1	$f5,  THREAD_VM_FPR5(\thread)
+	ldc1	$f7,  THREAD_VM_FPR7(\thread)
+	ldc1	$f9,  THREAD_VM_FPR9(\thread)
+	ldc1	$f11, THREAD_VM_FPR11(\thread)
+	ldc1	$f13, THREAD_VM_FPR13(\thread)
+	ldc1	$f15, THREAD_VM_FPR15(\thread)
+	ldc1	$f17, THREAD_VM_FPR17(\thread)
+	ldc1	$f19, THREAD_VM_FPR19(\thread)
+	ldc1	$f21, THREAD_VM_FPR21(\thread)
+	ldc1	$f23, THREAD_VM_FPR23(\thread)
+	ldc1	$f25, THREAD_VM_FPR25(\thread)
+	ldc1	$f27, THREAD_VM_FPR27(\thread)
+	ldc1	$f29, THREAD_VM_FPR29(\thread)
+	ldc1	$f31, THREAD_VM_FPR31(\thread)
+	.endm
+
+	.macro	fpu_vm_restore_double thread status tmp
+	sll	\tmp, \status, 5
+	bgez	\tmp, 1f				# 16 register mode?
+
+	fpu_vm_restore_16odd \thread
+1:	fpu_vm_restore_16even \thread \tmp
+	.endm
+#endif /* CONFIG_KVM */
+
 	.macro	fpu_save_16even thread tmp=t0
 	cfc1	\tmp, fcr31
 	sdc1	$f0,  THREAD_FPR0(\thread)
@@ -142,6 +240,23 @@
 	/* if guest id is zero, we do not need to save guest states */
 	beqz	k0, 31f
 
+
+	mfc0    t0, CP0_STATUS
+	li  t1, ST0_CU1
+	or  t0, t1
+	mtc0    t0, CP0_STATUS
+	ehb
+
+	PTR_L	t1, TASK_THREAD_INFO(\thread)
+	LONG_L	t0, (_THREAD_SIZE - 32 - PT_SIZE + PT_STATUS)(t1)
+	fpu_vm_save_double \thread t0 t1
+	
+	mfc0    t0, CP0_STATUS
+	li  t1, ST0_CU1
+	or  t0, t1
+	xor t0, t0, t1
+	mtc0    t0, CP0_STATUS
+	ehb
 	/* save gtoffset/count first and then some cycles later, to save guest_cause.
 	 * some lagging time to cover the cycles, we may potentially have
 	 * during restore.
@@ -310,6 +425,23 @@
 	j 60f
 
 	59:
+
+	mfc0    t0, CP0_STATUS
+	li  t1, ST0_CU1
+	or  t0, t1
+	mtc0    t0, CP0_STATUS
+	ehb
+
+	LONG_L	t0, THREAD_STATUS(\thread)
+	fpu_vm_restore_double \thread t0 t1
+
+	mfc0    t0, CP0_STATUS
+	li  t1, ST0_CU1
+	or  t0, t1
+	xor t0,t0,t1
+	mtc0    t0, CP0_STATUS
+	ehb
+
 	move	k1, k0
 	sll	k1, k1, 16
 	or	k0, k0, k1
diff --git a/arch/mips/include/asm/processor.h b/arch/mips/include/asm/processor.h
index 5ea6b37..4ace48e 100644
--- a/arch/mips/include/asm/processor.h
+++ b/arch/mips/include/asm/processor.h
@@ -250,6 +250,8 @@ struct kvm_vm_state {
 	unsigned long guest_wired_entrylo0[MAX_WIRED_TLBS];
 	unsigned long guest_wired_entrylo1[MAX_WIRED_TLBS];
 	unsigned long guest_wired_guestctl1[MAX_WIRED_TLBS];
+
+	struct mips_fpu_struct fpu;
 };
 	
 #endif
diff --git a/arch/mips/kernel/asm-offsets.c b/arch/mips/kernel/asm-offsets.c
index 69ab4d1..2e83d18 100644
--- a/arch/mips/kernel/asm-offsets.c
+++ b/arch/mips/kernel/asm-offsets.c
@@ -203,6 +203,41 @@ void output_thread_defines(void)
 	OFFSET(THREAD_VM_GUEST_WIRED_ENTRYLO0_0, task_struct, thread.vm.guest_wired_entrylo0[0]);
 	OFFSET(THREAD_VM_GUEST_WIRED_ENTRYLO1_0, task_struct, thread.vm.guest_wired_entrylo1[0]);
 	OFFSET(THREAD_VM_GUEST_WIRED_GUESTCTL1_0, task_struct, thread.vm.guest_wired_guestctl1[0]);
+
+	OFFSET(THREAD_VM_FPR0, task_struct, thread.vm.fpu.fpr[0]);
+	OFFSET(THREAD_VM_FPR1, task_struct, thread.vm.fpu.fpr[1]);
+	OFFSET(THREAD_VM_FPR2, task_struct, thread.vm.fpu.fpr[2]);
+	OFFSET(THREAD_VM_FPR3, task_struct, thread.vm.fpu.fpr[3]);
+	OFFSET(THREAD_VM_FPR4, task_struct, thread.vm.fpu.fpr[4]);
+	OFFSET(THREAD_VM_FPR5, task_struct, thread.vm.fpu.fpr[5]);
+	OFFSET(THREAD_VM_FPR6, task_struct, thread.vm.fpu.fpr[6]);
+	OFFSET(THREAD_VM_FPR7, task_struct, thread.vm.fpu.fpr[7]);
+	OFFSET(THREAD_VM_FPR8, task_struct, thread.vm.fpu.fpr[8]);
+	OFFSET(THREAD_VM_FPR9, task_struct, thread.vm.fpu.fpr[9]);
+	OFFSET(THREAD_VM_FPR10, task_struct, thread.vm.fpu.fpr[10]);
+	OFFSET(THREAD_VM_FPR11, task_struct, thread.vm.fpu.fpr[11]);
+	OFFSET(THREAD_VM_FPR12, task_struct, thread.vm.fpu.fpr[12]);
+	OFFSET(THREAD_VM_FPR13, task_struct, thread.vm.fpu.fpr[13]);
+	OFFSET(THREAD_VM_FPR14, task_struct, thread.vm.fpu.fpr[14]);
+	OFFSET(THREAD_VM_FPR15, task_struct, thread.vm.fpu.fpr[15]);
+	OFFSET(THREAD_VM_FPR16, task_struct, thread.vm.fpu.fpr[16]);
+	OFFSET(THREAD_VM_FPR17, task_struct, thread.vm.fpu.fpr[17]);
+	OFFSET(THREAD_VM_FPR18, task_struct, thread.vm.fpu.fpr[18]);
+	OFFSET(THREAD_VM_FPR19, task_struct, thread.vm.fpu.fpr[19]);
+	OFFSET(THREAD_VM_FPR20, task_struct, thread.vm.fpu.fpr[20]);
+	OFFSET(THREAD_VM_FPR21, task_struct, thread.vm.fpu.fpr[21]);
+	OFFSET(THREAD_VM_FPR22, task_struct, thread.vm.fpu.fpr[22]);
+	OFFSET(THREAD_VM_FPR23, task_struct, thread.vm.fpu.fpr[23]);
+	OFFSET(THREAD_VM_FPR24, task_struct, thread.vm.fpu.fpr[24]);
+	OFFSET(THREAD_VM_FPR25, task_struct, thread.vm.fpu.fpr[25]);
+	OFFSET(THREAD_VM_FPR26, task_struct, thread.vm.fpu.fpr[26]);
+	OFFSET(THREAD_VM_FPR27, task_struct, thread.vm.fpu.fpr[27]);
+	OFFSET(THREAD_VM_FPR28, task_struct, thread.vm.fpu.fpr[28]);
+	OFFSET(THREAD_VM_FPR29, task_struct, thread.vm.fpu.fpr[29]);
+	OFFSET(THREAD_VM_FPR30, task_struct, thread.vm.fpu.fpr[30]);
+	OFFSET(THREAD_VM_FPR31, task_struct, thread.vm.fpu.fpr[31]);
+
+	OFFSET(THREAD_VM_FCR31, task_struct, thread.vm.fpu.fcr31);
 #endif
 	BLANK();
 }
-- 
1.9.1

