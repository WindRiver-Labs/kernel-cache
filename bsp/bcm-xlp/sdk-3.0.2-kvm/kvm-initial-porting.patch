From bee020006f6b363d28f2c888971825684e1f9b6e Mon Sep 17 00:00:00 2001
From: Yonghong Song <ysong@broadcom.com>
Date: Tue, 23 Apr 2013 16:09:32 -0700
Subject: [PATCH] kvm: initial porting

Commit a1c37f11cc3c379dc9073bd111bf508422644452 from Broadcom SDK 3.0.2

o Two directory arch/mips/kvm and arch/mips/netlogic/kvm are added,
  plus a few new kvm related header files.
o All changes are in arch/mips area except one in include/uapi/linux/kvm.h
  where new VM EXIT codes are defined for XLP KVM porting. More study
  will be needed to see how to reduce/avoid these new EXIT codes.
o To enable KVM, do "make menuconfig", select "Virtualization" at
  top menu and then inside select "KVM".
o Current version can pass compilation when KVM is enabled, but its
  functionality has not been verified yet.

Signed-off-by: Yonghong Song <ysong@broadcom.com>
Signed-off-by: Nam Ninh <nam.ninh@windriver.com>

diff --git a/arch/mips/Kconfig b/arch/mips/Kconfig
index 74ca501..5d20397 100644
--- a/arch/mips/Kconfig
+++ b/arch/mips/Kconfig
@@ -2580,3 +2580,5 @@ source "security/Kconfig"
 source "crypto/Kconfig"
 
 source "lib/Kconfig"
+
+source "arch/mips/kvm/Kconfig"
diff --git a/arch/mips/Makefile b/arch/mips/Makefile
index b81f649..5df1cb6 100644
--- a/arch/mips/Makefile
+++ b/arch/mips/Makefile
@@ -170,6 +170,11 @@ endif
 endif
 
 #
+# Virtualization support
+#
+core-$(CONFIG_KVM)		+= arch/mips/kvm/
+
+#
 # Firmware support
 #
 libs-$(CONFIG_FW_ARC)		+= arch/mips/fw/arc/
diff --git a/arch/mips/include/asm/asmmacro-64.h b/arch/mips/include/asm/asmmacro-64.h
index 08a527d..ad115d1 100644
--- a/arch/mips/include/asm/asmmacro-64.h
+++ b/arch/mips/include/asm/asmmacro-64.h
@@ -136,4 +136,332 @@
 	LONG_L	ra, THREAD_REG31(\thread)
 	.endm
 
+#ifdef CONFIG_KVM
+	.macro cpu_save_vm_state thread
+	mfc0	k0, $10, 4
+	/* if guest id is zero, we do not need to save guest states */
+	beqz	k0, 31f
+
+	/* save gtoffset/count first and then some cycles later, to save guest_cause.
+	 * some lagging time to cover the cycles, we may potentially have
+	 * during restore.
+	 */
+	LONG_S	k0, THREAD_VM_GUESTCTL1(\thread)
+	mfc0	k0, $12, 7
+	LONG_S	k0, THREAD_VM_GTOFFSET(\thread)
+	mfc0	k0, $9, 0
+	LONG_S	k0, THREAD_VM_COUNT(\thread)
+	mfgc0   k0, $0, 0
+	LONG_S  k0, THREAD_VM_GUEST_INDEX(\thread)
+	mfgc0   k0, $1, 0
+	LONG_S  k0, THREAD_VM_GUEST_RANDOM(\thread)
+	dmfgc0  k0, $2, 0
+	LONG_S  k0, THREAD_VM_GUEST_ENTRYLO0(\thread)
+	dmfgc0  k0, $3, 0
+	LONG_S  k0, THREAD_VM_GUEST_ENTRYLO1(\thread)
+	dmfgc0  k0, $4, 0
+	LONG_S  k0, THREAD_VM_GUEST_CONTEXT(\thread)
+	dmfgc0  k0, $4, 2
+	LONG_S  k0, THREAD_VM_GUEST_USERLOCAL(\thread)
+	dmfgc0  k0, $5, 0
+	LONG_S  k0, THREAD_VM_GUEST_PAGEMASK(\thread)
+	mfgc0   k0, $5, 1
+	LONG_S  k0, THREAD_VM_GUEST_PAGEGRAIN(\thread)
+	dmfgc0  k0, $5, 5
+	LONG_S  k0, THREAD_VM_GUEST_PWBASE(\thread)
+	dmfgc0  k0, $5, 6
+	LONG_S  k0, THREAD_VM_GUEST_PWFIELD(\thread)
+	dmfgc0  k0, $5, 7
+	LONG_S  k0, THREAD_VM_GUEST_PWSIZE(\thread)
+	mfgc0   k0, $6, 0
+	LONG_S  k0, THREAD_VM_GUEST_WIRED(\thread)
+	mfgc0   k0, $6, 6
+	LONG_S  k0, THREAD_VM_GUEST_PWCTL(\thread)
+	mfgc0   k0, $7, 0
+	LONG_S  k0, THREAD_VM_GUEST_HWRENA(\thread)
+	dmfgc0  k0, $8, 0
+	LONG_S  k0, THREAD_VM_GUEST_BADVADDR(\thread)
+	dmfgc0  k0, $9, 6
+	LONG_S  k0, THREAD_VM_GUEST_EIRR(\thread)
+	dmfgc0  k0, $9, 7
+	LONG_S  k0, THREAD_VM_GUEST_EIMR(\thread)
+	dmfgc0  k0, $10, 0
+	LONG_S  k0, THREAD_VM_GUEST_ENTRYHI(\thread)
+	mfgc0   k0, $11, 0
+	LONG_S  k0, THREAD_VM_GUEST_COMPARE(\thread)
+	mfgc0   k0, $12, 0
+	LONG_S  k0, THREAD_VM_GUEST_STATUS(\thread)
+	mfgc0   k0, $12, 1
+	LONG_S  k0, THREAD_VM_GUEST_INTCTL(\thread)
+	dmfgc0  k0, $14, 0
+	LONG_S  k0, THREAD_VM_GUEST_EPC(\thread)
+	dmfgc0  k0, $15, 1
+	LONG_S  k0, THREAD_VM_GUEST_EBASE(\thread)
+	mfgc0   k0, $16, 0
+	LONG_S  k0, THREAD_VM_GUEST_CONFIG0(\thread)
+	mfgc0   k0, $16, 1
+	LONG_S  k0, THREAD_VM_GUEST_CONFIG1(\thread)
+	mfgc0   k0, $16, 4
+	LONG_S  k0, THREAD_VM_GUEST_CONFIG4(\thread)
+	dmfgc0  k0, $20, 0
+	LONG_S  k0, THREAD_VM_GUEST_XCONTEXT(\thread)
+	dmfgc0  k0, $30, 0
+	LONG_S  k0, THREAD_VM_GUEST_ERROREPC(\thread)
+	dmfgc0  k0, $22, 0
+	LONG_S  k0, THREAD_VM_GUEST_OSSCRATCH0(\thread)
+	dmfgc0  k0, $22, 1
+	LONG_S  k0, THREAD_VM_GUEST_OSSCRATCH1(\thread)
+	dmfgc0  k0, $22, 2
+	LONG_S  k0, THREAD_VM_GUEST_OSSCRATCH2(\thread)
+	dmfgc0  k0, $22, 3
+	LONG_S  k0, THREAD_VM_GUEST_OSSCRATCH3(\thread)
+	dmfgc0  k0, $22, 4
+	LONG_S  k0, THREAD_VM_GUEST_OSSCRATCH4(\thread)
+	dmfgc0  k0, $22, 5
+	LONG_S  k0, THREAD_VM_GUEST_OSSCRATCH5(\thread)
+	dmfgc0  k0, $22, 6
+	LONG_S  k0, THREAD_VM_GUEST_OSSCRATCH6(\thread)
+	dmfgc0  k0, $22, 7
+	LONG_S  k0, THREAD_VM_GUEST_OSSCRATCH7(\thread)
+	mfgc0   k0, $8, 1
+	LONG_S  k0, THREAD_VM_GUEST_BADINSTR(\thread)
+	mfgc0   k0, $8, 2
+	LONG_S  k0, THREAD_VM_GUEST_BADINSTRP(\thread)
+	mfgc0   k0, $4, 1
+	LONG_S  k0, THREAD_VM_GUEST_CONTEXTCONFIG(\thread)
+	dmfgc0  k0, $4, 3
+	LONG_S  k0, THREAD_VM_GUEST_XCONTEXTCONFIG(\thread)
+
+	/* some lagging time between saving gtoffset/count and cause */
+	mfgc0   k0, $13, 0
+	LONG_S  k0, THREAD_VM_GUEST_CAUSE(\thread)
+
+	/* wired entries, available registers: k0, k1, t0, t1 */
+	LONG_L  k0, THREAD_VM_GUEST_WIRED(\thread)
+	beqz	k0, 31f
+	li      k1, 0
+
+	29:
+	slt     t0, k1, k0
+	beqz    t0, 30f
+
+	mtgc0   k1, $0, 0
+	tlbgr
+	ehb
+
+	/* offset to the first wired entry */
+	sll     k1, k1, 3
+
+	dmfgc0   t0, $5, 0
+	daddiu  t1, \thread, THREAD_VM_GUEST_WIRED_PAGEMASK_0
+	daddu   t1, t1, k1
+	LONG_S  t0, 0(t1)
+
+	dmfgc0  t0, $10, 0
+	daddiu  t1, \thread, THREAD_VM_GUEST_WIRED_ENTRYHI_0
+	daddu   t1, t1, k1
+	LONG_S  t0, 0(t1)
+
+	dmfgc0  t0, $2, 0
+	daddiu  t1, \thread, THREAD_VM_GUEST_WIRED_ENTRYLO0_0
+	daddu   t1, t1, k1
+	LONG_S  t0, 0(t1)
+
+	dmfgc0  t0, $3, 0
+	daddiu  t1, \thread, THREAD_VM_GUEST_WIRED_ENTRYLO1_0
+	daddu   t1, t1, k1
+	LONG_S  t0, 0(t1)
+
+	mfc0	t0, $10, 4
+	daddiu  t1, \thread, THREAD_VM_GUEST_WIRED_GUESTCTL1_0
+	daddu   t1, t1, k1
+	LONG_S  t0, 0(t1)
+
+	srl     k1, k1, 3
+
+	addiu   k1, k1, 1
+	j       29b
+        30:
+	/* restore guestctl1 which will be used by root */
+	LONG_L	k0, THREAD_VM_GUESTCTL1(\thread)
+	mtc0	k0, $10, 4
+
+	/* other changed tlb registers */
+	LONG_L  k0, THREAD_VM_GUEST_INDEX(\thread)
+	mtgc0   k0, $0, 0
+	LONG_L  k0, THREAD_VM_GUEST_ENTRYLO0(\thread)
+	dmtgc0  k0, $2, 0
+	LONG_L  k0, THREAD_VM_GUEST_ENTRYLO1(\thread)
+	dmtgc0  k0, $3, 0
+	LONG_L  k0, THREAD_VM_GUEST_PAGEMASK(\thread)
+	dmtgc0  k0, $5, 0
+	LONG_L  k0, THREAD_VM_GUEST_ENTRYHI(\thread)
+	dmtgc0  k0, $10, 0
+	31:
+	.endm
+
+	.macro cpu_restore_vm_state thread
+
+	/* Flush the root/guest tlb and reinstall the wired entries */
+	LONG_L	k0, THREAD_VM_GUESTCTL1(\thread)
+	bnez	k0, 59f
+
+	mtc0	k0, $10, 4
+	j 60f
+
+	59:
+	move	k1, k0
+	sll	k1, k1, 16
+	or	k0, k0, k1
+	mtc0	k0, $10, 4
+	tlbinvf
+	tlbginvf
+
+	/* wired entries */
+	LONG_L  k0, THREAD_VM_GUEST_WIRED(\thread)
+	li      k1, 0
+        39:
+	slt     v0, k1, k0
+	beqz    v0, 40f
+
+	/* offset to the first wired entry */
+	sll     k1, k1, 3
+
+	daddiu  v1, \thread, THREAD_VM_GUEST_WIRED_PAGEMASK_0
+	daddu   v1, v1, k1
+	LONG_L  v0, 0(v1)
+	dmtgc0   v0, $5, 0
+
+	daddiu  v1, \thread, THREAD_VM_GUEST_WIRED_ENTRYHI_0
+	daddu   v1, v1, k1
+	LONG_L  v0, 0(v1)
+	dmtgc0  v0, $10, 0
+
+	daddiu  v1, \thread, THREAD_VM_GUEST_WIRED_ENTRYLO0_0
+	daddu   v1, v1, k1
+	LONG_L  v0, 0(v1)
+	dmtgc0  v0, $2, 0
+	daddiu  v1, \thread, THREAD_VM_GUEST_WIRED_ENTRYLO1_0
+	daddu   v1, v1, k1
+	LONG_L  v0, 0(v1)
+	dmtgc0  v0, $3, 0
+
+	srl     k1, k1, 3
+
+	mtgc0   k1, $0, 0
+	tlbgwi
+
+	addiu   k1, k1, 1
+	j       39b
+        40:
+
+	LONG_L	k0, THREAD_VM_GUESTCTL1(\thread)
+	mtc0	k0, $10, 4
+	LONG_L  k0, THREAD_VM_GUEST_INDEX(\thread)
+	mtgc0   k0, $0, 0
+	LONG_L  k0, THREAD_VM_GUEST_RANDOM(\thread)
+	mtgc0   k0, $1, 0
+	LONG_L  k0, THREAD_VM_GUEST_ENTRYLO0(\thread)
+	dmtgc0  k0, $2, 0
+	LONG_L  k0, THREAD_VM_GUEST_ENTRYLO1(\thread)
+	dmtgc0  k0, $3, 0
+	LONG_L  k0, THREAD_VM_GUEST_CONTEXT(\thread)
+	dmtgc0  k0, $4, 0
+	LONG_L  k0, THREAD_VM_GUEST_USERLOCAL(\thread)
+	dmtgc0  k0, $4, 2
+	LONG_L  k0, THREAD_VM_GUEST_PAGEMASK(\thread)
+	dmtgc0  k0, $5, 0
+	LONG_L  k0, THREAD_VM_GUEST_PAGEGRAIN(\thread)
+	mtgc0   k0, $5, 1
+	LONG_L  k0, THREAD_VM_GUEST_PWBASE(\thread)
+	dmtgc0  k0, $5, 5
+	LONG_L  k0, THREAD_VM_GUEST_PWFIELD(\thread)
+	dmtgc0  k0, $5, 6
+	LONG_L  k0, THREAD_VM_GUEST_PWSIZE(\thread)
+	dmtgc0  k0, $5, 7
+	LONG_L  k0, THREAD_VM_GUEST_WIRED(\thread)
+	mtgc0   k0, $6, 0
+	LONG_L  k0, THREAD_VM_GUEST_PWCTL(\thread)
+	mtgc0   k0, $6, 6
+	LONG_L  k0, THREAD_VM_GUEST_HWRENA(\thread)
+	mtgc0   k0, $7, 0
+	LONG_L  k0, THREAD_VM_GUEST_BADVADDR(\thread)
+	dmtgc0  k0, $8, 0
+	LONG_L  k0, THREAD_VM_GUEST_EIMR(\thread)
+	dmtgc0  k0, $9, 7
+	LONG_L  k0, THREAD_VM_GUEST_ENTRYHI(\thread)
+	dmtgc0  k0, $10, 0
+	LONG_L  k0, THREAD_VM_GUEST_STATUS(\thread)
+	mtgc0   k0, $12, 0
+	LONG_L  k0, THREAD_VM_GUEST_INTCTL(\thread)
+	mtgc0   k0, $12, 1
+	LONG_L  k0, THREAD_VM_GUEST_EPC(\thread)
+	dmtgc0  k0, $14, 0
+	LONG_L  k0, THREAD_VM_GUEST_EBASE(\thread)
+	dmtgc0  k0, $15, 1
+	LONG_L  k0, THREAD_VM_GUEST_CONFIG0(\thread)
+	mtgc0   k0, $16, 0
+	LONG_L  k0, THREAD_VM_GUEST_CONFIG1(\thread)
+	mtgc0   k0, $16, 1
+	LONG_L  k0, THREAD_VM_GUEST_CONFIG4(\thread)
+	mtgc0   k0, $16, 4
+	LONG_L  k0, THREAD_VM_GUEST_XCONTEXT(\thread)
+	mtgc0   k0, $20, 0
+	LONG_L  k0, THREAD_VM_GUEST_ERROREPC(\thread)
+	dmtgc0  k0, $30, 0
+	LONG_L  k0, THREAD_VM_GUEST_OSSCRATCH0(\thread)
+	dmtgc0  k0, $22, 0
+	LONG_L  k0, THREAD_VM_GUEST_OSSCRATCH1(\thread)
+	dmtgc0  k0, $22, 1
+	LONG_L  k0, THREAD_VM_GUEST_OSSCRATCH2(\thread)
+	dmtgc0  k0, $22, 2
+	LONG_L  k0, THREAD_VM_GUEST_OSSCRATCH3(\thread)
+	dmtgc0  k0, $22, 3
+	LONG_L  k0, THREAD_VM_GUEST_OSSCRATCH4(\thread)
+	dmtgc0  k0, $22, 4
+	LONG_L  k0, THREAD_VM_GUEST_OSSCRATCH5(\thread)
+	dmtgc0  k0, $22, 5
+	LONG_L  k0, THREAD_VM_GUEST_OSSCRATCH6(\thread)
+	dmtgc0  k0, $22, 6
+	LONG_L  k0, THREAD_VM_GUEST_OSSCRATCH7(\thread)
+	dmtgc0  k0, $22, 7
+	LONG_L  k0, THREAD_VM_GUEST_BADINSTR(\thread)
+	mtgc0   k0, $8, 1
+	LONG_L  k0, THREAD_VM_GUEST_BADINSTRP(\thread)
+	mtgc0   k0, $8, 2
+	LONG_L  k0, THREAD_VM_GUEST_CONTEXTCONFIG(\thread)
+	mtgc0   k0, $4, 1
+	LONG_L  k0, THREAD_VM_GUEST_XCONTEXTCONFIG(\thread)
+	dmtgc0  k0, $4, 3
+
+	/* avoid interrupt pass through, and restore guest cause */
+	LONG_L	v0, THREAD_VM_GUEST_CAUSE(\thread)
+	ori	v0, v0, 0xfc00
+	xori	v0, v0, 0xfc00
+
+	/* adjust the gtoffset */
+	LONG_L	k0, THREAD_VM_GTOFFSET(\thread)
+	LONG_L	k1, THREAD_VM_COUNT(\thread)
+	daddu	k0, k1
+	mfc0	k1, $9, 0
+	dsubu	k0, k1
+	mtc0	k0, $12, 7
+
+	/* Load the guest.compare register, and then restore guest.cause to avoid time interrupt lost */
+	LONG_L	k0, THREAD_VM_GUEST_COMPARE(\thread)
+	mtgc0	k0, $11, 0
+	mtgc0	v0, $13, 0
+
+	/* 
+	 * cleared EIRR[6:7] since it is not the right way to assert
+	 * time and performance counter interrupt.
+	 */
+	LONG_L	k0, THREAD_VM_GUEST_EIRR(\thread)
+	ori	k0, 0xc0
+	xori	k0, 0xc0
+	dmtgc0	k0, $9, 6
+	60:
+	.endm
+#endif
+
 #endif /* _ASM_ASMMACRO_64_H */
diff --git a/arch/mips/include/asm/kvm.h b/arch/mips/include/asm/kvm.h
new file mode 100644
index 0000000..e65a528
--- /dev/null
+++ b/arch/mips/include/asm/kvm.h
@@ -0,0 +1,106 @@
+#ifndef __MIPS_KVM_H__
+#define __MIPS_KVM_H__
+
+struct kvm_guest_input {
+	uint64_t	buf;
+	uint32_t	size;
+};
+
+struct kvm_guest_info {
+	uint32_t	guest_id;
+};
+
+#define KVM_MIPS_GUEST_INPUT      _IOW(KVMIO,  0xa0, struct kvm_guest_input)
+#define KVM_MIPS_EXIT_REQUEST	  _IOW(KVMIO,  0xa1, int)
+#define KVM_MIPS_INFO_REQUEST     _IOW(KVMIO,  0xa2, struct kvm_guest_info)
+#define KVM_MIPS_SYNC_GPA_MAP	  _IOW(KVMIO,  0xa3, struct kvm_guest_info)
+
+/* Used to capture a guest state */
+struct kvm_regs {
+
+	/* general purpose registers */
+	uint64_t	regs[32];
+	uint64_t	hi;
+	uint64_t	lo;
+
+	/* root cop0 register */
+	uint64_t	root_guestctl0;
+	uint64_t	root_guestctl1;
+	uint64_t	root_epc;
+	uint64_t	root_count;
+	uint64_t	root_gtoffset;
+
+	/* guest cop0 register */
+	uint64_t	guest_index;
+	uint64_t	guest_random;
+	uint64_t	guest_entrylo0;
+	uint64_t	guest_entrylo1;
+	uint64_t	guest_context;
+	uint64_t	guest_userlocal;
+	uint64_t	guest_pagemask;
+	uint64_t	guest_pagegrain;
+	uint64_t	guest_pwbase;
+	uint64_t	guest_pwfield;
+	uint64_t	guest_pwsize;
+	uint64_t	guest_wired;
+	uint64_t	guest_pwctl;
+	uint64_t	guest_hwrena;
+	uint64_t	guest_badvaddr;
+	uint64_t	guest_eirr;
+	uint64_t	guest_eimr;
+	uint64_t	guest_entryhi;
+	uint64_t	guest_compare;
+	uint64_t	guest_status;
+	uint64_t	guest_intctl;
+	uint64_t	guest_cause;
+	uint64_t	guest_epc;
+	uint64_t	guest_ebase;
+	uint64_t	guest_config0;
+	uint64_t	guest_config1;
+	uint64_t	guest_config4;
+	uint64_t	guest_xcontext;
+	uint64_t	guest_errorepc;
+	uint64_t	guest_osscratch0;
+	uint64_t	guest_osscratch1;
+	uint64_t	guest_osscratch2;
+	uint64_t	guest_osscratch3;
+	uint64_t	guest_osscratch4;
+	uint64_t	guest_osscratch5;
+	uint64_t	guest_osscratch6;
+	uint64_t	guest_osscratch7;
+	uint64_t	guest_badinstr;
+	uint64_t	guest_badinstrp;
+	uint64_t	guest_contextconfig;
+	uint64_t	guest_xcontextconfig;
+
+	/* wired entries */
+	uint64_t	guest_wired_pagemask[32];
+	uint64_t	guest_wired_entryhi[32];
+	uint64_t	guest_wired_entrylo0[32];
+	uint64_t	guest_wired_entrylo1[32];
+};
+
+struct kvm_sregs {
+};
+
+/* Used to capture a guest state */
+struct kvm_fpu {
+	uint64_t	regs[32];
+	uint32_t	fir;
+	uint32_t	fccr;
+	uint32_t	fexr;
+	uint32_t	fenr;
+	uint32_t	fcsr;
+};
+
+struct kvm_debug_exit_arch {
+};
+
+struct kvm_guest_debug_arch {
+};
+
+/* definition of registers in kvm_run */
+struct kvm_sync_regs {
+};
+
+#endif
diff --git a/arch/mips/include/asm/kvm_host.h b/arch/mips/include/asm/kvm_host.h
new file mode 100644
index 0000000..7c5311d
--- /dev/null
+++ b/arch/mips/include/asm/kvm_host.h
@@ -0,0 +1,100 @@
+
+#ifndef __MIPS_KVM_HOST_H__
+#define __MIPS_KVM_HOST_H__
+
+#define KVM_MAX_VCPUS		32
+#define KVM_USER_MEM_SLOTS	8
+#define	KVM_MEMORY_SLOTS	8
+#define KVM_PRIVATE_MEM_SLOTS	2
+#define KVM_NR_PAGE_SIZES	1
+#define KVM_PAGES_PER_HPAGE(x)	1
+#define KVM_HPAGE_GFN_SHIFT(x)  0
+
+struct kvm_vm_stat {
+	u32 remote_tlb_flush;
+};
+
+struct kvm_arch_memory_slot {
+	unsigned long *rmap[KVM_NR_PAGE_SIZES];
+	struct kvm_lpage_info *lpage_info[KVM_NR_PAGE_SIZES - 1];
+};
+
+/* put I/O PCIE config space here */
+#define MAX_XLP_PIC_REG_NUM		0x400
+#define MAX_XLP_UART_REG_NUM		0x46
+#define MAX_XLP_SYSMGT_REG_NUM		0xca
+
+/* uart input buffer size */
+#define KVM_MAX_UART_IN_SIZE		128
+
+struct kvm_arch {
+	unsigned int	guest_id;
+	unsigned int	exit_request;
+	unsigned long long gpa_pgd;
+	struct {
+		union {
+			unsigned int v32[MAX_XLP_PIC_REG_NUM];
+			unsigned long long v64[MAX_XLP_PIC_REG_NUM/2];
+		} u;
+		unsigned long long host_sys_time[8];
+	} pic;
+	struct {
+		unsigned int header[0x40];
+		unsigned int rhr; /* 0x40 */
+		unsigned int thr; /* 0x40 */
+		unsigned int ier; /* 0x41 */
+		unsigned int iir; /* 0x42 */
+		unsigned int fcr; /* 0x42 */
+		unsigned int lcr; /* 0x43 */
+		unsigned int mcr; /* 0x44 */
+		unsigned int lsr; /* 0x45 */
+		unsigned int msr; /* 0x46 */
+
+		unsigned int dlb1; /* 0x40 */
+		unsigned int dlb2; /* 0x41 */
+
+		/* input buffers */
+		unsigned int input_size;
+		unsigned char input_buf[KVM_MAX_UART_IN_SIZE];
+	} uart;
+	struct {
+		unsigned int regs[MAX_XLP_SYSMGT_REG_NUM];
+	} sysmgt;
+};
+
+struct kvm_vcpu_guest {
+	struct kvm_regs gpu;
+	struct kvm_fpu  fpu;
+};
+
+struct kvm_vcpu_root {
+	uint64_t	sregs[8];
+	uint64_t	gp;
+	uint64_t	sp;
+	uint64_t	fp;
+	uint64_t	ra;
+	uint64_t	fpr[32];
+	uint64_t	fcr;
+	uint64_t	status;
+};
+
+struct kvm_vcpu_arch {
+	uint64_t pip_vector; /* pending interrupts passthrough (to guest) */
+	uint64_t host_stack; /* host stack */
+	uint64_t init_guest; /* whether the guest has been initialized. */
+	uint64_t hva_pgd; /* host virtual address pgd */
+	uint64_t gpa_pgd; /* guest physical address pgd */
+	uint64_t guest_vcpu_p[32]; /* guest vcpu pointer */
+	uint64_t host_vcpuid; /* corresponding host vcpu id, for fast guest IPI delivery */
+
+	/* vcpu state */
+	struct kvm_vcpu_guest	guest;
+	struct kvm_vcpu_root	root;
+};
+
+
+struct kvm_vcpu_stat {
+	u32 halt_wakeup;
+};
+
+#endif
diff --git a/arch/mips/include/asm/mipsregs.h b/arch/mips/include/asm/mipsregs.h
index c12f9eb..eec4e59 100644
--- a/arch/mips/include/asm/mipsregs.h
+++ b/arch/mips/include/asm/mipsregs.h
@@ -783,6 +783,64 @@ do {									\
 			: : "Jr" (value));				\
 } while (0)
 
+#ifdef CONFIG_KVM
+
+#define __read_32bit_guest_c0_register(source, sel)			\
+({ int __res;								\
+	if (sel == 0)							\
+		__asm__ __volatile__(					\
+			"mfgc0\t%0, " #source "\n\t"			\
+			: "=r" (__res));				\
+	else								\
+		__asm__ __volatile__(					\
+			"mfgc0\t%0, " #source ", " #sel "\n\t"		\
+			: "=r" (__res));				\
+	__res;								\
+})
+
+#define __read_64bit_guest_c0_register(source, sel)			\
+({ unsigned long long __res;						\
+	if (sizeof(unsigned long) == 4)					\
+		__res = __read_64bit_c0_split(source, sel);		\
+	else if (sel == 0)						\
+		__asm__ __volatile__(					\
+			"dmfgc0\t%0, " #source "\n\t"			\
+			: "=r" (__res));				\
+	else								\
+		__asm__ __volatile__(					\
+			"dmfgc0\t%0, " #source ", " #sel "\n\t"		\
+			: "=r" (__res));				\
+	__res;								\
+})
+
+#define __write_32bit_guest_c0_register(register, sel, value)		\
+do {									\
+	if (sel == 0)							\
+		__asm__ __volatile__(					\
+			"mtgc0\t%z0, " #register "\n\t"			\
+			: : "Jr" ((unsigned int)(value)));		\
+	else								\
+		__asm__ __volatile__(					\
+			"mtgc0\t%z0, " #register ", " #sel "\n\t"	\
+			: : "Jr" ((unsigned int)(value)));		\
+} while (0)
+
+#define __write_64bit_guest_c0_register(register, sel, value)		\
+do {									\
+	if (sizeof(unsigned long) == 4)					\
+		__write_64bit_c0_split(register, sel, value);		\
+	else if (sel == 0)						\
+		__asm__ __volatile__(					\
+			"dmtgc0\t%z0, " #register "\n\t"		\
+			: : "Jr" (value));				\
+	else								\
+		__asm__ __volatile__(					\
+			"dmtgc0\t%z0, " #register ", " #sel "\n\t"	\
+			: : "Jr" (value));				\
+} while (0)
+
+#endif /* CONFIG_KVM */
+
 #define __read_ulong_c0_register(reg, sel)				\
 	((sizeof(unsigned long) == 4) ?					\
 	(unsigned long) __read_32bit_c0_register(reg, sel) :		\
diff --git a/arch/mips/include/asm/netlogic/kvm_xlp.h b/arch/mips/include/asm/netlogic/kvm_xlp.h
new file mode 100644
index 0000000..dcd0dbc
--- /dev/null
+++ b/arch/mips/include/asm/netlogic/kvm_xlp.h
@@ -0,0 +1,92 @@
+#ifndef __ASM_KVM_XLP
+#define __ASM_KVM_XLP
+
+#include <asm/branch.h>
+
+/* Maximum # of guest ids supported by hardware */
+#define KVM_MAX_NUM_GID		256
+
+static inline struct kvm_arch * kvm_get_arch(struct pt_regs *regs)
+{
+	struct kvm *kvm = ((struct kvm_vcpu *)regs->cp0_osscratch7)->kvm;
+	return &kvm->arch;
+}
+
+static inline struct kvm_vcpu_guest * kvm_get_vcpu_guest_regs(struct pt_regs *regs)
+{
+	return &((struct kvm_vcpu *)regs->cp0_osscratch7)->arch.guest;
+}
+
+static inline struct kvm_vcpu_root * kvm_get_vcpu_root_regs(struct pt_regs *regs)
+{
+	return &((struct kvm_vcpu *)regs->cp0_osscratch7)->arch.root;
+}
+
+static inline struct kvm_vcpu_arch * kvm_get_vcpu_arch(struct pt_regs *regs)
+{
+	return &((struct kvm_vcpu *)regs->cp0_osscratch7)->arch;
+}
+
+static inline struct kvm_vcpu_arch * kvm_get_vcpu_arch_with_cpuid(struct pt_regs *regs,
+	unsigned int cpuid)
+{
+	struct kvm *kvm = ((struct kvm_vcpu *)regs->cp0_osscratch7)->kvm;
+	unsigned int i;
+
+	for (i = 0; i < KVM_MAX_VCPUS; i++) {
+		if ((kvm->vcpus[i]->arch.guest.gpu.guest_ebase & 0x3ff) == cpuid)
+			return &kvm->vcpus[i]->arch;
+	}
+	return NULL;
+}
+
+static inline struct kvm_vcpu_arch * kvm_get_vcpu_arch_ext(struct kvm *kvm, unsigned int cpuid)
+{
+	unsigned int i;
+
+	for (i = 0; i < KVM_MAX_VCPUS; i++) {
+		if ((kvm->vcpus[i]->arch.guest.gpu.guest_ebase & 0x3ff) == cpuid)
+			return &kvm->vcpus[i]->arch;
+	}
+	return NULL;
+}
+
+static inline void kvm_get_badinstr(struct pt_regs *regs, unsigned int *badinstr,
+	unsigned int *epc_badinstr)
+{
+	*badinstr = regs->cp0_badinstr;
+	if (delay_slot(regs))
+		*epc_badinstr = regs->cp0_badinstrp;
+	else
+		*epc_badinstr = regs->cp0_badinstr;
+}
+
+extern int xlp_kvm_check_processor_compat(void);
+extern int __kvm_vcpu_run_guest(void *);
+extern void __kvm_vcpu_leave_guest(void *, int);
+extern int compute_guest_return_epc(struct pt_regs *regs, unsigned int badinstr);
+extern void xlp_kvm_init_vm(struct kvm *kvm);
+extern void kvm_save_guest_context(struct pt_regs *regs, struct kvm_vcpu_guest *guest);
+extern void kvm_uart_insert_char(struct kvm *kvm, char c);
+extern void xlp_kvm_destroy_vm(struct kvm *kvm);
+extern void kvm_xlp_check_exit_request(struct kvm_vcpu *);
+
+/* defined in arch/mips/mm/kvm_uart.c */
+extern void kvm_handle_pcie_uart(struct pt_regs *regs, unsigned long write,
+	unsigned long address, unsigned long reg_num);
+
+/* defined in arch/mips/mm/kvm_pic.c */
+extern void kvm_handle_pcie_pic(struct pt_regs *regs, unsigned long write,
+	unsigned long address, unsigned long reg_num);
+extern void kvm_pic_inject_guest(struct kvm_arch *arch, unsigned int irt,
+	struct pt_regs *regs);
+extern void kvm_pic_inject_guest_cpuid(struct kvm_arch *arch, unsigned int irt,
+	unsigned int cpuid);
+extern void kvm_pic_inject_guest_ext(struct kvm *kvm, struct kvm_arch *arch,
+	unsigned int irt, unsigned int cpuid);
+
+/* defined in arch/mips/mm/kvm_sysmgt.c */
+extern void kvm_handle_pcie_sysmgt(struct pt_regs *regs, unsigned long write,
+	unsigned long address, unsigned long reg_num);
+
+#endif
diff --git a/arch/mips/include/asm/processor.h b/arch/mips/include/asm/processor.h
index 1470b7b..5ea6b37 100644
--- a/arch/mips/include/asm/processor.h
+++ b/arch/mips/include/asm/processor.h
@@ -191,6 +191,69 @@ struct octeon_cvmseg_state {
 
 #endif
 
+#ifdef CONFIG_KVM
+
+struct kvm_vm_state {
+	unsigned long cp0_guestctl0;
+	unsigned long cp0_guestctl1;
+	unsigned long cp0_count;
+	unsigned long cp0_gtoffset;
+	unsigned long cp0_osscratch7;
+
+	/* Guest context to be saved */
+	unsigned long guest_cp0_index; /* 32: 0, 0 */
+	unsigned long guest_cp0_random; /* 32: 1, 0 */
+	unsigned long guest_cp0_entrylo0; /* 64: 2, 0 */
+	unsigned long guest_cp0_entrylo1; /* 64: 3, 0 */
+	unsigned long guest_cp0_context; /* 64: 4, 0 */
+	unsigned long guest_cp0_userlocal; /* 64: 4, 2 */
+	unsigned long guest_cp0_pagemask; /* 64: 5, 0 */
+	unsigned long guest_cp0_pagegrain; /* 32: 5, 1 */
+	unsigned long guest_cp0_pwbase; /* 64: 5, 5 */
+	unsigned long guest_cp0_pwfield; /* 64: 5, 6 */
+	unsigned long guest_cp0_pwsize; /* 64: 5, 7 */
+	unsigned long guest_cp0_wired; /* 32: 6, 0 */
+	unsigned long guest_cp0_pwctl; /* 32: 6, 6 */
+	unsigned long guest_cp0_hwrena; /* 32: 7, 0 */
+	unsigned long guest_cp0_badvaddr; /* 64: 8, 0 */
+	unsigned long guest_cp0_eirr; /* 64: 9, 6 */
+	unsigned long guest_cp0_eimr; /* 64: 9, 7 */
+	unsigned long guest_cp0_entryhi; /* 64: 10, 0 */
+	unsigned long guest_cp0_compare; /* 32: 11, 0 */
+	unsigned long guest_cp0_status; /* 32: 12, 0 */
+	unsigned long guest_cp0_intctl; /* 32: 12, 1 */
+	unsigned long guest_cp0_cause;  /* 32: 13, 0 */
+	unsigned long guest_cp0_epc; /* 64: 14, 0 */
+	unsigned long guest_cp0_ebase;  /* 64: 15, 1 */
+	unsigned long guest_cp0_config0; /* 32: 16, 0 */
+	unsigned long guest_cp0_config1; /* 32: 16, 1 */
+	unsigned long guest_cp0_config4; /* 32: 16, 4 */
+	unsigned long guest_cp0_xcontext; /* 64: 20, 0 */
+	unsigned long guest_cp0_errorepc; /* 64: 30, 0 */
+	unsigned long guest_cp0_osscratch0; /* 64: 22, 0 */
+	unsigned long guest_cp0_osscratch1; /* 64: 22, 1 */
+	unsigned long guest_cp0_osscratch2; /* 64: 22, 2 */
+	unsigned long guest_cp0_osscratch3; /* 64: 22, 3 */
+	unsigned long guest_cp0_osscratch4; /* 64: 22, 4 */
+	unsigned long guest_cp0_osscratch5; /* 64: 22, 5 */
+	unsigned long guest_cp0_osscratch6; /* 64: 22, 6 */
+	unsigned long guest_cp0_osscratch7; /* 64: 22, 7 */
+	unsigned long guest_cp0_badinstr; /* 32: 8, 1 */
+	unsigned long guest_cp0_badinstrp; /* 32: 8, 2 */
+	unsigned long guest_cp0_contextconfig; /* 32: 4, 1 */
+	unsigned long guest_cp0_xcontextconfig; /* 64: 4, 3 */
+
+	/* wired tlbs, also see asm/kvm.h */
+#define MAX_WIRED_TLBS  32
+	unsigned long guest_wired_pagemask[MAX_WIRED_TLBS];
+	unsigned long guest_wired_entryhi[MAX_WIRED_TLBS];
+	unsigned long guest_wired_entrylo0[MAX_WIRED_TLBS];
+	unsigned long guest_wired_entrylo1[MAX_WIRED_TLBS];
+	unsigned long guest_wired_guestctl1[MAX_WIRED_TLBS];
+};
+	
+#endif
+
 typedef struct {
 	unsigned long seg;
 } mm_segment_t;
@@ -235,6 +298,9 @@ struct thread_struct {
     struct octeon_cvmseg_state cvmseg __attribute__ ((__aligned__(128)));
 #endif
 	struct mips_abi *abi;
+#ifdef CONFIG_KVM
+	struct kvm_vm_state vm;
+#endif
 };
 
 #ifdef CONFIG_MIPS_MT_FPAFF
@@ -252,6 +318,15 @@ struct thread_struct {
 #define OCTEON_INIT
 #endif /* CONFIG_CPU_CAVIUM_OCTEON */
 
+#ifdef CONFIG_KVM
+#define KVM_INIT                                                \
+	.vm = {                                                 \
+		.cp0_guestctl1  = 0,                            \
+	}
+#else
+#define KVM_INIT
+#endif
+
 #define INIT_THREAD  {						\
 	/*							\
 	 * Saved main processor registers			\
@@ -303,6 +378,10 @@ struct thread_struct {
 	 * Cavium Octeon specifics (null if not Octeon)		\
 	 */							\
 	OCTEON_INIT						\
+	/*							\
+	 * KVM state initialization				\
+	 */							\
+	KVM_INIT						\
 }
 
 struct task_struct;
diff --git a/arch/mips/include/asm/ptrace.h b/arch/mips/include/asm/ptrace.h
index fa95cd8..abec462 100644
--- a/arch/mips/include/asm/ptrace.h
+++ b/arch/mips/include/asm/ptrace.h
@@ -55,6 +55,14 @@ struct pt_regs {
 	unsigned int msg_config;
 	unsigned int msg_err;
 #endif
+#ifdef CONFIG_KVM
+	unsigned long cp0_badinstr;
+	unsigned long cp0_badinstrp;
+	unsigned long cp0_guestctl0;
+	unsigned long cp0_guestctl1;
+	unsigned long cp0_osscratch7;
+	unsigned long guest_cp0_ebase;  /* 15, 1 */
+#endif
 } __aligned(8);
 
 struct task_struct;
diff --git a/arch/mips/include/asm/stackframe.h b/arch/mips/include/asm/stackframe.h
index 65ef2ec..83606c4 100644
--- a/arch/mips/include/asm/stackframe.h
+++ b/arch/mips/include/asm/stackframe.h
@@ -157,10 +157,102 @@
 		.endm
 #endif
 
+#ifdef CONFIG_KVM
+		.macro KVM_SAVE_GUEST
+
+		/* guestctl1 reg: save the guest id reg, clear guestctl1.rid */
+		mfc0    v1, $10, 4
+		LONG_S  v1, PT_GUESTCTL1(sp)
+		andi    v1, v1, 0xff
+		mtc0    v1, $10, 4
+		mfc0    v1, $8, 1
+		LONG_S  v1, PT_BADINSTR(sp)
+		mfc0    v1, $8, 2
+		LONG_S  v1, PT_BADINSTRP(sp)
+
+		dmfc0   v0, $22, 7
+		LONG_S  v0, PT_OSSCRATCH7(sp)
+
+		/* restore HVA page table */
+		mfc0    k0, $4, 0
+		srl     k0, k0, 23
+		PTR_LA  k1, pgd_current
+		daddu   v1, k1, k0
+		LONG_L  k1, VCPU_ARCH_HVA_PGD(v0)
+		LONG_S  k1, 0(v1)
+		mfc0    k0, $6, 6
+		srl     k0, k0, 31
+		beqz    k0, 81f
+		dmfc0   v1, $5, 5
+		LONG_S  k1, 0(v1)
+		81:
+
+		dmfgc0  k0, $15, 1
+		LONG_S  k0, PT_GUEST_EBASE(sp)
+		li      v1, 0
+		mtc0    v1, $12, 6
+		.endm
+
+		.macro KVM_RESTORE_GUEST
+		LONG_L  v0, PT_GUESTCTL1(sp)
+		mtc0    v0, $10, 4
+
+		LONG_L  v0, PT_OSSCRATCH7(sp)
+		dmtc0   v0, $22, 7
+
+		/* restore GPA page table */
+		mfc0    k0, $4, 0
+		srl     k0, k0, 23
+		PTR_LA  k1, pgd_current
+		daddu   v1, k1, k0
+		LONG_L  k1, 0(v1)
+		LONG_S  k1, VCPU_ARCH_HVA_PGD(v0)
+		LONG_L  k1, VCPU_ARCH_GPA_PGD(v0)
+		LONG_S  k1, 0(v1)
+		mfc0    k0, $6, 6
+		srl     k0, k0, 31
+		beqz    k0, 82f
+		dmfc0   v1, $5, 5
+		LONG_S  k1, 0(v1)
+		82:
+		/* Check whether we have other pending interrupts or not,
+ 		* assert EIRR bits if needed. EIRR[6:7] should be
+ 		* cleared since it is not the right way to assert
+ 		* time and performance counter interrupt.
+ 		*/
+		LONG_L  v0, PT_OSSCRATCH7(sp)
+		dmfgc0  v1, $9, 6 /* EIRR */
+		ori     v1, 0xc0
+		xori    v1, 0xc0
+
+		34:
+		lld     k0, VCPU_ARCH_PIP_VECTOR(v0)
+		or      k0, v1
+		li      k1, 0
+		scd     k1, VCPU_ARCH_PIP_VECTOR(v0)
+		beqz    k1, 34b
+
+		dmtgc0  k0, $9, 6
+		.endm
+#endif
+
 		.macro	SAVE_SOME
 		.set	push
 		.set	noat
 		.set	reorder
+
+#ifdef CONFIG_KVM
+		mfc0    k0, $12, 6
+		srl     k0, 31
+		beqz    k0, 7f
+
+		/* guest mode */
+		dmfc0   k1, $22, 7
+		LONG_L  k1, VCPU_ARCH_HOST_STACK(k1)
+		j       8f
+7:
+#endif
+
 		mfc0	k0, CP0_STATUS
 		sll	k0, 3		/* extract cu0 bit */
 		.set	noreorder
@@ -201,6 +293,22 @@
 		LONG_S	k0, PT_TCSTATUS(sp)
 #endif /* CONFIG_MIPS_MT_SMTC */
 		LONG_S	$4, PT_R4(sp)
+
+#ifdef CONFIG_KVM
+		/* guestctl0 reg: save and clear the guest mode bit */
+		mfc0	v1, $12, 6
+		LONG_S	v1, PT_GUESTCTL0(sp)
+		srl	v1, 31
+		beqz	v1, 9f
+		KVM_SAVE_GUEST
+		9:
+		/* k0, k1 */
+		dmfc0	v1, $31, 2
+		LONG_S	v1, PT_R26(sp)
+		dmfc0	v1, $31, 3
+		LONG_S	v1, PT_R27(sp)
+#endif
+
 		mfc0	v1, CP0_CAUSE
 		LONG_S	$5, PT_R5(sp)
 		LONG_S	v1, PT_CAUSE(sp)
@@ -256,6 +364,20 @@
 		.endm
 
 		.macro	RESTORE_TEMP
+#ifdef CONFIG_KVM
+		LONG_L  v1, PT_GUESTCTL0(sp)
+		srl     v1, 31
+		beqz    v1, 80f
+
+		/* check whether an exit request has been generated. */
+		LONG_L  a0, PT_OSSCRATCH7(sp)
+		LONG_L  v0, VCPU_KVM(a0)
+		lw      v0, KVM_ARCH_EXIT_REQUEST(v0)
+		beqz    v0, 80f
+		dla     v0, kvm_xlp_check_exit_request
+		jalr    v0
+	80:
+#endif
 #ifdef CONFIG_CPU_HAS_SMARTMIPS
 		LONG_L	$24, PT_ACX(sp)
 		mtlhx	$24
@@ -390,6 +512,19 @@
 		and	v0, v1
 		or	v0, a0
 		mtc0	v0, CP0_STATUS
+
+#ifdef CONFIG_KVM
+		/* Status register has changed. No more interrupt may happen.
+		 * restore the guest state.
+		 */
+		LONG_L  v0, PT_GUESTCTL0(sp)
+		mtc0    v0, $12, 6
+		srl     v0, 31
+		beqz    v0, 10f
+		KVM_RESTORE_GUEST
+	10:
+#endif
+
 #ifdef CONFIG_MIPS_MT_SMTC
 /*
  * Only after EXL/ERL have been restored to status can we
@@ -462,6 +597,10 @@
 		.endm
 
 		.macro	RESTORE_SP_AND_RET
+#ifdef CONFIG_KVM
+		LONG_L  k0, PT_R26(sp)
+		LONG_L  k1, PT_R27(sp)
+#endif
 		LONG_L	sp, PT_R29(sp)
 		.set	mips3
 		eret
diff --git a/arch/mips/include/uapi/asm/ptrace.h b/arch/mips/include/uapi/asm/ptrace.h
index 0868a1606..c71fa98 100644
--- a/arch/mips/include/uapi/asm/ptrace.h
+++ b/arch/mips/include/uapi/asm/ptrace.h
@@ -47,6 +47,14 @@ struct pt_regs {
 	unsigned int msg_config;
 	unsigned int msg_err;
 #endif
+#ifdef CONFIG_KVM
+	unsigned long cp0_badinstr;
+	unsigned long cp0_badinstrp;
+	unsigned long cp0_guestctl0;
+	unsigned long cp0_guestctl1;
+	unsigned long cp0_osscratch7;
+	unsigned long guest_cp0_ebase;  /* 15, 1 */
+#endif
 } __attribute__ ((aligned (8)));
 #endif /* __KERNEL__ */
 
diff --git a/arch/mips/kernel/asm-offsets.c b/arch/mips/kernel/asm-offsets.c
index 614f030..69ab4d1 100644
--- a/arch/mips/kernel/asm-offsets.c
+++ b/arch/mips/kernel/asm-offsets.c
@@ -17,6 +17,13 @@
 #include <asm/ptrace.h>
 #include <asm/processor.h>
 
+#ifdef CONFIG_KVM
+#include <linux/kvm.h>
+#include <linux/kvm_host.h>
+#include <asm/kvm.h>
+#include <asm/kvm_host.h>
+#endif
+
 void output_ptreg_defines(void)
 {
 	COMMENT("MIPS pt_regs offsets.");
@@ -77,6 +84,14 @@ void output_ptreg_defines(void)
 	OFFSET(NLM_COP2_MSG_CONFIG, pt_regs, msg_config);
 	OFFSET(NLM_COP2_MSG_ERR, pt_regs, msg_err);
 #endif
+#ifdef CONFIG_KVM
+	OFFSET(PT_BADINSTR,  pt_regs, cp0_badinstr);
+	OFFSET(PT_BADINSTRP, pt_regs, cp0_badinstrp);
+	OFFSET(PT_GUESTCTL0, pt_regs, cp0_guestctl0);
+	OFFSET(PT_GUESTCTL1, pt_regs, cp0_guestctl1);
+	OFFSET(PT_OSSCRATCH7, pt_regs, cp0_osscratch7);
+	OFFSET(PT_GUEST_EBASE, pt_regs, guest_cp0_ebase);
+#endif
 	DEFINE(PT_SIZE, sizeof(struct pt_regs));
 	BLANK();
 }
@@ -134,6 +149,61 @@ void output_thread_defines(void)
 	       thread.cp0_baduaddr);
 	OFFSET(THREAD_ECODE, task_struct, \
 	       thread.error_code);
+#ifdef CONFIG_KVM
+	OFFSET(THREAD_VM_GUESTCTL0, task_struct, thread.vm.cp0_guestctl0);
+	OFFSET(THREAD_VM_GUESTCTL1, task_struct, thread.vm.cp0_guestctl1);
+	OFFSET(THREAD_VM_COUNT,     task_struct, thread.vm.cp0_count);
+	OFFSET(THREAD_VM_GTOFFSET,  task_struct, thread.vm.cp0_gtoffset);
+	OFFSET(THREAD_VM_OSSCRATCH7, task_struct, thread.vm.cp0_osscratch7);
+
+	OFFSET(THREAD_VM_GUEST_INDEX, task_struct, thread.vm.guest_cp0_index);
+	OFFSET(THREAD_VM_GUEST_RANDOM, task_struct, thread.vm.guest_cp0_random);
+	OFFSET(THREAD_VM_GUEST_ENTRYLO0, task_struct, thread.vm.guest_cp0_entrylo0);
+	OFFSET(THREAD_VM_GUEST_ENTRYLO1, task_struct, thread.vm.guest_cp0_entrylo1);
+	OFFSET(THREAD_VM_GUEST_CONTEXT, task_struct, thread.vm.guest_cp0_context);
+	OFFSET(THREAD_VM_GUEST_USERLOCAL, task_struct, thread.vm.guest_cp0_userlocal);
+	OFFSET(THREAD_VM_GUEST_PAGEMASK, task_struct, thread.vm.guest_cp0_pagemask);
+	OFFSET(THREAD_VM_GUEST_PAGEGRAIN, task_struct, thread.vm.guest_cp0_pagegrain);
+	OFFSET(THREAD_VM_GUEST_PWBASE, task_struct, thread.vm.guest_cp0_pwbase);
+	OFFSET(THREAD_VM_GUEST_PWFIELD, task_struct, thread.vm.guest_cp0_pwfield);
+	OFFSET(THREAD_VM_GUEST_PWSIZE, task_struct, thread.vm.guest_cp0_pwsize);
+	OFFSET(THREAD_VM_GUEST_PWCTL, task_struct, thread.vm.guest_cp0_pwctl);
+	OFFSET(THREAD_VM_GUEST_WIRED, task_struct, thread.vm.guest_cp0_wired);
+	OFFSET(THREAD_VM_GUEST_HWRENA, task_struct, thread.vm.guest_cp0_hwrena);
+	OFFSET(THREAD_VM_GUEST_BADVADDR, task_struct, thread.vm.guest_cp0_badvaddr);
+	OFFSET(THREAD_VM_GUEST_EIRR, task_struct, thread.vm.guest_cp0_eirr);
+	OFFSET(THREAD_VM_GUEST_EIMR, task_struct, thread.vm.guest_cp0_eimr);
+	OFFSET(THREAD_VM_GUEST_ENTRYHI, task_struct, thread.vm.guest_cp0_entryhi);
+	OFFSET(THREAD_VM_GUEST_COMPARE, task_struct, thread.vm.guest_cp0_compare);
+	OFFSET(THREAD_VM_GUEST_STATUS, task_struct, thread.vm.guest_cp0_status);
+	OFFSET(THREAD_VM_GUEST_INTCTL, task_struct, thread.vm.guest_cp0_intctl);
+	OFFSET(THREAD_VM_GUEST_CAUSE, task_struct, thread.vm.guest_cp0_cause);
+	OFFSET(THREAD_VM_GUEST_EPC, task_struct, thread.vm.guest_cp0_epc);
+	OFFSET(THREAD_VM_GUEST_EBASE, task_struct, thread.vm.guest_cp0_ebase);
+	OFFSET(THREAD_VM_GUEST_CONFIG0, task_struct, thread.vm.guest_cp0_config0);
+	OFFSET(THREAD_VM_GUEST_CONFIG1, task_struct, thread.vm.guest_cp0_config1);
+	OFFSET(THREAD_VM_GUEST_CONFIG4, task_struct, thread.vm.guest_cp0_config4);
+	OFFSET(THREAD_VM_GUEST_XCONTEXT, task_struct, thread.vm.guest_cp0_xcontext);
+	OFFSET(THREAD_VM_GUEST_ERROREPC, task_struct, thread.vm.guest_cp0_errorepc);
+	OFFSET(THREAD_VM_GUEST_OSSCRATCH0, task_struct, thread.vm.guest_cp0_osscratch0);
+	OFFSET(THREAD_VM_GUEST_OSSCRATCH1, task_struct, thread.vm.guest_cp0_osscratch1);
+	OFFSET(THREAD_VM_GUEST_OSSCRATCH2, task_struct, thread.vm.guest_cp0_osscratch2);
+	OFFSET(THREAD_VM_GUEST_OSSCRATCH3, task_struct, thread.vm.guest_cp0_osscratch3);
+	OFFSET(THREAD_VM_GUEST_OSSCRATCH4, task_struct, thread.vm.guest_cp0_osscratch4);
+	OFFSET(THREAD_VM_GUEST_OSSCRATCH5, task_struct, thread.vm.guest_cp0_osscratch5);
+	OFFSET(THREAD_VM_GUEST_OSSCRATCH6, task_struct, thread.vm.guest_cp0_osscratch6);
+	OFFSET(THREAD_VM_GUEST_OSSCRATCH7, task_struct, thread.vm.guest_cp0_osscratch7);
+	OFFSET(THREAD_VM_GUEST_BADINSTR, task_struct, thread.vm.guest_cp0_badinstr);
+	OFFSET(THREAD_VM_GUEST_BADINSTRP, task_struct, thread.vm.guest_cp0_badinstrp);
+	OFFSET(THREAD_VM_GUEST_CONTEXTCONFIG, task_struct, thread.vm.guest_cp0_contextconfig);
+	OFFSET(THREAD_VM_GUEST_XCONTEXTCONFIG, task_struct, thread.vm.guest_cp0_xcontextconfig);
+
+	OFFSET(THREAD_VM_GUEST_WIRED_PAGEMASK_0, task_struct, thread.vm.guest_wired_pagemask[0]);
+	OFFSET(THREAD_VM_GUEST_WIRED_ENTRYHI_0,  task_struct, thread.vm.guest_wired_entryhi[0]);
+	OFFSET(THREAD_VM_GUEST_WIRED_ENTRYLO0_0, task_struct, thread.vm.guest_wired_entrylo0[0]);
+	OFFSET(THREAD_VM_GUEST_WIRED_ENTRYLO1_0, task_struct, thread.vm.guest_wired_entrylo1[0]);
+	OFFSET(THREAD_VM_GUEST_WIRED_GUESTCTL1_0, task_struct, thread.vm.guest_wired_guestctl1[0]);
+#endif
 	BLANK();
 }
 
@@ -337,3 +407,192 @@ void output_pbe_defines(void)
 	BLANK();
 }
 #endif
+
+#ifdef CONFIG_KVM
+void output_kvm_defines(void)
+{
+	COMMENT("KVM vcpu reg offsets ");
+	OFFSET(VCPU_G_R0,  kvm_vcpu, arch.guest.gpu.regs[0]);
+	OFFSET(VCPU_G_R1,  kvm_vcpu, arch.guest.gpu.regs[1]);
+	OFFSET(VCPU_G_R2,  kvm_vcpu, arch.guest.gpu.regs[2]);
+	OFFSET(VCPU_G_R3,  kvm_vcpu, arch.guest.gpu.regs[3]);
+	OFFSET(VCPU_G_R4,  kvm_vcpu, arch.guest.gpu.regs[4]);
+	OFFSET(VCPU_G_R5,  kvm_vcpu, arch.guest.gpu.regs[5]);
+	OFFSET(VCPU_G_R6,  kvm_vcpu, arch.guest.gpu.regs[6]);
+	OFFSET(VCPU_G_R7,  kvm_vcpu, arch.guest.gpu.regs[7]);
+	OFFSET(VCPU_G_R8,  kvm_vcpu, arch.guest.gpu.regs[8]);
+	OFFSET(VCPU_G_R9,  kvm_vcpu, arch.guest.gpu.regs[9]);
+	OFFSET(VCPU_G_R10, kvm_vcpu, arch.guest.gpu.regs[10]);
+	OFFSET(VCPU_G_R11, kvm_vcpu, arch.guest.gpu.regs[11]);
+	OFFSET(VCPU_G_R12, kvm_vcpu, arch.guest.gpu.regs[12]);
+	OFFSET(VCPU_G_R13, kvm_vcpu, arch.guest.gpu.regs[13]);
+	OFFSET(VCPU_G_R14, kvm_vcpu, arch.guest.gpu.regs[14]);
+	OFFSET(VCPU_G_R15, kvm_vcpu, arch.guest.gpu.regs[15]);
+	OFFSET(VCPU_G_R16, kvm_vcpu, arch.guest.gpu.regs[16]);
+	OFFSET(VCPU_G_R17, kvm_vcpu, arch.guest.gpu.regs[17]);
+	OFFSET(VCPU_G_R18, kvm_vcpu, arch.guest.gpu.regs[18]);
+	OFFSET(VCPU_G_R19, kvm_vcpu, arch.guest.gpu.regs[19]);
+	OFFSET(VCPU_G_R20, kvm_vcpu, arch.guest.gpu.regs[20]);
+	OFFSET(VCPU_G_R21, kvm_vcpu, arch.guest.gpu.regs[21]);
+	OFFSET(VCPU_G_R22, kvm_vcpu, arch.guest.gpu.regs[22]);
+	OFFSET(VCPU_G_R23, kvm_vcpu, arch.guest.gpu.regs[23]);
+	OFFSET(VCPU_G_R24, kvm_vcpu, arch.guest.gpu.regs[24]);
+	OFFSET(VCPU_G_R25, kvm_vcpu, arch.guest.gpu.regs[25]);
+	OFFSET(VCPU_G_R26, kvm_vcpu, arch.guest.gpu.regs[26]);
+	OFFSET(VCPU_G_R27, kvm_vcpu, arch.guest.gpu.regs[27]);
+	OFFSET(VCPU_G_R28, kvm_vcpu, arch.guest.gpu.regs[28]);
+	OFFSET(VCPU_G_R29, kvm_vcpu, arch.guest.gpu.regs[29]);
+	OFFSET(VCPU_G_R30, kvm_vcpu, arch.guest.gpu.regs[30]);
+	OFFSET(VCPU_G_R31, kvm_vcpu, arch.guest.gpu.regs[31]);
+	OFFSET(VCPU_G_HI,  kvm_vcpu, arch.guest.gpu.hi);
+	OFFSET(VCPU_G_LO,  kvm_vcpu, arch.guest.gpu.lo);
+
+	OFFSET(VCPU_G_R_GUESTCTL0, kvm_vcpu, arch.guest.gpu.root_guestctl0);
+	OFFSET(VCPU_G_R_GUESTCTL1, kvm_vcpu, arch.guest.gpu.root_guestctl1);
+	OFFSET(VCPU_G_R_EPC,       kvm_vcpu, arch.guest.gpu.root_epc);
+	OFFSET(VCPU_G_R_COUNT,     kvm_vcpu, arch.guest.gpu.root_count);
+	OFFSET(VCPU_G_R_GTOFFSET,  kvm_vcpu, arch.guest.gpu.root_gtoffset);
+
+	OFFSET(VCPU_G_INDEX,    kvm_vcpu, arch.guest.gpu.guest_index);
+	OFFSET(VCPU_G_RANDOM,   kvm_vcpu, arch.guest.gpu.guest_random);
+	OFFSET(VCPU_G_ENTRYLO0, kvm_vcpu, arch.guest.gpu.guest_entrylo0);
+	OFFSET(VCPU_G_ENTRYLO1, kvm_vcpu, arch.guest.gpu.guest_entrylo1);
+	OFFSET(VCPU_G_CONTEXT,  kvm_vcpu, arch.guest.gpu.guest_context);
+	OFFSET(VCPU_G_USERLOCAL,kvm_vcpu, arch.guest.gpu.guest_userlocal);
+	OFFSET(VCPU_G_PAGEMASK, kvm_vcpu, arch.guest.gpu.guest_pagemask);
+	OFFSET(VCPU_G_PAGEGRAIN,kvm_vcpu, arch.guest.gpu.guest_pagegrain);
+	OFFSET(VCPU_G_PWBASE,   kvm_vcpu, arch.guest.gpu.guest_pwbase);
+	OFFSET(VCPU_G_PWFIELD,  kvm_vcpu, arch.guest.gpu.guest_pwfield);
+	OFFSET(VCPU_G_PWSIZE,   kvm_vcpu, arch.guest.gpu.guest_pwsize);
+	OFFSET(VCPU_G_WIRED,    kvm_vcpu, arch.guest.gpu.guest_wired);
+	OFFSET(VCPU_G_PWCTL,    kvm_vcpu, arch.guest.gpu.guest_pwctl);
+	OFFSET(VCPU_G_HWRENA,   kvm_vcpu, arch.guest.gpu.guest_hwrena);
+	OFFSET(VCPU_G_BADVADDR, kvm_vcpu, arch.guest.gpu.guest_badvaddr);
+	OFFSET(VCPU_G_EIRR,     kvm_vcpu, arch.guest.gpu.guest_eirr);
+	OFFSET(VCPU_G_EIMR,     kvm_vcpu, arch.guest.gpu.guest_eimr);
+	OFFSET(VCPU_G_ENTRYHI,  kvm_vcpu, arch.guest.gpu.guest_entryhi);
+	OFFSET(VCPU_G_COMPARE,  kvm_vcpu, arch.guest.gpu.guest_compare);
+	OFFSET(VCPU_G_STATUS,   kvm_vcpu, arch.guest.gpu.guest_status);
+	OFFSET(VCPU_G_INTCTL,   kvm_vcpu, arch.guest.gpu.guest_intctl);
+	OFFSET(VCPU_G_CAUSE,    kvm_vcpu, arch.guest.gpu.guest_cause);
+	OFFSET(VCPU_G_EPC,      kvm_vcpu, arch.guest.gpu.guest_epc);
+	OFFSET(VCPU_G_EBASE,    kvm_vcpu, arch.guest.gpu.guest_ebase);
+	OFFSET(VCPU_G_CONFIG0,  kvm_vcpu, arch.guest.gpu.guest_config0);
+	OFFSET(VCPU_G_CONFIG1,  kvm_vcpu, arch.guest.gpu.guest_config1);
+	OFFSET(VCPU_G_CONFIG4,  kvm_vcpu, arch.guest.gpu.guest_config4);
+	OFFSET(VCPU_G_XCONTEXT, kvm_vcpu, arch.guest.gpu.guest_xcontext);
+	OFFSET(VCPU_G_ERROREPC, kvm_vcpu, arch.guest.gpu.guest_errorepc);
+	OFFSET(VCPU_G_OSSCRATCH0, kvm_vcpu, arch.guest.gpu.guest_osscratch0);
+	OFFSET(VCPU_G_OSSCRATCH1, kvm_vcpu, arch.guest.gpu.guest_osscratch1);
+	OFFSET(VCPU_G_OSSCRATCH2, kvm_vcpu, arch.guest.gpu.guest_osscratch2);
+	OFFSET(VCPU_G_OSSCRATCH3, kvm_vcpu, arch.guest.gpu.guest_osscratch3);
+	OFFSET(VCPU_G_OSSCRATCH4, kvm_vcpu, arch.guest.gpu.guest_osscratch4);
+	OFFSET(VCPU_G_OSSCRATCH5, kvm_vcpu, arch.guest.gpu.guest_osscratch5);
+	OFFSET(VCPU_G_OSSCRATCH6, kvm_vcpu, arch.guest.gpu.guest_osscratch6);
+	OFFSET(VCPU_G_OSSCRATCH7, kvm_vcpu, arch.guest.gpu.guest_osscratch7);
+	OFFSET(VCPU_G_BADINSTR,   kvm_vcpu, arch.guest.gpu.guest_badinstr);
+	OFFSET(VCPU_G_BADINSTRP,  kvm_vcpu, arch.guest.gpu.guest_badinstrp);
+	OFFSET(VCPU_G_CONTEXTCONFIG,   kvm_vcpu, arch.guest.gpu.guest_contextconfig);
+	OFFSET(VCPU_G_XCONTEXTCONFIG,  kvm_vcpu, arch.guest.gpu.guest_xcontextconfig);
+
+	OFFSET(VCPU_G_WIRED_PAGEMASK_0, kvm_vcpu, arch.guest.gpu.guest_wired_pagemask[0]);
+	OFFSET(VCPU_G_WIRED_ENTRYHI_0,  kvm_vcpu, arch.guest.gpu.guest_wired_entryhi[0]);
+	OFFSET(VCPU_G_WIRED_ENTRYLO0_0, kvm_vcpu, arch.guest.gpu.guest_wired_entrylo0[0]);
+	OFFSET(VCPU_G_WIRED_ENTRYLO1_0, kvm_vcpu, arch.guest.gpu.guest_wired_entrylo1[0]);
+
+	OFFSET(VCPU_G_FP0,  kvm_vcpu, arch.guest.fpu.regs[0]);
+	OFFSET(VCPU_G_FP1,  kvm_vcpu, arch.guest.fpu.regs[1]);
+	OFFSET(VCPU_G_FP2,  kvm_vcpu, arch.guest.fpu.regs[2]);
+	OFFSET(VCPU_G_FP3,  kvm_vcpu, arch.guest.fpu.regs[3]);
+	OFFSET(VCPU_G_FP4,  kvm_vcpu, arch.guest.fpu.regs[4]);
+	OFFSET(VCPU_G_FP5,  kvm_vcpu, arch.guest.fpu.regs[5]);
+	OFFSET(VCPU_G_FP6,  kvm_vcpu, arch.guest.fpu.regs[6]);
+	OFFSET(VCPU_G_FP7,  kvm_vcpu, arch.guest.fpu.regs[7]);
+	OFFSET(VCPU_G_FP8,  kvm_vcpu, arch.guest.fpu.regs[8]);
+	OFFSET(VCPU_G_FP9,  kvm_vcpu, arch.guest.fpu.regs[9]);
+	OFFSET(VCPU_G_FP10, kvm_vcpu, arch.guest.fpu.regs[10]);
+	OFFSET(VCPU_G_FP11, kvm_vcpu, arch.guest.fpu.regs[11]);
+	OFFSET(VCPU_G_FP12, kvm_vcpu, arch.guest.fpu.regs[12]);
+	OFFSET(VCPU_G_FP13, kvm_vcpu, arch.guest.fpu.regs[13]);
+	OFFSET(VCPU_G_FP14, kvm_vcpu, arch.guest.fpu.regs[14]);
+	OFFSET(VCPU_G_FP15, kvm_vcpu, arch.guest.fpu.regs[15]);
+	OFFSET(VCPU_G_FP16, kvm_vcpu, arch.guest.fpu.regs[16]);
+	OFFSET(VCPU_G_FP17, kvm_vcpu, arch.guest.fpu.regs[17]);
+	OFFSET(VCPU_G_FP18, kvm_vcpu, arch.guest.fpu.regs[18]);
+	OFFSET(VCPU_G_FP19, kvm_vcpu, arch.guest.fpu.regs[19]);
+	OFFSET(VCPU_G_FP20, kvm_vcpu, arch.guest.fpu.regs[20]);
+	OFFSET(VCPU_G_FP21, kvm_vcpu, arch.guest.fpu.regs[21]);
+	OFFSET(VCPU_G_FP22, kvm_vcpu, arch.guest.fpu.regs[22]);
+	OFFSET(VCPU_G_FP23, kvm_vcpu, arch.guest.fpu.regs[23]);
+	OFFSET(VCPU_G_FP24, kvm_vcpu, arch.guest.fpu.regs[24]);
+	OFFSET(VCPU_G_FP25, kvm_vcpu, arch.guest.fpu.regs[25]);
+	OFFSET(VCPU_G_FP26, kvm_vcpu, arch.guest.fpu.regs[26]);
+	OFFSET(VCPU_G_FP27, kvm_vcpu, arch.guest.fpu.regs[27]);
+	OFFSET(VCPU_G_FP28, kvm_vcpu, arch.guest.fpu.regs[28]);
+	OFFSET(VCPU_G_FP29, kvm_vcpu, arch.guest.fpu.regs[29]);
+	OFFSET(VCPU_G_FP30, kvm_vcpu, arch.guest.fpu.regs[30]);
+	OFFSET(VCPU_G_FP31, kvm_vcpu, arch.guest.fpu.regs[31]);
+	OFFSET(VCPU_G_FIR,  kvm_vcpu, arch.guest.fpu.fir);
+	OFFSET(VCPU_G_FCCR, kvm_vcpu, arch.guest.fpu.fccr);
+	OFFSET(VCPU_G_FEXR, kvm_vcpu, arch.guest.fpu.fexr);
+	OFFSET(VCPU_G_FENR, kvm_vcpu, arch.guest.fpu.fenr);
+	OFFSET(VCPU_G_FCSR, kvm_vcpu, arch.guest.fpu.fcsr);
+
+	OFFSET(VCPU_R_S0,   kvm_vcpu, arch.root.sregs[0]);
+	OFFSET(VCPU_R_S1,   kvm_vcpu, arch.root.sregs[1]);
+	OFFSET(VCPU_R_S2,   kvm_vcpu, arch.root.sregs[2]);
+	OFFSET(VCPU_R_S3,   kvm_vcpu, arch.root.sregs[3]);
+	OFFSET(VCPU_R_S4,   kvm_vcpu, arch.root.sregs[4]);
+	OFFSET(VCPU_R_S5,   kvm_vcpu, arch.root.sregs[5]);
+	OFFSET(VCPU_R_S6,   kvm_vcpu, arch.root.sregs[6]);
+	OFFSET(VCPU_R_S7,   kvm_vcpu, arch.root.sregs[7]);
+	OFFSET(VCPU_R_GP,   kvm_vcpu, arch.root.gp);
+	OFFSET(VCPU_R_SP,   kvm_vcpu, arch.root.sp);
+	OFFSET(VCPU_R_FP,   kvm_vcpu, arch.root.fp);
+	OFFSET(VCPU_R_RA,   kvm_vcpu, arch.root.ra);
+	OFFSET(VCPU_R_FP0,  kvm_vcpu, arch.root.fpr[0]);
+	OFFSET(VCPU_R_FP1,  kvm_vcpu, arch.root.fpr[1]);
+	OFFSET(VCPU_R_FP2,  kvm_vcpu, arch.root.fpr[2]);
+	OFFSET(VCPU_R_FP3,  kvm_vcpu, arch.root.fpr[3]);
+	OFFSET(VCPU_R_FP4,  kvm_vcpu, arch.root.fpr[4]);
+	OFFSET(VCPU_R_FP5,  kvm_vcpu, arch.root.fpr[5]);
+	OFFSET(VCPU_R_FP6,  kvm_vcpu, arch.root.fpr[6]);
+	OFFSET(VCPU_R_FP7,  kvm_vcpu, arch.root.fpr[7]);
+	OFFSET(VCPU_R_FP8,  kvm_vcpu, arch.root.fpr[8]);
+	OFFSET(VCPU_R_FP9,  kvm_vcpu, arch.root.fpr[9]);
+	OFFSET(VCPU_R_FP10, kvm_vcpu, arch.root.fpr[10]);
+	OFFSET(VCPU_R_FP11, kvm_vcpu, arch.root.fpr[11]);
+	OFFSET(VCPU_R_FP12, kvm_vcpu, arch.root.fpr[12]);
+	OFFSET(VCPU_R_FP13, kvm_vcpu, arch.root.fpr[13]);
+	OFFSET(VCPU_R_FP14, kvm_vcpu, arch.root.fpr[14]);
+	OFFSET(VCPU_R_FP15, kvm_vcpu, arch.root.fpr[15]);
+	OFFSET(VCPU_R_FP16, kvm_vcpu, arch.root.fpr[16]);
+	OFFSET(VCPU_R_FP17, kvm_vcpu, arch.root.fpr[17]);
+	OFFSET(VCPU_R_FP18, kvm_vcpu, arch.root.fpr[18]);
+	OFFSET(VCPU_R_FP19, kvm_vcpu, arch.root.fpr[19]);
+	OFFSET(VCPU_R_FP20, kvm_vcpu, arch.root.fpr[20]);
+	OFFSET(VCPU_R_FP21, kvm_vcpu, arch.root.fpr[21]);
+	OFFSET(VCPU_R_FP22, kvm_vcpu, arch.root.fpr[22]);
+	OFFSET(VCPU_R_FP23, kvm_vcpu, arch.root.fpr[23]);
+	OFFSET(VCPU_R_FP24, kvm_vcpu, arch.root.fpr[24]);
+	OFFSET(VCPU_R_FP25, kvm_vcpu, arch.root.fpr[25]);
+	OFFSET(VCPU_R_FP26, kvm_vcpu, arch.root.fpr[26]);
+	OFFSET(VCPU_R_FP27, kvm_vcpu, arch.root.fpr[27]);
+	OFFSET(VCPU_R_FP28, kvm_vcpu, arch.root.fpr[28]);
+	OFFSET(VCPU_R_FP29, kvm_vcpu, arch.root.fpr[29]);
+	OFFSET(VCPU_R_FP30, kvm_vcpu, arch.root.fpr[30]);
+	OFFSET(VCPU_R_FP31, kvm_vcpu, arch.root.fpr[31]);
+	OFFSET(VCPU_R_FCR,  kvm_vcpu, arch.root.fcr);
+	OFFSET(VCPU_R_STATUS, kvm_vcpu, arch.root.status);
+
+	OFFSET(VCPU_ARCH_PIP_VECTOR, kvm_vcpu, arch.pip_vector);
+	OFFSET(VCPU_ARCH_HOST_STACK, kvm_vcpu, arch.host_stack);
+	OFFSET(VCPU_ARCH_INIT_GUEST, kvm_vcpu, arch.init_guest);
+	OFFSET(VCPU_ARCH_HVA_PGD, kvm_vcpu, arch.hva_pgd);
+	OFFSET(VCPU_ARCH_GPA_PGD, kvm_vcpu, arch.gpa_pgd);
+	OFFSET(VCPU_KVM, kvm_vcpu, kvm);
+	OFFSET(KVM_ARCH_EXIT_REQUEST, kvm, arch.exit_request);
+
+	BLANK();
+}
+#endif
diff --git a/arch/mips/kernel/genex.S b/arch/mips/kernel/genex.S
index e93b4ee..ca694b6 100644
--- a/arch/mips/kernel/genex.S
+++ b/arch/mips/kernel/genex.S
@@ -46,8 +46,9 @@
 NESTED(except_vec3_generic, 0, sp)
 	.set	push
 	.set	noat
-#ifdef CONFIG_CPU_XLP
-	_ehb
+#ifdef CONFIG_KVM
+	dmtc0	k0, $31, 2
+	dmtc0	k1, $31, 3
 #endif
 #if R5432_CP0_INTERRUPT_WAR
 	mfc0	k0, CP0_INDEX
@@ -155,6 +156,11 @@ LEAF(__r4k_wait)
 	FEXPORT(rollback_\handler)
 	.set	push
 	.set	noat
+#ifdef CONFIG_KVM
+	MFC0	k0, $12, 6
+	srl	k0, 31
+	bnez	k0, 9f
+#endif
 	MFC0	k0, CP0_EPC
 	PTR_LA	k1, __r4k_wait
 	ori	k0, 0x1f	/* 32 byte rollback region */
@@ -484,6 +490,9 @@ NESTED(nmi_handler, PT_SIZE, sp)
 	BUILD_HANDLER mcheck mcheck cli verbose		/* #24 */
 	BUILD_HANDLER mt mt sti silent			/* #25 */
 	BUILD_HANDLER dsp dsp sti silent		/* #26 */
+#ifdef CONFIG_KVM
+	BUILD_HANDLER virt virt sti silent		/* #27 */
+#endif
 	BUILD_HANDLER reserved reserved sti verbose	/* others */
 
 	.align	5
diff --git a/arch/mips/kernel/traps.c b/arch/mips/kernel/traps.c
index d7ebd21..4ec5ea2 100644
--- a/arch/mips/kernel/traps.c
+++ b/arch/mips/kernel/traps.c
@@ -80,8 +80,14 @@ extern asmlinkage void handle_mdmx(void);
 extern asmlinkage void handle_watch(void);
 extern asmlinkage void handle_mt(void);
 extern asmlinkage void handle_dsp(void);
+#ifdef CONFIG_KVM
+extern asmlinkage void handle_virt(void);
+#endif
 extern asmlinkage void handle_mcheck(void);
 extern asmlinkage void handle_reserved(void);
+#ifdef CONFIG_KVM
+extern void process_virt_exception(struct pt_regs *);
+#endif
 
 void (*board_be_init)(void);
 int (*board_be_handler)(struct pt_regs *regs, int is_fixup);
@@ -309,6 +315,13 @@ static void __show_regs(const struct pt_regs *regs)
 
 	printk("Cause : %08x\n", cause);
 
+#ifdef CONFIG_KVM
+	if (regs->cp0_guestctl0 >> 31)
+		printk("Exception in KVM: due to guest ...\n");
+	else
+		printk("Exception in KVM: due to root ...\n");
+#endif
+
 	cause = (cause & CAUSEF_EXCCODE) >> CAUSEB_EXCCODE;
 	if (1 <= cause && cause <= 5)
 		printk("BadVA : %0*lx\n", field, regs->cp0_badvaddr);
@@ -1248,6 +1261,13 @@ asmlinkage void do_dsp(struct pt_regs *regs)
 	force_sig(SIGILL, current);
 }
 
+#ifdef CONFIG_KVM
+asmlinkage void do_virt(struct pt_regs *regs)
+{
+	process_virt_exception(regs);
+}
+#endif
+
 asmlinkage void do_reserved(struct pt_regs *regs)
 {
 	/*
@@ -1485,10 +1505,15 @@ void __init *set_except_vector(int n, void *addr)
 #endif
 		u32 *buf = (u32 *)(ebase + 0x200);
 		unsigned int k0 = 26;
-
+#ifdef CONFIG_KVM
+		unsigned int k1 = 27;
+#endif
 		if (current_cpu_type() == CPU_XLP)
 			uasm_i_ehb(&buf);
-
+#ifdef CONFIG_KVM
+		uasm_i_dmtc0(&buf, k0, 31, 2);
+		uasm_i_dmtc0(&buf, k1, 31, 3);
+#endif
 		if ((handler & jump_mask) == ((ebase + 0x200) & jump_mask)) {
 			uasm_i_j(&buf, handler & ~jump_mask);
 			uasm_i_nop(&buf);
@@ -1955,6 +1980,10 @@ void __init trap_init(void)
 
 	set_except_vector(26, handle_dsp);
 
+#ifdef CONFIG_KVM
+	set_except_vector(27, handle_virt);
+#endif
+
 	if (board_cache_error_setup)
 		board_cache_error_setup();
 
diff --git a/arch/mips/kvm/Kconfig b/arch/mips/kvm/Kconfig
new file mode 100644
index 0000000..ff63681
--- /dev/null
+++ b/arch/mips/kvm/Kconfig
@@ -0,0 +1,24 @@
+#
+# KVM configuration
+#
+
+source "virt/kvm/Kconfig"
+
+menuconfig VIRTUALIZATION
+	bool "Virtualization"
+	---help---
+	  Say Y here to get to see options for using your Linux host to run
+	  other operating systems inside virtual machines (guests).
+	  This option alone does not add any kernel code.
+
+	  If you say N, all options in this submenu will be skipped and
+	  disabled.
+
+if VIRTUALIZATION
+
+config KVM
+	bool "Kernel Virtual Machine support"
+	select PREEMPT_NOTIFIERS
+	select ANON_INODES
+
+endif
diff --git a/arch/mips/kvm/Makefile b/arch/mips/kvm/Makefile
new file mode 100644
index 0000000..d9f55b9
--- /dev/null
+++ b/arch/mips/kvm/Makefile
@@ -0,0 +1,5 @@
+# Makefile for kernel virtual machines on xlp
+
+common-objs = $(addprefix ../../../virt/kvm/, kvm_main.o)
+
+obj-$(CONFIG_KVM) += $(common-objs) kvm.o xlp.o context_switch.o
diff --git a/arch/mips/kvm/context_switch.S b/arch/mips/kvm/context_switch.S
new file mode 100644
index 0000000..62bb40f
--- /dev/null
+++ b/arch/mips/kvm/context_switch.S
@@ -0,0 +1,295 @@
+
+/**
+ * The context switch between guest and hypervisor.
+ *
+ * What we need to save and restore? For guest, we need to restore
+ *   . all the general purpose registers (gpr and cop1)
+ *   . the cop2 registers
+ *   . the guest cop0 registers
+ */
+
+#include <asm/asm.h>
+#include <asm/asmmacro.h>
+#include <asm/regdef.h>
+#include <asm/mipsregs.h>
+#include <asm/stackframe.h>
+
+	.macro  kvm_cpu_save_nonscratch savearea
+	LONG_S  s0, VCPU_R_S0(\savearea)
+	LONG_S  s1, VCPU_R_S1(\savearea)
+	LONG_S  s2, VCPU_R_S2(\savearea)
+	LONG_S  s3, VCPU_R_S3(\savearea)
+	LONG_S  s4, VCPU_R_S4(\savearea)
+	LONG_S  s5, VCPU_R_S5(\savearea)
+	LONG_S  s6, VCPU_R_S6(\savearea)
+	LONG_S  s7, VCPU_R_S7(\savearea)
+	LONG_S  gp, VCPU_R_GP(\savearea)
+	LONG_S  sp, VCPU_R_SP(\savearea)
+	LONG_S  fp, VCPU_R_FP(\savearea)
+	LONG_S  ra, VCPU_R_RA(\savearea)
+	.endm
+
+	.macro  kvm_cpu_restore_nonscratch restorearea
+	LONG_L  s0, VCPU_R_S0(\restorearea)
+	LONG_L  s1, VCPU_R_S1(\restorearea)
+	LONG_L  s2, VCPU_R_S2(\restorearea)
+	LONG_L  s3, VCPU_R_S3(\restorearea)
+	LONG_L  s4, VCPU_R_S4(\restorearea)
+	LONG_L  s5, VCPU_R_S5(\restorearea)
+	LONG_L  s6, VCPU_R_S6(\restorearea)
+	LONG_L  s7, VCPU_R_S7(\restorearea)
+	LONG_L  gp, VCPU_R_GP(\restorearea)
+	LONG_L  sp, VCPU_R_SP(\restorearea)
+	LONG_L  fp, VCPU_R_FP(\restorearea)
+	LONG_L  ra, VCPU_R_RA(\restorearea)
+	.endm
+
+	.macro	kvm_cpu_restore_guest_wired_tlbs restorearea
+	LONG_L	t0, VCPU_G_WIRED(\restorearea)
+	li	t1, 0
+  49:
+	slt	t2, t1, t0
+	beqz	t2, 50f
+
+	sll	t1, t1, 3
+
+	daddiu	t3, \restorearea, VCPU_G_WIRED_PAGEMASK_0
+	daddu	t3, t3, t1
+	LONG_L	t2, 0(t3)
+	dmtgc0	t2, $5, 0
+
+	daddiu	t3, \restorearea, VCPU_G_WIRED_ENTRYHI_0
+	daddu	t3, t3, t1
+	LONG_L	t2, 0(t3)
+	mtgc0	t2, $10, 0
+
+	daddiu	t3, \restorearea, VCPU_G_WIRED_ENTRYLO0_0
+	daddu	t3, t3, t1
+	LONG_L	t2, 0(t3)
+	mtgc0	t2, $2, 0
+
+	daddiu	t3, \restorearea, VCPU_G_WIRED_ENTRYLO1_0
+	daddu	t3, t3, t1
+	LONG_L	t2, 0(t3)
+	mtgc0	t2, $3, 0
+
+	srl	t1, t1, 3
+
+	mtgc0	t1, $0, 0
+	tlbgwi
+
+	addiu	t1, t1, 1
+	j	49b
+
+  50:
+	.endm
+
+	.macro	kvm_cpu_restore_guest_cp0 restorearea
+	LONG_L	t0, VCPU_G_INDEX(\restorearea)
+	mtgc0	t0, $0, 0
+	LONG_L	t0, VCPU_G_RANDOM(\restorearea)
+	mtgc0	t0, $1, 0
+	LONG_L	t0, VCPU_G_ENTRYLO0(\restorearea)
+	mtgc0	t0, $2, 0
+	LONG_L	t0, VCPU_G_ENTRYLO1(\restorearea)
+	mtgc0	t0, $3, 0
+	LONG_L	t0, VCPU_G_CONTEXT(\restorearea)
+	mtgc0	t0, $4, 0
+	LONG_L	t0, VCPU_G_USERLOCAL(\restorearea)
+	mtgc0	t0, $4, 2
+	LONG_L	t0, VCPU_G_PAGEMASK(\restorearea)
+	dmtgc0	t0, $5, 0
+	LONG_L	t0, VCPU_G_PAGEGRAIN(\restorearea)
+	mtgc0	t0, $5, 1
+	LONG_L	t0, VCPU_G_PWBASE(\restorearea)
+	mtgc0	t0, $5, 5
+	LONG_L	t0, VCPU_G_PWFIELD(\restorearea)
+	mtgc0	t0, $5, 6
+	LONG_L	t0, VCPU_G_PWSIZE(\restorearea)
+	mtgc0	t0, $5, 7
+	LONG_L	t0, VCPU_G_WIRED(\restorearea)
+	mtgc0	t0, $6, 0
+	LONG_L	t0, VCPU_G_PWCTL(\restorearea)
+	mtgc0	t0, $6, 6
+	LONG_L	t0, VCPU_G_HWRENA(\restorearea)
+	mtgc0	t0, $7, 0
+	LONG_L	t0, VCPU_G_BADVADDR(\restorearea)
+	mtgc0	t0, $8, 0
+	LONG_L	t0, VCPU_G_EIRR(\restorearea)
+	mtgc0	t0, $9, 6
+	LONG_L	t0, VCPU_G_EIMR(\restorearea)
+	mtgc0	t0, $9, 7
+	LONG_L	t0, VCPU_G_ENTRYHI(\restorearea)
+	mtgc0	t0, $10, 0
+	LONG_L	t0, VCPU_G_COMPARE(\restorearea)
+	mtgc0	t0, $11, 0
+	LONG_L	t0, VCPU_G_STATUS(\restorearea)
+	mtgc0	t0, $12, 0
+	LONG_L	t0, VCPU_G_INTCTL(\restorearea)
+	mtgc0	t0, $12, 1
+	LONG_L	t0, VCPU_G_CAUSE(\restorearea)
+	mtgc0	t0, $13, 0
+	LONG_L	t0, VCPU_G_EPC(\restorearea)
+	mtgc0	t0, $14, 0
+	LONG_L	t0, VCPU_G_EBASE(\restorearea)
+	mtgc0	t0, $15, 1
+	LONG_L	t0, VCPU_G_CONFIG0(\restorearea)
+	mtgc0	t0, $16, 0
+	LONG_L	t0, VCPU_G_CONFIG1(\restorearea)
+	mtgc0	t0, $16, 1
+	LONG_L	t0, VCPU_G_CONFIG4(\restorearea)
+	mtgc0	t0, $16, 4
+	LONG_L	t0, VCPU_G_XCONTEXT(\restorearea)
+	mtgc0	t0, $20, 0
+	LONG_L	t0, VCPU_G_ERROREPC(\restorearea)
+	mtgc0	t0, $30, 0
+	LONG_L	t0, VCPU_G_OSSCRATCH0(\restorearea)
+	mtgc0	t0, $22, 0
+	LONG_L	t0, VCPU_G_OSSCRATCH1(\restorearea)
+	mtgc0	t0, $22, 1
+	LONG_L	t0, VCPU_G_OSSCRATCH2(\restorearea)
+	mtgc0	t0, $22, 2
+	LONG_L	t0, VCPU_G_OSSCRATCH3(\restorearea)
+	mtgc0	t0, $22, 3
+	LONG_L	t0, VCPU_G_OSSCRATCH4(\restorearea)
+	mtgc0	t0, $22, 4
+	LONG_L	t0, VCPU_G_OSSCRATCH5(\restorearea)
+	mtgc0	t0, $22, 5
+	LONG_L	t0, VCPU_G_OSSCRATCH6(\restorearea)
+	mtgc0	t0, $22, 6
+	LONG_L	t0, VCPU_G_OSSCRATCH7(\restorearea)
+	mtgc0	t0, $22, 7
+	LONG_L	t0, VCPU_G_BADINSTR(\restorearea)
+	mtgc0	t0, $8, 1
+	LONG_L	t0, VCPU_G_BADINSTRP(\restorearea)
+	mtgc0	t0, $8, 2
+	LONG_L	t0, VCPU_G_CONTEXTCONFIG(\restorearea)
+	mtgc0	t0, $4, 1
+	LONG_L	t0, VCPU_G_XCONTEXTCONFIG(\restorearea)
+	mtgc0	t0, $4, 3
+	.endm
+
+FEXPORT(__kvm_vcpu_run_guest)
+
+	/* essentially, it is a context switch, we must
+	 * save all relevant registers, and then load
+	 * relevant guest registers.
+	 * a0: kvm_vcpu
+	 */
+	mfc0 	t1, CP0_STATUS
+	LONG_S	t1, VCPU_R_STATUS(a0)
+	kvm_cpu_save_nonscratch a0
+	move	a1, a0
+
+	/*
+	 * set the Root.status.EXL
+	 * permit the CU1 access so that the guess CU1 access will be satisfied.
+	 * permit the Guest access to CU2 (more elegant solution may be needed later)
+	 */
+	mfc0 t0, $12, 0
+	ori  t0, t0, 0x2
+	lui  t1, 0x2400
+	or   t0, t0, t1
+	mtc0 t0, $12, 0
+
+	/* kernel stack pointer for guest triggered exceptions */
+	LONG_S	sp, VCPU_ARCH_HOST_STACK(a0)
+	dmtc0	a0, $22, 7
+
+	/* restore the guestid. */
+	LONG_L	t0, VCPU_G_R_GUESTCTL1(a0)
+	mtc0	t0, $10, 4
+
+        /* Switch to gpa page table for refill and hardware page table */
+        mfc0    t0, $4, 0
+        srl     t0, t0, 23
+        PTR_LA  t1, pgd_current
+        daddu   t2, t1, t0
+
+        /* save HVA_PGD and restore GPA_PGD */
+        LONG_L  t1, 0(t2)
+        LONG_S  t1, VCPU_ARCH_HVA_PGD(a0)
+        LONG_L  t1, VCPU_ARCH_GPA_PGD(a0)
+        LONG_S  t1, 0(t2)
+
+        /* restore GPA_PGD for hardware page walker */
+        mfc0    t0, $6, 6
+        srl     t0, t0, 31
+        beqz    t0, 80f
+        dmfc0   t2, $5, 5
+        LONG_S  t1, 0(t2)
+        80:
+
+	/* Check whether we need to restore guest states. */
+	LONG_L	t0, VCPU_ARCH_INIT_GUEST(a0)
+	beqz	t0, 2f
+
+	/* intialize the guest state */
+	LONG_S	zero, VCPU_ARCH_INIT_GUEST(a0)
+
+	/* flush the root/guest tlb, restore wired tlb, restore guest cp0 */
+	tlbginvf
+	tlbinvf
+	kvm_cpu_restore_guest_wired_tlbs a1
+	kvm_cpu_restore_guest_cp0 a1
+
+2:
+	/* CuestCtl0 & EPC */
+	LONG_L	t0, VCPU_G_R_GUESTCTL0(a1)
+	mtc0	t0, $12, 6
+	LONG_L	t0, VCPU_G_R_EPC(a1)
+	dmtc0	t0, $14, 0
+
+	/* general purpose registers */
+	move	sp, a1
+	LONG_L 	$1,  VCPU_G_R1(sp)
+	LONG_L 	$2,  VCPU_G_R2(sp)
+	LONG_L 	$3,  VCPU_G_R3(sp)
+	LONG_L 	$4,  VCPU_G_R4(sp)
+	LONG_L 	$5,  VCPU_G_R5(sp)
+	LONG_L 	$6,  VCPU_G_R6(sp)
+	LONG_L 	$7,  VCPU_G_R7(sp)
+	LONG_L 	$8,  VCPU_G_R8(sp)
+	LONG_L 	$9,  VCPU_G_R9(sp)
+	LONG_L 	$10, VCPU_G_R10(sp)
+	LONG_L 	$11, VCPU_G_R11(sp)
+	LONG_L 	$12, VCPU_G_R12(sp)
+	LONG_L 	$13, VCPU_G_R13(sp)
+	LONG_L 	$14, VCPU_G_R14(sp)
+	LONG_L 	$15, VCPU_G_R15(sp)
+	LONG_L 	$16, VCPU_G_R16(sp)
+	LONG_L 	$17, VCPU_G_R17(sp)
+	LONG_L 	$18, VCPU_G_R18(sp)
+	LONG_L 	$19, VCPU_G_R19(sp)
+	LONG_L 	$20, VCPU_G_R20(sp)
+	LONG_L 	$21, VCPU_G_R21(sp)
+	LONG_L 	$22, VCPU_G_R22(sp)
+	LONG_L 	$23, VCPU_G_R23(sp)
+	LONG_L 	$24, VCPU_G_R24(sp)
+	LONG_L 	$25, VCPU_G_R25(sp)
+	LONG_L 	$28, VCPU_G_R28(sp)
+	LONG_L 	$30, VCPU_G_R30(sp)
+	LONG_L 	$31, VCPU_G_R31(sp)
+	LONG_L 	k0,  VCPU_G_HI(sp)
+	mthi 	k0
+	LONG_L 	k0,  VCPU_G_HI(sp)
+	mtlo 	k0
+
+	LONG_L 	$26, VCPU_G_R26(sp)
+	LONG_L 	$27, VCPU_G_R27(sp)
+	LONG_L 	$29, VCPU_G_R29(sp)
+
+	/* jump to the guest land */
+	eret
+
+FEXPORT(__kvm_vcpu_leave_guest)
+
+	/* Leave the guest and return to QEMU for service,
+	 * a0: kvm_vcpu_host
+	 * a1: exit_reason and exit information
+	 */
+	kvm_cpu_restore_nonscratch a0
+	LONG_L	t1, VCPU_R_STATUS(a0)
+	mtc0 	t1, CP0_STATUS
+	move	v0, a1
+	jr	ra
diff --git a/arch/mips/kvm/kvm.c b/arch/mips/kvm/kvm.c
new file mode 100644
index 0000000..ea6aff5
--- /dev/null
+++ b/arch/mips/kvm/kvm.c
@@ -0,0 +1,686 @@
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/errno.h>
+#include <linux/err.h>
+#include <linux/vmalloc.h>
+#include <linux/file.h>
+#include <linux/kvm.h>
+#include <linux/kvm_host.h>
+#include <linux/anon_inodes.h>
+
+#include <linux/kvm_host.h>
+#include <linux/kvm.h>
+#include <linux/module.h>
+#include <linux/errno.h>
+#include <linux/percpu.h>
+#include <linux/gfp.h>
+#include <linux/mm.h>
+#include <linux/miscdevice.h>
+#include <linux/vmalloc.h>
+#include <linux/reboot.h>
+#include <linux/debugfs.h>
+#include <linux/highmem.h>
+#include <linux/file.h>
+#include <linux/cpu.h>
+#include <linux/sched.h>
+#include <linux/cpumask.h>
+#include <linux/smp.h>
+#include <linux/anon_inodes.h>
+#include <linux/profile.h>
+#include <linux/kvm_para.h>
+#include <linux/pagemap.h>
+#include <linux/mman.h>
+#include <linux/swap.h>
+#include <linux/bitops.h>
+#include <linux/spinlock.h>
+
+#include <asm/pgalloc.h>
+#include <asm/mmu_context.h>
+#include <asm/netlogic/kvm_xlp.h>
+
+struct kvm_stats_debugfs_item debugfs_entries[] = {
+	{ NULL }
+};
+
+static void kvm_free_vcpus(struct kvm *kvm)
+{
+	unsigned int i;
+	struct kvm_vcpu *vcpu;
+
+	kvm_for_each_vcpu(i, vcpu, kvm)
+		kvm_arch_vcpu_destroy(vcpu);
+
+	mutex_lock(&kvm->lock);
+	for (i = 0; i < atomic_read(&kvm->online_vcpus); i++)
+		kvm->vcpus[i] = NULL;
+	atomic_set(&kvm->online_vcpus, 0);
+	mutex_unlock(&kvm->lock);
+}
+
+static void kvm_flush_rtlb(void *args)
+{
+	int tmp0, tmp1, guestid = (int)(long)args;
+
+	guestid <<=  16; /* rid */
+	__asm__ __volatile__ (
+		".set push		\n"
+		".set noreorder		\n"
+		"mfc0	%0, $10, 4	\n"
+		"move	%1, %2		\n"
+		"mtc0	%1, $10, 4	\n"
+		"tlbinvf		\n"
+		"mtc0	%0, $10, 4	\n"
+		".set pop		\n"
+		:"=&r"(tmp0), "=&r"(tmp1)
+		:"r"(guestid)
+	);
+
+	return;
+}
+
+/* map gpa->pa and also flush references for pa in the cache to provide a clean memory
+ * for guest.
+ */
+static void kvm_sync_gpa_map(struct kvm *kvm)
+{
+	int t;
+	pgd_t *gpa_pgd;
+
+	gpa_pgd = (pgd_t *)kvm->arch.gpa_pgd;
+
+	for (t = 0; t < KVM_MEMORY_SLOTS; t++) {
+		struct kvm_memory_slot *s = &kvm->memslots->memslots[t];
+		uint64_t msize, gpa, hva, address;
+
+		if (!s->npages)
+			continue;
+
+		msize = s->npages << PAGE_SHIFT;
+		gpa = s->base_gfn << PAGE_SHIFT;
+		hva = s->userspace_addr;
+
+		/* enumerate all hva pages */
+		for (address = hva; address < (hva + msize);) {
+			pgd_t *pgdp;
+			pud_t *pudp;
+			pmd_t *pmdp;
+			pte_t *ptep, pte;
+			int i;
+			unsigned long *ptr;
+
+			pgdp = (pgd_t *)pgd_current[smp_processor_id()] + __pgd_offset(address);
+			pudp = pud_offset(pgdp, address);
+			pmdp = pmd_offset(pudp, address);
+			ptep = pte_offset(pmdp, address);
+			pte = *	ptep;
+
+			/* map gpa to pte in page table pointed to by gpa_pgd */
+			pgdp = (pgd_t *)gpa_pgd + __pgd_offset(gpa);
+			pudp = (pud_t *)pgdp;
+			if (*(unsigned long *)pudp == (unsigned long)invalid_pmd_table) {
+				/* allocate a page for pmd and initialize it */
+				ptr = (unsigned long *)__get_free_page(GFP_KERNEL|GFP_DMA);
+				for (i = 0; i < PTRS_PER_PMD; i++)
+					ptr[i] = (unsigned long)invalid_pte_table;
+				*(unsigned long *)pudp = (unsigned long)ptr;
+
+			}
+
+			pmdp = (pmd_t *)*(unsigned long *)pudp + __pmd_offset(gpa);
+			if (*(unsigned long *)pmdp == (unsigned long)invalid_pte_table) {
+				/* allocate a page for pmd and initialize it */
+				ptr = (unsigned long *)__get_free_page(GFP_KERNEL|GFP_DMA);
+				for (i = 0; i < PTRS_PER_PTE; i++)
+					ptr[i] = 0;
+				*(unsigned long *)pmdp = (unsigned long)ptr;
+			}
+
+			ptep = (pte_t *)*(unsigned long *)pmdp + __pte_offset(gpa);
+			*ptep = pte;
+			address += PAGE_SIZE, gpa += PAGE_SIZE;
+		}
+	}
+
+	return;
+}
+
+static void kvm_sync_dirty_log(struct kvm *kvm)
+{
+	int t;
+	pgd_t *gpa_pgd;
+
+	gpa_pgd = (pgd_t *)kvm->arch.gpa_pgd;
+
+	for (t = 0; t < KVM_MEMORY_SLOTS; t++) {
+		struct kvm_memory_slot *s = &kvm->memslots->memslots[t];
+		uint64_t msize, gpa, address;
+
+		if (!s->npages)
+			continue;
+
+		if (!(s->flags & KVM_MEM_LOG_DIRTY_PAGES))
+			continue;
+
+		msize = s->npages << PAGE_SHIFT;
+		gpa = s->base_gfn << PAGE_SHIFT;
+
+		/* enumerate all gpa pages */
+		for (address = gpa; address < (gpa + msize); address += PAGE_SIZE) {
+			pgd_t *pgdp;
+			pud_t *pudp;
+			pmd_t *pmdp;
+			pte_t *ptep, pte;
+
+			pgdp = (pgd_t *)gpa_pgd + __pgd_offset(address);
+			pudp = pud_offset(pgdp, address);
+			pmdp = pmd_offset(pudp, address);
+			ptep = pte_offset(pmdp, address);
+			pte = *ptep;
+			if (pte_val(pte) & _PAGE_MODIFIED) {
+				mark_page_dirty_in_slot(kvm, s, address >> PAGE_SHIFT);
+				pte_val(pte) &= ~(_PAGE_MODIFIED | _PAGE_VALID | _PAGE_DIRTY);
+				*ptep = pte;
+			}
+
+			address += PAGE_SIZE;
+		}
+	}
+
+	/* restart log session on all cpus */
+	on_each_cpu(kvm_flush_rtlb, (void *)(long)kvm->arch.guest_id, 1);
+
+	return;
+}
+
+static void kvm_set_gpa_pa_map(struct kvm *kvm, struct kvm_userspace_memory_region *mem,
+	int write_protect)
+{
+	uint64_t msize, gpa, hva_base, address;
+	pgd_t *gpa_pgd, *pgdp;
+	pud_t *pudp;
+	pmd_t *pmdp;
+	pte_t *ptep, pte;
+
+	gpa_pgd = (pgd_t *)kvm->arch.gpa_pgd;
+	msize = mem->memory_size;
+	gpa = mem->guest_phys_addr;
+	hva_base = mem->userspace_addr;
+
+	for (address = hva_base; address < (hva_base + msize);
+	     address += PAGE_SIZE, gpa += PAGE_SIZE) {
+		pgdp = (pgd_t *)pgd_current[smp_processor_id()] + __pgd_offset(address);
+		pudp = pud_offset(pgdp, address);
+		pmdp = pmd_offset(pudp, address);
+		ptep = pte_offset(pmdp, address);
+		pte = *ptep;
+
+		/* _PAGE_MODIFIED: to record whether the page has been written,
+			will be set after tlbs/tlbm processed
+		 * _PAGE_VALID: force tlb invalid exception (tlbl/tlbs/tlbm)
+		 * _PAGE_DIRTY: force tlbs/tlbm exception
+		 */
+		if (write_protect)
+			pte_val(pte) &= ~(_PAGE_MODIFIED | _PAGE_VALID | _PAGE_DIRTY);
+
+		pgdp = (pgd_t *)(gpa_pgd + __pgd_offset(gpa));
+		pudp = pud_offset(pgdp, gpa);
+		pmdp = pmd_offset(pudp, gpa);
+		ptep = pte_offset(pmdp, gpa);
+		*ptep = pte;
+	}
+
+	/* flush all tlbs for this region */
+	on_each_cpu(kvm_flush_rtlb, (void *)(long)kvm->arch.guest_id, 1);
+}
+
+int kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)
+{
+	return 0;
+}
+
+int kvm_arch_vcpu_runnable(struct kvm_vcpu *v)
+{
+	return 1;
+}
+
+long kvm_arch_vcpu_ioctl(struct file *filp, unsigned int ioctl, unsigned long arg)
+{
+	/* We do not have arch-specific vcpu ioctl yet */
+	return -EINVAL;
+}
+
+int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)
+{
+	int 		ret;
+	sigset_t 	sigsaved;
+	struct kvm_regs *r = &vcpu->arch.guest.gpu;
+
+	r->root_guestctl1 = vcpu->kvm->arch.guest_id | (vcpu->kvm->arch.guest_id << 16);
+	if (vcpu->arch.init_guest == 1) {
+		int i;
+		struct kvm_vcpu *vcpu_p;
+
+		vcpu->arch.gpa_pgd = vcpu->kvm->arch.gpa_pgd;
+		vcpu->arch.host_vcpuid = r->guest_ebase & 0x3ff;
+
+		/* go through all vcpus to register its vcpu pointer */
+		kvm_for_each_vcpu(i, vcpu_p, vcpu->kvm) {
+			vcpu_p->arch.guest_vcpu_p[r->guest_ebase & 0x3ff] = (uint64_t)vcpu;
+		}
+	}
+
+#if 0
+	printk("Running guest (vcpu %p, kvm %p, Entry PC: 0x%llx, GuestId 0x%llx, Guest vcpu id: 0x%llx) ...\n",
+		vcpu, vcpu->kvm, r->root_epc, r->root_guestctl1 & 0xff, r->guest_ebase & 0x3ff);
+#endif
+
+	/* setup the signal handling */
+	if (vcpu->sigset_active)
+		sigprocmask(SIG_SETMASK, &vcpu->sigset, &sigsaved);
+
+	local_irq_disable();
+	kvm_guest_enter();
+	local_irq_enable();
+	ret = __kvm_vcpu_run_guest(vcpu);
+	local_irq_disable();
+	kvm_guest_exit();
+	local_irq_enable();
+
+	/* The execution reaches here because QEMU needs to be involved:
+	 * . I/O
+	 * . New vcpu
+	 */
+	switch(ret >> 24) {
+	case KVM_EXIT_SPAWN_THREADS:
+		run->exit_reason = ret >> 24;
+		run->mips_spawn_thread.num_vcpus = (ret >> 16) & 0xff;
+		run->mips_spawn_thread.start_hw_cpuid = ret & 0xff;
+		break;
+	case KVM_EXIT_ENABLE_CORE:
+		run->exit_reason = ret >> 24;
+		run->mips_spawn_thread.start_hw_cpuid = ret & 0xff;
+		break;
+	case KVM_EXIT_QUIT_KVM:
+		/* quit kvm */
+		run->exit_reason = ret >> 24;
+		break;
+	case KVM_EXIT_CHAR_PRINT:
+		/* print out a char */
+		run->exit_reason = ret >> 24;
+		run->mips_output_char.c = ret & 0xff;
+		break;
+#if 0
+	case KVM_EXIT_IRQ_WINDOW_OPEN:
+		run->exit_reason = ret >> 24;
+		break;
+#endif
+	default:
+		printk("Bug: unknown exit reason for guest %x\n", ret);
+	}
+
+	if (vcpu->sigset_active)
+		sigprocmask(SIG_SETMASK, &sigsaved, NULL);
+
+	return 0;
+}
+
+int kvm_arch_vcpu_fault(struct kvm_vcpu *vcpu, struct vm_fault *vmf)
+{
+	return VM_FAULT_SIGBUS;
+}
+
+/* Not used any more */
+int kvm_arch_vcpu_ioctl_translate(struct kvm_vcpu *vcpu, struct kvm_translation *tr)
+{
+	return -ENOTSUPP;
+}
+
+int kvm_arch_hardware_enable(void *garbage)
+{
+	return 0;
+}
+
+void kvm_arch_hardware_disable(void *garbage)
+{
+	/* do nothing */
+}
+
+int kvm_arch_hardware_setup(void)
+{
+	return 0;
+}
+
+void kvm_arch_hardware_unsetup(void)
+{
+	/* do nothing */
+}
+
+void kvm_arch_check_processor_compat(void *rtn)
+{
+	*(int *)rtn = xlp_kvm_check_processor_compat();
+}
+
+int kvm_arch_vcpu_ioctl_set_mpstate(struct kvm_vcpu *vcpu, struct kvm_mp_state *mp_state)
+{
+	return -ENOTSUPP;
+}
+
+int kvm_arch_vcpu_ioctl_get_mpstate(struct kvm_vcpu *vcpu, struct kvm_mp_state *mp_state)
+{
+	return -ENOTSUPP;
+}
+
+int kvm_arch_vcpu_ioctl_get_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)
+{
+	memcpy(fpu, &vcpu->arch.guest.fpu, sizeof(vcpu->arch.guest.fpu));
+	return 0;
+}
+
+int kvm_arch_vcpu_ioctl_set_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)
+{
+	memcpy(&vcpu->arch.guest.fpu, fpu, sizeof(vcpu->arch.guest.fpu));
+	return 0;
+}
+
+int kvm_arch_vcpu_ioctl_set_guest_debug(struct kvm_vcpu *vcpu, struct kvm_guest_debug *dbg)
+{
+	return -ENOTSUPP;
+}
+
+int kvm_arch_vcpu_ioctl_set_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)
+{
+	memcpy(&vcpu->arch.guest.gpu, regs, sizeof(vcpu->arch.guest.gpu));
+	return 0;
+}
+
+int kvm_arch_vcpu_ioctl_get_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)
+{
+	memcpy(regs, &vcpu->arch.guest.gpu, sizeof(vcpu->arch.guest.gpu));
+	return 0;
+}
+
+int kvm_arch_vcpu_ioctl_get_sregs(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs)
+{
+	return -ENOTSUPP;
+}
+
+int kvm_arch_vcpu_ioctl_set_sregs(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs)
+{
+	return -ENOTSUPP;
+}
+
+int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
+{
+	vcpu->arch.pip_vector = 0;
+	vcpu->arch.init_guest = 1;
+	vcpu->arch.hva_pgd = 0;
+	vcpu->arch.gpa_pgd = 0;
+	printk("Guest vcpu %p initialization\n", vcpu);
+	return 0;
+}
+
+void kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu)
+{
+}
+
+void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
+{
+	/* Set the guest ID */
+}
+
+void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
+{
+	/* Nullify the guest ID */
+}
+
+int kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu)
+{
+	/* This is needed to wake up the __waited__ VM */
+	return 0;
+}
+
+int kvm_arch_prepare_memory_region(struct kvm *kvm,
+				   struct kvm_memory_slot *memslot,
+				   struct kvm_userspace_memory_region *mem,
+				   enum kvm_mr_change change)
+{
+	return 0;
+}
+
+void kvm_arch_commit_memory_region(struct kvm *kvm,
+				   struct kvm_userspace_memory_region *mem,
+				   const struct kvm_memory_slot *old,
+				   enum kvm_mr_change change)
+{
+	/* first time for the memory region, do nothing */
+	if (old->npages == 0)
+		return;
+
+	/* no flag change, do nothing */
+	if (old->flags == mem->flags)
+		return;
+
+	if (!(mem->flags & KVM_MEM_LOG_DIRTY_PAGES))
+		kvm_set_gpa_pa_map(kvm, mem, 0);
+	else
+		kvm_set_gpa_pa_map(kvm, mem, 1);
+}
+
+void kvm_arch_flush_shadow(struct kvm *kvm)
+{
+}
+
+void kvm_arch_sync_events(struct kvm *kvm)
+{
+}
+
+struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm, unsigned int id)
+{
+	struct kvm_vcpu *vcpu;
+	int err;
+
+	vcpu = kzalloc(sizeof(struct kvm_vcpu), GFP_KERNEL);
+	if (!vcpu) {
+		err = -ENOMEM;
+		goto out;
+	}
+
+	err = kvm_vcpu_init(vcpu, kvm, id);
+	if (err)
+		goto free_vcpu;
+
+	return vcpu;
+
+free_vcpu:
+	kfree(vcpu);
+out:
+	return ERR_PTR(err);
+}
+
+void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
+{
+        kvm_vcpu_uninit(vcpu);
+	kfree(vcpu);
+}
+
+int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
+{
+	return 0;
+}
+
+int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
+{
+	if (type)
+		return -EINVAL;
+
+	xlp_kvm_init_vm(kvm);
+
+	return 0;
+}
+
+void kvm_arch_destroy_vm(struct kvm *kvm)
+{
+	xlp_kvm_destroy_vm(kvm);
+	kvm_free_vcpus(kvm);
+	kvm_free_physmem(kvm);
+}
+
+void kvm_arch_free_memslot(struct kvm_memory_slot *free,
+                           struct kvm_memory_slot *dont)
+{
+	if (!dont || free->arch.rmap[0] != dont->arch.rmap[0]) {
+		kvm_kvfree(free->arch.rmap[0]);
+		free->arch.rmap[0] = NULL;
+	}
+}
+
+int kvm_arch_create_memslot(struct kvm_memory_slot *slot, unsigned long npages)
+{
+
+	slot->arch.rmap[0] =
+		kvm_kvzalloc(npages * sizeof(*slot->arch.rmap[0]));
+	if (!slot->arch.rmap[0])
+		goto out_free;
+
+	return 0;
+
+out_free:
+	kvm_kvfree(slot->arch.rmap[0]);
+	slot->arch.rmap[0] = NULL;
+	return -ENOMEM;
+}
+
+void kvm_arch_flush_shadow_all(struct kvm *kvm)
+{
+}
+
+void kvm_arch_flush_shadow_memslot(struct kvm *kvm,
+				struct kvm_memory_slot *slot)
+{
+}
+
+int kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu)
+{
+	return 0;
+}
+
+long kvm_arch_dev_ioctl(struct file *filp, unsigned int ioctl, unsigned long arg)
+{
+	return -EINVAL;
+}
+
+int kvm_dev_ioctl_check_extension(long ext)
+{
+	return 0;
+}
+
+int kvm_vm_ioctl_get_dirty_log(struct kvm *kvm, struct kvm_dirty_log *log)
+{
+	struct kvm_memory_slot *memslot;
+	unsigned long gpa, gpa_end;
+	int is_dirty = 0;
+	int r;
+	unsigned long n;
+
+	mutex_lock(&kvm->slots_lock);
+
+	/* march through guest page table to get the dirty log */
+	kvm_sync_dirty_log(kvm);
+
+	r = kvm_get_dirty_log(kvm, log, &is_dirty);
+	if (r) {
+		mutex_unlock(&kvm->slots_lock);
+		return r;
+	}
+
+	if (is_dirty) {
+		memslot = &kvm->memslots->memslots[log->slot];
+
+		gpa = memslot->base_gfn << PAGE_SHIFT;
+		gpa_end = gpa + (memslot->npages << PAGE_SHIFT);
+
+		n = kvm_dirty_bitmap_bytes(memslot);
+		memset(memslot->dirty_bitmap, 0, n);
+	}
+
+	mutex_unlock(&kvm->slots_lock);
+	return 0;
+}
+
+long kvm_arch_vm_ioctl(struct file *filp, unsigned int ioctl, unsigned long arg)
+{
+	struct kvm *kvm = filp->private_data;
+	void __user *argp = (void __user *)arg;
+	int r = 0;
+
+	switch(ioctl) {
+	case KVM_MIPS_GUEST_INPUT:
+	{
+		struct kvm_guest_input input;
+		int i;
+		unsigned char *buf;
+
+		if (copy_from_user(&input, argp, sizeof(struct kvm_guest_input))) {
+			r = -EFAULT;
+			break;
+		}
+
+		buf = vmalloc(input.size);
+		if (copy_from_user(buf, (unsigned char *)input.buf, input.size)) {
+			r = -EFAULT;
+			break;
+		}
+		for (i = 0; i < input.size; i++) {
+			kvm_uart_insert_char(kvm, buf[i]);
+		}
+		break;
+	}
+	case KVM_MIPS_EXIT_REQUEST:
+	{
+		kvm->arch.exit_request = (int)(long)argp;
+		/* also clear guestctl1 */
+		__write_32bit_c0_register($10, 4, 0);
+		break;
+	}
+	case KVM_MIPS_INFO_REQUEST:
+	{
+		struct kvm_guest_info input;
+
+		input.guest_id = kvm->arch.guest_id;
+		if (copy_to_user(argp, &input, sizeof(struct kvm_guest_info))) {
+			r = -EFAULT;
+		}
+		break;
+	}
+	case KVM_MIPS_SYNC_GPA_MAP:
+		kvm_sync_gpa_map(kvm);
+		break;
+	default:
+		r = -EINVAL;
+		break;
+	}
+
+	return r;
+}
+
+int kvm_arch_init(void *opaque)
+{
+	return 0;
+}
+
+void kvm_arch_exit(void)
+{
+}
+
+static int __init kvm_mips_init(void)
+{
+	return kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+}
+
+static void __exit kvm_mips_exit(void)
+{
+	kvm_exit();
+}
+
+module_init(kvm_mips_init);
+module_exit(kvm_mips_exit);
diff --git a/arch/mips/kvm/xlp.c b/arch/mips/kvm/xlp.c
new file mode 100644
index 0000000..41314cc
--- /dev/null
+++ b/arch/mips/kvm/xlp.c
@@ -0,0 +1,377 @@
+
+#include <linux/kvm.h>
+#include <linux/kvm_host.h>
+#include <linux/spinlock.h>
+
+#include <asm/pgalloc.h>
+#include <asm/branch.h>
+#include <asm/fpu.h>
+#include <asm/inst.h>
+#include <asm/ptrace.h>
+#include <asm/kvm_host.h>
+#include <asm/netlogic/kvm_xlp.h>
+
+/* KVM_MAX_NUM_GID is multiple of 64 */
+static uint64_t occupied_gids[KVM_MAX_NUM_GID/sizeof(uint64_t)];
+static uint64_t overlapped_gid1_count;
+
+static DEFINE_SPINLOCK(kvm_gid_lock);
+
+int xlp_kvm_check_processor_compat(void)
+{
+	/* FIXME: read config 3 */
+        return 0;
+}
+
+/*
+ * Compute the return address for the guest trigger exception.
+ * If the original fault instruction is pointing to the delay slot,
+ * the badinstr should be the insn prior to the delay slot instr.
+ */
+int compute_guest_return_epc(struct pt_regs *regs, unsigned int badinstr)
+{
+	unsigned int bit, fcr31;
+	long epc;
+	union mips_instruction insn;
+
+	epc = regs->cp0_epc;
+	if (!delay_slot(regs)) {
+		regs->cp0_epc = epc + 4;
+		return 0;
+	}
+
+	insn.word = badinstr;
+
+	switch (insn.i_format.opcode) {
+	/*
+	 * jr and jalr are in r_format format.
+	 */
+	case spec_op:
+		switch (insn.r_format.func) {
+		case jalr_op:
+			regs->regs[insn.r_format.rd] = epc + 8;
+			/* Fall through */
+		case jr_op:
+			regs->cp0_epc = regs->regs[insn.r_format.rs];
+			break;
+		}
+		break;
+
+	/*
+	 * This group contains:
+	 * bltz_op, bgez_op, bltzl_op, bgezl_op,
+	 * bltzal_op, bgezal_op, bltzall_op, bgezall_op.
+	 */
+	case bcond_op:
+		switch (insn.i_format.rt) {
+	 	case bltz_op:
+		case bltzl_op:
+			if ((long)regs->regs[insn.i_format.rs] < 0)
+				epc = epc + 4 + (insn.i_format.simmediate << 2);
+			else
+				epc += 8;
+			regs->cp0_epc = epc;
+			break;
+
+		case bgez_op:
+		case bgezl_op:
+			if ((long)regs->regs[insn.i_format.rs] >= 0)
+				epc = epc + 4 + (insn.i_format.simmediate << 2);
+			else
+				epc += 8;
+			regs->cp0_epc = epc;
+			break;
+
+		case bltzal_op:
+		case bltzall_op:
+			regs->regs[31] = epc + 8;
+			if ((long)regs->regs[insn.i_format.rs] < 0)
+				epc = epc + 4 + (insn.i_format.simmediate << 2);
+			else
+				epc += 8;
+			regs->cp0_epc = epc;
+			break;
+
+		case bgezal_op:
+		case bgezall_op:
+			regs->regs[31] = epc + 8;
+			if ((long)regs->regs[insn.i_format.rs] >= 0)
+				epc = epc + 4 + (insn.i_format.simmediate << 2);
+			else
+				epc += 8;
+			regs->cp0_epc = epc;
+			break;
+		}
+		break;
+
+	/*
+	 * These are unconditional and in j_format.
+	 */
+	case jal_op:
+		regs->regs[31] = regs->cp0_epc + 8;
+	case j_op:
+		epc += 4;
+		epc >>= 28;
+		epc <<= 28;
+		epc |= (insn.j_format.target << 2);
+		regs->cp0_epc = epc;
+		break;
+
+	/*
+	 * These are conditional and in i_format.
+	 */
+	case beq_op:
+	case beql_op:
+		if (regs->regs[insn.i_format.rs] ==
+		    regs->regs[insn.i_format.rt])
+			epc = epc + 4 + (insn.i_format.simmediate << 2);
+		else
+			epc += 8;
+		regs->cp0_epc = epc;
+		break;
+
+	case bne_op:
+	case bnel_op:
+		if (regs->regs[insn.i_format.rs] !=
+		    regs->regs[insn.i_format.rt])
+			epc = epc + 4 + (insn.i_format.simmediate << 2);
+		else
+			epc += 8;
+		regs->cp0_epc = epc;
+		break;
+
+	case blez_op: /* not really i_format */
+	case blezl_op:
+		/* rt field assumed to be zero */
+		if ((long)regs->regs[insn.i_format.rs] <= 0)
+			epc = epc + 4 + (insn.i_format.simmediate << 2);
+		else
+			epc += 8;
+		regs->cp0_epc = epc;
+		break;
+
+	case bgtz_op:
+	case bgtzl_op:
+		/* rt field assumed to be zero */
+		if ((long)regs->regs[insn.i_format.rs] > 0)
+			epc = epc + 4 + (insn.i_format.simmediate << 2);
+		else
+			epc += 8;
+		regs->cp0_epc = epc;
+		break;
+
+	/*
+	 * And now the FPA/cp1 branch instructions.
+	 */
+	case cop1_op:
+		preempt_disable();
+		if (is_fpu_owner())
+			asm volatile("cfc1\t%0,$31" : "=r" (fcr31));
+		else
+			fcr31 = current->thread.fpu.fcr31;
+		preempt_enable();
+
+		bit = (insn.i_format.rt >> 2);
+		bit += (bit != 0);
+		bit += 23;
+		switch (insn.i_format.rt & 3) {
+		case 0:	/* bc1f */
+		case 2:	/* bc1fl */
+			if (~fcr31 & (1 << bit))
+				epc = epc + 4 + (insn.i_format.simmediate << 2);
+			else
+				epc += 8;
+			regs->cp0_epc = epc;
+			break;
+
+		case 1:	/* bc1t */
+		case 3:	/* bc1tl */
+			if (fcr31 & (1 << bit))
+				epc = epc + 4 + (insn.i_format.simmediate << 2);
+			else
+				epc += 8;
+			regs->cp0_epc = epc;
+			break;
+		}
+		break;
+
+	default:
+		panic("Unhandled compute_guest_return_epc (instr %x)\n", badinstr);
+	}
+
+	return 0;
+}
+
+static int kvm_get_new_guest_id(void)
+{
+	int gid, idx, oft;
+	unsigned long flags;
+
+	spin_lock_irqsave(&kvm_gid_lock, flags);
+	for (gid = 1; gid < KVM_MAX_NUM_GID; gid++) {
+		idx = gid >> 6;
+		oft = gid & 0x3f;
+
+		if (((occupied_gids[idx] >> oft) & 0x1) == 0) {
+			occupied_gids[idx] |= (1 << oft);
+			spin_unlock_irqrestore(&kvm_gid_lock, flags);
+			return gid;
+		}
+	}
+
+	/* All gids have been occupied.
+	 * The algorithm here is to overlap gid 1, so later on whenever any guest with
+	 * gid 1 is to be scheduled, the guest/roottlb will be flushed. All other guests
+	 * are not affected.
+	 */
+	gid = 1;
+	overlapped_gid1_count ++;
+	spin_unlock_irqrestore(&kvm_gid_lock, flags);
+	return gid;
+}
+
+static void xlp_kvm_free_gpa_pgd(pgd_t *gpa_pgd)
+{
+	unsigned long *p, *end;
+	int i;
+
+	p = (unsigned long *)gpa_pgd;
+	end = p + PTRS_PER_PGD;
+
+	for(; p < end; p++) {
+		if (*p != (unsigned long)invalid_pmd_table) {
+			unsigned long *p1 = (unsigned long*)*p;
+
+			for (i = 0; i < PTRS_PER_PMD; i++) {
+				if (p1[i] != (unsigned long)invalid_pte_table) {
+					__free_page((void *)p1[i]);
+				}
+			}
+			__free_page((void *)*p);
+		}
+	}
+
+	/* free pgd table page */
+	pgd_free(NULL, gpa_pgd);
+}
+
+void xlp_kvm_destroy_vm(struct kvm *kvm)
+{
+	int gid, idx, oft;
+	unsigned long flags;
+
+	/* free guest id */
+	gid = kvm->arch.guest_id;
+
+	spin_lock_irqsave(&kvm_gid_lock, flags);
+	if (gid != 1) {
+		idx = gid >> 6;
+		oft = gid & 0x3f;
+		occupied_gids[idx] &= ~(1 << oft);
+	} else {
+		if (overlapped_gid1_count)
+			overlapped_gid1_count --;
+		else
+			occupied_gids[0] &= ~(1 << 1);
+	}
+	spin_unlock_irqrestore(&kvm_gid_lock, flags);
+
+	/* free gpa pgd table */
+	xlp_kvm_free_gpa_pgd((pgd_t *)kvm->arch.gpa_pgd);
+}
+
+static void xlp_kvm_init_uart0(struct kvm_arch *arch)
+{
+	/* common headers */
+	arch->uart.header[0]    = 0x9010184e;
+	arch->uart.header[2]    = 0x07000200;
+	arch->uart.header[0x3d] = 0x00010085;
+
+	arch->uart.rhr  = 0x0;
+	arch->uart.thr  = 0x0;
+	arch->uart.ier  = 0x0;
+	arch->uart.iir  = 0xc1;
+	arch->uart.fcr  = 0xc0;
+	arch->uart.lcr  = 0x03;
+	arch->uart.mcr  = 0x0;
+	arch->uart.lsr  = 0x60;
+	arch->uart.msr  = 0x0;
+	arch->uart.dlb1 = 0x0;
+	arch->uart.dlb2 = 0x0;
+
+	arch->uart.input_size = 0;
+}
+
+static void xlp_kvm_init_pic(struct kvm_arch *arch)
+{
+	arch->pic.u.v32[0]    = 0x9003184e;
+	arch->pic.u.v32[2]    = 0x08000010;
+	arch->pic.u.v32[0x3d] = 0x0001008c;
+}
+
+static void xlp_kvm_init_sysmgt(struct kvm_arch *arch)
+{
+	arch->sysmgt.regs[0] = 0xffffffff;
+	arch->sysmgt.regs[1] = 0x0;
+	arch->sysmgt.regs[2] = 0x0;
+	arch->sysmgt.regs[3] = 0x0;
+	arch->sysmgt.regs[0x40] = 0x0;
+	arch->sysmgt.regs[0x41] = 0x0;
+	arch->sysmgt.regs[0x42] = 0xffffe;
+	arch->sysmgt.regs[0x43] = 0xffffe;
+}
+
+void xlp_kvm_init_vm(struct kvm *kvm)
+{
+	struct kvm_arch *arch = &kvm->arch;
+
+	/* Assign a new Guest ID */
+	arch->guest_id = kvm_get_new_guest_id();
+	arch->exit_request = 0;
+
+	arch->gpa_pgd = (uint64_t)pgd_alloc(NULL);
+
+	/* initialize SOCs: UART0/PIC/SYSMGT */
+	xlp_kvm_init_uart0(arch);
+	xlp_kvm_init_pic(arch);
+	xlp_kvm_init_sysmgt(arch);
+}
+
+void kvm_save_guest_context(struct pt_regs *regs, struct kvm_vcpu_guest *guest)
+{
+	memcpy(guest->gpu.regs, regs->regs, sizeof(guest->gpu.regs));
+	guest->gpu.hi = regs->hi;
+	guest->gpu.lo = regs->lo;
+
+	guest->gpu.root_guestctl0  = regs->cp0_guestctl0;
+	guest->gpu.root_epc        = regs->cp0_epc;
+}
+
+void kvm_uart_insert_char(struct kvm *kvm, char c)
+{
+	struct kvm_arch *arch = &kvm->arch;
+
+	if (arch->uart.input_size < KVM_MAX_UART_IN_SIZE) {
+		arch->uart.input_buf[arch->uart.input_size] = c;
+		arch->uart.input_size ++;
+	} else {
+		/* Overflows: ignore */
+		printk("%s: guest input overflows\n", __FUNCTION__);
+	}
+
+	/* We want to inject the char back to the guest, generate an interrupt */
+	arch->uart.lsr  |= 0x1;
+	arch->uart.iir  = (0x2 << 1) | 0x00;
+	kvm_pic_inject_guest_ext(kvm, arch, arch->uart.header[0x3d] & 0xffff, 0);
+}
+
+void kvm_xlp_check_exit_request(struct kvm_vcpu *vcpu)
+{
+	unsigned int val;
+
+	printk("Guest vcpu %lld exiting\n", vcpu->arch.guest.gpu.guest_ebase & 0x3ff);
+
+	/* return to qemu for exit */
+	val = (KVM_EXIT_QUIT_KVM << 24);
+	__kvm_vcpu_leave_guest(vcpu, val);
+}
diff --git a/arch/mips/mm/tlbex-fault.S b/arch/mips/mm/tlbex-fault.S
index 2507a08..ffb88d4 100644
--- a/arch/mips/mm/tlbex-fault.S
+++ b/arch/mips/mm/tlbex-fault.S
@@ -20,6 +20,22 @@
 	nop
 #endif
 
+#ifdef CONFIG_KVM
+	/* guest check for memory fault */
+	move	a0, sp
+	li	a1, \write
+	jal	do_guest_fault_check
+	nop
+
+	beqz	v0, 1f
+	nop
+
+	PTR_LA	ra, ret_from_exception
+	jr	ra
+	nop
+1:
+#endif
+
 	MFC0	a2, CP0_BADVADDR
 	KMODE
 	move	a0, sp
diff --git a/arch/mips/mm/tlbex.c b/arch/mips/mm/tlbex.c
index cd51dfd..8e0eb70 100644
--- a/arch/mips/mm/tlbex.c
+++ b/arch/mips/mm/tlbex.c
@@ -283,6 +283,11 @@ static inline void dump_handler(const u32 *handler, int count)
 #define C0_EPC		14, 0
 #define C0_XCONTEXT	20, 0
 
+#ifdef CONFIG_KVM
+#define	C0_KSCRATCH1	31, 2
+#define	C0_KSCRATCH2	31, 3
+#endif
+
 #ifdef CONFIG_64BIT
 # define GET_CONTEXT(buf, reg) UASM_i_MFC0(buf, reg, C0_XCONTEXT)
 #else
@@ -305,7 +310,15 @@ static struct uasm_reloc relocs[128] __cpuinitdata;
 
 static int check_for_high_segbits __cpuinitdata;
 
+#ifdef CONFIG_KVM
+#ifdef CONFIG_NLM_XLP
+static unsigned int kscratch_used_mask __cpuinitdata = (0x1 << 0) | (0x1 << 1);
+#else
+static unsigned int kscratch_used_mask __cpuinitdata = (0x1 << 2) | (0x1 << 3);
+#endif
+#else
 static unsigned int kscratch_used_mask __cpuinitdata;
+#endif
 
 static inline int __maybe_unused c0_kscratch(void)
 {
@@ -1299,6 +1312,11 @@ build_fast_tlb_refill_handler (u32 **p, struct uasm_label **l,
 		rv.restore_scratch = 1;
 	}
 
+#ifdef CONFIG_KVM
+	uasm_i_dmfc0(p, K0, C0_KSCRATCH1);
+	uasm_i_dmfc0(p, K1, C0_KSCRATCH2);
+#endif
+
 	uasm_i_eret(p); /* return from trap */
 
 	return rv;
@@ -1327,8 +1345,10 @@ static void __cpuinit build_r4000_tlb_refill_handler(void)
 	memset(relocs, 0, sizeof(relocs));
 	memset(final_handler, 0, sizeof(final_handler));
 
-	if (current_cpu_type() == CPU_XLP)
-		uasm_i_ehb(&p);
+#ifdef CONFIG_KVM
+	uasm_i_dmtc0(&p, K0, C0_KSCRATCH1);
+	uasm_i_dmtc0(&p, K1, C0_KSCRATCH2);
+#endif
 
 	if ((scratch_reg > 0 || scratchpad_available()) && use_bbit_insns()) {
 		htlb_info = build_fast_tlb_refill_handler(&p, &l, &r, K0, K1,
@@ -1375,6 +1395,12 @@ static void __cpuinit build_r4000_tlb_refill_handler(void)
 		build_update_entries(&p, K0, K1);
 		build_tlb_write_entry(&p, &l, &r, tlb_random);
 		uasm_l_leave(&l, p);
+
+#ifdef CONFIG_KVM
+		uasm_i_dmfc0(&p, K0, C0_KSCRATCH1);
+		uasm_i_dmfc0(&p, K1, C0_KSCRATCH2);
+#endif
+
 		uasm_i_eret(&p); /* return from trap */
 	}
 #ifdef CONFIG_MIPS_HUGE_TLB_SUPPORT
@@ -1993,6 +2019,12 @@ build_r4000_tlbchange_handler_tail(u32 **p, struct uasm_label **l,
 	build_tlb_write_entry(p, l, r, tlb_indexed);
 	uasm_l_leave(l, *p);
 	build_restore_work_registers(p);
+
+#ifdef CONFIG_KVM
+	uasm_i_dmfc0(p, K0, C0_KSCRATCH1);
+	uasm_i_dmfc0(p, K1, C0_KSCRATCH2);
+#endif
+
 	uasm_i_eret(p); /* return from trap */
 
 #ifdef CONFIG_64BIT
diff --git a/arch/mips/netlogic/Makefile b/arch/mips/netlogic/Makefile
index c3a9a30..6fada98 100644
--- a/arch/mips/netlogic/Makefile
+++ b/arch/mips/netlogic/Makefile
@@ -3,3 +3,4 @@ obj-$(CONFIG_CPU_XLR)		+=	xlr/
 obj-$(CONFIG_CPU_XLP)		+=	xlp/
 obj-$(CONFIG_CPU_XLP)		+=	dts/
 obj-$(CONFIG_NLM_XLP_BOARD)	+=	lib/
+obj-$(CONFIG_KVM)			+=	kvm/
diff --git a/arch/mips/netlogic/kvm/Makefile b/arch/mips/netlogic/kvm/Makefile
new file mode 100644
index 0000000..7ff9965
--- /dev/null
+++ b/arch/mips/netlogic/kvm/Makefile
@@ -0,0 +1,4 @@
+EXTRA_CFLAGS := -Werror
+EXTRA_CFLAGS := $(CFLAGS) -DNLM_HAL_LINUX_KERNEL -Iarch/mips/include/asm/netlogic/hal
+
+obj-y                    	= kvm_traps.o kvm_fault.o kvm_pic.o kvm_uart.o kvm_sysmgt.o
diff --git a/arch/mips/netlogic/kvm/kvm_fault.c b/arch/mips/netlogic/kvm/kvm_fault.c
new file mode 100644
index 0000000..07c2b0c
--- /dev/null
+++ b/arch/mips/netlogic/kvm/kvm_fault.c
@@ -0,0 +1,304 @@
+/***********************************************************************
+Copyright 2003-2010 Netlogic Microsystems Inc. (“Netlogic”).
+This is a derived work from software originally provided by the external
+entity identified below. The licensing terms and warranties specified in
+the header of the original work apply to this derived work.
+
+*****************************#NETL_1#********************************/
+
+#include <linux/signal.h>
+#include <linux/sched.h>
+#include <linux/interrupt.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/ptrace.h>
+#include <linux/mman.h>
+#include <linux/mm.h>
+#include <linux/smp.h>
+#include <linux/vt_kern.h>		/* For unblank_screen() */
+#include <linux/module.h>
+#include <linux/kvm.h>
+#include <linux/kvm_host.h>
+
+#include <asm/branch.h>
+#include <asm/mmu_context.h>
+#include <asm/uaccess.h>
+#include <asm/ptrace.h>
+#include <asm/highmem.h>		/* For VMALLOC_END */
+
+#include <asm/netlogic/kvm_xlp.h>
+
+/*
+ * This file contains the implementation to somehow virtualize
+ * PCIe configuration space for various I/O blocks.
+ * 
+ * 1. The physical memory region of 0x18000000 - 0x1c000000 (64MB) is reserved
+ *    in guest for PCIe configuration space access.
+ * 2. The physical memory region of 0xc0000000 - 0xc1000000 (16MB) is reserved
+ *    for memory mapped configuration spaces for: fmn/nae/poe/usb/regex/srio.
+ *
+ * For #2, In XLP, the following PCIE devices require memory mapped space
+ * for its internal registers:
+ *
+ *   . fmn   (0 - 0x2060)
+ *   . nae   (0 - 0x1ffff)
+ *   . poe   (0 - 0x12ff)
+ *   . usb   (0 - 0x3ffff)
+ *   . regex (0 - 0xffff)
+ *   . srio  (0 - 0x1ffff)
+ */
+
+#define FMN_PCIE_MEM_BASE	0xc0000000
+#define NAE_PCIE_MEM_BASE	0xc0100000
+#define POE_PCIE_MEM_BASE	0xc0200000
+#define USB_PCIE_MEM_BASE	0xc0300000
+#define RGX_PCIE_MEM_BASE	0xc0400000
+#define RIO_PCIE_MEM_BASE	0xc0500000
+
+static inline void unhandled_exception(const char *func, unsigned long addr,
+	unsigned long epc, unsigned long write)
+{
+	printk("%s: unhandled address/offset %lx, epc %lx, is_write %lx\n",
+		func, addr, epc, write);
+}
+
+static void handle_pcie_fmn(struct pt_regs *regs, unsigned long write,
+		unsigned long address, unsigned long reg_num)
+{
+	if (address == 0x0) {
+		/* not supported during PCIe enumeration */
+		regs->regs[reg_num] = 0xffffffff;
+		return;
+	} else if (address == 0x10) {
+		/* base address 0 register. */
+		if (!write) {
+			regs->regs[reg_num] = FMN_PCIE_MEM_BASE;
+			return;
+		}
+	}
+
+	unhandled_exception(__FUNCTION__, address, regs->cp0_epc, write);
+}
+
+static void handle_pcie_nae(struct pt_regs *regs, unsigned long write,
+		unsigned long address, unsigned long reg_num)
+{
+	if (address == 0x0) {
+		/* not supported during PCIe enumeration */
+		regs->regs[reg_num] = 0xffffffff;
+		return;
+	} else if (address == 0x10) {
+		/* base address 0 register. */
+		if (!write) {
+			regs->regs[reg_num] = NAE_PCIE_MEM_BASE;
+			return;
+		}
+	}
+
+	unhandled_exception(__FUNCTION__, address, regs->cp0_epc, write);
+}
+
+static void handle_pcie_poe(struct pt_regs *regs, unsigned long write,
+		unsigned long address, unsigned long reg_num)
+{
+	if (address == 0x0) {
+		/* not supported during PCIe enumeration */
+		regs->regs[reg_num] = 0xffffffff;
+		return;
+	} else if (address == 0x10) {
+		/* base address 0 register. */
+		if (!write) {
+			regs->regs[reg_num] = POE_PCIE_MEM_BASE;
+			return;
+		}
+	}
+
+	unhandled_exception(__FUNCTION__, address, regs->cp0_epc, write);
+}
+
+static void handle_pcie_nor(struct pt_regs *regs, unsigned long write,
+		unsigned long address, unsigned long reg_num)
+{
+	if (address == 0x0) {
+		/* not supported during PCIe enumeration */
+		regs->regs[reg_num] = 0xffffffff;
+		return;
+	}
+
+	unhandled_exception(__FUNCTION__, address, regs->cp0_epc, write);
+}
+
+static void handle_pci_config_space(struct pt_regs *regs, unsigned long write,
+		unsigned long address, unsigned long reg_num)
+{
+
+	/* FIXME: workaround */
+	if (write)
+		return;
+
+	regs->regs[reg_num] = *(unsigned int *)(0x9000000000000000ULL | address);
+
+#if 0
+	if (address == 0x1c000000) {
+		if (!write)
+			regs->regs[reg_num] = 0x1001184e;
+		else
+			printk("ignore guest pci write: %lx, epc %lx, val %lx\n",
+				address, regs->cp0_epc, regs->regs[reg_num]);
+	} else if (address == 0x1c000004) {
+		if (!write)
+			regs->regs[reg_num] = 0x4;
+		else
+			printk("ignore guest pci write: %lx, epc %lx, val %lx\n",
+				address, regs->cp0_epc, regs->regs[reg_num]);
+	} else if (address == 0x1c000008) {
+		if (!write)
+			regs->regs[reg_num] = 0xffffffffff000000;
+		else
+			printk("ignore guest pci write: %lx, epc %lx, val %lx\n",
+				address, regs->cp0_epc, regs->regs[reg_num]);
+	} else if (address == 0x1c00000c) {
+		if (!write)
+			regs->regs[reg_num] = 0x808008;
+		else
+			printk("ignore guest pci write: %lx, epc %lx, val %lx\n",
+				address, regs->cp0_epc, regs->regs[reg_num]);
+	} else if (address == 0x1c000010
+			|| address == 0x1c000014
+			|| address == 0x1c000018
+			|| address == 0x1c00001c
+			|| address == 0x1c000020
+			|| address == 0x1c000024
+			|| address == 0x1c000030
+			|| address == 0x1c00003c
+		  ) {
+		if (!write)
+			regs->regs[reg_num] = 0x0;
+		else
+			printk("ignore guest pci write: %lx, epc %lx, val %lx\n",
+				address, regs->cp0_epc, regs->regs[reg_num]);
+	} else if (address == 0x1c00002c) {
+		if (!write)
+			regs->regs[reg_num] = 0xffffffffaaaaaaaa;
+		else
+			printk("ignore guest pci write: %lx, epc %lx, val %lx\n",
+				address, regs->cp0_epc, regs->regs[reg_num]);
+	} else
+		panic("caught one guest pci address: %lx, epc %lx\n",
+			address, regs->cp0_epc);
+#endif
+}
+
+asmlinkage int do_guest_fault_check(struct pt_regs *regs, unsigned long write,
+				    unsigned long address)
+{
+	if (address >= 0x18000000 && address < 0x1c000000) {
+		/* pcie register configuration space */
+		unsigned int badinstr, epc_badinstr;
+		unsigned int reg_num = 0;
+
+		kvm_get_badinstr(regs, &badinstr, &epc_badinstr);
+
+		/* This is the pcie configuration space. The only instructions
+		 * which can access here is lw/sw/ld/sd.
+		 * Decode the badinstr.
+		 */
+		if ((badinstr & 0xfc000000) == 0x8c000000 /* lw */
+		    || (badinstr & 0xfc000000) == 0xac000000 /* sw */
+		    || (badinstr & 0xfc000000) == 0xdc000000 /* ld */
+		    || (badinstr & 0xfc000000) == 0xfc000000 /* sd */
+		   )
+			reg_num = (badinstr >> 16) & 0x1f;
+		else {
+			/* The guest gives us wrong information. What to do? */
+			printk("%s: unhandled epc %lx, address %lx\n",
+				__FUNCTION__, regs->cp0_epc, address);
+			compute_guest_return_epc(regs, epc_badinstr);
+			return 0;
+		}
+
+		if ((address & 0xfffff000) == 0x18110000) {
+			/* pic: dev 2, func 0 */
+			kvm_handle_pcie_pic(regs, write, address & 0xfff, reg_num);
+		}
+		else if ((address & 0xfffff000) == 0x18118000) {
+			/* nae: dev 3, func 0 */
+			handle_pcie_nae(regs, write, address & 0xfff, reg_num);
+		}
+		else if ((address & 0xfffff000) == 0x18119000) {
+			/* poe: dev 3, func 1 */
+			handle_pcie_poe(regs, write, address & 0xfff, reg_num);
+		}
+		else if ((address & 0xfffff000) == 0x18120000) {
+			/* fmn: dev 4, func 0 */
+			handle_pcie_fmn(regs, write, address & 0xfff, reg_num);
+		}
+		else if ((address & 0xfffff000) == 0x18112000) {
+			/* uart: dev 2, func 2 */
+			kvm_handle_pcie_uart(regs, write, address & 0xfff, reg_num);
+		}
+		else if ((address & 0xfffff000) == 0x18130000) {
+			/* sys management: dev 6, func 0 */
+			kvm_handle_pcie_sysmgt(regs, write, address & 0xfff, reg_num);
+		}
+		else if ((address & 0xfffff000) == 0x18138000) {
+			/* NOR flash: dev 7, func 0 */
+			handle_pcie_nor(regs, write, address & 0xfff, reg_num);
+		}
+		else {
+			/* disable the device */
+			if (!write && (address & 0xfff) == 0)
+				regs->regs[reg_num] = 0xffffffff;
+			else {
+				printk("caught one guest pcie address: %lx, epc %lx\n",
+						address, regs->cp0_epc);
+			}
+
+		}
+
+		compute_guest_return_epc(regs, epc_badinstr);
+		return 1;
+	} else if (address >= 0x1c000000 && address < 0x1d000000) {
+		/* pci register configuration space */
+		unsigned int badinstr, epc_badinstr;
+		unsigned int reg_num = 0;
+
+		kvm_get_badinstr(regs, &badinstr, &epc_badinstr);
+
+		if ((badinstr & 0xfc000000) == 0x8c000000 /* lw */
+		    || (badinstr & 0xfc000000) == 0xac000000 /* sw */
+		    || (badinstr & 0xfc000000) == 0xdc000000 /* ld */
+		    || (badinstr & 0xfc000000) == 0xfc000000 /* sd */
+		   )
+			reg_num = (badinstr >> 16) & 0x1f;
+		else {
+			printk("%s: unhandled epc %lx, address %lx\n",
+				__FUNCTION__, regs->cp0_epc, address);
+			compute_guest_return_epc(regs, epc_badinstr);
+			return 0;
+		}
+
+		handle_pci_config_space(regs, write, address, reg_num);
+		compute_guest_return_epc(regs, epc_badinstr);
+
+		return 1;
+	} else if (address >= 0xc0000000ul && address < 0xc0100000ul) {
+		unsigned int badinstr, epc_badinstr;
+
+		printk("Caught one pcim address 0x%lx, epc 0x%lx\n", address, regs->cp0_epc);
+
+		kvm_get_badinstr(regs, &badinstr, &epc_badinstr);
+
+		/* temporary ignore here */
+		printk("caught one guest pcim address: %lx, epc %lx\n",
+			address, regs->cp0_epc);
+
+		compute_guest_return_epc(regs, epc_badinstr);
+
+		return 1;
+	}
+
+	return 0;
+}
diff --git a/arch/mips/netlogic/kvm/kvm_pic.c b/arch/mips/netlogic/kvm/kvm_pic.c
new file mode 100644
index 0000000..c6f26f7
--- /dev/null
+++ b/arch/mips/netlogic/kvm/kvm_pic.c
@@ -0,0 +1,163 @@
+/***********************************************************************
+Copyright 2003-2010 Netlogic Microsystems Inc. (“Netlogic”).
+This is a derived work from software originally provided by the external
+entity identified below. The licensing terms and warranties specified in
+the header of the original work apply to this derived work.
+
+*****************************#NETL_1#********************************/
+
+#include <linux/signal.h>
+#include <linux/sched.h>
+#include <linux/interrupt.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/ptrace.h>
+#include <linux/mman.h>
+#include <linux/mm.h>
+#include <linux/smp.h>
+#include <linux/vt_kern.h>		/* For unblank_screen() */
+#include <linux/module.h>
+#include <linux/kvm.h>
+#include <linux/kvm_host.h>
+
+#include <asm/branch.h>
+#include <asm/mmu_context.h>
+#include <asm/uaccess.h>
+#include <asm/ptrace.h>
+#include <asm/highmem.h>		/* For VMALLOC_END */
+
+#include <asm/netlogic/kvm_xlp.h>
+#include <asm/netlogic/mips-extns.h>
+
+// #define DEBUG
+
+static inline void unhandled_exception(const char *func, unsigned long addr,
+	unsigned long epc, unsigned long write)
+{
+	printk("%s: unhandled address %lx, epc %lx, is_write %lx\n",
+		func, addr, epc, write);
+}
+
+void kvm_pic_inject_guest(struct kvm_arch *arch, unsigned int irt, struct pt_regs *regs)
+{
+	unsigned long long irt_entry;
+	unsigned int rvec;
+	struct kvm_vcpu_arch *vcpu_arch;
+
+	irt_entry = arch->pic.u.v64[(0x200 >> 1) + irt];
+	rvec = (irt_entry >> 24) & 0x3f;
+	vcpu_arch = kvm_get_vcpu_arch(regs);
+#ifdef DEBUG
+	printk("===== Inject IRT %u, RVEC %u to the guest (irt_entry %llx)\n", irt, rvec, irt_entry);
+#endif
+	atomic_or_llong(&vcpu_arch->pip_vector, 1ULL << rvec);
+}
+
+void kvm_pic_inject_guest_ext(struct kvm *kvm, struct kvm_arch *arch, unsigned int irt, unsigned int cpuid)
+{
+	unsigned long long irt_entry;
+	unsigned int rvec;
+	struct kvm_vcpu_arch *vcpu_arch;
+
+	/* FIXME: We should really look at irt_entry and deliver it to proper cpu
+	 * according to the schedule type.
+	 */
+	irt_entry = arch->pic.u.v64[(0x200 >> 1) + irt];
+	rvec = (irt_entry >> 24) & 0x3f;
+	vcpu_arch = kvm_get_vcpu_arch_ext(kvm, cpuid);
+#ifdef DEBUG
+	printk("===== Inject IRT %u, RVEC %u to the guest (irt_entry %llx)\n", irt, rvec, irt_entry);
+#endif
+	atomic_or_llong(&vcpu_arch->pip_vector, 1ULL << rvec);
+}
+
+static unsigned int xlp_kvm_get_ipi_cpuid(uint64_t ipi_ctrl)
+{
+	unsigned int mask = ipi_ctrl & 0x3ff;
+	return mask;
+}
+
+void kvm_handle_pcie_pic(struct pt_regs *regs, unsigned long write,
+		unsigned long address, unsigned long reg_num)
+{
+	unsigned int rindex = address >> 2;
+	struct kvm_arch *arch = kvm_get_arch(regs);
+	extern void nlm_send_ipi_single(int lcpu, unsigned int action);
+
+	if (rindex == 0x0 || rindex == 0x2 || rindex == 0x3d) {
+		if (!write)
+			regs->regs[reg_num] = arch->pic.u.v32[rindex];
+		else
+			unhandled_exception(__FUNCTION__, address, regs->cp0_epc, write);
+	} else if (rindex == 0x40) {
+		/* pic control */
+		if (write)
+			arch->pic.u.v64[rindex >> 1] = regs->regs[reg_num]; 
+		else
+			regs->regs[reg_num] = arch->pic.u.v64[rindex >> 1];
+	} else if (rindex == 0x44) {
+		/* watchdog/systimer/other_interrupt status */
+		if (write)
+			arch->pic.u.v64[rindex >> 1] = regs->regs[reg_num];
+		else
+			unhandled_exception(__FUNCTION__, address, regs->cp0_epc, write);
+	} else if (rindex == 0x4e) {
+		/* IPI control: set IPI pending at target cpus */
+		if (write) {
+			uint64_t  val = regs->regs[reg_num];
+			uint32_t  rvec, cpuid;
+			struct kvm_vcpu_arch *vcpu_arch;
+
+			arch->pic.u.v64[rindex >> 1] = val;
+
+			if (val & (1 << 23))
+				printk("===== NMI IPI is not supported\n");
+
+			if ((val & (0x7 << 20)) != 0)
+				printk("===== Non Unicast IPI is not supported\n");
+
+			rvec = (val >> 24) & 0x3f;
+			cpuid = xlp_kvm_get_ipi_cpuid (val);
+#ifdef DEBUG
+printk("Sending IPI from cpuid %d to cpuid %d\n", (int)(regs->guest_cp0_ebase & 0x3ff), cpuid);
+#endif
+			vcpu_arch = kvm_get_vcpu_arch_with_cpuid(regs, cpuid);
+			atomic_or_llong(&vcpu_arch->pip_vector, 1ULL << rvec);
+		} else
+			regs->regs[reg_num] = arch->pic.u.v64[rindex >> 1];
+
+	} else if (rindex == 0x50) {
+		/* ACK */
+		unsigned int irt = regs->regs[reg_num];
+		unsigned int rvec, irt_entry = arch->pic.u.v64[(0x200 >> 1) + irt];
+		struct kvm_vcpu_arch *vcpu_arch;
+
+		rvec = (irt_entry >> 24) & 0x3f;
+
+		vcpu_arch = kvm_get_vcpu_arch(regs);
+#ifdef DEBUG
+		printk("===== ACK IRT %u RVEC %u from guest\n", irt, rvec);
+#endif
+		atomic_and_llong(&vcpu_arch->pip_vector, ~(1ULL << rvec));
+
+	} else if (rindex >= 0x74 && rindex <= 0x82) {
+		/* pic timer 0-7 max value */
+		if (write)
+			arch->pic.u.v64[rindex >> 1] = regs->regs[reg_num];
+		else
+			regs->regs[reg_num] = arch->pic.u.v64[rindex >> 1];
+	} else if (rindex >= 0x200 && rindex < 0x400) {
+		/* register 0x200 - 0x400: interrupt redirection table (256 entries) */
+		if (write) {
+#ifdef DEBUG
+			printk("===== Guest IRT index %d, val %lx\n", (rindex - 0x200) >> 1, regs->regs[reg_num]);
+#endif
+			arch->pic.u.v64[rindex >> 1] = regs->regs[reg_num];
+		} else
+			regs->regs[reg_num] = arch->pic.u.v64[rindex >> 1];
+
+	} else
+		unhandled_exception(__FUNCTION__, address, regs->cp0_epc, write);
+}
diff --git a/arch/mips/netlogic/kvm/kvm_sysmgt.c b/arch/mips/netlogic/kvm/kvm_sysmgt.c
new file mode 100644
index 0000000..1fcbd57
--- /dev/null
+++ b/arch/mips/netlogic/kvm/kvm_sysmgt.c
@@ -0,0 +1,104 @@
+/***********************************************************************
+Copyright 2003-2010 Netlogic Microsystems Inc. (“Netlogic”).
+This is a derived work from software originally provided by the external
+entity identified below. The licensing terms and warranties specified in
+the header of the original work apply to this derived work.
+
+*****************************#NETL_1#********************************/
+
+#include <linux/signal.h>
+#include <linux/sched.h>
+#include <linux/interrupt.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/ptrace.h>
+#include <linux/mman.h>
+#include <linux/mm.h>
+#include <linux/smp.h>
+#include <linux/vt_kern.h>		/* For unblank_screen() */
+#include <linux/module.h>
+#include <linux/kvm.h>
+#include <linux/kvm_host.h>
+
+#include <asm/branch.h>
+#include <asm/mmu_context.h>
+#include <asm/uaccess.h>
+#include <asm/ptrace.h>
+#include <asm/highmem.h>		/* For VMALLOC_END */
+
+#include <asm/netlogic/kvm_xlp.h>
+
+static inline void unhandled_exception(const char *func, unsigned long addr,
+	unsigned long epc, unsigned long write)
+{
+	printk("%s: unhandled address %lx, epc %lx, is_write %lx\n",
+		func, addr, epc, write);
+}
+
+void kvm_handle_pcie_sysmgt(struct pt_regs *regs, unsigned long write,
+		unsigned long address, unsigned long reg_num)
+{
+	unsigned int rindex = address >> 2;
+	struct kvm_arch *arch = kvm_get_arch(regs);
+
+	if (rindex == 0x0) {
+		if (!write) {
+			regs->regs[reg_num] = arch->sysmgt.regs[rindex];
+			return;
+		}
+	} else if (rindex == 0x40) {
+		/* power on reset: used to calculate frequency */
+		if (!write) {
+			regs->regs[reg_num] = arch->sysmgt.regs[rindex];
+			return;
+		}
+	} else if (rindex == 0x41) {
+		/* chip reset */
+		if (write) {
+			int val;
+
+			/* Intention to exit. Make a mark here to notify other vcpus. */
+			arch->exit_request = 1;
+
+			printk("Guest vcpu %lu exiting ...\n", regs->guest_cp0_ebase & 0x3ff);
+			val = (KVM_EXIT_QUIT_KVM << 24);
+			 __kvm_vcpu_leave_guest((struct kvm_vcpu *)regs->cp0_osscratch7, val);
+			return;
+		}
+	} else if (rindex == 0x42) {
+		/* cpu reset register */
+		if (write) {
+			int i, v;
+
+			arch->sysmgt.regs[rindex] = regs->regs[reg_num];
+			v = regs->regs[reg_num] & 0xff;
+			for (i = 1; i < 8; i++) {
+				unsigned int core_enable = !(v & (1 << i));
+				if (core_enable) {
+					unsigned int val = i << 2, badinstr, epc_badinstr;
+					val |= (KVM_EXIT_ENABLE_CORE << 24);
+
+					printk("Enable core %d ...\n", i);
+					kvm_get_badinstr(regs, &badinstr, &epc_badinstr);
+					compute_guest_return_epc(regs, epc_badinstr);
+					kvm_save_guest_context(regs, kvm_get_vcpu_guest_regs(regs));	
+					__kvm_vcpu_leave_guest((struct kvm_vcpu *)regs->cp0_osscratch7, val);
+				}
+			}
+		} else {
+			regs->regs[reg_num] = arch->sysmgt.regs[rindex];	
+		}
+		return;
+	} else if (rindex == 0x43) {
+		/* cpu noncoherent mode */
+		if (write)
+			arch->sysmgt.regs[rindex] = regs->regs[reg_num];
+		else
+			regs->regs[reg_num] = arch->sysmgt.regs[rindex];
+		return;
+	}
+
+	unhandled_exception(__FUNCTION__, address, regs->cp0_epc, write);
+}
diff --git a/arch/mips/netlogic/kvm/kvm_traps.c b/arch/mips/netlogic/kvm/kvm_traps.c
new file mode 100644
index 0000000..1a75c7c
--- /dev/null
+++ b/arch/mips/netlogic/kvm/kvm_traps.c
@@ -0,0 +1,316 @@
+
+#include <linux/vmalloc.h>
+#include <linux/ptrace.h>
+#include <linux/kvm_host.h>
+
+#include <asm/uaccess.h>
+#include <asm/kvm.h>
+#include <asm/kvm_host.h>
+#include <asm/netlogic/kvm_xlp.h>
+
+/* Virtualization exceptions (as tagged in cause register) handled here */
+
+#define PSI_UNHANDLED(epc, cause, instr, epc_instr) \
+	printk("PSI (unhandled): epc %lx, cause %lx, badinstr: %x, epc_badinstr: %x\n", epc, cause, instr, epc_instr)
+
+#define FC_UNHANDLED(epc, cause, instr, epc_instr) \
+	printk("FC (captured): epc %lx, cause %lx, badinstr: %x, epc_badinstr: %x\n", epc, cause, instr, epc_instr)
+
+
+#define cacheop(op, base) __asm__ __volatile__ (".set push\n.set mips4\ncache %0, 0(%1)\n.set pop\n" : : "i"(op), "r"(base))
+
+/* Enable guest counting of guest mode */
+#define PERF_GUEST_EC   (0x2 << 23)
+
+/* Guest privileged sensitive instruction */
+static void process_psi(struct pt_regs *regs)
+{
+	unsigned int badinstr, epc_badinstr;
+	unsigned long epc;
+
+	epc = regs->cp0_epc;
+	kvm_get_badinstr(regs, &badinstr, &epc_badinstr);
+
+	if ((badinstr >> 26) == 0x10) {
+		/* cop0 */
+		if ((badinstr & 0xfe00003f) == 0x42000020) {
+			/* We do not want to simply bounce back. As for SMT,
+			 * this will have performance implication.
+			 */
+			__asm__ __volatile__ ("wait" : : : "memory");
+			return; 
+		} else if (((badinstr >> 21) & 0x1f) == 0x0) {
+			/* MF */
+			unsigned int rt = (badinstr >> 16) & 0x1f;
+			unsigned int rd = (badinstr >> 11) & 0x1f;
+			unsigned int sel = badinstr & 0x7;
+
+			if (rd == 15 && sel == 0) {
+				regs->regs[rt] = read_c0_prid();
+			}
+			else if (rd == 12 && sel == 2) {
+				/* srsctl */
+				regs->regs[rt] = 0;
+			} else if (rd == 25 && (sel == 0 || sel == 2 || sel == 4 || sel == 6)) {
+				/* perf counter control registers: enable guest use */
+				int32_t val = 0;
+
+				if (sel == 0) {
+					val = read_c0_perfctrl0();
+					write_c0_perfctrl0(val | PERF_GUEST_EC);
+				} else if (sel == 2) {
+					val = read_c0_perfctrl1();
+					write_c0_perfctrl1(val | PERF_GUEST_EC);
+				} else if (sel == 4) {
+					val = read_c0_perfctrl2();
+					write_c0_perfctrl2(val | PERF_GUEST_EC);
+				} else if (sel == 6) {
+					val = read_c0_perfctrl3();
+					write_c0_perfctrl3(val | PERF_GUEST_EC);
+				}
+				regs->regs[rt] = val;
+			} else
+				PSI_UNHANDLED(epc, regs->cp0_cause, badinstr, epc_badinstr);
+		} else if (((badinstr >> 21) & 0x1f) == 0x4) {
+			/* MT */
+			unsigned int rt = (badinstr >> 16) & 0x1f;
+			unsigned int rd = (badinstr >> 11) & 0x1f;
+			unsigned int sel = badinstr & 0x7;
+
+			if (rd == 9 && sel == 0) {
+				/* count */
+				unsigned int val = regs->regs[rt] - read_c0_count();
+				__write_32bit_c0_register($12, 7, val);
+			} else if (rd == 25 && (sel == 0 || sel == 2 || sel == 4 || sel == 6)) {
+				/* perf cnt registers */
+				if (sel == 0)
+					write_c0_perfctrl0(regs->regs[rt] | PERF_GUEST_EC);
+				else if (sel == 2)
+					write_c0_perfctrl1(regs->regs[rt] | PERF_GUEST_EC);
+				else if (sel == 4)
+					write_c0_perfctrl2(regs->regs[rt] | PERF_GUEST_EC);
+				else if (sel == 6)
+					write_c0_perfctrl3(regs->regs[rt] | PERF_GUEST_EC);
+			} else
+				PSI_UNHANDLED(epc, regs->cp0_cause, badinstr, epc_badinstr);
+		} else
+			PSI_UNHANDLED(epc, regs->cp0_cause, badinstr, epc_badinstr);
+	} else if ((badinstr >> 26) == 0x1c) {
+		if ((badinstr & 0xffff) == 0x18) {
+			/* mfcr */
+			unsigned int rs = (badinstr >> 21) & 0x1f;
+			unsigned int rt = (badinstr >> 16) & 0x1f;
+			unsigned int cr = regs->regs[rs];
+
+			if (cr == 0x304) {
+				/* LSU defeature register */
+			} else if (cr == 0x305) {
+				/* LSU debug addr */
+				/* regs->regs[rt] = (uint32_t)read_32bit_nlm_ctrl_reg(0x3, 0x5); */
+				regs->regs[rt] = 0x0;
+			} else if (cr == 0xa00) {
+				/* map thread mode */
+				regs->regs[rt] = 0x0;
+			} else if (cr == 0x400) {
+				/* mmu setup */
+				regs->regs[rt] = 0x0;
+			} else
+				printk("=== unhandled mfcr cr %x\n", cr);
+
+		} else if ((badinstr & 0xffff) == 0x19) {
+			/* mtcr */
+			unsigned int rs = (badinstr >> 21) & 0x1f;
+			unsigned int rt = (badinstr >> 16) & 0x1f;
+			unsigned int cr = regs->regs[rs];
+
+			if (cr == 0x304) {
+				/* LSU defeature register */
+				;
+			} else if (cr == 0x305) {
+				/* write_32bit_nlm_ctrl_reg (0x3, 0x5, (uint32_t)regs->regs[rt]); */
+				;
+			} else if (cr == 0x306) {
+				/* LSU debug data0 */
+				/* write_64bit_nlm_ctrl_reg (0x3, 0x6, regs->regs[rt]); */
+				;
+			} else if (cr == 0x400) {
+				/* MMU setup */
+				;
+			} else if (cr == 0x700) {
+				/* schedule defeature */
+				;
+			} else if (cr == 0xa00) {
+				/* map thread mode: this is the way to wake up
+				 * other threads in the same core.
+				 */
+				if (regs->regs[rt] != 0 && regs->regs[rt] != 1) {
+					int val;
+
+					/*
+					 * val[15:0]: start cpu id (ebase).
+					 * val[23:16]: # of cpus to spawn.
+					 * val[31:24]: exit code.
+					 */
+					val = regs->guest_cp0_ebase & 0x3ff;
+					val ++;
+					if (regs->regs[rt] == 2)
+						val |= (1 << 16);
+					else
+						val |= (3 << 16);
+					val |= (KVM_EXIT_SPAWN_THREADS << 24);
+
+					printk("Tentatively Leaving the guest (Spawning %d new threads)...\n",
+						(regs->regs[rt] == 2) ? 1 : 3);
+					compute_guest_return_epc(regs, epc_badinstr);
+					kvm_save_guest_context(regs, kvm_get_vcpu_guest_regs(regs));
+					__kvm_vcpu_leave_guest((struct kvm_vcpu *)regs->cp0_osscratch7, val);
+				}
+			} else
+				PSI_UNHANDLED(epc, regs->cp0_cause, badinstr, epc_badinstr);
+
+		} else
+			PSI_UNHANDLED(epc, regs->cp0_cause, badinstr, epc_badinstr);
+	} else if ((badinstr >> 26) == 0x2f) {
+		/* cache instruction */
+		int base, op;
+		uint64_t offset, addr;
+
+		base = (badinstr >> 21) & 0x1f;
+		op = (badinstr >> 16) & 0x1f;
+		offset = badinstr & 0xffff;
+		addr = regs->regs[base] + offset;
+		switch(op) {
+			case 0:  /* I, Index Invalidate */ cacheop(0, addr); break;
+			case 1:  /* D, Index Invalidate */ cacheop(1, addr); break;
+			case 2:  break;
+			case 3:  /* S, Index Invalidate */ cacheop(3, addr); break;
+			case 4:  break;
+			case 5:  break;
+			case 6:  break;
+			case 7:  break;
+			case 8:  /* I, Index Store Tag */ cacheop(8, addr); break;
+			case 9:  /* D, Index Store Tag */ cacheop(9, addr); break;
+			case 10: break;
+			case 11: /* S, Index Store Tag */ cacheop(11, addr); break;
+			case 12: break;
+			case 13: break;
+			case 14: break;
+			case 15: break;
+			case 16: /* I, Hit Invalidate */ cacheop(16, addr); break;
+			case 17: /* D, Hit Invalidate */ cacheop(17, addr); break;
+			case 18: break;
+			case 19: /* S, Hit Invalidate */ cacheop(19, addr); break;
+			case 20: /* I, Hit Writeback */ cacheop(20, addr); break;
+			case 21: /* D, Hit Writeback */ cacheop(21, addr); break;
+			case 22: break;
+			case 23: /* S, Hit Writeback */ cacheop(23, addr); break;
+			case 24: break;
+			case 25: break;
+			case 26: break;
+			case 27: break;
+			case 28: /* I, Fetch and Lock */ cacheop(28, addr); break;
+			case 29: /* D, Fetch and Lock */ cacheop(29, addr); break;
+			case 30: break;
+			case 31: /* S, Fetch and Lock */ cacheop(31, addr); break;
+		}
+	} else
+		PSI_UNHANDLED(epc, regs->cp0_cause, badinstr, epc_badinstr);
+
+	compute_guest_return_epc(regs, epc_badinstr);
+}
+
+/* Guest field change */
+static void process_fc(struct pt_regs *regs)
+{
+	unsigned int badinstr, epc_badinstr;
+	unsigned long epc;
+
+	epc = regs->cp0_epc;
+	kvm_get_badinstr(regs, &badinstr, &epc_badinstr);
+
+	if ((badinstr >> 26) == 0x10) {
+		if (((badinstr >> 21) & 0x1f) == 0x4) {
+			unsigned int rt = (badinstr >> 16) & 0x1f;
+			unsigned int rd = (badinstr >> 11) & 0x1f;
+			unsigned int sel = badinstr & 0x7;
+
+			if (rd == 12 && sel == 0) {
+				__write_32bit_guest_c0_register($12, 0, regs->regs[rt]);
+			} else if (rd == 13 && sel == 0) {
+				__write_32bit_guest_c0_register($13, 0, regs->regs[rt]);
+			} else if (rd == 12 && sel == 1) {
+				__write_32bit_guest_c0_register($12, 1, regs->regs[rt]);
+			} else if (rd == 25 && (sel == 0 || sel == 2 || sel == 4 || sel == 6)) {
+				/* perf cnt registers: event field may get changed */
+				if (sel == 0)
+					write_c0_perfctrl0(regs->regs[rt] | PERF_GUEST_EC);
+				else if (sel == 2)
+					write_c0_perfctrl1(regs->regs[rt] | PERF_GUEST_EC);
+				else if (sel == 4)
+					write_c0_perfctrl2(regs->regs[rt] | PERF_GUEST_EC);
+				else if (sel == 6)
+					write_c0_perfctrl3(regs->regs[rt] | PERF_GUEST_EC);
+			} else
+				FC_UNHANDLED(epc, regs->cp0_cause, badinstr, epc_badinstr);
+		} else
+			FC_UNHANDLED(epc, regs->cp0_cause, badinstr, epc_badinstr);
+	} else
+		FC_UNHANDLED(epc, regs->cp0_cause, badinstr, epc_badinstr);
+
+	compute_guest_return_epc(regs, epc_badinstr);
+}
+
+/* Guest hypercall */
+static void process_hc(struct pt_regs *regs)
+{
+	unsigned int badinstr, epc_badinstr;
+	unsigned long epc;
+
+	epc = regs->cp0_epc;
+	kvm_get_badinstr(regs, &badinstr, &epc_badinstr);
+
+	printk("Hypercall exception not implemented:\n");
+	printk("\tepc 0x%lx, badinstr: 0x%x\n", epc, badinstr);
+
+	compute_guest_return_epc(regs, epc_badinstr);
+}
+
+/* Guest reserved instruction redirect */
+static void process_grr(struct pt_regs *regs)
+{
+	panic("GRR exception not implemented\n");
+}
+
+/* Guest cp0 mode change event */
+static void process_mc(struct pt_regs *regs)
+{
+	panic("MC exception not implemented\n");
+}
+
+void process_virt_exception(struct pt_regs *regs)
+{
+	int excode = (regs->cp0_guestctl0 >> 2) & 0x1f;
+
+	switch(excode) {
+	case 0x0:
+		process_psi(regs);
+		break;
+	case 0x1:
+		process_fc(regs);
+		break;
+	case 0x2:
+		process_hc(regs);
+		break;
+	case 0x3:
+		process_grr(regs);
+		break;
+	case 0x9:
+		process_mc(regs);
+		break;
+	default:
+		panic("%s: Unimpled excode %d, epc 0x%lx, bad insn 0x%lx\n",
+			__FUNCTION__, excode, regs->cp0_epc, regs->cp0_badinstr);
+	}
+
+	return;
+}
diff --git a/arch/mips/netlogic/kvm/kvm_uart.c b/arch/mips/netlogic/kvm/kvm_uart.c
new file mode 100644
index 0000000..1f5ce60
--- /dev/null
+++ b/arch/mips/netlogic/kvm/kvm_uart.c
@@ -0,0 +1,159 @@
+/***********************************************************************
+Copyright 2003-2010 Netlogic Microsystems Inc. (“Netlogic”).
+This is a derived work from software originally provided by the external
+entity identified below. The licensing terms and warranties specified in
+the header of the original work apply to this derived work.
+
+*****************************#NETL_1#********************************/
+
+#include <linux/signal.h>
+#include <linux/sched.h>
+#include <linux/interrupt.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/ptrace.h>
+#include <linux/mman.h>
+#include <linux/mm.h>
+#include <linux/smp.h>
+#include <linux/vt_kern.h>		/* For unblank_screen() */
+#include <linux/module.h>
+#include <linux/kvm.h>
+#include <linux/kvm_host.h>
+
+#include <asm/branch.h>
+#include <asm/mmu_context.h>
+#include <asm/uaccess.h>
+#include <asm/ptrace.h>
+#include <asm/highmem.h>		/* For VMALLOC_END */
+
+#include <asm/netlogic/kvm_xlp.h>
+
+static inline void unhandled_exception(const char *func, unsigned long addr,
+	unsigned long epc, unsigned long write)
+{
+	printk("%s: unhandled address %lx, epc %lx, is_write %lx\n",
+		func, addr, epc, write);
+}
+
+void kvm_handle_pcie_uart(struct pt_regs *regs, unsigned long write,
+		unsigned long address, unsigned long reg_num)
+{
+	unsigned int rindex = address >> 2;
+	struct kvm_arch *arch = kvm_get_arch(regs);
+
+	if (rindex == 0x0 || rindex == 0x2 || rindex == 0x3d) {
+		if (!write)
+			regs->regs[reg_num] = arch->uart.header[rindex];
+		else
+			unhandled_exception(__FUNCTION__, address, regs->cp0_epc, write);
+	} else if (rindex == 0x40) {
+		if (write) {
+			if (arch->uart.lcr & (1 << 7))
+				arch->uart.dlb1 = regs->regs[reg_num];
+			else {
+				uint8_t c = regs->regs[reg_num];
+				uint32_t val, badinstr, epc_badinstr;
+
+				if (arch->uart.ier & 0x2) {
+					arch->uart.iir = 0x2;
+
+					/* populate an interrupt to guest */
+					kvm_pic_inject_guest(arch,
+						arch->uart.header[0x3d] & 0xffff,
+						regs);
+				}
+
+				/* Quit the guest and go to QEMU for real print */
+				val = (KVM_EXIT_CHAR_PRINT << 24) | c;
+				kvm_get_badinstr(regs, &badinstr, &epc_badinstr);
+				compute_guest_return_epc(regs, epc_badinstr);
+				kvm_save_guest_context(regs, kvm_get_vcpu_guest_regs(regs));
+				__kvm_vcpu_leave_guest((struct kvm_vcpu *)regs->cp0_osscratch7, val);
+			}
+		} else {
+			if (arch->uart.lcr & (1 << 7))
+				regs->regs[reg_num] = arch->uart.dlb1;
+			else {
+				/* read a character */
+				if (arch->uart.input_size > 0) {
+					int idx;
+					regs->regs[reg_num] = arch->uart.input_buf[0];
+
+					arch->uart.input_size --;
+					for (idx = 0; idx < arch->uart.input_size; idx++)
+						arch->uart.input_buf[idx] =
+							arch->uart.input_buf[idx + 1];
+				} else
+					regs->regs[reg_num] = 0x0;
+
+				if (arch->uart.input_size == 0) {
+					/* no data is available */
+					arch->uart.lsr &= 0xfe;
+
+					/* clear "receive data ready" and interrupt */
+					if ((arch->uart.iir & 0xe) == 4) {
+						arch->uart.iir &= ~0x4;
+						arch->uart.iir |= 0x1;
+					}
+				}
+			}
+		}
+	} else if (rindex == 0x41) {
+		if (write) {
+			if (arch->uart.lcr & (1 << 7))
+				arch->uart.dlb2 = regs->regs[reg_num];
+			else
+				arch->uart.ier = regs->regs[reg_num] & 0xf;
+		} else {
+			if (arch->uart.lcr & (1 << 7))
+				regs->regs[reg_num] = arch->uart.dlb2;
+			else
+				regs->regs[reg_num] = arch->uart.ier;
+		}
+	} else if (rindex == 0x42) {
+		/* uart interrupt source /fifo control */
+		if (write)
+			; /* do nothing */
+		else {
+			unsigned int tmp = arch->uart.iir;
+			regs->regs[reg_num] = tmp;
+
+			if ((tmp & 0x0e) == 0x02) {
+				arch->uart.iir &= ~0x02;
+				arch->uart.iir |= 0x1;
+			}
+		}
+	} else if (rindex == 0x43) {
+		/* line control */
+		if (write)
+			arch->uart.lcr = regs->regs[reg_num];
+		else
+			regs->regs[reg_num] = arch->uart.lcr;
+	} else if (rindex == 0x44) {
+		/* modem control */
+		if (write)
+			arch->uart.mcr = regs->regs[reg_num];
+		else
+			regs->regs[reg_num] = arch->uart.mcr;
+	} else if (rindex == 0x45) {
+		/* line status */
+		if (write)
+			arch->uart.lsr = regs->regs[reg_num];
+		else {
+			regs->regs[reg_num] = arch->uart.lsr;
+
+			/* clear the related bits and clear the interrupt upon read */
+			arch->uart.lsr &= 0x61;
+			arch->uart.iir |= 0x01;
+		}
+	} else if (rindex == 0x46) {
+		/* modem status  */
+		if (write)
+			arch->uart.msr = regs->regs[reg_num];
+		else
+			regs->regs[reg_num] = arch->uart.msr;
+	} else
+		unhandled_exception(__FUNCTION__, address, regs->cp0_epc, write);
+}
diff --git a/include/uapi/linux/kvm.h b/include/uapi/linux/kvm.h
index f1ee1e4..5ac6c8f 100644
--- a/include/uapi/linux/kvm.h
+++ b/include/uapi/linux/kvm.h
@@ -172,6 +172,16 @@ struct kvm_pit_config {
 #define KVM_EXIT_S390_TSCH        22
 #define KVM_EXIT_EPR              23
 
+#ifdef CONFIG_CPU_XLP
+/* Let us start from 25, to avoid future migration/merging where
+ * value 21-24 may be taken.
+ */
+#define	KVM_EXIT_SPAWN_THREADS	  25
+#define	KVM_EXIT_ENABLE_CORE	  26
+#define	KVM_EXIT_QUIT_KVM	  27
+#define	KVM_EXIT_CHAR_PRINT	  28
+#endif
+
 /* For KVM_EXIT_INTERNAL_ERROR */
 /* Emulate instruction failed. */
 #define KVM_INTERNAL_ERROR_EMULATION	1
@@ -301,7 +311,17 @@ struct kvm_run {
 		struct {
 			__u32 epr;
 		} epr;
-		/* Fix the size of the union. */
+#ifdef CONFIG_CPU_XLP
+		/* KVM_EXIT_SPAWN_THREAD */
+		struct {
+			__u8  num_vcpus;
+			__u16 start_hw_cpuid;
+		} mips_spawn_thread;
+		/* KVM_EXIT_CHAR_PRINT */
+		struct {
+			__u8 c;
+		} mips_output_char;
+#endif
 		char padding[256];
 	};
 
-- 
1.9.1

