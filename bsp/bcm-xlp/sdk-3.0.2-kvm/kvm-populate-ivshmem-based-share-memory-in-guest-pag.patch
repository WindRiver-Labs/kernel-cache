From d9ebbb02a522832ab4d7f25ed3713433db882b5f Mon Sep 17 00:00:00 2001
From: Yonghong Song <ysong@broadcom.com>
Date: Wed, 13 Nov 2013 14:39:08 -0800
Subject: [PATCH] kvm: populate ivshmem-based share memory in guest page table

Commit 241cc21637483ce7575b10462269efe3bc9fe949 from Broadcom SDK 3.0.2

o the ivshmem memory comes during guest pcie discovery for ivshmem device.

Signed-off-by: Yonghong Song <ysong@broadcom.com>
Signed-off-by: Nam Ninh <nam.ninh@windriver.com>

diff --git a/arch/mips/include/asm/kvm_host.h b/arch/mips/include/asm/kvm_host.h
index c832837..090eaba 100644
--- a/arch/mips/include/asm/kvm_host.h
+++ b/arch/mips/include/asm/kvm_host.h
@@ -64,6 +64,9 @@ struct kvm_arch {
 	unsigned int	guest_id;
 	unsigned int	exit_request;
 	unsigned long long gpa_pgd;
+	unsigned int	mem_synced:1;
+	unsigned int	slot_inited:16; /* The number of bits should
+				be at least KVM_MEMORY_SLOTS */
 	struct {
 		unsigned long long gpa_start;
 		unsigned long long rpa_start;
diff --git a/arch/mips/kvm/kvm.c b/arch/mips/kvm/kvm.c
index cb46737..7ab1c0a 100644
--- a/arch/mips/kvm/kvm.c
+++ b/arch/mips/kvm/kvm.c
@@ -119,6 +119,74 @@ static void kvm_flush_rtlb(void *args)
 	return;
 }
 
+static void kvm_populate_gpa_map(pgd_t *gpa_pgd, uint64_t msize,
+	uint64_t gpa, uint64_t hva)
+{
+	uint64_t address, pa;
+
+#if 0
+	printk("KVM: populate guest memory: hva = 0x%llx, gpa = 0x%llx, size = 0x%llx\n",
+		hva, gpa, msize);
+#endif
+
+	for (address = hva; address < (hva + msize);) {
+		pgd_t *pgdp;
+		pud_t *pudp;
+		pmd_t *pmdp;
+		pte_t *ptep, pte;
+		int i;
+		unsigned long *ptr;
+
+		pgdp = current->mm->pgd + __pgd_offset(address);
+		pudp = pud_offset(pgdp, address);
+		pmdp = pmd_offset(pudp, address);
+		ptep = pte_offset(pmdp, address);
+		pte = *	ptep;
+
+		/* map gpa to pte in page table pointed to by gpa_pgd */
+		pgdp = (pgd_t *)gpa_pgd + __pgd_offset(gpa);
+		pudp = (pud_t *)pgdp;
+#ifndef __PAGETABLE_PMD_FOLDED
+		if (*(unsigned long *)pudp == (unsigned long)invalid_pmd_table) {
+			/* allocate a page for pmd and initialize it */
+			ptr = (unsigned long *)__get_free_page(GFP_KERNEL|GFP_DMA);
+			for (i = 0; i < PTRS_PER_PMD; i++)
+				ptr[i] = (unsigned long)invalid_pte_table;
+			*(unsigned long *)pudp = (unsigned long)ptr;
+		}
+		pmdp = (pmd_t *)*(unsigned long *)pudp + __pmd_offset(gpa);
+		if (*(unsigned long *)pmdp == (unsigned long)invalid_pte_table) {
+			/* allocate a page for pmd and initialize it */
+			ptr = (unsigned long *)__get_free_page(GFP_KERNEL|GFP_DMA);
+			for (i = 0; i < PTRS_PER_PTE; i++)
+				ptr[i] = 0;
+			*(unsigned long *)pmdp = (unsigned long)ptr;
+		}
+		ptep = (pte_t *)*(unsigned long *)pmdp + __pte_offset(gpa);
+#else
+		if (*(unsigned long *)pudp == (unsigned long)invalid_pte_table) {
+			/* allocate a page for pmd and initialize it */
+			ptr = (unsigned long *)__get_free_page(GFP_KERNEL|GFP_DMA);
+			for (i = 0; i < PTRS_PER_PTE; i++)
+				ptr[i] = 0;
+			*(unsigned long *)pudp = (unsigned long)ptr;
+		}
+		ptep = (pte_t *)*(unsigned long *)pudp + __pte_offset(gpa);
+#endif
+		*ptep = pte;
+
+		/* flush the Reset Vector region as it will be accessed as uncached. */
+		if ((gpa & 0xfffffffffff00000ULL) == 0x1fc00000) {
+			if ((pte_val(pte) >> _PAGE_PRESENT_SHIFT) & 0x1) {
+				pa = (pte_val(pte) >> (_PAGE_GLOBAL_SHIFT + 6)) << 12;
+				nlm_flush_cache_L2L3(pa, PAGE_SIZE);
+			}
+		}
+
+		address += PAGE_SIZE, gpa += PAGE_SIZE;
+	}
+}
+
 /* map gpa->pa and also flush references for pa in the cache to provide a clean memory
  * for guest.
  */
@@ -127,11 +195,13 @@ static void kvm_sync_gpa_map(struct kvm *kvm)
 	int t;
 	pgd_t *gpa_pgd;
 
+	kvm->arch.mem_synced = 1; /* sync is called */
+
 	gpa_pgd = (pgd_t *)kvm->arch.gpa_pgd;
 
 	for (t = 0; t < KVM_MEMORY_SLOTS; t++) {
 		struct kvm_memory_slot *s = &kvm->memslots->memslots[t];
-		uint64_t msize, gpa, hva, address, pa;
+		uint64_t msize, gpa, hva;
 
 		if (!s->npages)
 			continue;
@@ -141,64 +211,8 @@ static void kvm_sync_gpa_map(struct kvm *kvm)
 		hva = s->userspace_addr;
 
 		/* enumerate all hva pages */
-		for (address = hva; address < (hva + msize);) {
-			pgd_t *pgdp;
-			pud_t *pudp;
-			pmd_t *pmdp;
-			pte_t *ptep, pte;
-			int i;
-			unsigned long *ptr;
-
-			pgdp = current->mm->pgd + __pgd_offset(address);
-			pudp = pud_offset(pgdp, address);
-			pmdp = pmd_offset(pudp, address);
-			ptep = pte_offset(pmdp, address);
-			pte = *	ptep;
-
-			/* map gpa to pte in page table pointed to by gpa_pgd */
-			pgdp = (pgd_t *)gpa_pgd + __pgd_offset(gpa);
-			pudp = (pud_t *)pgdp;
-#ifndef __PAGETABLE_PMD_FOLDED
-			if (*(unsigned long *)pudp == (unsigned long)invalid_pmd_table) {
-				/* allocate a page for pmd and initialize it */
-				ptr = (unsigned long *)__get_free_page(GFP_KERNEL|GFP_DMA);
-				for (i = 0; i < PTRS_PER_PMD; i++)
-					ptr[i] = (unsigned long)invalid_pte_table;
-				*(unsigned long *)pudp = (unsigned long)ptr;
-
-			}
-			pmdp = (pmd_t *)*(unsigned long *)pudp + __pmd_offset(gpa);
-			if (*(unsigned long *)pmdp == (unsigned long)invalid_pte_table) {
-				/* allocate a page for pmd and initialize it */
-				ptr = (unsigned long *)__get_free_page(GFP_KERNEL|GFP_DMA);
-				for (i = 0; i < PTRS_PER_PTE; i++)
-					ptr[i] = 0;
-				*(unsigned long *)pmdp = (unsigned long)ptr;
-			}
-			ptep = (pte_t *)*(unsigned long *)pmdp + __pte_offset(gpa);
-#else
-			if (*(unsigned long *)pudp == (unsigned long)invalid_pte_table) {
-				/* allocate a page for pmd and initialize it */
-				ptr = (unsigned long *)__get_free_page(GFP_KERNEL|GFP_DMA);
-				for (i = 0; i < PTRS_PER_PTE; i++)
-					ptr[i] = 0;
-				*(unsigned long *)pudp = (unsigned long)ptr;
-
-			}
-			ptep = (pte_t *)*(unsigned long *)pudp + __pte_offset(gpa);
-#endif
-			*ptep = pte;
-
-			/* flush the Reset Vector region as it will be accessed as uncached. */
-			if ((gpa & 0xfffffffffff00000ULL) == 0x1fc00000) {
-				if ((pte_val(pte) >> _PAGE_PRESENT_SHIFT) & 0x1) {
-					pa = (pte_val(pte) >> (_PAGE_GLOBAL_SHIFT + 6)) << 12;
-					nlm_flush_cache_L2L3(pa, PAGE_SIZE);
-				}
-			}
-
-			address += PAGE_SIZE, gpa += PAGE_SIZE;
-		}
+		kvm_populate_gpa_map(gpa_pgd, msize, gpa, hva);
+		kvm->arch.slot_inited |= (1 << t); /* slot has been populated into gpa->pa table */
 	}
 
 	/* packet memory */
@@ -652,9 +666,22 @@ void kvm_arch_commit_memory_region(struct kvm *kvm,
 				   const struct kvm_memory_slot *old,
 				   enum kvm_mr_change change)
 {
-	/* first time for the memory region, do nothing */
-	if (old->npages == 0)
+	if (old->npages == 0) {
+		/* if after sync and not initialized, go ahead to initialize the page table */
+		if (kvm->arch.mem_synced
+		    && (kvm->arch.slot_inited & (1 << mem->slot)) == 0) {
+			pgd_t *gpa_pgd = (pgd_t *)kvm->arch.gpa_pgd;
+			uint64_t msize, gpa, hva;
+
+			msize = mem->memory_size;
+			gpa = mem->guest_phys_addr;
+			hva = mem->userspace_addr;
+
+			kvm_populate_gpa_map(gpa_pgd, msize, gpa, hva);
+			kvm->arch.slot_inited |= (1 << mem->slot);
+		}
 		return;
+	}
 
 	/* no flag change, do nothing */
 	if (old->flags == mem->flags)
diff --git a/arch/mips/kvm/xlp.c b/arch/mips/kvm/xlp.c
index f90a14f..039ec48 100644
--- a/arch/mips/kvm/xlp.c
+++ b/arch/mips/kvm/xlp.c
@@ -397,6 +397,8 @@ void xlp_kvm_init_vm(struct kvm *kvm)
 	arch->guest_id = kvm_get_new_guest_id();
 	arch->exit_request = 0;
 	arch->pktmem.gpa_start = -1;
+	arch->mem_synced = 0;
+	arch->slot_inited = 0;
 
 	arch->gpa_pgd = (uint64_t)pgd_alloc(NULL);
 
diff --git a/drivers/iommu/xlp-iommu.c b/drivers/iommu/xlp-iommu.c
index 2e496df..32f26c7 100644
--- a/drivers/iommu/xlp-iommu.c
+++ b/drivers/iommu/xlp-iommu.c
@@ -63,7 +63,9 @@ static void xlp_iommu_detach_device(struct iommu_domain *domain,
 static int xlp_iommu_map(struct iommu_domain *domain, unsigned long iova,
 	phys_addr_t paddr, size_t size, int prot)
 {
+#if 0
 	printk("%s is called ...\n", __FUNCTION__);
+#endif
 	return 0;
 }
 
@@ -84,7 +86,9 @@ static int xlp_iommu_domain_has_cap(struct iommu_domain *domain,
 static phys_addr_t xlp_iommu_iova_to_phys(struct iommu_domain *domain,
                                           unsigned long iova)
 {
+#if 0
 	printk("%s is called (iova 0x%lx)...\n", __FUNCTION__, iova);
+#endif
 	return 0;
 }
 
@@ -95,7 +99,9 @@ static int xlp_iommu_add_device(struct device *dev)
 	struct iommu_group *group;
 	int ret;
 
+#if 0
 	printk("%s is called ...\n", __FUNCTION__);
+#endif
 
 	dma_pdev = pci_dev_get(pdev);
 	group = iommu_group_get(&dma_pdev->dev);
-- 
1.9.1

