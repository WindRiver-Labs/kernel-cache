From f1b08ebedbf790fd8b758b9d61944659d938a791 Mon Sep 17 00:00:00 2001
From: Wei Zhang <wezhang@broadcom.com>
Date: Mon, 25 Jun 2012 18:11:52 -0700
Subject: [PATCH 569/762] cache_exception:modify cache exception handler

when driver access the pcie device's mapped memory and the device stop clocking, the LSU will not
get response from the device, so a cache exception generated, the log1 will have the physical
address but log0 shows the exception is invalid.

Since this exception is related to pcie device, the cpu core still function properly, so modified
handler will check whether the error is valid and print out warning, then return back to normal
execution.

cex-gen.S: 	added saving state, which allowed cpu jump back.
cerr-nlm.c: added cache error check and print out.

Based on Broadcom SDK 2.3.

Signed-off-by: Wei Zhang <wezhang@broadcom.com>
Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 arch/mips/mm/cerr-nlm.c |  696 +++++++++++++++++++++++++++++++++++++++++++----
 arch/mips/mm/cex-gen.S  |  207 ++++++++++++--
 2 files changed, 827 insertions(+), 76 deletions(-)

diff --git a/arch/mips/mm/cerr-nlm.c b/arch/mips/mm/cerr-nlm.c
index 6903d77..0fa380a 100644
--- a/arch/mips/mm/cerr-nlm.c
+++ b/arch/mips/mm/cerr-nlm.c
@@ -27,21 +27,36 @@ THE POSSIBILITY OF SUCH DAMAGE.
 
 #include <asm/netlogic/iomap.h>
 #include <asm/netlogic/hal/nlm_hal.h>
-//#include <asm/netlogic/hal/nlm_hal_pic.h>
+#include <asm/netlogic/hal/nlm_hal_macros.h>
+#include <asm/netlogic/xlp.h>
 #include <asm/netlogic/xlp_hal_pic.h>
+#include <asm/netlogic/xlp_irq.h>
 #include <asm/netlogic/hal/nlm_hal_xlp_dev.h>
+#include <asm/inst.h>
+#include <asm/ptrace.h>
+#include <asm/fpu.h>
+#include <asm/asm.h>
+#include <asm/mipsregs.h>
 
 unsigned char nlm_cerr_stack[8192];
 volatile int nlm_cerr_lock;
 
+static void LoadRegisters(struct pt_regs *regs, unsigned long *pErrepc);
+static void RestoreRegisters(struct pt_regs *regs, unsigned long errepc);
+static int compute_return_registers(struct pt_regs *regs, unsigned long *pErrepc);
+static int update_registers(void);
+
 static __inline__ void cerr_cpu_halt(void)
 {
 	for(;;) {
-		__asm__ __volatile__(".set mips64\n"
-				     "1: wait \n"
-				     "   b 1b\n"
-				     "   nop\n"
-			);
+		__asm__ __volatile__(
+			".set push      \n"
+			".set mips64    \n"
+			"1: wait        \n"
+			"   b 1b        \n"
+			"   nop         \n"
+			".set pop       \n"
+		);
 	}
 }
 
@@ -94,6 +109,13 @@ static void cerr_printk(const char *fmt, ...)
 
 }
 
+static void print_error_header(void)
+{
+	cerr_printk("*******************************************************************************************\n");
+	cerr_printk("cpu_%d received a bus/cache error(warning)\n", hard_smp_processor_id());
+	cerr_printk("*******************************************************************************************\n");
+}
+
 #ifdef CONFIG_NLM_XLR
 
 static char *bridge_aerr_intr_devstat[] = {
@@ -116,13 +138,15 @@ static char *bridge_aerr_intr_devstat[] = {
 	[15] = "DMA",
 };
 
-static void print_cerr_info(void)
+static int print_cerr_info(void)
 {
 	__u64 cerr_cpu_log = 0;
 	int i = 0;
 	nlm_reg_t *mmio = netlogic_io_mmio(NETLOGIC_IO_BRIDGE_OFFSET);
 	__u32 tmp = 0;
 
+	print_error_header();
+
 	cerr_printk("Bridge: Phys Addr = 0x%010llx, Device_AERR = 0x%08x\n",
 			( ((__u64)netlogic_read_reg(mmio, 39)<<5) | ((__u64)netlogic_read_reg(mmio, 40)<<37) ),
 			netlogic_read_reg(mmio, 41));
@@ -136,11 +160,289 @@ static void print_cerr_info(void)
 	cerr_cpu_log = read_64bit_nlm_ctrl_reg(CPU_BLOCKID_LSU, LSU_CERRLOG_REGID);
 	cerr_printk("CPU: (XLR specific) Cache Error log = 0x%016llx, Phy Addr = 0x%010llx\n",
 			cerr_cpu_log, ((cerr_cpu_log >> 10) & 0xffffffffffULL) << 3);
+
+	return 1;
 }
 
 #else
 
-static char *nbu_reqsrc[] = {
+static void dump_cerr_info(void)
+{
+	int n, num_controllers, iRet;
+	uint32_t dram_log1, dram_log2;
+	int node = hard_smp_processor_id() / NLM_MAX_CPU_PER_NODE;
+	uint64_t mmio = nlm_hal_get_dev_base(node, 0, NLH_BRIDGE, 0);
+
+	uint32_t nbu_reg0 = nlm_hal_read_32bit_reg(mmio, 0xA2);
+	uint32_t nbu_reg1 = nlm_hal_read_32bit_reg(mmio, 0xA3);
+	uint32_t nbu_reg2 = nlm_hal_read_32bit_reg(mmio, 0xA4);
+
+	uint32_t icu_log0 = read_32bit_nlm_ctrl_reg(CPU_BLOCKID_ICU, ICU_CERRLOG0_REGID);
+	uint32_t icu_log1 = read_32bit_nlm_ctrl_reg(CPU_BLOCKID_ICU, ICU_CERRLOG1_REGID);
+	uint32_t icu_log2 = read_32bit_nlm_ctrl_reg(CPU_BLOCKID_ICU, ICU_CERRLOG2_REGID);
+
+	uint32_t lsu_log0 = read_32bit_nlm_ctrl_reg(CPU_BLOCKID_LSU, LSU_CERRLOG0_REGID);
+	uint64_t lsu_log1 = read_64bit_nlm_ctrl_reg(CPU_BLOCKID_LSU, LSU_CERRLOG1_REGID);
+
+	uint32_t scu_log0 = read_32bit_nlm_ctrl_reg(CPU_BLOCKID_SCU, SCU_CERRLOG0_REGID);
+	uint32_t scu_log1 = read_32bit_nlm_ctrl_reg(CPU_BLOCKID_SCU, SCU_CERRLOG1_REGID);
+	uint32_t scu_log2 = read_32bit_nlm_ctrl_reg(CPU_BLOCKID_SCU, SCU_CERRLOG2_REGID);
+
+	uint32_t l3_reg0 = nlm_hal_read_32bit_reg(mmio, 0xD9);
+	uint32_t l3_reg1 = nlm_hal_read_32bit_reg(mmio, 0xDA);
+	uint32_t l3_reg2 = nlm_hal_read_32bit_reg(mmio, 0xDB);
+
+	cerr_printk("CPU (XLP specific) registers dump: Node=%d \n", node);
+	cerr_printk("     ICU: log0 = 0x%08x, log1 = 0x%08x, log2 = 0x%08x\n",
+				icu_log0, icu_log1, icu_log2);
+
+	cerr_printk("     LSU: log0 = 0x%08x, log1 = 0x%016llx\n", lsu_log0, lsu_log1);
+
+	cerr_printk("     SCU: log0 = 0x%08x, log1 = 0x%08x, log2 = 0x%08x\n",
+				scu_log0, scu_log1, scu_log2);
+
+	cerr_printk("     TCU: reg0 = 0x%08x, reg1 = 0x%08x, reg2 = 0x%08x\n",
+				l3_reg0, l3_reg1, l3_reg2);
+
+	cerr_printk("     NBU: reg0 = 0x%08x, reg1 = 0x%08x, reg2 = 0x%08x\n",
+				nbu_reg0, nbu_reg1, nbu_reg2);
+
+	num_controllers = 4;	//8xx has 4 controller 
+	iRet=is_nlm_xlp(300, XLP_REVISION_ANY, CPU_EXTPID_XLP_3XX_ANY);
+	if( iRet ) num_controllers = 2;
+	else
+	{
+		iRet=is_nlm_xlp(200, XLP_REVISION_ANY, 0 );
+		if( iRet ) num_controllers = 1;
+	}
+
+	for(n=0; n<num_controllers; n++)
+	{
+		dram_log1 = nlm_hal_read_32bit_reg(mmio, n*0x80+0x11D);
+		dram_log2 = nlm_hal_read_32bit_reg(mmio, n*0x80+0x11E);
+		cerr_printk("  DRAM_%c: reg1 = 0x%08x, reg2 = 0x%08x\n",
+                (char)(n+'A'), dram_log1, dram_log2);
+	}
+
+	cerr_printk("\n     CPU: epc = 0x%lx, errorepc = 0x%016llx, cacheerr = 0x%08x\n",
+			read_c0_epc(), read_c0_errorepc(), read_c0_cacheerr());
+	cerr_printk("*******************************************************************************************\n");
+}
+
+static char * c_icu_errinfo[]={
+	[0] = "L1 tag RAM",
+	[1] = "L1 data RAM",
+	[2] = "Not defined"
+};
+
+static int Check_ICU_error(void)
+{
+	uint32_t icu_log0 = read_32bit_nlm_ctrl_reg(CPU_BLOCKID_ICU, ICU_CERRLOG0_REGID);
+	uint32_t icu_log1 = read_32bit_nlm_ctrl_reg(CPU_BLOCKID_ICU, ICU_CERRLOG1_REGID);
+	uint32_t icu_log2 = read_32bit_nlm_ctrl_reg(CPU_BLOCKID_ICU, ICU_CERRLOG2_REGID);
+	uint64_t phyaddr = ( (uint64_t)icu_log2 << 32) | icu_log1;
+	uint32_t errinfo = (icu_log0 >> 12) & 0xF;
+	uint32_t errtype = (icu_log0 >> 9) & 7;
+	uint32_t oprtype = (icu_log0 >> 6) & 7;
+
+	if( (icu_log0&0xF) == 0 && icu_log1 == 0 && icu_log2 == 0 ) return 0;
+
+	cerr_printk("ICU Cache %s:\n", (icu_log0&0xF) ? "Error" : "Warning" );
+	cerr_printk("   log0=0x08x, phyaddr=0x016llx\n", icu_log0, phyaddr);
+	if( (icu_log0 & 0xF) == 0 )
+	{
+		write_32bit_nlm_ctrl_reg(CPU_BLOCKID_ICU,ICU_CERRLOG1_REGID, 0);
+		write_32bit_nlm_ctrl_reg(CPU_BLOCKID_ICU,ICU_CERRLOG2_REGID, 0);
+		return 0;
+	}
+	else
+	{
+		cerr_printk("          errinfo: %s\n", errinfo < 2  ? c_icu_errinfo[errinfo] : c_icu_errinfo[2] );
+		cerr_printk("          errtype: %s\n", errtype == 0 ? "Single‐bit error" : "Not defined");
+		cerr_printk("          oprtype: %s\n", oprtype == 0 ? "Read access" : "Not defined");
+		cerr_printk("    Uncorrectable: %d\n", ( icu_log0 >> 5) & 1);
+		cerr_printk("         Overflow: %d\n", ( icu_log0 >> 4 ) & 1);
+		cerr_printk("     Threads Mask: %x\n", icu_log0 & 0xF );
+		return 1;
+	}
+}
+
+static int Check_LSU_error(void)
+{
+	uint32_t lsu_log0 = read_32bit_nlm_ctrl_reg(CPU_BLOCKID_LSU, LSU_CERRLOG0_REGID);
+	uint64_t lsu_log1 = read_64bit_nlm_ctrl_reg(CPU_BLOCKID_LSU, LSU_CERRLOG1_REGID);
+	if( (lsu_log0 & 0xF) == 0 && lsu_log1 == 0 ) return 0;
+
+	cerr_printk("LSU Cache %s:\n", (lsu_log0&0xF) ? "Error" : "Warning" );
+	cerr_printk("   log0=0x%016llx, phyaddr=0x%010llx\n", lsu_log0, lsu_log1);
+	if( (lsu_log0 & 0xF ) == 0 )
+	{
+		write_64bit_nlm_ctrl_reg(CPU_BLOCKID_LSU, LSU_CERRLOG1_REGID, 0);
+		return 0;
+	}
+	else
+	{
+		cerr_printk("                 Info: %s\n", ((lsu_log0>>11)&1)==0 ? "TAG" : "DATA" );
+		cerr_printk("              ErrType: %s\n", ((lsu_log0>>10)&1)==0 ? "Parity Error" :  "Reserved");
+		cerr_printk("               OpType: %s\n", ((lsu_log0>> 6)&1)==0 ? "Read" :  "Reserved" );
+		cerr_printk("   Ucorrectable Error: %d\n", (lsu_log0>>5)&1);
+		cerr_printk("       Error Overflow: %d\n", (lsu_log0>>4)&1);
+		cerr_printk("   Error threads mask: %x\n", lsu_log0&0xF);
+		return 1;
+	}
+}
+
+static char *c_l2_errtype[] = {
+	[0] = "Single‐bit Tag RAM error",
+	[1] = "Double‐bit Tag RAM Error",
+	[2] = "Valid Array Parity Error",
+	[3] = "Single‐bit Data RAM Error",
+	[4] = "Double‐bit Data RAM Error",
+	[5] = "External Error from Data or Completion for Fill",
+	[6] = "Evict Completion Error. Registers do not hold the correct address/way",
+	[7] = "Reserved"
+};
+
+static char *c_l2_erroptype[] = {
+	[0] = "Load",
+	[1] = "Probe",
+	[2] = "Store",
+	[3] = "Load Fill",
+	[4] = "Store Fill",
+	[5] = "Evict Completion",
+	[6] = "Reserved"
+};
+
+static int Check_SCU_error(void)
+{
+	uint32_t scu_log0 = read_32bit_nlm_ctrl_reg(CPU_BLOCKID_SCU, SCU_CERRLOG0_REGID);
+	uint32_t scu_log1 = read_32bit_nlm_ctrl_reg(CPU_BLOCKID_SCU, SCU_CERRLOG1_REGID);
+	uint32_t scu_log2 = read_32bit_nlm_ctrl_reg(CPU_BLOCKID_SCU, SCU_CERRLOG2_REGID);
+
+	uint64_t phyaddr = ((uint64_t)scu_log2 << 32) | scu_log1;
+	int errtype  = ( scu_log0 >> 8 ) & 0xF;
+	int erroptyp = ( scu_log0 >> 4 ) & 7;
+
+	if( ( scu_log0 & 1) == 0 && phyaddr == 0 )	return 0;
+
+	cerr_printk("SCU Cache %s:\n", (scu_log0&1) ? "Error" : "Warning" );
+	cerr_printk("   log0=0x08x, phyaddr=0x016llx\n", scu_log0, phyaddr);
+	if( ( scu_log0 & 1 ) == 0 )
+	{
+		write_32bit_nlm_ctrl_reg(CPU_BLOCKID_SCU, SCU_CERRLOG1_REGID, 0);
+		write_32bit_nlm_ctrl_reg(CPU_BLOCKID_SCU, SCU_CERRLOG2_REGID, 0);
+		return 0;
+	}
+	else
+	{
+		cerr_printk("          Way Info: %s\n", (scu_log0>>12)&0xF );
+		cerr_printk("          ErrType: %s\n", errtype<7 ? c_l2_errtype[errtype] : c_l2_errtype[7] );
+		cerr_printk("        ErrOpType: %s\n", erroptyp<6 ? c_l2_erroptype[erroptyp] : c_l2_erroptype[6] );
+		cerr_printk("    Uncorrectable: %d\n", (scu_log0>>2)&1);
+		cerr_printk("         Overflow: %d\n", (scu_log0>>1)&1);
+		cerr_printk("      Error Valid: %d\n", scu_log0&1);
+		return 1;
+	}
+}
+
+static char *c_l3_inf[] = {
+	[0] = "TAG",
+	[1] = "STATE",
+	[2] = "DATA",
+	[3] = "Not defined"
+};
+
+static char *c_l3_ErTy[] = {
+	[0] = "Single‐bit ECC",
+	[1] = "Multi‐bit ECC",
+	[2] = "Not defined"
+};
+
+static char *c_l3_ErOpTy[] = {
+	[0] = "Read",
+	[1] = "Write",
+	[2] = "Msg",
+	[3] = "Reserved"
+};
+
+static int Check_L3_error(void)
+{
+	int node = hard_smp_processor_id() / NLM_MAX_CPU_PER_NODE;
+	uint64_t mmio = nlm_hal_get_dev_base(node, 0, NLH_BRIDGE, 0);
+
+	uint32_t reg0 = nlm_hal_read_32bit_reg(mmio, 0xD9);
+	uint32_t reg1 = nlm_hal_read_32bit_reg(mmio, 0xDA);
+	uint32_t reg2 = nlm_hal_read_32bit_reg(mmio, 0xDB);
+
+	uint64_t phyaddr = ( (uint64_t)reg2 << 32 ) | reg1;
+	uint8_t  erinf = (reg0 >> 11) & 0x1F;
+	uint8_t  erty  = (reg0 >> 8 ) & 7;
+	uint8_t  opty  = (reg0 >> 4 ) & 7;
+
+	if( ( reg0 & 1) == 0 && phyaddr == 0 ) return 0;
+
+	cerr_printk("L3 Cache %s:\n", (reg0&1) ? "Error" : "Warning" );
+	cerr_printk("   log0=0x08x, phyaddr=0x016llx\n", reg0, phyaddr);
+	if( ( reg0 & 1) == 0)
+	{
+		nlm_hal_write_32bit_reg(mmio, 0xDA, 0);
+		nlm_hal_write_32bit_reg(mmio, 0xDB, 0);
+		return 0;
+	}
+	else
+	{
+		cerr_printk("      Error Info: %s\n", erinf < 3 ? c_l3_inf[erinf]   : c_l3_inf[3]   );
+		cerr_printk("      Error Type: %s\n", erty  < 2 ? c_l3_ErTy[erty]   : c_l3_ErTy[2]  );
+		cerr_printk("     Option Type: %s\n", opty  < 4 ? c_l3_ErOpTy[opty] : c_l3_ErOpTy[3]);
+		cerr_printk("   Uncorrectable: %d\n", ( reg0 >> 2 ) & 1);
+		cerr_printk("        Overflow: %d\n", ( reg0 >> 1 ) & 1);
+		cerr_printk("           Valid: %d\n", reg0 & 1 );
+		return 1;
+	}
+}
+
+static int Check_DRAM_error(void)
+{
+	int n, error=0;
+	uint32_t log1, log2;
+	int node = hard_smp_processor_id() / NLM_MAX_CPU_PER_NODE;
+	uint64_t mmio = nlm_hal_get_dev_base(node, 0, NLH_BRIDGE, 0);
+
+	int num_controllers = 4;	//8xx has 4 controller 
+	int iRet=is_nlm_xlp(300, XLP_REVISION_ANY, CPU_EXTPID_XLP_3XX_ANY);
+	if( iRet ) num_controllers = 2;
+	else
+	{
+		iRet=is_nlm_xlp(200, XLP_REVISION_ANY, 0 );
+		if( iRet ) num_controllers = 1;
+	}
+
+	for(n=0; n<num_controllers; n++)
+	{
+		log1 = nlm_hal_read_32bit_reg(mmio, n*0x80+0x11D);
+		log2 = nlm_hal_read_32bit_reg(mmio, n*0x80+0x11E);
+		if( ( log1 & 1 ) == 0 ) continue;
+
+		if(error==0)	cerr_printk("DRAM exception: \n");
+		cerr_printk("  Bank (%c): log1=0x08x, log1=0x08x\n", (char)(n+'A'),log1, log2);
+		cerr_printk("       Syndrome: %x\n",  (log1>>16) );
+		cerr_printk("     Error Type: %x\n",  (log1>>8) & 0xF );
+		cerr_printk("       Opt Type: %x\n",  (log1>>4) & 0xF );
+		cerr_printk("  Uncorrectable: %d\n",  (log1>>2) & 1 );
+		cerr_printk("       Overflow: %d\n",  (log1>>1) & 1 );
+		cerr_printk("    Error Valid: %d\n",  log1&1 );
+
+		cerr_printk("    Error Rank: %x\n", (log2>>12)&3 );
+		cerr_printk("       ChunkID: %d 16-byte chunk\n", (log2>>8)&3 );
+		cerr_printk(" Error Bit pos: %d\n",  log1&0x3F );
+
+		error++;
+	}
+
+	return 0 < error ? 1 : 0;
+}
+
+static char *c_nbu_reqsrc[] = {
 	[0] = "Core 0",
 	[1] = "Core 1",
 	[2] = "Core 2",
@@ -159,7 +461,7 @@ static char *nbu_reqsrc[] = {
 	[15] = "Invalid",
 };
 
-static char *nbu_reqtype[] = {
+static char *c_nbu_reqtype[] = {
 	[0] = "Invalidate",
 	[1] = "Read",
 	[2] = "Read Exclusive",
@@ -170,47 +472,63 @@ static char *nbu_reqtype[] = {
 	[7] = "IO Read Exclusive",
 };
 
-
-static void print_cerr_info(void)
+static int Check_NBU_error(void)
 {
 	int node = CPU_TO_NODE(nlm_hal_cpu_id());
 	uint64_t nbu_mmio = nlm_hal_get_dev_base(node, 0, NLH_BRIDGE, 0);
 
-	uint32_t nbu_reg0 = nlm_hal_read_32bit_reg(nbu_mmio, 162);
-	uint32_t nbu_reg1 = nlm_hal_read_32bit_reg(nbu_mmio, 163);
-	uint32_t nbu_reg2 = nlm_hal_read_32bit_reg(nbu_mmio, 164);
+	uint32_t nbu_reg0 = nlm_hal_read_32bit_reg(nbu_mmio, 0xA2);
+	uint32_t nbu_reg1 = nlm_hal_read_32bit_reg(nbu_mmio, 0xA3);
+	uint32_t nbu_reg2 = nlm_hal_read_32bit_reg(nbu_mmio, 0xA4);
 
-	uint8_t src = (nbu_reg0 >> 7) & 0xF;
+	uint8_t src  = (nbu_reg0 >> 7) & 0xF;
 	uint8_t type = (nbu_reg0 >> 4) & 0x7;
 	uint8_t overflow = (nbu_reg0 >> 3) & 0x1;
 	uint8_t valid = (nbu_reg0 >> 2) & 0x1;
+	uint64_t phyaddr = ( (uint64_t)nbu_reg2 << 32) | nbu_reg1;
 
-	uint64_t icu_log0 = read_64bit_nlm_ctrl_reg(CPU_BLOCKID_ICU, ICU_CERRLOG0_REGID);
-	uint64_t icu_log1 = read_64bit_nlm_ctrl_reg(CPU_BLOCKID_ICU, ICU_CERRLOG1_REGID);
-	uint64_t icu_log2 = read_64bit_nlm_ctrl_reg(CPU_BLOCKID_ICU, ICU_CERRLOG2_REGID);
-	uint64_t lsu_log0 = read_64bit_nlm_ctrl_reg(CPU_BLOCKID_LSU, LSU_CERRLOG0_REGID);
-	uint64_t lsu_log1 = read_64bit_nlm_ctrl_reg(CPU_BLOCKID_LSU, LSU_CERRLOG1_REGID);
-	uint64_t scu_log0 = read_64bit_nlm_ctrl_reg(CPU_BLOCKID_SCU, SCU_CERRLOG0_REGID);
-	uint64_t scu_log1 = read_64bit_nlm_ctrl_reg(CPU_BLOCKID_SCU, SCU_CERRLOG1_REGID);
-	uint64_t scu_log2 = read_64bit_nlm_ctrl_reg(CPU_BLOCKID_SCU, SCU_CERRLOG2_REGID);
-
-	cerr_printk("CPU: (XLP specific) ICU log0 = 0x%016llx, log1 = 0x%016llx, log2 = 0x%016llx\n"
-		    "                    LSU log0 = 0x%016llx, log1 = 0x%016llx\n"
-		    "                    SCU log0 = 0x%016llx, log1 = 0x%016llx, log2 = 0x%016llx\n",
-		    icu_log0, icu_log1, icu_log2,
-		    lsu_log0, lsu_log1,
-		    scu_log0, scu_log1, scu_log2);
-
-	cerr_printk("NBU: BAR Address Error reg0 = 0x%08x, reg1 = 0x%08x, reg2 = 0x%08x\n"
-			"                    %s Error%s, %s request from %s on Node %d at 0x%02x%08x.\n",
-			nbu_reg0, nbu_reg1, nbu_reg2,
-			valid? "Valid":"Invalid",
-			overflow? " (Overflow)":"",
-			nbu_reqtype[type], nbu_reqsrc[src],
-			node,
-			nbu_reg2, nbu_reg1);
+	if( valid == 0 && phyaddr == 0)	return 0;
+
+	cerr_printk("NBU Cache %s:\n", valid ? "Error" : "Warning" );
+	cerr_printk("   log0=0x08x, phyaddr=0x016llx\n", nbu_reg0, phyaddr);
+
+	if( valid == 0 )
+	{
+		nlm_hal_write_32bit_reg(nbu_mmio, 0xA3, 0);
+		nlm_hal_write_32bit_reg(nbu_mmio, 0xA4, 0);
+		return 0;
+	}
+	else
+	{
+		cerr_printk("          ReqSrc: %s\n", c_nbu_reqsrc[src] );
+		cerr_printk("         ReqType: %s\n", c_nbu_reqtype[type] );
+		cerr_printk("        Overflow: %d\n", overflow);
+		cerr_printk("  	  Error Valid: %d\n", valid);
+		return 1;
+	}
 }
 
+static int  print_cerr_info(void)
+{
+	int iRet = 0 ;
+
+	print_error_header();
+	dump_cerr_info();
+
+	iRet += Check_ICU_error();
+
+	iRet += Check_LSU_error();
+
+	iRet += Check_SCU_error();
+
+	iRet += Check_L3_error();
+
+	iRet += Check_NBU_error();
+
+	iRet += Check_DRAM_error();
+
+	return iRet;
+}
 #endif
 
 /* On XLR/XLP, errors reported by bridge (like misconfigured BARS etc) are also
@@ -220,23 +538,299 @@ static void print_cerr_info(void)
  */
 asmlinkage void nlm_cache_error(void)
 {
-	local_irq_disable();
+	int iRet = 0 ;
 
-	/* let the first cpu in */
-	while (nlm_cerr_lock) ;
-	nlm_cerr_lock = 1;
+	iRet = print_cerr_info();
 
-	cerr_printk("*********************************************\n");
-	cerr_printk("cpu_%d received a bus/cache error\n", hard_smp_processor_id());
-	cerr_printk("*********************************************\n");
+	if( iRet == 0 )	iRet=update_registers();
 
-	print_cerr_info();
+	if( iRet == 0 )	cerr_printk("\n ------- CPU return to normal execution! ------------\n\n");
+	else
+	{
+		cerr_printk("Can not handle bus/cache error - Halting cpu\n\n");
+		cerr_cpu_halt();
+	}
+}
 
-	cerr_printk("CPU: epc = 0x%lx, errorepc = 0x%lx, cacheerr = 0x%08x\n",
-			read_c0_epc(), read_c0_errorepc(), read_c0_cacheerr());
+static int update_registers(void)
+{
+	unsigned long errepc;
+	struct pt_regs regs;
+	int iRet=0;
 
-	cerr_printk("Can not handle bus/cache error - Halting cpu\n");
+	LoadRegisters(&regs, &errepc);
 
-	cerr_cpu_halt();
+	iRet=compute_return_registers(&regs, &errepc);
+	if(iRet!=0)
+	{
+		cerr_printk("  Update registers fail! CPU could not return to normal execution!\n");
+		return -1;
+	}
+
+	RestoreRegisters(&regs, errepc);
+	return 0;
+}
+
+static void LoadRegisters(struct pt_regs *regs, unsigned long *pErrepc)
+{
+	int iRegPos, n;
+	uint64_t *pReg;
+	unsigned long k0, k1;
+
+	if(regs==NULL || pErrepc==NULL)	return;
+	memset(regs, 0, sizeof(*regs));
+
+	//copy original registers value from stack
+	iRegPos=0x2000-40*8;
+	pReg = (uint64_t*)( nlm_cerr_stack + iRegPos );
+	for( n=0; n<32; n++) regs->regs[n] = (unsigned long)pReg[n] ;
+
+	//copy original register: sp, ra, errepc
+	pReg=(uint64_t*)(nlm_cerr_stack + 0x2000);	//top of the stack
+	regs->regs[29] = *(pReg - 1);	//sp
+	regs->regs[31] = *(pReg - 2);	//ra
+	*pErrepc = *( pReg -3 );		//ErrEpc
+
+	//copy k0, k1 from scratch register
+	#ifdef CONFIG_64BIT
+	__asm__ volatile(
+		".set push         \n"
+		".set mips64r2     \n"
+		"dmfc0 %0, $22, 3  \n"
+		"dmfc0 %1, $22, 4  \n"
+		".set pop          \n"
+		:"=r" (k0), "=r" (k1)
+	);
+	#else	//32 bits
+	__asm__ volatile(
+		".set push         \n"
+		".set mips64r2     \n"
+		"mfc0 %0, $22, 3   \n"
+		"mfc0 %1, $22, 4   \n"
+		".set pop          \n"
+		:"=r" (k0), "=r" (k1)
+	);
+	#endif //CONFIG_64BIT
+
+	regs->regs[26] = k0 ;
+	regs->regs[27] = k1 ;
+}
+
+static void RestoreRegisters(struct pt_regs *regs, unsigned long errepc)
+{
+	int iRegPos, n;
+	uint64_t *pReg;
+	unsigned long  k0, k1;
+
+	if(regs == NULL )	return;
+
+	//copy original registers value from stack
+	iRegPos=0x2000-40*8;
+	pReg=(uint64_t*)(nlm_cerr_stack + iRegPos );
+	for(n=0; n<32; n++) pReg[n] = regs->regs[n];
+
+	//copy original register: sp, ra, errepc
+	pReg=(uint64_t*)(nlm_cerr_stack + 0x2000);	//top of the stack
+	*(pReg - 1) = regs->regs[29] ;  //sp
+	*(pReg - 2) = regs->regs[31] ;  //ra
+	*(pReg - 3) = errepc;           //ErrEpc
+
+	//copy k0, k1 from scratch register
+	k0 = regs->regs[26] ;
+	k1 = regs->regs[27] ;
+
+	#ifdef CONFIG_64BIT
+	__asm__ volatile(
+		".set push         \n"
+		".set noreorder    \n"
+		".set mips64r2     \n"
+		"dmtc0 %0, $22, 3  \n"
+		"dmtc0 %1, $22, 4  \n"
+		".set pop          \n"
+		::"r" (k0), "r" (k1)
+	);
+	#else  //32bits
+	__asm__ volatile(
+		".set push         \n"
+		".set noreorder    \n"
+		".set mips64r2     \n"
+		"mtc0 %0, $22, 3   \n"
+		"mtc0 %1, $22, 4   \n"
+		".set pop          \n"
+		::"r" (k0), "r" (k1)
+	);
+	#endif //CONFIG_64BIT
 }
 
+static int compute_return_registers(struct pt_regs *regs, unsigned long *pErrepc)
+{
+	unsigned int  *addr;
+	unsigned int bit, fcr31, dspcontrol;
+	unsigned long errepc;
+	union mips_instruction insn;
+
+	if(pErrepc==NULL)	return -1;
+
+	/*	it has slight chance that prev_errepc may not in the tlb and tlb handling of prev_errepc may trigger
+		some issues, or prev_errepc may point to non insn. Since we do not have enough informaiton, have to 
+		access as normal.
+ 	*/
+	errepc = *pErrepc;
+	addr = (uint32_t*) errepc;
+	insn.word = *addr;
+
+	regs->regs[0] = 0;
+	switch (insn.i_format.opcode) {
+	// jr and jalr are in r_format format.
+	case spec_op:
+		switch (insn.r_format.func) {
+		case jalr_op:
+			regs->regs[insn.r_format.rd] = errepc + 8;
+			// Fall through
+		case jr_op:
+			errepc = regs->regs[insn.r_format.rs];
+			break;
+		}
+		break;
+
+	/*
+	 * This group contains:
+	 * bltz_op, bgez_op, bltzl_op, bgezl_op,
+	 * bltzal_op, bgezal_op, bltzall_op, bgezall_op.
+	 */
+	case bcond_op:
+		switch (insn.i_format.rt) {
+		case bltz_op:
+		case bltzl_op:
+			if ((long)regs->regs[insn.i_format.rs] < 0)
+				errepc = errepc + 4 + (insn.i_format.simmediate << 2);
+			else
+				errepc += 8;
+			break;
+
+		case bgez_op:
+		case bgezl_op:
+			if ((long)regs->regs[insn.i_format.rs] >= 0)
+				errepc = errepc + 4 + (insn.i_format.simmediate << 2);
+			else
+				errepc += 8;
+			break;
+
+		case bltzal_op:
+		case bltzall_op:
+			regs->regs[31] = errepc + 8;
+			if ((long)regs->regs[insn.i_format.rs] < 0)
+				errepc = errepc + 4 + (insn.i_format.simmediate << 2);
+			else
+				errepc += 8;
+			break;
+
+		case bgezal_op:
+		case bgezall_op:
+			regs->regs[31] = errepc + 8;
+			if ((long)regs->regs[insn.i_format.rs] >= 0)
+				errepc = errepc + 4 + (insn.i_format.simmediate << 2);
+			else
+				errepc += 8;
+			break;
+		case bposge32_op:
+			if (!cpu_has_dsp)  return -10;
+
+			dspcontrol = rddsp(0x01);
+
+			if (dspcontrol >= 32) {
+				errepc = errepc + 4 + (insn.i_format.simmediate << 2);
+			} else
+				errepc += 8;
+			break;
+		}
+		break;
+
+	/*
+	 * These are unconditional and in j_format.
+	 */
+	case jal_op:
+		regs->regs[31] = errepc + 8;
+	case j_op:
+		errepc += 4;
+		errepc >>= 28;
+		errepc <<= 28;
+		errepc |= (insn.j_format.target << 2);
+		break;
+
+	/*
+	 * These are conditional and in i_format.
+	 */
+	case beq_op:
+	case beql_op:
+		if (regs->regs[insn.i_format.rs] ==
+		    regs->regs[insn.i_format.rt])
+			errepc = errepc + 4 + (insn.i_format.simmediate << 2);
+		else
+			errepc += 8;
+		break;
+
+	case bne_op:
+	case bnel_op:
+		if (regs->regs[insn.i_format.rs] !=
+		    regs->regs[insn.i_format.rt])
+			errepc = errepc + 4 + (insn.i_format.simmediate << 2);
+		else
+			errepc += 8;
+		break;
+
+	case blez_op: /* not really i_format */
+	case blezl_op:
+		/* rt field assumed to be zero */
+		if ((long)regs->regs[insn.i_format.rs] <= 0)
+			errepc = errepc + 4 + (insn.i_format.simmediate << 2);
+		else
+			errepc += 8;
+		break;
+
+	case bgtz_op:
+	case bgtzl_op:
+		/* rt field assumed to be zero */
+		if ((long)regs->regs[insn.i_format.rs] > 0)
+			errepc = errepc + 4 + (insn.i_format.simmediate << 2);
+		else
+			errepc += 8;
+		break;
+
+	/*
+	 * And now the FPA/cp1 branch instructions.
+	 */
+	case cop1_op:
+		if (is_fpu_owner())
+			asm volatile("cfc1\t%0,$31" : "=r" (fcr31));
+		else
+			fcr31 = current->thread.fpu.fcr31;
+
+		bit = (insn.i_format.rt >> 2);
+		bit += (bit != 0);
+		bit += 23;
+		switch (insn.i_format.rt & 3) {
+		case 0:	/* bc1f */
+		case 2:	/* bc1fl */
+			if (~fcr31 & (1 << bit))
+				errepc = errepc + 4 + (insn.i_format.simmediate << 2);
+			else
+				errepc += 8;
+			break;
+
+		case 1:	/* bc1t */
+		case 3:	/* bc1tl */
+			if (fcr31 & (1 << bit))
+				errepc = errepc + 4 + (insn.i_format.simmediate << 2);
+			else
+				errepc += 8;
+			break;
+		}
+		break;
+	default:
+		errepc += 4;
+	}
+
+	*pErrepc = errepc;
+	return 0;
+}
diff --git a/arch/mips/mm/cex-gen.S b/arch/mips/mm/cex-gen.S
index 1eac431..e196d3c 100644
--- a/arch/mips/mm/cex-gen.S
+++ b/arch/mips/mm/cex-gen.S
@@ -20,50 +20,207 @@ the header of the original work apply to this derived work.
 #include <asm/regdef.h>
 #include <asm/mipsregs.h>
 #include <asm/stackframe.h>
+#include <asm/asm-offsets.h>
+#include <asm/netlogic/mips-exts.h>
 
 /*
  * Game over.  Go to the button.  Press gently.  Swear where allowed by
  * legislation.
  */
-	LEAF(except_vec2_generic)
-	.set	noreorder
-	.set	noat
-	.set    mips0
+LEAF( cache_save_regs )
+	.set push
+	.set mips64
+	.set noat
+	.set noreorder
+
+	LONG_S  $1 , PT_R1 (sp)
+	LONG_S  $2 , PT_R2 (sp)
+	LONG_S  $3 , PT_R3 (sp)
+	LONG_S  $4 , PT_R4 (sp)
+	LONG_S  $5 , PT_R5 (sp)
+	LONG_S  $6 , PT_R6 (sp)
+	LONG_S  $7 , PT_R7 (sp)
+	LONG_S  $8 , PT_R8 (sp)
+	LONG_S  $9 , PT_R9 (sp)
+	LONG_S  $10, PT_R10(sp)
+	LONG_S  $11, PT_R11(sp)
+	LONG_S  $12, PT_R12(sp)
+	LONG_S  $13, PT_R13(sp)
+	LONG_S  $14, PT_R14(sp)
+	LONG_S  $15, PT_R15(sp)
+	LONG_S  $16, PT_R16(sp)
+	LONG_S  $17, PT_R17(sp)
+	LONG_S  $18, PT_R18(sp)
+	LONG_S  $19, PT_R19(sp)
+	LONG_S  $20, PT_R20(sp)
+	LONG_S  $21, PT_R21(sp)
+	LONG_S  $22, PT_R22(sp)
+	LONG_S  $23, PT_R23(sp)
+	LONG_S  $24, PT_R24(sp)
+	LONG_S  $25, PT_R25(sp)
+	LONG_S  $26, PT_R26(sp)
+	LONG_S  $27, PT_R27(sp)
+	LONG_S  $28, PT_R28(sp)
+	LONG_S  $29, PT_R29(sp)
+	LONG_S  $30, PT_R30(sp)
+
+	j   ra
+	nop
+	.set pop
+END(cache_save_regs)
+
+LEAF(cache_restore_regs)
+	.set push
+	.set mips64
+	.set noat
+	.set noreorder
+
+	LONG_L  $1 , PT_R1 (sp)
+	LONG_L  $2 , PT_R2 (sp)
+	LONG_L  $3 , PT_R3 (sp)
+	LONG_L  $4 , PT_R4 (sp)
+	LONG_L  $5 , PT_R5 (sp)
+	LONG_L  $6 , PT_R6 (sp)
+	LONG_L  $7 , PT_R7 (sp)
+	LONG_L  $8 , PT_R8 (sp)
+	LONG_L  $9 , PT_R9 (sp)
+	LONG_L  $10, PT_R10(sp)
+	LONG_L  $11, PT_R11(sp)
+	LONG_L  $12, PT_R12(sp)
+	LONG_L  $13, PT_R13(sp)
+	LONG_L  $14, PT_R14(sp)
+	LONG_L  $15, PT_R15(sp)
+	LONG_L  $16, PT_R16(sp)
+	LONG_L  $17, PT_R17(sp)
+	LONG_L  $18, PT_R18(sp)
+	LONG_L  $19, PT_R19(sp)
+	LONG_L  $20, PT_R20(sp)
+	LONG_L  $21, PT_R21(sp)
+	LONG_L  $22, PT_R22(sp)
+	LONG_L  $23, PT_R23(sp)
+	LONG_L  $24, PT_R24(sp)
+	LONG_L  $25, PT_R25(sp)
+	LONG_L  $26, PT_R26(sp)
+	LONG_L  $27, PT_R27(sp)
+	LONG_L  $28, PT_R28(sp)
+	LONG_L  $29, PT_R29(sp)
+	LONG_L  $30, PT_R30(sp)
+
+	j ra
+	nop
+	.set pop
+END(cache_restore_regs)
+
+LEAF(except_vec2_generic)
+	.set push
+	.set noreorder
+	.set noat
+	.set mips64r2
+
+	ehb
+
+	MTC0      k0, CP0_DIAGNOSTIC, 3
+	MTC0      k1, CP0_DIAGNOSTIC, 4
+
+	/*If some other cpu is already in the handler just wait... */
+	PTR_LA    k0, nlm_cerr_lock
+1:	lw        k1, 0(k0)
+	bnez      k1, 1b
+	nop
+	li        k1, 1
+	sw        k1, 0(k0)
+
+	/*save: sp, ra */
+	PTR_LA    k0, nlm_cerr_stack
+	PTR_ADDU  k0, k0, 0x2000
+	LONG_S    sp, -1*8(k0)
+	LONG_S    ra, -2*8(k0)
+
+	PTR_LA    k1, except_vec2_generic_body
+	jal       k1
+	nop
+
+	/*recover: sp,ra*/
+	PTR_LA    k0, nlm_cerr_stack
+	PTR_ADDU  k0, k0, 0x2000
+	LONG_L    sp, -1*8(k0)
+	LONG_L    ra, -2*8(k0)
+
+	PTR_LA    k0, nlm_cerr_lock
+	li        k1, 0
+	sw        k1, 0(k0)
+
+	MFC0      k0, CP0_DIAGNOSTIC, 3
+	MFC0      k1, CP0_DIAGNOSTIC, 4
+	eret
+	nop
+	.set pop
+END(except_vec2_generic)
+
+LEAF(except_vec2_generic_body)
+	.set push
+	.set noreorder
+	.set noat
+	.set mips64r2
 	/*
 	 * This is a very bad place to be.  Our cache error
 	 * detection has triggered.  If we have write-back data
 	 * in the cache, we may not be able to recover.  As a
 	 * first-order desperate measure, turn off KSEG0 cacheing.
 	 */
-	mfc0	k0,CP0_CONFIG
-	li	k1,~CONF_CM_CMASK
-	and	k0,k0,k1
-	ori	k0,k0,CONF_CM_UNCACHED
-	mtc0	k0,CP0_CONFIG
+
+	mfc0    k0, CP0_CONFIG
+#ifdef CONFIG_NLM_XLP
+	move    sp, k0
+#endif
+	li      k1,~CONF_CM_CMASK
+	and     k0,k0,k1
+	ori     k0,k0,CONF_CM_UNCACHED
+	mtc0    k0,CP0_CONFIG
+#ifdef CONFIG_NLM_XLP	
+	ehb
+#else
 	/* Give it a few cycles to sink in... */
-	nop
-	nop
-	nop
+        nop
+        nop
+        nop
+#endif
+
+
 #ifdef CONFIG_NLM_XLP
+	
+	PTR_LA    k0, nlm_cerr_stack
+	PTR_ADDU  k0, k0, 0x2000
+	MFC0      k1, CP0_ERROREPC
+	LONG_S    k1, -3*8(k0)
+	LONG_S    ra, -4*8(k0)
+	LONG_S    sp, -5*8(k0) /*save CP0_CONFIG*/
+	PTR_ADDU  sp, k0, -40*8
 
-	/* If some other cpu is already in the handler
-	 * just wait... */
-	PTR_LA	k0, nlm_cerr_lock
-1:	lw	k1, 0(k0)
-	bnez	k1, 1b
+	jal   cache_save_regs
 	nop
 
-	/* switch stack to a new one */
-	PTR_LA		sp, nlm_cerr_stack
-	li		k1, 8192 - 64
-	PTR_ADDU	sp, sp, k1
+	jal   nlm_cache_error
+	nop
 
-	jal	nlm_cache_error
+	jal   cache_restore_regs
 	nop
-	/* should never get here */
 
+	PTR_LA    k0, nlm_cerr_stack
+	PTR_ADDU  k0, k0, 0x2000
+	LONG_L    ra, -4*8(k0)
+	LONG_L    k1, -5*8(k0) /*save CP0_CONFIG*/
+	mtc0      k1, CP0_CONFIG
+	LONG_L    k1, -3*8(k0)
+	MTC0      k1, CP0_ERROREPC
+	ehb
+
+	j     ra
+	nop
 #else
-	j	cache_parity_error
+	j       cache_parity_error
 	nop
 #endif
-	END(except_vec2_generic)
+
+	.set pop
+END(except_vec2_generic_body)
-- 
1.7.0.4

