From 552464c40230287b833ac675e48b44f85dbe4f66 Mon Sep 17 00:00:00 2001
From: Sreenidhi BR <sreenidhibr@netlogicmicro.com>
Date: Wed, 15 Jun 2011 13:00:23 +0530
Subject: [PATCH 290/762] Adding Raid6 Support. Fixed issues with Raid5.

Based on Broadcom SDK 2.3.

Signed-off-by: Sreenidhi BR <sreenidhibr@netlogicmicro.com>
Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 drivers/dma/nlm_adma.c |  517 +++++++++++++++++++++++++++++++++++++++++++++---
 drivers/dma/nlm_adma.h |    4 +-
 2 files changed, 493 insertions(+), 28 deletions(-)

diff --git a/drivers/dma/nlm_adma.c b/drivers/dma/nlm_adma.c
index bcf7179..65188b6 100644
--- a/drivers/dma/nlm_adma.c
+++ b/drivers/dma/nlm_adma.c
@@ -92,9 +92,10 @@ uint64_t gen_dtr_raid_msg_format_1 (const uint32_t raid_type,
 	return 0ULL << 63
 		| shift_lower_bits (1, 56, 1) /* Inform Source */
 		| shift_lower_bits (freeback_msg_dest_id, 44, 12)
+		| shift_lower_bits (DTRE_NLM_Q_POLY, 12, 8) /* Q polynomial*/
 		| shift_lower_bits (raid_type, 10, 2)
 		| shift_lower_bits (operation, 9, 1)
-		| shift_lower_bits (disks, 0, 4);
+		| shift_lower_bits (disks, 0, 5);
 }
 
 static __inline__
@@ -130,6 +131,12 @@ static void nlm_dtre_msgring_handler(uint32_t vc, uint32_t src_id,
 				vc, src_id, size, msg0, msg1, desc->optype);
 	}
 
+#if 0
+	if (desc->optype == DMA_MEMCPY)
+		printk("DTRE recv msg: vc %d sender 0x%x, size 0x%x, data0 0x%llx data1 0x%llx optype %d.\n",
+				vc, src_id, size, msg0, msg1, desc->optype);
+#endif
+
 	if ((desc->xor_check_result) != NULL)
 		*(desc->xor_check_result) = 0;
 
@@ -209,40 +216,68 @@ static dma_cookie_t nlm_tx_submit (struct dma_async_tx_descriptor *tx)
 			(1ULL << 63)
 			| shift_lower_bits(0, 60, 3)
 			| shift_lower_bits(8*(nlm_tx->len), 40, 20)
-			| shift_lower_bits (virt_to_phys ((volatile void *) nlm_tx->hw_desc.src), 0, 40);
+			| shift_lower_bits ((volatile void *)nlm_tx->hw_desc.src, 0, 40);
 
 		transfer_msg[1] = 
 			(0ULL << 63)
+			| shift_lower_bits (1, 59, 1) /* write control */
 			| shift_lower_bits (1, 56, 1) /* Inform Source */
 			| shift_lower_bits (freeback_msg_dest_id, 44, 12);
 
 		transfer_msg[2] = 
 			(0ULL << 63)
 			| shift_lower_bits (1, 40, 1) /* perform transfer */
-			| shift_lower_bits (virt_to_phys ((volatile void *) nlm_tx->hw_desc.dst), 0, 40);
+			| shift_lower_bits ((volatile void *)nlm_tx->hw_desc.dst, 0, 40);
 
-		transfer_msg[3] = gen_dtr_raid_msg_format_2(nlm_tx);
+		transfer_msg[3] = gen_dtr_raid_msg_format_2((void *)((unsigned long)nlm_tx>>1));
 
 		vc_id = DTRE_MIN_VC + 1;
 		msgtype = p2d;
 
-		rc = xlp_message_send(vc_id, msgtype, 0, transfer_msg);
+		rc = 0;
+		i = 0;
+		while(1)
+		{
+			rc = xlp_message_send(vc_id, msgtype, 0, transfer_msg);
+			if (rc == 0)
+				break;
+
+			/* pop out messages from vc, if any */
+			pop_vc = freeback_msg_dest_id;
+
+			if ((nlm_hal_recv_msg2(pop_vc, &pop_src, &pop_size, &pop_code, &pop_data[0], &pop_data[1])) == 0)
+			{
+				if (nlm_dtre_debug)
+					printk("POP msg found in vc.\n");
+				nlm_dtre_msgring_handler(pop_vc, pop_src, pop_size, pop_code, pop_data[0], pop_data[1], pop_data[2], pop_data[3], NULL);
+			}
+
+			if ((i%100) == 0) {
+				msgstatus1 = xlp_read_status1();
+				printk("DTRE:continuing retry, rc:0x%08x, msgstatus1: 0x%08x.\n", rc, msgstatus1);
+			}
+		}
+
 		if (rc != 0) {
-			printk("Error:unable to send DTRE Transf msg %d\n",rc);
+			msgstatus1 = xlp_read_status1();
+			printk("Error: unable to send DTRE Xfer msg: rc:%d, msgstatus1: 0x%08x.\n", rc, msgstatus1);
+			spin_unlock_bh(&chan->lock);
+			return -ENODEV;
 		}
-		else
-		{
-#if 0
-			printk("*sent msg CPY 0x%llx 0x%llx 0x%llx 0x%llx to %d optype %d cookie %d.\n",
-					transfer_msg[0],
-					transfer_msg[1],
-					transfer_msg[2],
-					transfer_msg[3],
-					vc_id, optype, cookie);
-#endif
+		else {
+			if (nlm_dtre_debug) {
+				printk("sent msg CPY 0x%llx 0x%llx 0x%llx 0x%llx to %d optype %d cookie %d.\n",
+						transfer_msg[0],
+						transfer_msg[1],
+						transfer_msg[2],
+						transfer_msg[3],
+						vc_id, optype, cookie);
+			}
 		}
 
-		return cookie;
+		/* do the post msg sending operations */
+		goto post_msg_sent;
+
 	}
 
 	if (optype == DMA_XOR)
@@ -283,8 +318,11 @@ static dma_cookie_t nlm_tx_submit (struct dma_async_tx_descriptor *tx)
 	raid_list_msg [1] = gen_dtr_raid_msg_format_1(raid_type, operation, disks, freeback_msg_dest_id);
 	raid_list_msg [2] = gen_dtr_raid_msg_format_2((void *)((unsigned long)nlm_tx>>1));
 
-	for (i=0; i<16; i++)
+	rc = 0;
+	i = 0;
+	while(1)
 	{
+		i++;
 		rc = xlp_message_send(vc_id, msgtype, 0, raid_list_msg);
 		if (rc == 0)
 			break;
@@ -298,11 +336,19 @@ static dma_cookie_t nlm_tx_submit (struct dma_async_tx_descriptor *tx)
 				printk("POP msg found in vc.\n");
 			nlm_dtre_msgring_handler(pop_vc, pop_src, pop_size, pop_code, pop_data[0], pop_data[1], pop_data[2], pop_data[3], NULL);
 		}
+
+		if ((i%100) == 0)
+		{
+			msgstatus1 = xlp_read_status1();
+			// printk("DTRE:continuing retry, rc:0x%08x, msgstatus1: 0x%08x.\n", rc, msgstatus1);
+		}
 	}
 
 	if (rc != 0) {
 		msgstatus1 = xlp_read_status1();
 		printk("Error: unable to send DTRE RAID msg: rc:%d, msgstatus1: 0x%08x.\n", rc, msgstatus1);
+
+		spin_unlock_bh(&chan->lock);
 		return -ENODEV;
 	}
 	else
@@ -316,6 +362,7 @@ static dma_cookie_t nlm_tx_submit (struct dma_async_tx_descriptor *tx)
 		}
 	}
 
+post_msg_sent:
 	if ((chan->tx_queue[chan->pending_idx]) != NULL)
 	{
 		/* TODO: error action TBD */
@@ -441,10 +488,14 @@ static void __process_completed_tx(struct nlm_adma_chan * chan)
 		{
 			if (nlm_dtre_debug)
 				printk("pro_idx: %d.\n", chan->process_idx);
+
 			ptr->done_flag = 0;
 			wmb();
 			chan->tx_queue[chan->process_idx] = NULL;
 			chan->process_idx = (chan->process_idx + 1) & DTRE_MAX_TX_Q_MASK;
+			if (ptr->pg)
+				put_page(ptr->pg);
+
 			free_tx_desc(ptr);
 		}
 
@@ -461,7 +512,7 @@ static void __process_completed_tx(struct nlm_adma_chan * chan)
 		}
 
 		/* exit clause */
-		if ((chan->process_idx<chan->pending_idx) &&
+		if ((chan->process_idx != chan->pending_idx) &&
 				(ptr != NULL) &&
 				(ptr->done_flag == 0))
 		{
@@ -549,6 +600,49 @@ static void nlm_adma_issue_pending(struct dma_chan *chan)
 }
 
 
+#if 0
+static struct dma_async_tx_descriptor *
+nlm_adma_prep_dma_memcpy(struct dma_chan *chan, dma_addr_t dma_dest,
+		dma_addr_t dma_src, size_t len, unsigned long flags)
+{
+	int i;
+	struct nlm_tx_desc *desc;
+	uint64_t * ent;
+	struct nlm_adma_chan *nlm_chan = to_nlm_adma_chan(chan);
+
+	if (nlm_dtre_debug) {
+		printk("prep_dma_memcpy len 0x%lx.\n", len);
+	}
+
+	desc = alloc_tx_desc(nlm_chan);
+	if (!desc)
+	{
+		printk("DTRE Error: cannot allocate desc\n");
+		return (NULL);
+	}
+
+	desc->optype = DMA_MEMCPY;
+	desc->len = (unsigned int)len;
+	desc->int_en = flags & DMA_PREP_INTERRUPT;
+	desc->async_tx.flags = flags;
+	desc->async_tx.cookie = -EBUSY;
+	desc->done_flag = 0;
+	desc->xor_check_result = 0;
+	desc->entries_count = 0;
+	desc->hw_desc.src_cnt = 0;
+	desc->hw_desc.src = dma_src;
+	desc->hw_desc.dst = dma_dest;
+	desc->pg = NULL;
+
+	ent = desc->hw_desc.entries;
+
+	for (i=0; i<DTRE_RAID_MAX_ENTRIES; i++)
+		desc->hw_desc.entries[i] = 0ULL;
+
+	return &desc->async_tx;
+}
+#endif
+
 
 static struct dma_async_tx_descriptor *
 nlm_adma_prep_dma_xor(struct dma_chan *chan, dma_addr_t dma_dest,
@@ -595,6 +689,7 @@ nlm_adma_prep_dma_xor(struct dma_chan *chan, dma_addr_t dma_dest,
 	desc->hw_desc.src_cnt = src_cnt;
 	desc->hw_desc.src = 0;
 	desc->hw_desc.dst = 0;
+	desc->pg = NULL;
 
 	for (i=0; i<DTRE_RAID_MAX_ENTRIES; i++)
 		desc->hw_desc.entries[i] = 0ULL;
@@ -651,6 +746,10 @@ nlm_adma_prep_dma_xor(struct dma_chan *chan, dma_addr_t dma_dest,
 	}
 
 	// printk("* check 2, entries %d.\n", desc->entries_count);
+	if (nlm_dtre_debug) {
+		for (i=0; i<desc->entries_count; i++)
+			printk(" entry %d, value: 0x%llx.\n", i, ent[i]);
+	}
 
 	return &desc->async_tx;
 }
@@ -702,6 +801,7 @@ nlm_adma_prep_dma_xor_val(struct dma_chan *chan, dma_addr_t *dma_src,
 	desc->hw_desc.src_cnt = src_cnt;
 	desc->hw_desc.src = 0;
 	desc->hw_desc.dst = 0;
+	desc->pg = NULL;
 
 	for (i=0; i<DTRE_RAID_MAX_ENTRIES; i++)
 		desc->hw_desc.entries[i] = 0ULL;
@@ -755,6 +855,369 @@ nlm_adma_prep_dma_xor_val(struct dma_chan *chan, dma_addr_t *dma_src,
 	}
 
 	// printk("* check 2, entries %d.\n", desc->entries_count);
+	if (nlm_dtre_debug) {
+		for (i=0; i<desc->entries_count; i++)
+			printk(" entry %d, value: 0x%llx.\n", i, ent[i]);
+	}
+
+	return &desc->async_tx;
+}
+
+
+
+static struct dma_async_tx_descriptor *
+nlm_adma_prep_dma_pq(struct dma_chan *chan, dma_addr_t *dst, dma_addr_t *src,
+		unsigned int src_cnt, const unsigned char *scf, size_t len,
+		unsigned long flags)
+{
+	int i, p_device_id, q_device_id, disks;
+	struct nlm_tx_desc *desc;
+	struct nlm_adma_chan *nlm_chan = to_nlm_adma_chan(chan);
+	uint64_t * ent;
+
+	dma_addr_t *local_src;
+	const unsigned char *local_scf;
+	unsigned int local_src_cnt;
+	void * pgaddr = NULL;
+
+	local_src = src;
+	local_scf = scf;
+
+	if (dmaf_p_disabled_continue(flags))
+		local_src_cnt = src_cnt + 1;
+	else if (dmaf_continue(flags))
+		local_src_cnt = src_cnt + 3;
+	else
+		local_src_cnt = src_cnt;
+
+	if (nlm_dtre_debug) {
+		printk(" fn dma_pq src_cnt %d new_src_cnt %d len 0x%lx flags 0x%lx.\n", 
+				src_cnt, local_src_cnt, len, flags);
+	}
+
+	if (nlm_dtre_debug) {
+		for (i=0; i<src_cnt; i++) 
+			printk("* dma_pq COEFF for %i is 0x%x.\n", i, (uint8_t)local_scf[i]);
+	}
+
+	disks = local_src_cnt + 2;
+	p_device_id = local_src_cnt + 1;
+	q_device_id = local_src_cnt + 2;
+
+	if (disks > DTRE_RAID_MAX_DEVICES)
+	{
+		printk("DTRE Error: device count %d exceeds max %d\n",
+				disks, DTRE_RAID_MAX_DEVICES);
+		return (NULL);
+	}
+
+	desc = alloc_tx_desc(nlm_chan);
+
+	if (!desc)
+	{
+		printk("DTRE Error: cannot allocate desc\n");
+		return (NULL);
+	}
+
+	ent = desc->hw_desc.entries;
+
+	desc->optype = DMA_PQ;
+	desc->len = (unsigned int)len;
+	desc->int_en = flags & DMA_PREP_INTERRUPT;
+	desc->async_tx.flags = flags;
+	desc->async_tx.cookie = -EBUSY;
+	desc->done_flag = 0;
+	desc->xor_check_result = 0;
+	desc->entries_count = 0;
+	desc->hw_desc.src_cnt = local_src_cnt;
+	desc->hw_desc.src = 0;
+	desc->hw_desc.dst = 0;
+	desc->pg = NULL;
+
+	if ((flags & DMA_PREP_PQ_DISABLE_P) || (flags & DMA_PREP_PQ_DISABLE_Q))
+	{
+		desc->pg = alloc_page(GFP_KERNEL);
+		if (desc->pg == NULL) {
+			printk("DTRE error: page allocation failed.\n");
+			return NULL;
+		}
+		pgaddr = (void *)page_to_phys(desc->pg);
+	}
+#if 0
+	pgaddr = (void *)page_to_phys(nlm_dtre_pg);
+#endif
+
+	/* specify valid address for disabled result */
+	if (flags & DMA_PREP_PQ_DISABLE_P)
+		dst[0] = (unsigned long) pgaddr;
+	if (flags & DMA_PREP_PQ_DISABLE_Q)
+		dst[1] = (unsigned long) pgaddr;
+
+	if (nlm_dtre_debug) {
+		for (i=0; i<src_cnt; i++)
+			printk("*src index %d, addr 0x%lx.\n", i, (unsigned long)src[i]);
+		printk("dst0 : 0x%lx.\n", (unsigned long)dst[0]);
+		printk("dst1 : 0x%lx.\n", (unsigned long)dst[1]);
+	}
+
+	for (i=0; i<DTRE_RAID_MAX_ENTRIES; i++)
+		desc->hw_desc.entries[i] = 0ULL;
+
+	/* fill the coefficients entry */
+	for (i=0; i<disks; i++)
+	{
+		/* set the Coefficient bit */
+		ent[i] = (1ULL << 63);
+
+		/* cache write control */
+		ent[i] |= shift_lower_bits (1, 59, 1);
+
+		/* PCoefficient */
+		if (i < src_cnt)
+			ent[i] |= shift_lower_bits (1, 0, 8);
+
+		/* QCoefficient */
+		if (i < src_cnt)
+			ent[i] |= shift_lower_bits((uint8_t)local_scf[i], 8, 8);
+
+		if ((i+1) == p_device_id)
+		{
+			/* fill for dest also , this is the p device id*/
+			ent[i] |= shift_lower_bits (1, 40, 2);
+
+			/* PCoefficient */
+			ent[i] |= shift_lower_bits (1, 0, 8);
+			ent[i] |= shift_lower_bits (0, 8, 8);
+		}
+
+		if ((i+1) == q_device_id)
+		{
+			ent[i] |= shift_lower_bits (2, 40, 2);
+
+			/* QCoefficient */
+			ent[i] |= shift_lower_bits (0, 0, 8);
+			ent[i] |= shift_lower_bits (1, 8, 8);
+		}
+		desc->entries_count++;
+	}
+
+	if (local_src_cnt == (src_cnt + 1)) {
+		ent[src_cnt] |= shift_lower_bits (0, 0, 8); /* P coef */
+		ent[src_cnt] |= shift_lower_bits (1, 8, 8); /* Q coef */
+	}
+
+	if (local_src_cnt == (src_cnt + 3)) {
+		/* P coef */
+		ent[src_cnt]   |= shift_lower_bits (0, 0, 8);
+		ent[src_cnt+1] |= shift_lower_bits (0, 0, 8);
+		ent[src_cnt+2] |= shift_lower_bits (0, 0, 8);
+
+		/* Q coef */
+		ent[src_cnt]   |= shift_lower_bits (0, 8, 8);
+		ent[src_cnt+1] |= shift_lower_bits (1, 8, 8);
+		ent[src_cnt+2] |= shift_lower_bits (0, 8, 8);
+	}
+	// printk("* check 1, entries %d.\n", desc->entries_count);
+
+	/* fill the segments entry */
+	for (i=0; i<disks; i++)
+	{
+		/* segment entry bit */
+		ent[disks + i] = (0ULL << 63);
+
+		/* SOD */
+		ent[disks + i] |= shift_lower_bits (1, 62, 1);
+
+		/* EOD */
+		ent[disks + i] |= shift_lower_bits (1, 61, 1);
+
+		/* segment length */
+		ent[disks + i] |= shift_lower_bits(desc->len, 40, 20);
+
+		/* segment address */
+		if (i < src_cnt)
+			ent[disks + i] |= shift_lower_bits(local_src[i], 0, 40);
+
+		if ((i+1) == p_device_id)
+			ent[disks + i] |= shift_lower_bits(dst[0], 0, 40);
+
+		if ((i+1) == q_device_id)
+			ent[disks + i] |= shift_lower_bits(dst[1], 0, 40);
+
+		desc->entries_count++;
+	}
+
+	if (local_src_cnt == (src_cnt + 1)) {
+		ent[disks + src_cnt] |= shift_lower_bits(dst[1], 0, 40);
+	}
+
+	if (local_src_cnt == (src_cnt + 3)) {
+		ent[disks + src_cnt]   |= shift_lower_bits(dst[0], 0, 40);
+		ent[disks + src_cnt+1] |= shift_lower_bits(dst[1], 0, 40);
+		ent[disks + src_cnt+2] |= shift_lower_bits(dst[1], 0, 40);
+	}
+	// printk("* check 2, entries %d.\n", desc->entries_count);
+
+	if (nlm_dtre_debug) {
+		for (i=0; i<desc->entries_count; i++)
+			printk(" entry %d, value: 0x%llx.\n", i, ent[i]);
+	}
+
+	return &desc->async_tx;
+}
+
+
+static struct dma_async_tx_descriptor *
+nlm_adma_prep_dma_pq_val(struct dma_chan *chan, dma_addr_t *pq, dma_addr_t *src,
+		unsigned int src_cnt, const unsigned char *scf,
+		size_t len, enum sum_check_flags *pqres,
+		unsigned long flags)
+{
+	int i, p_device_id, q_device_id, disks;
+	struct nlm_tx_desc *desc;
+	struct nlm_adma_chan *nlm_chan = to_nlm_adma_chan(chan);
+	uint64_t * ent;
+	void * pgaddr = NULL;
+
+	if (nlm_dtre_debug) {
+		printk("* fn dma_pq_val src_cnt %d len 0x%lx flags 0x%lx.\n", 
+				src_cnt, len, flags);
+	}
+
+	if (nlm_dtre_debug) {
+		for (i=0; i<src_cnt; i++)
+			printk("* dma_pq_val COEFF for %i is 0x%x \n", i, (uint8_t)scf[i]);
+	}
+
+	disks = src_cnt + 2;
+	p_device_id = src_cnt + 1;
+	q_device_id = src_cnt + 2;
+
+	if (disks > DTRE_RAID_MAX_DEVICES)
+	{
+		printk("DTRE Error: device count %d exceeds max %d\n",
+				disks, DTRE_RAID_MAX_DEVICES);
+		return (NULL);
+	}
+
+	desc = alloc_tx_desc(nlm_chan);
+
+	if (!desc)
+	{
+		printk("DTRE Error: cannot allocate desc\n");
+		return (NULL);
+	}
+
+	ent = desc->hw_desc.entries;
+
+	desc->optype = DMA_PQ_VAL;
+	desc->len = (unsigned int)len;
+	desc->int_en = flags & DMA_PREP_INTERRUPT;
+	desc->async_tx.flags = flags;
+	desc->async_tx.cookie = -EBUSY;
+	desc->done_flag = 0;
+	desc->xor_check_result = pqres;
+	desc->entries_count = 0;
+	desc->hw_desc.src_cnt = src_cnt;
+	desc->hw_desc.src = 0;
+	desc->hw_desc.dst = 0;
+	desc->pg = NULL;
+
+	if ((flags & DMA_PREP_PQ_DISABLE_P) || (flags & DMA_PREP_PQ_DISABLE_Q))
+	{
+		desc->pg = alloc_page(GFP_KERNEL);
+		if (desc->pg == NULL) {
+			printk("DTRE error: page allocation failed.\n");
+			return NULL;
+		}
+		pgaddr = (void *) page_to_phys(desc->pg);
+	}
+#if 0
+	pgaddr = (void *) page_to_phys(nlm_dtre_pg);
+#endif
+
+	for (i=0; i<DTRE_RAID_MAX_ENTRIES; i++)
+		desc->hw_desc.entries[i] = 0ULL;
+
+	/* specify valid address for disabled result */
+	if (flags & DMA_PREP_PQ_DISABLE_P)
+		pq[0] = (unsigned long) pgaddr;
+	if (flags & DMA_PREP_PQ_DISABLE_Q)
+		pq[1] = (unsigned long) pgaddr;
+
+	/* fill the coefficients entry */
+	for (i=0; i<disks; i++)
+	{
+		/* set the Coefficient bit */
+		ent[i] = (1ULL << 63);
+
+		/* cache write control */
+		ent[i] |= shift_lower_bits (1, 59, 1);
+
+		/* PCoefficient */
+		if (i < src_cnt)
+			ent[i] |= shift_lower_bits (1, 0, 8);
+
+		/* QCoefficient */
+		if (i < src_cnt)
+			ent[i] |= shift_lower_bits ((uint8_t)(scf[i]), 8, 8);
+
+		if ((i+1) == p_device_id)
+		{
+			/* fill for dest also , this is the p device id*/
+			ent[i] |= shift_lower_bits (1, 40, 2);
+
+			/* PCoefficient */
+			ent[i] |= shift_lower_bits (1, 0, 8);
+			ent[i] |= shift_lower_bits (0, 8, 8);
+
+		}
+
+		if ((i+1) == q_device_id)
+		{
+			ent[i] |= shift_lower_bits (2, 40, 2);
+
+			/* QCoefficient */
+			ent[i] |= shift_lower_bits (0, 0, 8);
+			ent[i] |= shift_lower_bits (1, 8, 8);
+		}
+		desc->entries_count++;
+	}
+	// printk("* pq_val check 1, entries %d.\n", desc->entries_count);
+
+	/* fill the segments entry */
+	for (i=0; i<disks; i++)
+	{
+		/* segment entry bit */
+		ent[disks + i] = (0ULL << 63);
+
+		/* SOD */
+		ent[disks + i] |= shift_lower_bits (1, 62, 1);
+
+		/* EOD */
+		ent[disks + i] |= shift_lower_bits (1, 61, 1);
+
+		/* segment length */
+		ent[disks + i] |= shift_lower_bits(desc->len, 40, 20);
+
+		/* segment address */
+		if (i < src_cnt)
+			ent[disks + i] |= shift_lower_bits(src[i], 0, 40);
+
+		if ((i+1) == p_device_id)
+			ent[disks + i] |= shift_lower_bits(pq[0], 0, 40);
+
+		if ((i+1) == q_device_id)
+			ent[disks + i] |= shift_lower_bits(pq[1], 0, 40);
+
+		desc->entries_count++;
+	}
+	// printk("* check 2, entries %d.\n", desc->entries_count);
+
+	if (nlm_dtre_debug) {
+		for (i=0; i<desc->entries_count; i++)
+			printk(" entry %d, value: 0x%llx.\n", i, ent[i]);
+	}
 
 	return &desc->async_tx;
 }
@@ -862,29 +1325,29 @@ static int __devinit nlm_adma_probe(struct platform_device *pdev)
 
 	dma_cap_set(DMA_XOR, dma_dev->cap_mask);
 	dma_cap_set(DMA_XOR_VAL, dma_dev->cap_mask);
-	// dma_cap_set(DMA_PQ, dma_dev->cap_mask);
-	// dma_cap_set(DMA_PQ_VAL, dma_dev->cap_mask);
+	dma_cap_set(DMA_PQ, dma_dev->cap_mask);
+	dma_cap_set(DMA_PQ_VAL, dma_dev->cap_mask);
 
-	// dma_cap_set(DMA_INTERRUPT, dma_dev->cap_mask);
 	// dma_cap_set(DMA_MEMCPY, dma_dev->cap_mask);
 	// dma_cap_set(DMA_MEMSET, dma_dev->cap_mask);
+	// dma_cap_set(DMA_INTERRUPT, dma_dev->cap_mask);
 
 	dma_dev->device_alloc_chan_resources = nlm_adma_alloc_chan_resources;
 	dma_dev->device_free_chan_resources = nlm_adma_free_chan_resources;
 	dma_dev->device_is_tx_complete = nlm_adma_is_complete;
 	dma_dev->device_issue_pending = nlm_adma_issue_pending;
 
-	// dma_dev->device_prep_dma_interrupt = nlm_adma_prep_dma_interrupt;
 	// dma_dev->device_prep_dma_memcpy = nlm_adma_prep_dma_memcpy;
 	// dma_dev->device_prep_dma_memset = nlm_adma_prep_dma_memset;
+	// dma_dev->device_prep_dma_interrupt = nlm_adma_prep_dma_interrupt;
 
 	dma_dev->max_xor = nlm_adma_get_max_xor();
 	dma_dev->device_prep_dma_xor = nlm_adma_prep_dma_xor;
 	dma_dev->device_prep_dma_xor_val = nlm_adma_prep_dma_xor_val;
 
-	// dma_set_maxpq(dma_dev, nlm_adma_get_max_pq(), 0);
-	// dma_dev->device_prep_dma_pq = nlm_adma_prep_dma_pq;
-	// dma_dev->device_prep_dma_pq_val = nlm_adma_prep_dma_pq_val;
+	dma_set_maxpq(dma_dev, nlm_adma_get_max_pq(), 0);
+	dma_dev->device_prep_dma_pq = nlm_adma_prep_dma_pq;
+	dma_dev->device_prep_dma_pq_val = nlm_adma_prep_dma_pq_val;
 
 	dma_dev->dev = &pdev->dev;
 	dma_async_device_register(dma_dev);
@@ -895,7 +1358,7 @@ static int __devinit nlm_adma_probe(struct platform_device *pdev)
 
 	base = nlm_hal_get_dev_base (0 /*node*/, 0 /*B*/, 5 /*D*/, 0 /*F*/);
 	value = nlm_hal_read_32bit_reg (base, 0x40);
-	nlm_hal_write_32bit_reg (base, 0x40, value | (0x10));  /*maybe 0x30? */
+	nlm_hal_write_32bit_reg (base, 0x40, value | (0x10));
 
 	/* Configure credits to non-n0c0 cores */
 	nlm_hal_fmn_init(0x10000000, 0x02000000, 50);
diff --git a/drivers/dma/nlm_adma.h b/drivers/dma/nlm_adma.h
index e702cf3..88f09aa 100644
--- a/drivers/dma/nlm_adma.h
+++ b/drivers/dma/nlm_adma.h
@@ -33,7 +33,7 @@ THE POSSIBILITY OF SUCH DAMAGE.
 #define DTRE_MAX_TX_DESC_PER_CHAN 	4096
 #define DTRE_MAX_TX_Q_LEN		(DTRE_MAX_TX_DESC_PER_CHAN *2)
 #define DTRE_MAX_TX_Q_MASK		(DTRE_MAX_TX_Q_LEN - 1)
-#define DTRE_MAX_LIST_LENGTH 		8
+#define DTRE_MAX_LIST_LENGTH 		24
 #define DTRE_RAID_LIST_MSG_SIZE 	3
 #define DTRE_TRANSFER_MSG_SIZE		4
 #define DTRE_RAID_MESSAGE_TYPE		6
@@ -41,6 +41,7 @@ THE POSSIBILITY OF SUCH DAMAGE.
 #define DTRE_RAID_MAX_ENTRIES		384
 #define DTRE_RAID_MAX_DEVICES		24
 #define DTRE_RAID_MAX_SRC		16
+#define DTRE_NLM_Q_POLY			0x1d
 
 #define DTRE_NUM_VC		4
 #define DTRE_MIN_VC		264
@@ -72,6 +73,7 @@ struct nlm_tx_desc {
 	int done_flag;
 	u32 *xor_check_result;
 	void    *chan; /* backpointer to channel */
+	struct page * pg;
 };
 
 struct nlm_adma_chan {
-- 
1.7.0.4

