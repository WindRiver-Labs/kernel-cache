From 0c09c322f85f231c3b2f48ef5aa3625e9977a9c6 Mon Sep 17 00:00:00 2001
From: Om Narasimhan <onarasimhan@netlogicmicro.com>
Date: Wed, 28 Sep 2011 17:35:02 -0700
Subject: [PATCH 412/761] Stack dump during request_irq() eliminated

Instead of calling on_each_cpu() for enabling eimr on each cpu, we have a
static mask that is set when smp initializes. This is slightly suboptimal,
but works reliably. Actual solution is to rewrite ipi functions and related
functions such that they can be called with spin lock held.

Based on Broadcom SDK 2.3.

Signed-off-by: Om Narasimhan <onarasimhan@netlogicmicro.com>
Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 arch/mips/netlogic/xlp/irq.c |   67 +++++++++++++++++++++++++++---------------
 1 file changed, 44 insertions(+), 23 deletions(-)

diff --git a/arch/mips/netlogic/xlp/irq.c b/arch/mips/netlogic/xlp/irq.c
index e7eb065..0009c1a 100644
--- a/arch/mips/netlogic/xlp/irq.c
+++ b/arch/mips/netlogic/xlp/irq.c
@@ -496,7 +496,10 @@ static void __nlm_irq_mask(unsigned int irq)
 	if (rvec < 0) {
 		return;
 	}
-	on_each_cpu(xlp_clear_eimr, (void *) (1ULL << rvec), 1);
+	if (read_64bit_cp0_eimr() & (1ULL << rvec)) {
+		/* We do not clear eimr, this is a TODO for later time */
+		//on_each_cpu(xlp_clear_eimr, (void *) (1ULL << rvec), 1);
+	}
 	return;
 }
 
@@ -516,8 +519,9 @@ static void nlm_irq_mask(unsigned int irq)
 		pr_err("irq = %d. Invalid irq requested\n", irq);
 		return;
 	}
+	// Once enabled, we don't mask it out
 	//spin_lock_irqsave(&xlp_pic_lock, flags);	// Remove XXX
-	__nlm_irq_mask(irq);
+	__nlm_irq_mask(irq);				// XXX
 	//spin_unlock_irqrestore(&xlp_pic_lock, flags);	// XXX remove
 	return;
 }
@@ -533,8 +537,11 @@ static void __nlm_irq_unmask(int irq)
 
 	if (rvec < 0) {
 		return;
+	} else if (((1ULL << rvec) & read_64bit_cp0_eimr()) == 0) {
+		/* This is only for those interrupts which are not statically
+		 * set in EIMR. Could dump stack if spin lock held */
+		 on_each_cpu(xlp_set_eimr, (void *) (1ULL << rvec), 1);
 	}
-	on_each_cpu(xlp_set_eimr, (void *) (1ULL << rvec), 1);
 	return;
 }
 
@@ -850,8 +857,7 @@ void __cpuinit nlm_smp_irq_init(void)
 	/* Set up kseg0 to be cachable coherent */
 	change_c0_config(CONF_CM_CMASK, CONF_CM_DEFAULT);
 #endif
-	/* set interrupt mask for non-zero cpus */
-	write_64bit_cp0_eimr(xlp_irq_mask | (1 << XLP_IRQ_TIMER));
+	write_64bit_cp0_eimr(xlp_irq_mask);
 }
 
 void destroy_irq(unsigned int irq)
@@ -1111,7 +1117,7 @@ static int xlp_msi_compose_msg(struct pci_dev *pdev, struct msi_desc *desc,
 		offset = irq - XLP_MSIX_INDEX_START;
 		msg->address_hi = (virt_to_phys(xlp_msix_addr_start(fn)) >> 32);
 		msg->address_lo = (virt_to_phys(xlp_msix_addr_start(fn)) & 0xffffffff);
-		dev_err(&pdev->dev, "MSI-X hi = %#x, lo = %#x, data = %#x\n", msg->address_hi, msg->address_lo, offset);
+		//dev_dbg(&pdev->dev, "MSI-X hi = %#x, lo = %#x, data = %#x\n", msg->address_hi, msg->address_lo, offset);
 	} else {
 		if (irq < XLP_MSI_IRQ_OFFSET) {	/* enforce minimum */
 			return -EINVAL;
@@ -1136,7 +1142,7 @@ u32 __xlp_msix_bitmask(int fn)
 
 	while (idx < XLP_MSIX_PER_SLOT) {
 		if (irq_map[XLP_MSIX_IRQ_START(fn) + idx].usage > 0) {
-			ret |= (1 << idx);
+			ret |= (1ULL << idx);
 		}
 		idx++;
 	}
@@ -1178,7 +1184,7 @@ void arch_teardown_msi_irq(unsigned int msi)
 		spin_lock_irqsave(&xlp_pic_lock, flags);
 		if (irq_map[msi].usage > 0) {
 			irq_map[msi].usage--;
-			msix_vec[fn].bitmap &= ~(1 << bit);
+			msix_vec[fn].bitmap &= ~(1ULL << bit);
 			msix_vec[fn].count--;
 		}
 		if (!__xlp_msix_bitmask(fn)) {
@@ -1214,7 +1220,7 @@ asmlinkage void plat_irq_dispatch(void)
 	eirr = read_64bit_cp0_eirr();
 	eimr = read_64bit_cp0_eimr();
 	eirr &= eimr;
-	if (eirr & (1 << XLP_IRQ_TIMER)) {
+	if (eirr & (1ULL << XLP_IRQ_TIMER)) {
 		nlm_common_timer_interrupt(pt_regs, XLP_IRQ_TIMER);
 		return;
 	}
@@ -1429,7 +1435,7 @@ int xlp_setup_msix_irq(struct pci_dev *dev, struct msi_desc *desc, int nvec)
 		}
 		/* We hit an unused entry */
 		irq_map[base_msix + idx].usage = 1;
-		msix_vec[fn].bitmap |= 1 << idx;
+		msix_vec[fn].bitmap |= (1ULL << idx);
 		msix_vec[fn].count++;
 		break;
 	}
@@ -1483,6 +1489,7 @@ EXPORT_SYMBOL(arch_setup_msi_irqs);
 void __init init_nlm_common_irqs(void)
 {
 	int i;
+	u64	mask = 0;
 
 	for (i = 0; i < XLP_IRQ_MAX; i++) {	// IRQ : 0 - 167
 		set_irq_chip(i, &nlm_irq_pic);
@@ -1496,12 +1503,11 @@ void __init init_nlm_common_irqs(void)
 	}
 #endif
 
-#ifdef CONFIG_REMOTE_DEBUG
+#ifdef CONFIG_REMOTE_DEBUG	/* REMOVE on XLP TODO */
 	irq_desc[XLP_IRQ_REMOTE_DEBUG].chip = &nlm_common_rsvd_pic;
 	irq_desc[XLP_IRQ_REMOTE_DEBUG].action = nlm_common_rsvd_action;
-	xlp_irq_mask |= (1ULL << XLP_IRQ_REMOTE_DEBUG);
+	// xlp_irq_mask |= (1ULL << XLP_IRQ_REMOTE_DEBUG);
 #endif
-
 #ifdef CONFIG_SMP
 	irq_desc[XLP_IRQ_IPI_SMP_FUNCTION].chip = &nlm_common_rsvd_pic;
 	irq_desc[XLP_IRQ_IPI_SMP_FUNCTION].action = &nlm_common_rsvd_action;
@@ -1509,31 +1515,46 @@ void __init init_nlm_common_irqs(void)
 	irq_desc[XLP_IRQ_IPI_SMP_RESCHEDULE].chip = &nlm_common_rsvd_pic;
 	irq_desc[XLP_IRQ_IPI_SMP_RESCHEDULE].action = &nlm_common_rsvd_action;
 
-#ifdef CONFIG_NLMCOMMON_IP_FLOW_AFFINITY
+#ifdef CONFIG_NLMCOMMON_IP_FLOW_AFFINITY	/* REMOVE on XLP TODO */
 	/* PR: New IPI added here for netrx balancing */
 	irq_desc[XLP_IRQ_IPI_NETRX].chip = &nlm_common_rsvd_pic;
 	irq_desc[XLP_IRQ_IPI_NETRX].action = &nlm_common_rsvd_action;
-	xlp_irq_mask |= (1ULL << XLP_IRQ_IPI_NETRX);
+	//xlp_irq_mask |= (1ULL << XLP_IRQ_IPI_NETRX);
 #endif				/* CONFIG_NLMCOMMON_IP_FLOW_AFFINITY */
 
-	xlp_irq_mask |= ((1ULL << XLP_IRQ_IPI_SMP_FUNCTION) |
-			     (1ULL << XLP_IRQ_IPI_SMP_RESCHEDULE));
 #endif
 
 	/* msgring interrupt */
 	irq_desc[XLP_IRQ_MSGRING].chip = &nlm_common_rsvd_pic;
 	irq_desc[XLP_IRQ_MSGRING].action = &nlm_common_rsvd_action;
-	xlp_irq_mask |= (1ULL << XLP_IRQ_MSGRING);
 
+	mask = (
+			(1ULL << XLP_IRQ_TIMER) |
+			(1ULL << 10) |	/* timer */
+			(1ULL << 49) |	/* msg_idx */
+			(0x3ULL << 48) |	/* msg_idx */
+			(0xfULL << 32) |	/* pci msix */
+			(0xfULL << 41) |	/* pci and msi */
+			(0x1ULL << 58) |	/* nae */
+			(0x3ULL << 24) |	/* usb */
+			(0x3ULL	<< 17) |	/* uart */
+			(0xfULL << 13) |	/* gpio */
+#ifdef CONFIG_SMP
+			(1ULL << XLP_IRQ_IPI_SMP_FUNCTION) |
+			(1ULL << XLP_IRQ_IPI_SMP_RESCHEDULE) |
+#endif
 #ifdef CONFIG_OPROFILE
-	xlp_irq_mask |= (1ULL << XLP_IRQ_IPI_OPROFILE);
+			(1ULL << XLP_IRQ_IPI_OPROFILE) |
 #endif
-
 #ifdef CONFIG_KGDB
-	xlp_irq_mask |= (1ULL << XLP_IRQ_IPI_SMP_KGDB);
+			(1ULL << XLP_IRQ_IPI_SMP_KGDB) |
 #endif
-
-	xlp_irq_mask |= (1ULL << XLP_IRQ_TIMER);
+			(1ULL << XLP_IRQ_MSGRING) |
+			(0xfULL << 20)		/* nor, nand, spi and mmc */
+	       );
+	/* set interrupt mask for non-zero cpus */
+	mask |= read_64bit_cp0_eimr();
+	xlp_irq_mask = mask;
 }
 
 
-- 
1.7.10.4

