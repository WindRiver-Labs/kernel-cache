From 89f19a7c9b130fa7d18647c6690579f3d8d6e780 Mon Sep 17 00:00:00 2001
From: Yonghong Song <ysong@broadcom.com>
Date: Wed, 8 Apr 2015 12:56:12 -0400
Subject: kvm: KVM support for XLP9xx

Add alternate KVM_NETL option to use the Netlogic KVM codebase on XLP.
This is different from the mainline KVM. The netlogic KVM files will
be under CONFIG_KVM_NETL ifdef and .../kvm-netl directory

Signed-off-by: Yonghong Song <ysong@broadcom.com>
Signed-off-by: Nam Ninh <nam.ninh@windriver.com>

diff --git a/arch/mips/Kbuild b/arch/mips/Kbuild
index d2cfe45..70f5fd0 100644
--- a/arch/mips/Kbuild
+++ b/arch/mips/Kbuild
@@ -19,5 +19,9 @@ obj-y += mm/
 obj-y += math-emu/
 
 ifdef CONFIG_KVM
+ifdef CONFIG_KVM_NETL
+obj-y += kvm-netl/
+else
 obj-y += kvm/
 endif
+endif
diff --git a/arch/mips/Kconfig b/arch/mips/Kconfig
index 4e68436..1f66806 100644
--- a/arch/mips/Kconfig
+++ b/arch/mips/Kconfig
@@ -2613,4 +2613,4 @@ source "crypto/Kconfig"
 
 source "lib/Kconfig"
 
-source "arch/mips/kvm/Kconfig"
+source "arch/mips/kvm-netl/Kconfig"
diff --git a/arch/mips/include/asm/asmmacro-64.h b/arch/mips/include/asm/asmmacro-64.h
index 08a527d..f981a40 100644
--- a/arch/mips/include/asm/asmmacro-64.h
+++ b/arch/mips/include/asm/asmmacro-64.h
@@ -13,6 +13,104 @@
 #include <asm/fpregdef.h>
 #include <asm/mipsregs.h>
 
+#ifdef CONFIG_KVM_NETL
+	.macro	fpu_vm_save_16even thread tmp=t0
+	cfc1	\tmp, fcr31
+	sdc1	$f0,  THREAD_VM_FPR0(\thread)
+	sdc1	$f2,  THREAD_VM_FPR2(\thread)
+	sdc1	$f4,  THREAD_VM_FPR4(\thread)
+	sdc1	$f6,  THREAD_VM_FPR6(\thread)
+	sdc1	$f8,  THREAD_VM_FPR8(\thread)
+	sdc1	$f10, THREAD_VM_FPR10(\thread)
+	sdc1	$f12, THREAD_VM_FPR12(\thread)
+	sdc1	$f14, THREAD_VM_FPR14(\thread)
+	sdc1	$f16, THREAD_VM_FPR16(\thread)
+	sdc1	$f18, THREAD_VM_FPR18(\thread)
+	sdc1	$f20, THREAD_VM_FPR20(\thread)
+	sdc1	$f22, THREAD_VM_FPR22(\thread)
+	sdc1	$f24, THREAD_VM_FPR24(\thread)
+	sdc1	$f26, THREAD_VM_FPR26(\thread)
+	sdc1	$f28, THREAD_VM_FPR28(\thread)
+	sdc1	$f30, THREAD_VM_FPR30(\thread)
+	sw	\tmp, THREAD_VM_FCR31(\thread)
+	.endm
+
+	.macro	fpu_vm_save_16odd thread
+	sdc1	$f1,  THREAD_VM_FPR1(\thread)
+	sdc1	$f3,  THREAD_VM_FPR3(\thread)
+	sdc1	$f5,  THREAD_VM_FPR5(\thread)
+	sdc1	$f7,  THREAD_VM_FPR7(\thread)
+	sdc1	$f9,  THREAD_VM_FPR9(\thread)
+	sdc1	$f11, THREAD_VM_FPR11(\thread)
+	sdc1	$f13, THREAD_VM_FPR13(\thread)
+	sdc1	$f15, THREAD_VM_FPR15(\thread)
+	sdc1	$f17, THREAD_VM_FPR17(\thread)
+	sdc1	$f19, THREAD_VM_FPR19(\thread)
+	sdc1	$f21, THREAD_VM_FPR21(\thread)
+	sdc1	$f23, THREAD_VM_FPR23(\thread)
+	sdc1	$f25, THREAD_VM_FPR25(\thread)
+	sdc1	$f27, THREAD_VM_FPR27(\thread)
+	sdc1	$f29, THREAD_VM_FPR29(\thread)
+	sdc1	$f31, THREAD_VM_FPR31(\thread)
+	.endm
+
+	.macro	fpu_vm_save_double thread status tmp
+	sll	\tmp, \status, 5
+	bgez	\tmp, 2f
+	fpu_vm_save_16odd \thread
+2:
+	fpu_vm_save_16even \thread \tmp
+	.endm
+
+	.macro	fpu_vm_restore_16even thread tmp=t0
+	lw	\tmp, THREAD_VM_FCR31(\thread)
+	ldc1	$f0,  THREAD_VM_FPR0(\thread)
+	ldc1	$f2,  THREAD_VM_FPR2(\thread)
+	ldc1	$f4,  THREAD_VM_FPR4(\thread)
+	ldc1	$f6,  THREAD_VM_FPR6(\thread)
+	ldc1	$f8,  THREAD_VM_FPR8(\thread)
+	ldc1	$f10, THREAD_VM_FPR10(\thread)
+	ldc1	$f12, THREAD_VM_FPR12(\thread)
+	ldc1	$f14, THREAD_VM_FPR14(\thread)
+	ldc1	$f16, THREAD_VM_FPR16(\thread)
+	ldc1	$f18, THREAD_VM_FPR18(\thread)
+	ldc1	$f20, THREAD_VM_FPR20(\thread)
+	ldc1	$f22, THREAD_VM_FPR22(\thread)
+	ldc1	$f24, THREAD_VM_FPR24(\thread)
+	ldc1	$f26, THREAD_VM_FPR26(\thread)
+	ldc1	$f28, THREAD_VM_FPR28(\thread)
+	ldc1	$f30, THREAD_VM_FPR30(\thread)
+	ctc1	\tmp, fcr31
+	.endm
+
+	.macro	fpu_vm_restore_16odd thread
+	ldc1	$f1,  THREAD_VM_FPR1(\thread)
+	ldc1	$f3,  THREAD_VM_FPR3(\thread)
+	ldc1	$f5,  THREAD_VM_FPR5(\thread)
+	ldc1	$f7,  THREAD_VM_FPR7(\thread)
+	ldc1	$f9,  THREAD_VM_FPR9(\thread)
+	ldc1	$f11, THREAD_VM_FPR11(\thread)
+	ldc1	$f13, THREAD_VM_FPR13(\thread)
+	ldc1	$f15, THREAD_VM_FPR15(\thread)
+	ldc1	$f17, THREAD_VM_FPR17(\thread)
+	ldc1	$f19, THREAD_VM_FPR19(\thread)
+	ldc1	$f21, THREAD_VM_FPR21(\thread)
+	ldc1	$f23, THREAD_VM_FPR23(\thread)
+	ldc1	$f25, THREAD_VM_FPR25(\thread)
+	ldc1	$f27, THREAD_VM_FPR27(\thread)
+	ldc1	$f29, THREAD_VM_FPR29(\thread)
+	ldc1	$f31, THREAD_VM_FPR31(\thread)
+	.endm
+
+	.macro	fpu_vm_restore_double thread status tmp
+	sll	\tmp, \status, 5
+	bgez	\tmp, 1f				# 16 register mode?
+
+	fpu_vm_restore_16odd \thread
+1:	fpu_vm_restore_16even \thread \tmp
+	.endm
+#endif /* CONFIG_KVM_NETL */
+
 	.macro	fpu_save_16even thread tmp=t0
 	cfc1	\tmp, fcr31
 	sdc1	$f0,  THREAD_FPR0(\thread)
@@ -136,4 +234,404 @@
 	LONG_L	ra, THREAD_REG31(\thread)
 	.endm
 
+#ifdef CONFIG_KVM_NETL
+	.macro cpu_save_vm_state thread
+	mfc0	k0, $10, 4
+	/* if guest id is zero, we do not need to save guest states */
+	beqz	k0, 31f
+
+
+	mfc0    t0, CP0_STATUS
+	li  t1, ST0_CU1
+	or  t0, t1
+	mtc0    t0, CP0_STATUS
+	ehb
+
+	PTR_L	t1, TASK_THREAD_INFO(\thread)
+	LONG_L	t0, (_THREAD_SIZE - 32 - PT_SIZE + PT_STATUS)(t1)
+	fpu_vm_save_double \thread t0 t1
+	
+	mfc0    t0, CP0_STATUS
+	li  t1, ST0_CU1
+	or  t0, t1
+	xor t0, t0, t1
+	mtc0    t0, CP0_STATUS
+	ehb
+	/* save gtoffset/count first and then some cycles later, to save guest_cause.
+	 * some lagging time to cover the cycles, we may potentially have
+	 * during restore.
+	 */
+	LONG_S	k0, THREAD_VM_GUESTCTL1(\thread)
+	mfc0	k0, $12, 7
+	LONG_S	k0, THREAD_VM_GTOFFSET(\thread)
+	mfc0	k0, $9, 0
+	LONG_S	k0, THREAD_VM_COUNT(\thread)
+	mfgc0   k0, $0, 0
+	LONG_S  k0, THREAD_VM_GUEST_INDEX(\thread)
+	mfgc0   k0, $1, 0
+	LONG_S  k0, THREAD_VM_GUEST_RANDOM(\thread)
+	dmfgc0  k0, $2, 0
+	LONG_S  k0, THREAD_VM_GUEST_ENTRYLO0(\thread)
+	dmfgc0  k0, $3, 0
+	LONG_S  k0, THREAD_VM_GUEST_ENTRYLO1(\thread)
+	dmfgc0  k0, $4, 0
+	LONG_S  k0, THREAD_VM_GUEST_CONTEXT(\thread)
+	dmfgc0  k0, $4, 2
+	LONG_S  k0, THREAD_VM_GUEST_USERLOCAL(\thread)
+	dmfgc0  k0, $5, 0
+	LONG_S  k0, THREAD_VM_GUEST_PAGEMASK(\thread)
+	mfgc0   k0, $5, 1
+	LONG_S  k0, THREAD_VM_GUEST_PAGEGRAIN(\thread)
+	dmfgc0  k0, $5, 5
+	LONG_S  k0, THREAD_VM_GUEST_PWBASE(\thread)
+	dmfgc0  k0, $5, 6
+	LONG_S  k0, THREAD_VM_GUEST_PWFIELD(\thread)
+	dmfgc0  k0, $5, 7
+	LONG_S  k0, THREAD_VM_GUEST_PWSIZE(\thread)
+	mfgc0   k0, $6, 0
+	LONG_S  k0, THREAD_VM_GUEST_WIRED(\thread)
+	mfgc0   k0, $6, 6
+	LONG_S  k0, THREAD_VM_GUEST_PWCTL(\thread)
+	mfgc0   k0, $7, 0
+	LONG_S  k0, THREAD_VM_GUEST_HWRENA(\thread)
+	dmfgc0  k0, $8, 0
+	LONG_S  k0, THREAD_VM_GUEST_BADVADDR(\thread)
+	dmfgc0  k0, $9, 6
+	LONG_S  k0, THREAD_VM_GUEST_EIRR(\thread)
+	dmfgc0  k0, $9, 7
+	LONG_S  k0, THREAD_VM_GUEST_EIMR(\thread)
+	dmfgc0  k0, $10, 0
+	LONG_S  k0, THREAD_VM_GUEST_ENTRYHI(\thread)
+	mfgc0   k0, $11, 0
+	LONG_S  k0, THREAD_VM_GUEST_COMPARE(\thread)
+	mfgc0   k0, $12, 0
+	LONG_S  k0, THREAD_VM_GUEST_STATUS(\thread)
+	mfgc0   k0, $12, 1
+	LONG_S  k0, THREAD_VM_GUEST_INTCTL(\thread)
+	dmfgc0  k0, $14, 0
+	LONG_S  k0, THREAD_VM_GUEST_EPC(\thread)
+	dmfgc0  k0, $15, 1
+	LONG_S  k0, THREAD_VM_GUEST_EBASE(\thread)
+	mfgc0   k0, $16, 0
+	LONG_S  k0, THREAD_VM_GUEST_CONFIG0(\thread)
+	mfgc0   k0, $16, 1
+	LONG_S  k0, THREAD_VM_GUEST_CONFIG1(\thread)
+	mfgc0   k0, $16, 4
+	LONG_S  k0, THREAD_VM_GUEST_CONFIG4(\thread)
+	dmfgc0  k0, $20, 0
+	LONG_S  k0, THREAD_VM_GUEST_XCONTEXT(\thread)
+	dmfgc0  k0, $30, 0
+	LONG_S  k0, THREAD_VM_GUEST_ERROREPC(\thread)
+	dmfgc0  k0, $22, 0
+	LONG_S  k0, THREAD_VM_GUEST_OSSCRATCH0(\thread)
+	dmfgc0  k0, $22, 1
+	LONG_S  k0, THREAD_VM_GUEST_OSSCRATCH1(\thread)
+	dmfgc0  k0, $22, 2
+	LONG_S  k0, THREAD_VM_GUEST_OSSCRATCH2(\thread)
+	dmfgc0  k0, $22, 3
+	LONG_S  k0, THREAD_VM_GUEST_OSSCRATCH3(\thread)
+	dmfgc0  k0, $22, 4
+	LONG_S  k0, THREAD_VM_GUEST_OSSCRATCH4(\thread)
+	dmfgc0  k0, $22, 5
+	LONG_S  k0, THREAD_VM_GUEST_OSSCRATCH5(\thread)
+	dmfgc0  k0, $22, 6
+	LONG_S  k0, THREAD_VM_GUEST_OSSCRATCH6(\thread)
+	dmfgc0  k0, $22, 7
+	LONG_S  k0, THREAD_VM_GUEST_OSSCRATCH7(\thread)
+	mfgc0   k0, $8, 1
+	LONG_S  k0, THREAD_VM_GUEST_BADINSTR(\thread)
+	mfgc0   k0, $8, 2
+	LONG_S  k0, THREAD_VM_GUEST_BADINSTRP(\thread)
+	mfgc0   k0, $4, 1
+	LONG_S  k0, THREAD_VM_GUEST_CONTEXTCONFIG(\thread)
+	dmfgc0  k0, $4, 3
+	LONG_S  k0, THREAD_VM_GUEST_XCONTEXTCONFIG(\thread)
+
+	/* some lagging time between saving gtoffset/count and cause */
+	mfgc0   k0, $13, 0
+	LONG_S  k0, THREAD_VM_GUEST_CAUSE(\thread)
+
+	/* wired entries, available registers: k0, k1, t0, t1 */
+	LONG_L  k0, THREAD_VM_GUEST_WIRED(\thread)
+	beqz	k0, 31f
+	li      k1, 0
+
+	29:
+	slt     t0, k1, k0
+	beqz    t0, 30f
+
+	mtgc0   k1, $0, 0
+	tlbgr
+	ehb
+
+	/* offset to the first wired entry */
+	sll     k1, k1, 3
+
+	dmfgc0   t0, $5, 0
+	daddiu  t1, \thread, THREAD_VM_GUEST_WIRED_PAGEMASK_0
+	daddu   t1, t1, k1
+	LONG_S  t0, 0(t1)
+
+	dmfgc0  t0, $10, 0
+	daddiu  t1, \thread, THREAD_VM_GUEST_WIRED_ENTRYHI_0
+	daddu   t1, t1, k1
+	LONG_S  t0, 0(t1)
+
+	dmfgc0  t0, $2, 0
+	daddiu  t1, \thread, THREAD_VM_GUEST_WIRED_ENTRYLO0_0
+	daddu   t1, t1, k1
+	LONG_S  t0, 0(t1)
+
+	dmfgc0  t0, $3, 0
+	daddiu  t1, \thread, THREAD_VM_GUEST_WIRED_ENTRYLO1_0
+	daddu   t1, t1, k1
+	LONG_S  t0, 0(t1)
+
+	mfc0	t0, $10, 4
+	daddiu  t1, \thread, THREAD_VM_GUEST_WIRED_GUESTCTL1_0
+	daddu   t1, t1, k1
+	LONG_S  t0, 0(t1)
+
+	srl     k1, k1, 3
+
+	addiu   k1, k1, 1
+	j       29b
+        30:
+	/* restore guestctl1 which will be used by root */
+	LONG_L	k0, THREAD_VM_GUESTCTL1(\thread)
+	mtc0	k0, $10, 4
+
+	/* other changed tlb registers */
+	LONG_L  k0, THREAD_VM_GUEST_INDEX(\thread)
+	mtgc0   k0, $0, 0
+	LONG_L  k0, THREAD_VM_GUEST_ENTRYLO0(\thread)
+	dmtgc0  k0, $2, 0
+	LONG_L  k0, THREAD_VM_GUEST_ENTRYLO1(\thread)
+	dmtgc0  k0, $3, 0
+	LONG_L  k0, THREAD_VM_GUEST_PAGEMASK(\thread)
+	dmtgc0  k0, $5, 0
+	LONG_L  k0, THREAD_VM_GUEST_ENTRYHI(\thread)
+	dmtgc0  k0, $10, 0
+	31:
+	.endm
+
+	.macro cpu_restore_vm_state thread
+
+	/* Flush the root/guest tlb and reinstall the wired entries */
+	LONG_L	k0, THREAD_VM_GUESTCTL1(\thread)
+	bnez	k0, 59f
+
+	mtc0	k0, $10, 4
+	j 60f
+
+	59:
+
+	mfc0    t0, CP0_STATUS
+	li  t1, ST0_CU1
+	or  t0, t1
+	mtc0    t0, CP0_STATUS
+	ehb
+
+	LONG_L	t0, THREAD_STATUS(\thread)
+	fpu_vm_restore_double \thread t0 t1
+
+	mfc0    t0, CP0_STATUS
+	li  t1, ST0_CU1
+	or  t0, t1
+	xor t0,t0,t1
+	mtc0    t0, CP0_STATUS
+	ehb
+
+	move	k1, k0
+	sll	k1, k1, 16
+	or	k0, k0, k1
+	mtc0	k0, $10, 4
+	tlbinvf
+
+	/* workaround for tlbginvf */
+#if 0
+	/* flush guest tlbs with the guestctl1.rid */
+	tlbginvf
+#else
+	/* read guest config6 register, invalidate all guest tlb entries */
+	mfgc0	v0, $16, 6
+	srl	v0, 16
+	addiu	v0, 1
+	li	v1, 0
+99:
+	slt	k0, v1, v0
+	beqz	k0, 100f
+	nop
+
+	/* guest entryhi, unique for each entry */
+	lui	k0, 0x8000
+	sll	k1, v1, 17
+	daddu	k0, k1
+	dmtgc0  k0, $10, 0
+
+	/* use the root pagemask. */
+	mfc0	k0, $5, 0
+	dmtgc0  k0, $5, 0
+
+	dmtgc0  $0, $2, 0
+	dmtgc0  $0, $3, 0
+
+	/* guest index */
+	mtgc0   v1, $0, 0
+
+	tlbgwi
+
+	addiu	v1, 1
+	j	99b
+	nop
+100:
+#endif
+
+	/* wired entries */
+	LONG_L  k0, THREAD_VM_GUEST_WIRED(\thread)
+	li      k1, 0
+        39:
+	slt     v0, k1, k0
+	beqz    v0, 40f
+
+	/* offset to the first wired entry */
+	sll     k1, k1, 3
+
+	daddiu  v1, \thread, THREAD_VM_GUEST_WIRED_PAGEMASK_0
+	daddu   v1, v1, k1
+	LONG_L  v0, 0(v1)
+	dmtgc0   v0, $5, 0
+
+	daddiu  v1, \thread, THREAD_VM_GUEST_WIRED_ENTRYHI_0
+	daddu   v1, v1, k1
+	LONG_L  v0, 0(v1)
+	dmtgc0  v0, $10, 0
+
+	daddiu  v1, \thread, THREAD_VM_GUEST_WIRED_ENTRYLO0_0
+	daddu   v1, v1, k1
+	LONG_L  v0, 0(v1)
+	dmtgc0  v0, $2, 0
+	daddiu  v1, \thread, THREAD_VM_GUEST_WIRED_ENTRYLO1_0
+	daddu   v1, v1, k1
+	LONG_L  v0, 0(v1)
+	dmtgc0  v0, $3, 0
+
+	srl     k1, k1, 3
+
+	mtgc0   k1, $0, 0
+	tlbgwi
+
+	addiu   k1, k1, 1
+	j       39b
+        40:
+
+	LONG_L	k0, THREAD_VM_GUESTCTL1(\thread)
+	mtc0	k0, $10, 4
+	LONG_L  k0, THREAD_VM_GUEST_INDEX(\thread)
+	mtgc0   k0, $0, 0
+	LONG_L  k0, THREAD_VM_GUEST_RANDOM(\thread)
+	mtgc0   k0, $1, 0
+	LONG_L  k0, THREAD_VM_GUEST_ENTRYLO0(\thread)
+	dmtgc0  k0, $2, 0
+	LONG_L  k0, THREAD_VM_GUEST_ENTRYLO1(\thread)
+	dmtgc0  k0, $3, 0
+	LONG_L  k0, THREAD_VM_GUEST_CONTEXT(\thread)
+	dmtgc0  k0, $4, 0
+	LONG_L  k0, THREAD_VM_GUEST_USERLOCAL(\thread)
+	dmtgc0  k0, $4, 2
+	LONG_L  k0, THREAD_VM_GUEST_PAGEMASK(\thread)
+	dmtgc0  k0, $5, 0
+	LONG_L  k0, THREAD_VM_GUEST_PAGEGRAIN(\thread)
+	mtgc0   k0, $5, 1
+	LONG_L  k0, THREAD_VM_GUEST_PWBASE(\thread)
+	dmtgc0  k0, $5, 5
+	LONG_L  k0, THREAD_VM_GUEST_PWFIELD(\thread)
+	dmtgc0  k0, $5, 6
+	LONG_L  k0, THREAD_VM_GUEST_PWSIZE(\thread)
+	dmtgc0  k0, $5, 7
+	LONG_L  k0, THREAD_VM_GUEST_WIRED(\thread)
+	mtgc0   k0, $6, 0
+	LONG_L  k0, THREAD_VM_GUEST_PWCTL(\thread)
+	mtgc0   k0, $6, 6
+	LONG_L  k0, THREAD_VM_GUEST_HWRENA(\thread)
+	mtgc0   k0, $7, 0
+	LONG_L  k0, THREAD_VM_GUEST_BADVADDR(\thread)
+	dmtgc0  k0, $8, 0
+	LONG_L  k0, THREAD_VM_GUEST_EIMR(\thread)
+	dmtgc0  k0, $9, 7
+	LONG_L  k0, THREAD_VM_GUEST_ENTRYHI(\thread)
+	dmtgc0  k0, $10, 0
+	LONG_L  k0, THREAD_VM_GUEST_STATUS(\thread)
+	mtgc0   k0, $12, 0
+	LONG_L  k0, THREAD_VM_GUEST_INTCTL(\thread)
+	mtgc0   k0, $12, 1
+	LONG_L  k0, THREAD_VM_GUEST_EPC(\thread)
+	dmtgc0  k0, $14, 0
+	LONG_L  k0, THREAD_VM_GUEST_EBASE(\thread)
+	dmtgc0  k0, $15, 1
+	LONG_L  k0, THREAD_VM_GUEST_CONFIG0(\thread)
+	mtgc0   k0, $16, 0
+	LONG_L  k0, THREAD_VM_GUEST_CONFIG1(\thread)
+	mtgc0   k0, $16, 1
+	LONG_L  k0, THREAD_VM_GUEST_CONFIG4(\thread)
+	mtgc0   k0, $16, 4
+	LONG_L  k0, THREAD_VM_GUEST_XCONTEXT(\thread)
+	mtgc0   k0, $20, 0
+	LONG_L  k0, THREAD_VM_GUEST_ERROREPC(\thread)
+	dmtgc0  k0, $30, 0
+	LONG_L  k0, THREAD_VM_GUEST_OSSCRATCH0(\thread)
+	dmtgc0  k0, $22, 0
+	LONG_L  k0, THREAD_VM_GUEST_OSSCRATCH1(\thread)
+	dmtgc0  k0, $22, 1
+	LONG_L  k0, THREAD_VM_GUEST_OSSCRATCH2(\thread)
+	dmtgc0  k0, $22, 2
+	LONG_L  k0, THREAD_VM_GUEST_OSSCRATCH3(\thread)
+	dmtgc0  k0, $22, 3
+	LONG_L  k0, THREAD_VM_GUEST_OSSCRATCH4(\thread)
+	dmtgc0  k0, $22, 4
+	LONG_L  k0, THREAD_VM_GUEST_OSSCRATCH5(\thread)
+	dmtgc0  k0, $22, 5
+	LONG_L  k0, THREAD_VM_GUEST_OSSCRATCH6(\thread)
+	dmtgc0  k0, $22, 6
+	LONG_L  k0, THREAD_VM_GUEST_OSSCRATCH7(\thread)
+	dmtgc0  k0, $22, 7
+	LONG_L  k0, THREAD_VM_GUEST_BADINSTR(\thread)
+	mtgc0   k0, $8, 1
+	LONG_L  k0, THREAD_VM_GUEST_BADINSTRP(\thread)
+	mtgc0   k0, $8, 2
+	LONG_L  k0, THREAD_VM_GUEST_CONTEXTCONFIG(\thread)
+	mtgc0   k0, $4, 1
+	LONG_L  k0, THREAD_VM_GUEST_XCONTEXTCONFIG(\thread)
+	dmtgc0  k0, $4, 3
+
+	/* avoid interrupt pass through, and restore guest cause */
+	LONG_L	v0, THREAD_VM_GUEST_CAUSE(\thread)
+	ori	v0, v0, 0xfc00
+	xori	v0, v0, 0xfc00
+
+	/* adjust the gtoffset */
+	LONG_L	k0, THREAD_VM_GTOFFSET(\thread)
+	LONG_L	k1, THREAD_VM_COUNT(\thread)
+	daddu	k0, k1
+	mfc0	k1, $9, 0
+	dsubu	k0, k1
+	mtc0	k0, $12, 7
+
+	/* Load the guest.compare register, and then restore guest.cause to avoid time interrupt lost */
+	LONG_L	k0, THREAD_VM_GUEST_COMPARE(\thread)
+	mtgc0	k0, $11, 0
+	mtgc0	v0, $13, 0
+
+	/* 
+	 * cleared EIRR[6:7] since it is not the right way to assert
+	 * time and performance counter interrupt.
+	 */
+	LONG_L	k0, THREAD_VM_GUEST_EIRR(\thread)
+	ori	k0, 0xc0
+	xori	k0, 0xc0
+	dmtgc0	k0, $9, 6
+	60:
+	.endm
+#endif
+
 #endif /* _ASM_ASMMACRO_64_H */
diff --git a/arch/mips/include/asm/kvm_host.h b/arch/mips/include/asm/kvm_host.h
index 4d6fa0b..d39ca61 100644
--- a/arch/mips/include/asm/kvm_host.h
+++ b/arch/mips/include/asm/kvm_host.h
@@ -10,6 +10,9 @@
 #ifndef __MIPS_KVM_HOST_H__
 #define __MIPS_KVM_HOST_H__
 
+#ifdef CONFIG_KVM_NETL
+#include <asm/kvm_host_netl.h>
+#else
 #include <linux/mutex.h>
 #include <linux/hrtimer.h>
 #include <linux/interrupt.h>
@@ -659,5 +662,5 @@ extern void mips32_SyncICache(unsigned long addr, unsigned long size);
 extern int kvm_mips_dump_stats(struct kvm_vcpu *vcpu);
 extern unsigned long kvm_mips_get_ramsize(struct kvm *kvm);
 
-
+#endif
 #endif /* __MIPS_KVM_HOST_H__ */
diff --git a/arch/mips/include/asm/kvm_host_netl.h b/arch/mips/include/asm/kvm_host_netl.h
new file mode 100644
index 0000000..ae0459d
--- /dev/null
+++ b/arch/mips/include/asm/kvm_host_netl.h
@@ -0,0 +1,175 @@
+/*
+ * Copyright (c) 2003-2013 Broadcom Corporation
+ * All Rights Reserved
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the Broadcom
+ * license below:
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in
+ *    the documentation and/or other materials provided with the
+ *    distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY BROADCOM ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL BROADCOM OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
+ * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
+ * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
+ * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef __MIPS_KVM_HOST_NETL_H__
+#define __MIPS_KVM_HOST_NETL_H__
+
+#include <linux/interrupt.h>
+#include <linux/types.h>
+#include <linux/kvm.h>
+
+#define KVM_MAX_VCPUS		32
+#define	KVM_USER_MEM_SLOTS	8
+#define KVM_PRIVATE_MEM_SLOTS	2
+#define KVM_NR_PAGE_SIZES	1
+#define KVM_PAGES_PER_HPAGE(x)	1
+#define KVM_HPAGE_GFN_SHIFT(x)  0
+#define KVM_IRQCHIP_NUM_PINS	KVM_IOAPIC_NUM_PINS
+
+struct kvm_vm_stat {
+	u32 remote_tlb_flush;
+};
+
+struct kvm_arch_memory_slot {
+	unsigned long *rmap[KVM_NR_PAGE_SIZES];
+	struct kvm_lpage_info *lpage_info[KVM_NR_PAGE_SIZES - 1];
+};
+
+/* put I/O PCIE config space here */
+#define MAX_XLP_PIC_REG_NUM		0x400
+#define MAX_XLP_UART_REG_NUM		0x46
+#define MAX_XLP_SYSMGT_REG_NUM		0xca
+
+/* uart input buffer size */
+#define KVM_MAX_UART_IN_SIZE		128
+
+struct kvm_arch {
+	unsigned int	guest_id;
+	unsigned int	exit_request;
+	unsigned long long gpa_pgd;
+	unsigned int	mem_synced:1;
+	unsigned int	slot_inited:16; /* The number of bits should
+				be at least KVM_MEMORY_SLOTS */
+	struct {
+		unsigned long long gpa_start;
+		unsigned long long rpa_start;
+		unsigned long long size;
+	} pktmem;
+	struct {
+		union {
+			unsigned int v32[MAX_XLP_PIC_REG_NUM];
+			unsigned long long v64[MAX_XLP_PIC_REG_NUM/2];
+		} u;
+		unsigned long long host_sys_time[8];
+	} pic;
+	struct kvm_ioapic *vioapic;
+	struct {
+		unsigned int regs[MAX_XLP_SYSMGT_REG_NUM];
+	} sysmgt;
+	struct {
+		unsigned int reg_0;
+		unsigned int reg_cpupllctrl[20*4];
+		unsigned int reg_cpupllchgctrl;
+	} clkmgt;
+	struct {
+		int32_t config[16]; /* 16 standard configuration register */
+		int32_t pcie_busnum[4];
+		int32_t pcie_membase[4];
+		int32_t pcie_memlimit[4];
+		int32_t pcie_iobase[4];
+		int32_t pcie_iolimit[4];
+	} bridge;
+};
+
+struct kvm_vcpu_guest {
+	struct kvm_regs gpu;
+	struct kvm_fpu  fpu;
+};
+
+struct kvm_vcpu_root {
+	uint64_t	sregs[8];
+	uint64_t	gp;
+	uint64_t	sp;
+	uint64_t	fp;
+	uint64_t	ra;
+	uint64_t	fpr[32];
+	uint64_t	fcr;
+	uint64_t	status;
+};
+
+struct kvm_vcpu_arch {
+	uint64_t pip_vector; /* pending interrupts passthrough (to guest) */
+	uint64_t host_stack; /* host stack */
+	uint64_t init_guest; /* whether the guest has been initialized. */
+	uint64_t hva_pgd; /* host virtual address pgd */
+	uint64_t gpa_pgd; /* guest physical address pgd */
+	uint64_t guest_vcpu_p[32]; /* guest vcpu pointer */
+	uint64_t host_vcpuid; /* corresponding host vcpu id, for fast guest IPI delivery */
+	uint64_t nmi;
+	uint64_t pio_needed;
+
+	struct {
+		int64_t  addr;
+		int64_t  val;
+		uint32_t is_write:1;
+		uint32_t reg_num:5;
+		uint32_t len;
+	} mmio;
+
+	struct {
+		uint8_t	 direction;
+		uint8_t	 size;
+		uint8_t  count;
+		uint8_t  reg_num;
+		uint16_t port;
+		uint16_t data_offset;
+		int64_t  val;
+	} pio;
+
+	/* vcpu state */
+	struct kvm_vcpu_guest	guest;
+	struct kvm_vcpu_root	root;
+
+	struct {
+		uint32_t domid;
+		uint32_t guestid;
+		uint32_t soc_type;
+		uint64_t cmd;
+		uint64_t args[4];
+	}mips_hypcall;
+};
+
+
+struct kvm_vcpu_stat {
+	u32 halt_wakeup;
+};
+
+struct kvm;
+void kvm_vcpu_request_scan_ioapic(struct kvm *kvm);
+static inline int irqchip_in_kernel(struct kvm *kvm)
+{
+        return 1;
+}
+
+#endif
diff --git a/arch/mips/include/asm/mipsregs.h b/arch/mips/include/asm/mipsregs.h
index 964ab75..7ace941 100644
--- a/arch/mips/include/asm/mipsregs.h
+++ b/arch/mips/include/asm/mipsregs.h
@@ -782,6 +782,64 @@ do {									\
 			: : "Jr" (value));				\
 } while (0)
 
+#ifdef CONFIG_KVM_NETL
+
+#define __read_32bit_guest_c0_register(source, sel)			\
+({ int __res;								\
+	if (sel == 0)							\
+		__asm__ __volatile__(					\
+			"mfgc0\t%0, " #source "\n\t"			\
+			: "=r" (__res));				\
+	else								\
+		__asm__ __volatile__(					\
+			"mfgc0\t%0, " #source ", " #sel "\n\t"		\
+			: "=r" (__res));				\
+	__res;								\
+})
+
+#define __read_64bit_guest_c0_register(source, sel)			\
+({ unsigned long long __res;						\
+	if (sizeof(unsigned long) == 4)					\
+		__res = __read_64bit_c0_split(source, sel);		\
+	else if (sel == 0)						\
+		__asm__ __volatile__(					\
+			"dmfgc0\t%0, " #source "\n\t"			\
+			: "=r" (__res));				\
+	else								\
+		__asm__ __volatile__(					\
+			"dmfgc0\t%0, " #source ", " #sel "\n\t"		\
+			: "=r" (__res));				\
+	__res;								\
+})
+
+#define __write_32bit_guest_c0_register(register, sel, value)		\
+do {									\
+	if (sel == 0)							\
+		__asm__ __volatile__(					\
+			"mtgc0\t%z0, " #register "\n\t"			\
+			: : "Jr" ((unsigned int)(value)));		\
+	else								\
+		__asm__ __volatile__(					\
+			"mtgc0\t%z0, " #register ", " #sel "\n\t"	\
+			: : "Jr" ((unsigned int)(value)));		\
+} while (0)
+
+#define __write_64bit_guest_c0_register(register, sel, value)		\
+do {									\
+	if (sizeof(unsigned long) == 4)					\
+		__write_64bit_c0_split(register, sel, value);		\
+	else if (sel == 0)						\
+		__asm__ __volatile__(					\
+			"dmtgc0\t%z0, " #register "\n\t"		\
+			: : "Jr" (value));				\
+	else								\
+		__asm__ __volatile__(					\
+			"dmtgc0\t%z0, " #register ", " #sel "\n\t"	\
+			: : "Jr" (value));				\
+} while (0)
+
+#endif /* CONFIG_KVM_NETL */
+
 #define __read_ulong_c0_register(reg, sel)				\
 	((sizeof(unsigned long) == 4) ?					\
 	(unsigned long) __read_32bit_c0_register(reg, sel) :		\
@@ -1185,6 +1243,14 @@ do {									\
 #define read_c0_brcm_sleepcount()	__read_32bit_c0_register($22, 7)
 #define write_c0_brcm_sleepcount(val)	__write_32bit_c0_register($22, 7, val)
 
+#ifdef CONFIG_KVM_NETL
+#define read_c0_guestctl0() __read_32bit_c0_register($12, 6)
+#define write_c0_guestctl0(val) __write_32bit_c0_register($12, 6, val)
+
+#define read_c0_guestctl1() __read_32bit_c0_register($10, 4)
+#define write_c0_guestctl1(val) __write_32bit_c0_register($10, 4, val)
+#endif
+
 /*
  * Macros to access the floating point coprocessor control registers
  */
diff --git a/arch/mips/include/asm/mmu_context.h b/arch/mips/include/asm/mmu_context.h
index c646959..c15b2a7 100644
--- a/arch/mips/include/asm/mmu_context.h
+++ b/arch/mips/include/asm/mmu_context.h
@@ -112,7 +112,7 @@ get_new_mmu_context(struct mm_struct *mm, unsigned long cpu)
 	if (! ((asid += ASID_INC) & ASID_MASK) ) {
 		if (cpu_has_vtag_icache)
 			flush_icache_all();
-#ifdef CONFIG_KVM
+#if defined(CONFIG_KVM) && !defined(CONFIG_KVM_NETL) 
 		kvm_local_flush_tlb_all();      /* start new asid cycle */
 #else
 		local_flush_tlb_all();	/* start new asid cycle */
diff --git a/arch/mips/include/asm/netlogic/kvm_para.h b/arch/mips/include/asm/netlogic/kvm_para.h
new file mode 100644
index 0000000..6910dda
--- /dev/null
+++ b/arch/mips/include/asm/netlogic/kvm_para.h
@@ -0,0 +1,103 @@
+/*
+ * Copyright (c) 2003-2013 Broadcom Corporation
+ * All Rights Reserved
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the Broadcom
+ * license below:
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in
+ *    the documentation and/or other materials provided with the
+ *    distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY BROADCOM ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL BROADCOM OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
+ * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
+ * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
+ * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef __ASM_KVM_PARA
+#define __ASM_KVM_PARA
+
+extern int is_nlm_guest_os;
+
+#define KVM_HC_GET_HARD_CPUID	0x1  /* p1r1 */
+#define KVM_HC_HALX_NETSOC	0x2
+
+/* Halx specific sub cmds */
+#define CMD_HALX_REGISTER	0x1
+#define CMD_HALX_NET_INIT	0x2
+#define CMD_HALX_OPEN_PORT	0x3
+#define CMD_HALX_CLOSE_PORT	0x4
+#define CMD_HALX_CONFIG_PORT	0x5
+#define CMD_HALX_DEV_INFO	0x6
+#define CMD_HALX_PKT_ENGINE	0x7
+#define CMD_HALX_PKT_PARSER	0x8
+#define CMD_HALX_MSG_INIT	0x09
+#define CMD_HALX_DTR_INIT	0xA
+#define CMD_HALX_CDE_INIT	0XB
+#define CMD_HALX_CDE_COMPRESS	0XC
+#define CMD_HALX_CDE_DECOMPRESS	0XD
+#define CMD_HALX_CDE_RESULT	0XE
+#define CMD_HALX_CDE_CLEANUP	0XF
+#define CMD_HALX_CMD_POE_ENQ_STORAGE	0x10
+
+static inline int do_hypcall_p1r1(int num, uint64_t param0, uint64_t *ret0)
+{
+	int	 ret;
+	uint64_t result;
+
+	__asm__ __volatile__(
+		"move $2, %2\n"
+		"move $3, %3\n"
+		"hypcall\n"
+		"move %0, $2\n"
+		"move %1, $3\n"
+		: "=r"(ret), "=r"(result)
+		: "r"(num), "r"(param0)
+		: "$2","$3", "memory"
+	);
+
+	*ret0 = result;
+	return ret;
+}
+
+static inline int do_hypcall_p2r1(int num, uint64_t param0, uint64_t param1, uint64_t *ret0)
+{
+        int      ret;
+        uint64_t result;
+
+        __asm__ __volatile__(
+                "move $2, %2\n"
+                "move $3, %3\n"
+		"move $4, %4\n"
+                "hypcall\n"
+                "move %0, $2\n"
+                "move %1, $3\n"
+                : "=r"(ret), "=r"(result)
+                : "r"(num), "r"(param0), "r"(param1)
+                : "$2","$3","$4", "memory"
+        );
+
+        *ret0 = result;
+        ret = 0;
+        return ret;
+}
+
+#endif
diff --git a/arch/mips/include/asm/netlogic/kvm_xlp.h b/arch/mips/include/asm/netlogic/kvm_xlp.h
new file mode 100644
index 0000000..84f54e0
--- /dev/null
+++ b/arch/mips/include/asm/netlogic/kvm_xlp.h
@@ -0,0 +1,143 @@
+/*
+ * Copyright (c) 2003-2013 Broadcom Corporation
+ * All Rights Reserved
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the Broadcom
+ * license below:
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in
+ *    the documentation and/or other materials provided with the
+ *    distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY BROADCOM ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL BROADCOM OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
+ * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
+ * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
+ * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef __ASM_KVM_XLP
+#define __ASM_KVM_XLP
+
+#include <asm/branch.h>
+
+/* Maximum # of guest ids supported by hardware */
+#define KVM_MAX_NUM_GID		256
+/*#define KVM_HYPERCALL_DEBUG*/
+
+static inline struct kvm_arch * kvm_get_arch(struct pt_regs *regs)
+{
+	struct kvm *kvm = ((struct kvm_vcpu *)regs->cp0_osscratch7)->kvm;
+	return &kvm->arch;
+}
+
+static inline struct kvm_vcpu_guest * kvm_get_vcpu_guest_regs(struct pt_regs *regs)
+{
+	return &((struct kvm_vcpu *)regs->cp0_osscratch7)->arch.guest;
+}
+
+static inline struct kvm_vcpu_root * kvm_get_vcpu_root_regs(struct pt_regs *regs)
+{
+	return &((struct kvm_vcpu *)regs->cp0_osscratch7)->arch.root;
+}
+
+static inline struct kvm_vcpu_arch * kvm_get_vcpu_arch(struct pt_regs *regs)
+{
+	return &((struct kvm_vcpu *)regs->cp0_osscratch7)->arch;
+}
+
+static inline struct kvm_vcpu_arch * kvm_get_vcpu_arch_with_cpuid(struct pt_regs *regs,
+	unsigned int cpuid)
+{
+	struct kvm *kvm = ((struct kvm_vcpu *)regs->cp0_osscratch7)->kvm;
+	unsigned int i;
+
+	for (i = 0; i < KVM_MAX_VCPUS; i++) {
+		if ((kvm->vcpus[i]->arch.guest.gpu.guest_ebase & 0x3ff) == cpuid)
+			return &kvm->vcpus[i]->arch;
+	}
+	return NULL;
+}
+
+static inline struct kvm_vcpu_arch * kvm_get_vcpu_arch_ext(struct kvm *kvm, unsigned int cpuid)
+{
+	unsigned int i;
+
+	for (i = 0; i < KVM_MAX_VCPUS; i++) {
+		if ((kvm->vcpus[i]->arch.guest.gpu.guest_ebase & 0x3ff) == cpuid)
+			return &kvm->vcpus[i]->arch;
+	}
+	return NULL;
+}
+
+static inline void kvm_get_badinstr(struct pt_regs *regs, unsigned int *badinstr,
+	unsigned int *epc_badinstr)
+{
+	*badinstr = regs->cp0_badinstr;
+	if (delay_slot(regs))
+		*epc_badinstr = regs->cp0_badinstrp;
+	else
+		*epc_badinstr = regs->cp0_badinstr;
+}
+
+extern int xlp_kvm_check_processor_compat(void);
+extern int __kvm_vcpu_run_guest(void *);
+extern void __kvm_vcpu_leave_guest(void *, int);
+extern int compute_guest_return_epc(struct pt_regs *regs, unsigned int badinstr);
+extern void xlp_kvm_init_vm(struct kvm *kvm);
+extern void kvm_save_guest_context(struct pt_regs *regs, struct kvm_vcpu_guest *guest);
+extern void xlp_kvm_destroy_vm(struct kvm *kvm);
+extern void kvm_xlp_check_exit_request(struct kvm_vcpu *);
+extern void nlm_flush_cache_L2L3(unsigned long, unsigned long);
+
+/* defined in arch/mips/netlogic/kvm/kvm_pic.c */
+extern void kvm_handle_pcie_pic(struct pt_regs *regs, unsigned long write,
+	unsigned long address, unsigned long reg_num);
+extern void kvm_pic_inject_guest(struct kvm_arch *arch, unsigned int irt,
+	struct pt_regs *regs);
+extern void kvm_pic_inject_guest_cpuid(struct kvm_arch *arch, unsigned int irt,
+	unsigned int cpuid);
+extern void kvm_pic_inject_guest_ext(struct kvm *kvm, struct kvm_arch *arch,
+	unsigned int irt, unsigned int cpuid);
+
+/* defined in arch/mips/netlogic/kvm/kvm_sysmgt.c */
+extern void kvm_handle_pcie_sysmgt(struct pt_regs *regs, unsigned long write,
+	unsigned long address, unsigned long reg_num);
+
+/* defined in arch/mips/netlogic/kvm/kvm_fuse.c */
+extern void kvm_handle_pcie_fuse(struct pt_regs *regs, unsigned long write,
+	unsigned long address, unsigned long reg_num);
+
+/* defined in arch/mips/netlogic/kvm/kvm_clk.c */
+extern void kvm_handle_pcie_clk(struct pt_regs *regs, unsigned long write,
+	unsigned long address, unsigned long reg_num);
+
+/* defined in arch/mips/netlogic/kvm/kvm_bridge.c */
+extern void kvm_handle_pcie_bridge(struct pt_regs *regs, unsigned long write,
+	unsigned long address, unsigned long reg_num);
+
+/* defined in arch/mips/netlogic/kvm/kvm_pcie.c */
+extern void kvm_handle_pcie_pcie(struct pt_regs *regs, unsigned long write,
+	unsigned long address, unsigned long reg_num, uint32_t badinstr,
+	uint32_t epc_badinstr);
+extern void kvm_handle_pcie_io(struct pt_regs *regs, unsigned long write,
+	unsigned long address, unsigned long reg_num, uint32_t badinstr,
+	uint32_t epc_badinstr);
+
+#endif
diff --git a/arch/mips/include/asm/netlogic/mips-extns.h b/arch/mips/include/asm/netlogic/mips-extns.h
index ab0c082..e218065 100644
--- a/arch/mips/include/asm/netlogic/mips-extns.h
+++ b/arch/mips/include/asm/netlogic/mips-extns.h
@@ -171,6 +171,21 @@ static inline void nlm_write_tsc(uint64_t val)
 	return;
 }
 
+static __inline__ void write_xlp_pausetime(unsigned long long val)
+{
+	__asm__ __volatile__(
+		".set\tpush\n\t"
+		".set\tnoreorder\n\t"
+		"move $12, %0\n\t"
+		"dmtur $12, $17\n\t"
+		".set\tpop"
+		:
+		: "r" (val)
+		: "$12");
+
+	return;
+}
+
 static inline int hard_smp_processor_id(void)
 {
 	return __read_32bit_c0_register($15, 1) & 0x3ff;
diff --git a/arch/mips/include/asm/processor.h b/arch/mips/include/asm/processor.h
index 1470b7b..32021ea 100644
--- a/arch/mips/include/asm/processor.h
+++ b/arch/mips/include/asm/processor.h
@@ -191,6 +191,71 @@ struct octeon_cvmseg_state {
 
 #endif
 
+#ifdef CONFIG_KVM_NETL
+
+struct kvm_vm_state {
+	unsigned long cp0_guestctl0;
+	unsigned long cp0_guestctl1;
+	unsigned long cp0_count;
+	unsigned long cp0_gtoffset;
+	unsigned long cp0_osscratch7;
+
+	/* Guest context to be saved */
+	unsigned long guest_cp0_index; /* 32: 0, 0 */
+	unsigned long guest_cp0_random; /* 32: 1, 0 */
+	unsigned long guest_cp0_entrylo0; /* 64: 2, 0 */
+	unsigned long guest_cp0_entrylo1; /* 64: 3, 0 */
+	unsigned long guest_cp0_context; /* 64: 4, 0 */
+	unsigned long guest_cp0_userlocal; /* 64: 4, 2 */
+	unsigned long guest_cp0_pagemask; /* 64: 5, 0 */
+	unsigned long guest_cp0_pagegrain; /* 32: 5, 1 */
+	unsigned long guest_cp0_pwbase; /* 64: 5, 5 */
+	unsigned long guest_cp0_pwfield; /* 64: 5, 6 */
+	unsigned long guest_cp0_pwsize; /* 64: 5, 7 */
+	unsigned long guest_cp0_wired; /* 32: 6, 0 */
+	unsigned long guest_cp0_pwctl; /* 32: 6, 6 */
+	unsigned long guest_cp0_hwrena; /* 32: 7, 0 */
+	unsigned long guest_cp0_badvaddr; /* 64: 8, 0 */
+	unsigned long guest_cp0_eirr; /* 64: 9, 6 */
+	unsigned long guest_cp0_eimr; /* 64: 9, 7 */
+	unsigned long guest_cp0_entryhi; /* 64: 10, 0 */
+	unsigned long guest_cp0_compare; /* 32: 11, 0 */
+	unsigned long guest_cp0_status; /* 32: 12, 0 */
+	unsigned long guest_cp0_intctl; /* 32: 12, 1 */
+	unsigned long guest_cp0_cause;  /* 32: 13, 0 */
+	unsigned long guest_cp0_epc; /* 64: 14, 0 */
+	unsigned long guest_cp0_ebase;  /* 64: 15, 1 */
+	unsigned long guest_cp0_config0; /* 32: 16, 0 */
+	unsigned long guest_cp0_config1; /* 32: 16, 1 */
+	unsigned long guest_cp0_config4; /* 32: 16, 4 */
+	unsigned long guest_cp0_xcontext; /* 64: 20, 0 */
+	unsigned long guest_cp0_errorepc; /* 64: 30, 0 */
+	unsigned long guest_cp0_osscratch0; /* 64: 22, 0 */
+	unsigned long guest_cp0_osscratch1; /* 64: 22, 1 */
+	unsigned long guest_cp0_osscratch2; /* 64: 22, 2 */
+	unsigned long guest_cp0_osscratch3; /* 64: 22, 3 */
+	unsigned long guest_cp0_osscratch4; /* 64: 22, 4 */
+	unsigned long guest_cp0_osscratch5; /* 64: 22, 5 */
+	unsigned long guest_cp0_osscratch6; /* 64: 22, 6 */
+	unsigned long guest_cp0_osscratch7; /* 64: 22, 7 */
+	unsigned long guest_cp0_badinstr; /* 32: 8, 1 */
+	unsigned long guest_cp0_badinstrp; /* 32: 8, 2 */
+	unsigned long guest_cp0_contextconfig; /* 32: 4, 1 */
+	unsigned long guest_cp0_xcontextconfig; /* 64: 4, 3 */
+
+	/* wired tlbs, also see asm/kvm.h */
+#define MAX_WIRED_TLBS  32
+	unsigned long guest_wired_pagemask[MAX_WIRED_TLBS];
+	unsigned long guest_wired_entryhi[MAX_WIRED_TLBS];
+	unsigned long guest_wired_entrylo0[MAX_WIRED_TLBS];
+	unsigned long guest_wired_entrylo1[MAX_WIRED_TLBS];
+	unsigned long guest_wired_guestctl1[MAX_WIRED_TLBS];
+
+	struct mips_fpu_struct fpu;
+};
+	
+#endif
+
 typedef struct {
 	unsigned long seg;
 } mm_segment_t;
@@ -235,6 +300,9 @@ struct thread_struct {
     struct octeon_cvmseg_state cvmseg __attribute__ ((__aligned__(128)));
 #endif
 	struct mips_abi *abi;
+#ifdef CONFIG_KVM_NETL
+	struct kvm_vm_state vm;
+#endif
 };
 
 #ifdef CONFIG_MIPS_MT_FPAFF
@@ -245,6 +313,15 @@ struct thread_struct {
 #define FPAFF_INIT
 #endif /* CONFIG_MIPS_MT_FPAFF */
 
+#ifdef CONFIG_KVM_NETL
+#define KVM_INIT                                                \
+	.vm = {                                                 \
+		.cp0_guestctl1  = 0,                            \
+	}
+#else
+#define KVM_INIT
+#endif
+
 #ifdef CONFIG_CPU_CAVIUM_OCTEON
 #define OCTEON_INIT						\
 	.cp2			= INIT_OCTEON_COP2,
@@ -303,6 +380,10 @@ struct thread_struct {
 	 * Cavium Octeon specifics (null if not Octeon)		\
 	 */							\
 	OCTEON_INIT						\
+	/*							\
+	 * KVM state initialization				\
+	 */							\
+	KVM_INIT						\
 }
 
 struct task_struct;
diff --git a/arch/mips/include/asm/ptrace.h b/arch/mips/include/asm/ptrace.h
index fa95cd8..a7c7d17 100644
--- a/arch/mips/include/asm/ptrace.h
+++ b/arch/mips/include/asm/ptrace.h
@@ -55,6 +55,14 @@ struct pt_regs {
 	unsigned int msg_config;
 	unsigned int msg_err;
 #endif
+#ifdef CONFIG_KVM_NETL
+	unsigned long cp0_badinstr;
+	unsigned long cp0_badinstrp;
+	unsigned long cp0_guestctl0;
+	unsigned long cp0_guestctl1;
+	unsigned long cp0_osscratch7;
+	unsigned long guest_cp0_ebase;  /* 15, 1 */
+#endif
 } __aligned(8);
 
 struct task_struct;
diff --git a/arch/mips/include/asm/stackframe.h b/arch/mips/include/asm/stackframe.h
index c881f61..70eb593 100644
--- a/arch/mips/include/asm/stackframe.h
+++ b/arch/mips/include/asm/stackframe.h
@@ -145,10 +145,102 @@
 		.endm
 #endif
 
+#ifdef CONFIG_KVM_NETL
+		.macro KVM_SAVE_GUEST
+
+		/* guestctl1 reg: save the guest id reg, clear guestctl1.rid */
+		mfc0    v1, $10, 4
+		LONG_S  v1, PT_GUESTCTL1(sp)
+		andi    v1, v1, 0xff
+		mtc0    v1, $10, 4
+		mfc0    v1, $8, 1
+		LONG_S  v1, PT_BADINSTR(sp)
+		mfc0    v1, $8, 2
+		LONG_S  v1, PT_BADINSTRP(sp)
+
+		dmfc0   v0, $22, 7
+		LONG_S  v0, PT_OSSCRATCH7(sp)
+
+		/* restore HVA page table */
+		dmfc0    k0, ASM_SMP_CPUID_REG
+		dsrl    k0, k0, SMP_CPUID_PTRSHIFT
+		PTR_LA  k1, pgd_current
+		daddu   v1, k1, k0
+		LONG_L  k1, VCPU_ARCH_HVA_PGD(v0)
+		LONG_S  k1, 0(v1)
+		mfc0    k0, $6, 6
+		srl     k0, k0, 31
+		beqz    k0, 81f
+		dmfc0   v1, $5, 5
+		LONG_S  k1, 0(v1)
+		81:
+
+		dmfgc0  k0, $15, 1
+		LONG_S  k0, PT_GUEST_EBASE(sp)
+		li      v1, 0
+		mtc0    v1, $12, 6
+		.endm
+
+		.macro KVM_RESTORE_GUEST
+		LONG_L  v0, PT_GUESTCTL1(sp)
+		mtc0    v0, $10, 4
+
+		LONG_L  v0, PT_OSSCRATCH7(sp)
+		dmtc0   v0, $22, 7
+
+		/* restore GPA page table */
+		dmfc0   k0, ASM_SMP_CPUID_REG
+		dsrl    k0, k0, SMP_CPUID_PTRSHIFT
+		PTR_LA  k1, pgd_current
+		daddu   v1, k1, k0
+		LONG_L  k1, 0(v1)
+		LONG_S  k1, VCPU_ARCH_HVA_PGD(v0)
+		LONG_L  k1, VCPU_ARCH_GPA_PGD(v0)
+		LONG_S  k1, 0(v1)
+		mfc0    k0, $6, 6
+		srl     k0, k0, 31
+		beqz    k0, 82f
+		dmfc0   v1, $5, 5
+		LONG_S  k1, 0(v1)
+		82:
+		/* Check whether we have other pending interrupts or not,
+ 		* assert EIRR bits if needed. EIRR[6:7] should be
+ 		* cleared since it is not the right way to assert
+ 		* time and performance counter interrupt.
+ 		*/
+		LONG_L  v0, PT_OSSCRATCH7(sp)
+		dmfgc0  v1, $9, 6 /* EIRR */
+		ori     v1, 0xc0
+		xori    v1, 0xc0
+
+		34:
+		lld     k0, VCPU_ARCH_PIP_VECTOR(v0)
+		or      k0, v1
+		li      k1, 0
+		scd     k1, VCPU_ARCH_PIP_VECTOR(v0)
+		beqz    k1, 34b
+
+		dmtgc0  k0, $9, 6
+		.endm
+#endif
+
 		.macro	SAVE_SOME
 		.set	push
 		.set	noat
 		.set	reorder
+
+#ifdef CONFIG_KVM_NETL
+		mfc0    k0, $12, 6
+		srl     k0, 31
+		beqz    k0, 7f
+
+		/* guest mode */
+		dmfc0   k1, $22, 7
+		LONG_L  k1, VCPU_ARCH_HOST_STACK(k1)
+		j       8f
+7:
+#endif
+
 		mfc0	k0, CP0_STATUS
 		sll	k0, 3		/* extract cu0 bit */
 		.set	noreorder
@@ -189,6 +281,20 @@
 		LONG_S	k0, PT_TCSTATUS(sp)
 #endif /* CONFIG_MIPS_MT_SMTC */
 		LONG_S	$4, PT_R4(sp)
+#ifdef CONFIG_KVM_NETL
+		/* guestctl0 reg: save and clear the guest mode bit */
+		mfc0	v1, $12, 6
+		LONG_S	v1, PT_GUESTCTL0(sp)
+		srl	v1, 31
+		beqz	v1, 9f
+		KVM_SAVE_GUEST
+        9:
+		/* k0, k1 */
+		dmfc0	v1, $31, 2
+		LONG_S	v1, PT_R26(sp)
+		dmfc0	v1, $31, 3
+		LONG_S	v1, PT_R27(sp)
+#endif
 		mfc0	v1, CP0_CAUSE
 		LONG_S	$5, PT_R5(sp)
 		LONG_S	v1, PT_CAUSE(sp)
@@ -244,6 +350,21 @@
 		.endm
 
 		.macro	RESTORE_TEMP
+#ifdef CONFIG_KVM_NETL
+		LONG_L  v1, PT_GUESTCTL0(sp)
+		srl     v1, 31
+		beqz    v1, 80f
+
+		/* check whether an exit request has been generated. */
+		LONG_L  a0, PT_OSSCRATCH7(sp)
+		LONG_L  v0, VCPU_KVM(a0)
+		lw      v0, KVM_ARCH_EXIT_REQUEST(v0)
+		beqz    v0, 80f
+		dla     v0, kvm_xlp_check_exit_request
+		jalr    v0
+		nop
+	80:
+#endif
 #ifdef CONFIG_CPU_HAS_SMARTMIPS
 		LONG_L	$24, PT_ACX(sp)
 		mtlhx	$24
@@ -378,6 +499,18 @@
 		and	v0, v1
 		or	v0, a0
 		mtc0	v0, CP0_STATUS
+
+#ifdef CONFIG_KVM_NETL
+		/* Status register has changed. No more interrupt may happen.
+		 * restore the guest state.
+		 */
+		LONG_L  v0, PT_GUESTCTL0(sp)
+		mtc0    v0, $12, 6
+		srl     v0, 31
+		beqz    v0, 10f
+		KVM_RESTORE_GUEST
+	10:
+#endif
 #ifdef CONFIG_MIPS_MT_SMTC
 /*
  * Only after EXL/ERL have been restored to status can we
@@ -450,6 +583,10 @@
 		.endm
 
 		.macro	RESTORE_SP_AND_RET
+#ifdef CONFIG_KVM_NETL
+		LONG_L  k0, PT_R26(sp)
+		LONG_L  k1, PT_R27(sp)
+#endif
 		LONG_L	sp, PT_R29(sp)
 		.set	mips3
 		eret
diff --git a/arch/mips/include/uapi/asm/kvm.h b/arch/mips/include/uapi/asm/kvm.h
index f09ff5a..a9413e0 100644
--- a/arch/mips/include/uapi/asm/kvm.h
+++ b/arch/mips/include/uapi/asm/kvm.h
@@ -11,6 +11,9 @@
 #ifndef __LINUX_KVM_MIPS_H
 #define __LINUX_KVM_MIPS_H
 
+#ifdef CONFIG_KVM_NETL
+#include <asm/kvm_netl.h>
+#else
 #include <linux/types.h>
 
 /*
@@ -132,4 +135,5 @@ struct kvm_mips_interrupt {
 	__u32 irq;
 };
 
+#endif
 #endif /* __LINUX_KVM_MIPS_H */
diff --git a/arch/mips/include/uapi/asm/kvm_netl.h b/arch/mips/include/uapi/asm/kvm_netl.h
new file mode 100644
index 0000000..4611de1
--- /dev/null
+++ b/arch/mips/include/uapi/asm/kvm_netl.h
@@ -0,0 +1,167 @@
+/*
+ * Copyright (c) 2003-2013 Broadcom Corporation
+ * All Rights Reserved
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the Broadcom
+ * license below:
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in
+ *    the documentation and/or other materials provided with the
+ *    distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY BROADCOM ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL BROADCOM OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
+ * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
+ * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
+ * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef __MIPS_KVM_NETL_H__
+#define __MIPS_KVM_NETL_H__
+
+/* MIPS KVM supports */
+#define __KVM_HAVE_IOAPIC
+#define __KVM_HAVE_IRQ_LINE
+#define __KVM_HAVE_DEVICE_ASSIGNMENT
+
+/* The following macros are needed in include/trace/events/kvm.h
+ * when __KVM_HAVE_IOAPIC is defined.
+ * The include/trace/events/kvm.h is included in kvm_main.c
+ */
+#define KVM_IRQCHIP_PIC_MASTER   0
+#define KVM_IRQCHIP_PIC_SLAVE    1
+#define KVM_IRQCHIP_IOAPIC       2
+
+/* The following macros are needed in include/linux/kvm_host.h
+ * when __KVM_HAVE_IOAPIC is defined.
+ */
+#define KVM_NR_IRQCHIPS          3
+#define KVM_IOAPIC_NUM_PINS     32
+
+struct kvm_ioapic_state {
+};
+
+struct kvm_guest_info {
+	uint32_t	guest_id;
+};
+
+struct kvm_pktmem_info {
+	uint64_t	gpa_start;
+	uint64_t	rpa_start;
+	uint64_t	size;
+};
+
+#define KVM_MIPS_EXIT_REQUEST	  _IOW(KVMIO,  0xa1, int)
+#define KVM_MIPS_INFO_REQUEST     _IOW(KVMIO,  0xa2, struct kvm_guest_info)
+#define KVM_MIPS_SYNC_GPA_MAP	  _IOW(KVMIO,  0xa3, int)
+#define KVM_MIPS_HUGEPAGE_SIZE	  _IOW(KVMIO,  0xa4, unsigned long long)
+#define KVM_MIPS_ADD_PKTMEM	  _IOW(KVMIO,  0xa5, struct kvm_pktmem_info)
+#define KVM_MIPS_SET_BINDING	  _IOW(KVMIO,  0xa6, int)
+#define KVM_MIPS_INJECT_PCIE_INTX _IOW(KVMIO,  0xa7, int)
+#define KVM_MIPS_INJECT_PIC _IOW(KVMIO,  0xa8, int)
+
+/* Used to capture a guest state */
+struct kvm_regs {
+
+	/* general purpose registers */
+	uint64_t	regs[32];
+	uint64_t	hi;
+	uint64_t	lo;
+
+	/* root cop0 register */
+	uint64_t	root_guestctl0;
+	uint64_t	root_guestctl1;
+	uint64_t	root_epc;
+	uint64_t	root_count;
+	uint64_t	root_gtoffset;
+
+	/* guest cop0 register */
+	uint64_t	guest_index;
+	uint64_t	guest_random;
+	uint64_t	guest_entrylo0;
+	uint64_t	guest_entrylo1;
+	uint64_t	guest_context;
+	uint64_t	guest_userlocal;
+	uint64_t	guest_pagemask;
+	uint64_t	guest_pagegrain;
+	uint64_t	guest_pwbase;
+	uint64_t	guest_pwfield;
+	uint64_t	guest_pwsize;
+	uint64_t	guest_wired;
+	uint64_t	guest_pwctl;
+	uint64_t	guest_hwrena;
+	uint64_t	guest_badvaddr;
+	uint64_t	guest_eirr;
+	uint64_t	guest_eimr;
+	uint64_t	guest_entryhi;
+	uint64_t	guest_compare;
+	uint64_t	guest_status;
+	uint64_t	guest_intctl;
+	uint64_t	guest_cause;
+	uint64_t	guest_epc;
+	uint64_t	guest_ebase;
+	uint64_t	guest_config0;
+	uint64_t	guest_config1;
+	uint64_t	guest_config4;
+	uint64_t	guest_xcontext;
+	uint64_t	guest_errorepc;
+	uint64_t	guest_osscratch0;
+	uint64_t	guest_osscratch1;
+	uint64_t	guest_osscratch2;
+	uint64_t	guest_osscratch3;
+	uint64_t	guest_osscratch4;
+	uint64_t	guest_osscratch5;
+	uint64_t	guest_osscratch6;
+	uint64_t	guest_osscratch7;
+	uint64_t	guest_badinstr;
+	uint64_t	guest_badinstrp;
+	uint64_t	guest_contextconfig;
+	uint64_t	guest_xcontextconfig;
+
+	/* wired entries */
+	uint64_t	guest_wired_pagemask[32];
+	uint64_t	guest_wired_entryhi[32];
+	uint64_t	guest_wired_entrylo0[32];
+	uint64_t	guest_wired_entrylo1[32];
+};
+
+struct kvm_sregs {
+};
+
+/* Used to capture a guest state */
+struct kvm_fpu {
+	uint64_t	regs[32];
+	uint32_t	fir;
+	uint32_t	fccr;
+	uint32_t	fexr;
+	uint32_t	fenr;
+	uint32_t	fcsr;
+};
+
+struct kvm_debug_exit_arch {
+};
+
+struct kvm_guest_debug_arch {
+};
+
+/* definition of registers in kvm_run */
+struct kvm_sync_regs {
+};
+
+#endif
diff --git a/arch/mips/kernel/asm-offsets.c b/arch/mips/kernel/asm-offsets.c
index bac2dff..3481322 100644
--- a/arch/mips/kernel/asm-offsets.c
+++ b/arch/mips/kernel/asm-offsets.c
@@ -17,7 +17,14 @@
 #include <asm/ptrace.h>
 #include <asm/processor.h>
 
+#ifdef CONFIG_KVM_NETL
+#include <linux/kvm.h>
 #include <linux/kvm_host.h>
+#include <asm/kvm.h>
+#include <asm/kvm_host.h>
+#else
+#include <linux/kvm_host.h>
+#endif
 
 void output_ptreg_defines(void)
 {
@@ -79,6 +86,14 @@ void output_ptreg_defines(void)
 	OFFSET(NLM_COP2_MSG_CONFIG, pt_regs, msg_config);
 	OFFSET(NLM_COP2_MSG_ERR, pt_regs, msg_err);
 #endif
+#ifdef CONFIG_KVM_NETL
+	OFFSET(PT_BADINSTR,  pt_regs, cp0_badinstr);
+	OFFSET(PT_BADINSTRP, pt_regs, cp0_badinstrp);
+	OFFSET(PT_GUESTCTL0, pt_regs, cp0_guestctl0);
+	OFFSET(PT_GUESTCTL1, pt_regs, cp0_guestctl1);
+	OFFSET(PT_OSSCRATCH7, pt_regs, cp0_osscratch7);
+	OFFSET(PT_GUEST_EBASE, pt_regs, guest_cp0_ebase);
+#endif
 	DEFINE(PT_SIZE, sizeof(struct pt_regs));
 	BLANK();
 }
@@ -136,6 +151,96 @@ void output_thread_defines(void)
 	       thread.cp0_baduaddr);
 	OFFSET(THREAD_ECODE, task_struct, \
 	       thread.error_code);
+#ifdef CONFIG_KVM_NETL
+	OFFSET(THREAD_VM_GUESTCTL0, task_struct, thread.vm.cp0_guestctl0);
+	OFFSET(THREAD_VM_GUESTCTL1, task_struct, thread.vm.cp0_guestctl1);
+	OFFSET(THREAD_VM_COUNT,     task_struct, thread.vm.cp0_count);
+	OFFSET(THREAD_VM_GTOFFSET,  task_struct, thread.vm.cp0_gtoffset);
+	OFFSET(THREAD_VM_OSSCRATCH7, task_struct, thread.vm.cp0_osscratch7);
+
+	OFFSET(THREAD_VM_GUEST_INDEX, task_struct, thread.vm.guest_cp0_index);
+	OFFSET(THREAD_VM_GUEST_RANDOM, task_struct, thread.vm.guest_cp0_random);
+	OFFSET(THREAD_VM_GUEST_ENTRYLO0, task_struct, thread.vm.guest_cp0_entrylo0);
+	OFFSET(THREAD_VM_GUEST_ENTRYLO1, task_struct, thread.vm.guest_cp0_entrylo1);
+	OFFSET(THREAD_VM_GUEST_CONTEXT, task_struct, thread.vm.guest_cp0_context);
+	OFFSET(THREAD_VM_GUEST_USERLOCAL, task_struct, thread.vm.guest_cp0_userlocal);
+	OFFSET(THREAD_VM_GUEST_PAGEMASK, task_struct, thread.vm.guest_cp0_pagemask);
+	OFFSET(THREAD_VM_GUEST_PAGEGRAIN, task_struct, thread.vm.guest_cp0_pagegrain);
+	OFFSET(THREAD_VM_GUEST_PWBASE, task_struct, thread.vm.guest_cp0_pwbase);
+	OFFSET(THREAD_VM_GUEST_PWFIELD, task_struct, thread.vm.guest_cp0_pwfield);
+	OFFSET(THREAD_VM_GUEST_PWSIZE, task_struct, thread.vm.guest_cp0_pwsize);
+	OFFSET(THREAD_VM_GUEST_PWCTL, task_struct, thread.vm.guest_cp0_pwctl);
+	OFFSET(THREAD_VM_GUEST_WIRED, task_struct, thread.vm.guest_cp0_wired);
+	OFFSET(THREAD_VM_GUEST_HWRENA, task_struct, thread.vm.guest_cp0_hwrena);
+	OFFSET(THREAD_VM_GUEST_BADVADDR, task_struct, thread.vm.guest_cp0_badvaddr);
+	OFFSET(THREAD_VM_GUEST_EIRR, task_struct, thread.vm.guest_cp0_eirr);
+	OFFSET(THREAD_VM_GUEST_EIMR, task_struct, thread.vm.guest_cp0_eimr);
+	OFFSET(THREAD_VM_GUEST_ENTRYHI, task_struct, thread.vm.guest_cp0_entryhi);
+	OFFSET(THREAD_VM_GUEST_COMPARE, task_struct, thread.vm.guest_cp0_compare);
+	OFFSET(THREAD_VM_GUEST_STATUS, task_struct, thread.vm.guest_cp0_status);
+	OFFSET(THREAD_VM_GUEST_INTCTL, task_struct, thread.vm.guest_cp0_intctl);
+	OFFSET(THREAD_VM_GUEST_CAUSE, task_struct, thread.vm.guest_cp0_cause);
+	OFFSET(THREAD_VM_GUEST_EPC, task_struct, thread.vm.guest_cp0_epc);
+	OFFSET(THREAD_VM_GUEST_EBASE, task_struct, thread.vm.guest_cp0_ebase);
+	OFFSET(THREAD_VM_GUEST_CONFIG0, task_struct, thread.vm.guest_cp0_config0);
+	OFFSET(THREAD_VM_GUEST_CONFIG1, task_struct, thread.vm.guest_cp0_config1);
+	OFFSET(THREAD_VM_GUEST_CONFIG4, task_struct, thread.vm.guest_cp0_config4);
+	OFFSET(THREAD_VM_GUEST_XCONTEXT, task_struct, thread.vm.guest_cp0_xcontext);
+	OFFSET(THREAD_VM_GUEST_ERROREPC, task_struct, thread.vm.guest_cp0_errorepc);
+	OFFSET(THREAD_VM_GUEST_OSSCRATCH0, task_struct, thread.vm.guest_cp0_osscratch0);
+	OFFSET(THREAD_VM_GUEST_OSSCRATCH1, task_struct, thread.vm.guest_cp0_osscratch1);
+	OFFSET(THREAD_VM_GUEST_OSSCRATCH2, task_struct, thread.vm.guest_cp0_osscratch2);
+	OFFSET(THREAD_VM_GUEST_OSSCRATCH3, task_struct, thread.vm.guest_cp0_osscratch3);
+	OFFSET(THREAD_VM_GUEST_OSSCRATCH4, task_struct, thread.vm.guest_cp0_osscratch4);
+	OFFSET(THREAD_VM_GUEST_OSSCRATCH5, task_struct, thread.vm.guest_cp0_osscratch5);
+	OFFSET(THREAD_VM_GUEST_OSSCRATCH6, task_struct, thread.vm.guest_cp0_osscratch6);
+	OFFSET(THREAD_VM_GUEST_OSSCRATCH7, task_struct, thread.vm.guest_cp0_osscratch7);
+	OFFSET(THREAD_VM_GUEST_BADINSTR, task_struct, thread.vm.guest_cp0_badinstr);
+	OFFSET(THREAD_VM_GUEST_BADINSTRP, task_struct, thread.vm.guest_cp0_badinstrp);
+	OFFSET(THREAD_VM_GUEST_CONTEXTCONFIG, task_struct, thread.vm.guest_cp0_contextconfig);
+	OFFSET(THREAD_VM_GUEST_XCONTEXTCONFIG, task_struct, thread.vm.guest_cp0_xcontextconfig);
+
+	OFFSET(THREAD_VM_GUEST_WIRED_PAGEMASK_0, task_struct, thread.vm.guest_wired_pagemask[0]);
+	OFFSET(THREAD_VM_GUEST_WIRED_ENTRYHI_0,  task_struct, thread.vm.guest_wired_entryhi[0]);
+	OFFSET(THREAD_VM_GUEST_WIRED_ENTRYLO0_0, task_struct, thread.vm.guest_wired_entrylo0[0]);
+	OFFSET(THREAD_VM_GUEST_WIRED_ENTRYLO1_0, task_struct, thread.vm.guest_wired_entrylo1[0]);
+	OFFSET(THREAD_VM_GUEST_WIRED_GUESTCTL1_0, task_struct, thread.vm.guest_wired_guestctl1[0]);
+
+	OFFSET(THREAD_VM_FPR0, task_struct, thread.vm.fpu.fpr[0]);
+	OFFSET(THREAD_VM_FPR1, task_struct, thread.vm.fpu.fpr[1]);
+	OFFSET(THREAD_VM_FPR2, task_struct, thread.vm.fpu.fpr[2]);
+	OFFSET(THREAD_VM_FPR3, task_struct, thread.vm.fpu.fpr[3]);
+	OFFSET(THREAD_VM_FPR4, task_struct, thread.vm.fpu.fpr[4]);
+	OFFSET(THREAD_VM_FPR5, task_struct, thread.vm.fpu.fpr[5]);
+	OFFSET(THREAD_VM_FPR6, task_struct, thread.vm.fpu.fpr[6]);
+	OFFSET(THREAD_VM_FPR7, task_struct, thread.vm.fpu.fpr[7]);
+	OFFSET(THREAD_VM_FPR8, task_struct, thread.vm.fpu.fpr[8]);
+	OFFSET(THREAD_VM_FPR9, task_struct, thread.vm.fpu.fpr[9]);
+	OFFSET(THREAD_VM_FPR10, task_struct, thread.vm.fpu.fpr[10]);
+	OFFSET(THREAD_VM_FPR11, task_struct, thread.vm.fpu.fpr[11]);
+	OFFSET(THREAD_VM_FPR12, task_struct, thread.vm.fpu.fpr[12]);
+	OFFSET(THREAD_VM_FPR13, task_struct, thread.vm.fpu.fpr[13]);
+	OFFSET(THREAD_VM_FPR14, task_struct, thread.vm.fpu.fpr[14]);
+	OFFSET(THREAD_VM_FPR15, task_struct, thread.vm.fpu.fpr[15]);
+	OFFSET(THREAD_VM_FPR16, task_struct, thread.vm.fpu.fpr[16]);
+	OFFSET(THREAD_VM_FPR17, task_struct, thread.vm.fpu.fpr[17]);
+	OFFSET(THREAD_VM_FPR18, task_struct, thread.vm.fpu.fpr[18]);
+	OFFSET(THREAD_VM_FPR19, task_struct, thread.vm.fpu.fpr[19]);
+	OFFSET(THREAD_VM_FPR20, task_struct, thread.vm.fpu.fpr[20]);
+	OFFSET(THREAD_VM_FPR21, task_struct, thread.vm.fpu.fpr[21]);
+	OFFSET(THREAD_VM_FPR22, task_struct, thread.vm.fpu.fpr[22]);
+	OFFSET(THREAD_VM_FPR23, task_struct, thread.vm.fpu.fpr[23]);
+	OFFSET(THREAD_VM_FPR24, task_struct, thread.vm.fpu.fpr[24]);
+	OFFSET(THREAD_VM_FPR25, task_struct, thread.vm.fpu.fpr[25]);
+	OFFSET(THREAD_VM_FPR26, task_struct, thread.vm.fpu.fpr[26]);
+	OFFSET(THREAD_VM_FPR27, task_struct, thread.vm.fpu.fpr[27]);
+	OFFSET(THREAD_VM_FPR28, task_struct, thread.vm.fpu.fpr[28]);
+	OFFSET(THREAD_VM_FPR29, task_struct, thread.vm.fpu.fpr[29]);
+	OFFSET(THREAD_VM_FPR30, task_struct, thread.vm.fpu.fpr[30]);
+	OFFSET(THREAD_VM_FPR31, task_struct, thread.vm.fpu.fpr[31]);
+
+	OFFSET(THREAD_VM_FCR31, task_struct, thread.vm.fpu.fcr31);
+#endif
 	BLANK();
 }
 
@@ -342,6 +447,189 @@ void output_pbe_defines(void)
 
 void output_kvm_defines(void)
 {
+#ifdef CONFIG_KVM_NETL
+	COMMENT("KVM vcpu reg offsets ");
+	OFFSET(VCPU_G_R0,  kvm_vcpu, arch.guest.gpu.regs[0]);
+	OFFSET(VCPU_G_R1,  kvm_vcpu, arch.guest.gpu.regs[1]);
+	OFFSET(VCPU_G_R2,  kvm_vcpu, arch.guest.gpu.regs[2]);
+	OFFSET(VCPU_G_R3,  kvm_vcpu, arch.guest.gpu.regs[3]);
+	OFFSET(VCPU_G_R4,  kvm_vcpu, arch.guest.gpu.regs[4]);
+	OFFSET(VCPU_G_R5,  kvm_vcpu, arch.guest.gpu.regs[5]);
+	OFFSET(VCPU_G_R6,  kvm_vcpu, arch.guest.gpu.regs[6]);
+	OFFSET(VCPU_G_R7,  kvm_vcpu, arch.guest.gpu.regs[7]);
+	OFFSET(VCPU_G_R8,  kvm_vcpu, arch.guest.gpu.regs[8]);
+	OFFSET(VCPU_G_R9,  kvm_vcpu, arch.guest.gpu.regs[9]);
+	OFFSET(VCPU_G_R10, kvm_vcpu, arch.guest.gpu.regs[10]);
+	OFFSET(VCPU_G_R11, kvm_vcpu, arch.guest.gpu.regs[11]);
+	OFFSET(VCPU_G_R12, kvm_vcpu, arch.guest.gpu.regs[12]);
+	OFFSET(VCPU_G_R13, kvm_vcpu, arch.guest.gpu.regs[13]);
+	OFFSET(VCPU_G_R14, kvm_vcpu, arch.guest.gpu.regs[14]);
+	OFFSET(VCPU_G_R15, kvm_vcpu, arch.guest.gpu.regs[15]);
+	OFFSET(VCPU_G_R16, kvm_vcpu, arch.guest.gpu.regs[16]);
+	OFFSET(VCPU_G_R17, kvm_vcpu, arch.guest.gpu.regs[17]);
+	OFFSET(VCPU_G_R18, kvm_vcpu, arch.guest.gpu.regs[18]);
+	OFFSET(VCPU_G_R19, kvm_vcpu, arch.guest.gpu.regs[19]);
+	OFFSET(VCPU_G_R20, kvm_vcpu, arch.guest.gpu.regs[20]);
+	OFFSET(VCPU_G_R21, kvm_vcpu, arch.guest.gpu.regs[21]);
+	OFFSET(VCPU_G_R22, kvm_vcpu, arch.guest.gpu.regs[22]);
+	OFFSET(VCPU_G_R23, kvm_vcpu, arch.guest.gpu.regs[23]);
+	OFFSET(VCPU_G_R24, kvm_vcpu, arch.guest.gpu.regs[24]);
+	OFFSET(VCPU_G_R25, kvm_vcpu, arch.guest.gpu.regs[25]);
+	OFFSET(VCPU_G_R26, kvm_vcpu, arch.guest.gpu.regs[26]);
+	OFFSET(VCPU_G_R27, kvm_vcpu, arch.guest.gpu.regs[27]);
+	OFFSET(VCPU_G_R28, kvm_vcpu, arch.guest.gpu.regs[28]);
+	OFFSET(VCPU_G_R29, kvm_vcpu, arch.guest.gpu.regs[29]);
+	OFFSET(VCPU_G_R30, kvm_vcpu, arch.guest.gpu.regs[30]);
+	OFFSET(VCPU_G_R31, kvm_vcpu, arch.guest.gpu.regs[31]);
+	OFFSET(VCPU_G_HI,  kvm_vcpu, arch.guest.gpu.hi);
+	OFFSET(VCPU_G_LO,  kvm_vcpu, arch.guest.gpu.lo);
+
+	OFFSET(VCPU_G_R_GUESTCTL0, kvm_vcpu, arch.guest.gpu.root_guestctl0);
+	OFFSET(VCPU_G_R_GUESTCTL1, kvm_vcpu, arch.guest.gpu.root_guestctl1);
+	OFFSET(VCPU_G_R_EPC,       kvm_vcpu, arch.guest.gpu.root_epc);
+	OFFSET(VCPU_G_R_COUNT,     kvm_vcpu, arch.guest.gpu.root_count);
+	OFFSET(VCPU_G_R_GTOFFSET,  kvm_vcpu, arch.guest.gpu.root_gtoffset);
+
+	OFFSET(VCPU_G_INDEX,    kvm_vcpu, arch.guest.gpu.guest_index);
+	OFFSET(VCPU_G_RANDOM,   kvm_vcpu, arch.guest.gpu.guest_random);
+	OFFSET(VCPU_G_ENTRYLO0, kvm_vcpu, arch.guest.gpu.guest_entrylo0);
+	OFFSET(VCPU_G_ENTRYLO1, kvm_vcpu, arch.guest.gpu.guest_entrylo1);
+	OFFSET(VCPU_G_CONTEXT,  kvm_vcpu, arch.guest.gpu.guest_context);
+	OFFSET(VCPU_G_USERLOCAL,kvm_vcpu, arch.guest.gpu.guest_userlocal);
+	OFFSET(VCPU_G_PAGEMASK, kvm_vcpu, arch.guest.gpu.guest_pagemask);
+	OFFSET(VCPU_G_PAGEGRAIN,kvm_vcpu, arch.guest.gpu.guest_pagegrain);
+	OFFSET(VCPU_G_PWBASE,   kvm_vcpu, arch.guest.gpu.guest_pwbase);
+	OFFSET(VCPU_G_PWFIELD,  kvm_vcpu, arch.guest.gpu.guest_pwfield);
+	OFFSET(VCPU_G_PWSIZE,   kvm_vcpu, arch.guest.gpu.guest_pwsize);
+	OFFSET(VCPU_G_WIRED,    kvm_vcpu, arch.guest.gpu.guest_wired);
+	OFFSET(VCPU_G_PWCTL,    kvm_vcpu, arch.guest.gpu.guest_pwctl);
+	OFFSET(VCPU_G_HWRENA,   kvm_vcpu, arch.guest.gpu.guest_hwrena);
+	OFFSET(VCPU_G_BADVADDR, kvm_vcpu, arch.guest.gpu.guest_badvaddr);
+	OFFSET(VCPU_G_EIRR,     kvm_vcpu, arch.guest.gpu.guest_eirr);
+	OFFSET(VCPU_G_EIMR,     kvm_vcpu, arch.guest.gpu.guest_eimr);
+	OFFSET(VCPU_G_ENTRYHI,  kvm_vcpu, arch.guest.gpu.guest_entryhi);
+	OFFSET(VCPU_G_COMPARE,  kvm_vcpu, arch.guest.gpu.guest_compare);
+	OFFSET(VCPU_G_STATUS,   kvm_vcpu, arch.guest.gpu.guest_status);
+	OFFSET(VCPU_G_INTCTL,   kvm_vcpu, arch.guest.gpu.guest_intctl);
+	OFFSET(VCPU_G_CAUSE,    kvm_vcpu, arch.guest.gpu.guest_cause);
+	OFFSET(VCPU_G_EPC,      kvm_vcpu, arch.guest.gpu.guest_epc);
+	OFFSET(VCPU_G_EBASE,    kvm_vcpu, arch.guest.gpu.guest_ebase);
+	OFFSET(VCPU_G_CONFIG0,  kvm_vcpu, arch.guest.gpu.guest_config0);
+	OFFSET(VCPU_G_CONFIG1,  kvm_vcpu, arch.guest.gpu.guest_config1);
+	OFFSET(VCPU_G_CONFIG4,  kvm_vcpu, arch.guest.gpu.guest_config4);
+	OFFSET(VCPU_G_XCONTEXT, kvm_vcpu, arch.guest.gpu.guest_xcontext);
+	OFFSET(VCPU_G_ERROREPC, kvm_vcpu, arch.guest.gpu.guest_errorepc);
+	OFFSET(VCPU_G_OSSCRATCH0, kvm_vcpu, arch.guest.gpu.guest_osscratch0);
+	OFFSET(VCPU_G_OSSCRATCH1, kvm_vcpu, arch.guest.gpu.guest_osscratch1);
+	OFFSET(VCPU_G_OSSCRATCH2, kvm_vcpu, arch.guest.gpu.guest_osscratch2);
+	OFFSET(VCPU_G_OSSCRATCH3, kvm_vcpu, arch.guest.gpu.guest_osscratch3);
+	OFFSET(VCPU_G_OSSCRATCH4, kvm_vcpu, arch.guest.gpu.guest_osscratch4);
+	OFFSET(VCPU_G_OSSCRATCH5, kvm_vcpu, arch.guest.gpu.guest_osscratch5);
+	OFFSET(VCPU_G_OSSCRATCH6, kvm_vcpu, arch.guest.gpu.guest_osscratch6);
+	OFFSET(VCPU_G_OSSCRATCH7, kvm_vcpu, arch.guest.gpu.guest_osscratch7);
+	OFFSET(VCPU_G_BADINSTR,   kvm_vcpu, arch.guest.gpu.guest_badinstr);
+	OFFSET(VCPU_G_BADINSTRP,  kvm_vcpu, arch.guest.gpu.guest_badinstrp);
+	OFFSET(VCPU_G_CONTEXTCONFIG,   kvm_vcpu, arch.guest.gpu.guest_contextconfig);
+	OFFSET(VCPU_G_XCONTEXTCONFIG,  kvm_vcpu, arch.guest.gpu.guest_xcontextconfig);
+
+	OFFSET(VCPU_G_WIRED_PAGEMASK_0, kvm_vcpu, arch.guest.gpu.guest_wired_pagemask[0]);
+	OFFSET(VCPU_G_WIRED_ENTRYHI_0,  kvm_vcpu, arch.guest.gpu.guest_wired_entryhi[0]);
+	OFFSET(VCPU_G_WIRED_ENTRYLO0_0, kvm_vcpu, arch.guest.gpu.guest_wired_entrylo0[0]);
+	OFFSET(VCPU_G_WIRED_ENTRYLO1_0, kvm_vcpu, arch.guest.gpu.guest_wired_entrylo1[0]);
+
+	OFFSET(VCPU_G_FP0,  kvm_vcpu, arch.guest.fpu.regs[0]);
+	OFFSET(VCPU_G_FP1,  kvm_vcpu, arch.guest.fpu.regs[1]);
+	OFFSET(VCPU_G_FP2,  kvm_vcpu, arch.guest.fpu.regs[2]);
+	OFFSET(VCPU_G_FP3,  kvm_vcpu, arch.guest.fpu.regs[3]);
+	OFFSET(VCPU_G_FP4,  kvm_vcpu, arch.guest.fpu.regs[4]);
+	OFFSET(VCPU_G_FP5,  kvm_vcpu, arch.guest.fpu.regs[5]);
+	OFFSET(VCPU_G_FP6,  kvm_vcpu, arch.guest.fpu.regs[6]);
+	OFFSET(VCPU_G_FP7,  kvm_vcpu, arch.guest.fpu.regs[7]);
+	OFFSET(VCPU_G_FP8,  kvm_vcpu, arch.guest.fpu.regs[8]);
+	OFFSET(VCPU_G_FP9,  kvm_vcpu, arch.guest.fpu.regs[9]);
+	OFFSET(VCPU_G_FP10, kvm_vcpu, arch.guest.fpu.regs[10]);
+	OFFSET(VCPU_G_FP11, kvm_vcpu, arch.guest.fpu.regs[11]);
+	OFFSET(VCPU_G_FP12, kvm_vcpu, arch.guest.fpu.regs[12]);
+	OFFSET(VCPU_G_FP13, kvm_vcpu, arch.guest.fpu.regs[13]);
+	OFFSET(VCPU_G_FP14, kvm_vcpu, arch.guest.fpu.regs[14]);
+	OFFSET(VCPU_G_FP15, kvm_vcpu, arch.guest.fpu.regs[15]);
+	OFFSET(VCPU_G_FP16, kvm_vcpu, arch.guest.fpu.regs[16]);
+	OFFSET(VCPU_G_FP17, kvm_vcpu, arch.guest.fpu.regs[17]);
+	OFFSET(VCPU_G_FP18, kvm_vcpu, arch.guest.fpu.regs[18]);
+	OFFSET(VCPU_G_FP19, kvm_vcpu, arch.guest.fpu.regs[19]);
+	OFFSET(VCPU_G_FP20, kvm_vcpu, arch.guest.fpu.regs[20]);
+	OFFSET(VCPU_G_FP21, kvm_vcpu, arch.guest.fpu.regs[21]);
+	OFFSET(VCPU_G_FP22, kvm_vcpu, arch.guest.fpu.regs[22]);
+	OFFSET(VCPU_G_FP23, kvm_vcpu, arch.guest.fpu.regs[23]);
+	OFFSET(VCPU_G_FP24, kvm_vcpu, arch.guest.fpu.regs[24]);
+	OFFSET(VCPU_G_FP25, kvm_vcpu, arch.guest.fpu.regs[25]);
+	OFFSET(VCPU_G_FP26, kvm_vcpu, arch.guest.fpu.regs[26]);
+	OFFSET(VCPU_G_FP27, kvm_vcpu, arch.guest.fpu.regs[27]);
+	OFFSET(VCPU_G_FP28, kvm_vcpu, arch.guest.fpu.regs[28]);
+	OFFSET(VCPU_G_FP29, kvm_vcpu, arch.guest.fpu.regs[29]);
+	OFFSET(VCPU_G_FP30, kvm_vcpu, arch.guest.fpu.regs[30]);
+	OFFSET(VCPU_G_FP31, kvm_vcpu, arch.guest.fpu.regs[31]);
+	OFFSET(VCPU_G_FIR,  kvm_vcpu, arch.guest.fpu.fir);
+	OFFSET(VCPU_G_FCCR, kvm_vcpu, arch.guest.fpu.fccr);
+	OFFSET(VCPU_G_FEXR, kvm_vcpu, arch.guest.fpu.fexr);
+	OFFSET(VCPU_G_FENR, kvm_vcpu, arch.guest.fpu.fenr);
+	OFFSET(VCPU_G_FCSR, kvm_vcpu, arch.guest.fpu.fcsr);
+
+	OFFSET(VCPU_R_S0,   kvm_vcpu, arch.root.sregs[0]);
+	OFFSET(VCPU_R_S1,   kvm_vcpu, arch.root.sregs[1]);
+	OFFSET(VCPU_R_S2,   kvm_vcpu, arch.root.sregs[2]);
+	OFFSET(VCPU_R_S3,   kvm_vcpu, arch.root.sregs[3]);
+	OFFSET(VCPU_R_S4,   kvm_vcpu, arch.root.sregs[4]);
+	OFFSET(VCPU_R_S5,   kvm_vcpu, arch.root.sregs[5]);
+	OFFSET(VCPU_R_S6,   kvm_vcpu, arch.root.sregs[6]);
+	OFFSET(VCPU_R_S7,   kvm_vcpu, arch.root.sregs[7]);
+	OFFSET(VCPU_R_GP,   kvm_vcpu, arch.root.gp);
+	OFFSET(VCPU_R_SP,   kvm_vcpu, arch.root.sp);
+	OFFSET(VCPU_R_FP,   kvm_vcpu, arch.root.fp);
+	OFFSET(VCPU_R_RA,   kvm_vcpu, arch.root.ra);
+	OFFSET(VCPU_R_FP0,  kvm_vcpu, arch.root.fpr[0]);
+	OFFSET(VCPU_R_FP1,  kvm_vcpu, arch.root.fpr[1]);
+	OFFSET(VCPU_R_FP2,  kvm_vcpu, arch.root.fpr[2]);
+	OFFSET(VCPU_R_FP3,  kvm_vcpu, arch.root.fpr[3]);
+	OFFSET(VCPU_R_FP4,  kvm_vcpu, arch.root.fpr[4]);
+	OFFSET(VCPU_R_FP5,  kvm_vcpu, arch.root.fpr[5]);
+	OFFSET(VCPU_R_FP6,  kvm_vcpu, arch.root.fpr[6]);
+	OFFSET(VCPU_R_FP7,  kvm_vcpu, arch.root.fpr[7]);
+	OFFSET(VCPU_R_FP8,  kvm_vcpu, arch.root.fpr[8]);
+	OFFSET(VCPU_R_FP9,  kvm_vcpu, arch.root.fpr[9]);
+	OFFSET(VCPU_R_FP10, kvm_vcpu, arch.root.fpr[10]);
+	OFFSET(VCPU_R_FP11, kvm_vcpu, arch.root.fpr[11]);
+	OFFSET(VCPU_R_FP12, kvm_vcpu, arch.root.fpr[12]);
+	OFFSET(VCPU_R_FP13, kvm_vcpu, arch.root.fpr[13]);
+	OFFSET(VCPU_R_FP14, kvm_vcpu, arch.root.fpr[14]);
+	OFFSET(VCPU_R_FP15, kvm_vcpu, arch.root.fpr[15]);
+	OFFSET(VCPU_R_FP16, kvm_vcpu, arch.root.fpr[16]);
+	OFFSET(VCPU_R_FP17, kvm_vcpu, arch.root.fpr[17]);
+	OFFSET(VCPU_R_FP18, kvm_vcpu, arch.root.fpr[18]);
+	OFFSET(VCPU_R_FP19, kvm_vcpu, arch.root.fpr[19]);
+	OFFSET(VCPU_R_FP20, kvm_vcpu, arch.root.fpr[20]);
+	OFFSET(VCPU_R_FP21, kvm_vcpu, arch.root.fpr[21]);
+	OFFSET(VCPU_R_FP22, kvm_vcpu, arch.root.fpr[22]);
+	OFFSET(VCPU_R_FP23, kvm_vcpu, arch.root.fpr[23]);
+	OFFSET(VCPU_R_FP24, kvm_vcpu, arch.root.fpr[24]);
+	OFFSET(VCPU_R_FP25, kvm_vcpu, arch.root.fpr[25]);
+	OFFSET(VCPU_R_FP26, kvm_vcpu, arch.root.fpr[26]);
+	OFFSET(VCPU_R_FP27, kvm_vcpu, arch.root.fpr[27]);
+	OFFSET(VCPU_R_FP28, kvm_vcpu, arch.root.fpr[28]);
+	OFFSET(VCPU_R_FP29, kvm_vcpu, arch.root.fpr[29]);
+	OFFSET(VCPU_R_FP30, kvm_vcpu, arch.root.fpr[30]);
+	OFFSET(VCPU_R_FP31, kvm_vcpu, arch.root.fpr[31]);
+	OFFSET(VCPU_R_FCR,  kvm_vcpu, arch.root.fcr);
+	OFFSET(VCPU_R_STATUS, kvm_vcpu, arch.root.status);
+
+	OFFSET(VCPU_ARCH_PIP_VECTOR, kvm_vcpu, arch.pip_vector);
+	OFFSET(VCPU_ARCH_HOST_STACK, kvm_vcpu, arch.host_stack);
+	OFFSET(VCPU_ARCH_INIT_GUEST, kvm_vcpu, arch.init_guest);
+	OFFSET(VCPU_ARCH_HVA_PGD, kvm_vcpu, arch.hva_pgd);
+	OFFSET(VCPU_ARCH_GPA_PGD, kvm_vcpu, arch.gpa_pgd);
+	OFFSET(VCPU_KVM, kvm_vcpu, kvm);
+	OFFSET(KVM_ARCH_EXIT_REQUEST, kvm, arch.exit_request);
+#else
 	COMMENT(" KVM/MIPS Specfic offsets. ");
 	DEFINE(VCPU_ARCH_SIZE, sizeof(struct kvm_vcpu_arch));
 	OFFSET(VCPU_RUN, kvm_vcpu, run);
@@ -401,5 +689,6 @@ void output_kvm_defines(void)
 
 	OFFSET(COP0_TLB_HI, mips_coproc, reg[MIPS_CP0_TLB_HI][0]);
 	OFFSET(COP0_STATUS, mips_coproc, reg[MIPS_CP0_STATUS][0]);
+#endif
 	BLANK();
 }
diff --git a/arch/mips/kernel/genex.S b/arch/mips/kernel/genex.S
index e93b4ee..c8c6687 100644
--- a/arch/mips/kernel/genex.S
+++ b/arch/mips/kernel/genex.S
@@ -49,6 +49,10 @@ NESTED(except_vec3_generic, 0, sp)
 #ifdef CONFIG_CPU_XLP
 	_ehb
 #endif
+#ifdef CONFIG_KVM_NETL
+	dmtc0	k0, $31, 2
+	dmtc0	k1, $31, 3
+#endif
 #if R5432_CP0_INTERRUPT_WAR
 	mfc0	k0, CP0_INDEX
 #endif
@@ -155,6 +159,11 @@ LEAF(__r4k_wait)
 	FEXPORT(rollback_\handler)
 	.set	push
 	.set	noat
+#ifdef CONFIG_KVM_NETL
+	MFC0	k0, $12, 6
+	srl	k0, 31
+	bnez	k0, 9f
+#endif
 	MFC0	k0, CP0_EPC
 	PTR_LA	k1, __r4k_wait
 	ori	k0, 0x1f	/* 32 byte rollback region */
@@ -484,6 +493,9 @@ NESTED(nmi_handler, PT_SIZE, sp)
 	BUILD_HANDLER mcheck mcheck cli verbose		/* #24 */
 	BUILD_HANDLER mt mt sti silent			/* #25 */
 	BUILD_HANDLER dsp dsp sti silent		/* #26 */
+#ifdef CONFIG_KVM_NETL
+	BUILD_HANDLER virt virt sti silent		/* #27 */
+#endif
 	BUILD_HANDLER reserved reserved sti verbose	/* others */
 
 	.align	5
diff --git a/arch/mips/kernel/r4k_switch.S b/arch/mips/kernel/r4k_switch.S
index 4381dc3..26e69bb 100644
--- a/arch/mips/kernel/r4k_switch.S
+++ b/arch/mips/kernel/r4k_switch.S
@@ -68,6 +68,10 @@
 						# clobbers t1
 1:
 
+#ifdef CONFIG_KVM_NETL
+	cpu_save_vm_state a0
+#endif
+
 	/*
 	 * The order of restoring the registers takes care of the race
 	 * updating $28, $29 and kernelsp without disabling ints.
@@ -101,6 +105,9 @@
 	and	a2, a3
 	or	a2, t1
 	mtc0	a2, CP0_STATUS
+#ifdef CONFIG_KVM_NETL
+	cpu_restore_vm_state a1
+#endif
 #ifdef CONFIG_MIPS_MT_SMTC
 	_ehb
 	andi	t0, t0, VPECONTROL_TE
diff --git a/arch/mips/kernel/traps.c b/arch/mips/kernel/traps.c
index 2bcba80..309dfc8 100644
--- a/arch/mips/kernel/traps.c
+++ b/arch/mips/kernel/traps.c
@@ -80,9 +80,15 @@ extern asmlinkage void handle_mdmx(void);
 extern asmlinkage void handle_watch(void);
 extern asmlinkage void handle_mt(void);
 extern asmlinkage void handle_dsp(void);
+#ifdef CONFIG_KVM_NETL
+extern asmlinkage void handle_virt(void);
+#endif
 extern asmlinkage void handle_mcheck(void);
 extern asmlinkage void handle_reserved(void);
 
+#ifdef CONFIG_KVM_NETL
+extern void process_virt_exception(struct pt_regs *);
+#endif
 void (*board_be_init)(void);
 int (*board_be_handler)(struct pt_regs *regs, int is_fixup);
 void (*board_nmi_handler_setup)(void);
@@ -227,6 +233,12 @@ static void __show_regs(const struct pt_regs *regs)
 	unsigned int cause = regs->cp0_cause;
 	int i;
 
+#ifdef CONFIG_KVM_NETL
+	if (regs->cp0_guestctl0 >> 31)
+		printk("Exception in KVM: due to guest ...\n");
+	else
+		printk("Exception in KVM: due to root ...\n");
+#endif
 	show_regs_print_info(KERN_DEFAULT);
 
 	/*
@@ -256,11 +268,21 @@ static void __show_regs(const struct pt_regs *regs)
 	/*
 	 * Saved cp0 registers
 	 */
+#ifdef CONFIG_KVM_NETL
+	if (regs->cp0_guestctl0 >> 31) {
+		printk("epc   : %0*lx\n", field, regs->cp0_epc);
+		printk("    %s\n", print_tainted());
+		printk("ra    : %0*lx\n", field, regs->regs[31]);
+	} else {
+#endif
 	printk("epc   : %0*lx %pS\n", field, regs->cp0_epc,
 	       (void *) regs->cp0_epc);
 	printk("    %s\n", print_tainted());
 	printk("ra    : %0*lx %pS\n", field, regs->regs[31],
 	       (void *) regs->regs[31]);
+#ifdef CONFIG_KVM_NETL
+	}
+#endif
 
 	printk("Status: %08x	", (uint32_t) regs->cp0_status);
 
@@ -315,6 +337,15 @@ static void __show_regs(const struct pt_regs *regs)
 
 	printk("PrId  : %08x (%s)\n", read_c0_prid(),
 	       cpu_name_string());
+
+#ifdef CONFIG_KVM_NETL
+	if (regs->cp0_guestctl0 >> 31) {
+		printk("Badinstr  : %08x\n", (uint32_t)regs->cp0_badinstr);
+		printk("Badinstrp : %08x\n", (uint32_t)regs->cp0_badinstrp);
+		printk("GuestCtl0 : %08x\n", (uint32_t)regs->cp0_guestctl0);
+		printk("GuestCtl1 : %08x\n", (uint32_t)regs->cp0_guestctl1);
+	}
+#endif
 }
 
 /*
@@ -342,9 +373,17 @@ void show_registers(struct pt_regs *regs)
 			printk("*HwTLS: %0*lx\n", field, tls);
 	}
 
+#ifdef CONFIG_KVM_NETL
+	if (!(regs->cp0_guestctl0 >> 31)) {
+		show_stacktrace(current, regs);
+		show_code((unsigned int __user *) regs->cp0_epc);
+		printk("\n");
+	}
+#else
 	show_stacktrace(current, regs);
 	show_code((unsigned int __user *) regs->cp0_epc);
 	printk("\n");
+#endif
 }
 
 static int regs_to_trapnr(struct pt_regs *regs)
@@ -1243,6 +1282,13 @@ asmlinkage void do_dsp(struct pt_regs *regs)
 	force_sig(SIGILL, current);
 }
 
+#ifdef CONFIG_KVM_NETL
+asmlinkage void do_virt(struct pt_regs *regs)
+{
+	process_virt_exception(regs);
+}
+#endif
+
 asmlinkage void do_reserved(struct pt_regs *regs)
 {
 	/*
@@ -1480,8 +1526,15 @@ void __init *set_except_vector(int n, void *addr)
 #endif
 		u32 *buf = (u32 *)(ebase + 0x200);
 		unsigned int k0 = 26;
+#ifdef CONFIG_KVM_NETL
+		unsigned int k1 = 27;
+#endif
 		if (current_cpu_type() == CPU_XLP)
 			uasm_i_ehb(&buf);
+#ifdef CONFIG_KVM_NETL
+		uasm_i_dmtc0(&buf, k0, 31, 2);
+		uasm_i_dmtc0(&buf, k1, 31, 3);
+#endif
 		if ((handler & jump_mask) == ((ebase + 0x200) & jump_mask)) {
 			uasm_i_j(&buf, handler & ~jump_mask);
 			uasm_i_nop(&buf);
@@ -1947,6 +2000,10 @@ void __init trap_init(void)
 
 	set_except_vector(26, handle_dsp);
 
+#ifdef CONFIG_KVM_NETL
+	set_except_vector(27, handle_virt);
+#endif
+
 	if (board_cache_error_setup)
 		board_cache_error_setup();
 
diff --git a/arch/mips/kvm-netl/Kconfig b/arch/mips/kvm-netl/Kconfig
new file mode 100644
index 0000000..a98e5e5
--- /dev/null
+++ b/arch/mips/kvm-netl/Kconfig
@@ -0,0 +1,45 @@
+#
+# KVM configuration
+#
+
+source "virt/kvm/Kconfig"
+
+menuconfig VIRTUALIZATION
+	bool "Virtualization"
+	---help---
+	  Say Y here to get to see options for using your Linux host to run
+	  other operating systems inside virtual machines (guests).
+	  This option alone does not add any kernel code.
+
+	  If you say N, all options in this submenu will be skipped and
+	  disabled.
+
+if VIRTUALIZATION
+
+config KVM
+	bool "Kernel Virtual Machine support"
+	select PREEMPT_NOTIFIERS
+	select ANON_INODES
+	select HAVE_KVM_EVENTFD
+	select HAVE_KVM_IRQCHIP
+	select HAVE_KVM_IRQ_ROUTING
+	select KVM_DEVICE_ASSIGNMENT
+	select KVM_NETL
+
+config KVM_DEVICE_ASSIGNMENT
+	bool "KVM legacy PCI device assignment support"
+	depends on KVM && PCI && IOMMU_API
+	default y
+	---help---
+	  Provide support for legacy PCI device assignment through KVM.  The
+	  kernel now also supports a full featured userspace device driver
+	  framework through VFIO, which supersedes much of this support.
+
+	  If unsure, say Y.
+
+config KVM_NETL
+	bool
+
+source drivers/vhost/Kconfig
+
+endif
diff --git a/arch/mips/kvm-netl/Makefile b/arch/mips/kvm-netl/Makefile
new file mode 100644
index 0000000..2505b87
--- /dev/null
+++ b/arch/mips/kvm-netl/Makefile
@@ -0,0 +1,10 @@
+# Makefile for kernel virtual machines on xlp
+obj-$(CONFIG_KVM_NETL) := kvm_xlp.o
+
+ccflags-y := -Ivirt/kvm
+
+common-objs = $(addprefix ../../../virt/kvm/, kvm_main.o eventfd.o)
+
+kvm_xlp-y += $(common-objs) kvm.o xlp.o context_switch.o kvm_vhost.o
+
+obj-$(CONFIG_IOMMU_API) += kvm_iommu.o
diff --git a/arch/mips/kvm-netl/context_switch.S b/arch/mips/kvm-netl/context_switch.S
new file mode 100644
index 0000000..ceb683b
--- /dev/null
+++ b/arch/mips/kvm-netl/context_switch.S
@@ -0,0 +1,328 @@
+/*
+ * Copyright (c) 2003-2013 Broadcom Corporation
+ * All Rights Reserved
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the Broadcom
+ * license below:
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in
+ *    the documentation and/or other materials provided with the
+ *    distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY BROADCOM ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL BROADCOM OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
+ * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
+ * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
+ * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/**
+ * The context switch between guest and hypervisor.
+ *
+ * What we need to save and restore? For guest, we need to restore
+ *   . all the general purpose registers (gpr and cop1)
+ *   . the cop2 registers
+ *   . the guest cop0 registers
+ */
+
+#include <asm/asm.h>
+#include <asm/asmmacro.h>
+#include <asm/regdef.h>
+#include <asm/mipsregs.h>
+#include <asm/stackframe.h>
+
+	.macro  kvm_cpu_save_nonscratch savearea
+	LONG_S  s0, VCPU_R_S0(\savearea)
+	LONG_S  s1, VCPU_R_S1(\savearea)
+	LONG_S  s2, VCPU_R_S2(\savearea)
+	LONG_S  s3, VCPU_R_S3(\savearea)
+	LONG_S  s4, VCPU_R_S4(\savearea)
+	LONG_S  s5, VCPU_R_S5(\savearea)
+	LONG_S  s6, VCPU_R_S6(\savearea)
+	LONG_S  s7, VCPU_R_S7(\savearea)
+	LONG_S  gp, VCPU_R_GP(\savearea)
+	LONG_S  sp, VCPU_R_SP(\savearea)
+	LONG_S  fp, VCPU_R_FP(\savearea)
+	LONG_S  ra, VCPU_R_RA(\savearea)
+	.endm
+
+	.macro  kvm_cpu_restore_nonscratch restorearea
+	LONG_L  s0, VCPU_R_S0(\restorearea)
+	LONG_L  s1, VCPU_R_S1(\restorearea)
+	LONG_L  s2, VCPU_R_S2(\restorearea)
+	LONG_L  s3, VCPU_R_S3(\restorearea)
+	LONG_L  s4, VCPU_R_S4(\restorearea)
+	LONG_L  s5, VCPU_R_S5(\restorearea)
+	LONG_L  s6, VCPU_R_S6(\restorearea)
+	LONG_L  s7, VCPU_R_S7(\restorearea)
+	LONG_L  gp, VCPU_R_GP(\restorearea)
+	LONG_L  sp, VCPU_R_SP(\restorearea)
+	LONG_L  fp, VCPU_R_FP(\restorearea)
+	LONG_L  ra, VCPU_R_RA(\restorearea)
+	.endm
+
+	.macro	kvm_cpu_restore_guest_wired_tlbs restorearea
+	LONG_L	t0, VCPU_G_WIRED(\restorearea)
+	li	t1, 0
+  49:
+	slt	t2, t1, t0
+	beqz	t2, 50f
+
+	sll	t1, t1, 3
+
+	daddiu	t3, \restorearea, VCPU_G_WIRED_PAGEMASK_0
+	daddu	t3, t3, t1
+	LONG_L	t2, 0(t3)
+	dmtgc0	t2, $5, 0
+
+	daddiu	t3, \restorearea, VCPU_G_WIRED_ENTRYHI_0
+	daddu	t3, t3, t1
+	LONG_L	t2, 0(t3)
+	mtgc0	t2, $10, 0
+
+	daddiu	t3, \restorearea, VCPU_G_WIRED_ENTRYLO0_0
+	daddu	t3, t3, t1
+	LONG_L	t2, 0(t3)
+	mtgc0	t2, $2, 0
+
+	daddiu	t3, \restorearea, VCPU_G_WIRED_ENTRYLO1_0
+	daddu	t3, t3, t1
+	LONG_L	t2, 0(t3)
+	mtgc0	t2, $3, 0
+
+	srl	t1, t1, 3
+
+	mtgc0	t1, $0, 0
+	tlbgwi
+
+	addiu	t1, t1, 1
+	j	49b
+
+  50:
+	.endm
+
+	.macro	kvm_cpu_restore_guest_cp0 restorearea
+	LONG_L	t0, VCPU_G_INDEX(\restorearea)
+	mtgc0	t0, $0, 0
+	LONG_L	t0, VCPU_G_RANDOM(\restorearea)
+	mtgc0	t0, $1, 0
+	LONG_L	t0, VCPU_G_ENTRYLO0(\restorearea)
+	mtgc0	t0, $2, 0
+	LONG_L	t0, VCPU_G_ENTRYLO1(\restorearea)
+	mtgc0	t0, $3, 0
+	LONG_L	t0, VCPU_G_CONTEXT(\restorearea)
+	mtgc0	t0, $4, 0
+	LONG_L	t0, VCPU_G_USERLOCAL(\restorearea)
+	mtgc0	t0, $4, 2
+	LONG_L	t0, VCPU_G_PAGEMASK(\restorearea)
+	dmtgc0	t0, $5, 0
+	LONG_L	t0, VCPU_G_PAGEGRAIN(\restorearea)
+	mtgc0	t0, $5, 1
+	LONG_L	t0, VCPU_G_PWBASE(\restorearea)
+	mtgc0	t0, $5, 5
+	LONG_L	t0, VCPU_G_PWFIELD(\restorearea)
+	mtgc0	t0, $5, 6
+	LONG_L	t0, VCPU_G_PWSIZE(\restorearea)
+	mtgc0	t0, $5, 7
+	LONG_L	t0, VCPU_G_WIRED(\restorearea)
+	mtgc0	t0, $6, 0
+	LONG_L	t0, VCPU_G_PWCTL(\restorearea)
+	mtgc0	t0, $6, 6
+	LONG_L	t0, VCPU_G_HWRENA(\restorearea)
+	mtgc0	t0, $7, 0
+	LONG_L	t0, VCPU_G_BADVADDR(\restorearea)
+	mtgc0	t0, $8, 0
+	LONG_L	t0, VCPU_G_EIRR(\restorearea)
+	mtgc0	t0, $9, 6
+	LONG_L	t0, VCPU_G_EIMR(\restorearea)
+	mtgc0	t0, $9, 7
+	LONG_L	t0, VCPU_G_ENTRYHI(\restorearea)
+	mtgc0	t0, $10, 0
+	LONG_L	t0, VCPU_G_COMPARE(\restorearea)
+	mtgc0	t0, $11, 0
+	LONG_L	t0, VCPU_G_STATUS(\restorearea)
+	mtgc0	t0, $12, 0
+	LONG_L	t0, VCPU_G_INTCTL(\restorearea)
+	mtgc0	t0, $12, 1
+	LONG_L	t0, VCPU_G_CAUSE(\restorearea)
+	mtgc0	t0, $13, 0
+	LONG_L	t0, VCPU_G_EPC(\restorearea)
+	mtgc0	t0, $14, 0
+	LONG_L	t0, VCPU_G_EBASE(\restorearea)
+	mtgc0	t0, $15, 1
+	LONG_L	t0, VCPU_G_CONFIG0(\restorearea)
+	mtgc0	t0, $16, 0
+	LONG_L	t0, VCPU_G_CONFIG1(\restorearea)
+	mtgc0	t0, $16, 1
+	LONG_L	t0, VCPU_G_CONFIG4(\restorearea)
+	mtgc0	t0, $16, 4
+	LONG_L	t0, VCPU_G_XCONTEXT(\restorearea)
+	mtgc0	t0, $20, 0
+	LONG_L	t0, VCPU_G_ERROREPC(\restorearea)
+	mtgc0	t0, $30, 0
+	LONG_L	t0, VCPU_G_OSSCRATCH0(\restorearea)
+	mtgc0	t0, $22, 0
+	LONG_L	t0, VCPU_G_OSSCRATCH1(\restorearea)
+	mtgc0	t0, $22, 1
+	LONG_L	t0, VCPU_G_OSSCRATCH2(\restorearea)
+	mtgc0	t0, $22, 2
+	LONG_L	t0, VCPU_G_OSSCRATCH3(\restorearea)
+	mtgc0	t0, $22, 3
+	LONG_L	t0, VCPU_G_OSSCRATCH4(\restorearea)
+	mtgc0	t0, $22, 4
+	LONG_L	t0, VCPU_G_OSSCRATCH5(\restorearea)
+	mtgc0	t0, $22, 5
+	LONG_L	t0, VCPU_G_OSSCRATCH6(\restorearea)
+	mtgc0	t0, $22, 6
+	LONG_L	t0, VCPU_G_OSSCRATCH7(\restorearea)
+	mtgc0	t0, $22, 7
+	LONG_L	t0, VCPU_G_BADINSTR(\restorearea)
+	mtgc0	t0, $8, 1
+	LONG_L	t0, VCPU_G_BADINSTRP(\restorearea)
+	mtgc0	t0, $8, 2
+	LONG_L	t0, VCPU_G_CONTEXTCONFIG(\restorearea)
+	mtgc0	t0, $4, 1
+	LONG_L	t0, VCPU_G_XCONTEXTCONFIG(\restorearea)
+	mtgc0	t0, $4, 3
+	.endm
+
+FEXPORT(__kvm_vcpu_run_guest)
+	.set	noat
+
+	/* essentially, it is a context switch, we must
+	 * save all relevant registers, and then load
+	 * relevant guest registers.
+	 * a0: kvm_vcpu
+	 */
+	mfc0 	t1, CP0_STATUS
+	LONG_S	t1, VCPU_R_STATUS(a0)
+	kvm_cpu_save_nonscratch a0
+	move	a1, a0
+
+	/*
+	 * set the Root.status.EXL.
+	 * permit the guest CU1 and CU2 access.
+	 */
+	mfc0 t0, $12, 0
+	ori  t0, t0, 0x2
+	lui  t1, 0x6400
+	or   t0, t0, t1
+	mtc0 t0, $12, 0
+
+	/* kernel stack pointer for guest triggered exceptions */
+	LONG_S	sp, VCPU_ARCH_HOST_STACK(a0)
+	dmtc0	a0, $22, 7
+
+	/* restore the guestid. */
+	LONG_L	t0, VCPU_G_R_GUESTCTL1(a0)
+	mtc0	t0, $10, 4
+
+        /* Switch to gpa page table for refill and hardware page table */
+        dmfc0    t0, ASM_SMP_CPUID_REG
+        dsrl    t0, t0, SMP_CPUID_PTRSHIFT
+        PTR_LA  t1, pgd_current
+        daddu   t2, t1, t0
+
+        /* save HVA_PGD and restore GPA_PGD */
+        LONG_L  t1, 0(t2)
+        LONG_S  t1, VCPU_ARCH_HVA_PGD(a0)
+        LONG_L  t1, VCPU_ARCH_GPA_PGD(a0)
+        LONG_S  t1, 0(t2)
+
+        /* restore GPA_PGD for hardware page walker */
+        mfc0    t0, $6, 6
+        srl     t0, t0, 31
+        beqz    t0, 80f
+        dmfc0   t2, $5, 5
+        LONG_S  t1, 0(t2)
+        80:
+
+	/* Check whether we need to restore guest states. */
+	LONG_L	t0, VCPU_ARCH_INIT_GUEST(a0)
+	beqz	t0, 2f
+
+	/* intialize the guest state */
+	LONG_S	zero, VCPU_ARCH_INIT_GUEST(a0)
+
+	/* flush the root/guest tlb, restore wired tlb, restore guest cp0 */
+	tlbginvf
+	tlbinvf
+	kvm_cpu_restore_guest_wired_tlbs a1
+	kvm_cpu_restore_guest_cp0 a1
+
+2:
+	/* CuestCtl0 & EPC */
+	LONG_L	t0, VCPU_G_R_GUESTCTL0(a1)
+	mtc0	t0, $12, 6
+	LONG_L	t0, VCPU_G_R_EPC(a1)
+	dmtc0	t0, $14, 0
+
+	/* general purpose registers */
+	move	sp, a1
+	LONG_L 	$1,  VCPU_G_R1(sp)
+	LONG_L 	$2,  VCPU_G_R2(sp)
+	LONG_L 	$3,  VCPU_G_R3(sp)
+	LONG_L 	$4,  VCPU_G_R4(sp)
+	LONG_L 	$5,  VCPU_G_R5(sp)
+	LONG_L 	$6,  VCPU_G_R6(sp)
+	LONG_L 	$7,  VCPU_G_R7(sp)
+	LONG_L 	$8,  VCPU_G_R8(sp)
+	LONG_L 	$9,  VCPU_G_R9(sp)
+	LONG_L 	$10, VCPU_G_R10(sp)
+	LONG_L 	$11, VCPU_G_R11(sp)
+	LONG_L 	$12, VCPU_G_R12(sp)
+	LONG_L 	$13, VCPU_G_R13(sp)
+	LONG_L 	$14, VCPU_G_R14(sp)
+	LONG_L 	$15, VCPU_G_R15(sp)
+	LONG_L 	$16, VCPU_G_R16(sp)
+	LONG_L 	$17, VCPU_G_R17(sp)
+	LONG_L 	$18, VCPU_G_R18(sp)
+	LONG_L 	$19, VCPU_G_R19(sp)
+	LONG_L 	$20, VCPU_G_R20(sp)
+	LONG_L 	$21, VCPU_G_R21(sp)
+	LONG_L 	$22, VCPU_G_R22(sp)
+	LONG_L 	$23, VCPU_G_R23(sp)
+	LONG_L 	$24, VCPU_G_R24(sp)
+	LONG_L 	$25, VCPU_G_R25(sp)
+	LONG_L 	$28, VCPU_G_R28(sp)
+	LONG_L 	$30, VCPU_G_R30(sp)
+	LONG_L 	$31, VCPU_G_R31(sp)
+	LONG_L 	k0,  VCPU_G_HI(sp)
+	mthi 	k0
+	LONG_L 	k0,  VCPU_G_LO(sp)
+	mtlo 	k0
+
+	LONG_L 	$26, VCPU_G_R26(sp)
+	LONG_L 	$27, VCPU_G_R27(sp)
+	LONG_L 	$29, VCPU_G_R29(sp)
+
+	/* jump to the guest land */
+	eret
+
+FEXPORT(__kvm_vcpu_leave_guest)
+
+	/* Leave the guest and return to QEMU for service,
+	 * a0: kvm_vcpu_host
+	 * a1: exit_reason and exit information
+	 */
+	kvm_cpu_restore_nonscratch a0
+	LONG_L	t1, VCPU_R_STATUS(a0)
+	mtc0 	t1, CP0_STATUS
+	move	v0, a1
+	jr	ra
diff --git a/arch/mips/kvm-netl/kvm.c b/arch/mips/kvm-netl/kvm.c
new file mode 100644
index 0000000..7f6cc67
--- /dev/null
+++ b/arch/mips/kvm-netl/kvm.c
@@ -0,0 +1,1015 @@
+/*
+ * Copyright (c) 2003-2013 Broadcom Corporation
+ * All Rights Reserved
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the Broadcom
+ * license below:
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in
+ *    the documentation and/or other materials provided with the
+ *    distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY BROADCOM ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL BROADCOM OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
+ * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
+ * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
+ * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/errno.h>
+#include <linux/err.h>
+#include <linux/vmalloc.h>
+#include <linux/file.h>
+#include <linux/kvm.h>
+#include <linux/kvm_host.h>
+#include <linux/anon_inodes.h>
+
+#include <linux/kvm_host.h>
+#include <linux/kvm.h>
+#include <linux/module.h>
+#include <linux/errno.h>
+#include <linux/percpu.h>
+#include <linux/gfp.h>
+#include <linux/mm.h>
+#include <linux/miscdevice.h>
+#include <linux/vmalloc.h>
+#include <linux/reboot.h>
+#include <linux/debugfs.h>
+#include <linux/highmem.h>
+#include <linux/file.h>
+#include <linux/cpu.h>
+#include <linux/sched.h>
+#include <linux/cpumask.h>
+#include <linux/smp.h>
+#include <linux/anon_inodes.h>
+#include <linux/profile.h>
+#include <linux/kvm_para.h>
+#include <linux/pagemap.h>
+#include <linux/mman.h>
+#include <linux/swap.h>
+#include <linux/bitops.h>
+#include <linux/spinlock.h>
+#include <linux/perf_event.h>
+
+#include <asm/pgalloc.h>
+#include <asm/mmu_context.h>
+
+#include <asm/netlogic/haldefs.h>
+#include <asm/netlogic/common.h>
+#include <asm/netlogic/kvm_xlp.h>
+#include <asm/netlogic/mips-extns.h>
+#include <asm/netlogic/xlp-hal/iomap.h>
+#include <asm/netlogic/xlp-hal/xlp.h>
+#include <asm/netlogic/xlp-hal/pic.h>
+
+struct kvm_stats_debugfs_item debugfs_entries[] = {
+	{ NULL }
+};
+
+static void kvm_free_vcpus(struct kvm *kvm)
+{
+	unsigned int i;
+	struct kvm_vcpu *vcpu;
+
+	kvm_for_each_vcpu(i, vcpu, kvm)
+		kvm_arch_vcpu_destroy(vcpu);
+
+	mutex_lock(&kvm->lock);
+	for (i = 0; i < atomic_read(&kvm->online_vcpus); i++)
+		kvm->vcpus[i] = NULL;
+	atomic_set(&kvm->online_vcpus, 0);
+	mutex_unlock(&kvm->lock);
+}
+
+static void kvm_flush_rtlb(void *args)
+{
+	int tmp0, tmp1, guestid = (int)(long)args;
+
+	guestid <<=  16; /* rid */
+	__asm__ __volatile__ (
+		".set push		\n"
+		".set noreorder		\n"
+		"mfc0	%0, $10, 4	\n"
+		"move	%1, %2		\n"
+		"mtc0	%1, $10, 4	\n"
+		"tlbinvf		\n"
+		"mtc0	%0, $10, 4	\n"
+		".set pop		\n"
+		:"=&r"(tmp0), "=&r"(tmp1)
+		:"r"(guestid)
+	);
+
+	return;
+}
+
+static void kvm_populate_gpa_map(pgd_t *gpa_pgd, uint64_t msize,
+	uint64_t gpa, uint64_t hva)
+{
+	uint64_t address, pa;
+
+#if 0
+	printk("KVM: populate guest memory: hva = 0x%llx, gpa = 0x%llx, size = 0x%llx\n",
+		hva, gpa, msize);
+#endif
+
+	for (address = hva; address < (hva + msize);) {
+		pgd_t *pgdp;
+		pud_t *pudp;
+		pmd_t *pmdp;
+		pte_t *ptep, pte;
+		int i;
+		unsigned long *ptr;
+
+		pgdp = current->mm->pgd + __pgd_offset(address);
+		pudp = pud_offset(pgdp, address);
+		pmdp = pmd_offset(pudp, address);
+		ptep = pte_offset(pmdp, address);
+		pte = *	ptep;
+
+		/* map gpa to pte in page table pointed to by gpa_pgd */
+		pgdp = (pgd_t *)gpa_pgd + __pgd_offset(gpa);
+		pudp = (pud_t *)pgdp;
+#ifndef __PAGETABLE_PMD_FOLDED
+		if (*(unsigned long *)pudp == (unsigned long)invalid_pmd_table) {
+			/* allocate a page for pmd and initialize it */
+			ptr = (unsigned long *)__get_free_page(GFP_KERNEL|GFP_DMA);
+			for (i = 0; i < PTRS_PER_PMD; i++)
+				ptr[i] = (unsigned long)invalid_pte_table;
+			*(unsigned long *)pudp = (unsigned long)ptr;
+		}
+		pmdp = (pmd_t *)*(unsigned long *)pudp + __pmd_offset(gpa);
+		if (*(unsigned long *)pmdp == (unsigned long)invalid_pte_table) {
+			/* allocate a page for pmd and initialize it */
+			ptr = (unsigned long *)__get_free_page(GFP_KERNEL|GFP_DMA);
+			for (i = 0; i < PTRS_PER_PTE; i++)
+				ptr[i] = 0;
+			*(unsigned long *)pmdp = (unsigned long)ptr;
+		}
+		ptep = (pte_t *)*(unsigned long *)pmdp + __pte_offset(gpa);
+#else
+		if (*(unsigned long *)pudp == (unsigned long)invalid_pte_table) {
+			/* allocate a page for pmd and initialize it */
+			ptr = (unsigned long *)__get_free_page(GFP_KERNEL|GFP_DMA);
+			for (i = 0; i < PTRS_PER_PTE; i++)
+				ptr[i] = 0;
+			*(unsigned long *)pudp = (unsigned long)ptr;
+		}
+		ptep = (pte_t *)*(unsigned long *)pudp + __pte_offset(gpa);
+#endif
+		*ptep = pte;
+
+		/* flush the Reset Vector region as it will be accessed as uncached. */
+		if ((gpa & 0xfffffffffff00000ULL) == 0x1fc00000) {
+			if ((pte_val(pte) >> _PAGE_PRESENT_SHIFT) & 0x1) {
+				pa = (pte_val(pte) >> (_PAGE_GLOBAL_SHIFT + 6)) << 12;
+				nlm_flush_cache_L2L3(pa, PAGE_SIZE);
+			}
+		}
+
+		address += PAGE_SIZE, gpa += PAGE_SIZE;
+	}
+}
+
+/* map gpa->pa and also flush references for pa in the cache to provide a clean memory
+ * for guest.
+ */
+static void kvm_sync_gpa_map(struct kvm *kvm)
+{
+	int t;
+	pgd_t *gpa_pgd;
+
+	kvm->arch.mem_synced = 1; /* sync is called */
+
+	gpa_pgd = (pgd_t *)kvm->arch.gpa_pgd;
+
+	for (t = 0; t < KVM_USER_MEM_SLOTS; t++) {
+		struct kvm_memory_slot *s = &kvm->memslots->memslots[t];
+		uint64_t msize, gpa, hva;
+
+		if (!s->npages)
+			continue;
+
+		msize = s->npages << PAGE_SHIFT;
+		gpa = s->base_gfn << PAGE_SHIFT;
+		hva = s->userspace_addr;
+
+		/* enumerate all hva pages */
+		kvm_populate_gpa_map(gpa_pgd, msize, gpa, hva);
+		kvm->arch.slot_inited |= (1 << t); /* slot has been populated into gpa->pa table */
+	}
+
+	/* packet memory */
+	if (kvm->arch.pktmem.gpa_start != (-1)) {
+		uint64_t msize, gpa, address, pa;
+
+		msize = kvm->arch.pktmem.size;
+		gpa = kvm->arch.pktmem.gpa_start;
+		pa = kvm->arch.pktmem.rpa_start;
+
+		/* enumerate all hva pages */
+		for (address = gpa; address < (gpa + msize);) {
+			pgd_t *pgdp;
+			pud_t *pudp;
+#ifndef __PAGETABLE_PMD_FOLDED
+			pmd_t *pmdp;
+#endif
+			pte_t *ptep;
+			int i;
+			unsigned long *ptr;
+
+			/* map gpa to pte in page table pointed to by gpa_pgd */
+			pgdp = (pgd_t *)gpa_pgd + __pgd_offset(address);
+			pudp = (pud_t *)pgdp;
+#ifndef __PAGETABLE_PMD_FOLDED
+			if (*(unsigned long *)pudp == (unsigned long)invalid_pmd_table) {
+				/* allocate a page for pmd and initialize it */
+				ptr = (unsigned long *)__get_free_page(GFP_KERNEL|GFP_DMA);
+				for (i = 0; i < PTRS_PER_PMD; i++)
+					ptr[i] = (unsigned long)invalid_pte_table;
+				*(unsigned long *)pudp = (unsigned long)ptr;
+
+			}
+			pmdp = (pmd_t *)*(unsigned long *)pudp + __pmd_offset(address);
+			if (*(unsigned long *)pmdp == (unsigned long)invalid_pte_table) {
+				/* allocate a page for pmd and initialize it */
+				ptr = (unsigned long *)__get_free_page(GFP_KERNEL|GFP_DMA);
+				for (i = 0; i < PTRS_PER_PTE; i++)
+					ptr[i] = 0;
+				*(unsigned long *)pmdp = (unsigned long)ptr;
+			}
+			ptep = (pte_t *)*(unsigned long *)pmdp + __pte_offset(address);
+#else
+			if (*(unsigned long *)pudp == (unsigned long)invalid_pte_table) {
+				/* allocate a page for pmd and initialize it */
+				ptr = (unsigned long *)__get_free_page(GFP_KERNEL|GFP_DMA);
+				for (i = 0; i < PTRS_PER_PTE; i++)
+					ptr[i] = 0;
+				*(unsigned long *)pudp = (unsigned long)ptr;
+
+			}
+			ptep = (pte_t *)*(unsigned long *)pudp + __pte_offset(address);
+#endif
+			*ptep = __pte(((pa >> 12) << (_PAGE_GLOBAL_SHIFT + 6))| pgprot_val(PAGE_SHARED));
+
+			address += PAGE_SIZE, pa += PAGE_SIZE;
+		}
+	}
+
+	return;
+}
+
+static void kvm_sync_dirty_log(struct kvm *kvm)
+{
+	int t;
+	pgd_t *gpa_pgd;
+
+	gpa_pgd = (pgd_t *)kvm->arch.gpa_pgd;
+
+	for (t = 0; t < KVM_USER_MEM_SLOTS; t++) {
+		struct kvm_memory_slot *s = &kvm->memslots->memslots[t];
+		uint64_t msize, gpa, address;
+
+		if (!s->npages)
+			continue;
+
+		if (!(s->flags & KVM_MEM_LOG_DIRTY_PAGES))
+			continue;
+
+		msize = s->npages << PAGE_SHIFT;
+		gpa = s->base_gfn << PAGE_SHIFT;
+
+		/* enumerate all gpa pages */
+		for (address = gpa; address < (gpa + msize); address += PAGE_SIZE) {
+			pgd_t *pgdp;
+			pud_t *pudp;
+			pmd_t *pmdp;
+			pte_t *ptep, pte;
+
+			pgdp = (pgd_t *)gpa_pgd + __pgd_offset(address);
+			pudp = pud_offset(pgdp, address);
+			pmdp = pmd_offset(pudp, address);
+			ptep = pte_offset(pmdp, address);
+			pte = *ptep;
+			if (pte_val(pte) & _PAGE_MODIFIED) {
+				mark_page_dirty_in_slot(kvm, s, address >> PAGE_SHIFT);
+				pte_val(pte) &= ~(_PAGE_MODIFIED | _PAGE_VALID | _PAGE_DIRTY);
+				*ptep = pte;
+			}
+
+			address += PAGE_SIZE;
+		}
+	}
+
+	/* restart log session on all cpus */
+	on_each_cpu(kvm_flush_rtlb, (void *)(long)kvm->arch.guest_id, 1);
+
+	return;
+}
+
+static void kvm_set_gpa_pa_map(struct kvm *kvm, struct kvm_userspace_memory_region *mem,
+	int write_protect)
+{
+	uint64_t msize, gpa, hva_base, address;
+	pgd_t *gpa_pgd, *pgdp;
+	pud_t *pudp;
+	pmd_t *pmdp;
+	pte_t *ptep, pte;
+
+	gpa_pgd = (pgd_t *)kvm->arch.gpa_pgd;
+	msize = mem->memory_size;
+	gpa = mem->guest_phys_addr;
+	hva_base = mem->userspace_addr;
+
+	for (address = hva_base; address < (hva_base + msize);
+	     address += PAGE_SIZE, gpa += PAGE_SIZE) {
+		pgdp = current->mm->pgd + __pgd_offset(address);
+		pudp = pud_offset(pgdp, address);
+		pmdp = pmd_offset(pudp, address);
+		ptep = pte_offset(pmdp, address);
+		pte = *ptep;
+
+		/* _PAGE_MODIFIED: to record whether the page has been written,
+			will be set after tlbs/tlbm processed
+		 * _PAGE_VALID: force tlb invalid exception (tlbl/tlbs/tlbm)
+		 * _PAGE_DIRTY: force tlbs/tlbm exception
+		 */
+		if (write_protect)
+			pte_val(pte) &= ~(_PAGE_MODIFIED | _PAGE_VALID | _PAGE_DIRTY);
+
+		pgdp = (pgd_t *)(gpa_pgd + __pgd_offset(gpa));
+		pudp = pud_offset(pgdp, gpa);
+		pmdp = pmd_offset(pudp, gpa);
+		ptep = pte_offset(pmdp, gpa);
+		*ptep = pte;
+	}
+
+	/* flush all tlbs for this region */
+	on_each_cpu(kvm_flush_rtlb, (void *)(long)kvm->arch.guest_id, 1);
+}
+
+int kvm_arch_vcpu_runnable(struct kvm_vcpu *v)
+{
+	return 1;
+}
+
+long kvm_arch_vcpu_ioctl(struct file *filp, unsigned int ioctl, unsigned long arg)
+{
+	/* We do not have arch-specific vcpu ioctl yet */
+	return -EINVAL;
+}
+
+int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)
+{
+	int 		ret;
+	sigset_t 	sigsaved;
+	struct kvm_regs *r = &vcpu->arch.guest.gpu;
+
+	r->root_guestctl1 = vcpu->kvm->arch.guest_id | (vcpu->kvm->arch.guest_id << 16);
+	if (vcpu->arch.init_guest == 1) {
+		int i;
+		struct kvm_vcpu *vcpu_p;
+
+		vcpu->arch.gpa_pgd = vcpu->kvm->arch.gpa_pgd;
+
+		/* go through all vcpus to register its vcpu pointer */
+		kvm_for_each_vcpu(i, vcpu_p, vcpu->kvm) {
+			vcpu_p->arch.guest_vcpu_p[r->guest_ebase & 0x3ff] = (uint64_t)vcpu;
+		}
+	}
+
+#if 0
+	printk("Running guest (vcpu %p, kvm %p, Entry PC: 0x%llx, GuestId 0x%llx, Guest vcpu id: 0x%llx) ...\n",
+		vcpu, vcpu->kvm, r->root_epc, r->root_guestctl1 & 0xff, r->guest_ebase & 0x3ff);
+#endif
+
+	/* setup the signal handling */
+	if (vcpu->sigset_active)
+		sigprocmask(SIG_SETMASK, &vcpu->sigset, &sigsaved);
+
+	if (vcpu->mmio_needed) {
+		/* complete mmio */
+		if (run->mmio.len == 1)
+			vcpu->arch.guest.gpu.regs[vcpu->arch.mmio.reg_num] = *(unsigned char *)run->mmio.data;
+		else if (run->mmio.len == 2)
+			vcpu->arch.guest.gpu.regs[vcpu->arch.mmio.reg_num] = *(unsigned short *)run->mmio.data;
+		else if (run->mmio.len == 4)
+			vcpu->arch.guest.gpu.regs[vcpu->arch.mmio.reg_num] = *(int *)run->mmio.data;
+		else
+			vcpu->arch.guest.gpu.regs[vcpu->arch.mmio.reg_num] = *(long *)run->mmio.data;
+		vcpu->mmio_needed = 0;
+	}
+
+	if (vcpu->arch.pio_needed) {
+		/* complete pio */
+		if (run->io.size == 1)
+			vcpu->arch.guest.gpu.regs[vcpu->arch.pio.reg_num] = *(unsigned char *)run->mips_exit_io.data;
+		else if (run->io.size == 2)
+			vcpu->arch.guest.gpu.regs[vcpu->arch.pio.reg_num] = *(unsigned short *)run->mips_exit_io.data;
+		else if (run->io.size == 4)
+			vcpu->arch.guest.gpu.regs[vcpu->arch.pio.reg_num] = *(int *)run->mips_exit_io.data;
+		else
+			vcpu->arch.guest.gpu.regs[vcpu->arch.pio.reg_num] = *(long *)run->mips_exit_io.data;
+		vcpu->arch.pio_needed = 0;
+	}
+
+	if (vcpu->kvm->arch.exit_request == 0) {
+		local_irq_disable();
+		kvm_guest_enter();
+		local_irq_enable();
+		ret = __kvm_vcpu_run_guest(vcpu);
+		local_irq_disable();
+		kvm_guest_exit();
+		local_irq_enable();
+	} else {
+		ret = KVM_EXIT_QUIT_KVM << 24;
+	}
+
+	/* The execution reaches here because QEMU needs to be involved:
+	 * . I/O
+	 * . New vcpu
+	 */
+	switch(ret >> 24) {
+	case KVM_EXIT_SPAWN_THREADS:
+		run->exit_reason = ret >> 24;
+		run->mips_spawn_thread.num_vcpus = (ret >> 16) & 0xff;
+		run->mips_spawn_thread.start_hw_cpuid = ret & 0xff;
+		break;
+	case KVM_EXIT_ENABLE_CORE:
+		printk("KVM_EXIT_ENABLE_CORE\n");
+		run->exit_reason = ret >> 24;
+		run->mips_spawn_thread.start_hw_cpuid = ret & 0xff;
+		break;
+	case KVM_EXIT_REQUEST_QUIT:
+		/* request to quit kvm */
+		run->exit_reason = ret >> 24;
+		break;
+	case KVM_EXIT_QUIT_KVM:
+#if 0
+		printk("KVM_EXIT_QUIT_KVM Running guest (vcpu %p, kvm %p, Entry PC: 0x%llx, GuestId 0x%llx, Guest vcpu id: 0x%llx) ...\n",
+                vcpu, vcpu->kvm, r->root_epc, r->root_guestctl1 & 0xff, r->guest_ebase & 0x3ff);
+#endif
+		/* quit kvm */
+		run->exit_reason = ret >> 24;
+		/* also clear guestctl1 */
+		__write_32bit_c0_register($10, 4, 0);
+		break;
+	case KVM_EXIT_MMIO:
+		/* mmio exit */
+		run->exit_reason = ret >> 24;
+		run->mmio.phys_addr = vcpu->arch.mmio.addr;
+		run->mmio.len = vcpu->arch.mmio.len;
+
+		if (run->mmio.len == 1)
+			run->mmio.data[0] = (uint8_t)vcpu->arch.mmio.val;
+		else if (run->mmio.len == 2)
+			*(uint16_t *)run->mmio.data = (uint16_t)vcpu->arch.mmio.val;
+		else if (run->mmio.len == 4)
+			*(uint32_t *)run->mmio.data = (uint32_t)vcpu->arch.mmio.val;
+		else if (run->mmio.len == 8)
+			*(uint64_t *)run->mmio.data = (uint64_t)vcpu->arch.mmio.val;
+
+		run->mmio.is_write = vcpu->arch.mmio.is_write;
+		if (!run->mmio.is_write)
+			vcpu->mmio_needed = 1;
+
+		break;
+	case KVM_EXIT_IO:
+		/* pio exit */
+		run->exit_reason = ret >> 24;
+		run->io.direction = vcpu->arch.pio.direction;
+		run->io.size = vcpu->arch.pio.size;
+		run->io.port = vcpu->arch.pio.port;
+		run->io.count = vcpu->arch.pio.count;
+		run->io.data_offset = vcpu->arch.pio.data_offset;
+
+		if (run->io.direction == KVM_EXIT_IO_OUT) {
+			if (run->io.size == 1)
+				run->mips_exit_io.data[0] = (uint8_t)vcpu->arch.pio.val;
+			else if (run->io.size == 2)
+				*(uint16_t *)run->mips_exit_io.data = (uint16_t)vcpu->arch.pio.val;
+			else if (run->io.size == 4)
+				*(uint32_t *)run->mips_exit_io.data = (uint32_t)vcpu->arch.pio.val;
+			else if (run->io.size == 8)
+				*(uint64_t *)run->mips_exit_io.data = (uint64_t)vcpu->arch.pio.val;
+		} else {
+			vcpu->arch.pio_needed = 1;
+		}
+		break;
+
+	case KVM_EXIT_HC_HALX_NETSOC:
+		run->mips_hypcall.args[0] = vcpu->arch.mips_hypcall.args[0];
+		run->mips_hypcall.cmd = vcpu->arch.mips_hypcall.cmd;
+		run->exit_reason = ret >> 24;
+#ifdef KVM_HYPERCALL_DEBUG
+		printk("KVM_EXIT_HC_HALX_NETSOC. run->mips_hypcall.args[0] = 0x%lx\n", run->mips_hypcall.args[0]);
+		printk("KVM_EXIT_HC_HALX_NETSOC. run->exit_reason = 0x%lx, run->mips_hypcall.cmd = %d\n", run->exit_reason, run->mips_hypcall.cmd);
+#endif
+		break;
+#if 0
+	case KVM_EXIT_IRQ_WINDOW_OPEN:
+		run->exit_reason = ret >> 24;
+		break;
+#endif
+	default:
+		printk("Bug: unknown exit reason for guest %x\n", ret);
+	}
+
+	if (vcpu->sigset_active)
+		sigprocmask(SIG_SETMASK, &sigsaved, NULL);
+
+	return 0;
+}
+
+int kvm_arch_vcpu_fault(struct kvm_vcpu *vcpu, struct vm_fault *vmf)
+{
+	return VM_FAULT_SIGBUS;
+}
+
+/* Not used any more */
+int kvm_arch_vcpu_ioctl_translate(struct kvm_vcpu *vcpu, struct kvm_translation *tr)
+{
+	return -ENOTSUPP;
+}
+
+int kvm_arch_hardware_enable(void *garbage)
+{
+	return 0;
+}
+
+void kvm_arch_hardware_disable(void *garbage)
+{
+	/* do nothing */
+}
+
+int kvm_arch_hardware_setup(void)
+{
+	return 0;
+}
+
+void kvm_arch_hardware_unsetup(void)
+{
+	/* do nothing */
+}
+
+void kvm_arch_check_processor_compat(void *rtn)
+{
+	*(int *)rtn = xlp_kvm_check_processor_compat();
+}
+
+int kvm_arch_vcpu_ioctl_set_mpstate(struct kvm_vcpu *vcpu, struct kvm_mp_state *mp_state)
+{
+	return -ENOTSUPP;
+}
+
+int kvm_arch_vcpu_ioctl_get_mpstate(struct kvm_vcpu *vcpu, struct kvm_mp_state *mp_state)
+{
+	return -ENOTSUPP;
+}
+
+int kvm_arch_vcpu_ioctl_get_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)
+{
+	memcpy(fpu, &vcpu->arch.guest.fpu, sizeof(vcpu->arch.guest.fpu));
+	return 0;
+}
+
+int kvm_arch_vcpu_ioctl_set_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)
+{
+	memcpy(&vcpu->arch.guest.fpu, fpu, sizeof(vcpu->arch.guest.fpu));
+	return 0;
+}
+
+int kvm_arch_vcpu_ioctl_set_guest_debug(struct kvm_vcpu *vcpu, struct kvm_guest_debug *dbg)
+{
+	return -ENOTSUPP;
+}
+
+int kvm_arch_vcpu_ioctl_set_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)
+{
+	memcpy(&vcpu->arch.guest.gpu, regs, sizeof(vcpu->arch.guest.gpu));
+	return 0;
+}
+
+int kvm_arch_vcpu_ioctl_get_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)
+{
+	memcpy(regs, &vcpu->arch.guest.gpu, sizeof(vcpu->arch.guest.gpu));
+	return 0;
+}
+
+int kvm_arch_vcpu_ioctl_get_sregs(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs)
+{
+	return -ENOTSUPP;
+}
+
+int kvm_arch_vcpu_ioctl_set_sregs(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs)
+{
+	return -ENOTSUPP;
+}
+
+int kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)
+{
+	return 0;
+}
+
+int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
+{
+	vcpu->arch.pip_vector = 0;
+	vcpu->arch.init_guest = 1;
+	vcpu->arch.hva_pgd = 0;
+	vcpu->arch.gpa_pgd = 0;
+	vcpu->arch.nmi = 0;
+	vcpu->arch.host_vcpuid = -1;
+	vcpu->arch.pio_needed = 0;
+#if 0
+	printk("Guest vcpu %p initialization\n", vcpu);
+#endif
+	return 0;
+}
+
+void kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu)
+{
+}
+
+void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
+{
+	/* Set the guest ID */
+}
+
+void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
+{
+	/* Nullify the guest ID */
+}
+
+int kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu)
+{
+	/* This is needed to wake up the __waited__ VM */
+	return 0;
+}
+
+int kvm_arch_prepare_memory_region(struct kvm *kvm,
+				struct kvm_memory_slot *memslot,
+				struct kvm_userspace_memory_region *mem,
+				enum kvm_mr_change change)
+{
+	return 0;
+}
+
+void kvm_arch_commit_memory_region(struct kvm *kvm,
+				struct kvm_userspace_memory_region *mem,
+				const struct kvm_memory_slot *old,
+				enum kvm_mr_change change)
+{
+
+	if (old->npages == 0) {
+		/* if after sync and not initialized, go ahead to initialize the page table */
+		if (kvm->arch.mem_synced
+		    && (kvm->arch.slot_inited & (1 << mem->slot)) == 0) {
+			pgd_t *gpa_pgd = (pgd_t *)kvm->arch.gpa_pgd;
+			uint64_t msize, gpa, hva;
+
+			msize = mem->memory_size;
+			gpa = mem->guest_phys_addr;
+			hva = mem->userspace_addr;
+
+			kvm_populate_gpa_map(gpa_pgd, msize, gpa, hva);
+			kvm->arch.slot_inited |= (1 << mem->slot);
+		}
+		return;
+	}
+
+	/* no flag change, do nothing */
+	if (old->flags == mem->flags)
+		return;
+
+	if (!(mem->flags & KVM_MEM_LOG_DIRTY_PAGES))
+		kvm_set_gpa_pa_map(kvm, mem, 0);
+	else
+		kvm_set_gpa_pa_map(kvm, mem, 1);
+}
+
+void kvm_arch_flush_shadow(struct kvm *kvm)
+{
+}
+
+void kvm_arch_sync_events(struct kvm *kvm)
+{
+}
+
+struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm, unsigned int id)
+{
+	struct kvm_vcpu *vcpu;
+	int err;
+
+	vcpu = kzalloc(sizeof(struct kvm_vcpu), GFP_KERNEL);
+	if (!vcpu) {
+		err = -ENOMEM;
+		goto out;
+	}
+
+	err = kvm_vcpu_init(vcpu, kvm, id);
+	if (err)
+		goto free_vcpu;
+
+	return vcpu;
+
+free_vcpu:
+	kfree(vcpu);
+out:
+	return ERR_PTR(err);
+}
+
+void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
+{
+        kvm_vcpu_uninit(vcpu);
+	kfree(vcpu);
+}
+
+int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)
+{
+	return 0;
+}
+
+int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
+{
+	if (type)
+		return -EINVAL;
+
+	xlp_kvm_init_vm(kvm);
+
+	return 0;
+}
+
+void kvm_arch_destroy_vm(struct kvm *kvm)
+{
+	xlp_kvm_destroy_vm(kvm);
+	kvm_free_vcpus(kvm);
+}
+
+void kvm_arch_free_memslot(struct kvm_memory_slot *free,
+                           struct kvm_memory_slot *dont)
+{
+	if (!dont || free->arch.rmap[0] != dont->arch.rmap[0]) {
+		kvm_kvfree(free->arch.rmap[0]);
+		free->arch.rmap[0] = NULL;
+	}
+}
+
+int kvm_arch_create_memslot(struct kvm_memory_slot *slot, unsigned long npages)
+{
+
+	slot->arch.rmap[0] =
+		kvm_kvzalloc(npages * sizeof(*slot->arch.rmap[0]));
+	if (!slot->arch.rmap[0])
+		goto out_free;
+
+	return 0;
+
+out_free:
+	kvm_kvfree(slot->arch.rmap[0]);
+	slot->arch.rmap[0] = NULL;
+	return -ENOMEM;
+}
+
+void kvm_arch_flush_shadow_all(struct kvm *kvm)
+{
+}
+
+void kvm_arch_flush_shadow_memslot(struct kvm *kvm,
+				struct kvm_memory_slot *slot)
+{
+}
+
+int kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu)
+{
+	return 0;
+}
+
+long kvm_arch_dev_ioctl(struct file *filp, unsigned int ioctl, unsigned long arg)
+{
+	return -EINVAL;
+}
+
+int kvm_dev_ioctl_check_extension(long ext)
+{
+	if (ext == KVM_CAP_IRQCHIP)
+		return 1;
+	return 0;
+}
+
+int kvm_vm_ioctl_get_dirty_log(struct kvm *kvm, struct kvm_dirty_log *log)
+{
+	struct kvm_memory_slot *memslot;
+	unsigned long gpa, gpa_end;
+	int is_dirty = 0;
+	int r;
+	unsigned long n;
+
+	mutex_lock(&kvm->slots_lock);
+
+	/* march through guest page table to get the dirty log */
+	kvm_sync_dirty_log(kvm);
+
+	r = kvm_get_dirty_log(kvm, log, &is_dirty);
+	if (r) {
+		mutex_unlock(&kvm->slots_lock);
+		return r;
+	}
+
+	if (is_dirty) {
+		memslot = &kvm->memslots->memslots[log->slot];
+
+		gpa = memslot->base_gfn << PAGE_SHIFT;
+		gpa_end = gpa + (memslot->npages << PAGE_SHIFT);
+
+		n = kvm_dirty_bitmap_bytes(memslot);
+		memset(memslot->dirty_bitmap, 0, n);
+	}
+
+	mutex_unlock(&kvm->slots_lock);
+	return 0;
+}
+
+long kvm_arch_vm_ioctl(struct file *filp, unsigned int ioctl, unsigned long arg)
+{
+	struct kvm *kvm = filp->private_data;
+	void __user *argp = (void __user *)arg;
+	int r = 0;
+
+	switch(ioctl) {
+	case KVM_CREATE_IRQCHIP:
+	{
+		/* Do nothing. Simply tell caller it is successful. */
+		break;
+	}
+	case KVM_MIPS_EXIT_REQUEST:
+	{
+		kvm->arch.exit_request = (int)(long)argp;
+		break;
+	}
+	case KVM_MIPS_INFO_REQUEST:
+	{
+		struct kvm_guest_info input;
+
+		input.guest_id = kvm->arch.guest_id;
+		if (copy_to_user(argp, &input, sizeof(struct kvm_guest_info))) {
+			r = -EFAULT;
+		}
+		break;
+	}
+	case KVM_MIPS_SYNC_GPA_MAP:
+		kvm_sync_gpa_map(kvm);
+		break;
+	case KVM_MIPS_HUGEPAGE_SIZE:
+	{
+		unsigned long long hugepage_size = 0;
+
+		/* Only support non-mips-default huge page size */
+#ifdef CONFIG_HUGEPAGE_NOT_MIPS_DEFAULT
+		hugepage_size = HPAGE_SIZE;
+#endif
+		if (copy_to_user(argp, &hugepage_size, sizeof(unsigned long long))) {
+			r = -EFAULT;
+		}
+		break;
+	}
+	case KVM_MIPS_ADD_PKTMEM:
+	{
+		struct kvm_pktmem_info pktmem;
+
+		if (copy_from_user(&pktmem, argp, sizeof(struct kvm_pktmem_info))) {
+			r = -EFAULT;
+			break;
+		}
+		/* Only support one packet memory region */
+		if (kvm->arch.pktmem.gpa_start != (-1)) {
+			r = -EFAULT;
+			break;
+		}
+		kvm->arch.pktmem.gpa_start = pktmem.gpa_start;
+		kvm->arch.pktmem.rpa_start = pktmem.rpa_start;
+		kvm->arch.pktmem.size = pktmem.size;
+		break;
+	}
+	case KVM_MIPS_SET_BINDING:
+	{
+		int i, binding[32];
+		struct kvm_vcpu *vcpu;
+
+		if (copy_from_user(binding, argp, sizeof(binding))) {
+			r = -EFAULT;
+			break;
+		}
+
+		kvm_for_each_vcpu(i, vcpu, kvm) {
+			vcpu->arch.host_vcpuid = binding[vcpu->vcpu_id];
+		}
+		break;
+	}
+	case KVM_MIPS_INJECT_PCIE_INTX:
+	{
+		struct kvm_arch *arch = &kvm->arch;
+		int type, val = (int)(long)argp;
+
+		type = val >> 16;
+		if (type == 0)
+			/* PCI Link INTx */
+			kvm_pic_inject_guest_ext(kvm, arch,
+				PIC_9XX_IRT_PCIE_LINK_0_INDEX + (val & 0xff), 0);
+		else if (type == 1)
+			/* PCI MSIX */
+			kvm_pic_inject_guest_ext(kvm, arch,
+				PIC_IRT_PCIE_MSIX_INDEX(val & 0xff), 0);
+		else
+			r = -EINVAL;
+		break;
+	}
+	case KVM_MIPS_INJECT_PIC:
+	{
+		struct kvm_arch *arch = &kvm->arch;
+		int val = (int)(long)argp;
+
+		kvm_pic_inject_guest_ext(kvm, arch, val, 0);
+
+		break;
+	}
+	default:
+		r = -ENOTTY;
+		break;
+	}
+
+	return r;
+}
+
+int kvm_is_in_guest(void)
+{
+	return (get_irq_regs()->cp0_guestctl0 >> 31);
+}
+
+static int kvm_is_user_mode(void)
+{
+	int user_mode = 0;
+	uint32_t guest_cp0_status;
+
+	__asm__ __volatile__ ("mfgc0 %0, $12, 0": "=r" (guest_cp0_status));
+	user_mode = ((guest_cp0_status & KU_MASK) == KU_USER);
+
+	return user_mode != 0;
+}
+
+static unsigned long kvm_get_guest_ip(void)
+{
+	uint64_t guest_epc = 0x0;
+
+	guest_epc = get_irq_regs()->cp0_epc;
+
+	return guest_epc;
+}
+
+
+static struct perf_guest_info_callbacks kvm_guest_cbs = {
+	.is_in_guest		= kvm_is_in_guest,
+	.is_user_mode		= kvm_is_user_mode,
+	.get_guest_ip		= kvm_get_guest_ip,
+};
+
+int kvm_arch_init(void *opaque)
+{
+	perf_register_guest_info_callbacks(&kvm_guest_cbs);
+
+	return 0;
+}
+
+void kvm_arch_exit(void)
+{
+	perf_unregister_guest_info_callbacks(&kvm_guest_cbs);
+}
+
+static int __init kvm_mips_init(void)
+{
+	return kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+}
+
+static void __exit kvm_mips_exit(void)
+{
+	kvm_exit();
+}
+
+module_init(kvm_mips_init);
+module_exit(kvm_mips_exit);
diff --git a/arch/mips/kvm-netl/kvm_iommu.c b/arch/mips/kvm-netl/kvm_iommu.c
new file mode 100644
index 0000000..2e7384e
--- /dev/null
+++ b/arch/mips/kvm-netl/kvm_iommu.c
@@ -0,0 +1,12 @@
+
+#include <linux/kvm_host.h>
+
+int kvm_iommu_map_pages(struct kvm *kvm, struct kvm_memory_slot *slot)
+{
+	return 0;
+}
+
+void kvm_iommu_unmap_pages(struct kvm *kvm, struct kvm_memory_slot *slot)
+{
+	return;
+}
diff --git a/arch/mips/kvm-netl/kvm_vhost.c b/arch/mips/kvm-netl/kvm_vhost.c
new file mode 100644
index 0000000..65e9fe4
--- /dev/null
+++ b/arch/mips/kvm-netl/kvm_vhost.c
@@ -0,0 +1,217 @@
+#include <linux/kvm_host.h>
+#include <linux/uaccess.h>
+#include <linux/vmalloc.h>
+
+#include <asm/netlogic/kvm_xlp.h>
+#include <asm/netlogic/haldefs.h>
+#include <asm/netlogic/xlp-hal/xlp.h>
+#include <asm/netlogic/xlp-hal/pic.h>
+
+/*
+ * Return value:
+ * < 0 Interrupt was ignored (masked or not delivered for other reasons)
+ * = 0 Interrupt was coalesced (previous irq is still pending)
+ * > 0 Number of CPUs interrupt was delivered to
+ */
+int kvm_set_irq(struct kvm *kvm, int irq_source_id, u32 irq, int level,
+		bool line_status)
+{
+	panic("%s: KVM is not support irqchip type set_irq\n", __FUNCTION__);
+	return 0;
+}
+
+int kvm_set_msi(struct kvm_kernel_irq_routing_entry *e, struct kvm *kvm,
+		int irq_source_id, int level, bool line_status)
+{
+	if (!level)
+		return -1;
+
+	/* inject an interrupt to vector e->msi.data & 0xff */
+	kvm_pic_inject_guest_ext(kvm, &kvm->arch, PIC_IRT_PCIE_MSIX_INDEX(e->msi.data & 0xff), 0);
+
+	return 0;
+}
+
+void kvm_free_irq_routing(struct kvm *kvm)
+{
+	/* Called only during vm destruction. Nobody can use the pointer
+	 * at this stage */
+        kfree(kvm->irq_routing);
+}
+
+void kvm_register_irq_ack_notifier(struct kvm *kvm,
+                                   struct kvm_irq_ack_notifier *kian)
+{
+        mutex_lock(&kvm->irq_lock);
+        hlist_add_head_rcu(&kian->link, &kvm->irq_ack_notifier_list);
+        mutex_unlock(&kvm->irq_lock);
+}
+
+void kvm_unregister_irq_ack_notifier(struct kvm *kvm,
+                                    struct kvm_irq_ack_notifier *kian)
+{
+        mutex_lock(&kvm->irq_lock);
+        hlist_del_init_rcu(&kian->link);
+        mutex_unlock(&kvm->irq_lock);
+        synchronize_rcu();
+}
+
+/* Copied and Simplied from irq_comm.c */
+static int setup_routing_entry(struct kvm_irq_routing_table *rt,
+                               struct kvm_kernel_irq_routing_entry *e,
+                               const struct kvm_irq_routing_entry *ue)
+{
+        int r = -EINVAL;
+        struct kvm_kernel_irq_routing_entry *ei;
+
+        /*
+         * Do not allow GSI to be mapped to the same irqchip more than once.
+         * Allow only one to one mapping between GSI and MSI.
+         */
+        hlist_for_each_entry(ei, &rt->map[ue->gsi], link)
+                if (ei->type == KVM_IRQ_ROUTING_MSI ||
+                    ue->type == KVM_IRQ_ROUTING_MSI ||
+                    ue->u.irqchip.irqchip == ei->irqchip.irqchip)
+                        return r;
+
+        e->gsi = ue->gsi;
+        e->type = ue->type;
+        switch (ue->type) {
+        case KVM_IRQ_ROUTING_IRQCHIP:
+		/* not supported */
+                goto out;
+        case KVM_IRQ_ROUTING_MSI:
+                e->set = kvm_set_msi;
+                e->msi.address_lo = ue->u.msi.address_lo;
+                e->msi.address_hi = ue->u.msi.address_hi;
+                e->msi.data = ue->u.msi.data;
+                break;
+        default:
+                goto out;
+        }
+
+        hlist_add_head(&e->link, &rt->map[e->gsi]);
+        r = 0;
+out:
+        return r;
+}
+
+/* Copied and Simplied from irq_comm.c */
+int kvm_set_irq_routing(struct kvm *kvm,
+                        const struct kvm_irq_routing_entry *ue,
+                        unsigned nr,
+                        unsigned flags)
+{
+        struct kvm_irq_routing_table *new, *old;
+        u32 i, j, nr_rt_entries = 0;
+        int r;
+
+        for (i = 0; i < nr; ++i) {
+                if (ue[i].gsi >= KVM_MAX_IRQ_ROUTES)
+                        return -EINVAL;
+                nr_rt_entries = max(nr_rt_entries, ue[i].gsi);
+        }
+
+        nr_rt_entries += 1;
+
+        new = kzalloc(sizeof(*new) + (nr_rt_entries * sizeof(struct hlist_head))
+                      + (nr * sizeof(struct kvm_kernel_irq_routing_entry)),
+                      GFP_KERNEL);
+        if (!new)
+                return -ENOMEM;
+
+        new->rt_entries = (void *)&new->map[nr_rt_entries];
+
+        new->nr_rt_entries = nr_rt_entries;
+        for (i = 0; i < 3; i++)
+                for (j = 0; j < KVM_IOAPIC_NUM_PINS; j++)
+                        new->chip[i][j] = -1;
+
+        for (i = 0; i < nr; ++i) {
+                r = -EINVAL;
+                if (ue->flags)
+                        goto out;
+                r = setup_routing_entry(new, &new->rt_entries[i], ue);
+                if (r)
+                        goto out;
+                ++ue;
+        }
+
+        mutex_lock(&kvm->irq_lock);
+        old = kvm->irq_routing;
+        kvm_irq_routing_update(kvm, new);
+        mutex_unlock(&kvm->irq_lock);
+
+        synchronize_rcu();
+
+        new = old;
+        r = 0;
+out:
+        kfree(new);
+        return r;
+}
+
+/* Largely copied from virt/kvm/assigned-dev.c */
+long kvm_vm_ioctl_assigned_device(struct kvm *kvm, unsigned ioctl,
+                                  unsigned long arg)
+{
+        void __user *argp = (void __user *)arg;
+        int r;
+
+        switch (ioctl) {
+#ifdef KVM_CAP_IRQ_ROUTING
+        case KVM_SET_GSI_ROUTING: {
+                struct kvm_irq_routing routing;
+                struct kvm_irq_routing __user *urouting;
+                struct kvm_irq_routing_entry *entries;
+
+                r = -EFAULT;
+                if (copy_from_user(&routing, argp, sizeof(routing)))
+                        goto out;
+                r = -EINVAL;
+                if (routing.nr >= KVM_MAX_IRQ_ROUTES)
+                        goto out;
+                if (routing.flags)
+                        goto out;
+                r = -ENOMEM;
+                entries = vmalloc(routing.nr * sizeof(*entries));
+                if (!entries)
+                        goto out;
+                r = -EFAULT;
+                urouting = argp;
+                if (copy_from_user(entries, urouting->entries,
+                                   routing.nr * sizeof(*entries)))
+                        goto out_free_irq_routing;
+                r = kvm_set_irq_routing(kvm, entries, routing.nr,
+                                        routing.flags);
+        out_free_irq_routing:
+                vfree(entries);
+                break;
+        }
+#endif /* KVM_CAP_IRQ_ROUTING */
+	       default:
+                r = -ENOTTY;
+                break;
+	}
+out:
+	return r;
+}
+
+void kvm_vcpu_request_scan_ioapic(struct kvm *kvm)
+{
+	return;
+}
+
+#ifdef __KVM_HAVE_IRQ_LINE
+int kvm_vm_ioctl_irq_line(struct kvm *kvm, struct kvm_irq_level *irq_event,
+			bool line_status)
+{
+	if (!irq_event->level)
+		return -1;
+
+	/* inject an interrupt to vector e->msi.data & 0xff */
+	kvm_pic_inject_guest_ext(kvm, &kvm->arch, PIC_IRT_PCIE_MSIX_INDEX(irq_event->irq), 0);
+
+	return 0;
+}
+#endif
diff --git a/arch/mips/kvm-netl/xlp.c b/arch/mips/kvm-netl/xlp.c
new file mode 100644
index 0000000..fe3fe6c
--- /dev/null
+++ b/arch/mips/kvm-netl/xlp.c
@@ -0,0 +1,424 @@
+/*
+ * Copyright (c) 2003-2013 Broadcom Corporation
+ * All Rights Reserved
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the Broadcom
+ * license below:
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in
+ *    the documentation and/or other materials provided with the
+ *    distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY BROADCOM ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL BROADCOM OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
+ * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
+ * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
+ * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <linux/kvm.h>
+#include <linux/kvm_host.h>
+#include <linux/spinlock.h>
+
+#include <asm/pgalloc.h>
+#include <asm/branch.h>
+#include <asm/fpu.h>
+#include <asm/inst.h>
+#include <asm/ptrace.h>
+#include <asm/kvm_host.h>
+#include <asm/netlogic/kvm_xlp.h>
+
+#include <asm/netlogic/haldefs.h>
+#include <asm/netlogic/common.h>
+#include <asm/netlogic/xlp-hal/sys.h>
+
+/* KVM_MAX_NUM_GID is multiple of 64 */
+static uint64_t occupied_gids[KVM_MAX_NUM_GID/sizeof(uint64_t)];
+static uint64_t overlapped_gid1_count;
+
+static DEFINE_SPINLOCK(kvm_gid_lock);
+
+int xlp_kvm_check_processor_compat(void)
+{
+	/* FIXME: read config 3 */
+        return 0;
+}
+
+/*
+ * Compute the return address for the guest trigger exception.
+ * If the original fault instruction is pointing to the delay slot,
+ * the badinstr should be the insn prior to the delay slot instr.
+ */
+int compute_guest_return_epc(struct pt_regs *regs, unsigned int badinstr)
+{
+	unsigned int bit, fcr31;
+	long epc;
+	union mips_instruction insn;
+
+	epc = regs->cp0_epc;
+	if (!delay_slot(regs)) {
+		regs->cp0_epc = epc + 4;
+		return 0;
+	}
+
+	insn.word = badinstr;
+
+	switch (insn.i_format.opcode) {
+	/*
+	 * jr and jalr are in r_format format.
+	 */
+	case spec_op:
+		switch (insn.r_format.func) {
+		case jalr_op:
+			regs->regs[insn.r_format.rd] = epc + 8;
+			/* Fall through */
+		case jr_op:
+			regs->cp0_epc = regs->regs[insn.r_format.rs];
+			break;
+		}
+		break;
+
+	/*
+	 * This group contains:
+	 * bltz_op, bgez_op, bltzl_op, bgezl_op,
+	 * bltzal_op, bgezal_op, bltzall_op, bgezall_op.
+	 */
+	case bcond_op:
+		switch (insn.i_format.rt) {
+	 	case bltz_op:
+		case bltzl_op:
+			if ((long)regs->regs[insn.i_format.rs] < 0)
+				epc = epc + 4 + (insn.i_format.simmediate << 2);
+			else
+				epc += 8;
+			regs->cp0_epc = epc;
+			break;
+
+		case bgez_op:
+		case bgezl_op:
+			if ((long)regs->regs[insn.i_format.rs] >= 0)
+				epc = epc + 4 + (insn.i_format.simmediate << 2);
+			else
+				epc += 8;
+			regs->cp0_epc = epc;
+			break;
+
+		case bltzal_op:
+		case bltzall_op:
+			regs->regs[31] = epc + 8;
+			if ((long)regs->regs[insn.i_format.rs] < 0)
+				epc = epc + 4 + (insn.i_format.simmediate << 2);
+			else
+				epc += 8;
+			regs->cp0_epc = epc;
+			break;
+
+		case bgezal_op:
+		case bgezall_op:
+			regs->regs[31] = epc + 8;
+			if ((long)regs->regs[insn.i_format.rs] >= 0)
+				epc = epc + 4 + (insn.i_format.simmediate << 2);
+			else
+				epc += 8;
+			regs->cp0_epc = epc;
+			break;
+		}
+		break;
+
+	/*
+	 * These are unconditional and in j_format.
+	 */
+	case jal_op:
+		regs->regs[31] = regs->cp0_epc + 8;
+	case j_op:
+		epc += 4;
+		epc >>= 28;
+		epc <<= 28;
+		epc |= (insn.j_format.target << 2);
+		regs->cp0_epc = epc;
+		break;
+
+	/*
+	 * These are conditional and in i_format.
+	 */
+	case beq_op:
+	case beql_op:
+		if (regs->regs[insn.i_format.rs] ==
+		    regs->regs[insn.i_format.rt])
+			epc = epc + 4 + (insn.i_format.simmediate << 2);
+		else
+			epc += 8;
+		regs->cp0_epc = epc;
+		break;
+
+	case bne_op:
+	case bnel_op:
+		if (regs->regs[insn.i_format.rs] !=
+		    regs->regs[insn.i_format.rt])
+			epc = epc + 4 + (insn.i_format.simmediate << 2);
+		else
+			epc += 8;
+		regs->cp0_epc = epc;
+		break;
+
+	case blez_op: /* not really i_format */
+	case blezl_op:
+		/* rt field assumed to be zero */
+		if ((long)regs->regs[insn.i_format.rs] <= 0)
+			epc = epc + 4 + (insn.i_format.simmediate << 2);
+		else
+			epc += 8;
+		regs->cp0_epc = epc;
+		break;
+
+	case bgtz_op:
+	case bgtzl_op:
+		/* rt field assumed to be zero */
+		if ((long)regs->regs[insn.i_format.rs] > 0)
+			epc = epc + 4 + (insn.i_format.simmediate << 2);
+		else
+			epc += 8;
+		regs->cp0_epc = epc;
+		break;
+
+	/*
+	 * And now the FPA/cp1 branch instructions.
+	 */
+	case cop1_op:
+		preempt_disable();
+		if (is_fpu_owner())
+			asm volatile("cfc1\t%0,$31" : "=r" (fcr31));
+		else
+			fcr31 = current->thread.fpu.fcr31;
+		preempt_enable();
+
+		bit = (insn.i_format.rt >> 2);
+		bit += (bit != 0);
+		bit += 23;
+		switch (insn.i_format.rt & 3) {
+		case 0:	/* bc1f */
+		case 2:	/* bc1fl */
+			if (~fcr31 & (1 << bit))
+				epc = epc + 4 + (insn.i_format.simmediate << 2);
+			else
+				epc += 8;
+			regs->cp0_epc = epc;
+			break;
+
+		case 1:	/* bc1t */
+		case 3:	/* bc1tl */
+			if (fcr31 & (1 << bit))
+				epc = epc + 4 + (insn.i_format.simmediate << 2);
+			else
+				epc += 8;
+			regs->cp0_epc = epc;
+			break;
+		}
+		break;
+
+	default:
+		panic("Unhandled compute_guest_return_epc (instr %x)\n", badinstr);
+	}
+
+	return 0;
+}
+
+static int kvm_get_new_guest_id(void)
+{
+	int gid, idx, oft;
+	unsigned long flags;
+
+	spin_lock_irqsave(&kvm_gid_lock, flags);
+	for (gid = 1; gid < KVM_MAX_NUM_GID; gid++) {
+		idx = gid >> 6;
+		oft = gid & 0x3f;
+
+		if (((occupied_gids[idx] >> oft) & 0x1) == 0) {
+			occupied_gids[idx] |= (1 << oft);
+			spin_unlock_irqrestore(&kvm_gid_lock, flags);
+			return gid;
+		}
+	}
+
+	/* All gids have been occupied.
+	 * The algorithm here is to overlap gid 1, so later on whenever any guest with
+	 * gid 1 is to be scheduled, the guest/roottlb will be flushed. All other guests
+	 * are not affected.
+	 */
+	gid = 1;
+	overlapped_gid1_count ++;
+	spin_unlock_irqrestore(&kvm_gid_lock, flags);
+	return gid;
+}
+
+static void xlp_kvm_free_gpa_pgd(pgd_t *gpa_pgd)
+{
+	unsigned long *p, *end;
+
+	p = (unsigned long *)gpa_pgd;
+	end = p + PTRS_PER_PGD;
+
+	for(; p < end; p++) {
+#ifndef __PAGETABLE_PMD_FOLDED
+		if (*p != (unsigned long)invalid_pmd_table) {
+			unsigned long *p1 = (unsigned long*)*p;
+			int i;
+
+			for (i = 0; i < PTRS_PER_PMD; i++) {
+				if (p1[i] != (unsigned long)invalid_pte_table) {
+					__free_page((void *)p1[i]);
+				}
+			}
+			__free_page((void *)*p);
+		}
+#else
+		if (*p != (unsigned long)invalid_pte_table) {
+			__free_page((void *)*p);
+		}
+#endif
+	}
+
+	/* free pgd table page */
+	pgd_free(NULL, gpa_pgd);
+}
+
+void xlp_kvm_destroy_vm(struct kvm *kvm)
+{
+	int gid, idx, oft;
+	unsigned long flags;
+
+	/* free guest id */
+	gid = kvm->arch.guest_id;
+
+	spin_lock_irqsave(&kvm_gid_lock, flags);
+	if (gid != 1) {
+		idx = gid >> 6;
+		oft = gid & 0x3f;
+		occupied_gids[idx] &= ~(1 << oft);
+	} else {
+		if (overlapped_gid1_count)
+			overlapped_gid1_count --;
+		else
+			occupied_gids[0] &= ~(1 << 1);
+	}
+	spin_unlock_irqrestore(&kvm_gid_lock, flags);
+
+	/* free gpa pgd table */
+	xlp_kvm_free_gpa_pgd((pgd_t *)kvm->arch.gpa_pgd);
+}
+
+static void xlp_kvm_init_pic(struct kvm_arch *arch)
+{
+	arch->pic.u.v32[0]    = 0xffffffff;
+	arch->pic.u.v32[2]    = 0x08000010;
+	arch->pic.u.v32[0x3d] = 0x0001008c;
+}
+
+static void xlp_kvm_init_sysmgt(struct kvm_arch *arch)
+{
+	uint64_t sysbase;
+
+	sysbase = nlm_get_node(0)->sysbase;
+
+	arch->sysmgt.regs[0] = 0xffffffff;
+	arch->sysmgt.regs[1] = 0x0;
+	arch->sysmgt.regs[2] = 0x0;
+	arch->sysmgt.regs[3] = 0x0;
+	arch->sysmgt.regs[0x40] = nlm_read_sys_reg(sysbase, 0x0);
+	arch->sysmgt.regs[0x41] = 0x0;
+	arch->sysmgt.regs[0x42] = 0xffffe;
+	arch->sysmgt.regs[0x43] = 0xffffe;
+}
+
+static void xlp_kvm_init_clkmgt(struct kvm_arch *arch)
+{
+	uint64_t clkbase;
+	int ii = 0;
+	clkbase = nlm_get_node(0)->sysbase +  (0x2 << 12);
+
+	arch->clkmgt.reg_0 = 0xffffffff;
+
+	for (ii = 0;ii < sizeof(arch->clkmgt.reg_cpupllctrl)/4;ii++)
+		arch->clkmgt.reg_cpupllctrl[ii] = nlm_read_sys_reg(clkbase, 0xc0+ii);
+
+	arch->clkmgt.reg_cpupllchgctrl = nlm_read_sys_reg(clkbase, 0x188);
+}
+
+
+static void xlp_kvm_init_bridge(struct kvm_arch *arch)
+{
+	int i;
+
+	memset(arch->bridge.config, 0, 16 * sizeof(uint32_t));
+	arch->bridge.config[0] = 0x900114e4;
+	arch->bridge.config[2] = 0x06800000;
+	arch->bridge.config[3] = 0x00810000;
+	arch->bridge.config[6] = 0x10101; /* primary: 1, secondary 1, subordinate 1 */
+
+	for (i = 0; i < 4; i++) {
+		arch->bridge.pcie_busnum[i]   = 0x0;
+		arch->bridge.pcie_membase[i]  = 0x0;
+		arch->bridge.pcie_memlimit[i] = 0x0;
+		arch->bridge.pcie_iobase[i]   = 0x0;
+		arch->bridge.pcie_iolimit[i]  = 0x0;
+	}
+}
+
+void xlp_kvm_init_vm(struct kvm *kvm)
+{
+	struct kvm_arch *arch = &kvm->arch;
+
+	/* Assign a new Guest ID */
+	arch->guest_id = kvm_get_new_guest_id();
+	arch->exit_request = 0;
+	arch->pktmem.gpa_start = -1;
+	arch->mem_synced = 0;
+	arch->slot_inited = 0;
+
+	arch->gpa_pgd = (uint64_t)pgd_alloc(NULL);
+
+	/* initialize SOCs: PIC/SYSMGT */
+	xlp_kvm_init_pic(arch);
+	xlp_kvm_init_sysmgt(arch);
+	xlp_kvm_init_clkmgt(arch);
+	xlp_kvm_init_bridge(arch);
+}
+
+void kvm_save_guest_context(struct pt_regs *regs, struct kvm_vcpu_guest *guest)
+{
+	memcpy(guest->gpu.regs, regs->regs, sizeof(guest->gpu.regs));
+	guest->gpu.hi = regs->hi;
+	guest->gpu.lo = regs->lo;
+
+	guest->gpu.root_guestctl0  = regs->cp0_guestctl0;
+	guest->gpu.root_epc        = regs->cp0_epc;
+}
+
+void kvm_xlp_check_exit_request(struct kvm_vcpu *vcpu)
+{
+	unsigned int val;
+
+#if 1
+	printk("Guest vcpu %lld exiting\n", vcpu->arch.guest.gpu.guest_ebase & 0x3ff);
+#endif
+
+	/* return to qemu for exit */
+	val = (KVM_EXIT_QUIT_KVM << 24);
+	__kvm_vcpu_leave_guest(vcpu, val);
+}
diff --git a/arch/mips/mm/c-netlogic.c b/arch/mips/mm/c-netlogic.c
index ab3f34f..c8c4aba 100644
--- a/arch/mips/mm/c-netlogic.c
+++ b/arch/mips/mm/c-netlogic.c
@@ -33,6 +33,19 @@
 #include <linux/kallsyms.h>
 #include <linux/mm.h>
 #include <linux/module.h>
+
+#ifdef CONFIG_KVM_NETL
+#include <asm/netlogic/mips-extns.h>
+#include <asm/netlogic/haldefs.h>
+#include <asm/netlogic/common.h>
+#include <asm/netlogic/xlp-hal/iomap.h>
+#include <asm/netlogic/xlp-hal/xlp.h>
+#include <asm/netlogic/xlp-hal/sys.h>
+#include <asm/netlogic/xlp-hal/pic.h>
+#include <asm/netlogic/xlp-hal/pcibus.h>
+#include <asm/netlogic/xlp-hal/bridge.h>
+#endif
+
 #ifdef CONFIG_CPU_XLP
 #include <asm/netlogic/xlp-hal/xlp.h>
 #include <asm/netlogic/xlp-hal/cpucontrol.h>
@@ -562,3 +575,94 @@ void __cpuinit nlm_cache_init(void)
 	coherency_setup();
 	board_cache_error_setup = cache_error_setup;
 }
+
+#ifdef CONFIG_KVM_NETL
+
+static DEFINE_SPINLOCK(nlm_l3_lock);
+
+static void nlm_flush_l3(int node, unsigned long start, unsigned long size)
+{
+	unsigned long address, flags;
+	uint64_t bridge_base;
+	int nbu_disable;
+	struct nlm_soc_info *nodep;
+	int max_nbu = cpu_is_xlp5xx() ? 4 : 8;
+
+	nodep = nlm_get_node(node);
+
+#if 0
+	/* This is for emulator */
+	nbu_disable = 0xfe;
+#else
+	nbu_disable = nlm_read_sys_reg(nodep->sysbase, 0x26);
+#endif
+	bridge_base = nlm_get_bridge_regbase(node);
+
+	spin_lock_irqsave(&nlm_l3_lock, flags);
+	for (address = start; address < (start + size); address += 64) {
+		int i;
+
+		nlm_write_bridge_reg(bridge_base, 0xE9 - 0x40, address & 0xffffffff);
+		nlm_write_bridge_reg(bridge_base, 0xEA - 0x40, (address >> 32) | (0 << 31)); /* address */
+		for (i = 0; i < max_nbu; i++) {
+			uint32_t val;
+
+			if (nbu_disable & (1 << i))
+				continue; /* nbu is disabled */
+
+			nlm_write_bridge_reg(bridge_base, 0x103 - 0x40, i);
+			nlm_write_bridge_reg(bridge_base, 0xEB - 0x40, 0x1);
+			do {
+				val = nlm_read_bridge_reg(bridge_base, 0xEB - 0x40);
+			} while (val != 0x3);
+		}
+	}
+	spin_unlock_irqrestore(&nlm_l3_lock, flags);
+}
+
+struct nlm_l2_flush_t {
+	unsigned long start;
+	unsigned long size;
+};
+
+static void local_nlm_flush_l2(void *args)
+{
+	struct nlm_l2_flush_t *flush;
+	unsigned long start, size, address;
+	unsigned long base = 0x9800000000000000ULL;
+
+	if ((hard_smp_processor_id() & 0x3) != 0)
+		return;
+
+	flush = (struct nlm_l2_flush_t *)args;
+	start = flush->start;
+	size  = flush->size;
+
+	for (address = start; address < (start + size); address += 64) {
+		cache_op(Hit_Writeback_Inv_SD, base + address);
+	}
+}
+
+/*
+ * For address "addr", poke the page table and find the physical address.
+ * Flush all the entries covered by that physical page.
+ */
+void nlm_flush_cache_L2L3 (unsigned long start_paddr, unsigned long size)
+{
+	struct nlm_l2_flush_t flush;
+	int n;
+
+	/* flush L2 */
+	flush.start = start_paddr;
+	flush.size  = size;
+	on_each_cpu(local_nlm_flush_l2, (void *)&flush, 1);
+
+	/* flush L3 */
+	for (n = 0; n < NLM_NR_NODES; n++) {
+		if (xlp9xx_get_socbus(n) == 0)
+			break;
+		nlm_flush_l3(n, start_paddr, size);
+	}
+}
+
+#endif
diff --git a/arch/mips/mm/fault.c b/arch/mips/mm/fault.c
index 666696c..2377a9c 100644
--- a/arch/mips/mm/fault.c
+++ b/arch/mips/mm/fault.c
@@ -20,6 +20,11 @@
 #include <linux/kprobes.h>
 #include <linux/perf_event.h>
 
+#ifdef CONFIG_KVM_NETL
+#include <linux/kvm.h>
+#include <linux/kvm_host.h>
+#endif
+
 #include <asm/branch.h>
 #include <asm/mmu_context.h>
 #include <asm/uaccess.h>
@@ -27,6 +32,66 @@
 #include <asm/highmem.h>		/* For VMALLOC_END */
 #include <linux/kdebug.h>
 
+#ifdef CONFIG_KVM_NETL
+#include <asm/kvm.h>
+#include <asm/kvm_host.h>
+#include <asm/netlogic/kvm_xlp.h>
+#endif
+
+#ifdef CONFIG_KVM_NETL
+static void nlm_kvm_populate_page(struct pt_regs *regs, unsigned long address,
+	unsigned long gpa_address, unsigned long *pa_address, unsigned long *page_size)
+{
+	pgd_t		*pgdp;
+	pud_t		*pudp;
+	pmd_t		*pmdp;
+	pte_t		*ptep, pte;
+	struct kvm_vcpu	*vcpu;
+	unsigned long	*ptr;
+	int		i;
+
+	pgdp = current->mm->pgd + __pgd_offset(address);
+	pudp = pud_offset(pgdp, address);
+	pmdp = pmd_offset(pudp, address);
+	ptep = pte_offset(pmdp, address);
+	pte = *ptep;
+
+	vcpu = (struct kvm_vcpu *)regs->cp0_osscratch7;
+	pgdp = (pgd_t *)vcpu->arch.gpa_pgd + __pgd_offset(gpa_address);
+	pudp = (pud_t *)pgdp;
+#ifndef __PAGETABLE_PMD_FOLDED
+	if (*(unsigned long *)pudp == (unsigned long)invalid_pmd_table) {
+		/* allocate a page for pmd and initialize it */
+		ptr = (unsigned long *)__get_free_page(GFP_KERNEL|GFP_DMA);
+		for (i = 0; i < PTRS_PER_PMD; i++)
+			ptr[i] = (unsigned long)invalid_pte_table;
+		*(unsigned long *)pudp = (unsigned long)ptr;
+	}
+	pmdp = pmd_offset(pudp, gpa_address);
+	if (*(unsigned long *)pmdp == (unsigned long)invalid_pte_table) {
+		/* allocate a page for pte table and initialize it */
+		ptr = (unsigned long *)__get_free_page(GFP_KERNEL|GFP_DMA);
+		for (i = 0; i < PTRS_PER_PTE; i++)
+			ptr[i] = 0;
+		*(unsigned long *)pmdp = (unsigned long)ptr;
+	}
+	ptep = pte_offset(pmdp, gpa_address);
+#else
+	if (*(unsigned long *)pudp == (unsigned long)invalid_pte_table) {
+		/* allocate a page for pte table and initialize it */
+		ptr = (unsigned long *)__get_free_page(GFP_KERNEL|GFP_DMA);
+		for (i = 0; i < PTRS_PER_PTE; i++)
+			ptr[i] = 0;
+		*(unsigned long *)pudp = (unsigned long)ptr;
+	}
+	ptep = pte_offset((pmd_t *)pudp, gpa_address);
+#endif
+	*page_size = PAGE_SIZE;
+	*ptep = pte;
+	*pa_address = (pte_val(pte) >> (_PAGE_GLOBAL_SHIFT + 6)) << 12;
+}
+#endif
+
 /*
  * This routine handles page faults.  It determines the address,
  * and the problem, and then passes it off to one of the appropriate
@@ -42,6 +107,9 @@ asmlinkage void __kprobes do_page_fault(struct pt_regs *regs, unsigned long writ
 	siginfo_t info;
 	int fault;
 	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
+#ifdef CONFIG_KVM_NETL
+	unsigned long gpa_address = 0;
+#endif
 
 #if 0
 	printk("Cpu%d[%s:%d:%0*lx:%ld:%0*lx]\n", raw_smp_processor_id(),
@@ -49,6 +117,29 @@ asmlinkage void __kprobes do_page_fault(struct pt_regs *regs, unsigned long writ
 	       field, regs->cp0_epc);
 #endif
 
+#ifdef CONFIG_KVM_NETL
+	/* If a page fault for a guest memory, change "address" (gpa) to hva */
+	if (regs->cp0_guestctl0 >> 31) {
+		int t;
+		struct kvm_vcpu *vcpu = (struct kvm_vcpu *)regs->cp0_osscratch7;
+
+		gpa_address = address;
+		for (t = 0; t < KVM_USER_MEM_SLOTS; t++) {
+			struct kvm_memory_slot *s = &vcpu->kvm->memslots->memslots[t];
+
+			if (gpa_address >= (s->base_gfn << PAGE_SHIFT)
+				&& gpa_address < ((s->base_gfn + s->npages) << PAGE_SHIFT)) {
+				address = s->userspace_addr + gpa_address - (s->base_gfn << PAGE_SHIFT);
+				break;
+			}
+		}
+
+		if (t == KVM_USER_MEM_SLOTS) {
+			goto bad_area_nosemaphore;
+		}
+	}
+#endif
+
 #ifdef CONFIG_KPROBES
 	/*
 	 * This is to notify the fault handler of the kprobes.	The
@@ -186,6 +277,21 @@ good_area:
 	}
 
 	up_read(&mm->mmap_sem);
+
+#ifdef CONFIG_KVM_NETL
+	if (regs->cp0_guestctl0 >> 31) {
+		unsigned long pa_address = 0, page_size;
+
+		/* Populate the gpa->pa page table. */
+		nlm_kvm_populate_page(regs, address, gpa_address, &pa_address, &page_size);
+
+		/* flush the Reset Vector region as it will be accessed as uncached. */
+		if ((gpa_address & 0xfffffffffff00000ULL) == 0x1fc00000) {
+			nlm_flush_cache_L2L3(pa_address, page_size);
+		}
+	}
+#endif
+
 	return;
 
 /*
diff --git a/arch/mips/mm/tlb-r4k.c b/arch/mips/mm/tlb-r4k.c
index 355bc81..851a670 100644
--- a/arch/mips/mm/tlb-r4k.c
+++ b/arch/mips/mm/tlb-r4k.c
@@ -368,6 +368,46 @@ void add_wired_entry(unsigned long entrylo0, unsigned long entrylo1,
 	local_flush_tlb_all();
 	EXIT_CRITICAL(flags);
 }
+#if defined(CONFIG_CPU_XLP) && defined(CONFIG_MIPS_HUGE_TLB_SUPPORT)
+asmlinkage void do_hugetlb_invalidate(__attribute__((unused)) struct pt_regs *regs)
+{
+	int idx;
+	uint64_t oldpid;
+#ifdef CONFIG_KVM_NETL
+	uint32_t oldgctl1 = 0x0;
+
+	if (regs->cp0_guestctl0 >> 31)
+	{
+		oldgctl1 = read_c0_guestctl1();
+		write_c0_guestctl1(regs->cp0_guestctl1);
+	}
+#endif
+
+	oldpid = read_c0_entryhi();
+	tlb_probe();
+	tlb_probe_hazard();
+	idx = read_c0_index();
+#ifdef CONFIG_KVM_NETL
+	if (regs->cp0_guestctl0 >> 31)
+		write_c0_guestctl1(oldgctl1);
+#endif
+
+	if (idx > 0) {
+		int ridx = idx & 0x1fff;
+
+		if (ridx > ((read_c0_config6() >> 6) & 0x3ff)) {
+			/* Make sure all entries differ. */
+			write_c0_entrylo0(0);
+			write_c0_entrylo1(0);
+			write_c0_entryhi(UNIQUE_ENTRYHI(ridx));
+			mtc0_tlbw_hazard();
+			tlb_write_indexed();
+			tlbw_use_hazard();
+			write_c0_entryhi(oldpid);
+		}
+	}
+}
+#endif
 
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
 
diff --git a/arch/mips/mm/tlbex-fault.S b/arch/mips/mm/tlbex-fault.S
index 318855e..66472fa 100644
--- a/arch/mips/mm/tlbex-fault.S
+++ b/arch/mips/mm/tlbex-fault.S
@@ -13,6 +13,30 @@
 	.macro tlb_do_page_fault, write
 	NESTED(tlb_do_page_fault_\write, PT_SIZE, sp)
 	SAVE_ALL
+
+#if defined(CONFIG_CPU_XLP) && defined(CONFIG_MIPS_HUGE_TLB_SUPPORT)
+	move	a0, sp
+	/* invalidate the tlb entry */
+	jal	do_hugetlb_invalidate
+	nop
+#endif
+#ifdef CONFIG_KVM_NETL
+	/* guest check for memory fault */
+	MFC0	a2, CP0_BADVADDR
+	move	a0, sp
+	li	a1, \write
+	jal	do_guest_fault_check
+	nop
+
+	beqz	v0, 1f
+	nop
+
+	PTR_LA	ra, ret_from_exception
+	jr	ra
+	nop
+1:
+#endif
+
 	MFC0	a2, CP0_BADVADDR
 	KMODE
 	move	a0, sp
diff --git a/arch/mips/mm/tlbex.c b/arch/mips/mm/tlbex.c
index 6c44494..144ae16 100644
--- a/arch/mips/mm/tlbex.c
+++ b/arch/mips/mm/tlbex.c
@@ -160,6 +160,9 @@ enum label_id {
 	label_large_segbits_fault,
 #ifdef CONFIG_MIPS_HUGE_TLB_SUPPORT
 	label_tlb_huge_update,
+#ifdef CONFIG_CPU_XLP
+	label_r4000_write_huge_probe_fail,
+#endif
 #endif
 };
 
@@ -280,6 +283,10 @@ static inline void dump_handler(const char *symbol, const u32 *handler, int coun
 #define C0_ENTRYHI	10, 0
 #define C0_EPC		14, 0
 #define C0_XCONTEXT	20, 0
+#ifdef CONFIG_KVM_NETL
+#define C0_KSCRATCH1	31, 2
+#define C0_KSCRATCH2	31, 3
+#endif
 
 #ifdef CONFIG_64BIT
 # define GET_CONTEXT(buf, reg) UASM_i_MFC0(buf, reg, C0_XCONTEXT)
@@ -303,7 +310,15 @@ static struct uasm_reloc relocs[128] __cpuinitdata;
 
 static int check_for_high_segbits __cpuinitdata;
 
+#ifdef CONFIG_KVM_NETL
+#ifdef CONFIG_CPU_XLP
+static unsigned int kscratch_used_mask __cpuinitdata = (0x1 << 0) | (0x1 << 1);
+#else
+static unsigned int kscratch_used_mask __cpuinitdata = (0x1 << 2) | (0x1 << 3);
+#endif
+#else
 static unsigned int kscratch_used_mask __cpuinitdata;
+#endif
 
 static inline int __maybe_unused c0_kscratch(void)
 {
@@ -1236,6 +1251,11 @@ build_fast_tlb_refill_handler (u32 **p, struct uasm_label **l,
 		rv.restore_scratch = 1;
 	}
 
+#ifdef CONFIG_KVM_NETL
+	uasm_i_dmfc0(p, K0, C0_KSCRATCH1);
+	uasm_i_dmfc0(p, K1, C0_KSCRATCH2);
+#endif
+
 	uasm_i_eret(p); /* return from trap */
 
 	return rv;
@@ -1264,6 +1284,10 @@ static void __cpuinit build_r4000_tlb_refill_handler(void)
 	memset(relocs, 0, sizeof(relocs));
 	memset(final_handler, 0, sizeof(final_handler));
 
+#ifdef CONFIG_KVM_NETL
+	uasm_i_dmtc0(&p, K0, C0_KSCRATCH1);
+	uasm_i_dmtc0(&p, K1, C0_KSCRATCH2);
+#endif
 	if (current_cpu_type() == CPU_XLP)
 		uasm_i_ehb(&p);
 
@@ -1307,6 +1331,10 @@ static void __cpuinit build_r4000_tlb_refill_handler(void)
 		build_update_entries(&p, K0, K1);
 		build_tlb_write_entry(&p, &l, &r, tlb_random);
 		uasm_l_leave(&l, p);
+#ifdef CONFIG_KVM_NETL
+		uasm_i_dmfc0(&p, K0, C0_KSCRATCH1);
+		uasm_i_dmfc0(&p, K1, C0_KSCRATCH2);
+#endif
 		uasm_i_eret(&p); /* return from trap */
 	}
 #ifdef CONFIG_MIPS_HUGE_TLB_SUPPORT
@@ -1449,7 +1477,14 @@ static void __cpuinit build_setup_pgd(void)
 					sizeof(tlbmiss_handler_setup_pgd[0]));
 	memset(labels, 0, sizeof(labels));
 	memset(relocs, 0, sizeof(relocs));
+#ifdef CONFIG_KVM_NETL
+	/* Keep it simple now, do not store pgd in register so that for guest run, we
+	 * do not need to populate this register.
+	 */
+	pgd_reg = -1;
+#else
 	pgd_reg = allocate_kscratch();
+#endif
 #ifdef CONFIG_MIPS_PGD_C0_CONTEXT
 	if (pgd_reg == -1) {
 		struct uasm_label *l = labels;
@@ -1897,6 +1932,11 @@ build_r4000_tlbchange_handler_tail(u32 **p, struct uasm_label **l,
 	build_tlb_write_entry(p, l, r, tlb_indexed);
 	uasm_l_leave(l, *p);
 	build_restore_work_registers(p);
+#ifdef CONFIG_KVM_NETL
+	uasm_i_dmfc0(p, K0, C0_KSCRATCH1);
+	uasm_i_dmfc0(p, K1, C0_KSCRATCH2);
+#endif
+
 	uasm_i_eret(p); /* return from trap */
 
 #ifdef CONFIG_64BIT
diff --git a/arch/mips/netlogic/Makefile b/arch/mips/netlogic/Makefile
index 93a849f..7ad0314 100644
--- a/arch/mips/netlogic/Makefile
+++ b/arch/mips/netlogic/Makefile
@@ -3,3 +3,4 @@ obj-$(CONFIG_CPU_XLR)		+=	xlr/
 obj-$(CONFIG_CPU_XLP)		+=	xlp/
 obj-$(CONFIG_CPU_XLP)		+=	dts/
 obj-$(CONFIG_NLM_XLP_BOARD)    	+=      lib/
+obj-$(CONFIG_KVM_NETL)		+=	kvm-netl/
diff --git a/arch/mips/netlogic/kvm-netl/Makefile b/arch/mips/netlogic/kvm-netl/Makefile
new file mode 100644
index 0000000..28ecf29
--- /dev/null
+++ b/arch/mips/netlogic/kvm-netl/Makefile
@@ -0,0 +1,5 @@
+EXTRA_CFLAGS := -Werror
+EXTRA_CFLAGS := $(CFLAGS)
+
+obj-y = kvm_traps.o kvm_fault.o kvm_pic.o kvm_sysmgt.o kvm_fuse.o \
+	kvm_bridge.o kvm_pcie.o kvm_clk.o
diff --git a/arch/mips/netlogic/kvm-netl/kvm_bridge.c b/arch/mips/netlogic/kvm-netl/kvm_bridge.c
new file mode 100644
index 0000000..840c216
--- /dev/null
+++ b/arch/mips/netlogic/kvm-netl/kvm_bridge.c
@@ -0,0 +1,112 @@
+/*
+ * Copyright (c) 2003-2013 Broadcom Corporation
+ * All Rights Reserved
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the Broadcom
+ * license below:
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in
+ *    the documentation and/or other materials provided with the
+ *    distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY BROADCOM ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL BROADCOM OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
+ * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
+ * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
+ * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <linux/signal.h>
+#include <linux/sched.h>
+#include <linux/interrupt.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/ptrace.h>
+#include <linux/mman.h>
+#include <linux/mm.h>
+#include <linux/smp.h>
+#include <linux/vt_kern.h>		/* For unblank_screen() */
+#include <linux/module.h>
+#include <linux/kvm.h>
+#include <linux/kvm_host.h>
+
+#include <asm/branch.h>
+#include <asm/mmu_context.h>
+#include <asm/uaccess.h>
+#include <asm/ptrace.h>
+#include <asm/highmem.h>		/* For VMALLOC_END */
+
+#include <asm/netlogic/kvm_xlp.h>
+
+static inline void unhandled_exception(const char *func, unsigned long addr,
+	unsigned long epc, unsigned long write)
+{
+	printk("%s: unhandled address %lx, epc %lx, is_write %lx\n",
+		func, addr, epc, write);
+}
+
+void kvm_handle_pcie_bridge(struct pt_regs *regs, unsigned long write,
+		unsigned long address, unsigned long reg_num)
+{
+	unsigned int rindex = address >> 2;
+	struct kvm_arch *arch = kvm_get_arch(regs);
+
+// printk("!!! bridge register: 0x%x, reg_num: %d, epc: 0x%lx\n", rindex, (int)reg_num, regs->cp0_epc);
+
+	if (rindex <= 0x0f) {
+		if (write) {
+			arch->bridge.config[rindex] = regs->regs[reg_num];
+		} else {
+			regs->regs[reg_num] = arch->bridge.config[rindex];
+		}
+		return;
+	}
+	else if (rindex == 0x99 || rindex == 0x9a || rindex == 0x9b
+		|| rindex == 0x9c) {
+		if (!write) {
+			regs->regs[reg_num] = arch->bridge.pcie_membase[rindex - 0x99];
+			return;
+		}
+	}
+	else if (rindex == 0x9d || rindex == 0x9e || rindex == 0x9f
+		|| rindex == 0xa0) {
+		if (!write) {
+			regs->regs[reg_num] = arch->bridge.pcie_memlimit[rindex - 0x9d];
+			return;
+		}
+	}
+	else if (rindex == 0xa1 || rindex == 0xa2 || rindex == 0xa3
+		|| rindex == 0xa4) {
+		if (!write) {
+			regs->regs[reg_num] = arch->bridge.pcie_iobase[rindex - 0xa1];
+			return;
+		}
+	}
+	else if (rindex == 0xa5 || rindex == 0xa6 || rindex == 0xa7
+		|| rindex == 0xa8) {
+		if (!write) {
+			regs->regs[reg_num] = arch->bridge.pcie_iolimit[rindex - 0xa5];
+			return;
+		}
+	}
+
+	unhandled_exception(__FUNCTION__, address, regs->cp0_epc, write);
+}
diff --git a/arch/mips/netlogic/kvm-netl/kvm_clk.c b/arch/mips/netlogic/kvm-netl/kvm_clk.c
new file mode 100644
index 0000000..55273e8
--- /dev/null
+++ b/arch/mips/netlogic/kvm-netl/kvm_clk.c
@@ -0,0 +1,94 @@
+/*
+ * Copyright (c) 2003-2013 Broadcom Corporation
+ * All Rights Reserved
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the Broadcom
+ * license below:
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in
+ *    the documentation and/or other materials provided with the
+ *    distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY BROADCOM ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL BROADCOM OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
+ * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
+ * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
+ * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <linux/signal.h>
+#include <linux/sched.h>
+#include <linux/interrupt.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/ptrace.h>
+#include <linux/mman.h>
+#include <linux/mm.h>
+#include <linux/smp.h>
+#include <linux/vt_kern.h>		/* For unblank_screen() */
+#include <linux/module.h>
+#include <linux/kvm.h>
+#include <linux/kvm_host.h>
+
+#include <asm/branch.h>
+#include <asm/mmu_context.h>
+#include <asm/uaccess.h>
+#include <asm/ptrace.h>
+#include <asm/highmem.h>		/* For VMALLOC_END */
+
+#include <asm/netlogic/kvm_xlp.h>
+#include <asm/netlogic/haldefs.h>
+#include <asm/netlogic/xlp-hal/iomap.h>
+
+static inline void unhandled_exception(const char *func, unsigned long addr,
+	unsigned long epc, unsigned long write)
+{
+	printk("%s: unhandled address %lx, epc %lx, is_write %lx\n",
+		func, addr, epc, write);
+}
+
+void kvm_handle_pcie_clk(struct pt_regs *regs, unsigned long write,
+		unsigned long address, unsigned long reg_num)
+{
+	unsigned int rindex = (address - XLP_IO_PCI_HDRSZ) >> 2;
+	struct kvm_arch *arch = kvm_get_arch(regs);
+
+	if (!write) {
+		switch (rindex) {
+			case 0:
+				regs->regs[reg_num] = arch->clkmgt.reg_0;
+				break;
+			case 0xc0 ... 0x10f:
+				regs->regs[reg_num] = arch->clkmgt.reg_cpupllctrl[rindex-0xc0];
+				break;
+			case 0x188:
+				regs->regs[reg_num] = arch->clkmgt.reg_cpupllchgctrl;
+				break;
+			default:
+				unhandled_exception(__FUNCTION__, address, regs->cp0_epc, write);
+				break;
+		}
+	}
+	else
+		unhandled_exception(__FUNCTION__, address, regs->cp0_epc, write);
+
+	return;
+}
diff --git a/arch/mips/netlogic/kvm-netl/kvm_fault.c b/arch/mips/netlogic/kvm-netl/kvm_fault.c
new file mode 100644
index 0000000..ed10aae
--- /dev/null
+++ b/arch/mips/netlogic/kvm-netl/kvm_fault.c
@@ -0,0 +1,303 @@
+/*
+ * Copyright (c) 2003-2013 Broadcom Corporation
+ * All Rights Reserved
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the Broadcom
+ * license below:
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in
+ *    the documentation and/or other materials provided with the
+ *    distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY BROADCOM ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL BROADCOM OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
+ * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
+ * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
+ * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <linux/signal.h>
+#include <linux/sched.h>
+#include <linux/interrupt.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/ptrace.h>
+#include <linux/mman.h>
+#include <linux/mm.h>
+#include <linux/smp.h>
+#include <linux/vt_kern.h>		/* For unblank_screen() */
+#include <linux/module.h>
+#include <linux/kvm.h>
+#include <linux/kvm_host.h>
+
+#include <asm/branch.h>
+#include <asm/mmu_context.h>
+#include <asm/uaccess.h>
+#include <asm/ptrace.h>
+#include <asm/highmem.h>		/* For VMALLOC_END */
+
+#include <asm/netlogic/kvm_xlp.h>
+
+/*
+ * This file contains the implementation to virtualize
+ * PCIe configuration space and PCIe memory mapped space
+ * for various I/O blocks.
+ * 
+ * 1. The physical memory region of 0x18000000 - 0x1c000000 (64MB) is reserved
+ *    in guest for PCIe configuration space access.
+ * 2. The physical memory region of 0x1c000000 - 0x1d000000 (16MB) is reserved
+ *    in guest for PCI configuration space access.
+ * 2. The physical memory region of 0xd0000000 - 0xe0000000 (256MB) is reserved
+ *    for memory mapped device.
+ */
+
+static inline void unhandled_exception(const char *func, unsigned long addr,
+	unsigned long epc, unsigned long write)
+{
+	printk("%s: unhandled address/offset %lx, epc %lx, is_write %lx\n",
+		func, addr, epc, write);
+}
+
+static void handle_pci_config_space(struct pt_regs *regs, unsigned long write,
+		unsigned long address, unsigned long reg_num)
+{
+
+	/* FIXME: workaround */
+	if (write)
+		return;
+
+	regs->regs[reg_num] = *(unsigned int *)(0x9000000000000000ULL | address);
+
+#if 0
+	if (address == 0x1c000000) {
+		if (!write)
+			regs->regs[reg_num] = 0x1001184e;
+		else
+			printk("ignore guest pci write: %lx, epc %lx, val %lx\n",
+				address, regs->cp0_epc, regs->regs[reg_num]);
+	} else if (address == 0x1c000004) {
+		if (!write)
+			regs->regs[reg_num] = 0x4;
+		else
+			printk("ignore guest pci write: %lx, epc %lx, val %lx\n",
+				address, regs->cp0_epc, regs->regs[reg_num]);
+	} else if (address == 0x1c000008) {
+		if (!write)
+			regs->regs[reg_num] = 0xffffffffff000000;
+		else
+			printk("ignore guest pci write: %lx, epc %lx, val %lx\n",
+				address, regs->cp0_epc, regs->regs[reg_num]);
+	} else if (address == 0x1c00000c) {
+		if (!write)
+			regs->regs[reg_num] = 0x808008;
+		else
+			printk("ignore guest pci write: %lx, epc %lx, val %lx\n",
+				address, regs->cp0_epc, regs->regs[reg_num]);
+	} else if (address == 0x1c000010
+			|| address == 0x1c000014
+			|| address == 0x1c000018
+			|| address == 0x1c00001c
+			|| address == 0x1c000020
+			|| address == 0x1c000024
+			|| address == 0x1c000030
+			|| address == 0x1c00003c
+		  ) {
+		if (!write)
+			regs->regs[reg_num] = 0x0;
+		else
+			printk("ignore guest pci write: %lx, epc %lx, val %lx\n",
+				address, regs->cp0_epc, regs->regs[reg_num]);
+	} else if (address == 0x1c00002c) {
+		if (!write)
+			regs->regs[reg_num] = 0xffffffffaaaaaaaa;
+		else
+			printk("ignore guest pci write: %lx, epc %lx, val %lx\n",
+				address, regs->cp0_epc, regs->regs[reg_num]);
+	} else
+		panic("caught one guest pci address: %lx, epc %lx\n",
+			address, regs->cp0_epc);
+#endif
+}
+
+asmlinkage int do_guest_fault_check(struct pt_regs *regs, unsigned long write,
+				    unsigned long address)
+{
+	unsigned int reg_num = 0;
+	unsigned int badinstr, epc_badinstr;
+
+	/* check whether the fault is from guest. */
+	if ((regs->cp0_guestctl0 >> 31) == 0)
+		return 0;
+
+	if (address >= 0x14000000 && address < 0x16000000) {
+		/* I/O address */
+		kvm_get_badinstr(regs, &badinstr, &epc_badinstr);
+
+		if ((badinstr & 0xfc000000) == 0x8c000000 /* lw */
+		    || (badinstr & 0xfc000000) == 0xac000000 /* sw */
+		    || (badinstr & 0xfc000000) == 0x80000000 /* lb */
+		    || (badinstr & 0xfc000000) == 0xa0000000 /* sb */
+		    || (badinstr & 0xfc000000) == 0x90000000 /* lbu */
+		    || (badinstr & 0xfc000000) == 0xa4000000 /* sh */
+		    || (badinstr & 0xfc000000) == 0x94000000 /* lhu */
+		    || (badinstr & 0xfc000000) == 0xdc000000 /* ld */
+		    || (badinstr & 0xfc000000) == 0xfc000000 /* sd */
+		   )
+			reg_num = (badinstr >> 16) & 0x1f;
+                else {
+			printk("%s: 0x%x, unhandled epc %lx, address %lx\n",
+				__FUNCTION__, badinstr, regs->cp0_epc, address);
+			return 0;
+                }
+
+		kvm_handle_pcie_io(regs, write, address, reg_num, badinstr, epc_badinstr);
+
+		compute_guest_return_epc(regs, epc_badinstr);
+
+		return 1;
+	} else if (address >= 0x18000000 && address < 0x1c000000) {
+		/* pcie register configuration space */
+
+		kvm_get_badinstr(regs, &badinstr, &epc_badinstr);
+
+		/* This is the pcie configuration space. The only instructions
+		 * which can access here is lw/sw/ld/sd.
+		 * Decode the badinstr.
+		 */
+		if ((badinstr & 0xfc000000) == 0x8c000000 /* lw */
+		    || (badinstr & 0xfc000000) == 0xac000000 /* sw */
+		    || (badinstr & 0xfc000000) == 0xdc000000 /* ld */
+		    || (badinstr & 0xfc000000) == 0xfc000000 /* sd */
+		   )
+			reg_num = (badinstr >> 16) & 0x1f;
+		else {
+			/* The guest gives us wrong information. What to do? */
+			printk("%s: unhandled epc %lx, address %lx\n",
+				__FUNCTION__, regs->cp0_epc, address);
+			return 0;
+		}
+
+		if ((address & 0xfffff000) == 0x18000000) {
+			/* bridge: bus 0, dev 0, func 0 */
+			kvm_handle_pcie_bridge(regs, write, address & 0xfff, reg_num);
+		}
+		else if ((address & 0xfffff000) == 0x18108000) {
+			/* pcie link 0: dev 1, func 0 */
+			kvm_handle_pcie_pcie(regs, write, address, reg_num, badinstr, epc_badinstr);
+		}
+		else if ((address & 0xfffff000) == 0x18109000) {
+			/* pcie link 1: dev 1, func 1 */
+			kvm_handle_pcie_pcie(regs, write, address, reg_num, badinstr, epc_badinstr);
+		}
+		else if ((address & 0xfffff000) == 0x1810a000) {
+			/* pcie link 2: dev 1, func 2 */
+			kvm_handle_pcie_pcie(regs, write, address, reg_num, badinstr, epc_badinstr);
+		}
+		else if ((address & 0xfffff000) == 0x1810b000) {
+			/* pcie link 3: dev 1, func 3 */
+			kvm_handle_pcie_pcie(regs, write, address, reg_num, badinstr, epc_badinstr);
+		}
+		else if ((address & 0xfffff000) == 0x18110000) {
+			/* pic: dev 2, func 0 */
+			kvm_handle_pcie_pic(regs, write, address & 0xfff, reg_num);
+		}
+		else if ((address & 0xfffff000) == 0x18112000) {
+			/* uart: dev 2, func 2. emulated using qemu */
+			kvm_handle_pcie_pcie(regs, write, address, reg_num, badinstr, epc_badinstr);
+		}
+		else if ((address & 0xfffff000) == 0x18130000) {
+			/* sys management: dev 6, func 0 */
+			kvm_handle_pcie_sysmgt(regs, write, address & 0xfff, reg_num);
+		}
+		else if ((address & 0xfffff000) == 0x18131000) {
+			/* fuse: dev 6, func 1 */
+			kvm_handle_pcie_fuse(regs, write, address & 0xfff, reg_num);
+		}
+		else if ((address & 0xfffff000) == 0x18132000) {
+			/* fuse: dev 6, func 2 */
+			kvm_handle_pcie_clk(regs, write, address & 0xfff, reg_num);
+		}
+		else if (((address & 0xfff00000) != 0x18000000) && ((address & 0xfff00000) != 0x18100000)) {
+			/* not bus 0, bus 1, a real bridge */
+			kvm_handle_pcie_pcie(regs, write, address, reg_num, badinstr, epc_badinstr);
+		} else {
+			/* disable the device */
+			if (!write && (address & 0xfff) == 0)
+				regs->regs[reg_num] = 0xffffffff;
+			else {
+				printk("caught one guest pcie address: %lx, epc %lx\n",
+						address, regs->cp0_epc);
+			}
+		}
+
+		compute_guest_return_epc(regs, epc_badinstr);
+		return 1;
+	} else if (address >= 0x1c000000 && address < 0x1d000000) {
+		/* pci register configuration space */
+
+		kvm_get_badinstr(regs, &badinstr, &epc_badinstr);
+
+		if ((badinstr & 0xfc000000) == 0x8c000000 /* lw */
+		    || (badinstr & 0xfc000000) == 0xac000000 /* sw */
+		    || (badinstr & 0xfc000000) == 0xdc000000 /* ld */
+		    || (badinstr & 0xfc000000) == 0xfc000000 /* sd */
+		   )
+			reg_num = (badinstr >> 16) & 0x1f;
+		else {
+			printk("%s: unhandled epc %lx, address %lx\n",
+				__FUNCTION__, regs->cp0_epc, address);
+			return 0;
+		}
+
+		handle_pci_config_space(regs, write, address, reg_num);
+		compute_guest_return_epc(regs, epc_badinstr);
+
+		return 1;
+	} else if (address >= 0xd0000000ul && address < 0xe0000000ul) {
+
+		kvm_get_badinstr(regs, &badinstr, &epc_badinstr);
+
+		if ((badinstr & 0xfc000000) == 0x8c000000 /* lw */
+		    || (badinstr & 0xfc000000) == 0xac000000 /* sw */
+		    || (badinstr & 0xfc000000) == 0x80000000 /* lb */
+		    || (badinstr & 0xfc000000) == 0xa0000000 /* sb */
+		    || (badinstr & 0xfc000000) == 0x90000000 /* lbu */
+		    || (badinstr & 0xfc000000) == 0xa4000000 /* sh */
+		    || (badinstr & 0xfc000000) == 0x94000000 /* lhu */
+		    || (badinstr & 0xfc000000) == 0xdc000000 /* ld */
+		    || (badinstr & 0xfc000000) == 0xfc000000 /* sd */
+		   )
+			reg_num = (badinstr >> 16) & 0x1f;
+                else {
+			printk("%s: 0x%x, unhandled epc %lx, address %lx\n",
+				__FUNCTION__, badinstr, regs->cp0_epc, address);
+			return 0;
+                }
+
+                kvm_handle_pcie_pcie(regs, write, address, reg_num, badinstr, epc_badinstr);
+
+		compute_guest_return_epc(regs, epc_badinstr);
+
+		return 1;
+	}
+
+	return 0;
+}
diff --git a/arch/mips/netlogic/kvm-netl/kvm_fuse.c b/arch/mips/netlogic/kvm-netl/kvm_fuse.c
new file mode 100644
index 0000000..30a150b
--- /dev/null
+++ b/arch/mips/netlogic/kvm-netl/kvm_fuse.c
@@ -0,0 +1,80 @@
+/*
+ * Copyright (c) 2003-2013 Broadcom Corporation
+ * All Rights Reserved
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the Broadcom
+ * license below:
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in
+ *    the documentation and/or other materials provided with the
+ *    distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY BROADCOM ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL BROADCOM OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
+ * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
+ * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
+ * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <linux/signal.h>
+#include <linux/sched.h>
+#include <linux/interrupt.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/ptrace.h>
+#include <linux/mman.h>
+#include <linux/mm.h>
+#include <linux/smp.h>
+#include <linux/vt_kern.h>		/* For unblank_screen() */
+#include <linux/module.h>
+#include <linux/kvm.h>
+#include <linux/kvm_host.h>
+
+#include <asm/branch.h>
+#include <asm/mmu_context.h>
+#include <asm/uaccess.h>
+#include <asm/ptrace.h>
+#include <asm/highmem.h>		/* For VMALLOC_END */
+
+#include <asm/netlogic/kvm_xlp.h>
+
+static inline void unhandled_exception(const char *func, unsigned long addr,
+	unsigned long epc, unsigned long write)
+{
+	printk("%s: unhandled address %lx, epc %lx, is_write %lx\n",
+		func, addr, epc, write);
+}
+
+void kvm_handle_pcie_fuse(struct pt_regs *regs, unsigned long write,
+		unsigned long address, unsigned long reg_num)
+{
+	unsigned int rindex = address >> 2;
+
+	if (rindex == 0x106) {
+		/* cpu disables */
+		if (!write) {
+			regs->regs[reg_num] = 0x0;
+			return;
+		}
+	}
+
+	unhandled_exception(__FUNCTION__, address, regs->cp0_epc, write);
+}
diff --git a/arch/mips/netlogic/kvm-netl/kvm_pcie.c b/arch/mips/netlogic/kvm-netl/kvm_pcie.c
new file mode 100644
index 0000000..8ee420b
--- /dev/null
+++ b/arch/mips/netlogic/kvm-netl/kvm_pcie.c
@@ -0,0 +1,157 @@
+/*
+ * Copyright (c) 2003-2013 Broadcom Corporation
+ * All Rights Reserved
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the Broadcom
+ * license below:
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in
+ *    the documentation and/or other materials provided with the
+ *    distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY BROADCOM ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL BROADCOM OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
+ * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
+ * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
+ * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <linux/signal.h>
+#include <linux/sched.h>
+#include <linux/interrupt.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/ptrace.h>
+#include <linux/mman.h>
+#include <linux/mm.h>
+#include <linux/smp.h>
+#include <linux/vt_kern.h>		/* For unblank_screen() */
+#include <linux/module.h>
+#include <linux/kvm.h>
+#include <linux/kvm_host.h>
+
+#include <asm/branch.h>
+#include <asm/mmu_context.h>
+#include <asm/uaccess.h>
+#include <asm/ptrace.h>
+#include <asm/highmem.h>		/* For VMALLOC_END */
+
+#include <asm/netlogic/kvm_xlp.h>
+
+void kvm_handle_pcie_pcie(struct pt_regs *regs, unsigned long write,
+		unsigned long address, unsigned long reg_num, uint32_t badinstr,
+		uint32_t epc_badinstr)
+{
+	uint32_t val;
+	struct kvm_vcpu *vcpu = (struct kvm_vcpu *)regs->cp0_osscratch7;
+
+#if 0
+	printk("=== pcie_pcie: address 0x%lx, epc 0x%lx, write %ld\n",
+		address, regs->cp0_epc, write);
+#endif
+
+	val = KVM_EXIT_MMIO << 24;
+	vcpu->arch.mmio.addr = address;
+	vcpu->arch.mmio.is_write = write;
+	vcpu->arch.mmio.reg_num = reg_num;
+
+	if ((badinstr & 0xfc000000) == 0x80000000
+	    || (badinstr & 0xfc000000) == 0x90000000
+	    || (badinstr & 0xfc000000) == 0xa0000000)
+		vcpu->arch.mmio.len = 1;
+	else if ((badinstr & 0xfc000000) == 0xa4000000
+	    || (badinstr & 0xfc000000) == 0x94000000)
+		vcpu->arch.mmio.len = 2;
+	else if ((badinstr & 0xfc000000) == 0x8c000000
+	    || (badinstr & 0xfc000000) == 0xac000000)
+		vcpu->arch.mmio.len = 4;
+	else
+		vcpu->arch.mmio.len = 8;
+
+	if (write)
+		vcpu->arch.mmio.val = regs->regs[reg_num];
+
+	compute_guest_return_epc(regs, epc_badinstr);
+	kvm_save_guest_context(regs, kvm_get_vcpu_guest_regs(regs));
+	__kvm_vcpu_leave_guest(vcpu, val);
+}
+
+void kvm_handle_pcie_io(struct pt_regs *regs, unsigned long write,
+		unsigned long address, unsigned long reg_num, uint32_t badinstr,
+		uint32_t epc_badinstr)
+{
+	uint32_t val, size;
+	struct kvm_vcpu *vcpu = (struct kvm_vcpu *)regs->cp0_osscratch7;
+	struct kvm *kvm = vcpu->kvm;
+
+#if 0
+	printk("=== pcie_io: address 0x%lx, epc 0x%lx, write %ld\n",
+		address, regs->cp0_epc, write);
+#endif
+
+	if ((badinstr & 0xfc000000) == 0x80000000
+	    || (badinstr & 0xfc000000) == 0x90000000
+	    || (badinstr & 0xfc000000) == 0xa0000000) {
+		size = 1;
+		if (write) *(char *)&val = regs->regs[reg_num];
+	} else if ((badinstr & 0xfc000000) == 0xa4000000
+	    || (badinstr & 0xfc000000) == 0x94000000) {
+		size = 2;
+		if (write) *(short *)&val = regs->regs[reg_num];
+	} else if ((badinstr & 0xfc000000) == 0x8c000000
+	    || (badinstr & 0xfc000000) == 0xac000000) {
+		size = 4;
+		if (write) *(int *)&val = regs->regs[reg_num];
+	} else {
+		panic("not supported\n");
+	}
+
+	if (write) {
+		if (kvm_io_bus_write(kvm, KVM_PIO_BUS, address & 0xffff, size, &val) == 0)
+			return;
+	} else {
+		if (kvm_io_bus_read(kvm, KVM_PIO_BUS, address & 0xffff, size, &val) == 0) {
+			if (size == 1)
+				regs->regs[reg_num] = *(char *)&val;
+			else if (size == 2)
+				regs->regs[reg_num] = *(short *)&val;
+			else
+				regs->regs[reg_num] = *(int *)&val;
+			return;
+		}
+	}
+
+	val = KVM_EXIT_IO << 24;
+	vcpu->arch.pio.direction = (write ? KVM_EXIT_IO_OUT : KVM_EXIT_IO_IN);
+	vcpu->arch.pio.port = address & 0xffff; /* ignore upper bits */
+	vcpu->arch.pio.size = size;
+	vcpu->arch.pio.count = 1;
+	vcpu->arch.pio.data_offset = (uint64_t)&(((struct kvm_run *)0)->mips_exit_io.data);
+
+	if (write)
+		vcpu->arch.pio.val = regs->regs[reg_num];
+
+	vcpu->arch.pio.reg_num = reg_num;
+
+	compute_guest_return_epc(regs, epc_badinstr);
+	kvm_save_guest_context(regs, kvm_get_vcpu_guest_regs(regs));
+	__kvm_vcpu_leave_guest(vcpu, val);
+}
diff --git a/arch/mips/netlogic/kvm-netl/kvm_pic.c b/arch/mips/netlogic/kvm-netl/kvm_pic.c
new file mode 100644
index 0000000..4cc63c8
--- /dev/null
+++ b/arch/mips/netlogic/kvm-netl/kvm_pic.c
@@ -0,0 +1,249 @@
+/*
+ * Copyright (c) 2003-2013 Broadcom Corporation
+ * All Rights Reserved
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the Broadcom
+ * license below:
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in
+ *    the documentation and/or other materials provided with the
+ *    distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY BROADCOM ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL BROADCOM OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
+ * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
+ * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
+ * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <linux/signal.h>
+#include <linux/sched.h>
+#include <linux/interrupt.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/ptrace.h>
+#include <linux/mman.h>
+#include <linux/mm.h>
+#include <linux/smp.h>
+#include <linux/vt_kern.h>		/* For unblank_screen() */
+#include <linux/module.h>
+#include <linux/kvm.h>
+#include <linux/kvm_host.h>
+
+#include <asm/branch.h>
+#include <asm/mmu_context.h>
+#include <asm/uaccess.h>
+#include <asm/ptrace.h>
+#include <asm/highmem.h>		/* For VMALLOC_END */
+
+#include <asm/netlogic/kvm_xlp.h>
+#include <asm/netlogic/mips-extns.h>
+#include <asm/netlogic/haldefs.h>
+
+#include <asm/netlogic/xlp-hal/xlp.h>
+#include <asm/netlogic/xlp-hal/pic.h>
+
+// #define DEBUG
+static __inline__ void atomic_or_llong(volatile unsigned long long *ptr, unsigned long long val)
+{
+	unsigned long long temp;
+
+	__asm__ volatile (
+		".set push\n"
+		".set noreorder\n"
+		"1: lld %0, %3\n"
+		"or %0, %2\n"
+		"scd %0, %1\n"
+		"beqz %0, 1b\n"
+		"nop\n"
+		".set pop\n"
+		: "=&r"(temp), "=m"(*ptr)
+		: "r"(val), "m"(*ptr)
+		: "memory"
+	);
+
+	return;
+}
+
+static __inline__ void atomic_and_llong(volatile unsigned long long *ptr, unsigned long long val)
+{
+	unsigned long long temp;
+
+	__asm__ volatile (
+		".set push\n"
+		".set noreorder\n"
+		"1: lld %0, %3\n"
+		"and %0, %2\n"
+		"scd %0, %1\n"
+		"beqz %0, 1b\n"
+		"nop\n"
+		".set pop\n"
+		: "=&r"(temp), "=m"(*ptr)
+		: "r"(val), "m"(*ptr)
+		: "memory"
+	);
+
+	return;
+}
+
+static inline void unhandled_exception(const char *func, unsigned long addr,
+	unsigned long epc, unsigned long write)
+{
+	printk("%s: unhandled address %lx, epc %lx, is_write %lx\n",
+		func, addr, epc, write);
+}
+
+void kvm_pic_inject_guest(struct kvm_arch *arch, unsigned int irt, struct pt_regs *regs)
+{
+	unsigned long long irt_entry;
+	unsigned int rvec;
+	struct kvm_vcpu_arch *vcpu_arch;
+
+	irt_entry = arch->pic.u.v64[(0x200 >> 1) + irt];
+	rvec = (irt_entry >> 24) & 0x3f;
+	vcpu_arch = kvm_get_vcpu_arch(regs);
+#ifdef DEBUG
+	printk("===== Inject IRT %u, RVEC %u to the guest (irt_entry %llx)\n", irt, rvec, irt_entry);
+#endif
+	atomic_or_llong(&vcpu_arch->pip_vector, 1ULL << rvec);
+}
+
+void kvm_pic_inject_guest_ext(struct kvm *kvm, struct kvm_arch *arch, unsigned int irt, unsigned int cpuid)
+{
+	unsigned long long irt_entry;
+	unsigned int rvec;
+	struct kvm_vcpu_arch *vcpu_arch;
+
+	/* FIXME: MSIX in 9XX doesn't use PIC and directly interrupts cpu
+	 * using rvec. hacked for now! */
+	if ((irt >= PIC_IRT_PCIE_MSIX_0_INDEX) &&
+		(irt <= (PIC_IRT_PCIE_MSIX_0_INDEX + 31))) {
+		rvec = (irt - PIC_IRT_PCIE_MSIX_0_INDEX) / 32 + PIC_PCIE_MSIX_IRQ_BASE;
+	}
+	else {
+		/* FIXME: We should really look at irt_entry and deliver it to proper cpu
+		 * according to the schedule type.
+		 */
+		irt_entry = arch->pic.u.v64[(0x200 >> 1) + irt];
+		rvec = (irt_entry >> 24) & 0x3f;
+	}
+	vcpu_arch = kvm_get_vcpu_arch_ext(kvm, cpuid);
+#ifdef DEBUG
+	printk("===== Inject IRT %u, RVEC %u to the guest (irt_entry %llx)\n", irt, rvec, irt_entry);
+#endif
+	atomic_or_llong(&vcpu_arch->pip_vector, 1ULL << rvec);
+}
+
+static unsigned int xlp_kvm_get_ipi_cpuid(uint64_t ipi_ctrl)
+{
+	unsigned int mask = ipi_ctrl & 0x3ff;
+	return mask;
+}
+
+void kvm_handle_pcie_pic(struct pt_regs *regs, unsigned long write,
+		unsigned long address, unsigned long reg_num)
+{
+	unsigned int rindex = address >> 2;
+	struct kvm_arch *arch = kvm_get_arch(regs);
+	extern void nlm_send_ipi_single(int lcpu, unsigned int action);
+
+	if (rindex == 0x0 || rindex == 0x2 || rindex == 0x3d) {
+		if (!write)
+			regs->regs[reg_num] = arch->pic.u.v32[rindex];
+		else
+			unhandled_exception(__FUNCTION__, address, regs->cp0_epc, write);
+	} else if (rindex == 0x40) {
+		/* pic control */
+		if (write)
+			arch->pic.u.v64[rindex >> 1] = regs->regs[reg_num]; 
+		else
+			regs->regs[reg_num] = arch->pic.u.v64[rindex >> 1];
+	} else if (rindex == 0x44) {
+		/* watchdog/systimer/other_interrupt status */
+		if (write)
+			arch->pic.u.v64[rindex >> 1] = regs->regs[reg_num];
+		else
+			unhandled_exception(__FUNCTION__, address, regs->cp0_epc, write);
+	} else if (rindex == 0x4e) {
+		/* IPI control: set IPI pending at target cpus */
+		if (write) {
+			uint64_t  val = regs->regs[reg_num];
+			uint32_t  rvec, cpuid;
+			struct kvm_vcpu_arch *vcpu_arch;
+
+			arch->pic.u.v64[rindex >> 1] = val;
+
+			if ((val & (0x7 << 20)) != 0) {
+				printk("===== Non Unicast IPI is not supported\n");
+				return;
+			}
+
+			cpuid = xlp_kvm_get_ipi_cpuid (val);
+#ifdef DEBUG
+printk("Sending %s from cpuid %d to cpuid %d\n", (val & (1 << 23)) ? "NMI" : "IPI",
+	(int)(regs->guest_cp0_ebase & 0x3ff), cpuid);
+#endif
+			vcpu_arch = kvm_get_vcpu_arch_with_cpuid(regs, cpuid);
+			if (val & (1 << 23)) {
+				vcpu_arch->nmi = 1;
+			} else {
+				rvec = (val >> 24) & 0x3f;
+				atomic_or_llong(&vcpu_arch->pip_vector, 1ULL << rvec);
+			}
+		} else
+			regs->regs[reg_num] = arch->pic.u.v64[rindex >> 1];
+
+	} else if (rindex == 0x50) {
+		/* ACK */
+		unsigned int irt = regs->regs[reg_num];
+		unsigned int rvec, irt_entry = arch->pic.u.v64[(0x200 >> 1) + irt];
+		struct kvm_vcpu_arch *vcpu_arch;
+
+		rvec = (irt_entry >> 24) & 0x3f;
+
+		vcpu_arch = kvm_get_vcpu_arch(regs);
+#ifdef DEBUG
+		printk("===== ACK IRT %u RVEC %u from guest\n", irt, rvec);
+#endif
+		/* No need to clear pip_vector as it is already 
+		 * cleared in RESTORE_GUEST. clearing here results in
+		 * interrupt loss in SMP scenario. check EPSW-1139 for more information */
+		//atomic_and_llong(&vcpu_arch->pip_vector, ~(1ULL << rvec));
+
+	} else if (rindex >= 0x74 && rindex <= 0x82) {
+		/* pic timer 0-7 max value */
+		if (write)
+			arch->pic.u.v64[rindex >> 1] = regs->regs[reg_num];
+		else
+			regs->regs[reg_num] = arch->pic.u.v64[rindex >> 1];
+	} else if (rindex >= 0x200 && rindex < 0x400) {
+		/* register 0x200 - 0x400: interrupt redirection table (256 entries) */
+		if (write) {
+#ifdef DEBUG
+			printk("===== Guest IRT index %d, val %lx\n", (rindex - 0x200) >> 1, regs->regs[reg_num]);
+#endif
+			arch->pic.u.v64[rindex >> 1] = regs->regs[reg_num];
+		} else
+			regs->regs[reg_num] = arch->pic.u.v64[rindex >> 1];
+
+	} else
+		unhandled_exception(__FUNCTION__, address, regs->cp0_epc, write);
+}
diff --git a/arch/mips/netlogic/kvm-netl/kvm_sysmgt.c b/arch/mips/netlogic/kvm-netl/kvm_sysmgt.c
new file mode 100644
index 0000000..e805fbd
--- /dev/null
+++ b/arch/mips/netlogic/kvm-netl/kvm_sysmgt.c
@@ -0,0 +1,135 @@
+/*
+ * Copyright (c) 2003-2013 Broadcom Corporation
+ * All Rights Reserved
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the Broadcom
+ * license below:
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in
+ *    the documentation and/or other materials provided with the
+ *    distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY BROADCOM ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL BROADCOM OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
+ * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
+ * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
+ * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <linux/signal.h>
+#include <linux/sched.h>
+#include <linux/interrupt.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/ptrace.h>
+#include <linux/mman.h>
+#include <linux/mm.h>
+#include <linux/smp.h>
+#include <linux/vt_kern.h>		/* For unblank_screen() */
+#include <linux/module.h>
+#include <linux/kvm.h>
+#include <linux/kvm_host.h>
+
+#include <asm/branch.h>
+#include <asm/mmu_context.h>
+#include <asm/uaccess.h>
+#include <asm/ptrace.h>
+#include <asm/highmem.h>		/* For VMALLOC_END */
+
+#include <asm/netlogic/kvm_xlp.h>
+
+static inline void unhandled_exception(const char *func, unsigned long addr,
+	unsigned long epc, unsigned long write)
+{
+	printk("%s: unhandled address %lx, epc %lx, is_write %lx\n",
+		func, addr, epc, write);
+}
+
+void kvm_handle_pcie_sysmgt(struct pt_regs *regs, unsigned long write,
+		unsigned long address, unsigned long reg_num)
+{
+	unsigned int rindex = address >> 2;
+	struct kvm_arch *arch = kvm_get_arch(regs);
+
+	if (rindex == 0x0) {
+		if (!write) {
+			regs->regs[reg_num] = arch->sysmgt.regs[rindex];
+			return;
+		}
+	} else if (rindex == 0x40) {
+		/* power on reset: used to calculate frequency */
+		if (!write) {
+			regs->regs[reg_num] = arch->sysmgt.regs[rindex];
+			return;
+		}
+	} else if (rindex == 0x41) {
+		/* chip reset */
+		if (write) {
+			int val;
+
+			printk("Guest vcpu %lu request exiting ...\n", regs->guest_cp0_ebase & 0x3ff);
+			val = (KVM_EXIT_REQUEST_QUIT << 24);
+			 __kvm_vcpu_leave_guest((struct kvm_vcpu *)regs->cp0_osscratch7, val);
+			return;
+		}
+	} else if (rindex == 0x42) {
+		/* cpu reset register*/
+		if (write) {
+			int i, nval, oval;
+
+			oval = arch->sysmgt.regs[rindex];
+			nval = regs->regs[reg_num];
+			arch->sysmgt.regs[rindex] = nval;
+
+			for (i = 1; i < (KVM_MAX_VCPUS >> 2); i++) {
+				/* if it is already out of reset, skip */
+				if (!(oval & (1 << i)))
+					continue;
+
+				/* not out of reset yet, should it? */
+				if (!(nval & (1 << i))) {
+					unsigned int val = i << 2, badinstr, epc_badinstr;
+					val |= (KVM_EXIT_ENABLE_CORE << 24);
+
+#if 0
+					printk("Enable core %d ...\n", i);
+#endif
+					kvm_get_badinstr(regs, &badinstr, &epc_badinstr);
+					compute_guest_return_epc(regs, epc_badinstr);
+					kvm_save_guest_context(regs, kvm_get_vcpu_guest_regs(regs));	
+					__kvm_vcpu_leave_guest((struct kvm_vcpu *)regs->cp0_osscratch7, val);
+				}
+			}
+		} else {
+			regs->regs[reg_num] = arch->sysmgt.regs[rindex];	
+		}
+		return;
+	} else if (rindex == 0x43) {
+		/* cpu noncoherent mode */
+		if (write)
+			arch->sysmgt.regs[rindex] = regs->regs[reg_num];
+		else
+			regs->regs[reg_num] = arch->sysmgt.regs[rindex];
+		return;
+	}
+
+	unhandled_exception(__FUNCTION__, address, regs->cp0_epc, write);
+}
diff --git a/arch/mips/netlogic/kvm-netl/kvm_traps.c b/arch/mips/netlogic/kvm-netl/kvm_traps.c
new file mode 100644
index 0000000..172242c
--- /dev/null
+++ b/arch/mips/netlogic/kvm-netl/kvm_traps.c
@@ -0,0 +1,606 @@
+/*
+ * Copyright (c) 2003-2013 Broadcom Corporation
+ * All Rights Reserved
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the Broadcom
+ * license below:
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in
+ *    the documentation and/or other materials provided with the
+ *    distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY BROADCOM ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL BROADCOM OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
+ * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
+ * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
+ * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <linux/vmalloc.h>
+#include <linux/ptrace.h>
+#include <linux/kvm_host.h>
+
+#include <asm/uaccess.h>
+#include <asm/kvm.h>
+#include <asm/kvm_host.h>
+#include <asm/netlogic/kvm_para.h>
+#include <asm/netlogic/kvm_xlp.h>
+#include <asm/netlogic/mips-extns.h>
+
+/* Virtualization exceptions (as tagged in cause register) handled here */
+
+#define PSI_UNHANDLED(epc, cause, instr, epc_instr) \
+	printk("PSI (unhandled): epc %lx, cause %lx, badinstr: %x, epc_badinstr: %x\n", epc, cause, instr, epc_instr)
+
+#define FC_UNHANDLED(epc, cause, instr, epc_instr) \
+	printk("FC (captured): epc %lx, cause %lx, badinstr: %x, epc_badinstr: %x\n", epc, cause, instr, epc_instr)
+
+
+#define cacheop(op, base) __asm__ __volatile__ (".set push\n.set mips4\ncache %0, 0(%1)\n.set pop\n" : : "i"(op), "r"(base))
+
+/* Enable guest counting of guest mode */
+#define PERF_GUEST_EC   (0x2 << 23)
+
+
+
+/*static int gpa_to_pa(struct kvm_vcpu *vcpu, unsigned long gpa_address, unsigned long *pa_address)
+{
+	pgd_t           *pgdp;
+	pud_t           *pudp;
+#ifndef __PAGETABLE_PMD_FOLDED
+	pmd_t           *pmdp;
+#endif
+	pte_t           *ptep, pte;
+	int t;
+	unsigned long address;
+
+	address = gpa_address;
+	printk("gpa_address %lx address %lx \n",gpa_address, address);
+	for (t = 0; t < KVM_USER_MEM_SLOTS; t++) {
+		struct kvm_memory_slot *s = &vcpu->kvm->memslots->memslots[t];
+
+		if (gpa_address >= (s->base_gfn << PAGE_SHIFT)
+				&& gpa_address < ((s->base_gfn + s->npages) << PAGE_SHIFT)) {
+			address = s->userspace_addr + gpa_address - (s->base_gfn << PAGE_SHIFT);
+			break;
+		}
+	}
+
+	printk("gpa_address %lx address %lx \n",gpa_address, address);
+
+	address = gpa_address;
+
+	pgdp = (pgd_t *)vcpu->arch.gpa_pgd + __pgd_offset(address);
+	pudp = (pud_t *)pgdp;
+#ifndef __PAGETABLE_PMD_FOLDED
+	pmdp = pmd_offset(pudp, address);
+	ptep = pte_offset(pmdp, address);
+#else
+	ptep = pte_offset((pmd_t *)pudp, address);
+#endif
+	pte = *ptep;
+
+	*pa_address = (pte_val(pte) >> (_PAGE_GLOBAL_SHIFT + 6)) << 12;
+
+	return 0;
+}*/
+
+static int gpa_to_hva (struct kvm_vcpu *vcpu, unsigned long gpa_address, unsigned long *hva_address)
+{
+	int t;
+	unsigned long address = 0;
+
+	for (t = 0; t < KVM_USER_MEM_SLOTS; t++) {
+		struct kvm_memory_slot *s = &vcpu->kvm->memslots->memslots[t];
+
+		if (gpa_address >= (s->base_gfn << PAGE_SHIFT)
+				&& gpa_address < ((s->base_gfn + s->npages) << PAGE_SHIFT)) {
+			address = s->userspace_addr + gpa_address - (s->base_gfn << PAGE_SHIFT);
+			break;
+		}
+	}
+#ifdef KVM_HYPERCALL_DEBUG
+	printk("hva address 0x%lx \n", address);
+#endif
+	*hva_address = address;
+	return 0;
+}
+
+
+/* Guest privileged sensitive instruction */
+static void process_psi(struct pt_regs *regs)
+{
+	unsigned int badinstr, epc_badinstr;
+	unsigned long epc;
+
+	epc = regs->cp0_epc;
+	kvm_get_badinstr(regs, &badinstr, &epc_badinstr);
+
+	if ((badinstr >> 26) == 0x10) {
+		/* cop0 */
+		if ((badinstr & 0xfe00003f) == 0x42000020) {
+			struct kvm_vcpu_arch *vcpu_arch;
+			uint32_t val, cause;
+
+			vcpu_arch = kvm_get_vcpu_arch(regs);
+			if (vcpu_arch->nmi == 1) {
+				regs->cp0_epc = 0xffffffffbfc00000;
+				__write_32bit_guest_c0_register($12, 0,
+					__read_32bit_guest_c0_register($12, 0) | 0x80000);
+				vcpu_arch->nmi = 0;
+				return;
+			}
+
+			/* We do not want to simply bounce back. As for SMT,
+			 * this will have performance implication.
+			 */
+#if 0
+			__asm__ __volatile__ ("wait" : : : "memory");
+#else
+			/* Let us do a tpause here */
+			/* compare - count */
+			val = __read_32bit_guest_c0_register($11, 0) -
+				__read_32bit_guest_c0_register($9,  0);
+			cause = __read_32bit_guest_c0_register($13, 0);
+			if (!((cause >> 30) & 0x1) && val > 500) {
+				write_xlp_pausetime(val - 500);
+				__asm__ __volatile__ ("tpause" : : : "memory");
+			}
+#endif
+			return; 
+		} else if (((badinstr >> 21) & 0x1f) == 0x0) {
+			/* MF */
+			unsigned int rt = (badinstr >> 16) & 0x1f;
+			unsigned int rd = (badinstr >> 11) & 0x1f;
+			unsigned int sel = badinstr & 0x7;
+
+			if (rd == 15 && sel == 0) {
+				regs->regs[rt] = read_c0_prid();
+			}
+			else if (rd == 12 && sel == 2) {
+				/* srsctl */
+				regs->regs[rt] = 0;
+			} else if (rd == 25 && (sel == 0 || sel == 2 || sel == 4 || sel == 6)) {
+				/* perf counter control registers: enable guest use */
+				int32_t val = 0;
+
+				if (sel == 0) {
+					val = read_c0_perfctrl0();
+					write_c0_perfctrl0(val | PERF_GUEST_EC);
+				} else if (sel == 2) {
+					val = read_c0_perfctrl1();
+					write_c0_perfctrl1(val | PERF_GUEST_EC);
+				} else if (sel == 4) {
+					val = read_c0_perfctrl2();
+					write_c0_perfctrl2(val | PERF_GUEST_EC);
+				} else {
+					val = read_c0_perfctrl3();
+					write_c0_perfctrl3(val | PERF_GUEST_EC);
+				}
+				regs->regs[rt] = val;
+			} else
+				PSI_UNHANDLED(epc, regs->cp0_cause, badinstr, epc_badinstr);
+		} else if (((badinstr >> 21) & 0x1f) == 0x1) {
+			/* DMF */
+			unsigned int rt = (badinstr >> 16) & 0x1f;
+			unsigned int rd = (badinstr >> 11) & 0x1f;
+			unsigned int sel = badinstr & 0x7;
+
+			if (rd == 25 && (sel == 1 || sel == 3 || sel == 5 || sel == 7)) {
+				/* perf counter control registers: enable guest use */
+
+				if (sel == 1) {
+					write_c0_perfctrl0(read_c0_perfctrl0() | PERF_GUEST_EC);
+					regs->regs[rt] = read_c0_perfcntr0_64();
+				} else if (sel == 3) {
+					write_c0_perfctrl1(read_c0_perfctrl1() | PERF_GUEST_EC);
+					regs->regs[rt] = read_c0_perfcntr1_64();
+				} else if (sel == 5) {
+					write_c0_perfctrl2(read_c0_perfctrl2() | PERF_GUEST_EC);
+					regs->regs[rt] = read_c0_perfcntr2_64();
+				} else {
+					write_c0_perfctrl3(read_c0_perfctrl3() | PERF_GUEST_EC);
+					regs->regs[rt] = read_c0_perfcntr3_64();
+				}
+			} else
+				PSI_UNHANDLED(epc, regs->cp0_cause, badinstr, epc_badinstr);
+		} else if (((badinstr >> 21) & 0x1f) == 0x4) {
+			/* MT */
+			unsigned int rt = (badinstr >> 16) & 0x1f;
+			unsigned int rd = (badinstr >> 11) & 0x1f;
+			unsigned int sel = badinstr & 0x7;
+
+			if (rd == 9 && sel == 0) {
+				/* count */
+				unsigned int val = regs->regs[rt] - read_c0_count();
+				__write_32bit_c0_register($12, 7, val);
+			} else if (rd == 25 && (sel == 0 || sel == 2 || sel == 4 || sel == 6)) {
+				/* perf cnt registers */
+				if (sel == 0)
+					write_c0_perfctrl0(regs->regs[rt] | PERF_GUEST_EC);
+				else if (sel == 2)
+					write_c0_perfctrl1(regs->regs[rt] | PERF_GUEST_EC);
+				else if (sel == 4)
+					write_c0_perfctrl2(regs->regs[rt] | PERF_GUEST_EC);
+				else
+					write_c0_perfctrl3(regs->regs[rt] | PERF_GUEST_EC);
+			} else
+				PSI_UNHANDLED(epc, regs->cp0_cause, badinstr, epc_badinstr);
+		} else if (((badinstr >> 21) & 0x1f) == 0x5) {
+			/* DMT */
+			unsigned int rt = (badinstr >> 16) & 0x1f;
+			unsigned int rd = (badinstr >> 11) & 0x1f;
+			unsigned int sel = badinstr & 0x7;
+
+			if (rd == 25 && (sel == 1 || sel == 3 || sel == 5 || sel == 7)) {
+				/* perf cnt registers */
+
+				if (sel == 1) {
+					write_c0_perfctrl0(read_c0_perfctrl0() | PERF_GUEST_EC);
+					write_c0_perfcntr0_64(regs->regs[rt]);
+				} else if (sel == 3) {
+					write_c0_perfctrl1(read_c0_perfctrl1() | PERF_GUEST_EC);
+					write_c0_perfcntr1_64(regs->regs[rt]);
+				} else if (sel == 5) {
+					write_c0_perfctrl2(read_c0_perfctrl2() | PERF_GUEST_EC);
+					write_c0_perfcntr2_64(regs->regs[rt]);
+				} else {
+					write_c0_perfctrl3(read_c0_perfctrl3() | PERF_GUEST_EC);
+					write_c0_perfcntr3_64(regs->regs[rt]);
+				}
+			} else
+				PSI_UNHANDLED(epc, regs->cp0_cause, badinstr, epc_badinstr);
+
+		} else
+			PSI_UNHANDLED(epc, regs->cp0_cause, badinstr, epc_badinstr);
+	} else if ((badinstr >> 26) == 0x1c) {
+		if ((badinstr & 0xffff) == 0x18) {
+			/* mfcr */
+			unsigned int rs = (badinstr >> 21) & 0x1f;
+			unsigned int rt = (badinstr >> 16) & 0x1f;
+			unsigned int cr = regs->regs[rs];
+
+			if (cr == 0x007) {
+				/* IFU BRUB reserve register */
+				;
+			} else if (cr == 0x100) {
+				/* ICU defeature register */
+				;
+			} else if (cr == 0x304) {
+				/* LSU defeature register */
+			} else if (cr == 0x305) {
+				/* LSU debug addr */
+				/* regs->regs[rt] = (uint32_t)read_32bit_nlm_ctrl_reg(0x3, 0x5); */
+				regs->regs[rt] = 0x0;
+			} else if (cr == 0xa00) {
+				/* map thread mode */
+				regs->regs[rt] = 0x0;
+			} else if (cr == 0x400) {
+				/* mmu setup */
+				regs->regs[rt] = 0x0;
+			} else if (cr == 0x800) {
+				/* scu config */
+				regs->regs[rt] = 0x0;
+			} else
+				printk("=== unhandled mfcr cr %x\n", cr);
+
+		} else if ((badinstr & 0xffff) == 0x19) {
+			/* mtcr */
+			unsigned int rs = (badinstr >> 21) & 0x1f;
+			unsigned int rt = (badinstr >> 16) & 0x1f;
+			unsigned int cr = regs->regs[rs];
+
+			if (cr == 0x007) {
+				/* IFU BRUB reserve register */
+				;
+			} else if (cr == 0x100) {
+				/* ICU defeature register */
+				;
+			} else if (cr == 0x304) {
+				/* LSU defeature register */
+				;
+			} else if (cr == 0x305) {
+				/* write_32bit_nlm_ctrl_reg (0x3, 0x5, (uint32_t)regs->regs[rt]); */
+				;
+			} else if (cr == 0x306) {
+				/* LSU debug data0 */
+				/* write_64bit_nlm_ctrl_reg (0x3, 0x6, regs->regs[rt]); */
+				;
+			} else if (cr == 0x400) {
+				/* MMU setup */
+				;
+			} else if (cr == 0x700) {
+				/* schedule defeature */
+				;
+			} else if (cr == 0x800) {
+				/* scu config */
+				;
+			} else if (cr == 0xa00) {
+				/* map thread mode: this is the way to wake up
+				 * other threads in the same core.
+				 */
+				if (regs->regs[rt] != 0 && regs->regs[rt] != 1) {
+					int val;
+
+					/*
+					 * val[15:0]: start cpu id (ebase).
+					 * val[23:16]: # of cpus to spawn.
+					 * val[31:24]: exit code.
+					 */
+					val = regs->guest_cp0_ebase & 0x3ff;
+					val ++;
+					if (regs->regs[rt] == 2)
+						val |= (1 << 16);
+					else
+						val |= (3 << 16);
+					val |= (KVM_EXIT_SPAWN_THREADS << 24);
+
+#if 0
+					printk("Tentatively Leaving the guest (Spawning %d new threads)...\n",
+						(regs->regs[rt] == 2) ? 1 : 3);
+#endif
+
+					compute_guest_return_epc(regs, epc_badinstr);
+					kvm_save_guest_context(regs, kvm_get_vcpu_guest_regs(regs));
+					__kvm_vcpu_leave_guest((struct kvm_vcpu *)regs->cp0_osscratch7, val);
+				}
+			} else
+				PSI_UNHANDLED(epc, regs->cp0_cause, badinstr, epc_badinstr);
+
+		} else
+			PSI_UNHANDLED(epc, regs->cp0_cause, badinstr, epc_badinstr);
+	} else if ((badinstr >> 26) == 0x2f) {
+		/* cache instruction */
+		int base, op;
+		uint64_t offset, addr;
+
+		base = (badinstr >> 21) & 0x1f;
+		op = (badinstr >> 16) & 0x1f;
+		offset = badinstr & 0xffff;
+		addr = regs->regs[base] + offset;
+		switch(op) {
+			case 0:  /* I, Index Invalidate */ cacheop(0, addr); break;
+			case 1:  /* D, Index Invalidate */ cacheop(1, addr); break;
+			case 2:  break;
+			case 3:  /* S, Index Invalidate */ cacheop(3, addr); break;
+			case 4:  break;
+			case 5:  break;
+			case 6:  break;
+			case 7:  break;
+			case 8:  /* I, Index Store Tag */ cacheop(8, addr); break;
+			case 9:  /* D, Index Store Tag */ cacheop(9, addr); break;
+			case 10: break;
+			case 11: /* S, Index Store Tag */ cacheop(11, addr); break;
+			case 12: break;
+			case 13: break;
+			case 14: break;
+			case 15: break;
+			case 16: /* I, Hit Invalidate */ cacheop(16, addr); break;
+			case 17: /* D, Hit Invalidate */ cacheop(17, addr); break;
+			case 18: break;
+			case 19:
+				/* S, Hit Invalidate, translation will be performed. */
+				PSI_UNHANDLED(epc, regs->cp0_cause, badinstr, epc_badinstr);
+				break;
+			case 20: /* I, Hit Writeback */ cacheop(20, addr); break;
+			case 21: /* D, Hit Writeback */ cacheop(21, addr); break;
+			case 22: break;
+			case 23:
+				/* S, Hit Writeback, translation will be performed. */
+				PSI_UNHANDLED(epc, regs->cp0_cause, badinstr, epc_badinstr);
+				break;
+			case 24: break;
+			case 25: break;
+			case 26: break;
+			case 27: break;
+			case 28: /* I, Fetch and Lock */ cacheop(28, addr); break;
+			case 29: /* D, Fetch and Lock */ cacheop(29, addr); break;
+			case 30: break;
+			case 31:
+				/* S, Fetch and Lock, translation will be performed. */
+				PSI_UNHANDLED(epc, regs->cp0_cause, badinstr, epc_badinstr);
+				break;
+		}
+	} else
+		PSI_UNHANDLED(epc, regs->cp0_cause, badinstr, epc_badinstr);
+
+	compute_guest_return_epc(regs, epc_badinstr);
+}
+
+/* Guest field change */
+static void process_fc(struct pt_regs *regs)
+{
+	unsigned int badinstr, epc_badinstr;
+	unsigned long epc;
+
+	epc = regs->cp0_epc;
+	kvm_get_badinstr(regs, &badinstr, &epc_badinstr);
+
+	if ((badinstr >> 26) == 0x10) {
+		if (((badinstr >> 21) & 0x1f) == 0x4) {
+			unsigned int rt = (badinstr >> 16) & 0x1f;
+			unsigned int rd = (badinstr >> 11) & 0x1f;
+			unsigned int sel = badinstr & 0x7;
+
+			if (rd == 12 && sel == 0) {
+				__write_32bit_guest_c0_register($12, 0, regs->regs[rt]);
+			} else if (rd == 13 && sel == 0) {
+				__write_32bit_guest_c0_register($13, 0, regs->regs[rt]);
+			} else if (rd == 12 && sel == 1) {
+				__write_32bit_guest_c0_register($12, 1, regs->regs[rt]);
+			} else if (rd == 25 && (sel == 0 || sel == 2 || sel == 4 || sel == 6)) {
+				/* perf cnt registers: event field may get changed */
+				if (sel == 0)
+					write_c0_perfctrl0(regs->regs[rt] | PERF_GUEST_EC);
+				else if (sel == 2)
+					write_c0_perfctrl1(regs->regs[rt] | PERF_GUEST_EC);
+				else if (sel == 4)
+					write_c0_perfctrl2(regs->regs[rt] | PERF_GUEST_EC);
+				else
+					write_c0_perfctrl3(regs->regs[rt] | PERF_GUEST_EC);
+			} else
+				FC_UNHANDLED(epc, regs->cp0_cause, badinstr, epc_badinstr);
+		} else
+			FC_UNHANDLED(epc, regs->cp0_cause, badinstr, epc_badinstr);
+	} else
+		FC_UNHANDLED(epc, regs->cp0_cause, badinstr, epc_badinstr);
+
+	compute_guest_return_epc(regs, epc_badinstr);
+}
+
+/* Guest hypercall */
+static void process_hc(struct pt_regs *regs)
+{
+	unsigned int badinstr, epc_badinstr;
+	unsigned long epc;
+	int hc_num;
+
+	epc = regs->cp0_epc;
+	kvm_get_badinstr(regs, &badinstr, &epc_badinstr);
+
+	hc_num = regs->regs[2];
+	switch (hc_num) {
+	case KVM_HC_GET_HARD_CPUID:
+	{
+		int i, guest_cpuid = regs->regs[3], host_cpuid = -1;
+		struct kvm *kvm;
+		struct kvm_vcpu *vcpu;
+
+		kvm = ((struct kvm_vcpu *)regs->cp0_osscratch7)->kvm;
+		printk("Hypercall exception (number %d) KVM_HC_GET_HARD_CPUID:\n",hc_num);
+
+		kvm_for_each_vcpu(i, vcpu, kvm) {
+			if (vcpu->vcpu_id == guest_cpuid) {
+				host_cpuid = __cpu_logical_map[vcpu->arch.host_vcpuid];
+				break;
+			}
+		}
+
+		/* result */
+		if (host_cpuid == (-1)) {
+			regs->regs[2] = -1; /* fail */
+		} else {
+			regs->regs[2] = 0; /* succeed */
+			regs->regs[3] = host_cpuid;
+		}
+		break;
+	}
+	case KVM_HC_HALX_NETSOC:
+	{
+		int val;
+		unsigned long gpa_addr, hva_addr = 0;
+		struct kvm *kvm;
+		struct kvm_vcpu *vcpu = NULL;
+		int sub_cmd;
+		int i;
+		/* TODO: check guest_cpuid, make sure it in regs[3] */
+		/*int guest_cpuid = regs->regs[3];*/
+		int guest_cpuid = 0;
+
+		kvm = ((struct kvm_vcpu *)regs->cp0_osscratch7)->kvm;
+#ifdef KVM_HYPERCALL_DEBUG
+		printk("Hypercall exception (number %d) netsoc: 0x%lx\n",hc_num, regs->regs[3]);
+#endif
+
+		kvm_for_each_vcpu(i, vcpu, kvm) {
+			if (vcpu->vcpu_id == guest_cpuid) {
+				break;
+			}
+		}
+		if (vcpu == NULL) {
+			printk(" *** ERROR NULL vcpu ***\n");
+		}
+
+		/*
+		 * val[15:0]: start cpu id (ebase).
+		 * val[23:16]: # of cpus to spawn.
+		 * val[31:24]: exit code.
+		 */
+		val = regs->guest_cp0_ebase & 0x3ff;
+		val ++;
+		val |= (1 << 3);
+		val |= (KVM_EXIT_HC_HALX_NETSOC << 24);
+
+		gpa_addr = regs->regs[3];
+		sub_cmd = regs->regs[4];
+		gpa_to_hva((struct kvm_vcpu *)regs->cp0_osscratch7, gpa_addr, &hva_addr);
+#ifdef KVM_HYPERCALL_DEBUG
+		printk("Tentatively Leaving the guest ...gpa 0x%lx hva 0x%lx\n", gpa_addr, hva_addr);
+		printk("KVM_HC_HALX_NETSOC: sub_cmd = %d\n",  sub_cmd);
+#endif
+		vcpu->arch.mips_hypcall.args[0] = hva_addr;
+		vcpu->arch.mips_hypcall.cmd = sub_cmd;
+
+		compute_guest_return_epc(regs, epc_badinstr);
+		kvm_save_guest_context(regs, kvm_get_vcpu_guest_regs(regs));
+		__kvm_vcpu_leave_guest((struct kvm_vcpu *)regs->cp0_osscratch7, val);
+
+		break;
+	}
+	default:
+		printk("Hypercall exception (number %d) not implemented:\n", hc_num);
+		printk("\tepc 0x%lx, badinstr: 0x%x\n", epc, badinstr);
+		break;
+	}
+
+	compute_guest_return_epc(regs, epc_badinstr);
+}
+
+/* Guest reserved instruction redirect */
+static void process_grr(struct pt_regs *regs)
+{
+	panic("GRR exception not implemented\n");
+}
+
+/* Guest cp0 mode change event */
+static void process_mc(struct pt_regs *regs)
+{
+	panic("MC exception not implemented\n");
+}
+
+void process_virt_exception(struct pt_regs *regs)
+{
+	int excode = (regs->cp0_guestctl0 >> 2) & 0x1f;
+
+	switch(excode) {
+	case 0x0:
+		process_psi(regs);
+		break;
+	case 0x1:
+		process_fc(regs);
+		break;
+	case 0x2:
+#ifdef KVM_HYPERCALL_DEBUG
+		printk("Hypercall\n");
+#endif
+		process_hc(regs);
+		break;
+	case 0x3:
+		process_grr(regs);
+		break;
+	case 0x9:
+		process_mc(regs);
+		break;
+	default:
+		panic("%s: Unimpled excode %d, epc 0x%lx, bad insn 0x%lx\n",
+			__FUNCTION__, excode, regs->cp0_epc, regs->cp0_badinstr);
+	}
+
+	return;
+}
diff --git a/arch/mips/netlogic/xlp/csrc-tsc.c b/arch/mips/netlogic/xlp/csrc-tsc.c
index 27a928b..0bd4e65 100644
--- a/arch/mips/netlogic/xlp/csrc-tsc.c
+++ b/arch/mips/netlogic/xlp/csrc-tsc.c
@@ -71,7 +71,7 @@ void nlm_init_tsc_timer(void)
 	clocksource_register_hz(&csrc_tsc, tsc_timer_freq());
 }
 
-#ifdef CONFIG_KVM
+#ifdef CONFIG_KVM_NETL
 
 static atomic_t count_count_start = ATOMIC_INIT(0);
 static atomic_t count_count_stop = ATOMIC_INIT(0);
diff --git a/drivers/iommu/Kconfig b/drivers/iommu/Kconfig
index c332fb9..faa8b0a 100644
--- a/drivers/iommu/Kconfig
+++ b/drivers/iommu/Kconfig
@@ -261,4 +261,14 @@ config SHMOBILE_IOMMU_L1SIZE
 	default 256 if SHMOBILE_IOMMU_ADDRSIZE_64MB
 	default 128 if SHMOBILE_IOMMU_ADDRSIZE_32MB
 
+config XLP_IOMMU
+	bool "XLP IOMMU Support"
+	depends on CPU_XLP
+	select IOMMU_API
+	default y
+	help
+	  Support for IOMMU of Broadcom XLP processor.
+
+	  If unsure, say N here.
+
 endif # IOMMU_SUPPORT
diff --git a/drivers/vfio/Kconfig b/drivers/vfio/Kconfig
index 7cd5dec..c2d2cc6 100644
--- a/drivers/vfio/Kconfig
+++ b/drivers/vfio/Kconfig
@@ -7,6 +7,7 @@ menuconfig VFIO
 	tristate "VFIO Non-Privileged userspace driver framework"
 	depends on IOMMU_API
 	select VFIO_IOMMU_TYPE1 if X86
+	select VFIO_IOMMU_TYPE1 if CPU_XLP
 	help
 	  VFIO provides a framework for secure userspace device drivers.
 	  See Documentation/vfio.txt for more details.
diff --git a/drivers/vfio/vfio.c b/drivers/vfio/vfio.c
index d30f44d..e2c5304 100644
--- a/drivers/vfio/vfio.c
+++ b/drivers/vfio/vfio.c
@@ -1423,12 +1423,17 @@ static int __init vfio_init(void)
 
 	pr_info(DRIVER_DESC " version: " DRIVER_VERSION "\n");
 
+#if 0
+	/* Let user explicitly insert module vfio_iommu_type1.
+	 * The reason is that user may pass additional module argument.
+	 */
 	/*
 	 * Attempt to load known iommu-drivers.  This gives us a working
 	 * environment without the user needing to explicitly load iommu
 	 * drivers.
 	 */
 	request_module_nowait("vfio_iommu_type1");
+#endif
 
 	return 0;
 
diff --git a/include/uapi/linux/kvm.h b/include/uapi/linux/kvm.h
index f1ee1e4..77762be 100644
--- a/include/uapi/linux/kvm.h
+++ b/include/uapi/linux/kvm.h
@@ -172,6 +172,17 @@ struct kvm_pit_config {
 #define KVM_EXIT_S390_TSCH        22
 #define KVM_EXIT_EPR              23
 
+#ifdef CONFIG_CPU_XLP
+/* Let us start from 25, to avoid future migration/merging where
+ * value 21-24 may be taken.
+ */
+#define	KVM_EXIT_SPAWN_THREADS	  25
+#define	KVM_EXIT_ENABLE_CORE	  26
+#define	KVM_EXIT_QUIT_KVM	  27
+#define	KVM_EXIT_REQUEST_QUIT	  29
+#define KVM_EXIT_HC_HALX_NETSOC	  30
+#endif
+
 /* For KVM_EXIT_INTERNAL_ERROR */
 /* Emulate instruction failed. */
 #define KVM_INTERNAL_ERROR_EMULATION	1
@@ -201,6 +212,12 @@ struct kvm_run {
 	__u64 psw_mask; /* psw upper half */
 	__u64 psw_addr; /* psw lower half */
 #endif
+#ifdef CONFIG_CPU_XLP
+	/* Data portion of KVM_EXIT_IO */
+	struct {
+		__u8 data[8];
+	} mips_exit_io;
+#endif
 	union {
 		/* KVM_EXIT_UNKNOWN */
 		struct {
@@ -301,6 +318,25 @@ struct kvm_run {
 		struct {
 			__u32 epr;
 		} epr;
+#ifdef CONFIG_CPU_XLP
+		/* KVM_EXIT_SPAWN_THREAD */
+		struct {
+			__u8  num_vcpus;
+			__u16 start_hw_cpuid;
+		} mips_spawn_thread;
+		/* KVM_EXIT_CHAR_PRINT */
+		struct {
+			__u8 c;
+		} mips_output_char;
+	        struct {
+		        __u32 domid;
+			__u32 guestid;
+	                __u32 soc_type;
+		        __u64 cmd;
+			__u64 args[4];
+		}mips_hypcall;
+
+#endif
 		/* Fix the size of the union. */
 		char padding[256];
 	};
-- 
1.7.1

