From 12100599c7791516f94ff49d4ff1ac3f5d1aaa49 Mon Sep 17 00:00:00 2001
From: Ashok Kumar <ashoks@broadcom.com>
Date: Wed, 8 Apr 2015 13:01:15 -0400
Subject: kvm: Make kvm support compatible to CONFIG_MIPS_HUGE_TLB_SUPPORT

Signed-off-by: Ashok Kumar <ashoks@broadcom.com>
Signed-off-by: Nam Ninh <nam.ninh@windriver.com>

diff --git a/arch/mips/kvm-netl/kvm.c b/arch/mips/kvm-netl/kvm.c
index 7145c9a..e7cc1f5 100644
--- a/arch/mips/kvm-netl/kvm.c
+++ b/arch/mips/kvm-netl/kvm.c
@@ -69,6 +69,7 @@
 #include <linux/spinlock.h>
 #include <linux/perf_event.h>
 #include <linux/sched.h>
+#include <linux/hugetlb.h>
 
 #include <asm/pgalloc.h>
 #include <asm/mmu_context.h>
@@ -125,6 +126,7 @@ static void kvm_populate_gpa_map(pgd_t *gpa_pgd, uint64_t msize,
 	uint64_t gpa, uint64_t hva)
 {
 	uint64_t address, pa;
+	bool is_hp = false;
 
 #if 0
 	printk("KVM: populate guest memory: hva = 0x%llx, gpa = 0x%llx, size = 0x%llx\n",
@@ -142,8 +144,15 @@ static void kvm_populate_gpa_map(pgd_t *gpa_pgd, uint64_t msize,
 		pgdp = current->mm->pgd + __pgd_offset(address);
 		pudp = pud_offset(pgdp, address);
 		pmdp = pmd_offset(pudp, address);
-		ptep = pte_offset(pmdp, address);
-		pte = *	ptep;
+		if (pmd_huge(*pmdp)) {
+			pte = *(pte_t *)pmdp;
+			is_hp = true;
+		}
+		else {
+			is_hp = false;
+			ptep = pte_offset(pmdp, address);
+			pte = *ptep;
+		}
 
 		/* map gpa to pte in page table pointed to by gpa_pgd */
 		pgdp = (pgd_t *)gpa_pgd + __pgd_offset(gpa);
@@ -157,23 +166,33 @@ static void kvm_populate_gpa_map(pgd_t *gpa_pgd, uint64_t msize,
 			*(unsigned long *)pudp = (unsigned long)ptr;
 		}
 		pmdp = (pmd_t *)*(unsigned long *)pudp + __pmd_offset(gpa);
-		if (*(unsigned long *)pmdp == (unsigned long)invalid_pte_table) {
-			/* allocate a page for pmd and initialize it */
-			ptr = (unsigned long *)__get_free_page(GFP_KERNEL|GFP_DMA);
-			for (i = 0; i < PTRS_PER_PTE; i++)
-				ptr[i] = 0;
-			*(unsigned long *)pmdp = (unsigned long)ptr;
+		if (is_hp) {
+			ptep = (pte_t *)pmdp;
+		}
+		else {
+			if (*(unsigned long *)pmdp == (unsigned long)invalid_pte_table) {
+				/* allocate a page for pmd and initialize it */
+				ptr = (unsigned long *)__get_free_page(GFP_KERNEL|GFP_DMA);
+				for (i = 0; i < PTRS_PER_PTE; i++)
+					ptr[i] = 0;
+				*(unsigned long *)pmdp = (unsigned long)ptr;
+			}
+			ptep = (pte_t *)*(unsigned long *)pmdp + __pte_offset(gpa);
 		}
-		ptep = (pte_t *)*(unsigned long *)pmdp + __pte_offset(gpa);
 #else
-		if (*(unsigned long *)pudp == (unsigned long)invalid_pte_table) {
-			/* allocate a page for pmd and initialize it */
-			ptr = (unsigned long *)__get_free_page(GFP_KERNEL|GFP_DMA);
-			for (i = 0; i < PTRS_PER_PTE; i++)
-				ptr[i] = 0;
-			*(unsigned long *)pudp = (unsigned long)ptr;
+		if (is_hp) {
+			ptep = (pte_t *)pudp;
+		}
+		else {
+			if (*(unsigned long *)pudp == (unsigned long)invalid_pte_table) {
+				/* allocate a page for pmd and initialize it */
+				ptr = (unsigned long *)__get_free_page(GFP_KERNEL|GFP_DMA);
+				for (i = 0; i < PTRS_PER_PTE; i++)
+					ptr[i] = 0;
+				*(unsigned long *)pudp = (unsigned long)ptr;
+			}
+			ptep = (pte_t *)*(unsigned long *)pudp + __pte_offset(gpa);
 		}
-		ptep = (pte_t *)*(unsigned long *)pudp + __pte_offset(gpa);
 #endif
 		*ptep = pte;
 
@@ -185,7 +204,10 @@ static void kvm_populate_gpa_map(pgd_t *gpa_pgd, uint64_t msize,
 			}
 		}
 
-		address += PAGE_SIZE, gpa += PAGE_SIZE;
+		if (is_hp)
+			address += HPAGE_SIZE, gpa += HPAGE_SIZE;
+		else
+			address += PAGE_SIZE, gpa += PAGE_SIZE;
 	}
 }
 
@@ -298,7 +320,7 @@ static void kvm_sync_dirty_log(struct kvm *kvm)
 		gpa = s->base_gfn << PAGE_SHIFT;
 
 		/* enumerate all gpa pages */
-		for (address = gpa; address < (gpa + msize); address += PAGE_SIZE) {
+		for (address = gpa; address < (gpa + msize);) {
 			pgd_t *pgdp;
 			pud_t *pudp;
 			pmd_t *pmdp;
@@ -307,7 +329,10 @@ static void kvm_sync_dirty_log(struct kvm *kvm)
 			pgdp = (pgd_t *)gpa_pgd + __pgd_offset(address);
 			pudp = pud_offset(pgdp, address);
 			pmdp = pmd_offset(pudp, address);
-			ptep = pte_offset(pmdp, address);
+			if (pmd_huge(*pmdp))
+				ptep = (pte_t *)pmdp;
+			else
+				ptep = pte_offset(pmdp, address);
 			pte = *ptep;
 			if (pte_val(pte) & _PAGE_MODIFIED) {
 				mark_page_dirty_in_slot(kvm, s, address >> PAGE_SHIFT);
@@ -315,7 +340,10 @@ static void kvm_sync_dirty_log(struct kvm *kvm)
 				*ptep = pte;
 			}
 
-			address += PAGE_SIZE;
+			if (pmd_huge(*pmdp))
+				address += HPAGE_SIZE;
+			else
+				address += PAGE_SIZE;
 		}
 	}
 
@@ -344,7 +372,10 @@ static void kvm_set_gpa_pa_map(struct kvm *kvm, struct kvm_userspace_memory_regi
 		pgdp = current->mm->pgd + __pgd_offset(address);
 		pudp = pud_offset(pgdp, address);
 		pmdp = pmd_offset(pudp, address);
-		ptep = pte_offset(pmdp, address);
+		if (pmd_huge(*pmdp))
+			ptep = (pte_t *)pmdp;
+		else
+			ptep = pte_offset(pmdp, address);
 		pte = *ptep;
 
 		/* _PAGE_MODIFIED: to record whether the page has been written,
@@ -358,7 +389,10 @@ static void kvm_set_gpa_pa_map(struct kvm *kvm, struct kvm_userspace_memory_regi
 		pgdp = (pgd_t *)(gpa_pgd + __pgd_offset(gpa));
 		pudp = pud_offset(pgdp, gpa);
 		pmdp = pmd_offset(pudp, gpa);
-		ptep = pte_offset(pmdp, gpa);
+		if (pmd_huge(*pmdp))
+			ptep = (pte_t *)pmdp;
+		else
+			ptep = pte_offset(pmdp, gpa);
 		*ptep = pte;
 	}
 
@@ -965,7 +999,7 @@ long kvm_arch_vm_ioctl(struct file *filp, unsigned int ioctl, unsigned long arg)
 		unsigned long long hugepage_size = 0;
 
 		/* Only support non-mips-default huge page size */
-#ifdef CONFIG_HUGEPAGE_NOT_MIPS_DEFAULT
+#ifdef CONFIG_MIPS_HUGE_TLB_SUPPORT
 		hugepage_size = HPAGE_SIZE;
 #endif
 		if (copy_to_user(argp, &hugepage_size, sizeof(unsigned long long))) {
diff --git a/arch/mips/kvm-netl/xlp.c b/arch/mips/kvm-netl/xlp.c
index 0037adf..6d8ce5a 100644
--- a/arch/mips/kvm-netl/xlp.c
+++ b/arch/mips/kvm-netl/xlp.c
@@ -35,6 +35,7 @@
 #include <linux/kvm.h>
 #include <linux/kvm_host.h>
 #include <linux/spinlock.h>
+#include <linux/hugetlb.h>
 
 #include <asm/pgalloc.h>
 #include <asm/branch.h>
@@ -280,16 +281,19 @@ static void xlp_kvm_free_gpa_pgd(pgd_t *gpa_pgd)
 			unsigned long *p1 = (unsigned long*)*p;
 			int i;
 
-			for (i = 0; i < PTRS_PER_PMD; i++) {
-				if (p1[i] != (unsigned long)invalid_pte_table) {
-					__free_page((void *)p1[i]);
+			if (!pmd_huge(*(pmd_t *)p1)) {
+				for (i = 0; i < PTRS_PER_PMD; i++) {
+					if (p1[i] != (unsigned long)invalid_pte_table) {
+						free_page((unsigned long)p1[i]);
+					}
 				}
 			}
-			__free_page((void *)*p);
+			free_page((unsigned long)*p);
 		}
 #else
-		if (*p != (unsigned long)invalid_pte_table) {
-			__free_page((void *)*p);
+		if (*p != (unsigned long)invalid_pte_table &&
+			!pud_huge(*(pud_t *)p)) {
+			free_page((unsigned long)*p);
 		}
 #endif
 	}
diff --git a/arch/mips/mm/fault.c b/arch/mips/mm/fault.c
index 2377a9c..01e4a46 100644
--- a/arch/mips/mm/fault.c
+++ b/arch/mips/mm/fault.c
@@ -39,6 +39,14 @@
 #endif
 
 #ifdef CONFIG_KVM_NETL
+/* couldn't include <linux/hugetlb.h>
+ * as <linux/highmem.h> via hugetlb.h and <asm/highmem.h>
+ * conflicts */
+#ifdef CONFIG_MIPS_HUGE_TLB_SUPPORT
+extern int pmd_huge(pmd_t pmd);
+#else
+#define pmd_huge(x) 0
+#endif
 static void nlm_kvm_populate_page(struct pt_regs *regs, unsigned long address,
 	unsigned long gpa_address, unsigned long *pa_address, unsigned long *page_size)
 {
@@ -49,12 +57,19 @@ static void nlm_kvm_populate_page(struct pt_regs *regs, unsigned long address,
 	struct kvm_vcpu	*vcpu;
 	unsigned long	*ptr;
 	int		i;
+	bool is_hp = false;
 
 	pgdp = current->mm->pgd + __pgd_offset(address);
 	pudp = pud_offset(pgdp, address);
 	pmdp = pmd_offset(pudp, address);
-	ptep = pte_offset(pmdp, address);
-	pte = *ptep;
+	if (pmd_huge(*pmdp)) {
+		pte = *(pte_t *)pmdp;
+		is_hp = true;
+	}
+	else {
+		ptep = pte_offset(pmdp, address);
+		pte = *ptep;
+	}
 
 	vcpu = (struct kvm_vcpu *)regs->cp0_osscratch7;
 	pgdp = (pgd_t *)vcpu->arch.gpa_pgd + __pgd_offset(gpa_address);
@@ -68,26 +83,42 @@ static void nlm_kvm_populate_page(struct pt_regs *regs, unsigned long address,
 		*(unsigned long *)pudp = (unsigned long)ptr;
 	}
 	pmdp = pmd_offset(pudp, gpa_address);
-	if (*(unsigned long *)pmdp == (unsigned long)invalid_pte_table) {
-		/* allocate a page for pte table and initialize it */
-		ptr = (unsigned long *)__get_free_page(GFP_KERNEL|GFP_DMA);
-		for (i = 0; i < PTRS_PER_PTE; i++)
-			ptr[i] = 0;
-		*(unsigned long *)pmdp = (unsigned long)ptr;
+	if (is_hp) {
+		ptep = (pte_t *)pmdp;
+	}
+	else {
+		if (*(unsigned long *)pmdp == (unsigned long)invalid_pte_table) {
+			/* allocate a page for pte table and initialize it */
+			ptr = (unsigned long *)__get_free_page(GFP_KERNEL|GFP_DMA);
+			for (i = 0; i < PTRS_PER_PTE; i++)
+				ptr[i] = 0;
+			*(unsigned long *)pmdp = (unsigned long)ptr;
+		}
+		ptep = pte_offset(pmdp, gpa_address);
 	}
-	ptep = pte_offset(pmdp, gpa_address);
 #else
-	if (*(unsigned long *)pudp == (unsigned long)invalid_pte_table) {
-		/* allocate a page for pte table and initialize it */
-		ptr = (unsigned long *)__get_free_page(GFP_KERNEL|GFP_DMA);
-		for (i = 0; i < PTRS_PER_PTE; i++)
-			ptr[i] = 0;
-		*(unsigned long *)pudp = (unsigned long)ptr;
+	if (is_hp) {
+		ptep = (pte_t *)pudp;
+	}
+	else {
+		if (*(unsigned long *)pudp == (unsigned long)invalid_pte_table) {
+			/* allocate a page for pte table and initialize it */
+			ptr = (unsigned long *)__get_free_page(GFP_KERNEL|GFP_DMA);
+			for (i = 0; i < PTRS_PER_PTE; i++)
+				ptr[i] = 0;
+			*(unsigned long *)pudp = (unsigned long)ptr;
+		}
+		ptep = pte_offset((pmd_t *)pudp, gpa_address);
 	}
-	ptep = pte_offset((pmd_t *)pudp, gpa_address);
 #endif
-	*page_size = PAGE_SIZE;
 	*ptep = pte;
+
+#ifdef CONFIG_MIPS_HUGE_TLB_SUPPORT
+	if (is_hp)  
+		*page_size = HPAGE_SIZE;
+	else
+#endif
+		*page_size = PAGE_SIZE;
 	*pa_address = (pte_val(pte) >> (_PAGE_GLOBAL_SHIFT + 6)) << 12;
 }
 #endif
-- 
1.7.1

