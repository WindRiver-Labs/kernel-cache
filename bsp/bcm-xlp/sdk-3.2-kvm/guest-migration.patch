From: Nam Ninh <nam.ninh@windriver.com>
Subject: bcm-xlp: guest migration support for sdk3.2
Date: Fri, 19 Jun 2015 11:59:31 -0400
X-Mailer: git-send-email 1.9.1
Content-Type: text/plain
MIME-Version: 1.0

This is a Broadcom provided patch.  While it clearly has additional
(presumably temporary) debugging related changes and non-essential
white space changes that greatly inflate the size of the change set,
it is integrated as-is in order to remain consistent with the silicon
vendor SCM the change was extracted from, in case further follow on
changes from that same silicon vendor kernel SCM are required.

This patch add codes to save guest context pt_regs before returning
to host. Fix the handlers of ioctl calls from qemu to save and retrieve
VM state registers properly.

Signed-off-by: Nam Ninh <nam.ninh@windriver.com>

diff --git a/arch/mips/include/asm/netlogic/kvm_xlp.h b/arch/mips/include/asm/netlogic/kvm_xlp.h
index da2b416..780ad6f 100644
--- a/arch/mips/include/asm/netlogic/kvm_xlp.h
+++ b/arch/mips/include/asm/netlogic/kvm_xlp.h
@@ -115,7 +115,7 @@ extern int compute_guest_return_epc(struct pt_regs *regs, unsigned int badinstr)
 extern void xlp_kvm_init_vm(struct kvm *kvm);
 extern void kvm_save_guest_context(struct pt_regs *regs, struct kvm_vcpu_guest *guest);
 extern void xlp_kvm_destroy_vm(struct kvm *kvm);
-extern void kvm_xlp_check_exit_request(struct kvm_vcpu *);
+extern int kvm_xlp_check_exit_request(struct kvm_vcpu *,struct pt_regs * regs);
 extern void nlm_flush_cache_L2L3(unsigned long, unsigned long);
 
 /* defined in arch/mips/netlogic/kvm/kvm_pic.c */
diff --git a/arch/mips/include/asm/processor.h b/arch/mips/include/asm/processor.h
index 32021ea..92f1631 100644
--- a/arch/mips/include/asm/processor.h
+++ b/arch/mips/include/asm/processor.h
@@ -193,65 +193,79 @@ struct octeon_cvmseg_state {
 
 #ifdef CONFIG_KVM_NETL
 
+struct pic_state2 {
+    uint64_t v64[0x200];
+};
+
+
 struct kvm_vm_state {
-	unsigned long cp0_guestctl0;
-	unsigned long cp0_guestctl1;
-	unsigned long cp0_count;
-	unsigned long cp0_gtoffset;
-	unsigned long cp0_osscratch7;
+	/* general purpose registers */
+	uint64_t	regs[32];
+	uint64_t	hi;
+	uint64_t	lo;
+
+
+	uint64_t cp0_guestctl0;
+	uint64_t cp0_guestctl1;
+	uint64_t root_epc;
+	uint64_t cp0_count;
+	uint64_t cp0_gtoffset;
+	uint64_t cp0_osscratch7;
 
 	/* Guest context to be saved */
-	unsigned long guest_cp0_index; /* 32: 0, 0 */
-	unsigned long guest_cp0_random; /* 32: 1, 0 */
-	unsigned long guest_cp0_entrylo0; /* 64: 2, 0 */
-	unsigned long guest_cp0_entrylo1; /* 64: 3, 0 */
-	unsigned long guest_cp0_context; /* 64: 4, 0 */
-	unsigned long guest_cp0_userlocal; /* 64: 4, 2 */
-	unsigned long guest_cp0_pagemask; /* 64: 5, 0 */
-	unsigned long guest_cp0_pagegrain; /* 32: 5, 1 */
-	unsigned long guest_cp0_pwbase; /* 64: 5, 5 */
-	unsigned long guest_cp0_pwfield; /* 64: 5, 6 */
-	unsigned long guest_cp0_pwsize; /* 64: 5, 7 */
-	unsigned long guest_cp0_wired; /* 32: 6, 0 */
-	unsigned long guest_cp0_pwctl; /* 32: 6, 6 */
-	unsigned long guest_cp0_hwrena; /* 32: 7, 0 */
-	unsigned long guest_cp0_badvaddr; /* 64: 8, 0 */
-	unsigned long guest_cp0_eirr; /* 64: 9, 6 */
-	unsigned long guest_cp0_eimr; /* 64: 9, 7 */
-	unsigned long guest_cp0_entryhi; /* 64: 10, 0 */
-	unsigned long guest_cp0_compare; /* 32: 11, 0 */
-	unsigned long guest_cp0_status; /* 32: 12, 0 */
-	unsigned long guest_cp0_intctl; /* 32: 12, 1 */
-	unsigned long guest_cp0_cause;  /* 32: 13, 0 */
-	unsigned long guest_cp0_epc; /* 64: 14, 0 */
-	unsigned long guest_cp0_ebase;  /* 64: 15, 1 */
-	unsigned long guest_cp0_config0; /* 32: 16, 0 */
-	unsigned long guest_cp0_config1; /* 32: 16, 1 */
-	unsigned long guest_cp0_config4; /* 32: 16, 4 */
-	unsigned long guest_cp0_xcontext; /* 64: 20, 0 */
-	unsigned long guest_cp0_errorepc; /* 64: 30, 0 */
-	unsigned long guest_cp0_osscratch0; /* 64: 22, 0 */
-	unsigned long guest_cp0_osscratch1; /* 64: 22, 1 */
-	unsigned long guest_cp0_osscratch2; /* 64: 22, 2 */
-	unsigned long guest_cp0_osscratch3; /* 64: 22, 3 */
-	unsigned long guest_cp0_osscratch4; /* 64: 22, 4 */
-	unsigned long guest_cp0_osscratch5; /* 64: 22, 5 */
-	unsigned long guest_cp0_osscratch6; /* 64: 22, 6 */
-	unsigned long guest_cp0_osscratch7; /* 64: 22, 7 */
-	unsigned long guest_cp0_badinstr; /* 32: 8, 1 */
-	unsigned long guest_cp0_badinstrp; /* 32: 8, 2 */
-	unsigned long guest_cp0_contextconfig; /* 32: 4, 1 */
-	unsigned long guest_cp0_xcontextconfig; /* 64: 4, 3 */
+	uint64_t guest_cp0_index; /* 32: 0, 0 */
+	uint64_t guest_cp0_random; /* 32: 1, 0 */
+	uint64_t guest_cp0_entrylo0; /* 64: 2, 0 */
+	uint64_t guest_cp0_entrylo1; /* 64: 3, 0 */
+	uint64_t guest_cp0_context; /* 64: 4, 0 */
+	uint64_t guest_cp0_userlocal; /* 64: 4, 2 */
+	uint64_t guest_cp0_pagemask; /* 64: 5, 0 */
+	uint64_t guest_cp0_pagegrain; /* 32: 5, 1 */
+	uint64_t guest_cp0_pwbase; /* 64: 5, 5 */
+	uint64_t guest_cp0_pwfield; /* 64: 5, 6 */
+	uint64_t guest_cp0_pwsize; /* 64: 5, 7 */
+	uint64_t guest_cp0_wired; /* 32: 6, 0 */
+	uint64_t guest_cp0_pwctl; /* 32: 6, 6 */
+	uint64_t guest_cp0_hwrena; /* 32: 7, 0 */
+	uint64_t guest_cp0_badvaddr; /* 64: 8, 0 */
+	uint64_t guest_cp0_eirr; /* 64: 9, 6 */
+	uint64_t guest_cp0_eimr; /* 64: 9, 7 */
+	uint64_t guest_cp0_entryhi; /* 64: 10, 0 */
+	uint64_t guest_cp0_compare; /* 32: 11, 0 */
+	uint64_t guest_cp0_status; /* 32: 12, 0 */
+	uint64_t guest_cp0_intctl; /* 32: 12, 1 */
+	uint64_t guest_cp0_cause;  /* 32: 13, 0 */
+	uint64_t guest_cp0_epc; /* 64: 14, 0 */
+	uint64_t guest_cp0_ebase;  /* 64: 15, 1 */
+	uint64_t guest_cp0_config0; /* 32: 16, 0 */
+	uint64_t guest_cp0_config1; /* 32: 16, 1 */
+	uint64_t guest_cp0_config4; /* 32: 16, 4 */
+	uint64_t guest_cp0_xcontext; /* 64: 20, 0 */
+	uint64_t guest_cp0_errorepc; /* 64: 30, 0 */
+	uint64_t guest_cp0_osscratch0; /* 64: 22, 0 */
+	uint64_t guest_cp0_osscratch1; /* 64: 22, 1 */
+	uint64_t guest_cp0_osscratch2; /* 64: 22, 2 */
+	uint64_t guest_cp0_osscratch3; /* 64: 22, 3 */
+	uint64_t guest_cp0_osscratch4; /* 64: 22, 4 */
+	uint64_t guest_cp0_osscratch5; /* 64: 22, 5 */
+	uint64_t guest_cp0_osscratch6; /* 64: 22, 6 */
+	uint64_t guest_cp0_osscratch7; /* 64: 22, 7 */
+	uint64_t guest_cp0_badinstr; /* 32: 8, 1 */
+	uint64_t guest_cp0_badinstrp; /* 32: 8, 2 */
+	uint64_t guest_cp0_contextconfig; /* 32: 4, 1 */
+	uint64_t guest_cp0_xcontextconfig; /* 64: 4, 3 */
 
 	/* wired tlbs, also see asm/kvm.h */
 #define MAX_WIRED_TLBS  32
-	unsigned long guest_wired_pagemask[MAX_WIRED_TLBS];
-	unsigned long guest_wired_entryhi[MAX_WIRED_TLBS];
-	unsigned long guest_wired_entrylo0[MAX_WIRED_TLBS];
-	unsigned long guest_wired_entrylo1[MAX_WIRED_TLBS];
-	unsigned long guest_wired_guestctl1[MAX_WIRED_TLBS];
+	uint64_t guest_wired_pagemask[MAX_WIRED_TLBS];
+	uint64_t guest_wired_entryhi[MAX_WIRED_TLBS];
+	uint64_t guest_wired_entrylo0[MAX_WIRED_TLBS];
+	uint64_t guest_wired_entrylo1[MAX_WIRED_TLBS];
+	uint64_t guest_wired_guestctl1[MAX_WIRED_TLBS];
 
 	struct mips_fpu_struct fpu;
+
+	struct pic_state2  pic_state;
 };
 	
 #endif
diff --git a/arch/mips/include/asm/stackframe.h b/arch/mips/include/asm/stackframe.h
index c4ef5f7..1238f5e 100644
--- a/arch/mips/include/asm/stackframe.h
+++ b/arch/mips/include/asm/stackframe.h
@@ -388,14 +388,27 @@
 		srl     v1, 31
 		beqz    v1, 80f
 
+#if 0
+
 		/* check whether an exit request has been generated. */
 		LONG_L  a0, PT_OSSCRATCH7(sp)
 		LONG_L  v0, VCPU_KVM(a0)
 		lw      v0, KVM_ARCH_EXIT_REQUEST(v0)
 		beqz    v0, 80f
+
+		/* detected QEMU request, prepare jump back to host */
+	        move    a1, sp
 		dla     v0, kvm_xlp_check_exit_request
 		jalr    v0
 		nop
+#else
+		/* check if any signal pending, if so, return to host */
+		LONG_L  a0, PT_OSSCRATCH7(sp)
+		dla     v0, kvm_xlp_check_exit_request
+	        move    a1, sp
+		jalr    v0
+
+#endif
 	80:
 #endif
 #ifdef CONFIG_CPU_HAS_SMARTMIPS
diff --git a/arch/mips/include/uapi/asm/kvm_netl.h b/arch/mips/include/uapi/asm/kvm_netl.h
index 4611de1..b8fbc3f 100644
--- a/arch/mips/include/uapi/asm/kvm_netl.h
+++ b/arch/mips/include/uapi/asm/kvm_netl.h
@@ -77,6 +77,17 @@ struct kvm_pktmem_info {
 #define KVM_MIPS_INJECT_PIC _IOW(KVMIO,  0xa8, int)
 
 /* Used to capture a guest state */
+struct kvm_mips_fpu {
+	uint64_t	fpr[32];
+	unsigned int	fcr31;
+};
+
+
+struct pic_state {
+    uint64_t v64[0x200];
+};
+
+/* Used to capture a guest state */
 struct kvm_regs {
 
 	/* general purpose registers */
@@ -90,6 +101,7 @@ struct kvm_regs {
 	uint64_t	root_epc;
 	uint64_t	root_count;
 	uint64_t	root_gtoffset;
+	uint64_t    cp0_osscratch7;
 
 	/* guest cop0 register */
 	uint64_t	guest_index;
@@ -139,19 +151,23 @@ struct kvm_regs {
 	uint64_t	guest_wired_entryhi[32];
 	uint64_t	guest_wired_entrylo0[32];
 	uint64_t	guest_wired_entrylo1[32];
+	uint64_t    guest_wired_guestctl1[32];
+
+	struct kvm_mips_fpu fpu;
+
+	struct pic_state   pic_state;
 };
 
 struct kvm_sregs {
 };
 
-/* Used to capture a guest state */
 struct kvm_fpu {
-	uint64_t	regs[32];
-	uint32_t	fir;
-	uint32_t	fccr;
-	uint32_t	fexr;
-	uint32_t	fenr;
-	uint32_t	fcsr;
+       uint64_t        regs[32];
+       uint32_t        fir;
+       uint32_t        fccr;
+       uint32_t        fexr;
+       uint32_t        fenr;
+       uint32_t        fcsr;
 };
 
 struct kvm_debug_exit_arch {
diff --git a/arch/mips/kvm-netl/context_switch.S b/arch/mips/kvm-netl/context_switch.S
index 4c79e5a..7fcee33 100644
--- a/arch/mips/kvm-netl/context_switch.S
+++ b/arch/mips/kvm-netl/context_switch.S
@@ -128,8 +128,17 @@
 	mtgc0	t0, $3, 0
 	LONG_L	t0, VCPU_G_CONTEXT(\restorearea)
 	mtgc0	t0, $4, 0
+	/* context config: $4, 1 */
+	LONG_L  t0, VCPU_G_CONTEXTCONFIG(\restorearea)
+	mtgc0   t0, $4, 1
+
 	LONG_L	t0, VCPU_G_USERLOCAL(\restorearea)
 	mtgc0	t0, $4, 2
+	/* xcontext config: $4, 3 */
+
+	LONG_L  t0, VCPU_G_XCONTEXTCONFIG(\restorearea)
+	mtgc0   t0, $4, 3
+
 	LONG_L	t0, VCPU_G_PAGEMASK(\restorearea)
 	dmtgc0	t0, $5, 0
 	LONG_L	t0, VCPU_G_PAGEGRAIN(\restorearea)
@@ -148,8 +157,25 @@
 	mtgc0	t0, $7, 0
 	LONG_L	t0, VCPU_G_BADVADDR(\restorearea)
 	mtgc0	t0, $8, 0
+
+	/* badinstr, badinstrp $8,1 / $8, 2 */
+	LONG_L	t0, VCPU_G_BADINSTR(\restorearea)
+	mtgc0	t0, $8, 1
+	LONG_L	t0, VCPU_G_BADINSTRP(\restorearea)
+	mtgc0	t0, $8, 2
+
+	/* restore guest count */
+	LONG_L	t0, VCPU_G_R_COUNT(\restorearea)     /* the same THREAD_VM_COUNT */
+	LONG_L	t1, VCPU_G_R_GTOFFSET(\restorearea)  /* the same THREAD_VM_GTOFFEST */
+	daddu	t0, t1
+	mfc0	t1, $9, 0
+	dsubu	t0, t1
+	mtc0	t0, $12, 7
+
+
 	LONG_L	t0, VCPU_G_EIRR(\restorearea)
 	mtgc0	t0, $9, 6
+
 	LONG_L	t0, VCPU_G_EIMR(\restorearea)
 	mtgc0	t0, $9, 7
 	LONG_L	t0, VCPU_G_ENTRYHI(\restorearea)
@@ -263,6 +289,7 @@ FEXPORT(__kvm_vcpu_run_guest)
 	/* flush the root/guest tlb, restore wired tlb, restore guest cp0 */
 	tlbginvf
 	tlbinvf
+
 	kvm_cpu_restore_guest_wired_tlbs a1
 	kvm_cpu_restore_guest_cp0 a1
 
diff --git a/arch/mips/kvm-netl/kvm.c b/arch/mips/kvm-netl/kvm.c
index d0aeda8..801dcc7 100644
--- a/arch/mips/kvm-netl/kvm.c
+++ b/arch/mips/kvm-netl/kvm.c
@@ -112,6 +112,7 @@ static void kvm_flush_rtlb(void *args)
 		"mfc0	%0, $10, 4	\n"
 		"move	%1, %2		\n"
 		"mtc0	%1, $10, 4	\n"
+		"EHB\n"
 		"tlbinvf		\n"
 		"mtc0	%0, $10, 4	\n"
 		".set pop		\n"
@@ -129,6 +130,7 @@ static void kvm_populate_gpa_map(pgd_t *gpa_pgd, uint64_t msize,
 	bool is_hp = false;
 
 #if 0
+	int count=0;
 	printk("KVM: populate guest memory: hva = 0x%llx, gpa = 0x%llx, size = 0x%llx\n",
 		hva, gpa, msize);
 #endif
@@ -196,6 +198,12 @@ static void kvm_populate_gpa_map(pgd_t *gpa_pgd, uint64_t msize,
 #endif
 		*ptep = pte;
 
+//		if(!pte_none(pte) && pte_present(pte))
+//		{
+//				printk("[%d] sync hva [0x%llx] gpa [0x%llx] hpa[0x%lx]\n",
+//						count++, address,gpa,pte_pfn(pte)<<PAGE_SHIFT);
+//		}
+
 		/* flush the Reset Vector region as it will be accessed as uncached. */
 		if ((gpa & 0xfffffffffff00000ULL) == 0x1fc00000) {
 			if ((pte_val(pte) >> _PAGE_PRESENT_SHIFT) & 0x1) {
@@ -219,6 +227,7 @@ static void kvm_sync_gpa_map(struct kvm *kvm)
 	int t;
 	pgd_t *gpa_pgd;
 
+	printk("***************** [%s] ***************** called\n", __FUNCTION__);
 	kvm->arch.mem_synced = 1; /* sync is called */
 
 	gpa_pgd = (pgd_t *)kvm->arch.gpa_pgd;
@@ -234,6 +243,9 @@ static void kvm_sync_gpa_map(struct kvm *kvm)
 		gpa = s->base_gfn << PAGE_SHIFT;
 		hva = s->userspace_addr;
 
+	//	printk("sync slot [%d] hva [0x%llx] gpa[0x%llx] size [0x%llx]\n",
+	//		          t, hva,gpa,msize);
+
 		/* enumerate all hva pages */
 		kvm_populate_gpa_map(gpa_pgd, msize, gpa, hva);
 		kvm->arch.slot_inited |= (1 << t); /* slot has been populated into gpa->pa table */
@@ -299,22 +311,19 @@ static void kvm_sync_gpa_map(struct kvm *kvm)
 	return;
 }
 
-static void kvm_sync_dirty_log(struct kvm *kvm)
+static void kvm_sync_dirty_log(struct kvm *kvm, struct kvm_dirty_log *log)
 {
-	int t;
 	pgd_t *gpa_pgd;
+	int count=0;
+	uint64_t msize, gpa, address;
+	struct kvm_memory_slot *s;
 
 	gpa_pgd = (pgd_t *)kvm->arch.gpa_pgd;
 
-	for (t = 0; t < KVM_USER_MEM_SLOTS; t++) {
-		struct kvm_memory_slot *s = &kvm->memslots->memslots[t];
-		uint64_t msize, gpa, address;
+                s= id_to_memslot(kvm->memslots, log->slot);
 
-		if (!s->npages)
-			continue;
-
-		if (!(s->flags & KVM_MEM_LOG_DIRTY_PAGES))
-			continue;
+		if ((!s->npages) || (!(s->flags & KVM_MEM_LOG_DIRTY_PAGES)))
+			return ;
 
 		msize = s->npages << PAGE_SHIFT;
 		gpa = s->base_gfn << PAGE_SHIFT;
@@ -338,6 +347,7 @@ static void kvm_sync_dirty_log(struct kvm *kvm)
 				mark_page_dirty_in_slot(kvm, s, address >> PAGE_SHIFT);
 				pte_val(pte) &= ~(_PAGE_MODIFIED | _PAGE_VALID | _PAGE_DIRTY);
 				*ptep = pte;
+				count++;
 			}
 
 			if (pmd_huge(*pmdp))
@@ -345,7 +355,9 @@ static void kvm_sync_dirty_log(struct kvm *kvm)
 			else
 				address += PAGE_SIZE;
 		}
-	}
+
+	printk("[%s:%d] total found [%d]/[%d] dirty pages for range [0x%llx --- 0x%llx]\n",
+                    __FUNCTION__,__LINE__,count,16*count,gpa,gpa+msize);
 
 	/* restart log session on all cpus */
 	on_each_cpu(kvm_flush_rtlb, (void *)(long)kvm->arch.guest_id, 1);
@@ -367,6 +379,9 @@ static void kvm_set_gpa_pa_map(struct kvm *kvm, struct kvm_userspace_memory_regi
 	gpa = mem->guest_phys_addr;
 	hva_base = mem->userspace_addr;
 
+	printk("[%s:%d] called: hva_base [0x%llx] gpa [0x%llx] szie [0x%llx]  write[%d]\n",
+                __FUNCTION__,__LINE__, hva_base,gpa, msize,write_protect);
+
 	for (address = hva_base; address < (hva_base + msize);
 	     address += PAGE_SIZE, gpa += PAGE_SIZE) {
 		pgdp = current->mm->pgd + __pgd_offset(address);
@@ -405,9 +420,27 @@ int kvm_arch_vcpu_runnable(struct kvm_vcpu *v)
 	return v->arch.runnable;
 }
 
+extern void dump_guest_gpa_and_slot(struct kvm *p_kvm);
 long kvm_arch_vcpu_ioctl(struct file *filp, unsigned int ioctl, unsigned long arg)
 {
 	/* We do not have arch-specific vcpu ioctl yet */
+
+	struct kvm_vcpu *vcpu = filp->private_data;
+
+	switch(ioctl)
+	{
+		case KVM_NMI: /* borrow this one */
+			vcpu->arch.init_guest=1;
+	//		printk("KVM_NMI received, will init guest cpu [%d] register (%p) \n",vcpu->vcpu_id,vcpu);
+			return 0;
+		case KVM_IA64_VCPU_GET_STACK:
+			dump_guest_gpa_and_slot(vcpu->kvm);
+			return 0;
+		default:
+			break;
+
+	}
+
 	return -EINVAL;
 }
 
@@ -465,7 +498,23 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)
 		vcpu->arch.pio_needed = 0;
 	}
 
-	if (vcpu->kvm->arch.exit_request == 0) {
+
+	if(vcpu->arch.init_guest)
+	{
+		struct kvm_regs * p_reg;
+
+		p_reg=&vcpu->arch.guest.gpu;
+		printk("check registers to be restored in guest cpu [%d]\n",vcpu->vcpu_id);
+
+		printk(" root_epc=[0x%llx] guest_epc=[0x%llx]\n",
+                    p_reg->root_epc,  p_reg->guest_epc);
+		printk("scrach 4 [0x%llx] scratch 5 [0x%llx]\n",
+			        p_reg->guest_osscratch4, p_reg->guest_osscratch5);
+
+		printk("check done\n");
+	}
+
+        {
 		local_irq_disable();
 		kvm_guest_enter();
 		local_irq_enable();
@@ -473,8 +522,6 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)
 		local_irq_disable();
 		kvm_guest_exit();
 		local_irq_enable();
-	} else {
-		ret = KVM_EXIT_QUIT_KVM << 24;
 	}
 
 	/* The execution reaches here because QEMU needs to be involved:
@@ -559,7 +606,7 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)
 		printk("KVM_EXIT_HC_HALX_NETSOC. run->exit_reason = 0x%lx, run->mips_hypcall.cmd = %d\n", run->exit_reason, run->mips_hypcall.cmd);
 #endif
 		break;
-#if 0
+#if 1
 	case KVM_EXIT_IRQ_WINDOW_OPEN:
 		run->exit_reason = ret >> 24;
 		break;
@@ -639,13 +686,60 @@ int kvm_arch_vcpu_ioctl_set_guest_debug(struct kvm_vcpu *vcpu, struct kvm_guest_
 
 int kvm_arch_vcpu_ioctl_set_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)
 {
-	memcpy(&vcpu->arch.guest.gpu, regs, sizeof(vcpu->arch.guest.gpu));
+
+	struct kvm_regs * p_reg; /* some register saved here when return to host */
+	struct kvm_arch * p_arch;
+
+	if(sizeof(struct kvm_regs) != sizeof(struct kvm_vm_state))
+	{
+			printk(KERN_ERR "[%s] mismatch struct kvm_regs and struct kvm_state",
+						__FUNCTION__);
+	}
+
+	p_reg=&vcpu->arch.guest.gpu;
+	p_arch=&vcpu->kvm->arch;
+
+	memcpy(p_reg, regs,sizeof(struct kvm_regs));
+
 	return 0;
 }
 
 int kvm_arch_vcpu_ioctl_get_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)
 {
-	memcpy(regs, &vcpu->arch.guest.gpu, sizeof(vcpu->arch.guest.gpu));
+	struct kvm_regs * p_reg; /* some register saved here when return to host */
+	struct kvm_vm_state * p_state; /*some registers saved here during context switch */
+	struct kvm_arch * p_arch;
+
+
+	if(sizeof(struct kvm_regs) != sizeof(struct kvm_vm_state))
+	{
+		printk(KERN_ERR "[%s] mismatch struct kvm_regs and struct kvm_state",
+						__FUNCTION__);
+	}
+
+
+	//force to create a schedule();
+	schedule_timeout(1);
+
+	p_reg=&vcpu->arch.guest.gpu;
+	p_state=&(current->thread.vm);
+
+
+
+	memcpy(regs, (struct kvm_regs *) p_state, sizeof(struct kvm_regs));
+	memcpy(regs->regs, p_reg->regs,sizeof(p_reg->regs));
+
+
+        p_arch=&vcpu->kvm->arch;
+
+	memcpy(&regs->pic_state,&p_arch->pic.u,sizeof(regs->pic_state));
+
+	regs->hi=p_reg->hi;
+	regs->lo=p_reg->lo;
+	regs->root_guestctl0=p_reg->root_guestctl0;
+	regs->root_epc=p_reg->root_epc;
+	regs->cp0_osscratch7=p_reg->cp0_osscratch7;
+
 	return 0;
 }
 
@@ -788,6 +882,204 @@ int kvm_arch_prepare_memory_region(struct kvm *kvm,
 	return 0;
 }
 
+void dump_pte(unsigned long i, unsigned long j, unsigned long l, unsigned long k,
+	            pte_t * p_pte, int count)
+{
+	unsigned long gpa;
+	unsigned long hpa;
+
+	gpa=(i<<PGDIR_SHIFT)|(j<<PUD_SHIFT)|(l<<PMD_SHIFT)|(k<<PAGE_SHIFT);
+
+	if(pte_present(*p_pte))
+	{
+			hpa=pte_pfn((*p_pte))<<PAGE_SHIFT;
+			printk("[%d] GAP [0x%lx] HPA [0x%lx] (%lu:%lu:%lu:%lu)\n",
+				count,gpa, hpa,i,j,l,k);
+	}
+	else
+	{
+			printk("[%d] GPA [0x%lx] HPA [NULL]\n",count, gpa);
+	}
+}
+
+void dump_slot_hva_hpa(int slot_idx, unsigned long hva, unsigned long gpa,
+	                          unsigned long page_number)
+{
+	unsigned long i;
+	unsigned long vaddr;
+	pgd_t * p_pgd;
+	pud_t * p_pud;
+	pmd_t * p_pmd;
+	pte_t * p_pte;
+
+
+	printk("phy addr for slot [%d] hva [0x%lx] gpa [0x%lx] page number[%lu]\n",
+		          slot_idx,hva,gpa, page_number);
+
+	printk("----------------------------------------------------\n");
+	for(i=0;i<page_number;i++)
+	{
+		vaddr=hva+(i<<PAGE_SHIFT);
+
+
+		p_pgd=pgd_offset(current->mm,vaddr);
+		p_pud=pud_offset(p_pgd,vaddr);
+		p_pmd=pmd_offset(p_pud,vaddr);
+
+		if(pmd_none(*p_pmd) || !pmd_present(*p_pmd))
+			continue;
+
+		p_pte=pte_offset(p_pmd,vaddr);
+
+		if(pte_none(*p_pte) || !pte_present(*p_pte))
+			continue;
+
+		printk("HVA [0x%lx] ----> GPA [0x%lx] ----> HPA [0x%lx]\n",vaddr,
+			    gpa+(i<<PAGE_SHIFT),pte_pfn(*p_pte)<<PAGE_SHIFT);
+
+
+	}
+	printk("---------------------DONE---------------------------\n");
+}
+
+
+void dump_gpa_hpa(struct kvm * kvm, unsigned long gpa,
+	                          unsigned long page_number)
+{
+	unsigned long i;
+	unsigned long vaddr;
+	pgd_t * p_pgd;
+	pud_t * p_pud;
+	pmd_t * p_pmd;
+	pte_t * p_pte;
+
+
+	printk("Dump GPA to HPA mapping from GPA_PGD. GPA base [0x%lx] page number[%lu]\n",
+		          gpa, page_number);
+
+	printk("----------------------------------------------------\n");
+	for(i=0;i<page_number;i++)
+	{
+		vaddr=gpa+(i<<PAGE_SHIFT);
+
+
+		p_pgd=(pgd_t *)kvm->arch.gpa_pgd+__pgd_offset(vaddr);
+
+		if(pgd_none(*p_pgd) || ! pgd_present(*p_pgd))
+			continue;
+
+		p_pud=pud_offset(p_pgd,vaddr);
+
+		if(pud_none(*p_pud) || ! pud_present(*p_pud))
+			continue;
+
+		p_pmd=pmd_offset(p_pud,vaddr);
+
+		if(pmd_none(*p_pmd) || !pmd_present(*p_pmd))
+			continue;
+
+		p_pte=pte_offset(p_pmd,vaddr);
+
+		if(pte_none(*p_pte) || !pte_present(*p_pte))
+			continue;
+
+		printk("GPA [0x%lx] ----> HPA [0x%lx]\n",vaddr,pte_pfn(*p_pte)<<PAGE_SHIFT);
+
+
+	}
+	printk("---------------------DONE---------------------------\n");
+}
+
+
+
+void dump_kvm_slot(struct kvm * kvm)
+{
+	struct kvm_memory_slot * slot;
+	int i;
+
+//	mutex_lock(&kvm->slots_lock);
+
+	for(i=0;i< KVM_MEM_SLOTS_NUM; i++)
+	{
+		slot=&kvm->memslots->memslots[i];
+		printk("[%d] hva [0x%lx] gpa [0x%lx] size [0x%lx] \n",
+                       i, slot->userspace_addr, (unsigned long)slot->base_gfn<<PAGE_SHIFT,
+                             slot->npages<<PAGE_SHIFT);
+
+
+	}
+//	mutex_unlock(&kvm->slots_lock);
+
+	for(i=0;i<KVM_MEM_SLOTS_NUM;i++)
+	{
+		slot=&kvm->memslots->memslots[i];
+
+		if(!slot->npages)
+			continue;
+
+		dump_slot_hva_hpa(i,slot->userspace_addr, slot->base_gfn<<PAGE_SHIFT,slot->npages);
+		dump_gpa_hpa(kvm,slot->base_gfn<<PAGE_SHIFT,slot->npages);
+	}
+
+}
+
+
+void dump_guest_gpa_and_slot(struct kvm * kvm)
+{
+	pgd_t *gpa_pgd = (pgd_t *)kvm->arch.gpa_pgd;
+	pgd_t * p_pgd;
+	pud_t * p_pud;
+	pmd_t * p_pmd;
+	pte_t * p_pte;
+	unsigned long i,j,l,k;
+	int count=0;
+
+	printk("PGDIR_SHIFT=%u\n",PGDIR_SHIFT);
+	printk("PUD_SHIFT=%u\n",PUD_SHIFT);
+	printk("PMD_SHIFT=%u\n",PMD_SHIFT);
+	printk("PAGE_SHIFT=%u\n",PAGE_SHIFT);
+
+		for (i = 0; i < PTRS_PER_PGD; i++)
+		{
+			p_pgd=gpa_pgd+i;
+
+			if(pgd_none(*p_pgd)|| !pgd_present(*p_pgd)) continue;
+
+
+			for(j=0;j<PTRS_PER_PUD;j++)
+			{
+			   p_pud=pud_offset(p_pgd,j<<PUD_SHIFT);
+
+			   if(pud_none(*p_pud) || !pud_present(*p_pud))
+						continue;
+
+			   for(l=0;l<PTRS_PER_PMD;l++)
+				{
+				   p_pmd=pmd_offset(p_pud,l<<PMD_SHIFT);
+
+				   if(pmd_none(*p_pmd) || ! pmd_present(*p_pmd))
+						continue;
+
+				   for(k=0;k<PTRS_PER_PTE;k++)
+					{
+
+					   p_pte=pte_offset(p_pmd,k<<PAGE_SHIFT);
+
+						if(pte_none(*p_pte))
+							continue;
+
+						dump_pte(i,j,l,k,p_pte, count ++);
+
+					}
+				}
+			}
+		}
+
+	   printk("GPA ---> HPA dump done. Total find [%d] entries\n", count);
+
+	  dump_kvm_slot(kvm);
+}
+
 void kvm_arch_commit_memory_region(struct kvm *kvm,
 				struct kvm_userspace_memory_region *mem,
 				const struct kvm_memory_slot *old,
@@ -943,7 +1235,7 @@ int kvm_vm_ioctl_get_dirty_log(struct kvm *kvm, struct kvm_dirty_log *log)
 	mutex_lock(&kvm->slots_lock);
 
 	/* march through guest page table to get the dirty log */
-	kvm_sync_dirty_log(kvm);
+	kvm_sync_dirty_log(kvm,log);
 
 	r = kvm_get_dirty_log(kvm, log, &is_dirty);
 	if (r) {
@@ -952,7 +1244,8 @@ int kvm_vm_ioctl_get_dirty_log(struct kvm *kvm, struct kvm_dirty_log *log)
 	}
 
 	if (is_dirty) {
-		memslot = &kvm->memslots->memslots[log->slot];
+	//	memslot = &kvm->memslots->memslots[log->slot];
+		memslot=id_to_memslot(kvm->memslots, log->slot);
 
 		gpa = memslot->base_gfn << PAGE_SHIFT;
 		gpa_end = gpa + (memslot->npages << PAGE_SHIFT);
diff --git a/arch/mips/kvm-netl/xlp.c b/arch/mips/kvm-netl/xlp.c
index 21d4c1c..bd31ebd 100644
--- a/arch/mips/kvm-netl/xlp.c
+++ b/arch/mips/kvm-netl/xlp.c
@@ -403,7 +403,6 @@ void xlp_kvm_init_vm(struct kvm *kvm)
 
 	/* Assign a new Guest ID */
 	arch->guest_id = kvm_get_new_guest_id();
-	arch->exit_request = 0;
 	arch->pktmem.gpa_start = -1;
 	arch->mem_synced = 0;
 	arch->slot_inited = 0;
@@ -451,18 +450,35 @@ void kvm_save_guest_context(struct pt_regs *regs, struct kvm_vcpu_guest *guest)
 	guest->gpu.lo = regs->lo;
 
 	guest->gpu.root_guestctl0  = regs->cp0_guestctl0;
+	guest->gpu.root_guestctl1  = regs->cp0_guestctl1;
 	guest->gpu.root_epc        = regs->cp0_epc;
+	guest->gpu.cp0_osscratch7 = regs->cp0_osscratch7;
 }
 
-void kvm_xlp_check_exit_request(struct kvm_vcpu *vcpu)
+int  kvm_xlp_check_exit_request(struct kvm_vcpu *vcpu,struct pt_regs *regs)
 {
 	unsigned int val;
+	struct thread_info * host_current;
 
-#if 1
-	printk("Guest vcpu %lld exiting\n", vcpu->arch.guest.gpu.guest_ebase & 0x3ff);
-#endif
+	host_current=(struct thread_info *) vcpu->arch.root.gp;
+
+	if(host_current->flags & _TIF_SIGPENDING)
+	{
+
+	printk("Guest vcpu %lld received SIGNAL\n", vcpu->arch.guest.gpu.guest_ebase & 0x3ff);
+
+
+        /* save guest context */
+	kvm_save_guest_context(regs, kvm_get_vcpu_guest_regs(regs));
+
+        val = (KVM_EXIT_IRQ_WINDOW_OPEN << 24);
 
-	/* return to qemu for exit */
-	val = (KVM_EXIT_QUIT_KVM << 24);
 	__kvm_vcpu_leave_guest(vcpu, val);
+
+	//NEVER GO HERE
+		return 1;
+	}
+
+	return 0;
+
 }
diff --git a/arch/mips/netlogic/kvm-netl/kvm_pic.c b/arch/mips/netlogic/kvm-netl/kvm_pic.c
index 76ff1e7..58e67bc 100644
--- a/arch/mips/netlogic/kvm-netl/kvm_pic.c
+++ b/arch/mips/netlogic/kvm-netl/kvm_pic.c
@@ -111,27 +111,9 @@ static inline void unhandled_exception(const char *func, unsigned long addr,
 		func, addr, epc, write);
 }
 
-void kvm_pic_inject_guest(struct kvm_arch *arch, unsigned int irt, struct pt_regs *regs)
-{
-	unsigned long long irt_entry;
-	unsigned int rvec;
-	struct kvm_vcpu_arch *vcpu_arch;
-	struct kvm_vcpu *vcpu = (struct kvm_vcpu *)regs->cp0_osscratch7;
-
-	irt_entry = arch->pic.u.v64[(0x200 >> 1) + irt];
-	rvec = (irt_entry >> 24) & 0x3f;
-	vcpu_arch = kvm_get_vcpu_arch(regs);
-#ifdef DEBUG
-	printk("===== Inject IRT %u, RVEC %u to the guest (irt_entry %llx)\n", irt, rvec, irt_entry);
-#endif
-	atomic_or_llong(&vcpu_arch->pip_vector, 1ULL << rvec);
-	vcpu->arch.runnable = 1;
-	kvm_vcpu_kick (vcpu);
-}
-
 void kvm_pic_inject_guest_ext(struct kvm *kvm, struct kvm_arch *arch, unsigned int irt, unsigned int cpuid)
 {
-	unsigned long long irt_entry;
+	unsigned long long irt_entry=0;
 	unsigned int rvec;
 	struct kvm_vcpu_arch *vcpu_arch;
 	struct kvm_vcpu *vcpu = kvm_get_vcpu_ext(kvm, cpuid);
@@ -156,14 +138,17 @@ void kvm_pic_inject_guest_ext(struct kvm *kvm, struct kvm_arch *arch, unsigned i
 		}
 	}
 	vcpu_arch = kvm_get_vcpu_arch_ext(kvm, cpuid);
-#ifdef DEBUG
-	printk("===== Inject IRT %u, RVEC %u to the guest (irt_entry %llx)\n", irt, rvec, irt_entry);
-#endif
 	atomic_or_llong(&vcpu_arch->pip_vector, 1ULL << rvec);
 	vcpu->arch.runnable = 1;
 	kvm_vcpu_kick (vcpu);
 }
 
+void kvm_pic_inject_guest(struct kvm_arch *arch, unsigned int irt, struct pt_regs *regs)
+{
+	struct kvm *kvm = ((struct kvm_vcpu *)regs->cp0_osscratch7)->kvm;
+	kvm_pic_inject_guest_ext(kvm, arch, irt, 0);
+}
+
 static unsigned int xlp_kvm_get_ipi_cpuid(uint64_t ipi_ctrl)
 {
 	unsigned int mask = ipi_ctrl & 0x3ff;
diff --git a/arch/mips/netlogic/xlp/csrc-tsc.c b/arch/mips/netlogic/xlp/csrc-tsc.c
index 0bd4e65..1b0b707 100644
--- a/arch/mips/netlogic/xlp/csrc-tsc.c
+++ b/arch/mips/netlogic/xlp/csrc-tsc.c
@@ -55,7 +55,11 @@ static uint32_t tsc_timer_freq(void)
 
 static cycle_t nlm_get_tsc_timer(struct clocksource *cs)
 {
-	return nlm_read_tsc();
+     uint64_t cur_cycle=nlm_read_tsc();
+	 uint64_t  tsc_offset=__read_64bit_c0_register($22,4); //GOD BLESS ME, NO ONE will use this one
+
+	  return (cycle_t)(cur_cycle+tsc_offset);
+
 }
 
 static struct clocksource csrc_tsc = {
@@ -68,6 +72,7 @@ static struct clocksource csrc_tsc = {
 
 void nlm_init_tsc_timer(void)
 {
+    __write_64bit_c0_register($22,4,0);
 	clocksource_register_hz(&csrc_tsc, tsc_timer_freq());
 }
 
-- 
1.7.1

