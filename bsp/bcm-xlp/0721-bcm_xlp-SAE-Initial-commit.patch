From 129f226c827ed1b5913b082a376d62feaf461039 Mon Sep 17 00:00:00 2001
From: Siva Pochiraju <sivap@netlogicmicro.com>
Date: Thu, 22 Dec 2011 14:32:28 +0530
Subject: [PATCH 721/762] bcm_xlp: SAE: Initial commit

SAE: Initial commit

Based on Broadcom SDK 2.3.

Signed-off-by: Siva Pochiraju <sivap@netlogicmicro.com>
Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 drivers/crypto/sae/cryptoapi.c |  939 ++++++++++++++++++++++++++++++++++++++++
 drivers/crypto/sae/cryptodev.h |  198 +++++++++
 drivers/crypto/sae/nlmcrypto.h |  670 ++++++++++++++++++++++++++++
 3 files changed, 1807 insertions(+), 0 deletions(-)
 create mode 100644 drivers/crypto/sae/cryptoapi.c
 create mode 100644 drivers/crypto/sae/cryptodev.h
 create mode 100644 drivers/crypto/sae/nlmcrypto.h

diff --git a/drivers/crypto/sae/cryptoapi.c b/drivers/crypto/sae/cryptoapi.c
new file mode 100644
index 0000000..9abb7f8
--- /dev/null
+++ b/drivers/crypto/sae/cryptoapi.c
@@ -0,0 +1,939 @@
+/*********************************************************************
+
+  Copyright 2003-2010 Netlogic Microsystem, Inc. ("Netlogic"). All rights
+  reserved.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+  1. Redistributions of source code must retain the above copyright
+  notice, this list of conditions and the following disclaimer.
+  2. Redistributions in binary form must reproduce the above copyright
+  notice, this list of conditions and the following disclaimer in
+  the documentation and/or other materials provided with the
+  distribution.
+
+  THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems, Inc. ``AS IS'' AND
+  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL RMI OR CONTRIBUTORS BE LIABLE
+  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+  INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+  CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+  THE POSSIBILITY OF SUCH DAMAGE.
+
+  *****************************#NLM_2#**********************************/
+#include <stdio.h>
+#include <stddef.h>
+#include <stdlib.h>
+#include <string.h>
+#include <unistd.h>
+#include <sys/mman.h>
+#include <fcntl.h>
+#include <errno.h>
+#include <sched.h>
+#include <sys/types.h>
+#ifdef NLM_CRYPTO_LINUX_U
+#include <sys/select.h>
+#include <sys/eventfd.h>
+#include <sys/poll.h>
+#include <sys/ioctl.h>
+#endif
+#include <sys/stat.h>
+#include <sys/errno.h>
+#include <nlm_hal_macros.h>
+#include <nlm_hal_xlp_dev.h>
+#include <nlm_hal_fmn.h>
+#include "cryptodev.h"
+#define CRYPTO_API_drivers/crypto/sae
+//#define NLM_CRYPTO_DEBUG_EN
+#include "nlmcrypto.h"
+
+/**
+* @file_name drivers/crypto/sae/cryptoapi.c
+*/
+
+/**
+* @defgroup crypto NLM Crypto API
+* @brief Description crypto APIS implemented for linux-userspace/netos applications. 
+*
+* In linux-userspace mode, the following two files are copied to the linux ramdisk.
+* - User should load the nlmcrypto.ko before using the apis(insmod /tmp/nlmcrypto.ko).
+* - libnlmcrypto.so is copied to the /lib64 directory. So export the following path
+* if it is not done "export LD_LIBRARY_PATH=/lib64/:$LD_LIBRARY_PATH"
+*
+* In netos/helinux mode, the library files are added to libnlm.a . In Helinux mode, 
+* user should load the nlmcrypto.ko before loading the application using ldapps. 
+*
+* The application should drivers/crypto/sae "nlmcrypto.h" which is in (SDK_BASE/cryptolib/drivers/crypto/sae). 
+* nlmcrypto.h internall drivers/crypto/saes nlm_hal_crypto.h(SDK_BASE/libraries/hal) and nlmcrypto_ifc.h
+* nlmcrypto_ifc.h is user level interface forthe crypto libarary. It should define the app_init
+* function, shared mem alloc/free and virt_to_phys/phys_to_virt macros.
+*
+* The implementation is provided for linux-userspace and netos. For linux-userspace the implementation
+* can be found in (SDK_BASE/linux-userspace/crypto/lib) and for netos(SDK_BASE/hyperexec/srcs/usr/drivers/crypto/sae/)
+* 
+* The application can modify this file if it requires another allocator/memory conversion.
+*/
+
+static cryptolock_t *crypto_sync_lock = NULL;
+struct nlm_crypto_init_params iparam;
+
+static inline int crypto_async_push_msg(struct nlm_crypto_ctx *ctx)
+{
+	if(fifo_full(ctx->mtail, ctx->max_msgs, ctx->mhead))
+		return NLM_CRYPTO_EBUSY;
+	ctx->mtail = (ctx->mtail + 1) % ctx->max_msgs;
+	ctx->msgs[ctx->mtail].msg[0] = 0ULL;
+	ctx->msgs[ctx->mtail].msg[1] = 0ULL;
+	ctx->rsp_pend++;
+	return ctx->mtail;
+}
+
+static inline int crypto_async_pop_msg(struct nlm_crypto_ctx *ctx, void **ctrl, void **param, void **arg)
+{
+	int tmp;
+	if(fifo_empty(ctx->mtail, ctx->mhead))
+		return 0;
+	tmp = (ctx->mhead + 1) % ctx->max_msgs;
+	if((!ctx->msgs[tmp].msg[0]) && (!ctx->msgs[tmp].msg[1]))
+	       return 0;	
+	ctx->mhead = tmp;
+	 ctx->rsp_pend--;
+	if(ctrl)
+		*ctrl = (void *)ctx->msgs[tmp].msg[2];
+	if(param)
+		*param = (void *)ctx->msgs[tmp].msg[3];
+	if(arg)
+		*arg = (void *)ctx->msgs[tmp].msg[4];
+	return 1;
+}
+
+
+/**
+* @brief The crypto lib init fuction when it is called from linux-userspace, it mmaps the shared memory 
+* to the calling process context. Also it initializes the memory allocater and creates a /dev/ node 
+* for the crypto device. In netos it retrieves the fdt parameters.
+* This should be called only once from every application.
+*
+* @return 
+*  - On sucess,  returns NLM_CRYPTO_OK. 
+*  - On failure, returns NLM_CRYPTO_ERROR.
+*	  
+* @ingroup crypto
+*/
+int nlm_crypto_lib_init(void)
+{
+	int rv;
+
+	if((rv = crypto_app_lib_init(&iparam)) < 0)
+		return rv;
+	
+	crypto_sync_lock = (cryptolock_t *)iparam.pcpu_sync_lock_ptr;
+
+	return NLM_CRYPTO_OK;
+}
+
+/**
+* @brief The crypto open sync session create a context memory for the given sync session. 
+* The context memory is allocated from the shared mem so that it can be accessed 
+* from both kernel and userspace. In netos mode, as the application affinity is always set, 
+* only sync-exclusive mode is supported.
+*
+* @param[in] sync_mode sync-exclusive/sync-shared
+* @param[in] cpu  Cpu affinity will be set for the given cpu. if cpu == -1, current cpu will be selected . 
+* @param[in] arg  application specific arg.
+*
+* @return 
+*  - On sucess,  returns pointer to context. 
+*  - On failure, returns NULL, errno will be set.
+*	  
+* @ingroup crypto
+*/
+nlm_crypto_ctx_t *nlm_crypto_open_sync_session(int sync_mode, int cpu, void *arg)
+{
+	int size;
+	struct nlm_crypto_ctx *ctx = NULL;
+	int max_outstanding_reqs = 1;
+
+	/* In sync, as there is no way to order the requests and response, only one 
+	 outstanding request is allowed. */
+
+	size =  sizeof(struct nlm_crypto_ctx) + max_outstanding_reqs * sizeof(struct nlm_crypto_msg);
+	if((ctx = crypto_malign(XLP_CACHELINE_SIZE, size)) == NULL) {
+		errno = ENOMEM;
+		goto err_exit;
+	}
+	
+	ctx->mode = sync_mode;
+	ctx->max_msgs = max_outstanding_reqs;
+	ctx->rsp_pend = 0;
+	ctx->arg = (uint64_t)arg;
+
+	if((sync_mode == NLM_CRYPTO_MODE_SYNC_EXLVC) && (iparam.sae_rx_sync_vc >= 0)) {
+		ctx->rx_vc = iparam.sae_rx_sync_vc;
+		if(cpu < 0)
+			cpu = my_cpu_id();
+		if(app_set_affinity(cpu) < 0) 
+			goto err_exit;
+		ctx->fd = 0;
+		nlm_dbg_print("affinity set on cpu %d\n", cpu);
+	}
+#ifdef NLM_CRYPTO_LINUX_U
+	else if((sync_mode == NLM_CRYPTO_MODE_SYNC_SHDVC) && (iparam.sae_rx_vc >= 0))  {
+		ctx->lock = 0;
+		ctx->rx_vc = iparam.sae_rx_vc;
+		ctx->fd = eventfd(0, 0);
+		if(ctx->fd < 0) 
+			goto err_exit;
+		ctx->fdctxt = crypto_get_eventfd_ctxt(crypto_virt_to_phys(ctx));
+		if(ctx->fdctxt == 0) {
+			close(ctx->fd);
+			goto err_exit;
+		}
+		nlm_dbg_print("Event fd created fd %d\n", ctx->fd);
+	}
+#endif	
+	else {
+		errno = EINVAL;
+		goto err_exit;
+	}
+
+	return ctx;
+err_exit:
+		
+	if(ctx) 
+		crypto_mfree(ctx);
+			
+	return NULL;
+}	
+	
+/**
+* @brief The crypto open async session create a context memory for the given async session. 
+* The context memory is allocated from the shared mem so that it can be accessed 
+* from both kernel and userspace. 
+*
+* @param[in] max_outstanding_reqs Maximum outstanding reqests for a session.
+* @param[in] callback Used only in netos mode. function handler to be called after 
+* receiving/decoding the fmn message.
+* @param[in] arg  application specific arg.
+*
+* @return 
+*  - On sucess,  returns pointer to context. 
+*  - On failure, returns NULL, errno will be set.
+*	  
+* @ingroup crypto
+*/
+nlm_crypto_ctx_t *nlm_crypto_open_async_session(int max_outstanding_reqs, 
+		int (*callback)(nlm_crypto_ctx_t *ctxt, void *ctrl, void *param, void *arg),
+		void *arg)
+{
+	int size;
+	struct nlm_crypto_ctx *ctx = NULL;
+
+	if(max_outstanding_reqs > NLM_CRYPTO_MAX_OUT_REQS || iparam.sae_rx_vc < 0) {
+		nlm_err_print("Error : Invalid param reqs %d(%d) or sae_rx_vc %d\n", 
+				max_outstanding_reqs, NLM_CRYPTO_MAX_OUT_REQS, iparam.sae_rx_vc);
+		errno = EINVAL;
+		return NULL;
+	}
+	/* We need one more for push and pop operation, where one entry will not be 
+	 used for storing data */
+	max_outstanding_reqs++;
+			       
+	size =  sizeof(struct nlm_crypto_ctx) + max_outstanding_reqs * sizeof(struct nlm_crypto_msg);
+	if((ctx = crypto_malign(XLP_CACHELINE_SIZE, size)) == NULL) {
+		errno = ENOMEM;
+		goto err_exit;
+	}
+	
+	ctx->lock = 0;
+	ctx->mode = NLM_CRYPTO_MODE_ASYNC;
+	ctx->rx_vc = iparam.sae_rx_vc;
+	ctx->max_msgs = max_outstanding_reqs;
+	ctx->rsp_pend = 0;
+	ctx->arg = (uint64_t)arg;
+
+#ifdef NLM_CRYPTO_LINUX_U
+	ctx->fd = eventfd(0, 0);
+	if(ctx->fd < 0) 
+		goto err_exit;
+	ctx->fdctxt = crypto_get_eventfd_ctxt(crypto_virt_to_phys(ctx));
+	if(ctx->fdctxt == 0) {
+		close(ctx->fd);
+		goto err_exit;
+	}
+#endif
+	ctx->async_callback = (uint64_t)callback;
+	ctx->mhead = 0;
+	ctx->mtail = 0;
+	
+	return ctx;
+
+err_exit:
+	if(ctx)
+		crypto_mfree(ctx);
+	return NULL;
+}
+
+/**
+* @brief The crypto close session closes the fd and removes the ctxt mem allocated. 
+* Returns error if the context has any pending responses from the security engine 
+*
+* @param[in] ctxt Pointer to the ctx structure returned by the open session. 
+*
+* @return 
+*  - On sucess,  returns NLM_CRYPTO_OK. 
+*  - On failure, returns NLM_CRYPTO_ERROR.
+*	  
+* @ingroup crypto
+*/
+int nlm_crypto_close_session(nlm_crypto_ctx_t *ctxt)
+{
+	struct nlm_crypto_ctx *ctx = ctxt;
+	int rv;
+
+	if(ctx->rsp_pend > 0) {
+	//	nlm_err_print("Error : Msg responses %d pending, cannot free the session now\n", ctx->rsp_pend);
+		return NLM_CRYPTO_EBUSY;
+	}
+	if(ctx->fd) {
+		if((rv = crypto_put_eventfd_ctxt(crypto_virt_to_phys(ctx))) < 0)
+			return rv;
+		if(close(ctx->fd) < 0)
+			return NLM_CRYPTO_ERROR;
+	}
+
+	crypto_mfree(ctx);
+	return NLM_CRYPTO_OK;
+}
+
+/**
+* @brief The reset session function unlocks if the context lock has been taken by the calling process. 
+* 
+* @param[in] ctxt Pointer to the ctx structure returned by the open session. 
+*
+* @return 
+*	  
+* @ingroup crypto
+*/
+void nlm_crypto_reset_session(nlm_crypto_ctx_t *ctxt)
+{
+	struct nlm_crypto_ctx *ctx = ctxt;
+	unsigned int pid = getpid();
+	int i;
+
+	if(ctx) {
+		if(ctx->lock == pid)
+			crypto_unlock((cryptolock_t *)&ctx->lock);
+	
+		if(ctx->mode == NLM_CRYPTO_MODE_SYNC_EXLVC) {
+			for(i = 0; i < MAX_CPUS; i++) {
+				if(crypto_sync_lock[i].lock == pid)
+					crypto_unlock(&crypto_sync_lock[i]);
+			}
+		}
+	} else {
+		for(i = 0; i < MAX_CPUS; i++) {
+			if(crypto_sync_lock[i].lock == pid)
+				crypto_unlock(&crypto_sync_lock[i]);
+		}
+	}
+	return;
+}
+
+
+/**
+* @brief The crypto rsa param alloc function allocs the rsa param memory from the shared pool. 
+*
+* @param[in] ctxt Pointer to the ctx structure returned by the open session. 
+* @param[in] op see nlm_crypto_rsa_op_t macro
+* @param[in] blksize_nbits block size in number of bits
+*
+* @return 
+*  - On sucess,  returns pointer to the param memory. 
+*  - On failure, returns NULL.
+*	  
+* @ingroup crypto
+*/
+struct nlm_crypto_rsa_param *nlm_crypto_rsa_param_alloc(nlm_crypto_ctx_t *ctxt, 
+		enum nlm_crypto_rsa_op_t op, int blksz_nbits)
+{
+	struct nlm_crypto_rsa_param *prsa;
+	char *pmem;
+	char **params;
+	int blksz_nbytes = blksz_nbits >> 3;
+	int size = sizeof(struct nlm_crypto_rsa_param) + XLP_CACHELINE_SIZE + 
+		(blksz_nbytes * NLM_CRYPTO_RSA_PARAMS_NELMNTS);
+	int i;
+
+	if((prsa = crypto_malign(XLP_CACHELINE_SIZE, size)) == NULL) {
+		errno = ENOMEM;
+		return NULL;
+	}
+	pmem = (char *)((unsigned long)prsa + sizeof(struct nlm_crypto_rsa_param));
+	pmem = (char *)crypto_roundup((unsigned long long)pmem, (unsigned long long)XLP_CACHELINE_SIZE);
+	params = (char **)&prsa->modexp;
+	for(i = 0; i < NLM_CRYPTO_RSA_PARAMS_NELMNTS; i++)
+		*(params + i) = (char *)((unsigned long)pmem + (blksz_nbytes * i));
+
+	prsa->result = NULL;	
+	return prsa;
+}
+
+/**
+* @brief The crypto ecc param alloc function allocs the ecc param memory from the shared pool. 
+*
+* @param[in] ctxt Pointer to the ctx structure returned by the open session. 
+* @param[in] op see nlm_crypto_ecc_op_t macro
+* @param[in] blksize_nbits block size in number of bits
+*
+* @return 
+*  - On sucess,  returns pointer to the param memory. 
+*  - On failure, returns NULL.
+*	  
+* @ingroup crypto
+*/
+struct  nlm_crypto_ecc_param *nlm_crypto_ecc_param_alloc(nlm_crypto_ctx_t *ctxt, 
+		enum nlm_crypto_ecc_op_t op, int blksz_nbits)
+{
+	struct  nlm_crypto_ecc_param *pecc;
+	char *pmem;
+	char **params;
+	int blksz_nbytes = blksz_nbits >> 3;
+	int size = sizeof(struct nlm_crypto_ecc_param) + XLP_CACHELINE_SIZE + 
+		(blksz_nbytes * NLM_CRYPTO_ECC_PARAMS_NELMNTS);
+	int i;
+
+	if((pecc = crypto_malign(XLP_CACHELINE_SIZE, size)) == NULL) {
+		errno = ENOMEM;
+		return NULL;
+	}
+	
+	pmem = (char *)((unsigned long)pecc + sizeof(struct nlm_crypto_ecc_param));
+	pmem = (char *)crypto_roundup((unsigned long long)pmem, (unsigned long long)XLP_CACHELINE_SIZE);
+	params = (char **)&pecc->padd;
+	for(i = 0; i < NLM_CRYPTO_ECC_PARAMS_NELMNTS; i++)
+		*(params + i) = (char *)((unsigned long)pmem + (blksz_nbytes * i));
+
+	if((blksz_nbits % 64) != 0) {
+		crypto_mfree(pecc);
+		errno = EINVAL;
+		return NULL;
+	}
+	pecc->result = NULL;	
+	return pecc;
+}
+
+
+/**
+* @brief The crypto param free function free the param memory . 
+*
+* @param[in] ctxt Pointer to the ctx structure returned by the open session. 
+* @param[in] param Pointer to the param
+*
+* @return 
+*  - On sucess,  returns pointer to the param memory. 
+*  - On failure, returns NULL.
+*	  
+* @ingroup crypto
+*/
+void nlm_crypto_param_free(nlm_crypto_ctx_t *ctxt, void *param)
+{
+	crypto_mfree(param);
+	return;
+}
+
+/**
+* @brief The crypto rsa result alloc function allocates the result pointer for the 
+* given rsa block size. 
+*
+* @param[in] ctxt Pointer to the ctx structure returned by the open session. 
+* @param[in] blksize_nbits block size in number of bits
+*
+* @return 
+*  - On sucess,  returns pointer to the result memory. 
+*  - On failure, returns NULL.
+*	  
+* @ingroup crypto
+*/
+struct nlm_crypto_rsa_result *nlm_crypto_rsa_result_alloc(nlm_crypto_ctx_t *ctxt, int blksz_nbits)
+{
+	uint32_t elmnts  = NLM_CRYPTO_RSA_RESULT_NELMNTS;
+	unsigned char **params;
+	unsigned char *result , *r;
+	int blksz_nbytes = blksz_nbits >> 3, i;
+
+	result = crypto_malign(XLP_CACHELINE_SIZE, (blksz_nbytes * elmnts) + 
+			XLP_CACHELINE_SIZE /* this covers the size of the structure*/);
+	if(!result) {
+		errno = ENOMEM;
+		return NULL;
+	}
+
+	r = result + XLP_CACHELINE_SIZE;
+	params = (unsigned char **)&(((struct nlm_crypto_rsa_result *)result)->r);
+	for(i = 0; i < elmnts; i++)
+		*(params + i) = r + (i * blksz_nbytes);
+		
+	return (struct nlm_crypto_rsa_result *)result;
+}
+
+
+
+/**
+* @brief The crypto ecc result alloc function allocates the result pointer for the 
+* given ecc type and block size. 
+*
+* @param[in] ctxt Pointer to the ctx structure returned by the open session. 
+* @param[in] op see nlm_crypto_ecc_op_t macro
+* @param[in] blksize_nbits block size in number of bits
+*
+* @return 
+*  - On sucess,  returns pointer to the result memory. 
+*  - On failure, returns NULL.
+*	  
+* @ingroup crypto
+*/
+struct nlm_crypto_ecc_result *nlm_crypto_ecc_result_alloc(nlm_crypto_ctx_t *ctxt, 
+		enum nlm_crypto_ecc_op_t op, int blksz_nbits)
+{
+	uint32_t elmnts  = NLM_CRYPTO_ECC_RESULT_NELMNTS;
+	unsigned char **params;
+	unsigned char *result , *r;
+	int blksz_nbytes = blksz_nbits >> 3, i;
+
+	result = crypto_malign(XLP_CACHELINE_SIZE, (blksz_nbytes * elmnts) + 
+			XLP_CACHELINE_SIZE /* this covers the size of the structure*/);
+	if(!result) {
+		errno = ENOMEM;
+		return NULL;
+	}
+
+	r = result + XLP_CACHELINE_SIZE;
+	params = (unsigned char **)&(((struct nlm_crypto_ecc_result *)result)->fres);
+	for(i = 0; i < elmnts; i++)
+		*(params + i) = r + (i * blksz_nbytes);
+		
+	return (struct nlm_crypto_ecc_result *)result;
+}
+
+/**
+* @brief The crypto result free function free the result memory. 
+*
+* @param[in] ctxt Pointer to the ctx structure returned by the open session. 
+* @param[in] result Pointer to the result
+*
+* @return 
+*	  
+* @ingroup crypto
+*/
+void nlm_crypto_result_free(nlm_crypto_ctx_t *ctxt, void *result)
+{
+	crypto_mfree(result);
+	return;
+}
+
+/**
+* @brief The crypto do op function generates the descritptor and send to the security engine. 
+* If the session is async, this function returns immediately after sending the message. 
+* But in sync model it waits for the responses.
+*
+* @param[in] ctxt Pointer to the ctx structure returned by the open session. 
+* @param[in] type see nlm_crypto_op_type_t macro
+* @param[in] ctrl Pointer to the control structure
+* @param[in] param Pointer to the param
+* @param[in] nsegs Number of packet segments, not considered for rsa
+* @param[in] args Pointer to the application specific arg
+*
+* @return 
+*  - On sucess,  returns NLM_CRYPTO_OK
+*  - On failure, returns NLM_CRYPTO_ERROR.
+*	  
+* @ingroup crypto
+*/
+int nlm_crypto_do_op(nlm_crypto_ctx_t *ctxt, enum nlm_crypto_op_type_t optype, 
+		void *ctrl,  void *param, int nsegs, void *arg)
+{
+	struct nlm_crypto_ctx *ctx = ctxt;
+	uint64_t msg0, msg1;
+	uint32_t size, code, src;
+	int cpu = my_cpu_id(), type, op;
+	int dstvc_base, dstvc_lmt, etype;
+	int fbvc = ctx->rx_vc + (cpu * NR_VCS_PER_THREAD);
+	void *result;
+
+	switch(optype) {
+		case NLM_CRYPTO_RSA:
+		{
+			struct nlm_crypto_rsa_ctrl *rsac = ctrl;
+			int blksz_nbits = rsac->blksz_nbits; 
+			int bitset;
+			op = rsac->op;
+			/* rsa starts from 512 bits, 9 is hardcoded */
+			bitset = get_flc(blksz_nbits) - 1;
+			type = bitset - 9 + NLM_CRYPTO_RSA_TYPE_SVALUE;
+			if(type > NLM_CRYPTO_RSA_TYPE_EVALUE || ((blksz_nbits - (1 << bitset)) > 0))
+				return NLM_CRYPTO_ERROR;
+			result = ((struct nlm_crypto_rsa_param *)param)->result;
+			break;
+		}
+		case NLM_CRYPTO_ECC:
+		{
+			struct nlm_crypto_ecc_ctrl *eccc = ctrl;
+			int blksz_nbits = eccc->blksz_nbits; 
+			int prime = eccc->prime;
+			op = eccc->op;
+			/* ecc starts from 64 bits*/
+			if(prime)
+				type =  (blksz_nbits / 64) - 1 + NLM_CRYPTO_ECCPRIME_TYPE_SVALUE;
+			else
+				type =  (blksz_nbits / 64) - 1 + NLM_CRYPTO_ECCBIN_TYPE_SVALUE;
+			if((blksz_nbits % 64) != 0) 
+				return NLM_CRYPTO_ERROR;
+			result = ((struct nlm_crypto_ecc_param *)param)->result;
+			break;
+		}
+		case NLM_CRYPTO_PKT:
+		{
+			int pktdescsize = 32 + nsegs * 16;
+			struct nlm_crypto_pkt_ctrl *pktctrl = ctrl;
+#ifdef NLM_CRYPTO_DEBUG_EN
+			struct nlm_crypto_pkt_param *pktparam = param;
+#endif
+			dstvc_base = iparam.sae_crypto_vc_base;
+			dstvc_lmt = iparam.sae_crypto_vc_limit;
+
+			nlm_dbg_print("Do op cdesc %016llx pdesc %016llx %016llx %016llx %016llx\n",
+				pktctrl->desc0, pktparam->desc0, pktparam->desc1, pktparam->desc2,pktparam->desc3);
+			nlm_dbg_print("Do op pseg %016llx %016llx\n", pktparam->segment[0][0],
+					pktparam->segment[0][1]);
+			msg0 = nlm_crypto_form_pkt_fmn_entry0(fbvc, 0, 0, pktctrl->cipherkeylen, 
+					crypto_virt_to_phys(ctrl));
+			msg1 = nlm_crypto_form_pkt_fmn_entry1(0, pktctrl->hashkeylen, pktdescsize, 
+					crypto_virt_to_phys(param));
+			etype = 1;
+			break;
+		}
+		default:
+			return NLM_CRYPTO_ERROR;
+	}
+
+	if(optype == NLM_CRYPTO_RSA || optype == NLM_CRYPTO_ECC) {
+		struct nlm_crypto_rsa_param *rsa = param;
+		struct nlm_crypto_rsa_result *rsar = result;
+		if(!rsar) {
+			nlm_err_print("Error : Result pointer is not specified\n");
+			return NLM_CRYPTO_ERROR;
+		}
+		dstvc_base = iparam.sae_rsa_vc_base;
+		dstvc_lmt = iparam.sae_rsa_vc_limit;
+		msg0 = nlm_crypto_form_rsa_ecc_fmn_entry0(1, type, op, crypto_virt_to_phys(rsa->modexp.x));
+		msg1 = nlm_crypto_form_rsa_ecc_fmn_entry1(0, 1, fbvc, crypto_virt_to_phys(rsar->r));
+		etype = 0;
+	}
+
+	nlm_dbg_print("Do op : cpu %d msg0 = %016llx msg1 %016llx rx_vc %d dst %d\n", cpu,
+				(unsigned long long)msg0, (unsigned long long)msg1,
+				ctx->rx_vc, dstvc_base);
+
+	if(ctx->mode == NLM_CRYPTO_MODE_SYNC_EXLVC) {
+		nlm_dbg_print("Mode is exclusive\n");
+
+		crypto_lock(&crypto_sync_lock[cpu]);
+		ctx->rsp_pend++;
+		xlp_message_send_block_fast_3(0, dstvc_base, msg0, msg1, crypto_virt_to_phys(ctx));
+cont_rcv:
+		while(xlp_message_receive_2(ctx->rx_vc, &src, &size, &code, &msg0, &msg1) != 0);
+
+		if((src < dstvc_base) || (src > dstvc_lmt)) {
+			nlm_err_print("Error : Msg recved from unknown src %d , continuing the loop\n", src);
+			goto cont_rcv;
+		}
+		ctx->rsp_pend--;
+		crypto_unlock(&crypto_sync_lock[cpu]);
+
+		if(SAE_ERROR(etype, msg0, msg1)) {
+			nlm_err_print("Error : Message rcv msg0 %llx msg1 %llx src %d err %x \n", 
+				(unsigned long long)msg0, (unsigned long long)msg1, src, 
+				(int)SAE_ERROR(etype, msg0, msg1));
+			return NLM_CRYPTO_ERROR;
+		}
+		nlm_dbg_print("Msg rcvd msg0 %llx msg1 %llx\n", (unsigned long long)msg0, (unsigned long long)msg1);
+
+		/* Result has been already copied by the engine to the result pointer passed, 
+		   no need to do anything */
+
+	} 
+#ifdef NLM_CRYPTO_LINUX_U
+	else if(ctx->mode == NLM_CRYPTO_MODE_SYNC_SHDVC) {
+		struct pollfd pfd[1];
+		int rv;
+		uint64_t cnt;
+		nlm_dbg_print("Mode is shared-vc\n");
+		
+		crypto_lock((cryptolock_t *)&ctx->lock);
+		ctx->msgs[0].msg[0] = 0ULL;
+		ctx->msgs[0].msg[1] = 0ULL;
+		xlp_message_send_block_fast_3(0, dstvc_base, msg0, msg1, crypto_virt_to_phys(ctx));
+		ctx->rsp_pend++;
+
+		pfd[0].fd = ctx->fd;
+		pfd[0].events = POLLIN;
+		pfd[0].revents = 0;
+cont_poll:
+		if((rv = poll(pfd, 1, -1)) < 0) {
+			if(errno == EINTR)
+				goto cont_poll;
+			crypto_unlock((cryptolock_t *)&ctx->lock);
+		       	return NLM_CRYPTO_ERROR;
+		}
+
+		if(rv == 0)
+			goto cont_poll;
+
+		if((rv = read(pfd[0].fd, &cnt, sizeof(cnt))) < 0) {
+			if(errno == EINTR)
+				goto cont_poll;
+			crypto_unlock((cryptolock_t *)&ctx->lock);
+			return NLM_CRYPTO_ERROR;;
+		}
+		
+		msg0 = ctx->msgs[0].msg[0];
+		msg1 = ctx->msgs[0].msg[1];
+		if((msg0 == 0ULL) && (msg1 == 0ULL))
+			goto cont_poll;
+
+		ctx->rsp_pend--;
+		crypto_unlock((cryptolock_t *)&ctx->lock);
+
+		if(SAE_ERROR(etype, msg0, msg1)) {
+			nlm_err_print("Error : Message rcv msg0 %llx msg1 %llx err %x \n", 
+				(unsigned long long)msg0, (unsigned long long)msg1, 
+				(int)SAE_ERROR(etype, msg0, msg1));
+			return NLM_CRYPTO_ERROR;
+		}
+	}
+#endif	
+	else {
+		int rv = 0;
+		nlm_dbg_print("Mode is async\n");
+
+		crypto_lock((cryptolock_t *)&ctx->lock);
+		if((rv = crypto_async_push_msg(ctx)) < 0) {
+			crypto_unlock((cryptolock_t *)&ctx->lock);
+			return rv;
+		}
+		crypto_unlock((cryptolock_t *)&ctx->lock);
+		/* save the param and result */
+		ctx->msgs[rv].msg[2] = (uint64_t)ctrl;
+		ctx->msgs[rv].msg[3] = (uint64_t)param;
+		ctx->msgs[rv].msg[4] = (uint64_t)arg;
+
+		xlp_message_send_block_fast_3(0, dstvc_base, msg0, msg1, (crypto_virt_to_phys(ctx) 
+				| (uint64_t)rv << NLM_CRYPTO_ASYNC_MSG_OUT_INDEX));
+	}
+
+	return NLM_CRYPTO_OK;
+}
+
+/**
+* @brief The crypto recv op result is relevent only in async model, where it retuns a positive value 
+* if there is a pending response. 
+* This will keep the ordering the requests and responses. Returns 1, only if the head of the 
+* pending request completes, and 0 if the head of the request is pending (even if the subsequent 
+* requests are completed
+*
+* @param[in] ctxt Pointer to the ctx structure returned by the open session. 
+*
+* @param[out] param Pointer to the param structure. 
+* @param[out] result Pointer to the result structure. 
+* @param[out] arg Pointer to the application specific arg
+*
+* @return 
+*  - Returns 1 if there is a pending response, 0 otherwise . 
+*	  
+* @ingroup crypto
+*/
+int nlm_crypto_rcv_op_result(nlm_crypto_ctx_t *ctxt, void **ctrl, void **param, void **arg)
+{
+	struct nlm_crypto_ctx *ctx = ctxt;
+	int rv;
+	
+	crypto_lock((cryptolock_t *)&ctx->lock);
+	rv = crypto_async_pop_msg(ctx, ctrl, param, arg);
+	crypto_unlock((cryptolock_t *)&ctx->lock);
+	return rv;
+}
+
+/**
+* @brief The crypto async callback is used only in netos mode. Here the caller should 
+* receive the message, and call this api if the message is from the security engine. 
+* This fucntion internally calls the aync callback registerd for this session.
+*
+* @param[in] crypto/rsa type
+* @param[in] msg0 Entry0 received from sae. 
+* @param[in] msg1 Entry1 received from sae. 
+*
+* @return 
+*  - Returns the callback function return value.
+*  - NLM_CRYPTO_ERROR if there is no callback handler registerd
+*	  
+* @ingroup crypto
+*/
+int nlm_crypto_aync_callback(enum nlm_crypto_op_type_t type, unsigned long long msg0, unsigned long long msg1)
+{
+	void *ctrl, *param, *arg;
+	int instance;
+	struct nlm_crypto_ctx *ctx;
+	int (*func)(nlm_crypto_ctx_t *, void *, void *, void *);
+
+	if(type == NLM_CRYPTO_PKT) {
+		instance = msg0 >> NLM_CRYPTO_ASYNC_MSG_OUT_INDEX;
+		ctx = crypto_phys_to_virt(msg0 & 0xffffffffffULL);
+	} else {
+		instance = msg1 >> NLM_CRYPTO_ASYNC_MSG_OUT_INDEX;
+		ctx = crypto_phys_to_virt(msg1 & 0xffffffffffULL);
+	}
+
+	ctrl = (void *)ctx->msgs[instance].msg[2];
+	param = (void *)ctx->msgs[instance].msg[3];
+	arg = (void *)ctx->msgs[instance].msg[4];
+
+	func = (void *)ctx->async_callback;
+
+	if(func)
+		return func(ctx, ctrl, param, arg);
+
+	return NLM_CRYPTO_ERROR;
+}
+
+/**
+* @brief The crypto malign function allocates memory from the shared pool for the given alignment
+*
+* @param[in] align alignment of the allocated memory
+* @param[in] size  size of the memory to be allocated
+*
+* @return 
+*  - On sucess,  returns pointer to the allocated memory. 
+*  - On failure, returns NULL.
+*	  
+* @ingroup crypto
+*/
+void *nlm_crypto_malign(unsigned int align, unsigned long long size)
+{
+	return crypto_malign(align, size);
+}
+
+/**
+* @brief The crypto mfree function frees the given shared memroy pointer
+*
+* @param[in] ptr pointer to the allocated memory
+*
+* @return 
+*	  
+* @ingroup crypto
+*/
+void nlm_crypto_mfree(void *ptr)
+{
+	crypto_mfree(ptr);
+}
+
+/**
+* @brief The crypto pkt control alloc function allocates memory for the control descriptor
+*
+* @param[in] ctxt 
+*
+* @return 
+*  - On sucess,  returns pointer to the control descriptor memory. 
+*  - On failure, returns NULL.
+*	  
+* @ingroup crypto
+*/
+struct nlm_crypto_pkt_ctrl *nlm_crypto_pkt_ctrl_alloc(nlm_crypto_ctx_t *ctxt)
+{
+	 return crypto_malign(XLP_CACHELINE_SIZE, sizeof(struct nlm_crypto_pkt_ctrl));
+}
+
+/**
+* @brief The crypto pkt param alloc function allocates memory for the packet descriptor
+*
+* @param[in] ctxt Pointer to the ctx structure returned by the open session
+* @param[in] nsegs Number of segments
+*
+* @return 
+*  - On sucess,  returns pointer to the packet descriptor memory. 
+*  - On failure, returns NULL.
+*	  
+* @ingroup crypto
+*/
+struct nlm_crypto_pkt_param *nlm_crypto_pkt_param_alloc(nlm_crypto_ctx_t *ctxt, unsigned int nsegs)
+{
+	void *ptr = crypto_malign(XLP_CACHELINE_SIZE, (sizeof(struct nlm_crypto_pkt_param) + (16 * nsegs)));
+	/* memset requires as the source and dst segments can be unequal but 
+	we copy only the valid ones */
+	memset(ptr, 0, sizeof(struct nlm_crypto_pkt_param) + (16 * nsegs));
+	return ptr;
+}
+
+/**
+* @brief The crypto control function free the control memory. 
+*
+* @param[in] ctxt Pointer to the ctx structure returned by the open session. 
+* @param[in] ctrl Pointer to the control memory
+*
+* @return 
+*	  
+* @ingroup crypto
+*/
+void nlm_crypto_pkt_ctrl_free(nlm_crypto_ctx_t *ctxt, void *ctrl)
+{
+	crypto_mfree(ctrl);
+	return;
+}
+
+/**
+* @brief The vc get function returns the sync/asynvc configured for the crypto library
+*
+* @param[out] rx_vc Asnyc/shared vc used. 
+* @param[out] rx_sync_vc Sync vc used
+*
+* @return 
+*	  
+* @ingroup crypto
+*/
+void nlm_crypto_get_configured_vc(int *rx_vc, int *rx_sync_vc)
+{
+	if(rx_vc)
+		*rx_vc = iparam.sae_rx_vc;
+	if(rx_sync_vc)
+		*rx_sync_vc = iparam.sae_rx_sync_vc;
+	return;
+}
+
+/**
+* @brief The sae vc get function returns the vc base and limit of the engine
+*
+* @param[in] type crypto/rsa type. 
+* @param[out] base_vc base vc of the engine
+* @param[out] limit_vc limit vc of the engine
+*
+* @return 
+*	  
+* @ingroup crypto
+*/
+void nlm_crypto_get_engine_vc(enum nlm_crypto_op_type_t type, int *base_vc, int *limit_vc)
+{
+	if(type == NLM_CRYPTO_PKT) {
+		*base_vc  = iparam.sae_crypto_vc_base;
+		*limit_vc = iparam.sae_crypto_vc_limit;
+	} else {
+		*base_vc  = iparam.sae_rsa_vc_base;
+		*limit_vc = iparam.sae_rsa_vc_limit;
+	}
+	return;
+}
diff --git a/drivers/crypto/sae/cryptodev.h b/drivers/crypto/sae/cryptodev.h
new file mode 100644
index 0000000..c7eec0f
--- /dev/null
+++ b/drivers/crypto/sae/cryptodev.h
@@ -0,0 +1,198 @@
+/*********************************************************************
+
+  Copyright 2003-2010 Netlogic Microsystem, Inc. ("Netlogic"). All rights
+  reserved.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+  1. Redistributions of source code must retain the above copyright
+  notice, this list of conditions and the following disclaimer.
+  2. Redistributions in binary form must reproduce the above copyright
+  notice, this list of conditions and the following disclaimer in
+  the documentation and/or other materials provided with the
+  distribution.
+
+  THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems, Inc. ``AS IS'' AND
+  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL RMI OR CONTRIBUTORS BE LIABLE
+  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+  INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+  CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+  THE POSSIBILITY OF SUCH DAMAGE.
+
+  *****************************#NLM_2#**********************************/
+#ifndef _NLM_CRYPTO_DEV_H
+#define _NLM_CRYPTO_DEV_H
+
+#define NLM_CRYPTO_DEV_NAME "nlmcrypto"
+
+enum crypto_ioctl_events { 
+		NLM_CRYPTO_GET_EVENTFD_CTXT = 1, NLM_CRYPTO_PUT_EVENTFD_CTXT,
+		NLM_CRYPTO_GET_RX_VC_NUMS, NLM_CRYPTO_GET_COMMON_SHM_ADDR_SZ,
+		NLM_CRYPTO_SHMMEM_ALLOC, NLM_CRYPTO_SHMMEM_FREE,
+	       NLM_CRYPTO_GET_SAE_VC_NUMS
+};
+
+#ifndef XLP_CACHELINE_SIZE
+#define XLP_CACHELINE_SIZE 64
+#endif 
+
+/* type value start and end */
+#define NLM_CRYPTO_ECCPRIME_TYPE_SVALUE 	0x0
+#define NLM_CRYPTO_ECCPRIME_TYPE_EVALUE 	0x8
+
+#define NLM_CRYPTO_ECCBIN_TYPE_SVALUE 		0x20
+#define NLM_CRYPTO_ECCBIN_TYPE_EVALUE 		0x28
+
+#define NLM_CRYPTO_RSA_TYPE_SVALUE 		0x40
+#define NLM_CRYPTO_RSA_TYPE_EVALUE 		0x44
+
+/* In async when max_msgs > 1, the message send id is specified in 40th bit onwards
+ int the msg3 */
+#define NLM_CRYPTO_ASYNC_MSG_OUT_INDEX 40
+
+/* Can be used by the kernel/netos for storing some data */
+#define NLM_CRYPOT_CTX_PRIV_DATA_SZ    64
+
+/* 0 and 1 are return msg, 
+   2 contains param vaddr, 3 contain result address, used only in async mode */
+struct nlm_crypto_msg {
+	volatile uint64_t msg[5]; 
+};
+
+struct nlm_crypto_ctx {
+	/* This should present in the top, don't change it. 
+	   see get_ctx_fd function */
+	int fd; 
+	unsigned short mode;
+	unsigned short rx_vc;
+
+	/* This should present in the uint64_t + 1 position, don't change it. 
+	   see get_ctx_arg function */
+	uint64_t arg; 
+	
+	unsigned short cpu;
+	unsigned short max_msgs;
+	unsigned int pad;
+
+	unsigned int rsp_pend; /* number of pending responses */
+	unsigned int lock; /* */
+	unsigned int mhead; /* head and tail for async operation */
+	unsigned int mtail;
+
+	/* Can be used by the kernel/netos for storing some data,
+	   cannot be accessed by the application  */
+	unsigned char priv_data[NLM_CRYPOT_CTX_PRIV_DATA_SZ];
+
+	uint64_t fdctxt;
+	uint64_t async_callback;
+	struct nlm_crypto_msg msgs[1];
+};
+
+typedef struct {
+	volatile unsigned int lock;
+} cryptolock_t;
+
+static __inline__ void crypto_lock(cryptolock_t *lock)
+{
+	unsigned int tmp, pid;
+#ifdef NLM_HAL_LINUX_USER
+	pid = getpid();
+#else
+	pid = 1;
+#endif
+
+	__asm__ __volatile__(
+			".set\tnoreorder\t\t\t# crypto_lock\n"
+			"1:\tll\t%1, %2\n\t"
+			"bgtz\t%1, 1b\n\t"
+			"move\t%1, %3\n\t"
+			"sc\t%1, %0\n\t"
+			"beqz\t%1, 1b\n\t"
+			" sync\n\t"
+			".set\treorder"
+			: "=m" (lock->lock), "=&r" (tmp)
+			: "m" (lock->lock), "r" (pid)
+			: "memory");
+}
+
+static __inline__ void crypto_unlock(cryptolock_t *lock)
+{
+	__asm__ __volatile__(
+			".set\tnoreorder\t\t\t# crypto_unlock\n\t"
+			"sync\n\t"
+			"sw\t$0, %0\n\t"
+			".set\treorder"
+			: "=m" (lock->lock)
+			: "m" (lock->lock)
+			: "memory");
+}
+
+/* cleared bit indication, instruction scans from msb to lsb 
+ get_flc(0) = 0, get_flc(1) = 1, get_flc(0x80000000) = 32*/
+static inline int get_flc(uint32_t x)
+{
+	__asm__(".set push	\n"
+		".set mips32	\n"
+		"clz %0, %1	\n"
+		".set pop	\n"
+		:"=r" (x) 
+		:"r" (x));
+	return 32 - x;
+}
+
+/* cleared bit indication, instruction scans from msb to lsb 
+ get_flc(0) = 0, get_flc(1) = 1, */
+static inline int get_flc64(uint64_t x)
+{
+	__asm__(".set push	\n"
+		".set mips64	\n"
+		"dclz %0, %1	\n"
+		".set pop	\n"
+		: "=r" (x) 
+		: "r" (x));
+	return 64 - x;
+}
+
+/* This should be provided by the application/os/hyperexec */
+struct nlm_crypto_init_params 
+{
+	int sae_rx_vc; /* async vc */
+	int sae_rx_sync_vc;
+	int sae_crypto_vc_base;
+	int sae_crypto_vc_limit;
+	int sae_rsa_vc_base;
+	int sae_rsa_vc_limit;
+	int pad[2];
+	unsigned long pcpu_sync_lock_ptr; /* if multiple threads per cpu is enabled */
+};
+
+extern int crypto_app_lib_init(struct nlm_crypto_init_params *);
+
+extern int contig_memory_init(void *shmaddr, unsigned long long paddr, unsigned long long size);
+/* alignment */
+#define   crypto_align(x, y)     ((x) & (~((y)-1)))
+#define   crypto_roundup(x, y)   (crypto_align((x)+(y)-1, (y)))
+
+#define  fifo_full(t, nentries, h) (((t+1) % nentries) == h)
+#define  fifo_empty(t, h) (t == h)
+
+#ifdef NLM_CRYPTO_LINUX_U
+/* Linux specific calls, no need to provide by others */
+extern int app_set_affinity(int cpu);
+extern uint64_t crypto_get_eventfd_ctxt(uint64_t ctxtpaddr);
+extern int crypto_put_eventfd_ctxt(uint64_t ctxtpaddr);
+#else
+static inline int app_set_affinity(int cpu) { return 0; }
+static inline uint64_t crypto_get_eventfd_ctxt(uint64_t ctxtpaddr) { return 0; }
+static inline int crypto_put_eventfd_ctxt(uint64_t ctxtpaddr) { return 0; }
+#endif //NLM_CRYPTO_LINUX_U
+
+
+#endif
diff --git a/drivers/crypto/sae/nlmcrypto.h b/drivers/crypto/sae/nlmcrypto.h
new file mode 100644
index 0000000..916c479
--- /dev/null
+++ b/drivers/crypto/sae/nlmcrypto.h
@@ -0,0 +1,670 @@
+/*********************************************************************
+
+  Copyright 2003-2010 Netlogic Microsystem, Inc. ("Netlogic"). All rights
+  reserved.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+  1. Redistributions of source code must retain the above copyright
+  notice, this list of conditions and the following disclaimer.
+  2. Redistributions in binary form must reproduce the above copyright
+  notice, this list of conditions and the following disclaimer in
+  the documentation and/or other materials provided with the
+  distribution.
+
+  THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems, Inc. ``AS IS'' AND
+  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL RMI OR CONTRIBUTORS BE LIABLE
+  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+  INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+  CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+  THE POSSIBILITY OF SUCH DAMAGE.
+
+  *****************************#NLM_2#**********************************/
+#ifndef _NLM_CRYPTO_H
+#define _NLM_CRYPTO_H
+
+#include "nlm_hal_crypto.h"
+
+/**
+* @file_name nlmcrypto.h
+*/
+
+/**
+* @defgroup crypto  Crypto structures
+* @brief Description about the crypto structures and enums used. Also descritption about
+* the higher level apis which calls the hal crypto apis internally.
+*/
+
+/**
+* @brief Pointer to the ctx structure
+* @ingroup crypto
+*/
+typedef void nlm_crypto_ctx_t;
+
+/**
+* @brief Get the fd value associated with the context
+* @ingroup crypto
+*/
+#define nlm_crypto_get_ctxt_fd(ctxt) (*(int *)ctxt)
+
+/**
+* @brief Get the application arg associated with the context
+* @ingroup crypto
+*/
+#define nlm_crypto_get_ctxt_arg(ctxt) (*((unsigned long long *)ctxt + 1))
+
+#include "nlmcrypto_ifc.h"
+/**
+* @brief Crypto return values
+* @ingroup crypto
+*/
+enum nlm_crypto_err { NLM_CRYPTO_OK = 0, NLM_CRYPTO_ERROR = -1, NLM_CRYPTO_EBUSY = -2 };
+
+/**
+* @brief Crypto modes
+* @ingroup crypto
+*/
+enum nlm_crypto_mode { NLM_CRYPTO_MODE_ASYNC = 1, NLM_CRYPTO_MODE_SYNC_EXLVC, NLM_CRYPTO_MODE_SYNC_SHDVC };
+
+/* Max value of this is 64, as we are using a uint64_t 
+   to find out the empty or nonemty status */
+#define NLM_CRYPTO_MAX_OUT_REQS 64
+
+/* seglen is 16 bits */
+#define NLM_CRYPTO_MAX_SEG_LEN (64 * 1024) 
+
+#ifndef NLM_HAL_LINUX_KERNEL
+#define nlm_err_print(fmt, args...)  { \
+	        fprintf(stderr, fmt, ##args);   \
+	        fflush(stderr); \
+}
+#else
+#define nlm_err_print(fmt, args...)  { printk(fmt, ##args); }
+#endif
+#ifdef NLM_CRYPTO_DEBUG_EN
+#define nlm_dbg_print(fmt, args...)  fprintf(stderr, fmt, ##args);
+#else
+#define nlm_dbg_print(fmt, args...)  { }
+#endif
+
+#define crypto_swap64(x)   ( (unsigned long long)((x & 0x000000FF) << 56) | \
+		(unsigned long long)((x & 0x0000FF00) << 40) | \
+		(unsigned long long)((x & 0x00FF0000) << 24) | \
+		(unsigned long long)((x & 0xFF000000) <<  8) | \
+		((x & (unsigned long long)0xFF << 32) >>  8) | \
+		((x & (unsigned long long)0xFF << 40) >> 24) | \
+		((x & (unsigned long long)0xFF << 48) >> 40) | \
+		((x & (unsigned long long)0xFF << 56) >> 56)   \
+		)
+
+#if !defined(__MIPSEL__) && !defined(__MIPSEB__)
+	$(error ENDIANESS is not specified(__MIPSEL__ , __MIPSEB__))
+#endif
+
+#ifdef __MIPSEL__
+#define ccpu_to_be64(x) crypto_swap64(x)
+#else
+#define ccpu_to_be64(x) (x)
+#endif
+
+
+enum nlm_crypto_op_type_t { NLM_CRYPTO_RSA, NLM_CRYPTO_ECC, NLM_CRYPTO_PKT };
+
+/* RSA definitions */
+/* c = x^y mod n or c = x * y mod n */
+struct nlm_crypto_mod_exp { 
+	unsigned char	*x;
+	unsigned char	*y;
+	unsigned char	*n;
+};
+ 
+typedef struct nlm_crypto_mod_exp nlm_crypto_mod_exp_t;
+typedef struct nlm_crypto_mod_exp nlm_crypto_mod_mul_t;
+typedef struct nlm_crypto_mod_exp nlm_crypto_mod_add_t;
+typedef struct nlm_crypto_mod_exp nlm_crypto_mod_sub_t;
+typedef struct nlm_crypto_mod_exp nlm_crypto_mod_div_t;
+
+struct nlm_crypto_rsa_result
+{
+	unsigned char *r;
+};
+typedef struct nlm_crypto_rsa_result nlm_crypto_rsa_result_t;
+typedef struct nlm_crypto_rsa_result nlm_crypto_field_result_t;
+#define NLM_CRYPTO_RSA_RESULT_NELMNTS 1
+
+/* the values are same as of the encoding of fn field */
+enum nlm_crypto_rsa_op_t { NLM_CRYPTO_RSA_MOD_EXP = 0, NLM_CRYPTO_RSA_MOD_MUL, NLM_CRYPTO_RSA_MAX_OPS };
+
+/* The first 3 elements are same for rsa and ecc, do op can just typecast anyone of these
+ so don't change it */
+struct nlm_crypto_rsa_param   {
+	union {
+		nlm_crypto_mod_exp_t modexp;
+		nlm_crypto_mod_mul_t modmul;
+	};
+	/* result need to be allocated seperately and point it before doing do_op */
+	nlm_crypto_rsa_result_t *result;
+};
+
+#define NLM_CRYPTO_RSA_PARAMS_NELMNTS 3 /* x, y and n */
+
+struct nlm_crypto_rsa_ctrl  {
+	int op;
+	int blksz_nbits;
+};
+
+/* ECC defnitions , the values are same as of the encoding of fn field */
+enum nlm_crypto_ecc_op_t { 
+	NLM_CRYPTO_ECC_POINT_MUL = 0, NLM_CRYPTO_ECC_POINT_ADD, 
+	NLM_CRYPTO_ECC_POINT_DBL, NLM_CRYPTO_ECC_POINT_VERIFY, 
+	NLM_CRYPTO_ECC_FIELD_MOD_ADD, NLM_CRYPTO_ECC_FIELD_MOD_SUB,
+	NLM_CRYPTO_ECC_FIELD_MOD_MUL, NLM_CRYPTO_ECC_FIELD_MOD_DIV,
+	NLM_CRYPTO_ECC_FIELD_MOD_INV, NLM_CRYPTO_ECC_FIELD_MOD_RED,
+	NLM_CRYPTO_ECC_MAX_OPS
+};
+
+/* R(x, y) = k * P(x, y) , a(curve parameter), n(modulus)*/
+struct nlm_crypto_ecc_point_mul {  
+	unsigned char		*x;
+	unsigned char 		*y;
+	unsigned char		*a;
+	unsigned char		*k;
+	unsigned char	 	*n;
+};
+typedef struct nlm_crypto_ecc_point_mul nlm_crypto_ecc_point_mul_t;
+
+/* R(x, y) = P(x, y)  + Q(x, y), a(curve parameter), n(modulus)*/
+struct nlm_crypto_ecc_point_add {  
+	unsigned char		*xp;
+	unsigned char 		*yp;
+	unsigned char		*xq;
+	unsigned char 		*yq;
+	unsigned char		*a;
+	unsigned char	 	*n;
+};
+typedef struct nlm_crypto_ecc_point_add nlm_crypto_ecc_point_add_t;
+
+/* R(x, y) = 2 * P(x, y) , a(curve parameter), n(modulus)*/
+struct nlm_crypto_ecc_point_dbl {  
+	unsigned char		*x;
+	unsigned char 		*y;
+	unsigned char		*a;
+	unsigned char	 	*n;
+};
+typedef struct nlm_crypto_ecc_point_dbl nlm_crypto_ecc_point_dbl_t;
+
+/* P(x, y) , a(curve parameter), b(curve parameter), n(modulus)*/
+struct nlm_crypto_ecc_point_verify {  
+	unsigned char		*x;
+	unsigned char 		*y;
+	unsigned char		*a;
+	unsigned char	 	*b;
+	unsigned char	 	*n;
+};
+typedef struct nlm_crypto_ecc_point_verify nlm_crypto_ecc_point_verify_t;
+
+/* Modular inversion c = 1/x mod n , modular reduction c = x mod n*/
+struct nlm_crypto_mod_inv { 
+	unsigned char	*x;
+	unsigned char	*n;
+};
+typedef struct nlm_crypto_mod_inv nlm_crypto_mod_inv_t;
+typedef struct nlm_crypto_mod_inv nlm_crypto_mod_red_t;
+
+struct nlm_crypto_ecc_point_result 
+{
+	unsigned char *rx;
+	unsigned char *ry;
+};
+typedef struct nlm_crypto_ecc_point_result nlm_crypto_ecc_point_result_t;
+#define NLM_CRYPTO_ECC_RESULT_NELMNTS 2
+
+struct nlm_crypto_ecc_result {
+	union {
+		nlm_crypto_field_result_t 		fres;
+		nlm_crypto_ecc_point_result_t 		pres;
+	};
+};
+typedef struct nlm_crypto_ecc_result nlm_crypto_ecc_result_t;
+
+/* The first 3 elements are same for rsa and ecc, do op can just typecast anyone of these
+ so don't change it */
+struct  nlm_crypto_ecc_param {
+	union {
+		nlm_crypto_ecc_point_mul_t    	pmul;
+		nlm_crypto_ecc_point_add_t    	padd;
+		nlm_crypto_ecc_point_dbl_t    	pdbl;
+		nlm_crypto_ecc_point_verify_t	pverify;
+		nlm_crypto_mod_add_t		       	fadd;
+		nlm_crypto_mod_sub_t		       	fsub;
+		nlm_crypto_mod_mul_t		       	fmul;
+		nlm_crypto_mod_div_t		       	fdiv;
+		nlm_crypto_mod_inv_t		       	finv;
+		nlm_crypto_mod_red_t		       	fred;
+	};
+	/* result needs to be allocated seperately and point it here before doint do op*/
+	nlm_crypto_ecc_result_t 	*result;
+};
+
+struct nlm_crypto_ecc_ctrl  {
+	int op;
+	int blksz_nbits;
+	int prime; /* 1 for prime curvers, 0 for binary */
+};
+
+#define NLM_CRYPTO_ECC_MAX_BLK_SIZE 576
+#define NLM_CRYPTO_ECC_PARAMS_NELMNTS 6
+
+#ifndef NLM_ENCRYPT
+#define NLM_ENCRYPT 1
+#endif
+
+#ifndef NLM_DECRYPT
+#define NLM_DECRYPT 0
+#endif
+
+/**
+* @brief cipher algorithms
+* @ingroup crypto
+*/
+enum nlm_cipher_algo {
+	NLM_CIPHER_BYPASS = 0,
+	NLM_CIPHER_DES = 1,
+	NLM_CIPHER_3DES = 2,     
+	NLM_CIPHER_AES128 = 3,
+	NLM_CIPHER_AES192 = 4,
+	NLM_CIPHER_AES256 = 5, 	
+	NLM_CIPHER_ARC4 = 6,     
+	NLM_CIPHER_KASUMI_F8 = 7,
+	NLM_CIPHER_SNOW3G_F8 = 8,     
+	NLM_CIPHER_CAMELLIA128 = 9, 
+	NLM_CIPHER_CAMELLIA192 = 0xA, 
+	NLM_CIPHER_CAMELLIA256 = 0xB, 
+	NLM_CIPHER_MAX = 0xC,
+};
+
+/**
+* @brief cipher modes
+* @ingroup crypto
+*/
+enum nlm_cipher_mode {
+	NLM_CIPHER_MODE_ECB = 0,
+	NLM_CIPHER_MODE_CBC = 1,
+	NLM_CIPHER_MODE_CFB = 2,
+	NLM_CIPHER_MODE_OFB = 3,
+	NLM_CIPHER_MODE_CTR = 4,
+	NLM_CIPHER_MODE_AES_F8 = 5,
+	NLM_CIPHER_MODE_GCM = 6,
+	NLM_CIPHER_MODE_CCM = 7,
+	NLM_CIPHER_MODE_UNDEFINED1 = 8,
+	NLM_CIPHER_MODE_UNDEFINED2 = 9,
+	NLM_CIPHER_MODE_LRW = 0xA,
+	NLM_CIPHER_MODE_XTS = 0xB,
+	NLM_CIPHER_MODE_MAX = 0xC,
+};
+
+/**
+* @brief hash algorithms
+* @ingroup crypto
+*/
+enum nlm_hash_algo {
+	NLM_HASH_BYPASS = 0,
+	NLM_HASH_MD5 = 1,
+	NLM_HASH_SHA = 2,
+	NLM_HASH_UNDEFINED = 3,
+	NLM_HASH_AES128 = 4,
+	NLM_HASH_AES192 = 5,
+	NLM_HASH_AES256 = 6,
+	NLM_HASH_KASUMI_F9 = 7,
+	NLM_HASH_SNOW3G_F9 = 8,
+	NLM_HASH_CAMELLIA128 = 9,
+	NLM_HASH_CAMELLIA192 = 0xA,
+	NLM_HASH_CAMELLIA256 = 0xB,
+	NLM_HASH_GHASH = 0xC,
+	NLM_HASH_MAX = 0xD
+};
+
+/**
+* @brief hash modes
+* @ingroup crypto
+*/
+enum nlm_hash_mode {
+	NLM_HASH_MODE_SHA1 = 0, 	/* Only SHA */
+	NLM_HASH_MODE_SHA224 = 1,       /* Only SHA */
+	NLM_HASH_MODE_SHA256 = 2,       /* Only SHA */
+	NLM_HASH_MODE_SHA384 = 3,       /* Only SHA */
+	NLM_HASH_MODE_SHA512 = 4,       /* Only SHA */
+	NLM_HASH_MODE_CMAC = 5, 	/* AES and Camellia */
+	NLM_HASH_MODE_XCBC = 6, 	/* AES and Camellia */
+	NLM_HASH_MODE_CBC_MAC = 7,      /* AES and Camellia */
+	NLM_HASH_MODE_CCM = 8,  	/* AES */
+	NLM_HASH_MODE_GCM = 9,  	/* AES */
+	NLM_HASH_MODE_MAX = 0xA,
+}; 
+
+#define MAX_KEY_LEN_IN_DW 20
+/**
+* @brief crypto control descriptor, should be cache aligned
+* @ingroup crypto
+*/
+struct nlm_crypto_pkt_ctrl {
+	unsigned long long desc0;
+	unsigned long long key[MAX_KEY_LEN_IN_DW]; /* combination of cipher and hash keys */
+	unsigned int cipherkeylen; 
+	unsigned int hashkeylen; 
+	unsigned int taglen;
+};
+
+/**
+* @brief crypto packet descriptor, should be cache aligned  
+* @ingroup crypto
+*/
+struct nlm_crypto_pkt_param {
+	unsigned long long desc0;
+ 	unsigned long long desc1;
+	unsigned long long desc2;
+	unsigned long long desc3;
+	unsigned long long segment[1][2];
+};
+
+
+static inline int nlm_crypto_getnibble(unsigned char a) 
+{ 
+	if (a >= 'a' && a <= 'f')
+		return a - 'a' + 10;
+	if (a >= 'A' && a <= 'F')
+		return a - 'A' + 10;
+	return a - '0';
+}
+
+static inline void nlm_crypto_hex2bin(unsigned char *dst, unsigned char *src, int len)
+{
+	int i;
+
+	for (i = 0; i < len * 2; i = i + 2)
+		dst[i/2] = (nlm_crypto_getnibble(src[i]) << 4 ) | (nlm_crypto_getnibble(src[i + 1])) ;
+	return;
+}
+
+#define nlm_crypto_num_segs_reqd(bufsize) ((bufsize + NLM_CRYPTO_MAX_SEG_LEN - 1) / NLM_CRYPTO_MAX_SEG_LEN)
+#define nlm_crypto_desc_size(nsegs) (32 + (nsegs * 16))
+
+static inline int nlm_crypto_get_taglen(enum nlm_hash_algo hashalg, enum nlm_hash_mode hashmode)
+{
+	if(hashalg == NLM_HASH_MD5)
+		return 128;
+	else if(hashalg == NLM_HASH_SHA) {
+		switch(hashmode) {
+			case NLM_HASH_MODE_SHA1 : 
+				return 160;
+			case  NLM_HASH_MODE_SHA224 : 
+				return 224;
+			case NLM_HASH_MODE_SHA256 : 
+				return 256;
+			case NLM_HASH_MODE_SHA384 : 
+				return 384;
+			case NLM_HASH_MODE_SHA512 : 
+				return 512;
+			default:
+				nlm_err_print("Error : invalid shaid (%s)\n", __FUNCTION__);
+				return -1;
+		}
+	} else if (hashalg == NLM_HASH_SNOW3G_F9)  
+		return 32;
+	else if(hashmode == NLM_HASH_MODE_XCBC)
+		return 128;
+	else if(hashalg == NLM_HASH_BYPASS)
+		return 0;
+	else
+		nlm_err_print("Error : Hashalg not found, Tag length is setting to zero\n");
+
+	/* TODO : Add remining cases */
+	return 0; 
+}
+
+/**
+* @brief Generate fill cryto control info structure
+* @ingroup crypto
+* - hmac : 1 for hash with hmac 
+* - hashalg: see above,  hash_alg enums
+* - hashmode: see above, hash_mode enums
+* - cipherhalg: see above,  cipher_alg enums
+* - ciphermode: see above, cipher_mode enums
+* - keys_instr: 1 if the keys are specified in ascii values and it needs to be converted to binary form
+*/
+static inline int nlm_crypto_fill_pkt_ctrl(struct nlm_crypto_pkt_ctrl *ctrl, 
+		unsigned int hmac, 
+		enum nlm_hash_algo hashalg, enum nlm_hash_mode hashmode,
+		enum nlm_cipher_algo cipheralg, enum nlm_cipher_mode ciphermode,
+		unsigned int keys_instr,
+		unsigned char *cipherkey, unsigned int cipherkeylen, 
+		unsigned char *hashkey,   unsigned int hashkeylen)
+{
+	int taglen;
+	ctrl->desc0 = ccpu_to_be64(nlm_crypto_form_pkt_ctrl_desc(hmac, hashalg, hashmode, 
+			cipheralg, ciphermode, 0, 0, 0));
+	memset((char *)ctrl->key, 0, sizeof(ctrl->key));
+	if(cipherkey) {
+		if(keys_instr)
+			nlm_crypto_hex2bin((unsigned char *)ctrl->key, cipherkey, cipherkeylen);
+		else
+			memcpy((unsigned char *)ctrl->key, cipherkey, cipherkeylen);
+	}
+	if(hashkey) {
+		if(keys_instr)
+			nlm_crypto_hex2bin((unsigned char *)&ctrl->key[(cipherkeylen + 7) / 8],  hashkey, hashkeylen);
+		else
+			memcpy((unsigned char *)&ctrl->key[(cipherkeylen + 7) / 8],  hashkey, hashkeylen);
+	}
+	ctrl->cipherkeylen = cipherkeylen;
+	ctrl->hashkeylen = hashkeylen;
+	if((taglen = nlm_crypto_get_taglen(hashalg, hashmode)) < 0)
+		return -1;
+	ctrl->taglen = taglen;
+	
+	/* TODO : add the invalid checks and return error */
+	return 0;
+}
+
+/**
+* @brief Top level function for generation pkt desc 0 to 3 for cipher auth
+* @ingroup crypto
+* - ctrl : pointer to control structure
+* - param : pointer to the param structure
+* - encrypt : 1(for encrypt) 0(for decrypt)
+* - hash_source : 1(encrypted data is sent to the auth engine) 0(plain data is sent to the auth engine)
+* - ivoff : iv offset from start of data
+* - ivlen : iv length in bytes
+* - hashoff : hash offset from start of data
+* - hashlen : hash length in bytes
+* - hmacpad : hmac padding required or not, 1 if already padded
+* - cipheroff : cipher offset from start of data
+* - cipherlen : cipher length in bytes
+* - hashdst_addr : hash destination physical address
+*/
+static inline void nlm_crypto_fill_cipher_auth_pkt_param(
+		struct nlm_crypto_pkt_ctrl *ctrl, struct nlm_crypto_pkt_param *param,
+		unsigned int encrypt, unsigned int hash_source,
+		unsigned int ivoff, unsigned int ivlen, 
+		unsigned int hashoff, unsigned int hashlen, unsigned int hmacpad,
+		unsigned int cipheroff, unsigned int chiperlen,
+		unsigned char *hashdst_addr)
+{
+	param->desc0 = ccpu_to_be64(nlm_crypto_form_pkt_desc0(0, hash_source, 1, encrypt, ivlen, crypto_virt_to_phys(hashdst_addr)));
+	param->desc1 = ccpu_to_be64(nlm_crypto_form_pkt_desc1(chiperlen, hashlen));
+	param->desc2 = ccpu_to_be64(nlm_crypto_form_pkt_desc2(ivoff, 0, cipheroff, 0, 0, hashoff));
+	param->desc3 = ccpu_to_be64(nlm_crypto_form_pkt_desc3(0, ctrl->taglen, 0, 0, hmacpad));
+	return;
+}
+
+/**
+* @brief Top level function for generation pkt desc 0 to 3 for cipher operation
+* @ingroup crypto
+* - ctrl : pointer to control structure
+* - param : pointer to the param structure
+* - encrypt : 1(for encrypt) 0(for decrypt)
+* - ivoff : iv offset from start of data
+* - ivlen : iv length in bytes
+* - cipheroff : cipher offset from start of data
+* - cipherlen : cipher length in bytes
+*/
+
+		
+static inline void nlm_crypto_fill_cipher_pkt_param(
+		struct nlm_crypto_pkt_ctrl *ctrl, struct nlm_crypto_pkt_param *param,
+		unsigned int encrypt, 
+		unsigned int ivoff, unsigned int ivlen, 
+		unsigned int cipheroff, unsigned int chiperlen )
+{
+	param->desc0 = ccpu_to_be64(nlm_crypto_form_pkt_desc0(0, 0, 0, encrypt, ivlen, 0ULL));
+	param->desc1 = ccpu_to_be64(nlm_crypto_form_pkt_desc1(chiperlen, 1));
+	param->desc2 = ccpu_to_be64(nlm_crypto_form_pkt_desc2(ivoff, 0, cipheroff, 0, 0, 0));
+	param->desc3 = ccpu_to_be64(nlm_crypto_form_pkt_desc3(0, ctrl->taglen, 0, 0, 0));
+
+	return;
+}
+
+/**
+* @brief Top level function for generation pkt desc 0 to 3 for auth operation
+* @ingroup crypto
+* - ctrl : pointer to control structure
+* - param : pointer to the param structure
+* - hashoff : hash offset from start of data
+* - hashlen : hash length in bytes
+* - hmacpad : hmac padding required or not, 1 if already padded
+* - hashdst_addr : hash destination physical address
+*/
+static inline void nlm_crypto_fill_auth_pkt_param(
+		struct nlm_crypto_pkt_ctrl *ctrl, struct nlm_crypto_pkt_param *param,
+		unsigned int hashoff, unsigned int hashlen, unsigned int hmacpad,
+		unsigned char *hashdst_addr)
+{
+
+	param->desc0 = ccpu_to_be64(nlm_crypto_form_pkt_desc0(0, 0, 1, 0, 1, crypto_virt_to_phys(hashdst_addr)));
+	param->desc1 = ccpu_to_be64(nlm_crypto_form_pkt_desc1(1, hashlen));
+	param->desc2 = ccpu_to_be64(nlm_crypto_form_pkt_desc2(0, 0, 0, 0, 0, hashoff));
+	param->desc3 = ccpu_to_be64(nlm_crypto_form_pkt_desc3(0, ctrl->taglen, 0, 0, hmacpad));
+
+
+	return;
+}
+
+/**
+* @brief Top level function for generating packet desc4 from source segments
+* @ingroup crypto
+* - param : pointer to the param structure
+* - seg : starting segment
+* - input : segment start address
+* - inlen : segment length
+*/
+static inline unsigned int nlm_crypto_fill_src_seg(struct nlm_crypto_pkt_param *param,  
+		int seg, unsigned char *input, unsigned int inlen)
+{
+	unsigned off = 0, len = 0;
+	unsigned int remlen = inlen;
+
+	for(; remlen > 0;) {
+		len = remlen > NLM_CRYPTO_MAX_SEG_LEN ? NLM_CRYPTO_MAX_SEG_LEN : remlen;
+
+		param->segment[seg][0] = ccpu_to_be64(nlm_crypto_form_pkt_desc4(len,  crypto_virt_to_phys((input + off))));
+		remlen -= len;
+		off += len;
+		seg++;
+	}
+	return seg;
+}
+
+/**
+* @brief Top level function for generating packet desc5 from cipher destination segments
+* @ingroup crypto
+* - param : pointer to the param structure
+* - seg : starting segment
+* - output : segment start address
+* - outlen : segment length
+*/
+static inline unsigned int nlm_crypto_fill_dst_seg(struct nlm_crypto_pkt_param *param, 
+		int seg, unsigned char *output, unsigned int outlen)
+{
+	unsigned off = 0, len = 0;
+	unsigned int remlen = outlen;
+
+	for(; remlen > 0;) {
+		len = remlen > NLM_CRYPTO_MAX_SEG_LEN ? NLM_CRYPTO_MAX_SEG_LEN : remlen;
+		param->segment[seg][1] = ccpu_to_be64(nlm_crypto_form_pkt_desc5(len, 1, 0, crypto_virt_to_phys(output + off)));
+		remlen -= len;
+		off += len;
+		seg++;
+	}
+	return seg;
+}
+
+#ifndef MAX_CPUS
+#define MAX_CPUS		128
+#endif
+static inline int my_cpu_id(void)
+{
+	unsigned int pid = 0;
+
+	__asm__ volatile (".set push\n"
+			".set noreorder\n"
+			".set arch=xlp\n"
+			"rdhwr %0, $0\n"
+			".set pop\n"
+			: "=r" (pid)
+			:);
+
+	return pid;
+}
+
+#ifndef XLP_CACHELINE_SIZE
+#define XLP_CACHELINE_SIZE 64
+#endif 
+
+#ifndef NR_VCS_PER_THREAD
+#define NR_VCS_PER_THREAD 4
+#endif
+
+#define RSA_ERROR(x) ((x >> 53) & 0x1f)
+#define RSA_ENGINE(x) ( x >> 60)
+
+#define CRYPTO_ERROR(msg1) ((unsigned int)msg1)
+
+#define SAE_ERROR(etype, msg0, msg1) (etype == 1 ? CRYPTO_ERROR(msg1) : RSA_ERROR(msg0))
+
+extern unsigned long xlp_rsa_base;
+extern unsigned long xlp_sae_base;
+int nlm_crypto_lib_init(void);
+nlm_crypto_ctx_t *nlm_crypto_open_sync_session(int sync_mode, int cpu, void *arg);
+nlm_crypto_ctx_t *nlm_crypto_open_async_session(int max_outstanding_reqs, 
+		int (*callback)(nlm_crypto_ctx_t *ctxt, void *ctrl, void *param, void *arg), void *arg);
+int nlm_crypto_close_session(nlm_crypto_ctx_t *ctxt);
+void nlm_crypto_reset_session(nlm_crypto_ctx_t *ctxt);
+struct nlm_crypto_rsa_param *nlm_crypto_rsa_param_alloc(nlm_crypto_ctx_t *ctxt, enum nlm_crypto_rsa_op_t op, 
+		int blksz_nbits);
+struct  nlm_crypto_ecc_param *nlm_crypto_ecc_param_alloc(nlm_crypto_ctx_t *ctxt, 
+		enum nlm_crypto_ecc_op_t op, int blksz_nbits);
+void nlm_crypto_param_free(nlm_crypto_ctx_t *ctxt, void *param);
+struct nlm_crypto_rsa_result *nlm_crypto_rsa_result_alloc(nlm_crypto_ctx_t *ctxt, int blksz_nbits);
+struct nlm_crypto_ecc_result *nlm_crypto_ecc_result_alloc(nlm_crypto_ctx_t *ctxt,  
+		enum nlm_crypto_ecc_op_t op, int blksz_nbits);
+void nlm_crypto_result_free(nlm_crypto_ctx_t *ctxt, void *result);
+int nlm_crypto_do_op(nlm_crypto_ctx_t *ctxt, enum nlm_crypto_op_type_t optype, void *ctrl, 
+		void *param, int nsegs, void *arg);
+int nlm_crypto_rcv_op_result(nlm_crypto_ctx_t *ctxt, void **ctrl, void **param, void **arg);
+int nlm_crypto_aync_callback(enum nlm_crypto_op_type_t type, unsigned long long msg0, unsigned long long msg1);
+void nlm_crypto_get_configured_vc(int *rx_vc, int *rx_sync_vc);
+struct nlm_crypto_pkt_ctrl *nlm_crypto_pkt_ctrl_alloc(nlm_crypto_ctx_t *ctxt);
+struct nlm_crypto_pkt_param *nlm_crypto_pkt_param_alloc(nlm_crypto_ctx_t *ctxt, unsigned int nsegs);
+void nlm_crypto_pkt_ctrl_free(nlm_crypto_ctx_t *ctxt, void *ctrl);
+void *nlm_crypto_malign(unsigned int align, unsigned long long size);
+void nlm_crypto_mfree(void *ptr);
+void nlm_crypto_get_engine_vc(enum nlm_crypto_op_type_t type, int *base_vc, int *limit_vc);
+
+#endif
-- 
1.7.0.4

