From 580f382cc6f5b60ec4fd9fa5a7bec5ad2500cf10 Mon Sep 17 00:00:00 2001
From: Yanjiang Jin <yanjiang.jin@windriver.com>
Date: Fri, 17 May 2013 10:48:46 +0800
Subject: [PATCH] bcm-xlp-rt: use raw_spin_(un)lock to replace spin_(un)lock

Use raw_spin_(un)lock to replace spin_(un)lock, else we got the below errors:

BUG: sleeping function called from invalid context at kernel/rtmutex.c:658
in_atomic(): 1, irqs_disabled(): 1, pid: 0, name: swapper/19
Call Trace:
[<ffffffffc17defa0>] dump_stack+0x1c/0x50
[<ffffffffc11314d8>] __might_sleep+0xf0/0x118
[<ffffffffc17ea890>] rt_spin_lock+0x38/0x100
[<ffffffffc10b7298>] xlp_interrupt_ack+0x40/0xa0
[<ffffffffc10b2124>] nlm_intx_ack+0x7c/0x98
[<ffffffffc11785e4>] handle_level_irq+0x44/0x180
[<ffffffffc117357c>] generic_handle_irq+0x54/0x88
[<ffffffffc17efb5c>] do_IRQ+0x2c/0x48
[<ffffffffc10b2770>] plat_irq_dispatch+0x208/0x3b8
[<ffffffffc10c0240>] ret_from_irq+0x0/0x4
[<ffffffffc10c0480>] r4k_wait+0x20/0x40
[<ffffffffc10c2a08>] cpu_idle+0x90/0xb8
[<ffffffffc1cccaf0>] start_secondary+0x3a0/0x3d4

Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 arch/mips/include/asm/netlogic/pic_hal.h |    2 +-
 arch/mips/netlogic/common/irq.c          |    8 ++--
 arch/mips/netlogic/common/pic_hal.c      |   10 ++--
 arch/mips/netlogic/xlp/irq.c             |    6 +-
 arch/mips/netlogic/xlp/msi.c             |   30 ++++++------
 arch/mips/netlogic/xlp/on_chip.c         |   24 +++++-----
 arch/mips/netlogic/xlp/pic/xlp_irq_map.c |   18 ++++----
 arch/mips/netlogic/xlp/pic/xlp_pic.c     |   32 ++++++------
 arch/mips/netlogic/xlp/xlp-cpufreq.c     |   12 ++--
 arch/mips/netlogic/xlp/xlp_gpio.c        |   12 ++--
 arch/mips/netlogic/xlp/xlp_srio.c        |   76 +++++++++++++++---------------
 11 files changed, 115 insertions(+), 115 deletions(-)

diff --git a/arch/mips/include/asm/netlogic/pic_hal.h b/arch/mips/include/asm/netlogic/pic_hal.h
index a10ba46..9e33207 100644
--- a/arch/mips/include/asm/netlogic/pic_hal.h
+++ b/arch/mips/include/asm/netlogic/pic_hal.h
@@ -38,7 +38,7 @@ struct pic_dev
 	char *name;	/* humar readable name */
 	int (*self_init)(struct pic_dev*);
 	int (*self_cleanup)(void);
-	spinlock_t pic_lock;
+	raw_spinlock_t pic_lock;
 	int max_irq_per_rvec;	/* max number of vectors sharing same rvec */
 	/* Platform specific IRQ setup calls */
 	int (*plat_request_irq)(struct pic_dev *, int, u64,
diff --git a/arch/mips/netlogic/common/irq.c b/arch/mips/netlogic/common/irq.c
index e52bfcb..07ea546 100644
--- a/arch/mips/netlogic/common/irq.c
+++ b/arch/mips/netlogic/common/irq.c
@@ -81,9 +81,9 @@ static void xlp_pic_enable(struct irq_data *d)
 	irt = nlm_irq_to_irt(d->irq);
 	if (irt == -1)
 		return;
-	spin_lock_irqsave(&nlm_pic_lock, flags);
+	raw_spin_lock_irqsave(&nlm_pic_lock, flags);
 	nlm_pic_enable_irt(nlm_pic_base, irt);
-	spin_unlock_irqrestore(&nlm_pic_lock, flags);
+	raw_spin_unlock_irqrestore(&nlm_pic_lock, flags);
 }
 
 static void xlp_pic_disable(struct irq_data *d)
@@ -94,9 +94,9 @@ static void xlp_pic_disable(struct irq_data *d)
 	irt = nlm_irq_to_irt(d->irq);
 	if (irt == -1)
 		return;
-	spin_lock_irqsave(&nlm_pic_lock, flags);
+	raw_spin_lock_irqsave(&nlm_pic_lock, flags);
 	nlm_pic_disable_irt(nlm_pic_base, irt);
-	spin_unlock_irqrestore(&nlm_pic_lock, flags);
+	raw_spin_unlock_irqrestore(&nlm_pic_lock, flags);
 }
 
 static void xlp_pic_mask_ack(struct irq_data *d)
diff --git a/arch/mips/netlogic/common/pic_hal.c b/arch/mips/netlogic/common/pic_hal.c
index ca2f8aa..8c6e93b 100644
--- a/arch/mips/netlogic/common/pic_hal.c
+++ b/arch/mips/netlogic/common/pic_hal.c
@@ -14,13 +14,13 @@ int register_pic_dev(u32 node, struct pic_dev *dev)
 	int ret = -EBUSY;
 
 	if (dev->self_init(dev) < 0) return -EFAULT;
-	spin_lock_irqsave(&gbl_pic_lock, flags);
+	raw_spin_lock_irqsave(&gbl_pic_lock, flags);
 	if (global_pic_dev[node] == NULL) {
 		global_pic_dev[node] = dev;
 		printk(KERN_DEBUG "pic_dev[%u] = 0x%p\n", node, dev);
 		ret = 0;
 	}
-	spin_unlock_irqrestore(&gbl_pic_lock, flags);
+	raw_spin_unlock_irqrestore(&gbl_pic_lock, flags);
 	return ret;
 }
 
@@ -35,18 +35,18 @@ int unregister_pic_dev(u32 node, struct pic_dev *dev)
 	int ret = -EINVAL;
 
 	if (dev->self_cleanup() < 0) return -EFAULT;
-	spin_lock_irqsave(&gbl_pic_lock, flags);
+	raw_spin_lock_irqsave(&gbl_pic_lock, flags);
 	if (global_pic_dev[node] == dev) {
 		global_pic_dev[node] = 0;
 		ret = 0;
 	}
-	spin_unlock_irqrestore(&gbl_pic_lock, flags);
+	raw_spin_unlock_irqrestore(&gbl_pic_lock, flags);
 	return ret;
 }
 
 /*
  * fetches node specific pic from the global registered set of pic devs"
- * Since there is no unregistration, spin_lock holding is not necessary
+ * Since there is no unregistration, raw_spin_lock holding is not necessary
  */
 int retrieve_node_pic_dev(u32 node, struct pic_dev **in)
 {
diff --git a/arch/mips/netlogic/xlp/irq.c b/arch/mips/netlogic/xlp/irq.c
index 6b02ad7..a12d586 100644
--- a/arch/mips/netlogic/xlp/irq.c
+++ b/arch/mips/netlogic/xlp/irq.c
@@ -89,7 +89,7 @@ void nlm_oprofile_interrupt(struct pt_regs *regs, int irq)
  */
 static volatile uint64_t xlp_irq_mask;
 
-/* spin lock for all interrupt related data structures
+/* raw_spin lock for all interrupt related data structures
  * This variable is used in timer init, so we export it
  */
 
@@ -116,7 +116,7 @@ int check_intx_range(struct pic_dev *pic, u32 oirq)
 
 /*
  * Interface function (unlocked version) to mask an IRQ
- * Calls helper function after input tests and spin_lock holds
+ * Calls helper function after input tests and raw_spin_lock holds
  *
  * @irq : IRQ number
  */
@@ -139,7 +139,7 @@ static void nlm_intx_mask(struct irq_data *data)
 
 /*
  * Interface function (unlocked version) to mask an IRQ
- * Calls helper function after input tests and spin_lock holds
+ * Calls helper function after input tests and raw_spin_lock holds
  *
  * @irq : IRQ number
  */
diff --git a/arch/mips/netlogic/xlp/msi.c b/arch/mips/netlogic/xlp/msi.c
index a9e881f..db3da8c 100644
--- a/arch/mips/netlogic/xlp/msi.c
+++ b/arch/mips/netlogic/xlp/msi.c
@@ -144,7 +144,7 @@ int xlp_setup_msi_irq(struct pci_dev *dev, int nvec)
 		return -EFAULT;
 	}
 	base_msi = XLP_MSI_IRQ_START(nfn.node, nfn.fn);
-	spin_lock_irqsave(&xlp_msi_lock, flags);
+	raw_spin_lock_irqsave(&xlp_msi_lock, flags);
 	ret = xlp_get_ctrl_intmode(nfn.node, nfn.fn);
 	if ((ret == XLP_INTMODE_MSIX ) || (ret == XLP_INTMODE_INTX)) {
 		ret = -EBUSY;
@@ -197,7 +197,7 @@ setup_fail:
 	msi_vec[nfn.node][nfn.fn].bitmap = old_bitmap;
 	msi_vec[nfn.node][nfn.fn].count = old_count;
 setup_end:
-	spin_unlock_irqrestore(&xlp_msi_lock, flags);
+	raw_spin_unlock_irqrestore(&xlp_msi_lock, flags);
 	return ret;
 }
 
@@ -226,7 +226,7 @@ int xlp_setup_msix_irq(struct pci_dev *dev, int nvec)
 		return -EFAULT;
 	}
 	base_msix = XLP_MSIX_IRQ_START(nfn.node, nfn.fn);
-	spin_lock_irqsave(&xlp_msi_lock, flags);
+	raw_spin_lock_irqsave(&xlp_msi_lock, flags);
 	old_bitmap = msix_vec[nfn.node][nfn.fn].bitmap;
 	old_count = msix_vec[nfn.node][nfn.fn].count;
 	old_mode = xlp_get_ctrl_intmode(nfn.node, nfn.fn);
@@ -270,14 +270,14 @@ int xlp_setup_msix_irq(struct pci_dev *dev, int nvec)
 		idx++;
 	}
 	setup_xlpep_msi_access(dev, XLP_INTMODE_MSIX, 0);
-	spin_unlock_irqrestore(&xlp_msi_lock, flags);
+	raw_spin_unlock_irqrestore(&xlp_msi_lock, flags);
 	return 0;
 fail_loop:
 	msi_vec[nfn.node][nfn.fn].bitmap = old_bitmap;
 	msi_vec[nfn.node][nfn.fn].count = old_count;
 	xlp_set_ctrl_intmode(nfn.node, nfn.fn, old_mode);
 setup_end:
-	spin_unlock_irqrestore(&xlp_msi_lock, flags);
+	raw_spin_unlock_irqrestore(&xlp_msi_lock, flags);
 	return ret;
 }
 
@@ -423,7 +423,7 @@ static u32 nlm_msi_change_mask(struct pic_dev *pic, unsigned int msi, int val)
 	BUG_ON(pic->node != XLP_MSI_TO_NODE(msi));
 	fn = XLP_MSI_TO_CTRL_FN(msi);
 	bit = msi - XLP_MSI_IRQ_START(pic->node, fn);
-	spin_lock_irqsave(&xlp_msi_lock, flags);
+	raw_spin_lock_irqsave(&xlp_msi_lock, flags);
 	mask = xlp_msi_set_mask(pic->node, fn, bit, val);
 	if (val == 0) {
 		if (mask == 0) { /* This was the last bit to clear */
@@ -433,7 +433,7 @@ static u32 nlm_msi_change_mask(struct pic_dev *pic, unsigned int msi, int val)
 			__xlp_irq_unmask(pic, xlp_msi_to_irq(msi));
 		}
 	}
-	spin_unlock_irqrestore(&xlp_msi_lock, flags);
+	raw_spin_unlock_irqrestore(&xlp_msi_lock, flags);
 	return mask;
 }
 
@@ -555,10 +555,10 @@ static void nlm_msix_mask(struct irq_data *data)
 	if (check_msix_range(msix) < 0) {
 		return;
 	}
-	spin_lock_irqsave(&xlp_msi_lock, flags);
+	raw_spin_lock_irqsave(&xlp_msi_lock, flags);
 	mask_msi_irq(data); /* This function masks MSI-X -- please note */
 	__xlp_irq_mask(pic, xlp_msix_to_irq(msix));
-	spin_unlock_irqrestore(&xlp_msi_lock, flags);
+	raw_spin_unlock_irqrestore(&xlp_msi_lock, flags);
 	return;
 }
 
@@ -576,10 +576,10 @@ static void nlm_msix_unmask(struct irq_data *data)
 	if (check_msix_range(msix) < 0) {
 		return;
 	}
-	spin_lock_irqsave(&xlp_msi_lock, flags);
+	raw_spin_lock_irqsave(&xlp_msi_lock, flags);
 	__xlp_irq_unmask(pic, xlp_msix_to_irq(msix));
 	unmask_msi_irq(data); /* Enable MSI-X -- please note */
-	spin_unlock_irqrestore(&xlp_msi_lock, flags);
+	raw_spin_unlock_irqrestore(&xlp_msi_lock, flags);
 	return;
 }
 
@@ -670,7 +670,7 @@ void arch_teardown_msi_irq(unsigned int msi)
 
 	switch (msi) {
 	case XLP_MSI_INDEX_START ... XLP_MSI_INDEX_END:
-		spin_lock_irqsave(&xlp_msi_lock, flags);
+		raw_spin_lock_irqsave(&xlp_msi_lock, flags);
 		fn = XLP_MSI_TO_CTRL_FN(msi);
 		bit = lmsi - XLP_MSI_IRQ_START(0, fn);
 		msi_vec[node][fn].count--;
@@ -679,19 +679,19 @@ void arch_teardown_msi_irq(unsigned int msi)
 			if (xlp_set_ctrl_intmode(node, fn, XLP_INTMODE_NONE) < 0){
 			}
 		}
-		spin_unlock_irqrestore(&xlp_msi_lock, flags);
+		raw_spin_unlock_irqrestore(&xlp_msi_lock, flags);
 		return;
 	case XLP_MSIX_INDEX_START ... XLP_MSIX_INDEX_END:
 		fn = XLP_MSIX_TO_CTRL_FN(msi);
 		bit = (msi % XLP_IRQS_PER_NODE) - XLP_MSIX_IRQ_START(0, fn);
-		spin_lock_irqsave(&xlp_msi_lock, flags);
+		raw_spin_lock_irqsave(&xlp_msi_lock, flags);
 		msix_vec[node][fn].count--;
 		msix_vec[node][fn].bitmap &= ~(1ULL << bit);
 		if (xlp_get_ctrl_intmode(node, fn) == XLP_INTMODE_MSIX) {
 			if (xlp_set_ctrl_intmode(node, fn, XLP_INTMODE_NONE) < 0){
 			}
 		}
-		spin_unlock_irqrestore(&xlp_msi_lock, flags);
+		raw_spin_unlock_irqrestore(&xlp_msi_lock, flags);
 		return;
 	default:
 		return;	/* Do not proceed if !(msi || msix) */
diff --git a/arch/mips/netlogic/xlp/on_chip.c b/arch/mips/netlogic/xlp/on_chip.c
index 8454bdc..1aa34b8 100644
--- a/arch/mips/netlogic/xlp/on_chip.c
+++ b/arch/mips/netlogic/xlp/on_chip.c
@@ -75,7 +75,7 @@ static intr_vchandler xlp_intr_vc_handler;
 unsigned int intr_vc_mask[NR_CPUS];
 
 /* make this a read/write spinlock */
-spinlock_t msgrng_lock;
+raw_spinlock_t msgrng_lock;
 static nlm_common_atomic_t msgring_registered;
 
 struct msgstn_handler {
@@ -350,9 +350,9 @@ int nlm_xlp_register_intr_vc(int cpu, int vc)
 	node = cpu / 32;
 	nlm_hal_enable_vc_intr(node, (cpu*4 + vc) & 0x7f);
 
-	spin_lock_irqsave(&msgrng_lock, flags);
+	raw_spin_lock_irqsave(&msgrng_lock, flags);
 	intr_vc_mask[cpu] |= (1 << vc);
-	spin_unlock_irqrestore(&msgrng_lock, flags);
+	raw_spin_unlock_irqrestore(&msgrng_lock, flags);
 	
 	/*printk("%s in, cpu %d intr_vc_mask %x\n", __FUNCTION__, cpu, intr_vc_mask[cpu]);*/
 	return 0;
@@ -368,9 +368,9 @@ int nlm_xlp_unregister_intr_vc(int cpu, int vc)
 		return -1;
 	}
 
-	spin_lock_irqsave(&msgrng_lock, flags);
+	raw_spin_lock_irqsave(&msgrng_lock, flags);
 	intr_vc_mask[cpu] &= (~(1 << vc));
-	spin_unlock_irqrestore(&msgrng_lock, flags);
+	raw_spin_unlock_irqrestore(&msgrng_lock, flags);
 	return 0;
 }
 EXPORT_SYMBOL(nlm_xlp_unregister_intr_vc);
@@ -555,7 +555,7 @@ int register_xlp_msgring_handler(int major,
 	}
 
 	/* Check if the message station is valid, if not return error */
-	spin_lock_irqsave(&msgrng_lock, flags);
+	raw_spin_lock_irqsave(&msgrng_lock, flags);
 
 	if(!xlp_fmn_init_done)
 		xlp_fmn_init_done = 1;
@@ -563,10 +563,10 @@ int register_xlp_msgring_handler(int major,
 	if (is_nlm_xlp8xx_ax()) {
 		if(msg_handler_timer_enabled == 0) {
 			msg_handler_timer_enabled = 1;
-			spin_unlock_irqrestore(&msgrng_lock, flags);
+			raw_spin_unlock_irqrestore(&msgrng_lock, flags);
 			// init_msg_bkp_timer(0);	Not required, taken care by on_each_cpu()
 			on_each_cpu(init_msg_bkp_timer, 0, 1);
-			spin_lock_irqsave(&msgrng_lock, flags);
+			raw_spin_lock_irqsave(&msgrng_lock, flags);
 		}
 	}
 
@@ -576,7 +576,7 @@ int register_xlp_msgring_handler(int major,
 	ret = 0;
 	msgring_registered.value = 1;
 
-	spin_unlock_irqrestore(&msgrng_lock, flags);
+	raw_spin_unlock_irqrestore(&msgrng_lock, flags);
 
 	return ret;
 }
@@ -593,14 +593,14 @@ int unregister_xlp_msgring_handler(int major, void *dev_id)
 		       XLP_MAX_TX_STNS);
 		return -1;
 	}
-	spin_lock_irqsave(&msgrng_lock, flags);
+	raw_spin_lock_irqsave(&msgrng_lock, flags);
 	if(msg_handler_map[major].dev_id == dev_id){
 		msg_handler_map[major].action = dummy_handler;
 		msg_handler_map[major].dev_id = NULL;
 		msg_handler_map[major].napi_final = NULL;
 		msg_handler_map[major].napi_final_arg = NULL;
 	}
-	spin_unlock_irqrestore(&msgrng_lock, flags);
+	raw_spin_unlock_irqrestore(&msgrng_lock, flags);
 	return 0;
 }
 
@@ -831,7 +831,7 @@ void on_chip_init(void)
 
 	/* Set netlogic_io_base to the run time value */
 #ifdef CONFIG_XLP_FMN_SUPPORT
-	spin_lock_init(&msgrng_lock);
+	raw_spin_lock_init(&msgrng_lock);
 
 	msgring_registered.value = 0;
 #endif
diff --git a/arch/mips/netlogic/xlp/pic/xlp_irq_map.c b/arch/mips/netlogic/xlp/pic/xlp_irq_map.c
index 1d2a964..ee7d07e 100644
--- a/arch/mips/netlogic/xlp/pic/xlp_irq_map.c
+++ b/arch/mips/netlogic/xlp/pic/xlp_irq_map.c
@@ -606,9 +606,9 @@ int xlp_get_irq_base_bitmap(struct pic_dev *pic, u32 rvec, u32 *irq, u64 *bitmap
 		return -EINVAL;
 	}
 	if (irq) *irq = lirq;
-	spin_lock_irqsave(&pic->pic_lock, flags);
+	raw_spin_lock_irqsave(&pic->pic_lock, flags);
 	if (bitmap) *bitmap = xlp_rvec_map[rvec].bitmap[pic->node];
-	spin_unlock_irqrestore(&pic->pic_lock, flags);
+	raw_spin_unlock_irqrestore(&pic->pic_lock, flags);
 	return 0;
 }
 
@@ -701,7 +701,7 @@ void __xlp_irq_unmask(struct pic_dev *pic, int irq)
 		return;
 	} else if (((1ULL << rvec) & read_64bit_cp0_eimr()) == 0) {
 		/* This is only for those interrupts which are not statically
-		 * set in EIMR. Could dump stack if spin lock held */
+		 * set in EIMR. Could dump stack if raw_spin lock held */
 		 on_each_cpu(xlp_set_eimr, (void *)(uintptr_t) (rvec), 1);
 	}
 	return;
@@ -725,7 +725,7 @@ int xlp_irq_startup(struct pic_dev *pic, unsigned int oirq)
 
 	cpumask_clear(&m);
 	cpumask_set_cpu(NLM_MAX_CPU_PER_NODE * pic->node, &m);
-	spin_lock_irqsave(&xlp_irq_lock, flags);
+	raw_spin_lock_irqsave(&xlp_irq_lock, flags);
 	if (xlp_irq_map[nirq].usage[pic->node] == 0) {
 		/* Currently unused => not enabled. So, setup and enable */
 		ret = pic->plat_request_irq(pic, nirq, UPIC_DTYPE_ITE, &m,NULL);
@@ -753,7 +753,7 @@ int xlp_irq_startup(struct pic_dev *pic, unsigned int oirq)
 		ret = -EFAULT;
 	}
 __failure:
-	spin_unlock_irqrestore(&xlp_irq_lock, flags);
+	raw_spin_unlock_irqrestore(&xlp_irq_lock, flags);
 	return ret;
 }
 
@@ -772,9 +772,9 @@ void xlp_irq_shutdown(struct pic_dev *pic, unsigned int irq)
 	unsigned long flags;
 	int rvec;
 
-	spin_lock_irqsave(&xlp_irq_lock, flags);
+	raw_spin_lock_irqsave(&xlp_irq_lock, flags);
 	if (xlp_irq_map[irq].usage[pic->node] == 0) {
-		spin_unlock_irqrestore(&xlp_irq_lock, flags);
+		raw_spin_unlock_irqrestore(&xlp_irq_lock, flags);
 		return;
 	} else if (xlp_irq_map[irq].usage[pic->node] > 0) {
 		xlp_irq_map[irq].usage[pic->node]--;
@@ -786,10 +786,10 @@ void xlp_irq_shutdown(struct pic_dev *pic, unsigned int irq)
 	 * free up the rvec */
 	if (xlp_irq_map[irq].usage[pic->node] == 0) {
 		pic->plat_release_irq(pic, irq, UPIC_DTYPE_ITE);
-		spin_unlock_irqrestore(&xlp_irq_lock, flags);
+		raw_spin_unlock_irqrestore(&xlp_irq_lock, flags);
 		__xlp_irq_mask(pic, irq); /* masks this IRQ */
 	} else {
-		spin_unlock_irqrestore(&xlp_irq_lock, flags);
+		raw_spin_unlock_irqrestore(&xlp_irq_lock, flags);
 	}
 	return;
 }
diff --git a/arch/mips/netlogic/xlp/pic/xlp_pic.c b/arch/mips/netlogic/xlp/pic/xlp_pic.c
index b14d61e..674c6c0 100644
--- a/arch/mips/netlogic/xlp/pic/xlp_pic.c
+++ b/arch/mips/netlogic/xlp/pic/xlp_pic.c
@@ -59,11 +59,11 @@ static void xlp_ite_cpu_op(struct pic_dev *pic, u8 cpu, u8 ite, u8 bitval)
 	reg = (cpu < 64) ? XLP_PIC_INT_THREADEN01(ite) :
 					XLP_PIC_INT_THREADEN23(ite);
 	bit = cpu % 64;
-	spin_lock_irqsave(&pic->pic_lock, flags);
+	raw_spin_lock_irqsave(&pic->pic_lock, flags);
 	val = __pic_r64r(pic->base, reg);
 	val = (bitval == 0) ?  (val & ~(1ULL << bit)) : (val | (1ULL << bit));
 	__pic_w64r(pic->base, reg, val);
-	spin_unlock_irqrestore(&pic->pic_lock, flags);
+	raw_spin_unlock_irqrestore(&pic->pic_lock, flags);
 }
 
 #define xlp_ite_cpu_set(pic,cpu,ite) xlp_ite_cpu_op(pic,cpu,ite,1)
@@ -284,12 +284,12 @@ static int xlp_get_irq_affinity(struct pic_dev *pic, int irq, struct cpumask *m)
 	int ite = -1;
 	u64 val;
 
-	spin_lock_irqsave(&pic->pic_lock, flags);
+	raw_spin_lock_irqsave(&pic->pic_lock, flags);
 	val = __pic_r64r(pic->base, XLP_PIC_IRT_ENTRY(irt));
 	if ((val & XLP_IRTENT_DT) == 0) {
 		ite = (val >> 16) & 7;
 	}
-	spin_unlock_irqrestore(&pic->pic_lock, flags);
+	raw_spin_unlock_irqrestore(&pic->pic_lock, flags);
 	if (ite < 0) {
 		return -EFAULT;
 	}
@@ -336,7 +336,7 @@ static int xlp_set_irq_affinity(struct pic_dev *pic, int irt, u64 type,
 	}
 	ite = xlp_get_closest_mask(pic, in, out);
 	//dump_all_ites();
-	spin_lock_irqsave(&pic->pic_lock, flags);
+	raw_spin_lock_irqsave(&pic->pic_lock, flags);
 	val = __pic_r64r(pic->base, XLP_PIC_IRT_ENTRY(irt));
 	if (val & XLP_IRTENT_DT) {
 		ret = -ENODEV;
@@ -346,7 +346,7 @@ static int xlp_set_irq_affinity(struct pic_dev *pic, int irt, u64 type,
 	val |= XLP_IRTENT_DB(ite);
 	__pic_w64r(pic->base, XLP_PIC_IRT_ENTRY(irt), val);
 unsupported:
-	spin_unlock_irqrestore(&pic->pic_lock, flags);
+	raw_spin_unlock_irqrestore(&pic->pic_lock, flags);
 	return ret;
 }
 
@@ -376,14 +376,14 @@ static int xlp_request_irq(struct pic_dev *pic, int irq, u64 type,
 		return -EINVAL;
 	}
 	idx = xlp_get_closest_mask(pic, in, out);
-	spin_lock_irqsave(&pic->pic_lock, flags);
+	raw_spin_lock_irqsave(&pic->pic_lock, flags);
 	val = __pic_r64r(pic->base, XLP_PIC_IRT_ENTRY(irt));
 	val |= (XLP_IRTENT_ENABLE | XLP_IRTENT_SCH_LCL | XLP_IRTENT_RVEC(rvec)
 			| XLP_IRTENT_DB(idx));
-	/* Do book keeping now, with spin lock held */
+	/* Do book keeping now, with raw_spin lock held */
 	__xlp_modify_irq_bitmap(pic, irq, UPIC_PARAM_SETBIT);
 	__pic_w64r(pic->base, XLP_PIC_IRT_ENTRY(irt), val);
-	spin_unlock_irqrestore(&pic->pic_lock, flags);
+	raw_spin_unlock_irqrestore(&pic->pic_lock, flags);
 	return 0;
 }
 
@@ -393,12 +393,12 @@ static int xlp_release_irq(struct pic_dev *pic, int irq, u64 type)
 	int irt = xlp_irq_to_irt(irq);
 	unsigned long flags;
 
-	spin_lock_irqsave(&pic->pic_lock, flags);
+	raw_spin_lock_irqsave(&pic->pic_lock, flags);
 	__xlp_modify_irq_bitmap(pic, irq, UPIC_PARAM_CLEARBIT);
 	val = __pic_r64r(pic->base, XLP_PIC_IRT_ENTRY(irt));
 	val &= ~XLP_IRTENT_ENABLE;
 	__pic_w64r(pic->base, XLP_PIC_IRT_ENTRY(irt), val);
-	spin_unlock_irqrestore(&pic->pic_lock, flags);
+	raw_spin_unlock_irqrestore(&pic->pic_lock, flags);
 	return 0;
 }
 
@@ -408,7 +408,7 @@ int xlp_pic_self_init(struct pic_dev *this)
 	u64 val;
 
 	/* Initializes PIC to a logical state before registering */
-	spin_lock_init(&this->pic_lock);
+	raw_spin_lock_init(&this->pic_lock);
 
 	/* We set the following in IRT entry
 	 * 28 : clear to indicate global delivery
@@ -429,14 +429,14 @@ int xlp_interrupt_pending(struct pic_dev *pic, void *data, u64 type)
 	struct xlp_ip *ip = (struct xlp_ip *)data;
 	unsigned long flags;
 
-	spin_lock_irqsave(&pic->pic_lock, flags);
+	raw_spin_lock_irqsave(&pic->pic_lock, flags);
 	if (ip->valid & XLP_INTPENDING_V0)
 		ip->ip[0] = __pic_r64r(pic->base, XLP_PIC_INT_PENDING(0));
 	if (ip->valid & XLP_INTPENDING_V1)
 		ip->ip[1] = __pic_r64r(pic->base, XLP_PIC_INT_PENDING(1));
 	if (ip->valid & XLP_INTPENDING_V2)
 		ip->ip[2] = __pic_r64r(pic->base, XLP_PIC_INT_PENDING(2));
-	spin_unlock_irqrestore(&pic->pic_lock, flags);
+	raw_spin_unlock_irqrestore(&pic->pic_lock, flags);
 	return UPIC_SUCCESS;
 }
 
@@ -444,7 +444,7 @@ int xlp_interrupt_ack(struct pic_dev *pic, u32 irt, u64 unused)
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&pic->pic_lock, flags);
+	raw_spin_lock_irqsave(&pic->pic_lock, flags);
 	/* Order of ack-ing pic_status and pic_int_ack is very important
 	 * Otherwise you might end up double the interrupt rate */
 	if (irt < 12) { /* Ack status register for WD and Sys.Timers */
@@ -455,7 +455,7 @@ int xlp_interrupt_ack(struct pic_dev *pic, u32 irt, u64 unused)
 	 * Please make sure this is the case while interrupt thread enable
 	 * registers (0x94 onwards) */
 	__pic_w64r(pic->base, XLP_PIC_INT_ACK, irt);
-	spin_unlock_irqrestore(&pic->pic_lock, flags);
+	raw_spin_unlock_irqrestore(&pic->pic_lock, flags);
 	return UPIC_SUCCESS;
 }
 
diff --git a/arch/mips/netlogic/xlp/xlp-cpufreq.c b/arch/mips/netlogic/xlp/xlp-cpufreq.c
index 47c2b48..8cf6db6 100644
--- a/arch/mips/netlogic/xlp/xlp-cpufreq.c
+++ b/arch/mips/netlogic/xlp/xlp-cpufreq.c
@@ -82,14 +82,14 @@ static void setup_affected_cpus(struct cpumask map)
 	cpumask_set_cpu(2, &thr);
 	cpumask_set_cpu(3, &thr);
 
-	spin_lock_irqsave(&freq_lock, flags);
+	raw_spin_lock_irqsave(&freq_lock, flags);
 	for (idx = 0; idx < NR_CPUS; idx++) {
 		cpumask_and(&xlp_affected_cpus[idx], &thr, &map);
 		if (((idx + 1) % XLP_THREADS_PER_CORE) == 0) {
 			cpumask_shift_left(&thr, &thr, XLP_THREADS_PER_CORE);
 		}
 	}
-	spin_unlock_irqrestore(&freq_lock, flags);
+	raw_spin_unlock_irqrestore(&freq_lock, flags);
 	return;
 }
 
@@ -130,14 +130,14 @@ static int xlp_freq_set(struct cpufreq_frequency_table *from,
 	unsigned long flags;
 	int i;
 
-	spin_lock_irqsave(&freq_lock, flags);
+	raw_spin_lock_irqsave(&freq_lock, flags);
 	for (i = from->index; i != to->index; ){
 		fdebug("i = %d\n", i);
 		change_cpu_freq(cpu, dec);
 		/* To decrement frequency, INC index */
 		i = (dec == 1) ? (i + 1) : (i - 1);
 	}
-	spin_unlock_irqrestore(&freq_lock, flags);
+	raw_spin_unlock_irqrestore(&freq_lock, flags);
 	return 0;
 }
 static int xlp_cpufreq_set_target(struct cpufreq_policy *policy,
@@ -205,9 +205,9 @@ __init static int xlp_cpufreq_driver_init(struct cpufreq_policy *policy)
 	}
 	/* Pick a conservative guess in ns: */
 	policy->cpuinfo.transition_latency = 2 * 1000 * 1000;
-	spin_lock_irqsave(&freq_lock, flags);
+	raw_spin_lock_irqsave(&freq_lock, flags);
 	cpumask_copy(policy->cpus, &xlp_affected_cpus[policy->cpu]);
-	spin_unlock_irqrestore(&freq_lock, flags);
+	raw_spin_unlock_irqrestore(&freq_lock, flags);
 	ret = cpufreq_frequency_table_cpuinfo(policy, xlp_freq_table);
 	if (ret != 0) {
 		pr_err("cpufreq: Failed to configure frequency table: %d\n", ret);
diff --git a/arch/mips/netlogic/xlp/xlp_gpio.c b/arch/mips/netlogic/xlp/xlp_gpio.c
index 75a8976..1532d4d 100644
--- a/arch/mips/netlogic/xlp/xlp_gpio.c
+++ b/arch/mips/netlogic/xlp/xlp_gpio.c
@@ -67,7 +67,7 @@ static inline void xlp_gpio_set_value(int gpio, int v)
 	else
 		val = gpio_reg_read(0, XLP_GPIO_OUTPUT1);
 
-	spin_lock_irqsave(&xlp_gpio_lock, flags);
+	raw_spin_lock_irqsave(&xlp_gpio_lock, flags);
 
 	if(v) {
 
@@ -88,7 +88,7 @@ static inline void xlp_gpio_set_value(int gpio, int v)
 	else
 		gpio_reg_write(0, XLP_GPIO_OUTPUT1, val);
 
-	spin_unlock_irqrestore(&xlp_gpio_lock, flags);
+	raw_spin_unlock_irqrestore(&xlp_gpio_lock, flags);
 
 	return;
 }
@@ -116,7 +116,7 @@ static inline int xlp_gpio_direction_input(int gpio)
 	else
 		val = gpio_reg_read( 0, XLP_GPIO_OUTPUT_EN1);
 
-	spin_lock_irqsave(&xlp_gpio_lock, flags);
+	raw_spin_lock_irqsave(&xlp_gpio_lock, flags);
 	if(gpio < XLP_GPIO1_BASE)
 		val &= ~(1 << gpio);
 	else
@@ -126,7 +126,7 @@ static inline int xlp_gpio_direction_input(int gpio)
 		gpio_reg_write(0, XLP_GPIO_OUTPUT_EN0, val);
 	else
 		gpio_reg_write(0, XLP_GPIO_OUTPUT_EN1, val);
-	spin_unlock_irqrestore(&xlp_gpio_lock, flags);
+	raw_spin_unlock_irqrestore(&xlp_gpio_lock, flags);
         return 0;
 }
 
@@ -142,7 +142,7 @@ static inline int xlp_gpio_direction_output(int gpio, int v)
 	else
 		val = gpio_reg_read( 0, XLP_GPIO_OUTPUT_EN1);
 
-	spin_lock_irqsave(&xlp_gpio_lock, flags);
+	raw_spin_lock_irqsave(&xlp_gpio_lock, flags);
 	if(gpio < XLP_GPIO1_BASE) {
 			val |= (1 << gpio);
 
@@ -154,7 +154,7 @@ static inline int xlp_gpio_direction_output(int gpio, int v)
 		gpio_reg_write(0, XLP_GPIO_OUTPUT_EN0, val);
 	else
 		gpio_reg_write(0, XLP_GPIO_OUTPUT_EN1, val);
-	spin_unlock_irqrestore(&xlp_gpio_lock, flags);
+	raw_spin_unlock_irqrestore(&xlp_gpio_lock, flags);
 
         return 0;
 }
diff --git a/arch/mips/netlogic/xlp/xlp_srio.c b/arch/mips/netlogic/xlp/xlp_srio.c
index 2fa6b68..0e57ed1 100644
--- a/arch/mips/netlogic/xlp/xlp_srio.c
+++ b/arch/mips/netlogic/xlp/xlp_srio.c
@@ -209,16 +209,16 @@ static int bcm_xlp_rio_datamsg_resp_handler(struct bcm_rio_port *rio, uint32_t s
 
         dmsg_txq = &rio->dmsg_txq[mbox];
         atomic_dec(&dmsg_txq->letter);
-        spin_lock_irqsave(&dmsg_txq->dmsg_txlock, flags);
+        raw_spin_lock_irqsave(&dmsg_txq->dmsg_txlock, flags);
         if (dmsg_txq->used == 0) {
 	        //error
-                spin_unlock_irqrestore(&dmsg_txq->dmsg_txlock, flags);
+                raw_spin_unlock_irqrestore(&dmsg_txq->dmsg_txlock, flags);
 		return -1;
         }
 	slot = dmsg_txq->tail;
 	dmsg_txq->tail = (dmsg_txq->tail + 1) % dmsg_txq->max_entries;
 	dmsg_txq->used--;
-	spin_unlock_irqrestore(&dmsg_txq->dmsg_txlock, flags);
+	raw_spin_unlock_irqrestore(&dmsg_txq->dmsg_txlock, flags);
         mport->outb_msg[mbox].mcback(mport, dmsg_txq->dev_id, -1,
                                                 -1);
         return 0;
@@ -277,16 +277,16 @@ static int bcm_xlp_rio_rxreq_handler(struct bcm_rio_port *rio, uint32_t ftype, i
                         paddr = SRIO_PAYLOAD_ADDR(msg2);
 			rxmsgq = &rio->dmsg_rxq[mbox];
 
-			spin_lock_irqsave(&rxmsgq->dmsg_rxlock, flags);
+			raw_spin_lock_irqsave(&rxmsgq->dmsg_rxlock, flags);
 			if (atomic_read(&rio->enabled) == 0) {
-                                spin_unlock_irqrestore(&rxmsgq->dmsg_rxlock, flags);
+                                raw_spin_unlock_irqrestore(&rxmsgq->dmsg_rxlock, flags);
                                 return -ESRIO_API_INTERNAL;
                         }
 
 			if ((msg1 & SRIO_DATAMSG_TIMEOUT) || (rxmsgq->status != NLM_QVALID) ||
 				(rxmsgq->used == rxmsgq->max_entries)) {
 				//FIXME send the buffer back to address Q. should not come here. 
-				spin_unlock_irqrestore(&rxmsgq->dmsg_rxlock, flags);
+				raw_spin_unlock_irqrestore(&rxmsgq->dmsg_rxlock, flags);
                                 if (bcm_rio_send_free_rx_buff(paddr))
                                         return -ESRIO_API_INTERNAL;
                                 break;
@@ -299,7 +299,7 @@ static int bcm_xlp_rio_rxreq_handler(struct bcm_rio_port *rio, uint32_t ftype, i
         	                *(uint64_t *)(rxmsgq->paddr + rxmsgq->head) = (uint64_t)phys_to_virt(paddr);
                 	        rxmsgq->head = (rxmsgq->head + 1) % rxmsgq->max_entries;
                         	rxmsgq->used++;
-				spin_unlock_irqrestore(&rxmsgq->dmsg_rxlock, flags);
+				raw_spin_unlock_irqrestore(&rxmsgq->dmsg_rxlock, flags);
 	                        if (mport->inb_msg[mbox].mcback)
         	                        mport->inb_msg[mbox].mcback(mport, rxmsgq->dev_id, mbox, -1);
                 	        else
@@ -380,13 +380,13 @@ int bcm_rio_config_dmsg_lookup(uint32_t start, uint32_t mask0, uint32_t mask1, u
 {
         unsigned long flags;
 
-        spin_lock_irqsave(&rio_block.dmesg_lock, flags);
+        raw_spin_lock_irqsave(&rio_block.dmesg_lock, flags);
         bcm_write_srio_devreg(rio_block.xlp_srio_dev_base, DATAMSGBYTE, start);
         bcm_write_srio_devreg(rio_block.xlp_srio_dev_base, DATAMSGEN3, mask0);
         bcm_write_srio_devreg(rio_block.xlp_srio_dev_base, DATAMSGEN2, mask1);
         bcm_write_srio_devreg(rio_block.xlp_srio_dev_base, DATAMSGEN1, mask2);
         bcm_write_srio_devreg(rio_block.xlp_srio_dev_base, DATAMSGEN0, mask3);
-        spin_unlock_irqrestore(&rio_block.dmesg_lock, flags);
+        raw_spin_unlock_irqrestore(&rio_block.dmesg_lock, flags);
         srio_dbg2(srio_debug_level, "DATAMSGBYTE 0x%x mask0 0x%x \n",bcm_read_srio_devreg(rio_block.xlp_srio_dev_base, DATAMSGBYTE), bcm_read_srio_devreg(rio_block.xlp_srio_dev_base, DATAMSGEN3));
         return 0;
 }
@@ -414,12 +414,12 @@ int bcm_rio_config_dbell_offset(uint32_t offset)
         if (offset > max_dbell_off)
                 return -ESRIO_API_PARAM;
 
-        spin_lock_irqsave(&rio_block.dbell_lock, flags);
+        raw_spin_lock_irqsave(&rio_block.dbell_lock, flags);
         rio_block.dbell_offset = offset;
         srio_comcfg = bcm_read_srio_devreg(rio_block.xlp_srio_dev_base, SRICOMCONFIG);
         srio_comcfg &= DBELLINDEX_MASK | (offset << DBELL_INDX_START_POS);
         bcm_write_srio_devreg(rio_block.xlp_srio_dev_base, SRICOMCONFIG, srio_comcfg);
-        spin_unlock_irqrestore(&rio_block.dbell_lock, flags);
+        raw_spin_unlock_irqrestore(&rio_block.dbell_lock, flags);
         return 0;
 }
 
@@ -585,10 +585,10 @@ int bcm_rio_config_read_##size(struct bcm_rio_port *port, type *data,  uint32_t
                 return -ESRIO_ADDRESS_ALIGN; \
         cmd = SRIO_DIRIO_CMD(dest_id, port->hw_port_id, port->sys_size, 0, 0, 0, hopcount); \
     /*    printk("%s Writing cmd 0x%x in %d offset 0x%x hopcount %d dest 0x%x \n",__func__, cmd, (DIRECTIO_CMD + dioindex), offset, hopcount, dest_id);i*/ \
-	spin_lock_irqsave(&rio_block.dirio_lock[dioindex], flags); \
+	raw_spin_lock_irqsave(&rio_block.dirio_lock[dioindex], flags); \
         bcm_write_srio_devreg(port->xlp_srio_dev_base, (DIRECTIO_CMD + dioindex ), cmd); \
         __sync(); \
-	spin_unlock_irqrestore(&rio_block.dirio_lock[dioindex], flags); \
+	raw_spin_unlock_irqrestore(&rio_block.dirio_lock[dioindex], flags); \
         *((type *)data) = bcm_srio_dirio_##func(offset, DIRIO_OP_MREAD); \
         return 0; \
 }
@@ -624,12 +624,12 @@ int bcm_rio_config_write_##size(struct bcm_rio_port *port, type data,  uint32_t
         \
         cmd = SRIO_DIRIO_CMD(dest_id, port->hw_port_id, port->sys_size, 0, 0, 0, hopcount); \
         srio_dbg2(0, "Writing cmd 0x%x in %d \n", cmd, DIRECTIO_CMD + dioindex); \
-	spin_lock_irqsave(&rio_block.dirio_lock[dioindex], flags); \
+	raw_spin_lock_irqsave(&rio_block.dirio_lock[dioindex], flags); \
         bcm_write_srio_devreg(port->xlp_srio_dev_base, (DIRECTIO_CMD + dioindex), cmd); \
         __sync(); \
         bcm_srio_dirio_##func(offset, DIRIO_OP_MWRITE, (type )data); \
         status = bcm_read_srio_devreg(port->xlp_srio_dev_base, DIRECTIO_RESP + dioindex) & DIRIO_RESP_MASK; \
-	spin_unlock_irqrestore(&rio_block.dirio_lock[dioindex], flags); \
+	raw_spin_unlock_irqrestore(&rio_block.dirio_lock[dioindex], flags); \
         if (status != 0) \
                 return -ESRIO_TRANS_REQ; \
         return status; \
@@ -918,14 +918,14 @@ bcm_rio_add_outb_message(struct rio_mport *mport, struct rio_dev *rdev, int mbox
         if (atomic_read(&txq->letter) == 4)
             return -EINVAL;
 
-	spin_lock_irqsave(&txq->dmsg_txlock, flags);
+	raw_spin_lock_irqsave(&txq->dmsg_txlock, flags);
 	letter = txq->used & 0x3;
 	*(uint64_t *)(txq->paddr + txq->head) = (uint64_t)buffer;
 	txq->used++;
 	if (++txq->head == txq->max_entries)
 		txq->head = 0;
         atomic_inc(&txq->letter);
-	spin_unlock_irqrestore(&txq->dmsg_txlock, flags);
+	raw_spin_unlock_irqrestore(&txq->dmsg_txlock, flags);
 
 	rio_add_outb_msg(port, mbox, virt_to_phys(buffer), len, dest_id, letter);
 	return 0;	
@@ -958,18 +958,18 @@ int bcm_rio_open_outb_mbox(struct rio_mport *mport, void *dev_id, int mbox, int
                 return -EINVAL;
 
 	txq = &port->dmsg_txq[mbox];
-	spin_lock_irqsave(&txq->dmsg_txlock, flags);
+	raw_spin_lock_irqsave(&txq->dmsg_txlock, flags);
 	txq->head = txq->tail = txq->used  = 0;
 	txq->max_entries = entries;		
 	txq->dev_id = dev_id;
         atomic_set(&txq->letter, 0);
 	txq->paddr = kmalloc(sizeof(uint64_t) * entries, GFP_KERNEL | GFP_DMA);
 	if (txq->paddr == NULL) {
-		spin_unlock_irqrestore(&txq->dmsg_txlock, flags);
+		raw_spin_unlock_irqrestore(&txq->dmsg_txlock, flags);
 		return -ENOMEM;
 	}
 	txq->status = NLM_QVALID;
-	spin_unlock_irqrestore(&txq->dmsg_txlock, flags);
+	raw_spin_unlock_irqrestore(&txq->dmsg_txlock, flags);
 
 	return 0;
 }
@@ -997,19 +997,19 @@ int bcm_rio_open_inb_mbox(struct rio_mport *mport, void *dev_id, int mbox,
 
 	srio_dbg2(srio_debug_level,"%s: dev_id %p mbox %d entries %d\n", __func__, dev_id, mbox, entries);
 	rxq = &port->dmsg_rxq[mbox];
-	spin_lock_irqsave(&rxq->dmsg_rxlock, flags);
+	raw_spin_lock_irqsave(&rxq->dmsg_rxlock, flags);
 	rxq->head = rxq->tail = rxq->used = 0;
 	rxq->max_entries = entries;
 	rxq->dev_id = dev_id;
 	rxq->paddr = kmalloc(sizeof(uint64_t) * entries, GFP_KERNEL | GFP_DMA);
 	if (rxq->paddr == NULL) {
 		printk("%s kmalloc failed\n",__func__);
-		spin_unlock_irqrestore(&rxq->dmsg_rxlock, flags);
+		raw_spin_unlock_irqrestore(&rxq->dmsg_rxlock, flags);
 		return -ENOMEM;
 	}
 	rxq->status = NLM_QVALID;
 	atomic_set(&port->enabled, 1);
-	spin_unlock_irqrestore(&rxq->dmsg_rxlock, flags);
+	raw_spin_unlock_irqrestore(&rxq->dmsg_rxlock, flags);
 	return 0;
 }
 
@@ -1032,7 +1032,7 @@ void bcm_rio_close_outb_mbox(struct rio_mport *mport, int mbox)
                 return;
 
 	txq = &port->dmsg_txq[mbox];
-	spin_lock_irqsave(&txq->dmsg_txlock, flags);	
+	raw_spin_lock_irqsave(&txq->dmsg_txlock, flags);	
 	txq->head = txq->tail = txq->used  = 0;
 	txq->max_entries = 0;
 	txq->dev_id = NULL;
@@ -1041,7 +1041,7 @@ void bcm_rio_close_outb_mbox(struct rio_mport *mport, int mbox)
 		kfree(txq->paddr);
 		txq->paddr = NULL;
 	}
-	spin_unlock_irqrestore(&txq->dmsg_txlock, flags);	
+	raw_spin_unlock_irqrestore(&txq->dmsg_txlock, flags);	
 }
 
 /**
@@ -1064,13 +1064,13 @@ void *bcm_rio_get_inb_message(struct rio_mport *mport, int mbox)
 
 	rxmsgq = &port->dmsg_rxq[mbox];
 	srio_dbg2(srio_debug_level,"%s from mbox %d : %p\n", __func__, mbox, &rxmsgq->dmsg_rxlock);
-	spin_lock_irqsave(&rxmsgq->dmsg_rxlock, flags);
+	raw_spin_lock_irqsave(&rxmsgq->dmsg_rxlock, flags);
 	if ((rxmsgq->status == NLM_QVALID) && (rxmsgq->used)) {
 		buf =(void *)(*(uint64_t *)(rxmsgq->paddr + rxmsgq->tail));
 		rxmsgq->tail = (rxmsgq->tail + 1) % rxmsgq->max_entries;
 		rxmsgq->used--;
 	}
-	spin_unlock_irqrestore(&rxmsgq->dmsg_rxlock, flags);
+	raw_spin_unlock_irqrestore(&rxmsgq->dmsg_rxlock, flags);
 	return buf;
 }
 
@@ -1152,7 +1152,7 @@ void bcm_rio_close_inb_mbox(struct rio_mport *mport, int mbox)
                 return;
 
 	rxq = &port->dmsg_rxq[mbox];
-	spin_lock_irqsave(&rxq->dmsg_rxlock, flags);
+	raw_spin_lock_irqsave(&rxq->dmsg_rxlock, flags);
 	if ((rxq->status == NLM_QVALID) && (rxq->used)) {
 		printk("Close mbox called when the Q is not empty \n");
 	}
@@ -1165,7 +1165,7 @@ void bcm_rio_close_inb_mbox(struct rio_mport *mport, int mbox)
                 rxq->paddr = NULL;
         }
 	atomic_set(&port->enabled, 0);
-	spin_unlock_irqrestore(&rxq->dmsg_rxlock, flags);
+	raw_spin_unlock_irqrestore(&rxq->dmsg_rxlock, flags);
 
 	xlp_rio_pop_msg(port);
 }
@@ -1211,15 +1211,15 @@ static int nlm_xlp3xx_ax_rio_handle_rxreq(struct bcm_rio_port *rio, uint32_t fty
                         paddr = SRIO_PAYLOAD_ADDR(msg2);
 			
 			rxmsgq = &rio->dmsg_rxq[mbox];
-			spin_lock_irqsave(&rxmsgq->dmsg_rxlock, flags);
+			raw_spin_lock_irqsave(&rxmsgq->dmsg_rxlock, flags);
 			if (atomic_read(&rio->enabled) == 0) {
-				spin_unlock_irqrestore(&rxmsgq->dmsg_rxlock, flags);
+				raw_spin_unlock_irqrestore(&rxmsgq->dmsg_rxlock, flags);
 				return -ESRIO_API_INTERNAL;
 			}
 			if ((rxmsgq->status != NLM_QVALID) || 
 			    (rxmsgq->used == rxmsgq->max_entries)) {				
 				//FIXME send the buffer back to address Q. should not come here.
-				spin_unlock_irqrestore(&rxmsgq->dmsg_rxlock, flags);
+				raw_spin_unlock_irqrestore(&rxmsgq->dmsg_rxlock, flags);
 				if (bcm_rio_send_free_rx_buff(paddr))
                                         return -ESRIO_API_INTERNAL;
 				break;
@@ -1228,7 +1228,7 @@ static int nlm_xlp3xx_ax_rio_handle_rxreq(struct bcm_rio_port *rio, uint32_t fty
 			*(uint64_t *)(rxmsgq->paddr + rxmsgq->head) = (uint64_t)phys_to_virt(paddr);
 			rxmsgq->head = (rxmsgq->head + 1) % rxmsgq->max_entries;
 			rxmsgq->used++;
-			spin_unlock_irqrestore(&rxmsgq->dmsg_rxlock, flags);
+			raw_spin_unlock_irqrestore(&rxmsgq->dmsg_rxlock, flags);
 			if (mport->inb_msg[mbox].mcback)
                 	        mport->inb_msg[mbox].mcback(mport, rxmsgq->dev_id, mbox, -1);
 	                else
@@ -1296,16 +1296,16 @@ void bcm_xlp3xx_ax_rio_msghandler(uint32_t vc, uint32_t src_id,
 			dmsg_txq = &rio->dmsg_txq[mbox];
                         atomic_dec(&dmsg_txq->letter); 
 			srio_dbg(srio_debug_level,"FTYPE_RESPONSE mbox %d status 0x%x\n", mbox, status);
-			spin_lock_irqsave(&dmsg_txq->dmsg_txlock, flags);
+			raw_spin_lock_irqsave(&dmsg_txq->dmsg_txlock, flags);
 			if (dmsg_txq->used == 0) {
 				//error
-				spin_unlock_irqrestore(&dmsg_txq->dmsg_txlock, flags);
+				raw_spin_unlock_irqrestore(&dmsg_txq->dmsg_txlock, flags);
 				break;
 			}
 			slot = dmsg_txq->tail;	
 			dmsg_txq->tail = (dmsg_txq->tail + 1) % dmsg_txq->max_entries;
 			dmsg_txq->used--;
-			spin_unlock_irqrestore(&dmsg_txq->dmsg_txlock, flags);
+			raw_spin_unlock_irqrestore(&dmsg_txq->dmsg_txlock, flags);
 			mport->outb_msg[mbox].mcback(mport, dmsg_txq->dev_id, -1,
                                                 slot);
                         break;
@@ -1453,8 +1453,8 @@ int bcm_rio_setup(struct bcm_rio_port *rio_port, int index, struct rio_mport *mp
 	rio_port->sys_size = rio_config.sys_size;
 
 	for(mbox = 0; mbox < MAX_MBOX; mbox++) {
-		spin_lock_init(&rio_port->dmsg_txq[mbox].dmsg_txlock);
-		spin_lock_init(&rio_port->dmsg_rxq[mbox].dmsg_rxlock);
+		raw_spin_lock_init(&rio_port->dmsg_txq[mbox].dmsg_txlock);
+		raw_spin_lock_init(&rio_port->dmsg_rxq[mbox].dmsg_rxlock);
 	}
 	printk("RIO port %d msgdst_id %d rxvc %d hw_port_id %d txstn_id %d hostdevice_id %d\n", index, rio_port->msgdst_id, rio_port->rxvc,
 			rio_port->hw_port_id, rio_port->txstn_id, rio_port->device_id);
-- 
1.7.4

