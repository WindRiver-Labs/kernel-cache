From bcfa263e0d9db30906165fd3d91e4e396c00e4d7 Mon Sep 17 00:00:00 2001
From: Om Narasimhan <omn@broadcom.com>
Date: Wed, 8 Aug 2012 11:33:08 -0700
Subject: [PATCH 584/762] PIC : new PIC abstraction layer

A new PIC layer implemented to abstract PIC h/w specific portions in the code.
Some standard entry points (APIs) are also defined so that porting would be
easier across different processors. This patch handles only PIC and related
code. PCI subsystem changes are not a part of this patch

Based on Broadcom SDK 2.3.

Signed-off-by: Om Narasimhan <omn@broadcom.com>
Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 arch/mips/include/asm/netlogic/nlm_dma.h     |    2 +-
 arch/mips/include/asm/netlogic/pci_hal.h     |   13 +
 arch/mips/include/asm/netlogic/pic_hal.h     |  113 ++++
 arch/mips/include/asm/netlogic/xlp_irq.h     |  180 +++---
 arch/mips/include/asm/netlogic/xlp_irq_map.h |   38 ++
 arch/mips/include/asm/netlogic/xlp_pic.h     |   40 ++
 arch/mips/kernel/nlm_fs_handler.S            |   13 +-
 arch/mips/mm/cerr-nlm.c                      |    8 +-
 arch/mips/netlogic/common/Makefile           |    2 +-
 arch/mips/netlogic/common/pci_hal.c          |   75 +++
 arch/mips/netlogic/common/pic_hal.c          |   87 +++
 arch/mips/netlogic/xlp/Makefile              |    3 +-
 arch/mips/netlogic/xlp/irq.c                 |  631 +++++++++++++-----
 arch/mips/netlogic/xlp/on_chip.c             |   14 +-
 arch/mips/netlogic/xlp/pic/Makefile          |    4 +
 arch/mips/netlogic/xlp/pic/xlp_irq_map.c     |  792 ++++++++++++++++++++++
 arch/mips/netlogic/xlp/pic/xlp_pic.c         |  540 +++++++++++++++
 arch/mips/netlogic/xlp/platform.c            |   21 +-
 arch/mips/netlogic/xlp/setup.c               |   94 ++--
 arch/mips/netlogic/xlp/smp.c                 |   57 +-
 arch/mips/netlogic/xlp/time.c                |   35 +-
 arch/mips/netlogic/xlp/xlp_gpio.c            |    2 +-
 arch/mips/pci/pci-xlp.c                      |  923 --------------------------
 23 files changed, 2363 insertions(+), 1324 deletions(-)
 create mode 100644 arch/mips/include/asm/netlogic/pci_hal.h
 create mode 100644 arch/mips/include/asm/netlogic/pic_hal.h
 create mode 100644 arch/mips/include/asm/netlogic/xlp_irq_map.h
 create mode 100644 arch/mips/include/asm/netlogic/xlp_pic.h
 create mode 100644 arch/mips/netlogic/common/pci_hal.c
 create mode 100644 arch/mips/netlogic/common/pic_hal.c
 create mode 100644 arch/mips/netlogic/xlp/pic/Makefile
 create mode 100644 arch/mips/netlogic/xlp/pic/xlp_irq_map.c
 create mode 100644 arch/mips/netlogic/xlp/pic/xlp_pic.c
 delete mode 100644 arch/mips/pci/pci-xlp.c

diff --git a/arch/mips/include/asm/netlogic/nlm_dma.h b/arch/mips/include/asm/netlogic/nlm_dma.h
index 32267fa..133592f 100644
--- a/arch/mips/include/asm/netlogic/nlm_dma.h
+++ b/arch/mips/include/asm/netlogic/nlm_dma.h
@@ -27,7 +27,7 @@
 
 #ifdef CONFIG_NLM_XLP
 #include <asm/netlogic/iomap.h>
-#include <asm/netlogic/xlp_hal_pic.h>
+#include <asm/netlogic/xlp_irq.h>
 #include <hal/nlm_hal.h>
 #include <hal/nlm_hal_macros.h>
 #include <hal/nlm_hal_fmn.h>
diff --git a/arch/mips/include/asm/netlogic/pci_hal.h b/arch/mips/include/asm/netlogic/pci_hal.h
new file mode 100644
index 0000000..ba83e1f
--- /dev/null
+++ b/arch/mips/include/asm/netlogic/pci_hal.h
@@ -0,0 +1,13 @@
+#ifndef ASM_BRCM_PCI_HAL_H
+#define ASM_BRCM_PCI_HAL_H
+
+u64 __nlh_pci_r32o(u8 nid, u8 fn, u64 offset);
+void __nlh_pci_w32o(u8 nid, u8 fn, u64 offset, u32 val);
+struct xlp_intmode_struct {
+	u32 mode;
+	int usage;
+};
+int xlp_ctrl_intmode_add(u8 node, int fn, int mode, int i);
+int xlp_get_ctrl_intmode(u8 node, int fn);
+int xlp_set_ctrl_intmode(u8 node, int fn, int mode);
+#endif
diff --git a/arch/mips/include/asm/netlogic/pic_hal.h b/arch/mips/include/asm/netlogic/pic_hal.h
new file mode 100644
index 0000000..a10ba46
--- /dev/null
+++ b/arch/mips/include/asm/netlogic/pic_hal.h
@@ -0,0 +1,113 @@
+#ifndef _BRCM_PIC_HAL_H_
+#define _BRCM_PIC_HAL_H_
+
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/interrupt.h>
+#include <linux/spinlock.h>
+#include <linux/mm.h>
+#include <linux/slab.h>
+#include <asm/errno.h>
+#include <asm/signal.h>
+#include <asm/ptrace.h>
+#include <asm/kgdb.h>
+#include <linux/module.h>
+
+#define XLP_BDF_BASE(b,d,f)	(0x18000000 + ((b) << 20) + ((d) << 15) + ((f) << 12))
+
+struct irq_map_elem {
+	int rvec;
+	int usage[NLM_MAX_CPU_NODE];	/* This is the usage count of an rvec,
+			   not the number of times a vector is used in s/w */
+};
+/*
+ * When startup function is called on an IRQ, that IRT's rvec map's bitmap
+ * would be set. This serves as a quick reverse lookup at the time of dispatch
+ */
+struct rvec_map_elem {
+	/* irt = elem.irt + ffs(bitmap), where bitmap != 0 */
+	int irt;	/* The first IRT corresponding to this rvec */
+	volatile unsigned long bitmap[NLM_MAX_CPU_NODE];	/* bit set is the offset from irt */
+};
+struct pic_dev;
+
+struct pic_dev
+{
+	void *base;	/* base address of this PIC */
+	u32 node;	/* Node id */
+	char *name;	/* humar readable name */
+	int (*self_init)(struct pic_dev*);
+	int (*self_cleanup)(void);
+	spinlock_t pic_lock;
+	int max_irq_per_rvec;	/* max number of vectors sharing same rvec */
+	/* Platform specific IRQ setup calls */
+	int (*plat_request_irq)(struct pic_dev *, int, u64,
+			const struct cpumask *, struct cpumask *);
+	int (*set_irq_affinity)(struct pic_dev *, int, u64,
+			const struct cpumask *, struct cpumask *);
+	int (*get_closest_mask)(struct pic_dev *, const struct cpumask *,
+			struct cpumask *);
+	int (*get_irq_affinity)(struct pic_dev *, int, struct cpumask *);
+	int (*plat_release_irq)(struct pic_dev *, int, u64);
+	int (*setup_pic_systimer)(struct pic_dev *, int, u64);
+	int (*setup_pic_wdtimer)(struct pic_dev *, int, u64);
+	int (*mod_pic_systimer)(struct pic_dev *, int, u64);
+	int (*mod_pic_wdtimer)(struct pic_dev *, int, u64);
+	int (*interrupt_pending) (struct pic_dev *, void*, u64);
+	int (*interrupt_ack)(struct pic_dev *, u32, u64);
+};
+
+int register_pic_dev(u32, struct pic_dev *);
+int unregister_pic_dev(u32, struct pic_dev *);
+int retrieve_node_pic_dev(u32, struct pic_dev **);
+int xlp_rvec_from_irq(int irq);
+int xlp_get_irq_base_bitmap(struct pic_dev *pic, u32 rvec, u32 *irq, u64 *bitmap);
+
+u64 __pic_r64o(void *, u64);
+void __pic_w64o(void *, u64, u64);
+u32 __pic_r32o(void *, u64);
+void __pic_w32o(void *, u64, u32);
+
+#define __pic_r64r(x, y)	__pic_r64o(x, (y << 2))
+#define __pic_w64r(x, y, z)	__pic_w64o(x, (y << 2), z)
+#define __pic_r32r(x, y)	__pic_r32o(x, (y << 2))
+#define __pic_w32r(x, y, z)	__pic_w32o(x, (y << 2), z)
+
+/* enum for command, status and return codes */
+
+enum upic_types {
+	UPIC_FAIL		= -1,
+	UPIC_SUCCESS		= 0,
+
+	/* bits 1 - 2 taken */
+	UPIC_INT_MASK		= 3 << 1,
+	UPIC_INTX		= 1 << 1,
+	UPIC_MSI		= 2 << 1,
+	UPIC_MSIX		= 3 << 1,
+
+	/* bits 3 - 4 taken */
+	UPIC_MCAST_MASK		= 3 << 3,
+	UPIC_MULTI_CAST		= 1 << 3,
+	UPIC_UNICAST		= 2 << 3,
+	UPIC_UCAST_DIST		= 3 << 3,
+
+	/* bits 5 - 6 taken */
+	UPIC_DTYPE_MASK		= 3 << 5,
+	UPIC_DTYPE_DTE		= 1 << 5,
+	UPIC_DTYPE_ITE		= 2 << 5,
+
+	/* bits 7 - 9 taken */
+	UPIC_PARAM_MASK		= 7 << 7,
+	UPIC_PARAM_IRT		= 1 << 7,
+	UPIC_PARAM_IRQ		= 2 << 7,
+	UPIC_PARAM_SETBIT	= 3 << 7,
+	UPIC_PARAM_CLEARBIT	= 4 << 7,
+
+	/* bits 10 - 11 taken */
+	UPIC_AFFINITY_MASK	= 3 << 10,
+	UPIC_AFFINITY_ITE	= 1 << 10,
+	UPIC_AFFINITY_DB	= 2 << 10,
+
+};
+
+#endif
diff --git a/arch/mips/include/asm/netlogic/xlp_irq.h b/arch/mips/include/asm/netlogic/xlp_irq.h
index 00e8098..aad99a5 100644
--- a/arch/mips/include/asm/netlogic/xlp_irq.h
+++ b/arch/mips/include/asm/netlogic/xlp_irq.h
@@ -1,6 +1,3 @@
-#ifndef CONFIG_NLM_XLP /* temporary tripwire */
-#error "Should only be used by XLP"
-#endif
 /***********************************************************************
 Copyright 2003-2010 Netlogic Microsystems (“Netlogic”). All rights
 reserved.
@@ -26,105 +23,90 @@ ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
 THE POSSIBILITY OF SUCH DAMAGE.
 *****************************#NETL_2#********************************/
 
-#ifndef _ASM_NLM_XLP_IRQ_H
-#define _ASM_NLM_XLP_IRQ_H
+#ifndef _ASM_BRCM_XLP_IRQ_H
+#define _ASM_BRCM_XLP_IRQ_H
 
-/* Maximum IRQ vectors */
-#define XLP_EIRR_SIZE		64
+#if defined __ASSEMBLY__
+#define ASM_XLP_IO_PIC_OFFSET        0xffffffffb8004100 /* TODO: This will change in to function */
+#define C_XLP_IO_PIC_OFFSET        0xffffffffb8004100ULL /* TODO: This will change in to function */
+#define XLP_IO_PIC_OFFSET        C_XLP_IO_PIC_OFFSET
 
-#define XLP_IRQ_IPI_SMP_KGDB         2
-#define XLP_IRQ_IPI_SMP_FUNCTION     3
-#define XLP_IRQ_IPI_SMP_RESCHEDULE   4
-#define XLP_IRQ_MSGRING              5
-#define XLP_IRQ_PERFCTR              6
-#define XLP_IRQ_TIMER                7
-#define XLP_IRQ_RESERVED_MAX         8
+#else
+#include <asm/netlogic/xlp_pic.h>
+#include <asm/netlogic/xlp_irq_map.h>
+/*
+ *     Register Offsets
+ */
+#define fdebug(fmt,arg...)\
+	printk(KERN_DEBUG "%s:%d " fmt, __FILE__, __LINE__, ##arg)
 
-#define arch_setup_msi_irqs	arch_setup_msi_irqs /* defines arch. specific msi setup function */
-#define xlp_irq_to_irt(x)	((x) - XLP_IRQ_RESERVED_MAX)
-#define xlp_irt_to_irq(x)	((x) + XLP_IRQ_RESERVED_MAX)
-
-#define XLP_UART_IRT_OFFSET			(133 + XLP_IRQ_RESERVED_MAX)
-#define XLP_UART_IRQ(x)	(XLP_UART_IRT_OFFSET + (x))
-
-#define XLP_PIC_IRQ_BASE             XLP_IRQ_RESERVED_MAX
-
-#define xlp_pic_irt_to_irq(irt) ((irt) + XLP_PIC_IRQ_BASE)
-#define xlp_pic_irq_to_irt(irq) ((irq) - XLP_PIC_IRQ_BASE)
-
-#define XLP_IRQ(irt) xlp_pic_irt_to_irq(irt)
-
-#include <asm/netlogic/xlp_hal_pic.h>
-
-#define NR_IRQS		(XLP_IRQ_RESERVED_MAX + XLP_PIC_NUM_IRTS)
-
-#if 0
-#define XLP_PIC_IRQ_WD_0		(XLP_PIC_IRT_WD_0 + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_WD_1		(XLP_PIC_IRT_WD_1 + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_WD(x)		((x) + XLP_PIC_IRQ_WD_0)
-#define XLP_PIC_IRQ_WDNMI_0		(XLP_PIC_IRT_WDNMI_0 + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_WDNMI_1		(XLP_PIC_IRT_WDNMI_1 + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_WDNMI(x)		((x) + XLP_PIC_IRQ_WDNMI_0)
-#define XLP_PIC_IRQ_SYSTIMER_0		(XLP_PIC_IRT_SYSTIMER_0 + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_SYSTIMER_7		(XLP_PIC_IRT_SYSTIMER_7 + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_SYSTIMER(x)		((x) + XLP_PIC_IRQ_SYSTIMER_0)
-#define XLP_PIC_IRQ_FMN_POPQ_0		(XLP_PIC_IRT_FMN_POPQ_0 + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_FMN_POPQ_31		(XLP_PIC_IRT_FMN_POPQ_31 + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_FMN_POPQ(x)		((x) + XLP_PIC_IRQ_FMN_POPQ_0)
-#define XLP_PIC_IRQ_FMN_FATAL		(XLP_PIC_IRT_FMN_NONFATAL + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_FMN_NONFATAL	(XLP_PIC_IRT_FMN_FATAL + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_PCIE_MSIX_0		(XLP_PIC_IRT_PCIE_MSIX_0 + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_PCIE_MSIX_31	(XLP_PIC_IRT_PCIE_MSIX_31 + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_PCIE_MSIX(x)	((x) + XLP_PIC_IRQ_PCIE_MSIX_0)
-#define XLP_PIC_IRQ_PCIE_0		(XLP_PIC_IRT_PCIE_0 + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_PCIE_3		(XLP_PIC_IRT_PCIE_3 + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_PCIE(x)		((x) + XLP_PIC_IRQ_PCIE_0)
-#define XLP_PIC_IRQ_NET_IF_0		(XLP_PIC_IRT_NET_IF_0 + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_NET_IF_18		(XLP_PIC_IRT_NET_IF_18 + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_NET_IF(x)		((x) + XLP_PIC_IRQ_NET_IF_0)
-#define XLP_PIC_IRQ_NET_COMMON_0	(XLP_PIC_IRT_NET_COMMON_0 + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_NET_COMMON_1	(XLP_PIC_IRT_NET_COMMON_1 + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_NET_COMMON(x)	((x) + XLP_PIC_IRQ_NET_COMMON_0)
-#define XLP_PIC_IRQ_POE			(XLP_PIC_IRT_POE + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_USB_0		(XLP_PIC_IRT_USB_0 + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_USB_5		(XLP_PIC_IRT_USB_5 + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_USB(x)		((x) + XLP_PIC_IRQ_USB_0)
-#define XLP_PIC_IRQ_DTR			(XLP_PIC_IRT_DTR + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_SAE			(XLP_PIC_IRT_SAE + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_RSA			(XLP_PIC_IRT_RSA + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_CDE_0		(XLP_PIC_IRT_CDE_0 + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_CDE_3		(XLP_PIC_IRT_CDE_3 + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_CDE(x)		((x) + XLP_PIC_IRQ_CDE_0)
-#define XLP_PIC_IRQ_ICI_0		(XLP_PIC_IRT_ICI_0 + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_ICI_2		(XLP_PIC_IRT_ICI_2 + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_ICI(x)		((x) + XLP_PIC_IRQ_ICI_0)
-#define XLP_PIC_IRQ_KBP			(XLP_PIC_IRT_KBP + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_UART_0		(XLP_PIC_IRT_UART_0 + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_UART_1		(XLP_PIC_IRT_UART_1 + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_UART(x)		((x) + XLP_PIC_IRQ_UART_0)
-#define XLP_PIC_IRQ_I2C_0		(XLP_PIC_IRT_I2C_0 + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_I2C_1		(XLP_PIC_IRT_I2C_1 + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_I2C(x)		((x) + XLP_PIC_IRQ_I2C_0)
-#define XLP_PIC_IRQ_SYS_0		(XLP_PIC_IRT_SYS_0 + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_SYS_1		(XLP_PIC_IRT_SYS_1 + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_SYS(x)		((x) + XLP_PIC_IRQ_SYS_0)
-#define XLP_PIC_IRQ_JTAG		(XLP_PIC_IRT_JTAG + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_PIC			(XLP_PIC_IRT_PIC + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_GPIO_0		(XLP_PIC_IRT_GPIO_0 + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_GPIO_3		(XLP_PIC_IRT_GPIO_3 + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_GPIO(x)		((x) + XLP_PIC_IRQ_GPIO_0)
-#define XLP_PIC_IRQ_NOR			(XLP_PIC_IRT_NOR + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_NAND		(XLP_PIC_IRT_NAND + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_SPI			(XLP_PIC_IRT_SPI + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_MMC			(XLP_PIC_IRT_MMC + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_NBU			(XLP_PIC_IRT_NBU + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_TCU			(XLP_PIC_IRT_TCU + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_GCU			(XLP_PIC_IRT_GCU + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_DMC_0		(XLP_PIC_IRT_DMC_0 + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_DMC_1		(XLP_PIC_IRT_DMC_1 + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_DMC(x)		((x) + XLP_PIC_IRQ_DMC_0)
-#define XLP_PIC_IRQ_TCB			(XLP_PIC_IRT_TCB + XLP_PIC_IRQ_BASE)
+#define XLP_PIT_TIMER_MAX	(u64)(~0ULL)
+
+u64 __nlh_pic_r64o(u8 nid, u64 offset);
+void __nlh_pic_w64o(u8 nid, u64 offset, u64 val);
+extern u64 __nlh_pci_r64o(u8, u64);
+extern void __nlh_pci_w64o(u8, u64, u64);
+extern void xlp_ack_pic(u8, int);
+
+#if !defined CONFIG_XLP_REPLACE_R4K_TIMER
+void nlm_hal_pic_update_control(u64);
 #endif
 
-#endif /* _ASM_NLM_XLP_IRQ_H */
+#define nlh_pic_r64r(nid, reg)	__nlh_pic_r64o(nid, (reg << 2))
+#define nlh_pic_w64r(nid, reg, val) __nlh_pic_w64o(nid, (reg << 2), val)
+/* Note PCI should take function into account, PIC has only one function */
+#define nlh_pci_r32r(nid, fn, reg)	__nlh_pci_r32o(nid, fn, (reg << 2))
+#define nlh_pci_w32r(nid, fn, reg, val) __nlh_pci_w32o(nid, fn, (reg << 2), val)
+
+/* We define NR_IRQs to be 254, but IRT entries are 160 in size
+ * Effectively, we cannot use anything more than 159 */
+#define NLM_MAX_CPU_NODE	4
+#define XLP_IRQS_PER_NODE	384
+#define NR_IRQS			(XLP_IRQS_PER_NODE * NLM_MAX_CPU_NODE)
+/* Maximum IRQ vector numbers supported by MIPS */
+#define XLP_EIRR_SIZE		64
+#define XLP_IRT_NUM	160
+#define XLP_IRQ_MAX	(XLP_IRT_NUM + XLP_IRQ_RESERVED_MAX)
+/*
+ *    IRT Map
+ */
+#define arch_setup_msi_irqs	arch_setup_msi_irqs /* defines arch. specific msi setup function */
+
+/* The following are the values supported per slot. A slot can have a device or
+ * a bridge, but only this much MSI/MSI-X can be alloted on that slot
+ * */
 
+#define XLP_INTX_TO_CTRL_FN(irq)\
+({\
+ u32 lirq = irq % XLP_IRQS_PER_NODE;\
+ ((lirq - XLP_PCIE_LINK_IRQ(0,0)) & 0x3);\
+})
+#define XLP_IRQ_TO_NODE(x)		((u8)((x) / XLP_IRQS_PER_NODE))
+
+#define XLP_MAX_SLOTS		4	/* Only 4 slots now */
+
+enum xlp_intmode {
+	XLP_INTMODE_NONE = 0,
+	XLP_INTMODE_INTX = 1,
+	XLP_INTMODE_MSI = 2,
+	XLP_INTMODE_MSIX = 4,
+};
+
+enum xlp_int_pending {
+	XLP_INTPENDING_MASK	= 0x7,
+	XLP_INTPENDING_V0	= 1 << 0,
+	XLP_INTPENDING_V1	= 1 << 1,
+	XLP_INTPENDING_V2	= 1 << 2,
+};
+
+struct xlp_ip {
+	u64 ip[3];	/* 160 bits used */
+	u32 valid;	/* can have at most 3 least significant bits set */
+};
+
+#define xlp_incr_ctrl_intmode(n, fn, mode) xlp_ctrl_intmode_add(n, fn, mode, 1)
+#define xlp_decr_ctrl_intmode(n, fn, mode) xlp_ctrl_intmode_add(n, fn, mode, -1)
+
+#endif		/* __ASSEMBLY__ */
+
+#endif
diff --git a/arch/mips/include/asm/netlogic/xlp_irq_map.h b/arch/mips/include/asm/netlogic/xlp_irq_map.h
new file mode 100644
index 0000000..a18d639
--- /dev/null
+++ b/arch/mips/include/asm/netlogic/xlp_irq_map.h
@@ -0,0 +1,38 @@
+#ifndef _XLP_IRQ_MAP_H
+#define _XLP_IRQ_MAP_H
+
+#define SMP_CALL_KGDB_HOOK_RVEC			2
+#define XLP_IRQ_IPI_SMP_FUNCTION_RVEC		3
+#define XLP_IRQ_IPI_SMP_RESCHEDULE_RVEC		4
+#define XLP_IRQ_MSGRING_RVEC			5
+#define XLP_IRQ_OPROFILE_RVEC			6
+#define XLP_IRQ_TIMER_RVEC			7
+#define SMP_OPROFILE_IPI_RVEC			8
+#define XLP_IRQ_IPI_SMP_KGDB_RVEC		9
+/* some idea about XLP_IRQ_RESERVED_MAX
+ * Any IRQ in the system must be > XLP_IRQ_RESERVED_MAX because IRQ is derived
+ * using node ID and (IRT number + XLP_IRQ_RESERVED_MAX)
+ * Any IRQ below is considered only for IPI and other PIC internal interrupts
+ * where RVEC == IRQ number.
+ */
+#define XLP_IRQ_RESERVED_MAX			10
+
+struct pic_dev;
+void xlp_init_irqmap(struct pic_dev *pic);
+void __xlp_irq_mask(struct pic_dev *pic, unsigned int irq);
+void __xlp_irq_unmask(struct pic_dev *pic, int irq);
+int xlp_irq_startup(struct pic_dev *pic, unsigned int oirq);
+void xlp_irq_shutdown(struct pic_dev *pic, unsigned int irq);
+int xlp_get_irq_base_bitmap(struct pic_dev *pic, u32 rvec, u32 *irq, u64 *bitmap);
+int __xlp_modify_irq_bitmap(struct pic_dev *pic, u32 irq, u64 type);
+int xlp_rvec_from_irq(int irq);
+
+#define xlp_irq_to_irt(x) (((x) % XLP_IRQS_PER_NODE) - XLP_IRQ_RESERVED_MAX)
+#define xlp_irt_to_irq(n,x)\
+	(((n) * XLP_IRQS_PER_NODE + XLP_IRQ_RESERVED_MAX + (x)))
+
+/* These are constant across all XLP versions.
+ * Compile time defined to make look up faster/
+ */
+#define XLP_PCIE_LINK_IRQ(node, fn) xlp_irt_to_irq(node, 78 + (fn))
+#endif
diff --git a/arch/mips/include/asm/netlogic/xlp_pic.h b/arch/mips/include/asm/netlogic/xlp_pic.h
new file mode 100644
index 0000000..8a6dc2a
--- /dev/null
+++ b/arch/mips/include/asm/netlogic/xlp_pic.h
@@ -0,0 +1,40 @@
+#ifndef _BRCM_XLP_PIC_H_
+#define _BRCM_XLP_PIC_H_
+
+/* interpretation of status register bits */
+#define XLP_PIC_STATUS_ICI_TIMEO(x)	((x >> 33) & 0x7)
+#define XLP_PIC_STATUS_ITE(x)		((x >> 32) & 1)
+
+#define XLP_PIC_CTRL			0x40
+#define XLP_PIC_BYTESWAP		0x42
+#define XLP_PIC_STATUS			0x44
+#define XLP_PIC_INT_TIMEOUT		0x46
+#define XLP_PIC_ICI0_INT_TIMEOUT	0x48
+#define XLP_PIC_ICI1_INT_TIMEOUT	0x4A
+#define XLP_PIC_ICI2_INT_TIMEOUT	0x4C
+#define XLP_PIC_IPI_CTL			0x4E
+#define XLP_PIC_INT_ACK			0x50
+#define XLP_PIC_INT_PENDING(x)		(0x52 + ((x) << 1))
+
+#define XLP_PIC_WD_MAXVAL(x)		(0x58 + (0xE) *(x))
+#define XLP_PIC_WD_COUNT(x)		(0x5A + (0xE) *(x))
+#define XLP_PIC_WD_THREN0(x)		(0x5C + (0xE) *(x))
+#define XLP_PIC_WD_THREN1(x)		(0x5E + (0xE) *(x))
+#define XLP_PIC_WD_BEATCMD(x)		(0x60 + (0xE) *(x))
+#define XLP_PIC_WD_HB1(x)		(0x62 + (0xE) *(x))
+#define XLP_PIC_WD_HB2(x)		(0x64 + (0xE) *(x))
+
+#define XLP_PIC_SYSTIMER_MAXVAL(x)	(0x74 + ((x) << 1))
+#define XLP_PIC_SYSTIMER_COUNT(x)	(0x84 + ((x) << 1))
+#define XLP_PIC_INT_THREADEN01(x)	(0x94 + ((x) << 2))
+#define XLP_PIC_INT_THREADEN23(x)	(0x96 + ((x) << 2))
+#define XLP_PIC_IRT_ENTRY(x)		(0xB4 + ((x) << 1))
+
+#define XLP_IRTENT_ENABLE		(1ULL << 31)
+#define XLP_IRTENT_NMI			(1ULL << 29)
+#define XLP_IRTENT_SCH_LCL		(1ULL << 28)
+#define XLP_IRTENT_RVEC(x)		(((x) & 0x3fULL) << 20)
+#define XLP_IRTENT_DT			(1ULL << 19)
+#define XLP_IRTENT_DB(x)		((x & 7) << 16)
+#define XLP_IRTENT_DTE(x)		((x) & 0xffff)
+#endif
diff --git a/arch/mips/kernel/nlm_fs_handler.S b/arch/mips/kernel/nlm_fs_handler.S
index 9e83ac2..8be23d0 100644
--- a/arch/mips/kernel/nlm_fs_handler.S
+++ b/arch/mips/kernel/nlm_fs_handler.S
@@ -353,17 +353,26 @@ END(nlm_fs_processorId)
 
 NESTED(nlm_fs_read_timer, PT_SIZE, sp)
 
-#include <asm/netlogic/xlp_hal_pic.h>
+#if defined(CONFIG_NLM_XLP)
+#include <asm/netlogic/xlp_irq.h>
+#define PIC_TIMER_6_COUNTER	0x28
+
         MFC0    k0, CP0_PRID, 1
         andi    k0, k0, 0x3ff
         srl     k0, k0, 5  /* grab node id */
         sll     k0, k0, 18 /* Use local PIC */
-        dli     k1, ASM_XLP_IO_PIC_OFFSET + (PIC_SYSTIMER_COUNT(6) << 3)
+        dli     k1, ASM_XLP_IO_PIC_OFFSET + (PIC_TIMER_6_COUNTER << 3)
         dadd    k0, k1, k0
 	ld	k1, 0(k0)
 	dsrl32	$9, k1, 0
 	dsll32  $8, k1, 0
 	dsrl32  $8, T0, 0
+#else
+	dli	k0, 0xffffffffbef00000 + NETLOGIC_IO_PIC_OFFSET + (PIC_TIMER_6_COUNTER_0 << 2)
+	lw	$8, 0(k0)
+	dli	k0, 0xffffffffbef00000 + NETLOGIC_IO_PIC_OFFSET + (PIC_TIMER_6_COUNTER_1 << 2)
+	lw	$9, 0(k0)
+#endif
 	fs_eret
 
 END(nlm_fs_read_timer)
diff --git a/arch/mips/mm/cerr-nlm.c b/arch/mips/mm/cerr-nlm.c
index 0e60900..b577a41 100644
--- a/arch/mips/mm/cerr-nlm.c
+++ b/arch/mips/mm/cerr-nlm.c
@@ -220,7 +220,7 @@ static void dump_cerr_info(void)
 	cerr_printk("     NBU: reg0 = 0x%08x, reg1 = 0x%08x, reg2 = 0x%08x\n",
 				nbu_reg0, nbu_reg1, nbu_reg2);
 
-	num_controllers = 4;	//8xx has 4 controller 
+	num_controllers = 4;	//8xx has 4 controller
 	ret=is_nlm_xlp(300, XLP_REVISION_ANY, CPU_EXTPID_XLP_3XX_ANY);
 	if( ret ) num_controllers = 2;
 	else
@@ -455,7 +455,7 @@ static int check_DRAM_error(void)
 	int node = hard_smp_processor_id() / NLM_MAX_CPU_PER_NODE;
 	uint64_t mmio = nlm_hal_get_dev_base(node, 0, NLH_BRIDGE, 0);
 
-	int num_controllers = 4;	//8xx has 4 controller 
+	int num_controllers = 4;	//8xx has 4 controller
 	int ret=is_nlm_xlp(300, XLP_REVISION_ANY, CPU_EXTPID_XLP_3XX_ANY);
 	if( ret ) num_controllers = 2;
 	else
@@ -521,7 +521,7 @@ static char *c_nbu_reqtype[] = {
 
 static int check_NBU_error(void)
 {
-	int node = CPU_TO_NODE(nlm_hal_cpu_id());
+	int node = hard_smp_processor_id() / NLM_MAX_CPU_PER_NODE;
 	uint64_t nbu_mmio = nlm_hal_get_dev_base(node, 0, NLH_BRIDGE, 0);
 
 	uint32_t nbu_reg0 = nlm_hal_read_32bit_reg(nbu_mmio, 0xA2);
@@ -743,7 +743,7 @@ static int compute_return_registers(struct pt_regs *regs, unsigned long *pErrepc
 	if(pErrepc==NULL)	return -1;
 
 	/*	it has slight chance that prev_errepc may not in the tlb and tlb handling of prev_errepc may trigger
-		some issues, or prev_errepc may point to non insn. Since we do not have enough informaiton, have to 
+		some issues, or prev_errepc may point to non insn. Since we do not have enough informaiton, have to
 		access as normal.
  	*/
 	errepc = *pErrepc;
diff --git a/arch/mips/netlogic/common/Makefile b/arch/mips/netlogic/common/Makefile
index af949ee..9fa7c3a 100644
--- a/arch/mips/netlogic/common/Makefile
+++ b/arch/mips/netlogic/common/Makefile
@@ -1,5 +1,5 @@
 EXTRA_CFLAGS := $(CFLAGS) -DNLM_HAL_LINUX_KERNEL -Iarch/mips/include/asm/netlogic/hal
-obj-$(CONFIG_NLM_XLP) :=   cpu_proc.o memory.o bootinfo.o
+obj-$(CONFIG_NLM_XLP) :=   cpu_proc.o memory.o bootinfo.o pic_hal.o pci_hal.o
 
 obj-$(CONFIG_NLM_XLP)			+= nlm_hal_sys.o nlm_evp_cpld.o nlm_hal_cpu_info.o
 obj-$(CONFIG_NLM_XLP)			+= nlm_hal.o fdt_helper.o
diff --git a/arch/mips/netlogic/common/pci_hal.c b/arch/mips/netlogic/common/pci_hal.c
new file mode 100644
index 0000000..dc7859e
--- /dev/null
+++ b/arch/mips/netlogic/common/pci_hal.c
@@ -0,0 +1,75 @@
+#include <asm/netlogic/hal/nlm_hal_macros.h>
+#include <asm/netlogic/pci_hal.h>
+#include <asm/netlogic/pic_hal.h>
+
+static u64 xlp_pci_base[NLM_MAX_CPU_NODE][XLP_MAX_SLOTS] = {
+	/* This should be accessed as xlp_pci_base[node][fn] */
+
+	{XLP_BDF_BASE(0,1,0), XLP_BDF_BASE(0,1,1),
+	XLP_BDF_BASE(0,1,2), XLP_BDF_BASE(0,1,3)},
+
+	{XLP_BDF_BASE(0,9,0), XLP_BDF_BASE(0,9,1),
+	XLP_BDF_BASE(0,9,2), XLP_BDF_BASE(0,9,3)},
+
+	{XLP_BDF_BASE(0,17,0), XLP_BDF_BASE(0,17,1),
+	XLP_BDF_BASE(0,17,2), XLP_BDF_BASE(0,17,3)},
+
+	{XLP_BDF_BASE(0,25,0), XLP_BDF_BASE(0,25,1),
+	XLP_BDF_BASE(0,25,2), XLP_BDF_BASE(0,25,3)}
+};
+
+u64 __nlh_pci_r32o(u8 nid, u8 fn, u64 offset)
+{
+	return (nlh_read_cfg_reg32(xlp_pci_base[nid][fn] + offset));
+}
+
+void __nlh_pci_w32o(u8 nid, u8 fn, u64 offset, u32 val)
+{
+	nlh_write_cfg_reg32(xlp_pci_base[nid][fn] + offset, val);
+}
+
+
+/* The following is the table describing current interrupt modes of
+ * XLP controllers. When an external switch is present, different devices
+ * can request different interrupt mode on the same controller which might lead
+ * to controller changing previous interrupt mode. If this happens, interrupt
+ * delivery will not work correctly. So, we need a way to prevent different
+ * devices requesting different interrupt modes. This is kind of impossible
+ * because we can't control all device drivers, but we can
+ *	1. fail pci_enable_msi{x} if intX is set and at least one interrupt
+ *	is allocated
+ *	2. fail request_irq() for any interrupt outside current interrupt
+ *	distribution range
+ */
+static struct xlp_intmode_struct intmode[NLM_MAX_CPU_NODE][XLP_MAX_SLOTS];
+
+int xlp_ctrl_intmode_add(u8 node, int fn, int mode, int i)
+{
+	if (intmode[node][fn].mode != mode) {
+		return -EBUSY;
+	}
+	intmode[node][fn].usage += i;
+	if ((intmode[node][fn].usage < 0) || (intmode[node][fn].usage == 0)) {
+		intmode[node][fn].usage = 0;
+	}
+	return intmode[node][fn].usage;
+}
+
+
+int xlp_get_ctrl_intmode(u8 node, int fn)
+{
+	return intmode[node][fn].mode;
+}
+
+int xlp_set_ctrl_intmode(u8 node, int fn, int mode)
+{
+	int ret = 0;
+	if (intmode[node][fn].mode == mode) {
+		/* do nothing */
+	} else if (intmode[node][fn].usage == 0) {
+		intmode[node][fn].mode = mode;
+	} else {
+		ret = -EBUSY;
+	}
+	return ret;
+}
diff --git a/arch/mips/netlogic/common/pic_hal.c b/arch/mips/netlogic/common/pic_hal.c
new file mode 100644
index 0000000..e161149
--- /dev/null
+++ b/arch/mips/netlogic/common/pic_hal.c
@@ -0,0 +1,87 @@
+/* Generic read/write routines for PIC dev */
+#include <asm/netlogic/pic_hal.h>
+#include <asm/netlogic/hal/nlm_hal_macros.h>
+
+static struct pic_dev *global_pic_dev[NLM_MAX_CPU_NODE];	/* global pic device */
+static DEFINE_SPINLOCK(gbl_pic_lock);	/* just for reg/dereg */
+
+/*
+ * This function registers the pic dev on a node
+ */
+int register_pic_dev(u32 node, struct pic_dev *dev)
+{
+	unsigned long flags;
+	int ret = -EBUSY;
+
+	if (dev->self_init(dev) < 0) return -EFAULT;
+	spin_lock_irqsave(&gbl_pic_lock, flags);
+	if (global_pic_dev[node] == NULL) {
+		global_pic_dev[node] = dev;
+		printk(KERN_DEBUG "pic_dev[%u] = 0x%p\n", node, dev);
+		ret = 0;
+	}
+	spin_unlock_irqrestore(&gbl_pic_lock, flags);
+	return ret;
+}
+
+/*
+ * PIC device unregistration is just a theoretical possibility,
+ * as XLP does not support node hot plugging
+ * Just given for the sake of completeness
+ */
+int unregister_pic_dev(u32 node, struct pic_dev *dev)
+{
+	unsigned long flags;
+	int ret = -EINVAL;
+
+	if (dev->self_cleanup() < 0) return -EFAULT;
+	spin_lock_irqsave(&gbl_pic_lock, flags);
+	if (global_pic_dev[node] == dev) {
+		global_pic_dev[node] = 0;
+		ret = 0;
+	}
+	spin_unlock_irqrestore(&gbl_pic_lock, flags);
+	return ret;
+}
+
+/*
+ * fetches node specific pic from the global registered set of pic devs"
+ * Since there is no unregistration, spin_lock holding is not necessary
+ */
+int retrieve_node_pic_dev(u32 node, struct pic_dev **in)
+{
+//	unsigned long flags;
+	int ret = -EFAULT;
+
+	//fdebug("node %d, dev %#llx\n", global_pic_dev[node]);
+	if ((in == NULL) || (global_pic_dev[node] == NULL)) return ret;
+//	spin_lock_irqsave(&gbl_pic_lock, flags);
+	*in = global_pic_dev[node];
+//	spin_unlock_irqrestore(&gbl_pic_lock, flags);
+	return 0;
+}
+
+u64 __pic_r64o(void *base, u64 offset)
+{
+	return ld_40bit_phys(((u64)base + offset), CCA_UNCACHED);
+}
+EXPORT_SYMBOL(__pic_r64o);
+
+void __pic_w64o(void *base, u64 offset, u64 val)
+{
+	sd_40bit_phys(((u64)base + offset), CCA_UNCACHED, val);
+}
+EXPORT_SYMBOL(__pic_w64o);
+
+u32 __pic_r32o(void *base, u64 offset)
+{
+	return lw_40bit_phys(((u64)base + offset), CCA_UNCACHED);
+}
+EXPORT_SYMBOL(__pic_r32o);
+
+void __pic_w32o(void *base, u64 offset, u32 val)
+{
+	sw_40bit_phys(((u64)base + offset), CCA_UNCACHED, val);
+}
+EXPORT_SYMBOL(__pic_w32o);
+
diff --git a/arch/mips/netlogic/xlp/Makefile b/arch/mips/netlogic/xlp/Makefile
index 2cfd9b0..517b8ca 100644
--- a/arch/mips/netlogic/xlp/Makefile
+++ b/arch/mips/netlogic/xlp/Makefile
@@ -3,8 +3,9 @@ EXTRA_CFLAGS := $(CFLAGS) -DNLM_HAL_LINUX_KERNEL -Iarch/mips/include/asm/netlogi
 
 obj-y                    	= setup.o nmi.o
 obj-y 				+= irq.o time.o on_chip.o mmu.o
-obj-$(CONFIG_NLM_XLP) 		+= platform.o board.o  xlp_gpio.o xlp_hal_pic.o
+obj-$(CONFIG_NLM_XLP) 		+= platform.o board.o  xlp_gpio.o
 obj-$(CONFIG_SMP)       	+= smp.o
+obj-y				+= pic/
 
 obj-$(CONFIG_KGDB)      += nmi.o
 obj-$(CONFIG_NLM_XLP) += cop2.o
diff --git a/arch/mips/netlogic/xlp/irq.c b/arch/mips/netlogic/xlp/irq.c
index 86070b9..14563d2 100644
--- a/arch/mips/netlogic/xlp/irq.c
+++ b/arch/mips/netlogic/xlp/irq.c
@@ -30,136 +30,285 @@ THE POSSIBILITY OF SUCH DAMAGE.
 #include <linux/spinlock.h>
 #include <linux/mm.h>
 #include <linux/slab.h>
-#include <linux/irq.h>
-
+#include <linux/pci.h>
 #include <asm/errno.h>
 #include <asm/signal.h>
 #include <asm/ptrace.h>
+#include <asm/kgdb.h>
 #include <asm/mipsregs.h>
-#include <asm/thread_info.h>
+#include <linux/irq.h>
 
-#include <asm/netlogic/mips-exts.h>
-#include <asm/netlogic/xlp_hal_pic.h>
-#include <asm/netlogic/xlp.h>
 #include <asm/netlogic/xlp_irq.h>
-#include <asm/netlogic/pic.h>
+#include <asm/netlogic/xlp.h>
+#include <asm/netlogic/mips-exts.h>
 #include <asm/netlogic/debug.h>
+#include <asm/thread_info.h>
+#include <asm/netlogic/pic_hal.h>
+#include <asm/netlogic/pci_hal.h>
+#include <asm/netlogic/xlp_irq_map.h>
+
+/* About this file: irq.c
+ * This file contains routines that handle all the low level interrupt stuff.
+ * Some of the platform specific portions are moved to arch/mips/pci/pci-xlp.c
+ * Actions handled here are: initialization of the interrupt map, requesting of
+ * interrupt lines by handlers, dispatching interrupts to handlers, probing
+ * for interrupt lines..etc.
+ */
+
+/* Externs */
+extern void nlm_xlp_msgring_int_handler(int , struct pt_regs *);
+extern int xlp_get_ctrl_intmode(u8, int);
+
+/* xlp_irq_mask is retained for legacy. It can be removed at a later point of
+ * time. Initially it was meant to keep a copy of present interrupt mask; with
+ * multi cpus each having its own mask register, we might not need this variable
+ */
+static volatile uint64_t xlp_irq_mask;
+
+/* spin lock for all interrupt related data structures
+ * This variable is used in timer init, so we export it
+ */
+
+/*
+ * This function checks if the irq has a valid range for IRQ
+ * @irq : irq number to check
+ */
+int check_intx_range(struct pic_dev *pic, u32 oirq)
+{
+	u32 n = oirq / XLP_IRQS_PER_NODE;
+	u32 irq = oirq % XLP_IRQS_PER_NODE;
+
+	if ((irq < XLP_IRQ_RESERVED_MAX) && (irq >= 0)) {
+		return -EINVAL;
+	} else if(irq >= XLP_IRQ_MAX) {
+		pr_err("irq = %d. Invalid irq requested\n", oirq);
+		return -EINVAL;
+	} else if (pic->node != n) {
+		pr_err("irq = %d node != pic->node (%d)\n", oirq, pic->node);
+		return -EINVAL;
+	}
+	return 0;
+}
 
-#define XLP_IRT_NUM	160
-#define XLP_IRQ_MAX	168	/* 0-7 are reserved + 160 IRT entries */
-
-#define XLP_PIC_IRT_UART_0		133
-#define XLP_PIC_IRT_UART_1		134
-#define XLP_PIC_IRT_UART(x)		((x) + XLP_PIC_IRT_UART_0)
-
-#define XLP_PIC_IRQ_UART_0		(XLP_PIC_IRT_UART_0 + XLP_PIC_IRQ_BASE)
-#define XLP_PIC_IRQ_UART_1		(XLP_PIC_IRT_UART_1 + XLP_PIC_IRQ_BASE)
+/*
+ * Interface function (unlocked version) to mask an IRQ
+ * Calls helper function after input tests and spin_lock holds
+ *
+ * @irq : IRQ number
+ */
+static void nlm_intx_mask(struct irq_data *data)
+{
+	unsigned int irq = data->irq;
+	struct pic_dev *pic = (struct pic_dev *)data->chip_data;
 
-DEFINE_SPINLOCK(xlp_pic_lock);
+	//unsigned long flags;
+	if(check_intx_range(pic, irq) < 0) {
+		return;
+	}
+	// Once enabled, we don't mask it out
+	//spin_lock_irqsave(&xlp_pic_lock, flags);	// Remove XXX
+	__xlp_irq_mask(pic, irq);				// XXX
+	//spin_unlock_irqrestore(&xlp_pic_lock, flags);	// XXX remove
+	return;
+}
 
-static uint64_t xlp_irq_mask;
-static int xlp_rvec_to_irq_map[XLP_EIRR_SIZE];
-static int xlp_irq_to_rvec_map[NR_IRQS];
 
-static void set_irq_mapping(int irq, int rvec)
+/*
+ * Interface function (unlocked version) to mask an IRQ
+ * Calls helper function after input tests and spin_lock holds
+ *
+ * @irq : IRQ number
+ */
+static void nlm_intx_unmask(struct irq_data *data)
 {
-	BUG_ON((irq < 0) || (irq >= NR_IRQS));
-	BUG_ON((rvec < 0) || (rvec >= XLP_EIRR_SIZE));
+	unsigned int irq = data->irq;
+	struct pic_dev *pic = (struct pic_dev *)data->chip_data;
 
-	xlp_irq_to_rvec_map[irq] = rvec;
-	xlp_rvec_to_irq_map[rvec] = irq;
-
-	/* TODO: handle shared rvec case */
+	//unsigned long flags;
+	if(check_intx_range(pic, irq) < 0) {
+		return;
+	}
+	//spin_lock_irqsave(&xlp_pic_lock, flags);
+	__xlp_irq_unmask(pic, irq);
+	//spin_unlock_irqrestore(&xlp_pic_lock, flags);
+	return;
 }
 
-static void set_pic_irq_mapping(int irq, int rvec)
+static void nlm_intx_ack(struct irq_data *data)
 {
-	BUG_ON((rvec < XLP_IRQ_RESERVED_MAX) || (irq < XLP_PIC_IRQ_BASE));
+	unsigned int irq = data->irq;
+	u32 irt = xlp_irq_to_irt(irq);
+	struct pic_dev *pic = (struct pic_dev *)data->chip_data;
 
-	set_irq_mapping(irq, rvec);
+	if (check_intx_range(pic, irq) < 0) {
+		/* No need to ack. Not by PIC */
+		return;
+	}
+#if !defined CONFIG_XLP_REPLACE_R4K_TIMER/* all are level triggered in XLP */
+	pic->interrupt_ack(pic, irt, 0);
+#endif
 }
 
-static void __init init_xlp_irq_map(void)
+
+static void nlm_intx_end(struct irq_data *data)
 {
-	int i;
+	unsigned int irq = data->irq;
+	struct pic_dev *pic = (struct pic_dev *)data->chip_data;
 
-	for (i = 0; i < XLP_EIRR_SIZE; i++)
-		xlp_rvec_to_irq_map[i] = -1;
+	if (check_intx_range(pic, irq) < 0) {
+		/* No need to end(ack). Not by PIC */
+		return;
+	}
+	/* If level triggered, ack it after the device condition is cleared */
+	pic->interrupt_ack(pic, xlp_irq_to_irt(irq), 0);
+	return;
+}
+/*
+ * Enables INTx on a controller
+ */
+static int __xlp_intx_enable(u8 node, int fn)
+{
+	u32 pci;
+
+	pci = nlh_pci_r32r(node, fn,  0x1);
+	pci &= ~(1 << 10);	/* Enable IntX assertion */
+	nlh_pci_w32r(node, fn, 0x1, pci);
+	pci = nlh_pci_r32r(node, fn,  0x261);
+	pci |= 0xf;	/* Enable INT A,B,C,D */
+	nlh_pci_w32r(node, fn, 0x261, pci);
+	return 0;
+}
 
-	for (i = 0; i < NR_IRQS; i++)
-		xlp_irq_to_rvec_map[i] = -1;
+int xlp_intx_enable(u8 node, int fn)
+{
+	int mode = xlp_get_ctrl_intmode(node, fn);
 
-	for (i = 0; i < XLP_IRQ_RESERVED_MAX; i++) {
-		switch (i) {
-#ifdef CONFIG_SMP
-		case XLP_IRQ_IPI_SMP_FUNCTION:
-		case XLP_IRQ_IPI_SMP_RESCHEDULE:
-#endif /* CONFIG_SMP */
-		case XLP_IRQ_TIMER:
-			set_irq_mapping(i, i);
-			break;
-		default:
-			break;
-		}
+	if ((mode & XLP_INTMODE_MSI) || (mode & XLP_INTMODE_MSIX)) {
+		return -EBUSY;
 	}
+	__xlp_intx_enable(node, fn);
+	xlp_incr_ctrl_intmode(node, fn, XLP_INTMODE_INTX);
+	return 0;
+}
 
-	for (i = XLP_PIC_IRQ_BASE; i < NR_IRQS; i++) {
-		switch (i) {
-		case XLP_PIC_IRQ_UART_0:
-			set_pic_irq_mapping(i, 17); break;
-		case XLP_PIC_IRQ_UART_1:
-			set_pic_irq_mapping(i, 18); break;
-		default:
-			break;
-		}
-	}
+/*
+ * Disables INTx on a controller
+ */
+static int __xlp_intx_disable(u8 node, int fn)
+{
+	u32 pci;
+
+	pci = nlh_pci_r32r(node, fn,  0x1);
+	pci |= (1 << 10);
+	nlh_pci_w32r(node, fn, 0x1, pci);
+	pci = nlh_pci_r32r(node, fn,  0x261);
+	pci &= ~(0xf);
+	nlh_pci_w32r(node, fn, 0x261, pci);
+	return 0;
 }
 
-static void xlp_pic_enable(struct irq_data *d)
+int xlp_intx_disable(u8 node, int fn)
 {
-	unsigned long flags;
-	int irt = xlp_pic_irq_to_irt(d->irq);
-	BUG_ON(irt < 0);
+	int mode = xlp_get_ctrl_intmode(node, fn);
 
-	spin_lock_irqsave(&xlp_pic_lock, flags);
-	nlm_hal_pic_enable_irt(irt);
-	spin_unlock_irqrestore(&xlp_pic_lock, flags);
+	if (!(mode & XLP_INTMODE_INTX)) {
+		return -EBUSY;
+	}
+	__xlp_intx_disable(node, fn);
+	xlp_decr_ctrl_intmode(node, fn, XLP_INTMODE_INTX);
+	return 0;
 }
 
-static void xlp_pic_disable(struct irq_data *d)
+static unsigned int nlm_intx_startup(struct irq_data *data)
 {
-	unsigned long flags;
-	int irt = xlp_pic_irq_to_irt(d->irq);
-	BUG_ON(irt < 0);
+	unsigned int irq = data->irq;
+	int fn, ret;
+	struct pic_dev *pic = (struct pic_dev *)data->chip_data;
 
-	spin_lock_irqsave(&xlp_pic_lock, flags);
-	nlm_hal_pic_disable_irt(irt);
-	spin_unlock_irqrestore(&xlp_pic_lock, flags);
+	if (check_intx_range(pic, irq) < 0) {
+#if !defined CONFIG_XLP_REPLACE_R4K_TIMER
+		if (irq !=  XLP_IRQ_TIMER_RVEC)
+#endif
+		fdebug("Invalid irq %#x\n", irq);
+		return -EINVAL;
+	}
+	/* if this irq correspond to any of the pci slots in any node,
+	 * enable intx on the controller of node */
+	if ((irq >= XLP_PCIE_LINK_IRQ(pic->node, 0))
+			&& (irq <= XLP_PCIE_LINK_IRQ(pic->node, 3))) {
+		fn = XLP_INTX_TO_CTRL_FN(irq);
+		if ((ret = xlp_intx_enable(pic->node, fn)) < 0) {
+			return (unsigned int)ret;
+		}
+	}
+	return xlp_irq_startup(pic, irq % XLP_IRQS_PER_NODE);
 }
 
-static void xlp_pic_mask_ack(struct irq_data *d)
+/*
+ * Shutdown function for intx
+ */
+static void nlm_intx_shutdown(struct irq_data *data)
 {
-	int rvec = xlp_irq_to_rvec_map[d->irq];
-	BUG_ON(rvec == -1);
+	int fn, ret;
+	unsigned int irq = data->irq;
+	struct pic_dev *pic = (struct pic_dev *)data->chip_data;
 
-	write_64bit_cp0_eirr(1ULL << rvec);
-	/* TODO: handle shared eirr case */
+	if (check_intx_range(pic, irq) < 0) {
+		fdebug("Invalid irq %#x\n", irq);
+		return;
+	}
+	/* if this irq correspond to any of the pci slots, disable intx on
+	 * the controller  before shutting it down */
+	if ((irq >= XLP_PCIE_LINK_IRQ(pic->node, 0))
+			&& (irq <= XLP_PCIE_LINK_IRQ(pic->node, 3))) {
+		fn = XLP_INTX_TO_CTRL_FN(irq);
+		if ((ret = xlp_intx_disable(pic->node, fn)) < 0) {
+			return;
+		}
+	}
+	return xlp_irq_shutdown(pic, irq % XLP_IRQS_PER_NODE);
 }
 
-static void xlp_pic_unmask(struct irq_data *d)
+/*
+ * Set affinity for the intx for chips
+ *
+ * When an interrupt is setup, its EIMR bit is set in all online cpus. That is,
+ * any cpu _can_ receive that interrupt. But it is the IRT entry that decides
+ * whether to send that interrupt (i.e, whether to set EIRR bit or not) to any
+ * particular CPU.
+ *
+ * IRT has two modes to decide the target CPUs for one interrupt.
+ * Method 1 : Using IRT table entry bits DT and DTE
+ * If DT==1, this interrupt can be routed to a max of 16 CPUs (well, hw threads)
+ * If DT==1, there is one more level of indirection called DTE. Each DTE entry
+ * has 128 bits and there are a total of 8 DTE entries. Each DTE entry contains
+ * the bitmask of target CPU for an interrupt. One of them is chosen based
+ * on the specified cpumask.
+ *
+ * The actual bitmask can be different from the specified bitmask based
+ * on the logic of xlp_closest_match_cpumask()
+ */
+static int nlm_intx_set_affinity(struct irq_data *data, const struct cpumask *mask, bool force)
 {
-	int irt = xlp_pic_irq_to_irt(d->irq);
-	BUG_ON(irt < 0);
+	unsigned int irq = data->irq;
+	struct cpumask m;
+	struct pic_dev *pic = (struct pic_dev *)data->chip_data;
 
-	nlm_hal_ack_pic(irt);
+	if (check_intx_range(pic, irq) < 0) {
+		return -EINVAL;
+	}
+	return pic->set_irq_affinity(pic, irq % XLP_IRQS_PER_NODE, UPIC_AFFINITY_ITE, mask, &m);
 }
 
-static struct irq_chip xlp_pic_intr = {
-	.name = "XLP-PIC",
-	.irq_enable	= xlp_pic_enable,
-	.irq_disable	= xlp_pic_disable,
-	.irq_mask_ack	= xlp_pic_mask_ack,
-	.irq_unmask	= xlp_pic_unmask,
+static struct irq_chip nlm_intx_pic = {
+	.name = "XLP-INTX",
+	.irq_mask = nlm_intx_mask,
+	.irq_unmask = nlm_intx_unmask,
+	.irq_startup = nlm_intx_startup,
+	.irq_shutdown = nlm_intx_shutdown,
+	.irq_ack = nlm_intx_ack,
+	.irq_eoi = nlm_intx_end,
+	.irq_set_affinity = nlm_intx_set_affinity,
 };
 
 static void clear_cp0_eimr_bit(unsigned int bit)
@@ -192,8 +341,9 @@ static void xlp_cpu_disable(struct irq_data *d)
 
 static void xlp_cpu_ack(struct irq_data *d)
 {
-	uint64_t mask = (1ULL << d->irq);
+	uint64_t mask = (1ULL << (d->irq));
 
+	//fdebug("irq %d, %#llx\n", d->irq, mask);
 	write_64bit_cp0_eirr(mask);
 }
 
@@ -210,127 +360,238 @@ struct irq_chip xlp_cpu_intr_ipi = {
 	.irq_ack	= xlp_cpu_ack,
 };
 
-static void __init init_xlp_cpu_irqs(void)
-{
-	int i;
-	for (i = 0; i < XLP_IRQ_RESERVED_MAX; i++) {
-		switch (i) {
-#ifdef CONFIG_SMP
-		case XLP_IRQ_IPI_SMP_FUNCTION:
-		case XLP_IRQ_IPI_SMP_RESCHEDULE:
-			xlp_irq_mask |= (1ULL << i);
-			irq_set_chip_and_handler(i, &xlp_cpu_intr_ipi,
-						 handle_percpu_irq);
-			break;
-#endif /* CONFIG_SMP */
-		case XLP_IRQ_TIMER:
-			xlp_irq_mask |= (1ULL << i);
-			irq_set_chip_and_handler(i, &xlp_cpu_intr_timer,
-						 handle_percpu_irq);
-			break;
-		default:
-			break;
-		}
-	}
-}
-
-static void xlp_pic_irt_setup(int irt, int rvec)
-{
-	int cpu;
-	int cpugroup;
-	uint32_t thread_mask;
-
-	BUG_ON((irt < 0) || (irt >= XLP_PIC_NUM_IRTS));
-
-	cpu = hard_smp_processor_id();
-	cpugroup = cpu >> 16;
-	thread_mask = 1 << (cpu & 0xffff);
-
-	nlm_hal_pic_write_irt(irt,
-			      0 /* en:0=disable */,
-			      0 /* nmi:0=maskable */,
-			      1 /* sch:1=local */,
-			      rvec /* vec */,
-			      1 /* dt:1=ID */,
-			      cpugroup /* db */,
-			      thread_mask /* dte */);
-
-	nlm_hal_ack_pic(irt);
-	if (rvec)
-		pr_info("(%s) cpu=%3d irt=%3d rvec=%2d\n", __func__, cpu, irt, rvec);
-}
+struct irq_chip xlp_cpu_intr = {
+	.name           = "XLP-CPU",
+	.irq_enable     = xlp_cpu_enable,
+	.irq_disable    = xlp_cpu_disable,
+	.irq_eoi	= xlp_cpu_ack,
+};
 
-#ifdef CONFIG_XLP_FMN_SUPPORT
-extern void msgring_irq_init(void);
-#endif
+/* reserved pic handler replacement by three different chips END*/
 
-static void __init init_xlp_irqs(void)
+static irqreturn_t nlm_common_rsvd_irq_handler(int irq, void *dev_id)
 {
-	int irt;
-	int irq;
-	int rvec;
-
-	xlp_irq_mask = 0;
-	init_xlp_irq_map();
-
-	init_xlp_cpu_irqs();
-
-	for (irq = XLP_PIC_IRQ_BASE; irq < NR_IRQS; irq++) {
-		irt = xlp_pic_irq_to_irt(irq);
-		if ((irt < 0) || (irt >= XLP_PIC_NUM_IRTS))
-			continue;
-
-		rvec = xlp_irq_to_rvec_map[irq];
-		if (rvec == -1) {
-			xlp_pic_irt_setup(irt, 0);
-			continue;
-		}
-
-		xlp_irq_mask |= (1ULL << rvec);
-		xlp_pic_irt_setup(irt, rvec);
-		irq_set_chip_and_handler(irq, &xlp_pic_intr,
-					 handle_level_irq);
+#if !defined CONFIG_XLP_REPLACE_R4K_TIMER
+	if ((irq % XLP_IRQS_PER_NODE) == XLP_IRQ_TIMER_RVEC) {
+		return IRQ_HANDLED;
+	}
+#else
+	switch (irq) {
+		case XLP_TIMER_IRQ(0, 0) ... XLP_TIMER_IRQ(0, 7):
+		case XLP_TIMER_IRQ(1, 0) ... XLP_TIMER_IRQ(1, 7):
+		case XLP_TIMER_IRQ(2, 0) ... XLP_TIMER_IRQ(2, 7):
+		case XLP_TIMER_IRQ(3, 0) ... XLP_TIMER_IRQ(3, 7):
+		return IRQ_HANDLED;
+		default:
+		break;
 	}
+#endif
+	return IRQ_NONE;
 }
 
-void __init arch_init_irq(void)
+/* Unused function? Remove later */
+void __cpuinit nlm_smp_irq_init(void)
 {
-	/* Initialize the irq descriptors */
-	init_xlp_irqs();
-
 	write_64bit_cp0_eimr(xlp_irq_mask);
 }
 
-void __cpuinit nlm_smp_irq_init(void)
+static int xlp_perf_irq(void)
 {
-	write_64bit_cp0_eimr(xlp_irq_mask);
+	return IRQ_HANDLED;
 }
 
+/*
+ * Entry function for interrupts
+ */
 asmlinkage void plat_irq_dispatch(void)
 {
-	uint64_t eirr;
-	int rvec;
-	int irq;
-
+	volatile u64 eirr;
+	volatile u64 eimr;
+	u64 bitmap;
+	struct pt_regs *pt_regs = current_thread_info()->regs;
+	int rvec = 0, idx = 0, base_irq, irq, fn;
+	struct pic_dev *pic;
+
+	retrieve_node_pic_dev(hard_smp_processor_id() / NLM_MAX_CPU_PER_NODE, &pic);
 	eirr = read_64bit_cp0_eirr();
-	if (eirr & (1ULL << XLP_IRQ_TIMER)) {
-		do_IRQ(XLP_IRQ_TIMER);
-		eirr &= ~(1ULL << XLP_IRQ_TIMER);
+	eimr = read_64bit_cp0_eimr();
+	eirr &= eimr;
+#if !defined CONFIG_XLP_REPLACE_R4K_TIMER
+	if (eirr & (1ULL << XLP_IRQ_TIMER_RVEC)) {
+		do_IRQ(XLP_IRQ_TIMER_RVEC);
+		eirr &= ~(1ULL << XLP_IRQ_TIMER_RVEC);
+		if (!eirr) return;
 	}
-
-	while (eirr) {
+#else
+	/* Dedicated processing only for timer interrupt (RVEC 26, IRT 12) */
+	if (eirr & (1ULL << XLP_PIC_SYSTIMER_RVEC)) {
 		rvec = __ilog2_u64(eirr);
+		write_64bit_cp0_eirr(1ULL << rvec);
+		do_IRQ(XLP_TIMER_IRQ(pic->node, 0));
+		eirr &= ~(1ULL << XLP_PIC_SYSTIMER_RVEC);
+	}
+#endif
+	/* Loop till all bits of eirr is cleared */
+	while (eirr) {
 
-		irq = xlp_rvec_to_irq_map[rvec];
-		if (irq == -1) {
-			spurious_interrupt();
+	rvec = __ilog2_u64(eirr);
+	if (rvec == -1) return;
+	eirr &= ~(1ULL << rvec);
+	write_64bit_cp0_eirr(1ULL << rvec);
+	if (rvec < XLP_IRQ_RESERVED_MAX) {
+		if (rvec == XLP_IRQ_MSGRING_RVEC) {
+			nlm_xlp_msgring_int_handler(rvec, pt_regs);
+		} else if ((rvec == XLP_IRQ_IPI_SMP_FUNCTION_RVEC) ||
+				(rvec == XLP_IRQ_IPI_SMP_RESCHEDULE_RVEC) ||
+				(rvec == XLP_IRQ_IPI_SMP_KGDB_RVEC)) {
+			do_IRQ(rvec);
+		}
+	} else {
+		/* We need to loop through all possible irqs for an rvec */
+		if (xlp_get_irq_base_bitmap(pic, rvec, &base_irq,
+					&bitmap) < 0) {
 			return;
 		}
+		switch(base_irq) {
+		/* For INTX, bitmap and base irq already set */
+#if defined CONFIG_PCI_MSI
+		/* These are not MSI vector numbers, but IRT #s */
+		case XLP_PCIE_LINK_IRQ(0, 0) ... XLP_PCIE_LINK_IRQ(0, 3):
+			/* Here fn # of controller is easily calculated
+			 * Check the IRT table : 0 -> 78, 1-> 79 ..etc */
+			fn = base_irq - XLP_PCIE_LINK_IRQ(0, 0);
+			if (is_msi_set(pic->node, fn) != 0) { /* this is an MSI */
+				/* find vectors set, overwrite bitmap */
+				bitmap = calc_msi_vector_offset(pic->node, fn);
+				/* recalculate base_irq for MSI */
+				base_irq = XLP_MSI_IRQ_START(pic->node, fn);
+				/* now handle it as any other interrupt */
+			} else { /* If MSI is not set, must be INTX */
+				base_irq += (pic->node * XLP_IRQS_PER_NODE);
+			}
+			break;
+		case XLP_PCIE_MSIX_IRQ(0, 0) ... XLP_PCIE_MSIX_IRQ(0, 31):
+			fn = XLP_MSIX_TO_CTRL_FN(base_irq -
+				XLP_PCIE_MSIX_IRQ(0, 0)); /* This _is_ correct because of((x >>3) &3) */
+			/* this is a MSI-X. Find vectors set */
+			bitmap = xlp_msix_status_clear(pic->node, fn);
+			/* recalculate base_irq for MSI */
+			base_irq = XLP_MSIX_IRQ_START(pic->node, fn);
+			/* now handle it as any other interrupt */
+			break;
+#endif
+		default:
+			/* Except MSIX and MSI, all are treated INTX */
+			base_irq += (pic->node * XLP_IRQS_PER_NODE);
+			break;
+		}
+		while (bitmap) {
+			/* now that we have bitmap, serve all of them */
+			idx = ffs(bitmap) - 1;	/* man ffs */
+			bitmap &= ~(1 << idx);
+			do_IRQ(base_irq + idx);
+		}
+	}
+
+	}	// End of while (eirr)...
+	return;
+}
+
+extern void msgring_irq_init(void);
+static inline void set_percpu_irq(unsigned int irq, struct irq_chip *chip)
+{
+	BUG_ON(irq >= XLP_IRQ_RESERVED_MAX);
 
-		do_IRQ(irq);
+	xlp_irq_mask |= (1ULL << irq);
+	irq_set_chip_and_handler(irq, chip, handle_percpu_irq);
+}
 
-		eirr &= ~(1ULL << rvec);
+void __init init_xlp_irqs(void)
+{
+	int i;
+	u64	mask = 0;
+	u8 node;
+	struct pic_dev *pic;
+
+	/* TODO_3.0 : Might have to change to irq_set_chip_and_handler */
+	for_each_online_node(node) {
+		retrieve_node_pic_dev(node, &pic);
+		for (i = 0; i < XLP_IRQ_MAX; i++) {	// IRQ : 0 - 167
+			irq_set_chip_and_handler((node * XLP_IRQS_PER_NODE) + i,
+					&nlm_intx_pic, handle_level_irq);
+			irq_set_chip_data((node * XLP_IRQS_PER_NODE) + i,
+					(void *)pic);
+		}
+#ifdef CONFIG_PCI_MSI
+		for (i = XLP_MSI_INDEX_START; i <= XLP_MSI_INDEX_END; i++) {
+			irq_set_chip_data((node * XLP_IRQS_PER_NODE) + i,
+					(void *)pic);
+			irq_set_chip_and_handler((node * XLP_IRQS_PER_NODE) + i,
+					&nlm_msi_pic, handle_level_irq);
+		}
+		for (i = XLP_MSIX_INDEX_START; i <= XLP_MSIX_INDEX_END; i++) {
+			irq_set_chip_and_handler((node * XLP_IRQS_PER_NODE) + i,
+					&nlm_msix_pic, handle_level_irq);
+			irq_set_chip_data((node * XLP_IRQS_PER_NODE) + i,
+					(void *)pic);
+		}
+#endif
 	}
+
+#ifdef CONFIG_SMP
+#ifdef CONFIG_KGDB
+	set_percpu_irq(XLP_IRQ_IPI_SMP_KGDB_RVEC, &xlp_cpu_intr_ipi);
+#endif
+	set_percpu_irq(XLP_IRQ_IPI_SMP_FUNCTION_RVEC, &xlp_cpu_intr_ipi);
+	set_percpu_irq(XLP_IRQ_IPI_SMP_RESCHEDULE_RVEC, &xlp_cpu_intr_ipi);
+#endif
+	/* msgring interrupt */
+	set_percpu_irq(XLP_IRQ_MSGRING_RVEC, &xlp_cpu_intr);
+	msgring_irq_init();
+//	set_percpu_irq(XLP_IRQ_PERFCTR, &xlp_cpu_intr);
+#if defined CONFIG_XLP_REPLACE_R4K_TIMER
+#error "Has to fix timer interrupt setup for CONFIG_XLP_REPLACE_R4K_TIMER"
+#else
+	set_percpu_irq(XLP_IRQ_TIMER_RVEC, &xlp_cpu_intr_timer);
+#endif
+	mask = (
+#if defined CONFIG_XLP_REPLACE_R4K_TIMER
+			(1ULL << (XLP_IRQ_RESERVED_MAX + 3)) | /* PIC Systimer (0)*/
+#else
+			(1ULL << XLP_IRQ_TIMER_RVEC) |
+#endif
+			(1ULL << (XLP_IRQ_RESERVED_MAX + 4)) |	/* Other PIC timers */
+			(3ULL << (XLP_IRQ_RESERVED_MAX + 5)) |	/* msg_idx */
+			(0xfULL << (XLP_IRQ_RESERVED_MAX + 8)) |	/* pci msix */
+			(0xfULL << (XLP_IRQ_RESERVED_MAX + 12)) |	/* pci and msi */
+			(0x1ULL << (XLP_IRQ_RESERVED_MAX + 16)) |	/* nae */
+			(0x3ULL << (XLP_IRQ_RESERVED_MAX + 18)) |	/* nae */
+			(0x3ULL	<< (XLP_IRQ_RESERVED_MAX + 26))| /* uart */
+	//		(0xfULL << 13) |	/* gpio */
+//			(0x1ULL << 31) |	/* SATA on xlp3xx */
+//			(0x1ULL << 30) |	/* SMSC  - xlp3xx */
+//			(0x1ULL << 26) |	/* MMC  */
+#ifdef CONFIG_SMP
+			(1ULL << XLP_IRQ_IPI_SMP_FUNCTION_RVEC) |
+			(1ULL << XLP_IRQ_IPI_SMP_RESCHEDULE_RVEC) |
+#ifdef CONFIG_KGDB
+			(1ULL << XLP_IRQ_IPI_SMP_KGDB_RVEC) |
+#endif
+#endif
+			(1ULL << XLP_IRQ_MSGRING_RVEC) |
+			(0x3ULL	<< (XLP_IRQ_RESERVED_MAX + 36)) /* nor, nand, spi and mmc */
+	       );
+	/* set interrupt mask for non-zero cpus */
+	mask |= read_64bit_cp0_eimr();
+	xlp_irq_mask = mask;
 }
 
 
+void __init arch_init_irq(void)
+{
+	/* Initialize the irq descriptors */
+	init_xlp_irqs();
+
+	write_64bit_cp0_eimr(xlp_irq_mask);
+
+}
diff --git a/arch/mips/netlogic/xlp/on_chip.c b/arch/mips/netlogic/xlp/on_chip.c
index 06662af..204c362 100644
--- a/arch/mips/netlogic/xlp/on_chip.c
+++ b/arch/mips/netlogic/xlp/on_chip.c
@@ -394,7 +394,7 @@ void nlm_xlp_msgring_int_handler(unsigned int irq, struct pt_regs *regs)
 
 	msg0 = msg1 = msg2 = msg3 = 0;
 	
-	if (irq == XLP_IRQ_MSGRING) {
+	if (irq == XLP_IRQ_MSGRING_RVEC) {
                 /* normal message ring interrupt */
                 /* xlr_inc_counter(MSGRNG_INT);  */
                 nlm_cpu_stat_update_msgring_int();
@@ -474,7 +474,7 @@ static void msg_timer_handler(unsigned long data)
 	struct timer_list *timer = &per_cpu(msg_int_bkup_timer, cpu);
 
 
-	nlm_xlp_msgring_int_handler(XLP_IRQ_MSGRING, NULL);
+	nlm_xlp_msgring_int_handler(XLP_IRQ_MSGRING_RVEC, NULL);
 
 	timer->expires = jiffies + (HZ/100);
 	add_timer(timer);
@@ -630,7 +630,7 @@ void enable_msgconfig_int(void)
 
 	/* Need write interrupt vector to cp2 msgconfig register */
 	msgrng_access_enable(flags);
-	nlm_hal_set_fmn_interrupt(XLP_IRQ_MSGRING);
+	nlm_hal_set_fmn_interrupt(XLP_IRQ_MSGRING_RVEC);
 	msgrng_access_disable(flags);
 }
 
@@ -797,10 +797,10 @@ static irqreturn_t msgring_irq_handler(int irq, void *dev_id)
 
 void msgring_irq_init(void)
 {
-	if (request_irq(XLP_IRQ_MSGRING, msgring_irq_handler,
+	if (request_irq(XLP_IRQ_MSGRING_RVEC, msgring_irq_handler,
 			IRQF_PERCPU | IRQF_NO_THREAD,
 			"FMN", NULL)) {
-		panic("Cannot request_irq(XLP_IRQ_MSGRING)\n");
+		panic("Cannot request_irq(XLP_IRQ_MSGRING_RVEC)\n");
 	}
 }
 EXPORT_SYMBOL(msgring_irq_init);
@@ -810,11 +810,14 @@ EXPORT_SYMBOL(msgring_irq_init);
  * on_chip_init
  *  
  ********************************************************************/
+extern void xlp_pic_init(u8);
 void on_chip_init(void)
 {
 	int i = 0, j = 0;
+	u8 node;
 
 	cpu_logical_map(0)  = hard_smp_processor_id();
+	node = hard_smp_processor_id() / NLM_MAX_CPU_PER_NODE;
 
 	/* Set netlogic_io_base to the run time value */
 #ifdef CONFIG_XLP_FMN_SUPPORT
@@ -824,6 +827,7 @@ void on_chip_init(void)
 #endif
 
 	nlm_hal_init();
+	xlp_pic_init(node);
 
 	for (i = 0; i < NR_CPUS; i++)
 		for (j = 0; j < NLM_MAX_COUNTERS; j++)
diff --git a/arch/mips/netlogic/xlp/pic/Makefile b/arch/mips/netlogic/xlp/pic/Makefile
new file mode 100644
index 0000000..66b5d21
--- /dev/null
+++ b/arch/mips/netlogic/xlp/pic/Makefile
@@ -0,0 +1,4 @@
+EXTRA_CFLAGS := -Werror
+EXTRA_CFLAGS := $(CFLAGS) -DNLM_HAL_LINUX_KERNEL -Iarch/mips/include/asm/netlogic/hal
+
+obj-y					+= xlp_irq_map.o xlp_pic.o
diff --git a/arch/mips/netlogic/xlp/pic/xlp_irq_map.c b/arch/mips/netlogic/xlp/pic/xlp_irq_map.c
new file mode 100644
index 0000000..74a148f
--- /dev/null
+++ b/arch/mips/netlogic/xlp/pic/xlp_irq_map.c
@@ -0,0 +1,792 @@
+#include <asm/netlogic/pic_hal.h>
+#include <asm/netlogic/xlp_irq_map.h>
+//#include <asm/netlogic/xlp_irq.h>
+#include <asm/netlogic/xlp.h>
+/*
+ * There are two data structures pivotal for interrupt delivery mechanism
+ * irq_map[] and rvec_map[]
+ * irq_map[] : An array of struct irq_map_elem.
+ * Number of elements in this array is equal to NR_IRQ => there should be an
+ * entry corresponding to each interrupt in the system.
+ * Initial 8 elements (0-XLP_IRQ_RESERVED_MAX) are unpopulated. They are
+ * reserved for system internal interrupts which are not explicitly handled
+ * by plat_irq_dispatch; the handlers for these interrupts are called
+ * differently.
+ *
+ * All other entries are handled by plat_irq_dispatch()
+ * An entry would contain the rvec entry for this interrupt. The offset of this
+ * entry would be presented as the interrupt number for any requests
+ * E.g, for UART1, index is 141. This is the value of uart1 interrupt.
+ * asm/netlogic/xlp_irq.h defines this value as XLP_UART_IRQ(1)
+ *
+ * Each irq_map_elem has two members : rvec -> the rvec entry for this entry,
+ * usage : the number of successful irq_request() called on this IRQ.
+ *
+ * rvec_map is meant to map rvec numbers back to Interrupts.
+ * RVEC is just a number for s/w; which is the bit offset that is set in EIRR
+ * Refer PRM : 9.3 for details.
+ *
+ * irq_map : {<IERR RVEC>, <#of s/w vector multiplexed on this RVEC> }
+ * Some RVECs are reserved : so use only 9 through 63
+ * Any irt index can be derived from irq using the macros
+ * xlp_irt_to_irq() or xlp_irq_to_irt()
+ * These macros are required because of the imposed 64 bits (if RVEC)
+ * to 160 entries (size of IRT table)
+ *
+ * It is further complicated by the fact that PCI LINK interrupts are
+ * multiplexed with MSI interrupts. In that mode, each PCI Link interrupts
+ * can be caused by 32 MSI interrupts. That means, we need a meachinsm to map
+ * 32 msi interrupts * 4 pci slots (128 interrupts) to 4 possible RVECs.
+ * the irq_map table serves this purpose as well as follows
+ *
+ * We limit the per pci slot interrupt (for the time being) to XLP_MSI_PER_SLOT
+ * (currently 8). This is done to keep total number of interrupts to NR_IRQ;
+ *
+ */
+static struct irq_map_elem *xlp_irq_map;	/* Dynamically assigned */
+static struct rvec_map_elem xlp_rvec_map[XLP_EIRR_SIZE];
+DEFINE_SPINLOCK(xlp_irq_lock); /* Only one for all nodes */
+
+static struct irq_map_elem irq_map_8xx[XLP_IRQS_PER_NODE] = {
+        {XLP_IRQ_RESERVED_MAX + 1, {0,0,0,0}}, /*XLP_WD_IDX(0) :0 */
+        {XLP_IRQ_RESERVED_MAX + 1, {0,0,0,0}}, /*XLP_WD_IDX(1) :1 */
+        {XLP_IRQ_RESERVED_MAX + 2, {0,0,0,0}}, /*XLP_WD_NMI_IDX(0) :2 */
+        {XLP_IRQ_RESERVED_MAX + 2, {0,0,0,0}}, /*XLP_WD_NMI_IDX(1) :3 */
+        {XLP_IRQ_RESERVED_MAX + 3, {0,0,0,0}}, /*XLP_TIMER_IDX(0): systimer:4 */
+        {XLP_IRQ_RESERVED_MAX + 4, {0,0,0,0}}, /*XLP_TIMER_IDX(1): clocksource:5 */
+        {XLP_IRQ_RESERVED_MAX + 4, {0,0,0,0}}, /*XLP_TIMER_IDX(2) :6 */
+        {XLP_IRQ_RESERVED_MAX + 4, {0,0,0,0}}, /*XLP_TIMER_IDX(3) :7 */
+        {XLP_IRQ_RESERVED_MAX + 4, {0,0,0,0}}, /*XLP_TIMER_IDX(4) :8 */
+        {XLP_IRQ_RESERVED_MAX + 4, {0,0,0,0}}, /*XLP_TIMER_IDX(5) :9 */
+        {XLP_IRQ_RESERVED_MAX + 4, {0,0,0,0}}, /*XLP_TIMER_IDX(6) :10 */
+        {XLP_IRQ_RESERVED_MAX + 4, {0,0,0,0}}, /*XLP_TIMER_IDX(7) :11 */
+        {XLP_IRQ_RESERVED_MAX + 5, {0,0,0,0}}, /*XLP_MSGQ_IDX(0) :12 */
+        {XLP_IRQ_RESERVED_MAX + 5, {0,0,0,0}}, /*XLP_MSGQ_IDX(1) :13 */
+        {XLP_IRQ_RESERVED_MAX + 5, {0,0,0,0}}, /*XLP_MSGQ_IDX(2) :14 */
+        {XLP_IRQ_RESERVED_MAX + 5, {0,0,0,0}}, /*XLP_MSGQ_IDX(3) :15 */
+        {XLP_IRQ_RESERVED_MAX + 5, {0,0,0,0}}, /*XLP_MSGQ_IDX(4) :16 */
+        {XLP_IRQ_RESERVED_MAX + 5, {0,0,0,0}}, /*XLP_MSGQ_IDX(5) :17 */
+        {XLP_IRQ_RESERVED_MAX + 5, {0,0,0,0}}, /*XLP_MSGQ_IDX(6) :18 */
+        {XLP_IRQ_RESERVED_MAX + 5, {0,0,0,0}}, /*XLP_MSGQ_IDX(7) :19 */
+        {XLP_IRQ_RESERVED_MAX + 5, {0,0,0,0}}, /*XLP_MSGQ_IDX(8) :20 */
+        {XLP_IRQ_RESERVED_MAX + 5, {0,0,0,0}}, /*XLP_MSGQ_IDX(9) :21 */
+        {XLP_IRQ_RESERVED_MAX + 5, {0,0,0,0}}, /*XLP_MSGQ_IDX(10) :22 */
+        {XLP_IRQ_RESERVED_MAX + 5, {0,0,0,0}}, /*XLP_MSGQ_IDX(11) :23 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(12) :24 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(13) :25 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(14) :26 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(15) :27 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(16) :28 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(17) :29 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(18) :30 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(19) :31 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(20) :32 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(21) :33 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(22) :34 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(23) :35 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(24) :36 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(25) :37 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(26) :38 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(27) :39 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(28) :40 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(29) :41 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(30) :42 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(31) :43 */
+        {XLP_IRQ_RESERVED_MAX + 7, {0,0,0,0}}, /*XLP_MSG_IDX(0) :44 */
+        {XLP_IRQ_RESERVED_MAX + 7, {0,0,0,0}}, /*XLP_MSG_IDX(1) :45 */
+        {XLP_IRQ_RESERVED_MAX + 8, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(0) :46 */
+        {XLP_IRQ_RESERVED_MAX + 8, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(1) :47 */
+        {XLP_IRQ_RESERVED_MAX + 8, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(2) :48 */
+        {XLP_IRQ_RESERVED_MAX + 8, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(3) :49 */
+        {XLP_IRQ_RESERVED_MAX + 8, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(4) :50 */
+        {XLP_IRQ_RESERVED_MAX + 8, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(5) :51 */
+        {XLP_IRQ_RESERVED_MAX + 8, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(6) :52 */
+        {XLP_IRQ_RESERVED_MAX + 8, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(7) :53 */
+        {XLP_IRQ_RESERVED_MAX + 9, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(8) :54 */
+        {XLP_IRQ_RESERVED_MAX + 9, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(9) :55 */
+        {XLP_IRQ_RESERVED_MAX + 9, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(10) :56 */
+        {XLP_IRQ_RESERVED_MAX + 9, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(11) :57 */
+        {XLP_IRQ_RESERVED_MAX + 9, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(12) :58 */
+        {XLP_IRQ_RESERVED_MAX + 9, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(13) :59 */
+        {XLP_IRQ_RESERVED_MAX + 9, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(14) :60 */
+        {XLP_IRQ_RESERVED_MAX + 9, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(15) :61 */
+        {XLP_IRQ_RESERVED_MAX + 10, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(16) :62 */
+        {XLP_IRQ_RESERVED_MAX + 10, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(17) :63 */
+        {XLP_IRQ_RESERVED_MAX + 10, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(18) :64 */
+        {XLP_IRQ_RESERVED_MAX + 10, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(19) :65 */
+        {XLP_IRQ_RESERVED_MAX + 10, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(20) :66 */
+        {XLP_IRQ_RESERVED_MAX + 10, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(21) :67 */
+        {XLP_IRQ_RESERVED_MAX + 10, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(22) :68 */
+        {XLP_IRQ_RESERVED_MAX + 10, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(23) :69 */
+        {XLP_IRQ_RESERVED_MAX + 11, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(24) :70 */
+        {XLP_IRQ_RESERVED_MAX + 11, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(25) :71 */
+        {XLP_IRQ_RESERVED_MAX + 11, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(26) :72 */
+        {XLP_IRQ_RESERVED_MAX + 11, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(27) :73 */
+        {XLP_IRQ_RESERVED_MAX + 11, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(28) :74 */
+        {XLP_IRQ_RESERVED_MAX + 11, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(29) :75 */
+        {XLP_IRQ_RESERVED_MAX + 11, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(30) :76 */
+        {XLP_IRQ_RESERVED_MAX + 11, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(31) :77 */
+        {XLP_IRQ_RESERVED_MAX + 12, {0,0,0,0}}, /*XLP_PCIE_LINK_IRQ(0) :78 */
+        {XLP_IRQ_RESERVED_MAX + 13, {0,0,0,0}}, /*XLP_PCIE_LINK_IRQ(1) :79 */
+        {XLP_IRQ_RESERVED_MAX + 14, {0,0,0,0}}, /*XLP_PCIE_LINK_IRQ(2) :80 */
+        {XLP_IRQ_RESERVED_MAX + 15, {0,0,0,0}}, /*XLP_PCIE_LINK_IRQ(3) :81 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(0) :82 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(1) :83 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(2) :84 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(3) :85 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(4) :86 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(5) :87 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(6) :88 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(7) :89 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(8) :90 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(9) :91 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(10) :92 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(11) :93 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(12) :94 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(13) :95 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(14) :96 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(15) :97 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(16) :98 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(17) :99 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(18) :100 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(19) :101 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(20) :102 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(21) :103 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(22) :104 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(23) :105 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(24) :106 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(25) :107 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(26) :108 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(27) :109 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(28) :110 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(29) :111 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(30) :112 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(31) :113 */
+        {XLP_IRQ_RESERVED_MAX + 17, {0,0,0,0}}, /*XLP_POE_IDX :114 */
+        {XLP_IRQ_RESERVED_MAX + 18, {0,0,0,0}}, /*XLP_USB_IDX(0) :115 */
+        {XLP_IRQ_RESERVED_MAX + 18, {0,0,0,0}}, /*XLP_USB_IDX(1) :116 */
+        {XLP_IRQ_RESERVED_MAX + 18, {0,0,0,0}}, /*XLP_USB_IDX(2) :117 */
+        {XLP_IRQ_RESERVED_MAX + 19, {0,0,0,0}}, /*XLP_USB_IDX(3) :118 */
+        {XLP_IRQ_RESERVED_MAX + 19, {0,0,0,0}}, /*XLP_USB_IDX(4) :119 */
+        {XLP_IRQ_RESERVED_MAX + 19, {0,0,0,0}}, /*XLP_USB_IDX(5) :120 */
+        {XLP_IRQ_RESERVED_MAX + 20, {0,0,0,0}}, /*XLP_GDX_IDX :121 */
+        {XLP_IRQ_RESERVED_MAX + 21, {0,0,0,0}}, /*XLP_SEC_IDX :122 */
+        {XLP_IRQ_RESERVED_MAX + 22, {0,0,0,0}}, /*XLP_RSA_IDX :123 */
+        {XLP_IRQ_RESERVED_MAX + 23, {0,0,0,0}}, /*XLP_COMP_IDX(0) :124 */
+        {XLP_IRQ_RESERVED_MAX + 23, {0,0,0,0}}, /*XLP_COMP_IDX(1) :125 */
+        {XLP_IRQ_RESERVED_MAX + 23, {0,0,0,0}}, /*XLP_COMP_IDX(2) :126 */
+        {XLP_IRQ_RESERVED_MAX + 23, {0,0,0,0}}, /*XLP_COMP_IDX(3) :127 */
+        {-1, {0,0,0,0}}, /*RESERVED_IDX :120 */
+        {XLP_IRQ_RESERVED_MAX + 24, {0,0,0,0}}, /*XLP_ICC_IDX(0) :129  ICC - Inter Chip Coherency*/
+        {XLP_IRQ_RESERVED_MAX + 24, {0,0,0,0}}, /*XLP_ICC_IDX(1) :130 */
+        {XLP_IRQ_RESERVED_MAX + 24, {0,0,0,0}}, /*XLP_ICC_IDX(2) :131 */
+        {XLP_IRQ_RESERVED_MAX + 25, {0,0,0,0}}, /*XLP_CAM_IDX :132 */
+        {XLP_IRQ_RESERVED_MAX + 26, {0,0,0,0}}, /*XLP_UART_IDX(0) :133 */
+        {XLP_IRQ_RESERVED_MAX + 26, {0,0,0,0}}, /*XLP_UART_IDX(0) :134 */
+        {XLP_IRQ_RESERVED_MAX + 27, {0,0,0,0}}, /*XLP_I2C_IDX(0) :135 */
+        {XLP_IRQ_RESERVED_MAX + 27, {0,0,0,0}}, /*XLP_I2C_IDX(0) :136 */
+        {XLP_IRQ_RESERVED_MAX + 28, {0,0,0,0}}, /*XLP_SYS_IDX(0) :137 */
+        {XLP_IRQ_RESERVED_MAX + 28, {0,0,0,0}}, /*XLP_SYS_IDX(1) :138 */
+        {XLP_IRQ_RESERVED_MAX + 29, {0,0,0,0}}, /*XLP_JTAG_IDX :139 */
+        {XLP_IRQ_RESERVED_MAX + 30, {0,0,0,0}}, /*XLP_PIC_IDX :140 */
+        {XLP_IRQ_RESERVED_MAX + 31, {0,0,0,0}}, /*XLP_NBU_IDX :141 */
+        {XLP_IRQ_RESERVED_MAX + 32, {0,0,0,0}}, /*XLP_TCU_IDX :142 */
+        {XLP_IRQ_RESERVED_MAX + 33, {0,0,0,0}}, /*XLP_SATA :143 */
+        {XLP_IRQ_RESERVED_MAX + 34, {0,0,0,0}}, /*XLP_DMC_IDX :144 */
+        {XLP_IRQ_RESERVED_MAX + 34, {0,0,0,0}}, /*XLP_DMC_IDX :145 */
+        {XLP_IRQ_RESERVED_MAX + 35, {0,0,0,0}}, /*XLP_GPIO_IDX(0) :146 */
+        {XLP_IRQ_RESERVED_MAX + 35, {0,0,0,0}}, /*XLP_GPIO_IDX(1) :147 */
+        {XLP_IRQ_RESERVED_MAX + 35, {0,0,0,0}}, /*XLP_GPIO_IDX(2) :148 */
+        {XLP_IRQ_RESERVED_MAX + 35, {0,0,0,0}}, /*XLP_GPIO_IDX(3) :149 */
+        {XLP_IRQ_RESERVED_MAX + 36, {0,0,0,0}}, /*XLP_NOR_IDX :150 */
+        {XLP_IRQ_RESERVED_MAX + 36, {0,0,0,0}}, /*XLP_NAND_IDX :151 */
+        {XLP_IRQ_RESERVED_MAX + 36, {0,0,0,0}}, /*XLP_SPI_IDX :152 */
+        {XLP_IRQ_RESERVED_MAX + 36, {0,0,0,0}}, /*XLP_MMC_IDX :153 */
+        {XLP_IRQ_RESERVED_MAX + 37, {0,0,0,0}}, /*XLP_NBU_IDX :154 */
+        {XLP_IRQ_RESERVED_MAX + 37, {0,0,0,0}}, /*XLP_TCU_IDX :155 */
+        {XLP_IRQ_RESERVED_MAX + 37, {0,0,0,0}}, /*XLP_GCU_IDX :156 */
+        {XLP_IRQ_RESERVED_MAX + 37, {0,0,0,0}}, /*XLP_DMC_IDX(1) :157 */
+        {XLP_IRQ_RESERVED_MAX + 37, {0,0,0,0}}, /*XLP_DMC_IDX(2) :158 */
+        {XLP_IRQ_RESERVED_MAX + 37, {0,0,0,0}}, /*XLP_TCB_IDX :159 */
+};
+
+static struct irq_map_elem irq_map_3xx[XLP_IRQS_PER_NODE] = {
+        {XLP_IRQ_RESERVED_MAX + 1, {0,0,0,0}}, /*XLP_WD_IDX(0) :0 */
+        {XLP_IRQ_RESERVED_MAX + 1, {0,0,0,0}}, /*XLP_WD_IDX(1) :1 */
+        {XLP_IRQ_RESERVED_MAX + 2, {0,0,0,0}}, /*XLP_WD_NMI_IDX(0) :2 */
+        {XLP_IRQ_RESERVED_MAX + 2, {0,0,0,0}}, /*XLP_WD_NMI_IDX(1) :3 */
+        {XLP_IRQ_RESERVED_MAX + 3, {0,0,0,0}}, /*XLP_TIMER_IDX(0): Dedicated systimer	:4 */
+        {XLP_IRQ_RESERVED_MAX + 4, {0,0,0,0}}, /*XLP_TIMER_IDX(1): Dedicated clocksource:5 */
+        {XLP_IRQ_RESERVED_MAX + 4, {0,0,0,0}}, /*XLP_TIMER_IDX(2) :6 */
+        {XLP_IRQ_RESERVED_MAX + 4, {0,0,0,0}}, /*XLP_TIMER_IDX(3) :7 */
+        {XLP_IRQ_RESERVED_MAX + 4, {0,0,0,0}}, /*XLP_TIMER_IDX(4) :8 */
+        {XLP_IRQ_RESERVED_MAX + 4, {0,0,0,0}}, /*XLP_TIMER_IDX(5) :9 */
+        {XLP_IRQ_RESERVED_MAX + 4, {0,0,0,0}}, /*XLP_TIMER_IDX(6) :10 */
+        {XLP_IRQ_RESERVED_MAX + 4, {0,0,0,0}}, /*XLP_TIMER_IDX(7) :11 */
+        {XLP_IRQ_RESERVED_MAX + 5, {0,0,0,0}}, /*XLP_GPIO_IDX(0) :12 */
+        {XLP_IRQ_RESERVED_MAX + 5, {0,0,0,0}}, /*XLP_GPIO_IDX(1) :13 */
+        {XLP_IRQ_RESERVED_MAX + 5, {0,0,0,0}}, /*XLP_GPIO_IDX(2) :14 */
+        {XLP_IRQ_RESERVED_MAX + 5, {0,0,0,0}}, /*XLP_GPIO_IDX(3) :15 */
+        {XLP_IRQ_RESERVED_MAX + 5, {0,0,0,0}}, /*XLP_GPIO_IDX(4) :16 */
+        {XLP_IRQ_RESERVED_MAX + 5, {0,0,0,0}}, /*XLP_GPIO_IDX(5) :17 */
+        {XLP_IRQ_RESERVED_MAX + 5, {0,0,0,0}}, /*XLP_GPIO_IDX(6) :18 */
+        {XLP_IRQ_RESERVED_MAX + 5, {0,0,0,0}}, /*XLP_GPIO_IDX(7) :19 */
+        {XLP_IRQ_RESERVED_MAX + 5, {0,0,0,0}}, /*XLP_GPIO_IDX(8) :20 */
+        {XLP_IRQ_RESERVED_MAX + 5, {0,0,0,0}}, /*XLP_GPIO_IDX(9) :21 */
+        {XLP_IRQ_RESERVED_MAX + 5, {0,0,0,0}}, /*XLP_GPIO_IDX(10) :22 */
+        {XLP_IRQ_RESERVED_MAX + 5, {0,0,0,0}}, /*XLP_GPIO_IDX(11) :23 */
+        {-1, {0,0,0,0}}, /*Reserved: :24 */
+        {-1, {0,0,0,0}}, /*Reserved: :25 */
+        {-1, {0,0,0,0}}, /*Reserved :26 */
+        {-1, {0,0,0,0}}, /*Reserved :27 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(0) :28 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(1) :29 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(2) :30 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(3) :31 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(4) :32 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(5) :33 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(6) :34 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(7) :35 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(8) :36 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(9) :37 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(10) :38 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(11) :39 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(12) :40 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(13) :41 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(14) :42 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(15) :43 */
+        {XLP_IRQ_RESERVED_MAX + 7, {0,0,0,0}}, /*XLP_MSG_IDX(0) :44 */
+        {XLP_IRQ_RESERVED_MAX + 7, {0,0,0,0}}, /*XLP_MSG_IDX(1) :45 */
+        {XLP_IRQ_RESERVED_MAX + 8, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(0) :46 */
+        {XLP_IRQ_RESERVED_MAX + 8, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(1) :47 */
+        {XLP_IRQ_RESERVED_MAX + 8, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(2) :48 */
+        {XLP_IRQ_RESERVED_MAX + 8, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(3) :49 */
+        {XLP_IRQ_RESERVED_MAX + 8, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(4) :50 */
+        {XLP_IRQ_RESERVED_MAX + 8, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(5) :51 */
+        {XLP_IRQ_RESERVED_MAX + 8, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(6) :52 */
+        {XLP_IRQ_RESERVED_MAX + 8, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(7) :53 */
+        {XLP_IRQ_RESERVED_MAX + 9, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(8) :54 */
+        {XLP_IRQ_RESERVED_MAX + 9, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(9) :55 */
+        {XLP_IRQ_RESERVED_MAX + 9, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(10) :56 */
+        {XLP_IRQ_RESERVED_MAX + 9, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(11) :57 */
+        {XLP_IRQ_RESERVED_MAX + 9, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(12) :58 */
+        {XLP_IRQ_RESERVED_MAX + 9, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(13) :59 */
+        {XLP_IRQ_RESERVED_MAX + 9, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(14) :60 */
+        {XLP_IRQ_RESERVED_MAX + 9, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(15) :61 */
+        {XLP_IRQ_RESERVED_MAX + 10, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(16) :62 */
+        {XLP_IRQ_RESERVED_MAX + 10, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(17) :63 */
+        {XLP_IRQ_RESERVED_MAX + 10, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(18) :64 */
+        {XLP_IRQ_RESERVED_MAX + 10, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(19) :65 */
+        {XLP_IRQ_RESERVED_MAX + 10, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(20) :66 */
+        {XLP_IRQ_RESERVED_MAX + 10, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(21) :67 */
+        {XLP_IRQ_RESERVED_MAX + 10, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(22) :68 */
+        {XLP_IRQ_RESERVED_MAX + 10, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(23) :69 */
+        {XLP_IRQ_RESERVED_MAX + 11, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(24) :70 */
+        {XLP_IRQ_RESERVED_MAX + 11, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(25) :71 */
+        {XLP_IRQ_RESERVED_MAX + 11, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(26) :72 */
+        {XLP_IRQ_RESERVED_MAX + 11, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(27) :73 */
+        {XLP_IRQ_RESERVED_MAX + 11, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(28) :74 */
+        {XLP_IRQ_RESERVED_MAX + 11, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(29) :75 */
+        {XLP_IRQ_RESERVED_MAX + 11, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(30) :76 */
+        {XLP_IRQ_RESERVED_MAX + 11, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(31) :77 */
+        {XLP_IRQ_RESERVED_MAX + 12, {0,0,0,0}}, /*XLP_PCIE_LINK_IRQ(0) :78 */
+        {XLP_IRQ_RESERVED_MAX + 13, {0,0,0,0}}, /*XLP_PCIE_LINK_IRQ(1) :79 */
+        {XLP_IRQ_RESERVED_MAX + 14, {0,0,0,0}}, /*XLP_PCIE_LINK_IRQ(2) :80 */
+        {XLP_IRQ_RESERVED_MAX + 15, {0,0,0,0}}, /*XLP_PCIE_LINK_IRQ(3) :81 */
+        {-1, {0,0,0,0}}, /*Reserved :82 */
+        {-1, {0,0,0,0}}, /*Reserved :83 */
+        {-1, {0,0,0,0}}, /*Reserved :84 */
+        {-1, {0,0,0,0}}, /*Reserved :85 */
+        {-1, {0,0,0,0}}, /*Reserved :86 */
+        {-1, {0,0,0,0}}, /*Reserved :87 */
+        {-1, {0,0,0,0}}, /*Reserved :88 */
+        {-1, {0,0,0,0}}, /*Reserved :89 */
+        {-1, {0,0,0,0}}, /*Reserved :90 */
+        {-1, {0,0,0,0}}, /*Reserved :91 */
+        {-1, {0,0,0,0}}, /*Reserved :92 */
+        {-1, {0,0,0,0}}, /*Reserved :93 */
+        {-1, {0,0,0,0}}, /*Reserved :94 */
+        {-1, {0,0,0,0}}, /*Reserved :95 */
+        {-1, {0,0,0,0}}, /*Reserved :96 */
+        {-1, {0,0,0,0}}, /*Reserved :97 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(0) :98 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(1) :99 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(2) :100 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(3) :101 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(4) :102 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(5) :103 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(6) :104 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(7) :105 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(8) :106 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(9) :107 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(10) :108 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(11) :109 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(12) :110 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(13) :111 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(14) :112 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(15) :113 */
+        {XLP_IRQ_RESERVED_MAX + 17, {0,0,0,0}}, /*XLP_POE_IDX :114 */
+        {XLP_IRQ_RESERVED_MAX + 18, {0,0,0,0}}, /*XLP_USB_IDX(0) :115 */
+        {XLP_IRQ_RESERVED_MAX + 18, {0,0,0,0}}, /*XLP_USB_IDX(1) :116 */
+        {XLP_IRQ_RESERVED_MAX + 18, {0,0,0,0}}, /*XLP_USB_IDX(2) :117 */
+        {XLP_IRQ_RESERVED_MAX + 19, {0,0,0,0}}, /*XLP_USB_IDX(3) :118 */
+        {XLP_IRQ_RESERVED_MAX + 19, {0,0,0,0}}, /*XLP_USB_IDX(4) :119 */
+        {XLP_IRQ_RESERVED_MAX + 19, {0,0,0,0}}, /*XLP_USB_IDX(5) :120 */
+        {XLP_IRQ_RESERVED_MAX + 20, {0,0,0,0}}, /*XLP_GDX_IDX :121 */
+        {XLP_IRQ_RESERVED_MAX + 21, {0,0,0,0}}, /*XLP_SEC_IDX :122 */
+        {XLP_IRQ_RESERVED_MAX + 22, {0,0,0,0}}, /*XLP_RSA_IDX :123 */
+        {-1, {0,0,0,0}}, /*Reserved :124 */
+        {-1, {0,0,0,0}}, /*Reserved :125 */
+        {-1, {0,0,0,0}}, /*Reserved :126 */
+        {-1, {0,0,0,0}}, /*Reserved :127 */
+        {-1, {0,0,0,0}}, /*reserved :128 */
+        {-1, {0,0,0,0}}, /*Reserved :129 */
+        {-1, {0,0,0,0}}, /*Reserved :130 */
+        {-1, {0,0,0,0}}, /*Reserved :131 */
+        {-1, {0,0,0,0}}, /*Reserved :132 */
+        {XLP_IRQ_RESERVED_MAX + 23, {0,0,0,0}}, /*XLP_UART_IDX(0) :133 */
+        {XLP_IRQ_RESERVED_MAX + 23, {0,0,0,0}}, /*XLP_UART_IDX(0) :134 */
+        {XLP_IRQ_RESERVED_MAX + 24, {0,0,0,0}}, /*XLP_I2C_IDX(0) :135 */
+        {XLP_IRQ_RESERVED_MAX + 24, {0,0,0,0}}, /*XLP_I2C_IDX(0) :136 */
+        {XLP_IRQ_RESERVED_MAX + 25, {0,0,0,0}}, /*XLP_SYS_IDX(0) :137 */
+        {XLP_IRQ_RESERVED_MAX + 25, {0,0,0,0}}, /*XLP_SYS_IDX(1) :138 */
+        {XLP_IRQ_RESERVED_MAX + 26, {0,0,0,0}}, /*XLP_JTAG_IDX :139 */
+        {XLP_IRQ_RESERVED_MAX + 27, {0,0,0,0}}, /*XLP_PIC_IDX :140 */
+        {XLP_IRQ_RESERVED_MAX + 28, {0,0,0,0}}, /*XLP_REGEX_IDX(0) :141 */
+        {XLP_IRQ_RESERVED_MAX + 28, {0,0,0,0}}, /*XLP_REGEX_IDX(1) :142 */
+        {XLP_IRQ_RESERVED_MAX + 29, {0,0,0,0}}, /*XLP_SATA_IDX :143 */
+        {XLP_IRQ_RESERVED_MAX + 30, {0,0,0,0}}, /*XLP_SRIO_IDX(0) :144 */
+        {XLP_IRQ_RESERVED_MAX + 30, {0,0,0,0}}, /*XLP_SRIO_IDX(0) :145 */
+        {XLP_IRQ_RESERVED_MAX + 30, {0,0,0,0}}, /*XLP_SRIO_IDX(0) :146 */
+        {XLP_IRQ_RESERVED_MAX + 30, {0,0,0,0}}, /*XLP_SRIO_IDX(1) :147 */
+        {XLP_IRQ_RESERVED_MAX + 30, {0,0,0,0}}, /*XLP_SRIO_IDX(2) :148 */
+        {-1, {0,0,0,0}}, /*Reserved :149 */
+        {XLP_IRQ_RESERVED_MAX + 31, {0,0,0,0}}, /*XLP_NOR_IDX :150 */
+        {XLP_IRQ_RESERVED_MAX + 31, {0,0,0,0}}, /*XLP_NAND_IDX :151 */
+        {XLP_IRQ_RESERVED_MAX + 31, {0,0,0,0}}, /*XLP_SPI_IDX :152 */
+        {XLP_IRQ_RESERVED_MAX + 31, {0,0,0,0}}, /*XLP_MMC_IDX :153 */
+        {XLP_IRQ_RESERVED_MAX + 32, {0,0,0,0}}, /*XLP_NBU_IDX :154 */
+        {XLP_IRQ_RESERVED_MAX + 32, {0,0,0,0}}, /*XLP_TCU_IDX :155 */
+        {XLP_IRQ_RESERVED_MAX + 32, {0,0,0,0}}, /*XLP_GCU_IDX :156 */
+        {XLP_IRQ_RESERVED_MAX + 32, {0,0,0,0}}, /*XLP_DMC_IDX(1) :157 */
+        {XLP_IRQ_RESERVED_MAX + 32, {0,0,0,0}}, /*XLP_DMC_IDX(2) :158 */
+        {XLP_IRQ_RESERVED_MAX + 32, {0,0,0,0}}, /*XLP_TCB_IDX :159 */
+};
+
+static struct irq_map_elem irq_map_2xx[XLP_IRQS_PER_NODE] = {
+        {XLP_IRQ_RESERVED_MAX + 1, {0,0,0,0}}, /*XLP_WD_IDX(0):	0 */
+        {XLP_IRQ_RESERVED_MAX + 1, {0,0,0,0}}, /*XLP_WD_IDX(1):	1 */
+        {XLP_IRQ_RESERVED_MAX + 2, {0,0,0,0}}, /*XLP_WD_NMI_IDX(0) :2 */
+        {XLP_IRQ_RESERVED_MAX + 2, {0,0,0,0}}, /*XLP_WD_NMI_IDX(1) :3 */
+        {XLP_IRQ_RESERVED_MAX + 3, {0,0,0,0}}, /*XLP_TIMER_IDX(0): systimer:4 */
+        {XLP_IRQ_RESERVED_MAX + 4, {0,0,0,0}}, /*XLP_TIMER_IDX(1): clocksource:5 */
+        {XLP_IRQ_RESERVED_MAX + 4, {0,0,0,0}}, /*XLP_TIMER_IDX(2) :6 */
+        {XLP_IRQ_RESERVED_MAX + 4, {0,0,0,0}}, /*XLP_TIMER_IDX(3) :7 */
+        {XLP_IRQ_RESERVED_MAX + 4, {0,0,0,0}}, /*XLP_TIMER_IDX(4) :8 */
+        {XLP_IRQ_RESERVED_MAX + 4, {0,0,0,0}}, /*XLP_TIMER_IDX(5) :9 */
+        {XLP_IRQ_RESERVED_MAX + 4, {0,0,0,0}}, /*XLP_TIMER_IDX(6) :10 */
+        {XLP_IRQ_RESERVED_MAX + 4, {0,0,0,0}}, /*XLP_TIMER_IDX(7) :11 */
+        {XLP_IRQ_RESERVED_MAX + 5, {0,0,0,0}}, /*XLP_GPIO_IDX(0) :12 */
+        {XLP_IRQ_RESERVED_MAX + 5, {0,0,0,0}}, /*XLP_GPIO_IDX(1) :13 */
+        {XLP_IRQ_RESERVED_MAX + 5, {0,0,0,0}}, /*XLP_GPIO_IDX(2) :14 */
+        {XLP_IRQ_RESERVED_MAX + 5, {0,0,0,0}}, /*XLP_GPIO_IDX(3) :15 */
+        {XLP_IRQ_RESERVED_MAX + 5, {0,0,0,0}}, /*XLP_GPIO_IDX(4) :16 */
+        {XLP_IRQ_RESERVED_MAX + 5, {0,0,0,0}}, /*XLP_GPIO_IDX(5) :17 */
+        {XLP_IRQ_RESERVED_MAX + 5, {0,0,0,0}}, /*XLP_GPIO_IDX(6) :18 */
+        {XLP_IRQ_RESERVED_MAX + 5, {0,0,0,0}}, /*XLP_GPIO_IDX(7) :19 */
+        {XLP_IRQ_RESERVED_MAX + 5, {0,0,0,0}}, /*XLP_GPIO_IDX(8) :20 */
+        {XLP_IRQ_RESERVED_MAX + 5, {0,0,0,0}}, /*XLP_GPIO_IDX(9) :21 */
+        {XLP_IRQ_RESERVED_MAX + 5, {0,0,0,0}}, /*XLP_GPIO_IDX(10) :22 */
+        {XLP_IRQ_RESERVED_MAX + 5, {0,0,0,0}}, /*XLP_GPIO_IDX(11) :23 */
+        {-1, {0,0,0,0}}, /*Reserved: :24 */
+        {-1, {0,0,0,0}}, /*Reserved: :25 */
+        {-1, {0,0,0,0}}, /*Reserved :26 */
+        {-1, {0,0,0,0}}, /*Reserved :27 */
+        {-1, {0,0,0,0}}, /*XLP_MSGQ_IDX(0) :28 */
+        {-1, {0,0,0,0}}, /*XLP_MSGQ_IDX(1) :29 */
+        {-1, {0,0,0,0}}, /*XLP_MSGQ_IDX(2) :30 */
+        {-1, {0,0,0,0}}, /*XLP_MSGQ_IDX(3) :31 */
+        {-1, {0,0,0,0}}, /*XLP_MSGQ_IDX(4) :32 */
+        {-1, {0,0,0,0}}, /*XLP_MSGQ_IDX(5) :33 */
+        {-1, {0,0,0,0}}, /*XLP_MSGQ_IDX(6) :34 */
+        {-1, {0,0,0,0}}, /*XLP_MSGQ_IDX(7) :35 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(8) :36 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(9) :37 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(10) :38 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(11) :39 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(12) :40 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(13) :41 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(14) :42 */
+        {XLP_IRQ_RESERVED_MAX + 6, {0,0,0,0}}, /*XLP_MSGQ_IDX(15) :43 */
+        {XLP_IRQ_RESERVED_MAX + 7, {0,0,0,0}}, /*XLP_MSG_IDX(0) :44 */
+        {XLP_IRQ_RESERVED_MAX + 7, {0,0,0,0}}, /*XLP_MSG_IDX(1) :45 */
+        {XLP_IRQ_RESERVED_MAX + 8, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(0) :46 */
+        {XLP_IRQ_RESERVED_MAX + 8, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(1) :47 */
+        {XLP_IRQ_RESERVED_MAX + 8, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(2) :48 */
+        {XLP_IRQ_RESERVED_MAX + 8, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(3) :49 */
+        {XLP_IRQ_RESERVED_MAX + 8, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(4) :50 */
+        {XLP_IRQ_RESERVED_MAX + 8, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(5) :51 */
+        {XLP_IRQ_RESERVED_MAX + 8, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(6) :52 */
+        {XLP_IRQ_RESERVED_MAX + 8, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(7) :53 */
+        {XLP_IRQ_RESERVED_MAX + 9, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(8) :54 */
+        {XLP_IRQ_RESERVED_MAX + 9, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(9) :55 */
+        {XLP_IRQ_RESERVED_MAX + 9, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(10) :56 */
+        {XLP_IRQ_RESERVED_MAX + 9, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(11) :57 */
+        {XLP_IRQ_RESERVED_MAX + 9, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(12) :58 */
+        {XLP_IRQ_RESERVED_MAX + 9, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(13) :59 */
+        {XLP_IRQ_RESERVED_MAX + 9, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(14) :60 */
+        {XLP_IRQ_RESERVED_MAX + 9, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(15) :61 */
+        {XLP_IRQ_RESERVED_MAX + 10, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(16) :62 */
+        {XLP_IRQ_RESERVED_MAX + 10, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(17) :63 */
+        {XLP_IRQ_RESERVED_MAX + 10, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(18) :64 */
+        {XLP_IRQ_RESERVED_MAX + 10, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(19) :65 */
+        {XLP_IRQ_RESERVED_MAX + 10, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(20) :66 */
+        {XLP_IRQ_RESERVED_MAX + 10, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(21) :67 */
+        {XLP_IRQ_RESERVED_MAX + 10, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(22) :68 */
+        {XLP_IRQ_RESERVED_MAX + 10, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(23) :69 */
+        {XLP_IRQ_RESERVED_MAX + 11, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(24) :70 */
+        {XLP_IRQ_RESERVED_MAX + 11, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(25) :71 */
+        {XLP_IRQ_RESERVED_MAX + 11, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(26) :72 */
+        {XLP_IRQ_RESERVED_MAX + 11, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(27) :73 */
+        {XLP_IRQ_RESERVED_MAX + 11, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(28) :74 */
+        {XLP_IRQ_RESERVED_MAX + 11, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(29) :75 */
+        {XLP_IRQ_RESERVED_MAX + 11, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(30) :76 */
+        {XLP_IRQ_RESERVED_MAX + 11, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(31) :77 */
+        {XLP_IRQ_RESERVED_MAX + 12, {0,0,0,0}}, /*XLP_PCIE_LINK_IRQ(0) :78 */
+        {XLP_IRQ_RESERVED_MAX + 13, {0,0,0,0}}, /*XLP_PCIE_LINK_IRQ(1) :79 */
+        {XLP_IRQ_RESERVED_MAX + 14, {0,0,0,0}}, /*XLP_PCIE_LINK_IRQ(2) :80 */
+        {XLP_IRQ_RESERVED_MAX + 15, {0,0,0,0}}, /*XLP_PCIE_LINK_IRQ(3) :81 */
+        {-1, {0,0,0,0}}, /*Reserved :82 */
+        {-1, {0,0,0,0}}, /*Reserved :83 */
+        {-1, {0,0,0,0}}, /*Reserved :84 */
+        {-1, {0,0,0,0}}, /*Reserved :85 */
+        {-1, {0,0,0,0}}, /*Reserved :86 */
+        {-1, {0,0,0,0}}, /*Reserved :87 */
+        {-1, {0,0,0,0}}, /*Reserved :88 */
+        {-1, {0,0,0,0}}, /*Reserved :89 */
+        {-1, {0,0,0,0}}, /*Reserved :90 */
+        {-1, {0,0,0,0}}, /*Reserved :91 */
+        {-1, {0,0,0,0}}, /*Reserved :92 */
+        {-1, {0,0,0,0}}, /*Reserved :93 */
+        {-1, {0,0,0,0}}, /*Reserved :94 */
+        {-1, {0,0,0,0}}, /*Reserved :95 */
+        {-1, {0,0,0,0}}, /*Reserved :96 */
+        {-1, {0,0,0,0}}, /*Reserved :97 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(0) :98 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(1) :99 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(2) :100 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(3) :101 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(4) :102 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(5) :103 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(6) :104 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(7) :105 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(8) :106 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(9) :107 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(10) :108 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(11) :109 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(12) :110 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(13) :111 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(14) :112 */
+        {XLP_IRQ_RESERVED_MAX + 16, {0,0,0,0}}, /*XLP_NAE_IDX(15) :113 */
+        {XLP_IRQ_RESERVED_MAX + 17, {0,0,0,0}}, /*XLP_POE_IDX :114 */
+        {XLP_IRQ_RESERVED_MAX + 18, {0,0,0,0}}, /*XLP_USB_IDX(0) :115 */
+        {XLP_IRQ_RESERVED_MAX + 18, {0,0,0,0}}, /*XLP_USB_IDX(1) :116 */
+        {XLP_IRQ_RESERVED_MAX + 18, {0,0,0,0}}, /*XLP_USB_IDX(2) :117 */
+        {-1, {0,0,0,0}}, /*XLP_USB_IDX(3) :118 */
+        {-1, {0,0,0,0}}, /*XLP_USB_IDX(4) :119 */
+        {-1, {0,0,0,0}}, /*XLP_USB_IDX(5) :120 */
+        {XLP_IRQ_RESERVED_MAX + 20, {0,0,0,0}}, /*XLP_GDX_IDX :121 */
+        {XLP_IRQ_RESERVED_MAX + 21, {0,0,0,0}}, /*XLP_SEC_IDX :122 */
+        {XLP_IRQ_RESERVED_MAX + 22, {0,0,0,0}}, /*XLP_RSA_IDX :123 */
+        {XLP_IRQ_RESERVED_MAX + 23, {0,0,0,0}}, /*Reserved :124 */
+        {XLP_IRQ_RESERVED_MAX + 24, {0,0,0,0}}, /*Reserved :125 */
+        {XLP_IRQ_RESERVED_MAX + 24, {0,0,0,0}}, /*Reserved :126 */
+        {XLP_IRQ_RESERVED_MAX + 24, {0,0,0,0}}, /*Reserved :127 */
+        {XLP_IRQ_RESERVED_MAX + 24, {0,0,0,0}}, /*reserved :128 */
+        {-1, {0,0,0,0}}, /*Reserved :129 */
+        {-1, {0,0,0,0}}, /*Reserved :130 */
+        {-1, {0,0,0,0}}, /*Reserved :131 */
+        {-1, {0,0,0,0}}, /*Reserved :132 */
+        {XLP_IRQ_RESERVED_MAX + 25, {0,0,0,0}}, /*XLP_UART_IDX(0) :133 */
+        {XLP_IRQ_RESERVED_MAX + 25, {0,0,0,0}}, /*XLP_UART_IDX(1) :134 */
+        {XLP_IRQ_RESERVED_MAX + 26, {0,0,0,0}}, /*XLP_I2C_IDX(0) :135 */
+        {XLP_IRQ_RESERVED_MAX + 26, {0,0,0,0}}, /*XLP_I2C_IDX(1) :136 */
+        {XLP_IRQ_RESERVED_MAX + 27, {0,0,0,0}}, /*XLP_SYS_IDX(2) :137 */
+        {XLP_IRQ_RESERVED_MAX + 27, {0,0,0,0}}, /*XLP_SYS_IDX(3) :138 */
+        {XLP_IRQ_RESERVED_MAX + 28, {0,0,0,0}}, /*XLP_JTAG_IDX :139 */
+        {XLP_IRQ_RESERVED_MAX + 29, {0,0,0,0}}, /*XLP_PIC_IDX :140 */
+        {XLP_IRQ_RESERVED_MAX + 30, {0,0,0,0}}, /*XLP_REGEX_IDX(0) :141 */
+        {XLP_IRQ_RESERVED_MAX + 30, {0,0,0,0}}, /*XLP_REGEX_IDX(1) :142 */
+        {XLP_IRQ_RESERVED_MAX + 31, {0,0,0,0}}, /*XLP_SATA_IDX :143 */
+        {XLP_IRQ_RESERVED_MAX + 32, {0,0,0,0}}, /*XLP_SRIO_IDX(0) :144 */
+        {XLP_IRQ_RESERVED_MAX + 32, {0,0,0,0}}, /*XLP_SRIO_IDX(1) :145 */
+        {XLP_IRQ_RESERVED_MAX + 32, {0,0,0,0}}, /*XLP_SRIO_IDX(2) :146 */
+        {-1, {0,0,0,0}}, /*Reserved :147 */
+        {-1, {0,0,0,0}}, /*Reserved :148 */
+        {-1, {0,0,0,0}}, /*Reserved :149 */
+        {XLP_IRQ_RESERVED_MAX + 31, {0,0,0,0}}, /*XLP_NOR_IDX :150 */
+        {XLP_IRQ_RESERVED_MAX + 31, {0,0,0,0}}, /*XLP_NAND_IDX :151 */
+        {XLP_IRQ_RESERVED_MAX + 31, {0,0,0,0}}, /*XLP_SPI_IDX :152 */
+        {XLP_IRQ_RESERVED_MAX + 31, {0,0,0,0}}, /*XLP_MMC_IDX :153 */
+        {XLP_IRQ_RESERVED_MAX + 32, {0,0,0,0}}, /*XLP_NBU_IDX :154 */
+        {XLP_IRQ_RESERVED_MAX + 32, {0,0,0,0}}, /*XLP_TCU_IDX :155 */
+        {XLP_IRQ_RESERVED_MAX + 32, {0,0,0,0}}, /*XLP_GCU_IDX :156 */
+        {XLP_IRQ_RESERVED_MAX + 32, {0,0,0,0}}, /*XLP_DMC_IDX(1) :157 */
+        {XLP_IRQ_RESERVED_MAX + 32, {0,0,0,0}}, /*XLP_DMC_IDX(2) :158 */
+        {XLP_IRQ_RESERVED_MAX + 32, {0,0,0,0}}, /*XLP_TCB_IDX :159 */
+};
+
+static void fill_rvec_map(struct irq_map_elem *map, int mapsize, struct rvec_map_elem *rvec_map, int rsize)
+{
+	int i, last = -1, r;
+	struct rvec_map_elem e_r = {-1, {0, 0, 0, 0}};
+	for (r = 0; r < rsize; r++) {
+		memcpy(&rvec_map[r], &e_r, sizeof(struct rvec_map_elem));
+	}
+	for (i = 0; i < mapsize; i++) {
+		if (map[i].rvec == -1) {
+			last = -1;
+			continue;
+		} else if (map[i].rvec != last) {
+			int tmp = map[i].rvec;
+			if (rvec_map[tmp].irt != -1) continue;
+			if (tmp >= rsize) {
+				printk("Index OOB while setting up rmap\n");
+				continue;
+			}
+			rvec_map[tmp].irt = i;
+			printk("rvec[%d] = %d\n", tmp, rvec_map[tmp].irt);
+			last = map[i].rvec;
+		}
+	}
+}
+
+/*
+ * returns rvec from an IRQ entry
+ * An IRQ entry is (irt table index + reserved max)
+ *
+ * @irq : irq number
+ */
+int xlp_rvec_from_irq(int irq)
+{
+	irq %= XLP_IRQS_PER_NODE;
+	if (irq < XLP_IRQ_RESERVED_MAX){
+		return -EINVAL;
+	}
+	return(xlp_irq_map[irq - XLP_IRQ_RESERVED_MAX].rvec);
+}
+
+int __xlp_modify_irq_bitmap(struct pic_dev *pic, u32 irq, u64 type)
+{
+	int rvec = xlp_rvec_from_irq(irq);
+	int lirt = xlp_irq_to_irt(irq);
+	int idx;
+
+	idx = lirt - xlp_rvec_map[rvec].irt;
+	if (type == UPIC_PARAM_SETBIT) {
+		set_bit(idx, &(xlp_rvec_map[rvec].bitmap[pic->node]));
+	} else if (type == UPIC_PARAM_CLEARBIT) {
+		clear_bit(idx, &(xlp_rvec_map[rvec].bitmap[pic->node]));
+	}
+	return 0;
+}
+
+int xlp_get_irq_base_bitmap(struct pic_dev *pic, u32 rvec, u32 *irq, u64 *bitmap)
+{
+	int lirq;
+	unsigned long flags;
+
+	if (rvec >= XLP_EIRR_SIZE) {
+		return -EINVAL;
+	}
+	lirq = xlp_rvec_map[rvec].irt + XLP_IRQ_RESERVED_MAX;
+	if (lirq < 0) {
+		return -EINVAL;
+	}
+	if (irq) *irq = lirq;
+	spin_lock_irqsave(&pic->pic_lock, flags);
+	if (bitmap) *bitmap = xlp_rvec_map[rvec].bitmap[pic->node];
+	spin_unlock_irqrestore(&pic->pic_lock, flags);
+	return 0;
+}
+
+/* This function will modify the underlying platform specific irq_map
+ * so as to suit the actual h/w platform
+ *
+ * Currently xlp8xx, xlp3xx are supported.
+ */
+static void xlp_setup_irqmap(struct irq_map_elem **map, struct rvec_map_elem *rvec_map)
+{
+	/* check if xlp8xx */
+	if (is_nlm_xlp8xx()) {
+		*map = irq_map_8xx;
+	} else if (is_nlm_xlp3xx()) {
+		*map = irq_map_3xx;
+#if 0
+	} else if (is_nlm_xlp2xx()) {
+		*map = irq_map_2xx;
+#endif
+	} else {
+		printk("IRQMOD: Using default irq map\n");
+	}
+	fill_rvec_map(*map, XLP_IRT_NUM, rvec_map, XLP_EIRR_SIZE);
+}
+
+void xlp_init_irqmap(struct pic_dev *pic)
+{
+	xlp_setup_irqmap(&xlp_irq_map, xlp_rvec_map);
+}
+
+/*
+ * Masks out one IRQ in the EIMR register
+ * Must NOT be called with xlp_pic_lock held
+ * @irq : IRQ number
+ */
+void __xlp_irq_mask(struct pic_dev *pic, unsigned int irq)
+{
+	int rvec;
+
+	rvec = xlp_rvec_from_irq(irq);
+	if (rvec < 0) {
+		return;
+	}
+	if (read_64bit_cp0_eimr() & (1ULL << rvec)) {
+		/* We do not clear eimr, this is a TODO for later time */
+		//on_each_cpu(xlp_clear_eimr, (void *) (1ULL << rvec), 1);
+	}
+	return;
+}
+
+static void xlp_set_eimr(void *param)
+{
+	u64 bitmask = (u64) param;
+	u64 eimr;
+
+	eimr = read_64bit_cp0_eimr();
+	eimr |= bitmask;
+	write_64bit_cp0_eimr(eimr);
+	return;
+}
+
+/*
+ * Clear some eimr bits on each cpu
+ * This function will be called on each cpu by on_each_cpu()
+ * @bitmask	: bitmask to clear in EIMR
+ */
+static void xlp_clear_eimr(void *param)
+{
+	u64 bitmask = (u64) param;
+	u64 eimr = read_64bit_cp0_eimr();
+	eimr &= ~bitmask;
+	write_64bit_cp0_eimr(eimr);
+	return;
+}
+
+
+/*
+ * Changes eimr bit value corresponding to IRT
+ * @irq : IRQ number
+ * TODO : Need to find a method to send messages to a subset of cpus as
+ * target for this vector is a node and only the cpus in that node should
+ * change eimr
+ */
+void __xlp_irq_unmask(struct pic_dev *pic, int irq)
+{
+	int rvec = xlp_rvec_from_irq(irq);
+
+	if (rvec < 0) {
+		return;
+	} else if (((1ULL << rvec) & read_64bit_cp0_eimr()) == 0) {
+		/* This is only for those interrupts which are not statically
+		 * set in EIMR. Could dump stack if spin lock held */
+		 on_each_cpu(xlp_set_eimr, (void *) (1ULL << rvec), 1);
+	}
+	return;
+}
+
+/*
+ * Startup function for any IRQ
+ * @irq: irq number
+ *
+ * When an interrupt is started, we force it to be enabled only in cpu0, it can
+ * be changed later by calling nlm_irq_set_affinity()
+ */
+int xlp_irq_startup(struct pic_dev *pic, unsigned int oirq)
+{
+	__label__ __failure;
+	int ret = 0;
+	unsigned long flags;
+	struct cpumask m;
+	int nirq = oirq % XLP_IRQS_PER_NODE;
+
+	cpumask_clear(&m);
+	cpumask_set_cpu(NLM_MAX_CPU_PER_NODE * pic->node, &m);
+	spin_lock_irqsave(&xlp_irq_lock, flags);
+	if (xlp_irq_map[nirq].usage[pic->node] == 0) {
+		/* Currently unused => not enabled. So, setup and enable */
+		ret = pic->plat_request_irq(pic, nirq, UPIC_DTYPE_ITE, &m,NULL);
+		if (ret != 0) {
+			printk(KERN_WARNING "Failed to setup IRQ %d\n", nirq);
+			goto __failure;
+		}
+		xlp_irq_map[nirq].usage[pic->node]++;
+		/* At this point, make sure that each CPU has eimr bit
+		 * corresponding to this IRQ set. Later the driver can set
+		 * the cpu affinity of this interrupt. The rationale for
+		 * setting up EIMR here is that it can be moved to any CPUs
+		 * (well, a subset of any CPUs) later
+		 */
+		__xlp_irq_unmask(pic, nirq);
+	} else if (xlp_irq_map[nirq].usage[pic->node] > 0) {
+		/* already being used. No need to check mask
+		 * if masked, will be unmasked later
+		 */
+		xlp_irq_map[nirq].usage[pic->node]++;
+		ret = 0;
+	} else {
+		pr_err("Error irq = %d, rvec = %d, usage count %d\n", nirq,
+				xlp_irq_map[nirq].rvec, xlp_irq_map[nirq].usage[pic->node]);
+		ret = -EFAULT;
+	}
+__failure:
+	spin_unlock_irqrestore(&xlp_irq_lock, flags);
+	return ret;
+}
+
+/*
+ * IRQ shut down function
+ * Disables one IRQ
+ *
+ * @irq : irq to shut down
+ *
+ * This function is called whenever release_irq() is called by means of
+ * chip->shutdown(). In this function, the rvec bit in every EIMR is cleared if
+ * usage falls to zero (in case of shared interrupts)
+ */
+void xlp_irq_shutdown(struct pic_dev *pic, unsigned int irq)
+{
+	unsigned long flags;
+	int rvec;
+
+	spin_lock_irqsave(&xlp_irq_lock, flags);
+	if (xlp_irq_map[irq].usage[pic->node] == 0) {
+		spin_unlock_irqrestore(&xlp_irq_lock, flags);
+		return;
+	} else if (xlp_irq_map[irq].usage[pic->node] > 0) {
+		xlp_irq_map[irq].usage[pic->node]--;
+	}
+	if ((rvec = xlp_rvec_from_irq(irq)) < 0) {
+		return;
+	}
+	/* If the usage reaches zero as a result of above subtraction,
+	 * free up the rvec */
+	if (xlp_irq_map[irq].usage[pic->node] == 0) {
+		pic->plat_release_irq(pic, irq, UPIC_DTYPE_ITE);
+		spin_unlock_irqrestore(&xlp_irq_lock, flags);
+		__xlp_irq_mask(pic, irq); /* masks this IRQ */
+	} else {
+		spin_unlock_irqrestore(&xlp_irq_lock, flags);
+	}
+	return;
+}
+
diff --git a/arch/mips/netlogic/xlp/pic/xlp_pic.c b/arch/mips/netlogic/xlp/pic/xlp_pic.c
new file mode 100644
index 0000000..f679210
--- /dev/null
+++ b/arch/mips/netlogic/xlp/pic/xlp_pic.c
@@ -0,0 +1,540 @@
+#include "xlp_pic.h"
+#include <asm/netlogic/pic_hal.h>
+#include <asm/netlogic/xlp.h>
+#include <asm/netlogic/xlp_irq_map.h>
+#define XLP_ITE_ENTRIES		8
+
+/*
+ * xlp_ites[node][0-3] = {0x1, 0xffffffff, 0x0000ffff, 0xffff0000};//local only
+ */
+static struct cpumask xlp_ites[NLM_MAX_NODES][XLP_ITE_ENTRIES];
+
+static void dump_all_ites(void)
+{
+	u8 node, i;
+	char buf[140];
+
+	for_each_online_node(node) {
+	for (i = 0; i < XLP_ITE_ENTRIES; i++) {
+		cpumask_scnprintf(buf, 140, &xlp_ites[node][i]);
+		printk(KERN_DEBUG "node %d: Supported CPUMASK (%d) -> %s\n",
+				node, i, buf);
+	}
+	}
+	return;
+}
+
+/* This function sets the given ITE's cpu bit on node node.
+ *
+ * @node	: node on which ITE is to be set
+ * @cpu		: target cpu id
+ * @ite		: ITE index
+ * @bitval	: 0 to clear, 1 to set
+ */
+static void xlp_ite_cpu_op(struct pic_dev *pic, u8 cpu, u8 ite, u8 bitval)
+{
+	unsigned long flags;
+	u64 val, reg;
+	u8 bit;
+
+	/* No param checking, must be checked before calling.
+	 * Target cpu id decices whether to use THREADEN01 or THREADEN23
+	 * i.e., if target cpu < 64, use THREADEN01 as base else THREADEN23 */
+	reg = (cpu < 64) ? XLP_PIC_INT_THREADEN01(ite) :
+					XLP_PIC_INT_THREADEN23(ite);
+	bit = cpu % 64;
+	spin_lock_irqsave(&pic->pic_lock, flags);
+	val = __pic_r64r(pic->base, reg);
+	val = (bitval == 0) ?  (val & ~(1ULL << bit)) : (val | (1ULL << bit));
+	__pic_w64r(pic->base, reg, val);
+	spin_unlock_irqrestore(&pic->pic_lock, flags);
+}
+
+#define xlp_ite_cpu_set(pic,cpu,ite) xlp_ite_cpu_op(pic,cpu,ite,1)
+#define xlp_ite_cpu_clear(pic,cpu,ite) xlp_ite_cpu_op(pic,cpu,ite,0)
+
+/* This function would program ITE values on node given by the cpumask
+ * @cpumask	: cpumask to program on ITE
+ * @node	: node on which ITE should be programmed
+ * @ite		: ITE to program
+ * @scope	: program ITE only on the given node (0) or all nodes (1)
+ */
+static void xlp_cpumask_to_node_ite(const struct cpumask *m,
+		struct pic_dev *pic, u8 ite, u8 scope)
+{
+	__label__ prog_all;
+	struct cpumask t;
+	int cpu = (pic->node * NLM_MAX_CPU_PER_NODE), last;
+
+	if (scope != 0) goto prog_all;
+
+	/* When the scope is 0, program node ITEs with target as
+	 * local cpus only */
+	last = cpu + NLM_MAX_CPU_PER_NODE - 1;
+	if (last >= NR_CPUS) return;
+	cpumask_and(&t, m, &phys_cpu_present_map);
+	for (; cpu <= last; cpu++) {
+		cpumask_test_cpu(cpu, &t) ? xlp_ite_cpu_set(pic, cpu, ite) : xlp_ite_cpu_clear(pic, cpu, ite);
+	}
+	return;
+prog_all:
+	/* Here we program the specified ITE in all nodes with the cpumask
+	 * passed. */
+	/* TBD TODO */
+	return;
+}
+
+/* Once all CPUs are up, walk through the node mask and program all
+ * ITEs in the PICs */
+void xlp_prog_all_node_ites(void)
+{
+	u8 i, node;
+	struct pic_dev *pic;
+
+	dump_all_ites();
+	for_each_online_node(node) {
+		if (retrieve_node_pic_dev(node, &pic) < 0) {
+			panic("Cannot retrieve node(%d) pic dev", node);
+		}
+		for (i = 0; i < XLP_ITE_ENTRIES; i++) {
+			/* 4 is the ITE that redirects int.s to all cpus */
+			xlp_cpumask_to_node_ite(&xlp_ites[node][i], pic, i, (i == 4));
+		}
+	}
+}
+
+/* Checks if a mask spans multiple nodes
+ *
+ * @mask	: cpumask to check for multiple node span
+ */
+static int xlp_span_multiple_nodes(const struct cpumask *mask)
+{
+	int l, f;
+	f = cpumask_first(mask);
+	l = find_last_bit(cpumask_bits(mask), NR_CPUS);
+	if ((f/NLM_MAX_CPU_PER_NODE) != (l/NLM_MAX_CPU_PER_NODE)) {
+		printk(KERN_DEBUG "Mask spans from cpu %#x to %#x. Spans across nodes are not supported\n", f, l);
+		return -EINVAL;
+	}
+	return 0;
+}
+
+/*
+ * In XLP cpu mask for setting affinity of an interrupt cannot span multiple
+ * nodes. Although this is not a h/w restriction, the effort to implement
+ * this feature does not justify the potential benefit; not only that handling
+ * non local interrupts are slightly slower, it could be expensive in terms of
+ * memory access and other resource utilization
+ *
+ * @node	: node to which mask `mask` to be restricted
+ * @mask	: mask to restrict
+ * @m1		: restricted mask
+ */
+static void constrict_mask_to_node(u8 node, struct cpumask *dst, const struct cpumask *src)
+{
+	char buf[140];
+	int i;
+
+	if (!dst || !src) return;
+	cpumask_clear(dst);
+	cpumask_scnprintf(buf, 140, src);
+	//printk(KERN_DEBUG "SRC cpumask %s\n", buf);
+	for (i = NLM_MAX_CPU_PER_NODE * node;
+			i < (NLM_MAX_CPU_PER_NODE *(node + 1)); i++) {
+		cpumask_set_cpu(i, dst);
+	}
+	cpumask_scnprintf(buf, 140, dst);
+	cpumask_and(dst, dst, &phys_cpu_present_map);
+	cpumask_and(dst, dst, src);
+	cpumask_scnprintf(buf, 140, dst);
+	//printk(KERN_DEBUG "DST cpumask %s\n", buf);
+	return;
+}
+
+
+/*
+ * This function returns closest match cpumask among the supported bitmasks
+ * in XLP
+ * Logic is moot, need to improve it later.
+ *
+ * @m	: user supplied cpumask
+ */
+static int xlp_closest_match_cpumask(u8 node, const struct cpumask *m)
+{
+	int i;
+	struct cpumask t, a;
+
+	if (!m) return 1;
+	/* Check with current online physical cpu mask */
+	cpumask_and(&a, m, &phys_cpu_present_map);
+	constrict_mask_to_node(node, &t, &a);
+	cpumask_clear(&a);
+	for (i = 0; i < XLP_ITE_ENTRIES; i++) {
+		cpumask_and(&a, &xlp_ites[node][i], &phys_cpu_present_map);
+		if (cpumask_equal(&t, &a)) {
+			return i;
+		}
+	}
+	/*printk(KERN_WARNING "Could not find a match for specified cpumask\n");*/
+	return 1; /* if no match, point to all local cpus */
+}
+
+/* helper function to create cpumask from unsigned long
+ * Easiest way is to create it directly using bitmap_copy.
+ * For some reason, this was not successful.
+ *
+ * @m	: cpumask pointer to populate. Lower 32 bits must be 0
+ * @u	: u32 variable pointer with bitmask
+ */
+static void u32_to_cpumask(struct cpumask *m, u32 bm)
+{
+	u8 bit = 0;
+	char buf[140];
+
+	/* should not clear the mask passed */
+	for ( ; bit < sizeof(u32) * BITS_PER_BYTE; bit++) {
+		if (bm & (1 << bit)) {
+			cpumask_set_cpu(bit, m);
+		} else {
+			cpumask_clear_cpu(bit, m);
+		}
+	}
+	cpumask_scnprintf(buf, 140, m);
+	fdebug("%#x => CPUMASK %s\n", bm, buf);
+}
+/*
+ * Initializes PIC ITE entries PRM 9.5.6.26
+ * XLP restricts CPU affinity to 8 groups. Though configurable, they are
+ * programmed to have the following patterns.
+ * 0 =>	Only 0th cpu on the node
+ * 1 => All local threads in node; mask = (0xffffffff) on node
+ * 2 => cpu0-15 on node; mask = 0x0000ffff & online_cpu_mask on nodes
+ * 3 => cpu15-31 on node; mask = 0xffff0000 & online_cpu_mask on node
+ * 4 => All cpus on all nodes; i.e.,
+ * mask = (0xffffffff_ffffffff_ffffffff_ffffffff & physical online cpu map)
+ * These are programmer defined groups and can be changed as warranted.
+ *
+ * There is a major issue that needs addressing when run in multi node mode
+ * Number of nodes must be determined and programmed correctly, if a bit in ITE
+ * is programmed without physical thread being present, when interrupt is
+ * dispatched to that CPU under global scheme, system would hang. Thus this
+ * scenario should be avoided. That is why phys_cpu_present_map is used
+ *
+ * This function simply initializes the xlp_ites entries with proposed
+ * CPUmasks.  */
+static void xlp_ites_init(void)
+{
+	u64 bm = 0x1;
+	u8 node;
+	struct cpumask m;
+
+	cpumask_clear(&m);
+	for_each_online_node(node) {
+	/* Simply set the static pattern in all */
+	bm = 1;
+	u32_to_cpumask(&xlp_ites[node][0], bm);
+	cpumask_shift_left(&xlp_ites[node][0], &xlp_ites[node][0], NLM_MAX_CPU_PER_NODE * node); /* directs only to cpu0 of node `node` */
+
+	bm = 0xffffffff;
+	u32_to_cpumask(&xlp_ites[node][1], bm);
+	cpumask_shift_left(&xlp_ites[node][1], &xlp_ites[node][1], NLM_MAX_CPU_PER_NODE * node); /* directs to all cpus of node `node` */
+	cpumask_or(&m, &m, &xlp_ites[node][1]);
+
+	bm = 0xffff;
+	u32_to_cpumask(&xlp_ites[node][2], bm);
+	cpumask_shift_left(&xlp_ites[node][2], &xlp_ites[node][2], NLM_MAX_CPU_PER_NODE * node); /* directs to specified cpus of node `node` */
+
+	bm = 0xffff0000;
+	u32_to_cpumask(&xlp_ites[node][3], bm);
+	cpumask_shift_left(&xlp_ites[node][3], &xlp_ites[node][3], NLM_MAX_CPU_PER_NODE * node); /* directs to specified cpus of node `node` */
+	}
+	for_each_online_node(node) {
+		cpumask_copy(&xlp_ites[node][4], &m);
+	}
+	dump_all_ites();
+}
+
+/* implementation of get_irq_affinity
+ *
+ * This function will find out the closest matching cpu affinity supported
+ * by the h/w to the passed mask and set it in the `out` parameter
+ *
+ * @pic		: pic device structure
+ * @in		: passed cpu mask
+ */
+static int xlp_get_irq_affinity(struct pic_dev *pic, int irq, struct cpumask *m)
+{
+	unsigned long flags;
+	int irt = xlp_irq_to_irt(irq);
+	int ite = -1;
+	u64 val;
+
+	spin_lock_irqsave(&pic->pic_lock, flags);
+	val = __pic_r64r(pic->base, XLP_PIC_IRT_ENTRY(irt));
+	if ((val & XLP_IRTENT_DT) == 0) {
+		ite = (val >> 16) & 7;
+	}
+	spin_unlock_irqrestore(&pic->pic_lock, flags);
+	if (ite < 0) {
+		return -EFAULT;
+	}
+	cpumask_clear(m);
+	cpumask_copy(m, &xlp_ites[pic->node][ite]);
+	return 0;
+}
+
+static int xlp_get_closest_mask(struct pic_dev *pic,
+		const struct cpumask *in, struct cpumask *out)
+{
+	u8 ite_index;
+
+	if (xlp_span_multiple_nodes(in) < 0) {
+		return -EINVAL;
+	}
+	ite_index = xlp_closest_match_cpumask(pic->node, in);
+	if (out) cpumask_copy(out, &xlp_ites[pic->node][ite_index]);
+	return ite_index;
+}
+
+/* Implementation of set_irq_affinity
+ *
+ * This function will find out the closest matched cpu affinity supported
+ * in the h/w to the passed mask and set it accordingly in the IRT entry
+ * corresponding to IRQ# passed
+ *
+ * @pic		: pic device structure
+ * @irq		: irq number
+ * @in		: passed cpumask
+ * @out		: actual set cpu mask
+ */
+static int xlp_set_irq_affinity(struct pic_dev *pic, int irq, u64 type,
+		const struct cpumask *in, struct cpumask *out)
+{
+	__label__ unsupported;
+	u8 ite;
+	u64 val;
+	unsigned long flags;
+	int irt = xlp_irq_to_irt(irq), ret = 0;
+
+	if (type != UPIC_AFFINITY_ITE) { /* We don't do DTE now */
+		return -EINVAL;
+	}
+	ite = xlp_get_closest_mask(pic, in, out);
+	//dump_all_ites();
+	spin_lock_irqsave(&pic->pic_lock, flags);
+	val = __pic_r64r(pic->base, XLP_PIC_IRT_ENTRY(irt));
+	if (val & XLP_IRTENT_DT) {
+		ret = -ENODEV;
+		goto unsupported;
+	}
+	val &= ~(0x7 << 16);
+	val |= XLP_IRTENT_DB(ite);
+	__pic_w64r(pic->base, XLP_PIC_IRT_ENTRY(irt), val);
+unsupported:
+	spin_unlock_irqrestore(&pic->pic_lock, flags);
+	return ret;
+}
+
+/*
+ * Implementation of plat_request_irq
+ * This function will take irq number and two cpu mask pointers.
+ * `in` is what is expected mask, `out` is what is actual
+ *
+ * @pic		: pic device structure
+ * @irq		: irq number
+ * @in		: cpumask that caller requests
+ * @out		: cpumask that platform granted
+ *
+ * returns : 0 if success, -ve if failure
+ * In any case, *out might be changed.
+ */
+static int xlp_request_irq(struct pic_dev *pic, int irq, u64 type,
+		const struct cpumask *in, struct cpumask *out)
+{
+	u32 idx;
+	u64 val;
+	int irt = xlp_irq_to_irt(irq);
+	int rvec = xlp_rvec_from_irq(irq);
+	unsigned long flags;
+
+	if ((UPIC_DTYPE_MASK & type) == UPIC_DTYPE_DTE) {
+		return -EINVAL;
+	}
+	idx = xlp_get_closest_mask(pic, in, out);
+	spin_lock_irqsave(&pic->pic_lock, flags);
+	val = __pic_r64r(pic->base, XLP_PIC_IRT_ENTRY(irt));
+	val |= (XLP_IRTENT_ENABLE | XLP_IRTENT_SCH_LCL | XLP_IRTENT_RVEC(rvec)
+			| XLP_IRTENT_DB(idx));
+	/* Do book keeping now, with spin lock held */
+	__xlp_modify_irq_bitmap(pic, irq, UPIC_PARAM_SETBIT);
+	__pic_w64r(pic->base, XLP_PIC_IRT_ENTRY(irt), val);
+	spin_unlock_irqrestore(&pic->pic_lock, flags);
+	return 0;
+}
+
+static int xlp_release_irq(struct pic_dev *pic, int irq, u64 type)
+{
+	u64 val;
+	int irt = xlp_irq_to_irt(irq);
+	unsigned long flags;
+
+	spin_lock_irqsave(&pic->pic_lock, flags);
+	__xlp_modify_irq_bitmap(pic, irq, UPIC_PARAM_CLEARBIT);
+	val = __pic_r64r(pic->base, XLP_PIC_IRT_ENTRY(irt));
+	val &= ~XLP_IRTENT_ENABLE;
+	__pic_w64r(pic->base, XLP_PIC_IRT_ENTRY(irt), val);
+	spin_unlock_irqrestore(&pic->pic_lock, flags);
+	return 0;
+}
+
+int xlp_pic_self_init(struct pic_dev *this)
+{
+	int i;
+	u64 val;
+
+	/* Initializes PIC to a logical state before registering */
+	spin_lock_init(&this->pic_lock);
+
+	/* We set the following in IRT entry
+	 * 28 : clear to indicate global delivery
+	 * 19 : set to indicate DB
+	 * 16-18 : 0
+	 * 0-15 : 0 => cpu0 */
+	preempt_disable();
+	for (i = 0; i < XLP_IRT_NUM; i++) {
+		val = XLP_IRTENT_SCH_LCL | XLP_IRTENT_DB(0) | XLP_IRTENT_DTE(0);
+		__pic_w64r(this->base, XLP_PIC_IRT_ENTRY(i), val);
+	}
+	preempt_enable();
+	return 0;
+}
+
+int xlp_interrupt_pending(struct pic_dev *pic, void *data, u64 type)
+{
+	struct xlp_ip *ip = (struct xlp_ip *)data;
+	unsigned long flags;
+
+	spin_lock_irqsave(&pic->pic_lock, flags);
+	if (ip->valid & XLP_INTPENDING_V0)
+		ip->ip[0] = __pic_r64r(pic->base, XLP_PIC_INT_PENDING(0));
+	if (ip->valid & XLP_INTPENDING_V1)
+		ip->ip[1] = __pic_r64r(pic->base, XLP_PIC_INT_PENDING(1));
+	if (ip->valid & XLP_INTPENDING_V2)
+		ip->ip[2] = __pic_r64r(pic->base, XLP_PIC_INT_PENDING(2));
+	spin_unlock_irqrestore(&pic->pic_lock, flags);
+	return UPIC_SUCCESS;
+}
+
+int xlp_interrupt_ack(struct pic_dev *pic, u32 irt, u64 unused)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&pic->pic_lock, flags);
+	/* Order of ack-ing pic_status and pic_int_ack is very important
+	 * Otherwise you might end up double the interrupt rate */
+	if (irt < 12) { /* Ack status register for WD and Sys.Timers */
+		__pic_w64r(pic->base, XLP_PIC_STATUS, 1 << irt);
+	}
+	/* Currently we have no way of figuring out the source PIC for an
+	 * interrupt. So, we restrict interrupt delivery to local node only
+	 * Please make sure this is the case while interrupt thread enable
+	 * registers (0x94 onwards) */
+	__pic_w64r(pic->base, XLP_PIC_INT_ACK, irt);
+	spin_unlock_irqrestore(&pic->pic_lock, flags);
+	return UPIC_SUCCESS;
+}
+
+int xlp_setup_pic_systimer(struct pic_dev *pic, int idx, u64 type)
+{
+	return 0;
+}
+
+int xlp_mod_pic_systimer(struct pic_dev *pic, int idx, u64 type)
+{
+	return 0;
+}
+
+int xlp_setup_pic_wdtimer(struct pic_dev *pic, int idx, u64 type)
+{
+	return 0;
+}
+
+int xlp_mod_pic_wdtimer(struct pic_dev *pic, int idx, u64 type)
+{
+	return 0;
+}
+
+static u64 xlp_pic_base[NLM_MAX_CPU_NODE] = {XLP_BDF_BASE(0,0,4),
+	XLP_BDF_BASE(0,8,4), XLP_BDF_BASE(0,16,4), XLP_BDF_BASE(0,24,4)};
+#define XLP_PIC_DEV_NAME	"xlp-pic-00"
+
+u64 __nlh_pic_r64o(u8 nid, u64 offset)
+{
+	return (__pic_r64o((void *)xlp_pic_base[nid], offset));
+}
+EXPORT_SYMBOL(__nlh_pic_r64o);
+
+void __nlh_pic_w64o(u8 nid, u64 offset, u64 val)
+{
+	__pic_w64o((void *)xlp_pic_base[nid], offset, val);
+}
+EXPORT_SYMBOL(__nlh_pic_w64o);
+
+static struct pic_dev xlp_pic_dev[NLM_MAX_CPU_NODE] = {
+	[0 ... NLM_MAX_CPU_NODE-1] = {
+	.name = XLP_PIC_DEV_NAME,
+	.self_init = xlp_pic_self_init,
+	.max_irq_per_rvec = 32,
+	.plat_request_irq = xlp_request_irq,
+	.plat_release_irq = xlp_release_irq,
+	.set_irq_affinity = xlp_set_irq_affinity,
+	.get_irq_affinity = xlp_get_irq_affinity,
+	.get_closest_mask = xlp_get_closest_mask,
+	.interrupt_ack = xlp_interrupt_ack,
+	.interrupt_pending = xlp_interrupt_pending,
+	.setup_pic_systimer = xlp_setup_pic_systimer,
+	.mod_pic_systimer = xlp_mod_pic_systimer,
+	.setup_pic_wdtimer = xlp_setup_pic_wdtimer,
+	.mod_pic_wdtimer = xlp_mod_pic_wdtimer,
+	}
+};
+
+/* Initializes the PIC and registers it
+ * Mainly sets up the IRT entries default values
+ * called from on_chip.c:pic_init()
+ * */
+void xlp_pic_init(u8 node)
+{
+	struct pic_dev *pic = &xlp_pic_dev[node];
+
+	pic->node = node;
+	pic->base = (void *)xlp_pic_base[node];
+	xlp_init_irqmap(pic);
+	register_pic_dev(node, pic);
+	if (node == 0) {
+		xlp_ites_init();
+		/* Program ITEs to direct interrupts only to cpu 0
+		 * This is mandatory for PIC timer to come up. */
+		xlp_cpumask_to_node_ite(&xlp_ites[node][0], &xlp_pic_dev[node], 0, 0);
+	}
+}
+
+
+#if !defined CONFIG_XLP_REPLACE_R4K_TIMER
+void nlm_hal_pic_update_control(u64 control)
+{
+	u64 val = nlh_pic_r64r(0, XLP_PIC_CTRL);
+	val |= control;
+	nlh_pic_w64r(0, XLP_PIC_CTRL, val);
+}
+#endif
+
+void nlm_hal_pic_send_ipi(int nmi, int vec, int node, int cpu)
+{
+	unsigned long long ipi = (nmi << 31) | (vec << 20) | (node << 17) | (1 << (cpu & 0xf));
+	if (cpu > 15) {
+		ipi |= 0x10000; // Setting bit 16 to select cpus 16-31
+	}
+	nlh_pic_w64r(0, XLP_PIC_IPI_CTL, ipi);
+}
+
diff --git a/arch/mips/netlogic/xlp/platform.c b/arch/mips/netlogic/xlp/platform.c
index 6a6066c..4d8eff8 100644
--- a/arch/mips/netlogic/xlp/platform.c
+++ b/arch/mips/netlogic/xlp/platform.c
@@ -75,7 +75,7 @@ unsigned int xlp_uart_in(struct uart_port *p, int offset) {
 	nlm_reg_t *mmio;
 	unsigned int value;
 
-	/* XLP uart does not need any mapping of regs 
+	/* XLP uart does not need any mapping of regs
 	 */
 	offset = offset << p->regshift;
 	mmio = (nlm_reg_t *)(p->membase + offset);
@@ -95,17 +95,14 @@ void xlp_uart_out(struct uart_port *p, int offset, int value)
 
 static void xlp_init_uart(int port_id)
 {
-        xlp_uart_port[port_id].mapbase       = DEFAULT_NETLOGIC_IO_BASE 
+        xlp_uart_port[port_id].mapbase       = DEFAULT_NETLOGIC_IO_BASE
 						+ NETLOGIC_IO_UART_0_OFFSET + port_id * XLP_UART_PORTIO_OFFSET;
-        xlp_uart_port[port_id].membase       = (void __iomem *)((unsigned long)xlp_uart_port[port_id].mapbase);
-        xlp_uart_port[port_id].irq           = xlp_pic_irt_to_irq(XLP_PIC_IRT_UART(port_id));
+        xlp_uart_port[port_id].membase       = (void __iomem *)xlp_uart_port[port_id].mapbase;
+        xlp_uart_port[port_id].irq           = xlp_irt_to_irq(0, 133 + port_id);
+
+        xlp_uart_port[port_id].uartclk       = XLP_PIT_TICK_RATE;
 
         xlp_uart_port[port_id].iotype        = UPIO_MEM32;
-	
-	if(nlm_hal_is_ref_clk_133MHz())
-		xlp_uart_port[port_id].uartclk       = UART_CLK_133MHz;
-	else
-		xlp_uart_port[port_id].uartclk       = UART_CLK_66MHz;
 
         xlp_uart_port[port_id].flags         = UPF_SKIP_TEST|UPF_FIXED_TYPE|UPF_BOOT_AUTOCONF;
         xlp_uart_port[port_id].type          = PORT_16550A;
@@ -164,11 +161,11 @@ struct dev2drv dev2drv_table[MAX_DEV2DRV] = {
 	{0x0, 			 "",	0,	0,	PLAT_DRV},
 };
 
-static int get_dev2drv(uint32_t x) 
+static int get_dev2drv(uint32_t x)
 {
 	int i;
 
-	for(i=0; i<MAX_DEV2DRV; i++) {	
+	for(i=0; i<MAX_DEV2DRV; i++) {
 		if(x == dev2drv_table[i].devid)
 			return i;
 	}
@@ -242,7 +239,7 @@ static int xlp_find_pci_dev(void)
 						pres[0].end	= mmio;
 						pres[0].flags	= IORESOURCE_MEM;
 						irt = (nlm_hal_read_32bit_reg(mmio, DEV_IRT_INFO) & 0xFFFF);
-						irq = xlp_pic_irt_to_irq(irt);
+						irq = xlp_irt_to_irq(0, irt);
 
 						pres[1].start = irq;
 						pres[1].end = irq;
diff --git a/arch/mips/netlogic/xlp/setup.c b/arch/mips/netlogic/xlp/setup.c
index a572185..6d4eb98 100644
--- a/arch/mips/netlogic/xlp/setup.c
+++ b/arch/mips/netlogic/xlp/setup.c
@@ -63,6 +63,7 @@
 #include <asm/netlogic/xlp_ici.h>
 #include <asm/netlogic/hal/nlm_hal_macros.h>
 #include <asm/netlogic/xlp_irq.h>
+#include <asm/netlogic/xlp_irq_map.h>
 #include <asm/netlogic/phnx_loader.h>
 #include "../boot/ops.h"
 #include <asm/netlogic/hal/nlm_hal.h>
@@ -172,19 +173,19 @@ struct nlm_ici_config
 }nlm_ici_config;
 
 struct boot_mem_map boot_physaddr_info;
-struct xlp_dram_mapping {          
-                unsigned long low_pfn; 
-                unsigned long high_pfn;        
-                int node;   
-};        
-#define NLM_NODES_MAX_DRAM_REGION (NLM_MAX_DRAM_REGION * MAX_NUMNODES)      
-struct xlp_dram_mapping  dram_map[NLM_NODES_MAX_DRAM_REGION];        
-          
-#define NLM_DRAM_BASE_REG_0     20 
-#define NLM_DRAM_LIMIT_REG_0    28 
-#define NLM_DRAM_NODEADDR_XLAT  36 
-#define HDR_OFFSET      0x100      
-#define BRIDGE  (0x00<<20) | (0x00<<15) | (0x00<<12)        
+struct xlp_dram_mapping {
+                unsigned long low_pfn;
+                unsigned long high_pfn;
+                int node;
+};
+#define NLM_NODES_MAX_DRAM_REGION (NLM_MAX_DRAM_REGION * MAX_NUMNODES)
+struct xlp_dram_mapping  dram_map[NLM_NODES_MAX_DRAM_REGION];
+
+#define NLM_DRAM_BASE_REG_0     20
+#define NLM_DRAM_LIMIT_REG_0    28
+#define NLM_DRAM_NODEADDR_XLAT  36
+#define HDR_OFFSET      0x100
+#define BRIDGE  (0x00<<20) | (0x00<<15) | (0x00<<12)
 
 int nlm_common_get_pgprot(unsigned long address)
 {
@@ -224,39 +225,39 @@ int valid_mmap_nlm_common_addr_range(unsigned long pfn)
 	return 0;
 }
 
-void read_node_bars(int node)    
-{         
+void read_node_bars(int node)
+{
         int i, idx;
-        uint32_t *membase = cpu_io_mmio(node, BRIDGE);      
-          
+        uint32_t *membase = cpu_io_mmio(node, BRIDGE);
+
         for (i = 0; i < NLM_MAX_DRAM_REGION; i++) {
                 uint64_t base_reg  = nlm_hal_read_32bit_reg((uint64_t)membase, NLM_DRAM_BASE_REG_0 + i);
                 uint64_t limit_reg = nlm_hal_read_32bit_reg((uint64_t)membase, NLM_DRAM_LIMIT_REG_0 + i);
                 uint32_t node_reg =  nlm_hal_read_32bit_reg((uint64_t)membase, NLM_DRAM_NODEADDR_XLAT + i);
 
-                if(((node_reg >> 1) & 0x3) != node) {       
-                        continue;  
-                }  
-                if(((limit_reg >> 12) << 20) == 0) {        
-                        continue;  
-                }  
+                if(((node_reg >> 1) & 0x3) != node) {
+                        continue;
+                }
+                if(((limit_reg >> 12) << 20) == 0) {
+                        continue;
+                }
 
-                idx = (node * NLM_MAX_DRAM_REGION) + i;     
-                dram_map[idx].low_pfn = ((base_reg >> 12) << 20) >> PAGE_SHIFT;          
+                idx = (node * NLM_MAX_DRAM_REGION) + i;
+                dram_map[idx].low_pfn = ((base_reg >> 12) << 20) >> PAGE_SHIFT;
                 dram_map[idx].high_pfn =
                         ((limit_reg >> 12) << 20) >> PAGE_SHIFT;
-                dram_map[idx].node = node;         
-          
+                dram_map[idx].node = node;
+
                 if(dram_map[idx].high_pfn == dram_map[idx].low_pfn){
                     continue;
                 }
-                boot_physaddr_info.map[boot_physaddr_info.nr_map].addr = dram_map[idx].low_pfn << PAGE_SHIFT;     
-                boot_physaddr_info.map[boot_physaddr_info.nr_map].size = 
-                        (dram_map[idx].high_pfn - dram_map[idx].low_pfn + (1<<(20-PAGE_SHIFT))) << PAGE_SHIFT;  
-                boot_physaddr_info.map[boot_physaddr_info.nr_map].type = BOOT_MEM_RAM;   
-                boot_physaddr_info.nr_map++;       
-        } 
-}         
+                boot_physaddr_info.map[boot_physaddr_info.nr_map].addr = dram_map[idx].low_pfn << PAGE_SHIFT;
+                boot_physaddr_info.map[boot_physaddr_info.nr_map].size =
+                        (dram_map[idx].high_pfn - dram_map[idx].low_pfn + (1<<(20-PAGE_SHIFT))) << PAGE_SHIFT;
+                boot_physaddr_info.map[boot_physaddr_info.nr_map].type = BOOT_MEM_RAM;
+                boot_physaddr_info.nr_map++;
+        }
+}
 
 void nlm_get_dram_mapping(void)
 {
@@ -326,8 +327,6 @@ void __init nlm_nmi_setup (void)
 
 /* setup early serial port driver */
 #ifdef CONFIG_SERIAL_8250
-#define UART_CLK_133MHz 133333333
-#define UART_CLK_66MHz   66666666
 
 static void __init nlm_early_serial_setup(int uart_id)
 {
@@ -341,15 +340,12 @@ static void __init nlm_early_serial_setup(int uart_id)
 	/* hardware int 4 - the serial int, is CPU int 6
 	 but poll for now */
 
-	if(nlm_hal_is_ref_clk_133MHz())
-		s.uartclk = UART_CLK_133MHz;
-	else
-		s.uartclk = UART_CLK_66MHz;
+	s.uartclk = XLP_PIT_TICK_RATE;
 
 	switch(uart_id){
 		default:
 		case 0:
-			s.irq = xlp_pic_irt_to_irq(XLP_PIC_IRT_UART(0));
+			s.irq = xlp_irt_to_irq(0,133);
 			s.membase = (unsigned char __iomem *)
 			(DEFAULT_NETLOGIC_IO_BASE + NETLOGIC_IO_UART_0_OFFSET);
 			s.mapbase = (DEFAULT_NETLOGIC_IO_BASE +
@@ -357,7 +353,7 @@ static void __init nlm_early_serial_setup(int uart_id)
 			s.line = 0;
 			break;
 		case 1:
-			s.irq = xlp_pic_irt_to_irq(XLP_PIC_IRT_UART(1));
+			s.irq = xlp_irt_to_irq(0,134);
 			s.membase = (unsigned char __iomem *)
 			(DEFAULT_NETLOGIC_IO_BASE + NETLOGIC_IO_UART_1_OFFSET);
 			s.mapbase = (DEFAULT_NETLOGIC_IO_BASE +
@@ -411,9 +407,9 @@ static void parse_fdt_sae_vc_config(void)
 		if (getprop(node, "nae-fb-vc", &nae_fb_vc, 4) > 0)
                         nae_fb_vc = fdt32_to_cpu(nae_fb_vc);
 
-		if (getprop(node, "sae-rx-vc", &sae_rx_vc, 4) > 0) 
+		if (getprop(node, "sae-rx-vc", &sae_rx_vc, 4) > 0)
 			sae_rx_vc = fdt32_to_cpu(sae_rx_vc);
-		
+
 		if (getprop(node, "sae-rx-sync-vc", &sae_rx_sync_vc, 4) > 0)
 			sae_rx_sync_vc = fdt32_to_cpu(sae_rx_sync_vc);
 
@@ -493,12 +489,12 @@ static void ici_read_vc_parameter(void *node, int max_vc, struct nlm_ici_vc_para
 static void ici_dump_vc_info(const char *header, int max_vc, struct nlm_ici_vc_param *vc)
 {
 	printk("[=== %s ===]\n", header);
-	
+
 	if(max_vc == 8){
-		printk("OWN:    %d %d %d %d %d %d %d %d\n",(vc+0)->own_credit, (vc+1)->own_credit, (vc+2)->own_credit, 
+		printk("OWN:    %d %d %d %d %d %d %d %d\n",(vc+0)->own_credit, (vc+1)->own_credit, (vc+2)->own_credit,
 				(vc+3)->own_credit,	(vc+4)->own_credit, (vc+5)->own_credit, (vc+6)->own_credit, (vc+7)->own_credit);
-		printk("SHARED: %d %d %d %d %d %d %d %d\n",(vc+0)->shared_credit, (vc+1)->shared_credit, (vc+2)->shared_credit, 
-				(vc+3)->shared_credit,	(vc+4)->shared_credit, (vc+5)->shared_credit, (vc+6)->shared_credit, 
+		printk("SHARED: %d %d %d %d %d %d %d %d\n",(vc+0)->shared_credit, (vc+1)->shared_credit, (vc+2)->shared_credit,
+				(vc+3)->shared_credit,	(vc+4)->shared_credit, (vc+5)->shared_credit, (vc+6)->shared_credit,
 				(vc+7)->shared_credit);
 		printk("TXWGHT: %d %d %d %d %d %d %d %d\n",(vc+0)->txwght, (vc+1)->txwght, (vc+2)->txwght, (vc+3)->txwght,
 				(vc+4)->txwght, (vc+5)->txwght, (vc+6)->txwght, (vc+7)->txwght);
@@ -506,7 +502,7 @@ static void ici_dump_vc_info(const char *header, int max_vc, struct nlm_ici_vc_p
 				(vc+4)->segth, (vc+5)->segth, (vc+6)->segth, (vc+7)->segth);
 	}else if (max_vc == 4){
 		printk("OWN:    %d %d %d %d\n",(vc+0)->own_credit, (vc+1)->own_credit, (vc+2)->own_credit, (vc+3)->own_credit);
-		printk("SHARED: %d %d %d %d \n",(vc+0)->shared_credit, (vc+1)->shared_credit, (vc+2)->shared_credit, 
+		printk("SHARED: %d %d %d %d \n",(vc+0)->shared_credit, (vc+1)->shared_credit, (vc+2)->shared_credit,
 				(vc+3)->shared_credit);
 		printk("TXWGHT: %d %d %d %d\n",(vc+0)->txwght, (vc+1)->txwght, (vc+2)->txwght, (vc+3)->txwght);
 		printk("SEGTH:  %d %d %d %d\n",(vc+0)->segth, (vc+1)->segth, (vc+2)->segth, (vc+3)->segth);
diff --git a/arch/mips/netlogic/xlp/smp.c b/arch/mips/netlogic/xlp/smp.c
index e18c170..22b5904 100644
--- a/arch/mips/netlogic/xlp/smp.c
+++ b/arch/mips/netlogic/xlp/smp.c
@@ -28,6 +28,7 @@ THE POSSIBILITY OF SUCH DAMAGE.
 #include <linux/delay.h>
 #include <linux/init.h>
 #include <linux/smp.h>
+#include <linux/irqreturn.h>
 #include <linux/interrupt.h>
 
 #include <asm/mipsregs.h>
@@ -37,8 +38,7 @@ THE POSSIBILITY OF SUCH DAMAGE.
 
 #include <asm/netlogic/xlp.h>
 #include <asm/netlogic/mips-exts.h>
-#include <asm/netlogic/interrupt.h>
-#include <asm/netlogic/xlp_hal_pic.h>
+#include <asm/netlogic/xlp_irq.h>
 #ifdef CONFIG_XLP_FMN_SUPPORT
 #include <asm/netlogic/hal/nlm_hal_fmn.h>
 #endif
@@ -49,10 +49,12 @@ THE POSSIBILITY OF SUCH DAMAGE.
 #include <asm/processor.h>
 
 #include <asm/netlogic/cpumask.h>
+#include <linux/nodemask.h>
 
 #include <asm/mach-netlogic/mmu.h>
 
 #include <asm/netlogic/xlp8xx/cpu_control_macros.h>
+#include <asm/netlogic/xlp_hal_pic.h>
 
 struct smp_boot_info smp_boot;
 EXPORT_SYMBOL(smp_boot);
@@ -62,6 +64,7 @@ cpumask_t phys_cpu_present_map;
 EXPORT_SYMBOL(phys_cpu_present_map);
 
 extern void ptr_smp_boot(unsigned long, unsigned long, unsigned long);
+extern void prom_reconfigure_thr_resources(void);
 extern unsigned long nlm_common_ebase;
 extern void enable_cpus(unsigned int node, unsigned online_mask);
 extern void nlm_smp_irq_init(void);
@@ -70,6 +73,8 @@ extern void asmlinkage smp_bootstrap(void);
 extern void enable_msgconfig_int(void);
 void nlm_enable_vc_intr(void);
 #endif
+extern void xlp_pic_ite_init(const struct cpumask *);
+
 /*
  * Input parameter is logical cpu number.
  * Should convert to physical cpu before using it
@@ -83,11 +88,11 @@ void nlm_send_ipi_single(int lcpu, unsigned int action)
 	phys_cpu = phys_cpu % 32;	/* This need to be changed for NUMA?? */
 
         if (action & SMP_CALL_FUNCTION) {
-                ipi |= XLP_IRQ_IPI_SMP_FUNCTION;
+                ipi |= XLP_IRQ_IPI_SMP_FUNCTION_RVEC;
 	} else if (action & SMP_RESCHEDULE_YOURSELF) {
-                ipi |= XLP_IRQ_IPI_SMP_RESCHEDULE;
-	} else if (action & SMP_CALL_KGDB_HOOK) {
-                ipi |= XLP_IRQ_IPI_SMP_KGDB;
+                ipi |= XLP_IRQ_IPI_SMP_RESCHEDULE_RVEC;
+	} else if (action & SMP_CALL_KGDB_HOOK_RVEC) {
+                ipi |= XLP_IRQ_IPI_SMP_KGDB_RVEC;
 		/* for KGDB enable NMI also */
 		nmi = 1;
         } else
@@ -107,29 +112,29 @@ void nlm_send_ipi_mask(const struct cpumask * mask, unsigned int action)
 	}
 }
 
-extern void xlp_timer_setup(void);
 /*
  * Code to run on secondary just after probing the CPU
  */
+
+void xlp_pic_init(u8 node);
+void xlp_timer_setup(void);
 static void __cpuinit nlm_init_secondary(void)
 {
-    /* Time init for this cpu is done in mips_clockevent_init() */
-    nlm_smp_irq_init();
+	int cpu;
+	cpu = hard_smp_processor_id();
+	/* Time init for this cpu is done in mips_clockevent_init() */
+	nlm_smp_irq_init();
 #ifdef CONFIG_XLP_FMN_SUPPORT
-    enable_msgconfig_int();
+	enable_msgconfig_int();
 #endif
 
-    /* Workaround for XLP A0 Multi-Node bug */
-    {
-	    int cpu = hard_smp_processor_id();
-
-	    if ( (cpu % 32) == 0) {
-		    /* If this cpu@0 of any of the nodes, initialize PIC */
-		    xlp_timer_setup();
-	    }
-    }
-
-
+	/* Workaround for XLP A0 Multi-Node bug */
+	if ((cpu % NLM_MAX_CPU_PER_NODE) == 0) {
+		xlp_pic_init(cpu/NLM_MAX_CPU_PER_NODE);
+#if defined CONFIG_NLM_XLP && !defined CONFIG_XLP_REPLACE_R4K_TIMER
+		xlp_timer_setup();
+#endif
+	}
 }
 
 void nlm_smp_finish(void)
@@ -137,6 +142,8 @@ void nlm_smp_finish(void)
 	local_irq_enable();
 }
 
+int irq_select_affinity_usr(unsigned int irq);
+void xlp_prog_all_node_ites(void );
 void nlm_cpus_done(void)
 {
 #ifdef CONFIG_XLP_FMN_SUPPORT
@@ -147,6 +154,10 @@ void nlm_cpus_done(void)
 	/* Enable vc interupts for the online cpus */
 	nlm_enable_vc_intr();
 #endif
+	xlp_prog_all_node_ites();
+#if defined CONFIG_XLP_REPLACE_R4K_TIMER
+	irq_select_affinity_usr(XLP_TIMER_IRQ(0));
+#endif
 }
 
 /*
@@ -304,12 +315,12 @@ static irqreturn_t smp_function_ipi_handler(int irq, void *dev_id)
 
 void nlm_prepare_cpus(unsigned int max_cpus)
 {
-	if (request_irq(XLP_IRQ_IPI_SMP_FUNCTION, smp_function_ipi_handler,
+	if (request_irq(XLP_IRQ_IPI_SMP_FUNCTION_RVEC, smp_function_ipi_handler,
 			IRQF_PERCPU | IRQF_NO_THREAD,
 			"IPI:smp function", NULL)) {
 		panic("Cannot request_irq(XLP_IRQ_IPI_SMP_FUNCTION)\n");
 	}
-	if (request_irq(XLP_IRQ_IPI_SMP_RESCHEDULE, smp_resched_ipi_handler,
+	if (request_irq(XLP_IRQ_IPI_SMP_RESCHEDULE_RVEC, smp_resched_ipi_handler,
 			IRQF_PERCPU | IRQF_NO_THREAD,
 			"IPI:smp reschedule", NULL)) {
 		panic("Cannot request_irq(XLP_IRQ_IPI_SMP_RESCHEDULE)\n");
diff --git a/arch/mips/netlogic/xlp/time.c b/arch/mips/netlogic/xlp/time.c
index ae6962e..0746c99 100644
--- a/arch/mips/netlogic/xlp/time.c
+++ b/arch/mips/netlogic/xlp/time.c
@@ -29,50 +29,52 @@ THE POSSIBILITY OF SUCH DAMAGE.
 #include <asm/time.h>
 
 #include <asm/netlogic/xlp_irq.h>
-#include <asm/netlogic/xlp_hal_pic.h>
 #include <asm/netlogic/xlp8xx/cpu_control_macros.h>
 #include <asm/netlogic/hal/nlm_hal.h>
 
-extern spinlock_t xlp_pic_lock;
+//extern spinlock_t xlp_pic_lock;
 
 /* Use PIC Timer 6 as a free running counter */
-#define FRC        6
-#define FRC_MAXVAL PIC_SYSTIMER_MAXVAL
+#define FRC        XLP_PIC_SYSTIMER_COUNT(6)
+#define FRC_MAXVAL XLP_PIT_TIMER_MAX
 
 u64 xlp_hpt_read(void)
 {
-	u64 counter = nlm_hal_pic_read_timer(FRC);
+	u64 counter = nlh_pic_r64r(0, FRC);
 	return (FRC_MAXVAL - counter);
 }
 EXPORT_SYMBOL(xlp_hpt_read);
 
+#if ! defined CONFIG_XLP_REPLACE_R4K_TIMER
 void xlp_timer_setup(void)
 {
-        unsigned long flags = 0;
-
-        spin_lock_irqsave(&xlp_pic_lock, flags);
-
-	nlm_hal_pic_init_timer(FRC, FRC_MAXVAL);
-	nlm_hal_pic_enable_timer(FRC);
-
-        spin_unlock_irqrestore(&xlp_pic_lock, flags);
+//        unsigned long flags = 0;
+//        spin_lock_irqsave(&xlp_pic_lock, flags);
+
+	/* Use PIC Timer 6 as a free running counter */
+	nlh_pic_w64r(0, FRC, FRC_MAXVAL);
+	/* enable the timer */
+	nlm_hal_pic_update_control(1 << (10 + 6));
+//        spin_unlock_irqrestore(&xlp_pic_lock, flags);
 }
 
 unsigned int __cpuinit get_c0_compare_int(void)
 {
-	return XLP_IRQ_TIMER;
+	return XLP_IRQ_TIMER_RVEC;
 }
 
 void __init plat_time_init(void)
 {
 	mips_hpt_frequency = (unsigned int) nlm_hal_cpu_freq();
 	pr_info("mips_hpt_frequency = %u\n", mips_hpt_frequency);
+#if !defined CONFIG_XLP_REPLACE_R4K_TIMER
 	xlp_timer_setup();
-
+#endif
 	/* Preset lpj to skip lengthy delay calibration */
 	preset_lpj = mips_hpt_frequency / HZ;
 }
 
+#endif
 static int nlm_timer_proc_read(char *page, char **start, off_t off, int count,
 			       int *eof, void *data)
 {
@@ -120,6 +122,3 @@ static void exit_pic_timer_procfs(void)
 	remove_proc_entry("debug", main_entry);
 	remove_proc_entry("nlm_timer", nlm_root_proc);
 }
-
-module_init(init_pic_timer_procfs);
-module_exit(exit_pic_timer_procfs);
diff --git a/arch/mips/netlogic/xlp/xlp_gpio.c b/arch/mips/netlogic/xlp/xlp_gpio.c
index a106c49..338b78d 100644
--- a/arch/mips/netlogic/xlp/xlp_gpio.c
+++ b/arch/mips/netlogic/xlp/xlp_gpio.c
@@ -31,7 +31,7 @@
 #include <linux/platform_device.h>
 #include <asm/netlogic/xlp.h>
 #include <asm/netlogic/hal/nlm_hal.h>
-#include <asm/netlogic/xlp_hal_pic.h>
+#include <asm/netlogic/xlp_irq.h>
 #include <asm/netlogic/gpio.h>
 
 #define XLP_GPIO_MAX		41
diff --git a/arch/mips/pci/pci-xlp.c b/arch/mips/pci/pci-xlp.c
deleted file mode 100644
index c58f165..0000000
--- a/arch/mips/pci/pci-xlp.c
+++ /dev/null
@@ -1,923 +0,0 @@
-/***********************************************************************
-Copyright 2003-2010 Netlogic Microsystems (“Netlogic”). All rights
-reserved.
-Redistribution and use in source and binary forms, with or without
-modification, are permitted provided that the following conditions are
-met:
-1. Redistributions of source code must retain the above copyright
-notice, this list of conditions and the following disclaimer.
-2. Redistributions in binary form must reproduce the above copyright
-notice, this list of conditions and the following disclaimer in
-the documentation and/or other materials provided with the
-distribution.
-THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
-ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
-IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
-PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
-FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
-CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
-SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
-INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
-CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
-ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
-THE POSSIBILITY OF SUCH DAMAGE.
-*****************************#NETL_2#********************************/
-
-/*
- * This file contains specific functions for XLP chipsets and
- * EVP boards.
- */
-#include <linux/types.h>
-#include <linux/pci.h>
-#include <linux/kernel.h>
-#include <linux/init.h>
-#include <linux/mm.h>
-#include <linux/console.h>
-
-#include <asm/io.h>
-
-#include <asm/netlogic/xlp_irq.h>
-#include <asm/netlogic/io.h>
-#include <asm/netlogic/iomap.h>
-#include <asm/netlogic/xlp_hal_pic.h>
-
-extern int pci_probe_only;
-static void *pci_config_base;
-static const volatile void *pci_io_base;
-
-int xlp_intx_enable(int);
-int xlp_intx_disable(int);
-int xlp_msi_enable(int, u32);
-int xlp_msix_enable(int);
-int xlp_msi_disable(int, u32);
-int xlp_msix_disable(int);
-u32 xlp_msi_set_mask(int, int, int);
-
-/*
- * Possible values are no more hard coded.
- * For mapping of these values to IRT, refer
- * arch/mips/netlogic/xlp/irq.c
- *
- * Here a table is defined to figure out the interrupt assignments to different
- * cards placed on any of the 4 PCI slots.
- *
- * We have some unique problems here.
- * 1. Board could be configured in different lane widths. That means, the cards
- * could be controlled by different functions of the controller on board
- * Eg. 2x8 config can have two cards (fn 0 and fn 2)
- *	4x4 config can also have two cards (under fn0 through fn 3)
- * 2. Cards can be placed on any available slot
- * 3. The card can have a switch built in, thus giving rise to multiple devices
- * on the slot.
- *
- * So, it is important to figure out the lanes on which cards are placed.
- * First we read the lane config from POWER_ON_RESET_CFG
- * Then each line's LTSSM state would give the card presence
- * Based on that we have to assign interrupt values; while keeping the
- * possibility of same interrupt assigned to multiple devices open.
- *
- * So, we have a map: XLP irq map is as follows
- *  \fn 0	1	2	3
- *plc\
- * 0	86	0	88	89
- * 1	86	87	88	0
- * 2	86	0	88	89
- * 3	86	87	88	89
- * This map changes from processor to processor. check PRM or RTL because
- * the values are a function of XLP_PCIE_LINK_IRT_OFFSET. To make them
- * somewhat independent, I have defined macros and used them here.
- *
- * This map is dynamically populated based on card presence in the slot.
- * If a card is present, and is a switch, then the secondary and subordinate
- * numbers would be different. Based on this fact, we can figure out from
- * pci_dev structure the slot where a card is placed at run time.
- */
-struct xlp_link_struct {
-	int intno;
-	int sec;
-	int sub;
-};
-
-struct xlp_plc_fn_struct {
-	int plc;
-	struct xlp_link_struct farray[4];
-};
-
-static struct xlp_plc_fn_struct xlp_irqmap[4] = {
-	{0, {{XLP_PCIE_LINK_IRQ(0), 0, 0}, {0, 0, 0},
-		{XLP_PCIE_LINK_IRQ(2), 0, 0}, {0, 0, 0}}},
-	{1, {{XLP_PCIE_LINK_IRQ(0), 0, 0}, {XLP_PCIE_LINK_IRQ(1), 0, 0},
-		{XLP_PCIE_LINK_IRQ(2), 0, 0}, {0, 0, 0}}},
-	{2, {{XLP_PCIE_LINK_IRQ(0), 0, 0}, {0, 0, 0},
-		{XLP_PCIE_LINK_IRQ(2), 0, 0}, {XLP_PCIE_LINK_IRQ(3), 0, 0}}},
-	{3, {{XLP_PCIE_LINK_IRQ(0), 0, 0}, {XLP_PCIE_LINK_IRQ(1), 0, 0},
-		{XLP_PCIE_LINK_IRQ(2), 0, 0}, {XLP_PCIE_LINK_IRQ(3), 0, 0}}},
-};
-
-/*
-static int xlp_irq_map[4][4][3] = {
-	{{XLP_PCIE_LINK_IRQ(0), 0, 0}, {0, 0, 0},
-		{XLP_PCIE_LINK_IRQ(2), 0, 0}, {0, 0, 0}},
-	{{XLP_PCIE_LINK_IRQ(0), 0, 0}, {XLP_PCIE_LINK_IRQ(1), 0, 0},
-		{XLP_PCIE_LINK_IRQ(2), 0, 0}, {0, 0, 0}},
-	{{XLP_PCIE_LINK_IRQ(0), 0, 0}, {0, 0, 0},
-		{XLP_PCIE_LINK_IRQ(2), 0, 0}, {XLP_PCIE_LINK_IRQ(3), 0, 0}},
-	{{XLP_PCIE_LINK_IRQ(0), 0, 0}, {XLP_PCIE_LINK_IRQ(1), 0, 0},
-		{XLP_PCIE_LINK_IRQ(2), 0, 0}, {XLP_PCIE_LINK_IRQ(3), 0, 0}},
-};
-*/
-/* The following is the table describing current interrupt modes of
- * XLP controllers. When an external switch is present, different devices
- * can request different interrupt mode on the same controller which might lead
- * to controller changing previous interrupt mode. If this happens, interrupt
- * delivery will not work correctly. So, we need a way to prevent different
- * devices requesting different interrupt modes. This is kind of impossible
- * because we can't control all device drivers, but we can
- *	1. fail pci_enable_msi{x} if intX is set and at least one interrupt
- *	is allocated
- *	2. fail request_irq() for any interrupt outside current interrupt
- *	distribution range
- */
-struct xlp_intmode_struct {
-	u32 mode;
-	int usage;
-};
-static struct xlp_intmode_struct intmode[4];
-
-int xlp_ctrl_intmode_add(int fn, int mode, int i)
-{
-	if (intmode[fn].mode != mode) {
-		return -EBUSY;
-	}
-	intmode[fn].usage += i;
-	if ((intmode[fn].usage < 0) || (intmode[fn].usage == 0)) {
-		intmode[fn].usage = 0;
-	}
-	return intmode[fn].usage;
-}
-
-
-int xlp_get_ctrl_intmode(int fn)
-{
-	return intmode[fn].mode;
-}
-
-int xlp_set_ctrl_intmode(int fn, int mode)
-{
-	int ret = 0;
-	if (intmode[fn].mode == mode) {
-		/* do nothing */
-	} else if (intmode[fn].usage == 0) {
-		intmode[fn].mode = mode;
-	} else {
-		ret = -EBUSY;
-	}
-	return ret;
-}
-
-/* Just a helper function to fill up xlp_irq_map table's entries
- * This function checks whether a PCIe slot is populated and if yes,
- * fills up the table with subordinate and secondary bus numbers. These
- * numbers would be different only if the PCIe device has a switch inside.
- */
-static int xlp_map_helper(int row, int fn)
-{
-	u64 xlp_pci_base;
-	u32 reg6, ltssm;
-
-	xlp_pci_base = XLP_BDF_BASE(0,1,fn);
-	ltssm = nlm_hal_read_32bit_reg(xlp_pci_base, 0x25E);
-	if (ltssm != 0x00446000) {
-		printk(KERN_WARNING "LTSSM state is %#x. Fn %x link not up\n",
-				ltssm, fn);
-		return -ENODEV;
-	}
-	reg6 = nlm_hal_read_32bit_reg(xlp_pci_base, 0x6);
-	xlp_irqmap[row].farray[fn].sec = (reg6 >> 8) & 0xff;
-	xlp_irqmap[row].farray[fn].sub = (reg6 >> 16) & 0xff;
-	return 0;
-}
-
-/*
- * Iterates over buses to find out the slot (thus pci controller fn)
- */
-int xlp_ctrl_fn_from_dev(const struct pci_dev *dev)
-{
-	__label__ out;
-	int row = 0, fn = 0;
-
-	while (row < 4) {
-		fn = 0;
-		while (fn < 4) {
-			if ((dev->bus->number >= xlp_irqmap[row].farray[fn].sec)
-			&&(dev->bus->number <= xlp_irqmap[row].farray[fn].sub)){
-				goto out; /* No `break', note two loops */;
-			}
-			fn++;
-		}
-		row++;
-	}
-out:
-	if (fn >= 4) {
-		return -ENODEV;
-	}
-	return fn;
-}
-
-/*
- * We discard the idea of a fixed address for MSI. But if that is ever required,
- * define CONFIG_XLP_MSI_ADDRESSES
- */
-#ifndef CONFIG_XLP_MSI_ADDRESSES
-static u64 XLP_MSI_ADDR = 0;
-#endif
-
-volatile const void *xlp_msix_addr_start(int fn)
-{
-	if (XLP_MSI_ADDR == 0) {
-		return 0;
-	}
-	return (volatile const void *)(XLP_MSI_ADDR + (fn * XLP_MSIX_ADDR_SIZE));
-}
-
-volatile const void *xlp_msi_addr_start(int fn)
-{
-	if (XLP_MSI_ADDR == 0) {
-		return 0;
-	}
-	return (volatile const void *)(XLP_MSI_ADDR + (fn * XLP_MSI_ADDR_SIZE));
-}
-
-/* Irrespective of any device requesting MSI/MSI-X, we keep the controller
- * ready by programming the corresponding registers. This action, per se,
- * does not start MSI/MSI-X for they have to be enabled explicitly.
- */
-static void xlp_msi_controller_init(int fn)
-{
-	u64 xlp_pci_base;
-	u8 mmc;
-	u32 msi;
-
-	xlp_pci_base = XLP_BDF_BASE(0,1,fn);
-	if (XLP_MSI_ADDR == 0) {
-		printk(KERN_ERR "MSI/MSI-X CANNOT be programmed\n");
-		return;
-	}
-	msi = nlm_hal_read_32bit_reg(xlp_pci_base, 0x14);
-	mmc = (msi >> 17) & 0x7;
-	/* Initialize MSI Base register */
-	nlm_hal_write_32bit_reg(xlp_pci_base, 0x15,
-		virt_to_phys(xlp_msi_addr_start(fn)) & 0xffffffff);
-	nlm_hal_write_32bit_reg(xlp_pci_base, 0x16,
-		(virt_to_phys(xlp_msi_addr_start(fn)) >> 32) & 0xffffffff);
-	nlm_hal_write_32bit_reg(xlp_pci_base, 0x17, 0x0);
-	msi |= ((mmc << 10) | (1 << 16));
-	nlm_hal_write_32bit_reg(xlp_pci_base, 0x14, msi);
-	/* Initialize MSI-X Base and Address reg. Note >> 8 in the address.
-	 * This is how 40bit address goes in 32bit registers.*/
-	nlm_hal_write_32bit_reg(xlp_pci_base, 0x24F,
-		(virt_to_phys(xlp_msix_addr_start(fn)) >> 8));
-	nlm_hal_write_32bit_reg(xlp_pci_base, 0x250,
-		(virt_to_phys(xlp_msix_addr_start(fn) + XLP_MSIX_ADDR_SIZE) >> 8));
-}
-
-/*
- * Controller is initialized and explicity disabled
- *
- * @fn : controller function no.
- */
-void xlp_pcie_controller_setup(int fn)
-{
-	xlp_msi_controller_init(fn);
-	//xlp_msix_disable(fn);
-	//xlp_msi_disable(fn, 0xf);
-	/* By default, leave INTX enabled */
-	xlp_intx_enable(fn);
-}
-
-/*
- * Utility function to get syscfg
- *
- * @node : node id in multi chip config
- */
-u32 xlp_get_power_on_reset_cfg(int node)
-{
-	u64 xlp_syscfg_base = XLP_BDF_BASE(0,6,5);
-	return nlm_hal_read_32bit_reg(xlp_syscfg_base, 0x41);
-}
-EXPORT_SYMBOL(xlp_get_power_on_reset_cfg);
-
-
-/*
- * Called from system startup routine
- */
-static void pcie_controller_init_done(void)
-{
-	u32 plc, syscfg, mode, count = 0;
-
-#ifndef CONFIG_XLP_MSI_ADDRESSES
-#ifdef CONFIG_32BIT
-	XLP_MSI_ADDR = (u64)__get_free_pages(GFP_KERNEL, get_order(0x100000));
-#else
-	XLP_MSI_ADDR = (u64)__get_free_pages(GFP_KERNEL, get_order(0x1000000));
-#endif
-	if (XLP_MSI_ADDR == 0) {
-		printk(KERN_ERR "Failed to get memory for MSI/MSI-X tables\n");
-	}
-#endif
-	if (!pci_probe_only){
-		printk(KERN_WARNING "PCIe bus IRQs configured incorrectly\n");
-		return;
-	}
-	syscfg = xlp_get_power_on_reset_cfg(0);
-	/* We don't manipulate pci_address space.
-	 * Get the link status from pcie lane config from 34.9.7.2 XLP PRM */
-	mode = (syscfg >> 19) & 0xf;
-	while (count < 4) {
-		printk(KERN_DEBUG "Controller %d is in %s mode\n",
-				count, (mode & (1 << count)) ? "RC" : "EP");
-		count++;
-	}
-	plc = (syscfg >> 23) & 0x3;
-	printk(KERN_DEBUG "PLC = %#x, mode = %#x\n", plc, mode);
-	switch (plc) {
-	/* The correlation between plc and lane config is very specific to XLP
-	 * and not very clear in PRM
-	 */
-	case 0:
-		/* controller 0 and 2 are active with 8lanes each */
-		if (mode & 0x1){
-			xlp_map_helper(plc, 0);
-			xlp_pcie_controller_setup(0);
-		}
-		if (mode & 0x4) {
-			xlp_map_helper(plc, 2);
-			xlp_pcie_controller_setup(2);
-		}
-		break;
-	case 1:
-		/* controllers 0,1 and 2 are active */
-		if (mode & 0x1){
-			xlp_map_helper(plc, 0);
-			xlp_pcie_controller_setup(0);
-		}
-		if (mode & 0x2){
-			xlp_map_helper(plc, 1);
-			xlp_pcie_controller_setup(1);
-		}
-		if (mode & 0x4){
-			xlp_map_helper(plc, 2);
-			xlp_pcie_controller_setup(2);
-		}
-		break;
-	case 2:
-		/* controllers 0,2 and 3 are active */
-		if (mode & 0x1){
-			xlp_map_helper(plc, 0);
-			xlp_pcie_controller_setup(0);
-		}
-		if (mode & 0x4){
-			xlp_map_helper(plc, 2);
-			xlp_pcie_controller_setup(2);
-		}
-		if (mode & 0x8){
-			xlp_map_helper(plc, 3);
-			xlp_pcie_controller_setup(3);
-		}
-		break;
-	case 3:
-		/* All four controllers are active with 4 lanes each */
-		if (mode & 0x1){
-			xlp_map_helper(plc, 0);
-			xlp_pcie_controller_setup(0);
-		}
-		if (mode & 0x2){
-			xlp_map_helper(plc, 1);
-			xlp_pcie_controller_setup(1);
-		}
-		if (mode & 0x4){
-			xlp_map_helper(plc, 2);
-			xlp_pcie_controller_setup(2);
-		}
-		if (mode & 0x8){
-			xlp_map_helper(plc, 3);
-			xlp_pcie_controller_setup(3);
-		}
-		break;
-	}
-	printk(KERN_DEBUG "[%s]: PCIE Controller initialization done\n", __FUNCTION__);
-	return;
-}
-
-static inline __u32 pci_cfg_read_32bit(__u32 addr)
-{
-	__u32 temp = 0;
-	__u32 *p = (__u32 *) (pci_config_base + (addr & ~3));
-
-	temp = *p;
-
-	return temp;
-}
-
-static inline void pci_cfg_write_32bit(__u32 addr, __u32 data)
-{
-        unsigned int *p = (unsigned int *)(pci_config_base + (addr & ~3));
-
-	*p = data;
-}
-
-static int pci_bus_status = 0;
-#define pci_cfg_offset(bus, devfn, where) (((bus)<<16)+((devfn)<<8)+(where))
-#define pci_cfg_addr(bus, devfn, where) pci_cfg_offset((bus)->number,(devfn),where)
-
-static int xlp_pcibios_read(struct pci_bus *bus, unsigned int devfn,
-				int where, int size, u32 * val)
-{
-	__u32 data = 0;
-
-	if ((size == 2) && (where & 1))
-		return PCIBIOS_BAD_REGISTER_NUMBER;
-	else if ((size == 4) && (where & 3))
-		return PCIBIOS_BAD_REGISTER_NUMBER;
-
-	if (pci_bus_status)
-		data = pci_cfg_read_32bit(pci_cfg_offset((bus->number), devfn, where));
-	else
-		data = 0xFFFFFFFF;
-
-	if (size == 1)
-		*val = (data >> ((where & 3) << 3)) & 0xff;
-	else if (size == 2)
-		*val = (data >> ((where & 3) << 3)) & 0xffff;
-	else
-		*val = data;
-
-	return PCIBIOS_SUCCESSFUL;
-}
-
-static int xlp_pcibios_write(struct pci_bus *bus, unsigned int devfn,
-				 int where, int size, u32 val)
-{
-	__u32 cfgaddr = pci_cfg_offset((bus->number), devfn, where);
-	__u32 data = 0;
-
-	if ((size == 2) && (where & 1))
-		return PCIBIOS_BAD_REGISTER_NUMBER;
-	else if ((size == 4) && (where & 3))
-		return PCIBIOS_BAD_REGISTER_NUMBER;
-
-	if (!pci_bus_status)
-		return PCIBIOS_BAD_REGISTER_NUMBER;
-
-	data = pci_cfg_read_32bit(cfgaddr);
-
-	if (size == 1)
-		data = (data & ~(0xff << ((where & 3) << 3))) |
-			(val << ((where & 3) << 3));
-	else if (size == 2)
-		data = (data & ~(0xffff << ((where & 3) << 3))) |
-			(val << ((where & 3) << 3));
-	else
-		data = val;
-
-	pci_cfg_write_32bit(cfgaddr, data);
-
-	return PCIBIOS_SUCCESSFUL;
-}
-
-static struct pci_ops xlp_pci_ops = {
-	.read  = xlp_pcibios_read,
-	.write = xlp_pcibios_write
-};
-
-/*
- * XLP PCIE Controller
- */
-#define DEFAULT_XLP_PCI_ECONFIG_BASE	(0x18000000ULL)
-#define DEFAULT_XLP_PCI_ECONFIG_SIZE	(32 << 20)
-#define DEFAULT_XLP_PCI_CONFIG_BASE	(0x1c000000ULL)
-#define DEFAULT_XLP_PCI_CONFIG_SIZE	(32 << 20)
-static struct resource xlp_mem_resource = {
-	.name           = "XLP PCI MEM",
-	.start          = 0xd0000000ULL,          /* 256MB PCI mem @ 0xd000_0000 */
-	.end            = 0xdfffffffULL,
-	.flags          = IORESOURCE_MEM,
-};
-static struct resource xlp_io_resource = {
-	.name           = "XLP IO MEM",
-	.start          = 0x14000000UL,         /* 32MB PCI IO @ 0x1400_0000 */
-	.end            = 0x15ffffffUL,
-	.flags          = IORESOURCE_IO,
-};
-struct pci_controller xlp_controller = {
-	.index          = 0,
-	.pci_ops        = &xlp_pci_ops,
-	.mem_resource   = &xlp_mem_resource,
-	.io_resource    = &xlp_io_resource,
-	.io_offset      = 0x00000000UL,
-	.mem_offset     = 0x00000000UL
-};
-
-/*
- * Apparently this function is called for all pci controller functions
- * viz. 0:1.0, 0:1.1, 0:1.2 and 0:1.3
- * In fact, we need not assign them any interrupt.
- * But for any devices connected on them, consult the populated table
- * and return corresponding interrupt.
- */
-int __init pcibios_map_irq(const struct pci_dev *dev, u8 slot, u8 pin)
-{
-	int row = 0, fn = 0;
-
-	switch (dev->devfn) {
-	case XLP_PCIE_CTRL_DEVFN(0, 0) ... XLP_PCIE_CTRL_DEVFN(0, 3):
-		return 0;
-	default:
-		break;
-	}
-	row = (xlp_get_power_on_reset_cfg(0) >> 23) & 0x3;
-	fn = xlp_ctrl_fn_from_dev(dev);
-	dev_printk(KERN_DEBUG, &dev->dev, "Assigning interrupt %#x\n", xlp_irqmap[row].farray[fn].intno);
-	return xlp_irqmap[row].farray[fn].intno;
-}
-
-/*
- * Enables INTx on a controller
- */
-static int __xlp_intx_enable(int fn)
-{
-	u64 xlp_pci_base;
-	u32 pci;
-
-	xlp_pci_base = XLP_BDF_BASE(0,1,fn);
-	pci = nlm_hal_read_32bit_reg(xlp_pci_base, 0x1);
-	pci &= ~(1 << 10);	/* Enable IntX assertion */
-	nlm_hal_write_32bit_reg(xlp_pci_base, 0x1, pci);
-	pci = nlm_hal_read_32bit_reg(xlp_pci_base, 0x261);
-	pci |= 0xf;	/* Enable INT A,B,C,D */
-	nlm_hal_write_32bit_reg(xlp_pci_base, 0x261, pci);
-	return 0;
-}
-
-int xlp_intx_enable(int fn)
-{
-	int mode = xlp_get_ctrl_intmode(fn);
-
-	if ((mode & XLP_INTMODE_MSI) || (mode & XLP_INTMODE_MSIX)) {
-		return -EBUSY;
-	}
-	__xlp_intx_enable(fn);
-	xlp_incr_ctrl_intmode(fn, XLP_INTMODE_INTX);
-	return 0;
-}
-
-/*
- * Disables INTx on a controller
- */
-static int __xlp_intx_disable(int fn)
-{
-	u64 xlp_pci_base;
-	u32 pci;
-
-	xlp_pci_base = XLP_BDF_BASE(0,1,fn);
-	pci = nlm_hal_read_32bit_reg(xlp_pci_base, 0x1);
-	pci |= (1 << 10);
-	nlm_hal_write_32bit_reg(xlp_pci_base, 0x1, pci);
-	pci = nlm_hal_read_32bit_reg(xlp_pci_base, 0x261);
-	pci &= ~(0xf);
-	nlm_hal_write_32bit_reg(xlp_pci_base, 0x261, pci);
-	return 0;
-}
-
-int xlp_intx_disable(int fn)
-{
-	int mode = xlp_get_ctrl_intmode(fn);
-
-	if (!(mode & XLP_INTMODE_INTX)) {
-		return -EBUSY;
-	}
-	__xlp_intx_disable(fn);
-	xlp_decr_ctrl_intmode(fn, XLP_INTMODE_INTX);
-	return 0;
-}
-
-/*
- * Finds the slot on which this device is placed and enables corresponding
- * MSI enable register on the _controller_ if not already enabled
- * @dev : pci device corresponding to this device
- */
-static int __xlp_msi_enable(int fn, u32 bit)
-{
-	u64 xlp_pci_base;
-	u32 msi_en;
-
-	xlp_pci_base = XLP_BDF_BASE(0,1,fn);
-
-	/* First, set PCIe MSI Enable register. __KEEP_THIS_ORDER__ */
-	msi_en = nlm_hal_read_32bit_reg(xlp_pci_base, 0x261);
-	msi_en &= ~(0xf);
-	if ((msi_en & (1 << 9)) == 0) {
-		msi_en |= (1 << 9);	/* controls ONLY MSI, Not MSI-X */
-		nlm_hal_write_32bit_reg(xlp_pci_base, 0x261, msi_en);
-	}
-	/* Now, set the individual bit */
-	xlp_msi_set_mask(fn, bit, 1);
-	return 0;
-}
-
-int xlp_msi_enable(int fn, u32 bit)
-{
-	int tmp = xlp_get_ctrl_intmode(fn);
-
-	if ((tmp & XLP_INTMODE_INTX) || (tmp & XLP_INTMODE_MSIX)) {
-		return -EBUSY;
-	}
-
-	/* Enable MSI bis
-	 * Multiple MSI can get enabled at different point of time (especially
-	 * with a switch present. So, setting the bitmap should not depend on
-	 * present value of reg 0x25b or 0x261
-	 */
-	__xlp_msi_enable(fn, bit);
-	xlp_incr_ctrl_intmode(fn, XLP_INTMODE_MSI);
-	return 0;
-}
-
-/*
- * Finds the slot on which this device is placed and enables corresponding
- * MSI-X enable register on the controller
- */
-static int __xlp_msix_enable(int fn)
-{
-	u64 xlp_pci_base;
-	u32 msix_ctrl;
-
-	xlp_pci_base = XLP_BDF_BASE(0,1,fn);
-	msix_ctrl = nlm_hal_read_32bit_reg(xlp_pci_base, 0x2C);
-	if (!(msix_ctrl & 0x80000000)) {
-		msix_ctrl |= 0x80000000;	/* MSI-X enable */
-		nlm_hal_write_32bit_reg(xlp_pci_base, 0x2C, msix_ctrl);
-	}
-	//nlm_hal_write_32bit_reg(xlp_pci_base, 0xf, 0xFF);
-	return 0;
-}
-
-int xlp_msix_enable(int fn)
-{
-	int mode = xlp_get_ctrl_intmode(fn);
-
-	if ((mode & XLP_INTMODE_MSI) || (mode & XLP_INTMODE_INTX)) {
-		return -EBUSY;
-	}
-	__xlp_msix_enable(fn);
-	xlp_incr_ctrl_intmode(fn, XLP_INTMODE_MSIX);
-	return 0;
-}
-
-/*
- * Disables MSI on controller function
- */
-static int __xlp_msi_disable(int fn)
-{
-	u64 xlp_pci_base;
-	u32 msi_en;
-
-	/* We dont call xlp_decr_ctrl.... here because it has already been 
-	 * called before xlp_msi_disable is called */
-	xlp_pci_base = XLP_BDF_BASE(0,1,fn);
-	/*set PCIe Int Enable register */
-	msi_en = nlm_hal_read_32bit_reg(xlp_pci_base, 0x261);
-	if ((msi_en & (1 << 9)) != 0) {
-		msi_en &= ~(1 << 9);
-		msi_en |= 0xf;
-		nlm_hal_write_32bit_reg(xlp_pci_base, 0x261, msi_en);
-	}
-	return 0;
-}
-
-int xlp_msi_disable(int fn, u32 bit)
-{
-	int tmp = xlp_get_ctrl_intmode(fn);
-	u32 r25b;
-
-	if (!(tmp & XLP_INTMODE_MSI)) {
-		return -EBUSY;
-	}
-	r25b = xlp_msi_set_mask(fn, bit, 0);
-	if (r25b == 0) {
-		__xlp_msi_disable(fn);
-	}
-	xlp_decr_ctrl_intmode(fn, XLP_INTMODE_MSI);
-	return 0;
-}
-
-/*
- * Disables MSI-X on a controller function
- */
-static int __xlp_msix_disable(int fn)
-{
-	u64 xlp_pci_base;
-	u32 msix_ctrl;
-
-	xlp_pci_base = XLP_BDF_BASE(0,1,fn);
-	msix_ctrl = nlm_hal_read_32bit_reg(xlp_pci_base, 0x2C);
-	msix_ctrl &= ~(0x80000000);	/* MSI-X disable */
-	nlm_hal_write_32bit_reg(xlp_pci_base, 0x2C, msix_ctrl);
-	//nlm_hal_write_32bit_reg(xlp_pci_base, 0xf, 0xFF);	/* TODO Get from dev */
-	return 0;
-}
-
-int xlp_msix_disable(int fn)
-{
-	int mode = xlp_get_ctrl_intmode(fn);
-
-	if (!(mode & XLP_INTMODE_MSIX)) {
-		return -EBUSY;
-	}
-	if (xlp_decr_ctrl_intmode(fn, XLP_INTMODE_MSIX) == 0) {
-		__xlp_msix_disable(fn);
-	}
-	return 0;
-}
-
-/*
- * checks if msi is enabled for this controller
- * @fn	: controller function number
- */
-int is_msi_set(int fn)
-{
-	u64 xlp_pci_base;
-	u32 msi_en, status;
-
-	xlp_pci_base = XLP_BDF_BASE(0,1,fn);
-	msi_en = nlm_hal_read_32bit_reg(xlp_pci_base, 0x261);
-	status = (msi_en >> 9) & 1 ;
-	return status;
-}
-
-
-u32 calc_msi_vector_offset(int fn)
-{
-	u64 xlp_pci_base;
-	u32 msi_en, msi_stat;
-
-	xlp_pci_base = XLP_BDF_BASE(0,1,fn);
-	msi_en = nlm_hal_read_32bit_reg(xlp_pci_base, 0x25B);
-	msi_stat = nlm_hal_read_32bit_reg(xlp_pci_base, 0x25A);
-	nlm_hal_write_32bit_reg(xlp_pci_base, 0x25A, msi_stat);
-	msi_stat &= msi_en;
-	return msi_stat;
-}
-
-#ifdef CONFIG_PCI_MSI_XLP
-/*
- * Clears MSI-X status bits for a controller
- * @fn : controller number
- *
- * status register is Read, Write 1 to clear.
- * Figure out the mask (the bits corresponding to fn), read register, clear
- * them and return the bits corresponding to fn
- */
-u32 xlp_msix_status_clear(int fn)
-{
-	u64 xlp_pci_base;
-	u32 msix_stat;
-	u32 mask = ((XLP_MSIX_PER_SLOT - 1) << (fn * XLP_MSIX_PER_SLOT));
-
-	xlp_pci_base = XLP_BDF_BASE(0,1,fn);
-	msix_stat = nlm_hal_read_32bit_reg(xlp_pci_base, 0x25D);
-	//fdebug("mask = %#x, fn = %d, MSIX status = %#x\n", mask, fn, msix_stat);
-	msix_stat &= mask;
-	//fdebug("Masked MSIX status = %#x\n", msix_stat);
-	nlm_hal_write_32bit_reg(xlp_pci_base, 0x25D, msix_stat);
-	//fdebug("Stat cleared %#x\n", nlm_hal_read_32bit_reg(xlp_pci_base, 0x25D));
-	return (msix_stat >> (fn * XLP_MSIX_PER_SLOT));
-}
-#endif /* CONFIG_PCI_MSI_XLP */
-
-#if 0
-/* required only if xlp_ctrl_fn_from_dev() is static */
-int xlp_msi_base_vector(struct pci_dev *dev)
-{
-	return(XLP_MSI_IRQ_START(xlp_ctrl_fn_from_dev(dev)));
-}
-
-
-int xlp_msix_base_vector(struct pci_dev *dev)
-{
-	return(XLP_MSIX_IRQ_START(xlp_ctrl_fn_from_dev(dev)));
-}
-
-#endif
-
-/*
- * Masks the bit corresponding to an MSI and return the resulting bitmask
- */
-u32 xlp_msi_set_mask(int fn, int bit, int val)
-{
-	u64 xlp_pci_base;
-	u32 bits;
-
-	xlp_pci_base = XLP_BDF_BASE(0,1,fn);
-	bits = nlm_hal_read_32bit_reg(xlp_pci_base, 0x25B);
-	if (val == 0) {	/* Clear bit `bit` */
-		bits &= ~( 1 << bit);
-	} else {	/* Set bit `bit` */
-		bits |= ( 1 << bit);
-	}
-	nlm_hal_write_32bit_reg(xlp_pci_base, 0x25B, bits);
-	return bits;
-}
-
-/*
- * Finds the slot on which this device is placed and clears the MSI status
- * register on the controller
- * @dev : pci device corresponding to this device
- */
-int xlp_msi_status_clear(struct pci_dev *dev, int bit)
-{
-	int fn = 0;
-	u64 xlp_pci_base;
-	u32 msi_en;
-
-	fn = xlp_ctrl_fn_from_dev(dev);
-	if ((fn >= 4) || (fn < 0)) {
-		return -ENODEV;
-	}
-	xlp_pci_base = XLP_BDF_BASE(0,1,fn);
-	msi_en = 1 << bit;
-	nlm_hal_write_32bit_reg(xlp_pci_base, 0x25B, msi_en);
-	return 0;
-}
-
-/* Do platform specific device initialization at pci_enable_device() time */
-int pcibios_plat_dev_init(struct pci_dev *dev)
-{
-        return 0;
-}
-
-/* Enabled by default */
-static int __initdata xlp_nopci = 0;
-
-static int __init xlp_nopci_setup(char *str)
-{
-	/* Disable PCI/X/E; disables HT also */
-	xlp_nopci = 1;
-
-	return 1;
-}
-__setup("xlp_nopci", xlp_nopci_setup);
-
-static int __init pcibios_init(void)
-{
-	unsigned long phys = 0;
-	unsigned long size = 0;
-
-	if (xlp_nopci) return 0;
-
-	/* Bootloader assigns PCI resources */
-	pci_probe_only = 1;
-
-	/* Map the PCIX CFG space */
-	pci_config_base = ioremap(DEFAULT_XLP_PCI_CONFIG_BASE, DEFAULT_XLP_PCI_CONFIG_SIZE);
-	if (!pci_config_base) {
-		printk(KERN_ERR "Unable to map PCI config space!\n");
-		return 1;
-	}
-
-	phys = xlp_io_resource.start;
-	size = xlp_io_resource.end - xlp_io_resource.start + 1;
-
-	pci_io_base = ioremap(phys, size);
-	if (!pci_io_base) {
-		printk(KERN_WARNING "[%s]: Unable to IO-Remap phys=%lx, size=%lx\n",
-		       __FUNCTION__, phys, size);
-		/* Eventually this is going to panic() */
-	}
-	else {
-		printk(KERN_DEBUG "[%s]: IO-Remapped phys=%lx, size=%lx to vaddr=%p\n",
-		       __FUNCTION__, phys, size, pci_io_base);
-	}
-	set_io_port_base((unsigned long) pci_io_base);
-	xlp_controller.io_map_base = (unsigned long) pci_io_base;
-	xlp_controller.io_map_base -= xlp_controller.io_offset;
-
-	/* IO Range for 16MB from where the MEM Range Ends */
-	ioport_resource.start =  0;
-	ioport_resource.end   = ~0;
-
-	printk(KERN_DEBUG "Registering XLP PCIE Controller. \n");
-	/* Setting up controller specific data */
-	pcie_controller_init_done();
-	register_pci_controller(&xlp_controller);
-
-	pci_bus_status = 1;
-	return 0;
-}
-
-arch_initcall(pcibios_init);
-
-struct pci_fixup pcibios_fixups[] = {
-	{0}
-};
-
-- 
1.7.0.4

