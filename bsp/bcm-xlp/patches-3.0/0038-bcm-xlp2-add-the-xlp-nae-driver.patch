From 8db84549d1ec4b1a3ea65e06ba3f1715b6253c40 Mon Sep 17 00:00:00 2001
From: Jack Tan <jack.tan@windriver.com>
Date: Fri, 14 Feb 2014 17:58:12 +0800
Subject: [PATCH 38/58] bcm-xlp2: add the xlp nae driver

Based on SDK 3.0 (2013-10-29)

Signed-off-by: Jack Tan <jack.tan@windriver.com>
---
 drivers/netlogic/nae/Makefile        |   22 +
 drivers/netlogic/nae/xlpge.h         |  578 +++++++++++++
 drivers/netlogic/nae/xlpge_board.c   |   87 ++
 drivers/netlogic/nae/xlpge_eeprom.c  |  103 +++
 drivers/netlogic/nae/xlpge_ethtool.c |  502 +++++++++++
 drivers/netlogic/nae/xlpge_lro.h     |  127 +++
 drivers/netlogic/nae/xlpge_main.c    |   88 ++
 drivers/netlogic/nae/xlpge_msec.c    |   96 +++
 drivers/netlogic/nae/xlpge_nae.c     | 1584 ++++++++++++++++++++++++++++++++++
 drivers/netlogic/nae/xlpge_proc.c    |  217 +++++
 drivers/netlogic/nae/xlpge_ptp.c     |   85 ++
 drivers/netlogic/nae/xlpge_rx.c      | 1091 +++++++++++++++++++++++
 drivers/netlogic/nae/xlpge_sgmii.c   |  124 +++
 drivers/netlogic/nae/xlpge_tso.h     |  360 ++++++++
 drivers/netlogic/nae/xlpge_tx.c      |  217 +++++
 15 files changed, 5281 insertions(+)

diff --git a/drivers/netlogic/nae/Makefile b/drivers/netlogic/nae/Makefile
new file mode 100644
index 0000000..9393f36
--- /dev/null
+++ b/drivers/netlogic/nae/Makefile
@@ -0,0 +1,22 @@
+################################################################################
+#
+# Makefile for xlp_nae network driver
+#
+
+EXTRA_CFLAGS := -DCONFIG_XLP_FMN_SUPPORT -DNLM_HAL_LINUX_KERNEL
+EXTRA_CFLAGS += -Iarch/mips/netlogic/lib/syslib/include
+EXTRA_CFLAGS += -Iarch/mips/netlogic/lib/fmnlib
+EXTRA_CFLAGS += -Iarch/mips/netlogic/lib/netlib/include
+EXTRA_CFLAGS += -Iarch/mips/netlogic/boot
+
+obj-$(CONFIG_XLP_NAE_SUPPORT)   += nae.o
+nae-objs        := xlpge_main.o
+nae-objs        += xlpge_nae.o
+nae-objs        += xlpge_eeprom.o
+nae-objs        += xlpge_board.o
+nae-objs        += xlpge_ethtool.o
+nae-objs        += xlpge_proc.o
+nae-objs        += xlpge_rx.o
+nae-objs        += xlpge_sgmii.o
+nae-objs        += xlpge_tx.o
+nae-objs        += xlpge_msec.o
diff --git a/drivers/netlogic/nae/xlpge.h b/drivers/netlogic/nae/xlpge.h
new file mode 100644
index 0000000..4e58aae
--- /dev/null
+++ b/drivers/netlogic/nae/xlpge.h
@@ -0,0 +1,578 @@
+/*-
+ * Copyright (c) 2003-2012 Broadcom Corporation
+ * All Rights Reserved
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in
+ *    the documentation and/or other materials provided with the
+ *    distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY BROADCOM ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL BROADCOM OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
+ * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
+ * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
+ * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ * #BRCM_2# */
+
+#ifndef	__XLPGE_H__
+#define	__XLPGE_H__
+#include <asm/atomic.h>
+//#include <nlm_hal_nae.h>
+#include "netsoc_nae.h"
+#include "netsoc_haliface.h"
+#include "netsoc_msg.h"
+#include "netsoc_msgiface.h"
+#include "netsoc_libiface.h"
+#include "nlm_nae.h"
+#include "ext_phy.h"
+#include "nlm_msgring.h"
+
+#define	XLP_SOC_MAC_DRIVER		"mac-xlp"
+#define DRV_VERSION			"0.2"
+#define	PCI_VENDOR_NETLOGIC		0x184e
+#define	PCI_DEVICE_ID_NLM_ILK		0x1008
+#define	PCI_DEVICE_ID_NLM_NAE		0x1009
+#define	PCI_DEVICE_ID_NLM_POE		0x100a
+
+#define	MAX_TSO_SKB_PEND_REQS		200
+#define	MAX_PACKET_SZ_PER_MSG		16384
+#define	MSEC_EXTRA_MEM			(8)
+#define	P2P_SKB_OFF			(MAX_SKB_FRAGS + P2P_EXTRA_DESCS + \
+						MSEC_EXTRA_MEM - 1)
+#define	RX_IP_CSUM_VALID		(1 << 3)
+#define	RX_TCP_CSUM_VALID		(1 << 2)
+#define CPU_INDEX(cpu)			((cpu) * 8)
+#define	BYTE_OFFSET			2
+#define	NULL_VFBID			127
+#define	FACTOR				3
+#define	NLM_EEPROM_DUMP			16
+#define	NLM_EEPROM_LEN			(NLM_EEPROM_DUMP * FACTOR)
+#define	NLM_NUM_REG_DUMP		9	/* Register 0xa0 to 0xa8 */
+#define	NLM_ETHTOOL_REG_LEN		(NLM_NUM_REG_DUMP * 4)
+#define	RX_PARSER_EN			1
+#ifdef CONFIG_NLM_ENABLE_LOAD_BALANCING
+#define	RX_PPAD_EN			1
+#define	RX_PPAD_SZ			0
+#else
+#define	RX_PPAD_EN			0
+#define	RX_PPAD_SZ			3
+#endif
+
+extern int num_cpus_per_node;
+
+/* TODO XXX: default enable prepad */
+#ifdef CONFIG_NLM_ENABLE_LOAD_BALANCING
+#define	PREPAD_LEN			16
+#define	NLM_LOAD_BALANCING_MAGIC	0xdeadbeefU
+#else
+#define	PREPAD_LEN			0
+#endif
+#define	ETH_JUMBO_DATA_LEN		16000
+#define	CACHELINE_SIZE			(1ULL << 6)
+#define	SKB_BACK_PTR_SIZE		CACHELINE_SIZE
+#define NAE_BACK_PTR_SIZE		CACHELINE_SIZE
+#define	LRO_MAX_DESCS			8
+#define MSG_DST_FC_FAIL			0x01
+#define	MSG_INFLIGHT_MSG_EX		0x02
+#define	MSG_TXQ_FULL			0x04
+#define	NLM_TCP_MODE			1
+#define	NLM_RT_MODE			2
+#define	NLM_TCP_LOAD_BALANCE_MODE	4
+#define NLM_PORT_FIFO_EN		8 // can co-exist with any of the above 3 modes 
+#define	TSO_ENABLED			1
+#define	R_TX_CONTROL			0x0a0
+#define	TX_PACKET_COUNTER		0x39
+#define	RX_PACKET_COUNTER		0x28
+#define	TX_BYTE_COUNTER			0x38
+#define	RX_BYTE_COUNTER			0x27
+#define	TX_FCS_ERROR_COUNTER		0x47
+#define	TX_JABBER_FRAME_COUNTER		0x46
+#define	RX_DROP_PACKET_COUNTER		0x37
+#define	TX_DROP_FRAME_COUNTER		0x45
+#define	RX_MULTICAST_PACKET_COUNTER	0x2a
+#define	TX_TOTAL_COLLISION_COUNTER	0x43
+#define	RX_FRAME_LENGTH_ERROR_COUNTER	0x30
+#define	RX_FCS_ERROR_COUNTER		0x29
+#define	RX_JABBER_COUNTER		0x36
+#define	RX_ALIGNMENT_ERROR_COUNTER	0x2f
+#define	RX_CARRIER_SENSE_ERROR_COUNTER	0x32
+#define	TX_EXCESSIVE_COLLISION_PACKET_COUNTER	0x42
+#define	MAC_HEADER_LEN			12
+#define	MAC_ADDR_LEN			6
+#define	MACSEC_ETHER_TYPE		0x88e5
+#define	PROTOCOL_TYPE_IP		0x0800
+#define	MAC_SEC_PADDING			(12+16+16+16)
+#ifdef	CONFIG_NLM_ENABLE_LOAD_BALANCING
+#define	KB(x)				(x<<10)
+#define	NLM_UCORE_SHARED_TABLE_SIZE	((KB(16))/(sizeof(uint32_t)))
+#define	NLM_UCORE_SHMEM_OFFSET		(KB(16))
+#define	NUM_LOAD_BALANCE_CPU		(num_cpus_per_node)
+#endif
+
+#define	NLM_RX_ETH_BUF_SIZE		(ETH_DATA_LEN + ETH_HLEN + 	\
+	ETH_FCS_LEN + BYTE_OFFSET + PREPAD_LEN + SKB_BACK_PTR_SIZE +	\
+	CACHELINE_SIZE + NAE_BACK_PTR_SIZE)
+#define	NLM_RX_JUMBO_BUF_SIZE		(ETH_JUMBO_DATA_LEN +		\
+	ETH_HLEN + ETH_FCS_LEN + BYTE_OFFSET + PREPAD_LEN +		\
+	SKB_BACK_PTR_SIZE + CACHELINE_SIZE + NAE_BACK_PTR_SIZE)
+
+#define	CACHELINE_ALIGNED_ADDR(addr)	\
+	(((ulong)(addr)) & ~(CACHELINE_SIZE - 1))
+#define	P2P_EXTRA_DESCS			\
+	((PAGE_SIZE / MAX_PACKET_SZ_PER_MSG) + 4)
+
+#ifndef __ASSEMBLY__
+/*
+ * This macro resets first 164 (offsetof(struct sk_buff, tail))bytes
+ *  of skb header
+ */
+#define fast_reset_skbptrs(skb) \
+		*(uint64_t *)(ulong)((uint64_t *)skb + 0) = 0;		\
+		*(uint64_t *)(ulong)((uint64_t *)skb + 1) = 0;		\
+		*(uint64_t *)(ulong)((uint64_t *)skb + 2) = 0;		\
+		*(uint64_t *)(ulong)((uint64_t *)skb + 3) = 0;		\
+		*(uint64_t *)(ulong)((uint64_t *)skb + 4) = 0;		\
+		*(uint64_t *)(ulong)((uint64_t *)skb + 5) = 0;		\
+		*(uint64_t *)(ulong)((uint64_t *)skb + 6) = 0;		\
+		*(uint64_t *)(ulong)((uint64_t *)skb + 7) = 0;		\
+		*(uint64_t *)(ulong)((uint64_t *)skb + 8) = 0;		\
+		*(uint64_t *)(ulong)((uint64_t *)skb + 9) = 0;		\
+		*(uint64_t *)(ulong)((uint64_t *)skb + 10) = 0;		\
+		*(uint64_t *)(ulong)((uint64_t *)skb + 11) = 0;		\
+		*(uint64_t *)(ulong)((uint64_t *)skb + 12) = 0;		\
+		*(uint64_t *)(ulong)((uint64_t *)skb + 13) = 0;		\
+		*(uint64_t *)(ulong)((uint64_t *)skb + 14) = 0;		\
+		*(uint64_t *)(ulong)((uint64_t *)skb + 15) = 0;		\
+		*(uint64_t *)(ulong)((uint64_t *)skb + 16) = 0;		\
+		*(uint64_t *)(ulong)((uint64_t *)skb + 17) = 0;		\
+		*(uint64_t *)(ulong)((uint64_t *)skb + 18) = 0;		\
+		*(uint64_t *)(ulong)((uint64_t *)skb + 19) = 0;		\
+		*(uint32_t *)(ulong)((uint64_t *)skb + 20) = 0;
+/*
+ * This helper macro resets SKB data pointers for reuse
+ * as free-in buffer
+*/
+#define skb_reset_ptrs(skb)						\
+do {									\
+	struct skb_shared_info *shinfo;					\
+									\
+	shinfo = skb_shinfo(skb);					\
+									\
+	/* Now reinitialize old skb, cut & paste from dev_alloc_skb */	\
+	/*memset(skb, 0, offsetof(struct sk_buff, tail));*/		\
+	fast_reset_skbptrs(skb);					\
+	skb->data = skb->head;						\
+	skb_reset_tail_pointer(skb);					\
+									\
+	atomic_set(&shinfo->dataref, 1);				\
+	shinfo->nr_frags  = 0;						\
+	shinfo->gso_size = 0;						\
+	shinfo->gso_segs = 0;						\
+	shinfo->gso_type = 0;						\
+	shinfo->ip6_frag_id = 0;					\
+	shinfo->frag_list = NULL;					\
+} while (0)
+
+#define Message(fmt, args...) { }
+
+enum msc_opcodes {
+	IP_CHKSUM = 1,
+	TCP_CHKSUM,
+	UDP_CHKSUM,
+	SCTP_CRC,
+	FCOE_CRC,
+	IP_TCP_CHKSUM,
+	TSO_IP_TCP_CHKSUM,
+	IP_UDP_CHKSUM,
+	IP_CHKSUM_SCTP_CRC
+};
+
+typedef enum {
+	xlp_mac_speed_10,
+	xlp_mac_speed_100,
+	xlp_mac_speed_1000,
+	xlp_mac_speed_rsvd
+} xlp_mac_speed_t;
+
+typedef enum {
+	xlp_mac_duplex_auto,
+	xlp_mac_duplex_half,
+	xlp_mac_duplex_full
+} xlp_mac_duplex_t;
+
+typedef enum {
+	xlp_mac_fc_auto,
+	xlp_mac_fc_disabled,
+	xlp_mac_fc_frame,
+	xlp_mac_fc_collision,
+	xlp_mac_fc_carrier
+} xlp_mac_fc_t;
+
+struct cpu_stat {
+	ulong tx_packets;
+	ulong txc_packets;
+	ulong rx_packets;
+	ulong interrupts;
+};
+
+struct phy_info {
+        int addr;
+        int mode;
+        uint32_t *mii_addr;
+        uint32_t *pcs_addr;
+        uint32_t *serdes_addr;
+};
+
+#ifdef CONFIG_NLM_ENABLE_LOAD_BALANCING
+/*Flow Meta Info*/
+struct flow_meta_info
+{
+	volatile uint64_t total_bytes_rcvd;
+	volatile uint64_t last_sample_bytes;
+	volatile uint64_t cpu_owner;
+	uint64_t pad[5];
+};
+
+/*Active flow list*/
+struct active_flow_list
+{
+	volatile int index_to_flow_meta_info[NLM_UCORE_SHARED_TABLE_SIZE];
+	volatile uint64_t cpu_data_rate;
+	spinlock_t lock;
+	volatile uint64_t nr_active_flows;
+	volatile uint64_t nr_flow_created;
+	volatile uint64_t nr_flow_processed;
+	uint64_t pad[3];
+};
+#endif
+
+struct dev_data
+{
+	struct net_device *dev;
+	struct net_device_stats stats;
+	struct cpu_stat cpu_stats[NR_CPUS];
+	struct timer_list link_timer;
+	struct napi_struct napi;
+	nae_t* nae;
+	net_port_t *nae_port;
+	spinlock_t lock;
+	unsigned short port;
+	unsigned short inited;
+	unsigned short node;
+	unsigned short block;
+	unsigned short index;
+	unsigned short type;
+	unsigned int   port_index;
+	struct sk_buff* skb;
+	int phy_oldlinkstat;
+	__u8 hwaddr[6];
+
+	xlp_mac_speed_t speed;		/* current speed */
+	xlp_mac_duplex_t duplex;	/* current duplex */
+	xlp_mac_fc_t flow_ctrl;		/* current flow control setting */
+	int advertising;
+	struct phy_info phy;
+	int nae_rx_qid;
+	int nae_tx_qid;
+	int hw_port_id;
+	int mgmt_port;
+	struct net_lro_mgr lro_mgr[NR_CPUS];
+	struct net_lro_desc lro_arr[NR_CPUS][LRO_MAX_DESCS];
+
+	/*1588 ptp timer*/
+	struct cyclecounter cycles;
+	struct timecounter clock;
+	//struct timecompare compare;
+};
+
+
+struct p2p_desc_mem {
+	void *mem;
+	uint64_t dsize;
+	uint64_t pad[6];
+};
+
+extern void *fdt;
+
+static __inline__ void cpu_halt(void)
+{
+	__asm__ volatile (
+			".set push		\n"
+			".set noreorder		\n"
+			"   wait		\n"
+			"1: b    1b		\n"
+			"   nop			\n"
+			".set pop		\n"
+			);
+}
+
+static __inline__ struct sk_buff *mac_get_skb_back_ptr(uint64_t addr)
+{
+	uint64_t *back_ptr = (uint64_t *)(ulong)(addr - SKB_BACK_PTR_SIZE);
+	return (struct sk_buff *)(ulong)(*back_ptr);
+}
+
+static __inline__ nae_t *mac_get_nae_back_ptr(uint64_t addr)
+{
+	uint64_t *back_ptr = (uint64_t *)(ulong)(addr - SKB_BACK_PTR_SIZE  + sizeof(void*));
+	return (nae_t*)(ulong)(*back_ptr);
+}
+
+/**********************************************************************
+ * cacheline_aligned_kmalloc -  64 bits cache aligned kmalloc
+ * return -  buffer address
+ *
+ **********************************************************************/
+static __inline__ void *cacheline_aligned_kmalloc(int size, int gfp_mask)
+{
+	void *buf = kmalloc(size + CACHELINE_SIZE, gfp_mask);
+
+	if (buf == NULL)
+		return NULL;
+
+	buf =(void *)(ulong)(CACHELINE_ALIGNED_ADDR((ulong)buf +
+				CACHELINE_SIZE));
+	return buf;
+}
+
+/**********************************************************************
+ * nlm_xlp_alloc_skb_atomic -  Atomically allocates cache aligned skb buffer
+ * return - skb buffer address
+ *
+ **********************************************************************/
+static __inline__ struct sk_buff *nlm_xlp_alloc_skb_atomic(int length, int node)
+{
+	int offset = 0;
+	gfp_t gfp_mask = GFP_ATOMIC;
+	struct sk_buff *skb = __alloc_skb(length + NET_SKB_PAD, gfp_mask, 0,
+						node);
+
+	if (!skb) 
+		return NULL;
+
+	skb_reserve(skb, NET_SKB_PAD);
+
+	/* align the data to the next cache line */
+	offset = ((ulong)skb->data + CACHELINE_SIZE) & ~(CACHELINE_SIZE - 1);
+	skb_reserve(skb, (offset - (ulong)skb->data));
+#ifdef CONFIG_NLM_NET_OPTS
+	skb->netl_skb = skb;
+#endif
+	return skb;
+}
+
+static __inline__ void mac_put_skb_back_ptr(struct sk_buff *skb, nae_t* naeptr)
+{
+	uint64_t *back_ptr = (uint64_t *)skb->data;
+
+	skb_reserve(skb, SKB_BACK_PTR_SIZE);
+	*back_ptr = (uint64_t)(ulong)skb;
+	back_ptr++;	
+	*back_ptr = (uint64_t)(ulong)naeptr;
+}
+
+static __inline__ void print_fmn_send_error(const char *str,
+					    uint32_t send_result)
+{
+	if(send_result & MSG_DST_FC_FAIL)
+	{
+		printk("[%s] Msg Destination flow control credit fail "
+			"(send_result=%08x)\n",
+			str, send_result);
+	}
+	else if (send_result & MSG_INFLIGHT_MSG_EX)
+		printk("[%s] MSG_INFLIGHT_MSG_EX(send_result=%08x)\n",
+			__func__, send_result);
+	else if (send_result & MSG_TXQ_FULL)
+		printk("[%s] TX message Q full(send_result=%08x)\n",
+			__func__, send_result);
+	else
+		printk("[%s] Unknown send error type(send_result=%08x)\n",
+			__func__, send_result);
+}
+
+static __inline__ void dump_buffer(unsigned char *buf, uint32_t len,
+				   unsigned char *msg)
+{
+	int k, olen = 0;
+	char out[512];
+
+	printk("\n%s\n", msg);
+
+	for (k = 0; k < len; k++) {
+		olen += snprintf(&out[olen], 512 - olen, "<%.2x> ", buf[k]);
+		if ((k + 1) % 16 == 0)
+			olen += snprintf(&out[olen], 512 - olen, "\n");
+	}
+	printk("%s\n", out);
+}
+
+static inline int mac_refill_frin_skb(nae_t* nae_cfg, int cpu, uint64_t paddr,
+			       uint32_t bufsize, int hw_port_id)
+{
+	/*
+	 * We have to use the logical map here as the below arrays are
+	 * indexed by logical cpu id
+	 */
+	int ret, code, qid;
+	ulong __attribute__ ((unused)) mflags;
+	int node = nae_cfg->node;
+	int nae_id = nae_cfg->nae_id;
+	int node_cpu;
+	extern uint32_t cpu_2_normal_frfifo[NLM_MAX_NODES][MAX_NAE_PERNODE][NR_CPUS];
+	extern uint32_t cpu_2_jumbo_frfifo[NLM_MAX_NODES][MAX_NAE_PERNODE][NR_CPUS];
+	extern int phys_cpu_to_log_map[NLM_MAX_NODES][MAX_NAE_PERNODE][NR_CPUS];
+	extern int __maybe_unused enable_jumbo;
+
+	node_cpu = phys_cpu_to_log_map[node][nae_id][cpu];
+
+	if (nae_cfg == NULL) {
+		printk("%s Error, Invalid node id %d\n", __FUNCTION__, node);
+		return -1;
+	}
+
+	if(nae_cfg->port_fifo_en)
+		qid = hw_port_id;
+	else
+		qid = (bufsize >= NLM_RX_JUMBO_BUF_SIZE) ?
+		cpu_2_jumbo_frfifo[node][nae_id][node_cpu] :
+		cpu_2_normal_frfifo[node][nae_id][node_cpu];
+
+	Message("%s in cpu %d bufsize %d node %d qid %d qbase %d node_cpu %d nae_id %d\n",
+		__FUNCTION__, cpu, bufsize, node, qid,
+		nae_cfg->frin_queue_base, node_cpu, nae_id);
+
+	ret = 0;
+	qid = nae_cfg->frin_queue_base + qid;
+
+	/* Assumption: SKB is all set to go */
+	/* Send the free Rx desc to the MAC */
+	code = 0;
+
+	//* Send the packet to nae rx  */
+	msgrng_access_enable(mflags);
+	for (;;) {
+		//ret = netsoc_nae_send_freein_buf(nae_cfg, qid, paddr & 0xffffffffffULL);
+	  	ret = xlp_message_send_1(qid, code, (paddr & 0xffffffffffULL));
+	  if (!ret) break;
+	}
+	msgrng_access_disable(mflags);
+
+	return ret;
+}
+
+static inline int mac_refill_frin_one_buffer(struct net_device *dev, int cpu,
+				      uint32_t truesize)
+{
+	struct dev_data* priv = netdev_priv(dev);
+	struct sk_buff * skb;
+	int buf_size = NLM_RX_ETH_BUF_SIZE;
+	nae_t* nae_cfg = priv->nae;
+	extern int enable_jumbo;
+
+	if (enable_jumbo)
+		if(truesize > NLM_RX_JUMBO_BUF_SIZE)
+			buf_size = NLM_RX_JUMBO_BUF_SIZE;
+
+	skb = nlm_xlp_alloc_skb_atomic(buf_size, priv->node);
+	if (!skb) {
+		printk("[%s] alloc skb failed\n",__FUNCTION__);
+		panic("panic...");
+		return -ENOMEM;
+	}
+
+	skb->dev = dev;
+
+	mac_put_skb_back_ptr(skb, nae_cfg);
+
+	return mac_refill_frin_skb(nae_cfg, cpu, (uint64_t)virt_to_bus(skb->data), buf_size, priv->hw_port_id);
+}
+
+
+void nlm_xlp_nae_init(void);
+void nlm_xlp_nae_remove(void);
+extern int xlpge_eeprom_init(void);
+void init_phy_state_timer(void *);
+inline void process_tx_complete(int , uint32_t , uint64_t);
+struct eeprom_data * get_nlm_eeprom(void);
+unsigned int nlm_xlp_mac_mii_read(struct dev_data *, int);
+void nlm_xlp_mac_mii_write(struct dev_data *, int , uint16_t);
+int xlp_enable_autoneg(struct net_device *, u32);
+int xlp_set_link_speed(struct net_device *, int , int);
+void nlm_nae_remove_procentries(void);
+void xlp_get_mac_stats(struct net_device *, struct net_device_stats *);
+int eth_mac_addr(struct net_device *, void *);
+int nlm_xlp_nae_start_xmit(struct sk_buff *, struct net_device *);
+void xlp_set_ethtool_ops(struct net_device *);
+cycle_t nlm_1588_read_clock0(const struct cyclecounter *);
+cycle_t nlm_1588_read_clock1(const struct cyclecounter *);
+cycle_t nlm_1588_read_clock2(const struct cyclecounter *);
+cycle_t nlm_1588_read_clock3(const struct cyclecounter *);
+void gen_mac_address(void);
+int xlp_mac_proc_show(struct seq_file *m, void *v);
+int nae_proc_show(struct seq_file *m, void *v);
+int xlp_mac_proc_read(char *, char **, off_t , int , int *, void *);
+int nae_proc_read(char *, char **, off_t , int , int *, void *);
+int nlm_xlp_disable_napi(void);
+void nlm_spawn_kthread(void);
+int nlm_xlp_enable_napi(void);
+//int mac_refill_frin_skb(nae_t* , int , uint64_t , uint32_t, int);
+//int mac_refill_frin_one_buffer(struct net_device *, int , uint32_t);
+void nlm_xlp_mac_set_enable(struct dev_data *priv, int flag);
+void xlp_poll_upper(int);
+//inline int tso_xmit_skb(struct sk_buff *, struct net_device *);
+int xlp_config_msec_tx(struct net_device *, struct ethtool_cmd *);
+int xlp_config_msec_tx_mem(struct net_device *, struct ethtool_cmd *);
+int xlp_config_msec_rx(struct net_device *, struct ethtool_cmd *);
+int xlp_config_msec_rx_mem(struct net_device *, struct ethtool_cmd *);
+int nlm_load_balance_proc_open(struct inode *, struct file *);
+void nlm_init_load_balance(void);
+
+extern int enable_lro; 	
+extern unsigned char eth_hw_addr[NLM_MAX_NODES][MAX_GMAC_PORT][6];
+extern struct proc_dir_entry *nlm_root_proc;
+extern uint64_t receive_count[NR_CPUS * 8] __cacheline_aligned;
+extern uint64_t fast_replenish_count[NR_CPUS * 8] __cacheline_aligned;
+extern uint64_t slow_replenish_count[NR_CPUS * 8] __cacheline_aligned;
+extern uint64_t err_replenish_count[NR_CPUS * 8] __cacheline_aligned;
+extern uint64_t p2p_dynamic_alloc_cnt[NR_CPUS * 8] __cacheline_aligned;
+extern struct net_device *xlp_dev_mac[NLM_MAX_NODES][MAX_GMAC_PORT];
+extern struct net_device *
+	per_cpu_netdev[NLM_MAX_NODES][NR_CPUS][2][24] __cacheline_aligned;
+extern int lro_flush_priv_cnt[NR_CPUS];
+extern int lro_flush_needed[NR_CPUS][20];
+extern struct dev_data *lro_flush_priv[NR_CPUS][20];
+extern uint32_t nae_rx_vc;
+extern uint32_t nae_fb_vc;
+extern int exclusive_vc;
+extern int enable_napi;
+extern int perf_mode;
+extern uint64_t nlm_mode[];
+extern int nlm_prepad_len;
+extern int load_balance_timer_run;
+
+#ifdef CONFIG_NLM_NET_OPTS
+extern struct sk_buff *last_rcvd_skb[NR_CPUS * 8] ____cacheline_aligned;
+extern uint32_t last_rcvd_len[NR_CPUS * 8] ____cacheline_aligned;
+extern uint32_t last_rcvd_node[NR_CPUS * 8] ____cacheline_aligned;
+extern uint64_t last_rcvd_skb_phys[NR_CPUS * 8] ____cacheline_aligned;
+extern int get_hw_frfifo_queue_id(int rxnode, nlm_nae_config_ptr nae_cfg,
+                                  int cpu, uint32_t truesize, int hw_port_id);
+#endif
+
+
+#endif /* __ASSEMBLY__ */
+#endif
diff --git a/drivers/netlogic/nae/xlpge_board.c b/drivers/netlogic/nae/xlpge_board.c
new file mode 100644
index 0000000..bf5c3ee
--- /dev/null
+++ b/drivers/netlogic/nae/xlpge_board.c
@@ -0,0 +1,87 @@
+/*-
+ * Copyright (c) 2003-2012 Broadcom Corporation
+ * All Rights Reserved
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in
+ *    the documentation and/or other materials provided with the
+ *    distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY BROADCOM ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL BROADCOM OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
+ * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
+ * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
+ * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ * #BRCM_2# */
+
+#include <linux/module.h>
+#include <linux/pci.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/inet_lro.h>
+#include <linux/clocksource.h>
+
+#include <nlm_hal_nae.h>
+#include <nlm_eeprom.h>
+#include "xlpge.h"
+
+extern struct eeprom_data * get_nlm_eeprom(void);
+unsigned char eth_hw_addr[NLM_MAX_NODES][MAX_GMAC_PORT][6];
+
+void gen_mac_address(void)
+{
+	struct eeprom_data *nlm_eeprom=NULL;
+        unsigned char mac_base[6],temp,buf_write[2],buf0_read[2],buf1_read[2];
+        int if_mac_set=0,mac0_set=0, mac1_set=0;
+        int i,j;
+        buf_write[0]= MAC_MAGIC_BYTE0;
+        buf_write[1]= MAC_MAGIC_BYTE1;
+
+        memset(mac_base, '0', 6);
+        nlm_eeprom = get_nlm_eeprom();
+
+        eeprom_get_magic_bytes(nlm_eeprom,buf0_read,0);/* signature*/
+        eeprom_get_magic_bytes(nlm_eeprom,buf1_read,1);
+
+        if((buf0_read[0]==buf_write[0]) && (buf0_read[1]==buf_write[1]))/*match the signature*/
+        {
+                mac0_set=1;
+                eeprom_get_mac_addr(nlm_eeprom, mac_base,0);/* get the mac address*/
+        }
+        else if((buf1_read[0]==buf_write[0]) && (buf1_read[1]==buf_write[1]))
+        {
+                mac1_set=1;
+                eeprom_get_mac_addr(nlm_eeprom, mac_base,1);/* get the mac address*/
+        }
+
+        for(temp=0;temp<6;temp++)
+        {
+                if(mac_base[temp]!=0)
+                {
+                        if_mac_set=1;
+                }
+        }
+        if( ((mac0_set | mac1_set) && if_mac_set) == 0){
+                random_ether_addr(mac_base);
+        }
+	for(i=0 ; i<NLM_MAX_NODES; i++){ /*poppulate the eth_hw_add array according to the get mac address*/
+                for(j=0;j<18;j++){
+                        memcpy(eth_hw_addr[i][j], mac_base, 6);
+                        mac_base[5] += 1;
+                }
+        }
+
+}
diff --git a/drivers/netlogic/nae/xlpge_eeprom.c b/drivers/netlogic/nae/xlpge_eeprom.c
new file mode 100644
index 0000000..f849871
--- /dev/null
+++ b/drivers/netlogic/nae/xlpge_eeprom.c
@@ -0,0 +1,103 @@
+/*-
+ * Copyright (c) 2003-2013 Broadcom Corporation
+ * All Rights Reserved
+ * 
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in
+ *    the documentation and/or other materials provided with the
+ *    distribution.
+ * 
+ * THIS SOFTWARE IS PROVIDED BY BROADCOM ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL BROADCOM OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
+ * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
+ * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
+ * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ * 
+ * #BRCM_2# */
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/slab.h>
+#include <linux/jiffies.h>
+#include <linux/i2c.h>
+#include <linux/mutex.h>
+#include <nlm_eeprom.h>
+
+struct eeprom_data nlm_eeprom;
+struct eeprom_data *get_nlm_eeprom(void)
+{
+        return &nlm_eeprom;
+}
+EXPORT_SYMBOL(get_nlm_eeprom);
+int eeprom_read (int devaddr, int addr, int alen, u8 * rd_buf, int len);
+int eeprom_write (int devaddr, int addr, int alen,u8 * wr_buf, int len);
+struct i2c_client *global_client=NULL;
+
+int eeprom_write (int devaddr, int addr, int alen, u8 * wr_buf, int len)
+{
+	struct i2c_client *client;
+        client=global_client;   
+       	if(client==NULL){
+		return -1;
+	}
+	i2c_smbus_write_i2c_block_data(client, addr, len, wr_buf);	
+	return 0;
+}
+
+int eeprom_read (int devaddr, int addr, int alen, u8 *rd_buf, int len)
+{
+	struct i2c_client *client;
+	client=global_client;	
+       	if(client==NULL){
+		return -1;
+	}
+	memset(rd_buf, '0', sizeof(rd_buf));	
+	i2c_smbus_read_i2c_block_data(client, addr, len, rd_buf);
+	return 0;
+}
+
+static int xlpge_eeprom_probe(struct i2c_client *client,
+			const struct i2c_device_id *id)	
+{
+  	
+	global_client=client;
+	return 0;
+}
+
+static const struct i2c_device_id xlpge_eeprom_id[] = {
+        { "xlpge_eeprom", 0 },
+        { }
+};
+
+static struct i2c_driver xlpge_eeprom_driver = {
+        .driver = {
+                .name   = "xlpge_eeprom",
+        },
+        .probe          = xlpge_eeprom_probe,
+        .id_table       = xlpge_eeprom_id,
+};
+
+
+int xlpge_eeprom_init(void)
+{
+	struct eeprom_data *nlm_eeprom=NULL;
+	
+	nlm_eeprom=get_nlm_eeprom();
+	nlm_eeprom->i2c_dev_addr=0x57;
+        nlm_eeprom->eeprom_i2c_read_bytes = (void *)eeprom_read;
+	nlm_eeprom->eeprom_i2c_write_bytes = (void *)eeprom_write;	
+        return i2c_add_driver(&xlpge_eeprom_driver);
+}
+EXPORT_SYMBOL(xlpge_eeprom_init);
diff --git a/drivers/netlogic/nae/xlpge_ethtool.c b/drivers/netlogic/nae/xlpge_ethtool.c
new file mode 100644
index 0000000..418c36b
--- /dev/null
+++ b/drivers/netlogic/nae/xlpge_ethtool.c
@@ -0,0 +1,502 @@
+/*-
+ * Copyright (c) 2003-2012 Broadcom Corporation
+ * All Rights Reserved
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in
+ *    the documentation and/or other materials provided with the
+ *    distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY BROADCOM ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL BROADCOM OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
+ * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
+ * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
+ * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ * #BRCM_2# */
+
+#include <linux/module.h>
+#include <linux/pci.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/inet_lro.h>
+#include <linux/clocksource.h>
+#include <linux/ethtool.h>
+#include <linux/mii.h>
+
+#include <nlm_hal_nae.h>
+#include <nlm_eeprom.h>
+#include "xlpge.h"
+static u32 xlp_get_link(struct net_device *dev);
+extern struct eeprom_data * get_nlm_eeprom(void);
+#define	NLM_STATS_KEY_LEN	\
+	(sizeof(struct net_device_stats) / sizeof(ulong))
+
+static struct {
+	const char string[ETH_GSTRING_LEN];
+} xlp_ethtool_stats_keys[NLM_STATS_KEY_LEN] = {
+	{ "rx_packets" },
+	{ "tx_packets" },
+	{ "rx_bytes" },
+	{ "tx_bytes" },
+	{ "rx_errors" },
+	{ "tx_errors" },
+	{ "rx_dropped" },
+	{ "tx_dropped" },
+	{ "multicast" },
+	{ "collisions" },
+	{ "rx_length_errors" },
+	{ "rx_over_errors" },
+	{ "rx_crc_errors" },
+	{ "rx_frame_errors" },
+	{ "rx_fifo_errors" },
+	{ "rx_missed_errors" },
+	{ "tx_aborted_errors" },
+	{ "tx_carrier_errors" },
+	{ "tx_fifo_errors" },
+	{ "tx_heartbeat_errors" },
+	{ "tx_window_errors" },
+	{ "rx_compressed" },
+	{ "tx_compressed" }
+};
+
+static int xlp_get_settings(struct net_device *dev, struct ethtool_cmd *cmd)
+{
+	struct dev_data *priv = netdev_priv(dev);
+	net_port_t* nae_port = priv->nae_port;
+	if (priv->type != SGMII_IF) {
+		cmd->supported = SUPPORTED_FIBRE | SUPPORTED_10000baseT_Full;
+		cmd->advertising = SUPPORTED_FIBRE | SUPPORTED_10000baseT_Full;
+		cmd->speed = SPEED_10000;
+		cmd->port = PORT_FIBRE;
+		cmd->duplex = DUPLEX_FULL;
+		cmd->phy_address = priv->port;
+		cmd->autoneg = AUTONEG_DISABLE;
+		cmd->maxtxpkt = 0;
+		cmd->maxrxpkt = 0;
+
+		return 0;
+	}
+
+	cmd->supported = SUPPORTED_10baseT_Full			|
+				SUPPORTED_10baseT_Half		|
+				SUPPORTED_100baseT_Full		|
+				SUPPORTED_100baseT_Half		|
+				SUPPORTED_1000baseT_Full	|
+				SUPPORTED_MII			|
+				SUPPORTED_Autoneg		|
+				SUPPORTED_TP;
+
+	cmd->advertising = priv->advertising;
+
+	//nlm_hal_status_ext_phy( priv->node, priv->phy.addr, &mii_info);
+	netsoc_get_phy_status(nae_port, (uint32_t *)&cmd->duplex, (uint32_t *)&cmd->speed);
+        //priv->speed = mii_info.speed;
+        //cmd->duplex = mii_info.duplex;
+	cmd->speed = (priv->speed == xlp_mac_speed_1000) ? SPEED_1000 :
+			(priv->speed == xlp_mac_speed_100) ?
+				SPEED_100 : SPEED_10;
+
+	cmd->port = PORT_TP;
+	//cmd->phy_address = mii_info.phyaddr;
+	cmd->phy_address = priv->phy.addr;
+	cmd->transceiver = XCVR_INTERNAL;
+	cmd->autoneg  = 1; /*Autoneg is always enabled by default*/
+	cmd->maxtxpkt = 0;
+	cmd->maxrxpkt = 0;
+
+	return 0;
+}
+
+static int xlp_set_settings(struct net_device *dev, struct ethtool_cmd *cmd)
+{
+	struct dev_data *priv = netdev_priv(dev);
+	int ret;
+
+	if (priv->type != SGMII_IF)
+		return -EIO;
+
+	if (cmd->autoneg == AUTONEG_ENABLE)
+		ret = xlp_enable_autoneg(dev, cmd->advertising);
+	else
+		ret = xlp_set_link_speed(dev, cmd->speed, cmd->duplex);
+
+	return ret;
+}
+
+static void xlp_get_drvinfo(struct net_device *dev,
+			    struct ethtool_drvinfo *info)
+{
+	strcpy(info->driver, XLP_SOC_MAC_DRIVER);
+	strcpy(info->version, DRV_VERSION);
+}
+
+static int xlp_get_regs_len(struct net_device *dev)
+{
+	return NLM_ETHTOOL_REG_LEN;
+}
+
+static int xlp_get_eeprom_len(struct net_device *dev)
+{
+	return NLM_EEPROM_LEN;
+}
+
+static int xlp_get_eeprom(struct net_device *dev,
+			  struct ethtool_eeprom *eeprom, u8* temp)
+{
+	int i=0;
+	struct eeprom_data *nlm_eeprom=NULL;
+	struct dev_data *priv = netdev_priv(dev);
+	ulong flags;
+	u8 buff[50];
+
+	nlm_eeprom = get_nlm_eeprom();
+
+	spin_lock_irqsave(&priv->lock, flags);
+
+	eeprom_dump(nlm_eeprom, buff, eeprom->offset,eeprom->len);
+
+	for(i = 0; i < eeprom->len; i++)
+		temp[i] = buff[i];
+
+	spin_unlock_irqrestore(&priv->lock, flags);
+
+	return 0;
+}
+
+static int xlp_set_eeprom(struct net_device *dev,
+			  struct ethtool_eeprom *eeprom, u8* temp)
+{
+	struct eeprom_data *nlm_eeprom=NULL;
+	struct dev_data *priv = netdev_priv(dev);
+	ulong flags;
+	u8 data[6];
+
+	nlm_eeprom = get_nlm_eeprom();
+
+	eeprom_get_mac_addr(nlm_eeprom, data, 0);
+
+	spin_lock_irqsave(&priv->lock, flags);
+
+	if (eeprom->magic == 0xAA) {
+		data[eeprom->offset] = *temp;
+		eeprom_set_mac_addr(nlm_eeprom, data, 0);
+	}
+
+	spin_unlock_irqrestore(&priv->lock, flags);
+
+	return 0;
+}
+
+static void xlp_get_pauseparam(struct net_device *dev,
+                                  struct ethtool_pauseparam *pause)
+{
+        struct dev_data *priv = netdev_priv(dev);
+
+	pause->autoneg = 1;
+	if(priv->flow_ctrl==1)
+	{
+        	pause->rx_pause= 1;
+        	pause->tx_pause= 1;
+	}
+        else
+	{
+                pause->rx_pause=0;
+                pause->tx_pause=0;
+        }
+}
+static int xlp_set_pauseparam(struct net_device *dev,
+                                struct ethtool_pauseparam *pause)
+{
+        struct dev_data *priv = netdev_priv(dev);
+        net_port_t *netport=&priv->nae->ports[priv->port];
+
+	if(pause->rx_pause && pause->tx_pause) {
+                netsoc_enable_flow_control(netport);
+                priv->flow_ctrl=1;
+        }
+        else {
+                netsoc_disable_flow_control(netport);
+                priv->flow_ctrl=0;
+        }
+        return 0;
+}
+
+static void xlp_get_regs(struct net_device *dev,
+			 struct ethtool_regs *regs, void *p)
+{
+	struct dev_data *priv = netdev_priv(dev);
+	ulong flags;
+	u32 *data = (u32 *)p;
+
+	memset((void *)data, 0, NLM_ETHTOOL_REG_LEN);
+
+	spin_lock_irqsave(&priv->lock, flags);
+//TODO:
+#if 0
+	for(i=0; i <= NLM_NUM_REG_DUMP; i++)
+		*(data + i) = nlm_hal_read_mac_reg(priv->node, priv->block,
+					priv->index, R_TX_CONTROL + i);
+#endif
+
+	spin_unlock_irqrestore(&priv->lock, flags);
+}
+
+static u32 xlp_get_msglevel(struct net_device *dev)
+{
+	return 0;
+}
+
+static void xlp_set_msglevel(struct net_device *dev, u32 value)
+{
+	//mac_debug = value;
+}
+
+static int xlp_nway_reset(struct net_device *dev)
+{
+	struct dev_data *priv = netdev_priv(dev);
+	ulong flags;
+	int mii_status;
+	int ret = -EINVAL;
+
+	if (priv->type != SGMII_IF)
+		return -EIO;
+
+	spin_lock_irqsave(&priv->lock, flags);
+
+	mii_status = nlm_xlp_mac_mii_read(priv, MII_BMCR);
+	if (mii_status & BMCR_ANENABLE) {
+		//nlm_xlp_mac_mii_write(priv, MII_BMCR,
+		//		BMCR_ANRESTART | mii_status);
+		ret = 0;
+	}
+
+	spin_unlock_irqrestore(&priv->lock, flags);
+
+	return ret;
+}
+
+static u32 xlp_get_link(struct net_device *dev)
+{
+	struct dev_data *priv = netdev_priv(dev);
+	struct nlm_hal_mii_info mii_info;
+	ulong flags;
+	if (priv->type != SGMII_IF)
+		return -EIO;
+
+	spin_lock_irqsave(&priv->lock, flags);
+	//nlm_hal_status_ext_phy( priv->node, priv->phy.addr, &mii_info);
+
+	spin_unlock_irqrestore(&priv->lock, flags);
+
+	if(mii_info.link_stat==1)
+                return 1;
+
+	return 0;
+}
+
+static void xlp_get_strings (struct net_device *dev, u32 stringset, u8 *buf)
+{
+	switch (stringset) {
+	case ETH_SS_STATS:
+		memcpy(buf, &xlp_ethtool_stats_keys,
+			sizeof(xlp_ethtool_stats_keys));
+		break;
+	default:
+		printk(KERN_WARNING "%s: Invalid stringset %d\n",
+			__func__, stringset);
+		break;
+	}
+}
+
+#define STATS_RD(x)		\
+	nlm_hal_read_mac_reg(priv->node, priv->block, priv->index, x)
+
+/**********************************************************************
+ * xlp_get_mac_stats -  collect stats info from Mac stats register
+ * @dev   -  this is per device based function
+ * @stats -  net device stats structure
+ **********************************************************************/
+
+void xlp_get_mac_stats(struct net_device *dev, struct net_device_stats *stats)
+{
+
+	struct dev_data *priv = netdev_priv(dev);
+	net_port_t* netport = priv->nae_port;
+	netsoc_get_port_stats( netport ,(dev_stat_t*)stats);
+#if 0
+
+
+	struct dev_data *priv = netdev_priv(dev);
+#ifdef CONFIG_64BIT
+	uint64_t val;
+#endif
+
+	if (priv->type == INTERLAKEN_IF) {
+		nlm_hal_get_ilk_mac_stats(priv->node, priv->block,
+			priv->phy.addr, stats);
+		return;
+	}
+
+	stats->tx_packets = STATS_RD(TX_PACKET_COUNTER);
+#ifdef CONFIG_64BIT
+	val = STATS_RD(0x1f);
+	stats->tx_packets |= ( val << 32);
+#endif
+
+	stats->rx_packets = STATS_RD(RX_PACKET_COUNTER);
+#ifdef CONFIG_64BIT
+	val = STATS_RD(0x1f);
+	stats->rx_packets |= ( val << 32);
+#endif
+
+	stats->tx_bytes = STATS_RD(TX_BYTE_COUNTER);
+#ifdef CONFIG_64BIT
+	val = STATS_RD(0x1f);
+	stats->tx_bytes |= ( val << 32);
+#endif
+
+	stats->rx_bytes = STATS_RD(RX_BYTE_COUNTER);
+#ifdef CONFIG_64BIT
+	val = STATS_RD(0x1f);
+	stats->rx_bytes |= ( val << 32);
+#endif
+
+	stats->tx_errors = STATS_RD(TX_FCS_ERROR_COUNTER) +
+				STATS_RD(TX_JABBER_FRAME_COUNTER);
+	stats->rx_dropped = STATS_RD(RX_DROP_PACKET_COUNTER);
+	stats->tx_dropped = STATS_RD(TX_DROP_FRAME_COUNTER);
+
+	stats->multicast = STATS_RD(RX_MULTICAST_PACKET_COUNTER);
+#ifdef CONFIG_64BIT
+	val = STATS_RD(0x1f);
+	stats->multicast |= ( val << 32);
+#endif
+
+	stats->collisions = STATS_RD(TX_TOTAL_COLLISION_COUNTER);
+	stats->rx_length_errors = STATS_RD(RX_FRAME_LENGTH_ERROR_COUNTER);
+	stats->rx_over_errors = STATS_RD(RX_DROP_PACKET_COUNTER);
+	stats->rx_crc_errors = STATS_RD(RX_FCS_ERROR_COUNTER) +
+				STATS_RD(RX_JABBER_COUNTER);
+	stats->rx_frame_errors = STATS_RD(RX_ALIGNMENT_ERROR_COUNTER);
+	stats->rx_fifo_errors = STATS_RD(RX_DROP_PACKET_COUNTER);
+	stats->rx_missed_errors = STATS_RD(RX_CARRIER_SENSE_ERROR_COUNTER);
+	stats->rx_errors = (stats->rx_over_errors + stats->rx_crc_errors +
+				stats->rx_frame_errors + stats->rx_fifo_errors +
+				stats->rx_missed_errors);
+	stats->tx_aborted_errors = STATS_RD(TX_EXCESSIVE_COLLISION_PACKET_COUNTER);
+	/*
+	stats->tx_carrier_errors = STATS_RD(TX_DROP_FRAME_COUNTER);
+	stats->tx_fifo_errors = STATS_RD(TX_DROP_FRAME_COUNTER);
+	*/
+#endif
+	return;
+}
+
+#undef STATS_RD
+
+/**********************************************************************
+ * xlp_get_ethtool_stats -  part of ethtool_ops member function
+ * @dev   -  this is per device based function
+ * @stats -  net device stats structure
+ **********************************************************************/
+static void xlp_get_ethtool_stats (struct net_device *dev,
+				   struct ethtool_stats *estats, u64 *stats)
+{
+	struct dev_data *priv = netdev_priv(dev);
+	ulong flags;
+	ulong *tmp_stats;
+	int i;
+
+	spin_lock_irqsave(&priv->lock, flags);
+
+//TODO:
+	//xlp_get_mac_stats(dev, &priv->stats);
+
+	spin_unlock_irqrestore(&priv->lock, flags);
+
+	tmp_stats = (ulong *)&priv->stats;
+	for(i = 0; i < NLM_STATS_KEY_LEN; i++) {
+		*stats = (u64)*tmp_stats;
+		stats++;
+		tmp_stats++;
+	}
+}
+
+/**********************************************************************
+ *  nlm_xlp_mac_mii_read - Read mac mii phy register
+ *
+ *  Input parameters:
+ *  	   priv - priv structure
+ *  	   phyaddr - PHY's address
+ *  	   regidx = index of register to read
+ *
+ *  Return value:
+ *  	   value read (16 bits), or 0xffffffff if an error occurred.
+ ********************************************************************* */
+unsigned int nlm_xlp_mac_mii_read(struct dev_data *priv, int regidx)
+{
+//TODO:
+	//return nlm_hal_mdio_read(priv->node, NLM_HAL_EXT_MDIO, 0,
+	//	BLOCK_7, LANE_CFG, priv->phy.addr, regidx);
+	return 0;
+}
+
+/**********************************************************************
+ *  nlm_xlp_mac_mii_write -Write mac mii PHY register.
+ *
+ *  Input parameters:
+ *  	   priv - priv structure
+ *  	   regidx - register within the PHY
+ *  	   regval - data to write to register
+ *
+ *  Return value:
+ *  	   nothing
+ ********************************************************************* */
+void nlm_xlp_mac_mii_write(struct dev_data *priv, int regidx,
+				  uint16_t regval)
+{
+//TODO:
+//	nlm_hal_mdio_write(priv->node, NLM_HAL_EXT_MDIO, 0, BLOCK_7,
+//		LANE_CFG, priv->phy.addr, regidx, regval);
+}
+
+static struct ethtool_ops xlp_ethtool_ops = {
+	.get_settings		= xlp_get_settings,
+	.set_settings		= xlp_set_settings,
+	.get_drvinfo		= xlp_get_drvinfo,
+	.get_regs_len		= xlp_get_regs_len,
+	.get_regs		= xlp_get_regs,
+	.get_msglevel		= xlp_get_msglevel,
+	.set_msglevel		= xlp_set_msglevel,
+	.nway_reset		= xlp_nway_reset,
+	.get_link		= xlp_get_link,
+	.get_strings		= xlp_get_strings,
+	.get_ethtool_stats	= xlp_get_ethtool_stats,
+	.get_eeprom_len		= xlp_get_eeprom_len,
+	.get_eeprom		= xlp_get_eeprom,
+	.set_eeprom		= xlp_set_eeprom,
+	.get_pauseparam         = xlp_get_pauseparam,
+        .set_pauseparam         = xlp_set_pauseparam,
+	.msec_tx_config		= xlp_config_msec_tx,
+	.msec_tx_mem_config	= xlp_config_msec_tx_mem,
+	.msec_rx_config		= xlp_config_msec_rx,
+	.msec_rx_mem_config	= xlp_config_msec_rx_mem,
+};
+
+void xlp_set_ethtool_ops(struct net_device *netdev)
+{
+	SET_ETHTOOL_OPS(netdev, &xlp_ethtool_ops);
+}
diff --git a/drivers/netlogic/nae/xlpge_lro.h b/drivers/netlogic/nae/xlpge_lro.h
new file mode 100644
index 0000000..153cfc1
--- /dev/null
+++ b/drivers/netlogic/nae/xlpge_lro.h
@@ -0,0 +1,127 @@
+/*-
+ * Copyright (c) 2003-2012 Broadcom Corporation
+ * All Rights Reserved
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in
+ *    the documentation and/or other materials provided with the
+ *    distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY BROADCOM ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL BROADCOM OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
+ * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
+ * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
+ * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ * #BRCM_2# */
+
+#ifndef	__XLPGE_LRO_H__
+#define __XLPGE_LRO_H__
+#include <linux/module.h>
+#include <linux/pci.h>
+#include <linux/netdevice.h>
+#include <linux/inet_lro.h>
+#include <linux/clocksource.h>
+
+#include <nlm_xlp.h>
+#include "xlpge.h"
+
+#ifdef CONFIG_INET_LRO
+
+extern int enable_lro;
+extern int lro_flush_priv_cnt[NR_CPUS];
+extern int lro_flush_needed[NR_CPUS][20];
+extern struct dev_data *lro_flush_priv[NR_CPUS][20];
+
+static int lro_get_skb_hdr(struct sk_buff *skb, void **iphdr, void **tcph,
+			   uint64_t *hdr_flags, void *priv)
+{
+	struct tcphdr *tcp_h;
+	skb_reset_network_header(skb);
+	skb_set_transport_header(skb, ip_hdrlen(skb));
+
+	if (ip_hdr(skb)->protocol != 0x6)
+		return -1;
+
+	*iphdr = ip_hdr(skb);
+	*tcph = tcp_hdr(skb);
+
+	*hdr_flags = LRO_IPV4 | LRO_TCP;
+	skb->ip_summed = CHECKSUM_UNNECESSARY;
+
+	/* set the ackbit */
+	tcp_h = tcp_hdr(skb);
+	//if (!tcp_h->cwr && !tcp_h->ece && !tcp_h->urg &&
+	  //  !tcp_h->rst && !tcp_h->syn && !tcp_h->fin)
+	if((((uint32_t *)tcp_h)[3] & (0xf7 << 16)) == 0)
+		tcp_h->ack = 1;
+
+	return 0;
+}
+
+static void __maybe_unused lro_init(struct net_device *dev)
+{
+	struct dev_data* priv;
+	struct net_lro_mgr *lp;
+	int cpu;
+
+	priv = netdev_priv(dev);
+
+	if (enable_lro) {
+		printk("LRO is enabled \n");
+		dev->features |= NETIF_F_LRO;
+	//	dev->features |= NETIF_F_GRO;
+	//	return;
+		for (cpu = 0; cpu < NR_CPUS; cpu++) {
+			lp = &priv->lro_mgr[cpu];
+			memset(lp, 0, sizeof(struct net_lro_mgr));
+			lp->max_aggr = 48;
+			lp->max_desc = LRO_MAX_DESCS;
+			lp->get_skb_header = lro_get_skb_hdr;
+			lp->features = LRO_F_NAPI;
+			lp->dev = dev;
+			lp->ip_summed = CHECKSUM_UNNECESSARY;
+			lp->ip_summed_aggr = CHECKSUM_UNNECESSARY;
+			lp->lro_arr = cacheline_aligned_kmalloc(
+					sizeof(struct net_lro_desc) *
+						LRO_MAX_DESCS, GFP_KERNEL);
+			memset(lp->lro_arr, 0,
+				sizeof(struct net_lro_desc) * LRO_MAX_DESCS);
+		}
+	}
+}
+
+static inline void napi_lro_flush(int cpu)
+{
+	struct dev_data *priv = NULL;
+	int i;
+
+	for (i = 0; i < lro_flush_priv_cnt[cpu]; i++) {
+		priv = lro_flush_priv[cpu][i];
+		lro_flush_all(&priv->lro_mgr[cpu]);
+		lro_flush_needed[cpu][priv->port_index] = 0;
+		Message("Lro flush cpu %d port %d\n", cpu, priv->port_index);
+	}
+
+	lro_flush_priv_cnt[cpu] = 0;
+}
+
+static void __maybe_unused xlp_napi_lro_flush(void *arg)
+{
+	int cpu = hard_smp_processor_id();
+	napi_lro_flush(cpu);
+}
+#endif //INET_LRO
+#endif //LRO_H_
diff --git a/drivers/netlogic/nae/xlpge_main.c b/drivers/netlogic/nae/xlpge_main.c
new file mode 100644
index 0000000..a56fcda
--- /dev/null
+++ b/drivers/netlogic/nae/xlpge_main.c
@@ -0,0 +1,88 @@
+/*-
+ * Copyright (c) 2003-2012 Broadcom Corporation
+ * All Rights Reserved
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in
+ *    the documentation and/or other materials provided with the
+ *    distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY BROADCOM ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL BROADCOM OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
+ * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
+ * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
+ * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ * #BRCM_2# */
+
+#include <linux/module.h>
+#include <linux/pci.h>
+#include <linux/netdevice.h>
+#include <linux/inet_lro.h>
+#include <linux/clocksource.h>
+
+#include "xlpge.h"
+
+
+static const struct pci_device_id xlpnae_pci_table[] = {
+	{
+		.vendor		= PCI_VENDOR_NETLOGIC,
+		.device		= PCI_DEVICE_ID_NLM_NAE,
+		.subvendor	= PCI_ANY_ID,
+		.subdevice	= PCI_ANY_ID,
+	},
+};
+
+static int brcmxlp_nae_pci_probe(struct pci_dev *pdev,
+					   const struct pci_device_id *ent)
+{
+	return pci_enable_device(pdev);
+}
+
+static void brcmxlp_nae_pci_remove(struct pci_dev *pdev)
+{
+	pci_disable_device(pdev);
+}
+
+static struct pci_driver brcmxlp_nae_driver = {
+	.name		= XLP_SOC_MAC_DRIVER,
+	.id_table	= xlpnae_pci_table,
+	.probe		= brcmxlp_nae_pci_probe,
+	.remove		= brcmxlp_nae_pci_remove,
+};
+
+static int __init brcmxlp_nae_init(void)
+{
+	xlpge_eeprom_init();
+	/* TODO:XXX Move to pci init? */
+	nlm_xlp_nae_init();
+	//init_phy_state_timer(NULL);
+
+	return pci_register_driver(&brcmxlp_nae_driver);
+}
+
+static void __init brcmxlp_nae_exit(void)
+{
+	nlm_xlp_nae_remove();
+	pci_unregister_driver(&brcmxlp_nae_driver);
+}
+
+module_init(brcmxlp_nae_init);
+module_exit(brcmxlp_nae_exit);
+
+MODULE_AUTHOR("Broadcom");
+MODULE_DESCRIPTION("Broadcom XLP SoC Network Driver");
+MODULE_LICENSE("Proprietary");
+MODULE_VERSION(DRV_VERSION);
diff --git a/drivers/netlogic/nae/xlpge_msec.c b/drivers/netlogic/nae/xlpge_msec.c
new file mode 100644
index 0000000..3622ccd
--- /dev/null
+++ b/drivers/netlogic/nae/xlpge_msec.c
@@ -0,0 +1,96 @@
+/*-
+ * Copyright (c) 2003-2012 Broadcom Corporation
+ * All Rights Reserved
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in
+ *    the documentation and/or other materials provided with the
+ *    distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY BROADCOM ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL BROADCOM OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
+ * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
+ * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
+ * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ * #BRCM_2# */
+
+#include <linux/module.h>
+#include <linux/pci.h>
+#include <linux/netdevice.h>
+#include <linux/inet_lro.h>
+#include <linux/clocksource.h>
+
+#include "xlpge.h"
+
+int xlp_config_msec_tx(struct net_device *dev, struct ethtool_cmd *cmd)
+{
+	nae_t *nae;
+
+	if(is_nlm_xlp2xx() || is_nlm_xlp9xx())
+	{
+		nae = get_nae(cmd->node, cmd->num_nae);
+		__netsoc_msec_tx_config(nae, cmd->port_enable,
+			cmd->preamble_len, cmd->packet_num,
+			cmd->win_size_thrshld);
+	}
+	return 0;
+}
+
+int xlp_config_msec_tx_mem(struct net_device *dev,
+				  struct ethtool_cmd *cmd)
+{
+	nae_t *nae;
+
+	if(is_nlm_xlp2xx() || is_nlm_xlp9xx())
+	{
+		nae = get_nae(cmd->node, cmd->num_nae);
+		__netsoc_msec_tx_mem_config(nae, cmd->port, cmd->tci,
+			cmd->sci, cmd->key);
+	}
+
+	return 0;
+}
+
+int xlp_config_msec_rx(struct net_device *dev, struct ethtool_cmd *cmd)
+{
+	nae_t *nae;
+
+	if(is_nlm_xlp2xx() || is_nlm_xlp9xx())
+	{
+		nae = get_nae(cmd->node, cmd->num_nae);
+		__netsoc_msec_rx_config(nae, cmd->port_enable,
+			cmd->preamble_len, cmd->packet_num,
+			cmd->win_size_thrshld);
+	}
+
+	return 0;
+}
+
+int xlp_config_msec_rx_mem(struct net_device *dev,
+				  struct ethtool_cmd *cmd)
+{
+	nae_t *nae;
+
+	if(is_nlm_xlp2xx() || is_nlm_xlp9xx())
+	{
+		nae = get_nae(cmd->node, cmd->num_nae);
+		__netsoc_msec_rx_mem_config(nae, cmd->port, cmd->index,
+			cmd->sci, cmd->key, cmd->sci_mask);
+	}
+
+	return 0;
+}
+
diff --git a/drivers/netlogic/nae/xlpge_nae.c b/drivers/netlogic/nae/xlpge_nae.c
new file mode 100644
index 0000000..d66923d
--- /dev/null
+++ b/drivers/netlogic/nae/xlpge_nae.c
@@ -0,0 +1,1584 @@
+/*-
+ * Copyright (c) 2003-2012 Broadcom Corporation
+ * All Rights Reserved
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in
+ *    the documentation and/or other materials provided with the
+ *    distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY BROADCOM ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL BROADCOM OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
+ * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
+ * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
+ * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ * #BRCM_2# */
+
+#include <linux/module.h>
+#include <linux/pci.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/inet_lro.h>
+#include <linux/clocksource.h>
+#include <linux/proc_fs.h>
+#include <linux/timer.h>
+//#include <nlm_xlp.h>
+//#include <nlm_msgring.h>
+//#include <nlm_hal_fmn.h>
+//#include <nlm_eeprom.h>
+
+#include "xlpge.h"
+#include "xlpge_lro.h"
+#include "xlpge_tso.h"
+
+uint32_t cpu_2_normal_frfifo[NLM_MAX_NODES][MAX_NAE_PERNODE][NR_CPUS];
+uint32_t cpu_2_jumbo_frfifo[NLM_MAX_NODES][MAX_NAE_PERNODE][NR_CPUS];
+int phys_cpu_to_log_map[NLM_MAX_NODES][MAX_NAE_PERNODE][NR_CPUS];
+int lcpu_2_pcpu[NR_CPUS];
+static unsigned int fmem[NR_CPUS];
+int num_cpus_per_node;
+
+uint64_t nlm_mode[NR_CPUS*8] ____cacheline_aligned;
+static struct nlm_nae_linux_shinfo *lnx_shinfo[3];
+uint64_t p2p_dynamic_alloc_cnt[NR_CPUS * 8] __cacheline_aligned;
+struct net_device *xlp_dev_mac[NLM_MAX_NODES][MAX_GMAC_PORT];
+struct net_device *
+	per_cpu_netdev[NLM_MAX_NODES][NR_CPUS][2][24] __cacheline_aligned;
+int exclusive_vc = 0;
+module_param(exclusive_vc, int, 0);
+#ifdef CONFIG_NLM_ENABLE_LOAD_BALANCING
+int load_balance_timer_run = 1;
+module_param(load_balance_timer_run, int, S_IRUGO|S_IWUSR);
+int load_balance_en = 0;
+module_param(load_balance_en, int, 0);
+#endif
+int enable_napi = 1;
+int nlm_prepad_len = 0;
+int perf_mode= NLM_TCP_MODE;
+extern cpumask_t phys_cpu_present_map;
+module_param(perf_mode, int, 0);
+/* Descriptors for each normal fifo. For xaui ports, if port fifo mode
+   is enabled, this will be multiplied by 4 (3 fifos are unused) */
+int num_descs_per_normalq = 64;
+module_param(num_descs_per_normalq, int, 0);
+/* Descriptors for each jumbo fifo. For xaui ports, if port fifo mode
+   is enabled, this will be multiplied by 4 (3 fifos are unused) */
+int num_descs_per_jumboq = 48;
+module_param(num_descs_per_jumboq, int, 0);
+
+int enable_lro = 0;
+module_param(enable_lro, int, 0);
+int lro_flush_priv_cnt[NR_CPUS];
+int lro_flush_needed[NR_CPUS][20];
+struct dev_data *lro_flush_priv[NR_CPUS][20];
+
+
+static uint32_t lnx_frfifo_normal_mask[NLM_MAX_NODES][MAX_NAE_PERNODE];
+static uint32_t lnx_frfifo_jumbo_mask[NLM_MAX_NODES][MAX_NAE_PERNODE];
+
+int enable_jumbo = 0;
+module_param(enable_jumbo, int, 0);
+struct p2p_desc_mem p2p_desc_mem[NR_CPUS] __cacheline_aligned;
+static unsigned int phys_cpu_map[NLM_MAX_NODES];
+struct timer_list phy_int_timer;
+#ifdef CONFIG_NLM_ENABLE_LOAD_BALANCING
+struct file_operations nlm_load_balance_proc_fops = {
+	.owner = THIS_MODULE,
+	.open = nlm_load_balance_proc_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = seq_release,
+};
+#endif
+
+#ifdef CONFIG_NLM_NET_OPTS
+struct sk_buff *last_rcvd_skb[NR_CPUS * 8] ____cacheline_aligned;
+uint32_t last_rcvd_len[NR_CPUS * 8] ____cacheline_aligned;
+uint32_t last_rcvd_node[NR_CPUS * 8] ____cacheline_aligned;
+uint64_t last_rcvd_skb_phys[NR_CPUS * 8] ____cacheline_aligned;
+#endif
+
+static unsigned short nlm_select_queue(struct net_device *dev,
+				       struct sk_buff *skb)
+{
+	return (unsigned short)smp_processor_id();
+}
+
+/*
+ * As there is a port level fifo checkup done in NAE h/w, we need to fill up
+ * the port fifos ( 0, 4, 8, 12 & 16) with some dummy entries if it is not
+ * owned by anyone.
+ */
+static int init_dummy_entries_for_port_fifos(nae_t* nae_cfg)
+{
+	struct sk_buff *skb;
+	static uint64_t msg;
+	uint32_t __attribute__ ((unused)) mflags;
+	uint32_t fifo_mask = 0;
+	int rv = 0, vc_index = 0, i, j, ret, code = 0, shdom;
+	int size = NLM_RX_JUMBO_BUF_SIZE;
+	int node = nae_cfg->node;
+
+	skb = nlm_xlp_alloc_skb_atomic(size, node);
+	if (!skb) {
+		printk("[%s] alloc skb failed\n", __func__);
+		panic("panic...");
+		return -ENOMEM;
+	}
+
+	msg = (uint64_t)virt_to_bus(skb->data) & 0xffffffffffULL;
+
+	for (shdom = 0; shdom <= NLM_NAE_MAX_SHARED_DOMS; shdom++) {
+		if (!nae_cfg->shinfo[shdom].valid)
+			continue;
+		fifo_mask |= nae_cfg->shinfo[shdom].freein_fifo_mask;
+	}
+	return 0;
+
+//	msgrng_access_enable(mflags);
+	printk("Total free ins = 0x%x\n", nae_cfg->frin_total_queue);
+	for (i = 0; i < nae_cfg->frin_total_queue; i++) {
+		/* nothing to do, if it is owned by some domain */
+		if((1 << i) & fifo_mask)
+			continue;
+
+		/* if it doesnt have any onchip space */
+		if(nae_cfg->freein_fifo_onchip_num_descs[i] == 0)
+			continue;
+
+		vc_index = i + nae_cfg->frin_queue_base;
+
+		for (j = 0; j < 4; j++) {
+			//netsoc_nae_send_freein_buf(nae_cfg, vc_index, msg);
+//#if 0
+				printk("Sending message check freein carving (qid=%d)\n",
+					vc_index);
+			if ((ret = xlp_message_send_1(vc_index, code, msg))
+				& 0x7) {
+				print_fmn_send_error(__func__, ret);
+				printk("Unable to send configured free desc\n");
+				printk("check freein carving (qid=%d)\n",
+					vc_index);
+				rv = -1;
+				goto err;
+			}
+//#endif
+		}
+		printk("Send %d dummy descriptors for queue %d(vc %d) of length %d\n",
+			j, i, vc_index, size);
+	}
+err:
+	//msgrng_access_disable(mflags);
+
+	/* if not used */
+	if(!vc_index)
+		dev_kfree_skb_any(skb);
+	return rv;
+}
+
+
+static int nlm_initialize_vfbid(nae_t* nae_cfg)
+{
+	uint32_t vfbid_tbl[128];
+	int start = nae_cfg->vfbtbl_sw_offset;
+	int end = start + nae_cfg->vfbtbl_sw_nentries;
+	int frin_q_base = nae_cfg->frin_queue_base;
+	int node = nae_cfg->node;
+	int cpu, tblidx, i;
+
+	/*
+	 * For s/w replenishment, each nodes tx completes can be send to his
+	 * own node cpus only
+	 */
+	for (tblidx = start, cpu = 0; tblidx < end ; tblidx++, cpu++)
+		vfbid_tbl[tblidx] = (cpu * 4) + nae_cfg->fb_vc +
+					(node * 1024);
+
+	netsoc_config_vfbid_table(nae_cfg, start, end - start,
+		&vfbid_tbl[start]);
+	/*
+	 * For h/w replenishment, each node fills up 20 entries for all other
+	 * nodes starting from node0's queue-id. Software should offset the
+	 * hw-offset + rx-node id to get the actual index
+	 */
+	start = nae_cfg->vfbtbl_hw_offset;
+	end = start + nae_cfg->vfbtbl_hw_nentries;
+	for (tblidx = start, i = 0; tblidx < end; tblidx++, i++) {
+		if(i >= NLM_NAE_MAX_FREEIN_FIFOS_PER_NODE) {
+			i = 0;
+			frin_q_base = 1024 + frin_q_base;
+		}
+		vfbid_tbl[tblidx] = frin_q_base + i;
+	}
+	netsoc_config_vfbid_table(nae_cfg, start, end - start, &vfbid_tbl[start]);
+
+	/* NULL FBID Should map to cpu0 to detect NAE send message errors*/
+	vfbid_tbl[127] = 0;
+	netsoc_config_vfbid_table(nae_cfg, 127, 1, &vfbid_tbl[127]);
+
+	/*IEEE-1588 timestamp*/
+	vfbid_tbl[126] = 0;
+	netsoc_config_vfbid_table(nae_cfg, 126, 1, &vfbid_tbl[126]);
+
+	return 0;
+}
+
+static inline uint32_t fdt32_to_cpu(uint32_t x)
+{
+	#define _BYT(n) ((unsigned long long)((unsigned char *)&x)[n])
+#ifdef __MIPSEL__
+	return (_BYT(0) << 24) | (_BYT(1) << 16) | (_BYT(2) << 8) | _BYT(3);
+#else
+	return x;
+#endif
+}
+
+static int nlm_configure_shared_freein_fifo(int node,
+					    nlm_nae_config_ptr nae_cfg)
+{
+	uint64_t paddr, psize, epaddr;
+	uint64_t msg;
+	uint32_t cnode, fmask, dsize, dppadsz, ndescs;
+	uint32_t *t;
+	uint32_t owner_replenish = 0, paddr_info_len, desc_info_len;
+	ulong __attribute__ ((unused)) mflags;
+	int vc_index, rv = 0, code = 0, descs, fifo;
+	int shdom, err = 0;
+	int len = 0, i = 0;
+	char *paddr_info, *desc_info;
+
+	printk("%s in \n", __FUNCTION__);
+
+	for(shdom = 0; shdom <= NLM_NAE_MAX_SHARED_DOMS; shdom++) {
+		if(!nae_cfg->shinfo[shdom].valid)
+			continue;
+		/* ignore my own domain id */
+		if(nae_cfg->shinfo[shdom].domid == 0)
+			continue;
+
+		owner_replenish = nae_cfg->shinfo[shdom].owner_replenish;
+		paddr_info = nae_cfg->shinfo[shdom].paddr_info;
+		paddr_info_len = nae_cfg->shinfo[shdom].paddr_info_len;
+		desc_info = nae_cfg->shinfo[shdom].desc_info;
+		desc_info_len = nae_cfg->shinfo[shdom].desc_info_len;
+
+		printk("shind %d dom %d repl %d paddr_ptr %lx len %d desc_ptr %lx dlen %d\n",
+				shdom, nae_cfg->shinfo[shdom].domid, owner_replenish,
+				(long)paddr_info, paddr_info_len, (long)desc_info, desc_info_len);
+
+		if(!owner_replenish)
+			continue;
+
+		t = (uint32_t *)paddr_info;
+		i = 0;
+		do {
+			/* extract the config */
+			cnode = fdt32_to_cpu(t[i]);
+			paddr = ((uint64_t)fdt32_to_cpu(t[i + 1])) << 32;
+			paddr |= ((uint64_t)fdt32_to_cpu(t[i + 2]));
+			psize = ((uint64_t)fdt32_to_cpu(t[i + 3])) << 32;
+			psize |= ((uint64_t)fdt32_to_cpu(t[i + 4]));
+
+			i += 5;
+			len = i * 4;
+			if(cnode == node)
+				break;
+		} while(len < paddr_info_len);
+
+		printk("domid %d node %d addr %llx size %llx\n",
+					nae_cfg->shinfo[shdom].domid, cnode,
+					paddr, psize);
+
+		epaddr = paddr + psize;
+		t = (uint32_t *)desc_info;
+		i = 0;
+		do {
+
+			/* extract the config */
+			cnode = fdt32_to_cpu(t[i]);
+			fmask = fdt32_to_cpu(t[i + 1]);
+			dsize = fdt32_to_cpu(t[i + 2]);
+			dppadsz = fdt32_to_cpu(t[i + 3]);
+			ndescs = fdt32_to_cpu(t[i + 4]);
+
+			i += 5;
+			len = i * 4;
+
+			if(cnode != node)
+				continue;
+
+			printk("node %d fmask %x dsize %d dppadsz %d ndescs %d \n",
+					cnode, fmask, dsize, dppadsz, ndescs);
+
+			for(fifo = 0; fifo < NLM_NAE_MAX_FREEIN_FIFOS_PER_NODE;
+				fifo++) {
+				if(!((1 << fifo) & fmask))
+					continue;
+				msgrng_access_enable(mflags);
+				vc_index = fifo + nae_cfg->frin_queue_base;
+				for(descs = 0; descs < ndescs; descs++) {
+					if((paddr + dsize) > epaddr) {
+						msgrng_access_disable(mflags);
+						printk("Error, descriptors \
+							buffer overflow \n");
+						err = -1;
+						goto err_exit;
+					}
+					msg = paddr + dppadsz;
+
+					__sync();
+					rv = xlp_message_send_1(vc_index, code, msg);
+					if(rv & 0x7) {
+						msgrng_access_disable(mflags);
+						printk("Unable to send \
+							configured free desc, \
+							check freein carving \
+							(qid=%d)\n", vc_index);
+						err = -1;
+						goto err_exit;
+					}
+
+					paddr += dsize;
+
+				}
+				msgrng_access_disable(mflags);
+				printk("Send %d descriptors for queue \
+						%d(%d) of length %d\n",
+						ndescs, fifo, vc_index, dsize);
+
+			}
+
+		} while(len < desc_info_len);
+
+	}
+
+err_exit:
+	return err;
+}
+
+static int initialize_nae_per_node(nae_t * nae_cfg, uint32_t *phys_cpu_map, int mode,
+				   int *jumbo_enabled)
+{
+	int i, len, rv = -1, node;
+	int cpus = 0, nae_id, size = 0;
+	void *mem = NULL;
+
+	if (nae_cfg == NULL)
+		goto err;
+	node = nae_cfg->node;
+	nae_id = nae_cfg->nae_id;
+
+	for (i = 0; i <= NLM_NAE_MAX_SHARED_DOMS; i++) {
+		 cpus += nae_cfg->shinfo[i].num_cpus;
+	}
+	size = ((NLM_NAE_MAX_SHARED_DOMS + 1) * (sizeof(struct nlm_nae_linux_shinfo))) + 
+		(sizeof(unsigned int) * cpus);
+	mem = kmalloc(size, GFP_KERNEL);
+	if(!mem)
+		goto err;
+
+
+	for (i = 0; i <= NLM_NAE_MAX_SHARED_DOMS; i++) {
+		printk("naeid %d shind %d dom %d valid %x numcpus %d\n", nae_id,
+				i, nae_cfg->shinfo[i].domid, nae_cfg->shinfo[i].valid, nae_cfg->shinfo[i].num_cpus);
+		lnx_shinfo[i] = (struct nlm_nae_linux_shinfo *)mem;
+		lnx_shinfo[i]->flags  = nae_cfg->shinfo[i].valid ? NLM_NAE_LNX_SHINFO_FL_VALID : 0;
+		lnx_shinfo[i]->rxvc = nae_cfg->shinfo[i].rxvc;
+		lnx_shinfo[i]->domid = nae_cfg->shinfo[i].domid;
+		lnx_shinfo[i]->naeid = nae_id;
+		lnx_shinfo[i]->num_cpus = nae_cfg->shinfo[i].num_cpus;
+		lnx_shinfo[i]->node = node;
+		for(cpus = 0; cpus < lnx_shinfo[i]->num_cpus; cpus++)  {
+			int phys_cpu;
+			lnx_shinfo[i]->fwd_info[cpus] = nae_cfg->shinfo[i].fwd_info[cpus];
+			printk("cpu %d value %08x\n", cpus, nae_cfg->shinfo[i].fwd_info[cpus]);
+			phys_cpu = (lnx_shinfo[i]->fwd_info[cpus] >> NLM_NAE_LNX_SHINFO_FWD_INFO_LCPU_2_PCPU_OFF) &
+				NLM_NAE_LNX_SHINFO_FWD_INFO_LCPU_2_PCPU_MASK;
+			phys_cpu_to_log_map[node][nae_id][phys_cpu] = cpus;
+		}
+		mem += sizeof(struct nlm_nae_linux_shinfo) + (sizeof(unsigned int) * cpus);
+	}
+
+	lnx_frfifo_normal_mask[node][nae_id] = nae_cfg->freein_fifo_dom_mask;
+
+	/* if jumbo enabled , we use half of the linux owned freein fifos
+	 * for jumbo skbs */
+	if (*jumbo_enabled) {
+		int mine = 1;
+
+		/* update rx xon/xoff thresholds for jumbo */
+		//TODO: function is commented in xon xoff thres.
+		//nlm_hal_set_context_xon_xoff_threshold(node, ETH_JUMBO_DATA_LEN);
+
+		for (i = 0; i < nae_cfg->frin_total_queue; i++) {
+			if ((1 << i) & nae_cfg->freein_fifo_dom_mask) {
+				if (mine) {
+					mine = 0;
+					continue;
+				}
+				lnx_frfifo_normal_mask[node][nae_id] &= (~(1 << i));
+				lnx_frfifo_jumbo_mask[node][nae_id] |= (1 << i);
+				mine = 1;
+			}
+		}
+
+		if (lnx_frfifo_jumbo_mask[node][nae_id]) {
+			for(cpus = 0; cpus < lnx_shinfo[0]->num_cpus; cpus++) {
+				lcpu_2_pcpu[cpus] = (lnx_shinfo[0]->fwd_info[cpus] >> 
+					NLM_NAE_LNX_SHINFO_FWD_INFO_LCPU_2_PCPU_OFF) &
+					NLM_NAE_LNX_SHINFO_FWD_INFO_LCPU_2_PCPU_MASK;
+			}
+			memset(fmem, 0, sizeof(fmem));
+			if(derive_cpu_to_freein_fifo_map(nae_cfg->frin_total_queue, 
+						   lnx_shinfo[0]->num_cpus,
+						   fmem, sizeof(fmem),
+			               lcpu_2_pcpu, lnx_frfifo_normal_mask[node][nae_id], 
+						   cpu_2_normal_frfifo[node][nae_id]) != 0) {
+				goto err;
+			}
+			memset(fmem, 0, sizeof(fmem));
+			if(derive_cpu_to_freein_fifo_map(nae_cfg->frin_total_queue, 
+						   lnx_shinfo[0]->num_cpus,
+						   fmem, sizeof(fmem),
+			               lcpu_2_pcpu, lnx_frfifo_jumbo_mask[node][nae_id], 
+						   cpu_2_jumbo_frfifo[node][nae_id]) != 0) {
+				goto err;
+			}
+
+			for(cpus = 0; cpus < lnx_shinfo[0]->num_cpus; cpus++) {
+				lnx_shinfo[0]->fwd_info[cpus] =  
+					(lcpu_2_pcpu[cpus] << NLM_NAE_LNX_SHINFO_FWD_INFO_LCPU_2_PCPU_OFF) |
+					(cpu_2_normal_frfifo[node][nae_id][cpus] << NLM_NAE_LNX_SHINFO_FWD_INFO_LCPU_2_RF_OFF)|
+					(cpu_2_jumbo_frfifo[node][nae_id][cpus] << NLM_NAE_LNX_SHINFO_FWD_INFO_LCPU_2_JF_OFF);
+			}
+		} else {
+			printk("freein-fifo unavailable: ");
+			printk("Disabling Jumbo\n");
+			*jumbo_enabled = 0;
+		}
+	} else if (*jumbo_enabled == 0) {
+		for(cpus = 0; cpus < lnx_shinfo[0]->num_cpus; cpus++) {
+			cpu_2_normal_frfifo[node][nae_id][cpus] =
+				(lnx_shinfo[0]->fwd_info[cpus] >> NLM_NAE_LNX_SHINFO_FWD_INFO_LCPU_2_RF_OFF) &
+				NLM_NAE_LNX_SHINFO_FWD_INFO_LCPU_2_RF_MASK;
+		}
+	}
+
+	lnx_shinfo[0]->mode = nae_cfg->port_fifo_en ? NLM_PORT_FIFO_EN : 0;
+#ifdef CONFIG_NLM_ENABLE_LOAD_BALANCING
+	if((mode == NLM_TCP_MODE) && load_balance_en)
+		lnx_shinfo[0]->mode |= NLM_TCP_LOAD_BALANCE_MODE;
+	else
+#endif
+	lnx_shinfo[0]->mode |= mode;
+	lnx_shinfo[0]->flags |= (*jumbo_enabled) ? NLM_NAE_LNX_SHINFO_FL_JUMBO_EN : 0;
+	if(is_nlm_xlp9xx()) {
+		lnx_shinfo[0]->flags |= NLM_NAE_LNX_SHINFO_FL_VER_1 | NLM_NAE_LNX_SHINFO_FL_ALE_BYPASS;
+	}
+	if (nae_cfg->owned) {
+		if(netsoc_write_ucore_shmem(nae_cfg,
+			(uint32_t *)lnx_shinfo[0],  size/sizeof(uint32_t)) != 0) {
+			printk("Error, Write ucore sram failed!!!, size %d\n", size);
+		} else 
+			printk("Write ucore sram success, size %d\n", size);
+
+		netsoc_restart_ucore_using_fdt(nae_cfg, fdt);
+	}
+
+
+	/* initialize my vfbid table */
+	if (!(nae_cfg->flags & VFBID_FROM_FDT))
+		nlm_initialize_vfbid(nae_cfg);
+
+	if (nae_cfg->owned == 0)
+		goto err;
+
+	/* Update RX_CONFIG for desc size */
+	len = (ETH_HLEN + ETH_FCS_LEN + SMP_CACHE_BYTES);
+	if (*jumbo_enabled)
+		netsoc_init_ingress (nae_cfg,
+			(len + ETH_JUMBO_DATA_LEN) & ~(SMP_CACHE_BYTES - 1));
+	else
+		netsoc_init_ingress (nae_cfg,
+			(len + ETH_DATA_LEN) & ~(SMP_CACHE_BYTES - 1));
+
+	if (nlm_configure_shared_freein_fifo(node, nae_cfg) != 0)
+		goto err;
+
+	init_dummy_entries_for_port_fifos(nae_cfg);
+
+	if (nae_cfg->poe->dist_en)
+	{
+		cpu_mask_t cpumap = {{0}};
+
+		/* FIXME: for multi-node */
+		for (i = 0;i < NLM_MAX_NODES;i++)
+			cpumap.mask[i] = phys_cpu_map[i];
+
+		netsoc_config_poe_distvec(nae_cfg->poe, nae_cfg->shinfo[0].domid, &cpumap,
+								  nae_cfg->shinfo[0].rxvc);
+	}
+
+#if 1
+	for (i = 0; i <= NLM_NAE_MAX_SHARED_DOMS; i++) {
+		printk("naeid %d domid %d node %d flag %x\n", nae_id, 
+				lnx_shinfo[i]->domid, lnx_shinfo[i]->node, lnx_shinfo[i]->flags);
+		for(cpus = 0; cpus < lnx_shinfo[i]->num_cpus; cpus++) {
+			printk("lcpu %d value %08x (pcpu %d rxfifo %d jfifo %d)\n",
+					cpus, lnx_shinfo[i]->fwd_info[cpus],
+					(lnx_shinfo[i]->fwd_info[cpus] >> NLM_NAE_LNX_SHINFO_FWD_INFO_LCPU_2_PCPU_OFF) & 0xff ,
+					(lnx_shinfo[i]->fwd_info[cpus] >> NLM_NAE_LNX_SHINFO_FWD_INFO_LCPU_2_RF_OFF) & 0xff,
+					(lnx_shinfo[i]->fwd_info[cpus] >> NLM_NAE_LNX_SHINFO_FWD_INFO_LCPU_2_JF_OFF) & 0xff);
+		}
+	}
+#endif
+
+
+#if 0
+	if (is_nlm_xlp2xx()) {
+		nlm_hal_msec_tx_default_config(node,
+			0xff,			/* port enable  */
+			0x5555,			/* preamble len */
+			0x0,			/* pkt num      */
+			0x7fffffff);		/* pn threshold */
+		nlm_hal_msec_rx_default_config(node,
+			0xff,			/* port enable  */
+			0xaaaa,			/* preamble len */
+			0x0,			/* packet num   */
+			0x0);			/* replay win size */
+	}
+#endif
+	rv = 0;
+
+err:
+	if(lnx_shinfo[0]) {
+		kfree(lnx_shinfo[0]);
+		lnx_shinfo[0] = NULL;
+	}
+
+	return rv;
+}
+
+static void *allocate_contig_phys(uint32_t align, uint32_t size)
+{
+	void *buf = kmalloc(size + align, GFP_KERNEL);
+        if (buf == NULL)
+                return NULL;
+
+        buf =(void *)(ulong)(CACHELINE_ALIGNED_ADDR((ulong)buf +
+                                align));
+        return buf;
+}
+
+static void __maybe_unused *netlib_malloc(uint32_t size)
+{
+	return kmalloc(size, GFP_KERNEL);
+}
+
+int initialize_nae(uint32_t *phys_cpu_map, int mode, int *jumbo_enabled)
+{
+	ulong __attribute__ ((unused)) mflags;
+	int dom_id = 0;
+	int node, ret, max_nae_units, num_nae;
+	nae_t* nae;
+	struct netsoc_lib_param mod_api;
+
+	msgrng_access_enable(mflags);
+	mod_api.contig_alloc = allocate_contig_phys;
+	mod_api.malloc = NULL;
+	mod_api.phys_to_virt = phys_to_virt;
+	mod_api.virt_to_phys = virt_to_phys;
+	mod_api.readl = NULL;
+	mod_api.writel = NULL;
+	mod_api.free = NULL;
+	mod_api.contig_free = NULL;
+
+	brcm_netsoc_lib_init(&mod_api);
+
+	if (init_netsoc(fdt, dom_id) < 0) {
+                printk("NETSOC initialization failed \n");
+		return -1;
+	}
+	printk("DONE WITH INIT NETSOC #######\n");
+
+	/*get max nae*/
+	max_nae_units = get_num_nae_pernode();
+	for (node = 0; node < NLM_MAX_NODES; node++) {
+		for(num_nae=0; num_nae<max_nae_units; num_nae++){
+			nae = get_nae(node, num_nae);
+			ret = initialize_nae_per_node(nae, phys_cpu_map,
+				mode, jumbo_enabled);
+		}
+	}
+
+	msgrng_access_disable(mflags);
+	printk("%s done\n", __FUNCTION__);
+	return 0;
+}
+
+static int nlm_replenish_per_cpu_buffer(nae_t* nae_cfg,
+					int qindex, int bufcnt)
+{
+	int i, port;
+	int vc_index = 0;
+	int __attribute__ ((unused)) mflags, code;
+	uint64_t msg;
+	struct sk_buff * skb;
+	int ret = 0;
+	int size = NLM_RX_ETH_BUF_SIZE;
+	int node = nae_cfg->node;
+
+	if ((1 << qindex) & lnx_frfifo_jumbo_mask[node][nae_cfg->nae_id])
+		size = NLM_RX_JUMBO_BUF_SIZE;
+
+	/* For queue index 16 and 17, we still use  the port level descriptor info */
+	if (qindex >= 16) {
+		for (port = 0; port < nae_cfg->num_ports; port++) {
+			if(nae_cfg->ports[port].hw_port_id == qindex)
+				bufcnt = nae_cfg->ports[port].num_free_desc;
+	 	}
+	}
+
+	for (i = 0; i < bufcnt; i++) {
+		vc_index = qindex + nae_cfg->frin_queue_base;
+		skb = nlm_xlp_alloc_skb_atomic(size, node);
+		if (!skb) {
+			printk("[%s] alloc skb failed\n",__FUNCTION__);
+			break;
+		}
+
+		/* Store skb in back_ptr */
+		mac_put_skb_back_ptr(skb, nae_cfg);
+		code = 0;
+
+		/* Send the free Rx desc to the MAC */
+		msgrng_access_enable(mflags);
+		msg = (uint64_t)virt_to_bus(skb->data) &
+			0xffffffffffULL;
+
+		/* Send the packet to nae rx  */
+		__sync();
+
+		if ((ret = xlp_message_send_1(vc_index, code, msg)) & 0x7) {
+			print_fmn_send_error(__func__, ret);
+			printk("Unable to send configured free desc, ");
+			printk("check freein carving (qid=%d)\n", vc_index);
+
+			/* free the buffer and return! */
+			dev_kfree_skb_any(skb);
+
+			msgrng_access_disable(mflags);
+			ret = -EBUSY;
+			break;
+		}
+		msgrng_access_disable(mflags);
+	}
+
+	printk("Send %d descriptors for queue %d(vc %d) of length %d\n",
+		bufcnt, qindex, vc_index, size);
+
+	return ret;
+}
+
+int replenish_freein_fifos(void)
+{
+	int node, i, rv = 0, nae_id;
+	nae_t* nae_cfg;
+	int max_descs_pqueue, num_descs, max_nae_units;
+	unsigned int blk_cmplx_map, cmplx;
+	unsigned int ndescs_nq, ndescs_jq;
+
+	/*get max nae*/
+	max_nae_units = get_num_nae_pernode();
+	for (node = 0; node < NLM_MAX_NODES; node++) {
+		int num_nae;
+		for(num_nae=0; num_nae<max_nae_units; num_nae++){
+			nae_cfg = get_nae(node, num_nae);
+			if (nae_cfg == NULL)
+				continue;
+			nae_id = nae_cfg->nae_id;
+			/* Xaui/rxaui/interlaken uses only one fifo per complex */
+			blk_cmplx_map = nae_cfg->xaui_complex_map |  nae_cfg->rxaui_complex_map |
+				nae_cfg->ilk_complex_map | nae_cfg->xlgmac_complex_map;
+
+			/* configure the descs */
+			for (i = 0; i < nae_cfg->frin_total_queue; i++) {
+				/* if no onchip space. when port fifo is enabled,
+				 we will unset all the unused fifo size */
+				if(nae_cfg->freein_fifo_onchip_num_descs[i] == 0)
+					continue;
+
+				max_descs_pqueue =
+					nae_cfg->freein_fifo_onchip_num_descs[i] +
+						nae_cfg->freein_fifo_spill_num_descs;
+
+				ndescs_nq = num_descs_per_normalq;
+				ndescs_jq = num_descs_per_jumboq;
+				/* if jumbo enabled and port fifo is enabled, all the fifos
+				 will be filled with jumbo packets as the ucore cannot select
+				 the fifos */
+				if(nae_cfg->port_fifo_en) {
+					if(lnx_frfifo_jumbo_mask[node][nae_id]) {
+						lnx_frfifo_jumbo_mask[node][nae_id] |= lnx_frfifo_normal_mask[node][nae_id];
+						lnx_frfifo_normal_mask[node][nae_id] = 0;
+					}
+
+					cmplx = i / MAX_PORTS_PERBLOCK;
+					if((1 << cmplx) & blk_cmplx_map) {
+						ndescs_nq = num_descs_per_normalq * MAX_PORTS_PERBLOCK;
+						ndescs_jq = num_descs_per_jumboq * MAX_PORTS_PERBLOCK;
+					}
+				}
+
+				if ((1 << i) & lnx_frfifo_normal_mask[node][nae_id])
+					num_descs = (ndescs_nq <=
+						max_descs_pqueue) ?
+						ndescs_nq :
+						max_descs_pqueue;
+				else if ((1 << i) & lnx_frfifo_jumbo_mask[node][nae_id])
+					num_descs = (ndescs_jq <=
+						max_descs_pqueue) ?
+						ndescs_jq :
+						max_descs_pqueue;
+				else
+					continue;
+
+				rv = nlm_replenish_per_cpu_buffer(nae_cfg, i, num_descs);
+			}
+			if(rv != 0)
+				break;
+			}
+		}
+	return rv;
+}
+
+void nlm_xlp_nae_remove(void)
+{
+	struct net_device *dev = NULL;
+	struct dev_data *priv = NULL;
+	int node, i;
+
+	for (node = 0; node < NLM_MAX_NODES; node++) {
+		for(i = 0; i < MAX_GMAC_PORT; i++) {
+			dev = xlp_dev_mac[node][i];
+			if (dev == NULL) continue;
+
+			priv = netdev_priv(dev);
+			unregister_netdev(dev);
+			free_netdev(dev);
+		}
+	}
+
+	nlm_nae_remove_procentries();
+}
+
+#if 0
+static void phy_st_timer_handler(unsigned long data)
+{
+        struct timer_list *timer = &phy_int_timer;
+        nlm_hal_restart_an(0, 0);
+
+        timer->expires = jiffies + (HZ * 2);
+        add_timer(timer);
+}
+void init_phy_state_timer(void *data)
+{
+
+        struct timer_list *timer = &phy_int_timer;
+        init_timer(timer);
+        timer->expires = jiffies + 10;
+        timer->data = 0;
+        timer->function = phy_st_timer_handler;
+        add_timer(timer);
+}
+#endif
+
+#if 0
+void nlm_xlp_mac_set_enable(struct dev_data *priv, int flag)
+{
+	int inf;
+	uint32_t speed = 0, duplex = 0, ifmode = 0;
+	uint32_t netwk_inf = 0, mac_cfg2 = 0;
+	struct nlm_hal_mii_info mii_info;
+	if ((priv->type != SGMII_IF) && (priv->type != XAUI_IF))
+		return;
+	switch (priv->type) {
+	case SGMII_IF:
+		inf = (priv->block * 4) + priv->index;
+		break;
+	case XAUI_IF:
+	case INTERLAKEN_IF:
+		inf = priv->block;
+		break;
+	default:
+		return;
+	}
+
+	if (flag) {
+		if (priv->type == SGMII_IF) {
+
+			if(nlm_hal_status_ext_phy(priv->node, inf, &mii_info))
+			{
+				speed= mii_info.speed;
+                                duplex= mii_info.duplex;
+				ifmode = ((speed == 2) ? 2: 1);
+				nlm_hal_mac_disable(priv->node, inf,
+					priv->type);
+				netwk_inf  = read_gmac_reg(priv->node, inf,
+					NETWK_INF_CTRL_REG);
+				netwk_inf &= (~(0x3));
+				write_gmac_reg(priv->node, inf,
+					NETWK_INF_CTRL_REG, netwk_inf | speed);
+				mac_cfg2 = read_gmac_reg(priv->node, inf,
+						MAC_CONF2);
+				mac_cfg2 &= (~((0x3 << 8) | 1));
+				write_gmac_reg(priv->node, inf , MAC_CONF2,
+				      mac_cfg2 | (ifmode << 8) | duplex);
+			}
+		}
+		nlm_hal_mac_enable(priv->node, inf, priv->type);
+		/* disabling the flow control */
+		if (priv->type == XAUI_IF) {
+			uint32_t xaui_cfg;
+			xaui_cfg = nlm_hal_read_mac_reg(priv->node, inf,
+					XGMAC, XAUI_CONFIG_1);
+			xaui_cfg &= (~(XAUI_CONFIG_TCTLEN |
+					XAUI_CONFIG_RCTLEN));
+			nlm_hal_write_mac_reg(priv->node, inf, XGMAC,
+					XAUI_CONFIG_1, xaui_cfg);
+		}
+	} else
+		nlm_hal_mac_disable(priv->node, inf, priv->type);
+}
+#endif
+static int p2p_desc_mem_init(void)
+{
+	int cpu, cnt;
+	int dsize, tsize, pktsize;
+	void *buf;
+	/* MAX_SKB_FRAGS + 4.  Out of 4, 2 will be used for skb and
+	 * freeback storage
+	 */
+	pktsize = MAX_SKB_FRAGS + P2P_EXTRA_DESCS + MSEC_EXTRA_MEM;
+	dsize = (((pktsize * sizeof(uint64_t)) +
+			CACHELINE_SIZE - 1) & (~((CACHELINE_SIZE)-1)));
+	tsize = dsize * MAX_TSO_SKB_PEND_REQS;
+
+	printk("%s in, dsize %d tsize %d \n", __FUNCTION__, dsize, tsize);
+
+	for (cpu = 0; cpu < NR_CPUS; cpu++) {
+		buf = cacheline_aligned_kmalloc(tsize, GFP_KERNEL);
+		if (!buf)
+			return -ENOMEM;
+
+		p2p_desc_mem[cpu].mem = buf;
+		for (cnt = 1; cnt < MAX_TSO_SKB_PEND_REQS; cnt++) {
+			*(ulong *)buf = (ulong)(buf + dsize);
+			buf += dsize;
+			*(ulong *)buf = 0;
+		}
+		p2p_desc_mem[cpu].dsize = dsize;
+	}
+	return 0;
+}
+
+static void nlm_enable_l3_l4_parser(nae_t *nae)
+{
+	l2_parser_config_t l2;
+	l3_parser_config_t l3;
+	l4_parser_config_t l4;
+
+	printk("Enabling parser for nae node %d naeid %d\n",
+			nae->node, nae->nae_id);
+
+	memset(&l2, 0, sizeof(l2));
+	memset(&l3, 0, sizeof(l3));
+	memset(&l4, 0, sizeof(l4));
+
+	l2.l2_proto = 1;
+	netsoc_config_nae_l2parser(nae, &l2);
+
+	l3.l2_proto_eth_mask =1;
+	l3.l3_hdr_off=0;
+	l3.l4_proto_off=9;
+	l3.l4_extract_en = 1;
+	l3.l2_proto = 1;
+	l3.eth_type = 0x800;
+	l3.l3_hdr_off0=12;
+	l3.l3_hdr_len0=4;
+	l3.l3_hdr_off1=16;
+	l3.l3_hdr_len1=4;
+	netsoc_config_nae_l3parser(nae, &l3, 0);
+	
+	l4.l4_proto_mask = 1;
+	l4.l4_proto = 0x6;
+	l4.l4_hdr_off0 = 0;
+	l4.l4_hdr_len0= 2;
+	l4.l4_hdr_off1 = 2;
+	l4.l4_hdr_len1= 2;
+	netsoc_config_nae_l4parser(nae, &l4, 0);
+
+	netsoc_enable_nae_hwparser(nae);
+
+}
+
+#ifdef CONFIG_NLM_NET_OPTS
+/* Get the hardware replenishment queue id */
+int get_hw_frfifo_queue_id(int rxnode, nae_t* nae_cfg,
+				  int cpu, uint32_t truesize, int hw_port_id)
+{
+	/*
+	 * We have to use the logical map here as the below arrays are
+	 * indexed by logical cpu id
+	 */
+	int qid;
+	int node_cpu = __cpu_number_map[cpu] % NLM_NCPUS_PER_NODE;
+
+	if(nae_cfg->port_fifo_en) {
+		qid = hw_port_id;
+	} else {
+		qid = cpu_2_normal_frfifo[rxnode][node_cpu];
+
+		if (enable_jumbo)
+			if(truesize > NLM_RX_JUMBO_BUF_SIZE)
+				qid = cpu_2_jumbo_frfifo[rxnode][node_cpu];
+	}
+	/*
+	 * all the nodes vfbtable should be filled with starting node of
+	 * 0 to ending node with 20 entries each
+	 */
+	return nae_cfg->vfbtbl_hw_offset +
+		(rxnode * NLM_NAE_MAX_FREEIN_FIFOS_PER_NODE) + qid;
+}
+#endif
+
+/**********************************************************************
+ * nlm_xlp_nae_open -  called when bring up a device interface
+ * @dev  -  this is per device based function
+ *
+ **********************************************************************/
+static int  nlm_xlp_nae_open (struct net_device *dev)
+{
+	struct dev_data *priv = netdev_priv(dev);
+	int i, flow_ctrl;
+	int ret = 0;
+	nae_t* nae_cfg = priv->nae;
+	static int done[NLM_MAX_NODES][MAX_NAE_PERNODE] = { 
+		{0,0}, {0,0}, {0,0}, {0,0} };
+	if (perf_mode == NLM_TCP_MODE) {
+#ifdef TSO_ENABLED
+		tso_enable(dev, 1);
+#endif
+#ifdef CONFIG_INET_LRO
+		lro_init(dev);
+#endif
+		if (!done[nae_cfg->node][nae_cfg->nae_id]) {
+			done[nae_cfg->node][nae_cfg->nae_id] = 1;
+			nlm_enable_l3_l4_parser(nae_cfg);
+		}
+
+		if(nlm_prepad_len)
+			netsoc_prepad_enable(nae_cfg, nlm_prepad_len);
+	}
+
+
+	if (priv->inited) {
+		spin_lock_irq(&priv->lock);
+		if(nae_cfg->owned)
+			netsoc_open_port(nae_cfg, priv->port);
+			//nlm_xlp_mac_set_enable(priv, 1);
+		netif_tx_wake_all_queues(dev);
+		spin_unlock_irq(&priv->lock);
+		return 0;
+	}
+
+#ifdef ENABLE_NAE_PIC_INT
+	{
+		int port = priv->port;
+		irq  = irt_irq_table[PIC_IRT_NA_INDEX(port)][0];
+		if (request_irq( irq, nlm_xlp_nae_int_handler,
+			IRQF_SHARED, dev->name, dev)) {
+			ret = -EBUSY;
+			printk("can't get mac interrupt line (%d)\n",dev->irq);
+		}
+		dump_irt_entry(PIC_IRT_NA_INDEX(port));
+	}
+#endif
+
+	netif_tx_start_all_queues(dev);
+
+	for (i = 0; i < NR_CPUS; i++) {
+		priv->cpu_stats[i].tx_packets	= 0;
+		priv->cpu_stats[i].txc_packets	= 0;
+		priv->cpu_stats[i].rx_packets	= 0;
+		priv->cpu_stats[i].interrupts	= 0;
+
+	}
+
+	priv->inited = 1;
+
+	if(nae_cfg->owned)
+		netsoc_open_port(nae_cfg, priv->port);
+		/* flow control */
+		flow_ctrl= netsoc_flow_control_status(nae_cfg, priv->port);
+		if(flow_ctrl)
+			priv->flow_ctrl=1;
+		else 
+			priv->flow_ctrl=0;
+		//nlm_xlp_mac_set_enable(priv, 1);
+		//netsoc_disable_flow_control(priv->nae_port);
+
+	return ret;
+}
+
+/**********************************************************************
+ * nlm_xlp_nae_stop -  called when bring down the interface
+ * @dev  -  this is per device based function
+ *
+ **********************************************************************/
+static int  nlm_xlp_nae_stop (struct net_device *dev)
+{
+	struct dev_data *priv = netdev_priv(dev);
+	nae_t* nae_cfg = priv->nae;
+
+	spin_lock_irq(&priv->lock);
+
+	if (nae_cfg->owned){
+		//nlm_xlp_mac_set_enable(priv, 0);
+	}
+	priv->inited = 0;
+	netif_tx_stop_all_queues(dev);
+
+	spin_unlock_irq(&priv->lock);
+	return 0;
+}
+
+/**********************************************************************
+ * nlm_xlp_set_multicast_list
+ *
+ **********************************************************************/
+#if 0
+static void  nlm_xlp_set_multicast_list (struct net_device *dev)
+{
+	struct dev_data *priv = netdev_priv(dev);
+        int reg_val=0;
+
+	if ((dev->flags & IFF_PROMISC)) {
+		reg_val=nlm_hal_read_mac_reg(priv->node, priv->block, priv->index,
+						MAC_FILTER_CONFIG);
+		reg_val |= ((1 << MAC_FILTER_MCAST_EN_POS)|
+						(1 << MAC_FILTER_ALL_UCAST_EN));
+		nlm_hal_write_mac_reg(priv->node, priv->block, priv->index,
+                	                        MAC_FILTER_CONFIG,reg_val);
+
+		reg_val=nlm_hal_read_mac_reg(priv->node, priv->block, priv->index,
+						NETIOR_VLANTYPE_FILTER);
+		reg_val &= ~(1<<VLAN_RxPAC);
+        	nlm_hal_write_mac_reg(priv->node, priv->block, priv->index,
+						NETIOR_VLANTYPE_FILTER,reg_val);
+	}
+	else {
+		reg_val=nlm_hal_read_mac_reg(priv->node, priv->block, priv->index,
+                        		        MAC_FILTER_CONFIG);
+		if(dev->flags & IFF_ALLMULTI) {
+			reg_val |= (1<<MAC_FILTER_MCAST_EN_POS);
+			reg_val &= ~(1<<MAC_FILTER_ALL_UCAST_EN);
+			nlm_hal_write_mac_reg(priv->node, priv->block, priv->index,
+                          			MAC_FILTER_CONFIG,reg_val);
+		}
+		else {
+			reg_val &= ~((1<<MAC_FILTER_MCAST_EN_POS)|(1 << MAC_FILTER_ALL_UCAST_EN));
+			nlm_hal_write_mac_reg(priv->node, priv->block, priv->index,
+                                MAC_FILTER_CONFIG,reg_val);
+		}
+
+        	reg_val  = nlm_hal_read_mac_reg(priv->node, priv->block, priv->index,
+						NETIOR_VLANTYPE_FILTER);
+		reg_val |= (1<<VLAN_RxPAC);
+        	nlm_hal_write_mac_reg(priv->node, priv->block, priv->index,
+						NETIOR_VLANTYPE_FILTER,reg_val);
+	}
+	return;
+}
+#endif
+
+#if 0
+static void xlp_mac_setup_hwaddr(struct dev_data *priv)
+{
+        struct net_device *dev = priv->dev;
+	int reg_val=0;
+
+	nlm_hal_write_mac_reg(priv->node, priv->block, priv->index,
+				MAC_ADDR0_LO,
+				(dev->dev_addr[5] << 24) |
+				(dev->dev_addr[4] << 16) |
+				(dev->dev_addr[3] << 8)  |
+				(dev->dev_addr[2]));
+
+	nlm_hal_write_mac_reg(priv->node, priv->block, priv->index,
+				MAC_ADDR0_HI,
+				(dev->dev_addr[1] << 24) |
+				(dev->dev_addr[0] << 16));
+
+	nlm_hal_write_mac_reg(priv->node, priv->block, priv->index,
+				MAC_ADDR0_MASK_LO,
+				0xFFFFFFFF);
+	nlm_hal_write_mac_reg(priv->node, priv->block, priv->index,
+				MAC_ADDR0_MASK_HI,
+				0xFFFFFFFF);
+
+
+	nlm_hal_write_mac_reg(priv->node, priv->block, priv->index,
+				MAC_FILTER_CONFIG,
+				(1 << MAC_FILTER_BCAST_EN_POS) |
+                                (1 << MAC_FILTER_ADDR0_VALID_POS));
+
+	reg_val=nlm_hal_read_mac_reg(priv->node, priv->block, priv->index,
+				NETIOR_VLANTYPE_FILTER);
+        reg_val |=(1<<VLAN_RxPAC);
+        nlm_hal_write_mac_reg(priv->node, priv->block, priv->index,
+				NETIOR_VLANTYPE_FILTER,reg_val);
+
+}
+#endif
+
+/**********************************************************************
+ * nlm_xlp_nae_ioctl
+ *
+ **********************************************************************/
+static int  nlm_xlp_nae_ioctl (struct net_device *dev, struct ifreq *rq, int cmd)
+{
+	int rc = 0;
+	printk("nlm_xlp_nae_ioctl called \n");
+	switch (cmd) {
+	case SIOCSHWTSTAMP:
+		printk("HW time stamping supported by HW\n");
+		return 0;
+	default:
+		rc = -EOPNOTSUPP;
+		break;
+	}
+
+	return rc;
+}
+
+/**********************************************************************
+ * nlm_xlp_nae_change_mtu
+ * @dev   -  this is per device based function
+ * @new_mtu -  this is new mtu to be set for the device
+ **********************************************************************/
+static int nlm_xlp_nae_change_mtu(struct net_device *dev, int new_mtu)
+{
+	struct dev_data *priv = netdev_priv(dev);
+	ulong flags;
+	ulong local_mtu, len;
+	nae_t* nae_cfg = priv->nae;
+	net_port_t* nae_port = priv->nae_port;
+
+	if (enable_jumbo &&
+		(new_mtu > ETH_JUMBO_DATA_LEN || new_mtu < ETH_ZLEN)) {
+		printk ("MTU should be between %d and %d\n",
+			ETH_ZLEN, ETH_JUMBO_DATA_LEN);
+		return -EINVAL;
+	}
+
+	if (!enable_jumbo &&
+		(new_mtu > ETH_DATA_LEN || new_mtu < ETH_ZLEN)) {
+		printk ("MTU should be between %d and %d\n",
+			ETH_ZLEN, ETH_DATA_LEN);
+		return -EINVAL;
+	}
+
+	spin_lock_irqsave(&priv->lock, flags);
+
+	len = new_mtu + ETH_HLEN + ETH_FCS_LEN + SMP_CACHE_BYTES;
+	local_mtu = len & ~(SMP_CACHE_BYTES - 1);
+	if (netif_running(dev))
+	{
+		netif_tx_stop_all_queues (dev);
+		if(nae_cfg->owned)
+			netsoc_mac_disable(nae_port);
+			//nlm_xlp_mac_set_enable(priv, 0); /* Disable MAC TX/RX */
+	}
+
+	if (priv->type==SGMII_IF || priv->type==XAUI_IF)
+		netsoc_set_framesize(nae_port, local_mtu);
+	else if (priv->type==INTERLAKEN_IF){
+		/*TODO: Add IL frame set in brcm_netsoc */
+		//nlm_hal_set_ilk_framesize(priv->node, priv->block,
+		//	priv->phy.addr, local_mtu);
+	}
+	else {
+		spin_unlock_irqrestore(&priv->lock, flags);
+		return -1;
+	}
+
+	dev->mtu = new_mtu;
+
+	if (netif_running(dev))
+	{
+		netif_tx_start_all_queues (dev);
+		if(nae_cfg->owned)
+			netsoc_mac_enable(nae_port);
+			//nlm_xlp_mac_set_enable(priv, 1);
+
+	}
+
+	spin_unlock_irqrestore(&priv->lock, flags);
+	return 0;
+}
+
+/**********************************************************************
+ * nlm_xlp_mac_get_stats - wrap function for xlp_get_mac_stats
+ * @dev   -  this is per device based function
+ **********************************************************************/
+static struct net_device_stats *nlm_xlp_mac_get_stats(struct net_device *dev)
+{
+	struct dev_data *priv = netdev_priv(dev);
+	ulong flags;
+
+	spin_lock_irqsave(&priv->lock, flags);
+
+	xlp_get_mac_stats(dev, &priv->stats);
+
+	/* XXX update other stats here */
+	spin_unlock_irqrestore(&priv->lock, flags);
+
+	return &priv->stats;
+}
+
+/**********************************************************************
+ * nlm_xlp_nae_tx_timeout -  called when transmiter timeout
+ * @dev  -  this is per device based function
+ *
+ **********************************************************************/
+static void  nlm_xlp_nae_tx_timeout (struct net_device *dev)
+{
+	struct dev_data *priv = netdev_priv(dev);
+
+	spin_lock_irq(&priv->lock);
+
+	priv->stats.tx_errors++;
+
+	spin_unlock_irq(&priv->lock);
+
+	netif_tx_wake_all_queues(dev);
+
+	printk(KERN_WARNING "%s: Transmit timed out\n", dev->name);
+	return;
+}
+#if 0
+static int nlm_xlp_nae_set_hwaddr(struct net_device *dev, void *p)
+{
+	struct sockaddr *addr = (struct sockaddr *)p;
+	struct dev_data *priv = netdev_priv(dev);
+	int rc = 0;
+
+	rc = eth_mac_addr(dev, p);
+
+	if (rc)
+		return rc;
+
+	if (priv->type == SGMII_IF) {
+	  nlm_hal_write_mac_reg(priv->node, priv->block, priv->index,
+				MAC_ADDR0_LO,
+				(addr->sa_data[5] << 24) |
+				(addr->sa_data[4] << 16) |
+				(addr->sa_data[3] << 8) |
+				(addr->sa_data[2]));
+
+	  nlm_hal_write_mac_reg(priv->node, priv->block, priv->index,
+				MAC_ADDR0_HI,
+				(addr->sa_data[1] << 24) |
+				(addr->sa_data[0] << 16));
+	}
+
+	return rc;
+}
+#endif
+
+#ifdef ENABLE_NAE_PIC_INT
+/**********************************************************************
+ * nlm_xlp_nae_int_handler -  interrupt handler
+ * @irq     -  irq number
+ * @dev_id  -  this device
+ *
+ **********************************************************************/
+static irqreturn_t nlm_xlp_nae_int_handler(int irq, void *dev_id)
+{
+        struct net_device *dev;
+        struct dev_data *priv;
+	int i;
+	int cpu = 0;
+
+	cpu = hard_smp_processor_id();
+	priv->cpu_stats[cpu].interrupts++;
+
+	if(!dev_id) {
+		printk("[%s]: NULL dev_id \n", __FUNCTION__ );
+		return IRQ_HANDLED;
+	}
+	dev = (struct net_device*)dev_id;
+	priv = netdev_priv(dev);
+
+	i = find_irt_from_irq(irq);
+
+
+	return IRQ_HANDLED;
+}
+#endif
+
+static const struct net_device_ops nlm_xlp_nae_ops = {
+	.ndo_open			= nlm_xlp_nae_open,
+	.ndo_stop			= nlm_xlp_nae_stop,
+	.ndo_start_xmit			= nlm_xlp_nae_start_xmit,
+#ifdef NOTYET
+	.ndo_set_multicast_list		= nlm_xlp_set_multicast_list,
+#endif
+	.ndo_do_ioctl			= nlm_xlp_nae_ioctl,
+	.ndo_tx_timeout 		= nlm_xlp_nae_tx_timeout,
+	.ndo_change_mtu			= nlm_xlp_nae_change_mtu,
+	//.ndo_set_mac_address		= nlm_xlp_nae_set_hwaddr,
+	.ndo_get_stats 			= nlm_xlp_mac_get_stats,
+	.ndo_select_queue		= nlm_select_queue,
+};
+
+static int nlm_per_port_nae_init(nae_t* nae_cfg, int port, int maxnae)
+{
+	struct net_device *dev;
+	struct dev_data *priv;
+	int cpu;
+	int node = nae_cfg->node;
+	static int port_index = 0;
+
+	if (!nae_cfg->ports[port].valid)
+		return -1;
+
+	dev = alloc_etherdev_mq(sizeof(struct dev_data),
+		NLM_MAX_NODES * num_cpus_per_node);
+	if(!dev)
+		return -1;
+
+	ether_setup(dev);
+	/* routing gives good performance with tx_queue_len = 0 */
+	dev->tx_queue_len = 0;
+
+	priv = netdev_priv(dev);
+	spin_lock_init(&priv->lock);
+	priv->dev = dev;
+	priv->nae = nae_cfg;
+	priv->nae_port = get_net_port(nae_cfg, port);
+	dev->netdev_ops = &nlm_xlp_nae_ops;
+
+	/* set ethtool_ops which is inside xlp_ethtool.c file*/
+	xlp_set_ethtool_ops(dev);
+
+	dev->dev_addr 	= eth_hw_addr[node][(nae_cfg->nae_id)*MAX_GMAC_PORT + port];
+	priv->port	= port;
+	priv->hw_port_id = nae_cfg->ports[port].hw_port_id;
+
+	priv->inited	= 0;
+	priv->node 	= node;
+	priv->block	= nae_cfg->ports[port].hw_port_id / 4;
+	priv->type	= nae_cfg->ports[port].iftype;
+	priv->mgmt_port	= nae_cfg->ports[port].mgmt;
+	priv->port_index = port_index++;
+
+	switch(nae_cfg->ports[port].iftype) {
+	case SGMII_IF:
+		priv->index = nae_cfg->ports[port].hw_port_id & 0x3;
+		priv->phy.addr = nae_cfg->ports[port].hw_port_id;
+		break;
+	case XAUI_IF:
+	#if 0	//TODO:
+		nlm_hal_write_mac_reg(priv->node,
+			(nae_cfg->ports[port].hw_port_id / 4),
+			XGMAC, XAUI_MAX_FRAME_LEN , 0x01800600);
+	#endif /*No frame length should be done here.*/
+		priv->index = XGMAC0;
+		break;
+	case INTERLAKEN_IF:
+		priv->index = INTERLAKEN;
+		priv->phy.addr = nae_cfg->ports[port].ext_phy_addr;
+		if (nae_cfg->ports[port].hw_port_id == 0) {
+			if (dev_alloc_name(dev, "ilk0-%d") < 0)
+				printk("alloc name failed \n");
+		}
+		else {
+			if (dev_alloc_name(dev, "ilk8-%d") < 0)
+				printk("alloc name failed \n");
+		}
+		break;
+	default:
+		priv->index = 0;
+		break;
+	}
+
+	priv->nae_tx_qid = nae_cfg->ports[port].txq;
+	priv->nae_rx_qid = nae_cfg->ports[port].rxq;
+	dev->features |= NETIF_F_LLTX;
+
+	register_netdev(dev);
+
+	//xlp_dev_mac[node][port] = dev;
+	//xlp_mac_setup_hwaddr(priv);
+
+
+	for (cpu = 0; cpu < NR_CPUS; cpu++)
+		per_cpu_netdev[node][cpu][nae_cfg->nae_id][port] = dev;
+
+	return 0;
+}
+
+static inline int get_num_cpus_per_node(void)
+{
+
+	if (is_nlm_xlp9xx())
+		return  XLP9XX_NCPUS_PER_NODE;
+	else
+		return XLP_NCPUS_PER_NODE;
+}
+
+static int nae_proc_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, nae_proc_show, NULL);
+}
+
+static const struct file_operations nae_proc_fops = {
+	.open		= nae_proc_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+static int xlp_mac_proc_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, xlp_mac_proc_show, NULL);
+}
+
+static const struct file_operations xlp_mac_proc_fops = {
+	.open		= xlp_mac_proc_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+/**********************************************************************
+ * nlm_xlp_nae_init -  xlp_nae device driver init function
+ * @dev  -  this is per device based function
+ *
+ **********************************************************************/
+void nlm_xlp_nae_init(void)
+{
+	int i, node = 0, maxnae;
+	struct proc_dir_entry *entry;
+	unsigned char *mode_str[3] = {"INVALID","TCP_PERF","ROUTE_PERF"};
+	nae_t* nae_cfg;
+
+	if (!(perf_mode == NLM_TCP_MODE || perf_mode == NLM_RT_MODE)) {
+		printk("Invalid perf mode passed -- Using TCP_PERF mode\n");
+		perf_mode = NLM_TCP_MODE;
+	}
+
+	printk("======= Module Parameters =========\n");
+	printk("num_descs_per_normalq=%d num_descs_per_jumboq=%d ",
+	       num_descs_per_normalq, num_descs_per_jumboq);
+	printk("perf_mode=%s enable_lro=%d enable_jumbo=%d \n",
+	       mode_str[perf_mode], enable_lro, enable_jumbo);
+
+	for (i = 0; i < NR_CPUS; i++)
+		nlm_mode[CPU_INDEX(i)] = perf_mode;
+
+	for (i = 0; i < NR_CPUS; i++) {
+	        if(!cpu_isset(i, phys_cpu_present_map))
+                        continue;
+		phys_cpu_map[i / NLM_NCPUS_PER_NODE] |=
+			(1 << (i % NLM_NCPUS_PER_NODE));
+	}
+
+	num_cpus_per_node = get_num_cpus_per_node();
+
+	if (perf_mode == NLM_TCP_MODE)
+		p2p_desc_mem_init();
+
+	gen_mac_address();
+
+	if (initialize_nae(phys_cpu_map, perf_mode, &enable_jumbo))
+		return;
+
+	maxnae = get_num_nae_pernode();
+	for (node = 0; node < NLM_MAX_NODES; node++) {
+		int num_nae;
+		for(num_nae=0; num_nae<maxnae; num_nae++){
+			nae_cfg = get_nae(node, num_nae);
+			if (nae_cfg == NULL)
+				continue;
+				for(i = 0; i < nae_cfg->num_ports; i++)
+					nlm_per_port_nae_init(nae_cfg, i, maxnae);
+		}
+	}
+
+	entry = proc_create_data("mac_stats", 0, /* def mode */
+					nlm_root_proc,	/* parent */
+					&xlp_mac_proc_fops,
+					0 /* no client data */);
+
+	if (!entry) {
+		printk("[%s]: Unable to create proc entry for xlp_mac!\n",
+		       __func__);
+	}
+	entry = proc_create_data("nae_stat", 0,
+					nlm_root_proc,	/* parent */
+					&nae_proc_fops,
+					0);
+	if (!entry) {
+		printk("[%s]: Unable to create proc entry for nae_proc!\n",
+		       __func__);
+	}
+
+	if (!enable_napi) {
+		nlm_xlp_disable_napi();
+		exclusive_vc = 1;
+		/*spawn percpu kthread*/
+		nlm_spawn_kthread();
+	}
+
+#ifdef CONFIG_NLM_ENABLE_LOAD_BALANCING
+	if ((perf_mode == NLM_TCP_MODE) && load_balance_en) {
+		nlm_prepad_len = PREPAD_LEN;
+
+		entry = create_proc_entry(
+				"load_info",
+				0,		/* def mode */
+				nlm_root_proc	/* parent   */
+				);
+		if (entry)
+			entry->proc_fops = &nlm_load_balance_proc_fops;
+		printk("Enabling load balance option\n");
+		nlm_init_load_balance();
+	} else {
+		printk("Disabling load balance option\n");
+		load_balance_en = 0;
+	}
+#endif
+
+	if (replenish_freein_fifos() != 0)
+		printk("Replenishmemt of freein fifos failed\n");
+
+	if (enable_napi)
+		nlm_xlp_enable_napi();
+}
+
diff --git a/drivers/netlogic/nae/xlpge_proc.c b/drivers/netlogic/nae/xlpge_proc.c
new file mode 100644
index 0000000..9eb7b5e
--- /dev/null
+++ b/drivers/netlogic/nae/xlpge_proc.c
@@ -0,0 +1,217 @@
+/*-
+ * Copyright (c) 2003-2012 Broadcom Corporation
+ * All Rights Reserved
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in
+ *    the documentation and/or other materials provided with the
+ *    distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY BROADCOM ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL BROADCOM OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
+ * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
+ * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
+ * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ * #BRCM_2# */
+
+#include <linux/module.h>
+#include <linux/pci.h>
+#include <linux/netdevice.h>
+#include <linux/inet_lro.h>
+#include <linux/clocksource.h>
+#include <linux/proc_fs.h>
+
+#include "xlpge.h"
+
+void nlm_nae_remove_procentries(void)
+{
+	/* TODO XXX: revisit nlm_root_proc */
+	remove_proc_entry("mac_stats", nlm_root_proc);
+}
+
+int nae_proc_read(char *page, char **start, off_t off,
+			     int count, int *eof, void *data)
+{
+	int len = 0;
+	int i = 0;
+	uint64_t total_err = 0, total_fast = 0;
+	uint64_t total_slow = 0, total_recv = 0;
+
+	for (i = 0; i < NR_CPUS; i++) {
+		if((receive_count[CPU_INDEX(i)]==0) &&
+				(err_replenish_count[CPU_INDEX(i)] == 0))
+			continue;
+
+		printk("cpu%d, recv %ld fast_repl %ld, slow_repl %ld "
+			"err_repl %ld p2pdalloc %lld\n", i,
+			(ulong)receive_count[CPU_INDEX(i)],
+			(ulong)fast_replenish_count[CPU_INDEX(i)],
+			(ulong)slow_replenish_count[CPU_INDEX(i)],
+			(ulong)err_replenish_count[CPU_INDEX(i)],
+			p2p_dynamic_alloc_cnt[CPU_INDEX(i)]);
+
+		total_err += err_replenish_count[CPU_INDEX(i)];
+		total_fast += fast_replenish_count[CPU_INDEX(i)];
+		total_slow += slow_replenish_count[CPU_INDEX(i)];
+		total_recv += receive_count[CPU_INDEX(i)];
+
+		p2p_dynamic_alloc_cnt[CPU_INDEX(i)] = 0;
+		slow_replenish_count[CPU_INDEX(i)] = 0;
+		fast_replenish_count[CPU_INDEX(i)] = 0;
+		err_replenish_count[CPU_INDEX(i)] = 0;
+		receive_count[CPU_INDEX(i)] = 0;
+	}
+	/*check how many hash are empty...*/
+	printk("TOTAL_FAST_REPL %ld, TOTAL_SLOW_REPL %ld,"
+	       " TOTAL_ERR_REPL %ld TOTAL_RECV %ld\n",
+			(ulong)total_fast,
+			(ulong)total_slow,
+			(ulong)total_err,
+			(ulong)total_recv);
+
+	*eof = 1;
+	return len;
+}
+
+/**********************************************************************
+ * xlp_mac_proc_read -  proc file system read routine
+ * @page     -  buffer address
+ * @dev_id  -  this device
+ *
+ **********************************************************************/
+int xlp_mac_proc_read(char *page, char **start, off_t off,
+			     int count, int *eof, void *data)
+{
+	int len = 0;
+	off_t begin = 0;
+	int i = 0, cpu = 0, node;
+	struct net_device *dev = 0;
+	struct dev_data *priv = 0;
+
+	for (node = 0; node < NLM_MAX_NODES; node++) {
+		for (i = 0; i < MAX_GMAC_PORT; i++) {
+
+			dev = xlp_dev_mac[node][i];
+
+			if(dev == 0) continue;
+
+			priv = netdev_priv(dev);
+
+			for (cpu = 0; cpu < NR_CPUS ; cpu++) {
+				ulong tx = priv->cpu_stats[cpu].tx_packets;
+				ulong txc = priv->cpu_stats[cpu].txc_packets;
+				ulong rx = priv->cpu_stats[cpu].rx_packets;
+				ulong ints = priv->cpu_stats[cpu].interrupts;
+
+				if (!tx && !txc && !rx && !ints) continue;
+
+				len += sprintf(page + len,
+					"per cpu@%d: %lu(txp) %lu(txcp) "
+					"%lu(rxp) %lu(int)\n",
+					cpu, tx, txc, rx, ints);
+			}
+		}
+	}
+
+	*eof = 1;
+
+	*start = page + (off - begin);
+	len -= (off - begin);
+	if (len > count)
+		len = count;
+	if (len < 0)
+		len = 0;
+
+	return len;
+}
+
+int nae_proc_show(struct seq_file *m, void *v)
+{
+	int i = 0;
+	uint64_t total_err = 0, total_fast = 0;
+	uint64_t total_slow = 0, total_recv = 0;
+
+	for (i = 0; i < NR_CPUS; i++) {
+		seq_printf(m, "cpu%d, recv %ld fast_repl %ld, slow_repl %ld "
+			"err_repl %ld p2pdalloc %lld\n", i,
+			(ulong)receive_count[CPU_INDEX(i)],
+			(ulong)fast_replenish_count[CPU_INDEX(i)],
+			(ulong)slow_replenish_count[CPU_INDEX(i)],
+			(ulong)err_replenish_count[CPU_INDEX(i)],
+			p2p_dynamic_alloc_cnt[CPU_INDEX(i)]);
+
+		total_err += err_replenish_count[CPU_INDEX(i)];
+		total_fast += fast_replenish_count[CPU_INDEX(i)];
+		total_slow += slow_replenish_count[CPU_INDEX(i)];
+		total_recv += receive_count[CPU_INDEX(i)];
+
+		p2p_dynamic_alloc_cnt[CPU_INDEX(i)] = 0;
+		slow_replenish_count[CPU_INDEX(i)] = 0;
+		fast_replenish_count[CPU_INDEX(i)] = 0;
+		err_replenish_count[CPU_INDEX(i)] = 0;
+		receive_count[CPU_INDEX(i)] = 0;
+	}
+	/*check how many hash are empty...*/
+	seq_printf(m, "TOTAL_FAST_REPL %ld, TOTAL_SLOW_REPL %ld,"
+	       " TOTAL_ERR_REPL %ld TOTAL_RECV %ld\n",
+			(ulong)total_fast,
+			(ulong)total_slow,
+			(ulong)total_err,
+			(ulong)total_recv);
+
+	return 0;
+}
+
+/**********************************************************************
+ * xlp_mac_proc_read -  proc file system read routine
+ * @page     -  buffer address
+ * @dev_id  -  this device
+ *
+ **********************************************************************/
+int xlp_mac_proc_show(struct seq_file *m, void *v)
+{
+	int i = 0, cpu = 0, node;
+	struct net_device *dev = 0;
+	struct dev_data *priv = 0;
+
+	for (node = 0; node < NLM_MAX_NODES; node++) {
+		for (i = 0; i < MAX_GMAC_PORT; i++) {
+
+			dev = xlp_dev_mac[node][i];
+
+			if(dev == 0) continue;
+
+			priv = netdev_priv(dev);
+
+			for (cpu = 0; cpu < NR_CPUS ; cpu++) {
+				ulong tx = priv->cpu_stats[cpu].tx_packets;
+				ulong txc = priv->cpu_stats[cpu].txc_packets;
+				ulong rx = priv->cpu_stats[cpu].rx_packets;
+				ulong ints = priv->cpu_stats[cpu].interrupts;
+
+				if (!tx && !txc && !rx && !ints) continue;
+
+				seq_printf(m,
+					"per cpu@%d: %lu(txp) %lu(txcp) "
+					"%lu(rxp) %lu(int)\n",
+					cpu, tx, txc, rx, ints);
+			}
+		}
+	}
+
+	return 0;
+}
+
diff --git a/drivers/netlogic/nae/xlpge_ptp.c b/drivers/netlogic/nae/xlpge_ptp.c
new file mode 100644
index 0000000..25b7b44
--- /dev/null
+++ b/drivers/netlogic/nae/xlpge_ptp.c
@@ -0,0 +1,85 @@
+/*-
+ * Copyright (c) 2003-2012 Broadcom Corporation
+ * All Rights Reserved
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in
+ *    the documentation and/or other materials provided with the
+ *    distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY BROADCOM ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL BROADCOM OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
+ * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
+ * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
+ * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ * #BRCM_2# */
+
+#include <linux/module.h>
+#include <linux/pci.h>
+#include <linux/netdevice.h>
+#include <linux/inet_lro.h>
+#include <linux/clocksource.h>
+//#include <linux/timecompare.h>
+
+#include "xlpge.h"
+
+static uint64_t acc_1588[NLM_MAX_NODES];
+
+cycle_t nlm_1588_read_clock0(const struct cyclecounter *tc)
+{
+	return acc_1588[0];
+}
+
+cycle_t nlm_1588_read_clock1(const struct cyclecounter *tc)
+{
+	return acc_1588[1];
+}
+
+cycle_t nlm_1588_read_clock2(const struct cyclecounter *tc)
+{
+	return acc_1588[2];
+}
+
+cycle_t nlm_1588_read_clock3(const struct cyclecounter *tc)
+{
+	return acc_1588[3];
+}
+
+#ifdef IEEE_1588_PTP_ENABLED	
+static void nlm_1588_ptp_hwtstamp_tx(struct sk_buff *skb)
+{
+	struct skb_shared_hwtstamps shhwtstamps;
+	uint64_t regval;
+	uint64_t ns;
+	
+	
+	struct dev_data *priv = netdev_priv(skb->dev);
+	int if_num = priv->hw_port_id;
+	int node = priv->node;
+
+	memset(&shhwtstamps, 0, sizeof(shhwtstamps));
+	regval = nlm_hal_ptp_timer_lo(node, if_num);
+	regval |= (uint64_t)nlm_hal_ptp_timer_hi(node, if_num)<<32;
+	acc_1588[node] = regval;
+	ns = timecounter_cyc2time(&priv->clock, regval);
+	//timecompare_update(&priv->compare, ns);
+	shhwtstamps.hwtstamp = ns_to_ktime(ns);
+	//shhwtstamps.syststamp =	timecompare_transform(&priv->compare, ns);
+	skb_tstamp_tx(skb, &shhwtstamps);
+	Message("nlm_1588_ptp_hwtstamp_tx regval=0x%llx ns=0x%llx node=0x%x\n",
+		regval, ns, node);	
+}
+#endif 
diff --git a/drivers/netlogic/nae/xlpge_rx.c b/drivers/netlogic/nae/xlpge_rx.c
new file mode 100644
index 0000000..e70c0a9
--- /dev/null
+++ b/drivers/netlogic/nae/xlpge_rx.c
@@ -0,0 +1,1091 @@
+/*-
+ * Copyright (c) 2003-2012 Broadcom Corporation
+ * All Rights Reserved
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in
+ *    the documentation and/or other materials provided with the
+ *    distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY BROADCOM ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL BROADCOM OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
+ * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
+ * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
+ * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ * #BRCM_2# */
+
+#include <linux/module.h>
+#include <linux/pci.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/inet_lro.h>
+#include <linux/clocksource.h>
+#include <linux/kthread.h>
+
+#include <nlm_xlp.h>
+#include <nlm_msgring.h>
+#include <nlm_hal_fmn.h>
+
+#include "xlpge.h"
+#include "xlpge_lro.h"
+#include "xlpge_tso.h"
+
+#ifdef CONFIG_NLM_ENABLE_LOAD_BALANCING
+#define NBITS_32 32
+static struct flow_meta_info *nlm_flow_meta_info;
+static struct active_flow_list *nlm_active_flow_list;
+static struct timer_list nlm_load_balance_timer[NR_CPUS];
+static int nlm_load_balance_search_cpu[NR_CPUS][NR_CPUS-1];
+static uint32_t nlm_pcpu_mask[NR_CPUS/NBITS_32] = {0};
+static uint32_t *ucore_shared_data = NULL;
+extern int load_balance_en;
+static uint32_t cpu_weight[NR_CPUS];
+//#define LOAD_BALANCE_DEBUG_ENABLE
+#endif
+
+#ifdef CONFIG_NLM_NET_OPTS
+extern struct sk_buff *last_rcvd_skb[NR_CPUS * 8] ____cacheline_aligned;
+extern uint32_t last_rcvd_len[NR_CPUS * 8] ____cacheline_aligned;
+extern uint32_t last_rcvd_node[NR_CPUS * 8] ____cacheline_aligned;
+extern uint64_t last_rcvd_skb_phys[NR_CPUS * 8] ____cacheline_aligned;
+struct dev_data *last_rcvd_priv[NR_CPUS * 8] ____cacheline_aligned;
+#endif
+
+uint64_t receive_count[NR_CPUS * 8] __cacheline_aligned;
+
+//#define MACSEC_DEBUG	1
+
+inline void process_tx_complete(int cpu, uint32_t src_id, uint64_t msg0)
+{
+        struct sk_buff* skb;
+#ifdef TSO_ENABLED
+        uint64_t *p2pfbdesc;
+#endif
+        uint64_t addr;
+        uint32_t context, node;
+
+        Message("%s cpu %d src_id %d\n", __FUNCTION__, cpu, src_id);
+
+        /* Process Transmit Complete, addr is the skb pointer */
+        addr = msg0 & 0xffffffffffULL;
+
+	if((is_nlm_xlp3xx() || is_nlm_xlp2xx() ||is_nlm_xlp9xx())){ 
+        	context = (msg0 >> 40) & 0x3f;
+	}
+	else{
+        	context = (msg0 >> 40) & 0x3ff;
+	}
+        node = (src_id >> 10) & 0x3;
+        //port = *(cntx2port[node] + context);
+
+
+#ifdef TSO_ENABLED
+        if(nlm_mode[CPU_INDEX(cpu)] == NLM_TCP_MODE){
+                p2pfbdesc = bus_to_virt(addr);
+                skb = (struct sk_buff *)(ulong)(p2pfbdesc[P2P_SKB_OFF]);
+                free_p2p_desc_mem(cpu, p2pfbdesc);
+        } else
+#endif
+                skb = (struct sk_buff *)bus_to_virt(addr);
+
+        if(skb)
+        {
+                dev_kfree_skb_any(skb);
+        }
+        else {
+                printk("[%s]: [txc] Null skb? "
+                       " paddr = %llx (halting cpu!)\n", __func__, addr);
+                cpu_halt();
+        }
+}
+
+#ifdef CONFIG_NLM_ENABLE_LOAD_BALANCING
+
+static void dump_cpu_active_flow_info(int cpu, struct seq_file *m, int weight)
+{
+	struct active_flow_list *afl = nlm_active_flow_list + cpu;
+	ulong mflags;
+
+	if (!nlm_active_flow_list || !nlm_flow_meta_info)
+		return;
+
+	spin_lock_irqsave(&afl->lock, mflags);
+	seq_printf(m, "Cpu%d ==> WeightInUcore %d ActiveFlows %llu, "
+		      "FlowCreated %llu, FlowProcessed %llu\n", cpu, weight,
+			(unsigned long long)afl->nr_active_flows,
+			(unsigned long long)afl->nr_flow_created,
+			(unsigned long long)afl->nr_flow_processed);
+	afl->nr_flow_created = 0;
+	afl->nr_flow_processed = 0;
+	spin_unlock_irqrestore(&afl->lock, mflags);
+	return;
+}
+
+static int nlm_dump_load_balance_stats(struct seq_file *m, void *v)
+{
+	int i = 0;
+
+	memset(cpu_weight, 0, sizeof(cpu_weight));
+
+	if (!nlm_active_flow_list || !nlm_flow_meta_info)
+		return 0;
+
+	for (i = 0; i < NLM_UCORE_SHARED_TABLE_SIZE; i++)
+		cpu_weight[*(ucore_shared_data + i)]++;
+
+	for (i = 0; i < NUM_LOAD_BALANCE_CPU; i++) {
+		if (!cpumask_test_cpu(i, &phys_cpu_present_map))
+			continue;
+		dump_cpu_active_flow_info(i, m,
+			cpu_weight[__cpu_number_map[i]]);
+	}
+	return 0;
+}
+
+int nlm_load_balance_proc_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, nlm_dump_load_balance_stats, NULL);
+}
+
+static void nlm_remove_inactive_flow(int cpu)
+{
+	struct active_flow_list *afl = nlm_active_flow_list + cpu;
+	struct flow_meta_info *fmi;
+	uint64_t bytes_received;
+	ulong mflags;
+	int index;
+	int i = 0, j = 0;
+
+	spin_lock_irqsave(&afl->lock, mflags);
+	for (i = 0; i < afl->nr_active_flows; i++) {
+		index = afl->index_to_flow_meta_info[i];
+		fmi = nlm_flow_meta_info + index;
+#ifdef LOAD_BALANCE_DEBUG_ENABLE
+		if (index > NLM_UCORE_SHARED_TABLE_SIZE)
+			printk("??? Index %d\n",index);
+#endif
+		bytes_received = fmi->total_bytes_rcvd - fmi->last_sample_bytes;
+		fmi->last_sample_bytes = fmi->total_bytes_rcvd;
+		mb();
+		if (bytes_received == 0) {
+#ifdef LOAD_BALANCE_DEBUG_ENABLE
+			if (fmi->cpu_owner != cpu) {
+				printk("Error!! fmi->cpu_owner = %lld, cpu %d, "
+					"entry %d, total_active_flows %lld\n",
+					fmi->cpu_owner, cpu, i,
+					afl->nr_active_flows);
+				continue;
+			}
+#endif
+			/*No Packet Received on this flow!!! Delete the entry */
+			if (i + 1 < afl->nr_active_flows) {
+				j = afl->nr_active_flows - 1;
+				/*Copy the last valid map to here..*/
+				afl->index_to_flow_meta_info[i] =
+					afl->index_to_flow_meta_info[j];
+				afl->index_to_flow_meta_info[j] = 0;
+			} else
+				afl->index_to_flow_meta_info[i] = 0;
+
+			i--;
+			afl->nr_active_flows--;
+			afl->nr_flow_processed++;
+			fmi->cpu_owner = -1;
+			mb();
+			continue;
+		}
+		mb();
+	}
+	spin_unlock_irqrestore(&afl->lock, mflags);
+}
+
+static void setup_search_path(void)
+{
+	uint32_t pcpu;
+	int i = 0;
+	int thrds[4] = {0, 1, 2, 3};
+	int core;
+	int previous, next;
+	int index = 0;
+	int num_phy_cpu=0;
+	int previous_core, next_core;
+#ifdef LOAD_BALANCE_DEBUG_ENABLE
+	int j;
+	unsigned char buf[256];
+#endif
+
+	for (i = 0; i < NUM_LOAD_BALANCE_CPU; i++) {
+		if (!cpumask_test_cpu(i, &phys_cpu_present_map))
+			continue;
+		nlm_pcpu_mask[i/NBITS_32] |= (1UL << (i % NBITS_32));
+		num_phy_cpu++;
+	}
+#ifdef LOAD_BALANCE_DEBUG_ENABLE
+	printk("num_phy_cpu %d, nlm_pcpu_mask %#x\n",
+		num_phy_cpu, nlm_pcpu_mask[0]);
+#endif
+	for (pcpu = 0; pcpu < NUM_LOAD_BALANCE_CPU; pcpu++) {
+		if(!((1U<<(pcpu % NBITS_32)) & nlm_pcpu_mask[pcpu/NBITS_32]))
+			continue;
+
+		core = pcpu / 4;
+		index = 0;
+		for (i = 0; i < 3; i++) {
+			nlm_load_balance_search_cpu[pcpu][index] =
+				thrds[(pcpu + i + 1) % 4] + core * 4;
+			if(((1U << (nlm_load_balance_search_cpu[pcpu][index] % NBITS_32))
+				& nlm_pcpu_mask[pcpu/NBITS_32]))
+				index++;
+		}
+		if (core >= 1)
+			previous = (core - 1) * 4;
+		else
+			previous = (NUM_LOAD_BALANCE_CPU - 4);
+		previous_core = previous / 4;
+
+		if (core < ((NUM_LOAD_BALANCE_CPU/4) - 1))
+			next = (core+1)*4;
+		else
+			next = 0;
+		next_core = next / 4;
+
+		while (index < (num_phy_cpu - 1)) {
+			if ((0xfU << (previous % NBITS_32)) & nlm_pcpu_mask[previous/NBITS_32]) {
+				for (i = 0; i < 4; i++) {
+					if ((1UL<<((i+previous)%NBITS_32)) &
+						nlm_pcpu_mask[(i+previous)/NBITS_32]) {
+						nlm_load_balance_search_cpu[pcpu][index] =
+							i + previous;
+						index++;
+					}
+				}
+			}
+			if ((previous_core) >= 1)
+				previous = (previous_core - 1) * 4;
+			else
+				previous = (NUM_LOAD_BALANCE_CPU - 4);
+			previous_core = previous / 4;
+			if ((0xfU << (next % NBITS_32)) & nlm_pcpu_mask[next/NBITS_32]) {
+				for (i = 0; i < 4; i++) {
+					if ((1UL << ((i + next)%NBITS_32)) & nlm_pcpu_mask[(i+next)/NBITS_32]) {
+						nlm_load_balance_search_cpu[pcpu][index] =
+							i + next;
+						index++;
+					}
+				}
+			}
+			if (next_core < ((NUM_LOAD_BALANCE_CPU/4) - 1))
+				next = (next_core + 1) * 4;
+			else
+				next = 0;
+			next_core = next / 4;
+		}
+	}
+#ifdef LOAD_BALANCE_DEBUG_ENABLE
+	printk("\nSearchPath:\n");
+	for (i = 0; i < NUM_LOAD_BALANCE_CPU; i++) {
+		int len;
+		sprintf(buf, "Cpu%d ==> ", i);
+		len = strlen(buf);
+		for (j = 0; j < NUM_LOAD_BALANCE_CPU - 1; j++) {
+			sprintf(buf + len, "%2d ",
+				nlm_load_balance_search_cpu[i][j]);
+			len = strlen(buf);
+		}
+		sprintf(buf + len,"\n");
+		printk(buf);
+	}
+#endif
+	return;
+}
+
+static void nlm_load_balance_timer_func(unsigned long arg)
+{
+	int cpu = hard_smp_processor_id();
+	struct active_flow_list *myafl = nlm_active_flow_list + cpu;
+	struct active_flow_list *afl = NULL;
+	struct flow_meta_info *fmi;
+	ulong mflags;
+	int idx_to_fmi;
+	int lcpu;
+	int i = 0, j, k;
+	int max_nae = get_num_nae_pernode();
+	int node = 0, nae;
+	nae_t *nae_cfg;
+
+	/*Remove inactive flows.*/
+	nlm_remove_inactive_flow(cpu);
+
+	/*Check if there is a `threshold %` of diff in load between us and any other cpus. If so
+	borrow few flows from it.
+	*/
+	for (i = 0; i < (NUM_LOAD_BALANCE_CPU - 1); i++) {
+		if (!cpumask_test_cpu(i, &phys_cpu_present_map))
+			continue;
+		afl = nlm_active_flow_list +
+			nlm_load_balance_search_cpu[cpu][i];
+		Message("Checking for CPU %d\n",
+			nlm_load_balance_search_cpu[cpu][i]);
+
+		if(afl->nr_active_flows <= 1)
+			continue;
+
+		if(myafl->nr_active_flows < (afl->nr_active_flows - 1)) {
+			/*Borrow a flow from this cpu*/
+			Message("NR_ACTIVE_FLOWS %ld", afl->nr_active_flows);
+
+restart:
+			spin_lock_irqsave(&afl->lock, mflags);
+			for (j = 0; j < afl->nr_active_flows; j++) {
+				fmi = nlm_flow_meta_info +
+					afl->index_to_flow_meta_info[j];
+
+				if(myafl->nr_active_flows >=
+					(afl->nr_active_flows - 1))
+					break;
+
+				if (fmi->cpu_owner !=
+					nlm_load_balance_search_cpu[cpu][i]) {
+					spin_unlock_irqrestore(&afl->lock, mflags);
+					Message("Flow is borrowed by cpu %lld\n",
+						fmi->cpu_owner);
+					goto restart;
+				}
+				/*Borrow a flow...*/
+				fmi->cpu_owner = cpu;
+				idx_to_fmi = afl->index_to_flow_meta_info[j];
+				/* Update the active flow list of the cpu from
+				 * which we just borrowed the flow.
+				 */
+				if (j + 1 < afl->nr_active_flows) {
+					k = afl->nr_active_flows - 1;
+					/*Copy the last valid map to here..*/
+					afl->index_to_flow_meta_info[j] = 
+						afl->index_to_flow_meta_info[k];
+					afl->index_to_flow_meta_info[k] = 0;
+				} else
+					afl->index_to_flow_meta_info[j] = 0;
+				j--;
+				afl->nr_active_flows--;
+				spin_unlock_irqrestore(&afl->lock, mflags);
+
+				spin_lock_irqsave(&myafl->lock, mflags);
+				/* Create a new entry for this FMI in 
+				 * ACTIVE_FLOW_LIST. Change the u-core shared
+				 * memroy once all data structures are in place.
+				 */
+				k = myafl->nr_active_flows;
+				myafl->index_to_flow_meta_info[k] = idx_to_fmi;
+				myafl->nr_active_flows++;
+				mb();
+				spin_unlock_irqrestore(&myafl->lock, mflags);
+
+				/*Update ucore shared memory*/
+				lcpu = __cpu_number_map[cpu];
+				for(nae =0; nae < max_nae; nae++) {
+					nae_cfg = get_nae(node, nae);
+					if(nae_cfg)
+						netsoc_modify_ucore_sram(nae_cfg, &lcpu,
+						NLM_UCORE_SHMEM_OFFSET +
+						(idx_to_fmi * 4), 1);
+				}
+				*(ucore_shared_data + idx_to_fmi) = 
+					__cpu_number_map[cpu];
+
+				spin_lock_irqsave(&afl->lock, mflags);
+			}
+			spin_unlock_irqrestore(&afl->lock, mflags);
+		}
+	}
+	if (!myafl->nr_active_flows)
+		mod_timer_pinned(&nlm_load_balance_timer[cpu], jiffies + 50);
+	else
+		mod_timer_pinned(&nlm_load_balance_timer[cpu],
+			jiffies + load_balance_timer_run * HZ);
+}
+
+void nlm_setup_load_balance_timer(void *data)
+{
+	int pcpu = hard_smp_processor_id();
+	nlm_load_balance_timer[pcpu].expires = jiffies + 5*HZ;
+	mod_timer_pinned(&nlm_load_balance_timer[pcpu],  jiffies + 5*HZ);
+}
+
+void nlm_init_load_balance(void)
+{
+	uint32_t signature;
+	int i = 0, j = 0;
+	int max_nae = get_num_nae_pernode();
+	int node = 0, nae;
+	nae_t *nae_cfg;
+
+
+
+	nlm_active_flow_list = vmalloc(sizeof(struct active_flow_list) *
+		NUM_LOAD_BALANCE_CPU);
+
+	if (!nlm_active_flow_list) {
+	printk("\nAllocation Failed!!! for size %lu bytes\n",
+		sizeof(struct active_flow_list) * NUM_LOAD_BALANCE_CPU);
+		return;
+	}
+	memset(nlm_active_flow_list, 0, sizeof(struct active_flow_list) *
+		NUM_LOAD_BALANCE_CPU);
+#ifdef LOAD_BALANCE_DEBUG_ENABLE
+	printk("Allocated active_flow_list @ %#lx, size %lu bytes\n",
+		(unsigned long)nlm_active_flow_list,
+		sizeof(struct active_flow_list) * NUM_LOAD_BALANCE_CPU);
+#endif 
+	nlm_flow_meta_info = vmalloc(sizeof(struct flow_meta_info) *
+		(NLM_UCORE_SHARED_TABLE_SIZE + 1));
+
+	if (!nlm_flow_meta_info) {
+		printk("\nAllocation Failed!!! for size %lu bytes\n",
+			sizeof(struct flow_meta_info) * NUM_LOAD_BALANCE_CPU);
+		return;
+	}
+	memset(nlm_flow_meta_info, 0, sizeof(struct flow_meta_info) *
+		(NLM_UCORE_SHARED_TABLE_SIZE + 1));
+#ifdef LOAD_BALANCE_DEBUG_ENABLE
+	printk("Allocated flow_meta_info @ %#lx, size %lu bytes\n",
+		(ulong)nlm_flow_meta_info, sizeof(struct flow_meta_info) *
+		(NLM_UCORE_SHARED_TABLE_SIZE + 1));
+#endif 
+
+	/*Init per cpu flow lock*/
+	for (i = 0; i < NUM_LOAD_BALANCE_CPU; i++)
+		spin_lock_init(&((nlm_active_flow_list + i)->lock));
+
+	/*Set owner field to -1*/
+	for (i = 0; i < NLM_UCORE_SHARED_TABLE_SIZE; i++)
+		(nlm_flow_meta_info+i)->cpu_owner = -1;
+
+	for (i = 0; i < NUM_LOAD_BALANCE_CPU; i++) {
+		if (!cpumask_test_cpu(i, &phys_cpu_present_map))
+			continue;
+		/* Register a timer for each cpu to calculate the flow rate */
+		setup_timer(&nlm_load_balance_timer[i],
+			nlm_load_balance_timer_func, i);
+	}
+	on_each_cpu(nlm_setup_load_balance_timer, NULL, 1);
+
+	setup_search_path();
+	/*Update ucore shared memory with the table*/
+	ucore_shared_data = vmalloc(NLM_UCORE_SHARED_TABLE_SIZE *
+		sizeof(uint32_t));
+	if (!ucore_shared_data) {
+		printk("Ucore updation failed!!\n");
+		return;
+	}
+
+	for (i = 0; i < NLM_UCORE_SHARED_TABLE_SIZE; ) {
+		for(j = 0; j < NUM_LOAD_BALANCE_CPU && 
+			i < NLM_UCORE_SHARED_TABLE_SIZE; j++) {
+			if (nlm_pcpu_mask[j/NBITS_32] & (1U << (j%NBITS_32))) {
+				*(ucore_shared_data + i) = __cpu_number_map[j];
+				i++;
+			}
+		}
+	}
+	for(nae=0; nae < max_nae; nae++) {
+		nae_cfg = get_nae(node, nae);
+		if(nae_cfg) {
+			netsoc_modify_ucore_sram(nae_cfg, ucore_shared_data,
+				NLM_UCORE_SHMEM_OFFSET, NLM_UCORE_SHARED_TABLE_SIZE);
+			mb();
+			signature = 0xdeadbeefU;
+			netsoc_modify_ucore_sram(nae_cfg, &signature,
+				NLM_UCORE_SHMEM_OFFSET - 4, 1);
+		}
+	}
+}
+
+#ifdef LOAD_BALANCE_DEBUG_ENABLE
+static void dump_packet(unsigned char *vaddr, int len)
+{
+	int i = 0;
+	for (i = 0; i < len; i+=8) {
+		printk("[%#x] [%#x] [%#x] [%#x] [%#x] [%#x] [%#x] [%#x]\n",
+			*(vaddr + i + 0), *(vaddr + i + 1), *(vaddr + i + 2),
+			*(vaddr + i + 3), *(vaddr + i + 4), *(vaddr + i + 5),
+			*(vaddr + i + 6), *(vaddr + i + 7));
+	}
+}
+
+static void dump_prepad(unsigned char *vaddr)
+{
+	unsigned int *tmp = (unsigned int *)vaddr;
+	int i = 0;
+
+	if (*tmp == 0xdeadbeef) {
+		for(i = 0; i < 4; i++)
+			printk("[%d] ==> [%#x]\n", i, *(tmp + i));
+	} else {
+#if 1
+	for (i = 0; i < 16; i+=8){
+		printk("[%#x] [%#x] [%#x] [%#x] [%#x] [%#x] [%#x] [%#x]\n",
+			*(vaddr + i + 0), *(vaddr + i + 1), *(vaddr + i + 2),
+			*(vaddr + i + 3), *(vaddr + i + 4), *(vaddr + i + 5),
+			*(vaddr + i + 6), *(vaddr + i + 7));
+	}
+#endif
+	}
+}
+#endif
+
+static inline void nlm_update_flow_stats(unsigned int *prepad,
+					 uint32_t len, uint32_t context)
+{
+	int cpu = hard_smp_processor_id();
+	struct active_flow_list *afl = nlm_active_flow_list + cpu;
+	struct flow_meta_info *fmi;
+	uint64_t index;
+	ulong mflags;
+	int hash_index;
+
+	if (perf_mode != NLM_TCP_MODE)
+		return;
+
+	if (*prepad != NLM_LOAD_BALANCING_MAGIC) {
+		/*No extractions have happend...*/
+#ifdef LOAD_BALANCE_DEBUG_ENABLE
+		printk("Invalid Packet!!\n");
+		dump_packet(prepad, 64 + 16);
+#endif
+		return;
+	}
+
+	hash_index = *(prepad + 1);
+	fmi = nlm_flow_meta_info + hash_index;
+
+	if (unlikely(fmi->cpu_owner == -1)) {
+		/*New flow, Create an entry in active flow list*/
+		local_irq_save(mflags);
+		if (!spin_trylock(&afl->lock)) {
+			local_irq_restore(mflags);
+			return;
+		}
+		if (fmi->cpu_owner != -1) {
+			spin_unlock(&afl->lock);
+			local_irq_restore(mflags);
+			return; 
+		}
+		fmi->cpu_owner = cpu;
+		index = afl->nr_active_flows;
+#ifdef LOAD_BALANCE_DEBUG_ENABLE
+		if (index >= NLM_UCORE_SHARED_TABLE_SIZE)
+			printk("Index %lld Crossing Table size...\n", index);
+#endif
+		afl->index_to_flow_meta_info[index] = hash_index;
+		mb();
+		afl->nr_active_flows++;
+		afl->nr_flow_created++;
+		spin_unlock(&afl->lock);
+		local_irq_restore(mflags);
+	}
+	fmi->total_bytes_rcvd += len;
+	mb();
+	return;
+}
+
+#endif
+
+static int inline valid_buffer_lifo(int cpu, uint64_t msg1, uint32_t src_id)
+{
+		int qid = 0;
+		int lifo = (msg1>>60) & 0xf;
+		int nae_id = (src_id>>7)&0x1;
+		nae_t *nae;
+		unsigned long mflags;
+		int ret;
+		int node = (src_id >> 10) & 0x3;
+		uint64_t addr = msg1 & 0xffffffffc0ULL;
+
+		if(is_nlm_xlp9xx()){
+				nae = get_nae(node, nae_id);
+				if(!(nae->freein_fifo_dom_mask & (1<<lifo))){
+						printk("Error Packet: Cpu %d, msg1 %#lx, lifo %d, Owner fifo mask %#lx\n", 
+										cpu, msg1, lifo, nae->freein_fifo_dom_mask);
+						msgrng_access_enable(mflags);
+						qid = nae->frin_queue_base + lifo;
+						for (;;) {
+								ret = xlp_message_send_1(qid, 0, (addr & 0xffffffffffULL));
+								if (!ret) break;
+						}
+						msgrng_access_disable(mflags);
+						return 0;
+				}
+		}
+		return 1;
+}
+
+static inline void process_rx_packets(void *arg, int cpu, unsigned int src_id, 
+		unsigned long long msg0, unsigned long long msg1, unsigned long long msg2)
+{
+	uint64_t addr;
+	uint32_t len, context, truesize;
+	int port, node, err;
+	struct net_device *pdev;
+	struct dev_data *priv = NULL;
+	uint64_t vaddr;
+	struct sk_buff* skb;
+	nae_t* nae_cfg;
+	uint32_t msec_port;
+	struct napi_struct *napi = (struct napi_struct *)arg;
+
+	if(is_nlm_xlp9xx()){
+		msg1 = msg2;	
+	}
+	err = (msg1 >> 4) & 0x1;
+
+	/* Rx packet */
+	addr	= msg1 & 0xffffffffc0ULL;
+
+	if(!valid_buffer_lifo(cpu, msg1, src_id))
+		return;
+
+	len	= (msg1 >> 40) & 0x3fff;
+	if((is_nlm_xlp3xx() || is_nlm_xlp2xx() ||is_nlm_xlp9xx())){ 
+		context = (msg1 >> 54) & 0x3f;
+	}else{
+		context = (msg1 >> 54) & 0x3ff;
+	}
+		
+	node = (src_id >> 10) & 0x3;
+
+	vaddr = (uint64_t)(unsigned long)bus_to_virt(addr);
+	nae_cfg = (nae_t*)mac_get_nae_back_ptr(vaddr);
+	port = nae_cfg->cntx2port[context]; 
+
+#ifdef ENABLE_SANITY_CHECKS
+	if (port >= MAX_GMAC_PORT) {
+		printk("[%s]: bad port=%d, context=%d\n", __func__,
+			port, context);
+		/*TODO: Where to replenish this packet ???? Context is out of range!*/
+		return;
+	}
+#endif
+
+
+#ifdef ENABLE_SANITY_CHECKS
+	if(nae_cfg->nae_id!=0 && nae_cfg->nae_id !=1)
+		printk("Nae ID is  wrong\n");
+#endif
+	pdev = per_cpu_netdev[node][cpu][nae_cfg->nae_id][port];
+	 
+#ifdef ENABLE_SANITY_CHECKS
+	if (!pdev) {
+		printk("[%s]: [rx] wrong port=%d(context=%d)? pdev = NULL!\n",
+			__func__, port, context);
+		return;
+	}
+#endif
+	priv = netdev_priv(pdev);
+
+	if (err) {
+		vaddr = (uint64_t)(unsigned long)bus_to_virt(addr);
+		skb = mac_get_skb_back_ptr(vaddr);
+		mac_refill_frin_skb(nae_cfg, cpu, addr, skb->truesize, priv->hw_port_id);
+		err_replenish_count[CPU_INDEX(cpu)]++;
+		return;
+	}
+	
+	len = len  - ETH_FCS_LEN - nlm_prepad_len;
+
+	skb = mac_get_skb_back_ptr(vaddr);
+
+#ifdef ENABLE_SANITY_CHECKS
+	if (!skb) {
+		printk("[%s] Null skb? addr=%llx, vaddr=%llx, "
+		       "dropping it and losing one buffer!\n",
+				__func__, addr, vaddr);
+		err_replenish_count[CPU_INDEX(cpu)]++;
+		return;
+	}
+#endif
+
+#ifdef CONFIG_NLM_ENABLE_LOAD_BALANCING
+	if(load_balance_en) {
+		if (!priv->mgmt_port)
+			nlm_update_flow_stats((unsigned int *)vaddr, len, context);
+		skb_reserve(skb, nlm_prepad_len);
+	}
+#endif
+
+	if (priv->index == XGMAC0)
+		msec_port = (priv->port | 0xf) << (4 * priv->block);
+	else
+		msec_port = 1 << port;
+
+	nae_cfg = priv->nae;
+
+#ifdef MACSEC_DEBUG
+	printk("%s nae_cfg->sectag_offset = %d sectag_len = %d icv_len = %d\n",
+		__func__, nae_cfg->sectag_offset[port],
+		nae_cfg->sectag_len[port], nae_cfg->icv_len[port]);
+	dump_buffer(skb->data, len, "RX skb pkt:");
+	printk("msec_port = %x port = %d len = %d \
+		nae_cfg->msec_rx_port_enable = %x\n",msec_port, port,
+		len, nae_cfg->msec_rx_port_enable);
+#endif
+
+	/* check if port is tx port is enabled for msec
+	 * else bypass MACSec
+	 */
+	if(nae_cfg->msec_rx_port_enable & msec_port) {
+		short ether_type = *(short*)(((char*)skb->data) +
+					MAC_HEADER_LEN);
+
+		/* Enable MACSec processing */
+		if((ether_type & 0xffff) == MACSEC_ETHER_TYPE) {
+			memcpy((char*)(skb->data + MAC_HEADER_LEN +
+				nae_cfg->sectag_len[port] -
+				MAC_ADDR_LEN /*DST MAC LEN*/),
+				(((char*)skb->data)+MAC_ADDR_LEN),
+				MAC_ADDR_LEN);
+			memcpy((char*)(skb->data + MAC_HEADER_LEN +
+				nae_cfg->sectag_len[port] -
+				(MAC_ADDR_LEN * 2) /*SRC MAC LEN*/),
+				((char*)skb->data), MAC_ADDR_LEN);
+			len = len - nae_cfg->sectag_len[port] -
+				nae_cfg->icv_len[port];
+			skb_reserve(skb, nae_cfg->sectag_len[port]);
+#ifdef MACSEC_DEBUG
+			dump_buffer(skb->data, len, "RX mod skb pkt:");
+#endif
+		}
+	}
+
+	skb->dev = pdev;
+	skb_put(skb, len);
+	skb->protocol = eth_type_trans(skb, pdev);
+
+	truesize = skb->truesize;
+
+#ifdef CONFIG_NLM_NET_OPTS
+	/* Pass the packet to Network stack */
+	last_rcvd_skb[CPU_INDEX(cpu)] = skb;
+	last_rcvd_skb_phys[CPU_INDEX(cpu)] = addr;
+	last_rcvd_len[CPU_INDEX(cpu)] = len;
+	last_rcvd_node[CPU_INDEX(cpu)] = node;
+	last_rcvd_priv[CPU_INDEX(cpu)] = priv;
+#endif
+
+#ifndef CONFIG_NLM_NET_OPTS
+	/* Setting this with NET_OPTS enabled can cause replenishment 
+	*  wrong for jumbo packets, as the hw-replenishment logic is 
+	*  depended on the skb->truesize */
+	skb->truesize = skb->len + sizeof(struct sk_buff);
+#endif
+
+#ifdef CONFIG_INET_LRO
+	if ((skb->dev->features & NETIF_F_LRO) &&
+			(msg1 & RX_IP_CSUM_VALID) && (msg1 & RX_TCP_CSUM_VALID)) {
+
+		lro_receive_skb(&priv->lro_mgr[cpu], skb, NULL);
+		if(!lro_flush_needed[cpu][priv->port_index]) {
+			lro_flush_priv[cpu][lro_flush_priv_cnt[cpu]] = priv;
+			lro_flush_needed[cpu][priv->port_index] = 1;
+			lro_flush_priv_cnt[cpu]++;
+			Message("Adding to lro flush queue cpu %d port %d\n",
+				cpu, priv->port_index);
+		}
+	} else
+#endif
+	{
+		if ((skb->dev->features & NETIF_F_GRO) && napi)
+			napi_gro_receive(napi, skb);
+		else
+			netif_receive_skb(skb);
+	}
+
+	/* Update Stats */
+	receive_count[CPU_INDEX(cpu)]++;
+
+#ifdef CONFIG_NLM_NET_OPTS
+	if (last_rcvd_skb[CPU_INDEX(cpu)]) {
+		slow_replenish_count[CPU_INDEX(cpu)]++;
+		mac_refill_frin_one_buffer(pdev, cpu, truesize);
+		last_rcvd_skb[CPU_INDEX(cpu)] = NULL;
+		last_rcvd_len[CPU_INDEX(cpu)] = 0;
+	}
+#else
+	slow_replenish_count[CPU_INDEX(cpu)]++;
+	mac_refill_frin_one_buffer(pdev, cpu, truesize);
+#endif
+}
+
+/*
+ * NAE poll function on freeback only if rx and freeback vcs are different
+*/
+void xlp_poll_upper(int cpu)
+{
+	unsigned int status;
+	uint64_t msg0 = 0;
+	uint32_t src_id = 0, size, code;
+	unsigned long __attribute__ ((unused)) mflags;
+
+	/* In non-exlusivevc , this vc can be shared with some other moduels */
+	if ((nae_rx_vc == nae_fb_vc) || (!exclusive_vc))
+		return;
+	
+	while (1) {
+			msgrng_access_enable(mflags);
+			status = xlp_message_receive_1(nae_fb_vc, &src_id,
+					&size, &code, &msg0);
+			msgrng_access_disable(mflags);
+
+			if(status) break;
+			__sync();
+
+			process_tx_complete(cpu, src_id, msg0);
+	}
+}
+
+/*
+ * NAE poll function on lower four buckets
+ */
+static int xlp_poll_lower(void *arg, int budget, int cpu)
+{
+	int status;
+	uint64_t msg0 = 0, msg1 = 0, msg2=0;
+	int no_rx_pkt_rcvd = 0;
+	uint32_t src_id = 0, size = 0, code;
+	unsigned long __attribute__ ((unused)) mflags;
+
+	while (budget--) {
+		msgrng_access_enable(mflags);
+		status = xlp_message_receive_3(nae_rx_vc, &src_id, &size,
+				&code, &msg0, &msg1, &msg2);
+		msgrng_access_disable(mflags);
+
+		if (status) {
+			if (enable_napi)
+				break;
+			continue;
+		}
+
+		no_rx_pkt_rcvd++;
+#ifdef ENABLE_SANITY_CHECKS
+		if ((size < 1) && (size > 3)) {
+			printk("Unexpected single entry packet\n");
+			continue;
+		}
+#endif
+		if (size >= 2)
+			process_rx_packets(arg, cpu, src_id, msg0, msg1, msg2);
+		else if (size == 1)
+			process_tx_complete(cpu, src_id, msg0);
+		else {
+			printk("%s , Error invalid message, size %d\n",
+				__func__, size);
+			continue;
+		}	
+	}
+#ifdef CONFIG_INET_LRO
+	if (enable_lro)
+		napi_lro_flush(cpu);
+#endif
+	return no_rx_pkt_rcvd;
+}
+
+
+
+/**********************************************************************
+ * nlm_xlp_nae_msgring_handler -  message ring interrupt handler
+ * @vc-  virtual channel number
+ * @dev_id  -  this device
+ *
+ **********************************************************************/
+static void nlm_xlp_nae_msgring_handler(uint32_t vc, uint32_t src_id,
+					uint32_t size, uint32_t code,
+					uint64_t msg0, uint64_t msg1,
+					uint64_t msg2, uint64_t msg3, void* data)
+{
+	int cpu = hard_smp_processor_id();
+
+	if (vc == nae_rx_vc && size >= 2)
+		 process_rx_packets(NULL, cpu, src_id, msg0, msg1, msg2);
+	else if (vc == nae_fb_vc && size == 1)
+		process_tx_complete(cpu, src_id, msg0);
+	else {
+		printk("%s , Error invalid message, vc %d size %d\n",
+			__func__, vc, size);
+	}
+}
+
+/*
+ * Main NAE napi poll loop for exclusive vc handler
+ */
+
+static int xlp_nae_napi_poll(void *arg, int vc, int budget)
+{
+	int rx_pkts = 0, rx, i;
+	int cpu = hard_smp_processor_id();
+
+	Message("%s in vc %d budget %d\n", __func__, vc, budget);
+
+	if(nlm_mode[CPU_INDEX(cpu)] == NLM_RT_MODE){
+	for(i =0; i < budget; i++) {
+		xlp_poll_upper(cpu);
+		rx = xlp_poll_lower(arg, 1, cpu);
+		if(!rx)
+			break;
+		rx_pkts += rx;
+	}
+	} else {
+		xlp_poll_upper(cpu);
+		rx_pkts = xlp_poll_lower(arg, budget, cpu);
+	}
+
+	return rx_pkts;
+}
+
+/*
+ * Main NAE  poll loop for kthread model
+ */
+static int xlp_nae_poll(void *buf)
+{
+	//unsigned int count=0;
+	int rx_pkts = 0;
+	int cpu = hard_smp_processor_id();
+	int budget = 96;
+
+	if (perf_mode == NLM_RT_MODE)
+		budget = 300000;
+
+	while (1) {
+
+		local_bh_disable();
+		xlp_poll_upper(cpu);
+		rx_pkts = xlp_poll_lower(NULL, budget, cpu);
+		local_bh_enable();
+
+
+		schedule();
+	}
+	return 0;
+}
+
+void nlm_spawn_kthread(void)
+{
+    unsigned int i = 0, nr_cpus;
+    char buf[20];
+    static struct task_struct *task[NR_CPUS];
+
+	//TODO:	
+	nr_cpus = 4*32;
+    /*Spawn kthread*/
+    for (i = 0; i < nr_cpus; i++) {
+	if (!cpumask_test_cpu(i, cpu_present_mask))
+		continue;
+        sprintf(buf,"nae_task_%d",i);
+        task[i] = kthread_create(xlp_nae_poll, (void *)(long)i, (void *)&buf);
+        if (!task[i])
+            break;
+    }
+    if (i == nr_cpus) {
+        for (i = 0; i < nr_cpus; i++) {
+	    if (!cpumask_test_cpu(i, cpu_present_mask))
+		    continue;
+            kthread_bind(task[i], i);
+            wake_up_process(task[i]);
+        }
+    }
+
+}
+
+
+/*
+ * Setup XLP NAPI subsystem
+ */
+extern int nlm_xlp_register_napi_vc_handler(int nae_rx_vc,
+	int (*napi_msgint_handler)(void *, int, int));
+extern int nlm_xlp_register_napi_final_handler(int major,
+	void (*napi_final)(void *arg), void *arg);
+
+int nlm_xlp_enable_napi(void)
+{
+	if (exclusive_vc) {
+		printk("Registering exclusive napi vc handler....\n");
+		nlm_xlp_register_napi_vc_handler(nae_rx_vc, xlp_nae_napi_poll);
+		nlm_xlp_register_napi_vc_handler(nae_fb_vc, xlp_nae_napi_poll);
+
+		return 0;
+	}
+
+	printk("Registering  poe0->cpu msgring handler\n");
+	if (register_xlp_msgring_handler(XLP_MSG_HANDLE_POE_0,
+		nlm_xlp_nae_msgring_handler, NULL)) {
+		printk("Fatal error! Can't register msgring handler "
+		       "for XLP_MSG_HANDLE_NAE_0\n");
+		return -1;
+	}
+	printk("Registering nae-0 FB msgring handler\n");
+	if (register_xlp_msgring_handler(XLP_MSG_HANDLE_NAE_0,
+		nlm_xlp_nae_msgring_handler, NULL)) {
+		printk("Fatal error! Can't register msgring handler "
+		       "for XLP_MSG_HANDLE_NAE_0\n");
+		return -1;
+	}
+
+	if(is_nlm_xlp9xx()){
+		printk("Registering  poe1->cpu msgring handler\n");
+		if (register_xlp_msgring_handler(XLP_MSG_HANDLE_POE_1,
+			nlm_xlp_nae_msgring_handler, NULL)) {
+			printk("Fatal error! Can't register msgring handler "
+			       "for XLP_MSG_HANDLE_NAE_0\n");
+			return -1;
+		}
+		printk("Registering nae-1 FB msgring handler\n");
+		if (register_xlp_msgring_handler(XLP_MSG_HANDLE_NAE_1,
+			nlm_xlp_nae_msgring_handler, NULL)) {
+			printk("Fatal error! Can't register msgring handler "
+		       	"for XLP_MSG_HANDLE_NAE_0\n");
+			return -1;
+		}
+	}
+
+#ifdef CONFIG_INET_LRO
+	if (enable_lro) {
+		printk("Registering poe0->cpu napi final handler\n");
+		nlm_xlp_register_napi_final_handler(XLP_MSG_HANDLE_POE_0,
+				xlp_napi_lro_flush, NULL);
+
+		if(is_nlm_xlp9xx()){
+			printk("Registering poe1->cpu napi final handler\n");
+			nlm_xlp_register_napi_final_handler(XLP_MSG_HANDLE_POE_1,
+					xlp_napi_lro_flush, NULL);
+		}
+	}
+#endif
+
+	return 0;
+}
+
+int nlm_xlp_disable_napi(void)
+{
+	int node, i, coff;
+	for (i = 0; i < NR_CPUS; i++) {
+		if (!cpumask_test_cpu(i, &phys_cpu_present_map))
+			continue;
+		node = i / 32;
+		coff = i * NLM_MAX_VC_PER_THREAD;
+		nlm_hal_disable_vc_intr(node, (coff + nae_rx_vc) & 0x7f);
+		nlm_hal_disable_vc_intr(node, (coff + nae_fb_vc) & 0x7f);
+	}
+	return 0;
+}
diff --git a/drivers/netlogic/nae/xlpge_sgmii.c b/drivers/netlogic/nae/xlpge_sgmii.c
new file mode 100644
index 0000000..3e95cc3
--- /dev/null
+++ b/drivers/netlogic/nae/xlpge_sgmii.c
@@ -0,0 +1,124 @@
+/*-
+ * Copyright (c) 2003-2012 Broadcom Corporation
+ * All Rights Reserved
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in
+ *    the documentation and/or other materials provided with the
+ *    distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY BROADCOM ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL BROADCOM OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
+ * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
+ * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
+ * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ * #BRCM_2# */
+
+#include <linux/module.h>
+#include <linux/pci.h>
+#include <linux/netdevice.h>
+#include <linux/inet_lro.h>
+#include <linux/clocksource.h>
+#include <linux/mii.h>
+
+#include "xlpge.h"
+
+int xlp_enable_autoneg(struct net_device *dev, u32 adv)
+{
+//TODO:
+#if 0
+	struct dev_data *priv = netdev_priv(dev);
+	int mii_status;
+	u32 adv1, adv2;
+	unsigned long flags;
+
+	spin_lock_irqsave(&priv->lock, flags);
+	nlm_xlp_mac_set_enable(priv, 0);
+	/* advertising for 10/100 Mbps */
+	adv1 = nlm_xlp_mac_mii_read(priv, MII_ADVERTISE);
+	adv1 &= ~(ADVERTISE_ALL | ADVERTISE_100BASE4);
+	/* advertising for 1000 Mbps */
+	adv2 = nlm_xlp_mac_mii_read(priv, 0x9);
+	adv2 &= ~(0x300);
+
+	if (adv & ADVERTISED_10baseT_Half)
+		adv1 |= ADVERTISE_10HALF;
+	if (adv & ADVERTISED_10baseT_Full)
+		adv1 |= ADVERTISE_10FULL;
+	if (adv & ADVERTISED_100baseT_Full)
+		adv1 |= ADVERTISE_100FULL;
+	if (adv & ADVERTISED_100baseT_Half)
+		adv1 |= ADVERTISE_100HALF;
+
+	if (adv & ADVERTISED_1000baseT_Full)
+		adv2 |= 0x200;
+	if (adv & ADVERTISED_1000baseT_Half)
+		adv2 |= 0x100;
+
+	/* Set the advertising parameters */
+	nlm_xlp_mac_mii_write(priv, MII_ADVERTISE, adv1);
+	nlm_xlp_mac_mii_write(priv, 0x9, adv2);
+
+	priv->advertising = adv1 | adv2;
+
+	mii_status = nlm_xlp_mac_mii_read(priv, MII_BMCR);
+	/* enable autoneg and force restart autoneg */
+	mii_status |= (BMCR_ANENABLE | BMCR_ANRESTART);
+	nlm_xlp_mac_mii_write(priv, MII_BMCR, mii_status);
+
+	nlm_xlp_mac_set_enable(priv, 1);
+	spin_unlock_irqrestore(&priv->lock, flags);
+
+	return 0;
+#endif
+	return 0;
+}
+
+int xlp_set_link_speed(struct net_device *dev, int speed, int duplex)
+{
+#if 0
+	u32 adv;
+	int ret =0;
+
+	switch(speed) {
+	case SPEED_10:
+		if ( duplex == DUPLEX_FULL )
+			adv = ADVERTISED_10baseT_Full;
+		else
+			adv = ADVERTISED_10baseT_Half;
+		break;
+	case SPEED_100:
+		if ( duplex == DUPLEX_FULL )
+			adv = ADVERTISED_100baseT_Full;
+		else
+			adv = ADVERTISED_100baseT_Half;
+		break;
+	case SPEED_1000:
+		if ( duplex == DUPLEX_FULL )
+			adv = ADVERTISED_1000baseT_Full;
+		else
+			adv = ADVERTISED_1000baseT_Half;
+		break;
+	default:
+		ret = -EINVAL;
+		return ret;
+	}
+	ret = xlp_enable_autoneg( dev,adv);
+	return ret;
+
+#endif
+	return 0;
+}
diff --git a/drivers/netlogic/nae/xlpge_tso.h b/drivers/netlogic/nae/xlpge_tso.h
new file mode 100644
index 0000000..93a1f55
--- /dev/null
+++ b/drivers/netlogic/nae/xlpge_tso.h
@@ -0,0 +1,360 @@
+/*-
+ * Copyright (c) 2003-2012 Broadcom Corporation
+ * All Rights Reserved
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in
+ *    the documentation and/or other materials provided with the
+ *    distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY BROADCOM ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL BROADCOM OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
+ * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
+ * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
+ * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ * #BRCM_2# */
+
+#ifndef	__XLPGE_TSO_H__
+#define __XLPGE_TSO_H__
+#include <linux/module.h>
+#include <linux/pci.h>
+#include <linux/netdevice.h>
+#include <linux/inet_lro.h>
+#include <linux/clocksource.h>
+#include <linux/ethtool.h>
+
+#include <nlm_msgring.h>
+#include <nlm_xlp.h>
+#include <nlm_hal_fmn.h>
+
+#include "xlpge.h"
+//#define MACSEC_DEBUG	1
+extern struct p2p_desc_mem p2p_desc_mem[NR_CPUS];
+
+static __inline__ uint64_t nae_tso_desc0(
+		unsigned int type,
+		unsigned int subtype,
+		unsigned int opcode,
+		unsigned int param_index,
+		unsigned int l3hdroff,
+		unsigned int l4hdroff,
+		unsigned int l3chksumoff,
+		unsigned int pseudohdrchksum,
+		unsigned int l4chksumoff,
+		unsigned int pyldoff)
+{
+
+	return ((uint64_t)(type & 0x3) << 62) |
+		((uint64_t)(subtype & 3) << 60) |
+		((uint64_t)(opcode & 0xf) << 56) |
+		((uint64_t)(param_index & 0xf) << 49) |
+		((uint64_t)(l3hdroff & 0x3f) << 43) |
+		((uint64_t)(l4hdroff & 0x7f) << 36) |
+		((uint64_t)(l3chksumoff & 0x1f) << 31) |
+		((uint64_t)(pseudohdrchksum & 0xffff) << 15) |
+		((uint64_t)(l4chksumoff & 0x7f) << 8) |
+		((uint64_t)(pyldoff & 0xff));
+}
+
+static __inline__ uint64_t nae_tso_desc1(
+		unsigned int type,
+		unsigned int subtype,
+		unsigned int poly,
+		unsigned int mss,
+		unsigned int crcstopoff,
+		unsigned int crcinsoff)
+{
+	return ((uint64_t)(type & 0x3) << 62) |
+		((uint64_t)(subtype & 3) << 60) |
+		((uint64_t)(poly & 0x3) << 48) |
+		((uint64_t)(mss & 0xffff) << 32) |
+		((uint64_t)(crcstopoff & 0xffff) << 16) |
+		((uint64_t)(crcinsoff & 0xffff));
+
+}
+
+static inline void *alloc_p2p_desc_mem(int cpu)
+{
+	void *buf;
+	buf = p2p_desc_mem[cpu].mem;
+	if (buf)
+		p2p_desc_mem[cpu].mem = (void *)*(ulong *)(buf);
+	else {
+		buf = cacheline_aligned_kmalloc(p2p_desc_mem[cpu].dsize,
+			GFP_KERNEL);
+		p2p_dynamic_alloc_cnt[CPU_INDEX(cpu)]++;
+	}
+	return buf;
+}
+
+static inline void free_p2p_desc_mem(int cpu, void *buf)
+{
+	*(ulong *)buf = (ulong)p2p_desc_mem[cpu].mem;
+	p2p_desc_mem[cpu].mem = buf;
+
+}
+
+static inline int create_p2p_desc(uint64_t paddr, uint64_t len,
+				  uint64_t *p2pmsg, int idx)
+{
+	int plen;
+	do {
+		plen = len >= MAX_PACKET_SZ_PER_MSG ?
+				(MAX_PACKET_SZ_PER_MSG - 64): len;
+		p2pmsg[idx] = cpu_to_be64(nae_tx_desc(DESC_TYPE_P2DNEOP, NULL_VFBID,
+				plen, paddr));
+		len -= plen;
+		paddr += plen;
+		idx++;
+	} while (len > 0);
+	return idx;
+}
+
+static inline void create_last_p2p_desc(uint64_t *p2pmsg,
+					struct sk_buff *skb, int idx)
+{
+	p2pmsg[idx -1 ] = cpu_to_be64(be64_to_cpu(p2pmsg[idx - 1]) |
+				((uint64_t)P2D_EOP << 62));
+	p2pmsg[P2P_SKB_OFF] = (uint64_t)(ulong)skb;
+}
+
+static inline uint16_t pseuodo_chksum(uint16_t *ipsrc, uint16_t proto)
+{
+	uint32_t sum = 0;
+	sum += cpu_to_be16(ipsrc[0]);
+	sum += cpu_to_be16(ipsrc[1]);
+	sum += cpu_to_be16(ipsrc[2]);
+	sum += cpu_to_be16(ipsrc[3]);
+	sum += proto;
+	while(sum >> 16)
+		sum = (sum & 0xffff)  + (sum >> 16);
+	//      sum = ~sum;
+	return (uint16_t)sum;
+}
+
+static inline int tso_xmit_skb(struct sk_buff *skb, struct net_device *dev)
+{
+	int mss  = 0, idx = 0, len, i ;
+	struct skb_shared_info *sp = skb_shinfo(skb);
+	struct iphdr *iph;
+	struct dev_data *priv = netdev_priv(dev);
+	uint64_t msg, mscmsg0 = 0, mscmsg1 = 0;
+	uint64_t *p2pdesc = NULL;
+	int cpu = hard_smp_processor_id();
+	int  ret, retry_cnt = 0, qid;
+	nae_t* nae_cfg = priv->nae;
+	unsigned long __attribute__ ((unused)) mflags;
+	uint32_t msec_port, send_msec = 0, msec_bypass = 0;
+	uint32_t pad_len = 0, icv_len = 0, param_index = 0;
+
+#ifdef MACSEC_DEBUG
+	printk("nae_cfg->sectag_offset = %d sectag_len = %d icv_len = %d\n",
+		nae_cfg->sectag_offset[priv->port],
+		nae_cfg->sectag_len[priv->port], nae_cfg->icv_len[priv->port]);
+#endif
+	if(priv->index == XGMAC0)
+		msec_port = (priv->port | 0xf) << (4 * priv->block);
+	else
+		msec_port = 1 << priv->port;
+
+#ifdef MACSEC_DEBUG
+	dump_buffer(skb->data, skb->len, "Org skb pkt:");
+	printk("msec_port = %x priv->port = %d priv->block = %d \
+		priv->index = %d skb->len = %d \
+		nae_cfg->msec_port_enable = %x\n",
+		msec_port, priv->port, priv->block, priv->index,
+		skb->len, nae_cfg->msec_port_enable);
+#endif
+	/* check if tx port is enabled for msec
+	 * else bypass MACSec
+	 */
+	if (nae_cfg->msec_port_enable & msec_port) {
+		short ether_type = *(short*)(((char*)skb->data) +
+					MAC_HEADER_LEN);
+
+#ifdef MACSEC_DEBUG
+	printk("skb->len = %d ether_type = %x\n",
+			skb->len, ether_type);
+#endif
+		/* Enable Macsec processing */
+		if((ether_type & 0xffff) == PROTOCOL_TYPE_IP) {
+			send_msec = 1;
+			/* param_index should be between 1 - 7 */
+			param_index = (priv->port)?priv->port:1;
+
+			pad_len =  nae_cfg->sectag_offset[priv->port] +
+					nae_cfg->sectag_len[priv->port];
+			icv_len = nae_cfg->icv_len[priv->port];
+
+#ifdef MACSEC_DEBUG
+	printk("pad_len = %d icv_len = %d ether_type = %x\n",
+			pad_len, icv_len, ether_type);
+#endif
+		}
+		else
+			msec_bypass = 1;
+	}
+
+	p2pdesc = alloc_p2p_desc_mem(cpu);
+	if (p2pdesc == NULL) {
+		goto out_unlock;
+	}
+
+	if (((mss = sp->gso_size) != 0) ||
+		(skb->ip_summed == CHECKSUM_PARTIAL)) {
+		u32 iphdroff, tcphdroff, pyldoff, pcsum, tcp_packet = 1;
+
+		if (skb_header_cloned(skb) &&
+			pskb_expand_head(skb, 0, 0, GFP_ATOMIC)) {
+			goto out_unlock;
+		}
+
+		iph = ip_hdr(skb);
+		iphdroff = (char *)iph - (char *)skb->data;
+		tcphdroff = iphdroff + ip_hdrlen(skb);
+		if (ip_hdr(skb)->protocol == 0x6) {
+			pyldoff = iphdroff + ip_hdrlen(skb) +
+				sizeof(struct tcphdr) + tcp_optlen(skb);
+			pcsum = pseuodo_chksum((uint16_t *)((char *)iph + 12),
+					0x6);
+			tcp_hdr(skb)->check = 0;
+		} else if (ip_hdr(skb)->protocol == 0x11) {
+			pyldoff = iphdroff + ip_hdrlen(skb) + sizeof(struct udphdr);
+			pcsum = pseuodo_chksum((uint16_t *)((char *)iph + 12),
+					0x11);
+			udp_hdr(skb)->check = 0;
+			tcp_packet = 0;
+		} else {
+			printk("Invalid packet in %s\n", __FUNCTION__);
+			goto out_unlock;
+		}
+
+		if(mss) {
+			iph->check = 0;
+			iph->tot_len = 0;
+			mscmsg0 = nae_tso_desc0(MSC, 1, TSO_IP_TCP_CHKSUM,
+				param_index, iphdroff, tcphdroff,
+				(iphdroff + 10), pcsum, tcphdroff + 16,
+				pyldoff);
+			mscmsg1 = nae_tso_desc1(MSC, 2, 0, mss, 0, 0);
+		} else if(tcp_packet) {
+			mscmsg0 = nae_tso_desc0(MSC, 0, TCP_CHKSUM,
+				param_index, iphdroff, tcphdroff,
+				(iphdroff + 10), pcsum, tcphdroff + 16,
+				pyldoff);
+		} else {
+			mscmsg0 = nae_tso_desc0(MSC, 0, UDP_CHKSUM,
+				param_index, iphdroff, tcphdroff,
+				(iphdroff + 10), pcsum, tcphdroff + 6,
+				pyldoff);
+		}
+
+	} else if (send_msec || msec_bypass) {
+		mscmsg0 = nae_tso_desc0(MSC, 0, 0, param_index,
+				0, 0, 0, 0, 0, 0);
+	}
+
+	if(((len = skb_headlen(skb)) != 0)) {
+		if (send_msec) {
+			memcpy((char*)&p2pdesc[P2P_SKB_OFF-8], skb->data,
+				MAC_HEADER_LEN);
+			idx = create_p2p_desc(virt_to_bus((char *)
+				&p2pdesc[P2P_SKB_OFF-8]), pad_len,
+				p2pdesc, idx);
+			idx = create_p2p_desc(virt_to_bus((((char *)skb->data) +
+				MAC_HEADER_LEN)), (len - MAC_HEADER_LEN),
+				p2pdesc, idx);
+#ifdef MACSEC_DEBUG
+			dump_buffer((char *)&p2pdesc[P2P_SKB_OFF-8],
+				pad_len, "first_seg:");
+			printk(" len = %d pad_len = %d icv_len = %d \
+				param_index = %d\n", len, pad_len, icv_len,
+				param_index);
+#endif
+		} else{
+			idx = create_p2p_desc(virt_to_bus((char *)skb->data), len,
+			p2pdesc, idx);
+		 }
+	}
+
+	for (i = 0; i < sp->nr_frags; i++)  {
+		skb_frag_t *fp = &sp->frags[i];
+		idx = create_p2p_desc(virt_to_bus(((char *)
+				page_address(skb_frag_page(fp))) + fp->page_offset),
+				fp->size, p2pdesc, idx);
+	}
+
+	if (send_msec) {
+		if (!param_index)
+			idx = create_p2p_desc(virt_to_bus((char *)
+				&p2pdesc[P2P_SKB_OFF-2]), icv_len,
+				p2pdesc, idx);
+	}
+
+
+	qid = nae_cfg->vfbtbl_sw_offset + (cpu % num_cpus_per_node);
+	{
+		create_last_p2p_desc(p2pdesc, skb, idx);
+		msg = nae_tx_desc(DESC_TYPE_P2P, qid, idx, virt_to_bus(p2pdesc));
+	}
+	
+
+	__sync();
+
+retry_send:
+	msgrng_access_enable(mflags);
+	if(mss)
+		ret = xlp_message_send_3(priv->nae_tx_qid, 0, mscmsg0,
+				mscmsg1, msg);
+	else if(skb->ip_summed == CHECKSUM_PARTIAL)
+		ret = xlp_message_send_2(priv->nae_tx_qid, 0, mscmsg0, msg);
+	else if (send_msec || msec_bypass)
+		ret = xlp_message_send_2(priv->nae_tx_qid, 0, mscmsg0, msg);
+	else
+		ret = xlp_message_send_1(priv->nae_tx_qid, 0, msg);
+	msgrng_access_disable(mflags);
+	if(ret)	{
+		printk("Transmit failed\n");
+		xlp_poll_upper(cpu);
+		retry_cnt++;
+		if(retry_cnt >= 128) {
+			goto out_unlock;
+		}
+		goto retry_send;
+	}
+
+//	dev->trans_start = jiffies;
+	priv->cpu_stats[cpu].tx_packets += idx;
+
+	return NETDEV_TX_OK;
+out_unlock:
+
+	dev_kfree_skb_any(skb);
+	if(p2pdesc)
+		free_p2p_desc_mem(cpu, p2pdesc);
+	return NETDEV_TX_OK;
+}
+
+static int __maybe_unused tso_enable(struct net_device *dev, u32 data)
+{
+	int rv = 0;
+
+	dev->hw_features = NETIF_F_SG | NETIF_F_IP_CSUM | NETIF_F_TSO;
+	dev->features = dev->hw_features;
+	dev->features |= NETIF_F_HIGHDMA;
+	return rv;
+}
+#endif //TSO_H_
diff --git a/drivers/netlogic/nae/xlpge_tx.c b/drivers/netlogic/nae/xlpge_tx.c
new file mode 100644
index 0000000..8854943
--- /dev/null
+++ b/drivers/netlogic/nae/xlpge_tx.c
@@ -0,0 +1,217 @@
+/*-
+ * Copyright (c) 2003-2012 Broadcom Corporation
+ * All Rights Reserved
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in
+ *    the documentation and/or other materials provided with the
+ *    distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY BROADCOM ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL BROADCOM OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
+ * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
+ * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
+ * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ * #BRCM_2# */
+
+#include <linux/module.h>
+#include <linux/pci.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/inet_lro.h>
+#include <linux/clocksource.h>
+#include <linux/kthread.h>
+
+#include <nlm_msgring.h>
+#include <nlm_xlp.h>
+#include <nlm_hal_fmn.h>
+
+#include "xlpge.h"
+#include "xlpge_tso.h"
+
+#ifdef CONFIG_NLM_NET_OPTS
+extern struct dev_data *last_rcvd_priv[NR_CPUS * 8] ____cacheline_aligned;
+#endif
+//#include <asm/netlogic/kvm_xlp.h>
+
+uint64_t fast_replenish_count[NR_CPUS * 8] __cacheline_aligned;
+uint64_t slow_replenish_count[NR_CPUS * 8] __cacheline_aligned;
+uint64_t err_replenish_count[NR_CPUS * 8] __cacheline_aligned;
+//#define MACSEC_DEBUG 1
+/**********************************************************************
+ * nlm_xlp_nae_start_xmit -  transmit a packet from buffer
+ * @dev  -  this is per device based function
+ * @skb  -  data buffer to send
+ **********************************************************************/
+int nlm_xlp_nae_start_xmit(struct sk_buff *skb, struct net_device *dev)
+{
+	struct dev_data *priv = netdev_priv(dev);
+	int cpu = hard_smp_processor_id(), ret = 0;
+	uint64_t msg0, msg1;
+	int retry_count = 128;
+	volatile int hw_repl = 0;
+	int  offset, qid;
+	unsigned long __attribute__ ((unused)) mflags;
+	uint32_t msec_port, send_msec = 0;
+	uint32_t pad_len = 0, icv_len = 0, param_index = 0 ;
+	unsigned char *buf, sec_tag_hdr[16], icv[16];
+
+	nae_t* nae_cfg = priv->nae;
+
+#ifdef MACSEC_DEBUG
+	printk("nae_cfg->sectag_offset = %d sectag_len = %d icv_len = %d\n",
+		nae_cfg->sectag_offset[priv->port],
+		nae_cfg->sectag_len[priv->port], nae_cfg->icv_len[priv->port]);
+#endif
+	if(priv->index == XGMAC0)
+		msec_port = (priv->port | 0xf) << (4 * priv->block);
+	else
+		msec_port = 1 << priv->port;
+
+#ifdef MACSEC_DEBUG
+	dump_buffer(skb->data, skb->len, "Org skb pkt:");
+	printk("msec_port = %x priv->port = %d priv->block = %d \
+		priv->index = %d skb->len = %d \
+		nae_cfg->msec_tx_port_enable = %x\n",
+		msec_port, priv->port, priv->block, priv->index,
+		skb->len, nae_cfg->msec_tx_port_enable);
+#endif
+	/* check if tx port is enabled for msec
+	 * else bypass MACSec
+	 */
+	if (nae_cfg->msec_tx_port_enable & msec_port) {
+
+#ifdef MACSEC_DEBUG
+	short ether_type = *(short*)(((char*)skb->data) +
+				MAC_HEADER_LEN);
+	printk("skb->len = %d ether_type = %x\n",
+			skb->len, ether_type);
+#endif
+		/* Enable Macsec processing */
+//		if((ether_type & 0xffff) == PROTOCOL_TYPE_IP)
+		{
+			send_msec = 1;
+			/* param_index should be between 1 - 7 */
+			param_index = (priv->port)?priv->port:1;
+
+			pad_len =  nae_cfg->sectag_offset[priv->port] +
+					nae_cfg->sectag_len[priv->port];
+			icv_len = nae_cfg->icv_len[priv->port];
+
+#ifdef MACSEC_DEBUG
+	printk("pad_len = %d icv_len = %d ether_type = %x\n",
+			pad_len, icv_len, ether_type);
+#endif
+		}
+	/*	else
+			msec_bypass = 1;*/
+	}
+
+#ifdef ENABLE_SANITY_CHECKS
+	if (!skb) {
+		printk("[%s] skb is NULL\n",__FUNCTION__);
+		return -1;
+	}
+	if (skb->len == 0) {
+		printk("[%s] skb empty packet\n",__FUNCTION__);
+		return -1;
+	}
+#endif
+#ifdef TSO_ENABLED
+	if(nlm_mode[CPU_INDEX(cpu)] == NLM_TCP_MODE)
+		return tso_xmit_skb(skb, dev);
+#endif
+
+#ifdef CONFIG_NLM_NET_OPTS
+	if (skb->netl_skb && (last_rcvd_skb[CPU_INDEX(cpu)] == skb->netl_skb)
+		&& !skb_shared(skb)
+		&& (last_rcvd_len[CPU_INDEX(cpu)] == skb->len)
+		&& !skb_cloned(skb) && nae_cfg->vfbtbl_hw_nentries) {
+		struct dev_data *rpriv = (struct dev_data *)last_rcvd_priv[CPU_INDEX(cpu)];
+		last_rcvd_skb[CPU_INDEX(cpu)] = NULL;
+		last_rcvd_len[CPU_INDEX(cpu)] = 0;
+
+		qid = get_hw_frfifo_queue_id(last_rcvd_node[CPU_INDEX(cpu)],
+			nae_cfg, cpu, skb->truesize, rpriv->hw_port_id);
+		msg0 = nae_tx_desc(DESC_TYPE_P2DNEOP, 0, last_rcvd_skb_phys[CPU_INDEX(cpu)]);
+		hw_repl = 1;
+
+		Message("Tx, tx complete to nae, cpu %d len %d qid %d\n",
+			cpu, skb->len, qid);
+
+		fast_replenish_count[CPU_INDEX(cpu)]++;
+	}
+	else
+#endif
+	{
+		qid = nae_cfg->vfbtbl_sw_offset + (cpu % num_cpus_per_node);
+		msg0 = nae_tx_desc(DESC_TYPE_P2DNEOP, qid, 0, virt_to_bus(skb));
+
+		Message("Tx, tx complete to cpu, cpu %d len %d qid %d\n",
+			cpu, skb->len, qid);
+	}
+
+	{
+		if(send_msec)
+		{
+			buf = kmalloc(skb->len, GFP_KERNEL);
+			memcpy(buf, &skb->data[MAC_HEADER_LEN], (skb->len - MAC_HEADER_LEN));
+			memcpy(&skb->data[MAC_HEADER_LEN], sec_tag_hdr, pad_len);
+			memcpy(&skb->data[MAC_HEADER_LEN + pad_len], buf, (skb->len - MAC_HEADER_LEN));
+			memcpy(&skb->data[skb->len+pad_len], icv, icv_len);
+			skb->len +=(pad_len + icv_len);
+			kfree(buf);
+
+		}
+		msg1 = nae_tx_desc(DESC_TYPE_P2DEOP, NULL_VFBID, skb->len,
+			       virt_to_bus(skb->data));
+	}
+	if(hw_repl) {
+		/* reset the skb for next rx */
+
+		//dst_release((struct dst_entry *)skb->_skb_dst);
+		skb_dst_drop(skb);
+
+		/* Reset all fields to 0, reset data pointers */
+		skb_reset_ptrs(skb);
+
+		offset = (((unsigned long)skb->data + CACHELINE_SIZE) &
+				~(CACHELINE_SIZE - 1));
+		skb_reserve(skb, (offset - (unsigned long)skb->data));
+
+		/*this buffer already has backptr...
+		mac_put_skb_back_ptr(skb); */
+		skb_reserve(skb, SKB_BACK_PTR_SIZE);
+	}
+
+
+retry_send:
+	msgrng_access_enable(mflags);
+	ret = xlp_message_send_2(priv->nae_tx_qid, 0, msg0, msg1);
+	msgrng_access_disable(mflags);
+	if (ret)
+	{
+		xlp_poll_upper(cpu);
+		retry_count--;
+		if(retry_count){
+			goto retry_send;
+		}
+		dev_kfree_skb_any(skb);
+        }
+
+	dev->trans_start = jiffies;
+	return NETDEV_TX_OK;
+}
-- 
1.7.9.5

