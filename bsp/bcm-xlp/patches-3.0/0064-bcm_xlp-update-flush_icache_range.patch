From 539bc9ab5a413ac038b7e354779a75cf23199d80 Mon Sep 17 00:00:00 2001
From: Jack Tan <jack.tan@windriver.com>
Date: Fri, 14 Mar 2014 00:55:05 +0800
Subject: [PATCH 2/7] bcm_xlp: update flush_icache_range

The function smp_call_function_many() can't be called with irqs_disabled.
But ftrace_init() disables interrupts before calling it, so use for_each_cpu()
to replace on_each_cpu(), else we got the below call trace.

  0:<4>------------[ cut here ]------------
  0:<4>WARNING: at ...kernel/smp.c:464 smp_call_function_many+0x13c/0x3f8()
  0:<d>Modules linked in:  0:
  0:Call Trace:
  0:[<ffffffffc17b3538>] dump_stack+0x1c/0x50
  0:[<ffffffffc10f086c>] warn_slowpath_common+0x8c/0xc0
  0:[<ffffffffc10f08cc>] warn_slowpath_null+0x2c/0x40
  0:[<ffffffffc114da4c>] smp_call_function_many+0x13c/0x3f8
  0:[<ffffffffc114dd70>] on_each_cpu_mask+0x68/0x108
  0:[<ffffffffc10dccfc>] nlm_common_flush_icache_range+0x10c/0x120
  0:[<ffffffffc10cb700>] ftrace_modify_code+0x30/0x48
  0:[<ffffffffc1c58430>] ftrace_dyn_arch_init+0x8c/0xb0
  0:[<ffffffffc1c5dfc8>] ftrace_init+0x38/0x1ac
  0:[<ffffffffc1c50a58>] start_kernel+0x408/0x43c

Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
Signed-off-by: Jack Tan <jack.tan@windriver.com>
---
 arch/mips/mm/c-netlogic.c |  116 +++++++++++++++++++++++++++++++++++++++++----
 1 file changed, 107 insertions(+), 9 deletions(-)

diff --git a/arch/mips/mm/c-netlogic.c b/arch/mips/mm/c-netlogic.c
index 658cf4d..c7fc68f 100644
--- a/arch/mips/mm/c-netlogic.c
+++ b/arch/mips/mm/c-netlogic.c
@@ -214,10 +214,21 @@ static void local_nlm_flush_cache_mm(void *args)
 
 static void nlm_flush_cache_mm(struct mm_struct *mm)
 {
+	int cpu;
+	cpumask_t mask;
+
 	if (!cpu_has_dc_aliases)
 		return;
 
-	on_each_cpu(local_nlm_flush_cache_mm, mm, 1);
+	preempt_disable();
+	cpu = smp_processor_id();
+	mask = *cpu_online_mask;
+	cpumask_clear_cpu(cpu, &mask);
+
+	for_each_cpu(cpu, &mask)
+		local_nlm_flush_cache_mm((void *)mm);
+
+	preempt_enable();
 }
 
 static inline void local_nlm_flush_cache_range(void *args)
@@ -239,9 +250,19 @@ static void nlm_flush_cache_range(struct vm_area_struct *vma,
 					unsigned long start, unsigned long end)
 {
 	int exec = vma->vm_flags & VM_EXEC;
+	int cpu;
+	cpumask_t mask;
+
+	preempt_disable();
+	cpu = smp_processor_id();
+	mask = *cpu_online_mask;
+	cpumask_clear_cpu(cpu, &mask);
 
 	if (cpu_has_dc_aliases || (exec && !cpu_has_ic_fills_f_dc))
-		on_each_cpu(local_nlm_flush_cache_range, vma, 1);
+		for_each_cpu(cpu, &mask)
+			local_nlm_flush_cache_range((void *)vma);
+
+	preempt_enable();
 }
 
 static inline void local_nlm_flush_cache_page(void *args)
@@ -267,12 +288,22 @@ static void nlm_flush_cache_page(struct vm_area_struct *vma,
 				unsigned long addr, unsigned long pfn)
 {
 	struct flush_cache_page_args args;
+	int cpu;
+	cpumask_t mask;
 
 	args.vma = vma;
 	args.addr = addr;
 	args.pfn = pfn;
 
-	on_each_cpu(local_nlm_flush_cache_page, &args, 1);
+	preempt_disable();
+	cpu = smp_processor_id();
+	mask = *cpu_online_mask;
+	cpumask_clear_cpu(cpu, &mask);
+
+	for_each_cpu(cpu, &mask)
+		local_nlm_flush_cache_page((void *)&args);
+
+	preempt_enable();
 }
 
 static void local_nlm_flush_dcache_page(void *addr)
@@ -282,7 +313,17 @@ static void local_nlm_flush_dcache_page(void *addr)
 
 static void nlm_flush_dcache_page(unsigned long addr)
 {
-	on_each_cpu(local_nlm_flush_dcache_page, (void *)addr, 1);
+	int cpu;
+	cpumask_t mask;
+
+	preempt_disable();
+	cpu = smp_processor_id();
+	mask = *cpu_online_mask;
+	cpumask_clear_cpu(cpu, &mask);
+
+	for_each_cpu(cpu, &mask)
+		local_nlm_flush_dcache_page((void *)addr);
+	preempt_enable();
 }
 
 static void nlm_flush_cache_vmap(void)
@@ -313,11 +354,20 @@ static inline void local_nlm_flush_cache_vmap_range(void *args)
 static void nlm_flush_cache_vmap_range(unsigned long vaddr, int size)
 {
 	struct flush_kernel_vmap_range_args args;
+	int cpu;
+	cpumask_t mask;
 
 	args.vaddr = (unsigned long) vaddr;
 	args.size = size;
 
-	on_each_cpu(local_nlm_flush_cache_vmap_range, &args, 1);
+	preempt_disable();
+	cpu = smp_processor_id();
+	mask = *cpu_online_mask;
+	cpumask_clear_cpu(cpu, &mask);
+
+	for_each_cpu(cpu, &mask)
+		local_nlm_flush_cache_vmap_range((void *)&args);
+	preempt_enable();
 }
 #endif /* CONFIG_CPU_XLP */
 
@@ -351,10 +401,26 @@ static void nlm_flush_icache_range_ipi(void *info)
 void nlm_flush_icache_range(unsigned long start, unsigned long end)
 {
 	struct flush_icache_range_args args;
+	int cpu;
+	cpumask_t mask;
 
 	args.start = start;
 	args.end = end;
-	on_each_cpu(nlm_flush_icache_range_ipi, &args, 1);
+
+	/* 
+	 * TODO: don't even send ipi to non-zero thread ids
+	 * This may require some changes to smp_call_function interface,
+	 * for now just avoid redundant cache ops
+	*/
+	preempt_disable();
+	cpu = smp_processor_id();
+	mask = *cpu_online_mask;
+	cpumask_clear_cpu(cpu, &mask);
+
+	for_each_cpu(cpu, &mask)
+		nlm_flush_icache_range_ipi((void *)&args);
+
+	preempt_enable();
 }
 
 static void nlm_flush_cache_sigtramp_ipi(void *info)
@@ -368,7 +434,18 @@ static void nlm_flush_cache_sigtramp_ipi(void *info)
 
 static void nlm_flush_cache_sigtramp(unsigned long addr)
 {
-	on_each_cpu(nlm_flush_cache_sigtramp_ipi, (void *)addr, 1);
+	int cpu;
+	cpumask_t mask;
+
+	preempt_disable();
+	cpu = smp_processor_id();
+	mask = *cpu_online_mask;
+	cpumask_clear_cpu(cpu, &mask);
+
+	for_each_cpu(cpu, &mask)
+		nlm_flush_cache_sigtramp_ipi((void *)addr);
+
+	preempt_enable();
 }
 
 static void nlm_flush_l1_caches_ipi(void *info)
@@ -379,7 +456,17 @@ static void nlm_flush_l1_caches_ipi(void *info)
 
 static void nlm_flush_l1_caches(void)
 {
-	on_each_cpu(nlm_flush_l1_caches_ipi, (void *)NULL, 1);
+	int cpu;
+	cpumask_t mask;
+
+	preempt_disable();
+	cpu = smp_processor_id();
+	mask = *cpu_online_mask;
+	cpumask_clear_cpu(cpu, &mask);
+
+	for_each_cpu(cpu, &mask)
+		nlm_flush_l1_caches_ipi((void *)NULL);
+	preempt_enable();
 }
 
 static void nlm_noflush(void)
@@ -641,11 +728,22 @@ void nlm_flush_cache_L2L3 (unsigned long start_paddr, unsigned long size)
 {
 	struct nlm_l2_flush_t flush;
 	int n;
+	int cpu;
+	cpumask_t mask;
 
 	/* flush L2 */
 	flush.start = start_paddr;
 	flush.size  = size;
-	on_each_cpu(local_nlm_flush_l2, (void *)&flush, 1);
+
+	preempt_disable();
+	cpu = smp_processor_id();
+	mask = *cpu_online_mask;
+	cpumask_clear_cpu(cpu, &mask);
+
+	for_each_cpu(cpu, &mask)
+		local_nlm_flush_l2((void *)&flush);
+
+	preempt_enable();
 
 	/* flush L3 */
 	for (n = 0; n < NLM_NR_NODES; n++) {
-- 
1.7.9.5

