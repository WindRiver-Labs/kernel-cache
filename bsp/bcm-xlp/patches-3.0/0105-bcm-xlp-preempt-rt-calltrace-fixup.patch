From 9b515a7c3d5cf7888dc3096dd7020c47f02d40ec Mon Sep 17 00:00:00 2001
From: Jack Tan <jack.tan@windriver.com>
Date: Tue, 25 Mar 2014 16:42:04 +0800
Subject: [PATCH] bcm-xlp: preempt-rt calltrace fixup

fixed the calltrace:

BUG: sleeping function called from invalid context at kernel/rtmutex.c:659
in_atomic(): 1, irqs_disabled(): 1, pid: 0, name: swapper/0
Preemption disabled at:[<          (null)>]           (null)

CPU: 0 PID: 0 Comm: swapper/0 Not tainted 3.10.19-rt11-WR6.0.0.0_preempt-rt #40
Stack : 0000000000000000 000000000000004f 0000000000000000 0000000000000000
          0000000000000004 ffffffffc0c20000 0000000000000000 0000000000000000
          ffffffffc0f90000 0000000000000000 000000000000004f 0000000000000006
          ffffffffc0b2f900 ffffffffc02262b0 0000000000000000 0000000000000000
          0000000000000000 0000000000000000 ffffffffc0f70000 ffffffffc0f70000
          ffffffffc0a7f1d8 ffffffffc0c1d8f7 ffffffffc0f6ade8 ffffffffc0c1dd90
          0000000000000000 0000000000000000 c0000000a8cd0040 ffffffffc0c90000
          ffffffffc0b2f980 ffffffffc0b2f890 ffffffffc0b2f9a8 ffffffffc0977e98
          ffffffffc0b2f9e0 ffffffffc0227f48 ffffffffc0c1d9f0 ffffffffc0a7f1d8
          0000000000000000 ffffffffc01f9158 0000000000000000 0000000000000000
          ...
Call Trace:
[<ffffffffc01f9158>] show_stack+0xd8/0xf8
[<ffffffffc0977e98>] rt_spin_lock+0x38/0x98
[<ffffffffc0265560>] __wake_up+0x40/0x80
[<ffffffffc065b7e0>] serial8250_tx_chars+0x1a8/0x228
[<ffffffffc065bde0>] serial8250_handle_irq.part.17+0xf0/0x128
[<ffffffffc065a970>] serial8250_interrupt+0x70/0x140
[<ffffffffc02b5c4c>] handle_irq_event_percpu+0xa4/0x328
[<ffffffffc02b5f54>] handle_irq_event+0x84/0xe0
[<ffffffffc02b9ecc>] handle_level_irq+0xe4/0x1a8
[<ffffffffc02b4fd4>] generic_handle_irq+0x54/0x88
[<ffffffffc0978bd4>] do_IRQ+0x2c/0x40
[<ffffffffc01f3740>] ret_from_irq+0x0/0x4
[<ffffffffc01f39e0>] __r4k_wait+0x20/0x40
[<ffffffffc0279e98>] cpu_startup_entry+0x190/0x2e8
[<ffffffffc0c90af0>] start_kernel+0x4a8/0x4c8

Signed-off-by: Jack Tan <jack.tan@windriver.com>
---
 drivers/tty/n_tty.c  |   32 ++++++++++++++++----------------
 fs/eventfd.c         |   32 ++++++++++++++++----------------
 fs/eventpoll.c       |   42 +++++++++++++++++++++---------------------
 fs/timerfd.c         |   26 +++++++++++++-------------
 include/linux/wait.h |   12 ++++++------
 kernel/sched/core.c  |   16 ++++++++--------
 kernel/wait.c        |   30 +++++++++++++++---------------
 mm/filemap.c         |    4 ++--
 net/sunrpc/sched.c   |    4 ++--
 9 files changed, 99 insertions(+), 99 deletions(-)

diff --git a/drivers/tty/n_tty.c b/drivers/tty/n_tty.c
index 6c7fe90..598ee84 100644
--- a/drivers/tty/n_tty.c
+++ b/drivers/tty/n_tty.c
@@ -100,7 +100,7 @@ struct n_tty_data {
 
 	struct mutex atomic_read_lock;
 	struct mutex output_lock;
-	struct mutex echo_lock;
+	struct rt_mutex echo_lock;
 	raw_spinlock_t read_lock;
 };
 
@@ -212,9 +212,9 @@ static void reset_buffer_flags(struct n_tty_data *ldata)
 	ldata->read_head = ldata->read_tail = ldata->read_cnt = 0;
 	raw_spin_unlock_irqrestore(&ldata->read_lock, flags);
 
-	mutex_lock(&ldata->echo_lock);
+	rt_mutex_lock(&ldata->echo_lock);
 	ldata->echo_pos = ldata->echo_cnt = ldata->echo_overrun = 0;
-	mutex_unlock(&ldata->echo_lock);
+	rt_mutex_unlock(&ldata->echo_lock);
 
 	ldata->canon_head = ldata->canon_data = ldata->erasing = 0;
 	bitmap_zero(ldata->read_flags, N_TTY_BUF_SIZE);
@@ -536,7 +536,7 @@ static void process_echoes(struct tty_struct *tty)
 		return;
 
 	mutex_lock(&ldata->output_lock);
-	mutex_lock(&ldata->echo_lock);
+	rt_mutex_lock(&ldata->echo_lock);
 
 	space = tty_write_room(tty);
 
@@ -681,7 +681,7 @@ static void process_echoes(struct tty_struct *tty)
 			ldata->echo_overrun = 0;
 	}
 
-	mutex_unlock(&ldata->echo_lock);
+	rt_mutex_unlock(&ldata->echo_lock);
 	mutex_unlock(&ldata->output_lock);
 
 	if (tty->ops->flush_chars)
@@ -746,10 +746,10 @@ static void add_echo_byte(unsigned char c, struct n_tty_data *ldata)
 
 static void echo_move_back_col(struct n_tty_data *ldata)
 {
-	mutex_lock(&ldata->echo_lock);
+	rt_mutex_lock(&ldata->echo_lock);
 	add_echo_byte(ECHO_OP_START, ldata);
 	add_echo_byte(ECHO_OP_MOVE_BACK_COL, ldata);
-	mutex_unlock(&ldata->echo_lock);
+	rt_mutex_unlock(&ldata->echo_lock);
 }
 
 /**
@@ -764,10 +764,10 @@ static void echo_move_back_col(struct n_tty_data *ldata)
 
 static void echo_set_canon_col(struct n_tty_data *ldata)
 {
-	mutex_lock(&ldata->echo_lock);
+	rt_mutex_lock(&ldata->echo_lock);
 	add_echo_byte(ECHO_OP_START, ldata);
 	add_echo_byte(ECHO_OP_SET_CANON_COL, ldata);
-	mutex_unlock(&ldata->echo_lock);
+	rt_mutex_unlock(&ldata->echo_lock);
 }
 
 /**
@@ -790,7 +790,7 @@ static void echo_set_canon_col(struct n_tty_data *ldata)
 static void echo_erase_tab(unsigned int num_chars, int after_tab,
 			   struct n_tty_data *ldata)
 {
-	mutex_lock(&ldata->echo_lock);
+	rt_mutex_lock(&ldata->echo_lock);
 
 	add_echo_byte(ECHO_OP_START, ldata);
 	add_echo_byte(ECHO_OP_ERASE_TAB, ldata);
@@ -804,7 +804,7 @@ static void echo_erase_tab(unsigned int num_chars, int after_tab,
 
 	add_echo_byte(num_chars, ldata);
 
-	mutex_unlock(&ldata->echo_lock);
+	rt_mutex_unlock(&ldata->echo_lock);
 }
 
 /**
@@ -822,14 +822,14 @@ static void echo_erase_tab(unsigned int num_chars, int after_tab,
 
 static void echo_char_raw(unsigned char c, struct n_tty_data *ldata)
 {
-	mutex_lock(&ldata->echo_lock);
+	rt_mutex_lock(&ldata->echo_lock);
 	if (c == ECHO_OP_START) {
 		add_echo_byte(ECHO_OP_START, ldata);
 		add_echo_byte(ECHO_OP_START, ldata);
 	} else {
 		add_echo_byte(c, ldata);
 	}
-	mutex_unlock(&ldata->echo_lock);
+	rt_mutex_unlock(&ldata->echo_lock);
 }
 
 /**
@@ -850,7 +850,7 @@ static void echo_char(unsigned char c, struct tty_struct *tty)
 {
 	struct n_tty_data *ldata = tty->disc_data;
 
-	mutex_lock(&ldata->echo_lock);
+	rt_mutex_lock(&ldata->echo_lock);
 
 	if (c == ECHO_OP_START) {
 		add_echo_byte(ECHO_OP_START, ldata);
@@ -861,7 +861,7 @@ static void echo_char(unsigned char c, struct tty_struct *tty)
 		add_echo_byte(c, ldata);
 	}
 
-	mutex_unlock(&ldata->echo_lock);
+	rt_mutex_unlock(&ldata->echo_lock);
 }
 
 /**
@@ -1630,7 +1630,7 @@ static int n_tty_open(struct tty_struct *tty)
 	ldata->overrun_time = jiffies;
 	mutex_init(&ldata->atomic_read_lock);
 	mutex_init(&ldata->output_lock);
-	mutex_init(&ldata->echo_lock);
+	rt_mutex_init(&ldata->echo_lock);
 	raw_spin_lock_init(&ldata->read_lock);
 
 	/* These are ugly. Currently a malloc failure here can panic */
diff --git a/fs/eventfd.c b/fs/eventfd.c
index 35470d9..d2ff1e5 100644
--- a/fs/eventfd.c
+++ b/fs/eventfd.c
@@ -55,13 +55,13 @@ __u64 eventfd_signal(struct eventfd_ctx *ctx, __u64 n)
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&ctx->wqh.lock, flags);
+	raw_spin_lock_irqsave(&ctx->wqh.lock, flags);
 	if (ULLONG_MAX - ctx->count < n)
 		n = ULLONG_MAX - ctx->count;
 	ctx->count += n;
 	if (waitqueue_active(&ctx->wqh))
 		wake_up_locked_poll(&ctx->wqh, POLLIN);
-	spin_unlock_irqrestore(&ctx->wqh.lock, flags);
+	raw_spin_unlock_irqrestore(&ctx->wqh.lock, flags);
 
 	return n;
 }
@@ -122,14 +122,14 @@ static unsigned int eventfd_poll(struct file *file, poll_table *wait)
 
 	poll_wait(file, &ctx->wqh, wait);
 
-	spin_lock_irqsave(&ctx->wqh.lock, flags);
+	raw_spin_lock_irqsave(&ctx->wqh.lock, flags);
 	if (ctx->count > 0)
 		events |= POLLIN;
 	if (ctx->count == ULLONG_MAX)
 		events |= POLLERR;
 	if (ULLONG_MAX - 1 > ctx->count)
 		events |= POLLOUT;
-	spin_unlock_irqrestore(&ctx->wqh.lock, flags);
+	raw_spin_unlock_irqrestore(&ctx->wqh.lock, flags);
 
 	return events;
 }
@@ -158,12 +158,12 @@ int eventfd_ctx_remove_wait_queue(struct eventfd_ctx *ctx, wait_queue_t *wait,
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&ctx->wqh.lock, flags);
+	raw_spin_lock_irqsave(&ctx->wqh.lock, flags);
 	eventfd_ctx_do_read(ctx, cnt);
 	__remove_wait_queue(&ctx->wqh, wait);
 	if (*cnt != 0 && waitqueue_active(&ctx->wqh))
 		wake_up_locked_poll(&ctx->wqh, POLLOUT);
-	spin_unlock_irqrestore(&ctx->wqh.lock, flags);
+	raw_spin_unlock_irqrestore(&ctx->wqh.lock, flags);
 
 	return *cnt != 0 ? 0 : -EAGAIN;
 }
@@ -188,7 +188,7 @@ ssize_t eventfd_ctx_read(struct eventfd_ctx *ctx, int no_wait, __u64 *cnt)
 	ssize_t res;
 	DECLARE_WAITQUEUE(wait, current);
 
-	spin_lock_irq(&ctx->wqh.lock);
+	raw_spin_lock_irq(&ctx->wqh.lock);
 	*cnt = 0;
 	res = -EAGAIN;
 	if (ctx->count > 0)
@@ -205,9 +205,9 @@ ssize_t eventfd_ctx_read(struct eventfd_ctx *ctx, int no_wait, __u64 *cnt)
 				res = -ERESTARTSYS;
 				break;
 			}
-			spin_unlock_irq(&ctx->wqh.lock);
+			raw_spin_unlock_irq(&ctx->wqh.lock);
 			schedule();
-			spin_lock_irq(&ctx->wqh.lock);
+			raw_spin_lock_irq(&ctx->wqh.lock);
 		}
 		__remove_wait_queue(&ctx->wqh, &wait);
 		__set_current_state(TASK_RUNNING);
@@ -217,7 +217,7 @@ ssize_t eventfd_ctx_read(struct eventfd_ctx *ctx, int no_wait, __u64 *cnt)
 		if (waitqueue_active(&ctx->wqh))
 			wake_up_locked_poll(&ctx->wqh, POLLOUT);
 	}
-	spin_unlock_irq(&ctx->wqh.lock);
+	raw_spin_unlock_irq(&ctx->wqh.lock);
 
 	return res;
 }
@@ -253,7 +253,7 @@ static ssize_t eventfd_write(struct file *file, const char __user *buf, size_t c
 		return -EFAULT;
 	if (ucnt == ULLONG_MAX)
 		return -EINVAL;
-	spin_lock_irq(&ctx->wqh.lock);
+	raw_spin_lock_irq(&ctx->wqh.lock);
 	res = -EAGAIN;
 	if (ULLONG_MAX - ctx->count > ucnt)
 		res = sizeof(ucnt);
@@ -269,9 +269,9 @@ static ssize_t eventfd_write(struct file *file, const char __user *buf, size_t c
 				res = -ERESTARTSYS;
 				break;
 			}
-			spin_unlock_irq(&ctx->wqh.lock);
+			raw_spin_unlock_irq(&ctx->wqh.lock);
 			schedule();
-			spin_lock_irq(&ctx->wqh.lock);
+			raw_spin_lock_irq(&ctx->wqh.lock);
 		}
 		__remove_wait_queue(&ctx->wqh, &wait);
 		__set_current_state(TASK_RUNNING);
@@ -281,7 +281,7 @@ static ssize_t eventfd_write(struct file *file, const char __user *buf, size_t c
 		if (waitqueue_active(&ctx->wqh))
 			wake_up_locked_poll(&ctx->wqh, POLLIN);
 	}
-	spin_unlock_irq(&ctx->wqh.lock);
+	raw_spin_unlock_irq(&ctx->wqh.lock);
 
 	return res;
 }
@@ -292,10 +292,10 @@ static int eventfd_show_fdinfo(struct seq_file *m, struct file *f)
 	struct eventfd_ctx *ctx = f->private_data;
 	int ret;
 
-	spin_lock_irq(&ctx->wqh.lock);
+	raw_spin_lock_irq(&ctx->wqh.lock);
 	ret = seq_printf(m, "eventfd-count: %16llx\n",
 			 (unsigned long long)ctx->count);
-	spin_unlock_irq(&ctx->wqh.lock);
+	raw_spin_unlock_irq(&ctx->wqh.lock);
 
 	return ret;
 }
diff --git a/fs/eventpoll.c b/fs/eventpoll.c
index 99c53c7..b2f6122 100644
--- a/fs/eventpoll.c
+++ b/fs/eventpoll.c
@@ -174,7 +174,7 @@ struct epitem {
  */
 struct eventpoll {
 	/* Protect the access to this structure */
-	spinlock_t lock;
+	raw_spinlock_t lock;
 
 	/*
 	 * This mutex is used to ensure that files are not removed
@@ -608,17 +608,17 @@ static int ep_scan_ready_list(struct eventpoll *ep,
 	 * because we want the "sproc" callback to be able to do it
 	 * in a lockless way.
 	 */
-	spin_lock_irqsave(&ep->lock, flags);
+	raw_spin_lock_irqsave(&ep->lock, flags);
 	list_splice_init(&ep->rdllist, &txlist);
 	ep->ovflist = NULL;
-	spin_unlock_irqrestore(&ep->lock, flags);
+	raw_spin_unlock_irqrestore(&ep->lock, flags);
 
 	/*
 	 * Now call the callback function.
 	 */
 	error = (*sproc)(ep, &txlist, priv);
 
-	spin_lock_irqsave(&ep->lock, flags);
+	raw_spin_lock_irqsave(&ep->lock, flags);
 	/*
 	 * During the time we spent inside the "sproc" callback, some
 	 * other events might have been queued by the poll callback.
@@ -660,7 +660,7 @@ static int ep_scan_ready_list(struct eventpoll *ep,
 		if (waitqueue_active(&ep->poll_wait))
 			pwake++;
 	}
-	spin_unlock_irqrestore(&ep->lock, flags);
+	raw_spin_unlock_irqrestore(&ep->lock, flags);
 
 	mutex_unlock(&ep->mtx);
 
@@ -698,10 +698,10 @@ static int ep_remove(struct eventpoll *ep, struct epitem *epi)
 
 	rb_erase(&epi->rbn, &ep->rbr);
 
-	spin_lock_irqsave(&ep->lock, flags);
+	raw_spin_lock_irqsave(&ep->lock, flags);
 	if (ep_is_linked(&epi->rdllink))
 		list_del_init(&epi->rdllink);
-	spin_unlock_irqrestore(&ep->lock, flags);
+	raw_spin_unlock_irqrestore(&ep->lock, flags);
 
 	wakeup_source_unregister(ep_wakeup_source(epi));
 
@@ -914,7 +914,7 @@ static int ep_alloc(struct eventpoll **pep)
 	if (unlikely(!ep))
 		goto free_uid;
 
-	spin_lock_init(&ep->lock);
+	raw_spin_lock_init(&ep->lock);
 	mutex_init(&ep->mtx);
 	init_waitqueue_head(&ep->wq);
 	init_waitqueue_head(&ep->poll_wait);
@@ -984,7 +984,7 @@ static int ep_poll_callback(wait_queue_t *wait, unsigned mode, int sync, void *k
 		list_del_init(&wait->task_list);
 	}
 
-	spin_lock_irqsave(&ep->lock, flags);
+	raw_spin_lock_irqsave(&ep->lock, flags);
 
 	/*
 	 * If the event mask does not contain any poll(2) event, we consider the
@@ -1042,7 +1042,7 @@ static int ep_poll_callback(wait_queue_t *wait, unsigned mode, int sync, void *k
 		pwake++;
 
 out_unlock:
-	spin_unlock_irqrestore(&ep->lock, flags);
+	raw_spin_unlock_irqrestore(&ep->lock, flags);
 
 	/* We have to call this outside the lock */
 	if (pwake)
@@ -1299,7 +1299,7 @@ static int ep_insert(struct eventpoll *ep, struct epoll_event *event,
 		goto error_remove_epi;
 
 	/* We have to drop the new item inside our item list to keep track of it */
-	spin_lock_irqsave(&ep->lock, flags);
+	raw_spin_lock_irqsave(&ep->lock, flags);
 
 	/* If the file is already "ready" we drop it inside the ready list */
 	if ((revents & event->events) && !ep_is_linked(&epi->rdllink)) {
@@ -1313,7 +1313,7 @@ static int ep_insert(struct eventpoll *ep, struct epoll_event *event,
 			pwake++;
 	}
 
-	spin_unlock_irqrestore(&ep->lock, flags);
+	raw_spin_unlock_irqrestore(&ep->lock, flags);
 
 	atomic_long_inc(&ep->user->epoll_watches);
 
@@ -1340,10 +1340,10 @@ error_unregister:
 	 * list, since that is used/cleaned only inside a section bound by "mtx".
 	 * And ep_insert() is called with "mtx" held.
 	 */
-	spin_lock_irqsave(&ep->lock, flags);
+	raw_spin_lock_irqsave(&ep->lock, flags);
 	if (ep_is_linked(&epi->rdllink))
 		list_del_init(&epi->rdllink);
-	spin_unlock_irqrestore(&ep->lock, flags);
+	raw_spin_unlock_irqrestore(&ep->lock, flags);
 
 	wakeup_source_unregister(ep_wakeup_source(epi));
 
@@ -1410,7 +1410,7 @@ static int ep_modify(struct eventpoll *ep, struct epitem *epi, struct epoll_even
 	 * list, push it inside.
 	 */
 	if (revents & event->events) {
-		spin_lock_irq(&ep->lock);
+		raw_spin_lock_irq(&ep->lock);
 		if (!ep_is_linked(&epi->rdllink)) {
 			list_add_tail(&epi->rdllink, &ep->rdllist);
 			ep_pm_stay_awake(epi);
@@ -1421,7 +1421,7 @@ static int ep_modify(struct eventpoll *ep, struct epitem *epi, struct epoll_even
 			if (waitqueue_active(&ep->poll_wait))
 				pwake++;
 		}
-		spin_unlock_irq(&ep->lock);
+		raw_spin_unlock_irq(&ep->lock);
 	}
 
 	/* We have to call this outside the lock */
@@ -1571,12 +1571,12 @@ static int ep_poll(struct eventpoll *ep, struct epoll_event __user *events,
 		 * caller specified a non blocking operation.
 		 */
 		timed_out = 1;
-		spin_lock_irqsave(&ep->lock, flags);
+		raw_spin_lock_irqsave(&ep->lock, flags);
 		goto check_events;
 	}
 
 fetch_events:
-	spin_lock_irqsave(&ep->lock, flags);
+	raw_spin_lock_irqsave(&ep->lock, flags);
 
 	if (!ep_events_available(ep)) {
 		/*
@@ -1601,11 +1601,11 @@ fetch_events:
 				break;
 			}
 
-			spin_unlock_irqrestore(&ep->lock, flags);
+			raw_spin_unlock_irqrestore(&ep->lock, flags);
 			if (!schedule_hrtimeout_range(to, slack, HRTIMER_MODE_ABS))
 				timed_out = 1;
 
-			spin_lock_irqsave(&ep->lock, flags);
+			raw_spin_lock_irqsave(&ep->lock, flags);
 		}
 		__remove_wait_queue(&ep->wq, &wait);
 
@@ -1615,7 +1615,7 @@ check_events:
 	/* Is it worth to try to dig for events ? */
 	eavail = ep_events_available(ep);
 
-	spin_unlock_irqrestore(&ep->lock, flags);
+	raw_spin_unlock_irqrestore(&ep->lock, flags);
 
 	/*
 	 * Try to transfer events to user space. In case we get 0 events and
diff --git a/fs/timerfd.c b/fs/timerfd.c
index 1844c97..f68ebf3 100644
--- a/fs/timerfd.c
+++ b/fs/timerfd.c
@@ -51,11 +51,11 @@ static enum hrtimer_restart timerfd_tmrproc(struct hrtimer *htmr)
 	struct timerfd_ctx *ctx = container_of(htmr, struct timerfd_ctx, tmr);
 	unsigned long flags;
 
-	spin_lock_irqsave(&ctx->wqh.lock, flags);
+	raw_spin_lock_irqsave(&ctx->wqh.lock, flags);
 	ctx->expired = 1;
 	ctx->ticks++;
 	wake_up_locked(&ctx->wqh);
-	spin_unlock_irqrestore(&ctx->wqh.lock, flags);
+	raw_spin_unlock_irqrestore(&ctx->wqh.lock, flags);
 
 	return HRTIMER_NORESTART;
 }
@@ -76,13 +76,13 @@ void timerfd_clock_was_set(void)
 	list_for_each_entry_rcu(ctx, &cancel_list, clist) {
 		if (!ctx->might_cancel)
 			continue;
-		spin_lock_irqsave(&ctx->wqh.lock, flags);
+		raw_spin_lock_irqsave(&ctx->wqh.lock, flags);
 		if (ctx->moffs.tv64 != moffs.tv64) {
 			ctx->moffs.tv64 = KTIME_MAX;
 			ctx->ticks++;
 			wake_up_locked(&ctx->wqh);
 		}
-		spin_unlock_irqrestore(&ctx->wqh.lock, flags);
+		raw_spin_unlock_irqrestore(&ctx->wqh.lock, flags);
 	}
 	rcu_read_unlock();
 }
@@ -171,10 +171,10 @@ static unsigned int timerfd_poll(struct file *file, poll_table *wait)
 
 	poll_wait(file, &ctx->wqh, wait);
 
-	spin_lock_irqsave(&ctx->wqh.lock, flags);
+	raw_spin_lock_irqsave(&ctx->wqh.lock, flags);
 	if (ctx->ticks)
 		events |= POLLIN;
-	spin_unlock_irqrestore(&ctx->wqh.lock, flags);
+	raw_spin_unlock_irqrestore(&ctx->wqh.lock, flags);
 
 	return events;
 }
@@ -188,7 +188,7 @@ static ssize_t timerfd_read(struct file *file, char __user *buf, size_t count,
 
 	if (count < sizeof(ticks))
 		return -EINVAL;
-	spin_lock_irq(&ctx->wqh.lock);
+	raw_spin_lock_irq(&ctx->wqh.lock);
 	if (file->f_flags & O_NONBLOCK)
 		res = -EAGAIN;
 	else
@@ -222,7 +222,7 @@ static ssize_t timerfd_read(struct file *file, char __user *buf, size_t count,
 		ctx->expired = 0;
 		ctx->ticks = 0;
 	}
-	spin_unlock_irq(&ctx->wqh.lock);
+	raw_spin_unlock_irq(&ctx->wqh.lock);
 	if (ticks)
 		res = put_user(ticks, (u64 __user *) buf) ? -EFAULT: sizeof(ticks);
 	return res;
@@ -304,10 +304,10 @@ static int do_timerfd_settime(int ufd, int flags,
 	 * it to the new values.
 	 */
 	for (;;) {
-		spin_lock_irq(&ctx->wqh.lock);
+		raw_spin_lock_irq(&ctx->wqh.lock);
 		if (hrtimer_try_to_cancel(&ctx->tmr) >= 0)
 			break;
-		spin_unlock_irq(&ctx->wqh.lock);
+		raw_spin_unlock_irq(&ctx->wqh.lock);
 		hrtimer_wait_for_timer(&ctx->tmr);
 	}
 
@@ -328,7 +328,7 @@ static int do_timerfd_settime(int ufd, int flags,
 	 */
 	ret = timerfd_setup(ctx, flags, new);
 
-	spin_unlock_irq(&ctx->wqh.lock);
+	raw_spin_unlock_irq(&ctx->wqh.lock);
 	fdput(f);
 	return ret;
 }
@@ -342,7 +342,7 @@ static int do_timerfd_gettime(int ufd, struct itimerspec *t)
 		return ret;
 	ctx = f.file->private_data;
 
-	spin_lock_irq(&ctx->wqh.lock);
+	raw_spin_lock_irq(&ctx->wqh.lock);
 	if (ctx->expired && ctx->tintv.tv64) {
 		ctx->expired = 0;
 		ctx->ticks +=
@@ -351,7 +351,7 @@ static int do_timerfd_gettime(int ufd, struct itimerspec *t)
 	}
 	t->it_value = ktime_to_timespec(timerfd_get_remaining(ctx));
 	t->it_interval = ktime_to_timespec(ctx->tintv);
-	spin_unlock_irq(&ctx->wqh.lock);
+	raw_spin_unlock_irq(&ctx->wqh.lock);
 	fdput(f);
 	return 0;
 }
diff --git a/include/linux/wait.h b/include/linux/wait.h
index c8e5760..a6183fd 100644
--- a/include/linux/wait.h
+++ b/include/linux/wait.h
@@ -31,7 +31,7 @@ struct wait_bit_queue {
 };
 
 struct __wait_queue_head {
-	spinlock_t lock;
+	raw_spinlock_t lock;
 	struct list_head task_list;
 };
 typedef struct __wait_queue_head wait_queue_head_t;
@@ -51,7 +51,7 @@ struct task_struct;
 	wait_queue_t name = __WAITQUEUE_INITIALIZER(name, tsk)
 
 #define __WAIT_QUEUE_HEAD_INITIALIZER(name) {				\
-	.lock		= __SPIN_LOCK_UNLOCKED(name.lock),		\
+	.lock		= __RAW_SPIN_LOCK_UNLOCKED(name.lock),		\
 	.task_list	= { &(name).task_list, &(name).task_list } }
 
 #define DECLARE_WAIT_QUEUE_HEAD(name) \
@@ -468,14 +468,14 @@ do {									\
 			break;						\
 		}							\
 		if (irq)						\
-			spin_unlock_irq(&(wq).lock);			\
+			raw_spin_unlock_irq(&(wq).lock);		\
 		else							\
-			spin_unlock(&(wq).lock);			\
+			raw_spin_unlock(&(wq).lock);			\
 		schedule();						\
 		if (irq)						\
-			spin_lock_irq(&(wq).lock);			\
+			raw_spin_lock_irq(&(wq).lock);			\
 		else							\
-			spin_lock(&(wq).lock);				\
+			raw_spin_lock(&(wq).lock);			\
 	} while (!(condition));						\
 	__remove_wait_queue(&(wq), &__wait);				\
 	__set_current_state(TASK_RUNNING);				\
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 6c3deca..ed7b17e 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -3402,9 +3402,9 @@ void __wake_up(wait_queue_head_t *q, unsigned int mode,
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&q->lock, flags);
+	raw_spin_lock_irqsave(&q->lock, flags);
 	__wake_up_common(q, mode, nr_exclusive, 0, key);
-	spin_unlock_irqrestore(&q->lock, flags);
+	raw_spin_unlock_irqrestore(&q->lock, flags);
 }
 EXPORT_SYMBOL(__wake_up);
 
@@ -3452,9 +3452,9 @@ void __wake_up_sync_key(wait_queue_head_t *q, unsigned int mode,
 	if (unlikely(!nr_exclusive))
 		wake_flags = 0;
 
-	spin_lock_irqsave(&q->lock, flags);
+	raw_spin_lock_irqsave(&q->lock, flags);
 	__wake_up_common(q, mode, nr_exclusive, wake_flags, key);
-	spin_unlock_irqrestore(&q->lock, flags);
+	raw_spin_unlock_irqrestore(&q->lock, flags);
 }
 EXPORT_SYMBOL_GPL(__wake_up_sync_key);
 
@@ -3761,13 +3761,13 @@ sleep_on_common(wait_queue_head_t *q, int state, long timeout)
 
 	__set_current_state(state);
 
-	spin_lock_irqsave(&q->lock, flags);
+	raw_spin_lock_irqsave(&q->lock, flags);
 	__add_wait_queue(q, &wait);
-	spin_unlock(&q->lock);
+	raw_spin_unlock(&q->lock);
 	timeout = schedule_timeout(timeout);
-	spin_lock_irq(&q->lock);
+	raw_spin_lock_irq(&q->lock);
 	__remove_wait_queue(q, &wait);
-	spin_unlock_irqrestore(&q->lock, flags);
+	raw_spin_unlock_irqrestore(&q->lock, flags);
 
 	return timeout;
 }
diff --git a/kernel/wait.c b/kernel/wait.c
index 6698e0c..e9ac11e 100644
--- a/kernel/wait.c
+++ b/kernel/wait.c
@@ -12,7 +12,7 @@
 
 void __init_waitqueue_head(wait_queue_head_t *q, const char *name, struct lock_class_key *key)
 {
-	spin_lock_init(&q->lock);
+	raw_spin_lock_init(&q->lock);
 	lockdep_set_class_and_name(&q->lock, key, name);
 	INIT_LIST_HEAD(&q->task_list);
 }
@@ -24,9 +24,9 @@ void add_wait_queue(wait_queue_head_t *q, wait_queue_t *wait)
 	unsigned long flags;
 
 	wait->flags &= ~WQ_FLAG_EXCLUSIVE;
-	spin_lock_irqsave(&q->lock, flags);
+	raw_spin_lock_irqsave(&q->lock, flags);
 	__add_wait_queue(q, wait);
-	spin_unlock_irqrestore(&q->lock, flags);
+	raw_spin_unlock_irqrestore(&q->lock, flags);
 }
 EXPORT_SYMBOL(add_wait_queue);
 
@@ -35,9 +35,9 @@ void add_wait_queue_exclusive(wait_queue_head_t *q, wait_queue_t *wait)
 	unsigned long flags;
 
 	wait->flags |= WQ_FLAG_EXCLUSIVE;
-	spin_lock_irqsave(&q->lock, flags);
+	raw_spin_lock_irqsave(&q->lock, flags);
 	__add_wait_queue_tail(q, wait);
-	spin_unlock_irqrestore(&q->lock, flags);
+	raw_spin_unlock_irqrestore(&q->lock, flags);
 }
 EXPORT_SYMBOL(add_wait_queue_exclusive);
 
@@ -45,9 +45,9 @@ void remove_wait_queue(wait_queue_head_t *q, wait_queue_t *wait)
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&q->lock, flags);
+	raw_spin_lock_irqsave(&q->lock, flags);
 	__remove_wait_queue(q, wait);
-	spin_unlock_irqrestore(&q->lock, flags);
+	raw_spin_unlock_irqrestore(&q->lock, flags);
 }
 EXPORT_SYMBOL(remove_wait_queue);
 
@@ -70,11 +70,11 @@ prepare_to_wait(wait_queue_head_t *q, wait_queue_t *wait, int state)
 	unsigned long flags;
 
 	wait->flags &= ~WQ_FLAG_EXCLUSIVE;
-	spin_lock_irqsave(&q->lock, flags);
+	raw_spin_lock_irqsave(&q->lock, flags);
 	if (list_empty(&wait->task_list))
 		__add_wait_queue(q, wait);
 	set_current_state(state);
-	spin_unlock_irqrestore(&q->lock, flags);
+	raw_spin_unlock_irqrestore(&q->lock, flags);
 }
 EXPORT_SYMBOL(prepare_to_wait);
 
@@ -84,11 +84,11 @@ prepare_to_wait_exclusive(wait_queue_head_t *q, wait_queue_t *wait, int state)
 	unsigned long flags;
 
 	wait->flags |= WQ_FLAG_EXCLUSIVE;
-	spin_lock_irqsave(&q->lock, flags);
+	raw_spin_lock_irqsave(&q->lock, flags);
 	if (list_empty(&wait->task_list))
 		__add_wait_queue_tail(q, wait);
 	set_current_state(state);
-	spin_unlock_irqrestore(&q->lock, flags);
+	raw_spin_unlock_irqrestore(&q->lock, flags);
 }
 EXPORT_SYMBOL(prepare_to_wait_exclusive);
 
@@ -120,9 +120,9 @@ void finish_wait(wait_queue_head_t *q, wait_queue_t *wait)
 	 *    the list).
 	 */
 	if (!list_empty_careful(&wait->task_list)) {
-		spin_lock_irqsave(&q->lock, flags);
+		raw_spin_lock_irqsave(&q->lock, flags);
 		list_del_init(&wait->task_list);
-		spin_unlock_irqrestore(&q->lock, flags);
+		raw_spin_unlock_irqrestore(&q->lock, flags);
 	}
 }
 EXPORT_SYMBOL(finish_wait);
@@ -151,12 +151,12 @@ void abort_exclusive_wait(wait_queue_head_t *q, wait_queue_t *wait,
 	unsigned long flags;
 
 	__set_current_state(TASK_RUNNING);
-	spin_lock_irqsave(&q->lock, flags);
+	raw_spin_lock_irqsave(&q->lock, flags);
 	if (!list_empty(&wait->task_list))
 		list_del_init(&wait->task_list);
 	else if (waitqueue_active(q))
 		__wake_up_locked_key(q, mode, key);
-	spin_unlock_irqrestore(&q->lock, flags);
+	raw_spin_unlock_irqrestore(&q->lock, flags);
 }
 EXPORT_SYMBOL(abort_exclusive_wait);
 
diff --git a/mm/filemap.c b/mm/filemap.c
index bf2060d..49a66dd 100644
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@ -585,9 +585,9 @@ void add_page_wait_queue(struct page *page, wait_queue_t *waiter)
 	wait_queue_head_t *q = page_waitqueue(page);
 	unsigned long flags;
 
-	spin_lock_irqsave(&q->lock, flags);
+	raw_spin_lock_irqsave(&q->lock, flags);
 	__add_wait_queue(q, waiter);
-	spin_unlock_irqrestore(&q->lock, flags);
+	raw_spin_unlock_irqrestore(&q->lock, flags);
 }
 EXPORT_SYMBOL_GPL(add_page_wait_queue);
 
diff --git a/net/sunrpc/sched.c b/net/sunrpc/sched.c
index 5356b12..c9f7d11 100644
--- a/net/sunrpc/sched.c
+++ b/net/sunrpc/sched.c
@@ -293,12 +293,12 @@ static int rpc_complete_task(struct rpc_task *task)
 
 	trace_rpc_task_complete(task->tk_client, task, NULL);
 
-	spin_lock_irqsave(&wq->lock, flags);
+	raw_spin_lock_irqsave(&wq->lock, flags);
 	clear_bit(RPC_TASK_ACTIVE, &task->tk_runstate);
 	ret = atomic_dec_and_test(&task->tk_count);
 	if (waitqueue_active(wq))
 		__wake_up_locked_key(wq, TASK_NORMAL, &k);
-	spin_unlock_irqrestore(&wq->lock, flags);
+	raw_spin_unlock_irqrestore(&wq->lock, flags);
 	return ret;
 }
 
-- 
1.7.9.5

