From f19d46df4ca1abed924a7681b1334027de2ad5ea Mon Sep 17 00:00:00 2001
From: Andrzej Datczuk <andrzej.datczuk@intel.com>
Date: Tue, 29 Sep 2015 20:08:31 +0200
Subject: [PATCH 4466/4706] drm/i915: Perfmon support for gen9

commit ba8d139b0b4a4375971ad494d39b54826711ae9c from
git://git.yoctoproject.org/linux-yocto-4.1

This patch implements Perfmon IOCTLs for gen9 (BXT).
Changes are required for supporting MDAPI used by such
tools as GPA.

Based on Android implementation.

Perfmon open/close IOCTL
Perfmon must be opened in order to set global perfmon
configuration and to get list of HW context IDs for arbitrary
context.

Reporting IOCTL of HW context
Enables user to get hardware context id for given context
(identified by user handle) as well as to query for list
of hardware context ids for arbitrary process (identified
by PID). This is needed for matching Perfmon sample from HW
with application/process/context. Perfmon must be opened
for given fd to enable caller to get list of HW context
IDs for arbitrary context.

Writing OA / Perfmon configuration to ring buffer
Some registers for OA configuration are part of context and
thus need to be written via LRIs inserted to the ring buffer.
It is convenient to send all OA configuration registers this way
since it enables us to implement multiconfiguration, meaning
multiple users each using different OA config. To have this
working programming of per-context workaround batch buffers
is required.

Storing process ID information in i915_gem_context.

Permitting user to pin/unpin a buffer using
I915_PERFMON_PIN_OA_BUFFER / I915_PERFMON_UNPIN_OA_BUFFER.

Signed-off-by: Andrzej Datczuk <andrzej.datczuk@intel.com>
[mattrope: Major rebasing]
Signed-off-by: Matt Roper <matthew.d.roper@intel.com>
[Kevin: Just some minor context mods in order to port to wrlinux]
Signed-off-by: Kevin Hao <kexin.hao@windriver.com>
---
 drivers/gpu/drm/i915/Makefile              |    3 +
 drivers/gpu/drm/i915/i915_drv.c            |   10 +
 drivers/gpu/drm/i915/i915_drv.h            |   27 +
 drivers/gpu/drm/i915/i915_gem_context.c    |    3 +
 drivers/gpu/drm/i915/i915_gem_execbuffer.c |  153 +++++
 drivers/gpu/drm/i915/i915_perfmon.c        |  908 ++++++++++++++++++++++++++++
 drivers/gpu/drm/i915/i915_perfmon_defs.h   |   62 ++
 drivers/gpu/drm/i915/i915_reg.h            |    5 +
 drivers/gpu/drm/i915/intel_lrc.c           |    1 +
 include/uapi/drm/i915_drm.h                |    4 +
 include/uapi/drm/i915_perfmon.h            |  124 ++++
 11 files changed, 1300 insertions(+), 0 deletions(-)
 create mode 100644 drivers/gpu/drm/i915/i915_perfmon.c
 create mode 100644 drivers/gpu/drm/i915/i915_perfmon_defs.h
 create mode 100644 include/uapi/drm/i915_perfmon.h

diff --git a/drivers/gpu/drm/i915/Makefile b/drivers/gpu/drm/i915/Makefile
index 56ce866..693b1cc 100644
--- a/drivers/gpu/drm/i915/Makefile
+++ b/drivers/gpu/drm/i915/Makefile
@@ -123,6 +123,9 @@ i915-y += i915_gem_userdata.o
 # initial modeset
 i915-y += intel_initial_modeset.o
 
+# perfmon
+i915-y += i915_perfmon.o
+
 obj-$(CONFIG_DRM_I915)  += i915.o
 
 CFLAGS_i915_trace_points.o := -I$(src)
diff --git a/drivers/gpu/drm/i915/i915_drv.c b/drivers/gpu/drm/i915/i915_drv.c
index d20310d..ffad578 100644
--- a/drivers/gpu/drm/i915/i915_drv.c
+++ b/drivers/gpu/drm/i915/i915_drv.c
@@ -819,6 +819,8 @@ static int i915_driver_init_early(struct drm_i915_private *dev_priv,
 	mutex_init(&dev_priv->av_mutex);
 	mutex_init(&dev_priv->wm.wm_mutex);
 	mutex_init(&dev_priv->pps_mutex);
+	mutex_init(&dev_priv->perfmon.config.lock);
+	mutex_init(&dev_priv->rc6_wa_bb.lock);
 
 	ret = i915_workqueues_init(dev_priv);
 	if (ret < 0)
@@ -1266,6 +1268,9 @@ int i915_driver_load(struct pci_dev *pdev, const struct pci_device_id *ent)
 
 	dev_priv->profile.driver_load = sched_clock() - start_tm;
 
+	/* Initialize all the resources for perf monitoring */
+	i915_perfmon_setup(dev_priv);
+
 	printk(KERN_INFO "IOTG i915 forklift 2016-12-15\n");
 
 	return 0;
@@ -1283,6 +1288,10 @@ out_pci_disable:
 	pci_disable_device(pdev);
 out_free_priv:
 	i915_load_error(dev_priv, "Device initialization failed (%d)\n", ret);
+
+	/* Clean up all the resources for perf monitoring */
+	i915_perfmon_cleanup(dev_priv);
+
 	drm_dev_unref(&dev_priv->drm);
 	return ret;
 }
@@ -2596,6 +2605,7 @@ static const struct drm_ioctl_desc i915_ioctls[] = {
  */
 	DRM_IOCTL_DEF_DRV(I915_EXT_IOCTL, i915_extended_ioctl,
 			  DRM_UNLOCKED|DRM_CONTROL_ALLOW|DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(I915_PERFMON, i915_perfmon_ioctl, DRM_UNLOCKED),
 };
 
 static struct drm_driver driver = {
diff --git a/drivers/gpu/drm/i915/i915_drv.h b/drivers/gpu/drm/i915/i915_drv.h
index 09b22d6..9dc72ea 100644
--- a/drivers/gpu/drm/i915/i915_drv.h
+++ b/drivers/gpu/drm/i915/i915_drv.h
@@ -68,6 +68,8 @@
 
 #include "i915_gem_userdata.h"
 
+#include "i915_perfmon_defs.h"
+
 /* General customization:
  */
 
@@ -406,6 +408,8 @@ struct drm_i915_file_private {
 	} rps;
 
 	unsigned int bsd_engine;
+
+	struct drm_i915_perfmon_file perfmon;
 };
 
 /* Used by dp and fdi links */
@@ -947,6 +951,9 @@ struct i915_gem_context {
 
 	u8 remap_slice;
 	bool closed:1;
+
+	/* perfmon configuration */
+	struct drm_i915_perfmon_context perfmon;
 };
 
 enum fb_op_origin {
@@ -1982,6 +1989,17 @@ struct drm_i915_private {
 
 	int dpio_phy_iosf_port[I915_NUM_PHYS_VLV];
 
+	struct drm_i915_perfmon_device perfmon;
+
+	struct {
+		struct drm_i915_gem_object *obj;
+		struct i915_vma *vma;
+		unsigned long offset;
+		void *address;
+		atomic_t enable;
+		struct mutex lock;
+	} rc6_wa_bb;
+
 	struct i915_workarounds workarounds;
 
 	struct i915_frontbuffer_tracking fb_tracking;
@@ -3763,6 +3781,15 @@ extern void intel_set_memory_cxsr(struct drm_i915_private *dev_priv,
 int i915_reg_read_ioctl(struct drm_device *dev, void *data,
 			struct drm_file *file);
 
+/* i915_perfmon.c */
+int i915_perfmon_ioctl(struct drm_device *dev, void *data, struct drm_file *file);
+void i915_perfmon_setup(struct drm_i915_private *dev_priv);
+void i915_perfmon_cleanup(struct drm_i915_private *dev_priv);
+void i915_perfmon_ctx_setup(struct i915_gem_context *ctx);
+void i915_perfmon_ctx_cleanup(struct i915_gem_context *ctx);
+int i915_perfmon_update_workaround_bb(struct drm_i915_private *dev_priv,
+				      struct drm_i915_perfmon_config *config);
+
 /* overlay */
 extern struct intel_overlay_error_state *
 intel_overlay_capture_error_state(struct drm_i915_private *dev_priv);
diff --git a/drivers/gpu/drm/i915/i915_gem_context.c b/drivers/gpu/drm/i915/i915_gem_context.c
index df10f4e9..577f742 100644
--- a/drivers/gpu/drm/i915/i915_gem_context.c
+++ b/drivers/gpu/drm/i915/i915_gem_context.c
@@ -159,6 +159,7 @@ void i915_gem_context_free(struct kref *ctx_ref)
 	}
 
 	put_pid(ctx->pid);
+	i915_perfmon_ctx_cleanup(ctx);
 	list_del(&ctx->link);
 
 	ida_simple_remove(&ctx->i915->context_hw_ida, ctx->hw_id);
@@ -368,6 +369,8 @@ i915_gem_create_context(struct drm_device *dev,
 
 	trace_i915_context_create(ctx);
 
+	i915_perfmon_ctx_setup(ctx);
+
 	return ctx;
 }
 
diff --git a/drivers/gpu/drm/i915/i915_gem_execbuffer.c b/drivers/gpu/drm/i915/i915_gem_execbuffer.c
index c394447..1ad4eb4 100644
--- a/drivers/gpu/drm/i915/i915_gem_execbuffer.c
+++ b/drivers/gpu/drm/i915/i915_gem_execbuffer.c
@@ -1402,6 +1402,155 @@ i915_reset_gen7_sol_offsets(struct drm_i915_gem_request *req)
 	return 0;
 }
 
+static void perfmon_send_config(
+		struct intel_ring *ring,
+		struct drm_i915_perfmon_config *config)
+{
+	int i;
+
+	for (i = 0; i < config->size; i++) {
+		DRM_DEBUG("perfmon config %x reg:%05x val:%08x\n",
+			config->id,
+			config->entries[i].offset,
+			config->entries[i].value);
+		intel_ring_emit(ring, MI_NOOP);
+		intel_ring_emit(ring, MI_LOAD_REGISTER_IMM(1));
+		intel_ring_emit(ring, config->entries[i].offset);
+		intel_ring_emit(ring, config->entries[i].value);
+	}
+
+}
+
+static inline struct drm_i915_perfmon_config *get_perfmon_config(
+		struct drm_i915_private *dev_priv,
+		struct i915_gem_context *ctx,
+		struct drm_i915_perfmon_config *config_global,
+		struct drm_i915_perfmon_config *config_context,
+		__u32 ctx_submitted_config_id)
+
+{
+	struct drm_i915_perfmon_config *config  = NULL;
+	enum DRM_I915_PERFMON_CONFIG_TARGET target;
+
+	BUG_ON(!mutex_is_locked(&dev_priv->perfmon.config.lock));
+
+	target = dev_priv->perfmon.config.target;
+	switch (target) {
+	case I915_PERFMON_CONFIG_TARGET_CTX:
+		config = config_context;
+		break;
+	case I915_PERFMON_CONFIG_TARGET_PID:
+		if (pid_vnr(ctx->pid) == dev_priv->perfmon.config.pid)
+			config = config_global;
+		break;
+	case I915_PERFMON_CONFIG_TARGET_ALL:
+		config = config_global;
+		break;
+	default:
+		BUG_ON(1);
+		break;
+	}
+
+	if (config != NULL) {
+		if (config->size == 0 || config->id == 0) {
+			/* configuration is empty or targets other context */
+			DRM_DEBUG("perfmon configuration empty\n");
+			config = NULL;
+		} else if (config->id == ctx_submitted_config_id) {
+			/* configuration is already submitted in this context*/
+			DRM_DEBUG("perfmon configuration %x is submitted\n",
+				config->id);
+			config = NULL;
+		}
+	}
+
+	if (config != NULL)
+		DRM_DEBUG("perfmon configuration TARGET:%u SIZE:%x ID:%x",
+			target,
+			config->size,
+			config->id);
+
+	return config;
+}
+
+static inline int
+i915_program_perfmon(struct drm_i915_private *dev_priv,
+		     struct intel_ring *ring,
+		     struct drm_i915_gem_request* req,
+		     struct i915_gem_context *ctx)
+{
+	struct drm_i915_perfmon_config *config_oa, *config_gp;
+
+	size_t size;
+	int ret = 0;
+
+	if (!atomic_read(&dev_priv->perfmon.config.enable) &&
+		ctx->perfmon.config.oa.submitted_id == 0)
+		return 0;
+
+	ret = mutex_lock_interruptible(&dev_priv->perfmon.config.lock);
+
+	if (ret)
+		return ret;
+
+	if (!atomic_read(&dev_priv->perfmon.config.enable)) {
+		if (ctx->perfmon.config.oa.submitted_id != 0) {
+			/* write 0 to OA_CTX_CONTROL to stop counters */
+			ret = intel_ring_begin(req, 4);
+			if (!ret) {
+				intel_ring_emit(ring, MI_NOOP);
+				intel_ring_emit(ring, MI_LOAD_REGISTER_IMM(1));
+				intel_ring_emit(ring, GEN8_OA_CTX_CONTROL);
+				intel_ring_emit(ring, 0);
+				intel_ring_advance(ring);
+			}
+			ctx->perfmon.config.oa.submitted_id = 0;
+		}
+		goto unlock;
+	}
+
+	/* check for pending OA config */
+	config_oa = get_perfmon_config(dev_priv, ctx,
+				       &dev_priv->perfmon.config.oa,
+				       &ctx->perfmon.config.oa.pending,
+				       ctx->perfmon.config.oa.submitted_id);
+
+	/* check for pending PERFMON config */
+	config_gp = get_perfmon_config(dev_priv, ctx,
+				       &dev_priv->perfmon.config.gp,
+				       &ctx->perfmon.config.gp.pending,
+				       ctx->perfmon.config.gp.submitted_id);
+
+	size = (config_oa ? config_oa->size : 0) +
+		(config_gp ? config_gp->size : 0);
+
+	if (size == 0)
+		goto unlock;
+
+	ret = intel_ring_begin(req, 4 * size);
+	if (ret)
+		goto unlock;
+
+	/* submit pending OA config */
+	if (config_oa) {
+		perfmon_send_config(ring, config_oa);
+		ctx->perfmon.config.oa.submitted_id = config_oa->id;
+
+		i915_perfmon_update_workaround_bb(dev_priv, config_oa);
+	}
+
+	/* submit pending general purpose perfmon counters config */
+	if (config_gp) {
+		perfmon_send_config(ring, config_gp);
+		ctx->perfmon.config.gp.submitted_id = config_gp->id;
+	}
+	intel_ring_advance(ring);
+
+unlock:
+	mutex_unlock(&dev_priv->perfmon.config.lock);
+	return ret;
+}
+
 static struct i915_vma *
 i915_gem_execbuffer_parse(struct intel_engine_cs *engine,
 			  struct drm_i915_gem_exec_object2 *shadow_exec_entry,
@@ -1525,6 +1674,10 @@ execbuf_submit(struct i915_execbuffer_params *params,
 			return ret;
 	}
 
+	if (IS_GEN9(dev_priv) && params->engine->id == RCS)
+		i915_program_perfmon(dev_priv, params->request->ring,
+				     params->request, params->ctx);
+
 	exec_len   = args->batch_len;
 	exec_start = params->batch->node.start +
 		     params->args_batch_start_offset;
diff --git a/drivers/gpu/drm/i915/i915_perfmon.c b/drivers/gpu/drm/i915/i915_perfmon.c
new file mode 100644
index 0000000..33a9480
--- /dev/null
+++ b/drivers/gpu/drm/i915/i915_perfmon.c
@@ -0,0 +1,908 @@
+/*
+ * Copyright  2013 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ *
+ */
+#include <drm/i915_drm.h>
+#include "i915_drv.h"
+#include "i915_trace.h"
+#include "intel_drv.h"
+#include "linux/wait.h"
+#include "i915_perfmon_defs.h"
+
+/**
+ * i915_get_render_hw_ctx_id
+ *
+ * Get render engine HW context ID for given context. This is the
+ * representation of context in the HW. This is *not* context ID as referenced
+ * by usermode. For legacy submission path this is logical ring context address.
+ * For execlist this is the kernel managed context ID written to execlist
+ * descriptor.
+ */
+static int  i915_get_render_hw_ctx_id(
+	struct drm_i915_private *dev_priv,
+	struct i915_gem_context *ctx,
+	__u32 *id)
+{
+	struct i915_vma *vma = ctx->engine[RCS].state;
+
+	if (!vma)
+		return -ENOENT;
+
+	*id = i915_ggtt_offset(vma) >> 12;
+
+	return 0;
+}
+
+/**
+ * i915_perfmon_get_hw_ctx_id
+ *
+ * Get HW context ID for given context ID and DRM file.
+ */
+static int i915_perfmon_get_hw_ctx_id(
+	struct drm_device *dev,
+	struct drm_file *file,
+	struct drm_i915_perfmon_get_hw_ctx_id *ioctl_data)
+{
+	struct drm_i915_private *dev_priv =
+		(struct drm_i915_private *) dev->dev_private;
+	struct i915_gem_context *ctx;
+	struct drm_i915_file_private *file_priv = file->driver_priv;
+	int ret;
+
+	if (!HAS_HW_CONTEXTS(dev))
+		return -ENODEV;
+
+	ret = i915_mutex_lock_interruptible(dev);
+	if (ret)
+		return ret;
+
+	ctx = i915_gem_context_lookup(file_priv, ioctl_data->ctx_id);
+	if (IS_ERR_OR_NULL(ctx))
+		ret = -ENOENT;
+	else
+		ret = i915_get_render_hw_ctx_id(dev_priv, ctx,
+			&ioctl_data->hw_ctx_id);
+
+	mutex_unlock(&dev->struct_mutex);
+	return ret;
+}
+
+struct i915_perfmon_hw_ctx_list {
+	__u32 *ids;
+	__u32 capacity;
+	__u32 size;
+	__u32 iterations_left;
+};
+
+/**
+ * process_context
+ *
+ * Check if context referenced by 'ptr' belongs to application with
+ * provided process ID. If so, increment total number of contexts
+ * found (list->size) and add context id to the list if
+ * its capacity is not reached.
+ */
+static int process_context(struct drm_i915_private *dev_priv,
+	struct i915_gem_context *ctx,
+	__u32 pid,
+	struct i915_perfmon_hw_ctx_list *list)
+{
+	bool ctx_match;
+	bool has_render_ring;
+	__u32 id;
+
+	if (list->iterations_left == 0)
+		return 0;
+	--list->iterations_left;
+
+	ctx_match = (pid == pid_vnr(ctx->pid) ||
+			 pid == 0 ||
+			 ctx == dev_priv->kernel_context);
+
+	if (ctx_match) {
+		has_render_ring =
+			(0 == i915_get_render_hw_ctx_id(
+				dev_priv, ctx, &id));
+	}
+
+	if (ctx_match && has_render_ring) {
+		if (list->size < list->capacity)
+			list->ids[list->size] = id;
+		list->size++;
+	}
+
+	return 0;
+}
+
+/**
+ * i915_perfmon_get_hw_ctx_ids
+ *
+ * Lookup the list of all contexts and return HW context IDs of those
+ * belonging to provided process id.
+ *
+ * User specifies maximum number of IDs to be written to provided block of
+ * memory: ioctl_data->count. Returned is the list of not more than
+ * ioctl_data->count HW context IDs together with total number of matching
+ * contexts found - potentially more than ioctl_data->count.
+ *
+ */
+static int i915_perfmon_get_hw_ctx_ids(
+	struct drm_device *dev,
+	struct drm_i915_perfmon_get_hw_ctx_ids *ioctl_data)
+{
+	struct drm_i915_private *dev_priv =
+		(struct drm_i915_private *) dev->dev_private;
+	struct i915_perfmon_hw_ctx_list list;
+	struct i915_gem_context *ctx;
+	unsigned int ids_to_copy;
+	int ret;
+
+	if (!HAS_HW_CONTEXTS(dev))
+		return -ENODEV;
+
+	if (ioctl_data->count > I915_PERFMON_MAX_HW_CTX_IDS)
+		return -EINVAL;
+
+	list.ids = kzalloc(
+		ioctl_data->count * sizeof(__u32), GFP_KERNEL);
+	if (!list.ids)
+		return -ENOMEM;
+	list.capacity = ioctl_data->count;
+	list.size = 0;
+	list.iterations_left = I915_PERFMON_MAX_HW_CTX_IDS;
+
+	ret = i915_mutex_lock_interruptible(dev);
+	if (ret)
+		goto exit;
+
+	list_for_each_entry(ctx, &dev_priv->context_list, link) {
+		process_context(dev_priv, ctx, ioctl_data->pid, &list);
+	}
+
+	mutex_unlock(&dev->struct_mutex);
+
+	/*
+	 * After we searched all the contexts list.size is the total number
+	 * of contexts matching the query. This is potentially more than
+	 * the capacity of user buffer (list.capacity).
+	 */
+	ids_to_copy = min(list.size, list.capacity);
+	if (copy_to_user(
+		(__u32 __user *)(uintptr_t)ioctl_data->ids,
+		list.ids,
+		ids_to_copy * sizeof(__u32))) {
+		ret = -EFAULT;
+		goto exit;
+	}
+
+	/* Return total number of matching ids to the user. */
+	ioctl_data->count = list.size;
+exit:
+	kfree(list.ids);
+	return ret;
+}
+
+/**
+ * copy_entries
+ *
+ * Helper function to copy OA configuration entries to new destination.
+ *
+ * Source configuration is first validated. In case of success pointer to newly
+ * allocated memory containing copy of source configuration is returned in *out.
+ *
+ */
+static int copy_entries(
+	struct drm_i915_perfmon_config *source,
+	bool user,
+	void **out)
+{
+	size_t size = 0;
+
+	*out = NULL;
+
+	/* basic validation of input */
+	if (source->id == 0 || source->size == 0 || source->entries == NULL)
+		return 0;
+
+	if (source->size > I915_PERFMON_CONFIG_SIZE)
+		return -EINVAL;
+
+	size = source->size  * sizeof(struct drm_i915_perfmon_config_entry);
+
+	*out = kzalloc(
+		   size,
+		   GFP_KERNEL);
+	if (*out == NULL) {
+		DRM_ERROR("failed to allocate configuration buffer\n");
+		return -ENOMEM;
+	}
+
+	if (user) {
+		int ret = copy_from_user(*out, source->entries, size);
+		if (ret) {
+			DRM_ERROR("failed to copy user provided config: %x\n",
+					ret);
+			kfree(*out);
+			*out = NULL;
+			return -EFAULT;
+		}
+	} else
+		memcpy(*out, source->entries, size);
+
+	return 0;
+}
+
+/**
+ * i915_perfmon_copy_config
+ *
+ * Utility function to copy OA and GP configuration to its destination.
+ *
+ * This is first used when global configuration is set by the user by calling
+ * I915_PERFMON_SET_CONFIG and then for the second time (optionally) when user
+ * calls I915_PERFMON_LOAD_CONFIG to copy the configuration from global storage
+ * to his context.
+ *
+ * 'user' boolean value indicates whether pointer to source config is provided
+ * by usermode (I915_PERFMON_SET_CONFIG case).
+ *
+ * If both OA and GP config are provided (!= NULL) then either both are copied
+ * to their respective locations or none of them (which is indicated by return
+ * value != 0).
+ *
+ * target_oa and target_gp are assumed to be non-NULL.
+ *
+ */
+static int i915_perfmon_copy_config(
+	struct drm_i915_private *dev_priv,
+	struct drm_i915_perfmon_config *target_oa,
+	struct drm_i915_perfmon_config *target_gp,
+	struct drm_i915_perfmon_config source_oa,
+	struct drm_i915_perfmon_config source_gp,
+	bool user)
+{
+	void *temp_oa = NULL;
+	void *temp_gp = NULL;
+	int ret = 0;
+
+	BUG_ON(!mutex_is_locked(&dev_priv->perfmon.config.lock));
+
+	/* copy configurations to temporary storage */
+
+	ret = copy_entries(&source_oa, user, &temp_oa);
+	if (ret)
+		return ret;
+	ret = copy_entries(&source_gp, user, &temp_gp);
+	if (ret) {
+		kfree(temp_oa);
+		return ret;
+	}
+
+	/*
+	 * Allocation and copy successful, free old config memory and swap
+	 * pointers
+	 */
+	if (temp_oa) {
+		kfree(target_oa->entries);
+		target_oa->entries = temp_oa;
+		target_oa->id = source_oa.id;
+		target_oa->size = source_oa.size;
+	}
+	if (temp_gp) {
+		kfree(target_gp->entries);
+		target_gp->entries = temp_gp;
+		target_gp->id = source_gp.id;
+		target_gp->size = source_gp.size;
+	}
+
+	return 0;
+}
+
+/**
+ * i915_perfmon_set_config
+ *
+ * Store OA/GP configuration for later use.
+ *
+ * Configuration content is not validated since it is provided by user who had
+ * previously called Perfmon Open with sysadmin privilege level.
+ *
+ */
+static int i915_perfmon_set_config(
+	struct drm_device *dev,
+	struct drm_i915_perfmon_set_config *args)
+{
+	struct drm_i915_private *dev_priv =
+		(struct drm_i915_private *) dev->dev_private;
+	int ret = 0;
+
+	struct drm_i915_perfmon_config user_config_oa;
+	struct drm_i915_perfmon_config user_config_gp;
+
+	if (!(IS_GEN8(dev) || IS_GEN9(dev)))
+		return -EINVAL;
+
+	/* validate target */
+	switch (args->target) {
+	case I915_PERFMON_CONFIG_TARGET_CTX:
+	case I915_PERFMON_CONFIG_TARGET_PID:
+	case I915_PERFMON_CONFIG_TARGET_ALL:
+		/* OK */
+		break;
+	default:
+		DRM_DEBUG("invalid target\n");
+		return -EINVAL;
+	}
+
+	/* setup input for i915_perfmon_copy_config */
+	user_config_oa.id = args->oa.id;
+	user_config_oa.size = args->oa.size;
+	user_config_oa.entries =
+		(struct drm_i915_perfmon_config_entry __user *)
+			(uintptr_t)args->oa.entries;
+
+	user_config_gp.id = args->gp.id;
+	user_config_gp.size = args->gp.size;
+	user_config_gp.entries =
+		(struct drm_i915_perfmon_config_entry __user *)
+			(uintptr_t)args->gp.entries;
+
+	ret = mutex_lock_interruptible(&dev_priv->perfmon.config.lock);
+	if (ret)
+		return ret;
+
+	if (!atomic_read(&dev_priv->perfmon.config.enable)) {
+		ret = -EINVAL;
+		goto unlock_perfmon;
+	}
+
+	ret = i915_perfmon_copy_config(dev_priv,
+			&dev_priv->perfmon.config.oa,
+			&dev_priv->perfmon.config.gp,
+			user_config_oa, user_config_gp,
+			true);
+
+	if (ret)
+		goto unlock_perfmon;
+
+	dev_priv->perfmon.config.target = args->target;
+	dev_priv->perfmon.config.pid = args->pid;
+
+unlock_perfmon:
+	mutex_unlock(&dev_priv->perfmon.config.lock);
+	return ret;
+}
+
+/**
+ * i915_perfmon_load_config
+ *
+ * Copy configuration from global storage to current context.
+ *
+ */
+static int i915_perfmon_load_config(
+	struct drm_device *dev,
+	struct drm_file *file,
+	struct drm_i915_perfmon_load_config *args)
+{
+	struct drm_i915_private *dev_priv =
+		(struct drm_i915_private *) dev->dev_private;
+	struct drm_i915_file_private *file_priv = file->driver_priv;
+	struct i915_gem_context *ctx;
+	struct drm_i915_perfmon_config user_config_oa;
+	struct drm_i915_perfmon_config user_config_gp;
+	int ret;
+
+	if (!(IS_GEN8(dev)) && !(IS_GEN9(dev)))
+		return -EINVAL;
+
+	ret = i915_mutex_lock_interruptible(dev);
+	if (ret)
+		return ret;
+
+	if (!atomic_read(&dev_priv->perfmon.config.enable)) {
+		ret = -EINVAL;
+		goto unlock_dev;
+	}
+
+	ctx = i915_gem_context_lookup(file_priv, args->ctx_id);
+
+	if (IS_ERR_OR_NULL(ctx)) {
+		DRM_DEBUG("invalid context\n");
+		ret = -EINVAL;
+		goto unlock_dev;
+	}
+
+	ret = mutex_lock_interruptible(&dev_priv->perfmon.config.lock);
+	if (ret)
+		goto unlock_dev;
+
+	user_config_oa = dev_priv->perfmon.config.oa;
+	user_config_gp = dev_priv->perfmon.config.gp;
+
+	/*
+	 * copy configuration to the context only if requested config ID matches
+	 * device configuration ID
+	 */
+	if (!(args->oa_id != 0 &&
+	      args->oa_id == dev_priv->perfmon.config.oa.id))
+		user_config_oa.entries = NULL;
+	if (!(args->gp_id != 0 &&
+	     args->gp_id == dev_priv->perfmon.config.gp.id))
+		user_config_gp.entries = NULL;
+
+	ret = i915_perfmon_copy_config(dev_priv,
+			&ctx->perfmon.config.oa.pending,
+			&ctx->perfmon.config.gp.pending,
+			dev_priv->perfmon.config.oa,
+			dev_priv->perfmon.config.gp,
+			false);
+
+	if (ret)
+		goto unlock_perfmon;
+
+	/*
+	 * return info about what is actualy set for submission in
+	 * target context
+	 */
+	args->gp_id = ctx->perfmon.config.gp.pending.id;
+	args->oa_id = ctx->perfmon.config.oa.pending.id;
+
+unlock_perfmon:
+	mutex_unlock(&dev_priv->perfmon.config.lock);
+unlock_dev:
+	mutex_unlock(&dev->struct_mutex);
+	return ret;
+}
+
+static void *emit_dword(void *mem, __u32 cmd)
+{
+	iowrite32(cmd, mem);
+	return ((__u32 *)mem) + 1;
+}
+
+static void *emit_load_register_imm(void *mem, __u32 reg, __u32 val)
+{
+	mem = emit_dword(mem, MI_NOOP);
+	mem = emit_dword(mem, MI_LOAD_REGISTER_IMM(1));
+	mem = emit_dword(mem, reg);
+	mem = emit_dword(mem, val);
+	return mem;
+}
+
+static void *emit_cs_stall_pipe_control(void *mem)
+{
+	mem = emit_dword(mem, GFX_OP_PIPE_CONTROL(6));
+	mem = emit_dword(mem, PIPE_CONTROL_CS_STALL|PIPE_CONTROL_WRITE_FLUSH|
+			      PIPE_CONTROL_GLOBAL_GTT_IVB);
+	mem = emit_dword(mem, 0);
+	mem = emit_dword(mem, 0);
+	mem = emit_dword(mem, 0);
+	mem = emit_dword(mem, 0);
+	return mem;
+}
+
+int i915_perfmon_update_workaround_bb(struct drm_i915_private *dev_priv,
+				      struct drm_i915_perfmon_config *config)
+{
+	const size_t commands_size = 6 + /* pipe control */
+				     config->size * 4 + /* NOOP + LRI */
+				     6 + /* pipe control */
+				     1;  /* BB end */
+	void *buffer_tail;
+	unsigned int i = 0;
+	int ret = 0;
+
+	if (commands_size > PAGE_SIZE) {
+		DRM_ERROR("OA cfg too long to fit into workarond BB\n");
+		return -ENOSPC;
+	}
+
+	BUG_ON(!mutex_is_locked(&dev_priv->perfmon.config.lock));
+
+	if (atomic_read(&dev_priv->perfmon.config.enable) == 0 ||
+	    !dev_priv->rc6_wa_bb.obj) {
+		DRM_ERROR("not ready to write WA BB commands\n");
+		return -EINVAL;
+	}
+
+	ret = mutex_lock_interruptible(&dev_priv->rc6_wa_bb.lock);
+	if (ret)
+		return ret;
+
+	if (!dev_priv->rc6_wa_bb.obj) {
+		mutex_unlock(&dev_priv->rc6_wa_bb.lock);
+		return 0;
+	}
+
+	/* disable RC6 WA BB */
+	I915_WRITE(GEN8_RC6_WA_BB, 0x0);
+
+	buffer_tail = dev_priv->rc6_wa_bb.address;
+	buffer_tail = emit_cs_stall_pipe_control(buffer_tail);
+
+	/* OA/NOA config */
+	for (i = 0; i < config->size; i++)
+		buffer_tail = emit_load_register_imm(
+			buffer_tail,
+			config->entries[i].offset,
+			config->entries[i].value);
+
+	buffer_tail = emit_cs_stall_pipe_control(buffer_tail);
+
+	/* BB END */
+	buffer_tail = emit_dword(buffer_tail, MI_BATCH_BUFFER_END);
+
+	/* enable WA BB */
+	I915_WRITE(GEN8_RC6_WA_BB, dev_priv->rc6_wa_bb.offset | 0x1);
+
+	mutex_unlock(&dev_priv->rc6_wa_bb.lock);
+
+	return 0;
+}
+
+static int allocate_wa_bb(struct drm_i915_private *dev_priv)
+{
+	struct i915_vma *vma;
+	int ret = 0;
+
+	BUG_ON(!mutex_is_locked(&dev_priv->drm.struct_mutex));
+
+	ret = mutex_lock_interruptible(&dev_priv->rc6_wa_bb.lock);
+	if (ret)
+		return ret;
+
+	if (atomic_inc_return(&dev_priv->rc6_wa_bb.enable) > 1) {
+		mutex_unlock(&dev_priv->rc6_wa_bb.lock);
+		return 0;
+	}
+
+	BUG_ON(dev_priv->rc6_wa_bb.obj != NULL);
+
+	dev_priv->rc6_wa_bb.obj = i915_gem_object_create(&dev_priv->drm,
+							 PAGE_SIZE);
+	if (!dev_priv->rc6_wa_bb.obj) {
+		ret = -ENOMEM;
+		goto unlock;
+	}
+
+	vma = i915_gem_object_ggtt_pin(dev_priv->rc6_wa_bb.obj, NULL, 0,
+				       PAGE_SIZE, PIN_MAPPABLE);
+	if (ret) {
+		ret = PTR_ERR(vma);
+		i915_gem_object_put_unlocked(dev_priv->rc6_wa_bb.obj);
+		goto unlock;
+	}
+	dev_priv->rc6_wa_bb.vma = vma;
+
+	ret = i915_gem_object_set_to_gtt_domain(dev_priv->rc6_wa_bb.obj,
+						true);
+	if (ret) {
+		i915_vma_unpin(vma);
+		i915_gem_object_put_unlocked(dev_priv->rc6_wa_bb.obj);
+		goto unlock;
+	}
+
+	dev_priv->rc6_wa_bb.offset = i915_ggtt_offset(vma);
+
+	dev_priv->rc6_wa_bb.address = ioremap_wc(
+		dev_priv->ggtt.mappable_base + dev_priv->rc6_wa_bb.offset,
+		PAGE_SIZE);
+
+	if (!dev_priv->rc6_wa_bb.address) {
+		i915_vma_unpin(vma);
+		i915_gem_object_put_unlocked(dev_priv->rc6_wa_bb.obj);
+		ret =  -ENOMEM;
+		goto unlock;
+	}
+
+	DRM_DEBUG("RC6 WA BB, offset %lx address %p",
+		  dev_priv->rc6_wa_bb.offset,
+		  dev_priv->rc6_wa_bb.address);
+
+	memset(dev_priv->rc6_wa_bb.address, 0, PAGE_SIZE);
+
+unlock:
+	if (ret) {
+		dev_priv->rc6_wa_bb.obj = NULL;
+		dev_priv->rc6_wa_bb.vma = NULL;
+		dev_priv->rc6_wa_bb.offset = 0;
+	}
+	mutex_unlock(&dev_priv->rc6_wa_bb.lock);
+
+	return ret;
+}
+
+static void deallocate_wa_bb(struct drm_i915_private *dev_priv)
+{
+	BUG_ON(!mutex_is_locked(&dev_priv->drm.struct_mutex));
+
+	mutex_lock(&dev_priv->rc6_wa_bb.lock);
+
+	if (atomic_read(&dev_priv->rc6_wa_bb.enable) == 0)
+		goto unlock;
+
+	if (atomic_dec_return(&dev_priv->rc6_wa_bb.enable) > 1)
+		goto unlock;
+
+	I915_WRITE(GEN8_RC6_WA_BB, 0);
+
+	if (dev_priv->rc6_wa_bb.obj != NULL) {
+		iounmap(dev_priv->rc6_wa_bb.address);
+		i915_vma_unpin(dev_priv->rc6_wa_bb.vma);
+		i915_gem_object_put(dev_priv->rc6_wa_bb.obj);
+		dev_priv->rc6_wa_bb.obj = NULL;
+		dev_priv->rc6_wa_bb.offset = 0;
+	}
+unlock:
+	mutex_unlock(&dev_priv->rc6_wa_bb.lock);
+}
+
+/**
+* i915_perfmon_config_enable_disable
+*
+* Enable/disable OA/GP configuration transport.
+*/
+static int i915_perfmon_config_enable_disable(
+	struct drm_device *dev,
+	int enable)
+{
+	int ret;
+
+	struct drm_i915_private *dev_priv =
+		(struct drm_i915_private *) dev->dev_private;
+
+	if (!(IS_GEN8(dev)) && !(IS_GEN9(dev)))
+		return -EINVAL;
+
+	ret = i915_mutex_lock_interruptible(&dev_priv->drm);
+	if (ret)
+		return ret;
+
+	ret = mutex_lock_interruptible(&dev_priv->perfmon.config.lock);
+	if (ret) {
+		mutex_unlock(&dev->struct_mutex);
+		return ret;
+	}
+
+	if (enable) {
+		ret = allocate_wa_bb(dev_priv);
+		if (!ret &&
+		    atomic_inc_return(&dev_priv->perfmon.config.enable) == 1) {
+			dev_priv->perfmon.config.target =
+				I915_PERFMON_CONFIG_TARGET_ALL;
+			dev_priv->perfmon.config.oa.id = 0;
+			dev_priv->perfmon.config.gp.id = 0;
+		}
+	} else if (atomic_read(&dev_priv->perfmon.config.enable)) {
+		atomic_dec(&dev_priv->perfmon.config.enable);
+		deallocate_wa_bb(dev_priv);
+	}
+
+	mutex_unlock(&dev_priv->perfmon.config.lock);
+	mutex_unlock(&dev->struct_mutex);
+
+	return ret;
+}
+
+
+/**
+ * i915_perfmon_open
+ *
+ * open perfmon for current file
+ */
+static int i915_perfmon_open(
+	struct drm_file *file)
+{
+	struct drm_i915_file_private *file_priv = file->driver_priv;
+	int ret = 0;
+
+	if (!capable(CAP_SYS_ADMIN))
+		ret = -EACCES;
+	else
+		file_priv->perfmon.opened = true;
+
+	return ret;
+}
+
+/**
+ * i915_perfmon_close
+ *
+ * close perfmon for current file
+ */
+static int i915_perfmon_close(
+	struct drm_file *file)
+{
+	struct drm_i915_file_private *file_priv = file->driver_priv;
+
+	file_priv->perfmon.opened = false;
+
+	return 0;
+}
+
+
+int i915_perfmon_pin_oa_buffer(struct drm_device *dev,
+	struct drm_file *file,
+	struct drm_i915_perfmon_pin_oa_buffer* oa_buffer)
+{
+	int ret = 0;
+	struct drm_i915_gem_object *obj = NULL;
+	struct i915_vma *vma;
+
+	ret = i915_mutex_lock_interruptible(dev);
+	if (ret)
+		return ret;
+
+	obj = i915_gem_object_lookup(file, oa_buffer->handle);
+	if (&obj->base == NULL) {
+		ret = -ENOENT;
+		goto unlock;
+	}
+
+	vma = i915_gem_object_ggtt_pin(obj, NULL, 0, oa_buffer->alignment,
+				       PIN_MAPPABLE);
+	if (IS_ERR(vma))
+		ret = PTR_ERR(vma);
+	else
+		oa_buffer->offset = i915_ggtt_offset(vma);
+
+	i915_gem_object_put(obj);
+unlock:
+	mutex_unlock(&dev->struct_mutex);
+
+	return ret;
+}
+
+int i915_perfmon_unpin_oa_buffer(struct drm_device *dev,
+	struct drm_file *file,
+	struct drm_i915_perfmon_unpin_oa_buffer* oa_buffer)
+{
+	int ret = 0;
+	struct drm_i915_gem_object *obj = NULL;
+	struct i915_address_space *vm = &to_i915(dev)->ggtt.base;
+	struct i915_vma *vma;
+
+	ret = i915_mutex_lock_interruptible(dev);
+	if (ret)
+		return ret;
+
+	obj = i915_gem_object_lookup(file, oa_buffer->handle);
+	if (&obj->base == NULL) {
+		ret = -ENOENT;
+		goto unlock;
+	}
+
+	vma = i915_gem_obj_to_vma(obj, vm, NULL);
+	if (vma)
+		i915_vma_unpin(vma);
+
+	i915_gem_object_put(obj);
+unlock:
+	mutex_unlock(&dev->struct_mutex);
+	return ret;
+}
+
+/**
+ * i915_perfmon_ioctl - performance monitoring support
+ *
+ * Main entry point to performance monitoring support
+ * IOCTLs.
+ */
+int i915_perfmon_ioctl(struct drm_device *dev, void *data,
+	struct drm_file *file)
+{
+	struct drm_i915_perfmon *perfmon = data;
+	struct drm_i915_file_private *file_priv = file->driver_priv;
+	int ret = 0;
+
+	switch (perfmon->op) {
+	case I915_PERFMON_OPEN:
+		ret = i915_perfmon_open(file);
+		break;
+
+	case I915_PERFMON_CLOSE:
+		ret = i915_perfmon_close(file);
+		break;
+	case I915_PERFMON_ENABLE_CONFIG:
+		ret = i915_perfmon_config_enable_disable(dev, 1);
+		break;
+
+	case I915_PERFMON_DISABLE_CONFIG:
+		ret = i915_perfmon_config_enable_disable(dev, 0);
+		break;
+
+	case I915_PERFMON_SET_CONFIG:
+		if (!file_priv->perfmon.opened) {
+			ret = -EACCES;
+			break;
+		}
+		ret = i915_perfmon_set_config(
+			dev,
+			&perfmon->data.set_config);
+		break;
+
+	case I915_PERFMON_LOAD_CONFIG:
+		ret = i915_perfmon_load_config(
+			dev,
+			file,
+			&perfmon->data.load_config);
+		break;
+
+	case I915_PERFMON_GET_HW_CTX_ID:
+		ret = i915_perfmon_get_hw_ctx_id(
+			dev,
+			file,
+			&perfmon->data.get_hw_ctx_id);
+		break;
+
+	case I915_PERFMON_GET_HW_CTX_IDS:
+		if (!file_priv->perfmon.opened) {
+			ret = -EACCES;
+			break;
+		}
+		ret = i915_perfmon_get_hw_ctx_ids(
+			dev,
+			&perfmon->data.get_hw_ctx_ids);
+		break;
+	case I915_PERFMON_PIN_OA_BUFFER:
+		ret = i915_perfmon_pin_oa_buffer(
+			dev,
+			file,
+			&perfmon->data.pin_oa_buffer);
+		break;
+	case I915_PERFMON_UNPIN_OA_BUFFER:
+		ret = i915_perfmon_unpin_oa_buffer(
+			dev,
+			file,
+			&perfmon->data.unpin_oa_buffer);
+		break;
+	default:
+		DRM_DEBUG("UNKNOWN OP\n");
+		/* unknown operation */
+		ret = -EINVAL;
+		break;
+	}
+
+	return ret;
+}
+
+void i915_perfmon_setup(struct drm_i915_private *dev_priv)
+{
+	atomic_set(&dev_priv->perfmon.config.enable, 0);
+	dev_priv->perfmon.config.oa.entries = NULL;
+	dev_priv->perfmon.config.gp.entries = NULL;
+}
+
+void i915_perfmon_cleanup(struct drm_i915_private *dev_priv)
+{
+	kfree(dev_priv->perfmon.config.oa.entries);
+	kfree(dev_priv->perfmon.config.gp.entries);
+}
+
+void i915_perfmon_ctx_setup(struct i915_gem_context *ctx)
+{
+	ctx->perfmon.config.oa.pending.entries = NULL;
+	ctx->perfmon.config.gp.pending.entries = NULL;
+}
+
+void i915_perfmon_ctx_cleanup(struct i915_gem_context *ctx)
+{
+	kfree(ctx->perfmon.config.oa.pending.entries);
+	kfree(ctx->perfmon.config.gp.pending.entries);
+}
diff --git a/drivers/gpu/drm/i915/i915_perfmon_defs.h b/drivers/gpu/drm/i915/i915_perfmon_defs.h
new file mode 100644
index 0000000..300f3e5
--- /dev/null
+++ b/drivers/gpu/drm/i915/i915_perfmon_defs.h
@@ -0,0 +1,62 @@
+/*
+ * Copyright  2014 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ */
+
+#ifndef _I915_PERFMON_DEFS_H_
+#define _I915_PERFMON_DEFS_H_
+
+struct drm_i915_perfmon_config {
+	struct drm_i915_perfmon_config_entry *entries;
+	__u32 size;
+	__u32  id;
+};
+
+struct drm_i915_perfmon_context {
+	struct {
+		struct {
+			struct drm_i915_perfmon_config pending;
+			__u32 submitted_id;
+		} oa, gp;
+	} config;
+};
+
+struct drm_i915_perfmon_device {
+	/* perfmon interrupt support */
+	wait_queue_head_t	buffer_queue;
+	atomic_t		buffer_interrupts;
+
+	/* perfmon counters configuration */
+	struct {
+		struct drm_i915_perfmon_config oa;
+		struct drm_i915_perfmon_config gp;
+		enum DRM_I915_PERFMON_CONFIG_TARGET target;
+		pid_t pid;
+		atomic_t enable;
+		struct mutex lock;
+	} config;
+};
+
+struct drm_i915_perfmon_file {
+	bool opened;
+};
+
+#endif	/* _I915_PERFMON_DEFS_H_ */
diff --git a/drivers/gpu/drm/i915/i915_reg.h b/drivers/gpu/drm/i915/i915_reg.h
index 307f39d..af163a7 100644
--- a/drivers/gpu/drm/i915/i915_reg.h
+++ b/drivers/gpu/drm/i915/i915_reg.h
@@ -2064,6 +2064,9 @@ enum skl_disp_power_wells {
 #define GEN9_EU_DISABLE(slice)		_MMIO(0x9134 + (slice)*0x4)
 
 #define GEN6_BSD_SLEEP_PSMI_CONTROL	_MMIO(0x12050)
+#define GEN8_RC6_WA_BB			_MMIO(0x2058)
+#define GEN8_OA_CTX_CONTROL		0x2360
+
 #define   GEN6_BSD_SLEEP_MSG_DISABLE	(1 << 0)
 #define   GEN6_BSD_SLEEP_FLUSH_DISABLE	(1 << 2)
 #define   GEN6_BSD_SLEEP_INDICATOR	(1 << 3)
@@ -6138,6 +6141,8 @@ enum {
 #define GEN8_PCU_IIR _MMIO(0x444e8)
 #define GEN8_PCU_IER _MMIO(0x444ec)
 
+#define GEN8_OA_IMR  0x2b20
+
 #define ILK_DISPLAY_CHICKEN2	_MMIO(0x42004)
 /* Required on all Ironlake and Sandybridge according to the B-Spec. */
 #define  ILK_ELPIN_409_SELECT	(1 << 25)
diff --git a/drivers/gpu/drm/i915/intel_lrc.c b/drivers/gpu/drm/i915/intel_lrc.c
index 0adb879..f72b00d 100644
--- a/drivers/gpu/drm/i915/intel_lrc.c
+++ b/drivers/gpu/drm/i915/intel_lrc.c
@@ -237,6 +237,7 @@ static void execlists_init_reg_state(u32 *reg_state,
 				     struct intel_engine_cs *engine,
 				     struct intel_ring *ring);
 
+
 /**
  * intel_sanitize_enable_execlists() - sanitize i915.enable_execlists
  * @dev_priv: i915 device private
diff --git a/include/uapi/drm/i915_drm.h b/include/uapi/drm/i915_drm.h
index c3a3b51..dbae77a 100644
--- a/include/uapi/drm/i915_drm.h
+++ b/include/uapi/drm/i915_drm.h
@@ -28,6 +28,7 @@
 #define _UAPI_I915_DRM_H_
 
 #include <drm/drm.h>
+#include <drm/i915_perfmon.h>
 
 /* Please note that modifications to all structs defined here are
  * subject to backwards-compatibility constraints.
@@ -277,6 +278,8 @@ struct i915_ext_ioctl_data {
 	DRM_IOWR(DRM_I915_EXT_USERDATA, struct drm_i915_gem_userdata_blk)
 
 
+#define DRM_I915_PERFMON		0x3e
+
 #define DRM_IOCTL_I915_INIT		DRM_IOW( DRM_COMMAND_BASE + DRM_I915_INIT, drm_i915_init_t)
 #define DRM_IOCTL_I915_FLUSH		DRM_IO ( DRM_COMMAND_BASE + DRM_I915_FLUSH)
 #define DRM_IOCTL_I915_FLIP		DRM_IO ( DRM_COMMAND_BASE + DRM_I915_FLIP)
@@ -329,6 +332,7 @@ struct i915_ext_ioctl_data {
 #define DRM_IOCTL_I915_GEM_USERPTR			DRM_IOWR (DRM_COMMAND_BASE + DRM_I915_GEM_USERPTR, struct drm_i915_gem_userptr)
 #define DRM_IOCTL_I915_GEM_CONTEXT_GETPARAM	DRM_IOWR (DRM_COMMAND_BASE + DRM_I915_GEM_CONTEXT_GETPARAM, struct drm_i915_gem_context_param)
 #define DRM_IOCTL_I915_GEM_CONTEXT_SETPARAM	DRM_IOWR (DRM_COMMAND_BASE + DRM_I915_GEM_CONTEXT_SETPARAM, struct drm_i915_gem_context_param)
+#define DRM_IOCTL_I915_PERFMON 			DRM_IOWR (DRM_COMMAND_BASE + DRM_I915_PERFMON, struct drm_i915_perfmon)
 
 #define DRM_IOCTL_I915_EXT_IOCTL	\
 		DRM_IOW(DRM_COMMAND_BASE + DRM_I915_EXT_IOCTL, \
diff --git a/include/uapi/drm/i915_perfmon.h b/include/uapi/drm/i915_perfmon.h
new file mode 100644
index 0000000..d252ca3
--- /dev/null
+++ b/include/uapi/drm/i915_perfmon.h
@@ -0,0 +1,124 @@
+/*
+ * Copyright  2013 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ */
+
+#ifndef _I915_PERFMON_H_
+#define _I915_PERFMON_H_
+
+#define I915_PERFMON_IOCTL_VERSION	5
+
+struct drm_i915_perfmon_config_entry {
+	__u32 offset;
+	__u32 value;
+};
+
+static const unsigned int I915_PERFMON_CONFIG_SIZE = 256;
+
+/* Explicitly aligned to 8 bytes to avoid mismatch
+   between 64-bit KM and 32-bit UM. */
+typedef __u64 drm_i915_perfmon_shared_ptr __aligned(8);
+
+struct drm_i915_perfmon_user_config {
+	/* This is pointer to struct drm_i915_perfmon_config_entry.*/
+	drm_i915_perfmon_shared_ptr entries;
+	__u32 size;
+	__u32 id;
+};
+
+enum DRM_I915_PERFMON_CONFIG_TARGET {
+	I915_PERFMON_CONFIG_TARGET_CTX,
+	I915_PERFMON_CONFIG_TARGET_PID,
+	I915_PERFMON_CONFIG_TARGET_ALL,
+};
+
+struct drm_i915_perfmon_set_config {
+	enum DRM_I915_PERFMON_CONFIG_TARGET target;
+	struct drm_i915_perfmon_user_config oa;
+	struct drm_i915_perfmon_user_config gp;
+	__u32 pid;
+};
+
+struct drm_i915_perfmon_load_config {
+	__u32 ctx_id;
+	__u32 oa_id;
+	__u32 gp_id;
+};
+
+
+static const unsigned int I915_PERFMON_MAX_HW_CTX_IDS = 1024;
+
+struct drm_i915_perfmon_get_hw_ctx_ids {
+	__u32 pid;
+	__u32 count;
+	 /* This is pointer to __u32. */
+	drm_i915_perfmon_shared_ptr ids;
+};
+
+struct drm_i915_perfmon_get_hw_ctx_id {
+	__u32 ctx_id;
+	__u32 hw_ctx_id;
+};
+
+struct drm_i915_perfmon_pin_oa_buffer {
+	/** Handle of the buffer to be pinned. */
+	__u32 handle;
+	__u32 pad;
+
+	/** alignment required within the aperture */
+	__u64 alignment;
+
+	/** Returned GTT offset of the buffer. */
+	__u64 offset;
+};
+
+struct drm_i915_perfmon_unpin_oa_buffer {
+	/** Handle of the buffer to be pinned. */
+	__u32 handle;
+	__u32 pad;
+};
+
+enum I915_PERFMON_IOCTL_OP {
+	I915_PERFMON_OPEN = 8,
+	I915_PERFMON_CLOSE,
+	I915_PERFMON_ENABLE_CONFIG,
+	I915_PERFMON_DISABLE_CONFIG,
+	I915_PERFMON_SET_CONFIG,
+	I915_PERFMON_LOAD_CONFIG,
+	I915_PERFMON_GET_HW_CTX_ID,
+	I915_PERFMON_GET_HW_CTX_IDS,
+	I915_PERFMON_PIN_OA_BUFFER,
+	I915_PERFMON_UNPIN_OA_BUFFER,
+};
+
+struct drm_i915_perfmon {
+	enum I915_PERFMON_IOCTL_OP op;
+	union {
+		struct drm_i915_perfmon_set_config	set_config;
+		struct drm_i915_perfmon_load_config	load_config;
+		struct drm_i915_perfmon_get_hw_ctx_id	get_hw_ctx_id;
+		struct drm_i915_perfmon_get_hw_ctx_ids	get_hw_ctx_ids;
+		struct drm_i915_perfmon_pin_oa_buffer	pin_oa_buffer;
+		struct drm_i915_perfmon_unpin_oa_buffer	unpin_oa_buffer;
+	} data;
+};
+
+#endif	/* _I915_PERFMON_H_ */
-- 
1.7.5.4

