From 059fab51cbf0146ba1f7168c180756c315ece416 Mon Sep 17 00:00:00 2001
From: Andrzej Datczuk <andrzej.datczuk@intel.com>
Date: Mon, 16 May 2016 15:25:38 -0700
Subject: [PATCH 1379/2508] drm/i915: Perfmon support for gen9

commit ec40833bad7fe08696ad4c184f495466609aaf65 from
https://github.com/01org/linux-apollolake-i

This patch implements Perfmon IOCTLs for gen9 (BXT).
Changes are required for supporting MDAPI used by such
tools as GPA.

Based on Android implementation.

Perfmon open/close IOCTL
Perfmon must be opened in order to set global perfmon
configuration and to get list of HW context IDs for arbitrary
context.

Reporting IOCTL of HW context
Enables user to get hardware context id for given context
(identified by user handle) as well as to query for list
of hardware context ids for arbitrary process (identified
by PID). This is needed for matching Perfmon sample from HW
with application/process/context. Perfmon must be opened
for given fd to enable caller to get list of HW context
IDs for arbitrary context.

Writing OA / Perfmon configuration to ring buffer
Some registers for OA configuration are part of context and
thus need to be written via LRIs inserted to the ring buffer.
It is convenient to send all OA configuration registers this way
since it enables us to implement multiconfiguration, meaning
multiple users each using different OA config. To have this
working programming of per-context workaround batch buffers
is required.

Storing process ID information in intel_context.

Permitting user to pin/unpin a buffer using
I915_PERFMON_PIN_OA_BUFFER / I915_PERFMON_UNPIN_OA_BUFFER.

Signed-off-by: Andrzej Datczuk <andrzej.datczuk@intel.com>
Signed-off-by: Matt Roper <matthew.d.roper@intel.com>
(cherry picked from commit 2cabedb47a0313acaeda1c3f506c4e8b7641d073)
Signed-off-by: Matt Roper <matthew.d.roper@intel.com>

Conflicts:
	drivers/gpu/drm/i915/i915_dma.c
	drivers/gpu/drm/i915/i915_drv.h
	drivers/gpu/drm/i915/intel_lrc.c
---
 drivers/gpu/drm/i915/Makefile            |    3 +
 drivers/gpu/drm/i915/i915_dma.c          |    9 +
 drivers/gpu/drm/i915/i915_drv.h          |   27 +
 drivers/gpu/drm/i915/i915_gem_context.c  |    7 +
 drivers/gpu/drm/i915/i915_perfmon.c      |  906 ++++++++++++++++++++++++++++++
 drivers/gpu/drm/i915/i915_perfmon_defs.h |   62 ++
 drivers/gpu/drm/i915/i915_reg.h          |    5 +
 drivers/gpu/drm/i915/intel_lrc.c         |  158 ++++++
 include/uapi/drm/i915_drm.h              |    4 +
 include/uapi/drm/i915_perfmon.h          |  124 ++++
 10 files changed, 1305 insertions(+), 0 deletions(-)
 create mode 100644 drivers/gpu/drm/i915/i915_perfmon.c
 create mode 100644 drivers/gpu/drm/i915/i915_perfmon_defs.h
 create mode 100644 include/uapi/drm/i915_perfmon.h

diff --git a/drivers/gpu/drm/i915/Makefile b/drivers/gpu/drm/i915/Makefile
index 8e9661b..902d44c 100644
--- a/drivers/gpu/drm/i915/Makefile
+++ b/drivers/gpu/drm/i915/Makefile
@@ -101,6 +101,9 @@ i915-y += i915_vgpu.o
 # legacy horrors
 i915-y += i915_dma.o
 
+# perfmon
+i915-y += i915_perfmon.o
+
 obj-$(CONFIG_DRM_I915)  += i915.o
 
 CFLAGS_i915_trace_points.o := -I$(src)
diff --git a/drivers/gpu/drm/i915/i915_dma.c b/drivers/gpu/drm/i915/i915_dma.c
index 05cf51b..967d11c 100644
--- a/drivers/gpu/drm/i915/i915_dma.c
+++ b/drivers/gpu/drm/i915/i915_dma.c
@@ -1031,6 +1031,8 @@ static int i915_driver_init_early(struct drm_i915_private *dev_priv,
 	mutex_init(&dev_priv->av_mutex);
 	mutex_init(&dev_priv->wm.wm_mutex);
 	mutex_init(&dev_priv->pps_mutex);
+	mutex_init(&dev_priv->perfmon.config.lock);
+	mutex_init(&dev_priv->rc6_wa_bb.lock);
 
 	ret = i915_workqueues_init(dev_priv);
 	if (ret < 0)
@@ -1392,6 +1394,9 @@ int i915_driver_load(struct drm_device *dev, unsigned long flags)
 
 	intel_runtime_pm_put(dev_priv);
 
+	/* Initialize all the resources for perf monitoring */
+	i915_perfmon_setup(dev_priv);
+
 	dev_priv->profile.driver_load = sched_clock() - start_tm;
 
 	printk(KERN_INFO "IOTG i915 forklift 2016-03-25\n");
@@ -1410,6 +1415,9 @@ out_runtime_pm_put:
 out_free_priv:
 	i915_load_error(dev_priv, "Device initialization failed (%d)\n", ret);
 
+	/* Clean up all the resources for perf monitoring */
+	i915_perfmon_cleanup(dev_priv);
+
 	kfree(dev_priv);
 
 	return ret;
@@ -1591,6 +1599,7 @@ const struct drm_ioctl_desc i915_ioctls[] = {
 	DRM_IOCTL_DEF_DRV(I915_GEM_USERPTR, i915_gem_userptr_ioctl, DRM_RENDER_ALLOW),
 	DRM_IOCTL_DEF_DRV(I915_GEM_CONTEXT_GETPARAM, i915_gem_context_getparam_ioctl, DRM_RENDER_ALLOW),
 	DRM_IOCTL_DEF_DRV(I915_GEM_CONTEXT_SETPARAM, i915_gem_context_setparam_ioctl, DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(I915_PERFMON, i915_perfmon_ioctl, DRM_UNLOCKED),
 };
 
 int i915_max_ioctl = ARRAY_SIZE(i915_ioctls);
diff --git a/drivers/gpu/drm/i915/i915_drv.h b/drivers/gpu/drm/i915/i915_drv.h
index 1f3f990..8a6f8c2 100644
--- a/drivers/gpu/drm/i915/i915_drv.h
+++ b/drivers/gpu/drm/i915/i915_drv.h
@@ -56,6 +56,8 @@
 #include "intel_dpll_mgr.h"
 #include <linux/fence.h>
 
+#include "i915_perfmon_defs.h"
+
 /* General customization:
  */
 
@@ -379,6 +381,8 @@ struct drm_i915_file_private {
 	unsigned int bsd_ring;
 
 	u32 scheduler_queue_length;
+
+	struct drm_i915_perfmon_file perfmon;
 };
 
 /* Used by dp and fdi links */
@@ -878,6 +882,7 @@ struct i915_fence_timeline {
  */
 struct intel_context {
 	struct kref ref;
+	struct pid *pid;
 	int user_handle;
 	uint8_t remap_slice;
 	struct drm_i915_private *i915;
@@ -907,6 +912,9 @@ struct intel_context {
 	} engine[I915_NUM_ENGINES];
 
 	struct list_head link;
+
+	/* perfmon configuration */
+	struct drm_i915_perfmon_context perfmon;
 };
 
 enum fb_op_origin {
@@ -1929,6 +1937,16 @@ struct drm_i915_private {
 
 	int dpio_phy_iosf_port[I915_NUM_PHYS_VLV];
 
+	struct drm_i915_perfmon_device perfmon;
+
+	struct {
+		struct drm_i915_gem_object *obj;
+		unsigned long offset;
+		void *address;
+		atomic_t enable;
+		struct mutex lock;
+	} rc6_wa_bb;
+
 	struct i915_workarounds workarounds;
 
 	/* Reclocking support */
@@ -3548,6 +3566,15 @@ int i915_reg_read_ioctl(struct drm_device *dev, void *data,
 int i915_get_reset_stats_ioctl(struct drm_device *dev, void *data,
 			       struct drm_file *file);
 
+/* i915_perfmon.c */
+int i915_perfmon_ioctl(struct drm_device *dev, void *data, struct drm_file *file);
+void i915_perfmon_setup(struct drm_i915_private *dev_priv);
+void i915_perfmon_cleanup(struct drm_i915_private *dev_priv);
+void i915_perfmon_ctx_setup(struct intel_context *ctx);
+void i915_perfmon_ctx_cleanup(struct intel_context *ctx);
+int i915_perfmon_update_workaround_bb(struct drm_i915_private *dev_priv,
+				      struct drm_i915_perfmon_config *config);
+
 /* overlay */
 extern struct intel_overlay_error_state *intel_overlay_capture_error_state(struct drm_device *dev);
 extern void intel_overlay_print_error_state(struct drm_i915_error_state_buf *e,
diff --git a/drivers/gpu/drm/i915/i915_gem_context.c b/drivers/gpu/drm/i915/i915_gem_context.c
index e06a9e9..b4b3838 100644
--- a/drivers/gpu/drm/i915/i915_gem_context.c
+++ b/drivers/gpu/drm/i915/i915_gem_context.c
@@ -168,6 +168,10 @@ void i915_gem_context_free(struct kref *ctx_ref)
 
 	if (ctx->legacy_hw_ctx.rcs_state)
 		drm_gem_object_unreference(&ctx->legacy_hw_ctx.rcs_state->base);
+
+	put_pid(ctx->pid);
+	i915_perfmon_ctx_cleanup(ctx);
+
 	list_del(&ctx->link);
 	kfree(ctx);
 }
@@ -327,6 +331,9 @@ i915_gem_create_context(struct drm_device *dev,
 
 	trace_i915_context_create(ctx);
 
+	ctx->pid = get_pid(task_tgid(current));
+	i915_perfmon_ctx_setup(ctx);
+
 	return ctx;
 
 err_unpin:
diff --git a/drivers/gpu/drm/i915/i915_perfmon.c b/drivers/gpu/drm/i915/i915_perfmon.c
new file mode 100644
index 0000000..6356a99
--- /dev/null
+++ b/drivers/gpu/drm/i915/i915_perfmon.c
@@ -0,0 +1,906 @@
+/*
+ * Copyright  2013 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ *
+ */
+#include <drm/i915_drm.h>
+#include "i915_drv.h"
+#include "i915_trace.h"
+#include "intel_drv.h"
+#include "linux/wait.h"
+#include "i915_perfmon_defs.h"
+
+/**
+ * i915_get_render_hw_ctx_id
+ *
+ * Get render engine HW context ID for given context. This is the
+ * representation of context in the HW. This is *not* context ID as referenced
+ * by usermode. For legacy submission path this is logical ring context address.
+ * For execlist this is the kernel managed context ID written to execlist
+ * descriptor.
+ */
+static int  i915_get_render_hw_ctx_id(
+	struct drm_i915_private *dev_priv,
+	struct intel_context *ctx,
+	__u32 *id)
+{
+	struct drm_i915_gem_object *ctx_obj = ctx->legacy_hw_ctx.rcs_state;
+
+	if (!ctx_obj)
+		return -ENOENT;
+
+	*id = i915_gem_obj_ggtt_offset(ctx_obj) >> 12;
+
+	return 0;
+}
+
+/**
+ * i915_perfmon_get_hw_ctx_id
+ *
+ * Get HW context ID for given context ID and DRM file.
+ */
+static int i915_perfmon_get_hw_ctx_id(
+	struct drm_device *dev,
+	struct drm_file *file,
+	struct drm_i915_perfmon_get_hw_ctx_id *ioctl_data)
+{
+	struct drm_i915_private *dev_priv =
+		(struct drm_i915_private *) dev->dev_private;
+	struct intel_context *ctx;
+	struct drm_i915_file_private *file_priv = file->driver_priv;
+	int ret;
+
+	if (!HAS_HW_CONTEXTS(dev))
+		return -ENODEV;
+
+	ret = i915_mutex_lock_interruptible(dev);
+	if (ret)
+		return ret;
+
+	ctx = i915_gem_context_get(file_priv, ioctl_data->ctx_id);
+	if (IS_ERR_OR_NULL(ctx))
+		ret = -ENOENT;
+	else
+		ret = i915_get_render_hw_ctx_id(dev_priv, ctx,
+			&ioctl_data->hw_ctx_id);
+
+	mutex_unlock(&dev->struct_mutex);
+	return ret;
+}
+
+struct i915_perfmon_hw_ctx_list {
+	__u32 *ids;
+	__u32 capacity;
+	__u32 size;
+	__u32 iterations_left;
+};
+
+/**
+ * process_context
+ *
+ * Check if context referenced by 'ptr' belongs to application with
+ * provided process ID. If so, increment total number of contexts
+ * found (list->size) and add context id to the list if
+ * its capacity is not reached.
+ */
+static int process_context(struct drm_i915_private *dev_priv,
+	struct intel_context *ctx,
+	__u32 pid,
+	struct i915_perfmon_hw_ctx_list *list)
+{
+	bool ctx_match;
+	bool has_render_ring;
+	__u32 id;
+
+	if (list->iterations_left == 0)
+		return 0;
+	--list->iterations_left;
+
+	ctx_match = (pid == pid_vnr(ctx->pid) ||
+			 pid == 0 ||
+			 ctx == dev_priv->kernel_context);
+
+	if (ctx_match) {
+		has_render_ring =
+			(0 == i915_get_render_hw_ctx_id(
+				dev_priv, ctx, &id));
+	}
+
+	if (ctx_match && has_render_ring) {
+		if (list->size < list->capacity)
+			list->ids[list->size] = id;
+		list->size++;
+	}
+
+	return 0;
+}
+
+/**
+ * i915_perfmon_get_hw_ctx_ids
+ *
+ * Lookup the list of all contexts and return HW context IDs of those
+ * belonging to provided process id.
+ *
+ * User specifies maximum number of IDs to be written to provided block of
+ * memory: ioctl_data->count. Returned is the list of not more than
+ * ioctl_data->count HW context IDs together with total number of matching
+ * contexts found - potentially more than ioctl_data->count.
+ *
+ */
+static int i915_perfmon_get_hw_ctx_ids(
+	struct drm_device *dev,
+	struct drm_i915_perfmon_get_hw_ctx_ids *ioctl_data)
+{
+	struct drm_i915_private *dev_priv =
+		(struct drm_i915_private *) dev->dev_private;
+	struct i915_perfmon_hw_ctx_list list;
+	struct intel_context *ctx;
+	unsigned int ids_to_copy;
+	int ret;
+
+	if (!HAS_HW_CONTEXTS(dev))
+		return -ENODEV;
+
+	if (ioctl_data->count > I915_PERFMON_MAX_HW_CTX_IDS)
+		return -EINVAL;
+
+	list.ids = kzalloc(
+		ioctl_data->count * sizeof(__u32), GFP_KERNEL);
+	if (!list.ids)
+		return -ENOMEM;
+	list.capacity = ioctl_data->count;
+	list.size = 0;
+	list.iterations_left = I915_PERFMON_MAX_HW_CTX_IDS;
+
+	ret = i915_mutex_lock_interruptible(dev);
+	if (ret)
+		goto exit;
+
+	list_for_each_entry(ctx, &dev_priv->context_list, link) {
+		process_context(dev_priv, ctx, ioctl_data->pid, &list);
+	}
+
+	mutex_unlock(&dev->struct_mutex);
+
+	/*
+	 * After we searched all the contexts list.size is the total number
+	 * of contexts matching the query. This is potentially more than
+	 * the capacity of user buffer (list.capacity).
+	 */
+	ids_to_copy = min(list.size, list.capacity);
+	if (copy_to_user(
+		(__u32 __user *)(uintptr_t)ioctl_data->ids,
+		list.ids,
+		ids_to_copy * sizeof(__u32))) {
+		ret = -EFAULT;
+		goto exit;
+	}
+
+	/* Return total number of matching ids to the user. */
+	ioctl_data->count = list.size;
+exit:
+	kfree(list.ids);
+	return ret;
+}
+
+/**
+ * copy_entries
+ *
+ * Helper function to copy OA configuration entries to new destination.
+ *
+ * Source configuration is first validated. In case of success pointer to newly
+ * allocated memory containing copy of source configuration is returned in *out.
+ *
+ */
+static int copy_entries(
+	struct drm_i915_perfmon_config *source,
+	bool user,
+	void **out)
+{
+	size_t size = 0;
+
+	*out = NULL;
+
+	/* basic validation of input */
+	if (source->id == 0 || source->size == 0 || source->entries == NULL)
+		return 0;
+
+	if (source->size > I915_PERFMON_CONFIG_SIZE)
+		return -EINVAL;
+
+	size = source->size  * sizeof(struct drm_i915_perfmon_config_entry);
+
+	*out = kzalloc(
+		   size,
+		   GFP_KERNEL);
+	if (*out == NULL) {
+		DRM_ERROR("failed to allocate configuration buffer\n");
+		return -ENOMEM;
+	}
+
+	if (user) {
+		int ret = copy_from_user(*out, source->entries, size);
+		if (ret) {
+			DRM_ERROR("failed to copy user provided config: %x\n",
+					ret);
+			kfree(*out);
+			*out = NULL;
+			return -EFAULT;
+		}
+	} else
+		memcpy(*out, source->entries, size);
+
+	return 0;
+}
+
+/**
+ * i915_perfmon_copy_config
+ *
+ * Utility function to copy OA and GP configuration to its destination.
+ *
+ * This is first used when global configuration is set by the user by calling
+ * I915_PERFMON_SET_CONFIG and then for the second time (optionally) when user
+ * calls I915_PERFMON_LOAD_CONFIG to copy the configuration from global storage
+ * to his context.
+ *
+ * 'user' boolean value indicates whether pointer to source config is provided
+ * by usermode (I915_PERFMON_SET_CONFIG case).
+ *
+ * If both OA and GP config are provided (!= NULL) then either both are copied
+ * to their respective locations or none of them (which is indicated by return
+ * value != 0).
+ *
+ * target_oa and target_gp are assumed to be non-NULL.
+ *
+ */
+static int i915_perfmon_copy_config(
+	struct drm_i915_private *dev_priv,
+	struct drm_i915_perfmon_config *target_oa,
+	struct drm_i915_perfmon_config *target_gp,
+	struct drm_i915_perfmon_config source_oa,
+	struct drm_i915_perfmon_config source_gp,
+	bool user)
+{
+	void *temp_oa = NULL;
+	void *temp_gp = NULL;
+	int ret = 0;
+
+	BUG_ON(!mutex_is_locked(&dev_priv->perfmon.config.lock));
+
+	/* copy configurations to temporary storage */
+
+	ret = copy_entries(&source_oa, user, &temp_oa);
+	if (ret)
+		return ret;
+	ret = copy_entries(&source_gp, user, &temp_gp);
+	if (ret) {
+		kfree(temp_oa);
+		return ret;
+	}
+
+	/*
+	 * Allocation and copy successful, free old config memory and swap
+	 * pointers
+	 */
+	if (temp_oa) {
+		kfree(target_oa->entries);
+		target_oa->entries = temp_oa;
+		target_oa->id = source_oa.id;
+		target_oa->size = source_oa.size;
+	}
+	if (temp_gp) {
+		kfree(target_gp->entries);
+		target_gp->entries = temp_gp;
+		target_gp->id = source_gp.id;
+		target_gp->size = source_gp.size;
+	}
+
+	return 0;
+}
+
+/**
+ * i915_perfmon_set_config
+ *
+ * Store OA/GP configuration for later use.
+ *
+ * Configuration content is not validated since it is provided by user who had
+ * previously called Perfmon Open with sysadmin privilege level.
+ *
+ */
+static int i915_perfmon_set_config(
+	struct drm_device *dev,
+	struct drm_i915_perfmon_set_config *args)
+{
+	struct drm_i915_private *dev_priv =
+		(struct drm_i915_private *) dev->dev_private;
+	int ret = 0;
+
+	struct drm_i915_perfmon_config user_config_oa;
+	struct drm_i915_perfmon_config user_config_gp;
+
+	if (!(IS_GEN8(dev) || IS_GEN9(dev)))
+		return -EINVAL;
+
+	/* validate target */
+	switch (args->target) {
+	case I915_PERFMON_CONFIG_TARGET_CTX:
+	case I915_PERFMON_CONFIG_TARGET_PID:
+	case I915_PERFMON_CONFIG_TARGET_ALL:
+		/* OK */
+		break;
+	default:
+		DRM_DEBUG("invalid target\n");
+		return -EINVAL;
+	}
+
+	/* setup input for i915_perfmon_copy_config */
+	user_config_oa.id = args->oa.id;
+	user_config_oa.size = args->oa.size;
+	user_config_oa.entries =
+		(struct drm_i915_perfmon_config_entry __user *)
+			(uintptr_t)args->oa.entries;
+
+	user_config_gp.id = args->gp.id;
+	user_config_gp.size = args->gp.size;
+	user_config_gp.entries =
+		(struct drm_i915_perfmon_config_entry __user *)
+			(uintptr_t)args->gp.entries;
+
+	ret = mutex_lock_interruptible(&dev_priv->perfmon.config.lock);
+	if (ret)
+		return ret;
+
+	if (!atomic_read(&dev_priv->perfmon.config.enable)) {
+		ret = -EINVAL;
+		goto unlock_perfmon;
+	}
+
+	ret = i915_perfmon_copy_config(dev_priv,
+			&dev_priv->perfmon.config.oa,
+			&dev_priv->perfmon.config.gp,
+			user_config_oa, user_config_gp,
+			true);
+
+	if (ret)
+		goto unlock_perfmon;
+
+	dev_priv->perfmon.config.target = args->target;
+	dev_priv->perfmon.config.pid = args->pid;
+
+unlock_perfmon:
+	mutex_unlock(&dev_priv->perfmon.config.lock);
+	return ret;
+}
+
+/**
+ * i915_perfmon_load_config
+ *
+ * Copy configuration from global storage to current context.
+ *
+ */
+static int i915_perfmon_load_config(
+	struct drm_device *dev,
+	struct drm_file *file,
+	struct drm_i915_perfmon_load_config *args)
+{
+	struct drm_i915_private *dev_priv =
+		(struct drm_i915_private *) dev->dev_private;
+	struct drm_i915_file_private *file_priv = file->driver_priv;
+	struct intel_context *ctx;
+	struct drm_i915_perfmon_config user_config_oa;
+	struct drm_i915_perfmon_config user_config_gp;
+	int ret;
+
+	if (!(IS_GEN8(dev)) && !(IS_GEN9(dev)))
+		return -EINVAL;
+
+	ret = i915_mutex_lock_interruptible(dev);
+	if (ret)
+		return ret;
+
+	if (!atomic_read(&dev_priv->perfmon.config.enable)) {
+		ret = -EINVAL;
+		goto unlock_dev;
+	}
+
+	ctx = i915_gem_context_get(
+				file_priv,
+				args->ctx_id);
+
+	if (IS_ERR_OR_NULL(ctx)) {
+		DRM_DEBUG("invalid context\n");
+		ret = -EINVAL;
+		goto unlock_dev;
+	}
+
+	ret = mutex_lock_interruptible(&dev_priv->perfmon.config.lock);
+	if (ret)
+		goto unlock_dev;
+
+	user_config_oa = dev_priv->perfmon.config.oa;
+	user_config_gp = dev_priv->perfmon.config.gp;
+
+	/*
+	 * copy configuration to the context only if requested config ID matches
+	 * device configuration ID
+	 */
+	if (!(args->oa_id != 0 &&
+	      args->oa_id == dev_priv->perfmon.config.oa.id))
+		user_config_oa.entries = NULL;
+	if (!(args->gp_id != 0 &&
+	     args->gp_id == dev_priv->perfmon.config.gp.id))
+		user_config_gp.entries = NULL;
+
+	ret = i915_perfmon_copy_config(dev_priv,
+			&ctx->perfmon.config.oa.pending,
+			&ctx->perfmon.config.gp.pending,
+			dev_priv->perfmon.config.oa,
+			dev_priv->perfmon.config.gp,
+			false);
+
+	if (ret)
+		goto unlock_perfmon;
+
+	/*
+	 * return info about what is actualy set for submission in
+	 * target context
+	 */
+	args->gp_id = ctx->perfmon.config.gp.pending.id;
+	args->oa_id = ctx->perfmon.config.oa.pending.id;
+
+unlock_perfmon:
+	mutex_unlock(&dev_priv->perfmon.config.lock);
+unlock_dev:
+	mutex_unlock(&dev->struct_mutex);
+	return ret;
+}
+
+static void *emit_dword(void *mem, __u32 cmd)
+{
+	iowrite32(cmd, mem);
+	return ((__u32 *)mem) + 1;
+}
+
+static void *emit_load_register_imm(void *mem, __u32 reg, __u32 val)
+{
+	mem = emit_dword(mem, MI_NOOP);
+	mem = emit_dword(mem, MI_LOAD_REGISTER_IMM(1));
+	mem = emit_dword(mem, reg);
+	mem = emit_dword(mem, val);
+	return mem;
+}
+
+static void *emit_cs_stall_pipe_control(void *mem)
+{
+	mem = emit_dword(mem, GFX_OP_PIPE_CONTROL(6));
+	mem = emit_dword(mem, PIPE_CONTROL_CS_STALL|PIPE_CONTROL_WRITE_FLUSH|
+			      PIPE_CONTROL_GEN7_GLOBAL_GTT);
+	mem = emit_dword(mem, 0);
+	mem = emit_dword(mem, 0);
+	mem = emit_dword(mem, 0);
+	mem = emit_dword(mem, 0);
+	return mem;
+}
+
+int i915_perfmon_update_workaround_bb(struct drm_i915_private *dev_priv,
+				      struct drm_i915_perfmon_config *config)
+{
+	const size_t commands_size = 6 + /* pipe control */
+				     config->size * 4 + /* NOOP + LRI */
+				     6 + /* pipe control */
+				     1;  /* BB end */
+	void *buffer_tail;
+	unsigned int i = 0;
+	int ret = 0;
+
+	if (commands_size > PAGE_SIZE) {
+		DRM_ERROR("OA cfg too long to fit into workarond BB\n");
+		return -ENOSPC;
+	}
+
+	BUG_ON(!mutex_is_locked(&dev_priv->perfmon.config.lock));
+
+	if (atomic_read(&dev_priv->perfmon.config.enable) == 0 ||
+	    !dev_priv->rc6_wa_bb.obj) {
+		DRM_ERROR("not ready to write WA BB commands\n");
+		return -EINVAL;
+	}
+
+	ret = mutex_lock_interruptible(&dev_priv->rc6_wa_bb.lock);
+	if (ret)
+		return ret;
+
+	if (!dev_priv->rc6_wa_bb.obj) {
+		mutex_unlock(&dev_priv->rc6_wa_bb.lock);
+		return 0;
+	}
+
+	/* disable RC6 WA BB */
+	I915_WRITE(GEN8_RC6_WA_BB, 0x0);
+
+	buffer_tail = dev_priv->rc6_wa_bb.address;
+	buffer_tail = emit_cs_stall_pipe_control(buffer_tail);
+
+	/* OA/NOA config */
+	for (i = 0; i < config->size; i++)
+		buffer_tail = emit_load_register_imm(
+			buffer_tail,
+			config->entries[i].offset,
+			config->entries[i].value);
+
+	buffer_tail = emit_cs_stall_pipe_control(buffer_tail);
+
+	/* BB END */
+	buffer_tail = emit_dword(buffer_tail, MI_BATCH_BUFFER_END);
+
+	/* enable WA BB */
+	I915_WRITE(GEN8_RC6_WA_BB, dev_priv->rc6_wa_bb.offset | 0x1);
+
+	mutex_unlock(&dev_priv->rc6_wa_bb.lock);
+
+	return 0;
+}
+
+static int allocate_wa_bb(struct drm_i915_private *dev_priv)
+{
+	int ret = 0;
+
+	BUG_ON(!mutex_is_locked(&dev_priv->dev->struct_mutex));
+
+	ret = mutex_lock_interruptible(&dev_priv->rc6_wa_bb.lock);
+	if (ret)
+		return ret;
+
+	if (atomic_inc_return(&dev_priv->rc6_wa_bb.enable) > 1) {
+		mutex_unlock(&dev_priv->rc6_wa_bb.lock);
+		return 0;
+	}
+
+	BUG_ON(dev_priv->rc6_wa_bb.obj != NULL);
+
+	dev_priv->rc6_wa_bb.obj = i915_gem_alloc_object(
+						dev_priv->dev,
+						PAGE_SIZE);
+	if (!dev_priv->rc6_wa_bb.obj) {
+		ret = -ENOMEM;
+		goto unlock;
+	}
+
+	ret = i915_gem_obj_ggtt_pin(
+			dev_priv->rc6_wa_bb.obj,
+			PAGE_SIZE, PIN_MAPPABLE);
+
+	if (ret) {
+		drm_gem_object_unreference_unlocked(
+			&dev_priv->rc6_wa_bb.obj->base);
+		goto unlock;
+	}
+
+	ret = i915_gem_object_set_to_gtt_domain(dev_priv->rc6_wa_bb.obj,
+						true);
+	if (ret) {
+		i915_gem_object_ggtt_unpin(dev_priv->rc6_wa_bb.obj);
+		drm_gem_object_unreference_unlocked(
+			&dev_priv->rc6_wa_bb.obj->base);
+		goto unlock;
+	}
+
+	dev_priv->rc6_wa_bb.offset = i915_gem_obj_ggtt_offset(
+						dev_priv->rc6_wa_bb.obj);
+
+	dev_priv->rc6_wa_bb.address = ioremap_wc(
+		dev_priv->ggtt.mappable_base + dev_priv->rc6_wa_bb.offset,
+		PAGE_SIZE);
+
+	if (!dev_priv->rc6_wa_bb.address) {
+		i915_gem_object_ggtt_unpin(dev_priv->rc6_wa_bb.obj);
+		drm_gem_object_unreference_unlocked(
+			&dev_priv->rc6_wa_bb.obj->base);
+		ret =  -ENOMEM;
+		goto unlock;
+	}
+
+	DRM_DEBUG("RC6 WA BB, offset %lx address %p",
+		  dev_priv->rc6_wa_bb.offset,
+		  dev_priv->rc6_wa_bb.address
+		);
+
+	memset(dev_priv->rc6_wa_bb.address, 0, PAGE_SIZE);
+
+unlock:
+	if (ret) {
+		dev_priv->rc6_wa_bb.obj = NULL;
+		dev_priv->rc6_wa_bb.offset = 0;
+	}
+	mutex_unlock(&dev_priv->rc6_wa_bb.lock);
+
+	return ret;
+}
+
+static void deallocate_wa_bb(struct drm_i915_private *dev_priv)
+{
+	BUG_ON(!mutex_is_locked(&dev_priv->dev->struct_mutex));
+
+	mutex_lock(&dev_priv->rc6_wa_bb.lock);
+
+	if (atomic_read(&dev_priv->rc6_wa_bb.enable) == 0)
+		goto unlock;
+
+	if (atomic_dec_return(&dev_priv->rc6_wa_bb.enable) > 1)
+		goto unlock;
+
+	I915_WRITE(GEN8_RC6_WA_BB, 0);
+
+	if (dev_priv->rc6_wa_bb.obj != NULL) {
+		iounmap(dev_priv->rc6_wa_bb.address);
+		i915_gem_object_ggtt_unpin(dev_priv->rc6_wa_bb.obj);
+		drm_gem_object_unreference(&dev_priv->rc6_wa_bb.obj->base);
+		dev_priv->rc6_wa_bb.obj = NULL;
+		dev_priv->rc6_wa_bb.offset = 0;
+	}
+unlock:
+	mutex_unlock(&dev_priv->rc6_wa_bb.lock);
+}
+
+/**
+* i915_perfmon_config_enable_disable
+*
+* Enable/disable OA/GP configuration transport.
+*/
+static int i915_perfmon_config_enable_disable(
+	struct drm_device *dev,
+	int enable)
+{
+	int ret;
+
+	struct drm_i915_private *dev_priv =
+		(struct drm_i915_private *) dev->dev_private;
+
+	if (!(IS_GEN8(dev)) && !(IS_GEN9(dev)))
+		return -EINVAL;
+
+	ret = i915_mutex_lock_interruptible(dev_priv->dev);
+	if (ret)
+		return ret;
+
+	ret = mutex_lock_interruptible(&dev_priv->perfmon.config.lock);
+	if (ret) {
+		mutex_unlock(&dev->struct_mutex);
+		return ret;
+	}
+
+	if (enable) {
+		ret = allocate_wa_bb(dev_priv);
+		if (!ret &&
+		    atomic_inc_return(&dev_priv->perfmon.config.enable) == 1) {
+			dev_priv->perfmon.config.target =
+				I915_PERFMON_CONFIG_TARGET_ALL;
+			dev_priv->perfmon.config.oa.id = 0;
+			dev_priv->perfmon.config.gp.id = 0;
+		}
+	} else if (atomic_read(&dev_priv->perfmon.config.enable)) {
+		atomic_dec(&dev_priv->perfmon.config.enable);
+		deallocate_wa_bb(dev_priv);
+	}
+
+	mutex_unlock(&dev_priv->perfmon.config.lock);
+	mutex_unlock(&dev->struct_mutex);
+
+	return ret;
+}
+
+
+/**
+ * i915_perfmon_open
+ *
+ * open perfmon for current file
+ */
+static int i915_perfmon_open(
+	struct drm_file *file)
+{
+	struct drm_i915_file_private *file_priv = file->driver_priv;
+	int ret = 0;
+
+	if (!capable(CAP_SYS_ADMIN))
+		ret = -EACCES;
+	else
+		file_priv->perfmon.opened = true;
+
+	return ret;
+}
+
+/**
+ * i915_perfmon_close
+ *
+ * close perfmon for current file
+ */
+static int i915_perfmon_close(
+	struct drm_file *file)
+{
+	struct drm_i915_file_private *file_priv = file->driver_priv;
+
+	file_priv->perfmon.opened = false;
+
+	return 0;
+}
+
+
+int i915_perfmon_pin_oa_buffer(struct drm_device *dev,
+	struct drm_file *file,
+	struct drm_i915_perfmon_pin_oa_buffer* oa_buffer)
+{
+	int ret = 0;
+	struct drm_i915_gem_object *obj = NULL;
+
+	ret = i915_mutex_lock_interruptible(dev);
+	if (ret)
+		return ret;
+
+	obj = to_intel_bo(drm_gem_object_lookup(dev, file, oa_buffer->handle));
+	if (&obj->base == NULL) {
+		ret = -ENOENT;
+		goto unlock;
+	}
+
+	ret = i915_gem_obj_ggtt_pin(obj, oa_buffer->alignment, PIN_MAPPABLE);
+
+	if (ret == 0)
+		oa_buffer->offset = i915_gem_obj_ggtt_offset(obj);
+
+	drm_gem_object_unreference(&obj->base);
+unlock:
+	mutex_unlock(&dev->struct_mutex);
+
+	return ret;
+}
+
+int i915_perfmon_unpin_oa_buffer(struct drm_device *dev,
+	struct drm_file *file,
+	struct drm_i915_perfmon_unpin_oa_buffer* oa_buffer)
+{
+	int ret = 0;
+	struct drm_i915_gem_object *obj = NULL;
+
+	ret = i915_mutex_lock_interruptible(dev);
+	if (ret)
+		return ret;
+
+	obj = to_intel_bo(drm_gem_object_lookup(dev, file, oa_buffer->handle));
+	if (&obj->base == NULL) {
+		ret = -ENOENT;
+		goto unlock;
+	}
+
+	i915_gem_object_ggtt_unpin(obj);
+	drm_gem_object_unreference(&obj->base);
+unlock:
+	mutex_unlock(&dev->struct_mutex);
+	return ret;
+}
+
+/**
+ * i915_perfmon_ioctl - performance monitoring support
+ *
+ * Main entry point to performance monitoring support
+ * IOCTLs.
+ */
+int i915_perfmon_ioctl(struct drm_device *dev, void *data,
+	struct drm_file *file)
+{
+	struct drm_i915_perfmon *perfmon = data;
+	struct drm_i915_file_private *file_priv = file->driver_priv;
+	int ret = 0;
+
+	switch (perfmon->op) {
+	case I915_PERFMON_OPEN:
+		ret = i915_perfmon_open(file);
+		break;
+
+	case I915_PERFMON_CLOSE:
+		ret = i915_perfmon_close(file);
+		break;
+	case I915_PERFMON_ENABLE_CONFIG:
+		ret = i915_perfmon_config_enable_disable(dev, 1);
+		break;
+
+	case I915_PERFMON_DISABLE_CONFIG:
+		ret = i915_perfmon_config_enable_disable(dev, 0);
+		break;
+
+	case I915_PERFMON_SET_CONFIG:
+		if (!file_priv->perfmon.opened) {
+			ret = -EACCES;
+			break;
+		}
+		ret = i915_perfmon_set_config(
+			dev,
+			&perfmon->data.set_config);
+		break;
+
+	case I915_PERFMON_LOAD_CONFIG:
+		ret = i915_perfmon_load_config(
+			dev,
+			file,
+			&perfmon->data.load_config);
+		break;
+
+	case I915_PERFMON_GET_HW_CTX_ID:
+		ret = i915_perfmon_get_hw_ctx_id(
+			dev,
+			file,
+			&perfmon->data.get_hw_ctx_id);
+		break;
+
+	case I915_PERFMON_GET_HW_CTX_IDS:
+		if (!file_priv->perfmon.opened) {
+			ret = -EACCES;
+			break;
+		}
+		ret = i915_perfmon_get_hw_ctx_ids(
+			dev,
+			&perfmon->data.get_hw_ctx_ids);
+		break;
+	case I915_PERFMON_PIN_OA_BUFFER:
+		ret = i915_perfmon_pin_oa_buffer(
+			dev,
+			file,
+			&perfmon->data.pin_oa_buffer);
+		break;
+	case I915_PERFMON_UNPIN_OA_BUFFER:
+		ret = i915_perfmon_unpin_oa_buffer(
+			dev,
+			file,
+			&perfmon->data.unpin_oa_buffer);
+		break;
+	default:
+		DRM_DEBUG("UNKNOWN OP\n");
+		/* unknown operation */
+		ret = -EINVAL;
+		break;
+	}
+
+	return ret;
+}
+
+void i915_perfmon_setup(struct drm_i915_private *dev_priv)
+{
+	atomic_set(&dev_priv->perfmon.config.enable, 0);
+	dev_priv->perfmon.config.oa.entries = NULL;
+	dev_priv->perfmon.config.gp.entries = NULL;
+}
+
+void i915_perfmon_cleanup(struct drm_i915_private *dev_priv)
+{
+	kfree(dev_priv->perfmon.config.oa.entries);
+	kfree(dev_priv->perfmon.config.gp.entries);
+}
+
+void i915_perfmon_ctx_setup(struct intel_context *ctx)
+{
+	ctx->perfmon.config.oa.pending.entries = NULL;
+	ctx->perfmon.config.gp.pending.entries = NULL;
+}
+
+void i915_perfmon_ctx_cleanup(struct intel_context *ctx)
+{
+	kfree(ctx->perfmon.config.oa.pending.entries);
+	kfree(ctx->perfmon.config.gp.pending.entries);
+}
diff --git a/drivers/gpu/drm/i915/i915_perfmon_defs.h b/drivers/gpu/drm/i915/i915_perfmon_defs.h
new file mode 100644
index 0000000..300f3e5
--- /dev/null
+++ b/drivers/gpu/drm/i915/i915_perfmon_defs.h
@@ -0,0 +1,62 @@
+/*
+ * Copyright  2014 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ */
+
+#ifndef _I915_PERFMON_DEFS_H_
+#define _I915_PERFMON_DEFS_H_
+
+struct drm_i915_perfmon_config {
+	struct drm_i915_perfmon_config_entry *entries;
+	__u32 size;
+	__u32  id;
+};
+
+struct drm_i915_perfmon_context {
+	struct {
+		struct {
+			struct drm_i915_perfmon_config pending;
+			__u32 submitted_id;
+		} oa, gp;
+	} config;
+};
+
+struct drm_i915_perfmon_device {
+	/* perfmon interrupt support */
+	wait_queue_head_t	buffer_queue;
+	atomic_t		buffer_interrupts;
+
+	/* perfmon counters configuration */
+	struct {
+		struct drm_i915_perfmon_config oa;
+		struct drm_i915_perfmon_config gp;
+		enum DRM_I915_PERFMON_CONFIG_TARGET target;
+		pid_t pid;
+		atomic_t enable;
+		struct mutex lock;
+	} config;
+};
+
+struct drm_i915_perfmon_file {
+	bool opened;
+};
+
+#endif	/* _I915_PERFMON_DEFS_H_ */
diff --git a/drivers/gpu/drm/i915/i915_reg.h b/drivers/gpu/drm/i915/i915_reg.h
index 242d45f..31c9773 100644
--- a/drivers/gpu/drm/i915/i915_reg.h
+++ b/drivers/gpu/drm/i915/i915_reg.h
@@ -2032,6 +2032,9 @@ enum skl_disp_power_wells {
 #define GEN9_EU_DISABLE(slice)		_MMIO(0x9134 + (slice)*0x4)
 
 #define GEN6_BSD_SLEEP_PSMI_CONTROL	_MMIO(0x12050)
+#define GEN8_RC6_WA_BB			_MMIO(0x2058)
+#define GEN8_OA_CTX_CONTROL		0x2360
+
 #define   GEN6_BSD_SLEEP_MSG_DISABLE	(1 << 0)
 #define   GEN6_BSD_SLEEP_FLUSH_DISABLE	(1 << 2)
 #define   GEN6_BSD_SLEEP_INDICATOR	(1 << 3)
@@ -5996,6 +5999,8 @@ enum skl_disp_power_wells {
 #define GEN8_PCU_IIR _MMIO(0x444e8)
 #define GEN8_PCU_IER _MMIO(0x444ec)
 
+#define GEN8_OA_IMR  0x2b20
+
 #define ILK_DISPLAY_CHICKEN2	_MMIO(0x42004)
 /* Required on all Ironlake and Sandybridge according to the B-Spec. */
 #define  ILK_ELPIN_409_SELECT	(1 << 25)
diff --git a/drivers/gpu/drm/i915/intel_lrc.c b/drivers/gpu/drm/i915/intel_lrc.c
index 80221bd..4e034e6 100644
--- a/drivers/gpu/drm/i915/intel_lrc.c
+++ b/drivers/gpu/drm/i915/intel_lrc.c
@@ -233,6 +233,158 @@ static void lrc_setup_hardware_status_page(struct intel_engine_cs *engine,
 					   struct drm_i915_gem_object *default_ctx_obj);
 
 
+
+static void perfmon_send_config(
+		struct intel_ringbuffer *ringbuf,
+		struct drm_i915_perfmon_config *config)
+{
+	int i;
+
+	for (i = 0; i < config->size; i++) {
+		DRM_DEBUG("perfmon config %x reg:%05x val:%08x\n",
+			config->id,
+			config->entries[i].offset,
+			config->entries[i].value);
+		intel_logical_ring_emit(ringbuf, MI_NOOP);
+		intel_logical_ring_emit(ringbuf, MI_LOAD_REGISTER_IMM(1));
+		intel_logical_ring_emit(ringbuf, config->entries[i].offset);
+		intel_logical_ring_emit(ringbuf, config->entries[i].value);
+	}
+
+}
+
+static inline struct drm_i915_perfmon_config *get_perfmon_config(
+		struct drm_i915_private *dev_priv,
+		struct intel_context *ctx,
+		struct drm_i915_perfmon_config *config_global,
+		struct drm_i915_perfmon_config *config_context,
+		__u32 ctx_submitted_config_id)
+
+{
+	struct drm_i915_perfmon_config *config  = NULL;
+	enum DRM_I915_PERFMON_CONFIG_TARGET target;
+
+	BUG_ON(!mutex_is_locked(&dev_priv->perfmon.config.lock));
+
+	target = dev_priv->perfmon.config.target;
+	switch (target) {
+	case I915_PERFMON_CONFIG_TARGET_CTX:
+		config = config_context;
+		break;
+	case I915_PERFMON_CONFIG_TARGET_PID:
+		if (pid_vnr(ctx->pid) == dev_priv->perfmon.config.pid)
+			config = config_global;
+		break;
+	case I915_PERFMON_CONFIG_TARGET_ALL:
+		config = config_global;
+		break;
+	default:
+		BUG_ON(1);
+		break;
+	}
+
+	if (config != NULL) {
+		if (config->size == 0 || config->id == 0) {
+			/* configuration is empty or targets other context */
+			DRM_DEBUG("perfmon configuration empty\n");
+			config = NULL;
+		} else if (config->id == ctx_submitted_config_id) {
+			/* configuration is already submitted in this context*/
+			DRM_DEBUG("perfmon configuration %x is submitted\n",
+				config->id);
+			config = NULL;
+		}
+	}
+
+	if (config != NULL)
+		DRM_DEBUG("perfmon configuration TARGET:%u SIZE:%x ID:%x",
+			target,
+			config->size,
+			config->id);
+
+	return config;
+}
+
+static inline int i915_program_perfmon(struct drm_device *dev,
+		struct intel_ringbuffer *ringbuf,
+		struct drm_i915_gem_request* req,
+		struct intel_context *ctx)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_perfmon_config *config_oa, *config_gp;
+
+	size_t size;
+	int ret = 0;
+
+	if (!atomic_read(&dev_priv->perfmon.config.enable) &&
+		ctx->perfmon.config.oa.submitted_id == 0)
+		return 0;
+
+	ret = mutex_lock_interruptible(&dev_priv->perfmon.config.lock);
+
+	if (ret)
+		return ret;
+
+	if (!atomic_read(&dev_priv->perfmon.config.enable)) {
+		if (ctx->perfmon.config.oa.submitted_id != 0) {
+			/* write 0 to OA_CTX_CONTROL to stop counters */
+			ret = intel_logical_ring_begin(req, 4);
+			if (!ret) {
+				intel_logical_ring_emit(ringbuf, MI_NOOP);
+				intel_logical_ring_emit(ringbuf,
+					MI_LOAD_REGISTER_IMM(1));
+				intel_logical_ring_emit(ringbuf,
+					GEN8_OA_CTX_CONTROL);
+				intel_logical_ring_emit(ringbuf, 0);
+				intel_logical_ring_advance(ringbuf);
+			}
+			ctx->perfmon.config.oa.submitted_id = 0;
+		}
+		goto unlock;
+	}
+
+	/* check for pending OA config */
+	config_oa = get_perfmon_config(dev_priv, ctx,
+				       &dev_priv->perfmon.config.oa,
+				       &ctx->perfmon.config.oa.pending,
+				       ctx->perfmon.config.oa.submitted_id);
+
+	/* check for pending PERFMON config */
+	config_gp = get_perfmon_config(dev_priv, ctx,
+				       &dev_priv->perfmon.config.gp,
+				       &ctx->perfmon.config.gp.pending,
+				       ctx->perfmon.config.gp.submitted_id);
+
+	size = (config_oa ? config_oa->size : 0) +
+		(config_gp ? config_gp->size : 0);
+
+	if (size == 0)
+		goto unlock;
+
+	ret = intel_logical_ring_begin(req, 4 * size);
+	if (ret)
+		goto unlock;
+
+	/* submit pending OA config */
+	if (config_oa) {
+		perfmon_send_config(ringbuf, config_oa);
+		ctx->perfmon.config.oa.submitted_id = config_oa->id;
+
+		i915_perfmon_update_workaround_bb(dev_priv, config_oa);
+	}
+
+	/* submit pending general purpose perfmon counters config */
+	if (config_gp) {
+		perfmon_send_config(ringbuf, config_gp);
+		ctx->perfmon.config.gp.submitted_id = config_gp->id;
+	}
+	intel_logical_ring_advance(ringbuf);
+
+unlock:
+	mutex_unlock(&dev_priv->perfmon.config.lock);
+	return ret;
+}
+
 /**
  * intel_sanitize_enable_execlists() - sanitize i915.enable_execlists
  * @dev: DRM device.
@@ -1212,6 +1364,7 @@ emit_relconsts_mode(struct i915_execbuffer_params *params)
  */
 int intel_execlists_submission_final(struct i915_execbuffer_params *params)
 {
+	struct drm_device *dev = params->dev;
 	struct drm_i915_private *dev_priv = params->dev->dev_private;
 	struct drm_i915_gem_request *req = params->request;
 	struct intel_ringbuffer *ringbuf = req->ringbuf;
@@ -1307,6 +1460,11 @@ int intel_execlists_submission_final(struct i915_execbuffer_params *params)
 		if (engine == &dev_priv->engine[RCS])
 			emit_relconsts_mode(params);
 
+		if ((IS_GEN8(dev) || IS_GEN9(dev)) &&
+		    (engine == &dev_priv->engine[RCS]))
+			i915_program_perfmon(params->dev, ringbuf,
+					     params->request, params->ctx);
+
 		exec_start = params->batch_obj_vm_offset +
 			     params->args_batch_start_offset;
 
diff --git a/include/uapi/drm/i915_drm.h b/include/uapi/drm/i915_drm.h
index 819752e..8e4ad9c 100644
--- a/include/uapi/drm/i915_drm.h
+++ b/include/uapi/drm/i915_drm.h
@@ -28,6 +28,7 @@
 #define _UAPI_I915_DRM_H_
 
 #include <drm/drm.h>
+#include <drm/i915_perfmon.h>
 
 /* Please note that modifications to all structs defined here are
  * subject to backwards-compatibility constraints.
@@ -231,6 +232,8 @@ typedef struct _drm_i915_sarea {
 #define DRM_I915_GEM_CONTEXT_GETPARAM	0x34
 #define DRM_I915_GEM_CONTEXT_SETPARAM	0x35
 
+#define DRM_I915_PERFMON		0x3e
+
 #define DRM_IOCTL_I915_INIT		DRM_IOW( DRM_COMMAND_BASE + DRM_I915_INIT, drm_i915_init_t)
 #define DRM_IOCTL_I915_FLUSH		DRM_IO ( DRM_COMMAND_BASE + DRM_I915_FLUSH)
 #define DRM_IOCTL_I915_FLIP		DRM_IO ( DRM_COMMAND_BASE + DRM_I915_FLIP)
@@ -283,6 +286,7 @@ typedef struct _drm_i915_sarea {
 #define DRM_IOCTL_I915_GEM_USERPTR			DRM_IOWR (DRM_COMMAND_BASE + DRM_I915_GEM_USERPTR, struct drm_i915_gem_userptr)
 #define DRM_IOCTL_I915_GEM_CONTEXT_GETPARAM	DRM_IOWR (DRM_COMMAND_BASE + DRM_I915_GEM_CONTEXT_GETPARAM, struct drm_i915_gem_context_param)
 #define DRM_IOCTL_I915_GEM_CONTEXT_SETPARAM	DRM_IOWR (DRM_COMMAND_BASE + DRM_I915_GEM_CONTEXT_SETPARAM, struct drm_i915_gem_context_param)
+#define DRM_IOCTL_I915_PERFMON 			DRM_IOWR (DRM_COMMAND_BASE + DRM_I915_PERFMON, struct drm_i915_perfmon)
 
 /* Allow drivers to submit batchbuffers directly to hardware, relying
  * on the security mechanisms provided by hardware.
diff --git a/include/uapi/drm/i915_perfmon.h b/include/uapi/drm/i915_perfmon.h
new file mode 100644
index 0000000..d252ca3
--- /dev/null
+++ b/include/uapi/drm/i915_perfmon.h
@@ -0,0 +1,124 @@
+/*
+ * Copyright  2013 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ */
+
+#ifndef _I915_PERFMON_H_
+#define _I915_PERFMON_H_
+
+#define I915_PERFMON_IOCTL_VERSION	5
+
+struct drm_i915_perfmon_config_entry {
+	__u32 offset;
+	__u32 value;
+};
+
+static const unsigned int I915_PERFMON_CONFIG_SIZE = 256;
+
+/* Explicitly aligned to 8 bytes to avoid mismatch
+   between 64-bit KM and 32-bit UM. */
+typedef __u64 drm_i915_perfmon_shared_ptr __aligned(8);
+
+struct drm_i915_perfmon_user_config {
+	/* This is pointer to struct drm_i915_perfmon_config_entry.*/
+	drm_i915_perfmon_shared_ptr entries;
+	__u32 size;
+	__u32 id;
+};
+
+enum DRM_I915_PERFMON_CONFIG_TARGET {
+	I915_PERFMON_CONFIG_TARGET_CTX,
+	I915_PERFMON_CONFIG_TARGET_PID,
+	I915_PERFMON_CONFIG_TARGET_ALL,
+};
+
+struct drm_i915_perfmon_set_config {
+	enum DRM_I915_PERFMON_CONFIG_TARGET target;
+	struct drm_i915_perfmon_user_config oa;
+	struct drm_i915_perfmon_user_config gp;
+	__u32 pid;
+};
+
+struct drm_i915_perfmon_load_config {
+	__u32 ctx_id;
+	__u32 oa_id;
+	__u32 gp_id;
+};
+
+
+static const unsigned int I915_PERFMON_MAX_HW_CTX_IDS = 1024;
+
+struct drm_i915_perfmon_get_hw_ctx_ids {
+	__u32 pid;
+	__u32 count;
+	 /* This is pointer to __u32. */
+	drm_i915_perfmon_shared_ptr ids;
+};
+
+struct drm_i915_perfmon_get_hw_ctx_id {
+	__u32 ctx_id;
+	__u32 hw_ctx_id;
+};
+
+struct drm_i915_perfmon_pin_oa_buffer {
+	/** Handle of the buffer to be pinned. */
+	__u32 handle;
+	__u32 pad;
+
+	/** alignment required within the aperture */
+	__u64 alignment;
+
+	/** Returned GTT offset of the buffer. */
+	__u64 offset;
+};
+
+struct drm_i915_perfmon_unpin_oa_buffer {
+	/** Handle of the buffer to be pinned. */
+	__u32 handle;
+	__u32 pad;
+};
+
+enum I915_PERFMON_IOCTL_OP {
+	I915_PERFMON_OPEN = 8,
+	I915_PERFMON_CLOSE,
+	I915_PERFMON_ENABLE_CONFIG,
+	I915_PERFMON_DISABLE_CONFIG,
+	I915_PERFMON_SET_CONFIG,
+	I915_PERFMON_LOAD_CONFIG,
+	I915_PERFMON_GET_HW_CTX_ID,
+	I915_PERFMON_GET_HW_CTX_IDS,
+	I915_PERFMON_PIN_OA_BUFFER,
+	I915_PERFMON_UNPIN_OA_BUFFER,
+};
+
+struct drm_i915_perfmon {
+	enum I915_PERFMON_IOCTL_OP op;
+	union {
+		struct drm_i915_perfmon_set_config	set_config;
+		struct drm_i915_perfmon_load_config	load_config;
+		struct drm_i915_perfmon_get_hw_ctx_id	get_hw_ctx_id;
+		struct drm_i915_perfmon_get_hw_ctx_ids	get_hw_ctx_ids;
+		struct drm_i915_perfmon_pin_oa_buffer	pin_oa_buffer;
+		struct drm_i915_perfmon_unpin_oa_buffer	unpin_oa_buffer;
+	} data;
+};
+
+#endif	/* _I915_PERFMON_H_ */
-- 
1.7.5.4

