From 142dae6da0618abdf231bf77875b8f9785caf12f Mon Sep 17 00:00:00 2001
From: Guoqing Jiang <Guoqing.Jiang@windriver.com>
Date: Fri, 24 Jun 2011 18:22:23 +0800
Subject: [PATCH 2/2] TI816x usb: add usb host support

Add usb host support for 816x, these modifications include
cppi dma support, different interrupt handle mechanism compare
to other omap chips and multiple musb host support etc.

This patch refers to below commits in arago git tree.
git://arago-project.org/git/projects/linux-omap3.git

c5b09e ti816x:usb use pre-defined IRQ defintion for usb
240935 usb: add enable usb platform initialization for ti816x
16c487 usb: added multiple musb instances support
daa03a usb: disable musb multipoint support for ti816x
fe1a16 usb: added multiple musb instances support
844411 usb: configure the usb-mode register for hostmode for ti816x
21afd1 usb-netra-usb-port-added-the-platform-init
d4f4d6 usb: musb dual instance and cppi41 dma fixes
4bf4ac musb: cppi41: adding musb CPPI41 DMA support (Sergei's take3)
59c92d cppi41: Move CPPI41 core to musb temporarily
bc09c9 cppi41: fix moduler build for musb driver
e4e048 cppi41: add cppi41_exit() to free up the memories
f5dbd6 cppi41: fix RGN0 memory alloc in queue_mgr_init()
fbfeff cppi41: fix scheduler table init in dma_block_init()
14faa7 cppi41: device fix from Ravi
56992b cppi41: teardown fix from Ravi
7aa425 usb: musb added dma queueing support for netra
e36583 resolving the compiler errors while building 2.6.34 (merge commit)
e54612 musb: Revert musb ti816x patches
4c4ef0 cppi41: fix macro definition in cppi41.h
3aa03f cppi41: change number of DMA BLOCK from
3243cb usb: cppi41 dma queue boost for host and device mode
35c397 cppi41: fix channel program for actual len
eef608 usb:musb removing compiler warning
d5a962 usb:musb DMA queue-logic revised(2)
d704ae usb: cleanup debug prints musb driver
adbab9 musb: cppi41: Remove platform definitions from cppi41_dma.h
49e4d2 usb:musb adding netra support cppi41-dma
737653 Removing compiler error cppidma.h patch
ed2d5b musb: Add workqueue for URB giveback
ad55f7 musb: Revert "otg timer cleanups"
32c3b9 USB: MUSB: save hardware revision at init
5a7b78 WP + DISPING patches
09e3f6 musb: cppi: ISO IN fixes for CPPI DMA
192dd7 3517: musb: workaround for bytewise read issue
b637f3 usb:netra adding netra usb platform file am_netra.c
8d7041 usb: added unaligned fifo read acess ti816x.c
828f8e usb:musb cpp41 can support dma queue
7f3787 usb: musb removed netra tags in ti816x.c
bc80f4 usb:musb added usb phycontrol for ti816xx
f49509 usb: added usb clock support for TI816x platform
549efb usb: enable frame threshold logic for usb subsystem and misc fixes
f19d2e usb: disable the otg enable in utmi phy regs to enable host mode
4ed0b1 usb: excluding the musb read issue seen omap/shiva platform
3e515f usb:musb ti816x.h removed unused definitions, cleanup
54ad80 usb:netra adding platform header file ti816x.h

Integrated-by: Guoqing Jiang <Guoqing.Jiang@windriver.com>
---
 arch/arm/mach-omap2/usb-musb.c |   81 ++-
 drivers/usb/musb/Kconfig       |   10 +
 drivers/usb/musb/Makefile      |    8 +
 drivers/usb/musb/cppi41.c      |  870 ++++++++++++++++++++++++++
 drivers/usb/musb/cppi41.h      |  727 ++++++++++++++++++++++
 drivers/usb/musb/cppi41_dma.c  | 1341 ++++++++++++++++++++++++++++++++++++++++
 drivers/usb/musb/cppi41_dma.h  |   69 ++
 drivers/usb/musb/musb_core.c   |  208 ++++++-
 drivers/usb/musb/musb_core.h   |   42 ++-
 drivers/usb/musb/musb_dma.h    |    6 +
 drivers/usb/musb/musb_host.c   |   98 +++-
 drivers/usb/musb/musb_host.h   |    2 +
 drivers/usb/musb/musb_io.h     |   19 +-
 drivers/usb/musb/musb_regs.h   |    1 +
 drivers/usb/musb/ti816x.c      | 1227 ++++++++++++++++++++++++++++++++++++
 drivers/usb/musb/ti816x.h      |  143 +++++
 16 files changed, 4821 insertions(+), 31 deletions(-)
 create mode 100644 drivers/usb/musb/cppi41.c
 create mode 100644 drivers/usb/musb/cppi41.h
 create mode 100644 drivers/usb/musb/cppi41_dma.c
 create mode 100644 drivers/usb/musb/cppi41_dma.h
 create mode 100644 drivers/usb/musb/ti816x.c
 create mode 100644 drivers/usb/musb/ti816x.h

diff --git a/arch/arm/mach-omap2/usb-musb.c b/arch/arm/mach-omap2/usb-musb.c
index 96f6787..02d1f5b 100644
--- a/arch/arm/mach-omap2/usb-musb.c
+++ b/arch/arm/mach-omap2/usb-musb.c
@@ -30,9 +30,18 @@
 #include <mach/irqs.h>
 #include <plat/mux.h>
 #include <plat/usb.h>
+#include <plat/irqs-ti816x.h>
 
 #ifdef CONFIG_USB_MUSB_SOC
 
+#define MULTI_MUSB_INSTANCE
+
+#ifdef MULTI_MUSB_INSTANCE
+#define MAX_MUSB_CONTROLLERS	2
+#else
+#define MAX_MUSB_CONTROLLERS	1
+#endif
+
 static struct resource musb_resources[] = {
 	[0] = { /* start and end set dynamically */
 		.flags	= IORESOURCE_MEM,
@@ -42,13 +51,16 @@ static struct resource musb_resources[] = {
 		.flags	= IORESOURCE_IRQ,
 	},
 	[2] = {	/* DMA IRQ */
+		.flags	= IORESOURCE_MEM,
+	},
+	[3] = {	/* DMA IRQ */
 		.start	= INT_243X_HS_USB_DMA,
 		.flags	= IORESOURCE_IRQ,
 	},
 };
 
 static struct musb_hdrc_config musb_config = {
-	.multipoint	= 1,
+	.multipoint	= 0,
 	.dyn_fifo	= 1,
 	.num_eps	= 16,
 	.ram_bits	= 12,
@@ -74,43 +86,84 @@ static struct musb_hdrc_platform_data musb_plat = {
 
 static u64 musb_dmamask = DMA_BIT_MASK(32);
 
-static struct platform_device musb_device = {
-	.name		= "musb_hdrc",
-	.id		= -1,
-	.dev = {
-		.dma_mask		= &musb_dmamask,
-		.coherent_dma_mask	= DMA_BIT_MASK(32),
-		.platform_data		= &musb_plat,
+static struct platform_device musb_devices[] = {
+	{
+		.name		= "musb_hdrc",
+		.id		= 0,
+		.dev = {
+			.dma_mask		= &musb_dmamask,
+			.coherent_dma_mask	= DMA_BIT_MASK(32),
+			.platform_data		= &musb_plat,
+		},
+		.num_resources	= ARRAY_SIZE(musb_resources)/2,
+		.resource	= &musb_resources[0],
+	},
+	{
+		.name		= "musb_hdrc",
+		.id		= 1,
+		.dev = {
+			.dma_mask		= &musb_dmamask,
+			.coherent_dma_mask	= DMA_BIT_MASK(32),
+			.platform_data		= &musb_plat,
+		},
+		.num_resources	= ARRAY_SIZE(musb_resources)/2,
+		.resource	= &musb_resources[2],
 	},
-	.num_resources	= ARRAY_SIZE(musb_resources),
-	.resource	= musb_resources,
 };
 
+struct clk *usbotg_clk;
+
 void __init usb_musb_init(struct omap_musb_board_data *board_data)
 {
+	int i;
+
+	usbotg_clk = clk_get(NULL, "usbotg_ick");
+	if (IS_ERR(usbotg_clk)) {
+		pr_err("usb : Failed to get usbotg clock\n");
+		return ;
+	}
+
+	if (clk_enable(usbotg_clk)) {
+		pr_err("usb : Clock Enable Failed\n");
+		return ;
+	}
+
 	if (cpu_is_omap243x()) {
 		musb_resources[0].start = OMAP243X_HS_BASE;
+		musb_resources[0].end = musb_resources[0].start + SZ_4K - 1;
 	} else if (cpu_is_omap34xx()) {
 		musb_resources[0].start = OMAP34XX_HSUSB_OTG_BASE;
+		musb_resources[0].end = musb_resources[0].start + SZ_4K - 1;
 	} else if (cpu_is_omap44xx()) {
 		musb_resources[0].start = OMAP44XX_HSUSB_OTG_BASE;
 		musb_resources[1].start = OMAP44XX_IRQ_HS_USB_MC_N;
 		musb_resources[2].start = OMAP44XX_IRQ_HS_USB_DMA_N;
+		musb_resources[0].end = musb_resources[0].start + SZ_4K - 1;
+	} else if (cpu_is_ti816x()) {
+		musb_resources[0].start = TI816X_USB0_BASE;
+		musb_resources[1].start = TI816X_IRQ_USB0;
+		musb_resources[0].end = musb_resources[0].start + SZ_2K - 1;
+#ifdef MULTI_MUSB_INSTANCE
+		musb_resources[2].start = TI816X_USB1_BASE;
+		musb_resources[3].start = TI816X_IRQ_USB1;
+		musb_resources[2].end = musb_resources[2].start + SZ_2K - 1;
+#endif
 	}
-	musb_resources[0].end = musb_resources[0].start + SZ_4K - 1;
 
 	/*
 	 * REVISIT: This line can be removed once all the platforms using
 	 * musb_core.c have been converted to use use clkdev.
 	 */
-	musb_plat.clock = "ick";
+	musb_plat.clock = "usbotg_ick";
 	musb_plat.board_data = board_data;
 	musb_plat.power = board_data->power >> 1;
 	musb_plat.mode = board_data->mode;
 	musb_plat.extvbus = board_data->extvbus;
 
-	if (platform_device_register(&musb_device) < 0)
-		printk(KERN_ERR "Unable to register HS-USB (MUSB) device\n");
+	for (i = 0; i < MAX_MUSB_CONTROLLERS; i++) {
+		if (platform_device_register(&musb_devices[i]) < 0)
+			printk(KERN_ERR "Unable to register HS-USB (MUSB) device\n");
+	}
 }
 
 #else
diff --git a/drivers/usb/musb/Kconfig b/drivers/usb/musb/Kconfig
index f518339..c970e3e 100644
--- a/drivers/usb/musb/Kconfig
+++ b/drivers/usb/musb/Kconfig
@@ -11,6 +11,7 @@ config USB_MUSB_HDRC
 	depends on (USB || USB_GADGET)
 	depends on (ARM || (BF54x && !BF544) || (BF52x && !BF522 && !BF523))
 	select NOP_USB_XCEIV if (ARCH_DAVINCI || MACH_OMAP3EVM || BLACKFIN)
+	select NOP_USB_XCEIV if ARCH_TI816X
 	select TWL4030_USB if MACH_OMAP_3430SDP
 	select USB_OTG_UTILS
 	tristate 'Inventra Highspeed Dual Role Controller (TI, ADI, ...)'
@@ -38,6 +39,7 @@ config USB_MUSB_SOC
 	default y if ARCH_DAVINCI
 	default y if ARCH_OMAP2430
 	default y if ARCH_OMAP3
+	default y if ARCH_TI816X
 	default y if (BF54x && !BF544)
 	default y if (BF52x && !BF522 && !BF523)
 
@@ -179,6 +181,14 @@ config USB_TI_CPPI_DMA
 	help
 	  Enable DMA transfers when TI CPPI DMA is available.
 
+config USB_TI_CPPI41_DMA
+	bool
+	depends on USB_MUSB_HDRC && !MUSB_PIO_ONLY
+	default ARCH_TI816X
+	select CPPI41
+	help
+	  Enable DMA transfers when TI CPPI 4.1 DMA is available.
+
 config USB_TUSB_OMAP_DMA
 	bool
 	depends on USB_MUSB_HDRC && !MUSB_PIO_ONLY
diff --git a/drivers/usb/musb/Makefile b/drivers/usb/musb/Makefile
index 3a485da..be55877 100644
--- a/drivers/usb/musb/Makefile
+++ b/drivers/usb/musb/Makefile
@@ -18,6 +18,10 @@ ifeq ($(CONFIG_ARCH_OMAP2430),y)
 	musb_hdrc-objs	+= omap2430.o
 endif
 
+ifeq ($(CONFIG_ARCH_TI816X),y)
+       musb_hdrc-objs  += ti816x.o
+endif
+
 ifeq ($(CONFIG_ARCH_OMAP3430),y)
 	musb_hdrc-objs	+= omap2430.o
 endif
@@ -53,9 +57,13 @@ ifneq ($(CONFIG_MUSB_PIO_ONLY),y)
       musb_hdrc-objs		+= cppi_dma.o
 
     else
+      ifeq ($(CONFIG_USB_TI_CPPI41_DMA),y)
+	musb_hdrc-objs        += cppi41_dma.o cppi41.o
+
       ifeq ($(CONFIG_USB_TUSB_OMAP_DMA),y)
         musb_hdrc-objs		+= tusb6010_omap.o
 
+	endif
       endif
     endif
   endif
diff --git a/drivers/usb/musb/cppi41.c b/drivers/usb/musb/cppi41.c
new file mode 100644
index 0000000..8152f9c
--- /dev/null
+++ b/drivers/usb/musb/cppi41.c
@@ -0,0 +1,870 @@
+/*
+ * CPPI 4.1 support
+ *
+ * Copyright (C) 2008-2009 MontaVista Software, Inc. <source@mvista.com>
+ *
+ * Based on the PAL CPPI 4.1 implementation
+ * Copyright (C) 1998-2006 Texas Instruments Incorporated
+ *
+ * This file contains the main implementation for CPPI 4.1 common peripherals,
+ * including the DMA Controllers and the Queue Managers.
+ *
+ * This program is free software; you can distribute it and/or modify it
+ * under the terms of the GNU General Public License (Version 2) as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * for more details.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write to the Free Software Foundation, Inc.,
+ * 59 Temple Place - Suite 330, Boston MA 02111-1307, USA.
+ *
+ */
+
+#include <linux/io.h>
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/dma-mapping.h>
+#include <linux/slab.h>
+
+#include "cppi41.h"
+
+#undef	CPPI41_DEBUG
+
+#ifdef	CPPI41_DEBUG
+#define DBG(format, args...) printk(format, ##args)
+#else
+#define DBG(format, args...)
+#endif
+
+static struct {
+	void *virt_addr;
+	dma_addr_t phys_addr;
+	u32	size;
+} linking_ram[CPPI41_NUM_QUEUE_MGR];
+
+static u32 *allocated_queues[CPPI41_NUM_QUEUE_MGR];
+
+/* First 32 packet descriptors are reserved for unallocated memory regions. */
+static u32 next_desc_index[CPPI41_NUM_QUEUE_MGR] = { 1 << 5 };
+static u8  next_mem_rgn[CPPI41_NUM_QUEUE_MGR];
+
+static struct {
+	size_t rgn_size;
+	void *virt_addr;
+	dma_addr_t phys_addr;
+	struct cppi41_queue_obj queue_obj;
+	u8 mem_rgn;
+	u16 q_mgr;
+	u16 q_num;
+} dma_teardown[CPPI41_NUM_DMA_BLOCK];
+
+/******************** CPPI 4.1 Functions (External Interface) *****************/
+
+int cppi41_queue_mgr_init(u8 q_mgr, dma_addr_t rgn0_base, u16 rgn0_size)
+{
+	void __iomem *q_mgr_regs;
+	void *ptr;
+
+	if (q_mgr >= cppi41_num_queue_mgr)
+		return -EINVAL;
+
+	q_mgr_regs = cppi41_queue_mgr[q_mgr].q_mgr_rgn_base;
+	ptr = dma_alloc_coherent(NULL, rgn0_size * 4,
+				 &linking_ram[q_mgr].phys_addr,
+				 GFP_KERNEL | GFP_DMA);
+	if (ptr == NULL) {
+		printk(KERN_ERR "ERROR: %s: Unable to allocate "
+		       "linking RAM.\n", __func__);
+		return -ENOMEM;
+	}
+	linking_ram[q_mgr].virt_addr = ptr;
+	linking_ram[q_mgr].size = rgn0_size * 4;
+
+	__raw_writel(linking_ram[q_mgr].phys_addr,
+			q_mgr_regs + QMGR_LINKING_RAM_RGN0_BASE_REG);
+	DBG("Linking RAM region 0 base @ %p, value: %x\n",
+	    q_mgr_regs + QMGR_LINKING_RAM_RGN0_BASE_REG,
+	    __raw_readl(q_mgr_regs + QMGR_LINKING_RAM_RGN0_BASE_REG));
+
+	__raw_writel(rgn0_size, q_mgr_regs + QMGR_LINKING_RAM_RGN0_SIZE_REG);
+	DBG("Linking RAM region 0 size @ %p, value: %x\n",
+	    q_mgr_regs + QMGR_LINKING_RAM_RGN0_SIZE_REG,
+	    __raw_readl(q_mgr_regs + QMGR_LINKING_RAM_RGN0_SIZE_REG));
+
+	ptr = kzalloc(BITS_TO_LONGS(cppi41_queue_mgr[q_mgr].num_queue),
+		      GFP_KERNEL);
+	if (ptr == NULL) {
+		printk(KERN_ERR "ERROR: %s: Unable to allocate queue bitmap.\n",
+		       __func__);
+		dma_free_coherent(NULL, rgn0_size * 4,
+				  linking_ram[q_mgr].virt_addr,
+				  linking_ram[q_mgr].phys_addr);
+		return -ENOMEM;
+	}
+	allocated_queues[q_mgr] = ptr;
+
+	return 0;
+}
+EXPORT_SYMBOL(cppi41_queue_mgr_init);
+
+int cppi41_dma_sched_tbl_init(u8 dma_num, u8 q_mgr,
+			u32 *sched_tbl, u8 tbl_size)
+{
+	struct cppi41_dma_block *dma_block;
+	int num_reg, k, i, val = 0;
+
+	dma_block = (struct cppi41_dma_block *)&cppi41_dma_block[dma_num];
+
+	num_reg = (tbl_size + 3) / 4;
+	for (k = i = 0; i < num_reg; i++) {
+#if 0
+		for (val = j = 0; j < 4; j++, k++) {
+			val >>= 8;
+			if (k < tbl_size)
+				val |= sched_tbl[k] << 24;
+		}
+#endif
+		val = sched_tbl[i];
+		__raw_writel(val, dma_block->sched_table_base +
+			DMA_SCHED_TABLE_WORD_REG(i));
+		DBG("DMA scheduler table @ %p, value written: %x\n",
+		dma_block->sched_table_base + DMA_SCHED_TABLE_WORD_REG(i),
+			val);
+	}
+	return 0;
+}
+EXPORT_SYMBOL(cppi41_dma_sched_tbl_init);
+
+int cppi41_dma_block_init(u8 dma_num, u8 q_mgr, u8 num_order,
+				 u32 *sched_tbl, u8 tbl_size)
+{
+	const struct cppi41_dma_block *dma_block;
+	struct cppi41_teardown_desc *curr_td;
+	dma_addr_t td_addr;
+	unsigned num_desc, num_reg;
+	void *ptr;
+	int error, i;
+	u16 q_num;
+	u32 val;
+
+	if (dma_num >= cppi41_num_dma_block ||
+	    q_mgr >= cppi41_num_queue_mgr ||
+	    !tbl_size || sched_tbl == NULL)
+		return -EINVAL;
+
+	error = cppi41_queue_alloc(CPPI41_FREE_DESC_QUEUE |
+				   CPPI41_UNASSIGNED_QUEUE, q_mgr, &q_num);
+	if (error) {
+		printk(KERN_ERR "ERROR: %s: Unable to allocate teardown "
+		       "descriptor queue.\n", __func__);
+		return error;
+	}
+	DBG("Teardown descriptor queue %d in queue manager 0 "
+	    "allocated\n", q_num);
+
+	/*
+	 * Tell the hardware about the Teardown descriptor
+	 * queue manager and queue number.
+	 */
+	dma_block = &cppi41_dma_block[dma_num];
+	__raw_writel((q_mgr << DMA_TD_DESC_QMGR_SHIFT) |
+		     (q_num << DMA_TD_DESC_QNUM_SHIFT),
+		     dma_block->global_ctrl_base +
+		     DMA_TEARDOWN_FREE_DESC_CTRL_REG);
+	DBG("Teardown free descriptor control @ %p, value: %x\n",
+	    dma_block->global_ctrl_base + DMA_TEARDOWN_FREE_DESC_CTRL_REG,
+	    __raw_readl(dma_block->global_ctrl_base +
+			DMA_TEARDOWN_FREE_DESC_CTRL_REG));
+
+	num_desc = 1 << num_order;
+	dma_teardown[dma_num].rgn_size = num_desc *
+					 sizeof(struct cppi41_teardown_desc);
+
+	/* Pre-allocate teardown descriptors. */
+	ptr = dma_alloc_coherent(NULL, dma_teardown[dma_num].rgn_size,
+				 &dma_teardown[dma_num].phys_addr,
+				 GFP_KERNEL | GFP_DMA);
+	if (ptr == NULL) {
+		printk(KERN_ERR "ERROR: %s: Unable to allocate teardown "
+		       "descriptors.\n", __func__);
+		error = -ENOMEM;
+		goto free_queue;
+	}
+	dma_teardown[dma_num].virt_addr = ptr;
+
+	error = cppi41_mem_rgn_alloc(q_mgr, dma_teardown[dma_num].phys_addr, 5,
+				     num_order, &dma_teardown[dma_num].mem_rgn);
+	if (error) {
+		printk(KERN_ERR "ERROR: %s: Unable to allocate queue manager "
+		       "memory region for teardown descriptors.\n", __func__);
+		goto free_mem;
+	}
+
+	error = cppi41_queue_init(&dma_teardown[dma_num].queue_obj, 0, q_num);
+	if (error) {
+		printk(KERN_ERR "ERROR: %s: Unable to initialize teardown "
+		       "free descriptor queue.\n", __func__);
+		goto free_rgn;
+	}
+	dma_teardown[dma_num].q_num = q_num;
+	dma_teardown[dma_num].q_mgr = q_mgr;
+	/*
+	 * Push all teardown descriptors to the free teardown queue
+	 * for the CPPI 4.1 system.
+	 */
+	curr_td = dma_teardown[dma_num].virt_addr;
+	td_addr = dma_teardown[dma_num].phys_addr;
+
+	for (i = 0; i < num_desc; i++) {
+		cppi41_queue_push(&dma_teardown[dma_num].queue_obj, td_addr,
+				  sizeof(*curr_td), 0);
+		td_addr += sizeof(*curr_td);
+	}
+
+	/* Initialize the DMA scheduler. */
+	num_reg = (tbl_size + 3) / 4;
+	for (i = 0; i < num_reg; i++) {
+		val = sched_tbl[i];
+		__raw_writel(val, dma_block->sched_table_base +
+			     DMA_SCHED_TABLE_WORD_REG(i));
+		DBG("DMA scheduler table @ %p, value written: %x\n",
+		    dma_block->sched_table_base + DMA_SCHED_TABLE_WORD_REG(i),
+		    val);
+	}
+
+	__raw_writel((tbl_size - 1) << DMA_SCHED_LAST_ENTRY_SHIFT |
+		     DMA_SCHED_ENABLE_MASK,
+		     dma_block->sched_ctrl_base + DMA_SCHED_CTRL_REG);
+	DBG("DMA scheduler control @ %p, value: %x\n",
+	    dma_block->sched_ctrl_base + DMA_SCHED_CTRL_REG,
+	    __raw_readl(dma_block->sched_ctrl_base + DMA_SCHED_CTRL_REG));
+
+	return 0;
+
+free_rgn:
+	cppi41_mem_rgn_free(q_mgr, dma_teardown[dma_num].mem_rgn);
+free_mem:
+	dma_free_coherent(NULL, dma_teardown[dma_num].rgn_size,
+			  dma_teardown[dma_num].virt_addr,
+			  dma_teardown[dma_num].phys_addr);
+free_queue:
+	cppi41_queue_free(q_mgr, q_num);
+	return error;
+}
+EXPORT_SYMBOL(cppi41_dma_block_init);
+
+/*
+ * cppi41_mem_rgn_alloc - allocate a memory region within the queue manager
+ */
+int cppi41_mem_rgn_alloc(u8 q_mgr, dma_addr_t rgn_addr, u8 size_order,
+			 u8 num_order, u8 *mem_rgn)
+{
+	void __iomem *desc_mem_regs;
+	u32 num_desc = 1 << num_order, index, ctrl;
+	int rgn;
+
+	DBG("%s called with rgn_addr = %08x, size_order = %d, num_order = %d\n",
+	    __func__, rgn_addr, size_order, num_order);
+
+	if (q_mgr >= cppi41_num_queue_mgr ||
+	    size_order < 5 || size_order > 13 ||
+	    num_order  < 5 || num_order  > 12 ||
+	    (rgn_addr & ((1 << size_order) - 1)))
+		return -EINVAL;
+
+	rgn = next_mem_rgn[q_mgr];
+	index = next_desc_index[q_mgr];
+	if (rgn >= CPPI41_MAX_MEM_RGN || index + num_desc > 0x4000)
+		return -ENOSPC;
+
+	next_mem_rgn[q_mgr] = rgn + 1;
+	next_desc_index[q_mgr] = index + num_desc;
+
+	desc_mem_regs = cppi41_queue_mgr[q_mgr].desc_mem_rgn_base;
+
+	/* Write the base register */
+	__raw_writel(rgn_addr, desc_mem_regs + QMGR_MEM_RGN_BASE_REG(rgn));
+	DBG("Descriptor region base @ %p, value: %x\n",
+	    desc_mem_regs + QMGR_MEM_RGN_BASE_REG(rgn),
+	    __raw_readl(desc_mem_regs + QMGR_MEM_RGN_BASE_REG(rgn)));
+
+	/* Write the control register */
+	ctrl = ((index << QMGR_MEM_RGN_INDEX_SHIFT) &
+		QMGR_MEM_RGN_INDEX_MASK) |
+	       (((size_order - 5) << QMGR_MEM_RGN_DESC_SIZE_SHIFT) &
+		QMGR_MEM_RGN_DESC_SIZE_MASK) |
+	       (((num_order - 5) << QMGR_MEM_RGN_SIZE_SHIFT) &
+		QMGR_MEM_RGN_SIZE_MASK);
+	__raw_writel(ctrl, desc_mem_regs + QMGR_MEM_RGN_CTRL_REG(rgn));
+	DBG("Descriptor region control @ %p, value: %x\n",
+	    desc_mem_regs + QMGR_MEM_RGN_CTRL_REG(rgn),
+	    __raw_readl(desc_mem_regs + QMGR_MEM_RGN_CTRL_REG(rgn)));
+
+	*mem_rgn = rgn;
+	return 0;
+}
+EXPORT_SYMBOL(cppi41_mem_rgn_alloc);
+
+/*
+ * cppi41_mem_rgn_free - free the memory region within the queue manager
+ */
+int cppi41_mem_rgn_free(u8 q_mgr, u8 mem_rgn)
+{
+	void __iomem *desc_mem_regs;
+
+	DBG("%s called.\n", __func__);
+
+	if (q_mgr >= cppi41_num_queue_mgr || mem_rgn >= next_mem_rgn[q_mgr])
+		return -EINVAL;
+
+	desc_mem_regs = cppi41_queue_mgr[q_mgr].desc_mem_rgn_base;
+
+	if (__raw_readl(desc_mem_regs + QMGR_MEM_RGN_BASE_REG(mem_rgn)) == 0)
+		return -ENOENT;
+
+	__raw_writel(0, desc_mem_regs + QMGR_MEM_RGN_BASE_REG(mem_rgn));
+	__raw_writel(0, desc_mem_regs + QMGR_MEM_RGN_CTRL_REG(mem_rgn));
+
+	return 0;
+}
+EXPORT_SYMBOL(cppi41_mem_rgn_free);
+
+/*
+ * cppi41_tx_ch_init - initialize a CPPI 4.1 Tx channel object
+ *
+ * Verify the channel info (range checking, etc.) and store the channel
+ * information within the object structure.
+ */
+int cppi41_tx_ch_init(struct cppi41_dma_ch_obj *tx_ch_obj,
+		      u8 dma_num, u8 ch_num)
+{
+	if (dma_num >= cppi41_num_dma_block ||
+	    ch_num  >= cppi41_dma_block[dma_num].num_tx_ch)
+		return -EINVAL;
+
+	/* Populate the channel object structure */
+	tx_ch_obj->base_addr  = cppi41_dma_block[dma_num].ch_ctrl_stat_base +
+				DMA_CH_TX_GLOBAL_CFG_REG(ch_num);
+	tx_ch_obj->global_cfg = __raw_readl(tx_ch_obj->base_addr);
+	return 0;
+}
+EXPORT_SYMBOL(cppi41_tx_ch_init);
+
+/*
+ * cppi41_rx_ch_init - initialize a CPPI 4.1 Rx channel object
+ *
+ * Verify the channel info (range checking, etc.) and store the channel
+ * information within the object structure.
+ */
+int cppi41_rx_ch_init(struct cppi41_dma_ch_obj *rx_ch_obj,
+		      u8 dma_num, u8 ch_num)
+{
+	if (dma_num >= cppi41_num_dma_block ||
+	    ch_num  >= cppi41_dma_block[dma_num].num_rx_ch)
+		return -EINVAL;
+
+	/* Populate the channel object structure */
+	rx_ch_obj->base_addr  = cppi41_dma_block[dma_num].ch_ctrl_stat_base +
+				DMA_CH_RX_GLOBAL_CFG_REG(ch_num);
+	rx_ch_obj->global_cfg = __raw_readl(rx_ch_obj->base_addr);
+	return 0;
+}
+EXPORT_SYMBOL(cppi41_rx_ch_init);
+
+/*
+ * We have to cache the last written Rx/Tx channel global configration register
+ * value due to its bits other than enable/teardown being write-only. Yet there
+ * is a caveat related to caching the enable bit: this bit may be automatically
+ * cleared as a result of teardown, so we can't trust its cached value!
+ * When modifying the write only register fields, we're making use of the fact
+ * that they read back as zeros, and not clearing them explicitly...
+ */
+
+/*
+ * cppi41_dma_ch_default_queue - set CPPI 4.1 channel default completion queue
+ */
+void cppi41_dma_ch_default_queue(struct cppi41_dma_ch_obj *dma_ch_obj,
+				 u8 q_mgr, u16 q_num)
+{
+	u32 val = dma_ch_obj->global_cfg;
+
+	/* Clear the fields to be modified. */
+	val &= ~(DMA_CH_TX_DEFAULT_QMGR_MASK | DMA_CH_TX_DEFAULT_QNUM_MASK |
+		 DMA_CH_TX_ENABLE_MASK);
+
+	/* Set the default completion queue. */
+	val |= ((q_mgr << DMA_CH_TX_DEFAULT_QMGR_SHIFT) &
+		DMA_CH_TX_DEFAULT_QMGR_MASK) |
+	       ((q_num << DMA_CH_TX_DEFAULT_QNUM_SHIFT) &
+		DMA_CH_TX_DEFAULT_QNUM_MASK);
+
+	/* Get the current state of the enable bit. */
+	dma_ch_obj->global_cfg = val |= __raw_readl(dma_ch_obj->base_addr);
+	__raw_writel(val, dma_ch_obj->base_addr);
+	DBG("Channel global configuration @ %p, value written: %x, "
+	    "value read: %x\n", dma_ch_obj->base_addr, val,
+	    __raw_readl(dma_ch_obj->base_addr));
+
+}
+EXPORT_SYMBOL(cppi41_dma_ch_default_queue);
+
+/*
+ * cppi41_rx_ch_configure - configure CPPI 4.1 Rx channel
+ */
+void cppi41_rx_ch_configure(struct cppi41_dma_ch_obj *rx_ch_obj,
+			    struct cppi41_rx_ch_cfg  *cfg)
+{
+	void __iomem *base = rx_ch_obj->base_addr;
+	u32 val = __raw_readl(rx_ch_obj->base_addr);
+
+	val |= ((cfg->sop_offset << DMA_CH_RX_SOP_OFFSET_SHIFT) &
+		DMA_CH_RX_SOP_OFFSET_MASK) |
+	       ((cfg->default_desc_type << DMA_CH_RX_DEFAULT_DESC_TYPE_SHIFT) &
+		DMA_CH_RX_DEFAULT_DESC_TYPE_MASK) |
+	       ((cfg->retry_starved << DMA_CH_RX_ERROR_HANDLING_SHIFT) &
+		DMA_CH_RX_ERROR_HANDLING_MASK) |
+	       ((cfg->rx_queue.q_mgr << DMA_CH_RX_DEFAULT_RQ_QMGR_SHIFT) &
+		DMA_CH_RX_DEFAULT_RQ_QMGR_MASK) |
+	       ((cfg->rx_queue.q_num << DMA_CH_RX_DEFAULT_RQ_QNUM_SHIFT) &
+		DMA_CH_RX_DEFAULT_RQ_QNUM_MASK);
+
+	rx_ch_obj->global_cfg = val;
+	__raw_writel(val, base);
+	DBG("Rx channel global configuration @ %p, value written: %x, "
+	    "value read: %x\n", base, val, __raw_readl(base));
+
+	base -= DMA_CH_RX_GLOBAL_CFG_REG(0);
+
+	/*
+	 * Set up the packet configuration register
+	 * based on the descriptor type...
+	 */
+	switch (cfg->default_desc_type) {
+	case DMA_CH_RX_DEFAULT_DESC_EMBED:
+		val = ((cfg->cfg.embed_pkt.fd_queue.q_mgr <<
+			DMA_CH_RX_EMBED_FDQ_QMGR_SHIFT) &
+		       DMA_CH_RX_EMBED_FDQ_QMGR_MASK) |
+		      ((cfg->cfg.embed_pkt.fd_queue.q_num <<
+			DMA_CH_RX_EMBED_FDQ_QNUM_SHIFT) &
+		       DMA_CH_RX_EMBED_FDQ_QNUM_MASK) |
+		      ((cfg->cfg.embed_pkt.num_buf_slot <<
+			DMA_CH_RX_EMBED_NUM_SLOT_SHIFT) &
+		       DMA_CH_RX_EMBED_NUM_SLOT_MASK) |
+		      ((cfg->cfg.embed_pkt.sop_slot_num <<
+			DMA_CH_RX_EMBED_SOP_SLOT_SHIFT) &
+		       DMA_CH_RX_EMBED_SOP_SLOT_MASK);
+
+		__raw_writel(val, base + DMA_CH_RX_EMBED_PKT_CFG_REG_B(0));
+		DBG("Rx channel embedded packet configuration B @ %p, "
+		    "value written: %x\n",
+		    base + DMA_CH_RX_EMBED_PKT_CFG_REG_B(0), val);
+
+		val = ((cfg->cfg.embed_pkt.free_buf_pool[0].b_pool <<
+			DMA_CH_RX_EMBED_FBP_PNUM_SHIFT(0)) &
+		       DMA_CH_RX_EMBED_FBP_PNUM_MASK(0)) |
+		      ((cfg->cfg.embed_pkt.free_buf_pool[0].b_mgr <<
+			DMA_CH_RX_EMBED_FBP_BMGR_SHIFT(0)) &
+		       DMA_CH_RX_EMBED_FBP_BMGR_MASK(0)) |
+		      ((cfg->cfg.embed_pkt.free_buf_pool[1].b_pool <<
+			DMA_CH_RX_EMBED_FBP_PNUM_SHIFT(1)) &
+		       DMA_CH_RX_EMBED_FBP_PNUM_MASK(1)) |
+		      ((cfg->cfg.embed_pkt.free_buf_pool[1].b_mgr <<
+			DMA_CH_RX_EMBED_FBP_BMGR_SHIFT(1)) &
+		       DMA_CH_RX_EMBED_FBP_BMGR_MASK(1)) |
+		      ((cfg->cfg.embed_pkt.free_buf_pool[2].b_pool <<
+			DMA_CH_RX_EMBED_FBP_PNUM_SHIFT(2)) &
+		       DMA_CH_RX_EMBED_FBP_PNUM_MASK(2)) |
+		      ((cfg->cfg.embed_pkt.free_buf_pool[2].b_mgr <<
+			DMA_CH_RX_EMBED_FBP_BMGR_SHIFT(2)) &
+		       DMA_CH_RX_EMBED_FBP_BMGR_MASK(2)) |
+		      ((cfg->cfg.embed_pkt.free_buf_pool[3].b_pool <<
+			DMA_CH_RX_EMBED_FBP_PNUM_SHIFT(3)) &
+		       DMA_CH_RX_EMBED_FBP_PNUM_MASK(3)) |
+		      ((cfg->cfg.embed_pkt.free_buf_pool[3].b_mgr <<
+			DMA_CH_RX_EMBED_FBP_BMGR_SHIFT(3)) &
+		       DMA_CH_RX_EMBED_FBP_BMGR_MASK(3));
+
+		__raw_writel(val, base + DMA_CH_RX_EMBED_PKT_CFG_REG_A(0));
+		DBG("Rx channel embedded packet configuration A @ %p, "
+		    "value written: %x\n",
+		    base + DMA_CH_RX_EMBED_PKT_CFG_REG_A(0), val);
+		break;
+	case DMA_CH_RX_DEFAULT_DESC_HOST:
+		val = ((cfg->cfg.host_pkt.fdb_queue[0].q_num <<
+			DMA_CH_RX_HOST_FDQ_QNUM_SHIFT(0)) &
+		       DMA_CH_RX_HOST_FDQ_QNUM_MASK(0)) |
+		      ((cfg->cfg.host_pkt.fdb_queue[0].q_mgr <<
+			DMA_CH_RX_HOST_FDQ_QMGR_SHIFT(0)) &
+		       DMA_CH_RX_HOST_FDQ_QMGR_MASK(0)) |
+		      ((cfg->cfg.host_pkt.fdb_queue[1].q_num <<
+			DMA_CH_RX_HOST_FDQ_QNUM_SHIFT(1)) &
+		       DMA_CH_RX_HOST_FDQ_QNUM_MASK(1)) |
+		      ((cfg->cfg.host_pkt.fdb_queue[1].q_mgr <<
+			DMA_CH_RX_HOST_FDQ_QMGR_SHIFT(1)) &
+		       DMA_CH_RX_HOST_FDQ_QMGR_MASK(1));
+
+		__raw_writel(val, base + DMA_CH_RX_HOST_PKT_CFG_REG_A(0));
+		DBG("Rx channel host packet configuration A @ %p, "
+		    "value written: %x\n",
+		    base + DMA_CH_RX_HOST_PKT_CFG_REG_A(0), val);
+
+		val = ((cfg->cfg.host_pkt.fdb_queue[2].q_num <<
+			DMA_CH_RX_HOST_FDQ_QNUM_SHIFT(2)) &
+		       DMA_CH_RX_HOST_FDQ_QNUM_MASK(2)) |
+		      ((cfg->cfg.host_pkt.fdb_queue[2].q_mgr <<
+			DMA_CH_RX_HOST_FDQ_QMGR_SHIFT(2)) &
+		       DMA_CH_RX_HOST_FDQ_QMGR_MASK(2)) |
+		      ((cfg->cfg.host_pkt.fdb_queue[3].q_num <<
+		       DMA_CH_RX_HOST_FDQ_QNUM_SHIFT(3)) &
+		       DMA_CH_RX_HOST_FDQ_QNUM_MASK(3)) |
+		      ((cfg->cfg.host_pkt.fdb_queue[3].q_mgr <<
+			DMA_CH_RX_HOST_FDQ_QMGR_SHIFT(3)) &
+		       DMA_CH_RX_HOST_FDQ_QMGR_MASK(3));
+
+		__raw_writel(val, base + DMA_CH_RX_HOST_PKT_CFG_REG_B(0));
+		DBG("Rx channel host packet configuration B @ %p, "
+		    "value written: %x\n",
+		    base + DMA_CH_RX_HOST_PKT_CFG_REG_B(0), val);
+		break;
+	case DMA_CH_RX_DEFAULT_DESC_MONO:
+		val = ((cfg->cfg.mono_pkt.fd_queue.q_num <<
+			DMA_CH_RX_MONO_FDQ_QNUM_SHIFT) &
+		       DMA_CH_RX_MONO_FDQ_QNUM_MASK) |
+		      ((cfg->cfg.mono_pkt.fd_queue.q_mgr <<
+			DMA_CH_RX_MONO_FDQ_QMGR_SHIFT) &
+		       DMA_CH_RX_MONO_FDQ_QMGR_MASK) |
+		      ((cfg->cfg.mono_pkt.sop_offset <<
+			DMA_CH_RX_MONO_SOP_OFFSET_SHIFT) &
+		       DMA_CH_RX_MONO_SOP_OFFSET_MASK);
+
+		__raw_writel(val, base + DMA_CH_RX_MONO_PKT_CFG_REG(0));
+		DBG("Rx channel monolithic packet configuration @ %p, "
+		    "value written: %x\n",
+		    base + DMA_CH_RX_MONO_PKT_CFG_REG(0), val);
+		break;
+	}
+}
+EXPORT_SYMBOL(cppi41_rx_ch_configure);
+
+/*
+ * cppi41_dma_ch_teardown - teardown a given Tx/Rx channel
+ */
+void cppi41_dma_ch_teardown(struct cppi41_dma_ch_obj *dma_ch_obj)
+{
+	u32 val = __raw_readl(dma_ch_obj->base_addr);
+
+	/* Initiate channel teardown. */
+	val |= dma_ch_obj->global_cfg & ~DMA_CH_TX_ENABLE_MASK;
+	dma_ch_obj->global_cfg = val |= DMA_CH_TX_TEARDOWN_MASK;
+	__raw_writel(val, dma_ch_obj->base_addr);
+	DBG("Tear down channel @ %p, value written: %x, value read: %x\n",
+	    dma_ch_obj->base_addr, val, __raw_readl(dma_ch_obj->base_addr));
+}
+EXPORT_SYMBOL(cppi41_dma_ch_teardown);
+
+/*
+ * cppi41_dma_ch_enable - enable Tx/Rx DMA channel in hardware
+ *
+ * Makes the channel ready for data transmission/reception.
+ */
+void cppi41_dma_ch_enable(struct cppi41_dma_ch_obj *dma_ch_obj)
+{
+	u32 val = dma_ch_obj->global_cfg | DMA_CH_TX_ENABLE_MASK;
+
+	/* Teardown bit remains set after completion, so clear it now... */
+	dma_ch_obj->global_cfg = val &= ~DMA_CH_TX_TEARDOWN_MASK;
+	__raw_writel(val, dma_ch_obj->base_addr);
+	DBG("Enable channel @ %p, value written: %x, value read: %x\n",
+	    dma_ch_obj->base_addr, val, __raw_readl(dma_ch_obj->base_addr));
+}
+EXPORT_SYMBOL(cppi41_dma_ch_enable);
+
+/*
+ * cppi41_dma_ch_disable - disable Tx/Rx DMA channel in hardware
+ */
+void cppi41_dma_ch_disable(struct cppi41_dma_ch_obj *dma_ch_obj)
+{
+	dma_ch_obj->global_cfg &= ~DMA_CH_TX_ENABLE_MASK;
+	__raw_writel(dma_ch_obj->global_cfg, dma_ch_obj->base_addr);
+	DBG("Disable channel @ %p, value written: %x, value read: %x\n",
+	    dma_ch_obj->base_addr, dma_ch_obj->global_cfg,
+	    __raw_readl(dma_ch_obj->base_addr));
+}
+EXPORT_SYMBOL(cppi41_dma_ch_disable);
+
+void cppi41_free_teardown_queue(int dma_num)
+{
+	unsigned long td_addr;
+
+	while ((td_addr =
+		cppi41_queue_pop(&dma_teardown[dma_num].queue_obj)) != 0)
+		DBG("pop tdDesc(%p) from tdQueue\n", td_addr);
+}
+EXPORT_SYMBOL(cppi41_free_teardown_queue);
+
+void cppi41_exit(void)
+{
+	int i;
+	static int init;
+
+	if (!init) {
+		init = 1;
+		return ;
+	}
+	/*
+	 * pop all the teardwon descriptor queued to tdQueue
+	 */
+	cppi41_free_teardown_queue(0);
+
+	/* Free the teardown completion queue */
+	if (cppi41_queue_free(dma_teardown[0].q_mgr, dma_teardown[0].q_num))
+		DBG(1, "ERROR: failed to free teardown completion queue\n");
+
+	for (i = 0; i < CPPI41_NUM_QUEUE_MGR; i++) {
+		if (linking_ram[i].virt_addr != NULL) {
+			dma_free_coherent(NULL, linking_ram[i].size,
+				linking_ram[i].virt_addr,
+				linking_ram[i].phys_addr);
+			linking_ram[i].virt_addr = 0;
+			linking_ram[i].phys_addr = 0;
+		}
+		if (allocated_queues[i] != NULL) {
+			kfree(allocated_queues[i]);
+			allocated_queues[i] = 0;
+		}
+	}
+	for (i = 0; i < CPPI41_NUM_DMA_BLOCK; i++)
+		if (dma_teardown[i].virt_addr != NULL) {
+
+			cppi41_mem_rgn_free(0,  dma_teardown[i].mem_rgn);
+			dma_free_coherent(NULL, dma_teardown[i].rgn_size,
+					dma_teardown[i].virt_addr,
+					dma_teardown[i].phys_addr);
+			dma_teardown[i].virt_addr = 0;
+			dma_teardown[i].phys_addr = 0;
+		}
+	init = 1;
+}
+EXPORT_SYMBOL(cppi41_exit);
+
+/**
+ * alloc_queue - allocate a queue in the given range
+ * @allocated:	pointer to the bitmap of the allocated queues
+ * @excluded:	pointer to the bitmap of the queues excluded from allocation
+ *		(optional)
+ * @start:	starting queue number
+ * @count:	number of queues available
+ *
+ * Returns queue number on success, -ENOSPC otherwise.
+ */
+static int alloc_queue(u32 *allocated, const u32 *excluded, unsigned start,
+		       unsigned count)
+{
+	u32 bit, mask = 0;
+	int index = -1;
+
+	/*
+	 * We're starting the loop as if we've just wrapped around 32 bits
+	 * in order to save on preloading the bitmasks.
+	 */
+	for (bit = 0; count--; start++, bit <<= 1) {
+		/* Have we just wrapped around 32 bits? */
+		if (!bit) {
+			/* Start over with the next bitmask word */
+			bit = 1;
+			index++;
+			/* Have we just entered the loop? */
+			if (!index) {
+				/* Calculate the starting values */
+				bit <<= start & 0x1f;
+				index = start >> 5;
+			}
+			/*
+			 * Load the next word of the allocated bitmask OR'ing
+			 * it with the excluded bitmask if it's been passed.
+			 */
+			mask = allocated[index];
+			if (excluded != NULL)
+				mask |= excluded[index];
+		}
+		/*
+		 * If the bit in the combined bitmask is zero,
+		 * we've just found a free queue.
+		 */
+		if (!(mask & bit)) {
+			allocated[index] |= bit;
+			return start;
+		}
+	}
+	return -ENOSPC;
+}
+
+/*
+ * cppi41_queue_alloc - allocate a queue of a given type in the queue manager
+ */
+int cppi41_queue_alloc(u8 type, u8 q_mgr, u16 *q_num)
+{
+	int res = -ENOSPC;
+
+	if (q_mgr >= cppi41_num_queue_mgr)
+		return -EINVAL;
+
+	/* Mask out the unsupported queue types */
+	type &= cppi41_queue_mgr[q_mgr].queue_types;
+	/* First see if a free descriptor queue was requested... */
+	if (type & CPPI41_FREE_DESC_QUEUE)
+		res = alloc_queue(allocated_queues[q_mgr], NULL,
+				  cppi41_queue_mgr[q_mgr].base_fdq_num,  16);
+
+	/* Then see if a free descriptor/buffer queue was requested... */
+	if (res < 0 && (type & CPPI41_FREE_DESC_BUF_QUEUE))
+		res = alloc_queue(allocated_queues[q_mgr], NULL,
+				  cppi41_queue_mgr[q_mgr].base_fdbq_num, 16);
+
+	/* Last see if an unassigned queue was requested... */
+	if (res < 0 && (type & CPPI41_UNASSIGNED_QUEUE))
+		res = alloc_queue(allocated_queues[q_mgr],
+				  cppi41_queue_mgr[q_mgr].assigned, 0,
+				  cppi41_queue_mgr[q_mgr].num_queue);
+
+	/* See if any queue was allocated... */
+	if (res < 0)
+		return res;
+
+	/* Return the queue allocated */
+	*q_num = res;
+	return 0;
+}
+EXPORT_SYMBOL(cppi41_queue_alloc);
+
+/*
+ * cppi41_queue_free - free the given queue in the queue manager
+ */
+int cppi41_queue_free(u8 q_mgr, u16 q_num)
+{
+	int index = q_num >> 5, bit = 1 << (q_num & 0x1f);
+
+	if (allocated_queues[q_mgr] != NULL) {
+		if (q_mgr >= cppi41_num_queue_mgr ||
+		    q_num >= cppi41_queue_mgr[q_mgr].num_queue ||
+		    !(allocated_queues[q_mgr][index] & bit))
+			return -EINVAL;
+
+		allocated_queues[q_mgr][index] &= ~bit;
+	}
+	return 0;
+}
+EXPORT_SYMBOL(cppi41_queue_free);
+
+/*
+ * cppi41_queue_init - initialize a CPPI 4.1 queue object
+ */
+int cppi41_queue_init(struct cppi41_queue_obj *queue_obj, u8 q_mgr, u16 q_num)
+{
+	if (q_mgr >= cppi41_num_queue_mgr ||
+	    q_num >= cppi41_queue_mgr[q_mgr].num_queue)
+		return -EINVAL;
+
+	queue_obj->base_addr = cppi41_queue_mgr[q_mgr].q_mgmt_rgn_base +
+			       QMGR_QUEUE_STATUS_REG_A(q_num);
+
+	return 0;
+}
+EXPORT_SYMBOL(cppi41_queue_init);
+
+/*
+ * cppi41_queue_push - push a descriptor into the given queue
+ */
+void cppi41_queue_push(const struct cppi41_queue_obj *queue_obj, u32 desc_addr,
+		       u32 desc_size, u32 pkt_size)
+{
+	u32 val;
+
+	/*
+	 * Write to the tail of the queue.
+	 * TODO: Can't think of a reason why a queue to head may be required.
+	 * If it is, the API may have to be extended.
+	 */
+#if 0
+	/*
+	 * Also, can't understand why packet size is required to queue up a
+	 * descriptor. The spec says packet size *must* be written prior to
+	 * the packet write operation.
+	 */
+	if (pkt_size)
+		val = (pkt_size << QMGR_QUEUE_PKT_SIZE_SHIFT) &
+		      QMGR_QUEUE_PKT_SIZE_MASK;
+	__raw_writel(val, queue_obj->base_addr + QMGR_QUEUE_REG_C(0));
+#endif
+
+	val = (((desc_size - 24) >> (2 - QMGR_QUEUE_DESC_SIZE_SHIFT)) &
+	       QMGR_QUEUE_DESC_SIZE_MASK) |
+	      (desc_addr & QMGR_QUEUE_DESC_PTR_MASK);
+
+	DBG("Pushing value %x to queue @ %p\n", val, queue_obj->base_addr);
+
+	__raw_writel(val, queue_obj->base_addr + QMGR_QUEUE_REG_D(0));
+}
+EXPORT_SYMBOL(cppi41_queue_push);
+
+/*
+ * cppi41_queue_pop - pop a descriptor from a given queue
+ */
+unsigned long cppi41_queue_pop(const struct cppi41_queue_obj *queue_obj)
+{
+	u32 val = __raw_readl(queue_obj->base_addr + QMGR_QUEUE_REG_D(0));
+
+	DBG("Popping value %x from queue @ %p\n", val, queue_obj->base_addr);
+
+	return val & QMGR_QUEUE_DESC_PTR_MASK;
+}
+EXPORT_SYMBOL(cppi41_queue_pop);
+
+/*
+ * cppi41_get_teardown_info - extract information from a teardown descriptor
+ */
+int cppi41_get_teardown_info(unsigned long addr, u32 *info)
+{
+	struct cppi41_teardown_desc *desc;
+	int dma_num;
+
+	for (dma_num = 0; dma_num < cppi41_num_dma_block; dma_num++)
+		if (addr >= dma_teardown[dma_num].phys_addr &&
+		    addr <  dma_teardown[dma_num].phys_addr +
+			    dma_teardown[dma_num].rgn_size)
+			break;
+
+	if (dma_num == cppi41_num_dma_block)
+		return -EINVAL;
+
+	desc = addr - dma_teardown[dma_num].phys_addr +
+	       dma_teardown[dma_num].virt_addr;
+
+	if ((desc->teardown_info & CPPI41_DESC_TYPE_MASK) !=
+	    (CPPI41_DESC_TYPE_TEARDOWN << CPPI41_DESC_TYPE_SHIFT))
+		return -EINVAL;
+
+	*info = desc->teardown_info;
+#if 1
+	/* Hardware is not giving the current DMA number as of now. :-/ */
+	*info |= (dma_num << CPPI41_TEARDOWN_DMA_NUM_SHIFT) &
+		 CPPI41_TEARDOWN_DMA_NUM_MASK;
+#else
+	dma_num = (desc->teardown_info & CPPI41_TEARDOWN_DMA_NUM_MASK) >>
+		 CPPI41_TEARDOWN_DMA_NUM_SHIFT;
+#endif
+
+	cppi41_queue_push(&dma_teardown[dma_num].queue_obj, addr,
+			  sizeof(struct cppi41_teardown_desc), 0);
+
+	return 0;
+}
+EXPORT_SYMBOL(cppi41_get_teardown_info);
+
+MODULE_DESCRIPTION("TI CPPI 4.1 support");
+MODULE_AUTHOR("MontaVista Software");
+MODULE_LICENSE("GPL");
diff --git a/drivers/usb/musb/cppi41.h b/drivers/usb/musb/cppi41.h
new file mode 100644
index 0000000..50e6db5
--- /dev/null
+++ b/drivers/usb/musb/cppi41.h
@@ -0,0 +1,727 @@
+/*
+ * CPPI 4.1 definitions
+ *
+ * Copyright (c) 2008-2009, MontaVista Software, Inc. <source@mvista.com>
+ *
+ * This program is free software; you can distribute it and/or modify it
+ * under the terms of the GNU General Public License (Version 2) as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * for more details.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write to the Free Software Foundation, Inc.,
+ * 59 Temple Place - Suite 330, Boston MA 02111-1307, USA.
+ *
+ */
+
+#include <linux/types.h>
+
+/*
+ * Queue Manager - Control Registers Region
+ */
+#define QMGR_REVISION_REG		0x00	/* Major and minor versions */
+						/* of the module */
+#define QMGR_QUEUE_DIVERSION_REG	0x08	/* Queue Diversion register */
+#define QMGR_FREE_DESC_BUF_STARVED_REG(n) (0x20 + ((n) << 2)) /* Free Desc./ */
+						/* Buffer Starvation Count */
+#define QMGR_FREE_DESC_STARVED_REG(n)	(0x30 + ((n) << 2)) /* Free Desc. */
+						/* Starvation Count */
+#define QMGR_LINKING_RAM_RGN0_BASE_REG	0x80	/* Linking RAM Region 0 Base */
+						/* Address */
+#define QMGR_LINKING_RAM_RGN0_SIZE_REG	0x84	/* Linking RAM Region 0 Size */
+#define QMGR_LINKING_RAM_RGN1_BASE_REG	0x88	/* Linking RAM Region 1 Base */
+						/* Address */
+#define QMGR_QUEUE_PENDING_REG(n)	(0x90 + ((n) << 2)) /* Pending status */
+						/* for all queues */
+
+/*
+ * Queue Manager - Memory Region Registers
+ */
+#define QMGR_MEM_RGN_BASE_REG(r)	(0x00 + ((r) << 4))
+#define QMGR_MEM_RGN_CTRL_REG(r)	(0x04 + ((r) << 4))
+
+/* Memory Region R Control Register bits */
+#define QMGR_MEM_RGN_INDEX_SHIFT	16
+#define QMGR_MEM_RGN_INDEX_MASK		(0x3fff << QMGR_MEM_RGN_INDEX_SHIFT)
+#define QMGR_MEM_RGN_DESC_SIZE_SHIFT	8
+#define QMGR_MEM_RGN_DESC_SIZE_MASK	(0xf << QMGR_MEM_RGN_DESC_SIZE_SHIFT)
+#define QMGR_MEM_RGN_SIZE_SHIFT		0
+#define QMGR_MEM_RGN_SIZE_MASK		(7 << QMGR_MEM_RGN_SIZE_SHIFT)
+
+/*
+ * Queue Manager - Queues Region
+ */
+#define QMGR_QUEUE_REG_A(n)		(0x00 + ((n) << 4))
+#define QMGR_QUEUE_REG_B(n)		(0x04 + ((n) << 4))
+#define QMGR_QUEUE_REG_C(n)		(0x08 + ((n) << 4))
+#define QMGR_QUEUE_REG_D(n)		(0x0C + ((n) << 4))
+
+/* Queue N Register C bits */
+#define QMGR_QUEUE_HEAD_TAIL_SHIFT	31
+#define QMGR_QUEUE_HEAD_TAIL_MASK	(1 << QMGR_QUEUE_HEAD_TAIL_SHIFT)
+#define QMGR_QUEUE_PKT_SIZE_SHIFT	0
+#define QMGR_QUEUE_PKT_SIZE_MASK	(0x3fff << QMGR_QUEUE_PKT_SIZE_SHIFT)
+/* Queue N Register D bits */
+#define QMGR_QUEUE_DESC_PTR_SHIFT	5
+#define QMGR_QUEUE_DESC_PTR_MASK	(0x7ffffff << QMGR_QUEUE_DESC_PTR_SHIFT)
+#define QMGR_QUEUE_DESC_SIZE_SHIFT	0
+#define QMGR_QUEUE_DESC_SIZE_MASK	(0x1f << QMGR_QUEUE_DESC_SIZE_SHIFT)
+
+/*
+ * Queue Manager - Queue Status Region
+ */
+#define QMGR_QUEUE_STATUS_REG_A(n)	(0x00 + ((n) << 4))
+#define QMGR_QUEUE_STATUS_REG_B(n)	(0x04 + ((n) << 4))
+#define QMGR_QUEUE_STATUS_REG_C(n)	(0x08 + ((n) << 4))
+
+/*
+ * DMA Controller - Global Control Registers Region
+ */
+#define DMA_REVISION_REG		0x00	/* Major and minor versions */
+						/* of the module */
+#define DMA_TEARDOWN_FREE_DESC_CTRL_REG 0x04	/* Queue  manager and queue */
+						/* number for Teardown free */
+						/* descriptor queue */
+#define DMA_EMULATION_CTRL_REG		0x08	/* Emulation control register */
+
+/* Teardown Free Descriptor Queue Control Register bits */
+#define DMA_TD_DESC_QMGR_SHIFT		12
+#define DMA_TD_DESC_QMGR_MASK		(3 << DMA_TD_DESC_QMGR_SHIFT)
+#define DMA_TD_DESC_QNUM_SHIFT		0
+#define DMA_TD_DESC_QNUM_MASK		(0xfff << DMA_TD_DESC_QNUM_SHIFT)
+
+/*
+ * DMA Controller - Channel Control / Status Registers Region
+ */
+#define DMA_CH_TX_GLOBAL_CFG_REG(n)	 (0x00 + ((n) << 5))
+#define DMA_CH_RX_GLOBAL_CFG_REG(n)	 (0x08 + ((n) << 5))
+#define DMA_CH_RX_HOST_PKT_CFG_REG_A(n)  (0x0C + ((n) << 5))
+#define DMA_CH_RX_HOST_PKT_CFG_REG_B(n)  (0x10 + ((n) << 5))
+#define DMA_CH_RX_EMBED_PKT_CFG_REG_A(n) (0x14 + ((n) << 5))
+#define DMA_CH_RX_EMBED_PKT_CFG_REG_B(n) (0x18 + ((n) << 5))
+#define DMA_CH_RX_MONO_PKT_CFG_REG(n)	 (0x1C + ((n) << 5))
+
+/* Tx Channel N Global Configuration Register bits */
+#define DMA_CH_TX_ENABLE_SHIFT		31
+#define DMA_CH_TX_ENABLE_MASK		(1 << DMA_CH_TX_ENABLE_SHIFT)
+#define DMA_CH_TX_TEARDOWN_SHIFT	30
+#define DMA_CH_TX_TEARDOWN_MASK		(1 << DMA_CH_TX_TEARDOWN_SHIFT)
+#define DMA_CH_TX_DEFAULT_QMGR_SHIFT	12
+#define DMA_CH_TX_DEFAULT_QMGR_MASK	(3 << DMA_CH_TX_DEFAULT_QMGR_SHIFT)
+#define DMA_CH_TX_DEFAULT_QNUM_SHIFT	0
+#define DMA_CH_TX_DEFAULT_QNUM_MASK	(0xfff << DMA_CH_TX_DEFAULT_QNUM_SHIFT)
+
+/* Rx Channel N Global Configuration Register bits */
+#define DMA_CH_RX_ENABLE_SHIFT		31
+#define DMA_CH_RX_ENABLE_MASK		(1 << DMA_CH_RX_ENABLE_SHIFT)
+#define DMA_CH_RX_TEARDOWN_SHIFT	30
+#define DMA_CH_RX_TEARDOWN_MASK		(1 << DMA_CH_RX_TEARDOWN_SHIFT)
+#define DMA_CH_RX_ERROR_HANDLING_SHIFT	24
+#define DMA_CH_RX_ERROR_HANDLING_MASK	(1 << DMA_CH_RX_ERROR_HANDLING_SHIFT)
+#define DMA_CH_RX_SOP_OFFSET_SHIFT	16
+#define DMA_CH_RX_SOP_OFFSET_MASK	(0xff << DMA_CH_RX_SOP_OFFSET_SHIFT)
+#define DMA_CH_RX_DEFAULT_DESC_TYPE_SHIFT 14
+#define DMA_CH_RX_DEFAULT_DESC_TYPE_MASK  (3 << \
+					   DMA_CH_RX_DEFAULT_DESC_TYPE_SHIFT)
+#define DMA_CH_RX_DEFAULT_DESC_EMBED	0
+#define DMA_CH_RX_DEFAULT_DESC_HOST	1
+#define DMA_CH_RX_DEFAULT_DESC_MONO	2
+#define DMA_CH_RX_DEFAULT_RQ_QMGR_SHIFT 12
+#define DMA_CH_RX_DEFAULT_RQ_QMGR_MASK	(3 << DMA_CH_RX_DEFAULT_RQ_QMGR_SHIFT)
+#define DMA_CH_RX_DEFAULT_RQ_QNUM_SHIFT 0
+#define DMA_CH_RX_DEFAULT_RQ_QNUM_MASK	(0xfff << \
+					 DMA_CH_RX_DEFAULT_RQ_QNUM_SHIFT)
+
+/* Rx Channel N Host Packet Configuration Register A/B bits */
+#define DMA_CH_RX_HOST_FDQ_QMGR_SHIFT(n) (12 + 16 * ((n) & 1))
+#define DMA_CH_RX_HOST_FDQ_QMGR_MASK(n)  (3 << DMA_CH_RX_HOST_FDQ_QMGR_SHIFT(n))
+#define DMA_CH_RX_HOST_FDQ_QNUM_SHIFT(n) (0 + 16 * ((n) & 1))
+#define DMA_CH_RX_HOST_FDQ_QNUM_MASK(n)  (0xfff << \
+					  DMA_CH_RX_HOST_FDQ_QNUM_SHIFT(n))
+
+/* Rx Channel N Embedded Packet Configuration Register A bits */
+#define DMA_CH_RX_EMBED_FBP_BMGR_SHIFT(n) (6 + 8 * (n))
+#define DMA_CH_RX_EMBED_FBP_BMGR_MASK(n)  (3 << \
+					   DMA_CH_RX_EMBED_FBP_BMGR_SHIFT(n))
+#define DMA_CH_RX_EMBED_FBP_PNUM_SHIFT(n) (0 + 8 * (n))
+#define DMA_CH_RX_EMBED_FBP_PNUM_MASK(n)  (0x1f << \
+					   DMA_CH_RX_EMBED_FBP_PNUM_SHIFT(n))
+
+/* Rx Channel N Embedded Packet Configuration Register B bits */
+#define DMA_CH_RX_EMBED_NUM_SLOT_SHIFT	24
+#define DMA_CH_RX_EMBED_NUM_SLOT_MASK	(7 << DMA_CH_RX_EMBED_NUM_SLOT_SHIFT)
+#define DMA_CH_RX_EMBED_SOP_SLOT_SHIFT	16
+#define DMA_CH_RX_EMBED_SOP_SLOT_MASK	(7 << DMA_CH_RX_EMBED_SOP_SLOT_SHIFT)
+#define DMA_CH_RX_EMBED_FDQ_QMGR_SHIFT	12
+#define DMA_CH_RX_EMBED_FDQ_QMGR_MASK	(3 << DMA_CH_RX_EMBED_FDQ_QMGR_SHIFT)
+#define DMA_CH_RX_EMBED_FDQ_QNUM_SHIFT	0
+#define DMA_CH_RX_EMBED_FDQ_QNUM_MASK	(0xfff << \
+					 DMA_CH_RX_EMBED_FDQ_QNUM_SHIFT)
+
+/* Rx Channel N Monolithic Packet Configuration Register bits */
+#define DMA_CH_RX_MONO_SOP_OFFSET_SHIFT 16
+#define DMA_CH_RX_MONO_SOP_OFFSET_MASK	(0xff << \
+					 DMA_CH_RX_MONO_SOP_OFFSET_SHIFT)
+#define DMA_CH_RX_MONO_FDQ_QMGR_SHIFT	12
+#define DMA_CH_RX_MONO_FDQ_QMGR_MASK	(3 << DMA_CH_RX_MONO_FDQ_QMGR_SHIFT)
+#define DMA_CH_RX_MONO_FDQ_QNUM_SHIFT	0
+#define DMA_CH_RX_MONO_FDQ_QNUM_MASK	(0xfff << DMA_CH_RX_MONO_FDQ_QNUM_SHIFT)
+
+/*
+ * DMA Scheduler - Control Region
+ */
+#define DMA_SCHED_CTRL_REG		0x00
+
+/* DMA Scheduler Control Register bits */
+#define DMA_SCHED_ENABLE_SHIFT		31
+#define DMA_SCHED_ENABLE_MASK		(1 << DMA_SCHED_ENABLE_SHIFT)
+#define DMA_SCHED_LAST_ENTRY_SHIFT	0
+#define DMA_SCHED_LAST_ENTRY_MASK	(0xff << DMA_SCHED_LAST_ENTRY_SHIFT)
+
+/*
+ * DMA Scheduler - Table Region
+ */
+#define DMA_SCHED_TABLE_WORD_REG(n)	((n) << 2)
+
+/*
+ * CPPI 4.1 Host Packet Descriptor
+ */
+struct cppi41_host_pkt_desc {
+	u32 desc_info;		/* Descriptor type, protocol specific word */
+				/* count, packet length */
+	u32 tag_info;		/* Source tag (31:16), destination tag (15:0) */
+	u32 pkt_info;		/* Packet error state, type, protocol flags, */
+				/* return info, descriptor location */
+	u32 buf_len;		/* Number of valid data bytes in the buffer */
+	u32 buf_ptr;		/* Pointer to the buffer associated with */
+				/* this descriptor */
+	u32 next_desc_ptr;	/* Pointer to the next buffer descriptor */
+	u32 orig_buf_len;	/* Original buffer length */
+	u32 orig_buf_ptr;	/* Original buffer pointer */
+	u32 stk_comms_info[2];	/* Network stack private communications info */
+};
+
+/*
+ * CPPI 4.1 Host Buffer Descriptor
+ */
+struct cppi41_host_buf_desc {
+	u32 reserved[2];
+	u32 buf_recl_info;	/* Return info, descriptor location */
+	u32 buf_len;		/* Number of valid data bytes in the buffer */
+	u32 buf_ptr;		/* Pointer to the buffer associated with */
+				/* this descriptor */
+	u32 next_desc_ptr;	/* Pointer to the next buffer descriptor */
+	u32 orig_buf_len;	/* Original buffer length */
+	u32 orig_buf_ptr;	/* Original buffer pointer */
+};
+
+#define CPPI41_DESC_TYPE_SHIFT		27
+#define CPPI41_DESC_TYPE_MASK		(0x1f << CPPI41_DESC_TYPE_SHIFT)
+#define CPPI41_DESC_TYPE_HOST		16
+#define CPPI41_DESC_TYPE_MONOLITHIC	18
+#define CPPI41_DESC_TYPE_TEARDOWN	19
+#define CPPI41_PROT_VALID_WORD_CNT_SHIFT 22
+#define CPPI41_PROT_VALID_WORD_CNT_MASK	(0x1f << CPPI41_PROT_WORD_CNT_SHIFT)
+#define CPPI41_PKT_LEN_SHIFT		0
+#define CPPI41_PKT_LEN_MASK		(0x1fffff << CPPI41_PKT_LEN_SHIFT)
+
+#define CPPI41_PKT_ERROR_SHIFT		31
+#define CPPI41_PKT_ERROR_MASK		(1 << CPPI41_PKT_ERROR_SHIFT)
+#define CPPI41_PKT_TYPE_SHIFT		26
+#define CPPI41_PKT_TYPE_MASK		(0x1f << CPPI41_PKT_TYPE_SHIFT)
+#define CPPI41_PKT_TYPE_ATM_AAL5	0
+#define CPPI41_PKT_TYPE_ATM_NULL_AAL	1
+#define CPPI41_PKT_TYPE_ATM_OAM		2
+#define CPPI41_PKT_TYPE_ATM_TRANSPARENT	3
+#define CPPI41_PKT_TYPE_EFM		4
+#define CPPI41_PKT_TYPE_USB		5
+#define CPPI41_PKT_TYPE_GENERIC		6
+#define CPPI41_PKT_TYPE_ETHERNET	7
+#define CPPI41_RETURN_POLICY_SHIFT	15
+#define CPPI41_RETURN_POLICY_MASK	(1 << CPPI41_RETURN_POLICY_SHIFT)
+#define CPPI41_RETURN_LINKED		0
+#define CPPI41_RETURN_UNLINKED		1
+#define CPPI41_ONCHIP_SHIFT		14
+#define CPPI41_ONCHIP_MASK		(1 << CPPI41_ONCHIP_SHIFT)
+#define CPPI41_RETURN_QMGR_SHIFT	12
+#define CPPI41_RETURN_QMGR_MASK		(3 << CPPI41_RETURN_QMGR_SHIFT)
+#define CPPI41_RETURN_QNUM_SHIFT	0
+#define CPPI41_RETURN_QNUM_MASK		(0xfff << CPPI41_RETURN_QNUM_SHIFT)
+
+#define CPPI41_SRC_TAG_PORT_NUM_SHIFT	27
+#define CPPI41_SRC_TAG_PORT_NUM_MASK	(0x1f << CPPI41_SRC_TAG_PORT_NUM_SHIFT)
+#define CPPI41_SRC_TAG_CH_NUM_SHIFT	21
+#define CPPI41_SRC_TAG_CH_NUM_MASK	(0x3f << CPPI41_SRC_TAG_CH_NUM_SHIFT)
+#define CPPI41_SRC_TAG_SUB_CH_NUM_SHIFT 16
+#define CPPI41_SRC_TAG_SUB_CH_NUM_MASK	(0x1f << \
+					CPPI41_SRC_TAG_SUB_CH_NUM_SHIFT)
+#define CPPI41_DEST_TAG_SHIFT		0
+#define CPPI41_DEST_TAG_MASK		(0xffff << CPPI41_DEST_TAG_SHIFT)
+#define CPPI41_PKT_INTR_FLAG		(1 << 31)
+
+/*
+ * CPPI 4.1 Teardown Descriptor
+ */
+struct cppi41_teardown_desc {
+	u32 teardown_info;	/* Teardown information */
+	u32 reserved[7];	/* 28 byte padding */
+};
+
+#define CPPI41_TEARDOWN_TX_RX_SHIFT	16
+#define CPPI41_TEARDOWN_TX_RX_MASK	(1 << CPPI41_TEARDOWN_TX_RX_SHIFT)
+#define CPPI41_TEARDOWN_DMA_NUM_SHIFT	10
+#define CPPI41_TEARDOWN_DMA_NUM_MASK	(0x3f << CPPI41_TEARDOWN_DMA_NUM_SHIFT)
+#define CPPI41_TEARDOWN_CHAN_NUM_SHIFT	0
+#define CPPI41_TEARDOWN_CHAN_NUM_MASK	(0x3f << CPPI41_TEARDOWN_CHAN_NUM_SHIFT)
+
+#define CPPI41_MAX_MEM_RGN		16
+
+/* CPPI 4.1 configuration for AM3517 */
+#define CPPI41_NUM_QUEUE_MGR		1	/* 4  max */
+#define CPPI41_NUM_DMA_BLOCK		1	/* 64 max */
+#define cppi41_num_queue_mgr	CPPI41_NUM_QUEUE_MGR
+#define cppi41_num_dma_block	CPPI41_NUM_DMA_BLOCK
+
+/**
+ * struct cppi41_queue - Queue Tuple
+ *
+ * The basic queue tuple in CPPI 4.1 used across all data structures
+ * where a definition of a queue is required.
+ */
+struct cppi41_queue {
+	u8  q_mgr;		/* The queue manager number */
+	u16 q_num;		/* The queue number */
+};
+
+/**
+ * struct cppi41_buf_pool - Buffer Pool Tuple
+ *
+ * The basic buffer pool tuple in CPPI 4.1 used across all data structures
+ * where a definition of a buffer pool is required.
+ */
+struct cppi41_buf_pool {
+	u8  b_mgr;		/* The buffer manager number */
+	u16 b_pool;		/* The buffer pool number */
+};
+
+/**
+ * struct cppi41_queue_mgr - Queue Manager information
+ *
+ * Contains the information about the queue manager which should be copied from
+ * the hardware spec as is.
+ */
+struct cppi41_queue_mgr {
+	void __iomem *q_mgr_rgn_base; /* Base address of the Control region. */
+	void __iomem *desc_mem_rgn_base; /* Base address of the descriptor */
+				/* memory region. */
+	void __iomem *q_mgmt_rgn_base; /* Base address of the queues region. */
+	void __iomem *q_stat_rgn_base; /* Base address of the queue status */
+				/* region. */
+	u16 num_queue;		/* Number of the queues supported. */
+	u8 queue_types;		/* Bitmask of the supported queue types. */
+	u16 base_fdq_num;	/* The base free descriptor queue number. */
+				/* If present, there's always 16 such queues. */
+	u16 base_fdbq_num;	/* The base free descriptor/buffer queue */
+				/* number.  If present, there's always 16 */
+				/* such queues. */
+	const u32 *assigned;	/* Pointer to the bitmask of the pre-assigned */
+				/* queues. */
+};
+
+/* Queue type flags */
+#define CPPI41_FREE_DESC_QUEUE		0x01
+#define CPPI41_FREE_DESC_BUF_QUEUE	0x02
+#define CPPI41_UNASSIGNED_QUEUE		0x04
+
+/**
+ * struct cppi41_embed_pkt_cfg - Rx Channel Embedded packet configuration
+ *
+ * An instance of this structure forms part of the Rx channel information
+ * structure.
+ */
+struct cppi41_embed_pkt_cfg {
+	struct cppi41_queue fd_queue; /* Free Descriptor queue.*/
+	u8 num_buf_slot;	/* Number of buffer slots in the descriptor */
+	u8 sop_slot_num;	/* SOP buffer slot number. */
+	struct cppi41_buf_pool free_buf_pool[4]; /* Free Buffer pool. Element */
+				/* 0 used for the 1st Rx buffer, etc. */
+};
+
+/**
+ * struct cppi41_host_pkt_cfg - Rx Channel Host Packet Configuration
+ *
+ * An instance of this structure forms part of the Rx channel information
+ * structure.
+ */
+struct cppi41_host_pkt_cfg {
+	struct cppi41_queue fdb_queue[4]; /* Free Desc/Buffer queue. Element */
+				/* 0 used for 1st Rx buffer, etc. */
+};
+
+/**
+ * struct cppi41_mono_pkt_cfg - Rx Channel Monolithic Packet Configuration
+ *
+ * An instance of this structure forms part of the Rx channel information
+ * structure.
+ */
+struct cppi41_mono_pkt_cfg {
+	struct cppi41_queue fd_queue; /* Free descriptor queue */
+	u8 sop_offset;		/* Number of bytes to skip before writing */
+				/* payload */
+};
+
+enum cppi41_rx_desc_type {
+	cppi41_rx_embed_desc,
+	cppi41_rx_host_desc,
+	cppi41_rx_mono_desc,
+};
+
+/**
+ * struct cppi41_rx_ch_cfg - Rx Channel Configuration
+ *
+ * Must be allocated and filled by the caller of cppi41_rx_ch_configure().
+ *
+ * The same channel can be configured to receive different descripor type
+ * packets (not simaltaneously). When the Rx packets on a port need to be sent
+ * to the SR, the channels default descriptor type is set to Embedded and the
+ * Rx completion queue is set to the queue which CPU polls for input packets.
+ * When in SR bypass mode, the same channel's default descriptor type will be
+ * set to Host and the Rx completion queue set to one of the queues which host
+ * can get interrupted on (via the Queuing proxy/accumulator). In this example,
+ * the embedded mode configuration fetches free descriptor from the Free
+ * descriptor queue (as defined by struct cppi41_embed_pkt_cfg) and host
+ * mode configuration fetches free descriptors/buffers from the free descriptor/
+ * buffer queue (as defined by struct cppi41_host_pkt_cfg).
+ *
+ * NOTE: There seems to be no separate configuration for teardown completion
+ * descriptor. The assumption is rxQueue tuple is used for this purpose as well.
+ */
+struct cppi41_rx_ch_cfg {
+	enum cppi41_rx_desc_type default_desc_type; /* Describes which queue */
+				/* configuration is used for the free */
+				/* descriptors and/or buffers */
+	u8 sop_offset;		/* Number of bytes to skip in SOP buffer */
+				/* before writing payload */
+	u8 retry_starved;	/* 0 = Drop packet on descriptor/buffer */
+				/* starvartion, 1 = DMA retries FIFO block */
+				/* transfer at a later time */
+	struct cppi41_queue rx_queue; /* Rx complete packets queue */
+	union {
+		struct cppi41_host_pkt_cfg host_pkt; /* Host packet */
+				/* configuration. This defines where channel */
+				/* picks free descriptors from. */
+		struct cppi41_embed_pkt_cfg embed_pkt; /* Embedded packet */
+				/* configuration. This defines where channel */
+				/* picks free descriptors/buffers from. */
+				/* from. */
+		struct cppi41_mono_pkt_cfg mono_pkt; /* Monolithic packet */
+				/* configuration. This defines where channel */
+				/* picks free descriptors from. */
+	} cfg;			/* Union of packet configuration structures */
+				/* to be filled in depending on the */
+				/* defDescType field. */
+};
+
+/**
+ * struct cppi41_tx_ch - Tx channel information
+ *
+ * NOTE: The queues that feed into the Tx channel are fixed at SoC design time.
+ */
+struct cppi41_tx_ch {
+	u8 port_num;		/* Port number. */
+	u8 ch_num;		/* Channel number within port. */
+	u8 sub_ch_num;		/* Sub-channel number within channel. */
+	u8 num_tx_queue;	/* Number of queues from which the channel */
+				/* can feed. */
+	struct cppi41_queue tx_queue[4]; /* List of queues from which the */
+				/* channel can feed. */
+};
+
+/**
+ * struct cppi41_dma_block - CPPI 4.1 DMA configuration
+ *
+ * Configuration information for CPPI DMA functionality. Includes the Global
+ * configuration, Channel configuration, and the Scheduler configuration.
+ */
+struct cppi41_dma_block {
+	void __iomem *global_ctrl_base; /* Base address of the Global Control */
+				/* registers. */
+	void __iomem *ch_ctrl_stat_base; /* Base address of the Channel */
+				/* Control/Status registers. */
+	void __iomem *sched_ctrl_base; /* Base address of the Scheduler */
+				/* Control register. */
+	void __iomem *sched_table_base; /* Base address of the Scheduler */
+				/* Table registers. */
+	u8 num_tx_ch;		/* Number of the Tx channels. */
+	u8 num_rx_ch;		/* Number of the Rx channels. */
+	const struct cppi41_tx_ch *tx_ch_info;
+};
+
+extern struct cppi41_queue_mgr cppi41_queue_mgr[];
+extern struct cppi41_dma_block cppi41_dma_block[];
+
+/**
+ * struct cppi41_dma_ch_obj - CPPI 4.1 DMA Channel object
+ */
+struct cppi41_dma_ch_obj {
+	void __iomem *base_addr; /* The address of the channel global */
+				/* configuration register */
+	u32 global_cfg;		/* Tx/Rx global configuration backed-up value */
+};
+
+/**
+ * struct cppi41_queue_obj - CPPI 4.1 queue object
+ */
+struct cppi41_queue_obj {
+	void __iomem *base_addr; /* The base address of the queue management */
+				/* registers */
+};
+
+/**
+ * cppi41_queue_mgr_init - CPPI 4.1 queue manager initialization.
+ * @q_mgr:	the queue manager to initialize
+ * @rgn0_base:	linking RAM region 0 physical address
+ * @rgn0_size:	linking RAM region 0 size in 32-bit words (0 to 0x3fff)
+ *
+ * Returns 0 on success, error otherwise.
+ */
+int cppi41_queue_mgr_init(u8 q_mgr, dma_addr_t rgn0_base, u16 rgn0_size);
+
+/*
+ * CPPI 4.1 Queue Manager Memory Region Allocation and De-allocation APIs.
+ */
+
+/**
+ * cppi41_mem_rgn_alloc - CPPI 4.1 queue manager memory region allocation.
+ * @q_mgr:	the queue manager whose memory region to allocate
+ * @rgn_addr:	physical address of the memory region
+ * @size_order:	descriptor size as a power of two (between 5 and 13)
+ * @num_order:	number of descriptors as a power of two (between 5 and 12)
+ * @mem_rgn:	pointer to the index of the memory region allocated
+ *
+ * This function allocates a memory region within the queue manager
+ * consisiting of the descriptors of paricular size and number.
+ *
+ * Returns 0 on success, error otherwise.
+ */
+int cppi41_mem_rgn_alloc(u8 q_mgr, dma_addr_t rgn_addr, u8 size_order,
+			 u8 num_order, u8 *mem_rgn);
+
+/**
+ * cppi41_mem_rgn_free - CPPI 4.1 queue manager memory region de-allocation.
+ * @q_mgr:	the queue manager whose memory region was allocated
+ * @mem_rgn:	index of the memory region
+ *
+ * This function frees the memory region allocated by cppi41_mem_rgn_alloc().
+ *
+ * Returns 0 on success, -EINVAL otherwise.
+ */
+int cppi41_mem_rgn_free(u8 q_mgr, u8 mem_rgn);
+
+/**
+ * cppi41_dma_block_init - CPPI 4.1 DMA block initialization.
+ * @dma_num:	number of the DMA block
+ * @q_mgr:	the queue manager in which to allocate the free teardown
+ *		descriptor queue
+ * @num_order:	number of teardown descriptors as a power of two (at least 5)
+ * @sched_tbl:	the DMA scheduler table
+ * @tbl_size:	number of entries in the DMA scheduler table
+ *
+ * This function frees the memory region allocated by cppi41_mem_rgn_alloc().
+ *
+ * Returns 0 on success, error otherwise.
+ */
+int cppi41_dma_block_init(u8 dma_num, u8 q_mgr, u8 num_order,
+				 u32 *sched_tbl, u8 tbl_size);
+
+/*
+ * CPPI 4.1 DMA Channel Management APIs
+ */
+
+/**
+ * cppi41_tx_ch_init - initialize CPPI 4.1 transmit channel object
+ * @tx_ch_obj:	pointer to Tx channel object
+ * @dma_num:	DMA block to which this channel belongs
+ * @ch_num:	DMA channel number
+ *
+ * Returns 0 if valid Tx channel, -EINVAL otherwise.
+ */
+int cppi41_tx_ch_init(struct cppi41_dma_ch_obj *tx_ch_obj,
+		      u8 dma_num, u8 ch_num);
+
+/**
+ * cppi41_rx_ch_init - initialize CPPI 4.1 receive channel object
+ * @rx_ch_obj:	pointer to Rx channel object
+ * @dma_num:	DMA block to which this channel belongs
+ * @ch_num:	DMA channel number
+ *
+ * Returns 0 if valid Rx channel, -EINVAL otherwise.
+ */
+int cppi41_rx_ch_init(struct cppi41_dma_ch_obj *rx_ch_obj,
+		      u8 dma_num, u8 ch_num);
+
+/**
+ * cppi41_dma_ch_default_queue - set CPPI 4.1 channel default completion queue
+ * @dma_ch_obj: pointer to DMA channel object
+ * @q_mgr:	default queue manager
+ * @q_num:	default queue number
+ *
+ * This function configures the specified channel.  The caller is required to
+ * provide the default queue onto which the teardown descriptors will be queued.
+ */
+void cppi41_dma_ch_default_queue(struct cppi41_dma_ch_obj *dma_ch_obj,
+				 u8 q_mgr, u16 q_num);
+
+/**
+ * cppi41_rx_ch_configure - configure CPPI 4.1 receive channel
+ * @rx_ch_obj:	pointer to Rx channel object
+ * @cfg:	pointer to Rx channel configuration
+ *
+ * This function configures and opens the specified Rx channel.  The caller
+ * is required to provide channel configuration information by initializing
+ * a struct cppi41_rx_ch_cfg.
+ */
+void cppi41_rx_ch_configure(struct cppi41_dma_ch_obj *rx_ch_obj,
+			    struct cppi41_rx_ch_cfg  *cfg);
+
+/**
+ * cppi41_dma_ch_enable - enable CPPI 4.1 Tx/Rx DMA channel
+ * @dma_ch_obj:	pointer to DMA channel object
+ *
+ * This function enables  a specified Tx channel.  The caller is required to
+ * provide a reference to a channel object initialized by an earlier call of
+ * the cppi41_dma_ch_init() function.  After the successful completion of this
+ * function, the Tx DMA channel will be active and ready for data transmission.
+ */
+void cppi41_dma_ch_enable(struct cppi41_dma_ch_obj *dma_ch_obj);
+
+/**
+ * cppi41_dma_ch_disable - disable CPPI 4.1 Tx/Rx DMA channel
+ * @dma_ch_obj:	pointer to DMA channel object
+ *
+ * This function disables a specific Tx channel.  The caller is required to
+ * provide a reference to a channel object initialized by an earlier call of
+ * the cppi41_dma_ch_init() function.  After the successful completion of this
+ * function, the Tx DMA channel will be deactived.
+ */
+void cppi41_dma_ch_disable(struct cppi41_dma_ch_obj *dma_ch_obj);
+
+/**
+ * cppi41_dma_ch_teardown - tear down CPPI 4.1 transmit channel
+ * @dma_ch_obj:	pointer DMA channel object
+ *
+ * This function triggers the teardown of the given DMA channel.
+ *
+ * ATTENTION: Channel disable should not be called before the teardown is
+ * completed as a disable will stop the DMA scheduling on the channel resulting
+ * in the teardown complete event not being registered at all.
+ *
+ * NOTE: A successful channel teardown event is reported via queueing of a
+ * teardown descriptor.
+ *
+ * This function just sets up for the teardown of the channel and returns. The
+ * caller must detect the channel teardown event to assume that the channel is
+ * disabled.
+ *
+ * See cppi41_get_teardown_info() for the teardown completion processing.
+ */
+void cppi41_dma_ch_teardown(struct cppi41_dma_ch_obj *dma_ch_obj);
+
+/*
+ * CPPI 4.1 Queue Allocation and De-allocation APIs.
+ */
+
+/**
+ * cppi41_queue_alloc - allocate CPPI 4.1 queue
+ * @type:	queue type bitmask
+ * @q_mgr:	queue manager
+ * @q_num:	pointer to the queue number
+ *
+ * Returns 0 if queue allocated, error otherwise.
+ */
+int cppi41_queue_alloc(u8 type, u8 q_mgr, u16 *q_num);
+
+/**
+ * cppi41_queue_free - de-allocate CPPI 4.1 queue
+ * @q_mgr:	queue manager
+ * @q_num:	queue number
+ *
+ * Returns 0 on success, -EINVAL otherwise.
+ */
+int cppi41_queue_free(u8 q_mgr, u16 q_num);
+
+/*
+ *  CPPI 4.1 Queue Management APIs
+ */
+
+/**
+ * cppi41_queue_init - initialize CPPI 4.1 queue object
+ * @queue_obj:	pointer to the queue object
+ * @q_mgr:	queue manager
+ * @q_num:	queue number
+ *
+ * Returns 0 if valid queue, -EINVAL otherwise.
+ */
+int cppi41_queue_init(struct cppi41_queue_obj *queue_obj, u8 q_mgr, u16 q_num);
+
+/**
+ * cppi41_queue_push - push to CPPI 4.1 queue
+ * @queue_obj:	pointer to the queue object
+ * @desc_addr:	descriptor physical address
+ * @desc_size:	descriptor size
+ * @pkt_size:	packet size
+ *
+ * This function is called to queue a descriptor onto a queue.
+ * NOTE: pSize parameter is optional. Pass 0 in case not required.
+ */
+void cppi41_queue_push(const struct cppi41_queue_obj *queue_obj, u32 desc_addr,
+		       u32 desc_size, u32 pkt_size);
+
+/**
+ * cppi41_queue_pop - pop from CPPI 4.1 queue
+ * @queue_obj:	pointer to the queue object
+ *
+ * This function is called to pop a single descriptor from the queue.
+ *
+ * Returns a packet descriptor's physical address.
+ */
+unsigned long cppi41_queue_pop(const struct cppi41_queue_obj *queue_obj);
+
+/*
+ * CPPI 4.1 Miscellaneous APIs
+ */
+
+/**
+ * cppi41_get_teardown_info - CPPI 4.1 teardown completion processing function
+ *
+ * @addr:	physical address of teardown descriptor
+ * @info:	pointer to the teardown information word
+ *
+ * This function is called to complete the teardown processing on a channel
+ * and provides teardown information from the teardown descriptor passed to it.
+ * It also recycles the teardown descriptor back to the teardown descriptor
+ * queue.
+ *
+ * Returns 0 if valid descriptor, -EINVAL otherwise.
+ */
+int cppi41_get_teardown_info(unsigned long addr, u32 *info);
+
+/**
+ * cppi41_exit - delete the instance created via cppi41_init()
+ */
+void cppi41_exit(void);
+
+/**
+ * cppi41_dma_sched_tbl_init
+ */
+int cppi41_dma_sched_tbl_init(u8 dma_num, u8 q_mgr,
+				u32 *sched_tbl, u8 tbl_size);
+
+/**
+ * cppi41_free_teardown_queue
+ */
+void cppi41_free_teardown_queue(int dma_num);
diff --git a/drivers/usb/musb/cppi41_dma.c b/drivers/usb/musb/cppi41_dma.c
new file mode 100644
index 0000000..2ab2450
--- /dev/null
+++ b/drivers/usb/musb/cppi41_dma.c
@@ -0,0 +1,1341 @@
+/*
+ * Copyright (C) 2005-2006 by Texas Instruments
+ * Copyright (c) 2008, MontaVista Software, Inc. <source@mvista.com>
+ *
+ * This file implements a DMA interface using TI's CPPI 4.1 DMA.
+ *
+ * This program is free software; you can distribute it and/or modify it
+ * under the terms of the GNU General Public License (Version 2) as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * for more details.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write to the Free Software Foundation, Inc.,
+ * 59 Temple Place - Suite 330, Boston MA 02111-1307, USA.
+ *
+ */
+
+#include <linux/errno.h>
+#include <linux/dma-mapping.h>
+
+#include "cppi41.h"
+
+#include "musb_core.h"
+#include "musb_dma.h"
+#include "cppi41_dma.h"
+
+/* Configuration */
+#define USB_CPPI41_DESC_SIZE_SHIFT 6
+#define USB_CPPI41_DESC_ALIGN	(1 << USB_CPPI41_DESC_SIZE_SHIFT)
+#define USB_CPPI41_CH_NUM_PD	64	/* 4K bulk data at full speed */
+#define USB_CPPI41_MAX_PD	(USB_CPPI41_CH_NUM_PD * USB_CPPI41_NUM_CH)
+
+#undef DEBUG_CPPI_TD
+#undef USBDRV_DEBUG
+
+#ifdef USBDRV_DEBUG
+#define dprintk(x, ...) printk(x, ## __VA_ARGS__)
+#else
+#define dprintk(x, ...)
+#endif
+
+/*
+ * Data structure definitions
+ */
+
+/*
+ * USB Packet Descriptor
+ */
+struct usb_pkt_desc;
+
+struct usb_pkt_desc {
+	/* Hardware descriptor fields from this point */
+	struct cppi41_host_pkt_desc hw_desc;
+	/* Protocol specific data */
+	dma_addr_t dma_addr;
+	struct usb_pkt_desc *next_pd_ptr;
+	u8 ch_num;
+	u8 ep_num;
+	u8 eop;
+};
+
+/**
+ * struct cppi41_channel - DMA Channel Control Structure
+ *
+ * Using the same for Tx/Rx.
+ */
+struct cppi41_channel {
+	struct dma_channel channel;
+
+	struct cppi41_dma_ch_obj dma_ch_obj; /* DMA channel object */
+	struct cppi41_queue src_queue;	/* Tx queue or Rx free descriptor/ */
+					/* buffer queue */
+	struct cppi41_queue_obj queue_obj; /* Tx queue object or Rx free */
+					/* descriptor/buffer queue object */
+
+	u32 tag_info;			/* Tx PD Tag Information field */
+
+	/* Which direction of which endpoint? */
+	struct musb_hw_ep *end_pt;
+	u8 transmit;
+	u8 ch_num;			/* Channel number of Tx/Rx 0..3 */
+
+	/* DMA mode: "transparent", RNDIS, CDC, or Generic RNDIS */
+	u8 dma_mode;
+	u8 autoreq;
+
+	/* Book keeping for the current transfer request */
+	dma_addr_t start_addr;
+	u32 length;
+	u32 curr_offset;
+	u16 pkt_size;
+	u8  transfer_mode;
+	u8  zlp_queued;
+};
+
+/**
+ * struct cppi41 - CPPI 4.1 DMA Controller Object
+ *
+ * Encapsulates all book keeping and data structures pertaining to
+ * the CPPI 1.4 DMA controller.
+ */
+struct cppi41 {
+	struct dma_controller controller;
+	struct musb *musb;
+
+	struct cppi41_channel tx_cppi_ch[USB_CPPI41_NUM_CH];
+	struct cppi41_channel rx_cppi_ch[USB_CPPI41_NUM_CH];
+
+	struct usb_pkt_desc *pd_pool_head; /* Free PD pool head */
+	dma_addr_t pd_mem_phys;		/* PD memory physical address */
+	void *pd_mem;			/* PD memory pointer */
+	u8 pd_mem_rgn;			/* PD memory region number */
+
+	u16 teardownQNum;		/* Teardown completion queue number */
+	struct cppi41_queue_obj queue_obj; /* Teardown completion queue */
+					/* object */
+	u32 pkt_info;			/* Tx PD Packet Information field */
+	u8 en_bd_intr;			/* enable bd interrupt */
+	struct usb_cppi41_info *cppi_info;
+};
+
+#ifdef DEBUG_CPPI_TD
+static void print_pd_list(struct usb_pkt_desc *pd_pool_head)
+{
+	struct usb_pkt_desc *curr_pd = pd_pool_head;
+	int cnt = 0;
+
+	while (curr_pd != NULL) {
+		if (cnt % 8 == 0)
+			dprintk("\n%02x ", cnt);
+		cnt++;
+		dprintk(" %p", curr_pd);
+		curr_pd = curr_pd->next_pd_ptr;
+	}
+	dprintk("\n");
+}
+#endif
+
+static struct usb_pkt_desc *usb_get_free_pd(struct cppi41 *cppi)
+{
+	struct usb_pkt_desc *free_pd = cppi->pd_pool_head;
+
+	if (free_pd != NULL) {
+		cppi->pd_pool_head = free_pd->next_pd_ptr;
+		free_pd->next_pd_ptr = NULL;
+	}
+	return free_pd;
+}
+
+static void usb_put_free_pd(struct cppi41 *cppi, struct usb_pkt_desc *free_pd)
+{
+	free_pd->next_pd_ptr = cppi->pd_pool_head;
+	cppi->pd_pool_head = free_pd;
+}
+
+/**
+ * cppi41_controller_start - start DMA controller
+ * @controller: the controller
+ *
+ * This function initializes the CPPI 4.1 Tx/Rx channels.
+ */
+static int __init cppi41_controller_start(struct dma_controller *controller)
+{
+	struct cppi41 *cppi;
+	struct cppi41_channel *cppi_ch;
+	void __iomem *reg_base;
+	struct usb_pkt_desc *curr_pd;
+	unsigned long pd_addr;
+	int i, dma_block;
+
+	cppi = container_of(controller, struct cppi41, controller);
+
+	/*
+	 * TODO: We may need to check USB_CPPI41_MAX_PD here since CPPI 4.1
+	 * requires the descriptor count to be a multiple of 2 ^ 5 (i.e. 32).
+	 * Similarly, the descriptor size should also be a multiple of 32.
+	 */
+
+	/*
+	 * Allocate free packet descriptor pool for all Tx/Rx endpoints --
+	 * dma_alloc_coherent()  will return a page aligned address, so our
+	 * alignment requirement will be honored.
+	 */
+	cppi->pd_mem = dma_alloc_coherent(cppi->musb->controller,
+			  USB_CPPI41_MAX_PD * USB_CPPI41_DESC_ALIGN,
+			  &cppi->pd_mem_phys, GFP_KERNEL | GFP_DMA);
+	if (cppi->pd_mem == NULL) {
+		DBG(1, "ERROR: packet descriptor memory allocation failed\n");
+		return 0;
+	}
+	if (cppi41_mem_rgn_alloc(cppi->cppi_info->q_mgr, cppi->pd_mem_phys,
+			 USB_CPPI41_DESC_SIZE_SHIFT,
+			 get_count_order(USB_CPPI41_MAX_PD),
+			 &cppi->pd_mem_rgn)) {
+		DBG(1, "ERROR: queue manager memory region allocation "
+		    "failed\n");
+		goto free_pds;
+	}
+
+	/* Allocate the teardown completion queue */
+	if (cppi41_queue_alloc(CPPI41_UNASSIGNED_QUEUE,
+			       0, &cppi->teardownQNum)) {
+		DBG(1, "ERROR: teardown completion queue allocation failed\n");
+		goto free_mem_rgn;
+	}
+	DBG(4, "Allocated teardown completion queue %d in queue manager 0\n",
+	    cppi->teardownQNum);
+
+	if (cppi41_queue_init(&cppi->queue_obj, 0, cppi->teardownQNum)) {
+		DBG(1, "ERROR: teardown completion queue initialization "
+		    "failed\n");
+		goto free_queue;
+	}
+
+	/*
+	 * "Slice" PDs one-by-one from the big chunk and
+	 * add them to the free pool.
+	 */
+	curr_pd = (struct usb_pkt_desc *)cppi->pd_mem;
+	pd_addr = cppi->pd_mem_phys;
+	for (i = 0; i < USB_CPPI41_MAX_PD; i++) {
+		curr_pd->dma_addr = pd_addr;
+
+		usb_put_free_pd(cppi, curr_pd);
+		curr_pd = (struct usb_pkt_desc *)((char *)curr_pd +
+						  USB_CPPI41_DESC_ALIGN);
+		pd_addr += USB_CPPI41_DESC_ALIGN;
+	}
+
+	/* Configure the Tx channels */
+	for (i = 0, cppi_ch = cppi->tx_cppi_ch;
+	     i < ARRAY_SIZE(cppi->tx_cppi_ch); ++i, ++cppi_ch) {
+		const struct cppi41_tx_ch *tx_info;
+
+		memset(cppi_ch, 0, sizeof(struct cppi41_channel));
+		cppi_ch->transmit = 1;
+		cppi_ch->ch_num = i;
+		cppi_ch->channel.private_data = cppi;
+
+		/*
+		 * Extract the CPPI 4.1 DMA Tx channel configuration and
+		 * construct/store the Tx PD tag info field for later use...
+		 */
+		dma_block = cppi->cppi_info->dma_block;
+		tx_info = cppi41_dma_block[dma_block].tx_ch_info
+			+ cppi->cppi_info->ep_dma_ch[i];
+		cppi_ch->src_queue = tx_info->tx_queue[0];
+		cppi_ch->tag_info = (tx_info->port_num <<
+				     CPPI41_SRC_TAG_PORT_NUM_SHIFT) |
+				    (tx_info->ch_num <<
+				     CPPI41_SRC_TAG_CH_NUM_SHIFT) |
+				    (tx_info->sub_ch_num <<
+				     CPPI41_SRC_TAG_SUB_CH_NUM_SHIFT);
+	}
+
+	/* Configure the Rx channels */
+	for (i = 0, cppi_ch = cppi->rx_cppi_ch;
+	     i < ARRAY_SIZE(cppi->rx_cppi_ch); ++i, ++cppi_ch) {
+		memset(cppi_ch, 0, sizeof(struct cppi41_channel));
+		cppi_ch->ch_num = i;
+		cppi_ch->channel.private_data = cppi;
+	}
+
+	/* Construct/store Tx PD packet info field for later use */
+	cppi->pkt_info = (CPPI41_PKT_TYPE_USB << CPPI41_PKT_TYPE_SHIFT) |
+			 (CPPI41_RETURN_LINKED << CPPI41_RETURN_POLICY_SHIFT) |
+			 (cppi->cppi_info->q_mgr << CPPI41_RETURN_QMGR_SHIFT) |
+			 (cppi->cppi_info->tx_comp_q[0] <<
+			  CPPI41_RETURN_QNUM_SHIFT);
+
+	/* Do necessary configuartion in hardware to get started */
+	reg_base = cppi->musb->ctrl_base;
+
+	/* Disable auto request mode */
+	musb_writel(reg_base, USB_AUTOREQ_REG, 0);
+
+	/* Disable the CDC/RNDIS modes */
+	musb_writel(reg_base, USB_TX_MODE_REG, 0);
+	musb_writel(reg_base, USB_RX_MODE_REG, 0);
+
+	return 1;
+
+ free_queue:
+	if (cppi41_queue_free(0, cppi->teardownQNum))
+		DBG(1, "ERROR: failed to free teardown completion queue\n");
+
+ free_mem_rgn:
+	if (cppi41_mem_rgn_free(cppi->cppi_info->q_mgr, cppi->pd_mem_rgn))
+		DBG(1, "ERROR: failed to free queue manager memory region\n");
+
+ free_pds:
+	dma_free_coherent(cppi->musb->controller,
+			  USB_CPPI41_MAX_PD * USB_CPPI41_DESC_ALIGN,
+			  cppi->pd_mem, cppi->pd_mem_phys);
+
+	return 0;
+}
+
+/**
+ * cppi41_controller_stop - stop DMA controller
+ * @controller: the controller
+ *
+ * De-initialize the DMA Controller as necessary.
+ */
+static int cppi41_controller_stop(struct dma_controller *controller)
+{
+	struct cppi41 *cppi;
+	void __iomem *reg_base;
+
+	cppi = container_of(controller, struct cppi41, controller);
+
+	/* Free the teardown completion queue */
+	if (cppi41_queue_free(cppi->cppi_info->q_mgr, cppi->teardownQNum))
+		DBG(1, "ERROR: failed to free teardown completion queue\n");
+	/*
+	 * Free the packet descriptor region allocated
+	 * for all Tx/Rx channels.
+	 */
+	if (cppi41_mem_rgn_free(cppi->cppi_info->q_mgr, cppi->pd_mem_rgn))
+		DBG(1, "ERROR: failed to free queue manager memory region\n");
+
+	dma_free_coherent(cppi->musb->controller,
+			  USB_CPPI41_MAX_PD * USB_CPPI41_DESC_ALIGN,
+			  cppi->pd_mem, cppi->pd_mem_phys);
+
+	reg_base = cppi->musb->ctrl_base;
+
+	/* Disable auto request mode */
+	musb_writel(reg_base, USB_AUTOREQ_REG, 0);
+
+	/* Disable the CDC/RNDIS modes */
+	musb_writel(reg_base, USB_TX_MODE_REG, 0);
+	musb_writel(reg_base, USB_RX_MODE_REG, 0);
+	return 1;
+}
+
+/**
+ * cppi41_channel_alloc - allocate a CPPI channel for DMA.
+ * @controller: the controller
+ * @ep:		the endpoint
+ * @is_tx:	1 for Tx channel, 0 for Rx channel
+ *
+ * With CPPI, channels are bound to each transfer direction of a non-control
+ * endpoint, so allocating (and deallocating) is mostly a way to notice bad
+ * housekeeping on the software side.  We assume the IRQs are always active.
+ */
+static struct dma_channel *cppi41_channel_alloc(struct dma_controller
+						*controller,
+						struct musb_hw_ep *ep, u8 is_tx)
+{
+	struct cppi41 *cppi;
+	struct cppi41_channel  *cppi_ch;
+	u32 ch_num, ep_num = ep->epnum;
+
+	cppi = container_of(controller, struct cppi41, controller);
+
+	/* Remember, ep_num: 1 .. Max_EP, and CPPI ch_num: 0 .. Max_EP - 1 */
+	ch_num = ep_num - 1;
+
+	if (ep_num > USB_CPPI41_NUM_CH) {
+		DBG(1, "No %cx DMA channel for EP%d\n",
+		    is_tx ? 'T' : 'R', ep_num);
+		return NULL;
+	}
+
+	cppi_ch = (is_tx ? cppi->tx_cppi_ch : cppi->rx_cppi_ch) + ch_num;
+
+	/* As of now, just return the corresponding CPPI 4.1 channel handle */
+	if (is_tx) {
+		/* Initialize the CPPI 4.1 Tx DMA channel */
+		if (cppi41_tx_ch_init(&cppi_ch->dma_ch_obj,
+				      cppi->cppi_info->dma_block,
+				      cppi->cppi_info->ep_dma_ch[ch_num])) {
+			DBG(1, "ERROR: cppi41_tx_ch_init failed for "
+			    "channel %d\n", ch_num);
+			return NULL;
+		}
+		/*
+		 * Teardown descriptors will be pushed to the dedicated
+		 * completion queue.
+		 */
+		cppi41_dma_ch_default_queue(&cppi_ch->dma_ch_obj,
+					    0, cppi->teardownQNum);
+	} else {
+		struct cppi41_rx_ch_cfg rx_cfg;
+		u8 q_mgr = cppi->cppi_info->q_mgr;
+		int i;
+
+		/* Initialize the CPPI 4.1 Rx DMA channel */
+		if (cppi41_rx_ch_init(&cppi_ch->dma_ch_obj,
+				      cppi->cppi_info->dma_block,
+				      cppi->cppi_info->ep_dma_ch[ch_num])) {
+			DBG(1, "ERROR: cppi41_rx_ch_init failed\n");
+			return NULL;
+		}
+
+		if (cppi41_queue_alloc(CPPI41_FREE_DESC_BUF_QUEUE |
+				       CPPI41_UNASSIGNED_QUEUE,
+				       q_mgr, &cppi_ch->src_queue.q_num)) {
+			DBG(1, "ERROR: cppi41_queue_alloc failed for "
+			    "free descriptor/buffer queue\n");
+			return NULL;
+		}
+		DBG(4, "Allocated free descriptor/buffer queue %d in "
+		    "queue manager %d\n", cppi_ch->src_queue.q_num, q_mgr);
+
+		rx_cfg.default_desc_type = cppi41_rx_host_desc;
+		rx_cfg.sop_offset = 0;
+		rx_cfg.retry_starved = 1;
+		rx_cfg.rx_queue.q_mgr = cppi_ch->src_queue.q_mgr = q_mgr;
+		rx_cfg.rx_queue.q_num = cppi->cppi_info->rx_comp_q[ch_num];
+		for (i = 0; i < 4; i++)
+			rx_cfg.cfg.host_pkt.fdb_queue[i] = cppi_ch->src_queue;
+		cppi41_rx_ch_configure(&cppi_ch->dma_ch_obj, &rx_cfg);
+	}
+
+	/* Initialize the CPPI 4.1 DMA source queue */
+	if (cppi41_queue_init(&cppi_ch->queue_obj, cppi_ch->src_queue.q_mgr,
+			       cppi_ch->src_queue.q_num)) {
+		DBG(1, "ERROR: cppi41_queue_init failed for %s queue",
+		    is_tx ? "Tx" : "Rx free descriptor/buffer");
+		if (is_tx == 0 &&
+		    cppi41_queue_free(cppi_ch->src_queue.q_mgr,
+				      cppi_ch->src_queue.q_num))
+			DBG(1, "ERROR: failed to free Rx descriptor/buffer "
+			    "queue\n");
+		 return NULL;
+	}
+
+	/* Enable the DMA channel */
+	cppi41_dma_ch_enable(&cppi_ch->dma_ch_obj);
+
+	if (cppi_ch->end_pt)
+		DBG(1, "Re-allocating DMA %cx channel %d (%p)\n",
+		    is_tx ? 'T' : 'R', ch_num, cppi_ch);
+
+	cppi_ch->end_pt = ep;
+	cppi_ch->ch_num = ch_num;
+	cppi_ch->channel.status = MUSB_DMA_STATUS_FREE;
+
+	DBG(4, "Allocated DMA %cx channel %d for EP%d\n", is_tx ? 'T' : 'R',
+	    ch_num, ep_num);
+
+	return &cppi_ch->channel;
+}
+
+/**
+ * cppi41_channel_release - release a CPPI DMA channel
+ * @channel: the channel
+ */
+static void cppi41_channel_release(struct dma_channel *channel)
+{
+	struct cppi41_channel *cppi_ch;
+
+	/* REVISIT: for paranoia, check state and abort if needed... */
+	cppi_ch = container_of(channel, struct cppi41_channel, channel);
+	if (cppi_ch->end_pt == NULL)
+		DBG(1, "Releasing idle DMA channel %p\n", cppi_ch);
+
+	/* But for now, not its IRQ */
+	cppi_ch->end_pt = NULL;
+	channel->status = MUSB_DMA_STATUS_UNKNOWN;
+
+	cppi41_dma_ch_disable(&cppi_ch->dma_ch_obj);
+
+	/* De-allocate Rx free descriptior/buffer queue */
+	if (cppi_ch->transmit == 0 &&
+	    cppi41_queue_free(cppi_ch->src_queue.q_mgr,
+			      cppi_ch->src_queue.q_num))
+		DBG(1, "ERROR: failed to free Rx descriptor/buffer queue\n");
+}
+
+static void cppi41_mode_update(struct cppi41_channel *cppi_ch, u8 mode)
+{
+	if (mode != cppi_ch->dma_mode) {
+		struct cppi41 *cppi = cppi_ch->channel.private_data;
+		void *__iomem reg_base = cppi->musb->ctrl_base;
+		u32 reg_val;
+		u8 ep_num = cppi_ch->ch_num + 1;
+
+		if (cppi_ch->transmit) {
+			reg_val = musb_readl(reg_base, USB_TX_MODE_REG);
+			reg_val &= ~USB_TX_MODE_MASK(ep_num);
+			reg_val |= mode << USB_TX_MODE_SHIFT(ep_num);
+			musb_writel(reg_base, USB_TX_MODE_REG, reg_val);
+		} else {
+			reg_val = musb_readl(reg_base, USB_RX_MODE_REG);
+			reg_val &= ~USB_RX_MODE_MASK(ep_num);
+			reg_val |= mode << USB_RX_MODE_SHIFT(ep_num);
+			musb_writel(reg_base, USB_RX_MODE_REG, reg_val);
+		}
+		cppi_ch->dma_mode = mode;
+	}
+}
+
+/*
+ * CPPI 4.1 Tx:
+ * ============
+ * Tx is a lot more reasonable than Rx: RNDIS mode seems to behave well except
+ * how it handles the exactly-N-packets case. It appears that there's a hiccup
+ * in that case (maybe the DMA completes before a ZLP gets written?) boiling
+ * down to not being able to rely on the XFER DMA writing any terminating zero
+ * length packet before the next transfer is started...
+ *
+ * The generic RNDIS mode does not have this misfeature, so we prefer using it
+ * instead.  We then send the terminating ZLP *explictly* using DMA instead of
+ * doing it by PIO after an IRQ.
+ *
+ */
+
+/**
+ * cppi41_next_tx_segment - DMA write for the next chunk of a buffer
+ * @tx_ch:	Tx channel
+ *
+ * Context: controller IRQ-locked
+ */
+static unsigned cppi41_next_tx_segment(struct cppi41_channel *tx_ch)
+{
+	struct cppi41 *cppi = tx_ch->channel.private_data;
+	struct usb_pkt_desc *curr_pd;
+	u32 length = tx_ch->length - tx_ch->curr_offset;
+	u32 pkt_size = tx_ch->pkt_size;
+	unsigned num_pds, n;
+	u8 en_bd_intr = cppi->en_bd_intr;
+	u8 ch_num = tx_ch->ch_num;
+
+	/*
+	 * Tx can use the generic RNDIS mode where we can probably fit this
+	 * transfer in one PD and one IRQ.  The only time we would NOT want
+	 * to use it is when the hardware constraints prevent it...
+	 */
+	if ((pkt_size & 0x3f) == 0 && length > pkt_size) {
+		num_pds  = 1;
+		pkt_size = length;
+		cppi41_mode_update(tx_ch, USB_GENERIC_RNDIS_MODE);
+	} else {
+		num_pds  = (length + pkt_size - 1) / pkt_size;
+		cppi41_mode_update(tx_ch, USB_TRANSPARENT_MODE);
+	}
+
+	/*
+	 * If length of transmit buffer is 0 or a multiple of the endpoint size,
+	 * then send the zero length packet.
+	 */
+	if (!length || (tx_ch->transfer_mode && length % pkt_size == 0))
+		num_pds++;
+
+	DBG(4, "TX DMA%u, %s, maxpkt %u, %u PDs, addr %#x, len %u\n",
+	    tx_ch->ch_num, tx_ch->dma_mode ? "accelerated" : "transparent",
+	    pkt_size, num_pds, tx_ch->start_addr + tx_ch->curr_offset, length);
+
+	for (n = 0; n < num_pds; n++) {
+		struct cppi41_host_pkt_desc *hw_desc;
+
+		/* Get Tx host packet descriptor from the free pool */
+		curr_pd = usb_get_free_pd(cppi);
+		if (curr_pd == NULL) {
+			DBG(1, "No Tx PDs\n");
+			break;
+		}
+
+		if (length < pkt_size)
+			pkt_size = length;
+
+		hw_desc = &curr_pd->hw_desc;
+		hw_desc->desc_info = (CPPI41_DESC_TYPE_HOST <<
+				      CPPI41_DESC_TYPE_SHIFT) | pkt_size;
+		hw_desc->tag_info = tx_ch->tag_info;
+		hw_desc->pkt_info =
+			(CPPI41_PKT_TYPE_USB << CPPI41_PKT_TYPE_SHIFT) |
+			(CPPI41_RETURN_LINKED << CPPI41_RETURN_POLICY_SHIFT) |
+			(cppi->cppi_info->q_mgr <<
+				CPPI41_RETURN_QMGR_SHIFT) |
+			(cppi->cppi_info->tx_comp_q[ch_num] <<
+		CPPI41_RETURN_QNUM_SHIFT);
+
+		hw_desc->buf_ptr = tx_ch->start_addr + tx_ch->curr_offset;
+		hw_desc->buf_len = pkt_size;
+		hw_desc->next_desc_ptr = 0;
+		hw_desc->orig_buf_len = pkt_size;
+
+		curr_pd->ch_num = tx_ch->ch_num;
+		curr_pd->ep_num = tx_ch->end_pt->epnum;
+
+		tx_ch->curr_offset += pkt_size;
+		length -= pkt_size;
+
+		if (pkt_size == 0)
+			tx_ch->zlp_queued = 1;
+
+		if (en_bd_intr)
+			hw_desc->orig_buf_len |= CPPI41_PKT_INTR_FLAG;
+
+		DBG(5, "id(%d) TX PD %p: buf %08x, len %08x, pkt info %08x\n",
+			cppi->musb->id, curr_pd, hw_desc->buf_ptr,
+			hw_desc->buf_len, hw_desc->pkt_info);
+
+		cppi41_queue_push(&tx_ch->queue_obj, curr_pd->dma_addr,
+				  USB_CPPI41_DESC_ALIGN, pkt_size);
+	}
+
+	return n;
+}
+
+static void cppi41_autoreq_update(struct cppi41_channel *rx_ch, u8 autoreq)
+{
+	struct cppi41 *cppi = rx_ch->channel.private_data;
+
+	if (is_host_active(cppi->musb) &&
+	    autoreq != rx_ch->autoreq) {
+		void *__iomem reg_base = cppi->musb->ctrl_base;
+		u32 reg_val = musb_readl(reg_base, USB_AUTOREQ_REG);
+		u8 ep_num = rx_ch->ch_num + 1;
+
+		reg_val &= ~USB_RX_AUTOREQ_MASK(ep_num);
+		reg_val |= autoreq << USB_RX_AUTOREQ_SHIFT(ep_num);
+
+		musb_writel(reg_base, USB_AUTOREQ_REG, reg_val);
+		rx_ch->autoreq = autoreq;
+	}
+}
+
+static void cppi41_set_ep_size(struct cppi41_channel *rx_ch, u32 pkt_size)
+{
+	struct cppi41 *cppi = rx_ch->channel.private_data;
+	void *__iomem reg_base = cppi->musb->ctrl_base;
+	u8 ep_num = rx_ch->ch_num + 1;
+
+	musb_writel(reg_base, USB_GENERIC_RNDIS_EP_SIZE_REG(ep_num), pkt_size);
+}
+
+/*
+ * CPPI 4.1 Rx:
+ * ============
+ * Consider a 1KB bulk Rx buffer in two scenarios: (a) it's fed two 300 byte
+ * packets back-to-back, and (b) it's fed two 512 byte packets back-to-back.
+ * (Full speed transfers have similar scenarios.)
+ *
+ * The correct behavior for Linux is that (a) fills the buffer with 300 bytes,
+ * and the next packet goes into a buffer that's queued later; while (b) fills
+ * the buffer with 1024 bytes.  How to do that with accelerated DMA modes?
+ *
+ * Rx queues in RNDIS mode (one single BD) handle (a) correctly but (b) loses
+ * BADLY because nothing (!) happens when that second packet fills the buffer,
+ * much less when a third one arrives -- which makes it not a "true" RNDIS mode.
+ * In the RNDIS protocol short-packet termination is optional, and it's fine if
+ * the peripherals (not hosts!) pad the messages out to end of buffer. Standard
+ * PCI host controller DMA descriptors implement that mode by default... which
+ * is no accident.
+ *
+ * Generic RNDIS mode is the only way to reliably make both cases work.  This
+ * mode is identical to the "normal" RNDIS mode except for the case where the
+ * last packet of the segment matches the max USB packet size -- in this case,
+ * the packet will be closed when a value (0x10000 max) in the Generic RNDIS
+ * EP Size register is reached.  This mode will work for the network drivers
+ * (CDC/RNDIS) as well as for the mass storage drivers where there is no short
+ * packet.
+ *
+ * BUT we can only use non-transparent modes when USB packet size is a multiple
+ * of 64 bytes. Let's see what happens when  this is not the case...
+ *
+ * Rx queues (2 BDs with 512 bytes each) have converse problems to RNDIS mode:
+ * (b) is handled right but (a) loses badly.  DMA doesn't stop after receiving
+ * a short packet and processes both of those PDs; so both packets are loaded
+ * into the buffer (with 212 byte gap between them), and the next buffer queued
+ * will NOT get its 300 bytes of data.  Even in the case when there should be
+ * no short packets (URB_SHORT_NOT_OK is set), queueing several packets in the
+ * host mode doesn't win us anything since we have to manually "prod" the Rx
+ * process after each packet is received by setting ReqPkt bit in endpoint's
+ * RXCSR; in the peripheral mode without short packets, queueing could be used
+ * BUT we'll have to *teardown* the channel if a short packet still arrives in
+ * the peripheral mode, and to "collect" the left-over packet descriptors from
+ * the free descriptor/buffer queue in both cases...
+ *
+ * One BD at a time is the only way to make make both cases work reliably, with
+ * software handling both cases correctly, at the significant penalty of needing
+ * an IRQ per packet.  (The lack of I/O overlap can be slightly ameliorated by
+ * enabling double buffering.)
+ *
+ * There seems to be no way to identify for sure the cases where the CDC mode
+ * is appropriate...
+ *
+ */
+
+/**
+ * cppi41_next_rx_segment - DMA read for the next chunk of a buffer
+ * @rx_ch:	Rx channel
+ *
+ * Context: controller IRQ-locked
+ *
+ * NOTE: In the transparent mode, we have to queue one packet at a time since:
+ *	 - we must avoid starting reception of another packet after receiving
+ *	   a short packet;
+ *	 - in host mode we have to set ReqPkt bit in the endpoint's RXCSR after
+ *	   receiving each packet but the last one... ugly!
+ */
+static unsigned cppi41_next_rx_segment(struct cppi41_channel *rx_ch)
+{
+	struct cppi41 *cppi = rx_ch->channel.private_data;
+	struct usb_pkt_desc *curr_pd;
+	struct cppi41_host_pkt_desc *hw_desc;
+	u32 length = rx_ch->length - rx_ch->curr_offset;
+	u32 pkt_size = rx_ch->pkt_size;
+	u32 max_rx_transfer_size = 128 * 1024;
+	u32 i, n_bd , pkt_len;
+	struct usb_gadget_driver *gadget_driver;
+	u8 en_bd_intr = cppi->en_bd_intr;
+
+	if (is_peripheral_active(cppi->musb)) {
+		/* TODO: temporary fix for CDC/RNDIS which needs to be in
+		 * GENERIC_RNDIS mode. Without this RNDIS gadget taking
+		 * more then 2K ms for a 64 byte pings.
+		 */
+#ifdef CONFIG_USB_GADGET_MUSB_HDRC
+		gadget_driver = cppi->musb->gadget_driver;
+#endif
+		if (!strcmp(gadget_driver->driver.name, "g_ether")) {
+			cppi41_mode_update(rx_ch, USB_GENERIC_RNDIS_MODE);
+		} else {
+			max_rx_transfer_size = 512;
+			cppi41_mode_update(rx_ch, USB_TRANSPARENT_MODE);
+		}
+		pkt_len = 0;
+		if (rx_ch->length < max_rx_transfer_size)
+			pkt_len = rx_ch->length;
+		cppi41_set_ep_size(rx_ch, pkt_len);
+	} else {
+		/*
+		 * Rx can use the generic RNDIS mode where we can
+		 * probably fit this transfer in one PD and one IRQ
+		 * (or two with a short packet).
+		 */
+		if ((pkt_size & 0x3f) == 0 && length >= 2 * pkt_size) {
+			cppi41_mode_update(rx_ch, USB_GENERIC_RNDIS_MODE);
+			cppi41_autoreq_update(rx_ch, USB_AUTOREQ_ALL_BUT_EOP);
+
+			if (likely(length < 0x10000))
+				pkt_size = length - length % pkt_size;
+			else
+				pkt_size = 0x10000;
+			cppi41_set_ep_size(rx_ch, pkt_size);
+		} else {
+			cppi41_mode_update(rx_ch, USB_TRANSPARENT_MODE);
+			cppi41_autoreq_update(rx_ch, USB_NO_AUTOREQ);
+		}
+	}
+
+	DBG(4, "RX DMA%u, %s, maxpkt %u, addr %#x, rec'd %u/%u\n",
+	    rx_ch->ch_num, rx_ch->dma_mode ? "accelerated" : "transparent",
+	    pkt_size, rx_ch->start_addr + rx_ch->curr_offset,
+	    rx_ch->curr_offset, rx_ch->length);
+
+	/* calculate number of bd required */
+	n_bd = (length + max_rx_transfer_size - 1)/max_rx_transfer_size;
+
+	for (i = 0; i < n_bd ; ++i) {
+		/* Get Rx packet descriptor from the free pool */
+		curr_pd = usb_get_free_pd(cppi);
+		if (curr_pd == NULL) {
+			/* Shouldn't ever happen! */
+			DBG(4, "No Rx PDs\n");
+			goto sched;
+		}
+
+		pkt_len =
+		(length > max_rx_transfer_size) ? max_rx_transfer_size : length;
+
+		hw_desc = &curr_pd->hw_desc;
+		hw_desc->orig_buf_ptr = rx_ch->start_addr + rx_ch->curr_offset;
+		hw_desc->orig_buf_len = pkt_len;
+
+		curr_pd->ch_num = rx_ch->ch_num;
+		curr_pd->ep_num = rx_ch->end_pt->epnum;
+
+		curr_pd->eop = (length -= pkt_len) ? 0 : 1;
+		rx_ch->curr_offset += pkt_len;
+
+		if (en_bd_intr)
+			hw_desc->orig_buf_len |= CPPI41_PKT_INTR_FLAG;
+		/*
+		 * Push the free Rx packet descriptor
+		 * to the free descriptor/buffer queue.
+		 */
+		cppi41_queue_push(&rx_ch->queue_obj, curr_pd->dma_addr,
+			USB_CPPI41_DESC_ALIGN, 0);
+	}
+
+sched:
+	/*
+	 * HCD arranged ReqPkt for the first packet.
+	 * We arrange it for all but the last one.
+	 */
+	if (is_host_active(cppi->musb) && rx_ch->channel.actual_len) {
+		void __iomem *epio = rx_ch->end_pt->regs;
+		u16 csr = musb_readw(epio, MUSB_RXCSR);
+
+		csr |= MUSB_RXCSR_H_REQPKT | MUSB_RXCSR_H_WZC_BITS;
+		musb_writew(epio, MUSB_RXCSR, csr);
+	}
+
+	/* enable schedular if not enabled */
+	if (is_peripheral_active(cppi->musb) && (n_bd > 0))
+		cppi41_enable_sched_rx();
+	return 1;
+}
+
+/**
+ * cppi41_channel_program - program channel for data transfer
+ * @channel:	the channel
+ * @maxpacket:	max packet size
+ * @mode:	for Rx, 1 unless the USB protocol driver promised to treat
+ *		all short reads as errors and kick in high level fault recovery;
+ *		for Tx, 0 unless the protocol driver _requires_ short-packet
+ *		termination mode
+ * @dma_addr:	DMA address of buffer
+ * @length:	length of buffer
+ *
+ * Context: controller IRQ-locked
+ */
+static int cppi41_channel_program(struct dma_channel *channel,	u16 maxpacket,
+				  u8 mode, dma_addr_t dma_addr, u32 length)
+{
+	struct cppi41_channel *cppi_ch;
+	unsigned queued;
+
+	cppi_ch = container_of(channel, struct cppi41_channel, channel);
+
+	switch (channel->status) {
+	case MUSB_DMA_STATUS_BUS_ABORT:
+	case MUSB_DMA_STATUS_CORE_ABORT:
+		/* Fault IRQ handler should have handled cleanup */
+		WARNING("%cx DMA%d not cleaned up after abort!\n",
+			cppi_ch->transmit ? 'T' : 'R', cppi_ch->ch_num);
+		break;
+	case MUSB_DMA_STATUS_BUSY:
+		WARNING("Program active channel? %cx DMA%d\n",
+			cppi_ch->transmit ? 'T' : 'R', cppi_ch->ch_num);
+		break;
+	case MUSB_DMA_STATUS_UNKNOWN:
+		DBG(1, "%cx DMA%d not allocated!\n",
+		    cppi_ch->transmit ? 'T' : 'R', cppi_ch->ch_num);
+		return 0;
+	case MUSB_DMA_STATUS_FREE:
+		break;
+	}
+
+	channel->status = MUSB_DMA_STATUS_BUSY;
+
+	/* Set the transfer parameters, then queue up the first segment */
+	cppi_ch->start_addr = dma_addr;
+	cppi_ch->curr_offset = 0;
+	cppi_ch->pkt_size = maxpacket;
+	cppi_ch->length = length;
+	cppi_ch->transfer_mode = mode;
+	cppi_ch->zlp_queued = 0;
+	cppi_ch->channel.actual_len = 0;
+
+	/* Tx or Rx channel? */
+	if (cppi_ch->transmit)
+		queued = cppi41_next_tx_segment(cppi_ch);
+	else
+		queued = cppi41_next_rx_segment(cppi_ch);
+
+	return	queued > 0;
+}
+
+static struct usb_pkt_desc *usb_get_pd_ptr(struct cppi41 *cppi,
+					   unsigned long pd_addr)
+{
+	if (pd_addr >= cppi->pd_mem_phys && pd_addr < cppi->pd_mem_phys +
+	    USB_CPPI41_MAX_PD * USB_CPPI41_DESC_ALIGN)
+		return pd_addr - cppi->pd_mem_phys + cppi->pd_mem;
+	else
+		return NULL;
+}
+
+static int usb_check_teardown(struct cppi41_channel *cppi_ch,
+			      unsigned long pd_addr)
+{
+	u32 info;
+	struct cppi41 *cppi = cppi_ch->channel.private_data;
+
+	if (cppi41_get_teardown_info(pd_addr, &info)) {
+		DBG(1, "ERROR: not a teardown descriptor\n");
+		return 0;
+	}
+
+	if ((info & CPPI41_TEARDOWN_TX_RX_MASK) ==
+	    (!cppi_ch->transmit << CPPI41_TEARDOWN_TX_RX_SHIFT) &&
+	    (info & CPPI41_TEARDOWN_DMA_NUM_MASK) ==
+	    (cppi->cppi_info->dma_block << CPPI41_TEARDOWN_DMA_NUM_SHIFT) &&
+	    (info & CPPI41_TEARDOWN_CHAN_NUM_MASK) ==
+	    (cppi->cppi_info->ep_dma_ch[cppi_ch->ch_num] <<
+	     CPPI41_TEARDOWN_CHAN_NUM_SHIFT))
+		return 1;
+
+	DBG(1, "ERROR: unexpected values in teardown descriptor\n");
+	return 0;
+}
+
+/*
+ * We can't handle the channel teardown via the default completion queue in
+ * context of the controller IRQ-locked, so we use the dedicated teardown
+ * completion queue which we can just poll for a teardown descriptor, not
+ * interfering with the Tx completion queue processing.
+ */
+static void usb_tx_ch_teardown(struct cppi41_channel *tx_ch)
+{
+	struct cppi41 *cppi = tx_ch->channel.private_data;
+	struct musb *musb = cppi->musb;
+	void __iomem *reg_base = musb->ctrl_base;
+	u32 td_reg, timeout = 0xfffff;
+	u8 ep_num = tx_ch->ch_num + 1;
+	unsigned long pd_addr;
+
+	/* Initiate teardown for Tx DMA channel */
+	cppi41_dma_ch_teardown(&tx_ch->dma_ch_obj);
+
+	/* Wait for a descriptor to be queued and pop it... */
+	do {
+		td_reg  = musb_readl(reg_base, USB_TEARDOWN_REG);
+		td_reg |= USB_TX_TDOWN_MASK(ep_num);
+		musb_writel(reg_base, USB_TEARDOWN_REG, td_reg);
+
+		pd_addr = cppi41_queue_pop(&cppi->queue_obj);
+	} while (!pd_addr && timeout--);
+
+	if (pd_addr) {
+
+		dprintk("Descriptor (%08lx) popped from teardown completion "
+			"queue\n", pd_addr);
+
+		if (usb_check_teardown(tx_ch, pd_addr)) {
+			dprintk("Teardown Desc (%p) rcvd\n", pd_addr);
+		} else
+			ERR("Invalid PD(%08lx)popped from TearDn completion"
+				"queue\n", pd_addr);
+	} else {
+		if (timeout <= 0)
+			ERR("Teardown Desc not rcvd\n");
+	}
+}
+
+/*
+ * For Rx DMA channels, the situation is more complex: there's only a single
+ * completion queue for all our needs, so we have to temporarily redirect the
+ * completed descriptors to our teardown completion queue, with a possibility
+ * of a completed packet landing there as well...
+ */
+static void usb_rx_ch_teardown(struct cppi41_channel *rx_ch)
+{
+	struct cppi41 *cppi = rx_ch->channel.private_data;
+	u32 timeout = 0xfffff;
+
+	cppi41_dma_ch_default_queue(&rx_ch->dma_ch_obj, 0, cppi->teardownQNum);
+
+	/* Initiate teardown for Rx DMA channel */
+	cppi41_dma_ch_teardown(&rx_ch->dma_ch_obj);
+
+	do {
+		struct usb_pkt_desc *curr_pd;
+		unsigned long pd_addr;
+
+		/* Wait for a descriptor to be queued and pop it... */
+		do {
+			pd_addr = cppi41_queue_pop(&cppi->queue_obj);
+		} while (!pd_addr && timeout--);
+
+		if (timeout <= 0) {
+			ERR("teardown Desc not found\n");
+			break;
+		}
+
+
+		if (pd_addr) {
+			dprintk("Descriptor (%08lx) popped from teardown "
+				"completion queue\n", pd_addr);
+			/*
+			 * We might have popped a completed Rx PD, so check
+			 * if the physical address is within the PD region
+			 * first.  If it's not the case, it must be a teardown
+			 * descriptor...
+			 * */
+			curr_pd = usb_get_pd_ptr(cppi, pd_addr);
+			if (curr_pd == NULL) {
+				if (usb_check_teardown(rx_ch, pd_addr))
+					break;
+				continue;
+			}
+
+			/* Paranoia: check if PD is from the right channel... */
+			if (curr_pd->ch_num != rx_ch->ch_num) {
+				ERR("Unexpected channel %d in Rx PD\n",
+				    curr_pd->ch_num);
+				continue;
+			}
+
+			/* Extract the buffer length from the completed PD */
+			rx_ch->channel.actual_len += curr_pd->hw_desc.buf_len;
+
+			/*
+			 * Return Rx PDs to the software list --
+			 * this is protected by critical section.
+			 */
+			usb_put_free_pd(cppi, curr_pd);
+		}
+	} while (0);
+
+	/* Now restore the default Rx completion queue... */
+	cppi41_dma_ch_default_queue(&rx_ch->dma_ch_obj,
+				cppi->cppi_info->q_mgr,
+				cppi->cppi_info->rx_comp_q[0]);
+}
+
+/*
+ * cppi41_channel_abort
+ *
+ * Context: controller IRQ-locked, endpoint selected.
+ */
+static int cppi41_channel_abort(struct dma_channel *channel)
+{
+	struct cppi41 *cppi;
+	struct cppi41_channel *cppi_ch;
+	struct musb  *musb;
+	void __iomem *reg_base, *epio;
+	unsigned long pd_addr;
+	u32 csr, td_reg;
+	u8 ch_num, ep_num;
+
+	cppi_ch = container_of(channel, struct cppi41_channel, channel);
+	ch_num = cppi_ch->ch_num;
+
+	switch (channel->status) {
+	case MUSB_DMA_STATUS_BUS_ABORT:
+	case MUSB_DMA_STATUS_CORE_ABORT:
+		/* From Rx or Tx fault IRQ handler */
+	case MUSB_DMA_STATUS_BUSY:
+		/* The hardware needs shutting down... */
+		dprintk("%s: DMA busy, status = %x\n",
+			__func__, channel->status);
+		break;
+	case MUSB_DMA_STATUS_UNKNOWN:
+		DBG(1, "%cx DMA%d not allocated\n",
+		    cppi_ch->transmit ? 'T' : 'R', ch_num);
+		/* FALLTHROUGH */
+	case MUSB_DMA_STATUS_FREE:
+		return 0;
+	}
+
+	cppi = cppi_ch->channel.private_data;
+	musb = cppi->musb;
+	reg_base = musb->ctrl_base;
+	epio = cppi_ch->end_pt->regs;
+	ep_num = ch_num + 1;
+
+#ifdef DEBUG_CPPI_TD
+	printk("Before teardown:");
+	print_pd_list(cppi->pd_pool_head);
+#endif
+
+	if (cppi_ch->transmit) {
+		dprintk("Tx channel teardown, cppi_ch = %p\n", cppi_ch);
+
+		/* Tear down Tx DMA channel */
+		usb_tx_ch_teardown(cppi_ch);
+
+		/* Issue CPPI FIFO teardown for Tx channel */
+		td_reg  = musb_readl(reg_base, USB_TEARDOWN_REG);
+		td_reg |= USB_TX_TDOWN_MASK(ep_num);
+		musb_writel(reg_base, USB_TEARDOWN_REG, td_reg);
+
+		/* Flush FIFO of the endpoint */
+		csr  = musb_readw(epio, MUSB_TXCSR);
+		csr |= MUSB_TXCSR_FLUSHFIFO | MUSB_TXCSR_H_WZC_BITS;
+		musb_writew(epio, MUSB_TXCSR, csr);
+		musb_writew(epio, MUSB_TXCSR, csr);
+	} else { /* Rx */
+		dprintk("Rx channel teardown, cppi_ch = %p\n", cppi_ch);
+
+		/* Flush FIFO of the endpoint */
+		csr  = musb_readw(epio, MUSB_RXCSR);
+		csr |= MUSB_RXCSR_FLUSHFIFO | MUSB_RXCSR_H_WZC_BITS;
+		musb_writew(epio, MUSB_RXCSR, csr);
+		musb_writew(epio, MUSB_RXCSR, csr);
+
+		/* Issue CPPI FIFO teardown for Rx channel */
+		td_reg  = musb_readl(reg_base, USB_TEARDOWN_REG);
+		td_reg |= USB_RX_TDOWN_MASK(ep_num);
+		musb_writel(reg_base, USB_TEARDOWN_REG, td_reg);
+
+		/* Tear down Rx DMA channel */
+		usb_rx_ch_teardown(cppi_ch);
+
+		/*
+		 * NOTE: docs don't guarantee any of this works...  we expect
+		 * that if the USB core stops telling the CPPI core to pull
+		 * more data from it, then it'll be safe to flush current Rx
+		 * DMA state iff any pending FIFO transfer is done.
+		 */
+
+		/* For host, ensure ReqPkt is never set again */
+		cppi41_autoreq_update(cppi_ch, USB_NO_AUTOREQ);
+
+		/* For host, clear (just) ReqPkt at end of current packet(s) */
+		if (is_host_active(cppi->musb))
+			csr &= ~MUSB_RXCSR_H_REQPKT;
+		csr |= MUSB_RXCSR_H_WZC_BITS;
+
+		/* Clear DMA enable */
+		csr &= ~MUSB_RXCSR_DMAENAB;
+		musb_writew(epio, MUSB_RXCSR, csr);
+
+		/* Flush the FIFO of endpoint once again */
+		csr  = musb_readw(epio, MUSB_RXCSR);
+		csr |= MUSB_RXCSR_FLUSHFIFO | MUSB_RXCSR_H_WZC_BITS;
+		musb_writew(epio, MUSB_RXCSR, csr);
+
+		udelay(50);
+	}
+
+	/*
+	 * There might be PDs in the Rx/Tx source queue that were not consumed
+	 * by the DMA controller -- they need to be recycled properly.
+	 */
+	while ((pd_addr = cppi41_queue_pop(&cppi_ch->queue_obj)) != 0) {
+		struct usb_pkt_desc *curr_pd;
+
+		curr_pd = usb_get_pd_ptr(cppi, pd_addr);
+		if (curr_pd == NULL) {
+			ERR("Invalid PD popped from source queue\n");
+			continue;
+		}
+
+		/*
+		 * Return Rx/Tx PDs to the software list --
+		 * this is protected by critical section.
+		 */
+		dprintk("Returning PD %p to the free PD list\n", curr_pd);
+		usb_put_free_pd(cppi, curr_pd);
+	}
+
+#ifdef DEBUG_CPPI_TD
+	printk("After teardown:");
+	print_pd_list(cppi->pd_pool_head);
+#endif
+
+	/* Re-enable the DMA channel */
+	cppi41_dma_ch_enable(&cppi_ch->dma_ch_obj);
+
+	channel->status = MUSB_DMA_STATUS_FREE;
+
+	return 0;
+}
+
+/**
+ * dma_controller_create - instantiate an object representing DMA controller.
+ */
+struct dma_controller *dma_controller_create(struct musb *musb,
+					     void __iomem *mregs)
+{
+	struct cppi41 *cppi;
+
+	cppi = kzalloc(sizeof *cppi, GFP_KERNEL);
+	if (!cppi)
+		return NULL;
+
+	/* Initialize the CPPI 4.1 DMA controller structure */
+	cppi->musb  = musb;
+	cppi->controller.start = cppi41_controller_start;
+	cppi->controller.stop  = cppi41_controller_stop;
+	cppi->controller.channel_alloc = cppi41_channel_alloc;
+	cppi->controller.channel_release = cppi41_channel_release;
+	cppi->controller.channel_program = cppi41_channel_program;
+	cppi->controller.channel_abort = cppi41_channel_abort;
+
+	cppi->cppi_info = &usb_cppi41_info[musb->id];
+	cppi->en_bd_intr = cppi->cppi_info->bd_intr_ctrl;
+
+	return &cppi->controller;
+}
+
+/**
+ * dma_controller_destroy - destroy a previously instantiated DMA controller
+ * @controller: the controller
+ */
+void dma_controller_destroy(struct dma_controller *controller)
+{
+	struct cppi41 *cppi;
+
+	cppi = container_of(controller, struct cppi41, controller);
+
+	/* Free the CPPI object */
+	kfree(cppi);
+}
+
+static void usb_process_tx_queue(struct cppi41 *cppi, unsigned index)
+{
+	struct cppi41_queue_obj tx_queue_obj;
+	unsigned long pd_addr;
+
+	if (cppi41_queue_init(&tx_queue_obj, cppi->cppi_info->q_mgr,
+			      cppi->cppi_info->tx_comp_q[index])) {
+		DBG(1, "ERROR: cppi41_queue_init failed for "
+		    "Tx completion queue");
+		return;
+	}
+
+	while ((pd_addr = cppi41_queue_pop(&tx_queue_obj)) != 0) {
+		struct usb_pkt_desc *curr_pd;
+		struct cppi41_channel *tx_ch;
+		u8 ch_num, ep_num;
+		u32 length;
+
+		curr_pd = usb_get_pd_ptr(cppi, pd_addr);
+		if (curr_pd == NULL) {
+			ERR("Invalid PD popped from Tx completion queue\n");
+			continue;
+		}
+
+		/* Extract the data from received packet descriptor */
+		ch_num = curr_pd->ch_num;
+		ep_num = curr_pd->ep_num;
+		length = curr_pd->hw_desc.buf_len;
+
+		tx_ch = &cppi->tx_cppi_ch[ch_num];
+		tx_ch->channel.actual_len += length;
+
+		/*
+		 * Return Tx PD to the software list --
+		 * this is protected by critical section
+		 */
+		usb_put_free_pd(cppi, curr_pd);
+
+		if ((tx_ch->curr_offset < tx_ch->length) ||
+		    (tx_ch->transfer_mode && !tx_ch->zlp_queued))
+			cppi41_next_tx_segment(tx_ch);
+		else if (tx_ch->channel.actual_len >= tx_ch->length) {
+			tx_ch->channel.status = MUSB_DMA_STATUS_FREE;
+
+			/*
+			 * We get Tx DMA completion interrupt even when
+			 * data is still in FIFO and not moved out to
+			 * USB bus. As we program the next request we
+			 * flush out and old data in FIFO which affects
+			 * USB functionality. So far, we have obsered
+			 * failure with iperf.
+			 */
+			udelay(20);
+			/* Tx completion routine callback */
+			musb_dma_completion(cppi->musb, ep_num, 1);
+		}
+	}
+}
+
+static void usb_process_rx_queue(struct cppi41 *cppi, unsigned index)
+{
+	struct cppi41_queue_obj rx_queue_obj;
+	unsigned long pd_addr;
+
+	if (cppi41_queue_init(&rx_queue_obj, cppi->cppi_info->q_mgr,
+			      cppi->cppi_info->rx_comp_q[index])) {
+		DBG(1, "ERROR: cppi41_queue_init failed for Rx queue\n");
+		return;
+	}
+
+	while ((pd_addr = cppi41_queue_pop(&rx_queue_obj)) != 0) {
+		struct usb_pkt_desc *curr_pd;
+		struct cppi41_channel *rx_ch;
+		u8 ch_num, ep_num;
+		u32 length;
+
+		curr_pd = usb_get_pd_ptr(cppi, pd_addr);
+		if (curr_pd == NULL) {
+			ERR("Invalid PD popped from Rx completion queue\n");
+			continue;
+		}
+
+		/* Extract the data from received packet descriptor */
+		ch_num = curr_pd->ch_num;
+		ep_num = curr_pd->ep_num;
+		length = curr_pd->hw_desc.buf_len;
+
+		rx_ch = &cppi->rx_cppi_ch[ch_num];
+		rx_ch->channel.actual_len += length;
+
+		if (curr_pd->eop) {
+			curr_pd->eop = 0;
+			/* disable the rx dma schedular */
+			if (is_peripheral_active(cppi->musb)) {
+				cppi41_disable_sched_rx();
+				musb_dma_completion(cppi->musb, ep_num, 0);
+			}
+		}
+
+		/*
+		 * Return Rx PD to the software list --
+		 * this is protected by critical section
+		 */
+		usb_put_free_pd(cppi, curr_pd);
+
+		if (unlikely(rx_ch->channel.actual_len >= rx_ch->length ||
+			     length < curr_pd->hw_desc.orig_buf_len)) {
+			rx_ch->channel.status = MUSB_DMA_STATUS_FREE;
+
+			/* Rx completion routine callback */
+			musb_dma_completion(cppi->musb, ep_num, 0);
+		} else {
+			if (is_peripheral_active(cppi->musb) &&
+				((rx_ch->length - rx_ch->curr_offset) > 0))
+				cppi41_next_rx_segment(rx_ch);
+		}
+	}
+}
+
+/*
+ * cppi41_completion - handle interrupts from the Tx/Rx completion queues
+ *
+ * NOTE: since we have to manually prod the Rx process in the transparent mode,
+ *	 we certainly want to handle the Rx queues first.
+ */
+void cppi41_completion(struct musb *musb, u32 rx, u32 tx)
+{
+	struct cppi41 *cppi;
+	unsigned index;
+
+	cppi = container_of(musb->dma_controller, struct cppi41, controller);
+
+	/* Process packet descriptors from the Rx queues */
+	for (index = 0; rx != 0; rx >>= 1, index++)
+		if (rx & 1)
+			usb_process_rx_queue(cppi, index);
+
+	/* Process packet descriptors from the Tx completion queues */
+	for (index = 0; tx != 0; tx >>= 1, index++)
+		if (tx & 1)
+			usb_process_tx_queue(cppi, index);
+}
diff --git a/drivers/usb/musb/cppi41_dma.h b/drivers/usb/musb/cppi41_dma.h
new file mode 100644
index 0000000..2f23dc8
--- /dev/null
+++ b/drivers/usb/musb/cppi41_dma.h
@@ -0,0 +1,69 @@
+/*
+ * Copyright (C) 2005-2006 by Texas Instruments
+ * Copyright (c) 2008, MontaVista Software, Inc. <source@mvista.com>
+ *
+ * This program is free software; you can distribute it and/or modify it
+ * under the terms of the GNU General Public License (Version 2) as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * for more details.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write to the Free Software Foundation, Inc.,
+ * 59 Temple Place - Suite 330, Boston MA 02111-1307, USA.
+ *
+ */
+
+#ifndef _CPPI41_DMA_H_
+#define _CPPI41_DMA_H_
+
+#ifdef CONFIG_ARCH_TI816X
+#include "ti816x.h"
+#else
+#include <plat/usb.h>
+#endif
+
+/**
+ * struct usb_cppi41_info - CPPI 4.1 USB implementation details
+ * @dma_block:	DMA block number
+ * @ep_dma_ch:	DMA channel numbers used for EPs 1 .. Max_EP
+ * @q_mgr:	queue manager number
+ * @num_tx_comp_q: number of the Tx completion queues
+ * @num_rx_comp_q: number of the Rx queues
+ * @tx_comp_q:	pointer to the list of the Tx completion queue numbers
+ * @rx_comp_q:	pointer to the list of the Rx queue numbers
+ */
+struct usb_cppi41_info {
+	u8 dma_block;
+	u8 ep_dma_ch[USB_CPPI41_NUM_CH];
+	u8 q_mgr;
+	u8 num_tx_comp_q;
+	u8 num_rx_comp_q;
+	const u16 *tx_comp_q;
+	const u16 *rx_comp_q;
+	const u8 bd_intr_ctrl;
+};
+
+extern struct usb_cppi41_info usb_cppi41_info[];
+
+/**
+ * cppi41_completion - Tx/Rx completion queue interrupt handling hook
+ * @musb:	the controller
+ * @rx:		bitmask having bit N set if Rx queue N is not empty
+ * @tx:		bitmask having bit N set if Tx completion queue N is not empty
+ */
+void cppi41_completion(struct musb *musb, u32 rx, u32 tx);
+
+/**
+ * cppi41_disable_sched_rx
+ */
+int cppi41_disable_sched_rx(void);
+
+/**
+ * cppi41_enable_sched_rx
+ */
+int cppi41_enable_sched_rx(void);
+#endif	/* _CPPI41_DMA_H_ */
diff --git a/drivers/usb/musb/musb_core.c b/drivers/usb/musb/musb_core.c
index 685dcad..25ae738 100644
--- a/drivers/usb/musb/musb_core.c
+++ b/drivers/usb/musb/musb_core.c
@@ -114,6 +114,9 @@
 
 #define TA_WAIT_BCON(m) max_t(int, (m)->a_wait_bcon, OTG_TIME_A_WAIT_BCON)
 
+#ifdef CONFIG_ARCH_TI816X
+#include "ti816x.h"
+#endif
 
 unsigned musb_debug;
 module_param_named(debug, musb_debug, uint, S_IRUGO | S_IWUSR);
@@ -191,6 +194,27 @@ void musb_write_fifo(struct musb_hw_ep *hw_ep, u16 len, const u8 *src)
 	}
 }
 
+static inline void musb_fifo_read_unaligned(void __iomem *fifo,
+						void __iomem *buf, u16 len)
+{
+	u32		val;
+	int		i;
+
+	if (len > 4) {
+		for (i = 0; i < (len >> 2); i++) {
+			val = musb_readl(fifo, 0);
+			memcpy(buf, &val, 4);
+			buf += 4;
+		}
+		len %= 4;
+	}
+	if (len > 0) {
+		/* Read the rest 1 - 3 bytes from FIFO */
+		val = musb_readl(fifo, 0);
+		memcpy(buf, &val, len);
+	}
+}
+
 /*
  * Unload an endpoint's FIFO
  */
@@ -201,8 +225,10 @@ void musb_read_fifo(struct musb_hw_ep *hw_ep, u16 len, u8 *dst)
 	DBG(4, "%cX ep%d fifo %p count %d buf %p\n",
 			'R', hw_ep->epnum, fifo, len, dst);
 
-	/* we can't assume unaligned writes work */
-	if (likely((0x01 & (unsigned long) dst) == 0)) {
+	if (cpu_is_ti816x()) {
+		musb_fifo_read_unaligned(fifo, dst, len);
+		return;
+	} else if (likely((0x01 & (unsigned long) dst) == 0)) {
 		u16	index = 0;
 
 		/* best case is 32bit-aligned destination address */
@@ -562,6 +588,8 @@ static irqreturn_t musb_stage0_irq(struct musb *musb, u8 int_usb,
 	}
 
 #endif
+
+#ifndef CONFIG_ARCH_TI816X
 	if (int_usb & MUSB_INTR_SUSPEND) {
 		DBG(1, "SUSPEND (%s) devctl %02x power %02x\n",
 				otg_state_string(musb), devctl, power);
@@ -613,6 +641,7 @@ static irqreturn_t musb_stage0_irq(struct musb *musb, u8 int_usb,
 			break;
 		}
 	}
+#endif /* CONFIG_ARCH_TI816X */
 
 #ifdef CONFIG_USB_MUSB_HDRC_HCD
 	if (int_usb & MUSB_INTR_CONNECT) {
@@ -681,6 +710,7 @@ static irqreturn_t musb_stage0_irq(struct musb *musb, u8 int_usb,
 	}
 #endif	/* CONFIG_USB_MUSB_HDRC_HCD */
 
+#ifndef CONFIG_ARCH_TI816X
 	if ((int_usb & MUSB_INTR_DISCONNECT) && !musb->ignore_disconnect) {
 		DBG(1, "DISCONNECT (%s) as %s, devctl %02x\n",
 				otg_state_string(musb),
@@ -722,12 +752,15 @@ static irqreturn_t musb_stage0_irq(struct musb *musb, u8 int_usb,
 			break;
 		}
 	}
+#endif /* CONFIG_ARCH_TI816X */
 
 	/* mentor saves a bit: bus reset and babble share the same irq.
 	 * only host sees babble; only peripheral sees bus reset.
 	 */
 	if (int_usb & MUSB_INTR_RESET) {
+#ifndef CONFIG_ARCH_TI816X
 		handled = IRQ_HANDLED;
+#endif
 		if (is_host_capable() && (devctl & MUSB_DEVCTL_HM) != 0) {
 			/*
 			 * Looks like non-HS BABBLE can be ignored, but
@@ -783,6 +816,10 @@ static irqreturn_t musb_stage0_irq(struct musb *musb, u8 int_usb,
 					otg_state_string(musb));
 			}
 		}
+
+#ifdef CONFIG_ARCH_TI816X
+		handled = IRQ_HANDLED;
+#endif
 	}
 
 #if 0
@@ -835,6 +872,116 @@ static irqreturn_t musb_stage0_irq(struct musb *musb, u8 int_usb,
 	return handled;
 }
 
+#ifdef CONFIG_ARCH_TI816X
+/*
+ * Interrupt Service Routine to record USB "global" interrupts.
+ * Since these do not happen often and signify things of
+ * paramount importance, it seems OK to check them individually;
+ * the order of the tests is specified in the manual
+ *
+ * @param musb instance pointer
+ * @param int_usb register contents
+ * @param devctl
+ * @param power
+ */
+static irqreturn_t musb_stage2_irq(struct musb *musb, u8 int_usb,
+				u8 devctl, u8 power)
+{
+	irqreturn_t handled = IRQ_NONE;
+#endif
+
+	if ((int_usb & MUSB_INTR_DISCONNECT) && !musb->ignore_disconnect) {
+		DBG(1, "DISCONNECT (%s) as %s, devctl %02x\n",
+				otg_state_string(musb),
+				MUSB_MODE(musb), devctl);
+		handled = IRQ_HANDLED;
+
+		switch (musb->xceiv->state) {
+#ifdef CONFIG_USB_MUSB_HDRC_HCD
+		case OTG_STATE_A_HOST:
+		case OTG_STATE_A_SUSPEND:
+			usb_hcd_resume_root_hub(musb_to_hcd(musb));
+			musb_root_disconnect(musb);
+			if (musb->a_wait_bcon != 0 && is_otg_enabled(musb))
+				musb_platform_try_idle(musb, jiffies
+					+ msecs_to_jiffies(musb->a_wait_bcon));
+			break;
+#endif	/* HOST */
+#ifdef CONFIG_USB_MUSB_OTG
+		case OTG_STATE_B_HOST:
+			musb_hnp_stop(musb);
+			break;
+		case OTG_STATE_A_PERIPHERAL:
+			musb_hnp_stop(musb);
+			musb_root_disconnect(musb);
+			/* FALLTHROUGH */
+		case OTG_STATE_B_WAIT_ACON:
+			/* FALLTHROUGH */
+#endif	/* OTG */
+#ifdef CONFIG_USB_GADGET_MUSB_HDRC
+		case OTG_STATE_B_PERIPHERAL:
+		case OTG_STATE_B_IDLE:
+			printk(KERN_INFO "musb %s gadget disconnected.\n",
+				musb->gadget_driver
+				? musb->gadget_driver->driver.name
+				: "");
+			musb_g_disconnect(musb);
+			break;
+#endif	/* GADGET */
+		default:
+			WARNING("unhandled DISCONNECT transition (%s)\n",
+				otg_state_string(musb));
+			break;
+		}
+
+		schedule_work(&musb->irq_work);
+	}
+
+	if (int_usb & MUSB_INTR_SUSPEND) {
+		DBG(1, "SUSPEND (%s) devctl %02x power %02x\n",
+				otg_state_string(musb), devctl, power);
+		handled = IRQ_HANDLED;
+
+		switch (musb->xceiv->state) {
+#ifdef	CONFIG_USB_MUSB_OTG
+		case OTG_STATE_A_PERIPHERAL:
+			/*
+			 * We cannot stop HNP here, devctl BDEVICE might be
+			 * still set.
+			 */
+			break;
+#endif
+		case OTG_STATE_B_PERIPHERAL:
+			musb_g_suspend(musb);
+			musb->is_active = is_otg_enabled(musb)
+					&& musb->xceiv->gadget->b_hnp_enable;
+			break;
+		case OTG_STATE_A_WAIT_BCON:
+			if (musb->a_wait_bcon != 0)
+				musb_platform_try_idle(musb, jiffies
+					+ msecs_to_jiffies(musb->a_wait_bcon));
+			break;
+		case OTG_STATE_A_HOST:
+			musb->xceiv->state = OTG_STATE_A_SUSPEND;
+			musb->is_active = is_otg_enabled(musb)
+					&& musb->xceiv->host->b_hnp_enable;
+			break;
+		case OTG_STATE_B_HOST:
+			/* Transition to B_PERIPHERAL, see 6.8.2.6 p 44 */
+			DBG(1, "REVISIT: SUSPEND as B_HOST\n");
+			break;
+		default:
+			/* "should not happen" */
+			musb->is_active = 0;
+			break;
+		}
+		schedule_work(&musb->irq_work);
+	}
+
+
+	return handled;
+}
+
 /*-------------------------------------------------------------------------*/
 
 /*
@@ -962,7 +1109,7 @@ static void musb_shutdown(struct platform_device *pdev)
  * We don't currently use dynamic fifo setup capability to do anything
  * more than selecting one of a bunch of predefined configurations.
  */
-#if defined(CONFIG_USB_TUSB6010) || \
+#if defined(CONFIG_USB_TUSB6010) || defined(CONFIG_ARCH_TI816X) || \
 	defined(CONFIG_ARCH_OMAP2430) || defined(CONFIG_ARCH_OMAP3)
 static ushort __initdata fifo_mode = 4;
 #else
@@ -1551,6 +1698,13 @@ irqreturn_t musb_interrupt(struct musb *musb)
 		ep_num++;
 	}
 
+#ifdef CONFIG_ARCH_TI816X
+	/* finish handling "global" interrupts after handling fifos */
+	if (musb->int_usb)
+		retval |= musb_stage2_irq(musb,
+				musb->int_usb, devctl, power);
+#endif
+
 	return retval;
 }
 
@@ -1570,7 +1724,7 @@ void musb_dma_completion(struct musb *musb, u8 epnum, u8 transmit)
 
 	if (!epnum) {
 #ifndef CONFIG_USB_TUSB_OMAP_DMA
-		if (!is_cppi_enabled()) {
+		if (!is_cppi_enabled() && !is_cppi41_enabled()) {
 			/* endpoint 0 */
 			if (devctl & MUSB_DEVCTL_HM)
 				musb_h_ep0_irq(musb);
@@ -1830,6 +1984,11 @@ static void musb_free(struct musb *musb)
 #endif
 
 #ifdef CONFIG_USB_MUSB_HDRC_HCD
+#ifdef CONFIG_ARCH_TI816X
+	if (musb->gb_queue)
+		destroy_workqueue(musb->gb_queue);
+	free_queue(musb);
+#endif
 	usb_put_hcd(musb_to_hcd(musb));
 #else
 	kfree(musb);
@@ -1844,9 +2003,15 @@ static void musb_free(struct musb *musb)
  * @mregs: virtual address of controller registers,
  *	not yet corrected for platform-specific offsets
  */
+#ifndef CONFIG_ARCH_TI816X
 static int __init
 musb_init_controller(struct device *dev, int nIrq, void __iomem *ctrl,
 			phys_addr_t ctrl_phys_addr)
+#else
+static int __init
+musb_init_controller(struct device *dev, int nIrq, void __iomem *ctrl,
+			u8 musb_id)
+#endif
 {
 	int			status;
 	struct musb		*musb;
@@ -1894,11 +2059,19 @@ bad_config:
 	}
 
 	spin_lock_init(&musb->lock);
+#ifndef CONFIG_ARCH_TI816X
 	musb->ctrl_phys_base = ctrl_phys_addr;
+#endif
+#ifdef CONFIG_USB_MUSB_HDRC_HCD
+	spin_lock_init(&musb->qlock);
+#endif
 	musb->board_mode = plat->mode;
 	musb->board_set_power = plat->set_power;
 	musb->set_clock = plat->set_clock;
 	musb->min_power = plat->min_power;
+#ifdef CONFIG_ARCH_TI816X
+	musb->id	= musb_id;
+#endif
 
 	/* Clock usage is chip-specific ... functional clock (DaVinci,
 	 * OMAP2430), or PHY ref (some TUSB6010 boards).  All this core
@@ -2043,6 +2216,21 @@ bad_config:
 		goto fail4;
 #endif
 
+#if CONFIG_ARCH_TI816X
+#ifdef CONFIG_USB_MUSB_HDRC_HCD
+	musb->gb_queue = create_singlethread_workqueue(dev_name(dev));
+	if (musb->gb_queue == NULL)
+		goto fail2;
+	/* Init giveback workqueue */
+	INIT_WORK(&musb->gb_work, musb_gb_work);
+
+	/* init queue */
+	init_queue(musb);
+	if (!musb->qhead)
+		goto fail3;
+#endif
+#endif
+
 	dev_info(dev, "USB %s mode controller at %p using %s, IRQ %d\n",
 			({char *s;
 			 switch (musb->board_mode) {
@@ -2064,6 +2252,9 @@ fail4:
 		musb_gadget_cleanup(musb);
 
 fail3:
+#ifdef CONFIG_ARCH_TI816X
+	destroy_workqueue(musb->gb_queue);
+#endif
 	if (musb->irq_wake)
 		device_init_wakeup(dev, 0);
 	musb_platform_exit(musb);
@@ -2101,6 +2292,9 @@ static int __init musb_probe(struct platform_device *pdev)
 	int		status;
 	struct resource	*iomem;
 	void __iomem	*base;
+#ifdef CONFIG_ARCH_TI816X
+	u8	musb_id = pdev->id;
+#endif
 
 	iomem = platform_get_resource(pdev, IORESOURCE_MEM, 0);
 	if (!iomem || irq == 0)
@@ -2116,7 +2310,11 @@ static int __init musb_probe(struct platform_device *pdev)
 	/* clobbered by use_dma=n */
 	orig_dma_mask = dev->dma_mask;
 #endif
+#ifdef CONFIG_ARCH_TI816X
+	status = musb_init_controller(dev, irq, base, musb_id);
+#else
 	status = musb_init_controller(dev, irq, base, iomem->start);
+#endif
 	if (status < 0)
 		iounmap(base);
 
@@ -2385,6 +2583,8 @@ static int __init musb_init(void)
 		"pio"
 #elif defined(CONFIG_USB_TI_CPPI_DMA)
 		"cppi-dma"
+#elif defined(CONFIG_USB_TI_CPPI41_DMA)
+		"cppi4.1-dma"
 #elif defined(CONFIG_USB_INVENTRA_DMA)
 		"musb-dma"
 #elif defined(CONFIG_USB_TUSB_OMAP_DMA)
diff --git a/drivers/usb/musb/musb_core.h b/drivers/usb/musb/musb_core.h
index c2bd174..9086e69 100644
--- a/drivers/usb/musb/musb_core.h
+++ b/drivers/usb/musb/musb_core.h
@@ -201,7 +201,11 @@ enum musb_g_ep0_state {
  * sections 5.5 "Device Timings" and 6.6.5 "Timers".
  */
 #define OTG_TIME_A_WAIT_VRISE	100		/* msec (max) */
+#ifndef CONFIG_ARCH_TI816X
 #define OTG_TIME_A_WAIT_BCON	1100		/* min 1 second */
+#else
+#define OTG_TIME_A_WAIT_BCON	0		/* 0=infinite; min 1000 msec */
+#endif
 #define OTG_TIME_A_AIDL_BDIS	200		/* min 200 msec */
 #define OTG_TIME_B_ASE0_BRST	100		/* min 3.125 ms */
 
@@ -213,7 +217,8 @@ enum musb_g_ep0_state {
  */
 
 #if defined(CONFIG_ARCH_DAVINCI) || defined(CONFIG_ARCH_OMAP2430) \
-		|| defined(CONFIG_ARCH_OMAP3430) || defined(CONFIG_BLACKFIN)
+		|| defined(CONFIG_ARCH_OMAP3430) || defined(CONFIG_BLACKFIN) \
+		|| defined(CONFIG_ARCH_TI816X)
 /* REVISIT indexed access seemed to
  * misbehave (on DaVinci) for at least peripheral IN ...
  */
@@ -322,6 +327,16 @@ static inline struct usb_request *next_out_request(struct musb_hw_ep *hw_ep)
 #endif
 }
 
+#ifdef CONFIG_USB_MUSB_HDRC_HCD
+/*
+ * struct queue - Queue data structure
+ */
+struct queue {
+	struct urb *urb;
+	struct queue *next;
+};
+#endif
+
 /*
  * struct musb - Driver instance data.
  */
@@ -331,6 +346,13 @@ struct musb {
 	struct clk		*clock;
 	irqreturn_t		(*isr)(int, void *);
 	struct work_struct	irq_work;
+#define MUSB_HWVERS_MAJOR(x)	((x >> 10) & 0x1f)
+#define MUSB_HWVERS_MINOR(x)	(x & 0x3ff)
+#define MUSB_HWVERS_RC		0x8000
+#define MUSB_HWVERS_1300	0x52C
+#define MUSB_HWVERS_1400	0x590
+#define MUSB_HWVERS_1800	0x720
+#define MUSB_HWVERS_2000	0x800
 	u16			hwvers;
 
 /* this hub status bit is reserved by USB 2.0 and not seen by usbcore */
@@ -356,6 +378,10 @@ struct musb {
 	struct list_head	out_bulk;	/* of musb_qh */
 
 	struct timer_list	otg_timer;
+	struct workqueue_struct *gb_queue;
+	struct work_struct      gb_work;
+	spinlock_t		qlock;
+	struct queue		*qhead;
 #endif
 
 	/* called with IRQs blocked; ON/nonzero implies starting a session,
@@ -417,12 +443,20 @@ struct musb {
 	unsigned		dyn_fifo:1;	/* dynamic FIFO supported? */
 
 	unsigned		bulk_split:1;
+#ifdef C_MP_TX
 #define	can_bulk_split(musb,type) \
 	(((type) == USB_ENDPOINT_XFER_BULK) && (musb)->bulk_split)
+#else
+#define	can_bulk_split(musb, type)	0
+#endif
 
 	unsigned		bulk_combine:1;
+#ifdef C_MP_RX
 #define	can_bulk_combine(musb,type) \
 	(((type) == USB_ENDPOINT_XFER_BULK) && (musb)->bulk_combine)
+#else
+#define	can_bulk_combine(musb, type)	0
+#endif
 
 #ifdef CONFIG_USB_GADGET_MUSB_HDRC
 	/* is_suspended means USB B_PERIPHERAL suspend */
@@ -455,6 +489,7 @@ struct musb {
 #ifdef MUSB_CONFIG_PROC_FS
 	struct proc_dir_entry *proc_entry;
 #endif
+	u8	id;			/* id for multiple musb instances */
 };
 
 #ifdef CONFIG_PM
@@ -486,7 +521,7 @@ struct musb_context_registers {
 };
 
 #if defined(CONFIG_ARCH_OMAP2430) || defined(CONFIG_ARCH_OMAP3) || \
-    defined(CONFIG_ARCH_OMAP4)
+    defined(CONFIG_ARCH_OMAP4) || defined(CONFIG_ARCH_TI816X)
 extern void musb_platform_save_context(struct musb *musb,
 		struct musb_context_registers *musb_context);
 extern void musb_platform_restore_context(struct musb *musb,
@@ -613,5 +648,8 @@ extern int musb_platform_get_vbus_status(struct musb *musb);
 
 extern int __init musb_platform_init(struct musb *musb);
 extern int musb_platform_exit(struct musb *musb);
+#ifdef CONFIG_USB_MUSB_HDRC_HCD
+extern void musb_gb_work(struct work_struct *data);
+#endif
 
 #endif	/* __MUSB_CORE_H__ */
diff --git a/drivers/usb/musb/musb_dma.h b/drivers/usb/musb/musb_dma.h
index 916065b..6d40dee 100644
--- a/drivers/usb/musb/musb_dma.h
+++ b/drivers/usb/musb/musb_dma.h
@@ -74,6 +74,12 @@ struct musb_hw_ep;
 #define	is_cppi_enabled()	0
 #endif
 
+#ifdef CONFIG_USB_TI_CPPI41_DMA
+#define is_cppi41_enabled()     1
+#else
+#define is_cppi41_enabled()     0
+#endif
+
 #ifdef CONFIG_USB_TUSB_OMAP_DMA
 #define tusb_dma_omap()			1
 #else
diff --git a/drivers/usb/musb/musb_host.c b/drivers/usb/musb/musb_host.c
index 877d20b..9d3ba70 100644
--- a/drivers/usb/musb/musb_host.c
+++ b/drivers/usb/musb/musb_host.c
@@ -99,7 +99,41 @@
 static void musb_ep_program(struct musb *musb, u8 epnum,
 			struct urb *urb, int is_out,
 			u8 *buf, u32 offset, u32 len);
+struct queue *create(void)
+{
+	struct queue *new;
+	new = kmalloc(sizeof(struct queue), GFP_ATOMIC);
+	if (!new)
+		return NULL;
+	new->next = NULL;
+	return new;
+}
+
+struct urb *pop_queue(struct musb *musb)
+{
+	struct urb *urb;
+	struct queue *head = musb->qhead;
+	struct queue *temp;
+	unsigned long flags;
+
+	spin_lock_irqsave(&musb->qlock, flags);
+	temp = head->next;
+	if (!temp) {
+		spin_unlock_irqrestore(&musb->qlock, flags);
+		return NULL;
+	}
+	head->next = head->next->next;
+	spin_unlock_irqrestore(&musb->qlock, flags);
+
+	urb = temp->urb;
+	kfree(temp);
+	return urb;
+}
 
+void init_queue(struct musb *musb)
+{
+	musb->qhead = create();
+}
 /*
  * Clear TX fifo. Needed to avoid BABBLE errors.
  */
@@ -163,7 +197,8 @@ static inline void musb_h_tx_start(struct musb_hw_ep *ep)
 		txcsr |= MUSB_TXCSR_TXPKTRDY | MUSB_TXCSR_H_WZC_BITS;
 		musb_writew(ep->regs, MUSB_TXCSR, txcsr);
 	} else {
-		txcsr = MUSB_CSR0_H_SETUPPKT | MUSB_CSR0_TXPKTRDY;
+		txcsr = MUSB_CSR0_H_SETUPPKT | MUSB_CSR0_TXPKTRDY
+				| MUSB_CSR0_DISPING;
 		musb_writew(ep->regs, MUSB_CSR0, txcsr);
 	}
 
@@ -176,7 +211,7 @@ static inline void musb_h_tx_dma_start(struct musb_hw_ep *ep)
 	/* NOTE: no locks here; caller should lock and select EP */
 	txcsr = musb_readw(ep->regs, MUSB_TXCSR);
 	txcsr |= MUSB_TXCSR_DMAENAB | MUSB_TXCSR_H_WZC_BITS;
-	if (is_cppi_enabled())
+	if (is_cppi_enabled() || is_cppi41_enabled())
 		txcsr |= MUSB_TXCSR_DMAMODE;
 	musb_writew(ep->regs, MUSB_TXCSR, txcsr);
 }
@@ -290,7 +325,8 @@ start:
 
 		if (!hw_ep->tx_channel)
 			musb_h_tx_start(hw_ep);
-		else if (is_cppi_enabled() || tusb_dma_omap())
+		else if (is_cppi_enabled() ||
+				tusb_dma_omap() || is_cppi41_enabled())
 			musb_h_tx_dma_start(hw_ep);
 	}
 }
@@ -329,6 +365,19 @@ __acquires(musb->lock)
 	spin_lock(&musb->lock);
 }
 
+void free_queue(struct musb *musb)
+{
+	struct urb *urb;
+
+	/* giveback any urb still in queue */
+	while ((urb = pop_queue(musb)) != 0)
+		musb_giveback(musb, urb, 0);
+
+	/* free up the queue head memory */
+	kfree(musb->qhead);
+	musb->qhead = NULL;
+}
+
 /* For bulk/interrupt endpoints only */
 static inline void musb_save_toggle(struct musb_qh *qh, int is_in,
 				    struct urb *urb)
@@ -348,6 +397,15 @@ static inline void musb_save_toggle(struct musb_qh *qh, int is_in,
 
 	usb_settoggle(urb->dev, qh->epnum, !is_in, csr ? 1 : 0);
 }
+/* Used to complete urb giveback */
+void musb_gb_work(struct work_struct *data)
+{
+	struct musb *musb = container_of(data, struct musb, gb_work);
+	struct urb *urb;
+
+	while ((urb = pop_queue(musb)) != 0)
+		musb_giveback(musb, urb, 0);
+}
 
 /*
  * Advance this hardware endpoint's queue, completing the specified URB and
@@ -646,7 +704,7 @@ static bool musb_tx_dma_program(struct dma_controller *dma,
 	channel->desired_mode = mode;
 	musb_writew(epio, MUSB_TXCSR, csr);
 #else
-	if (!is_cppi_enabled() && !tusb_dma_omap())
+	if (!is_cppi_enabled() && !tusb_dma_omap() && !is_cppi41_enabled())
 		return false;
 
 	channel->actual_len = 0;
@@ -832,7 +890,8 @@ static void musb_ep_program(struct musb *musb, u8 epnum,
 			csr = musb_readw(hw_ep->regs, MUSB_RXCSR);
 
 			if (csr & (MUSB_RXCSR_RXPKTRDY
-					| MUSB_RXCSR_DMAENAB
+				| is_cppi41_enabled()
+					? 0 : MUSB_RXCSR_DMAENAB
 					| MUSB_RXCSR_H_REQPKT))
 				ERR("broken !rx_reinit, ep%d csr %04x\n",
 						hw_ep->epnum, csr);
@@ -843,7 +902,8 @@ static void musb_ep_program(struct musb *musb, u8 epnum,
 
 		/* kick things off */
 
-		if ((is_cppi_enabled() || tusb_dma_omap()) && dma_channel) {
+		if ((is_cppi_enabled() || is_cppi41_enabled() ||
+				tusb_dma_omap()) && dma_channel) {
 			/* candidate for DMA */
 			if (dma_channel) {
 				dma_channel->actual_len = 0L;
@@ -1308,7 +1368,8 @@ void musb_host_tx(struct musb *musb, u8 epnum)
 	} else	if (usb_pipeisoc(pipe) && dma) {
 		if (musb_tx_dma_program(musb->dma_controller, hw_ep, qh, urb,
 				offset, length)) {
-			if (is_cppi_enabled() || tusb_dma_omap())
+			if (is_cppi_enabled() || tusb_dma_omap() ||
+					is_cppi41_enabled())
 				musb_h_tx_dma_start(hw_ep);
 			return;
 		}
@@ -1571,6 +1632,10 @@ void musb_host_rx(struct musb *musb, u8 epnum)
 			| MUSB_RXCSR_H_AUTOREQ
 			| MUSB_RXCSR_AUTOCLEAR
 			| MUSB_RXCSR_RXPKTRDY);
+
+		if (is_cppi41_enabled())
+			val |= MUSB_RXCSR_DMAENAB;
+
 		musb_writew(hw_ep->regs, MUSB_RXCSR, val);
 
 #ifdef CONFIG_USB_INVENTRA_DMA
@@ -1588,8 +1653,25 @@ void musb_host_rx(struct musb *musb, u8 epnum)
 
 			if (++qh->iso_idx >= urb->number_of_packets)
 				done = true;
-			else
+			else if (is_cppi41_enabled()) {
+				struct dma_controller	*c;
+				void *buf;
+				u32 length, ret;
+
+				c = musb->dma_controller;
+				buf = (void *)
+					urb->iso_frame_desc[qh->iso_idx].offset
+					+ (u32)urb->transfer_dma;
+
+				length =
+					urb->iso_frame_desc[qh->iso_idx].length;
+
+				ret = c->channel_program(dma, qh->maxpacket,
+						0, (u32) buf, length);
 				done = false;
+			} else {
+				done = false;
+			}
 
 		} else  {
 		/* done if urb buffer is full or short packet is recd */
diff --git a/drivers/usb/musb/musb_host.h b/drivers/usb/musb/musb_host.h
index 14b0077..a1a01df 100644
--- a/drivers/usb/musb/musb_host.h
+++ b/drivers/usb/musb/musb_host.h
@@ -83,6 +83,8 @@ static inline struct musb_qh *first_qh(struct list_head *q)
 
 
 extern void musb_root_disconnect(struct musb *musb);
+extern void init_queue(struct musb *musb);
+extern void free_queue(struct musb *musb);
 
 struct usb_hcd;
 
diff --git a/drivers/usb/musb/musb_io.h b/drivers/usb/musb/musb_io.h
index b06e9ef..2949e9e 100644
--- a/drivers/usb/musb/musb_io.h
+++ b/drivers/usb/musb/musb_io.h
@@ -56,7 +56,7 @@ static inline void writesb(const void __iomem *addr, const void *buf, int len)
 
 #endif
 
-#ifndef CONFIG_BLACKFIN
+#if !defined(CONFIG_BLACKFIN) && !defined(CONFIG_ARCH_TI816X)
 
 /* NOTE:  these offsets are all in bytes */
 
@@ -116,7 +116,7 @@ static inline void musb_writeb(void __iomem *addr, unsigned offset, u8 data)
 
 #endif	/* CONFIG_USB_TUSB6010 */
 
-#else
+#elif defined(CONFIG_BLACKFIN)
 
 static inline u8 musb_readb(const void __iomem *addr, unsigned offset)
 	{ return (u8) (bfin_read16(addr + offset)); }
@@ -136,6 +136,19 @@ static inline void musb_writew(void __iomem *addr, unsigned offset, u16 data)
 static inline void musb_writel(void __iomem *addr, unsigned offset, u32 data)
 	{ bfin_write16(addr + offset, (u16) data); }
 
-#endif /* CONFIG_BLACKFIN */
+#elif defined(CONFIG_ARCH_TI816X)
+
+extern u16 musb_readw(const void __iomem *addr, unsigned offset);
+extern void musb_writew(void __iomem *addr, unsigned offset, u16 data);
+extern u8 musb_readb(const void __iomem *addr, unsigned offset);
+extern void musb_writeb(void __iomem *addr, unsigned offset, u8 data);
+
+static inline u32 musb_readl(const void __iomem *addr, unsigned offset)
+	{ return __raw_readl(addr + offset); }
+
+static inline void musb_writel(void __iomem *addr, unsigned offset, u32 data)
+	{ __raw_writel(data, addr + offset); }
+
+#endif /* CONFIG_BLACKFIN || CONFIG_ARCH_TI816X */
 
 #endif
diff --git a/drivers/usb/musb/musb_regs.h b/drivers/usb/musb/musb_regs.h
index fa55aac..5fc3521 100644
--- a/drivers/usb/musb/musb_regs.h
+++ b/drivers/usb/musb/musb_regs.h
@@ -92,6 +92,7 @@
 #define MUSB_FIFOSZ_SIZE	0x0f
 
 /* CSR0 */
+#define MUSB_CSR0_DISPING	0x0800
 #define MUSB_CSR0_FLUSHFIFO	0x0100
 #define MUSB_CSR0_TXPKTRDY	0x0002
 #define MUSB_CSR0_RXPKTRDY	0x0001
diff --git a/drivers/usb/musb/ti816x.c b/drivers/usb/musb/ti816x.c
new file mode 100644
index 0000000..16b0012
--- /dev/null
+++ b/drivers/usb/musb/ti816x.c
@@ -0,0 +1,1227 @@
+/*
+ * Texas Instruments TI816X "glue layer"
+ *
+ * Copyright (c) 2008, MontaVista Software, Inc. <source@mvista.com>
+ *
+ * Based on the DaVinci "glue layer" code.
+ * Copyright (C) 2005-2006 by Texas Instruments
+ *
+ * This file is part of the Inventra Controller Driver for Linux.
+ *
+ * The Inventra Controller Driver for Linux is free software; you
+ * can redistribute it and/or modify it under the terms of the GNU
+ * General Public License version 2 as published by the Free Software
+ * Foundation.
+ *
+ * The Inventra Controller Driver for Linux is distributed in
+ * the hope that it will be useful, but WITHOUT ANY WARRANTY;
+ * without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public
+ * License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with The Inventra Controller Driver for Linux ; if not,
+ * write to the Free Software Foundation, Inc., 59 Temple Place,
+ * Suite 330, Boston, MA  02111-1307  USA
+ *
+ */
+
+#include <linux/init.h>
+#include <linux/clk.h>
+#include <linux/io.h>
+#include <linux/usb/otg.h>
+#include <plat/control.h>
+
+#include "cppi41.h"
+#include "ti816x.h"
+
+#include "musb_core.h"
+#include "cppi41_dma.h"
+
+struct musb *g_musb, *gmusb[2] = {NULL, NULL};
+
+#undef USB_TI816X_DEBUG
+
+#ifdef USB_TI816X_DEBUG
+#define	dprintk(x, ...) printk(x, ## __VA_ARGS__)
+#else
+#define dprintk(x, ...)
+#endif
+
+#define MAX_MUSB_INSTANCE	2
+/* CPPI 4.1 queue manager registers */
+#define QMGR_PEND0_REG		0x4090
+#define QMGR_PEND1_REG		0x4094
+#define QMGR_PEND2_REG		0x4098
+
+#define TI816X_USBCTRL_OFFS			0x0620
+#define	TI816X_USBPHY0_NORMAL_MODE		(1 << 0)
+#define	TI816X_USBPHY1_NORMAL_MODE		(1 << 1)
+#define	TI816X_USBPHY_REFCLK_OSC		(1 << 8)
+#define TI816X_USBPHY_CTRL0_OFFS		0x0624
+#define TI816X_USBPHY_CTRL1_OFFS		0x0628
+
+/* USB 2.0 PHY Control */
+#define CONF2_PHY_GPIOMODE     (1 << 23)
+#define CONF2_OTGMODE          (3 << 14)
+#define CONF2_SESENDEN         (1 << 13)       /* Vsess_end comparator */
+#define CONF2_VBDTCTEN         (1 << 12)       /* Vbus comparator */
+#define CONF2_REFFREQ_24MHZ    (2 << 8)
+#define CONF2_REFFREQ_26MHZ    (7 << 8)
+#define CONF2_REFFREQ_13MHZ    (6 << 8)
+#define CONF2_REFFREQ          (0xf << 8)
+#define CONF2_PHYCLKGD         (1 << 7)
+#define CONF2_VBUSSENSE        (1 << 6)
+#define CONF2_PHY_PLLON        (1 << 5)        /* override PLL suspend */
+#define CONF2_RESET            (1 << 4)
+#define CONF2_PHYPWRDN         (1 << 3)
+#define CONF2_OTGPWRDN         (1 << 2)
+#define CONF2_DATPOL           (1 << 1)
+
+
+#define USB_TX_EP_MASK	0xffff		/* EP0 + 15 Tx EPs */
+#define USB_RX_EP_MASK	0xfffe		/* 15 Rx EPs */
+
+#define USB_TX_INTR_MASK	(USB_TX_EP_MASK << USB_INTR_TX_SHIFT)
+#define USB_RX_INTR_MASK	(USB_RX_EP_MASK << USB_INTR_RX_SHIFT)
+
+#define A_WAIT_BCON_TIMEOUT	1100		/* in ms */
+
+#define USBSS_INTR_RX_STARV	0x00000001
+#define USBSS_INTR_PD_CMPL	0x00000004
+#define USBSS_INTR_TX_CMPL	0x00000500
+#define USBSS_INTR_RX_CMPL	0x00000A00
+#define USBSS_INTR_FLAGS	(USBSS_INTR_PD_CMPL | USBSS_INTR_TX_CMPL \
+					| USBSS_INTR_RX_CMPL)
+#ifdef CONFIG_USB_TI_CPPI41_DMA
+static irqreturn_t cppi41dma_Interrupt(int irq, void *hci);
+#endif
+
+struct usbotg_ss {
+	/* usbss access lock */
+	spinlock_t	lock;
+	void		*base;
+	void		*intc_base;
+	int		init_done;
+};
+static struct usbotg_ss usbss;
+u8 cppi41_init_done;
+
+u32 usbss_read(u32 offs)
+{
+	unsigned long flags;
+	u32 val = 0;
+
+	if (!usbss.init_done)
+		return val;
+
+	spin_lock_irqsave(&usbss.lock, flags);
+	val = *(volatile u32 *)(u32)((u32)usbss.base + offs);
+	spin_unlock_irqrestore(&usbss.lock, flags);
+
+	return val;
+}
+
+void usbss_write(u32 offs, u32 val)
+{
+	unsigned long flags;
+
+	if (!usbss.init_done)
+		return;
+
+	spin_lock_irqsave(&usbss.lock, flags);
+	*(volatile u32 *)((u32)usbss.base + offs) = val;
+	spin_unlock_irqrestore(&usbss.lock, flags);
+}
+
+int usbotg_ss_init(struct musb *musb)
+{
+	int status = 0;
+
+	if (!usbss.init_done) {
+		usbss.base = ioremap(TI816X_USBSS_BASE,
+				TI816X_USBSS_LEN);
+		usbss.intc_base = ioremap(0x50000000, 0x400);
+		spin_lock_init(&usbss.lock);
+		usbss.init_done = 1;
+		/* eoi to usbss */
+		usbss_write(USBSS_IRQ_EOI, 0);
+		/* clear any USBSS interrupts */
+		usbss_write(USBSS_IRQ_STATUS, usbss_read(USBSS_IRQ_STATUS));
+	}
+	return status;
+}
+
+void set_threshold(u8 ctrl_id, u8 epn, u8 count, u8 is_tx)
+{
+	u32 ctrl_base;
+	u32 reg_offset;
+	u32 val;
+
+	if (ctrl_id == 0) {
+		if (is_tx)
+			ctrl_base = USBSS_IRQ_DMA_THRESHOLD_TX0;
+		else
+			ctrl_base = USBSS_IRQ_DMA_THRESHOLD_RX0;
+	} else {
+		if (is_tx)
+			ctrl_base = USBSS_IRQ_DMA_THRESHOLD_TX1;
+		else
+			ctrl_base = USBSS_IRQ_DMA_THRESHOLD_RX1;
+	}
+
+	reg_offset = ctrl_base + (4 * (epn/4));
+	val = usbss_read(reg_offset);
+	val |= count << ((epn % 4) * 8);
+
+	DBG(4, "threshold write (usb%d-%s): offset=0x%x, val=0x%x\n",
+		ctrl_id, is_tx ? "Tx" : "Rx", reg_offset, val);
+
+	usbss_write(reg_offset, val);
+}
+
+/* ti8167 specific read/write functions */
+u16 musb_readw(const void __iomem *addr, unsigned offset)
+{
+	u32 tmp;
+	u16 val;
+
+	tmp = __raw_readl(addr + (offset & ~3));
+
+	switch (offset & 0x3) {
+	case 0:
+		val = (tmp & 0xffff);
+		break;
+	case 1:
+		val = (tmp >> 8) & 0xffff;
+		break;
+	case 2:
+	case 3:
+	default:
+		val = (tmp >> 16) & 0xffff;
+		break;
+	}
+	return val;
+}
+
+void musb_writew(void __iomem *addr, unsigned offset, u16 data)
+{
+	__raw_writew(data, addr + offset);
+}
+
+u8 musb_readb(const void __iomem *addr, unsigned offset)
+{
+	u32 tmp;
+	u8 val;
+
+	tmp = __raw_readl(addr + (offset & ~3));
+
+	switch (offset & 0x3) {
+	case 0:
+		val = tmp & 0xff;
+		break;
+	case 1:
+		val = (tmp >> 8);
+		break;
+	case 2:
+		val = (tmp >> 16);
+		break;
+	case 3:
+	default:
+		val = (tmp >> 24);
+		break;
+	}
+	return val;
+}
+void musb_writeb(void __iomem *addr, unsigned offset, u8 data)
+{
+	__raw_writeb(data, addr + offset);
+}
+
+void set_frame_threshold(u8 musb_id, u8 is_tx, u8 epnum, u8 value, u8 en_intr)
+{
+	u32     base, reg_val, frame_intr = 0, frame_base = 0;
+	u32     offs = epnum/4*4;
+	u8      indx = (epnum % 4) * 8;
+
+	en_intr = 0;
+	if (is_tx)
+		base = (musb_id) ? USBSS_IRQ_FRAME_THRESHOLD_TX1 : \
+					USBSS_IRQ_FRAME_THRESHOLD_TX0;
+	else
+		base = (musb_id) ? USBSS_IRQ_FRAME_THRESHOLD_RX1 : \
+					USBSS_IRQ_FRAME_THRESHOLD_RX0;
+
+	reg_val = usbss_read(base + offs);
+	reg_val &= ~(0xFF << indx);
+	reg_val |= (value << indx);
+	usbss_write(base + offs, reg_val);
+
+	if (en_intr) {
+		frame_base = (musb_id) ? USBSS_IRQ_FRAME_ENABLE_1 :
+			USBSS_IRQ_FRAME_ENABLE_0;
+		frame_intr = (musb_id) ? usbss_read(USBSS_IRQ_FRAME_ENABLE_0) :
+			usbss_read(USBSS_IRQ_FRAME_ENABLE_1);
+		frame_intr |= is_tx ? (1 << epnum) : (1 << (16 + epnum));
+		usbss_write(frame_base, frame_intr);
+		DBG(4, "%s: framebase=%x, frame_intr=%x\n", is_tx ? "tx" : "rx",
+			frame_base, frame_intr);
+	}
+}
+
+void set_dma_threshold(u8 musb_id, u8 is_tx, u8 epnum, u8 value)
+{
+	u32     base, reg_val;
+	u32     offs = epnum/4*4;
+	u8      indx = (epnum % 4) * 8;
+
+	if (is_tx)
+		base = (musb_id) ? USBSS_IRQ_DMA_THRESHOLD_TX1 : \
+					USBSS_IRQ_DMA_THRESHOLD_TX0;
+	else
+		base = (musb_id) ? USBSS_IRQ_DMA_THRESHOLD_RX1 : \
+					USBSS_IRQ_DMA_THRESHOLD_RX0;
+
+	reg_val = usbss_read(base + offs);
+	reg_val &= ~(0xFF << indx);
+	reg_val |= (value << indx);
+	DBG(4, "base=%x, offs=%x, indx=%d, reg_val = (%x)%x\n",
+		base, offs, indx, reg_val, usbss_read(base + offs));
+	usbss_write(base + offs, reg_val);
+}
+
+#ifdef CONFIG_USB_TI_CPPI41_DMA
+/*
+ * CPPI 4.1 resources used for USB OTG controller module:
+ *
+ tx/rx completion queues for usb0 */
+static const u16 tx_comp_q[] = {93, 94, 95, 96, 97,
+				98, 99, 100, 101, 102,
+				103, 104, 105, 106, 107 };
+
+static const u16 rx_comp_q[] = {109, 110, 111, 112, 113,
+				114, 115, 116, 117, 118,
+				119, 120, 121, 122, 123 };
+
+/* tx/rx completion queues for usb1 */
+static const u16 tx_comp_q1[] = {125, 126, 127, 128, 129,
+				 130, 131, 132, 133, 134,
+				 135, 136, 137, 138, 139 };
+
+static const u16 rx_comp_q1[] = {141, 142, 143, 144, 145,
+				 146, 147, 148, 149, 150,
+				 151, 152, 153, 154, 155 };
+
+struct usb_cppi41_info usb_cppi41_info[MAX_MUSB_INSTANCE] = {
+	{
+		.dma_block	= 0,
+		.ep_dma_ch	= {0, 1, 2, 3, 4, 5, 6, 7, 8, 9,
+					10, 11, 12, 13, 14 },
+		.q_mgr		= 0,
+		.num_tx_comp_q	= 15,
+		.num_rx_comp_q	= 15,
+		.tx_comp_q	= tx_comp_q,
+		.rx_comp_q	= rx_comp_q,
+		.bd_intr_ctrl	= 1,
+	},
+	{
+		.dma_block      = 0,
+		.ep_dma_ch      = { 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,
+					25, 26, 27, 28, 29 },
+		.q_mgr          = 0,
+		.num_tx_comp_q  = 15,
+		.num_rx_comp_q  = 15,
+		.tx_comp_q      = tx_comp_q1,
+		.rx_comp_q      = rx_comp_q1,
+		.bd_intr_ctrl	= 1,
+	},
+};
+
+/* Fair scheduling */
+u32 dma_sched_table[] = {
+	0x81018000, 0x83038202, 0x85058404, 0x87078606,
+	0x89098808, 0x8b0b8a0a, 0x8d0d8c0c, 0x8f0f8e0e,
+	0x91119010, 0x93139212, 0x95159414, 0x97179616,
+	0x99199818, 0x9b1b9a1a, 0x9d1d9c1c, 0x00009e1e,
+};
+
+/* DMA block configuration */
+static const struct cppi41_tx_ch tx_ch_info[] = {
+	[0] = {
+		.port_num	= 1,
+		.num_tx_queue	= 2,
+		.tx_queue	= { {0, 32} , {0, 33} }
+	},
+	[1] = {
+		.port_num	= 2,
+		.num_tx_queue	= 2,
+		.tx_queue	= { {0, 34} , {0, 35} }
+	},
+	[2] = {
+		.port_num	= 3,
+		.num_tx_queue	= 2,
+		.tx_queue	= { {0, 36} , {0, 37} }
+	},
+	[3] = {
+		.port_num	= 4,
+		.num_tx_queue	= 2,
+		.tx_queue	= { {0, 38} , {0, 39} }
+	},
+	[4] = {
+		.port_num	= 5,
+		.num_tx_queue	= 2,
+		.tx_queue	= { {0, 40} , {0, 41} }
+	},
+	[5] = {
+		.port_num	= 6,
+		.num_tx_queue	= 2,
+		.tx_queue	= { {0, 42} , {0, 43} }
+	},
+	[6] = {
+		.port_num	= 7,
+		.num_tx_queue	= 2,
+		.tx_queue	= { {0, 44} , {0, 45} }
+	},
+	[7] = {
+		.port_num	= 8,
+		.num_tx_queue	= 2,
+		.tx_queue	= { {0, 46} , {0, 47} }
+	},
+	[8] = {
+		.port_num	= 9,
+		.num_tx_queue	= 2,
+		.tx_queue	= { {0, 48} , {0, 49} }
+	},
+	[9] = {
+		.port_num	= 10,
+		.num_tx_queue	= 2,
+		.tx_queue	= { {0, 50} , {0, 51} }
+	},
+	[10] = {
+		.port_num	= 11,
+		.num_tx_queue	= 2,
+		.tx_queue	= { {0, 52} , {0, 53} }
+	},
+	[11] = {
+		.port_num	= 12,
+		.num_tx_queue	= 2,
+		.tx_queue	= { {0, 54} , {0, 55} }
+	},
+	[12] = {
+		.port_num	= 13,
+		.num_tx_queue	= 2,
+		.tx_queue	= { {0, 56} , {0, 57} }
+	},
+	[13] = {
+		.port_num	= 14,
+		.num_tx_queue	= 2,
+		.tx_queue	= { {0, 58} , {0, 59} }
+	},
+	[14] = {
+		.port_num	= 15,
+		.num_tx_queue	= 2,
+		.tx_queue	= { {0, 60} , {0, 61} }
+	},
+	[15] = {
+		.port_num	= 1,
+		.num_tx_queue	= 2,
+		.tx_queue	= { {0, 62} , {0, 63} }
+	},
+	[16] = {
+		.port_num	= 2,
+		.num_tx_queue	= 2,
+		.tx_queue	= { {0, 64} , {0, 65} }
+	},
+	[17] = {
+		.port_num	= 3,
+		.num_tx_queue	= 2,
+		.tx_queue	= { {0, 66} , {0, 67} }
+	},
+	[18] = {
+		.port_num	= 4,
+		.num_tx_queue	= 2,
+		.tx_queue	= { {0, 68} , {0, 69} }
+	},
+	[19] = {
+		.port_num	= 5,
+		.num_tx_queue	= 2,
+		.tx_queue	= { {0, 70} , {0, 71} }
+	},
+	[20] = {
+		.port_num	= 6,
+		.num_tx_queue	= 2,
+		.tx_queue	= { {0, 72} , {0, 73} }
+	},
+	[21] = {
+		.port_num	= 7,
+		.num_tx_queue	= 2,
+		.tx_queue	= { {0, 74} , {0, 75} }
+	},
+	[22] = {
+		.port_num	= 8,
+		.num_tx_queue	= 2,
+		.tx_queue	= { {0, 76} , {0, 77} }
+	},
+	[23] = {
+		.port_num	= 9,
+		.num_tx_queue	= 2,
+		.tx_queue	= { {0, 78} , {0, 79} }
+	},
+	[24] = {
+		.port_num	= 10,
+		.num_tx_queue	= 2,
+		.tx_queue	= { {0, 80} , {0, 81} }
+	},
+	[25] = {
+		.port_num	= 11,
+		.num_tx_queue	= 2,
+		.tx_queue	= { {0, 82} , {0, 83} }
+	},
+	[26] = {
+		.port_num	= 12,
+		.num_tx_queue	= 2,
+		.tx_queue	= { {0, 84} , {0, 85} }
+	},
+	[27] = {
+		.port_num	= 13,
+		.num_tx_queue	= 2,
+		.tx_queue	= { {0, 86} , {0, 87} }
+	},
+	[28] = {
+		.port_num	= 14,
+		.num_tx_queue	= 2,
+		.tx_queue	= { {0, 88} , {0, 89} }
+	},
+	[29] = {
+		.port_num	= 15,
+		.num_tx_queue	= 2,
+		.tx_queue	= { {0, 90} , {0, 91} }
+	}
+};
+
+struct cppi41_dma_block cppi41_dma_block[CPPI41_NUM_DMA_BLOCK] = {
+	[0] = {
+		.num_tx_ch	= 30,
+		.num_rx_ch	= 30,
+		.tx_ch_info	= tx_ch_info
+	}
+};
+EXPORT_SYMBOL(cppi41_dma_block);
+
+/* Queues 0 to 66 are pre-assigned, others are spare */
+static const u32 assigned_queues[] = {	0xffffffff, /* queue 0..31 */
+					0xffffffff, /* queue 32..63 */
+					0xffffffff, /* queue 64..95 */
+					0xffffffff, /* queue 96..127 */
+					0x0fffffff  /* queue 128..155 */
+					};
+
+/* Queue manager information */
+struct cppi41_queue_mgr cppi41_queue_mgr[CPPI41_NUM_QUEUE_MGR] = {
+	[0] = {
+		.num_queue	= 159,
+		.queue_types	= CPPI41_FREE_DESC_BUF_QUEUE |
+					CPPI41_UNASSIGNED_QUEUE,
+		.base_fdbq_num	= 0,
+		.assigned	= assigned_queues
+	}
+};
+EXPORT_SYMBOL(cppi41_queue_mgr);
+
+
+static void *cppi41_dma_base;
+
+int __init cppi41_init(struct musb *musb)
+{
+	struct usb_cppi41_info *cppi_info = &usb_cppi41_info[musb->id];
+	u16 numch, blknum = cppi_info->dma_block, order;
+	u32 offs = 0x2000;
+	u32 nIrq = TI816X_IRQ_USBSS;
+
+	if (cppi41_init_done)
+		return 0;
+
+	cppi41_dma_base = ioremap(TI816X_USB_CPPIDMA_BASE,
+					TI816X_USB_CPPIDMA_LEN);
+
+	DBG(4, "cppi41_dma_base = %p\n", cppi41_dma_base);
+
+	/* init mappings */
+	cppi41_queue_mgr[0].q_mgr_rgn_base	= (void *)((u32)cppi41_dma_base
+							+ (0x4000 - offs));
+	cppi41_queue_mgr[0].desc_mem_rgn_base	= (void *)((u32)cppi41_dma_base
+							+ (0x5000 - offs));
+	cppi41_queue_mgr[0].q_mgmt_rgn_base	= (void *)((u32)cppi41_dma_base
+							+ (0x6000 - offs));
+	cppi41_queue_mgr[0].q_stat_rgn_base	= (void *)((u32)cppi41_dma_base
+							 + (0x7000 - offs));
+
+	cppi41_dma_block[0].global_ctrl_base	= (void *)((u32)cppi41_dma_base
+							 + (0x2000 - offs));
+	cppi41_dma_block[0].ch_ctrl_stat_base	= (void *)((u32)cppi41_dma_base
+							 + (0x2800 - offs));
+	cppi41_dma_block[0].sched_ctrl_base	= (void *)((u32)cppi41_dma_base
+							 + (0x3000 - offs));
+	cppi41_dma_block[0].sched_table_base	= (void *)((u32)cppi41_dma_base
+							 + (0x3800 - offs));
+
+	DBG(4, "dma-glb-base = %p\n", cppi41_dma_block[0].global_ctrl_base);
+	DBG(4, "q_mgr_rgn_base= %p\n", cppi41_queue_mgr[0].q_mgr_rgn_base);
+
+	/* Initialize for Linking RAM region 0 alone */
+	cppi41_queue_mgr_init(cppi_info->q_mgr, 0, 0x3fff);
+
+	printk(KERN_INFO "cppi41_queue_mgr_init done\n");
+
+	numch =  USB_CPPI41_NUM_CH * 2 * 2;
+	order = get_count_order(numch);
+
+	/* TODO: check two teardown desc per channel (5 or 7 ?)*/
+	if (order < 5)
+		order = 5;
+
+	cppi41_dma_block_init(blknum, cppi_info->q_mgr, order,
+			dma_sched_table, numch);
+
+#ifdef CONFIG_USB_TI_CPPI41_DMA
+	/* attach to the IRQ */
+	if (request_irq(nIrq, cppi41dma_Interrupt, 0, "cppi41_dma", 0))
+		printk(KERN_INFO "request_irq %d failed!\n", nIrq);
+	else
+		printk(KERN_INFO "registerd cppi-dma Intr @ IRQ %d\n", nIrq);
+#endif
+
+
+	/* enable all the interrupts */
+	usbss_write(USBSS_IRQ_EOI, 0);
+	usbss_write(USBSS_IRQ_ENABLE_SET, USBSS_INTR_FLAGS);
+	usbss_write(USBSS_IRQ_DMA_ENABLE_0, 0xFFFeFFFe);
+
+	cppi41_init_done = 1;
+
+	return 0;
+}
+
+void cppi41_free(struct musb *musb)
+{
+	u32 nIrq = TI816X_IRQ_USBSS;
+
+	if (!cppi41_init_done)
+		return ;
+
+	free_irq(nIrq, 0);
+	iounmap(cppi41_dma_base);
+	cppi41_dma_base = 0;
+	cppi41_init_done = 0;
+}
+
+int cppi41_disable_sched_rx(void)
+{
+	cppi41_dma_sched_tbl_init(0, 0, dma_sched_table, 30);
+	return 0;
+}
+
+int cppi41_enable_sched_rx(void)
+{
+	cppi41_dma_sched_tbl_init(0, 0, dma_sched_table, 30);
+	return 0;
+}
+#endif /* CONFIG_USB_TI_CPPI41_DMA */
+
+/*
+ * REVISIT (PM): we should be able to keep the PHY in low power mode most
+ * of the time (24 MHZ oscillator and PLL off, etc) by setting POWER.D0
+ * and, when in host mode, autosuspending idle root ports... PHYPLLON
+ * (overriding SUSPENDM?) then likely needs to stay off.
+ */
+
+static inline void phy_on(void)
+{
+	u32 usbphycfg;
+
+	DBG(4, "phy_on..\n");
+	/*
+	 * Start the on-chip PHY and its PLL.
+	 */
+	usbphycfg = omap_ctrl_readl(TI816X_USBCTRL_OFFS);
+	usbphycfg |= (TI816X_USBPHY0_NORMAL_MODE | TI816X_USBPHY1_NORMAL_MODE);
+	usbphycfg &= ~(TI816X_USBPHY_REFCLK_OSC);
+	omap_ctrl_writel(usbphycfg, TI816X_USBCTRL_OFFS);
+
+	DBG(4, "usbphy_ctrl0=%x\n", omap_ctrl_readl(TI816X_USBPHY_CTRL0_OFFS));
+	DBG(4, "usbphy_ctrl1=%x\n", omap_ctrl_readl(TI816X_USBPHY_CTRL0_OFFS));
+}
+
+static inline void phy_off(void)
+{
+	u32 usbphycfg;
+
+	DBG(4, "phy_off..\n");
+
+	usbphycfg = omap_ctrl_readl(TI816X_USBCTRL_OFFS);
+	usbphycfg &= ~(TI816X_USBPHY0_NORMAL_MODE | TI816X_USBPHY1_NORMAL_MODE
+			| TI816X_USBPHY_REFCLK_OSC);
+	omap_ctrl_writel(usbphycfg, TI816X_USBCTRL_OFFS);
+}
+
+/*
+ * Because we don't set CTRL.UINT, it's "important" to:
+ *	- not read/write INTRUSB/INTRUSBE (except during
+ *	  initial setup, as a workaround);
+ *	- use INTSET/INTCLR instead.
+ */
+
+/**
+ * musb_platform_enable - enable interrupts
+ */
+void musb_platform_enable(struct musb *musb)
+{
+	void __iomem *reg_base = musb->ctrl_base;
+	u32 epmask, coremask;
+
+	/* Workaround: setup IRQs through both register sets. */
+	epmask = ((musb->epmask & USB_TX_EP_MASK) << USB_INTR_TX_SHIFT) |
+	       ((musb->epmask & USB_RX_EP_MASK) << USB_INTR_RX_SHIFT);
+	coremask = (0x01ff << USB_INTR_USB_SHIFT);
+
+	coremask &= ~0x8; /* disable the SOF */
+	coremask |= 0x8;
+
+	musb_writel(reg_base, USB_EP_INTR_SET_REG, epmask);
+	musb_writel(reg_base, USB_CORE_INTR_SET_REG, coremask);
+	/* Force the DRVVBUS IRQ so we can start polling for ID change. */
+	if (is_otg_enabled(musb))
+		musb_writel(reg_base, USB_CORE_INTR_SET_REG,
+			    USB_INTR_DRVVBUS << USB_INTR_USB_SHIFT);
+}
+
+/**
+ * musb_platform_disable - disable HDRC and flush interrupts
+ */
+void musb_platform_disable(struct musb *musb)
+{
+	void __iomem *reg_base = musb->ctrl_base;
+
+	musb_writel(reg_base, USB_CORE_INTR_CLEAR_REG, USB_INTR_USB_MASK);
+	musb_writel(reg_base, USB_EP_INTR_CLEAR_REG,
+			 USB_TX_INTR_MASK | USB_RX_INTR_MASK);
+	musb_writeb(musb->mregs, MUSB_DEVCTL, 0);
+	musb_writel(reg_base, USB_IRQ_EOI, 0);
+}
+
+static int vbus_state = -1;
+
+#ifdef CONFIG_USB_MUSB_HDRC_HCD
+#define portstate(stmt)	stmt
+#else
+#define portstate(stmt)
+#endif
+
+static void ti816x_source_power(struct musb *musb, int is_on, int immediate)
+{
+	if (is_on)
+		is_on = 1;
+
+	if (vbus_state == is_on)
+		return;
+	vbus_state = is_on;		/* 0/1 vs "-1 == unknown/init" */
+}
+
+static void ti816x_set_vbus(struct musb *musb, int is_on)
+{
+	WARN_ON(is_on && is_peripheral_active(musb));
+	ti816x_source_power(musb, is_on, 0);
+}
+
+#define	POLL_SECONDS	2
+
+static struct timer_list otg_workaround;
+
+static void otg_timer(unsigned long _musb)
+{
+	struct musb		*musb = (void *)_musb;
+	void __iomem		*mregs = musb->mregs;
+	u8			devctl;
+	unsigned long		flags;
+
+	/* We poll because DaVinci's won't expose several OTG-critical
+	* status change events (from the transceiver) otherwise.
+	 */
+	devctl = musb_readb(mregs, MUSB_DEVCTL);
+	DBG(7, "Poll devctl %02x (%s)\n", devctl, otg_state_string(musb));
+
+	spin_lock_irqsave(&musb->lock, flags);
+	switch (musb->xceiv->state) {
+	case OTG_STATE_A_WAIT_BCON:
+		devctl &= ~MUSB_DEVCTL_SESSION;
+		musb_writeb(musb->mregs, MUSB_DEVCTL, devctl);
+
+		devctl = musb_readb(musb->mregs, MUSB_DEVCTL);
+		if (devctl & MUSB_DEVCTL_BDEVICE) {
+			musb->xceiv->state = OTG_STATE_B_IDLE;
+			MUSB_DEV_MODE(musb);
+		} else {
+			musb->xceiv->state = OTG_STATE_A_IDLE;
+			MUSB_HST_MODE(musb);
+		}
+		break;
+	case OTG_STATE_A_WAIT_VFALL:
+		/*
+		 * Wait till VBUS falls below SessionEnd (~0.2 V); the 1.3
+		 * RTL seems to mis-handle session "start" otherwise (or in
+		 * our case "recover"), in routine "VBUS was valid by the time
+		 * VBUSERR got reported during enumeration" cases.
+		 */
+		if (devctl & MUSB_DEVCTL_VBUS) {
+			mod_timer(&otg_workaround, jiffies + POLL_SECONDS * HZ);
+			break;
+		}
+		musb->xceiv->state = OTG_STATE_A_WAIT_VRISE;
+		musb_writel(musb->ctrl_base, USB_CORE_INTR_SET_REG,
+			    MUSB_INTR_VBUSERROR << USB_INTR_USB_SHIFT);
+		break;
+	case OTG_STATE_B_IDLE:
+		if (!is_peripheral_enabled(musb))
+			break;
+
+		/*
+		 * There's no ID-changed IRQ, so we have no good way to tell
+		 * when to switch to the A-Default state machine (by setting
+		 * the DEVCTL.SESSION flag).
+		 *
+		 * Workaround:  whenever we're in B_IDLE, try setting the
+		 * session flag every few seconds.  If it works, ID was
+		 * grounded and we're now in the A-Default state machine.
+		 *
+		 * NOTE: setting the session flag is _supposed_ to trigger
+		 * SRP but clearly it doesn't.
+		 */
+		devctl = musb_readb(mregs, MUSB_DEVCTL);
+		if (devctl & MUSB_DEVCTL_BDEVICE)
+			mod_timer(&otg_workaround, jiffies + POLL_SECONDS * HZ);
+		else
+			musb->xceiv->state = OTG_STATE_A_IDLE;
+		break;
+	default:
+		break;
+	}
+	spin_unlock_irqrestore(&musb->lock, flags);
+}
+
+void musb_platform_try_idle_x(struct musb *musb, unsigned long timeout)
+{
+	static unsigned long last_timer;
+
+	if (!is_otg_enabled(musb))
+		return;
+
+	if (timeout == 0)
+		timeout = jiffies + msecs_to_jiffies(3);
+
+	/* Never idle if active, or when VBUS timeout is not set as host */
+	if (musb->is_active || (musb->a_wait_bcon == 0 &&
+				musb->xceiv->state == OTG_STATE_A_WAIT_BCON)) {
+		DBG(4, "%s active, deleting timer\n", otg_state_string(musb));
+		del_timer(&otg_workaround);
+		last_timer = jiffies;
+		return;
+	}
+
+	if (time_after(last_timer, timeout) && timer_pending(&otg_workaround)) {
+		DBG(4, "Longer idle timer already pending, ignoring...\n");
+		return;
+	}
+	last_timer = timeout;
+
+	DBG(4, "%s inactive, starting idle timer for %u ms\n",
+	    otg_state_string(musb), jiffies_to_msecs(timeout - jiffies));
+	mod_timer(&otg_workaround, timeout);
+}
+
+#ifdef CONFIG_USB_TI_CPPI41_DMA
+static irqreturn_t cppi41dma_Interrupt(int irq, void *hci)
+{
+	struct musb  *musb = hci;
+	u32 intr_status;
+	irqreturn_t ret = IRQ_NONE;
+	u32 q_cmpl_status_0, q_cmpl_status_1, q_cmpl_status_2;
+	u32 usb0_tx_intr, usb0_rx_intr;
+	u32 usb1_tx_intr, usb1_rx_intr;
+	void *q_mgr_base = cppi41_queue_mgr[0].q_mgr_rgn_base;
+	unsigned long flags;
+
+	musb = hci;
+	spin_lock_irqsave(&musb->lock, flags);
+	/*
+	 * CPPI 4.1 interrupts share the same IRQ and the EOI register but
+	 * don't get reflected in the interrupt source/mask registers.
+	 */
+	if (is_cppi41_enabled()) {
+		/*
+		 * Check for the interrupts from Tx/Rx completion queues; they
+		 * are level-triggered and will stay asserted until the queues
+		 * are emptied.  We're using the queue pending register 0 as a
+		 * substitute for the interrupt status register and reading it
+		 * directly for speed.
+		 */
+		intr_status = usbss_read(USBSS_IRQ_STATUS);
+
+		if (intr_status)
+			usbss_write(USBSS_IRQ_STATUS, intr_status);
+		else
+			printk(KERN_DEBUG "spurious usbss intr\n");
+
+		q_cmpl_status_0 = musb_readl(q_mgr_base, 0x98);
+		q_cmpl_status_1 = musb_readl(q_mgr_base, 0x9c);
+		q_cmpl_status_2 = musb_readl(q_mgr_base, 0xa0);
+
+		/* USB0 tx/rx completion */
+		/* usb0 tx completion interrupt for ep1..15 */
+		usb0_tx_intr = (q_cmpl_status_0 >> 29) |
+				((q_cmpl_status_1 & 0xFFF) << 3);
+		usb0_rx_intr = ((q_cmpl_status_1 & 0x07FFe000) >> 13);
+
+		usb1_tx_intr = (q_cmpl_status_1 >> 29) |
+				((q_cmpl_status_2 & 0xFFF) << 3);
+		usb1_rx_intr = ((q_cmpl_status_2 & 0x0fffe000) >> 13);
+
+		/* get proper musb handle based usb0/usb1 ctrl-id */
+
+		DBG(4, "CPPI 4.1 IRQ: Tx %x, Rx %x\n", usb0_tx_intr,
+					usb0_rx_intr);
+		if (usb0_tx_intr || usb0_rx_intr) {
+			cppi41_completion(gmusb[0], usb0_rx_intr,
+						usb0_tx_intr);
+			ret = IRQ_HANDLED;
+		}
+
+		DBG(4, "CPPI 4.1 IRQ: Tx %x, Rx %x\n", usb1_tx_intr,
+			usb1_rx_intr);
+		if (usb1_rx_intr || usb1_tx_intr) {
+			cppi41_completion(gmusb[1], usb1_rx_intr,
+				usb1_tx_intr);
+			ret = IRQ_HANDLED;
+		}
+		usbss_write(USBSS_IRQ_EOI, 0);
+	}
+	spin_unlock_irqrestore(&musb->lock, flags);
+	return ret;
+}
+#endif
+static irqreturn_t ti816x_interrupt(int irq, void *hci)
+{
+	struct musb  *musb = hci;
+	void __iomem *reg_base = musb->ctrl_base;
+	unsigned long flags;
+	irqreturn_t ret = IRQ_NONE;
+	u32 pend1 = 0, pend2 = 0;
+	u32 epintr, usbintr;
+
+	spin_lock_irqsave(&musb->lock, flags);
+
+	/*
+	 * NOTE: AM3517/ti816x shadows the Mentor IRQs.  Don't manage them
+	 * through the Mentor registers (except for setup), use the TI ones
+	 *and EOI.
+	 */
+	/* Acknowledge and handle non-CPPI interrupts */
+	/* Get endpoint interrupts */
+	musb->int_rx = musb->int_tx = musb->int_usb = 0;
+	epintr = musb_readl(reg_base, USB_EP_INTR_STATUS_REG);
+	if (epintr) {
+		musb_writel(reg_base, USB_EP_INTR_STATUS_REG, epintr);
+
+		musb->int_rx =
+			(epintr & USB_RX_INTR_MASK) >> USB_INTR_RX_SHIFT;
+		musb->int_tx =
+			(epintr & USB_TX_INTR_MASK) >> USB_INTR_TX_SHIFT;
+	}
+
+	/* Get usb core interrupts */
+	usbintr = musb_readl(reg_base, USB_CORE_INTR_STATUS_REG);
+	if (!usbintr && !epintr) {
+		DBG(4, "sprious interrupt\n");
+		goto eoi;
+	}
+
+	if (usbintr) {
+		musb_writel(reg_base, USB_CORE_INTR_STATUS_REG, usbintr);
+		musb->int_usb =
+			(usbintr & USB_INTR_USB_MASK) >> USB_INTR_USB_SHIFT;
+	}
+	/*
+	 * DRVVBUS IRQs are the only proxy we have (a very poor one!) for
+	 * AM3517's missing ID change IRQ.  We need an ID change IRQ to
+	 * switch appropriately between halves of the OTG state machine.
+	 * Managing DEVCTL.SESSION per Mentor docs requires that we know its
+	 * value but DEVCTL.BDEVICE is invalid without DEVCTL.SESSION set.
+	 * Also, DRVVBUS pulses for SRP (but not at 5V) ...
+	 */
+	if (usbintr & (USB_INTR_DRVVBUS << USB_INTR_USB_SHIFT)) {
+		int drvvbus = musb_readl(reg_base, USB_STAT_REG);
+		void __iomem *mregs = musb->mregs;
+		u8 devctl = musb_readb(mregs, MUSB_DEVCTL);
+		int err;
+
+		err = is_host_enabled(musb) && (musb->int_usb &
+						MUSB_INTR_VBUSERROR);
+		if (err) {
+			/*
+			 * The Mentor core doesn't debounce VBUS as needed
+			 * to cope with device connect current spikes. This
+			 * means it's not uncommon for bus-powered devices
+			 * to get VBUS errors during enumeration.
+			 *
+			 * This is a workaround, but newer RTL from Mentor
+			 * seems to allow a better one: "re"-starting sessions
+			 * without waiting for VBUS to stop registering in
+			 * devctl.
+			 */
+			musb->int_usb &= ~MUSB_INTR_VBUSERROR;
+			musb->xceiv->state = OTG_STATE_A_WAIT_VFALL;
+			mod_timer(&otg_workaround, jiffies + POLL_SECONDS * HZ);
+			WARNING("VBUS error workaround (delay coming)\n");
+		} else if (is_host_enabled(musb) && drvvbus) {
+			musb->is_active = 1;
+			MUSB_HST_MODE(musb);
+			musb->xceiv->default_a = 1;
+			musb->xceiv->state = OTG_STATE_A_WAIT_VRISE;
+			portstate(musb->port1_status |= USB_PORT_STAT_POWER);
+			del_timer(&otg_workaround);
+		} else {
+			musb->is_active = 0;
+			MUSB_DEV_MODE(musb);
+			musb->xceiv->default_a = 0;
+			musb->xceiv->state = OTG_STATE_B_IDLE;
+			portstate(musb->port1_status &= ~USB_PORT_STAT_POWER);
+		}
+
+		/* NOTE: this must complete power-on within 100 ms. */
+		ti816x_source_power(musb, drvvbus, 0);
+		DBG(2, "VBUS %s (%s)%s, devctl %02x\n",
+				drvvbus ? "on" : "off",
+				otg_state_string(musb),
+				err ? " ERROR" : "",
+				devctl);
+		ret = IRQ_HANDLED;
+	}
+
+	if (musb->int_tx || musb->int_rx || musb->int_usb) {
+		irqreturn_t mret;
+		mret = musb_interrupt(musb);
+		if (mret == IRQ_HANDLED)
+			ret = IRQ_HANDLED;
+	}
+
+ eoi:
+	/* EOI needs to be written for the IRQ to be re-asserted. */
+	if (ret == IRQ_HANDLED || epintr || usbintr) {
+		/* write EOI */
+		musb_writel(reg_base, USB_IRQ_EOI, 0);
+	}
+
+	ret = IRQ_HANDLED;
+
+	/* Poll for ID change */
+	if (is_otg_enabled(musb) && musb->xceiv->state == OTG_STATE_B_IDLE)
+		mod_timer(&otg_workaround, jiffies + POLL_SECONDS * HZ);
+
+	spin_unlock_irqrestore(&musb->lock, flags);
+
+	if (ret != IRQ_HANDLED) {
+		if (epintr || usbintr)
+			/*
+			 * We sometimes get unhandled IRQs in the peripheral
+			 * mode from EP0 and SOF...
+			 */
+			DBG(2, "Unhandled USB IRQ %08x-%08x\n",
+					 epintr, usbintr);
+		else if (printk_ratelimit())
+			/*
+			 * We've seen series of spurious interrupts in the
+			 * peripheral mode after USB reset and then after some
+			 * time a real interrupt storm starting...
+			 */
+			DBG(2, "Spurious IRQ, CPPI 4.1 status %08x, %08x\n",
+					 pend1, pend2);
+	}
+	return ret;
+}
+int musb_platform_set_mode(struct musb *musb, u8 musb_mode)
+{
+	void __iomem *reg_base = musb->ctrl_base;
+
+	/* TODO: implement this using CONF0 */
+	if (musb_mode == MUSB_HOST) {
+		musb_writel(reg_base, USB_MODE_REG, 0);
+		musb_writel(musb->ctrl_base, USB_PHY_UTMI_REG, 0x02);
+		DBG(4, "host: %s: value of mode reg=%x\n\n", __func__,
+					musb_readl(reg_base, USB_MODE_REG));
+	} else
+	if (musb_mode == MUSB_PERIPHERAL) {
+		/* TODO commmented writing 8 to USB_MODE_REG device
+			mode is not working */
+		musb_writel(reg_base, USB_MODE_REG, 0x100);
+		DBG(4, "device: %s: value of mode reg=%x\n\n", __func__,
+					musb_readl(reg_base, USB_MODE_REG));
+	}
+	return -EIO;
+}
+
+int musb_platform_init(struct musb *musb)
+{
+	void __iomem *reg_base = musb->ctrl_base;
+	struct clk              *otg_fck;
+	u32 rev;
+	u8 mode;
+
+	/* usb subsystem init */
+	usbotg_ss_init(musb);
+
+	if (musb->id < 2)
+		gmusb[musb->id] = musb;
+
+	usb_nop_xceiv_register();
+
+	musb->xceiv = otg_get_transceiver();
+	if (!musb->xceiv)
+		return -ENODEV;
+
+	/* mentor is at offset of 0x400 in am3517/ti816x */
+	musb->mregs += USB_MENTOR_CORE_OFFSET;
+
+	/* not required as clock is set in usb-musb.c file in arch */
+	/* musb->clock = clk_get(NULL, "usbotg_ck"); */
+	if (IS_ERR(musb->clock))
+		return PTR_ERR(musb->clock);
+
+	if (musb->set_clock)
+		musb->set_clock(musb->clock, 1);
+	else
+		clk_enable(musb->clock);
+
+	DBG(2, "usbotg_ick=%lud\n", clk_get_rate(musb->clock));
+	otg_fck = clk_get(NULL, "usbotg_ick");
+	clk_enable(otg_fck);
+
+	DBG(2, "usbotg_ick=%lud\n", clk_get_rate(otg_fck));
+	/* Returns zero if e.g. not clocked */
+	rev = musb_readl(reg_base, USB_REVISION_REG);
+	if (!rev)
+		return -ENODEV;
+
+	if (is_host_enabled(musb))
+		setup_timer(&otg_workaround, otg_timer, (unsigned long) musb);
+
+	musb->board_set_vbus = ti816x_set_vbus;
+	ti816x_source_power(musb, 0, 1);
+
+	/* set musb controller to host mode */
+	if (is_host_enabled(musb))
+		musb_platform_set_mode(musb, MUSB_HOST);
+	else
+		musb_platform_set_mode(musb, MUSB_PERIPHERAL);
+
+	/* follow recommended reset procedure */
+	/* Reset the controller */
+	musb_writel(reg_base, USB_CTRL_REG, USB_SOFT_RESET_MASK);
+
+	/* wait till reset bit clears */
+	while ((musb_readl(reg_base, USB_CTRL_REG) & 0x1))
+		cpu_relax();
+
+	/* clock disable */
+	clk_disable(musb->clock);
+
+	/* Start the on-chip PHY and its PLL. */
+	phy_on();
+
+	msleep(5);
+
+	/* clock enable */
+	clk_enable(musb->clock);
+
+#ifdef CONFIG_USB_TI_CPPI41_DMA
+	cppi41_init(musb);
+#endif
+
+	musb->a_wait_bcon = A_WAIT_BCON_TIMEOUT;
+	musb->isr = ti816x_interrupt;
+
+#ifdef CONFIG_USB_MUSB_OTG
+	if (musb->id == 1)
+		mode = MUSB_HOST;
+	else
+		mode = MUSB_PERIPHERAL;
+#else
+	/* set musb controller to host mode */
+	if (is_host_enabled(musb))
+		mode = MUSB_HOST;
+	else
+		mode = MUSB_PERIPHERAL;
+#endif
+
+	musb_platform_set_mode(musb, mode);
+
+	musb_writel(reg_base, USB_IRQ_EOI, 0);
+	usbss_write(USBSS_IRQ_EOI, 0);
+
+	return 0;
+}
+
+int musb_platform_exit(struct musb *musb)
+{
+	if (is_host_enabled(musb))
+		del_timer_sync(&otg_workaround);
+
+	ti816x_source_power(musb, 0, 1);
+
+	/* Delay to avoid problems with module reload... */
+	if (is_host_enabled(musb) && musb->xceiv->default_a) {
+		int maxdelay = 30;
+		u8 devctl, warn = 0;
+
+		/*
+		 * If there's no peripheral connected, this can take a
+		 * long time to fall...
+		 */
+		do {
+			devctl = musb_readb(musb->mregs, MUSB_DEVCTL);
+			if (!(devctl & MUSB_DEVCTL_VBUS))
+				break;
+			if ((devctl & MUSB_DEVCTL_VBUS) != warn) {
+				warn = devctl & MUSB_DEVCTL_VBUS;
+				DBG(1, "VBUS %d\n",
+					warn >> MUSB_DEVCTL_VBUS_SHIFT);
+			}
+			msleep(1000);
+			maxdelay--;
+		} while (maxdelay > 0);
+
+		/* In OTG mode, another host might be connected... */
+		if (devctl & MUSB_DEVCTL_VBUS)
+			DBG(1, "VBUS off timeout (devctl %02x)\n", devctl);
+	}
+
+	phy_off();
+
+#ifdef CONFIG_USB_TI_CPPI41_DMA
+	cppi41_exit();
+	cppi41_free(musb);
+#endif
+	return 0;
+}
+
+#ifdef CONFIG_PM
+void musb_platform_save_context(struct musb *musb,
+		 struct musb_context_registers *musb_context)
+{
+	/* Save CPPI41 DMA related registers */
+}
+
+void musb_platform_restore_context(struct musb *musb,
+		 struct musb_context_registers *musb_context)
+{
+	/* Restore CPPI41 DMA related registers */
+}
+#endif
diff --git a/drivers/usb/musb/ti816x.h b/drivers/usb/musb/ti816x.h
new file mode 100644
index 0000000..a19ab9d
--- /dev/null
+++ b/drivers/usb/musb/ti816x.h
@@ -0,0 +1,143 @@
+/*
+ * Copyright (C) 2005-2006 by Texas Instruments
+ *
+ * The Inventra Controller Driver for Linux is free software; you
+ * can redistribute it and/or modify it under the terms of the GNU
+ * General Public License version 2 as published by the Free Software
+ * Foundation.
+ */
+
+#ifndef __MUSB_HDRDF_H__
+#define __MUSB_HDRDF_H__
+
+/* Netra USB susbsystem register offsets */
+#define USBSS_REVISION			0x0000
+#define USBSS_SYSCONFIG			0x0010
+/* USBSS EOI interrupt register */
+#define USBSS_IRQ_EOI			0x0020
+/* USBSS interrupt generation/status register */
+#define USBSS_IRQ_STATUS_RAW		0x0024
+/* USBSS interrupt status register */
+#define USBSS_IRQ_STATUS		0x0028
+/* USBSS interrupt enable register */
+#define USBSS_IRQ_ENABLE_SET		0x002c
+/* USBSS interrupt clear register */
+#define USBSS_IRQ_ENABLE_CLEAR		0x0030
+/* USB0: TxDMA 8bit tx completion interrupt pacing
+	threshold value for ep1..15 */
+#define USBSS_IRQ_DMA_THRESHOLD_TX0	0x0100
+/* USB0: RxDMA 8bit rx completion interrupt pacing
+	threshold value for ep1..15 */
+#define USBSS_IRQ_DMA_THRESHOLD_RX0	0x0110
+/* USB1: TxDMA 8bit tx completion interrupt pacing
+	threshold value for ep1..15 */
+#define USBSS_IRQ_DMA_THRESHOLD_TX1	0x0120
+/* USB1: RxDMA 8bit rx completion interrupt pacing
+	threshold value for ep1..15 */
+#define USBSS_IRQ_DMA_THRESHOLD_RX1	0x0130
+/* USB0: TxDMA threshold enable tx completion for ep1..ep15
+	RxDMA threshold enable rx completion for ep1..ep15 */
+#define USBSS_IRQ_DMA_ENABLE_0		0x0140
+/* USB1: TxDMA threshold enable for ep1..ep15
+	RxDMA threshold enable for ep1..ep15 */
+#define USBSS_IRQ_DMA_ENABLE_1		0x0144
+/* USB0: TxDMA Frame threshold for tx completion for ep1..ep15
+	RxDMA Frame threshold for rx completion for ep1..ep15 */
+#define USBSS_IRQ_FRAME_THRESHOLD_TX0	0x0200
+#define USBSS_IRQ_FRAME_THRESHOLD_RX0	0x0210
+/* USB1: TxDMA Frame threshold for tx completion for ep1..ep15
+	RxDMA Frame threshold for rx completion for ep1..ep15 */
+#define USBSS_IRQ_FRAME_THRESHOLD_TX1	0x0220
+#define USBSS_IRQ_FRAME_THRESHOLD_RX1	0x0230
+/* USB0: Frame threshold enable tx completion for ep1..ep15
+	Frame threshold enable rx completion for ep1..ep15 */
+#define USBSS_IRQ_FRAME_ENABLE_0	0x0240
+#define USBSS_IRQ_FRAME_ENABLE_1	0x0244
+
+
+/* USB 2.0 OTG module registers */
+#define USB_REVISION_REG        0x0000
+#define USB_CTRL_REG            0x0014
+#define USB_STAT_REG            0x0018
+#define	USB_IRQ_MERGED_STATUS	0x0020
+#define USB_IRQ_EOI		0x0024
+#define	USB_IRQ_STATUS_RAW_0	0x0028
+#define	USB_IRQ_STATUS_RAW_1	0x002c
+#define	USB_IRQ_STATUS_0	0x0030
+#define	USB_IRQ_STATUS_1	0x0034
+#define	USB_IRQ_ENABLE_SET_0	0x0038
+#define	USB_IRQ_ENABLE_SET_1	0x003c
+#define	USB_IRQ_ENABLE_CLR_0	0x0040
+#define	USB_IRQ_ENABLE_CLR_1	0x0044
+
+#define USB_EP_INTR_SET_REG		(USB_IRQ_ENABLE_SET_0)
+#define USB_CORE_INTR_SET_REG		(USB_IRQ_ENABLE_SET_1)
+#define USB_EP_INTR_CLEAR_REG		(USB_IRQ_ENABLE_CLR_0)
+#define USB_CORE_INTR_CLEAR_REG		(USB_IRQ_ENABLE_CLR_1)
+#define USB_EP_INTR_STATUS_REG		(USB_IRQ_STATUS_0)
+#define USB_CORE_INTR_STATUS_REG	(USB_IRQ_STATUS_1)
+
+#define USB_TX_MODE_REG		0x0070
+#define USB_RX_MODE_REG		0x0074
+
+#define USB_GRNDIS_EPSIZE_OFFS	0X0080
+#define USB_GENERIC_RNDIS_EP_SIZE_REG(n) (0x0080 + (((n) - 1) << 2))
+#define USB_AUTOREQ_REG         0x00d0
+#define USB_SRP_FIX_TIME_REG    0x00d4
+#define USB_TEARDOWN_REG        0x00d8
+#define USB_PHY_UTMI_REG	0x00e0
+#define USB_PHY_UTMI_LB_REG	0x00e4
+#define USB_MODE_REG		0x00e8
+
+#define QUEUE_THRESHOLD_INTR_ENABLE_REG 0xc0
+#define QUEUE_63_THRESHOLD_REG  0xc4
+#define QUEUE_63_THRESHOLD_INTR_CLEAR_REG 0xc8
+#define QUEUE_65_THRESHOLD_REG  0xd4
+#define QUEUE_65_THRESHOLD_INTR_CLEAR_REG 0xd8
+
+/* Control register bits */
+#define USB_SOFT_RESET_MASK     1
+
+/* Mode register bits */
+#define USB_MODE_SHIFT(n)       ((((n) - 1) << 1))
+#define USB_MODE_MASK(n)        (3 << USB_MODE_SHIFT(n))
+#define USB_RX_MODE_SHIFT(n)    USB_MODE_SHIFT(n)
+#define USB_TX_MODE_SHIFT(n)    USB_MODE_SHIFT(n)
+#define USB_RX_MODE_MASK(n)     USB_MODE_MASK(n)
+#define USB_TX_MODE_MASK(n)     USB_MODE_MASK(n)
+#define USB_TRANSPARENT_MODE    0
+#define USB_RNDIS_MODE          1
+#define USB_CDC_MODE            2
+#define USB_GENERIC_RNDIS_MODE  3
+
+/* AutoReq register bits */
+#define USB_RX_AUTOREQ_SHIFT(n) (((n) - 1) << 1)
+#define USB_RX_AUTOREQ_MASK(n)  (3 << USB_RX_AUTOREQ_SHIFT(n))
+#define USB_NO_AUTOREQ          0
+#define USB_AUTOREQ_ALL_BUT_EOP 1
+#define USB_AUTOREQ_ALWAYS      3
+
+/* Teardown register bits */
+#define USB_TX_TDOWN_SHIFT(n)   (16 + (n))
+#define USB_TX_TDOWN_MASK(n)    (1 << USB_TX_TDOWN_SHIFT(n))
+#define USB_RX_TDOWN_SHIFT(n)   (n)
+#define USB_RX_TDOWN_MASK(n)    (1 << USB_RX_TDOWN_SHIFT(n))
+
+/* USB interrupt register bits */
+#define USB_INTR_USB_SHIFT      0
+#define USB_INTR_USB_MASK       (0x1ff << USB_INTR_USB_SHIFT) /* 8 Mentor */
+				/* interrupts and DRVVBUS interrupt */
+#define USB_INTR_DRVVBUS        0x100
+#define USB_INTR_RX_SHIFT       16
+#define USB_INTR_TX_SHIFT       0
+
+#define USB_MENTOR_CORE_OFFSET  0x400
+
+#define USB_CPPI41_NUM_CH       15
+
+void set_frame_threshold(u8 musb_id, u8 is_tx, u8 epnum, u8 value, u8 en_intr);
+void set_dma_threshold(u8 musb_id, u8 is_tx, u8 epnum, u8 value);
+
+extern void set_threshold(u8 ctrl_id, u8 epn, u8 count, u8 is_tx);
+extern void usb_nop_xceiv_register(void);
+#endif
-- 
1.7.0.4

