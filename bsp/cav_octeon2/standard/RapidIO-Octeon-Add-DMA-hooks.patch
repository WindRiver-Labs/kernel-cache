From add3f65ff6fbe1c6ce4e2c0ac84be971dceeee75 Mon Sep 17 00:00:00 2001
From: Phil Staub <Phil.Staub@windriver.com>
Date: Thu, 16 Jun 2011 16:15:14 -0700
Subject: [PATCH 040/236] RapidIO: Octeon: Add DMA hooks

Source: Cavium SDK 2.1.0-407

Enable Octeon to use DMA for large memory transfers.

Signed-off-by: Phil Staub <Phil.Staub@windriver.com>
---
 arch/mips/cavium-octeon/octeon-rapidio.c |   77 ++++++++++++++++++++++++++++++
 drivers/rapidio/rio-sysfs.c              |   20 ++++++++
 2 files changed, 97 insertions(+), 0 deletions(-)

diff --git a/arch/mips/cavium-octeon/octeon-rapidio.c b/arch/mips/cavium-octeon/octeon-rapidio.c
index f76e3f0..a799c5b 100644
--- a/arch/mips/cavium-octeon/octeon-rapidio.c
+++ b/arch/mips/cavium-octeon/octeon-rapidio.c
@@ -13,11 +13,17 @@
 #include <linux/rio_drv.h>
 #include <linux/interrupt.h>
 #include <linux/netdevice.h>
+#include <linux/sched.h>
 
 #include <asm/octeon/cvmx.h>
 #include <asm/octeon/cvmx-srio.h>
 #include <asm/octeon/cvmx-sriox-defs.h>
+#include <asm/octeon/cvmx-sli-defs.h>
+#include <asm/octeon/cvmx-pexp-defs.h>
 #include <asm/octeon/cvmx-sriomaintx-defs.h>
+#include <asm/octeon/cvmx-dma-engine.h>
+#include <asm/octeon/cvmx-fpa.h>
+#include <asm/octeon/cvmx-config.h>
 
 #define RIO_PRINTK(mport, fmt, ...) \
 	printk(KERN_INFO "SRIO%d: " fmt, (mport)->id, ##__VA_ARGS__)
@@ -297,6 +303,65 @@ static void octeon_rio_mem_unmap(struct rio_mport *mport, struct rio_dev *rdev,
 }
 
 /**
+ * DMA to/from a SRIO device using Octeon's internal DMA engines
+ *
+ * @param rdev       Device to DMA to/from
+ * @param local_addr Local memory physical address to DMA to
+ * @param remote_addr
+ *                   SRIO device memory address
+ * @param size       Size ofthe DMA in bytes
+ * @param is_outbound
+ *                   Non zero of the DMA is from Octoen to the device
+ *
+ * @return Zero on success, negative on failure
+ */
+int octeon_rio_dma_mem(struct rio_dev *rdev, uint64_t local_addr,
+	uint64_t remote_addr, int size, int is_outbound)
+{
+	int result;
+	volatile uint8_t dma_busy = 1;
+	cvmx_dma_engine_header_t header;
+	phys_t memmap;
+	int subdid;
+	cvmx_sli_mem_access_subidx_t sli_mem_access;
+	uint64_t sli_address;
+
+	/* Setup the SLI memmory mappings to access the SRIO device */
+	memmap = octeon_rio_mem_map(rdev->net->hport, rdev, remote_addr, size);
+	if (!memmap)
+		return -1;
+
+	/* Extract the SLI address from the core physical address */
+	subdid = (((memmap >> 40) & 7) << 2) | ((memmap >> 34) & 3);
+	sli_mem_access.u64 =
+		cvmx_read_csr(CVMX_PEXP_SLI_MEM_ACCESS_SUBIDX(subdid));
+	sli_address = (uint64_t)sli_mem_access.s.ba << 34;
+	sli_address += memmap & 0x3ffffffffull;
+
+	/* Create the DMA header */
+	header.u64 = 0;
+	header.s.fport = 0;
+	header.s.lport = rdev->net->hport->id;
+	header.s.type = (is_outbound) ? CVMX_DMA_ENGINE_TRANSFER_OUTBOUND :
+		CVMX_DMA_ENGINE_TRANSFER_INBOUND;
+	header.s.addr = virt_to_phys(&dma_busy);
+
+	/* Do the DMA */
+	result = cvmx_dma_engine_transfer(0, header, local_addr, sli_address,
+					  size);
+	if (result == 0) {
+		/* Wait for the DMA to complete */
+		while (dma_busy)
+			yield();
+	}
+
+	/* Unmap the SLI memory region */
+	octeon_rio_mem_unmap(rdev->net->hport, rdev, remote_addr, size, memmap);
+
+	return result;
+}
+
+/**
  * Add message to outbound mailbox
  *
  * @param mport  RapidIO Master port info
@@ -625,6 +690,8 @@ static irqreturn_t octeon_rio_irq(int irq, void *irq_arg)
 	return IRQ_HANDLED;
 }
 
+extern int cvm_oct_mem_fill_fpa(int pool, int size, int elements);
+
 /**
  * Initialize the RapidIO system
  *
@@ -634,10 +701,12 @@ static int __init octeon_rio_init(void)
 {
 	static struct octeon_rio_port srio_ports[2];
 	static struct rio_ops srio_ops;
+	int count = 0;
 
 	int srio_port;
 	if (!octeon_has_feature(OCTEON_FEATURE_SRIO))
 		return 0;
+
 	memset(&srio_ops, 0, sizeof(srio_ops));
 	srio_ops.lcread = octeon_rio_lcread;
 	srio_ops.lcwrite = octeon_rio_lcwrite;
@@ -683,6 +752,7 @@ static int __init octeon_rio_init(void)
 		sprintf(srio_ports[srio_port].mport.name, "SRIO%d", srio_port);
 		RIO_PRINTK(&srio_ports[srio_port].mport, "Registering port\n");
 		if (cvmx_srio_initialize(srio_port, 0) == 0) {
+			count++;
 			rio_register_mport(&srio_ports[srio_port].mport);
 			if (request_irq(OCTEON_IRQ_SRIO0 + srio_port,
 				octeon_rio_irq, IRQF_SHARED, "SRIO",
@@ -693,6 +763,13 @@ static int __init octeon_rio_init(void)
 				octeon_rio_irq_set_enable(&srio_ports[srio_port].mport, 1);
 		}
 	}
+	if (count) {
+		cvmx_fpa_enable();
+		cvm_oct_mem_fill_fpa(CVMX_FPA_OUTPUT_BUFFER_POOL,
+				     CVMX_FPA_OUTPUT_BUFFER_POOL_SIZE, 128);
+		cvmx_dma_engine_initialize();
+	}
+
 	return rio_init_mports();
 }
 
diff --git a/drivers/rapidio/rio-sysfs.c b/drivers/rapidio/rio-sysfs.c
index c53fffd..93babca 100644
--- a/drivers/rapidio/rio-sysfs.c
+++ b/drivers/rapidio/rio-sysfs.c
@@ -17,6 +17,10 @@
 #include <linux/capability.h>
 
 #include "rio.h"
+#ifdef CONFIG_CAVIUM_OCTEON_RAPIDIO
+extern int octeon_rio_dma_mem(struct rio_dev *rdev, uint64_t local_addr,
+			      uint64_t remote_addr, int size, int is_outbound);
+#endif
 
 /* Sysfs support */
 #define rio_config_attr(field, format_string)					\
@@ -217,6 +221,14 @@ rio_read_memory(struct kobject *kobj, struct bin_attribute *bin_attr,
 	if (off + count > bin_attr->size)
 		count = bin_attr->size - off;
 
+#ifdef CONFIG_CAVIUM_OCTEON_RAPIDIO
+	if (count > 8) {
+		if (octeon_rio_dma_mem(dev, virt_to_phys(buf), off, count, 0))
+			return 0;
+		else
+			return count;
+	}
+#endif
 	map = rio_map_memory(dev, off, count);
 	if (!map) {
 		dev_err(&dev->dev, "Unable to map RapidIO device resource\n");
@@ -240,6 +252,14 @@ rio_write_memory(struct kobject *kobj, struct bin_attribute *bin_attr,
 	if (off + count > bin_attr->size)
 		count = bin_attr->size - off;
 
+#ifdef CONFIG_CAVIUM_OCTEON_RAPIDIO
+	if (count > 8) {
+		if (octeon_rio_dma_mem(dev, virt_to_phys(buf), off, count, 1))
+			return 0;
+		else
+			return count;
+	}
+#endif
 	map = rio_map_memory(dev, off, count);
 	if (!map) {
 		dev_err(&dev->dev, "Unable to map RapidIO device resource\n");
-- 
1.7.0

