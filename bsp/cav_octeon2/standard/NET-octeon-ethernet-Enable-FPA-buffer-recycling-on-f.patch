From caac430a440bdee1354fe0793e13fd9292502819 Mon Sep 17 00:00:00 2001
From: David Daney <david.daney@cavium.com>
Date: Fri, 20 Jan 2012 12:02:29 -0800
Subject: [PATCH 211/236] NET: octeon-ethernet: Enable FPA buffer recycling on forwarded jumbo frames.

Source: Cavium SDK 2.3-427

This allows close to line rate forwarding once we start forwarding
fragmented SKBs.

Signed-off-by: David Daney <david.daney@cavium.com>
Integrated-by: Jiang Bin <bin.jiang@windriver.com>
---
 drivers/net/octeon/ethernet-mem.c  |    9 ++--
 drivers/net/octeon/ethernet-napi.c |    5 ++-
 drivers/net/octeon/ethernet-tx.c   |   37 ++++++++++++++++
 drivers/net/octeon/ethernet-util.h |    9 +++-
 drivers/net/octeon/ethernet-xmit.c |   84 ++++++++++++++++++++----------------
 5 files changed, 100 insertions(+), 44 deletions(-)

diff --git a/drivers/net/octeon/ethernet-mem.c b/drivers/net/octeon/ethernet-mem.c
index 66cdefc..804b5ec 100644
--- a/drivers/net/octeon/ethernet-mem.c
+++ b/drivers/net/octeon/ethernet-mem.c
@@ -65,21 +65,20 @@ static struct fpa_pool cvm_oct_pools[] = {
 static int cvm_oct_fill_hw_skbuff(struct fpa_pool *pool, int elements)
 {
 	int freed = elements;
-	int padding = 128 + sizeof(void *) + NET_SKB_PAD - 1;
 	int size = pool->size;
 	int pool_num = pool->pool;
 	while (freed) {
 		int extra_reserve;
-		unsigned char *desired_data;
-		struct sk_buff *skb = alloc_skb(size + padding, GFP_ATOMIC);
+		u8 *desired_data;
+		struct sk_buff *skb = alloc_skb(size + CVM_OCT_SKB_TO_FPA_PADDING, GFP_ATOMIC);
 		if (unlikely(skb == NULL)) {
 			pr_warning("Failed to allocate skb for hardware pool %d\n", pool_num);
 			break;
 		}
-		desired_data = (unsigned char *)((unsigned long)(skb->data + padding) & ~0x7fUl);
+		desired_data = cvm_oct_get_fpa_head(skb);
 		extra_reserve = desired_data - skb->data;
 		skb_reserve(skb, extra_reserve);
-		*(struct sk_buff **)(skb->data - NET_SKB_PAD - sizeof(void *)) = skb;
+		*(struct sk_buff **)(skb->data - sizeof(void *)) = skb;
 		cvmx_fpa_free(skb->data, pool_num, DONT_WRITEBACK(size / 128));
 		freed--;
 	}
diff --git a/drivers/net/octeon/ethernet-napi.c b/drivers/net/octeon/ethernet-napi.c
index 7309948..64946ef 100644
--- a/drivers/net/octeon/ethernet-napi.c
+++ b/drivers/net/octeon/ethernet-napi.c
@@ -234,8 +234,11 @@ static int CVM_OCT_NAPI_POLL(struct napi_struct *napi, int budget)
 						(packet_ptr.s.addr - (((packet_ptr.s.addr >> 7) - packet_ptr.s.back) << 7));
 					if (segment_size > packet_len)
 						segment_size = packet_len;
-					if (!first_frag)
+					if (!first_frag) {
 						current_skb->len = segment_size;
+						skb->data_len += segment_size;
+						skb->truesize += current_skb->truesize;
+					}
 					skb_set_tail_pointer(current_skb, segment_size);
 					packet_len -= segment_size;
 					segments--;
diff --git a/drivers/net/octeon/ethernet-tx.c b/drivers/net/octeon/ethernet-tx.c
index 65aab1c..117b263 100644
--- a/drivers/net/octeon/ethernet-tx.c
+++ b/drivers/net/octeon/ethernet-tx.c
@@ -68,6 +68,43 @@
 
 #endif
 
+#if REUSE_SKBUFFS_WITHOUT_FREE
+
+
+static bool cvm_oct_skb_ok_for_reuse(struct sk_buff *skb)
+{
+	unsigned char *fpa_head = cvm_oct_get_fpa_head(skb);
+
+	if (unlikely(skb->data < fpa_head))
+		return false;
+
+	if (unlikely((skb_end_pointer(skb) - fpa_head) < CVMX_FPA_PACKET_POOL_SIZE))
+		return false;
+	return true;
+}
+
+static void cvm_oct_skb_prepare_for_reuse(struct sk_buff *skb,
+					  union cvmx_buf_ptr *hw_buffer)
+{
+	unsigned char *fpa_head = cvm_oct_get_fpa_head(skb);
+
+	hw_buffer->s.back = ((unsigned long)skb->data >> 7) - ((unsigned long)fpa_head >> 7);
+	*(struct sk_buff **)(fpa_head - sizeof(void *)) = skb;
+	skb->truesize = sizeof(*skb) + skb_end_pointer(skb) - skb->head;
+}
+#else
+static bool cvm_oct_skb_ok_for_reuse(struct sk_buff *skb)
+{
+	return false;
+}
+static void cvm_oct_skb_prepare_for_reuse(struct sk_buff *skb,
+					  union cvmx_buf_ptr *hw_buffer)
+{
+	/* Do nothing */
+}
+#endif
+
+
 #ifdef CONFIG_OCTEON_ETHERNET_LOCKLESS_IF_SUPPORTED
 #define CVM_OCT_LOCKLESS 1
 #include "ethernet-xmit.c"
diff --git a/drivers/net/octeon/ethernet-util.h b/drivers/net/octeon/ethernet-util.h
index c275113..721eb72 100644
--- a/drivers/net/octeon/ethernet-util.h
+++ b/drivers/net/octeon/ethernet-util.h
@@ -46,5 +46,12 @@ static inline void *cvm_oct_get_buffer_ptr(union cvmx_buf_ptr pd)
 static inline struct sk_buff **cvm_oct_packet_to_skb(void *packet)
 {
 	char *p = packet;
-	return (struct sk_buff **)(p - NET_SKB_PAD - sizeof(void *));
+	return (struct sk_buff **)(p - sizeof(void *));
+}
+
+#define CVM_OCT_SKB_TO_FPA_PADDING (128 + sizeof(void *) - 1)
+
+static inline u8 *cvm_oct_get_fpa_head(struct sk_buff *skb)
+{
+	return (unsigned char *)((unsigned long)(skb->head + CVM_OCT_SKB_TO_FPA_PADDING) & ~0x7fUl);
 }
diff --git a/drivers/net/octeon/ethernet-xmit.c b/drivers/net/octeon/ethernet-xmit.c
index 09ffc2e..55c86e1 100644
--- a/drivers/net/octeon/ethernet-xmit.c
+++ b/drivers/net/octeon/ethernet-xmit.c
@@ -59,11 +59,9 @@ CVM_OCT_XMIT
 	struct octeon_ethernet *priv = netdev_priv(dev);
 	s32 queue_depth;
 	s32 buffers_to_free;
+	s32 buffers_being_recycled;
 	unsigned long flags;
 	cvmx_wqe_t *work = NULL;
-#if REUSE_SKBUFFS_WITHOUT_FREE
-	unsigned char *fpa_head;
-#endif
 
 	/*
 	 * Prefetch the private data structure.  It is larger than one
@@ -170,13 +168,15 @@ CVM_OCT_XMIT
 	pko_command.s.dontfree = 1;
 
 	/* Build the PKO buffer pointer */
-	hw_buffer.u64 = 0;
+	hw_buffer.u64 = 0; /* Implies pool == 0, i == 0 */
 	if (skb_shinfo(skb)->nr_frags == 0 && frag_count == 0) {
 		hw_buffer.s.addr = virt_to_phys(skb->data);
-		hw_buffer.s.pool = 0;
 		hw_buffer.s.size = skb->len;
+		buffers_being_recycled = 1;
 	} else {
 		u64 *hw_buffer_list;
+		bool can_do_reuse = true;
+
 		work = cvmx_fpa_alloc(cvm_oct_tx_wqe_pool);
 		if (unlikely(!work)) {
 			DEBUGPRINT("%s: Failed WQE allocate\n", dev->name);
@@ -185,26 +185,42 @@ CVM_OCT_XMIT
 		}
 		hw_buffer_list = (u64 *)work->packet_data;
 		hw_buffer.s.addr = virt_to_phys(skb->data);
-		hw_buffer.s.pool = 0;
 		hw_buffer.s.size = skb_headlen(skb);
+		if (skb_shinfo(skb)->nr_frags == 0 && cvm_oct_skb_ok_for_reuse(skb)) {
+			cvm_oct_skb_prepare_for_reuse(skb, &hw_buffer);
+		} else {
+			hw_buffer.s.back = 0;
+			can_do_reuse = false;
+		}
 		hw_buffer_list[0] = hw_buffer.u64;
 		for (i = 1; i <= skb_shinfo(skb)->nr_frags; i++) {
 			struct skb_frag_struct *fs = skb_shinfo(skb)->frags + i - 1;
 			hw_buffer.s.addr = virt_to_phys((u8 *)page_address(fs->page) + fs->page_offset);
 			hw_buffer.s.size = fs->size;
 			hw_buffer_list[i] = hw_buffer.u64;
+			can_do_reuse = false;
 		}
 		skb_walk_frags(skb, skb_tmp) {
 			hw_buffer.s.addr = virt_to_phys(skb_tmp->data);
 			hw_buffer.s.size = skb_tmp->len;
+			if (cvm_oct_skb_ok_for_reuse(skb_tmp)) {
+				cvm_oct_skb_prepare_for_reuse(skb_tmp, &hw_buffer);
+			} else {
+				hw_buffer.s.back = 0;
+				can_do_reuse = false;
+			}
 			hw_buffer_list[i] = hw_buffer.u64;
 			i++;
 		}
 		hw_buffer.s.addr = virt_to_phys(hw_buffer_list);
 		hw_buffer.s.size = i;
+		hw_buffer.s.back = 0;
+		hw_buffer.s.pool = cvm_oct_tx_wqe_pool;
+		buffers_being_recycled = i;
 		pko_command.s.segs = hw_buffer.s.size;
 		pko_command.s.gather = 1;
-		goto dont_put_skbuff_in_hw;
+		if (!can_do_reuse)
+			goto dont_put_skbuff_in_hw;
 	}
 
 	/*
@@ -222,20 +238,9 @@ CVM_OCT_XMIT
 	if (unlikely(priv->tx_timestamp_sw || priv->tx_timestamp_hw))
 		goto dont_put_skbuff_in_hw;
 
-	fpa_head = (unsigned char*)((unsigned long)(skb->head + 128 + sizeof(void *) + NET_SKB_PAD - 1) & ~0x7fUl);
-	if (unlikely(skb->data < fpa_head)) {
-		/*
-		 * printk("TX buffer beginning can't meet FPA
-		 * alignment constraints\n");
-		 */
+	if (!cvm_oct_skb_ok_for_reuse(skb))
 		goto dont_put_skbuff_in_hw;
-	}
-	if (unlikely((skb_end_pointer(skb) - fpa_head) < CVMX_FPA_PACKET_POOL_SIZE)) {
-		/*
-		   printk("TX buffer isn't large enough for the FPA\n");
-		 */
-		goto dont_put_skbuff_in_hw;
-	}
+
 	if (unlikely(skb_shared(skb))) {
 		/*
 		   printk("TX buffer sharing data with someone else\n");
@@ -260,18 +265,6 @@ CVM_OCT_XMIT
 		 */
 		goto dont_put_skbuff_in_hw;
 	}
-	if (unlikely(skb_shinfo(skb)->nr_frags)) {
-		/*
-		   printk("TX buffer has fragments\n");
-		 */
-		goto dont_put_skbuff_in_hw;
-	}
-	if (unlikely(skb->truesize != sizeof(*skb) + skb_end_pointer(skb) - skb->head)) {
-		/*
-		   printk("TX buffer truesize has been changed\n");
-		 */
-		goto dont_put_skbuff_in_hw;
-	}
 
 	/*
 	 * We can use this buffer in the FPA.  We don't need the FAU
@@ -279,8 +272,8 @@ CVM_OCT_XMIT
 	 */
 	pko_command.s.dontfree = 0;
 
-	hw_buffer.s.back = ((unsigned long)skb->data >> 7) - ((unsigned long)fpa_head >> 7);
-	*(struct sk_buff **)(fpa_head - NET_SKB_PAD - sizeof(void *)) = skb;
+	if (buffers_being_recycled == 1)
+		cvm_oct_skb_prepare_for_reuse(skb, &hw_buffer);
 
 	/*
 	 * The skbuff will be reused without ever being freed. We must
@@ -344,10 +337,27 @@ dont_put_skbuff_in_hw:
 		}
 	}
 
-	if (pko_command.s.dontfree)
+	if (pko_command.s.dontfree) {
 		queue_type = QUEUE_WQE;
-	else
+	} else {
 		queue_type = QUEUE_HW;
+		if (buffers_being_recycled > 1) {
+			struct sk_buff *tskb, *nskb;
+			/*
+			 * We are committed to use hardware free, restore the
+			 * frag list to empty on the first SKB
+			 */
+			tskb = skb_shinfo(skb)->frag_list;
+			while (tskb) {
+				nskb = tskb->next;
+				tskb->next = NULL;
+				tskb->prev = NULL;
+				tskb = nskb;
+			}
+			skb_frag_list_init(skb);
+			skb->data_len = 0;
+		}
+	}
 
 	if (queue_type == QUEUE_WQE) {
 		if (!work) {
@@ -422,7 +432,7 @@ skip_xmit:
 		dev->stats.tx_dropped++;
 		break;
 	case QUEUE_HW:
-		cvmx_fau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE, -1);
+		cvmx_fau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE, -buffers_being_recycled);
 		break;
 	case QUEUE_WQE:
 		/* Cleanup is done on the RX path when the WQE returns */
-- 
1.7.0

