From 5afb6f496480d5bf5a03077917653924b826179a Mon Sep 17 00:00:00 2001
From: Yang Shi <yang.shi@windriver.com>
Date: Tue, 8 Nov 2011 15:45:36 -0800
Subject: [PATCH 058/238] netdev: octeon-ethernet: Add handling for cn68XX hardware

Source: Cavium SDK 2.1.0-407

Signed-off-by: David Daney <ddaney@caviumnetworks.com>
Integrated-by: Phil Staub <Phil.Staub@windriver.com>
Integrated-by: Yang Shi <yang.shi@windriver.com>
---
 drivers/net/octeon/ethernet-mdio.c       |   46 ++--
 drivers/net/octeon/ethernet-napi.c       |  439 ++++++++++++++++++++++
 drivers/net/octeon/ethernet-proc.c       |    4 +-
 drivers/net/octeon/ethernet-rgmii.c      |   65 ++--
 drivers/net/octeon/ethernet-rx.c         |  586 ++++++++++--------------------
 drivers/net/octeon/ethernet-rx.h         |    4 +-
 drivers/net/octeon/ethernet-sgmii.c      |   18 +-
 drivers/net/octeon/ethernet-spi.c        |   10 +-
 drivers/net/octeon/ethernet-tx.c         |   27 +-
 drivers/net/octeon/ethernet-util.h       |    3 +-
 drivers/net/octeon/ethernet-xaui.c       |   18 +-
 drivers/net/octeon/ethernet-xmit.c       |   33 +-
 drivers/net/octeon/ethernet.c            |  276 +++++++++------
 drivers/net/octeon/octeon-ethernet.h     |   17 +-
 drivers/net/octeon/octeon-pow-ethernet.c |   39 ++-
 15 files changed, 948 insertions(+), 637 deletions(-)
 create mode 100644 drivers/net/octeon/ethernet-napi.c

diff --git a/drivers/net/octeon/ethernet-mdio.c b/drivers/net/octeon/ethernet-mdio.c
index 1229efd..c7c9b85 100644
--- a/drivers/net/octeon/ethernet-mdio.c
+++ b/drivers/net/octeon/ethernet-mdio.c
@@ -156,9 +156,14 @@ static int cvm_oct_ioctl_hwtstamp(struct net_device *dev,
 			cvmx_write_csr(CVMX_MIO_PTP_CLOCK_CFG, ptp.u64);
 		}
 		have_hw_timestamps = 1;
-		/* Only the first two interfaces support hardware timestamps */
-		if (priv->port >= 32)
+		switch(priv->imode) {
+		case CVMX_HELPER_INTERFACE_MODE_XAUI:
+		case CVMX_HELPER_INTERFACE_MODE_SGMII:
+			break;
+		default:
 			have_hw_timestamps = 0;
+			break;
+		}
 	}
 
 	/* Require hardware if ALLOW_TIMESTAMPS_WITHOUT_HARDWARE=0 */
@@ -184,18 +189,16 @@ static int cvm_oct_ioctl_hwtstamp(struct net_device *dev,
 		priv->flags &= ~(OCTEON_ETHERNET_FLAG_RX_TIMESTAMP_HW |
 				 OCTEON_ETHERNET_FLAG_RX_TIMESTAMP_SW);
 		if (have_hw_timestamps) {
-			int interface = INTERFACE(priv->port);
-			int index = INDEX(priv->port);
 			union cvmx_gmxx_rxx_frm_ctl gmxx_rxx_frm_ctl;
 			union cvmx_pip_prt_cfgx pip_prt_cfgx;
 
-			gmxx_rxx_frm_ctl.u64 = cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(index, interface));
+			gmxx_rxx_frm_ctl.u64 = cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface));
 			gmxx_rxx_frm_ctl.s.ptp_mode = 0;
-			cvmx_write_csr(CVMX_GMXX_RXX_FRM_CTL(index, interface), gmxx_rxx_frm_ctl.u64);
+			cvmx_write_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface), gmxx_rxx_frm_ctl.u64);
 
-			pip_prt_cfgx.u64 = cvmx_read_csr(CVMX_PIP_PRT_CFGX(priv->port));
+			pip_prt_cfgx.u64 = cvmx_read_csr(CVMX_PIP_PRT_CFGX(priv->ipd_pkind));
 			pip_prt_cfgx.s.skip = 0;
-			cvmx_write_csr(CVMX_PIP_PRT_CFGX(priv->port), pip_prt_cfgx.u64);
+			cvmx_write_csr(CVMX_PIP_PRT_CFGX(priv->ipd_pkind), pip_prt_cfgx.u64);
 		}
 		break;
 	case HWTSTAMP_FILTER_ALL:
@@ -217,18 +220,16 @@ static int cvm_oct_ioctl_hwtstamp(struct net_device *dev,
 			OCTEON_ETHERNET_FLAG_RX_TIMESTAMP_SW;
 		config.rx_filter = HWTSTAMP_FILTER_ALL;
 		if (have_hw_timestamps) {
-			int interface = INTERFACE(priv->port);
-			int index = INDEX(priv->port);
 			union cvmx_gmxx_rxx_frm_ctl gmxx_rxx_frm_ctl;
 			union cvmx_pip_prt_cfgx pip_prt_cfgx;
 
-			gmxx_rxx_frm_ctl.u64 = cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(index, interface));
+			gmxx_rxx_frm_ctl.u64 = cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface));
 			gmxx_rxx_frm_ctl.s.ptp_mode = 1;
-			cvmx_write_csr(CVMX_GMXX_RXX_FRM_CTL(index, interface), gmxx_rxx_frm_ctl.u64);
+			cvmx_write_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface), gmxx_rxx_frm_ctl.u64);
 
-			pip_prt_cfgx.u64 = cvmx_read_csr(CVMX_PIP_PRT_CFGX(priv->port));
+			pip_prt_cfgx.u64 = cvmx_read_csr(CVMX_PIP_PRT_CFGX(priv->ipd_pkind));
 			pip_prt_cfgx.s.skip = 8;
-			cvmx_write_csr(CVMX_PIP_PRT_CFGX(priv->port), pip_prt_cfgx.u64);
+			cvmx_write_csr(CVMX_PIP_PRT_CFGX(priv->ipd_pkind), pip_prt_cfgx.u64);
 		}
 		break;
 	default:
@@ -348,12 +349,12 @@ void cvm_oct_set_carrier(struct octeon_ethernet *priv, cvmx_helper_link_info_t l
 			DEBUGPRINT("%s: %u Mbps %s duplex, port %2d, queue %2d\n",
 				   priv->netdev->name, link_info.s.speed,
 				   (link_info.s.full_duplex) ? "Full" : "Half",
-				   priv->port, priv->tx_queue[0].queue);
+				   priv->ipd_port, priv->tx_queue[0].queue);
 		else
 			DEBUGPRINT("%s: %u Mbps %s duplex, port %2d, POW\n",
 				   priv->netdev->name, link_info.s.speed,
 				   (link_info.s.full_duplex) ? "Full" : "Half",
-				   priv->port);
+				   priv->ipd_port);
 	} else {
 		if (netif_carrier_ok(priv->netdev))
 			netif_carrier_off(priv->netdev);
@@ -372,7 +373,7 @@ static void cvm_oct_adjust_link(struct net_device *dev)
 		link_info.s.link_up = priv->last_link ? 1 : 0;
 		link_info.s.full_duplex = priv->phydev->duplex ? 1 : 0;
 		link_info.s.speed = priv->phydev->speed;
-		cvmx_helper_link_set(priv->port, link_info);
+		cvmx_helper_link_set(priv->ipd_port, link_info);
 		cvm_oct_set_carrier(priv, link_info);
 	}
 }
@@ -389,14 +390,15 @@ int cvm_oct_phy_setup_device(struct net_device *dev)
 {
 	struct octeon_ethernet *priv = netdev_priv(dev);
 
-	int phy_addr = cvmx_helper_board_get_mii_address(priv->port);
+	int phy_addr = cvmx_helper_board_get_mii_address(priv->ipd_port);
 	if (phy_addr != -1) {
 		char phy_id[20];
+		char bus_number[2];
 
-		if (phy_addr & 0x100)
-			snprintf(phy_id, sizeof(phy_id), PHY_ID_FMT, "1", phy_addr & 0xff);
-		else
-			snprintf(phy_id, sizeof(phy_id), PHY_ID_FMT, "0", phy_addr);
+		bus_number[0] = '0' + ((phy_addr >> 8) & 3);
+		bus_number[1] = 0;
+
+		snprintf(phy_id, sizeof(phy_id), PHY_ID_FMT, bus_number, phy_addr & 0xff);
 
 		priv->phydev = phy_connect(dev, phy_id, cvm_oct_adjust_link, 0,
 					   PHY_INTERFACE_MODE_GMII);
diff --git a/drivers/net/octeon/ethernet-napi.c b/drivers/net/octeon/ethernet-napi.c
new file mode 100644
index 0000000..a314c2c
--- /dev/null
+++ b/drivers/net/octeon/ethernet-napi.c
@@ -0,0 +1,439 @@
+/**********************************************************************
+ * Author: Cavium Networks
+ *
+ * Contact: support@caviumnetworks.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2010 Cavium Networks
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium Networks for more information
+**********************************************************************/
+
+#undef CVM_OCT_NAPI_POLL
+#undef CVM_OCT_NAPI_HAS_CN68XX_SSO
+
+#ifdef CVM_OCT_NAPI_68
+#define CVM_OCT_NAPI_POLL cvm_oct_napi_poll_68
+#define CVM_OCT_NAPI_HAS_CN68XX_SSO 1
+#else
+#define CVM_OCT_NAPI_POLL cvm_oct_napi_poll_38
+#define CVM_OCT_NAPI_HAS_CN68XX_SSO 0
+#endif
+
+/**
+ * cvm_oct_napi_poll - the NAPI poll function.
+ * @napi: The NAPI instance, or null if called from cvm_oct_poll_controller
+ * @budget: Maximum number of packets to receive.
+ *
+ * Returns the number of packets processed.
+ */
+static int CVM_OCT_NAPI_POLL(struct napi_struct *napi, int budget)
+{
+	const int	coreid = cvmx_get_core_num();
+	uint64_t	old_group_mask;
+	uint64_t	old_scratch;
+	int		rx_count = 0;
+	int		did_work_request = 0;
+	int		packet_not_copied;
+
+	char		*p = (char *)cvm_oct_by_pkind;
+	/* Prefetch cvm_oct_device since we know we need it soon */
+	prefetch(&p[0]);
+	prefetch(&p[SMP_CACHE_BYTES]);
+	prefetch(&p[2 * SMP_CACHE_BYTES]);
+
+	if (USE_ASYNC_IOBDMA) {
+		/* Save scratch in case userspace is using it */
+		CVMX_SYNCIOBDMA;
+		old_scratch = cvmx_scratch_read64(CVMX_SCR_SCRATCH);
+	}
+
+	/* Only allow work for our group (and preserve priorities) */
+	if (CVM_OCT_NAPI_HAS_CN68XX_SSO) {
+		old_group_mask = cvmx_read_csr(CVMX_SSO_PPX_GRP_MSK(coreid));
+		cvmx_write_csr(CVMX_SSO_PPX_GRP_MSK(coreid),
+			       1ull << pow_receive_group);
+		/* Read it back so it takes effect before we request work */
+		cvmx_read_csr(CVMX_SSO_PPX_GRP_MSK(coreid));
+	} else {
+		old_group_mask = cvmx_read_csr(CVMX_POW_PP_GRP_MSKX(coreid));
+		cvmx_write_csr(CVMX_POW_PP_GRP_MSKX(coreid),
+			       (old_group_mask & ~0xFFFFull) | 1 << pow_receive_group);
+	}
+
+	if (USE_ASYNC_IOBDMA) {
+		cvmx_pow_work_request_async(CVMX_SCR_SCRATCH, CVMX_POW_NO_WAIT);
+		did_work_request = 1;
+	}
+
+	while (rx_count < budget) {
+		struct sk_buff *skb = NULL;
+		struct sk_buff **pskb = NULL;
+		struct octeon_ethernet *priv;
+		enum cvm_oct_callback_result callback_result;
+		int skb_in_hw;
+		cvmx_wqe_t *work;
+		int port;
+
+		if (USE_ASYNC_IOBDMA && did_work_request)
+			work = cvmx_pow_work_response_async(CVMX_SCR_SCRATCH);
+		else
+			work = cvmx_pow_work_request_sync(CVMX_POW_NO_WAIT);
+
+		prefetch(work);
+		did_work_request = 0;
+		if (unlikely(work == NULL)) {
+			if (CVM_OCT_NAPI_HAS_CN68XX_SSO) {
+				cvmx_write_csr(CVMX_SSO_WQ_INT, 1ULL << pow_receive_group);
+				cvmx_write_csr(CVMX_SSO_WQ_IQ_DIS, 1ULL << pow_receive_group);
+			} else {
+				union cvmx_pow_wq_int wq_int;
+				wq_int.u64 = 0;
+				wq_int.s.iq_dis = 1 << pow_receive_group;
+				wq_int.s.wq_int = 1 << pow_receive_group;
+				cvmx_write_csr(CVMX_POW_WQ_INT, wq_int.u64);
+			}
+			break;
+		}
+		pskb = (struct sk_buff **)(cvm_oct_get_buffer_ptr(work->packet_ptr) - sizeof(void *));
+		prefetch(pskb);
+
+		if (USE_ASYNC_IOBDMA && rx_count < (budget - 1)) {
+			cvmx_pow_work_request_async_nocheck(CVMX_SCR_SCRATCH, CVMX_POW_NO_WAIT);
+			did_work_request = 1;
+		}
+
+		if (rx_count == 0) {
+			/*
+			 * First time through, see if there is enough
+			 * work waiting to merit waking another
+			 * CPU.
+			 */
+			int backlog;
+			int cores_in_use = core_state.active_cores;
+			if (CVM_OCT_NAPI_HAS_CN68XX_SSO) {
+				union cvmx_sso_wq_int_cntx counts;
+				counts.u64 = cvmx_read_csr(CVMX_SSO_WQ_INT_CNTX(pow_receive_group));
+				backlog = counts.s.iq_cnt + counts.s.ds_cnt;
+			} else {
+				union cvmx_pow_wq_int_cntx counts;
+				counts.u64 = cvmx_read_csr(CVMX_POW_WQ_INT_CNTX(pow_receive_group));
+				backlog = counts.s.iq_cnt + counts.s.ds_cnt;
+			}
+			if (backlog > budget * cores_in_use &&
+			    napi != NULL &&
+			    cores_in_use < core_state.baseline_cores)
+				cvm_oct_enable_one_cpu();
+		}
+
+		/*
+		 * If WORD2[SOFTWARE] then this WQE is a complete for
+		 * a TX packet.
+		 */
+		if (work->word2.s.software) {
+			struct octeon_ethernet *priv;
+			union skb_shared_tx *shtx;
+			int packet_qos = work->word0.raw.unused;
+			skb = (struct sk_buff *)work->packet_ptr.u64;
+			priv = netdev_priv(skb->dev);
+			if (!netif_running(skb->dev))
+				netif_wake_queue(skb->dev);
+			shtx = skb_tx(skb);
+			if (unlikely(shtx->hardware)) {
+				if (priv->flags & OCTEON_ETHERNET_FLAG_TX_TIMESTAMP_HW) {
+					uint64_t ns = *(uint64_t *)work->packet_data;
+					struct skb_shared_hwtstamps ts;
+					ts.syststamp = cvm_oct_ptp_to_ktime(ns);
+					ts.hwtstamp = ns_to_ktime(ns);
+					skb_tstamp_tx(skb, &ts);
+				}
+				if (priv->flags & OCTEON_ETHERNET_FLAG_TX_TIMESTAMP_SW) {
+					uint64_t ns = *(uint64_t *)work->packet_data;
+					struct skb_shared_hwtstamps ts;
+					ts.syststamp = ns_to_ktime(ns);
+					ts.hwtstamp = ns_to_ktime(0);
+					skb_tstamp_tx(skb, &ts);
+				}
+			}
+
+			dev_kfree_skb_any(skb);
+
+			cvmx_fpa_free(work, CVMX_FPA_TX_WQE_POOL, DONT_WRITEBACK(1));
+
+			/*
+			 * We are done with this one, adjust the queue
+			 * depth.
+			 */
+			cvmx_fau_atomic_add32(priv->tx_queue[packet_qos].fau, -1);
+			continue;
+		}
+
+		skb_in_hw = USE_SKBUFFS_IN_HW && work->word2.s.bufs == 1;
+		if (likely(skb_in_hw)) {
+			skb = *pskb;
+			prefetch(&skb->head);
+			prefetch(&skb->len);
+		}
+
+		if (CVM_OCT_NAPI_HAS_CN68XX_SSO)
+			port = work->word0.pip.cn68xx.pknd;
+		else
+			port = work->word1.cn38xx.ipprt;
+
+		prefetch(cvm_oct_by_pkind[port]);
+
+		/* Immediately throw away all packets with receive errors */
+		if (unlikely(work->word2.snoip.rcv_error)) {
+			if (cvm_oct_check_rcv_error(work))
+				continue;
+		}
+
+		/*
+		 * We can only use the zero copy path if skbuffs are
+		 * in the FPA pool and the packet fits in a single
+		 * buffer.
+		 */
+		if (likely(skb_in_hw)) {
+			skb->data = skb->head + work->packet_ptr.s.addr - cvmx_ptr_to_phys(skb->head);
+			prefetch(skb->data);
+			skb->len = work->word1.len;
+			skb_set_tail_pointer(skb, skb->len);
+			packet_not_copied = 1;
+		} else {
+			int len = work->word1.len;
+			/*
+			 * We have to copy the packet. First allocate
+			 * an skbuff for it.
+			 */
+			skb = dev_alloc_skb(len);
+			if (!skb) {
+				DEBUGPRINT("Port %d failed to allocate skbuff, packet dropped\n",
+					   port);
+				cvm_oct_free_work(work);
+				continue;
+			}
+
+			/*
+			 * Check if we've received a packet that was
+			 * entirely stored in the work entry.
+			 */
+			if (unlikely(work->word2.s.bufs == 0)) {
+				uint8_t *ptr = work->packet_data;
+
+				if (likely(!work->word2.s.not_IP)) {
+					/*
+					 * The beginning of the packet
+					 * moves for IP packets.
+					 */
+					if (work->word2.s.is_v6)
+						ptr += 2;
+					else
+						ptr += 6;
+				}
+				memcpy(skb_put(skb, len), ptr, len);
+				/* No packet buffers to free */
+			} else {
+				int segments = work->word2.s.bufs;
+				union cvmx_buf_ptr segment_ptr = work->packet_ptr;
+
+				while (segments--) {
+					union cvmx_buf_ptr  next_ptr;
+					int segment_size;
+
+					next_ptr = *(union cvmx_buf_ptr *)cvmx_phys_to_ptr(segment_ptr.s.addr - 8);
+
+			/*
+			 * Octeon Errata PKI-100: The segment size is
+			 * wrong. Until it is fixed, calculate the
+			 * segment size based on the packet pool
+			 * buffer size. When it is fixed, the
+			 * following line should be replaced with this
+			 * one: int segment_size =
+			 * segment_ptr.s.size;
+			 */
+					segment_size = CVMX_FPA_PACKET_POOL_SIZE -
+						(segment_ptr.s.addr - (((segment_ptr.s.addr >> 7) - segment_ptr.s.back) << 7));
+					/*
+					 * Don't copy more than what
+					 * is left in the packet.
+					 */
+					if (segment_size > len)
+						segment_size = len;
+					/* Copy the data into the packet */
+					memcpy(skb_put(skb, segment_size),
+					       cvmx_phys_to_ptr(segment_ptr.s.addr),
+					       segment_size);
+					len -= segment_size;
+					segment_ptr = next_ptr;
+				}
+			}
+			packet_not_copied = 0;
+		}
+
+		if (CVM_OCT_NAPI_HAS_CN68XX_SSO) {
+			if (unlikely(cvm_oct_by_pkind[port] == NULL))
+				priv = cvm_oct_dev_for_port(work->word2.s_cn68xx.port);
+			else
+				priv = cvm_oct_by_pkind[port];
+		} else {
+			priv = cvm_oct_by_pkind[port];
+		}
+
+		if (likely(priv)) {
+#ifdef CONFIG_RAPIDIO
+			if (unlikely(priv->imode == CVMX_HELPER_INTERFACE_MODE_SRIO)) {
+				const cvmx_srio_rx_message_header_t *rx_header = (const cvmx_srio_rx_message_header_t *)skb->data;
+                                __skb_pull(skb, sizeof(cvmx_srio_rx_message_header_t));
+				priv = cvm_oct_by_srio_mbox[(port - 40) >> 1][rx_header->word0.s.mbox];
+
+				atomic64_add(1, (atomic64_t *)&priv->netdev->stats.rx_packets);
+				atomic64_add(skb->len, (atomic64_t *)&priv->netdev->stats.rx_bytes);
+			}
+#endif
+			/*
+			 * Only accept packets for devices that are
+			 * currently up.
+			 */
+			if (likely(priv->netdev->flags & IFF_UP)) {
+				if (priv->flags & OCTEON_ETHERNET_FLAG_RX_TIMESTAMP_SW) {
+					struct skb_shared_hwtstamps *ts;
+					ts = skb_hwtstamps(skb);
+					ts->syststamp = ktime_get_real();
+					ts->hwtstamp = ns_to_ktime(0);
+				}
+				if (priv->flags & OCTEON_ETHERNET_FLAG_RX_TIMESTAMP_HW) {
+					/* The first 8 bytes are the timestamp */
+					uint64_t ns = *(uint64_t*)skb->data;
+					struct skb_shared_hwtstamps *ts;
+					ts = skb_hwtstamps(skb);
+					ts->hwtstamp = ns_to_ktime(ns);
+					ts->syststamp = cvm_oct_ptp_to_ktime(ns);
+					__skb_pull(skb, 8);
+				}
+				skb->protocol = eth_type_trans(skb, priv->netdev);
+				skb->dev = priv->netdev;
+
+				if (unlikely(work->word2.s.not_IP || work->word2.s.IP_exc || work->word2.s.L4_error))
+					skb->ip_summed = CHECKSUM_NONE;
+				else
+					skb->ip_summed = CHECKSUM_UNNECESSARY;
+
+				/* Increment RX stats for virtual ports */
+				if (port >= CVMX_PIP_NUM_INPUT_PORTS) {
+					atomic64_add(1, (atomic64_t *)&priv->netdev->stats.rx_packets);
+					atomic64_add(skb->len, (atomic64_t *)&priv->netdev->stats.rx_bytes);
+				}
+				if (priv->intercept_cb) {
+					callback_result = priv->intercept_cb(priv->netdev, work, skb);
+					switch (callback_result) {
+					case CVM_OCT_PASS:
+						netif_receive_skb(skb);
+						rx_count++;
+						break;
+					case CVM_OCT_DROP:
+						dev_kfree_skb_irq(skb);
+						atomic64_add(1, (atomic64_t *)&priv->netdev->stats.rx_dropped);
+						break;
+					case CVM_OCT_TAKE_OWNERSHIP_WORK:
+						/*
+						 * Interceptor took
+						 * our work, but we
+						 * need to free the
+						 * skbuff
+						 */
+						if (USE_SKBUFFS_IN_HW && likely(packet_not_copied)) {
+							/*
+							 * We can't free the skbuff since its data is
+							 * the same as the work. In this case we don't
+							 * do anything
+							 */
+						} else {
+							dev_kfree_skb_irq(skb);
+						}
+						break;
+					case CVM_OCT_TAKE_OWNERSHIP_SKB:
+						/* Interceptor took our packet */
+						break;
+					}
+				} else {
+					netif_receive_skb(skb);
+					callback_result = CVM_OCT_PASS;
+					rx_count++;
+				}
+			} else {
+				/* Drop any packet received for a device that isn't up */
+				/*
+				DEBUGPRINT("%s: Device not up, packet dropped\n",
+					   dev->name);
+				*/
+				atomic64_add(1, (atomic64_t *)&priv->netdev->stats.rx_dropped);
+				dev_kfree_skb_irq(skb);
+				callback_result = CVM_OCT_DROP;
+			}
+		} else {
+			/*
+			 * Drop any packet received for a device that
+			 * doesn't exist.
+			 */
+			DEBUGPRINT("Port %d not controlled by Linux, packet dropped\n",
+				   port);
+			dev_kfree_skb_irq(skb);
+			callback_result = CVM_OCT_DROP;
+		}
+		/* We only need to free the work if the interceptor didn't
+		   take over ownership of it */
+		if (callback_result != CVM_OCT_TAKE_OWNERSHIP_WORK) {
+			/*
+			 * Check to see if the skbuff and work share
+			 * the same packet buffer.
+			 */
+			if (USE_SKBUFFS_IN_HW && likely(packet_not_copied)) {
+				/*
+				 * This buffer needs to be replaced,
+				 * increment the number of buffers we
+				 * need to free by one.
+				 */
+				cvmx_fau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE, 1);
+				cvmx_fpa_free(work, CVMX_FPA_WQE_POOL, DONT_WRITEBACK(1));
+			} else {
+				cvm_oct_free_work(work);
+			}
+		}
+	}
+	/* Restore the original POW group mask */
+	if (CVM_OCT_NAPI_HAS_CN68XX_SSO) {
+		cvmx_write_csr(CVMX_SSO_PPX_GRP_MSK(coreid), old_group_mask);
+		/* Read it back so it takes effect before ?? */
+		cvmx_read_csr(CVMX_SSO_PPX_GRP_MSK(coreid));
+	} else {
+		cvmx_write_csr(CVMX_POW_PP_GRP_MSKX(coreid), old_group_mask);
+	}
+	if (USE_ASYNC_IOBDMA) {
+		/* Restore the scratch area */
+		cvmx_scratch_write64(CVMX_SCR_SCRATCH, old_scratch);
+	}
+	cvm_oct_rx_refill_pool(0);
+
+	if (rx_count < budget && napi != NULL) {
+		/* No more work */
+		napi_complete(napi);
+		cvm_oct_no_more_work(napi);
+	}
+	return rx_count;
+}
diff --git a/drivers/net/octeon/ethernet-proc.c b/drivers/net/octeon/ethernet-proc.c
index a39c3b8..e9da7cb 100644
--- a/drivers/net/octeon/ethernet-proc.c
+++ b/drivers/net/octeon/ethernet-proc.c
@@ -152,7 +152,7 @@ static int cvm_oct_stats_show(struct seq_file *m, void *v)
 {
 	struct octeon_ethernet *priv;
 	list_for_each_entry(priv, &cvm_oct_list, list) {
-		seq_printf(m, "\nOcteon Port %d (%s)\n", priv->port, priv->netdev->name);
+		seq_printf(m, "\nOcteon Port %d (%s)\n", priv->ipd_port, priv->netdev->name);
 		seq_printf(m,
 			   "rx_packets:             %12lu\t"
 			   "tx_packets:             %12lu\n",
@@ -206,7 +206,7 @@ static int cvm_oct_stats_show(struct seq_file *m, void *v)
 	}
 
 	priv = list_first_entry(&cvm_oct_list, struct octeon_ethernet, list);
-	if ((priv->port == 0) && (priv->imode == CVMX_HELPER_INTERFACE_MODE_GMII))
+	if ((priv->ipd_port == 0) && (priv->imode == CVMX_HELPER_INTERFACE_MODE_GMII))
 		cvm_oct_stats_switch_show(priv, m, v);
 	return 0;
 }
diff --git a/drivers/net/octeon/ethernet-rgmii.c b/drivers/net/octeon/ethernet-rgmii.c
index f604123..021edab 100644
--- a/drivers/net/octeon/ethernet-rgmii.c
+++ b/drivers/net/octeon/ethernet-rgmii.c
@@ -47,7 +47,7 @@ static void cvm_oct_rgmii_poll(struct net_device *dev)
 	struct octeon_ethernet *priv = netdev_priv(dev);
 	cvmx_helper_link_info_t link_info;
 
-	link_info = cvmx_helper_link_get(priv->port);
+	link_info = cvmx_helper_link_get(priv->ipd_port);
 	if (link_info.u64 == priv->link_info) {
 
 		/*
@@ -60,10 +60,8 @@ static void cvm_oct_rgmii_poll(struct net_device *dev)
 			 * Read the GMXX_RXX_INT_REG[PCTERR] bit and
 			 * see if we are getting preamble errors.
 			 */
-			int interface = INTERFACE(priv->port);
-			int index = INDEX(priv->port);
 			union cvmx_gmxx_rxx_int_reg gmxx_rxx_int_reg;
-			gmxx_rxx_int_reg.u64 = cvmx_read_csr(CVMX_GMXX_RXX_INT_REG(index, interface));
+			gmxx_rxx_int_reg.u64 = cvmx_read_csr(CVMX_GMXX_RXX_INT_REG(priv->interface_port, priv->interface));
 			if (gmxx_rxx_int_reg.s.pcterr) {
 				/*
 				 * We are getting preamble errors at
@@ -77,17 +75,17 @@ static void cvm_oct_rgmii_poll(struct net_device *dev)
 				union cvmx_ipd_sub_port_fcs ipd_sub_port_fcs;
 
 				/* Disable preamble checking */
-				gmxx_rxx_frm_ctl.u64 = cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(index, interface));
+				gmxx_rxx_frm_ctl.u64 = cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface));
 				gmxx_rxx_frm_ctl.s.pre_chk = 0;
-				cvmx_write_csr(CVMX_GMXX_RXX_FRM_CTL(index, interface), gmxx_rxx_frm_ctl.u64);
+				cvmx_write_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface), gmxx_rxx_frm_ctl.u64);
 
 				/* Disable FCS stripping */
 				ipd_sub_port_fcs.u64 = cvmx_read_csr(CVMX_IPD_SUB_PORT_FCS);
-				ipd_sub_port_fcs.s.port_bit &= 0xffffffffull ^ (1ull << priv->port);
+				ipd_sub_port_fcs.s.port_bit &= 0xffffffffull ^ (1ull << priv->ipd_port);
 				cvmx_write_csr(CVMX_IPD_SUB_PORT_FCS, ipd_sub_port_fcs.u64);
 
 				/* Clear any error bits */
-				cvmx_write_csr(CVMX_GMXX_RXX_INT_REG(index, interface), gmxx_rxx_int_reg.u64);
+				cvmx_write_csr(CVMX_GMXX_RXX_INT_REG(priv->interface_port, priv->interface), gmxx_rxx_int_reg.u64);
 				DEBUGPRINT("%s: Using 10Mbps with software preamble removal\n",
 				     dev->name);
 			}
@@ -104,23 +102,21 @@ static void cvm_oct_rgmii_poll(struct net_device *dev)
 		union cvmx_gmxx_rxx_frm_ctl gmxx_rxx_frm_ctl;
 		union cvmx_ipd_sub_port_fcs ipd_sub_port_fcs;
 		union cvmx_gmxx_rxx_int_reg gmxx_rxx_int_reg;
-		int interface = INTERFACE(priv->port);
-		int index = INDEX(priv->port);
 
 		/* Enable preamble checking */
-		gmxx_rxx_frm_ctl.u64 = cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(index, interface));
+		gmxx_rxx_frm_ctl.u64 = cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface));
 		gmxx_rxx_frm_ctl.s.pre_chk = 1;
-		cvmx_write_csr(CVMX_GMXX_RXX_FRM_CTL(index, interface), gmxx_rxx_frm_ctl.u64);
+		cvmx_write_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface), gmxx_rxx_frm_ctl.u64);
 		/* Enable FCS stripping */
 		ipd_sub_port_fcs.u64 = cvmx_read_csr(CVMX_IPD_SUB_PORT_FCS);
-		ipd_sub_port_fcs.s.port_bit |= 1ull << priv->port;
+		ipd_sub_port_fcs.s.port_bit |= 1ull << priv->ipd_port;
 		cvmx_write_csr(CVMX_IPD_SUB_PORT_FCS, ipd_sub_port_fcs.u64);
 		/* Clear any error bits */
-		gmxx_rxx_int_reg.u64 = cvmx_read_csr(CVMX_GMXX_RXX_INT_REG(index, interface));
-		cvmx_write_csr(CVMX_GMXX_RXX_INT_REG(index, interface), gmxx_rxx_int_reg.u64);
+		gmxx_rxx_int_reg.u64 = cvmx_read_csr(CVMX_GMXX_RXX_INT_REG(priv->interface_port, priv->interface));
+		cvmx_write_csr(CVMX_GMXX_RXX_INT_REG(priv->interface_port, priv->interface), gmxx_rxx_int_reg.u64);
 	}
 	if (priv->phydev == NULL) {
-		link_info = cvmx_helper_link_autoconf(priv->port);
+		link_info = cvmx_helper_link_autoconf(priv->ipd_port);
 		priv->link_info = link_info.u64;
 	}
 
@@ -153,7 +149,7 @@ static irqreturn_t cvm_oct_rgmii_rml_interrupt(int cpl, void *dev_id)
 			if (gmx_rx_int_reg.s.phy_dupx
 			    || gmx_rx_int_reg.s.phy_link
 			    || gmx_rx_int_reg.s.phy_spd) {
-				struct octeon_ethernet *priv = cvm_oct_by_port[cvmx_helper_get_ipd_port(interface, index)];
+				struct octeon_ethernet *priv = cvm_oct_dev_for_port(cvmx_helper_get_ipd_port(interface, index));
 
 				if (priv && !atomic_read(&cvm_oct_poll_queue_stopping))
 					queue_work(cvm_oct_poll_queue, &priv->port_work);
@@ -185,7 +181,7 @@ static irqreturn_t cvm_oct_rgmii_rml_interrupt(int cpl, void *dev_id)
 			if (gmx_rx_int_reg.s.phy_dupx
 			    || gmx_rx_int_reg.s.phy_link
 			    || gmx_rx_int_reg.s.phy_spd) {
-				struct octeon_ethernet *priv = cvm_oct_by_port[cvmx_helper_get_ipd_port(interface, index)];
+				struct octeon_ethernet *priv = cvm_oct_dev_for_port(cvmx_helper_get_ipd_port(interface, index));
 
 				if (priv && !atomic_read(&cvm_oct_poll_queue_stopping))
 					queue_work(cvm_oct_poll_queue, &priv->port_work);
@@ -206,16 +202,14 @@ int cvm_oct_rgmii_open(struct net_device *dev)
 {
 	union cvmx_gmxx_prtx_cfg gmx_cfg;
 	struct octeon_ethernet *priv = netdev_priv(dev);
-	int interface = INTERFACE(priv->port);
-	int index = INDEX(priv->port);
 	cvmx_helper_link_info_t link_info;
 
-	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
+	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface));
 	gmx_cfg.s.en = 1;
-	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmx_cfg.u64);
+	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface), gmx_cfg.u64);
 
 	if (!octeon_is_simulation()) {
-		link_info = cvmx_helper_link_get(priv->port);
+		link_info = cvmx_helper_link_get(priv->ipd_port);
 		if (!link_info.s.link_up)
 			netif_carrier_off(dev);
 	}
@@ -227,12 +221,10 @@ int cvm_oct_rgmii_stop(struct net_device *dev)
 {
 	union cvmx_gmxx_prtx_cfg gmx_cfg;
 	struct octeon_ethernet *priv = netdev_priv(dev);
-	int interface = INTERFACE(priv->port);
-	int index = INDEX(priv->port);
 
-	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
+	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface));
 	gmx_cfg.s.en = 0;
-	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmx_cfg.u64);
+	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface), gmx_cfg.u64);
 	return 0;
 }
 
@@ -276,24 +268,20 @@ int cvm_oct_rgmii_init(struct net_device *dev)
 	 * Only true RGMII ports need to be polled. In GMII mode, port
 	 * 0 is really a RGMII port.
 	 */
-	if (((priv->imode == CVMX_HELPER_INTERFACE_MODE_GMII) && (priv->port == 0))
+	if (((priv->imode == CVMX_HELPER_INTERFACE_MODE_GMII) && (priv->ipd_port == 0))
 	    || (priv->imode == CVMX_HELPER_INTERFACE_MODE_RGMII)) {
 
 		if (!octeon_is_simulation()) {
-
 			union cvmx_gmxx_rxx_int_en gmx_rx_int_en;
-			int interface = INTERFACE(priv->port);
-			int index = INDEX(priv->port);
-
 			/*
 			 * Enable interrupts on inband status changes
 			 * for this port.
 			 */
-			gmx_rx_int_en.u64 = cvmx_read_csr(CVMX_GMXX_RXX_INT_EN(index, interface));
+			gmx_rx_int_en.u64 = cvmx_read_csr(CVMX_GMXX_RXX_INT_EN(priv->interface_port, priv->interface));
 			gmx_rx_int_en.s.phy_dupx = 1;
 			gmx_rx_int_en.s.phy_link = 1;
 			gmx_rx_int_en.s.phy_spd = 1;
-			cvmx_write_csr(CVMX_GMXX_RXX_INT_EN(index, interface), gmx_rx_int_en.u64);
+			cvmx_write_csr(CVMX_GMXX_RXX_INT_EN(priv->interface_port, priv->interface), gmx_rx_int_en.u64);
 			priv->poll = cvm_oct_rgmii_poll;
 		}
 	}
@@ -310,24 +298,21 @@ void cvm_oct_rgmii_uninit(struct net_device *dev)
 	 * Only true RGMII ports need to be polled. In GMII mode, port
 	 * 0 is really a RGMII port.
 	 */
-	if (((priv->imode == CVMX_HELPER_INTERFACE_MODE_GMII) && (priv->port == 0))
+	if (((priv->imode == CVMX_HELPER_INTERFACE_MODE_GMII) && (priv->ipd_port == 0))
 	    || (priv->imode == CVMX_HELPER_INTERFACE_MODE_RGMII)) {
 
 		if (!octeon_is_simulation()) {
-
 			union cvmx_gmxx_rxx_int_en gmx_rx_int_en;
-			int interface = INTERFACE(priv->port);
-			int index = INDEX(priv->port);
 
 			/*
 			 * Disable interrupts on inband status changes
 			 * for this port.
 			 */
-			gmx_rx_int_en.u64 = cvmx_read_csr(CVMX_GMXX_RXX_INT_EN(index, interface));
+			gmx_rx_int_en.u64 = cvmx_read_csr(CVMX_GMXX_RXX_INT_EN(priv->interface_port, priv->interface));
 			gmx_rx_int_en.s.phy_dupx = 0;
 			gmx_rx_int_en.s.phy_link = 0;
 			gmx_rx_int_en.s.phy_spd = 0;
-			cvmx_write_csr(CVMX_GMXX_RXX_INT_EN(index, interface), gmx_rx_int_en.u64);
+			cvmx_write_csr(CVMX_GMXX_RXX_INT_EN(priv->interface_port, priv->interface), gmx_rx_int_en.u64);
 		}
 	}
 
diff --git a/drivers/net/octeon/ethernet-rx.c b/drivers/net/octeon/ethernet-rx.c
index 9c56865..0c3cf3d 100644
--- a/drivers/net/octeon/ethernet-rx.c
+++ b/drivers/net/octeon/ethernet-rx.c
@@ -36,6 +36,7 @@
 #include <linux/prefetch.h>
 #include <linux/smp.h>
 #include <linux/ktime.h>
+#include <linux/delay.h>
 #include <net/dst.h>
 #ifdef CONFIG_XFRM
 #include <linux/xfrm.h>
@@ -54,6 +55,8 @@
 #include <asm/octeon/cvmx-scratch.h>
 
 #include <asm/octeon/cvmx-gmxx-defs.h>
+#include <asm/octeon/cvmx-sso-defs.h>
+#include <asm/octeon/cvmx-fpa-defs.h>
 
 #include "ethernet-defines.h"
 #include "ethernet-mem.h"
@@ -61,7 +64,6 @@
 #include "octeon-ethernet.h"
 #include "ethernet-util.h"
 
-
 struct cvm_napi_wrapper {
 	struct napi_struct napi;
 	int available;
@@ -182,7 +184,9 @@ static irqreturn_t cvm_oct_do_interrupt(int cpl, void *dev_id)
  */
 static inline int cvm_oct_check_rcv_error(cvmx_wqe_t *work)
 {
-	if ((work->word2.snoip.err_code == 10) && (work->len <= 64)) {
+	int port = cvmx_wqe_get_port(work);
+
+	if ((work->word2.snoip.err_code == 10) && (work->word1.len <= 64)) {
 		/*
 		 * Ignore length errors on min size packets. Some
 		 * equipment incorrectly pads packets to 64+4FCS
@@ -202,8 +206,8 @@ static inline int cvm_oct_check_rcv_error(cvmx_wqe_t *work)
 		 * packet to determine if we can remove a non spec
 		 * preamble and generate a correct packet.
 		 */
-		int interface = cvmx_helper_get_interface_num(work->ipprt);
-		int index = cvmx_helper_get_interface_index_num(work->ipprt);
+		int interface = cvmx_helper_get_interface_num(port);
+		int index = cvmx_helper_get_interface_index_num(port);
 		union cvmx_gmxx_rxx_frm_ctl gmxx_rxx_frm_ctl;
 		gmxx_rxx_frm_ctl.u64 = cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(index, interface));
 		if (gmxx_rxx_frm_ctl.s.pre_chk == 0) {
@@ -211,7 +215,7 @@ static inline int cvm_oct_check_rcv_error(cvmx_wqe_t *work)
 			uint8_t *ptr = cvmx_phys_to_ptr(work->packet_ptr.s.addr);
 			int i = 0;
 
-			while (i < work->len - 1) {
+			while (i < work->word1.len - 1) {
 				if (*ptr != 0x55)
 					break;
 				ptr++;
@@ -223,20 +227,20 @@ static inline int cvm_oct_check_rcv_error(cvmx_wqe_t *work)
 				   DEBUGPRINT("Port %d received 0xd5 preamble\n", work->ipprt);
 				 */
 				work->packet_ptr.s.addr += i + 1;
-				work->len -= i + 5;
+				work->word1.len -= i + 5;
 			} else if ((*ptr & 0xf) == 0xd) {
 				/*
 				   DEBUGPRINT("Port %d received 0x?d preamble\n", work->ipprt);
 				 */
 				work->packet_ptr.s.addr += i;
-				work->len -= i + 4;
-				for (i = 0; i < work->len; i++) {
+				work->word1.len -= i + 4;
+				for (i = 0; i < work->word1.len; i++) {
 					*ptr = ((*ptr & 0xf0) >> 4) | ((*(ptr + 1) & 0xf) << 4);
 					ptr++;
 				}
 			} else {
 				DEBUGPRINT("Port %d unknown preamble, packet dropped\n",
-				     work->ipprt);
+				     port);
 				/* cvmx_helper_dump_packet(work); */
 				cvm_oct_free_work(work);
 				return 1;
@@ -244,7 +248,7 @@ static inline int cvm_oct_check_rcv_error(cvmx_wqe_t *work)
 		}
 	} else {
 		DEBUGPRINT("Port %d receive error code %d, packet dropped\n",
-			   work->ipprt, work->word2.snoip.err_code);
+			   port, work->word2.snoip.err_code);
 		cvm_oct_free_work(work);
 		return 1;
 	}
@@ -279,375 +283,16 @@ static ktime_t cvm_oct_ptp_to_ktime(uint64_t ptptime)
 
 	return ktime_sub_ns(ktimebase, ptpbase - ptptime);
 }
+#undef CVM_OCT_NAPI_68
+#include "ethernet-napi.c"
 
-/**
- * cvm_oct_napi_poll - the NAPI poll function.
- * @napi: The NAPI instance, or null if called from cvm_oct_poll_controller
- * @budget: Maximum number of packets to receive.
- *
- * Returns the number of packets processed.
- */
-static int cvm_oct_napi_poll(struct napi_struct *napi, int budget)
-{
-	const int	coreid = cvmx_get_core_num();
-	uint64_t	old_group_mask;
-	uint64_t	old_scratch;
-	int		rx_count = 0;
-	int		did_work_request = 0;
-	int		packet_not_copied;
-
-	char		*p = (char *)cvm_oct_by_port;
-	/* Prefetch cvm_oct_device since we know we need it soon */
-	prefetch(&p[0]);
-	prefetch(&p[SMP_CACHE_BYTES]);
-	prefetch(&p[2 * SMP_CACHE_BYTES]);
-
-	if (USE_ASYNC_IOBDMA) {
-		/* Save scratch in case userspace is using it */
-		CVMX_SYNCIOBDMA;
-		old_scratch = cvmx_scratch_read64(CVMX_SCR_SCRATCH);
-	}
-
-	/* Only allow work for our group (and preserve priorities) */
-	old_group_mask = cvmx_read_csr(CVMX_POW_PP_GRP_MSKX(coreid));
-	cvmx_write_csr(CVMX_POW_PP_GRP_MSKX(coreid),
-		       (old_group_mask & ~0xFFFFull) | 1 << pow_receive_group);
-
-	if (USE_ASYNC_IOBDMA) {
-		cvmx_pow_work_request_async(CVMX_SCR_SCRATCH, CVMX_POW_NO_WAIT);
-		did_work_request = 1;
-	}
-
-	while (rx_count < budget) {
-		struct sk_buff *skb = NULL;
-		struct sk_buff **pskb = NULL;
-		enum cvm_oct_callback_result callback_result;
-		int skb_in_hw;
-		cvmx_wqe_t *work;
-
-		if (USE_ASYNC_IOBDMA && did_work_request)
-			work = cvmx_pow_work_response_async(CVMX_SCR_SCRATCH);
-		else
-			work = cvmx_pow_work_request_sync(CVMX_POW_NO_WAIT);
-
-		prefetch(work);
-		did_work_request = 0;
-		if (work == NULL) {
-			union cvmx_pow_wq_int wq_int;
-			wq_int.u64 = 0;
-			wq_int.s.iq_dis = 1 << pow_receive_group;
-			wq_int.s.wq_int = 1 << pow_receive_group;
-			cvmx_write_csr(CVMX_POW_WQ_INT, wq_int.u64);
-			break;
-		}
-		pskb = (struct sk_buff **)(cvm_oct_get_buffer_ptr(work->packet_ptr) - sizeof(void *));
-		prefetch(pskb);
-
-		if (USE_ASYNC_IOBDMA && rx_count < (budget - 1)) {
-			cvmx_pow_work_request_async_nocheck(CVMX_SCR_SCRATCH, CVMX_POW_NO_WAIT);
-			did_work_request = 1;
-		}
-
-		if (rx_count == 0) {
-			/*
-			 * First time through, see if there is enough
-			 * work waiting to merit waking another
-			 * CPU.
-			 */
-			union cvmx_pow_wq_int_cntx counts;
-			int backlog;
-			int cores_in_use = core_state.active_cores;
-
-			counts.u64 = cvmx_read_csr(CVMX_POW_WQ_INT_CNTX(pow_receive_group));
-			backlog = counts.s.iq_cnt + counts.s.ds_cnt;
-			if (backlog > budget * cores_in_use &&
-			    napi != NULL &&
-			    cores_in_use < core_state.baseline_cores)
-				cvm_oct_enable_one_cpu();
-		}
-
-		/*
-		 * If WORD2[SOFTWARE] then this WQE is a complete for
-		 * a TX packet.
-		 */
-		if (work->word2.s.software) {
-			struct octeon_ethernet *priv;
-			union skb_shared_tx *shtx;
-			int packet_qos = work->unused;
-			skb = (struct sk_buff *)work->packet_ptr.u64;
-			priv = netdev_priv(skb->dev);
-			if (!netif_running(skb->dev))
-				netif_wake_queue(skb->dev);
-			shtx = skb_tx(skb);
-			if (unlikely(shtx->hardware)) {
-				if (priv->flags &
-				    OCTEON_ETHERNET_FLAG_TX_TIMESTAMP_HW) {
-					uint64_t ns =
-						*(uint64_t *)work->packet_data;
-					struct skb_shared_hwtstamps ts;
-					ts.syststamp = cvm_oct_ptp_to_ktime(ns);
-					ts.hwtstamp = ns_to_ktime(ns);
-					skb_tstamp_tx(skb, &ts);
-				}
-				if (priv->flags &
-				    OCTEON_ETHERNET_FLAG_TX_TIMESTAMP_SW) {
-					uint64_t ns =
-						*(uint64_t *)work->packet_data;
-					struct skb_shared_hwtstamps ts;
-					ts.syststamp = ns_to_ktime(ns);
-					ts.hwtstamp = ns_to_ktime(0);
-					skb_tstamp_tx(skb, &ts);
-				}
-			}
-
-			dev_kfree_skb_any(skb);
-
-			cvmx_fpa_free(work, CVMX_FPA_TX_WQE_POOL, DONT_WRITEBACK(1));
-
-			/*
-			 * We are done with this one, adjust the queue
-			 * depth.
-			 */
-			cvmx_fau_atomic_add32(priv->tx_queue[packet_qos].fau, -1);
-			continue;
-		}
-
-		skb_in_hw = USE_SKBUFFS_IN_HW && work->word2.s.bufs == 1;
-		if (likely(skb_in_hw)) {
-			skb = *pskb;
-			prefetch(&skb->head);
-			prefetch(&skb->len);
-		}
-		prefetch(cvm_oct_by_port[work->ipprt]);
-
-		/* Immediately throw away all packets with receive errors */
-		if (unlikely(work->word2.snoip.rcv_error)) {
-			if (cvm_oct_check_rcv_error(work))
-				continue;
-		}
-
-		/*
-		 * We can only use the zero copy path if skbuffs are
-		 * in the FPA pool and the packet fits in a single
-		 * buffer.
-		 */
-		if (likely(skb_in_hw)) {
-			skb->data = skb->head + work->packet_ptr.s.addr - cvmx_ptr_to_phys(skb->head);
-			prefetch(skb->data);
-			skb->len = work->len;
-			skb_set_tail_pointer(skb, skb->len);
-			packet_not_copied = 1;
-		} else {
-			/*
-			 * We have to copy the packet. First allocate
-			 * an skbuff for it.
-			 */
-			skb = dev_alloc_skb(work->len);
-			if (!skb) {
-				DEBUGPRINT("Port %d failed to allocate skbuff, packet dropped\n",
-					   work->ipprt);
-				cvm_oct_free_work(work);
-				continue;
-			}
-
-			/*
-			 * Check if we've received a packet that was
-			 * entirely stored in the work entry.
-			 */
-			if (unlikely(work->word2.s.bufs == 0)) {
-				uint8_t *ptr = work->packet_data;
-
-				if (likely(!work->word2.s.not_IP)) {
-					/*
-					 * The beginning of the packet
-					 * moves for IP packets.
-					 */
-					if (work->word2.s.is_v6)
-						ptr += 2;
-					else
-						ptr += 6;
-				}
-				memcpy(skb_put(skb, work->len), ptr, work->len);
-				/* No packet buffers to free */
-			} else {
-				int segments = work->word2.s.bufs;
-				union cvmx_buf_ptr segment_ptr = work->packet_ptr;
-				int len = work->len;
-
-				while (segments--) {
-					union cvmx_buf_ptr  next_ptr;
-					int segment_size;
-
-					next_ptr = *(union cvmx_buf_ptr *)cvmx_phys_to_ptr(segment_ptr.s.addr - 8);
-
-			/*
-			 * Octeon Errata PKI-100: The segment size is
-			 * wrong. Until it is fixed, calculate the
-			 * segment size based on the packet pool
-			 * buffer size. When it is fixed, the
-			 * following line should be replaced with this
-			 * one: int segment_size =
-			 * segment_ptr.s.size;
-			 */
-					segment_size = CVMX_FPA_PACKET_POOL_SIZE -
-						(segment_ptr.s.addr - (((segment_ptr.s.addr >> 7) - segment_ptr.s.back) << 7));
-					/*
-					 * Don't copy more than what
-					 * is left in the packet.
-					 */
-					if (segment_size > len)
-						segment_size = len;
-					/* Copy the data into the packet */
-					memcpy(skb_put(skb, segment_size),
-					       cvmx_phys_to_ptr(segment_ptr.s.addr),
-					       segment_size);
-					len -= segment_size;
-					segment_ptr = next_ptr;
-				}
-			}
-			packet_not_copied = 0;
-		}
-
-		if (likely((work->ipprt < TOTAL_NUMBER_OF_PORTS) && cvm_oct_by_port[work->ipprt])) {
-			struct octeon_ethernet *priv = cvm_oct_by_port[work->ipprt];
-#ifdef CONFIG_RAPIDIO
-			if (unlikely(priv->imode == CVMX_HELPER_INTERFACE_MODE_SRIO)) {
-				const cvmx_srio_rx_message_header_t *rx_header = (const cvmx_srio_rx_message_header_t *)skb->data;
-				__skb_pull(skb, sizeof(cvmx_srio_rx_message_header_t));
-				priv = cvm_oct_by_srio_mbox[(work->ipprt - 40) >> 1][rx_header->word0.s.mbox];
-
-				atomic64_add(1, (atomic64_t *)&priv->netdev->stats.rx_packets);
-				atomic64_add(skb->len, (atomic64_t *)&priv->netdev->stats.rx_bytes);
-			}
-#endif
-			/*
-			 * Only accept packets for devices that are
-			 * currently up.
-			 */
-			if (likely(priv->netdev->flags & IFF_UP)) {
-				if (priv->flags & OCTEON_ETHERNET_FLAG_RX_TIMESTAMP_SW) {
-					struct skb_shared_hwtstamps *ts;
-					ts = skb_hwtstamps(skb);
-					ts->syststamp = ktime_get_real();
-					ts->hwtstamp = ns_to_ktime(0);
-				}
-				if (priv->flags & OCTEON_ETHERNET_FLAG_RX_TIMESTAMP_HW) {
-					/* The first 8 bytes are the timestamp */
-					uint64_t ns = *(uint64_t *)skb->data;
-					struct skb_shared_hwtstamps *ts;
-					ts = skb_hwtstamps(skb);
-					ts->hwtstamp = ns_to_ktime(ns);
-					ts->syststamp = cvm_oct_ptp_to_ktime(ns);
-					__skb_pull(skb, 8);
-				}
-				skb->protocol = eth_type_trans(skb, priv->netdev);
-				skb->dev = priv->netdev;
-
-				if (unlikely(work->word2.s.not_IP || work->word2.s.IP_exc ||
-					    work->word2.s.L4_error || !work->word2.s.tcp_or_udp))
-					skb->ip_summed = CHECKSUM_NONE;
-				else
-					skb->ip_summed = CHECKSUM_UNNECESSARY;
-
-				/* Increment RX stats for virtual ports */
-				if (work->ipprt >= CVMX_PIP_NUM_INPUT_PORTS) {
-					atomic64_add(1, (atomic64_t *)&priv->netdev->stats.rx_packets);
-					atomic64_add(skb->len, (atomic64_t *)&priv->netdev->stats.rx_bytes);
-				}
-				if (priv->intercept_cb) {
-					callback_result = priv->intercept_cb(priv->netdev, work, skb);
-					switch (callback_result) {
-					case CVM_OCT_PASS:
-						netif_receive_skb(skb);
-						rx_count++;
-						break;
-					case CVM_OCT_DROP:
-						dev_kfree_skb_irq(skb);
-						atomic64_add(1, (atomic64_t *)&priv->netdev->stats.rx_dropped);
-						break;
-					case CVM_OCT_TAKE_OWNERSHIP_WORK:
-						/*
-						 * Interceptor took
-						 * our work, but we
-						 * need to free the
-						 * skbuff
-						 */
-						if (USE_SKBUFFS_IN_HW && likely(packet_not_copied)) {
-							/*
-							 * We can't free the skbuff since its data is
-							 * the same as the work. In this case we don't
-							 * do anything
-							 */
-						} else {
-							dev_kfree_skb_irq(skb);
-						}
-						break;
-					case CVM_OCT_TAKE_OWNERSHIP_SKB:
-						/* Interceptor took our packet */
-						break;
-					}
-				} else {
-					netif_receive_skb(skb);
-					callback_result = CVM_OCT_PASS;
-					rx_count++;
-				}
-			} else {
-				/* Drop any packet received for a device that isn't up */
-				/*
-				DEBUGPRINT("%s: Device not up, packet dropped\n",
-					   dev->name);
-				*/
-				atomic64_add(1, (atomic64_t *)&priv->netdev->stats.rx_dropped);
-				dev_kfree_skb_irq(skb);
-				callback_result = CVM_OCT_DROP;
-			}
-		} else {
-			/*
-			 * Drop any packet received for a device that
-			 * doesn't exist.
-			 */
-			DEBUGPRINT("Port %d not controlled by Linux, packet dropped\n",
-				   work->ipprt);
-			dev_kfree_skb_irq(skb);
-			callback_result = CVM_OCT_DROP;
-		}
-		/* We only need to free the work if the interceptor didn't
-		   take over ownership of it */
-		if (callback_result != CVM_OCT_TAKE_OWNERSHIP_WORK) {
-			/*
-			 * Check to see if the skbuff and work share
-			 * the same packet buffer.
-			 */
-			if (USE_SKBUFFS_IN_HW && likely(packet_not_copied)) {
-				/*
-				 * This buffer needs to be replaced,
-				 * increment the number of buffers we
-				 * need to free by one.
-				 */
-				cvmx_fau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE, 1);
-				cvmx_fpa_free(work, CVMX_FPA_WQE_POOL, DONT_WRITEBACK(1));
-			} else {
-				cvm_oct_free_work(work);
-			}
-		}
-	}
-	/* Restore the original POW group mask */
-	cvmx_write_csr(CVMX_POW_PP_GRP_MSKX(coreid), old_group_mask);
-	if (USE_ASYNC_IOBDMA) {
-		/* Restore the scratch area */
-		cvmx_scratch_write64(CVMX_SCR_SCRATCH, old_scratch);
-	}
-	cvm_oct_rx_refill_pool(0);
+#define CVM_OCT_NAPI_68
+#include "ethernet-napi.c"
 
-	if (rx_count < budget && napi != NULL) {
-		/* No more work */
-		napi_complete(napi);
-		cvm_oct_no_more_work(napi);
-	}
-	return rx_count;
-}
+static int (*cvm_oct_napi_poll)(struct napi_struct *, int);
 
 #ifdef CONFIG_NET_POLL_CONTROLLER
+
 /**
  * cvm_oct_poll_controller - poll for receive packets
  * device.
@@ -660,17 +305,22 @@ void cvm_oct_poll_controller(struct net_device *dev)
 }
 #endif
 
-void cvm_oct_rx_initialize(void)
+static struct kmem_cache *cvm_oct_kmem_sso;
+
+void cvm_oct_rx_initialize(int num_wqe)
 {
 	int i;
 	struct net_device *dev_for_napi = NULL;
-	union cvmx_pow_wq_int_thrx int_thr;
-	union cvmx_pow_wq_int_pc int_pc;
 	unsigned long irq_flags = 0;
 
 	if (list_empty(&cvm_oct_list))
 		panic("No net_devices were allocated.");
 
+	if (OCTEON_IS_MODEL(OCTEON_CN68XX))
+		cvm_oct_napi_poll = cvm_oct_napi_poll_68;
+	else
+		cvm_oct_napi_poll = cvm_oct_napi_poll_38;
+
 	dev_for_napi = list_first_entry(&cvm_oct_list,
 					struct octeon_ethernet,
 					list)->netdev;
@@ -707,29 +357,187 @@ void cvm_oct_rx_initialize(void)
 
 	disable_irq_nosync(OCTEON_IRQ_WORKQ0 + pow_receive_group);
 
-	int_thr.u64 = 0;
-	int_thr.s.tc_en = 1;
-	int_thr.s.tc_thr = 1;
-	/* Enable POW interrupt when our port has at least one packet */
-	cvmx_write_csr(CVMX_POW_WQ_INT_THRX(pow_receive_group), int_thr.u64);
+	if (OCTEON_IS_MODEL(OCTEON_CN68XX)) {
+		union cvmx_sso_wq_int_thrx int_thr;
+		union cvmx_sso_wq_int_pc int_pc;
+		union cvmx_sso_cfg sso_cfg;
+		union cvmx_fpa_fpfx_marks fpa_marks;
+		int i;
+		int rwq_bufs = 48 + DIV_ROUND_UP(num_wqe, 26);
+
+		cvm_oct_kmem_sso = kmem_cache_create("octeon_ethernet_sso", 256, 128, 0, NULL);
+		if (cvm_oct_kmem_sso == NULL) {
+			pr_err("cannot create kmem_cache for octeon_ethernet_sso\n");
+			goto err;
+		}
 
-	int_pc.u64 = 0;
-	int_pc.s.pc_thr = 5;
-	cvmx_write_csr(CVMX_POW_WQ_INT_PC, int_pc.u64);
+		/*
+		 * CN68XX-P1 may reset with the wrong values, put in
+		 * the correct values.
+		 */
+		fpa_marks.u64 = 0;
+		fpa_marks.s.fpf_wr = 0xa4;
+		fpa_marks.s.fpf_rd = 0x40;
+		cvmx_write_csr(CVMX_FPA_FPF8_MARKS, fpa_marks.u64);
+
+		/* Make sure RWI/RWO is disabled. */
+		sso_cfg.u64 = cvmx_read_csr(CVMX_SSO_CFG);
+		sso_cfg.s.rwen = 0;
+		cvmx_write_csr(CVMX_SSO_CFG, sso_cfg.u64);
+
+		while (rwq_bufs) {
+			union cvmx_sso_rwq_psh_fptr fptr;
+			void *mem;
+
+			mem = kmem_cache_alloc(cvm_oct_kmem_sso, GFP_KERNEL);
+			if (mem == NULL) {
+				pr_err("cannot allocate memory from octeon_ethernet_sso\n");
+				goto err;
+			}
+			for (;;) {
+				fptr.u64 = cvmx_read_csr(CVMX_SSO_RWQ_PSH_FPTR);
+				if (!fptr.s.full)
+					break;
+				__delay(1000);
+			}
+			fptr.s.fptr = virt_to_phys(mem) >> 7;
+			cvmx_write_csr(CVMX_SSO_RWQ_PSH_FPTR, fptr.u64);
+			rwq_bufs--;
+		}
+		for (i = 0; i < 8; i++) {
+			cvmx_sso_rwq_head_ptrx_t head_ptr;
+			cvmx_sso_rwq_tail_ptrx_t tail_ptr;
+			void *mem;
+
+			mem = kmem_cache_alloc(cvm_oct_kmem_sso, GFP_KERNEL);
+			if (mem == NULL) {
+				pr_err("cannot allocate memory from octeon_ethernet_sso\n");
+				goto err;
+			}
 
+			head_ptr.u64 = 0;
+			tail_ptr.u64 = 0;
+			head_ptr.s.ptr = virt_to_phys(mem) >> 7;
+			tail_ptr.s.ptr = head_ptr.s.ptr;
+			cvmx_write_csr(CVMX_SSO_RWQ_HEAD_PTRX(i), head_ptr.u64);
+			cvmx_write_csr(CVMX_SSO_RWQ_TAIL_PTRX(i), tail_ptr.u64);
+		}
+		/* Now enable the SS0  RWI/RWO */
+		sso_cfg.u64 = cvmx_read_csr(CVMX_SSO_CFG);
+		sso_cfg.s.rwen = 1;
+		sso_cfg.s.rwq_byp_dis = 0;
+		sso_cfg.s.rwio_byp_dis = 0;
+		cvmx_write_csr(CVMX_SSO_CFG, sso_cfg.u64);
+
+		int_thr.u64 = 0;
+		int_thr.s.tc_en = 1;
+		int_thr.s.tc_thr = 1;
+		/* Enable POW interrupt when our port has at least one packet */
+		cvmx_write_csr(CVMX_SSO_WQ_INT_THRX(pow_receive_group), int_thr.u64);
+
+		int_pc.u64 = 0;
+		int_pc.s.pc_thr = 5;
+		cvmx_write_csr(CVMX_SSO_WQ_INT_PC, int_pc.u64);
+	} else {
+		union cvmx_pow_wq_int_thrx int_thr;
+		union cvmx_pow_wq_int_pc int_pc;
+		int_thr.u64 = 0;
+		int_thr.s.tc_en = 1;
+		int_thr.s.tc_thr = 1;
+		/* Enable POW interrupt when our port has at least one packet */
+		cvmx_write_csr(CVMX_POW_WQ_INT_THRX(pow_receive_group), int_thr.u64);
+
+		int_pc.u64 = 0;
+		int_pc.s.pc_thr = 5;
+		cvmx_write_csr(CVMX_POW_WQ_INT_PC, int_pc.u64);
+	}
 
 	/* Scheduld NAPI now.  This will indirectly enable interrupts. */
 	cvm_oct_enable_one_cpu();
+
+	return;
+err:
+	free_irq(OCTEON_IRQ_WORKQ0 + pow_receive_group, &cvm_oct_list);
+	return;
 }
 
-void cvm_oct_rx_shutdown(void)
+void cvm_oct_rx_shutdown(int num_wqe)
 {
 	int i;
+
+	/* Disable POW/SSO interrupt */
+	if (OCTEON_IS_MODEL(OCTEON_CN68XX))
+		cvmx_write_csr(CVMX_SSO_WQ_INT_THRX(pow_receive_group), 0);
+	else
+		cvmx_write_csr(CVMX_POW_WQ_INT_THRX(pow_receive_group), 0);
+
+	/* Free the interrupt handler */
+	free_irq(OCTEON_IRQ_WORKQ0 + pow_receive_group, &cvm_oct_list);
+
 	/* Shutdown all of the NAPIs */
 	for_each_possible_cpu(i)
 		netif_napi_del(&cvm_oct_napi[i].napi);
 
-	/* Free the interrupt handler */
-	free_irq(OCTEON_IRQ_WORKQ0 + pow_receive_group, &cvm_oct_list);
+	if (OCTEON_IS_MODEL(OCTEON_CN68XX)) {
+		union cvmx_fpa_quex_available queue_available;
+		union cvmx_sso_cfg sso_cfg;
+		union cvmx_sso_rwq_pop_fptr pop_fptr;
+		union cvmx_sso_rwq_psh_fptr fptr;
+		union cvmx_sso_fpage_cnt fpage_cnt;
+		int num_to_transfer, count, i;
+		void *mem;
+
+		sso_cfg.u64 = cvmx_read_csr(CVMX_SSO_CFG);
+		sso_cfg.s.rwen = 0;
+		sso_cfg.s.rwq_byp_dis = 1;
+		cvmx_write_csr(CVMX_SSO_CFG, sso_cfg.u64);
+
+		queue_available.u64 = cvmx_read_csr(CVMX_FPA_QUEX_AVAILABLE(8));
+
+		/* Make CVMX_FPA_QUEX_AVAILABLE(8) % 16 == 0*/
+		for (num_to_transfer = (16 - queue_available.s.que_siz) % 16;
+		     num_to_transfer > 0; num_to_transfer--) {
+			do {
+				pop_fptr.u64 = cvmx_read_csr(CVMX_SSO_RWQ_POP_FPTR);
+			} while (!pop_fptr.s.val);
+			for (;;) {
+				fptr.u64 = cvmx_read_csr(CVMX_SSO_RWQ_PSH_FPTR);
+				if (!fptr.s.full)
+					break;
+				__delay(1000);
+			}
+			fptr.s.fptr = pop_fptr.s.fptr;
+			cvmx_write_csr(CVMX_SSO_RWQ_PSH_FPTR, fptr.u64);
+		}
+
+		sso_cfg.s.rwen = 1;
+		sso_cfg.s.rwq_byp_dis = 0;
+		cvmx_write_csr(CVMX_SSO_CFG, sso_cfg.u64);
 
+		fpage_cnt.u64 = cvmx_read_csr(CVMX_SSO_FPAGE_CNT);
+		count = fpage_cnt.s.fpage_cnt;
+		while (count > 0) {
+			do {
+				pop_fptr.u64 = cvmx_read_csr(CVMX_SSO_RWQ_POP_FPTR);
+			} while (!pop_fptr.s.val);
+
+			mem = phys_to_virt(pop_fptr.s.fptr << 7);
+			kmem_cache_free(cvm_oct_kmem_sso, mem);
+			count--;
+		}
+		for (i = 0; i < 8; i++) {
+			cvmx_sso_rwq_head_ptrx_t head_ptr;
+
+			void *mem;
+
+			head_ptr.u64 = cvmx_read_csr(CVMX_SSO_RWQ_HEAD_PTRX(i));
+			mem = phys_to_virt(head_ptr.s.ptr << 7);
+			kmem_cache_free(cvm_oct_kmem_sso, mem);
+		}
+		sso_cfg.s.rwen = 0;
+		sso_cfg.s.rwq_byp_dis = 0;
+		cvmx_write_csr(CVMX_SSO_CFG, sso_cfg.u64);
+		kmem_cache_destroy(cvm_oct_kmem_sso);
+		cvm_oct_kmem_sso = NULL;
+	}
 }
diff --git a/drivers/net/octeon/ethernet-rx.h b/drivers/net/octeon/ethernet-rx.h
index 9240c85..ce7dc2e 100644
--- a/drivers/net/octeon/ethernet-rx.h
+++ b/drivers/net/octeon/ethernet-rx.h
@@ -27,8 +27,8 @@
 #include <asm/octeon/cvmx-fau.h>
 
 void cvm_oct_poll_controller(struct net_device *dev);
-void cvm_oct_rx_initialize(void);
-void cvm_oct_rx_shutdown(void);
+void cvm_oct_rx_initialize(int);
+void cvm_oct_rx_shutdown(int);
 
 static inline void cvm_oct_rx_refill_pool(int fill_threshold)
 {
diff --git a/drivers/net/octeon/ethernet-sgmii.c b/drivers/net/octeon/ethernet-sgmii.c
index dcff1e5..5060785 100644
--- a/drivers/net/octeon/ethernet-sgmii.c
+++ b/drivers/net/octeon/ethernet-sgmii.c
@@ -42,16 +42,14 @@ int cvm_oct_sgmii_open(struct net_device *dev)
 {
 	union cvmx_gmxx_prtx_cfg gmx_cfg;
 	struct octeon_ethernet *priv = netdev_priv(dev);
-	int interface = INTERFACE(priv->port);
-	int index = INDEX(priv->port);
 	cvmx_helper_link_info_t link_info;
 
-	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
+	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface));
 	gmx_cfg.s.en = 1;
-	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmx_cfg.u64);
+	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface), gmx_cfg.u64);
 
 	if (!octeon_is_simulation()) {
-		link_info = cvmx_helper_link_get(priv->port);
+		link_info = cvmx_helper_link_get(priv->ipd_port);
 		if (!link_info.s.link_up)
 			netif_carrier_off(dev);
 	}
@@ -63,12 +61,10 @@ int cvm_oct_sgmii_stop(struct net_device *dev)
 {
 	union cvmx_gmxx_prtx_cfg gmx_cfg;
 	struct octeon_ethernet *priv = netdev_priv(dev);
-	int interface = INTERFACE(priv->port);
-	int index = INDEX(priv->port);
 
-	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
+	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface));
 	gmx_cfg.s.en = 0;
-	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmx_cfg.u64);
+	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface), gmx_cfg.u64);
 	return 0;
 }
 
@@ -77,11 +73,11 @@ static void cvm_oct_sgmii_poll(struct net_device *dev)
 	struct octeon_ethernet *priv = netdev_priv(dev);
 	cvmx_helper_link_info_t link_info;
 
-	link_info = cvmx_helper_link_get(priv->port);
+	link_info = cvmx_helper_link_get(priv->ipd_port);
 	if (link_info.u64 == priv->link_info)
 		return;
 
-	link_info = cvmx_helper_link_autoconf(priv->port);
+	link_info = cvmx_helper_link_autoconf(priv->ipd_port);
 	priv->link_info = link_info.u64;
 
 	/* Tell the core */
diff --git a/drivers/net/octeon/ethernet-spi.c b/drivers/net/octeon/ethernet-spi.c
index 858f48c..1de7463 100644
--- a/drivers/net/octeon/ethernet-spi.c
+++ b/drivers/net/octeon/ethernet-spi.c
@@ -218,7 +218,7 @@ static void cvm_oct_spi_poll(struct net_device *dev)
 
 	for (interface = 0; interface < 2; interface++) {
 
-		if ((priv->port == interface * 16) && need_retrain[interface]) {
+		if ((priv->ipd_port == interface * 16) && need_retrain[interface]) {
 
 			if (cvmx_spi_restart_interface(interface, CVMX_SPI_MODE_DUPLEX, 10) == 0) {
 				need_retrain[interface] = 0;
@@ -234,12 +234,12 @@ static void cvm_oct_spi_poll(struct net_device *dev)
 		 * we don't waste absurd amounts of time waiting for
 		 * TWSI.
 		 */
-		if (priv->port == spi4000_port) {
+		if (priv->ipd_port == spi4000_port) {
 			/*
 			 * This function does nothing if it is called on an
 			 * interface without a SPI4000.
 			 */
-			cvmx_spi4000_check_speed(interface, priv->port);
+			cvmx_spi4000_check_speed(interface, priv->ipd_port);
 			/*
 			 * Normal ordering increments. By decrementing
 			 * we only match once per iteration.
@@ -272,8 +272,8 @@ int cvm_oct_spi_init(struct net_device *dev)
 	}
 	number_spi_ports++;
 
-	if ((priv->port == 0) || (priv->port == 16)) {
-		cvm_oct_spi_enable_error_reporting(INTERFACE(priv->port));
+	if ((priv->ipd_port == 0) || (priv->ipd_port == 16)) {
+		cvm_oct_spi_enable_error_reporting(priv->interface);
 		priv->poll = cvm_oct_spi_poll;
 	}
 	cvm_oct_common_init(dev);
diff --git a/drivers/net/octeon/ethernet-tx.c b/drivers/net/octeon/ethernet-tx.c
index 65c4131..04087aa 100644
--- a/drivers/net/octeon/ethernet-tx.c
+++ b/drivers/net/octeon/ethernet-tx.c
@@ -107,14 +107,18 @@ int cvm_oct_xmit_pow(struct sk_buff *skb, struct net_device *dev)
 
 	/* Fill in some of the work queue fields. We may need to add more
 	   if the software at the other end needs them */
-	work->hw_chksum     = skb->csum;
-	work->len           = skb->len;
-	work->ipprt         = priv->port;
-	work->qos           = priv->port & 0x7;
-	work->grp           = pow_send_group;
-	work->tag_type      = CVMX_HELPER_INPUT_TAG_TYPE;
-	work->tag           = pow_send_group; /* FIXME */
+	work->word0.u64 = 0;
 	work->word2.u64     = 0;    /* Default to zero. Sets of zero later are commented out */
+#if 0
+	work->hw_chksum = skb->csum;
+#endif
+	work->word1.len           = skb->len;
+	cvmx_wqe_set_port(work, priv->ipd_port);
+	cvmx_wqe_set_qos(work, (priv->ipd_port & 0x7));
+	cvmx_wqe_set_grp(work, pow_send_group);
+	work->word1.tag_type      = CVMX_HELPER_INPUT_TAG_TYPE;
+	work->word1.tag           = pow_send_group; /* FIXME */
+
 	work->word2.s.bufs  = 0;
 	work->packet_ptr.u64 = 0;
 
@@ -230,7 +234,8 @@ int cvm_oct_xmit_pow(struct sk_buff *skb, struct net_device *dev)
 	}
 
 	/* Submit the packet to the POW */
-	cvmx_pow_work_submit(work, work->tag, work->tag_type, work->qos, work->grp);
+	/* FIXME: should tab be other than pow_send_group? */
+	cvmx_pow_work_submit(work, pow_send_group, CVMX_HELPER_INPUT_TAG_TYPE, priv->ipd_port & 0x7, pow_send_group);
 	dev->stats.tx_packets++;
 	dev->stats.tx_bytes += skb->len;
 	dev_kfree_skb(skb);
@@ -299,7 +304,7 @@ int cvm_oct_transmit_qos(struct net_device *dev,
 
 	local_irq_save(flags);
 
-	cvmx_pko_send_packet_prepare(priv->port, priv->tx_queue[qos].queue, lock_type);
+	cvmx_pko_send_packet_prepare_pkoid(priv->pko_port, priv->tx_queue[qos].queue, lock_type);
 
 	/* Build the PKO buffer pointer */
 	hw_buffer.u64 = 0;
@@ -313,7 +318,7 @@ int cvm_oct_transmit_qos(struct net_device *dev,
 	pko_command.s.n2 = 1; /* Don't pollute L2 with the outgoing packet */
 	pko_command.s.dontfree = !do_free;
 	pko_command.s.segs = work->word2.s.bufs;
-	pko_command.s.total_bytes = work->len;
+	pko_command.s.total_bytes = work->word1.len;
 
 	/* Check if we can use the hardware checksumming */
 	if (unlikely(work->word2.s.not_IP || work->word2.s.IP_exc))
@@ -322,7 +327,7 @@ int cvm_oct_transmit_qos(struct net_device *dev,
 		pko_command.s.ipoffp1 = sizeof(struct ethhdr) + 1;
 
 	/* Send the packet to the output queue */
-	if (unlikely(cvmx_pko_send_packet_finish(priv->port, priv->tx_queue[qos].queue, pko_command, hw_buffer, lock_type))) {
+	if (unlikely(cvmx_pko_send_packet_finish_pkoid(priv->pko_port, priv->tx_queue[qos].queue, pko_command, hw_buffer, lock_type))) {
 		DEBUGPRINT("%s: Failed to send the packet\n", dev->name);
 		dropped = -1;
 	}
diff --git a/drivers/net/octeon/ethernet-util.h b/drivers/net/octeon/ethernet-util.h
index 5b1a5f6..113c1ce 100644
--- a/drivers/net/octeon/ethernet-util.h
+++ b/drivers/net/octeon/ethernet-util.h
@@ -42,7 +42,7 @@ static inline void *cvm_oct_get_buffer_ptr(union cvmx_buf_ptr packet_ptr)
 	return cvmx_phys_to_ptr(((packet_ptr.s.addr >> 7) - packet_ptr.s.back)
 				<< 7);
 }
-
+#if 0
 /**
  * INTERFACE - convert IPD port to locgical interface
  * @ipd_port: Port to check
@@ -78,3 +78,4 @@ static inline int INDEX(int ipd_port)
 
 	return cvmx_helper_get_interface_index_num(ipd_port);
 }
+#endif
diff --git a/drivers/net/octeon/ethernet-xaui.c b/drivers/net/octeon/ethernet-xaui.c
index 9004469..0cccab1 100644
--- a/drivers/net/octeon/ethernet-xaui.c
+++ b/drivers/net/octeon/ethernet-xaui.c
@@ -40,16 +40,14 @@ int cvm_oct_xaui_open(struct net_device *dev)
 {
 	union cvmx_gmxx_prtx_cfg gmx_cfg;
 	struct octeon_ethernet *priv = netdev_priv(dev);
-	int interface = INTERFACE(priv->port);
-	int index = INDEX(priv->port);
 	cvmx_helper_link_info_t link_info;
 
-	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
+	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface));
 	gmx_cfg.s.en = 1;
-	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmx_cfg.u64);
+	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface), gmx_cfg.u64);
 
 	if (!octeon_is_simulation()) {
-		link_info = cvmx_helper_link_get(priv->port);
+		link_info = cvmx_helper_link_get(priv->ipd_port);
 		if (!link_info.s.link_up)
 			netif_carrier_off(dev);
 	}
@@ -60,12 +58,10 @@ int cvm_oct_xaui_stop(struct net_device *dev)
 {
 	union cvmx_gmxx_prtx_cfg gmx_cfg;
 	struct octeon_ethernet *priv = netdev_priv(dev);
-	int interface = INTERFACE(priv->port);
-	int index = INDEX(priv->port);
 
-	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
+	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface));
 	gmx_cfg.s.en = 0;
-	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmx_cfg.u64);
+	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface), gmx_cfg.u64);
 	return 0;
 }
 
@@ -74,11 +70,11 @@ static void cvm_oct_xaui_poll(struct net_device *dev)
 	struct octeon_ethernet *priv = netdev_priv(dev);
 	cvmx_helper_link_info_t link_info;
 
-	link_info = cvmx_helper_link_get(priv->port);
+	link_info = cvmx_helper_link_get(priv->ipd_port);
 	if (link_info.u64 == priv->link_info)
 		return;
 
-	link_info = cvmx_helper_link_autoconf(priv->port);
+	link_info = cvmx_helper_link_autoconf(priv->ipd_port);
 	priv->link_info = link_info.u64;
 
 	/* Tell the core. */
diff --git a/drivers/net/octeon/ethernet-xmit.c b/drivers/net/octeon/ethernet-xmit.c
index eaad15f..e3218ca 100644
--- a/drivers/net/octeon/ethernet-xmit.c
+++ b/drivers/net/octeon/ethernet-xmit.c
@@ -85,7 +85,6 @@ CVM_OCT_XMIT
 
 	}
 
-
 #ifdef CVM_OCT_LOCKLESS
 	qos = cvmx_get_core_num();
 #else
@@ -133,12 +132,10 @@ CVM_OCT_XMIT
 	 */
 	if ((skb->len < 64) && OCTEON_IS_MODEL(OCTEON_CN3XXX)) {
 		union cvmx_gmxx_prtx_cfg gmx_prt_cfg;
-		int interface = INTERFACE(priv->port);
-		int index = INDEX(priv->port);
 
-		if (interface < 2) {
+		if (priv->interface < 2) {
 			/* We only need to pad packet in half duplex mode */
-			gmx_prt_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
+			gmx_prt_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface));
 			if (gmx_prt_cfg.s.duplex == 0) {
 				int add_bytes = 64 - skb->len;
 				if ((skb_tail_pointer(skb) + add_bytes) <= skb_end_pointer(skb))
@@ -335,7 +332,6 @@ dont_put_skbuff_in_hw:
 
 		pko_command.s.rsp = 1;
 		pko_command.s.wqp = 1;
-		work->hw_chksum = 0;
 		/*
 		 * work->unused will carry the qos for this packet,
 		 * this allows us to find the proper FAU when freeing
@@ -343,26 +339,23 @@ dont_put_skbuff_in_hw:
 		 * replaced in the pool.
 		 */
 		pko_command.s.reg0 = 0;
-		work->unused = (uint8_t)qos;
+		work->word0.u64 = 0;
+		work->word0.raw.unused = (uint8_t)qos;
 
-		work->next_ptr = 0;
-		work->len = 0;
-		work->tag_type = CVMX_POW_TAG_TYPE_NULL;
-		work->tag = 0;
+		work->word1.u64 = 0;
+		work->word1.tag_type = CVMX_POW_TAG_TYPE_NULL;
+		work->word1.tag = 0;
 		work->word2.u64 = 0;
 		work->word2.s.software = 1;
-		work->ipprt = priv->port;
-		/* Use a different queue for return packet than rx so a
-		    high load on one port is spread across multiple
-		    queues */
-		work->qos = (~priv->port) & 7;
-		work->grp = pow_receive_group;
+		cvmx_wqe_set_port(work, priv->ipd_port);
+		cvmx_wqe_set_qos(work, ((~priv->ipd_port) & 7));
+		cvmx_wqe_set_grp(work, pow_receive_group);
 		work->packet_ptr.u64 = (unsigned long)skb;
 	}
 
 	local_irq_save(flags);
 
-	cvmx_pko_send_packet_prepare(priv->port, priv->tx_queue[qos].queue, CVM_OCT_PKO_LOCK_TYPE);
+	cvmx_pko_send_packet_prepare_pkoid(priv->pko_port, priv->tx_queue[qos].queue, CVM_OCT_PKO_LOCK_TYPE);
 
 	/* Send the packet to the output queue */
 	if (queue_type == QUEUE_WQE) {
@@ -376,7 +369,7 @@ dont_put_skbuff_in_hw:
 			*(uint64_t *)work->packet_data = 0;
 			word2 |= 1ull<<40; /* Bit 40 controls timestamps */
 		}
-		if (unlikely(cvmx_pko_send_packet_finish3(priv->port,
+		if (unlikely(cvmx_pko_send_packet_finish3_pkoid(priv->pko_port,
 							  priv->tx_queue[qos].queue, pko_command, hw_buffer,
 							  word2, CVM_OCT_PKO_LOCK_TYPE))) {
 				queue_type = QUEUE_DROP;
@@ -384,7 +377,7 @@ dont_put_skbuff_in_hw:
 				DEBUGPRINT("%s: Failed to send the packet with wqe\n", dev->name);
 		}
 	} else {
-		if (unlikely(cvmx_pko_send_packet_finish(priv->port,
+		if (unlikely(cvmx_pko_send_packet_finish_pkoid(priv->pko_port,
 							 priv->tx_queue[qos].queue,
 							 pko_command, hw_buffer,
 							 CVM_OCT_PKO_LOCK_TYPE))) {
diff --git a/drivers/net/octeon/ethernet.c b/drivers/net/octeon/ethernet.c
index cfbdb36..6db2eef 100644
--- a/drivers/net/octeon/ethernet.c
+++ b/drivers/net/octeon/ethernet.c
@@ -142,10 +142,13 @@ struct workqueue_struct *cvm_oct_poll_queue;
 atomic_t cvm_oct_poll_queue_stopping = ATOMIC_INIT(0);
 
 /*
- * cvm_oct_by_port is an array of every ethernet device owned by this driver indexed by
- * the ipd input port number.
+ * cvm_oct_by_pkind is an array of every ethernet device owned by this
+ * driver indexed by the IPD pkind/port_number.  If an entry is empty
+ * (NULL) it either doesn't exist, or there was a collision.  The two
+ * cases can be distinguished by trying to look up via
+ * cvm_oct_dev_for_port();
  */
-struct octeon_ethernet *cvm_oct_by_port[TOTAL_NUMBER_OF_PORTS] __cacheline_aligned;
+struct octeon_ethernet *cvm_oct_by_pkind[64] __cacheline_aligned;
 
 /*
  * cvm_oct_by_srio_mbox is indexed by the SRIO mailbox.
@@ -288,51 +291,47 @@ static struct net_device_stats *cvm_oct_common_get_stats(struct net_device *dev)
 	u32 current_tx_packets;
 	struct octeon_ethernet *priv = netdev_priv(dev);
 
-	if (priv->port < CVMX_PIP_NUM_INPUT_PORTS) {
-		if (octeon_is_simulation()) {
-			/* The simulator doesn't support statistics */
-			memset(&rx_status, 0, sizeof(rx_status));
-			memset(&tx_status, 0, sizeof(tx_status));
-		} else {
-			cvmx_pip_get_port_status(priv->port, 1, &rx_status);
-
-			spin_lock_irqsave(&cvm_oct_tx_stat_lock, flags);
-			cvmx_pko_get_port_status(priv->port, 0, &tx_status);
-			current_tx_packets = tx_status.packets;
-			current_tx_octets = tx_status.octets;
-			/*
-			 * The tx_packets counter is 32-bits as are
-			 * all these variables.  No truncation
-			 * necessary.
-			 */
-			tx_status.packets = current_tx_packets - priv->last_tx_packets;
-			/*
-			 * The tx_octets counter is only 48-bits, so
-			 * we need to truncate in case there was a
-			 * wrap-around
-			 */
-			tx_status.octets = (current_tx_octets - priv->last_tx_octets) & 0xffffffffffffull;
-			priv->last_tx_packets = current_tx_packets;
-			priv->last_tx_octets = current_tx_octets;
-			spin_unlock_irqrestore(&cvm_oct_tx_stat_lock, flags);
-		}
-
-		dev->stats.rx_packets += rx_status.inb_packets;
-		dev->stats.tx_packets += tx_status.packets;
-		dev->stats.rx_bytes += rx_status.inb_octets;
-		dev->stats.tx_bytes += tx_status.octets;
-		dev->stats.multicast += rx_status.multicast_packets;
-		dev->stats.rx_crc_errors += rx_status.inb_errors;
-		dev->stats.rx_frame_errors += rx_status.fcs_align_err_packets;
-
+	if (octeon_is_simulation()) {
+		/* The simulator doesn't support statistics */
+		memset(&rx_status, 0, sizeof(rx_status));
+		memset(&tx_status, 0, sizeof(tx_status));
+	} else {
+		cvmx_pip_get_port_status(priv->ipd_port, 1, &rx_status);
+
+		spin_lock_irqsave(&cvm_oct_tx_stat_lock, flags);
+		cvmx_pko_get_port_status(priv->ipd_port, 0, &tx_status);
+		current_tx_packets = tx_status.packets;
+		current_tx_octets = tx_status.octets;
+		/*
+		 * The tx_packets counter is 32-bits as are all these
+		 * variables.  No truncation necessary.
+		 */
+		tx_status.packets = current_tx_packets - priv->last_tx_packets;
 		/*
-		 * The drop counter must be incremented atomically
-		 * since the RX tasklet also increments it.
+		 * The tx_octets counter is only 48-bits, so we need
+		 * to truncate in case there was a wrap-around
 		 */
-		atomic64_add(rx_status.dropped_packets,
-			     (atomic64_t *)&dev->stats.rx_dropped);
+		tx_status.octets = (current_tx_octets - priv->last_tx_octets) & 0xffffffffffffull;
+		priv->last_tx_packets = current_tx_packets;
+		priv->last_tx_octets = current_tx_octets;
+		spin_unlock_irqrestore(&cvm_oct_tx_stat_lock, flags);
 	}
 
+	dev->stats.rx_packets += rx_status.inb_packets;
+	dev->stats.tx_packets += tx_status.packets;
+	dev->stats.rx_bytes += rx_status.inb_octets;
+	dev->stats.tx_bytes += tx_status.octets;
+	dev->stats.multicast += rx_status.multicast_packets;
+	dev->stats.rx_crc_errors += rx_status.inb_errors;
+	dev->stats.rx_frame_errors += rx_status.fcs_align_err_packets;
+
+	/*
+	 * The drop counter must be incremented atomically since the
+	 * RX tasklet also increments it.
+	 */
+	atomic64_add(rx_status.dropped_packets,
+		     (atomic64_t *)&dev->stats.rx_dropped);
+
 	return &dev->stats;
 }
 
@@ -346,8 +345,6 @@ static struct net_device_stats *cvm_oct_common_get_stats(struct net_device *dev)
 static int cvm_oct_common_change_mtu(struct net_device *dev, int new_mtu)
 {
 	struct octeon_ethernet *priv = netdev_priv(dev);
-	int interface = INTERFACE(priv->port);
-	int index = INDEX(priv->port);
 #if defined(CONFIG_VLAN_8021Q) || defined(CONFIG_VLAN_8021Q_MODULE)
 	int vlan_bytes = 4;
 #else
@@ -366,8 +363,8 @@ static int cvm_oct_common_change_mtu(struct net_device *dev, int new_mtu)
 	}
 	dev->mtu = new_mtu;
 
-	if ((interface < 2)
-	    && (cvmx_helper_interface_get_mode(interface) !=
+	if ((priv->interface < 2)
+	    && (cvmx_helper_interface_get_mode(priv->interface) !=
 		CVMX_HELPER_INTERFACE_MODE_SPI)) {
 		/* Add ethernet header and FCS, and VLAN if configured. */
 		int max_packet = new_mtu + 14 + 4 + vlan_bytes;
@@ -375,7 +372,7 @@ static int cvm_oct_common_change_mtu(struct net_device *dev, int new_mtu)
 		if (OCTEON_IS_MODEL(OCTEON_CN3XXX)
 		    || OCTEON_IS_MODEL(OCTEON_CN58XX)) {
 			/* Signal errors on packets larger than the MTU */
-			cvmx_write_csr(CVMX_GMXX_RXX_FRM_MAX(index, interface),
+			cvmx_write_csr(CVMX_GMXX_RXX_FRM_MAX(priv->interface_port, priv->interface),
 				       max_packet);
 		} else {
 			/*
@@ -386,14 +383,14 @@ static int cvm_oct_common_change_mtu(struct net_device *dev, int new_mtu)
 			frm_len_chk.u64 = 0;
 			frm_len_chk.s.minlen = 64;
 			frm_len_chk.s.maxlen = max_packet;
-			cvmx_write_csr(CVMX_PIP_FRM_LEN_CHKX(interface), frm_len_chk.u64);
+			cvmx_write_csr(CVMX_PIP_FRM_LEN_CHKX(priv->interface), frm_len_chk.u64);
 		}
 		/*
 		 * Set the hardware to truncate packets larger than
 		 * the MTU. The jabber register must be set to a
 		 * multiple of 8 bytes, so round up.
 		 */
-		cvmx_write_csr(CVMX_GMXX_RXX_JABBER(index, interface),
+		cvmx_write_csr(CVMX_GMXX_RXX_JABBER(priv->interface_port, priv->interface),
 			       (max_packet + 7) & ~7u);
 	}
 	return 0;
@@ -407,11 +404,9 @@ static void cvm_oct_common_set_multicast_list(struct net_device *dev)
 {
 	union cvmx_gmxx_prtx_cfg gmx_cfg;
 	struct octeon_ethernet *priv = netdev_priv(dev);
-	int interface = INTERFACE(priv->port);
-	int index = INDEX(priv->port);
 
-	if ((interface < 2)
-	    && (cvmx_helper_interface_get_mode(interface) !=
+	if ((priv->interface < 2)
+	    && (cvmx_helper_interface_get_mode(priv->interface) !=
 		CVMX_HELPER_INTERFACE_MODE_SPI)) {
 		union cvmx_gmxx_rxx_adr_ctl control;
 		control.u64 = 0;
@@ -436,20 +431,20 @@ static void cvm_oct_common_set_multicast_list(struct net_device *dev)
 			control.s.cam_mode = 1;
 
 		gmx_cfg.u64 =
-		    cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
-		cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface),
+		    cvmx_read_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface));
+		cvmx_write_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface),
 			       gmx_cfg.u64 & ~1ull);
 
-		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CTL(index, interface),
+		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CTL(priv->interface_port, priv->interface),
 			       control.u64);
 		if (dev->flags & IFF_PROMISC)
 			cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM_EN
-				       (index, interface), 0);
+				       (priv->interface_port, priv->interface), 0);
 		else
 			cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM_EN
-				       (index, interface), 1);
+				       (priv->interface_port, priv->interface), 1);
 
-		cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface),
+		cvmx_write_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface),
 			       gmx_cfg.u64);
 	}
 }
@@ -465,13 +460,11 @@ static int cvm_oct_common_set_mac_address(struct net_device *dev, void *addr)
 {
 	struct octeon_ethernet *priv = netdev_priv(dev);
 	union cvmx_gmxx_prtx_cfg gmx_cfg;
-	int interface = INTERFACE(priv->port);
-	int index = INDEX(priv->port);
 
 	memcpy(dev->dev_addr, addr + 2, 6);
 
-	if ((interface < 2)
-	    && (cvmx_helper_interface_get_mode(interface) !=
+	if ((priv->interface < 2)
+	    && (cvmx_helper_interface_get_mode(priv->interface) !=
 		CVMX_HELPER_INTERFACE_MODE_SPI)) {
 		int i;
 		uint8_t *ptr = addr;
@@ -480,19 +473,19 @@ static int cvm_oct_common_set_mac_address(struct net_device *dev, void *addr)
 			mac = (mac << 8) | (uint64_t) (ptr[i + 2]);
 
 		gmx_cfg.u64 =
-		    cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
-		cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface),
+		    cvmx_read_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface));
+		cvmx_write_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface),
 			       gmx_cfg.u64 & ~1ull);
 
-		cvmx_write_csr(CVMX_GMXX_SMACX(index, interface), mac);
-		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM0(index, interface), ptr[2]);
-		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM1(index, interface), ptr[3]);
-		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM2(index, interface), ptr[4]);
-		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM3(index, interface), ptr[5]);
-		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM4(index, interface), ptr[6]);
-		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM5(index, interface), ptr[7]);
+		cvmx_write_csr(CVMX_GMXX_SMACX(priv->interface_port, priv->interface), mac);
+		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM0(priv->interface_port, priv->interface), ptr[2]);
+		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM1(priv->interface_port, priv->interface), ptr[3]);
+		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM2(priv->interface_port, priv->interface), ptr[4]);
+		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM3(priv->interface_port, priv->interface), ptr[5]);
+		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM4(priv->interface_port, priv->interface), ptr[6]);
+		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM5(priv->interface_port, priv->interface), ptr[7]);
 		cvm_oct_common_set_multicast_list(dev);
-		cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmx_cfg.u64);
+		cvmx_write_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface), gmx_cfg.u64);
 	}
 	return 0;
 }
@@ -547,7 +540,7 @@ int cvm_oct_common_init(struct net_device *dev)
 	dev->netdev_ops->ndo_change_mtu(dev, dev->mtu);
 
 	spin_lock_irqsave(&cvm_oct_tx_stat_lock, flags);
-	cvmx_pko_get_port_status(priv->port, 0, &tx_status);
+	cvmx_pko_get_port_status(priv->ipd_port, 0, &tx_status);
 	priv->last_tx_packets = tx_status.packets;
 	priv->last_tx_octets = tx_status.octets;
 	/*
@@ -720,7 +713,6 @@ static const struct net_device_ops cvm_oct_pow_netdev_ops = {
 	.ndo_set_mac_address	= cvm_oct_common_set_mac_address,
 	.ndo_do_ioctl		= cvm_oct_ioctl,
 	.ndo_change_mtu		= cvm_oct_common_change_mtu,
-	.ndo_get_stats		= cvm_oct_common_get_stats,
 #ifdef CONFIG_NET_POLL_CONTROLLER
 	.ndo_poll_controller	= cvm_oct_poll_controller,
 #endif
@@ -745,12 +737,53 @@ static void cvm_oct_override_pko_queue_priority(int pko_port, uint64_t prioritie
 extern void octeon_shutdown_network_hw(void);
 #endif
 
+static struct rb_root cvm_oct_ipd_tree = RB_ROOT;
+
+void cvm_oct_add_ipd_port(struct octeon_ethernet *port)
+{
+	struct rb_node **link = &cvm_oct_ipd_tree.rb_node;
+	struct rb_node *parent = NULL;
+	struct octeon_ethernet *n;
+	int value = port->key;
+
+	while (*link) {
+		parent = *link;
+		n = rb_entry(parent, struct octeon_ethernet, ipd_tree);
+
+		if (value < n->key)
+			link = &(*link)->rb_left;
+		else if (value > n->key)
+			link = &(*link)->rb_right;
+		else
+			BUG();
+	}
+	rb_link_node(&port->ipd_tree, parent, link);
+	rb_insert_color(&port->ipd_tree, &cvm_oct_ipd_tree);
+}
+
+struct octeon_ethernet *cvm_oct_dev_for_port(int port_number)
+{
+	struct rb_node *n = cvm_oct_ipd_tree.rb_node;
+	while (n) {
+		struct octeon_ethernet *s = rb_entry(n, struct octeon_ethernet, ipd_tree);
+
+		if (s->key > port_number)
+			n = n->rb_left;
+		else if (s->key < port_number)
+			n = n->rb_left;
+		else
+			return s;
+	}
+	return NULL;
+}
+
 static int __init cvm_oct_init_module(void)
 {
 	int num_interfaces;
 	int interface;
 	int fau = FAU_NUM_PACKET_BUFFERS_TO_FREE;
 	int qos;
+	int i;
 
 #ifdef CONFIG_KEXEC
 	if (reset_devices)
@@ -764,7 +797,7 @@ static int __init cvm_oct_init_module(void)
 
 	if (OCTEON_IS_MODEL(OCTEON_CN52XX) || OCTEON_IS_MODEL(OCTEON_CN63XX))
 		cvm_oct_mac_addr_offset = 2; /* First two are the mgmt ports. */
-	else if (OCTEON_IS_MODEL(OCTEON_CN56XX))
+	else if (OCTEON_IS_MODEL(OCTEON_CN56XX) || OCTEON_IS_MODEL(OCTEON_CN68XX))
 		cvm_oct_mac_addr_offset = 1; /* First one is the mgmt port. */
 	else
 		cvm_oct_mac_addr_offset = 0;
@@ -785,14 +818,18 @@ static int __init cvm_oct_init_module(void)
 	for (interface = 0; interface < num_interfaces; interface++) {
 		int num_ports = cvmx_helper_ports_on_interface(interface);
 		int port;
-
-		for (port = cvmx_helper_get_ipd_port(interface, 0);
-		     port < cvmx_helper_get_ipd_port(interface, num_ports);
-		     port++) {
+		for (port = 0; port < num_ports; port++) {
 			union cvmx_pip_prt_tagx pip_prt_tagx;
-			pip_prt_tagx.u64 = cvmx_read_csr(CVMX_PIP_PRT_TAGX(port));
+			int pkind;
+
+			if (octeon_has_feature(OCTEON_FEATURE_PKND))
+				pkind = cvmx_helper_get_pknd(interface, port);
+			else
+				pkind = cvmx_helper_get_ipd_port(interface, port);
+
+			pip_prt_tagx.u64 = cvmx_read_csr(CVMX_PIP_PRT_TAGX(pkind));
 			pip_prt_tagx.s.grp = pow_receive_group;
-			cvmx_write_csr(CVMX_PIP_PRT_TAGX(port), pip_prt_tagx.u64);
+			cvmx_write_csr(CVMX_PIP_PRT_TAGX(pkind), pip_prt_tagx.u64);
 		}
 	}
 
@@ -813,10 +850,14 @@ static int __init cvm_oct_init_module(void)
 			struct octeon_ethernet *priv = netdev_priv(dev);
 
 			priv->netdev = dev;
+			priv->interface = -1;
+			priv->interface_port = -1;
 			dev->netdev_ops = &cvm_oct_pow_netdev_ops;
 			priv->num_tx_queues = 0;
 			priv->imode = CVMX_HELPER_INTERFACE_MODE_DISABLED;
-			priv->port = CVMX_PIP_NUM_INPUT_PORTS;
+			priv->ipd_port = CVMX_PIP_NUM_INPUT_PORTS;
+			priv->pko_port = -1;
+			priv->ipd_pkind = -1;
 			strcpy(dev->name, "pow%d");
 
 			if (register_netdev(dev) < 0) {
@@ -824,7 +865,6 @@ static int __init cvm_oct_init_module(void)
 				kfree(dev);
 			} else {
 				list_add_tail(&priv->list, &cvm_oct_list);
-				cvm_oct_by_port[CVMX_PIP_NUM_INPUT_PORTS] = priv;
 				pr_info("%s: POW send group %d, receive group %d\n",
 					dev->name, pow_send_group,
 					pow_receive_group);
@@ -837,19 +877,18 @@ static int __init cvm_oct_init_module(void)
 	for (interface = 0; interface < num_interfaces; interface++) {
 		cvmx_helper_interface_mode_t imode = cvmx_helper_interface_get_mode(interface);
 		int num_ports = cvmx_helper_ports_on_interface(interface);
-		int port;
+		int interface_port;
 
 		if (imode == CVMX_HELPER_INTERFACE_MODE_SRIO)
 			num_ports = 4;
 
-		for (port = cvmx_helper_get_ipd_port(interface, 0);
-		     port < cvmx_helper_get_ipd_port(interface, num_ports);
-		     port++) {
+		for (interface_port = 0; num_ports > 0;
+		     interface_port++, num_ports--) {
 			struct octeon_ethernet *priv;
 			int base_queue;
 			struct net_device *dev = alloc_etherdev(sizeof(struct octeon_ethernet));
 			if (!dev) {
-				pr_err("Failed to allocate ethernet device for port %d\n", port);
+				pr_err("Failed to allocate ethernet device for port %d:%d\n", interface, interface_port);
 				continue;
 			}
 
@@ -858,14 +897,17 @@ static int __init cvm_oct_init_module(void)
 
 			/* Initialize the device private structure. */
 			priv = netdev_priv(dev);
+			RB_CLEAR_NODE(&priv->ipd_tree);
 			priv->netdev = dev;
+			priv->interface = interface;
+			priv->interface_port = interface_port;
 
 			INIT_DELAYED_WORK(&priv->port_periodic_work,
 					  cvm_oct_periodic_worker);
 			priv->imode = imode;
 
 			if (imode == CVMX_HELPER_INTERFACE_MODE_SRIO) {
-				int mbox = port - cvmx_helper_get_ipd_port(interface, 0);
+                                int mbox = cvmx_helper_get_ipd_port(interface, interface_port) - cvmx_helper_get_ipd_port(interface, 0);
 				cvmx_srio_tx_message_header_t tx_header;
 				tx_header.u64 = 0;
 				tx_header.s.tt = 1;
@@ -874,15 +916,23 @@ static int __init cvm_oct_init_module(void)
 				tx_header.s.lns = 1;
 				tx_header.s.intr = 1;
 				priv->srio_tx_header = tx_header.u64;
-				priv->port = cvmx_helper_get_ipd_port(interface, mbox >> 1);
-				base_queue = cvmx_pko_get_base_queue(priv->port) + (mbox & 1);
+				priv->ipd_port = cvmx_helper_get_ipd_port(interface, mbox >> 1);
+				priv->pko_port = priv->ipd_port;
+				priv->key = priv->ipd_port + (0x10000 * mbox);
+				base_queue = cvmx_pko_get_base_queue(priv->ipd_port) + (mbox & 1);
 				priv->num_tx_queues = 1;
 				cvm_oct_by_srio_mbox[interface - 4][mbox] = priv;
 			} else {
-				priv->port = port;
-				base_queue = cvmx_pko_get_base_queue(priv->port);
-				priv->num_tx_queues = cvmx_pko_get_num_queues(priv->port);
+				priv->ipd_port = cvmx_helper_get_ipd_port(interface, interface_port);
+				priv->key = priv->ipd_port;
+				priv->pko_port = cvmx_helper_get_pko_port(interface, interface_port);
+				base_queue = cvmx_pko_get_base_queue(priv->ipd_port);
+				priv->num_tx_queues = cvmx_pko_get_num_queues(priv->ipd_port);
 			}
+			if (octeon_has_feature(OCTEON_FEATURE_PKND))
+				priv->ipd_pkind = cvmx_helper_get_pknd(interface, interface_port);
+			else
+				priv->ipd_pkind = priv->ipd_port;
 
 			for (qos = 0; qos < priv->num_tx_queues; qos++) {
 				priv->tx_queue[qos].queue = base_queue + qos;
@@ -965,11 +1015,16 @@ static int __init cvm_oct_init_module(void)
 				kfree(dev);
 			} else if (register_netdev(dev) < 0) {
 				pr_err("Failed to register ethernet device for interface %d, port %d\n",
-					 interface, priv->port);
+					 interface, priv->ipd_port);
 				kfree(dev);
 			} else {
 				list_add_tail(&priv->list, &cvm_oct_list);
-				cvm_oct_by_port[priv->port] = priv;
+				if (cvm_oct_by_pkind[priv->ipd_pkind] == NULL)
+					cvm_oct_by_pkind[priv->ipd_pkind] = priv;
+				else
+					cvm_oct_by_pkind[priv->ipd_pkind] = (void *)-1L;
+
+				cvm_oct_add_ipd_port(priv);
 				/*
 				 * Each transmit queue will need its
 				 * own MAX_OUT_QUEUE_DEPTH worth of
@@ -983,7 +1038,12 @@ static int __init cvm_oct_init_module(void)
 		}
 	}
 
-	cvm_oct_rx_initialize();
+	/* Set collision locations to NULL */
+	for (i = 0; i < ARRAY_SIZE(cvm_oct_by_pkind); i++)
+		if (cvm_oct_by_pkind[i] == (void *)-1L)
+			cvm_oct_by_pkind[i] = NULL;
+
+	cvm_oct_rx_initialize(num_packet_buffers + num_devices_extra_wqe * PER_DEVICE_EXTRA_WQE);
 
 	queue_delayed_work(cvm_oct_poll_queue, &cvm_oct_rx_refill_work, HZ);
 
@@ -995,22 +1055,19 @@ static void __exit cvm_oct_cleanup_module(void)
 	struct octeon_ethernet *priv;
 	struct octeon_ethernet *tmp;
 
-	/* Disable POW interrupt */
-	cvmx_write_csr(CVMX_POW_WQ_INT_THRX(pow_receive_group), 0);
-
 	cvmx_ipd_disable();
 
 	atomic_inc_return(&cvm_oct_poll_queue_stopping);
 	cancel_delayed_work_sync(&cvm_oct_rx_refill_work);
 
-	cvm_oct_rx_shutdown();
+	cvm_oct_rx_shutdown(num_packet_buffers + num_devices_extra_wqe * PER_DEVICE_EXTRA_WQE);
 
 	/* Free the ethernet devices */
 	list_for_each_entry_safe_reverse(priv, tmp, &cvm_oct_list, list) {
 		list_del(&priv->list);
 		cancel_delayed_work_sync(&priv->port_periodic_work);
 		unregister_netdev(priv->netdev);
-		cvm_oct_by_port[priv->port] = NULL;
+
 		kfree(priv->netdev);
 	}
 
@@ -1027,6 +1084,15 @@ static void __exit cvm_oct_cleanup_module(void)
 
 	if (CVMX_FPA_OUTPUT_BUFFER_POOL != CVMX_FPA_PACKET_POOL)
 		cvm_oct_mem_empty_fpa(CVMX_FPA_OUTPUT_BUFFER_POOL, CVMX_FPA_OUTPUT_BUFFER_POOL_SIZE, cvm_oct_num_output_buffers);
+
+	/*
+	 * Disable FPA, all buffers are free, not done by helper
+	 * shutdown.  But don't do it on 68XX as this causes SSO to
+	 * malfunction on subsequent initialization.
+	 */
+	if (!OCTEON_IS_MODEL(OCTEON_CN68XX))
+		cvmx_fpa_disable();
+
 }
 
 MODULE_LICENSE("GPL");
diff --git a/drivers/net/octeon/octeon-ethernet.h b/drivers/net/octeon/octeon-ethernet.h
index 34deff9..a02d57f 100644
--- a/drivers/net/octeon/octeon-ethernet.h
+++ b/drivers/net/octeon/octeon-ethernet.h
@@ -31,6 +31,8 @@
 #ifndef OCTEON_ETHERNET_H
 #define OCTEON_ETHERNET_H
 
+#include <linux/rbtree.h>
+
 #include <asm/octeon/cvmx-helper.h>
 
 #include <asm/octeon/octeon-ethernet-user.h>
@@ -40,8 +42,13 @@
  * driver state stored in netdev_priv(dev).
  */
 struct octeon_ethernet {
-	/* PKO hardware output port */
-	int port;
+	struct rb_node ipd_tree;
+	int key;
+	int ipd_port;
+	int pko_port;
+	int ipd_pkind;
+	int interface;
+	int interface_port;
 	/* My netdev. */
 	struct net_device *netdev;
 	/* My location in the cvm_oct_list */
@@ -90,6 +97,8 @@ struct octeon_ethernet {
 	u32 last_tx_packets;
 };
 
+struct octeon_ethernet *cvm_oct_dev_for_port(int);
+
 int cvm_oct_free_work(void *work_queue_entry);
 
 extern int cvm_oct_rgmii_init(struct net_device *dev);
@@ -131,7 +140,9 @@ extern int pow_send_group;
 extern int pow_receive_group;
 extern char pow_send_list[];
 extern struct list_head cvm_oct_list;
-extern struct octeon_ethernet *cvm_oct_by_port[];
+
+extern struct octeon_ethernet *cvm_oct_by_pkind[];
+
 extern struct octeon_ethernet *cvm_oct_by_srio_mbox[4][4];
 
 extern struct workqueue_struct *cvm_oct_poll_queue;
diff --git a/drivers/net/octeon/octeon-pow-ethernet.c b/drivers/net/octeon/octeon-pow-ethernet.c
index 359d12f..97363ea 100644
--- a/drivers/net/octeon/octeon-pow-ethernet.c
+++ b/drivers/net/octeon/octeon-pow-ethernet.c
@@ -258,15 +258,19 @@ static int packet_transmit(struct sk_buff *skb, struct net_device *dev)
 
 		/* Fill in some of the work queue fields. We may need to add
 		   more if the software at the other end needs them */
-		work->hw_chksum = skb->csum;
-		work->len = skb->len;
-		work->ipprt = VIRTUAL_PORT;
-		work->qos = 0;
-		work->grp = send_group;
-		work->tag_type = 2;
-		work->tag = 0;
+		work->word0.u64 = 0;
 		work->word2.u64 = 0;	/* Default to zero. Sets of zero later
 					   are commented out */
+#if 0
+		work->hw_chksum = skb->csum;
+#endif
+		work->word1.len = skb->len;
+		cvmx_wqe_set_port(work, VIRTUAL_PORT);
+		cvmx_wqe_set_qos(work, 0);
+		cvmx_wqe_set_grp(work, send_group);
+		work->word1.tag_type = 2;
+		work->word1.tag = 0;
+		
 		work->word2.s.bufs = 1;
 		work->packet_ptr.u64 = 0;
 		work->packet_ptr.s.addr = virt_to_phys(copy_location);
@@ -344,9 +348,14 @@ static int packet_transmit(struct sk_buff *skb, struct net_device *dev)
 			       sizeof(work->packet_data));
 		}
 
-		/* Submit the packet to the POW */
-		pow_work_submit(work, work->tag, work->tag_type, work->qos,
-				work->grp);
+		/*
+		 * Submit the packet to the POW
+		 * tag: 0
+		 * tag_type: 2
+		 * qos: 0
+		 * grp: send_group
+		 */
+		pow_work_submit(work, 0, 2, 0, send_group);
 		work = NULL;
 		packet_buffer = NULL;
 	}
@@ -403,7 +412,7 @@ static irqreturn_t do_interrupt(int cpl, void *dev_id)
 			break;
 
 		/* Silently drop packets that have the wrong input port */
-		if (work->ipprt != VIRTUAL_PORT) {
+		if (cvmx_wqe_get_port(work) != VIRTUAL_PORT) {
 			free_work(work);
 			continue;
 		}
@@ -425,7 +434,7 @@ static irqreturn_t do_interrupt(int cpl, void *dev_id)
 		}
 
 		/* We have to copy the packet. First allocate an skbuff for it */
-		skb = dev_alloc_skb(work->len);
+		skb = dev_alloc_skb(work->word1.len);
 		if (!skb) {
 			DEBUGPRINT
 				("%s: Failed to allocate skbuff, packet dropped\n",
@@ -441,12 +450,12 @@ static irqreturn_t do_interrupt(int cpl, void *dev_id)
 			DEBUGPRINT
 				("%s: Received a work with work->word2.s.bufs=0, untested\n",
 				 dev->name);
-			memcpy(skb_put(skb, work->len), work->packet_data,
-			       work->len);
+			memcpy(skb_put(skb, work->word1.len), work->packet_data,
+			       work->word1.len);
 		} else {
 			int segments = work->word2.s.bufs;
 			cvmx_buf_ptr_t segment_ptr = work->packet_ptr;
-			int len = work->len;
+			int len = work->word1.len;
 			while (segments--) {
 				cvmx_buf_ptr_t next_ptr =
 					*(cvmx_buf_ptr_t *)
-- 
1.7.0

