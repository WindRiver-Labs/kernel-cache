From 861fac864504cee74b99351828a6b74373846938 Mon Sep 17 00:00:00 2001
From: David Daney <ddaney@caviumnetworks.com>
Date: Wed, 2 Feb 2011 11:45:55 -0800
Subject: [PATCH 074/238] netdev: octeon_ethernet: Use kmem_caches for working memory.

Source: Cavium SDK 2.1.0-407

Each WQE was being padded out by 256 bytes for alignment and and
pointer to the original block.  Using a kmem_cache eliminates the need
for any padding, and saves several MB of wasted memory.

Signed-off-by: David Daney <ddaney@caviumnetworks.com>
Integrated-by: Phil Staub <Phil.Staub@windriver.com>
---
 drivers/net/octeon/ethernet-mem.c |  110 +++++++++++++++++++++++++++++--------
 drivers/net/octeon/ethernet-mem.h |    3 +
 drivers/net/octeon/ethernet.c     |   24 ++++++--
 3 files changed, 108 insertions(+), 29 deletions(-)

diff --git a/drivers/net/octeon/ethernet-mem.c b/drivers/net/octeon/ethernet-mem.c
index 71cf15d..a5576fa 100644
--- a/drivers/net/octeon/ethernet-mem.c
+++ b/drivers/net/octeon/ethernet-mem.c
@@ -25,6 +25,7 @@
  * Contact Cavium Networks for more information
 **********************************************************************/
 #include <linux/kernel.h>
+#include <linux/slab.h>
 #include <linux/netdevice.h>
 
 #include <asm/octeon/octeon.h>
@@ -36,6 +37,10 @@
 #include <asm/octeon/cvmx-bootmem.h>
 #endif
 
+static struct kmem_cache *cvm_oct_kmem_128;
+static struct kmem_cache *cvm_oct_kmem_1024;
+
+
 /**
  * cvm_oct_fill_hw_skbuff - fill the supplied hardware pool with skbuffs
  * @pool:     Pool to allocate an skbuff for
@@ -100,7 +105,6 @@ static void cvm_oct_free_hw_skbuff(int pool, int size, int elements)
 static int cvm_oct_fill_hw_memory(int pool, int size, int elements)
 {
 	char *memory;
-	char *fpa;
 	int freed = elements;
 
 	if (USE_32BIT_SHARED) {
@@ -121,26 +125,27 @@ static int cvm_oct_fill_hw_memory(int pool, int size, int elements)
 		}
 		return elements - freed;
 	}
+
 	while (freed) {
-		/*
-		 * FPA memory must be 128 byte aligned.  Since we are
-		 * aligning we need to save the original pointer so we
-		 * can feed it to kfree when the memory is returned to
-		 * the kernel.
-		 *
-		 * We allocate an extra 256 bytes to allow for
-		 * alignment and space for the original pointer saved
-		 * just before the block.
-		 */
-		memory = kmalloc(size + 256, GFP_ATOMIC);
-		if (unlikely(memory == NULL)) {
-			pr_warning("Unable to allocate %u bytes for FPA pool %d\n",
-				   elements * size, pool);
-			break;
+		if (size == 128) {
+			memory = kmem_cache_alloc(cvm_oct_kmem_128, GFP_KERNEL);
+			if (unlikely(memory == NULL)) {
+				pr_warning("Unable to allocate %u bytes for FPA pool %d\n",
+					elements * size, pool);
+				break;
+			}
+		} else if (size == 1024) {
+			memory = kmem_cache_alloc(cvm_oct_kmem_1024,
+						  GFP_KERNEL);
+			if (unlikely(memory == NULL)) {
+				pr_warning("Unable to allocate %u bytes for FPA pool %d\n",
+					elements * size, pool);
+				break;
+			}
+		} else {
+			BUG();
 		}
-		fpa = (char *)(((unsigned long)memory + 256) & ~0x7fUL);
-		*((char **)fpa - 1) = memory;
-		cvmx_fpa_free(fpa, pool, 0);
+		cvmx_fpa_free(memory, pool, 0);
 		freed--;
 	}
 	return elements - freed;
@@ -154,19 +159,21 @@ static int cvm_oct_fill_hw_memory(int pool, int size, int elements)
  */
 static void cvm_oct_free_hw_memory(int pool, int size, int elements)
 {
-	char *memory;
 	char *fpa;
 	if (USE_32BIT_SHARED) {
 		printk(KERN_WARNING "Warning: 32 shared memory is not freeable\n");
 		return;
 	}
-
 	do {
 		fpa = cvmx_fpa_alloc(pool);
 		if (fpa) {
 			elements--;
-			memory = *((char **)fpa - 1);
-			kfree(memory);
+			if (size == 128)
+				kmem_cache_free(cvm_oct_kmem_128, fpa);
+			else if (size == 1024)
+				kmem_cache_free(cvm_oct_kmem_1024, fpa);
+			else
+				BUG();
 		}
 	} while (fpa);
 
@@ -181,6 +188,7 @@ static void cvm_oct_free_hw_memory(int pool, int size, int elements)
 int cvm_oct_mem_fill_fpa(int pool, int size, int elements)
 {
 	int freed;
+
 	if (USE_SKBUFFS_IN_HW && pool == CVMX_FPA_PACKET_POOL)
 		freed = cvm_oct_fill_hw_skbuff(pool, size, elements);
 	else
@@ -195,3 +203,59 @@ void cvm_oct_mem_empty_fpa(int pool, int size, int elements)
 	else
 		cvm_oct_free_hw_memory(pool, size, elements);
 }
+
+int __init cvm_oct_mem_init(void)
+{
+	int r = 0;
+
+	if (USE_32BIT_SHARED)
+		goto out;
+
+	cvm_oct_kmem_128 = kmem_cache_create("octeon_ethernet-128",
+					     128, 128, 0, NULL);
+	if (!cvm_oct_kmem_128) {
+		r = -ENOMEM;
+		goto out;
+	}
+
+	cvm_oct_kmem_1024 = kmem_cache_create("octeon_ethernet-1024",
+					      1024, 128, 0, NULL);
+	if (!cvm_oct_kmem_1024) {
+		r = -ENOMEM;
+		goto err;
+	}
+	goto out;
+
+err:
+	kmem_cache_destroy(cvm_oct_kmem_128);
+	cvm_oct_kmem_128 = NULL;
+out:
+	return r;
+}
+
+void cvm_oct_mem_uninit(void)
+{
+	if (USE_32BIT_SHARED)
+		return;
+
+	if (cvm_oct_kmem_128)
+		kmem_cache_destroy(cvm_oct_kmem_128);
+	if (cvm_oct_kmem_1024)
+		kmem_cache_destroy(cvm_oct_kmem_1024);
+
+	cvm_oct_kmem_128 = NULL;
+	cvm_oct_kmem_1024 = NULL;
+}
+EXPORT_SYMBOL(cvm_oct_mem_uninit);
+
+void cvm_oct_mem_cleanup(void)
+{
+	if (USE_32BIT_SHARED)
+		return;
+
+	if (cvm_oct_kmem_128)
+		kmem_cache_shrink(cvm_oct_kmem_128);
+	if (cvm_oct_kmem_1024)
+		kmem_cache_shrink(cvm_oct_kmem_1024);
+}
+EXPORT_SYMBOL(cvm_oct_mem_cleanup);
diff --git a/drivers/net/octeon/ethernet-mem.h b/drivers/net/octeon/ethernet-mem.h
index 89cdda4..c3533ce 100644
--- a/drivers/net/octeon/ethernet-mem.h
+++ b/drivers/net/octeon/ethernet-mem.h
@@ -27,5 +27,8 @@
 
 int cvm_oct_mem_fill_fpa(int pool, int size, int elements);
 void cvm_oct_mem_empty_fpa(int pool, int size, int elements);
+int cvm_oct_mem_init(void);
+void cvm_oct_mem_cleanup(void);
+void cvm_oct_mem_uninit(void);
 void cvm_oct_mem_cleanup(void);
 void cvm_oct_mem_uninit(void);
diff --git a/drivers/net/octeon/ethernet.c b/drivers/net/octeon/ethernet.c
index dd5ec45..8515061 100644
--- a/drivers/net/octeon/ethernet.c
+++ b/drivers/net/octeon/ethernet.c
@@ -738,6 +738,7 @@ static int __init cvm_oct_init_module(void)
 	int fau = FAU_NUM_PACKET_BUFFERS_TO_FREE;
 	int qos;
 	int i;
+	int rv = 0;
 
 #ifdef CONFIG_KEXEC
 	if (reset_devices)
@@ -757,10 +758,15 @@ static int __init cvm_oct_init_module(void)
 	else
 		cvm_oct_mac_addr_offset = 0;
 
+	rv = cvm_oct_mem_init();
+	if (rv)
+		goto err;
+
 	cvm_oct_poll_queue = create_singlethread_workqueue("octeon-ethernet");
 	if (cvm_oct_poll_queue == NULL) {
 		pr_err("octeon-ethernet: Cannot create workqueue");
-		return -ENOMEM;
+		rv = -ENOMEM;
+		goto err_cleanup;
 	}
 
 	cvm_oct_proc_initialize();
@@ -975,7 +981,10 @@ static int __init cvm_oct_init_module(void)
 
 	queue_delayed_work(cvm_oct_poll_queue, &cvm_oct_rx_refill_work, HZ);
 
-	return 0;
+	return rv;
+err_cleanup:
+	cvm_oct_mem_cleanup();
+	return rv;
 }
 
 static void __exit cvm_oct_cleanup_module(void)
@@ -1011,7 +1020,7 @@ static void __exit cvm_oct_cleanup_module(void)
 	atomic_inc_return(&cvm_oct_poll_queue_stopping);
 	cancel_delayed_work_sync(&cvm_oct_rx_refill_work);
 
-	cvm_oct_rx_shutdown(num_packet_buffers + num_devices_extra_wqe * PER_DEVICE_EXTRA_WQE);
+	cvm_oct_rx_shutdown0();
 
 	/* unregister the ethernet devices */
 	list_for_each_entry(priv, &cvm_oct_list, list) {
@@ -1024,15 +1033,17 @@ static void __exit cvm_oct_cleanup_module(void)
 	/* Free the ethernet devices */
 	list_for_each_entry_safe_reverse(priv, tmp, &cvm_oct_list, list) {
 		list_del(&priv->list);
-		kfree(priv->netdev);
+		free_netdev(priv->netdev);
 	}
 
+	cvmx_helper_shutdown_packet_io_global();
+
+	cvm_oct_rx_shutdown1();
+
 	destroy_workqueue(cvm_oct_poll_queue);
 
 	cvm_oct_proc_shutdown();
 
-	cvmx_helper_shutdown_packet_io_global();
-
 	/* Free the HW pools */
 	cvm_oct_mem_empty_fpa(CVMX_FPA_PACKET_POOL, CVMX_FPA_PACKET_POOL_SIZE, num_packet_buffers);
 	cvm_oct_mem_empty_fpa(CVMX_FPA_WQE_POOL, CVMX_FPA_WQE_POOL_SIZE, num_packet_buffers);
@@ -1049,6 +1060,7 @@ static void __exit cvm_oct_cleanup_module(void)
 	if (!OCTEON_IS_MODEL(OCTEON_CN68XX))
 		cvmx_fpa_disable();
 
+	cvm_oct_mem_cleanup();
 }
 
 MODULE_LICENSE("GPL");
-- 
1.7.0

