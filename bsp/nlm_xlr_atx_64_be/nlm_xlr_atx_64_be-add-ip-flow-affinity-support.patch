From 1a632deda7ad5d9ee21abbcf2aebd134be9229ad Mon Sep 17 00:00:00 2001
From: Jack Tan <jack.tan@windriver.com>
Date: Fri, 21 May 2010 13:07:28 +0800
Subject: [PATCH 33/47] nlm_xlr_atx_64_be: add ip flow affinity support

Extracted from RMI SDK 1.7.0.

Experimental feature of GMAC driver guranteeing that IP flows are
processed on logical CPUs corresponding to buckets assigned by packet
classifier engine.

E.g. for XLR core N, packets arriving to buckets 0 & 4 are processed by
thread 0, packets arriving to buckets 1 & 5 are processed by thread 1
and so on..

Such feature might be important for applications which require IP flows
be seen on one logcal CPUs. Use of this feature involves performance
cost.

Signed-off-by: Jack Tan <jack.tan@windriver.com>
---
 arch/mips/include/asm/rmi/interrupt.h |    7 +++
 arch/mips/rmi/Kconfig                 |   14 +++++
 arch/mips/rmi/phoenix/irq.c           |   19 ++++++-
 arch/mips/rmi/phoenix/smp.c           |   22 ++++++++-
 drivers/net/phoenix_mac.c             |   92 +++++++++++++++++++++++++++++++++
 5 files changed, 150 insertions(+), 4 deletions(-)

diff --git a/arch/mips/include/asm/rmi/interrupt.h b/arch/mips/include/asm/rmi/interrupt.h
index 5e44062..4fa6fcd 100644
--- a/arch/mips/include/asm/rmi/interrupt.h
+++ b/arch/mips/include/asm/rmi/interrupt.h
@@ -47,6 +47,13 @@
 #define IRQ_IPI_CRF_MGMT_IPI	45 /* */
 #define IRQ_IPI_CRF_EVENTQ_IPI 46 
 
+#ifdef CONFIG_PHOENIX_IP_FLOW_AFFINITY
+
+#define IRQ_IPI_NETRX           49
+#define SMP_NETRX_IPI           32
+
+#endif /* CONFIG_PHOENIX_IP_FLOW_AFFINITY */
+
 
 #define SMP_CALL_KGDB_HOOK 	8
 #define SMP_OPROFILE_IPI        16
diff --git a/arch/mips/rmi/Kconfig b/arch/mips/rmi/Kconfig
index 1bc5fe8..ae5a16f 100644
--- a/arch/mips/rmi/Kconfig
+++ b/arch/mips/rmi/Kconfig
@@ -33,6 +33,20 @@ config PHOENIX_HW_BUFFER_MGMT
 	  If in doubt, say N.
 
 
+config PHOENIX_IP_FLOW_AFFINITY
+	bool "Enable support for IP flow affinity"
+	depends on RMI_PHOENIX
+	default n
+	help
+	  Experimental feature of GMAC driver guranteeing that IP flows are processed 
+	  on logical CPUs corresponding to buckets assigned by packet classifier engine.
+	  E.g. for XLR core #X, packets arriving to buckets 0 & 4 are processed by thread 0,
+	  packets arriving to buckets 1 & 5 are processed by thread 1 and so on..
+	  Such feature might be important for applications which require IP flows 
+	  be seen on one logcal CPUs. Use of this feature involves performance cost.
+
+	  If in doubt, say N.
+
 config RMI_PHOENIX_LOAD_ADDRESS
 	hex "RMI Linux kernel start address"
 	depends on RMI_PHOENIX
diff --git a/arch/mips/rmi/phoenix/irq.c b/arch/mips/rmi/phoenix/irq.c
index fdf6e8a..e521cdc 100644
--- a/arch/mips/rmi/phoenix/irq.c
+++ b/arch/mips/rmi/phoenix/irq.c
@@ -297,8 +297,16 @@ void __init init_phoenix_irqs(void)
 	irq_desc[IRQ_IPI_SMP_RESCHEDULE].action = &phnx_rsvd_action;
 
 	phnx_irq_mask |=
-	    ((1ULL << IRQ_IPI_SMP_FUNCTION) | (1ULL << IRQ_IPI_SMP_RESCHEDULE));
-#endif				/* CONFIG_SMP */
+	    	((1ULL << IRQ_IPI_SMP_FUNCTION) | (1ULL << IRQ_IPI_SMP_RESCHEDULE));
+
+#ifdef CONFIG_PHOENIX_IP_FLOW_AFFINITY
+	/* PR: New IPI added here for netrx balancing */
+	irq_desc[IRQ_IPI_NETRX].chip = &phnx_rsvd_pic;
+	irq_desc[IRQ_IPI_NETRX].action = &phnx_rsvd_action;
+	phnx_irq_mask |= (1ULL<<IRQ_IPI_NETRX);
+#endif
+
+#endif	/* CONFIG_SMP */
 
 	/* msgring interrupt */
 	irq_desc[IRQ_MSGRING].chip = &phnx_rsvd_pic;
@@ -336,7 +344,13 @@ extern void phoenix_oprofile_int_handler(int irq, void *dev_id, struct pt_regs *
 void do_phnx_IRQ(unsigned int irq, struct pt_regs *regs)
 {
 #ifdef CONFIG_SMP
+
+#ifdef CONFIG_PHOENIX_IP_FLOW_AFFINITY
+	if (irq == IRQ_IPI_SMP_FUNCTION || irq == IRQ_IPI_SMP_RESCHEDULE 
+			|| irq== IRQ_IPI_NETRX) {
+#else
 	if (irq == IRQ_IPI_SMP_FUNCTION || irq == IRQ_IPI_SMP_RESCHEDULE) {
+#endif
 		phoenix_ipi_handler(irq, regs);
 		return;
 	}
@@ -357,7 +371,6 @@ void __cpuinit rmi_smp_irq_init(void)
 {
 	/* set interrupt mask for non-zero cpus */
 	write_64bit_cp0_eimr(phnx_irq_mask | (1 << IRQ_TIMER));
-
 }
 
 void __init arch_init_irq(void)
diff --git a/arch/mips/rmi/phoenix/smp.c b/arch/mips/rmi/phoenix/smp.c
index da95ce0..7793b5c 100644
--- a/arch/mips/rmi/phoenix/smp.c
+++ b/arch/mips/rmi/phoenix/smp.c
@@ -59,6 +59,14 @@ void core_send_ipi(int logical_cpu, unsigned int action)
 		pr_debug("Sending OPROFILE IPI 0x%08x to tid %d pid %d\n", ipi, tid, pid);
 	}
 #endif
+
+#ifdef CONFIG_PHOENIX_IP_FLOW_AFFINITY
+	else if (action & SMP_NETRX_IPI) {
+		ipi = (tid << 16) | (pid << 20) | IRQ_IPI_NETRX;
+		pr_debug("%s: Sending NETRX IPI 0x%08x to tid %d pid %d\n", __func__, ipi, tid, pid);
+	}
+#endif /* CONFIG_PHOENIX_IP_FLOW_AFFINITY */
+
 	else
 		BUG();
 
@@ -74,7 +82,19 @@ void phoenix_ipi_handler(int irq, struct pt_regs *regs)
 		pr_debug("[%s]: cpu_%d processing ipi_%d\n", __func__,
 			 smp_processor_id(), irq);
 		smp_call_function_interrupt();
-	} else {
+	}
+
+#ifdef CONFIG_PHOENIX_IP_FLOW_AFFINITY
+	else if (irq == IRQ_IPI_NETRX) {
+		extern void skb_transfer_finish(void);
+		irq_enter();
+		skb_transfer_finish();
+		/* run soft IRQ at the end */
+		irq_exit();
+	}
+#endif /* CONFIG_PHOENIX_IP_FLOW_AFFINITY */
+
+	else {
 		pr_debug("[%s]: cpu_%d processing ipi_%d\n", __func__, smp_processor_id(), irq);
 		/* Announce that we are for reschduling */
 		set_need_resched();
diff --git a/drivers/net/phoenix_mac.c b/drivers/net/phoenix_mac.c
index 7b995d5..7f321f4 100644
--- a/drivers/net/phoenix_mac.c
+++ b/drivers/net/phoenix_mac.c
@@ -326,6 +326,30 @@ static inline void prefetch_local(const void *addr)
 
 
 
+#ifdef CONFIG_PHOENIX_IP_FLOW_AFFINITY
+/* skb transfer statistics */
+unsigned long long skb_transfer_stat[NR_CPUS][NR_CPUS];
+void skb_transfer_finish(void);
+static void skb_transfer(int bucket, struct sk_buff *skb);
+
+
+/* skb transfer queues, one per CPU */
+static struct sk_buff_head cpu_skb_tqueue[NR_CPUS];
+
+static void
+cpu_tx_queue_init(void)
+{
+	int i;
+
+	for (i = 0; i < NR_CPUS; i++)
+	{
+		skb_queue_head_init(&(cpu_skb_tqueue[i]));
+	}
+}
+
+#endif /* CONFIG_PHOENIX_IP_FLOW_AFFINITY */
+
+
 /* This message ring interrupt type, can be adjusted by NAPI setup callback */
 extern int msgring_int_type;
 extern struct user_mac_data *user_mac;
@@ -2579,8 +2603,12 @@ do { \
 				 (read_c0_count() - msgrng_msg_cycles));
 
 
+#ifdef CONFIG_PHOENIX_IP_FLOW_AFFINITY
+		skb_transfer(bucket, skb);
+#else
 		skb->dev->last_rx = jiffies;
 		netif_rx(skb);
+#endif /* CONFIG_PHOENIX_IP_FLOW_AFFINITY */
 
 	} else {
 		printk("[%s]: unrecognized ctrl=%d!\n", __FUNCTION__,
@@ -2690,6 +2718,66 @@ void rmi_phnx_station_unowned_msgring_handler(int bucket, int size,
 }
 
 
+#ifdef CONFIG_PHOENIX_IP_FLOW_AFFINITY
+extern void core_send_ipi(int cpu, unsigned int action);
+
+static void
+skb_transfer(int bucket, struct sk_buff *skb)
+{
+	u_long my_cpu_no, my_thread_no, my_core_no, target_cpu_no, target_thread_no;
+
+
+	target_thread_no = bucket & 0x3;
+	my_cpu_no = smp_processor_id();
+	my_thread_no = phoenix_thr_id();
+	my_core_no = phoenix_cpu_id();
+	target_cpu_no = cpu_number_map((my_core_no << 2) | target_thread_no);
+
+  /*
+   * Version with NETRX IPI aggregation
+  */
+	if (target_thread_no != my_thread_no && cpu_isset(target_cpu_no, cpu_online_map))
+	{
+		unsigned long flags;
+		struct sk_buff_head *ptqueue = &cpu_skb_tqueue[target_cpu_no];
+
+		spin_lock_irqsave(&ptqueue->lock, flags);
+		if (ptqueue->qlen)
+		{
+			__skb_queue_tail(ptqueue, skb);
+		}
+		else{
+			__skb_queue_tail(ptqueue, skb);
+			core_send_ipi(target_cpu_no, SMP_NETRX_IPI);
+		}
+		spin_unlock_irqrestore(&ptqueue->lock, flags);
+
+		skb_transfer_stat[my_cpu_no][target_cpu_no]++;
+	}
+	else{
+		skb_transfer_stat[my_cpu_no][my_cpu_no]++;
+
+		skb_queue_tail(&cpu_skb_tqueue[my_cpu_no], skb);
+		skb_transfer_finish();
+	}
+}
+
+
+/* second part of SKB transfer logic, called from IRQ_IPI_NETRX handler */
+void
+skb_transfer_finish(void)
+{
+	struct sk_buff *skb;
+	u_long cpu = smp_processor_id();
+
+	while ((skb = skb_dequeue(&cpu_skb_tqueue[cpu])) != NULL)
+	{
+		skb->dev->last_rx = jiffies;
+		netif_rx(skb);
+	}
+}
+
+#endif /* CONFIG_PHOENIX_IP_FLOW_AFFINITY */
 
 
 /**********************************************************************
@@ -4309,6 +4397,10 @@ int rmi_phnx_mac_init_module(void)
 		priv->phy.serdes_addr =
 		    mac_addr_to_ptr(port_cfg->serdes_addr);
 
+#ifdef CONFIG_PHOENIX_IP_FLOW_AFFINITY
+		/* initialize cpu skb queues */
+		cpu_tx_queue_init();
+#endif /* CONFIG_PHOENIX_IP_FLOW_AFFINITY */
 
 		phnx_mac_get_hwaddr(dev);
 
-- 
1.7.0.4

