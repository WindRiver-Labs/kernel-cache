From 553b54db4511e73f8983ad3f07b7b78343edb867 Mon Sep 17 00:00:00 2001
From: Jack Tan <jack.tan@windriver.com>
Date: Fri, 11 Jun 2010 16:07:28 +0800
Subject: [PATCH 16/47] nlm_xlr_atx_64_be: add hugetlb support

Extracted from RMI SDK 1.7.0.

Introduce hugetlb support to the RMI (CPU_PHOENIX) class of boards. This
is done via the introduction of <foo>_pte and <blah>_pte that manage the
page tables in a manner specific to these boards.

Signed-off-by: Jack Tan <jack.tan@windriver.com>
[ For MIPS, HUGETLB_PAGE_SIZE(1 << HPAGE_SHIFT) depends on PAGE_SIZE,
the default HUGETLB_PAGE_SIZE is 2MB, please refer to
arch/mips/include/asm/page.h ]
Integrated-by: Wu Zhangjin <zhangjin.wu@windriver.com>
---
 arch/mips/Kconfig               |    1 +
 arch/mips/include/asm/hugetlb.h |    8 +--
 arch/mips/mm/hugetlbpage.c      |  130 +++++++++++++++++++++++++++++++++++++++
 3 files changed, 133 insertions(+), 6 deletions(-)

diff --git a/arch/mips/Kconfig b/arch/mips/Kconfig
index 67f5c2c..5980fb7 100644
--- a/arch/mips/Kconfig
+++ b/arch/mips/Kconfig
@@ -1337,6 +1337,7 @@ config CPU_PHOENIX
 	select CPU_SUPPORTS_32BIT_KERNEL
 	select CPU_SUPPORTS_64BIT_KERNEL
 	select CPU_SUPPORTS_HIGHMEM
+	select CPU_SUPPORTS_HUGEPAGES
 	select CPU_HAS_LLSC
 	select WEAK_ORDERING
 	select WEAK_REORDERING_BEYOND_LLSC
diff --git a/arch/mips/include/asm/hugetlb.h b/arch/mips/include/asm/hugetlb.h
index f5e8560..399281c 100644
--- a/arch/mips/include/asm/hugetlb.h
+++ b/arch/mips/include/asm/hugetlb.h
@@ -11,6 +11,8 @@
 
 #include <asm/page.h>
 
+extern void set_huge_pte_at(struct mm_struct *mm, unsigned long addr,
+				pte_t *ptep, pte_t entry);
 
 static inline int is_hugepage_only_range(struct mm_struct *mm,
 					 unsigned long addr,
@@ -50,12 +52,6 @@ static inline void hugetlb_free_pgd_range(struct mmu_gather *tlb,
 	free_pgd_range(tlb, addr, end, floor, ceiling);
 }
 
-static inline void set_huge_pte_at(struct mm_struct *mm, unsigned long addr,
-				   pte_t *ptep, pte_t pte)
-{
-	set_pte_at(mm, addr, ptep, pte);
-}
-
 static inline pte_t huge_ptep_get_and_clear(struct mm_struct *mm,
 					    unsigned long addr, pte_t *ptep)
 {
diff --git a/arch/mips/mm/hugetlbpage.c b/arch/mips/mm/hugetlbpage.c
index a7fee0d..1173826 100644
--- a/arch/mips/mm/hugetlbpage.c
+++ b/arch/mips/mm/hugetlbpage.c
@@ -22,6 +22,135 @@
 #include <asm/tlb.h>
 #include <asm/tlbflush.h>
 
+#ifdef CONFIG_CPU_PHOENIX
+static pte_t *get_pte_offset(struct mm_struct *mm, unsigned long addr)
+{
+	pgd_t *pgd;
+	pud_t *pud;
+	pmd_t *pmd;
+	pte_t *pte = NULL;
+
+	pgd = pgd_offset(mm, addr);
+	if (!pgd_none(*pgd)) {
+		pud = pud_offset(pgd, addr);
+		if (!pud_none(*pud)) {
+			pmd = pmd_offset(pud, addr);
+			if (!pmd_none(*pmd))
+				pte = pte_offset_map(pmd, addr);
+		}
+	}
+	return pte;
+}
+
+void set_huge_pte_at(struct mm_struct *mm, unsigned long addr, pte_t *ptep,
+		pte_t entry)
+{
+	int i;
+	uint32_t htlb_entries = 1 << HUGETLB_PAGE_ORDER;
+	struct vm_area_struct *vma = NULL;
+	uint32_t len = htlb_entries*PAGE_SIZE;
+	pte_t first_entry = entry;
+	unsigned long orig_addr ;
+
+	/*
+	 * We must align the address, because our caller will run
+	 * set_huge_pte_at() on whatever we return, which writes out
+	 * all of the sub-ptes for the hugepage range.  So we have
+	 * to give it the first such sub-pte.
+	 */
+	addr &= HPAGE_MASK;
+	orig_addr = addr;
+	/*Fill each entry with its own physical address map*/
+	for (i = 0; i < htlb_entries; i++) {
+		/*Get the pte offset, we may cross the pte table*/
+		ptep = get_pte_offset(mm, addr);
+
+		set_pte_at(mm, addr, ptep, entry);
+		addr += PAGE_SIZE;
+		pte_val(entry) += PAGE_SIZE;
+	}
+	addr = orig_addr;
+	vma = find_vma(mm, addr);
+
+	/*addr must belong to this vma*/
+	if (!((addr >= vma->vm_start) && ((addr+len) <= vma->vm_end)))
+		panic("set_huge_pte_at: No vma found for hugtlb page!!");
+
+	/*Don't know below loop is required or not.*/
+	pte_val(entry) = pte_val(first_entry);
+	for (i = 0 ; i < htlb_entries; i++) {
+		__update_cache(vma, addr, entry);
+		addr += PAGE_SIZE;
+		pte_val(entry) += PAGE_SIZE;
+	}
+}
+
+static pte_t *huge_pte_alloc_single(struct mm_struct *mm, unsigned long addr)
+{
+	pgd_t *pgd;
+	pud_t *pud;
+	pmd_t *pmd;
+	pte_t *pte = NULL;
+
+	pgd = pgd_offset(mm, addr);
+	pud = pud_alloc(mm, pgd, addr);
+	if (pud) {
+		pmd = pmd_alloc(mm, pud, addr);
+		if (pmd)
+			pte = pte_alloc_map(mm, pmd, addr);
+	}
+	return pte;
+}
+
+pte_t *huge_pte_alloc(struct mm_struct *mm, unsigned long addr, unsigned long sz)
+{
+	pte_t *first_pte = NULL;
+	pte_t *pte = NULL;
+	int i = 0;
+	uint32_t htlb_entries = 1 << HUGETLB_PAGE_ORDER;
+	uint32_t total_pte_required = htlb_entries / PTRS_PER_PTE;
+
+	/*increment number of pte required if htlb_entries
+	  is not multiple of PTRS_PER_PTE
+	*/
+	if (htlb_entries % PTRS_PER_PTE)
+		total_pte_required++;
+
+	addr &= HPAGE_MASK;
+	for (i = 0; i < total_pte_required; i++) {
+		pte = huge_pte_alloc_single(mm, addr);
+		if (!pte)
+			return NULL;
+		if (!first_pte)
+			first_pte = pte;
+		addr = addr + (PTRS_PER_PTE*PAGE_SIZE);
+	}
+	return first_pte;
+}
+
+pte_t *huge_pte_offset(struct mm_struct *mm, unsigned long addr)
+{
+	pgd_t *pgd;
+	pud_t *pud;
+	pmd_t *pmd;
+	pte_t *pte = NULL;
+
+	addr &= HPAGE_MASK;
+
+	pgd = pgd_offset(mm, addr);
+	if (!pgd_none(*pgd)) {
+		pud = pud_offset(pgd, addr);
+		if (!pud_none(*pud)) {
+			pmd = pmd_offset(pud, addr);
+			if (!pmd_none(*pmd))
+				pte = pte_offset_map(pmd, addr);
+		}
+	}
+	return pte;
+}
+
+#else
+
 pte_t *huge_pte_alloc(struct mm_struct *mm, unsigned long addr,
 		      unsigned long sz)
 {
@@ -51,6 +180,7 @@ pte_t *huge_pte_offset(struct mm_struct *mm, unsigned long addr)
 	}
 	return (pte_t *) pmd;
 }
+#endif
 
 int huge_pmd_unshare(struct mm_struct *mm, unsigned long *addr, pte_t *ptep)
 {
-- 
1.7.0.4

