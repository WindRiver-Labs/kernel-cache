From df3c107f10f1c20424de42373ee334edf40bece8 Mon Sep 17 00:00:00 2001
From: Jack Tan <jack.tan@windriver.com>
Date: Wed, 19 May 2010 23:57:28 +0800
Subject: [PATCH 07/47] nlm_xlr_atx_64_be: add cache ops support

Extracted from RMI SDK 1.7.0.

XLR is a multi-core multi-thread processor. It has hardware machanism to
make each core's cache coherent. In spite of it's a MIPS64
implementation it has a diffrent cache style against R4K. So it need own
cache flush operations.

Signed-off-by: Jack Tan <jack.tan@windriver.com>
[ Move the __uncached_access() to a standalone file and fix/clean up the
c-phoenix.c ]
Signed-off-by: Wu Zhangjin <zhangjin.wu@windriver.com>
---
 arch/mips/include/asm/cache.h  |    2 +
 arch/mips/mm/c-phoenix.c       |  380 ++++++++++++++++++++++++++++++++++++++++
 arch/mips/mm/cache.c           |    5 +
 arch/mips/rmi/phoenix/Makefile |    2 +-
 arch/mips/rmi/phoenix/memory.c |   35 ++++
 5 files changed, 423 insertions(+), 1 deletions(-)
 create mode 100644 arch/mips/mm/c-phoenix.c
 create mode 100644 arch/mips/rmi/phoenix/memory.c

diff --git a/arch/mips/include/asm/cache.h b/arch/mips/include/asm/cache.h
index 37f175c..9f32a4e 100644
--- a/arch/mips/include/asm/cache.h
+++ b/arch/mips/include/asm/cache.h
@@ -17,4 +17,6 @@
 #define SMP_CACHE_SHIFT		L1_CACHE_SHIFT
 #define SMP_CACHE_BYTES		L1_CACHE_BYTES
 
+#define ARCH_KMALLOC_MINALIGN   8
+
 #endif /* _ASM_CACHE_H */
diff --git a/arch/mips/mm/c-phoenix.c b/arch/mips/mm/c-phoenix.c
new file mode 100644
index 0000000..07b5d2b
--- /dev/null
+++ b/arch/mips/mm/c-phoenix.c
@@ -0,0 +1,380 @@
+/*
+ * Copyright 2003-2006 RMI Corporation, Inc.(RMI).
+ *
+ * Copyright (C) 2010 Wind River Systems,
+ *
+ * This is a derived work from software originally provided by the
+ * external entity identified below. The licensing terms and warranties
+ * specified in the header of the original work apply to this derived
+ * work.
+ *
+ * Copyright (C) 1996 David S. Miller (dm@engr.sgi.com) Copyright (C)
+ * 1997, 2001 Ralf Baechle (ralf@gnu.org)
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or (at
+ * your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA
+ * 02111-1307, USA.
+ */
+
+#include <linux/init.h>
+#include <linux/smp.h>
+#include <linux/kallsyms.h>
+#include <linux/mm.h>
+#include <linux/module.h>
+
+#include <asm/asm.h>
+#include <asm/bootinfo.h>
+#include <asm/cpu.h>
+#include <asm/cacheops.h>
+#include <asm/uaccess.h>
+#include <asm/system.h>
+#include <asm/mmu_context.h>
+#include <asm/page.h>
+
+static unsigned long icache_lsize __read_mostly;
+static unsigned long icache_lmask __read_mostly;
+static unsigned long icache_lines __read_mostly;
+
+static unsigned long dcache_lsize __read_mostly;
+static unsigned long dcache_lmask __read_mostly;
+static unsigned long dcache_lines __read_mostly;
+
+#define cacheop(op, base) do {			\
+	__asm__ __volatile__ (			\
+		".set push\n"			\
+		".set mips4\n"			\
+		"cache %0, 0(%1)\n"		\
+		".set pop\n"			\
+		: : "i"(op), "r"(base));	\
+} while (0)
+
+#define cacheop_extable(op, base) do {		\
+	__asm__ __volatile__(			\
+		"    .set push\n"		\
+		"    .set noreorder\n"		\
+		"    .set mips4\n"		\
+		"1:  cache %0, 0(%1)\n"		\
+		"2:  .set pop\n"		\
+		"    .section __ex_table,\"a\"\n"\
+		    STR(PTR)"\t1b, 2b\n\t"	\
+		"     .previous\n"		\
+		: : "i" (op), "r" (base));	\
+} while (0)
+
+static inline void sync_istream(void)
+{
+	__asm__ __volatile__ (
+		".set push\n"
+		".set noreorder\n"
+		"nop\n"
+		"nop\n"
+		"nop\n"
+		"nop\n"
+		"nop\n"
+		"nop\n"
+		"nop\n"
+		"nop\n"
+		"nop\n"
+		"nop\n"
+		".set pop\n"
+		: : : "$8"
+		);
+}
+
+static inline void cacheop_hazard(void)
+{
+	__asm__ __volatile__ (
+		".set push\n"
+		".set noreorder\n"
+		" nop;nop;nop;nop\n"
+		" nop;nop;nop;nop\n"
+		".set pop\n"
+		);
+}
+
+static inline void cacheop_sync_istream(void)
+{
+	cacheop_hazard();
+	sync_istream();
+}
+
+/*
+ * These routines support Generic Kernel cache flush requirements
+ */
+void phoenix_flush_dcache_page(struct page *page)
+{
+	ClearPageDcacheDirty(page);
+}
+
+void phoenix_flush_data_cache_page(unsigned long addr)
+{
+	struct page *page = (struct page *)virt_to_page((void *)addr);
+	ClearPageDcacheDirty(page);
+}
+
+static void phoenix_local_flush_icache_range(unsigned long start,
+		unsigned long end)
+{
+	unsigned long addr = start & icache_lmask;
+	unsigned long aend = (end - 1) & icache_lmask;
+
+	while (1) {
+		cacheop_extable(Hit_Invalidate_I, addr);
+		if (addr == aend)
+			break;
+		addr += icache_lsize;
+	}
+
+	cacheop_sync_istream();
+}
+
+struct flush_icache_range_args {
+	unsigned long start;
+	unsigned long end;
+};
+
+static void phoenix_flush_icache_range_ipi(void *info)
+{
+	struct flush_icache_range_args *args = info;
+
+	phoenix_local_flush_icache_range(args->start, args->end);
+}
+
+void phoenix_flush_icache_range(unsigned long start, unsigned long end)
+{
+	struct flush_icache_range_args args;
+
+	if ((end - start) > PAGE_SIZE)
+		pr_debug("flushing more than page size of icache addresses starting @ %lx\n",
+			start);
+
+	args.start = start;
+	args.end = end;
+
+	/*
+	 * TODO: don't even send ipi to non-zero thread ids This may
+	 * require some changes to smp_call_function interface, for now
+	 * just avoid redundant cache ops
+	 */
+	on_each_cpu(phoenix_flush_icache_range_ipi, &args, 1);
+}
+
+static void phoenix_flush_cache_sigtramp_ipi(void *info)
+{
+	unsigned long addr = (unsigned long)info & icache_lmask;
+
+	cacheop_extable(Hit_Invalidate_I, addr);
+	cacheop_sync_istream();
+}
+
+static void phoenix_flush_cache_sigtramp(unsigned long addr)
+{
+	on_each_cpu(phoenix_flush_cache_sigtramp_ipi, (void *)addr, 1);
+}
+
+/*
+ * These routines support MIPS specific cache flush requirements. These
+ * are called only during bootup or special system calls
+ */
+static void phoenix_local_flush_icache(void)
+{
+	unsigned long base = CKSEG0;
+	int i;
+
+	/* Index Invalidate all the lines and the ways */
+	for (i = 0; i < icache_lines; i++) {
+		cacheop(Index_Invalidate_I, base);
+		base += icache_lsize;
+	}
+
+	cacheop_sync_istream();
+}
+
+static void phoenix_local_flush_dcache(void)
+{
+	unsigned long base = CKSEG0;
+	int i;
+
+	/* Index Invalidate all the lines and the ways */
+	for (i = 0; i < dcache_lines; i++) {
+		cacheop(Index_Writeback_Inv_D, base);
+		base += dcache_lsize;
+	}
+
+	cacheop_hazard();
+}
+
+static void phoenix_flush_l1_caches_ipi(void *info)
+{
+	phoenix_local_flush_dcache();
+	phoenix_local_flush_icache();
+}
+
+static void phoenix_flush_l1_caches(void)
+{
+	on_each_cpu(phoenix_flush_l1_caches_ipi, (void *)NULL, 1);
+}
+
+static void phoenix_noflush(void) { /* do nothing */ }
+
+static __init void probe_l1_cache(void)
+{
+	struct cpuinfo_mips *c = &current_cpu_data;
+	unsigned long icache_size, dcache_size;
+	unsigned int config1 = read_c0_config1();
+	unsigned int lsize;
+
+	c->icache.linesz = 0;
+	lsize = ((config1 >> 19) & 7);
+	if (lsize)
+		c->icache.linesz = 2 << lsize;
+	c->icache.sets = 64 << ((config1 >> 22) & 7);
+	c->icache.ways = 1 + ((config1 >> 16) & 7);
+	icache_size =
+		c->icache.sets * c->icache.ways * c->icache.linesz;
+	c->icache.waybit = ffs(icache_size / c->icache.ways) - 1;
+
+	c->dcache.linesz = 0;
+	lsize = ((config1 >> 10) & 7);
+	if (lsize)
+		c->dcache.linesz = 2 << lsize;
+	c->dcache.sets = 64 << ((config1 >> 13) & 7);
+	c->dcache.ways = 1 + ((config1 >> 7) & 7);
+	c->dcache.flags = 0;
+	dcache_size =
+		c->dcache.sets * c->dcache.ways * c->dcache.linesz;
+	c->dcache.waybit = ffs(dcache_size/c->dcache.ways) - 1;
+
+	if (smp_processor_id() == 0) {
+		pr_info("Primary instruction cache %lukB, %d-way, linesize %d bytes.\n",
+			icache_size >> 10, c->icache.ways, c->icache.linesz);
+		pr_info("Primary data cache %lukB %d-way, linesize %d bytes.\n",
+			dcache_size >> 10, c->dcache.ways, c->dcache.linesz);
+	}
+}
+
+static inline void install_cerr_handler(void)
+{
+	extern char except_vec2_generic;
+
+	memcpy((void *)(ebase + 0x100), &except_vec2_generic, 0x80);
+}
+
+static void update_kseg0_coherency(void)
+{
+	int attr = read_c0_config() & CONF_CM_CMASK;
+
+	if (attr != 0x3) {
+		phoenix_local_flush_dcache();
+		phoenix_local_flush_icache();
+
+		change_c0_config(CONF_CM_CMASK, 0x3);
+
+		sync_istream();
+	}
+	_page_cachable_default = (0x3 << _CACHE_SHIFT);
+}
+
+void phoenix_cache_init(void)
+{
+	/* update cpu_data */
+	probe_l1_cache();
+
+	if (smp_processor_id()) {
+		phoenix_local_flush_icache();
+		update_kseg0_coherency();
+		return;
+	}
+
+	/* These values are assumed to be the same for all cores */
+	icache_lines =
+		current_cpu_data.icache.ways * current_cpu_data.icache.sets;
+	icache_lsize = cpu_icache_line_size();
+	icache_lmask = ~(icache_lsize - 1);
+
+	dcache_lines =
+		current_cpu_data.dcache.ways * current_cpu_data.dcache.sets;
+	dcache_lsize = cpu_dcache_line_size();
+	dcache_lmask = ~(dcache_lsize - 1);
+
+	/*
+	 * When does this function get called? Looks like MIPS has some syscalls
+	 * to flush the caches.
+	 */
+	__flush_cache_all = phoenix_flush_l1_caches;
+
+	/*
+	 * flush_cache_all: makes all kernel data coherent.
+	 * This gets called just before changing or removing
+	 * a mapping in the page-table-mapped kernel segment (kmap).
+	 * Physical Cache -> do nothing
+	 */
+	flush_cache_all = phoenix_noflush;
+
+	/*
+	 * flush_icache_range: makes the range of addresses coherent
+	 * w.r.t I-cache and D-cache
+	 *
+	 * This gets called after the instructions are written to memory
+	 * All addresses are valid kernel or mapped user-space virtual
+	 * addresses
+	 */
+	flush_icache_range = phoenix_flush_icache_range;
+
+	/*
+	 * flush_cache_{mm, range, page}: make these memory locations,
+	 * that may have been written by a user process, coherent
+	 *
+	 * These get called when virtual->physical translation of a user
+	 * address space is about to be changed. These are closely
+	 * related to TLB coherency (flush_tlb_{mm, range, page})
+	 */
+	flush_cache_mm = (void (*)(struct mm_struct *))phoenix_noflush;
+	flush_cache_range = (void *)phoenix_noflush;
+	flush_cache_page = (void *)phoenix_flush_l1_caches;
+
+	/*
+	 * flush_icache_page: flush_dcache_page + update_mmu_cache takes
+	 * care of this
+	 */
+	flush_data_cache_page = (void *)phoenix_flush_data_cache_page;
+
+	/*
+	 * flush_cache_sigtramp: flush the single I-cache line with the
+	 * proper fixup code
+	 */
+
+	flush_cache_sigtramp = phoenix_flush_cache_sigtramp;
+
+	/*
+	 * flush_icache_all: This should get called only for Virtuall
+	 * Tagged I-Caches
+	 */
+	flush_icache_all = (void *)phoenix_noflush;
+
+	local_flush_icache_range = phoenix_local_flush_icache_range;
+	local_flush_data_cache_page = (void *)phoenix_noflush;
+
+	__flush_cache_vmap = (void *)phoenix_noflush;
+	__flush_cache_vunmap = (void *)phoenix_noflush;
+
+	install_cerr_handler();
+
+	build_clear_page();
+	build_copy_page();
+
+	phoenix_local_flush_icache();
+
+	update_kseg0_coherency();
+}
diff --git a/arch/mips/mm/cache.c b/arch/mips/mm/cache.c
index 12af739..e7bd0ff 100644
--- a/arch/mips/mm/cache.c
+++ b/arch/mips/mm/cache.c
@@ -209,6 +209,11 @@ void __cpuinit cpu_cache_init(void)
 
 		octeon_cache_init();
 	}
+	if (cpu_has_phoenix_cache) {
+		extern void __weak phoenix_cache_init(void);
+
+		phoenix_cache_init();
+	}
 
 	setup_protection_map();
 }
diff --git a/arch/mips/rmi/phoenix/Makefile b/arch/mips/rmi/phoenix/Makefile
index 8a3b61e..6479364 100644
--- a/arch/mips/rmi/phoenix/Makefile
+++ b/arch/mips/rmi/phoenix/Makefile
@@ -1,4 +1,4 @@
-obj-y := irq.o cpu.o time.o
+obj-y := irq.o cpu.o time.o memory.o
 obj-$(CONFIG_SMP) += smp.o
 
 EXTRA_AFLAGS := $(CFLAGS)
diff --git a/arch/mips/rmi/phoenix/memory.c b/arch/mips/rmi/phoenix/memory.c
new file mode 100644
index 0000000..09ff294
--- /dev/null
+++ b/arch/mips/rmi/phoenix/memory.c
@@ -0,0 +1,35 @@
+/*
+ * This program is free software; you can redistribute  it and/or modify it
+ * under  the terms of  the GNU General  Public License as published by the
+ * Free Software Foundation;  either version 2 of the  License, or (at your
+ * option) any later version.
+ */
+
+#include <linux/fs.h>
+#include <linux/fcntl.h>
+#include <linux/mm.h>
+
+#include <asm/bootinfo.h>
+
+#include <asm/rmi/io.h>
+
+void __init plat_mem_setup(void)
+{
+}
+
+int __uncached_access(struct file *file, unsigned long addr)
+{
+	if (file->f_flags & O_DSYNC)
+		return 1;
+
+	/*
+	 * Return 1 if uncached(IO) and return 0(MEM) if cached access
+	 * is required.
+	 *
+	 * TODO: We need a actual "physical memory map" to implement
+	 * this fully.  For now, treat anything in 256MB to 512MB as
+	 * uncached access
+	 */
+
+	return ((addr >= RMI_UNCACHED_START) && (addr < RMI_UNCACHED_END));
+}
-- 
1.7.0.4

