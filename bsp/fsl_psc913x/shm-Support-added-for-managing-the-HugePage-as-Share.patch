From e24f27915fb2918b2376422ff69483f249058824 Mon Sep 17 00:00:00 2001
From: Jiang Bin <bin.jiang@windriver.com>
Date: Tue, 15 Nov 2011 11:01:35 +0800
Subject: [PATCH 78/92] shm: Support added for managing the HugePage as Shared Memory Resource for user space applications.

Extracted from 913x_WUSDK_REL_0.9.tar.gz vendor drop.

User space application can dynamically allocate shared memory areas
backed up by a HugePage.

Signed-off-by: Naveen Burmi <NaveenBurmi@freescale.com>
Integrated-by: Jiang Bin <bin.jiang@windriver.com>
---
 drivers/misc/Kconfig   |   12 +
 drivers/misc/Makefile  |    1 +
 drivers/misc/fsl_shm.c |  718 ++++++++++++++++++++++++++++++++++++++++++++++++
 drivers/misc/fsl_shm.h |   55 ++++
 4 files changed, 786 insertions(+), 0 deletions(-)
 create mode 100644 drivers/misc/fsl_shm.c
 create mode 100644 drivers/misc/fsl_shm.h

diff --git a/drivers/misc/Kconfig b/drivers/misc/Kconfig
index a878ba8..ebd4f71 100644
--- a/drivers/misc/Kconfig
+++ b/drivers/misc/Kconfig
@@ -433,4 +433,16 @@ config PSC9131_HET_MGR
 	tristate "Freescale PSC9131 Heterogeneous Manager driver"
 	depends on PSC9131_RDB
 
+config FSL_HUGEPAGE_SHM
+	tristate "Management of Huge Page as Shared Memory"
+	depends on HUGETLBFS && (PSC9131_RDB)
+	default y
+	help
+	  Selecting this will enable user space applications to do
+	  dynamic allocation and de-allocation of memory area backed up
+	  by a Huge Page.
+
+	  To compile this as a module, choose M here: the module
+	  will be called fsl_shm.
+
 endif # MISC_DEVICES
diff --git a/drivers/misc/Makefile b/drivers/misc/Makefile
index 1faad49..fad5e13 100644
--- a/drivers/misc/Makefile
+++ b/drivers/misc/Makefile
@@ -30,6 +30,7 @@ obj-$(CONFIG_C2PORT)		+= c2port/
 obj-$(CONFIG_IWMC3200TOP)      += iwmc3200top/
 obj-$(CONFIG_PSC9131_HET_MGR)	+= fsl_psc913x_hetmgr.o
 obj-$(CONFIG_PSC9131_HET_IPC)	+= fsl_psc913x_ipc.o
+obj-$(CONFIG_FSL_HUGEPAGE_SHM)	+= fsl_shm.o
 obj-y				+= eeprom/
 obj-y				+= cb710/
 obj-$(CONFIG_VMWARE_BALLOON)	+= vmware_balloon.o
diff --git a/drivers/misc/fsl_shm.c b/drivers/misc/fsl_shm.c
new file mode 100644
index 0000000..b88a821
--- /dev/null
+++ b/drivers/misc/fsl_shm.c
@@ -0,0 +1,718 @@
+/*
+ * Copyright (C) 2011 Freescale Semiconductor, Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.  IN
+ * NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+ * TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+ * LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+ * NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ *	Author: Naveen Burmi <naveenburmi@freescale.com>
+ */
+
+#include <linux/init.h>
+#include <linux/mm.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/fs.h>
+#include <asm/uaccess.h>
+#include <linux/slab.h>
+#include "fsl_shm.h"
+
+#define MIN_FREE_SIZE_LIST		0x1000
+#define NUMBER_OF_LISTS			8
+#define MAX_FREE_SIZE_LIST		(MIN_FREE_SIZE_LIST * NUMBER_OF_LISTS)
+#define MASK				(~(MIN_FREE_SIZE_LIST - 1))
+#define LIST_INDEX_FROM_SIZE(size)	((size & MASK)/MIN_FREE_SIZE_LIST)
+#define SPLIT_THREASHOLD		0
+#define OFFSET_OF(node)			(offsetof(node, g_entry))
+
+#define MIN_ADDRESS_LIST		0x1000
+#define MAX_ADDRESS_LIST		(MIN_ADDRESS_LIST * NUMBER_OF_LISTS)
+#define ADDRESS_MASK			(~(MIN_FREE_SIZE_LIST - 1))
+#define LIST_INDEX_FROM_ADDRESS(addr)	((addr & ADDRESS_MASK)/MIN_ADDRESS_LIST)
+
+#define HUGE_PAGE_256M		0x10000000
+
+static int fsl_shm_major;
+static spinlock_t mm_lock;
+static void *psa;
+static unsigned long shm_size;
+
+struct global_node {
+	struct global_node *g_next;
+	struct global_node *g_prev;
+};
+
+typedef struct node {
+	unsigned char tag;
+	unsigned long address;
+	unsigned int size;
+	struct node *next;
+	struct node *prev;
+	struct global_node g_entry;
+} node;
+
+typedef node free_node;
+typedef node allocated_node;
+
+struct global_node global_list_head;
+
+free_node free_area_lists[NUMBER_OF_LISTS + 1];
+allocated_node allocated_area_lists[NUMBER_OF_LISTS + 1];
+
+static allocated_node *get_allocated_list_from_address(unsigned long address)
+{
+	int index = -1;
+
+	if (address > MAX_ADDRESS_LIST)
+		return &allocated_area_lists[NUMBER_OF_LISTS];
+
+	index = LIST_INDEX_FROM_ADDRESS(address);
+
+	return &allocated_area_lists[index];
+}
+
+static free_node *get_list(unsigned int size)
+{
+	int index = -1;
+
+	if (size > MAX_FREE_SIZE_LIST)
+		return &free_area_lists[NUMBER_OF_LISTS];
+
+	index = LIST_INDEX_FROM_SIZE(size);
+
+	return &free_area_lists[index];
+}
+
+static free_node *get_free_area_list(unsigned int size)
+{
+	int index = -1;
+
+	if (size > MAX_FREE_SIZE_LIST)
+		return &free_area_lists[NUMBER_OF_LISTS];
+
+	index = LIST_INDEX_FROM_SIZE(size);
+	do {
+		if (free_area_lists[index].size > size) {
+			/* chech whether this list contains an entry
+			 * which is bigger then OR equal to the required
+			 * size. This can be done by verifying the size of
+			 * entrty present at tail.
+			 */
+			if (free_area_lists[index].prev->size >= size)
+				return &free_area_lists[index];
+		}
+
+		index++;
+	} while (index < (NUMBER_OF_LISTS + 1));
+
+	return NULL;
+}
+
+/************** list add/remove operations **********/
+
+static void list_add_allocated_area(allocated_node *entry)
+{
+	allocated_node *head;
+
+	head = get_allocated_list_from_address(entry->address);
+	head->size = head->size + entry->size;
+
+	if (head->prev->size <= entry->size) {
+		/* add allocated area at tail */
+		entry->next = head;
+		entry->prev = head->prev;
+		head->prev->next = entry;
+		head->prev = entry;
+	} else if (head->next->size > entry->size) {
+		/* add allocated area at head */
+		entry->next = head->next;
+		entry->prev = head;
+		head->next->prev = entry;
+		head->next = entry;
+	} else {
+		/* add allocated area after 1st entry at head */
+		entry->next = head->next->next;
+		entry->prev = head->next;
+		entry->next->prev = entry;
+		entry->prev->next = entry;
+	}
+}
+
+static void list_remove_allocated_area(allocated_node *entry)
+{
+	allocated_node *head;
+
+	head = get_allocated_list_from_address(entry->address);
+	head->size -= entry->size;
+
+	entry->prev->next = entry->next;
+	entry->next->prev = entry->prev;
+	entry->next = NULL;
+	entry->prev = NULL;
+}
+
+static void list_add_free_area(free_node *entry)
+{
+	free_node *head;
+
+	head = get_list(entry->size);
+	head->size = head->size + entry->size;
+
+	if (head->prev->size <= entry->size) {
+		/* add free area at tail */
+		entry->next = head;
+		entry->prev = head->prev;
+		head->prev->next = entry;
+		head->prev = entry;
+	} else if (head->next->size > entry->size) {
+		/* add free area at head */
+		entry->next = head->next;
+		entry->prev = head;
+		head->next->prev = entry;
+		head->next = entry;
+	} else {
+		/* add free area after 1st entry at head */
+		entry->next = head->next->next;
+		entry->prev = head->next;
+		entry->next->prev = entry;
+		entry->prev->next = entry;
+	}
+}
+
+static void list_remove_free_area(free_node *entry)
+{
+	free_node *head;
+
+	head = get_list(entry->size);
+	head->size -= entry->size;
+
+	entry->prev->next = entry->next;
+	entry->next->prev = entry->prev;
+	entry->next = NULL;
+	entry->prev = NULL;
+}
+
+static void list_add_global_before(struct global_node *entry,
+				   struct global_node *list)
+{
+	entry->g_next = list;
+	entry->g_prev = list->g_prev;
+	entry->g_next->g_prev = entry;
+	entry->g_prev->g_next = entry;
+}
+
+static void list_add_global_after(struct global_node *entry,
+				  struct global_node *list)
+{
+	entry->g_prev = list;
+	entry->g_next = list->g_next;
+	entry->g_next->g_prev = entry;
+	entry->g_prev->g_next = entry;
+}
+
+static void list_remove_global(struct global_node *entry)
+{
+	entry->g_prev->g_next = entry->g_next;
+	entry->g_next->g_prev = entry->g_prev;
+	entry->g_next = NULL;
+	entry->g_prev = NULL;
+}
+
+/*****************************************************************/
+
+static void *allocate_free_area(free_node * nptr, unsigned int size)
+{
+	allocated_node *anptr = NULL;
+	int free_area_available;
+
+	free_area_available = nptr->size;
+
+	if (nptr->size == size) {
+		anptr = (allocated_node *)nptr;
+		anptr->tag = 1;
+		list_remove_free_area(nptr);
+		list_add_allocated_area(anptr);
+
+		return anptr;
+	}
+
+	if (free_area_available > size) {
+		anptr = (allocated_node *)nptr;
+
+		if ((free_area_available - size) < SPLIT_THREASHOLD) {
+			/* do not split the free area */
+			anptr->tag = 1;
+			list_remove_free_area(nptr);
+			list_add_allocated_area(anptr);
+		} else {
+			anptr =
+			    (allocated_node *) kmalloc(sizeof(allocated_node),
+						       GFP_KERNEL);
+			if (!anptr) {
+				printk(KERN_ERR "%s %d: kmalloc failed\n",
+				       __func__, __LINE__);
+				return NULL;
+			}
+
+			list_remove_free_area(nptr);
+
+			anptr->address = nptr->address;
+			anptr->size = size;
+			anptr->tag = 1;
+			nptr->address = nptr->address + size;
+			nptr->size = free_area_available - size;
+			nptr->tag = 0;
+
+			list_add_free_area(nptr);
+			list_add_allocated_area(anptr);
+			list_add_global_before(&anptr->g_entry, &nptr->g_entry);
+		}
+	}
+
+	return anptr;
+}
+
+void *do_align_valloc(free_node * ptr, unsigned long align, unsigned long size)
+{
+	allocated_node *anptr = NULL;
+	free_node *fnptr = NULL;
+	void *align_addr, *addr, *end_addr;
+
+	if (ptr->size < size)
+		return NULL;
+
+	addr = (void *)(ptr->address);
+	align_addr = (void *)((unsigned long)addr & (~(align - 1)));
+
+	if (!addr || (align_addr && (align_addr == addr))) {
+		anptr = allocate_free_area(ptr, size);
+	} else {
+		align_addr =
+		    (void *)(((unsigned long)addr + align) & (~(align - 1)));
+		end_addr = (void *)ptr->address + ptr->size;
+		if ((align_addr < end_addr) &&
+			(align_addr + size <= end_addr)) {
+			anptr =
+			    (allocated_node *) kmalloc(sizeof(allocated_node),
+						       GFP_KERNEL);
+			if (!anptr) {
+				printk(KERN_ERR "%s %d: kmalloc failed\n",
+				       __func__, __LINE__);
+				return NULL;
+			}
+
+			anptr->address = (unsigned long)align_addr;
+			anptr->size = size;
+			anptr->tag = 1;
+
+			/* chech if there is lower free area */
+			if (align_addr + size != end_addr) {
+				fnptr =
+				    (free_node *) kmalloc(sizeof(free_node),
+							  GFP_KERNEL);
+				if (!fnptr) {
+					kfree(anptr);
+					printk(KERN_ERR "kmalloc failed\n");
+					return NULL;
+				}
+
+				fnptr->address =
+				    (unsigned long)align_addr + size;
+				fnptr->size = end_addr - (align_addr + size);
+				fnptr->tag = 0;
+			}
+
+			list_remove_free_area(ptr);
+
+			ptr->size = ptr->size - anptr->size;
+			if (fnptr)
+				ptr->size = ptr->size - fnptr->size;
+
+			list_add_free_area(ptr);
+			list_add_allocated_area(anptr);
+			list_add_global_after(&anptr->g_entry, &ptr->g_entry);
+
+			if (fnptr) {
+				list_add_free_area(fnptr);
+				list_add_global_after(&fnptr->g_entry,
+						      &anptr->g_entry);
+			}
+		}
+	}
+
+	return anptr;
+}
+
+static int try_merge_upward(struct node *curr_node)
+{
+	struct node *prev_node;
+	int ret = 0;
+
+	if (curr_node->g_entry.g_prev != &global_list_head) {
+		prev_node =
+		    (void *)(curr_node->g_entry.g_prev) - OFFSET_OF(node);
+
+		if (prev_node->tag == 0) {
+			ret = 1;
+			list_remove_free_area(prev_node);
+			prev_node->size += curr_node->size;
+			list_add_free_area(prev_node);
+		}
+	}
+
+	return ret;
+}
+
+static void try_merge_downward(struct node *curr_node)
+{
+	struct node *next_node;
+
+	if (curr_node->g_entry.g_next != &global_list_head) {
+		next_node =
+		    (void *)(curr_node->g_entry.g_next) - OFFSET_OF(node);
+		if (next_node->tag == 0) {
+			curr_node->size += next_node->size;
+			list_remove_free_area(next_node);
+			list_remove_global(&next_node->g_entry);
+			kfree(next_node);
+		}
+	}
+}
+
+static void initialize_free_area_lists(void *start_address)
+{
+	int i;
+
+	free_node *nptr = kmalloc(sizeof(free_node), GFP_KERNEL);
+
+	for (i = 0; i <= NUMBER_OF_LISTS; i++) {
+		free_area_lists[i].address = 0;
+		free_area_lists[i].size = 0;
+		free_area_lists[i].next = &free_area_lists[i];
+		free_area_lists[i].prev = &free_area_lists[i];
+	}
+
+	nptr->address = (unsigned long)start_address;
+	nptr->size = shm_size;
+	nptr->tag = 0;
+
+	list_add_free_area(nptr);
+	list_add_global_before(&nptr->g_entry, &global_list_head);
+}
+
+static void initialize_address_lists(void)
+{
+	int i;
+
+	for (i = 0; i <= NUMBER_OF_LISTS; i++) {
+		allocated_area_lists[i].address = 0;
+		allocated_area_lists[i].size = 0;
+		allocated_area_lists[i].next = &allocated_area_lists[i];
+		allocated_area_lists[i].prev = &allocated_area_lists[i];
+	}
+}
+
+static void initialize_global_list(struct global_node *global_list_head)
+{
+	global_list_head->g_next = global_list_head;
+	global_list_head->g_prev = global_list_head;
+}
+
+static void initialize(unsigned long paddr, unsigned long size)
+{
+	psa = (void *)paddr;
+	shm_size = size;
+
+	initialize_global_list(&global_list_head);
+	initialize_free_area_lists(psa);
+	initialize_address_lists();
+}
+
+static void *__alloc(unsigned long size)
+{
+	allocated_node *allocated_node = NULL;
+	free_node *ptr, *tptr = NULL;
+	free_node *head;
+
+	if (!size) {
+		printk(KERN_WARNING
+		       "Allocation size must be greater then zero\n");
+		return NULL;
+	}
+
+	head = get_free_area_list(size);
+
+	if (head) {
+		ptr = head->next;
+		tptr = head;
+
+		do {
+			if (ptr->size >= size) {
+				allocated_node = allocate_free_area(ptr, size);
+				break;
+			} else {
+				if (tptr == head)
+					tptr = head->next;
+				else if (tptr->size < ptr->size)
+					tptr = ptr;
+
+				ptr = ptr->next;
+			}
+		} while (ptr != head);
+	}
+
+	if (allocated_node) {
+		if (tptr != head) {
+			list_remove_free_area(tptr);
+			list_add_free_area(tptr);
+		}
+		return (void *)(allocated_node->address);
+	}
+
+	return NULL;
+}
+
+static void *__memalign(unsigned long size, unsigned long align)
+{
+	allocated_node *allocated_node = NULL;
+	free_node *ptr, *tptr = NULL;
+	free_node *head;
+	int index = -1;
+
+	if (!size) {
+		printk("Allocation size must be greater then zero\n");
+		return NULL;
+	}
+
+	if (!align)
+		return __alloc(size);
+
+	head = get_free_area_list(size);
+
+	if (head) {
+		do {
+			ptr = head->next;
+			tptr = head;
+			do {
+				allocated_node =
+				     do_align_valloc(ptr, align, size);
+				if (allocated_node)
+					break;
+				else {
+					if (tptr == head)
+						tptr = head->next;
+					else if (tptr->size < ptr->size)
+						tptr = ptr;
+
+					ptr = ptr->next;
+				}
+			} while (ptr != head);
+
+			if (allocated_node) {
+				if (tptr != head) {
+					list_remove_free_area(tptr);
+					list_add_free_area(tptr);
+				}
+
+				return (void *)(allocated_node->address);
+			}
+
+			head = (char *)head + sizeof(free_node);
+			index =
+			    ((unsigned long)head -
+			     (unsigned long)&free_area_lists) /
+			    sizeof(free_node);
+		} while (index < NUMBER_OF_LISTS + 1);
+	}
+
+	return NULL;
+}
+
+static void __free(void *ptr)
+{
+	allocated_node *list_head;
+	allocated_node *anptr;
+	int merged = 0;
+
+	list_head = get_allocated_list_from_address((unsigned long)ptr);
+
+	anptr = list_head->next;
+
+	while (anptr != list_head) {
+		if (anptr->address == (unsigned long)ptr)
+			break;
+
+		anptr = anptr->next;
+	}
+
+	if (anptr->address == (unsigned long)ptr) {
+		list_remove_allocated_area(anptr);
+		anptr->tag = 0;
+		try_merge_downward(anptr);
+		merged = try_merge_upward(anptr);
+
+		if (merged) {
+			list_remove_global(&anptr->g_entry);
+			kfree(anptr);
+		} else
+			list_add_free_area((free_node *) anptr);
+	} else
+		printk(KERN_WARNING "%s: Trying to free %p invalid address\n",
+		       __func__, ptr);
+}
+
+static void *fsl_shm_alloc(unsigned long size)
+{
+	void *ptr;
+
+	spin_lock(&mm_lock);
+	ptr = __alloc(size);
+	spin_unlock(&mm_lock);
+
+	return ptr;
+}
+
+static void *fsl_shm_memalign(unsigned long size, unsigned long align)
+{
+	void *ptr;
+
+	spin_lock(&mm_lock);
+	ptr = __memalign(size, align);
+	spin_unlock(&mm_lock);
+
+	return ptr;
+}
+
+static void fsl_shm_free(void *ptr)
+{
+	spin_lock(&mm_lock);
+	if (ptr)
+		__free(ptr);
+	spin_unlock(&mm_lock);
+}
+
+static int fsl_shm_get_paddr(shm_seg_t *mem)
+{
+	int ret = 0;
+	struct page *pg, *pages = NULL;
+	struct vm_area_struct *vmas = NULL;
+	int npages;
+
+	down_read(&current->mm->mmap_sem);
+	ret =
+	    get_user_pages(current, current->mm, (unsigned long)mem->vaddr, 1,
+			   0, 0, &pages, &vmas);
+	up_read(&current->mm->mmap_sem);
+	if (ret > 0) {
+		npages = ret;
+		pg = &pages[0];
+		mem->paddr = page_to_phys(pg);
+		put_page(pg);
+	}
+
+	return 0;
+}
+
+static int fsl_shm_ioctl(struct inode *inode, struct file *file,
+			 unsigned int cmd, unsigned long arg)
+{
+	shm_seg_t seg;
+	alloc_req_t mem;
+	memalign_req_t memalign;
+	static int initialized = -1;
+
+	switch (cmd) {
+	case IOCTL_FSL_SHM_INIT:
+		copy_from_user(&seg, (shm_seg_t *) arg, sizeof(shm_seg_t));
+		fsl_shm_get_paddr(&seg);
+
+		spin_lock(&mm_lock);
+		if (initialized) {
+			initialized++;
+			initialize((unsigned long)seg.paddr, HUGE_PAGE_256M);
+		}
+		spin_unlock(&mm_lock);
+
+		copy_to_user((shm_seg_t *) arg, &seg, sizeof(shm_seg_t));
+		break;
+
+	case IOCTL_FSL_SHM_ALLOC:
+		copy_from_user(&mem, (alloc_req_t *) arg, sizeof(alloc_req_t));
+		mem.paddr = fsl_shm_alloc(mem.size);
+		copy_to_user((alloc_req_t *) arg, &mem, sizeof(alloc_req_t));
+
+		break;
+
+	case IOCTL_FSL_SHM_MEMALIGN:
+		copy_from_user(&memalign, (memalign_req_t *) arg,
+			       sizeof(memalign_req_t));
+		memalign.paddr =
+		    fsl_shm_memalign(memalign.size, memalign.align);
+		copy_to_user((memalign_req_t *) arg, &memalign,
+			     sizeof(memalign_req_t));
+		break;
+
+	case IOCTL_FSL_SHM_FREE:
+		fsl_shm_free((void *)arg);
+		break;
+
+	default:
+		break;
+	}
+
+	return 1;
+}
+
+static const struct file_operations fsl_shm_fops = {
+	.owner = THIS_MODULE,
+	.ioctl = fsl_shm_ioctl,
+};
+
+static __init int fsl_shm_init(void)
+{
+	int ret;
+
+	fsl_shm_major = 0;
+	ret = register_chrdev(fsl_shm_major, "fsl_shm", &fsl_shm_fops);
+	if (ret < 0) {
+		printk(KERN_ERR "%s: Can't register character device\n",
+			__func__);
+		return ret;
+	}
+
+	fsl_shm_major = ret;
+	spin_lock_init(&mm_lock);
+
+	printk(KERN_INFO "fsl_shm module installed successfully with"
+	       "major num : %d\n", fsl_shm_major);
+
+	return 0;
+}
+
+static void __exit fsl_shm_exit(void)
+{
+	unregister_chrdev(fsl_shm_major, "fsl_shm");
+}
+
+module_init(fsl_shm_init);
+module_exit(fsl_shm_exit);
+
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("FSL Shared Memory Manager");
+MODULE_AUTHOR("naveenburmi@freescale.com");
diff --git a/drivers/misc/fsl_shm.h b/drivers/misc/fsl_shm.h
new file mode 100644
index 0000000..c7829b3
--- /dev/null
+++ b/drivers/misc/fsl_shm.h
@@ -0,0 +1,55 @@
+/*
+ * Copyright (C) 2011 Freescale Semiconductor, Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.  IN
+ * NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+ * TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+ * LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+ * NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ *	Author: Naveen Burmi <naveenburmi@freescale.com>
+ */
+
+#ifndef __FSL_MM_IPC_H
+#define __FSL_MM_IPC_H
+
+#include <linux/ioctl.h>
+
+typedef struct shm_seg {
+	void *vaddr;
+	void *paddr;
+	u32 size;
+} shm_seg_t;
+
+typedef struct alloc_req {
+	void *paddr;
+	size_t size;
+} alloc_req_t;
+
+typedef struct memalign_req {
+	void *paddr;
+	unsigned long align;
+	size_t size;
+} memalign_req_t;
+
+#define FSL_MM_MAGIC	'F'
+#define IOCTL_FSL_SHM_INIT	_IOWR(FSL_MM_MAGIC, 1, shm_seg_t *)
+#define IOCTL_FSL_SHM_ALLOC	_IOWR(FSL_MM_MAGIC, 2, alloc_req_t *)
+#define IOCTL_FSL_SHM_MEMALIGN  _IOWR(FSL_MM_MAGIC, 3, memalign_req_t *)
+#define IOCTL_FSL_SHM_FREE	_IOR(FSL_MM_MAGIC, 4, void *)
+
+#endif
-- 
1.7.0

