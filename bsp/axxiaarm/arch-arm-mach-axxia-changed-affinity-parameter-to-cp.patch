From bbd1b1a51f50706db832d21e9d365383b0380086 Mon Sep 17 00:00:00 2001
From: Charlie Paul <cpaul.windriver@gmail.com>
Date: Mon, 27 Apr 2015 21:50:21 -0700
Subject: [PATCH 37/59] arch/arm/mach-axxia: changed affinity parameter to cpu

git://git.yoctoproject.org/linux-yocto-4.1 standard/axxia/base
	commit 9be55d0af240593ad880fcd6b1e23b9ddfa051b0

Signed-off-by: Charlie Paul <cpaul.windriver@gmail.com>
Signed-off-by: Xulin Sun <xulin.sun@windriver.com>
---
 arch/arm/mach-axxia/axxia-gic.c |   84 +++++++++++++++++++++++++++-----------
 1 files changed, 59 insertions(+), 25 deletions(-)

diff --git a/arch/arm/mach-axxia/axxia-gic.c b/arch/arm/mach-axxia/axxia-gic.c
index a87ef9c..a4a9611 100644
--- a/arch/arm/mach-axxia/axxia-gic.c
+++ b/arch/arm/mach-axxia/axxia-gic.c
@@ -447,13 +447,12 @@ static int gic_retrigger(struct irq_data *d)
 }
 
 static int _gic_set_affinity(struct irq_data *d,
-			     const struct cpumask *mask_val,
-			     bool do_clear)
+		unsigned int cpu,
+		bool do_clear)
 {
 	void __iomem *reg  = gic_dist_base(d) + GIC_DIST_TARGET +
 			     (gic_irq(d) & ~3);
 	unsigned int shift = (gic_irq(d) % 4) * 8;
-	unsigned int cpu = cpumask_any_and(mask_val, cpu_online_mask);
 	u32 val, mask, bit;
 	u32 enable_mask, enable_offset;
 
@@ -495,15 +494,46 @@ static void gic_set_affinity_remote(void *info)
 {
 	struct gic_rpc_data *rpc = (struct gic_rpc_data *)info;
 
-	_gic_set_affinity(rpc->d, rpc->mask_val, false);
+	_gic_set_affinity(rpc->d, rpc->cpu, false);
 
 }
 static void gic_clr_affinity_remote(void *info)
 {
 	struct gic_rpc_data *rpc = (struct gic_rpc_data *)info;
 
-	_gic_set_affinity(rpc->d, rpc->mask_val, true);
+	_gic_set_affinity(rpc->d, rpc->oldcpu, true);
+
+}
 
+static bool on_same_cluster(u32 pcpu1, u32 pcpu2)
+{
+	return pcpu1 / CORES_PER_CLUSTER == pcpu2 / CORES_PER_CLUSTER;
+}
+
+static int exec_remote_set_affinity(bool clr_affinity, u32 cpu,
+				    struct irq_data *d,
+				    const struct cpumask *mask_val,
+				    bool force)
+{
+	int ret = IRQ_SET_MASK_OK;
+
+	if (force == false) {
+		gic_rpc_data.d = d;
+		gic_rpc_data.mask_val = mask_val;
+		if (clr_affinity == true) {
+			gic_rpc_data.oldcpu = cpu;
+			gic_rpc_data.func_mask |= CLR_AFFINITY;
+		} else {
+			gic_rpc_data.cpu = cpu;
+			gic_rpc_data.func_mask |= SET_AFFINITY;
+		}
+
+	} else {
+		pr_warn("Set affinity for hotplug not implemented.\n");
+		return -ENOSYS;
+	}
+
+	return ret;
 }
 
 static int gic_set_affinity(struct irq_data *d,
@@ -513,6 +543,7 @@ static int gic_set_affinity(struct irq_data *d,
 	unsigned int cpu = cpumask_any_and(mask_val, cpu_online_mask);
 	u32 pcpu = cpu_logical_map(smp_processor_id());
 	unsigned int irqid = gic_irq(d);
+	int ret = IRQ_SET_MASK_OK;
 
 	BUG_ON(!irqs_disabled());
 
@@ -538,16 +569,15 @@ static int gic_set_affinity(struct irq_data *d,
 	 * cluster as the cpu we're currently running on, set the IRQ
 	 * affinity directly. Otherwise, use the RPC mechanism.
 	 */
-	if ((cpu_logical_map(cpu) / CORES_PER_CLUSTER) ==
-			(pcpu / CORES_PER_CLUSTER)) {
-		gic_set_affinity(d, mask_val, false);
+	if (on_same_cluster(cpu_logical_map(cpu), pcpu)) {
+		_gic_set_affinity(d, cpu_logical_map(cpu), false);
+	}
 
-	} else {
+	else{
+		ret = exec_remote_set_affinity(false, cpu_logical_map(cpu), d, mask_val, force);
 
-		gic_rpc_data.func_mask |= SET_AFFINITY;
-		gic_rpc_data.cpu = cpu;
-		gic_rpc_data.d = d;
-		gic_rpc_data.mask_val = mask_val;
+		if (ret != IRQ_SET_MASK_OK)
+			return ret;
 	}
 
 	/*
@@ -555,25 +585,29 @@ static int gic_set_affinity(struct irq_data *d,
 	 * different than the prior cluster, clear the IRQ affinity
 	 * on the old cluster.
 	 */
-	if ((irqid != IRQ_PMU) && ((cpu_logical_map(cpu) / CORES_PER_CLUSTER) !=
-			(irq_cpuid[irqid] / CORES_PER_CLUSTER))) {
+	if (!on_same_cluster(cpu_logical_map(cpu), irq_cpuid[irqid]) && (irqid != IRQ_PMU)) {
 		/*
 		 * If old cpu assignment falls within the same cluster as
 		 * the cpu we're currently running on, clear the IRQ affinity
 		 * directly. Otherwise, use RPC mechanism.
 		 */
-		if ((irq_cpuid[irqid] / CORES_PER_CLUSTER) ==
-				(pcpu / CORES_PER_CLUSTER)) {
-			gic_set_affinity(d, mask_val, true);
-		} else {
-			gic_rpc_data.func_mask |= CLR_AFFINITY;
-			gic_rpc_data.oldcpu = irq_cpuid[irqid];
-			gic_rpc_data.d = d;
-			gic_rpc_data.mask_val = mask_val;			kfree((void *) gic_rpc_ptr);
+		if (on_same_cluster(irq_cpuid[irqid], pcpu)) {
+			_gic_set_affinity(d, irq_cpuid[irqid], true);
 		}
-		schedule_work_on(0, &axxia_gic_affinity_work);
+		else {
+			ret = exec_remote_set_affinity(true,
+					get_logical_index(irq_cpuid[irqid]), d, mask_val, force);
+		}
+		if (ret != IRQ_SET_MASK_OK) {
+			/* Need to back out the set operation */
+			if (on_same_cluster(cpu_logical_map(cpu), pcpu))
+				_gic_set_affinity(d, irq_cpuid[irqid], true);
+			else
+				exec_remote_set_affinity(true, cpu, d, mask_val, force);
+
+			return ret;
+	}
 	}
-
 
 	/* Update Axxia IRQ affinity table with the new physical CPU number. */
 	irq_cpuid[irqid] = cpu_logical_map(cpu);
-- 
1.7.5.4

