From 3ec0001131692c97dce6736f529651a3693e2aa5 Mon Sep 17 00:00:00 2001
From: Charlie Paul <cpaul.windriver@gmail.com>
Date: Tue, 31 Mar 2015 16:47:10 -0700
Subject: [PATCH 10/59] drivers/i2c: Changes to support axxia BSP

git://git.yoctoproject.org/linux-yocto-4.1 standard/axxia/base
	commit 1fc54da4a3611bac0311f0a634d4bb576953ab98

These files are used to add the i2c functionality to the LSI axxia
5500 board.

Signed-off-by: Charlie Paul <cpaul.windriver@gmail.com>
Signed-off-by: Xulin Sun <xulin.sun@windriver.com>
---
 drivers/dma/lsi-dma32.c          |  883 ++++++++++++++++++++++++++++++++++++++
 drivers/dma/lsi-dma32.h          |  221 ++++++++++
 drivers/edac/Kconfig             |   24 +
 drivers/edac/Makefile            |    3 +
 drivers/edac/axxia_edac-l2_cpu.c |  333 ++++++++++++++
 drivers/edac/axxia_edac-l3.c     |  185 ++++++++
 drivers/edac/axxia_edac-mc.c     |  351 +++++++++++++++
 drivers/edac/axxia_edac.c        |  461 ++++++++++++++++++++
 drivers/i2c/busses/Kconfig       |   21 +
 drivers/i2c/busses/acp3400-i2c.c |  515 ++++++++++++++++++++++
 drivers/i2c/busses/i2c-axxia.c   |  607 +++++++++++++++-----------
 include/linux/i2c-axxia.h        |   39 ++
 12 files changed, 3390 insertions(+), 253 deletions(-)
 create mode 100644 drivers/dma/lsi-dma32.c
 create mode 100644 drivers/dma/lsi-dma32.h
 create mode 100644 drivers/edac/axxia_edac-l2_cpu.c
 create mode 100644 drivers/edac/axxia_edac-l3.c
 create mode 100644 drivers/edac/axxia_edac-mc.c
 create mode 100644 drivers/edac/axxia_edac.c
 create mode 100644 drivers/i2c/busses/acp3400-i2c.c
 create mode 100644 include/linux/i2c-axxia.h

diff --git a/drivers/dma/lsi-dma32.c b/drivers/dma/lsi-dma32.c
new file mode 100644
index 0000000..6d1cc1a
--- /dev/null
+++ b/drivers/dma/lsi-dma32.c
@@ -0,0 +1,883 @@
+/*
+ * Driver for the LSI DMA controller DMA-32.
+ *
+ * The driver is based on:
+ *
+ * lsi-dma32.c -
+ * lsi-dma.c - Copyright 2011 Mentor Graphics
+ * acp_gpdma.c - Copyright (c) 2011, Ericsson AB
+ *               Niclas Bengtsson <niklas.x.bengtsson@ericsson.com>
+ *               Kerstin Jonsson <kerstin.jonsson@ericsson.com>
+ *
+ * This is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ * You may obtain a copy of the GNU General Public License
+ * Version 2 or later at the following locations:
+ *
+ * http://www.opensource.org/licenses/gpl-license.html
+ * http://www.gnu.org/copyleft/gpl.html
+ */
+#include <linux/export.h>
+#include <linux/stat.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/mm.h>
+#include <linux/interrupt.h>
+#include <linux/slab.h>
+#include <linux/io.h>
+#include <linux/platform_device.h>
+#include <linux/of_platform.h>
+#include <linux/of_address.h>
+#include <linux/of_irq.h>
+#include <linux/of.h>
+#include <linux/delay.h>
+#include <asm/page.h>
+#include <linux/bitops.h>
+#include <linux/atomic.h>
+#include <linux/sizes.h>
+#include "virt-dma.h"
+#include "lsi-dma32.h"
+
+#ifdef DEBUG
+#define engine_dbg(engine, fmt, ...) \
+	do { \
+		struct gpdma_engine *_e = engine; \
+		(void)_e; \
+		pr_debug("dma0: " fmt, ##__VA_ARGS__); \
+	} while (0)
+
+#define ch_dbg(dmac, fmt, ...) \
+	do { \
+		struct gpdma_channel *_c = dmac; \
+		(void)_c; \
+		pr_debug("dma0ch%d: [%s] " fmt, \
+			dmac->id, __func__, ##__VA_ARGS__); \
+	} while (0)
+#else
+#define engine_dbg(engine, fmt, ...) do {} while (0)
+#define ch_dbg(dmac, fmt, ...)       do {} while (0)
+#endif
+
+static unsigned int burst = 5;
+module_param(burst, uint, 0644);
+MODULE_PARM_DESC(burst,
+		 "Preferred burst setting (0=SINGLE,3=INCR4,5=INCR8,7=INCR16)");
+
+static void reset_channel(struct gpdma_channel *dmac)
+{
+	const int WAIT = 1024;
+	int i;
+
+	/* Pause channel */
+	writel(DMA_STATUS_CH_PAUS_WR_EN | DMA_STATUS_CH_PAUSE,
+	     dmac->base+DMA_STATUS);
+	/* Memory Barrier */
+	wmb();
+
+	/* Disable channel */
+	writel(0, dmac->base+DMA_CHANNEL_CONFIG);
+	for (i = 0; readl(dmac->base+DMA_CHANNEL_CONFIG) && i < WAIT; i++)
+		cpu_relax();
+	if (i == WAIT)
+		ch_dbg(dmac, "Failed to DISABLE channel\n");
+
+	/* Clear FIFO */
+	writel(DMA_CONFIG_CLEAR_FIFO, dmac->base+DMA_CHANNEL_CONFIG);
+	for (i = 0; readl(dmac->base+DMA_CHANNEL_CONFIG) && i < WAIT; i++)
+		cpu_relax();
+	if (i == WAIT)
+		ch_dbg(dmac, "Failed to clear FIFO\n");
+}
+
+static void soft_reset(struct gpdma_engine *engine)
+{
+	int i;
+	u32 cfg;
+
+	/* Reset all channels */
+	for (i = 0; i < engine->chip->num_channels; i++)
+		reset_channel(&engine->channel[i]);
+
+	/* Reset GPDMA by writing Magic Number to reset reg */
+	writel(GPDMA_MAGIC, engine->gbase + SOFT_RESET);
+	/* Memory Barrier */
+	wmb();
+
+	cfg = (engine->pool.phys & 0xfff00000) | GEN_CONFIG_EXT_MEM;
+
+	if (engine->chip->flags & LSIDMA_EDGE_INT) {
+		for (i = 0; i < engine->chip->num_channels; i++)
+			cfg |= GEN_CONFIG_INT_EDGE(i);
+		engine_dbg(engine, "Using edge-triggered interrupts\n");
+	}
+	writel(cfg, engine->gbase + GEN_CONFIG);
+	engine_dbg(engine, "engine->desc.phys & 0xfff00000 == %llx\n",
+		   (engine->pool.phys & 0xfff00000));
+
+	engine->ch_busy = 0;
+}
+
+static int alloc_desc_table(struct gpdma_engine *engine)
+{
+	/*
+	 * For controllers that doesn't support full descriptor addresses, all
+	 * descriptors must be in the same 1 MB page, i.e address bits 31..20
+	 * must be the same for all descriptors.
+	 */
+	u32 order = 20 - PAGE_SHIFT;
+	int i;
+
+	if (engine->chip->flags & LSIDMA_NEXT_FULL) {
+		/*
+		 * Controller can do full descriptor addresses, then we need no
+		 * special alignment on the descriptor block.
+		 */
+		order = get_order(GPDMA_MAX_DESCRIPTORS *
+				  sizeof(struct gpdma_desc));
+	}
+
+	engine->pool.va = (struct gpdma_desc *)
+			  __get_free_pages(GFP_KERNEL|GFP_DMA, order);
+	if (!engine->pool.va)
+		return -ENOMEM;
+	engine->pool.order = order;
+	engine->pool.phys = virt_to_phys(engine->pool.va);
+	engine_dbg(engine, "order=%d pa=%#llx va=%p\n",
+		   engine->pool.order, engine->pool.phys, engine->pool.va);
+
+	INIT_LIST_HEAD(&engine->free_list);
+	for (i = 0; i < GPDMA_MAX_DESCRIPTORS; i++) {
+		struct gpdma_desc *desc = &engine->pool.va[i];
+
+		async_tx_ack(&desc->vdesc.tx);
+		desc->engine = engine;
+		list_add_tail(&desc->vdesc.node, &engine->free_list);
+	}
+
+	return 0;
+}
+
+static void free_desc_table(struct gpdma_engine *engine)
+{
+	if (engine->pool.va)
+		free_pages((unsigned long)engine->pool.va, engine->pool.order);
+}
+
+static struct gpdma_desc *get_descriptor(struct gpdma_engine *engine)
+{
+	unsigned long flags;
+	struct gpdma_desc *new = NULL, *desc, *tmp;
+
+	spin_lock_irqsave(&engine->lock, flags);
+	list_for_each_entry_safe(desc, tmp, &engine->free_list, vdesc.node) {
+		if (async_tx_test_ack(&desc->vdesc.tx)) {
+			list_del(&desc->vdesc.node);
+			new = desc;
+			new->chain = NULL;
+			break;
+		}
+	}
+	spin_unlock_irqrestore(&engine->lock, flags);
+
+	return new;
+}
+
+/**
+ * init_descriptor - Fill out all descriptor fields
+ */
+static void init_descriptor(struct gpdma_desc *desc,
+			    dma_addr_t src, u32 src_acc,
+			    dma_addr_t dst, u32 dst_acc,
+			    size_t len)
+{
+	u32 src_count = len >> src_acc;
+	u32 dst_count = len >> dst_acc;
+	u32 rot_len = (2 * (1 << src_acc)) - 1;
+
+	BUG_ON(src_count * (1<<src_acc) != len);
+	BUG_ON(dst_count * (1<<dst_acc) != len);
+
+	desc->src = src;
+	desc->dst = dst;
+
+	desc->hw.src_x_ctr     = cpu_to_le16(src_count - 1);
+	desc->hw.src_y_ctr     = 0;
+	desc->hw.src_x_mod     = cpu_to_le32(1 << src_acc);
+	desc->hw.src_y_mod     = 0;
+	desc->hw.src_addr      = cpu_to_le32(src & 0xffffffff);
+	desc->hw.src_data_mask = ~0;
+	desc->hw.src_access    = cpu_to_le16((rot_len << 6) |
+					    (src_acc << 3) |
+					    (burst & 7));
+	desc->hw.dst_access    = cpu_to_le16((dst_acc << 3) |
+					    (burst & 7));
+	desc->hw.ch_config     = cpu_to_le32(DMA_CONFIG_ONE_SHOT(1));
+	desc->hw.next_ptr      = 0;
+	desc->hw.dst_x_ctr     = cpu_to_le16(dst_count - 1);
+	desc->hw.dst_y_ctr     = 0;
+	desc->hw.dst_x_mod     = cpu_to_le32(1 << dst_acc);
+	desc->hw.dst_y_mod     = 0;
+	desc->hw.dst_addr      = cpu_to_le32(dst & 0xffffffff);
+}
+
+static phys_addr_t desc_to_paddr(const struct gpdma_channel *dmac,
+				 const struct gpdma_desc *desc)
+{
+	phys_addr_t paddr = virt_to_phys(&desc->hw);
+
+	WARN_ON(paddr & 0xf);
+	if (dmac->engine->chip->flags & LSIDMA_NEXT_FULL)
+		paddr |= 0x8;
+	else
+		paddr &= 0xfffff;
+
+	return paddr;
+}
+
+static void free_descriptor(struct virt_dma_desc *vd)
+{
+	struct gpdma_desc *desc = to_gpdma_desc(vd);
+	struct gpdma_engine *engine = desc->engine;
+	unsigned long flags;
+
+	BUG_ON(desc == NULL);
+
+	spin_lock_irqsave(&engine->lock, flags);
+	while (desc) {
+		list_add_tail(&desc->vdesc.node, &engine->free_list);
+		desc = desc->chain;
+	}
+	spin_unlock_irqrestore(&engine->lock, flags);
+}
+
+static int segment_match(struct gpdma_engine *engine, struct gpdma_desc *desc)
+{
+	unsigned int gpreg_dma = readl(engine->gpreg);
+	unsigned int seg_src = (gpreg_dma >> 0) & 0x3f;
+	unsigned int seg_dst = (gpreg_dma >> 8) & 0x3f;
+
+	return (seg_src == ((desc->src >> 32) & 0x3f) &&
+		seg_dst == ((desc->dst >> 32) & 0x3f));
+}
+
+static void gpdma_start(struct gpdma_channel *dmac)
+{
+	struct virt_dma_desc *vdesc;
+	struct gpdma_desc    *desc;
+	phys_addr_t           paddr;
+
+	vdesc = vchan_next_desc(&dmac->vc);
+	if (!vdesc) {
+		clear_bit(dmac->id, &dmac->engine->ch_busy);
+		dmac->active = NULL;
+		return;
+	}
+
+	/* Remove from list and mark as active */
+	list_del(&vdesc->node);
+	desc = to_gpdma_desc(vdesc);
+	dmac->active = desc;
+
+	if (!(dmac->engine->chip->flags & LSIDMA_SEG_REGS)) {
+		/*
+		 * No segment registers -> descriptor address bits must match
+		 * running descriptor on any other channel.
+		 */
+		if (dmac->engine->ch_busy && !segment_match(dmac->engine, desc))
+			return;
+	}
+
+	/* Physical address of descriptor to load */
+	paddr = desc_to_paddr(dmac, desc);
+	writel((u32)paddr, dmac->base + DMA_NXT_DESCR);
+
+	if (dmac->engine->chip->flags & LSIDMA_SEG_REGS) {
+		/* Segment bits [39..32] of descriptor, src and dst addresses */
+		writel(paddr >> 32, dmac->base + DMA_DESCR_ADDR_SEG);
+		writel(desc->src >> 32, dmac->base + DMA_SRC_ADDR_SEG);
+		writel(desc->dst >> 32, dmac->base + DMA_DST_ADDR_SEG);
+	} else {
+		unsigned int seg_src = (desc->src >> 32) & 0x3f;
+		unsigned int seg_dst = (desc->dst >> 32) & 0x3f;
+
+		writel((seg_dst << 8) | seg_src, dmac->engine->gpreg);
+	}
+	/* Memory barrier */
+	wmb();
+	writel(DMA_CONFIG_DSC_LOAD, dmac->base + DMA_CHANNEL_CONFIG);
+	set_bit(dmac->id, &dmac->engine->ch_busy);
+}
+
+static irqreturn_t gpdma_isr_err(int irqno, void *_engine)
+{
+	struct gpdma_engine *engine = _engine;
+	u32 status = readl(engine->gbase + GEN_STAT);
+	u32 ch = (status & GEN_STAT_CH0_ERROR) ? 0 : 1;
+	struct gpdma_channel *dmac = &engine->channel[ch];
+
+	if (0 == (status & (GEN_STAT_CH0_ERROR | GEN_STAT_CH1_ERROR)))
+		return IRQ_NONE;
+
+	/* Read the channel status bits and dump the error */
+	status = readl(dmac->base + DMA_STATUS);
+	pr_err("dma: channel%d error %08x\n", dmac->id, status);
+	/* Clear the error indication */
+	writel(DMA_STATUS_ERROR, dmac->base+DMA_STATUS);
+
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t gpdma_isr(int irqno, void *_dmac)
+{
+	struct gpdma_channel *dmac = _dmac;
+	struct gpdma_desc    *desc = dmac->active;
+	u32                  status;
+	u32	             error;
+
+	status = readl(dmac->base+DMA_STATUS);
+	error = status & DMA_STATUS_ERROR;
+	writel(DMA_STATUS_CLEAR, dmac->base+DMA_STATUS);
+
+	ch_dbg(dmac, "irq%u channel status %08x, error %08x\n",
+		irqno, status, error);
+
+	WARN_ON((status & DMA_STATUS_CH_ACTIVE) != 0);
+
+	if (error) {
+		if (error & DMA_STATUS_UNALIGNED_ERR) {
+			dev_warn(dmac->engine->dev,
+				 "Unaligned transaction on ch%d (status=%#x)\n",
+				 dmac->id, status);
+			reset_channel(dmac);
+		} else {
+			dev_warn(dmac->engine->dev,
+				 "DMA transaction error on ch%d (status=%#x)\n",
+				 dmac->id, status);
+		}
+	}
+
+	BUG_ON(desc == NULL);
+
+	spin_lock(&dmac->vc.lock);
+	vchan_cookie_complete(&desc->vdesc);
+	dmac->active = NULL;
+	if (vchan_next_desc(&dmac->vc)) {
+		gpdma_start(dmac);
+	} else {
+		/* Stop channel */
+		writel(0, dmac->base + DMA_CHANNEL_CONFIG);
+		writel(DMA_CONFIG_CLEAR_FIFO, dmac->base + DMA_CHANNEL_CONFIG);
+		clear_bit(dmac->id, &dmac->engine->ch_busy);
+	}
+	spin_unlock(&dmac->vc.lock);
+
+	return IRQ_HANDLED;
+}
+
+static void flush_channel(struct gpdma_channel *dmac)
+{
+	unsigned long flags;
+	LIST_HEAD(head);
+
+	spin_lock_irqsave(&dmac->vc.lock, flags);
+	reset_channel(dmac);
+	if (dmac->active) {
+		free_descriptor(&dmac->active->vdesc);
+		dmac->active = NULL;
+	}
+	vchan_get_all_descriptors(&dmac->vc, &head);
+	spin_unlock_irqrestore(&dmac->vc.lock, flags);
+	vchan_dma_desc_free_list(&dmac->vc, &head);
+}
+
+/*
+ * Perform soft reset procedure on DMA Engine.  Needed occasionally to work
+ * around nasty bug ACP3400 sRIO HW.
+ */
+static ssize_t __ref
+reset_engine(struct device *dev,
+	     struct device_attribute *attr,
+	     const char *buf, size_t count)
+{
+	struct gpdma_engine *engine = dev_get_drvdata(dev);
+	int i;
+
+	if (!engine)
+		return -EINVAL;
+
+	/* Disable interrupts and tasklet and acquire each channel lock */
+	for (i = 0; i < engine->chip->num_channels; i++) {
+		struct gpdma_channel *dmac = &engine->channel[i];
+
+		disable_irq(dmac->irq);
+		spin_lock(&dmac->vc.lock);
+		tasklet_disable(&dmac->vc.task);
+	}
+
+	soft_reset(engine);
+
+	for (i = 0; i < engine->chip->num_channels; i++) {
+		struct gpdma_channel *dmac = &engine->channel[i];
+
+		tasklet_enable(&dmac->vc.task);
+		enable_irq(dmac->irq);
+		/* Restart any active jobs */
+		if (dmac->active) {
+			struct gpdma_desc *active = dmac->active;
+
+			dmac->active = NULL;
+			list_add(&active->vdesc.node, &dmac->vc.desc_submitted);
+			if (vchan_issue_pending(&dmac->vc))
+				gpdma_start(dmac);
+		}
+		spin_unlock(&dmac->vc.lock);
+	}
+
+	return count;
+}
+static DEVICE_ATTR(soft_reset, S_IWUSR, NULL, reset_engine);
+
+/*
+ *===========================================================================
+ *
+ *                       DMA DEVICE INTERFACE
+ *
+ *===========================================================================
+ *
+ */
+
+/**
+ * gpdma_alloc_chan_resources - Allocate resources and return the number of
+ * allocated descriptors.
+ *
+ */
+static int gpdma_alloc_chan_resources(struct dma_chan *chan)
+{
+	struct gpdma_channel *dmac = to_gpdma_chan(chan);
+
+	(void) dmac;
+	return 1;
+}
+
+/**
+ * gpdma_free_chan_resources - Release DMA channel's resources.
+ *
+ */
+static void gpdma_free_chan_resources(struct dma_chan *chan)
+{
+	struct gpdma_channel *dmac = to_gpdma_chan(chan);
+
+	(void) dmac;
+}
+
+/**
+ * gpdma_prep_sg - Prepares a transfer using sg lists.
+ *
+ */
+static struct dma_async_tx_descriptor *
+gpdma_prep_sg(struct dma_chan *chan,
+	      struct scatterlist *dst_sg, unsigned int dst_nents,
+	      struct scatterlist *src_sg, unsigned int src_nents,
+	      unsigned long flags)
+{
+	struct gpdma_channel *dmac = to_gpdma_chan(chan);
+	struct gpdma_desc *first = NULL, *prev = NULL, *new;
+	size_t dst_avail, src_avail;
+	dma_addr_t dst, src;
+	u32 src_acc, dst_acc;
+	size_t len;
+
+	if (dst_nents == 0 || src_nents == 0)
+		return NULL;
+
+	if (dst_sg == NULL || src_sg == NULL)
+		return NULL;
+
+	dst_avail = sg_dma_len(dst_sg);
+	src_avail = sg_dma_len(src_sg);
+
+	/* Loop until we run out of entries... */
+	for (;;) {
+		/* Descriptor count is limited to 64K */
+		len = min_t(size_t, src_avail, dst_avail);
+		len = min_t(size_t, len, (size_t)SZ_64K);
+
+		if (len > 0) {
+			dst = sg_dma_address(dst_sg) +
+				sg_dma_len(dst_sg) - dst_avail;
+			src = sg_dma_address(src_sg) +
+				sg_dma_len(src_sg) - src_avail;
+
+			src_acc = min(ffs((u32)src | len) - 1, 4);
+			dst_acc = min(ffs((u32)dst | len) - 1, 4);
+
+			new = get_descriptor(dmac->engine);
+			if (!new) {
+				ch_dbg(dmac, "ERROR: No descriptor\n");
+				goto fail;
+			}
+
+			init_descriptor(new, src, src_acc, dst, dst_acc, len);
+
+			/* Link descriptors together */
+			if (!first) {
+				first = new;
+			} else {
+				prev->hw.next_ptr = desc_to_paddr(dmac, new);
+				prev->chain = new;
+			}
+			prev = new;
+
+			/* update metadata */
+			dst_avail -= len;
+			src_avail -= len;
+		}
+
+		/* dst: Advance to next sg-entry */
+		if (dst_avail == 0) {
+			/* no more entries: we're done */
+			if (dst_nents == 0)
+				break;
+			/* fetch the next entry: if there are no more: done */
+			dst_sg = sg_next(dst_sg);
+			if (dst_sg == NULL)
+				break;
+
+			dst_nents--;
+			dst_avail = sg_dma_len(dst_sg);
+		}
+
+		/* src: Advance to next sg-entry */
+		if (src_avail == 0) {
+			/* no more entries: we're done */
+			if (src_nents == 0)
+				break;
+			/* fetch the next entry: if there are no more: done */
+			src_sg = sg_next(src_sg);
+			if (src_sg == NULL)
+				break;
+
+			src_nents--;
+			src_avail = sg_dma_len(src_sg);
+		}
+	}
+
+	/* Interrupt on last descriptor in chain */
+	prev->hw.ch_config |= cpu_to_le32(DMA_CONFIG_END);
+
+	return vchan_tx_prep(&dmac->vc, &first->vdesc, flags);
+
+fail:
+	if (first)
+		free_descriptor(&first->vdesc);
+	return NULL;
+}
+
+/**
+ * gpdma_prep_memcpy - Prepares a memcpy operation.
+ *
+ */
+static struct dma_async_tx_descriptor *
+gpdma_prep_memcpy(struct dma_chan *chan,
+		  dma_addr_t dst,
+		  dma_addr_t src,
+		  size_t size,
+		  unsigned long dma_flags)
+{
+	struct gpdma_channel *dmac = to_gpdma_chan(chan);
+	struct gpdma_desc *first = NULL, *prev = NULL, *new;
+	u32 src_acc, dst_acc;
+	size_t len;
+
+	if (size == 0)
+		return NULL;
+
+	do {
+		new = get_descriptor(dmac->engine);
+		if (new == NULL) {
+			ch_dbg(dmac, "ERROR: No descriptor\n");
+			goto fail;
+		}
+
+		len = min_t(size_t, size, (size_t)SZ_64K);
+
+		/* Maximize access width based on address and length alignmet */
+		src_acc = min(ffs((u32)src | len) - 1, 4);
+		dst_acc = min(ffs((u32)dst | len) - 1, 4);
+
+		init_descriptor(new, src, src_acc, dst, dst_acc, len);
+
+		if (!first) {
+			first = new;
+		} else {
+			prev->hw.next_ptr = desc_to_paddr(dmac, new);
+			prev->chain = new;
+		}
+		prev = new;
+
+		size -= len;
+		src  += len;
+		dst  += len;
+
+	} while (size > 0);
+
+	prev->hw.ch_config |= cpu_to_le32(DMA_CONFIG_END);
+
+	return vchan_tx_prep(&dmac->vc, &first->vdesc, DMA_CTRL_ACK);
+
+fail:
+	if (first)
+		free_descriptor(&first->vdesc);
+	return NULL;
+}
+
+/**
+ * gpdma_issue_pending - Push pending transactions to hardware.
+ *
+ */
+static void gpdma_issue_pending(struct dma_chan *chan)
+{
+	struct gpdma_channel *dmac = to_gpdma_chan(chan);
+	unsigned long flags;
+
+	spin_lock_irqsave(&dmac->vc.lock, flags);
+	if (vchan_issue_pending(&dmac->vc) && !dmac->active)
+		gpdma_start(dmac);
+	spin_unlock_irqrestore(&dmac->vc.lock, flags);
+}
+
+/**
+ * gpdma_tx_status - Poll for transaction completion, the optional txstate
+ * parameter can be supplied with a pointer to get a struct with auxiliary
+ * transfer status information, otherwise the call will just return a simple
+ * status code.
+ */
+static enum dma_status gpdma_tx_status(struct dma_chan *chan,
+				       dma_cookie_t cookie,
+				       struct dma_tx_state *txstate)
+{
+	return dma_cookie_status(chan, cookie, txstate);
+}
+
+
+/**
+ * gpdma_device_control - Manipulate all pending operations on a channel,
+ * returns zero or error code.
+ *
+ */
+static int gpdma_device_control(struct dma_chan *chan,
+				enum dma_ctrl_cmd cmd,
+				unsigned long arg)
+{
+	struct gpdma_channel *dmac = to_gpdma_chan(chan);
+
+	if (!dmac)
+		return -EINVAL;
+
+	switch (cmd) {
+	case DMA_TERMINATE_ALL:
+		flush_channel(dmac);
+		break;
+
+	default:
+		return -EOPNOTSUPP;
+	}
+
+	return 0;
+}
+
+static int setup_channel(struct gpdma_channel *dmac, struct device_node *child)
+{
+	struct gpdma_engine *engine = dmac->engine;
+	int rc;
+
+	dmac->base = engine->iobase + dmac->id * engine->chip->chregs_offset;
+	dev_dbg(engine->dev, "channel%d base @ %p\n", dmac->id, dmac->base);
+
+	/* Find the IRQ line, if it exists in the device tree */
+	dmac->irq = irq_of_parse_and_map(child, 0);
+	dev_dbg(engine->dev, "channel %d, irq %d\n", dmac->id, dmac->irq);
+	rc = devm_request_irq(engine->dev, dmac->irq, gpdma_isr, 0,
+			      "lsi-dma", dmac);
+	if (rc) {
+		dev_err(engine->dev, "failed to request_irq, error = %d\n", rc);
+		return rc;
+	}
+	/* Initialize the virt-channel */
+	dmac->vc.desc_free = free_descriptor;
+	vchan_init(&dmac->vc, &engine->dma_device);
+
+	return 0;
+}
+
+static struct lsidma_hw lsi_dma32 = {
+	.num_channels   = 2,
+	.chregs_offset  = 0x80,
+	.genregs_offset = 0xF00,
+	.flags          = (LSIDMA_NEXT_FULL |
+			   LSIDMA_SEG_REGS)
+};
+
+static struct lsidma_hw lsi_dma31 = {
+	.num_channels   = 4,
+	.chregs_offset  = 0x40,
+	.genregs_offset = 0x400,
+	.flags          = 0
+};
+
+static const struct of_device_id gpdma_of_ids[] = {
+	{
+		.compatible = "lsi,dma32",
+		.data       = &lsi_dma32
+	},
+	{
+		.compatible = "lsi,dma31",
+		.data       = &lsi_dma31
+	},
+	{
+		.compatible = "gp-dma,acp-dma",
+		.data       = &lsi_dma31
+	},
+	{
+		.compatible = "gp-dma,acp-gpdma",
+		.data       = &lsi_dma31
+	},
+	{ }
+};
+
+static int gpdma_of_probe(struct platform_device *op)
+{
+	struct gpdma_engine *engine;
+	struct dma_device   *dma;
+	struct device_node *child;
+	struct resource *res;
+	const struct of_device_id *match;
+	int rc = -ENOMEM;
+	int id = 0;
+
+	match = of_match_device(gpdma_of_ids, &op->dev);
+	if (!match)
+		return -EINVAL;
+
+	engine = devm_kzalloc(&op->dev, sizeof(*engine), GFP_KERNEL);
+	if (!engine)
+		return -ENOMEM;
+
+	spin_lock_init(&engine->lock);
+	engine->dev = &op->dev;
+	engine->chip = (struct lsidma_hw *)match->data;
+
+	/* Initialize dma_device struct */
+	dma = &engine->dma_device;
+	dma->dev = &op->dev;
+	dma_cap_zero(dma->cap_mask);
+	dma_cap_set(DMA_MEMCPY, dma->cap_mask);
+	dma_cap_set(DMA_SG, dma->cap_mask);
+	dma->copy_align = 2;
+	dma->chancnt = engine->chip->num_channels;
+	dma->device_alloc_chan_resources = gpdma_alloc_chan_resources;
+	dma->device_free_chan_resources = gpdma_free_chan_resources;
+	dma->device_tx_status = gpdma_tx_status;
+	dma->device_prep_dma_memcpy = gpdma_prep_memcpy;
+	dma->device_prep_dma_sg = gpdma_prep_sg;
+	dma->device_issue_pending = gpdma_issue_pending;
+	dma->device_control = gpdma_device_control;
+	INIT_LIST_HEAD(&dma->channels);
+
+	/* Map device I/O memory
+	 */
+	res = platform_get_resource(op, IORESOURCE_MEM, 0);
+	engine->iobase = devm_ioremap_resource(&op->dev, res);
+	if (IS_ERR(engine->iobase))
+		return PTR_ERR(engine->iobase);
+	dev_dbg(&op->dev, "mapped base @ %p\n", engine->iobase);
+
+	res = platform_get_resource(op, IORESOURCE_MEM, 1);
+	if (res) {
+		engine->gpreg = devm_ioremap_nocache(&op->dev,
+						     res->start,
+						     resource_size(res));
+		if (IS_ERR(engine->gpreg))
+			return PTR_ERR(engine->gpreg);
+		dev_dbg(&op->dev, "mapped gpreg @ %p\n", engine->gpreg);
+	}
+
+	engine->err_irq = platform_get_irq(op, 1);
+	if (engine->err_irq) {
+		rc = devm_request_irq(&op->dev, engine->err_irq,
+				      gpdma_isr_err, 0, "lsi-dma-err", engine);
+		if (rc) {
+			dev_err(engine->dev, "failed to request irq%d\n",
+				engine->err_irq);
+			engine->err_irq = 0;
+		}
+	}
+
+	/* General registes at device specific offset */
+	engine->gbase = engine->iobase + engine->chip->genregs_offset;
+
+	rc = alloc_desc_table(engine);
+	if (rc)
+		return rc;
+
+	/* Setup channels */
+	for_each_child_of_node(op->dev.of_node, child) {
+		struct gpdma_channel *dmac = &engine->channel[id];
+
+		if (id >= engine->chip->num_channels) {
+			dev_dbg(engine->dev, "Too many channels (%d)\n", id);
+			return -ENODEV;
+		}
+
+		dmac->id = id;
+		dmac->engine = engine;
+		rc = setup_channel(dmac, child);
+		if (rc)
+			return rc;
+		++id;
+	}
+
+	soft_reset(engine);
+
+	rc = dma_async_device_register(&engine->dma_device);
+	if (rc) {
+		dev_err(engine->dev, "unable to register\n");
+		return rc;
+	}
+
+	device_create_file(&op->dev, &dev_attr_soft_reset);
+	dev_set_drvdata(&op->dev, engine);
+
+	return 0;
+}
+
+static int gpdma_of_remove(struct platform_device *op)
+{
+	struct gpdma_engine *engine = dev_get_drvdata(&op->dev);
+
+	dev_dbg(&op->dev, "%s\n", __func__);
+
+	device_remove_file(&op->dev, &dev_attr_soft_reset);
+	dma_async_device_unregister(&engine->dma_device);
+	free_desc_table(engine);
+	dev_set_drvdata(&op->dev, NULL);
+
+	return 0;
+}
+
+static struct platform_driver gpdma_of_driver = {
+	.driver = {
+		.name           = "lsi-dma32",
+		.owner          = THIS_MODULE,
+		.of_match_table = gpdma_of_ids,
+	},
+	.probe  = gpdma_of_probe,
+	.remove = gpdma_of_remove,
+};
+
+module_platform_driver(gpdma_of_driver);
+
+MODULE_DESCRIPTION("LSI DMA driver");
+MODULE_LICENSE("GPL");
diff --git a/drivers/dma/lsi-dma32.h b/drivers/dma/lsi-dma32.h
new file mode 100644
index 0000000..c5a2701
--- /dev/null
+++ b/drivers/dma/lsi-dma32.h
@@ -0,0 +1,221 @@
+/*
+ * Copyright (C) 2012 Ericsson AB. All rights reserved.
+ *
+ * Author:
+ *   Kerstin Jonsson <kerstin.jonsson@ericsson.com>, Feb 2012
+ *
+ * This is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ */
+#ifndef __LSI_DMA32_H
+#define __LSI_DMA32_H
+
+#include <linux/dmaengine.h>
+#include <linux/dma-mapping.h>
+#include <linux/dma-mapping.h>
+#include <linux/device.h>
+#include <linux/spinlock.h>
+#include <linux/types.h>
+#include "virt-dma.h"
+
+#define MAX_GPDMA_CHANNELS      4
+#define GPDMA_MAX_DESCRIPTORS  128
+#define GPDMA_MAGIC            0xABCD1234UL
+
+#define DMA_X_SRC_COUNT				0x00
+#define DMA_Y_SRC_COUNT				0x04
+#define DMA_X_MODIF_SRC				0x08
+#define DMA_Y_MODIF_SRC				0x0c
+#define DMA_SRC_CUR_ADDR			0x10
+#define DMA_SRC_ACCESS				0x14
+#define    DMA_SRC_ACCESS_BURST_TYPE		(1<<15)
+#define    DMA_SRC_ACCESS_TAIL_LENGTH(x)	(((x) & 0xF) << 11)
+#define    DMA_SRC_ACCESS_ROTATOR_LENGTH(x)	(((x) & 1F) << 6)
+#define    DMA_SRC_ACCESS_SRC_SIZE(x)		(((x) & 7) << 3)
+#define    DMA_SRC_ACCESS_SRC_BURST(x)		(((x) & 7) << 0)
+#define DMA_SRC_MASK				0x18
+#define DMA_X_DST_COUNT				0x1c
+#define DMA_Y_DST_COUNT				0x20
+#define DMA_X_MODIF_DST				0x24
+#define DMA_Y_MODIF_DST				0x28
+#define DMA_DST_CUR_ADDR			0x2C
+#define DMA_DST_ACCESS				0x30
+#define    DMA_DST_ACCESS_DST_SIZE(x)		(((x) & 7) << 3)
+#define    DMA_DST_ACCESS_DST_BURST(x)		(((x) & 7) << 0)
+#define DMA_NXT_DESCR				0x34
+#define DMA_CHANNEL_CONFIG			0x38
+#define    DMA_CONFIG_DST_SPACE(x)		(((x) & 7) << 26)
+#define    DMA_CONFIG_SRC_SPACE(x)		(((x) & 7) << 23)
+#define    DMA_CONFIG_PRIORITY_ROW		(1<<21)
+#define    DMA_CONFIG_PRIORITY			(1<<20)
+#define    DMA_CONFIG_LAST_BLOCK		(1<<15)
+#define    DMA_CONFIG_CLEAR_FIFO		(1<<14)
+#define    DMA_CONFIG_START_MEM_LOAD		(1<<13)
+#define    DMA_CONFIG_STOP_DST_EOB		(1<<11)
+#define    DMA_CONFIG_FULL_DESCR_ADDR		(1<<8)
+#define    DMA_CONFIG_INT_DST_EOT		(1<<7)
+#define    DMA_CONFIG_INT_DST_EOB		(1<<6)
+#define    DMA_CONFIG_WAIT_FOR_TASK_CNT2	(1<<5)
+#define    DMA_CONFIG_TASK_CNT2_RESET		(1<<4)
+#define    DMA_CONFIG_WAIT_FOR_TASK_CNT1	(1<<3)
+#define    DMA_CONFIG_TASK_CNT1_RESET		(1<<2)
+#define    DMA_CONFIG_TX_EN			(1<<1)
+#define    DMA_CONFIG_CHAN_EN			(1<<0)
+#define DMA_STATUS				0x3C
+#define    DMA_STATUS_WAIT_TASK_CNT2		(1<<20)
+#define    DMA_STATUS_TASK_CNT2_OVERFLOW	(1<<19)
+#define    DMA_STATUS_WAIT_TASK_CNT1		(1<<18)
+#define    DMA_STATUS_TASK_CNT1_OVERFLOW	(1<<17)
+#define    DMA_STATUS_CH_PAUS_WR_EN		(1<<16)
+#define    DMA_STATUS_ERR_ACC_DESCR		(1<<14)
+#define    DMA_STATUS_ERR_ACC_DST		(1<<13)
+#define    DMA_STATUS_ERR_ACC_SRC		(1<<12)
+#define    DMA_STATUS_ERR_OVERFLOW		(1<<9)
+#define    DMA_STATUS_ERR_UNDERFLOW		(1<<8)
+#define    DMA_STATUS_CH_PAUSE			(1<<7)
+#define    DMA_STATUS_CH_WAITING		(1<<5)
+#define    DMA_STATUS_CH_ACTIVE			(1<<4)
+#define    DMA_STATUS_TR_COMPLETE		(1<<3)
+#define    DMA_STATUS_BLK_COMPLETE		(1<<2)
+#define    DMA_STATUS_UNALIGNED_READ		(1<<1)
+#define    DMA_STATUS_UNALIGNED_WRITE		(1<<0)
+#define    DMA_STATUS_UNALIGNED_ERR		(DMA_STATUS_UNALIGNED_READ | \
+						 DMA_STATUS_UNALIGNED_WRITE)
+#define DMA_TASK_CNT_1				0x40
+#define DMA_TASK_CNT_2				0x44
+#define DMA_MODE_CONFIG				0x48
+#define DMA_CURR_DESCR				0x4c
+#define DMA_PREV_DESCR				0x50
+#define DMA_SRC_ADDR_SEG			0x54
+#define DMA_DST_ADDR_SEG			0x58
+#define DMA_DESCR_ADDR_SEG			0x5c
+
+#define DMA_STATUS_ERROR		(DMA_STATUS_ERR_ACC_DESCR | \
+					 DMA_STATUS_ERR_ACC_DST   | \
+					 DMA_STATUS_ERR_ACC_SRC   | \
+					 DMA_STATUS_ERR_OVERFLOW  | \
+					 DMA_STATUS_ERR_UNDERFLOW | \
+					 DMA_STATUS_UNALIGNED_ERR)
+
+#define DMA_STATUS_CLEAR		(DMA_STATUS_CH_PAUS_WR_EN | \
+					 DMA_STATUS_TR_COMPLETE   | \
+					 DMA_STATUS_BLK_COMPLETE)
+
+#define DMA_CONFIG_END			(DMA_CONFIG_LAST_BLOCK | \
+					 DMA_CONFIG_INT_DST_EOT)
+
+#define DMA_CONFIG_ONE_SHOT(__ext)	(DMA_CONFIG_DST_SPACE((__ext)) | \
+					 DMA_CONFIG_SRC_SPACE((__ext)) | \
+					 DMA_CONFIG_TX_EN              | \
+					 DMA_CONFIG_CHAN_EN)
+
+#define DMA_CONFIG_DSC_LOAD		(DMA_CONFIG_START_MEM_LOAD  | \
+					 DMA_CONFIG_FULL_DESCR_ADDR | \
+					 DMA_CONFIG_CHAN_EN)
+
+#define GEN_STAT       0x0
+#define   GEN_STAT_CH0_ACTIVE (1<<0)
+#define   GEN_STAT_CH1_ACTIVE (1<<2)
+#define   GEN_STAT_CH1_ACTIVE (1<<2)
+#define   GEN_STAT_CH0_ERROR  (1<<16)
+#define   GEN_STAT_CH1_ERROR  (1<<17)
+#define GEN_CONFIG     0x4
+#define  GEN_CONFIG_EXT_MEM                     (1<<19)
+#define  GEN_CONFIG_INT_EDGE(_ch)               (1<<(_ch))
+#define SOFT_RESET     0x8
+
+#define GPDMA_GEN_STAT(__p) ((__p)->gbase + GEN_STAT)
+#define GPDMA_GEN_CONFIG(__p) ((__p)->gbase + GEN_CONFIG)
+#define GPDMA_SOFT_RESET(__p) ((__p)->gbase + SOFT_RESET)
+
+
+struct descriptor {
+	u16 src_x_ctr;
+	u16 src_y_ctr;
+	s32 src_x_mod;
+	s32 src_y_mod;
+	u32 src_addr;
+	u32 src_data_mask;
+	u16 src_access;
+	u16 dst_access;
+	u32 ch_config;
+	u32 next_ptr;
+	u16 dst_x_ctr;
+	u16 dst_y_ctr;
+	s32 dst_x_mod;
+	s32 dst_y_mod;
+	u32 dst_addr;
+} __aligned(32);
+
+struct gpdma_engine;
+
+struct gpdma_desc {
+	struct descriptor               hw;
+	struct gpdma_desc              *chain;
+	dma_addr_t                      src;
+	dma_addr_t                      dst;
+	struct gpdma_engine            *engine;
+	struct virt_dma_desc	        vdesc;
+} __aligned(32);
+
+static struct gpdma_desc *to_gpdma_desc(struct virt_dma_desc *vdesc)
+{
+	return container_of(vdesc, struct gpdma_desc, vdesc);
+}
+
+struct gpdma_channel {
+	/* Back reference to DMA engine */
+	struct gpdma_engine		*engine;
+	/* Channel registers */
+	void __iomem			*base;
+	/* Channel id */
+	int			        id;
+	/* IRQ number as passed to request_irq() */
+	int				irq;
+	/* Currently running descriptor */
+	struct gpdma_desc               *active;
+	/* Channel parameters (DMA engine framework) */
+	struct virt_dma_chan		vc;
+};
+
+static inline struct gpdma_channel *to_gpdma_chan(struct dma_chan *chan)
+{
+	return container_of(chan, struct gpdma_channel, vc.chan);
+}
+
+struct lsidma_hw {
+	unsigned int num_channels;
+	unsigned int chregs_offset;
+	unsigned int genregs_offset;
+	unsigned int flags;
+#define LSIDMA_NEXT_FULL     (1<<0)
+#define LSIDMA_SEG_REGS      (1<<1)
+#define LSIDMA_EDGE_INT      (1<<2)
+};
+
+struct gpdma_engine {
+	struct device			*dev;
+	struct lsidma_hw		*chip;
+	struct gpdma_channel		channel[MAX_GPDMA_CHANNELS];
+	/** Bit mask where bit[n] == 1 if channel busy */
+	unsigned long                   ch_busy;
+	int                             err_irq;
+	void __iomem			*iobase;
+	void __iomem			*gbase;
+	void __iomem			*gpreg;
+	spinlock_t			lock;
+	struct list_head                free_list;
+	struct {
+		u32                     order;
+		dma_addr_t              phys;
+		struct gpdma_desc       *va;
+	} pool;
+	struct dma_device		dma_device;
+};
+
+#define desc_to_engine(n) container_of(n, struct gpdma_engine, desc)
+
+#endif
diff --git a/drivers/edac/Kconfig b/drivers/edac/Kconfig
index cb59619..9ba8b3a 100644
--- a/drivers/edac/Kconfig
+++ b/drivers/edac/Kconfig
@@ -376,6 +376,30 @@ config EDAC_OCTEON_PCI
 	  Support for error detection and correction on the
 	  Cavium Octeon family of SOCs.
 
+config EDAC_AXXIA_SYSMEM
+	tristate "AXXIA EDAC SysMem Controller"
+	help
+	  Support for System Memory Denali controller error
+	  detection on the AXXIA AXM55xx devices. This enables
+	  the System Memory error detection. System Memory error
+	  detection is interrupt driven.
+
+config EDAC_AXXIA_L3
+	tristate "AXXIA EDAC L3 Controller"
+	help
+	  Support for the eight L3 caches error detection
+	  on the AXXIA AXM55xx devices. This enables the
+	  L3 cache error detection. L3 cache error detection
+	  uses polling mechanism.
+
+config EDAC_AXXIA_L2_CPU
+	tristate "AXXIA EDAC L2/CPU Controller"
+	help
+	  Support for L2 cache and A15 CPU error detection
+	  on AXXIA AXM55xx devices. This enables the L2
+	  cache and A15 CPU error detction. L2 cache and A15
+	  CPU error detection uses polling mechanism.
+
 config EDAC_ALTERA_MC
 	tristate "Altera SDRAM Memory Controller EDAC"
 	depends on EDAC_MM_EDAC && ARCH_SOCFPGA
diff --git a/drivers/edac/Makefile b/drivers/edac/Makefile
index b255f36..55485e8 100644
--- a/drivers/edac/Makefile
+++ b/drivers/edac/Makefile
@@ -16,6 +16,9 @@ ifdef CONFIG_PCI
 edac_core-y	+= edac_pci.o edac_pci_sysfs.o
 endif
 
+obj-$(CONFIG_EDAC_AXXIA_SYSMEM)	+= axxia_edac-mc.o
+obj-$(CONFIG_EDAC_AXXIA_L3)		+= axxia_edac-l3.o
+obj-$(CONFIG_EDAC_AXXIA_L2_CPU)	+= axxia_edac-l2_cpu.o
 obj-$(CONFIG_EDAC_GHES)			+= ghes_edac.o
 obj-$(CONFIG_EDAC_MCE_INJ)		+= mce_amd_inj.o
 
diff --git a/drivers/edac/axxia_edac-l2_cpu.c b/drivers/edac/axxia_edac-l2_cpu.c
new file mode 100644
index 0000000..534562d
--- /dev/null
+++ b/drivers/edac/axxia_edac-l2_cpu.c
@@ -0,0 +1,333 @@
+ /*
+  * drivers/edac/axxia_edac-l2_cpu.c
+  *
+  * EDAC Driver for Avago's Axxia 5500 A15 CPUs and L2 caches
+  *
+  * Copyright (C) 2010 LSI Inc.
+  *
+  * This program is free software; you can redistribute it and/or modify
+  * it under the terms of the GNU General Public License as published by
+  * the Free Software Foundation; either version 2 of the License, or
+  * (at your option) any later version.
+  *
+  * This program is distributed in the hope that it will be useful,
+  * but WITHOUT ANY WARRANTY; without even the implied warranty of
+  * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+  * GNU General Public License for more details.
+  *
+  * You should have received a copy of the GNU General Public License
+  * along with this program; if not, write to the Free Software
+  * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
+  *
+  */
+
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/slab.h>
+#include <linux/io.h>
+#include <linux/edac.h>
+#include <mach/ncr.h>
+#include <linux/of_platform.h>
+#include <linux/of.h>
+#include <linux/of_address.h>
+#include <linux/platform_device.h>
+#include <linux/reboot.h>
+#include <linux/mfd/syscon.h>
+#include <linux/regmap.h>
+#include "edac_core.h"
+#include "edac_module.h"
+
+#define LSI_EDAC_MOD_STR     "lsi_edac"
+#define CORES_PER_CLUSTER 4
+
+#define APB2_PERSIST_SCRATCH 0xdc
+#define L2_PERSIST_SCRATCH_BIT (0x1 << 5)
+#define CPU_PERSIST_SCRATCH_BIT (0x1 << 6)
+
+/* Private structure for common edac device */
+struct lsi_edac_dev_info {
+	struct platform_device *pdev;
+	char *ctl_name;
+	char *blk_name;
+	int edac_idx;
+	struct regmap *syscon;
+	struct edac_device_ctl_info *edac_dev;
+	void (*check)(struct edac_device_ctl_info *edac_dev);
+};
+
+static inline u64 read_cpumerrsr(void)
+{
+	u64 val;
+
+	asm volatile("mrrc\tp15, 0, %Q0, %R0, c15" : "=r" (val));
+	return val;
+}
+
+static inline u64 read_l2merrsr(void)
+{
+	u64 val;
+
+	asm volatile("mrrc\tp15, 1, %Q0, %R0, c15" : "=r"(val));
+	return val;
+}
+
+inline void write_cpumerrsr(u64 val)
+{
+	asm volatile("mcrr\tp15, 0, %Q0, %R0, c15" : : "r" (val));
+}
+
+inline void write_l2merrsr(u64 val)
+{
+	asm volatile("mcrr\tp15, 1, %Q0, %R0, c15" : : "r"(val));
+}
+
+void log_cpumerrsr(void *edac)
+{
+	struct edac_device_ctl_info *edac_dev = edac;
+	u64 val, clear_val;
+	u32 count0, count1;
+	int i;
+	struct lsi_edac_dev_info *dev_info;
+
+	dev_info = edac_dev->pvt_info;
+
+	/* Read cp15 for CPUMERRSR counts */
+	val = read_cpumerrsr();
+	if (val & 0x80000000) {
+		int cpu = get_cpu();
+
+		count0 = ((val) & 0x000000ff00000000) >> 20;
+		count1 = ((val) & 0x0000ff0000000000) >> 28;
+
+		/* increment correctable error counts */
+		for (i = 0; i < count0+count1; i++) {
+			edac_device_handle_ce(edac_dev, 0,
+				cpu, edac_dev->ctl_name);
+		}
+
+		/* Clear the valid bit */
+		clear_val = 0x80000000;
+		write_cpumerrsr(clear_val);
+		put_cpu();
+	}
+	if (val & 0x8000000000000000) {
+		regmap_update_bits(dev_info->syscon,
+				   APB2_PERSIST_SCRATCH,
+				   CPU_PERSIST_SCRATCH_BIT,
+				   CPU_PERSIST_SCRATCH_BIT);
+		pr_emerg("CPU uncorrectable error\n");
+		machine_restart(NULL);
+	}
+}
+
+
+/* Check for CPU Errors */
+static void lsi_cpu_error_check(struct edac_device_ctl_info *edac_dev)
+{
+	/* execute on current cpu */
+	log_cpumerrsr(edac_dev);
+
+	/* send ipis to execute on other cpus */
+	smp_call_function(log_cpumerrsr, edac_dev, 1);
+
+}
+
+void log_l2merrsr(void *edac)
+{
+	struct edac_device_ctl_info *edac_dev = edac;
+	u64 val, clear_val;
+	u32 count0, count1;
+	int i;
+	struct lsi_edac_dev_info *dev_info;
+
+	dev_info = edac_dev->pvt_info;
+
+	val = read_l2merrsr();
+	if (val & 0x80000000) {
+		int cpu = get_cpu();
+
+		count0 = ((val) & 0x000000ff00000000) >> 20;
+		count1 = ((val) & 0x0000ff0000000000) >> 28;
+
+		/* increment correctable error counts */
+		for (i = 0; i < count0+count1; i++) {
+			edac_device_handle_ce(edac_dev, 0,
+				cpu/CORES_PER_CLUSTER,
+				edac_dev->ctl_name);
+		}
+
+		/* Clear the valid bit */
+		clear_val = 0x80000000;
+		write_l2merrsr(clear_val);
+		put_cpu();
+	}
+	if (val & 0x8000000000000000) {
+		regmap_update_bits(dev_info->syscon,
+				   APB2_PERSIST_SCRATCH,
+				   L2_PERSIST_SCRATCH_BIT,
+				   L2_PERSIST_SCRATCH_BIT);
+		pr_emerg("L2 uncorrectable error\n");
+		machine_restart(NULL);
+	}
+}
+
+/* Check for L2 Errors */
+static void lsi_l2_error_check(struct edac_device_ctl_info *edac_dev)
+{
+	/* 4 cores per cluster */
+	int nr_cluster_ids = ((nr_cpu_ids - 1) / CORES_PER_CLUSTER) + 1;
+	int i, j, cpu;
+
+	/* execute on current cpu */
+	log_l2merrsr(edac_dev);
+
+	for (i = 0; i < nr_cluster_ids; i++) {
+		/* No need to run on local cluster. */
+		if (i == (get_cpu() / CORES_PER_CLUSTER)) {
+			put_cpu();
+			continue;
+		}
+		/*
+		 * Have some core in each cluster execute this,
+		 * Start with the first core on that cluster.
+		 */
+		cpu = i * CORES_PER_CLUSTER;
+		for (j = cpu; j < cpu + CORES_PER_CLUSTER; j++) {
+			if (cpu_online(j)) {
+				smp_call_function_single(j, log_l2merrsr,
+					edac_dev, 1);
+				break;
+			}
+		}
+		put_cpu();
+	}
+}
+
+static void lsi_add_edac_devices(struct platform_device *pdev,
+	int num)
+{
+	struct lsi_edac_dev_info *dev_info = NULL;
+	/* 4 cores per cluster */
+	int nr_cluster_ids = ((nr_cpu_ids - 1) / CORES_PER_CLUSTER) + 1;
+	struct device_node *np = pdev->dev.of_node;
+
+	dev_info = devm_kzalloc(&pdev->dev, sizeof(*dev_info), GFP_KERNEL);
+	if (!dev_info)
+		return;
+
+	dev_info->ctl_name = kstrdup(np->name, GFP_KERNEL);
+	if (num == 0) {
+		dev_info->blk_name = "cpumerrsr";
+		dev_info->check = lsi_cpu_error_check;
+	} else {
+		dev_info->blk_name = "l2merrsr";
+		dev_info->check = lsi_l2_error_check;
+	}
+	dev_info->pdev = pdev;
+	dev_info->edac_idx = edac_device_alloc_index();
+	dev_info->syscon =
+			syscon_regmap_lookup_by_phandle(np, "syscon");
+	if (IS_ERR(dev_info->syscon)) {
+		pr_info("%s: syscon lookup failed\n", np->name);
+		goto err1;
+	}
+
+	if (strcmp(dev_info->ctl_name, "edac_cpu") == 0) {
+		dev_info->edac_dev =
+		edac_device_alloc_ctl_info(0, dev_info->ctl_name,
+		1, dev_info->blk_name, num_possible_cpus(), 0, NULL,
+		0, dev_info->edac_idx);
+	} else if (strcmp(dev_info->ctl_name, "edac_l2") == 0) {
+		dev_info->edac_dev =
+		edac_device_alloc_ctl_info(0, dev_info->ctl_name,
+			1, dev_info->blk_name, nr_cluster_ids, 0, NULL,
+			0, dev_info->edac_idx);
+	}
+	if (!dev_info->edac_dev) {
+		pr_info("No memory for edac device\n");
+		goto err1;
+	}
+
+	dev_info->edac_dev->pvt_info = dev_info;
+	dev_info->edac_dev->dev = &dev_info->pdev->dev;
+	dev_info->edac_dev->ctl_name = dev_info->ctl_name;
+	dev_info->edac_dev->mod_name = LSI_EDAC_MOD_STR;
+	dev_info->edac_dev->dev_name = dev_name(&dev_info->pdev->dev);
+
+	dev_info->edac_dev->edac_check = dev_info->check;
+
+	if (edac_device_add_device(dev_info->edac_dev) != 0) {
+		pr_info("Unable to add edac device for %s\n",
+				dev_info->ctl_name);
+		goto err2;
+	}
+
+	return;
+err2:
+	edac_device_free_ctl_info(dev_info->edac_dev);
+err1:
+	platform_device_unregister(dev_info->pdev);
+}
+
+static int lsi_edac_cpu_probe(struct platform_device *pdev)
+{
+	edac_op_state = EDAC_OPSTATE_POLL;
+	lsi_add_edac_devices(pdev, 0);
+	return 0;
+}
+
+static int lsi_edac_cpu_remove(struct platform_device *pdev)
+{
+	platform_device_unregister(pdev);
+	return 0;
+}
+
+static int lsi_edac_l2_probe(struct platform_device *pdev)
+{
+	edac_op_state = EDAC_OPSTATE_POLL;
+	lsi_add_edac_devices(pdev, 1);
+	return 0;
+}
+
+static int lsi_edac_l2_remove(struct platform_device *pdev)
+{
+	platform_device_unregister(pdev);
+	return 0;
+}
+
+static struct of_device_id lsi_edac_l2_match[] = {
+	{
+	.compatible = "lsi,cortex-a15-l2-cache",
+	},
+	{},
+};
+
+static struct platform_driver lsi_edac_l2_driver = {
+	.probe = lsi_edac_l2_probe,
+	.remove = lsi_edac_l2_remove,
+	.driver = {
+		.name = "lsi_edac_l2",
+		.of_match_table = lsi_edac_l2_match,
+	}
+};
+static struct of_device_id lsi_edac_cpu_match[] = {
+	{
+	.compatible = "lsi,cortex-a15-cpu",
+	},
+	{},
+};
+
+static struct platform_driver lsi_edac_cpu_driver = {
+	.probe = lsi_edac_cpu_probe,
+	.remove = lsi_edac_cpu_remove,
+	.driver = {
+		.name = "lsi_edac_cpu",
+		.of_match_table = lsi_edac_cpu_match,
+	}
+};
+
+module_platform_driver(lsi_edac_cpu_driver);
+module_platform_driver(lsi_edac_l2_driver);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Sangeetha Rao <sangeetha.rao@avagotech.com>");
diff --git a/drivers/edac/axxia_edac-l3.c b/drivers/edac/axxia_edac-l3.c
new file mode 100644
index 0000000..2732490
--- /dev/null
+++ b/drivers/edac/axxia_edac-l3.c
@@ -0,0 +1,185 @@
+ /*
+  * drivers/edac/axxia_edac-l3.c
+  *
+  * EDAC Driver for Avago's Axxia 5500 for L3 cache
+  *
+  * Copyright (C) 2010 LSI Inc.
+  *
+  * This program is free software; you can redistribute it and/or modify
+  * it under the terms of the GNU General Public License as published by
+  * the Free Software Foundation; either version 2 of the License, or
+  * (at your option) any later version.
+  *
+  * This program is distributed in the hope that it will be useful,
+  * but WITHOUT ANY WARRANTY; without even the implied warranty of
+  * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+  * GNU General Public License for more details.
+  *
+  * You should have received a copy of the GNU General Public License
+  * along with this program; if not, write to the Free Software
+  * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
+  *
+  */
+
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/slab.h>
+#include <linux/io.h>
+#include <linux/edac.h>
+#include <mach/ncr.h>
+#include <linux/of_platform.h>
+#include <linux/of.h>
+#include <linux/of_address.h>
+#include <linux/platform_device.h>
+#include <linux/reboot.h>
+#include <linux/mfd/syscon.h>
+#include <linux/regmap.h>
+#include "edac_core.h"
+#include "edac_module.h"
+
+#define LSI_EDAC_MOD_STR     "lsi_edac"
+
+#define APB2_PERSIST_SCRATCH 0xdc
+#define L3_PERSIST_SCRATCH_BIT (0x1 << 4)
+
+/* Private structure for common edac device */
+struct lsi_edac_dev_info {
+	struct platform_device *pdev;
+	char *ctl_name;
+	char *blk_name;
+	int edac_idx;
+	struct regmap *syscon;
+	void __iomem *dickens_L3;
+	struct edac_device_ctl_info *edac_dev;
+	void (*check)(struct edac_device_ctl_info *edac_dev);
+};
+
+/* Check for L3 Errors */
+static void lsi_l3_error_check(struct edac_device_ctl_info *edac_dev)
+{
+	unsigned long regVal1, regVal2;
+	unsigned count = 0;
+	int i, instance;
+	struct lsi_edac_dev_info *dev_info;
+
+	dev_info = (struct lsi_edac_dev_info *) edac_dev->pvt_info;
+
+	for (instance = 0; instance < 8; instance++) {
+		regVal1 = readl(dev_info->dickens_L3 + (instance * 0x10000));
+		regVal2 = readl(dev_info->dickens_L3 +
+			(instance * 0x10000) + 4);
+		/* First error valid */
+		if (regVal2 & 0x40000000) {
+			if (regVal2 & 0x30000000) {
+				regmap_update_bits(dev_info->syscon,
+						   APB2_PERSIST_SCRATCH,
+						   L3_PERSIST_SCRATCH_BIT,
+						   L3_PERSIST_SCRATCH_BIT);
+				/* Fatal error */
+				pr_emerg("L3 uncorrectable error\n");
+				machine_restart(NULL);
+			}
+			count = (regVal2 & 0x07fff800) >> 11;
+			for (i = 0; i < count; i++)
+				edac_device_handle_ce(edac_dev, 0,
+					instance, edac_dev->ctl_name);
+			/* clear the valid bit */
+			writel(0x48000000, dev_info->dickens_L3 +
+				(instance * 0x10000) + 84);
+		}
+	}
+}
+
+static int lsi_edac_l3_probe(struct platform_device *pdev)
+{
+	struct lsi_edac_dev_info *dev_info = NULL;
+	struct device_node *np = pdev->dev.of_node;
+	struct resource *r;
+
+	dev_info = devm_kzalloc(&pdev->dev, sizeof(*dev_info), GFP_KERNEL);
+	if (!dev_info)
+		return -ENOMEM;
+
+	dev_info->ctl_name = kstrdup(np->name, GFP_KERNEL);
+	dev_info->blk_name = "l3merrsr";
+	dev_info->pdev = pdev;
+	dev_info->edac_idx = edac_device_alloc_index();
+
+	r = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!r) {
+		pr_err("Unable to get mem resource\n");
+		goto err1;
+	}
+
+	dev_info->dickens_L3 = devm_ioremap(&pdev->dev, r->start,
+					    resource_size(r));
+	if (!dev_info->dickens_L3) {
+		pr_err("LSI_L3 devm_ioremap error\n");
+		goto err1;
+	}
+
+	dev_info->syscon =
+		syscon_regmap_lookup_by_phandle(np, "syscon");
+	if (IS_ERR(dev_info->syscon)) {
+		pr_info("%s: syscon lookup failed\n",
+			np->name);
+		goto err1;
+	}
+	dev_info->edac_dev =
+		edac_device_alloc_ctl_info(0, dev_info->ctl_name,
+					   1, dev_info->blk_name,
+					   8, 0, NULL, 0,
+					   dev_info->edac_idx);
+	if (!dev_info->edac_dev) {
+		pr_info("No memory for edac device\n");
+		goto err1;
+	}
+
+	dev_info->edac_dev->pvt_info = dev_info;
+	dev_info->edac_dev->dev = &dev_info->pdev->dev;
+	dev_info->edac_dev->ctl_name = dev_info->ctl_name;
+	dev_info->edac_dev->mod_name = LSI_EDAC_MOD_STR;
+	dev_info->edac_dev->dev_name = dev_name(&dev_info->pdev->dev);
+	edac_op_state = EDAC_OPSTATE_POLL;
+	dev_info->edac_dev->edac_check = lsi_l3_error_check;
+
+	if (edac_device_add_device(dev_info->edac_dev) != 0) {
+		pr_info("Unable to add edac device for %s\n",
+				dev_info->ctl_name);
+		goto err2;
+	}
+
+	return 0;
+err2:
+	edac_device_free_ctl_info(dev_info->edac_dev);
+err1:
+	platform_device_unregister(dev_info->pdev);
+	return 1;
+}
+
+static int lsi_edac_l3_remove(struct platform_device *pdev)
+{
+	platform_device_unregister(pdev);
+	return 0;
+}
+
+static struct of_device_id lsi_edac_l3_match[] = {
+	{
+	.compatible = "lsi,ccn504-l3-cache",
+	},
+	{},
+};
+
+static struct platform_driver lsi_edac_l3_driver = {
+	.probe = lsi_edac_l3_probe,
+	.remove = lsi_edac_l3_remove,
+	.driver = {
+		.name = "lsi_edac_l3",
+		.of_match_table = lsi_edac_l3_match,
+	}
+};
+
+module_platform_driver(lsi_edac_l3_driver);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Sangeetha Rao <sangeetha.rao@avagotech.com>");
diff --git a/drivers/edac/axxia_edac-mc.c b/drivers/edac/axxia_edac-mc.c
new file mode 100644
index 0000000..643ed38
--- /dev/null
+++ b/drivers/edac/axxia_edac-mc.c
@@ -0,0 +1,351 @@
+ /*
+  * drivers/edac/axxia_edac-mc.c
+  *
+  * EDAC Driver for Avago's Axxia 5500 System Memory Controller
+  *
+  * Copyright (C) 2010 LSI Inc.
+  *
+  * This program is free software; you can redistribute it and/or modify
+  * it under the terms of the GNU General Public License as published by
+  * the Free Software Foundation; either version 2 of the License, or
+  * (at your option) any later version.
+  *
+  * This program is distributed in the hope that it will be useful,
+  * but WITHOUT ANY WARRANTY; without even the implied warranty of
+  * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+  * GNU General Public License for more details.
+  *
+  * You should have received a copy of the GNU General Public License
+  * along with this program; if not, write to the Free Software
+  * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
+  *
+  */
+
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/slab.h>
+#include <linux/io.h>
+#include <linux/edac.h>
+#include <mach/ncr.h>
+#include <linux/of_platform.h>
+#include <linux/of.h>
+#include <linux/of_address.h>
+#include <linux/of_irq.h>
+#include <linux/irq.h>
+#include <linux/platform_device.h>
+#include <linux/reboot.h>
+#include <linux/mfd/syscon.h>
+#include <linux/regmap.h>
+#include <linux/interrupt.h>
+#include "edac_core.h"
+#include "edac_module.h"
+
+#define LSI_EDAC_MOD_STR     "lsi_edac"
+
+#define APB2_SER3_PHY_ADDR        0x002010030000ULL
+#define APB2_SER3_PHY_SIZE   0x1000
+
+#define SM_INT_STATUS_REG 0x410
+#define SM_INT_STATUS_CLEAR_REG 0x548
+#define SM_INT_STATUS_MASK_REG 0x414
+
+#define APB2_PERSIST_SCRATCH 0xdc
+#define SMEM_PERSIST_SCRATCH_BIT (0x1 << 3)
+
+static int log = 1;
+module_param(log, int, S_IRUGO|S_IWUSR);
+MODULE_PARM_DESC(log, "Log each error to kernel log.");
+
+static int machine_restrt = 1;
+module_param(machine_restrt, int, S_IRUGO|S_IWUSR);
+MODULE_PARM_DESC(machine_restrt, "Machine restart on fatal error.");
+
+/*
+  AXM55xx memory controller interrupt status bits:
+
+  Bit [24] = The software-initiated control word write has completed.
+  Bit [23] = The user-initiated DLL resync has completed.
+  Bit [22] = A state change has been detected on the dfi_init_complete signal
+	     after initialization.
+  Bit [21] = The assertion of the INHIBIT_DRAM_CMD parameter has successfully
+	     inhibited the command queue.
+  Bit [20] = The register interface-initiated mode register write has completed
+	     and another mode register write may be issued.
+  Bit [19] = A parity error has been detected on the address/control bus on a
+	     registered DIMM.
+  Bit [18] = The leveling operation has completed.
+  Bit [17] = A leveling operation has been requested.
+  Bit [16] = A DFI update error has occurred. Error information can be found in
+	     the UPDATE_ERROR_STATUS parameter.
+  Bit [15] = A write leveling error has occurred. Error information can be found
+	     in the WRLVL_ERROR_STATUS parameter.
+  Bit [14] = A read leveling gate training error has occurred. Error information
+	     can be found in the RDLVL_ERROR_STATUS parameter.
+  Bit [13] = A read leveling error has occurred. Error information can be found
+	     in the RDLVL_ERROR_STATUS parameter.
+  Bit [12] = The user has programmed an invalid setting associated with user
+	     words per burst. Examples: Setting param_reduc when burst
+	     length = 2. A 1:2 MC:PHY clock ratio with burst length = 2.
+  Bit [11] = A wrap cycle crossing a DRAM page has been detected. This is
+	     unsupported & may result in memory data corruption.
+  Bit [10] = The BIST operation has been completed.
+  Bit [09] = The low power operation has been completed.
+  Bit [08] = The MC initialization has been completed.
+  Bit [07] = An error occurred on the port command channel.
+  Bit [06] = Multiple uncorrectable ECC events have been detected.
+  Bit [05] = An uncorrectable ECC event has been detected.
+  Bit [04] = Multiple correctable ECC events have been detected.
+  Bit [03] = A correctable ECC event has been detected.
+  Bit [02] = Multiple accesses outside the defined PHYSICAL memory space have
+	     occurred.
+  Bit [01] = A memory access outside the defined PHYSICAL memory space has
+	     occurred.
+  Bit [00] = The memory reset is valid on the DFI bus.
+
+  Of these, 1, 2, 3, 4, 5, 6, 7, 11, and 19 are of interest.
+*/
+#define SM_INT_MASK (0x1f7f719)
+
+enum events {
+	EV_ILLEGAL = 0,
+	EV_MULT_ILLEGAL,
+	EV_CORR_ECC,
+	EV_MULT_CORR_ECC,
+	EV_UNCORR_ECC,
+	EV_MULT_UNCORR_ECC,
+	EV_PORT_ERROR,
+	EV_WRAP_ERROR,
+	EV_PARITY_ERROR,
+	NR_EVENTS
+};
+
+static const u32 event_mask[NR_EVENTS] = {
+	[EV_ILLEGAL]          = 0x00000002,
+	[EV_MULT_ILLEGAL]     = 0x00000004,
+	[EV_CORR_ECC]         = 0x00000008,
+	[EV_MULT_CORR_ECC]    = 0x00000010,
+	[EV_UNCORR_ECC]       = 0x00000020,
+	[EV_MULT_UNCORR_ECC]  = 0x00000040,
+	[EV_PORT_ERROR]       = 0x00000080,
+	[EV_WRAP_ERROR]       = 0x00000800,
+	[EV_PARITY_ERROR]     = 0x00080000,
+};
+
+static const struct event_logging {
+	int         fatal;
+	const char *level;
+	const char *name;
+} event_logging[NR_EVENTS] = {
+	[EV_ILLEGAL]         = {0, KERN_ERR, "Illegal access"},
+	[EV_MULT_ILLEGAL]    = {0, KERN_ERR, "Illegal access"},
+	[EV_CORR_ECC]        = {0, KERN_NOTICE, "Correctable ECC error"},
+	[EV_MULT_CORR_ECC]   = {0, KERN_NOTICE, "Correctable ECC error"},
+	[EV_UNCORR_ECC]      = {1, KERN_CRIT, "Uncorrectable ECC error"},
+	[EV_MULT_UNCORR_ECC] = {1, KERN_CRIT, "Uncorrectable ECC error"},
+	[EV_PORT_ERROR]      = {0, KERN_CRIT, "Port error"},
+	[EV_WRAP_ERROR]      = {0, KERN_CRIT, "Wrap error"},
+	[EV_PARITY_ERROR]    = {0, KERN_CRIT, "Parity error"},
+};
+
+/* Private structure for common edac device */
+struct lsi_edac_dev_info {
+	struct platform_device *pdev;
+	char *ctl_name;
+	char *blk_name;
+	int edac_idx;
+	u32 sm_region;
+	struct regmap *syscon;
+	void __iomem *apb2ser3_region;
+	struct edac_device_ctl_info *edac_dev;
+	void (*check)(struct edac_device_ctl_info *edac_dev);
+};
+
+static irqreturn_t
+smmon_isr(int interrupt, void *device)
+{
+	struct lsi_edac_dev_info *edac_dev = device;
+	u32 status;
+	unsigned long set_val;
+	int i;
+
+	if (ncr_read(edac_dev->sm_region, SM_INT_STATUS_REG,
+		4, &status)) {
+		pr_err("%s: Error reading interrupt status\n",
+		       dev_name(&edac_dev->pdev->dev));
+		return IRQ_NONE;
+	}
+
+	for (i = 0; i < NR_EVENTS; ++i) {
+		if ((status & event_mask[i]) != 0) {
+			if (machine_restrt && event_logging[i].fatal) {
+				if (IS_ERR(edac_dev->syscon)) {
+					set_val = readl(
+						edac_dev->apb2ser3_region
+						+ APB2_PERSIST_SCRATCH);
+					/* set bit 3 in pscratch reg */
+					set_val = set_val
+						| SMEM_PERSIST_SCRATCH_BIT;
+					writel(set_val,
+						edac_dev->apb2ser3_region +
+						APB2_PERSIST_SCRATCH);
+				} else
+					regmap_update_bits(edac_dev->syscon,
+					APB2_PERSIST_SCRATCH,
+					SMEM_PERSIST_SCRATCH_BIT,
+					SMEM_PERSIST_SCRATCH_BIT);
+				pr_emerg("%s uncorrectable error\n",
+					edac_dev->ctl_name);
+				machine_restart(NULL);
+			}
+		}
+	}
+
+	/* Clear interrupt */
+	ncr_write(edac_dev->sm_region, SM_INT_STATUS_CLEAR_REG,
+		4, &status);
+
+	return IRQ_HANDLED;
+}
+
+static void lsi_sm_error_check(struct edac_device_ctl_info *edac_dev)
+{
+	unsigned long sm_reg_val, clear_val;
+	struct lsi_edac_dev_info *dev_info;
+
+	dev_info = (struct lsi_edac_dev_info *) edac_dev->pvt_info;
+
+	/* SM0 is instance 0 */
+	ncr_read(dev_info->sm_region, SM_INT_STATUS_REG, 4, &sm_reg_val);
+	if (sm_reg_val & 0x18) {
+		/* single bit and multiple bit correctable errors */
+		edac_device_handle_ce(edac_dev, 0, 0, edac_dev->ctl_name);
+		/* Clear bits */
+		clear_val = 0x18;
+		ncr_write(dev_info->sm_region, SM_INT_STATUS_CLEAR_REG,
+			4, &clear_val);
+	}
+}
+
+
+static int lsi_edac_mc_probe(struct platform_device *pdev)
+{
+	static int count;
+	struct lsi_edac_dev_info *dev_info = NULL;
+	/* 4 cores per cluster */
+	struct resource *io;
+	struct device_node *np = pdev->dev.of_node;
+	int irq = -1, rc = 0;
+	u32 mask;
+
+	dev_info = devm_kzalloc(&pdev->dev, sizeof(*dev_info), GFP_KERNEL);
+	if (!dev_info)
+		return -ENOMEM;
+
+	dev_info->ctl_name = kstrdup(np->name, GFP_KERNEL);
+	dev_info->blk_name = "ECC";
+	edac_op_state = EDAC_OPSTATE_POLL;
+
+	dev_info->pdev = pdev;
+	dev_info->edac_idx = edac_device_alloc_index();
+
+	io = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!io) {
+		dev_err(&pdev->dev, "Unable to get mem resource\n");
+		goto err1;
+	}
+	dev_info->sm_region = io->start;
+	dev_info->syscon =
+		syscon_regmap_lookup_by_phandle(np, "syscon");
+	if (IS_ERR(dev_info->syscon)) {
+		pr_info("%s: syscon lookup failed hence using hardcoded register address\n",
+			np->name);
+		dev_info->apb2ser3_region = ioremap(APB2_SER3_PHY_ADDR,
+			APB2_SER3_PHY_SIZE);
+		if (!dev_info->apb2ser3_region) {
+			pr_err("ioremap of apb2ser3 region failed\n");
+			goto err1;
+		}
+
+	}
+	/* Disable all memory controller interrupts */
+	mask = 0xffffffff;
+	ncr_write(dev_info->sm_region, SM_INT_STATUS_MASK_REG,
+		4, &mask);
+	irq = platform_get_irq(pdev, 0);
+	if (irq < 0)
+		goto err1;
+	if (count == 0) {
+		rc = devm_request_irq(&pdev->dev, irq,
+			smmon_isr, IRQF_ONESHOT,
+			"smmon0", dev_info);
+	} else {
+		rc = devm_request_irq(&pdev->dev, irq,
+			smmon_isr, IRQF_ONESHOT,
+			"smmon1", dev_info);
+	}
+	if (rc)
+		goto err1;
+	dev_info->edac_dev =
+		edac_device_alloc_ctl_info(0, dev_info->ctl_name,
+					   1, dev_info->blk_name, 1, 0,
+					   NULL, 0, dev_info->edac_idx);
+	if (!dev_info->edac_dev) {
+		pr_info("No memory for edac device\n");
+		goto err1;
+	}
+	/* Enable memory controller interrupts. We need to disable
+	 * the interrupt while unmasking it, since otherwise there
+	 * will be a locking conflict in ncr_write/ncr_read when the
+	 * ISR tries to read interrupt status.
+	 */
+	disable_irq(irq);
+	mask = SM_INT_MASK;
+	ncr_write(dev_info->sm_region, SM_INT_STATUS_MASK_REG,
+			4, &mask);
+
+	dev_info->edac_dev->pvt_info = dev_info;
+	dev_info->edac_dev->dev = &dev_info->pdev->dev;
+	dev_info->edac_dev->ctl_name = dev_info->ctl_name;
+	dev_info->edac_dev->mod_name = LSI_EDAC_MOD_STR;
+	dev_info->edac_dev->dev_name = dev_name(&dev_info->pdev->dev);
+	dev_info->edac_dev->edac_check = lsi_sm_error_check;
+
+
+	if (edac_device_add_device(dev_info->edac_dev) != 0) {
+		pr_info("Unable to add edac device for %s\n",
+				dev_info->ctl_name);
+		goto err2;
+	}
+	enable_irq(irq);
+	return 0;
+err2:
+	edac_device_free_ctl_info(dev_info->edac_dev);
+err1:
+	platform_device_unregister(dev_info->pdev);
+	return 1;
+}
+
+static int lsi_edac_mc_remove(struct platform_device *pdev)
+{
+	platform_device_unregister(pdev);
+	return 0;
+}
+
+static const struct of_device_id lsi_edac_smmon_match[] = {
+	{ .compatible = "lsi,smmon" },
+	{ }
+};
+MODULE_DEVICE_TABLE(platform, lsi_edac_smmon_match);
+
+static struct platform_driver lsi_edac_mc_driver = {
+	.probe = lsi_edac_mc_probe,
+	.remove = lsi_edac_mc_remove,
+	.driver = {
+		.name = "lsi_edac_smmon",
+		.of_match_table = lsi_edac_smmon_match,
+	}
+};
+module_platform_driver(lsi_edac_mc_driver);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Sangeetha Rao <sangeetha.rao@avagotech.com>");
diff --git a/drivers/edac/axxia_edac.c b/drivers/edac/axxia_edac.c
new file mode 100644
index 0000000..d638141
--- /dev/null
+++ b/drivers/edac/axxia_edac.c
@@ -0,0 +1,461 @@
+ /*
+  * drivers/edac/axxia_edac.c
+  *
+  * EDAC Driver for Avago's Axxia 5500
+  *
+  * Copyright (C) 2010 LSI Inc.
+  *
+  * This program is free software; you can redistribute it and/or modify
+  * it under the terms of the GNU General Public License as published by
+  * the Free Software Foundation; either version 2 of the License, or
+  * (at your option) any later version.
+  *
+  * This program is distributed in the hope that it will be useful,
+  * but WITHOUT ANY WARRANTY; without even the implied warranty of
+  * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+  * GNU General Public License for more details.
+  *
+  * You should have received a copy of the GNU General Public License
+  * along with this program; if not, write to the Free Software
+  * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
+  *
+  */
+
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/slab.h>
+#include <linux/io.h>
+#include <linux/edac.h>
+#include <mach/ncr.h>
+#include <linux/of_platform.h>
+#include <linux/of.h>
+#include <linux/of_address.h>
+#include <linux/of_irq.h>
+#include <linux/irq.h>
+#include <linux/platform_device.h>
+#include <linux/reboot.h>
+
+#include "edac_core.h"
+#include "edac_module.h"
+
+#define LSI_EDAC_MOD_STR     "lsi_edac"
+#define CORES_PER_CLUSTER 4
+
+/* Private structure for common edac device */
+struct lsi_edac_dev_info {
+	void __iomem *vbase;
+	struct platform_device *pdev;
+	char *ctl_name;
+	char *blk_name;
+	int edac_idx;
+	u32 sm0_region;
+	u32 sm1_region;
+	void __iomem *apb2ser3_region;
+	void __iomem *dickens_L3[8];
+	struct edac_device_ctl_info *edac_dev;
+	void (*init)(struct lsi_edac_dev_info *dev_info);
+	void (*exit)(struct lsi_edac_dev_info *dev_info);
+	void (*check)(struct edac_device_ctl_info *edac_dev);
+};
+
+static void lsi_error_init(struct lsi_edac_dev_info *dev_info)
+{
+}
+
+static void lsi_error_exit(struct lsi_edac_dev_info *dev_info)
+{
+}
+
+void log_cpumerrsr(void *edac)
+{
+	struct edac_device_ctl_info *edac_dev =
+		(struct edac_device_ctl_info *)edac;
+	u32 tmp1, tmp2, count0, count1;
+	unsigned long setVal;
+	int i;
+	struct lsi_edac_dev_info *dev_info;
+
+	dev_info = (struct lsi_edac_dev_info *) edac_dev->pvt_info;
+
+	/* Read cp15 for CPUMERRSR counts */
+	asm volatile("mrrc\tp15, 0, %0, %1, c15" : "=r"(tmp1),
+		"=r"(tmp2));
+	if (tmp1 & 0x80000000) {
+		count0 = (tmp2) & 0x000000ff;
+		count1 = ((tmp2) & 0x0000ff00) >> 8;
+
+		/* increment correctable error counts */
+		for (i = 0; i < count0+count1; i++) {
+			edac_device_handle_ce(edac_dev, 0,
+				raw_smp_processor_id(), edac_dev->ctl_name);
+		}
+
+		/* Clear the valid bit */
+		tmp1 = 0x80000000;
+		tmp2 = 0;
+		asm volatile("mcrr\tp15, 0, %0, %1, c15" : : "r"(tmp1),
+			"r"(tmp2));
+	}
+	if (tmp2 & 0x80000000) {
+		setVal = readl(dev_info->apb2ser3_region + 0xdc);
+		/* set bit 3 in pscratch reg */
+		setVal = (setVal) | (0x1 << 3);
+		writel(setVal, dev_info->apb2ser3_region + 0xdc);
+		pr_info("CPU uncorrectable error\n");
+		machine_restart(NULL);
+	}
+}
+
+
+/* Check for CPU Errors */
+static void lsi_cpu_error_check(struct edac_device_ctl_info *edac_dev)
+{
+	/* execute on current cpu */
+	log_cpumerrsr(edac_dev);
+
+	/* send ipis to execute on other cpus */
+	smp_call_function(log_cpumerrsr, edac_dev, 1);
+
+}
+
+void log_l2merrsr(void *edac)
+{
+	struct edac_device_ctl_info *edac_dev =
+			(struct edac_device_ctl_info *)edac;
+	u32 tmp1, tmp2, count0, count1;
+	unsigned long setVal;
+	int i;
+	struct lsi_edac_dev_info *dev_info;
+
+	dev_info = (struct lsi_edac_dev_info *) edac_dev->pvt_info;
+
+	/* Read cp15 for L2MERRSR counts */
+	asm volatile("mrrc\tp15, 1, %0, %1, c15" : "=r"(tmp1),
+		"=r"(tmp2));
+	if (tmp1 & 0x80000000) {
+		count0 = (tmp2) & 0x000000ff;
+		count1 = ((tmp2) & 0x0000ff00) >> 8;
+
+		/* increment correctable error counts */
+		for (i = 0; i < count0+count1; i++) {
+			edac_device_handle_ce(edac_dev, 0,
+				raw_smp_processor_id()/CORES_PER_CLUSTER,
+				edac_dev->ctl_name);
+		}
+
+		/* Clear the valid bit */
+		tmp1 = 0x80000000;
+		tmp2 = 0;
+		asm volatile("mcrr\tp15, 1, %0, %1, c15" : : "r"(tmp1),
+			"r"(tmp2));
+	}
+	if (tmp2 & 0x80000000) {
+		setVal = readl(dev_info->apb2ser3_region + 0xdc);
+		/* set bit 3 in pscratch reg */
+		setVal = (setVal) | (0x1 << 3);
+		writel(setVal, dev_info->apb2ser3_region + 0xdc);
+		pr_info("L2 uncorrectable error\n");
+		machine_restart(NULL);
+	}
+}
+
+/* Check for L2 Errors */
+static void lsi_l2_error_check(struct edac_device_ctl_info *edac_dev)
+{
+	/* 4 cores per cluster */
+	int nr_cluster_ids = ((nr_cpu_ids - 1) / CORES_PER_CLUSTER) + 1;
+	int i, j, cpu;
+
+	/* execute on current cpu */
+	log_l2merrsr(edac_dev);
+
+	for (i = 0; i < nr_cluster_ids; i++) {
+		/* No need to run on local cluster. */
+		if (i == (raw_smp_processor_id() / CORES_PER_CLUSTER))
+			continue;
+		/*
+		 * Have some core in each cluster execute this,
+		 * Start with the first core on that cluster.
+		 */
+		cpu = i * CORES_PER_CLUSTER;
+		for (j = cpu; j < cpu + CORES_PER_CLUSTER; j++) {
+			if (cpu_online(j)) {
+				smp_call_function_single(j, log_l2merrsr,
+					edac_dev, 1);
+				break;
+			}
+		}
+	}
+}
+
+/* Check for L3 Errors */
+static void lsi_l3_error_check(struct edac_device_ctl_info *edac_dev)
+{
+	unsigned long regVal1, regVal2, setVal;
+	unsigned count = 0;
+	int i, instance;
+	struct lsi_edac_dev_info *dev_info;
+
+	dev_info = (struct lsi_edac_dev_info *) edac_dev->pvt_info;
+
+	for (instance = 0; instance < 8; instance++) {
+		regVal1 = readl(dev_info->dickens_L3[instance]);
+		regVal2 = readl(dev_info->dickens_L3[instance] + 4);
+		/* First error valid */
+		if (regVal2 & 0x40000000) {
+			if (regVal2 & 0x30000000) {
+				setVal = readl(dev_info->apb2ser3_region +
+					0xdc);
+				/* set bit 3 in pscratch reg */
+				setVal = (setVal) | (0x1 << 3);
+				writel(setVal, dev_info->apb2ser3_region +
+					0xdc);
+				/* Fatal error */
+				pr_info("L3 uncorrectable error\n");
+				machine_restart(NULL);
+			}
+			count = (regVal2 & 0x07fff800) >> 11;
+			for (i = 0; i < count; i++)
+				edac_device_handle_ce(edac_dev, 0,
+					instance, edac_dev->ctl_name);
+			/* clear the valid bit */
+			writel(0x48000000, dev_info->dickens_L3[instance] + 84);
+		}
+	}
+}
+
+/* Check for SysMem Errors */
+static void lsi_sm_error_check(struct edac_device_ctl_info *edac_dev)
+{
+	unsigned long sm0_regVal, sm1_regVal, clearVal, setVal;
+	struct lsi_edac_dev_info *dev_info;
+
+	dev_info = (struct lsi_edac_dev_info *) edac_dev->pvt_info;
+
+	/* SM0 is instance 0 */
+	ncr_read(dev_info->sm0_region, 0x410, 4, &sm0_regVal);
+	if (sm0_regVal & 0x8) {
+		/* single bit and multiple bit correctable errors */
+		edac_device_handle_ce(edac_dev, 0, 0, edac_dev->ctl_name);
+		/* Clear bits */
+		clearVal = 0x8;
+		ncr_write(dev_info->sm0_region, 0x548, 4, &clearVal);
+	}
+	if (sm0_regVal & 0x40) {
+		setVal = readl(dev_info->apb2ser3_region + 0xdc);
+		/* set bit 3 in pscratch reg */
+		setVal = (setVal) | (0x1 << 3);
+		writel(setVal, dev_info->apb2ser3_region + 0xdc);
+		/* single bit and multiple bit uncorrectable errors */
+		pr_info("SM0 uncorrectable error\n");
+		machine_restart(NULL);
+	}
+
+	/* SM1 is instance 1 */
+	ncr_read(dev_info->sm1_region, 0x410, 4, &sm1_regVal);
+	if (sm1_regVal & 0x8) {
+		/* single bit and multiple bit correctable errors */
+		edac_device_handle_ce(edac_dev, 0, 1, edac_dev->ctl_name);
+		/* Clear bits */
+		clearVal = 0x8;
+		ncr_write(dev_info->sm1_region, 0x548, 4, &clearVal);
+	}
+	if (sm1_regVal & 0x40) {
+		setVal = readl(dev_info->apb2ser3_region + 0xdc);
+		/* set bit 3 in pscratch reg */
+		setVal = (setVal) | (0x1 << 3);
+		writel(setVal, dev_info->apb2ser3_region + 0xdc);
+		/* single bit and multiple bit uncorrectable errors */
+		pr_info("SM1 uncorrectable error\n");
+		machine_restart(NULL);
+	}
+}
+
+
+static struct lsi_edac_dev_info lsi_edac_devs[] = {
+	{
+		.ctl_name = "LSI_CPU",
+		.blk_name = "cpumerrsr",
+		.init = lsi_error_init,
+		.exit = lsi_error_exit,
+		.check = lsi_cpu_error_check
+	},
+	{
+		.ctl_name = "LSI_L2",
+		.blk_name = "l2merrsr",
+		.init = lsi_error_init,
+		.exit = lsi_error_exit,
+		.check = lsi_l2_error_check
+	},
+	{
+		.ctl_name = "LSI_L3",
+		.blk_name = "l3merrsr",
+		.init = lsi_error_init,
+		.exit = lsi_error_exit,
+		.check = lsi_l3_error_check
+	},
+	{
+		.ctl_name = "LSI_SM",
+		.blk_name = "ECC",
+		.init = lsi_error_init,
+		.exit = lsi_error_exit,
+		.check = lsi_sm_error_check
+	},
+	{0} /* Terminated by NULL */
+};
+
+
+
+/* static void lsi_add_edac_devices(void __iomem *vbase) */
+static void lsi_add_edac_devices(struct platform_device *pdev)
+{
+	struct lsi_edac_dev_info *dev_info = NULL;
+	/* 4 cores per cluster */
+	int nr_cluster_ids = ((nr_cpu_ids - 1) / CORES_PER_CLUSTER) + 1;
+	struct resource *io0, *io1;
+	struct device_node *np = pdev->dev.of_node;
+	void __iomem *apb2ser3_region;
+	int i;
+
+	apb2ser3_region = of_iomap(np, 2);
+	if (!apb2ser3_region) {
+		dev_err(&pdev->dev, "LSI_apb2ser3_region iomap error\n");
+		goto err2;
+	}
+
+	for (dev_info = &lsi_edac_devs[0]; dev_info->init; dev_info++) {
+		dev_info->pdev = platform_device_register_simple(
+		dev_info->ctl_name, 0, NULL, 0);
+		if (IS_ERR(dev_info->pdev)) {
+			pr_info("Can't register platform device for %s\n",
+				dev_info->ctl_name);
+			continue;
+		}
+		/*
+		 * Don't have to allocate private structure but
+		 * make use of cpc925_devs[] instead.
+		 */
+		dev_info->edac_idx = edac_device_alloc_index();
+
+		if (strcmp(dev_info->ctl_name, "LSI_SM") == 0) {
+			io0 = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+			if (!io0) {
+				dev_err(&pdev->dev, "Unable to get mem resource\n");
+				goto err2;
+			}
+			io1 = platform_get_resource(pdev, IORESOURCE_MEM, 1);
+			if (!io1) {
+				dev_err(&pdev->dev, "Unable to get mem resource\n");
+				goto err2;
+			}
+
+			dev_info->sm0_region = io0->start;
+			dev_info->sm1_region = io1->start;
+			dev_info->apb2ser3_region = apb2ser3_region;
+			dev_info->edac_dev =
+			edac_device_alloc_ctl_info(0, dev_info->ctl_name,
+			1, dev_info->blk_name, 2, 0,
+				NULL, 0, dev_info->edac_idx);
+		} else if (strcmp(dev_info->ctl_name, "LSI_L3") == 0) {
+			/* 8 L3 caches */
+			for (i = 0; i < 8; i++) {
+				dev_info->dickens_L3[i] = of_iomap(np, i+3);
+				if (!dev_info->dickens_L3[i]) {
+					dev_err(&pdev->dev,
+						"LSI_L3 iomap error\n");
+					goto err2;
+				}
+			}
+			dev_info->apb2ser3_region = apb2ser3_region;
+			dev_info->edac_dev =
+			edac_device_alloc_ctl_info(0, dev_info->ctl_name,
+			1, dev_info->blk_name, 8, 0, NULL, 0,
+			dev_info->edac_idx);
+		} else if (strcmp(dev_info->ctl_name, "LSI_CPU") == 0) {
+			dev_info->apb2ser3_region = apb2ser3_region;
+			dev_info->edac_dev =
+			edac_device_alloc_ctl_info(0, dev_info->ctl_name,
+			1, dev_info->blk_name, num_possible_cpus(), 0, NULL,
+			0, dev_info->edac_idx);
+		} else if (strcmp(dev_info->ctl_name, "LSI_L2") == 0) {
+			dev_info->apb2ser3_region = apb2ser3_region;
+			dev_info->edac_dev =
+			edac_device_alloc_ctl_info(0, dev_info->ctl_name,
+				1, dev_info->blk_name, nr_cluster_ids, 0, NULL,
+				0, dev_info->edac_idx);
+		} else {
+			dev_info->edac_dev =
+			edac_device_alloc_ctl_info(0, dev_info->ctl_name,
+			1, dev_info->blk_name, 1, 0,
+			NULL, 0, dev_info->edac_idx);
+		}
+		if (!dev_info->edac_dev) {
+			pr_info("No memory for edac device\n");
+			goto err1;
+		}
+
+		dev_info->edac_dev->pvt_info = dev_info;
+		dev_info->edac_dev->dev = &dev_info->pdev->dev;
+		dev_info->edac_dev->ctl_name = dev_info->ctl_name;
+		dev_info->edac_dev->mod_name = LSI_EDAC_MOD_STR;
+		dev_info->edac_dev->dev_name = dev_name(&dev_info->pdev->dev);
+
+		if (edac_op_state == EDAC_OPSTATE_POLL)
+			dev_info->edac_dev->edac_check = dev_info->check;
+
+		if (dev_info->init)
+			dev_info->init(dev_info);
+
+		if (edac_device_add_device(dev_info->edac_dev) > 0) {
+			pr_info("Unable to add edac device for %s\n",
+					dev_info->ctl_name);
+			goto err2;
+		}
+		pr_info("Successfully added edac device for %s\n",
+				dev_info->ctl_name);
+
+		continue;
+err2:
+		if (dev_info->exit)
+			dev_info->exit(dev_info);
+		edac_device_free_ctl_info(dev_info->edac_dev);
+err1:
+		platform_device_unregister(dev_info->pdev);
+	}
+}
+
+
+static int lsi_edac_probe(struct platform_device *pdev)
+{
+	edac_op_state = EDAC_OPSTATE_POLL;
+	lsi_add_edac_devices(pdev);
+	return 0;
+}
+
+static int lsi_edac_remove(struct platform_device *pdev)
+{
+	platform_device_unregister(pdev);
+	return 0;
+}
+
+static struct of_device_id lsi_edac_match[] = {
+	{
+	.type   = "edac",
+	.compatible = "lsi,edac",
+	},
+	{},
+};
+
+static struct platform_driver lsi_edac_driver = {
+	.probe = lsi_edac_probe,
+	.remove = lsi_edac_remove,
+	.driver = {
+		.name = "lsi_edac",
+		.of_match_table = lsi_edac_match,
+	}
+};
+
+module_platform_driver(lsi_edac_driver);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Sangeetha Rao <sangeetha.rao@avagotech.com>");
diff --git a/drivers/i2c/busses/Kconfig b/drivers/i2c/busses/Kconfig
index 2255af2..e8aaa66 100644
--- a/drivers/i2c/busses/Kconfig
+++ b/drivers/i2c/busses/Kconfig
@@ -1110,6 +1110,12 @@ config I2C_CROS_EC_TUNNEL
 	  connected there. This will work whatever the interface used to
 	  talk to the EC (SPI, I2C or LPC).
 
+config I2C_AXXIA
+	tristate "Axxia I2C bus support"
+	select I2C_ALGOBIT
+	help
+	  Say yes if you want to support the I2C bus on Axxia platforms.
+
 config SCx200_ACB
 	tristate "Geode ACCESS.bus support"
 	depends on X86_32 && PCI
@@ -1122,6 +1128,21 @@ config SCx200_ACB
 	  This support is also available as a module.  If so, the module
 	  will be called scx200_acb.
 
+config I2C_AXXIA
+	tristate "Axxia I2C bus support"
+	select I2C_ALGOBIT
+	help
+	  Say yes if you want to support the I2C bus on Axxia platforms.
+
+	  If you don't know, say Y.
+
+config ACP3400_I2C
+	tristate "ACP3400 I2C support"
+	depends on ACP
+	default y
+	help
+	  I2C adapter for acp476 based boards
+
 config I2C_OPAL
 	tristate "IBM OPAL I2C driver"
 	depends on PPC_POWERNV
diff --git a/drivers/i2c/busses/acp3400-i2c.c b/drivers/i2c/busses/acp3400-i2c.c
new file mode 100644
index 0000000..7cdf452
--- /dev/null
+++ b/drivers/i2c/busses/acp3400-i2c.c
@@ -0,0 +1,515 @@
+/*
+ * ACP3400 I2C adapter
+ *
+ * Based on DU-TS I2C Adapter Driver
+ *
+ * This program is free software; you can redistribute  it and/or modify it
+ * under  the terms of  the GNU General  Public License as published by the
+ * Free Software Foundation;  either version 2 of the  License, or (at your
+ * option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/sched.h>
+#include <linux/init.h>
+#include <linux/of_platform.h>
+#include <linux/slab.h>
+
+#include <linux/io.h>
+#include <linux/i2c.h>
+#include <linux/interrupt.h>
+#include <linux/delay.h>
+
+#define DRV_NAME "acp3400-i2c"
+
+struct acp3400_i2c_regs {
+	unsigned int txconfig;
+	unsigned int rxconfig;
+	unsigned int txstatus;
+	unsigned int rxstatus;
+	unsigned int irqenable;
+	unsigned int irqclear;
+	unsigned int irqstatus;
+	unsigned int clkconfig;
+	unsigned int startsetup;
+	unsigned int stopsetup;
+	unsigned int datasetup;
+	unsigned int bypassmode;
+	unsigned int slaveaddr;
+	unsigned int txdata0;
+	unsigned int txdata1;
+	unsigned int rxdata0;
+	unsigned int rxdata1;
+};
+
+struct acp3400_i2c_timer_regs {
+	unsigned int loadval; /* 0x20 */
+	unsigned int val;     /* 0x24 */
+	unsigned int control; /* 0x28 */
+	unsigned int irq_clr; /* 0x2c */
+	unsigned int irq_stat_raw; /* 0x30 */
+	unsigned int irq_stat; /* 0x34 */
+	unsigned int bg_loadval; /* 0x38 */
+};
+
+
+/* Master Clock Configuration */
+/* Configured clock frequency i2c_freq = 100kHz. */
+
+/* I2C register values */
+#define ACPI2C_CLK_100KHZ	(1000 | (1000 << 16))
+#define ACPI2C_MSTSHC		(940 | (800 << 16))
+#define ACPI2C_MSPSHC		(800 | (0 << 16))
+#define ACPI2C_MDSHC		(255 | (127 << 16))
+
+#define ACPI2C_XFER_START		0x00000001
+#define ACPI2C_XFER_DONE		0x00000001
+#define ACPI2C_READ_MODE		0x00000200
+#define ACPI2C_STOP_MODE		0x20000000
+#define ACPI2C_MASTER_MODE		0x00000100
+#define ACPI2C_MASTER_OP_CLEAR		0x00000400
+#define ACPI2C_10BIT_ADDR		0x00000080
+#define ACPI2C_CLEAR_IRQ		0x0000000F
+#define ACPI2C_DELAY			500 /* us */
+#define ACPI2C_RETRIES			100
+#define ACPI2C_REG_BSIZE		4 /* bytes */
+#define ACPI2C_DATA_REGS		2
+
+struct acp3400_i2c {
+	struct device *dev;
+	struct i2c_adapter adap;
+	struct acp3400_i2c_regs __iomem *i2c_regs;
+	struct acp3400_i2c_timer_regs __iomem *timer_regs;
+	struct mutex i2c_lock;
+};
+
+#ifdef ACP3400_I2C_DEBUG
+static void dump_regs(struct acp3400_i2c *i2c)
+{
+	pr_info("i2c-reg: txconfig    %8.8x\n",
+		 in_le32(&i2c->i2c_regs->txconfig));
+	pr_info("i2c-reg: rxconfig    %8.8x\n",
+		 in_le32(&i2c->i2c_regs->rxconfig));
+	pr_info("i2c-reg: txstatus    %8.8x\n",
+		 in_le32(&i2c->i2c_regs->txstatus));
+	pr_info("i2c-reg: rxstatus    %8.8x\n",
+		 in_le32(&i2c->i2c_regs->rxstatus));
+	pr_info("i2c-reg: irqenable   %8.8x\n",
+		 in_le32(&i2c->i2c_regs->irqenable));
+	pr_info("i2c-reg: irqclear    %8.8x\n",
+		 in_le32(&i2c->i2c_regs->irqclear));
+	pr_info("i2c-reg: irqstatus   %8.8x\n",
+		 in_le32(&i2c->i2c_regs->irqstatus));
+	pr_info("i2c-reg: clkconfig   %8.8x\n",
+		 in_le32(&i2c->i2c_regs->clkconfig));
+	pr_info("i2c-reg: startsetup  %8.8x\n",
+		 in_le32(&i2c->i2c_regs->startsetup));
+	pr_info("i2c-reg: stopsetup   %8.8x\n",
+		 in_le32(&i2c->i2c_regs->stopsetup));
+	pr_info("i2c-reg: datasetup   %8.8x\n",
+		 in_le32(&i2c->i2c_regs->datasetup));
+	pr_info("i2c-reg: bypassmode  %8.8x\n",
+		 in_le32(&i2c->i2c_regs->bypassmode));
+	pr_info("i2c-reg: slaveaddr   %8.8x\n",
+		 in_le32(&i2c->i2c_regs->slaveaddr));
+	pr_info("i2c-reg: txdata0     %8.8x\n",
+		 in_le32(&i2c->i2c_regs->txdata0));
+	pr_info("i2c-reg: txdata1     %8.8x\n",
+		 in_le32(&i2c->i2c_regs->txdata1));
+	pr_info("i2c-reg: rxdata0     %8.8x\n",
+		 in_le32(&i2c->i2c_regs->rxdata0));
+	pr_info("i2c-reg: rxdata1     %8.8x\n",
+		 in_le32(&i2c->i2c_regs->rxdata1));
+	pr_info("i2c-timer-reg: loadval %8.8x\n",
+		 in_le32(&i2c->timer_regs->loadval));
+	pr_info("i2c-timer-reg: val     %8.8x\n",
+		 in_le32(&i2c->timer_regs->val));
+	pr_info("i2c-timer-reg: control %8.8x\n",
+		 in_le32(&i2c->timer_regs->control));
+}
+#endif
+/*
+ * Low level write routine
+ */
+static int acp3400_i2c_write_bytes(struct acp3400_i2c *i2c,
+				   struct i2c_msg *msgs)
+{
+	unsigned char *bufp = msgs->buf;
+	unsigned int reg_value, data[2] = {0, 0};
+	int cnt, ret = 0;
+
+	if (msgs->len > (ACPI2C_REG_BSIZE * ACPI2C_DATA_REGS))
+		msgs->len = ACPI2C_REG_BSIZE * ACPI2C_DATA_REGS;
+
+	/* Set message */
+	for (cnt = 0; cnt < msgs->len; cnt++) {
+		data[1] <<= 8;
+		data[1] |= ((data[0] >> 24) & 0xFF);
+		data[0] <<= 8;
+		data[0] |= bufp[cnt];
+	}
+	out_le32(&i2c->i2c_regs->txdata0, data[0]);
+	out_le32(&i2c->i2c_regs->txdata1, data[1]);
+
+	/* setup and start a transmission */
+	reg_value = ACPI2C_MASTER_MODE | ACPI2C_STOP_MODE;
+	reg_value |= (msgs->len << 1) & 0x1e;
+	if (msgs->flags & I2C_M_TEN) {
+		reg_value |= ACPI2C_10BIT_ADDR;
+		/* TODO update slave address accordingly */
+	}
+	out_le32(&i2c->i2c_regs->txconfig, reg_value);
+
+	reg_value &= ~ACPI2C_STOP_MODE;
+	out_le32(&i2c->i2c_regs->txconfig, reg_value);
+
+	reg_value |= ACPI2C_XFER_START;
+	out_le32(&i2c->i2c_regs->txconfig, reg_value);
+
+	/* Check if the message has been sent
+	 * Wait a totally of 1 s for the transmission */
+	reg_value = cnt = 0;
+	while (0 == reg_value && cnt++ < ACPI2C_RETRIES) {
+		udelay(ACPI2C_DELAY);
+		/* Read transmission status */
+		reg_value = in_le32(&i2c->i2c_regs->txstatus);
+	}
+#ifdef ACP3400_I2C_DEBUG
+	if (ACPI2C_XFER_DONE != reg_value)
+		dump_regs(i2c);
+#endif
+	/* Clear registers */
+	out_le32(&i2c->i2c_regs->irqclear, ACPI2C_CLEAR_IRQ);
+	out_le32(&i2c->i2c_regs->txconfig, ACPI2C_MASTER_OP_CLEAR);
+
+	out_le32(&i2c->i2c_regs->txconfig, ACPI2C_STOP_MODE);
+
+	if (ACPI2C_XFER_DONE == reg_value)
+		ret = msgs->len;
+	else
+		ret = -EIO;
+
+	return ret;
+}
+
+/*
+ * Low level read routine
+ */
+
+static int acp3400_i2c_read_bytes(struct acp3400_i2c *i2c,
+		struct i2c_msg *msgs)
+{
+	unsigned char *bufp = msgs->buf;
+	unsigned int reg_value, data[2];
+	int cnt, ret = 0;
+
+	if (msgs->len > (ACPI2C_REG_BSIZE * ACPI2C_DATA_REGS))
+		msgs->len = ACPI2C_REG_BSIZE * ACPI2C_DATA_REGS;
+
+	/* Setup a reception */
+	reg_value = (msgs->len << 1) & 0x1e;
+	if (msgs->flags & I2C_M_TEN) {
+		reg_value |= ACPI2C_10BIT_ADDR;
+		/* TODO update slave address accordingly */
+	}
+	out_le32(&i2c->i2c_regs->rxconfig, reg_value);
+
+	/* set read mode and start clock */
+	reg_value |= ACPI2C_XFER_START;
+	out_le32(&i2c->i2c_regs->rxconfig, reg_value);
+
+	reg_value = ACPI2C_STOP_MODE | ACPI2C_MASTER_MODE | ACPI2C_READ_MODE;
+	out_le32(&i2c->i2c_regs->txconfig, reg_value);
+
+	reg_value &= ~ACPI2C_STOP_MODE;
+	out_le32(&i2c->i2c_regs->txconfig, reg_value);
+
+	reg_value |= ACPI2C_XFER_START;
+	out_le32(&i2c->i2c_regs->txconfig, reg_value);
+
+	/* Check if the message has been received
+	 * Wait a totally of 1 s for the reception */
+	reg_value = cnt = 0;
+	while (0 == (ACPI2C_XFER_DONE & reg_value) &&
+	       cnt++ < ACPI2C_RETRIES) {
+		udelay(ACPI2C_DELAY);
+		/* Read transmission status */
+		reg_value = in_le32(&i2c->i2c_regs->rxstatus);
+	}
+
+	/* get message */
+	data[0] = in_le32(&i2c->i2c_regs->rxdata0);
+	data[1] = in_le32(&i2c->i2c_regs->rxdata1);
+	for (cnt = 0; cnt < msgs->len; cnt++) {
+		if (cnt < ACPI2C_REG_BSIZE)
+			bufp[cnt] = data[0] >> ((8 * cnt) & 0xFF);
+		else
+			bufp[cnt] = data[1] >>
+				((8 * (cnt - ACPI2C_REG_BSIZE)) & 0xFF);
+	}
+#ifdef ACP3400_I2C_DEBUG
+	if (ACPI2C_XFER_DONE != (reg_value & 0x03))
+		dump_regs(i2c);
+#endif
+	/* clear registers */
+	out_le32(&i2c->i2c_regs->irqclear, ACPI2C_CLEAR_IRQ);
+	out_le32(&i2c->i2c_regs->txconfig, ACPI2C_MASTER_OP_CLEAR);
+
+	out_le32(&i2c->i2c_regs->txconfig, ACPI2C_STOP_MODE);
+
+	if (ACPI2C_XFER_DONE == (reg_value & 0x03))
+		ret = msgs->len;
+	else
+		ret = -EIO;
+
+	return ret;
+}
+
+/*
+ * I2C timer setup
+ */
+static void acp3400_i2c_timer_setup(struct acp3400_i2c *i2c)
+{
+	/* disable timer 1 */
+	out_le32(&i2c->timer_regs->control, 0);
+	/* Program the Timer1 Load Value register with a value that sets
+	 * the timer period to 250 ns (that is, 4 MHz frequency). When you
+	 * configure the ACP peripheral clock (clk_per) for 200 MHz, the
+	 * Timer1 Load Value is 0x31. */
+	out_le32(&i2c->timer_regs->loadval, 0x31);
+	out_le32(&i2c->timer_regs->bg_loadval, 0x31);
+
+	/* Configure and enable Timer1 for periodic wrapping mode
+	 * with a prescaler of 1 by writing 0xc0 to the Timer1 Control
+	 * Register. */
+	out_le32(&i2c->timer_regs->control, 0xc0);
+}
+/*
+ * Low level master transfer routine
+ */
+static int acp3400_i2c_xfer_bytes(struct acp3400_i2c *i2c,
+		struct i2c_msg *msgs)
+{
+	int ret = 0;
+
+	mutex_lock(&i2c->i2c_lock);
+	/* Prepare ACP3400 I2C for a transaction */
+	out_le32(&i2c->i2c_regs->txconfig,
+		 ACPI2C_MASTER_OP_CLEAR | ACPI2C_MASTER_MODE);
+
+	out_le32(&i2c->i2c_regs->txconfig,
+		 ACPI2C_MASTER_MODE | ACPI2C_STOP_MODE);
+
+	/* I2C clock frequency and duty cycle */
+	out_le32(&i2c->i2c_regs->clkconfig, ACPI2C_CLK_100KHZ);
+	/* The setup and hold durations for the START condition. */
+	out_le32(&i2c->i2c_regs->startsetup, ACPI2C_MSTSHC);
+	/* The setup and hold durations for the STOP condition. */
+	out_le32(&i2c->i2c_regs->stopsetup, ACPI2C_MSPSHC);
+	/* The setup and hold durations for the data bits. */
+	out_le32(&i2c->i2c_regs->datasetup, ACPI2C_MDSHC);
+	/* Set Slave Address */
+	out_le32(&i2c->i2c_regs->slaveaddr, msgs->addr);
+	/* Disable the actions for which the host requires to be interrupted */
+	out_le32(&i2c->i2c_regs->irqenable, 0);
+
+	/* Send/Receive Data */
+	if (msgs->flags & I2C_M_RD)
+		ret = acp3400_i2c_read_bytes(i2c, msgs);
+	else
+		ret = acp3400_i2c_write_bytes(i2c, msgs);
+
+	mutex_unlock(&i2c->i2c_lock);
+	return ret;
+}
+static void acp3400_i2c_dummy_xfer(struct acp3400_i2c *i2c)
+{
+	/* Prepare ACP3400 I2C for a transaction */
+	out_le32(&i2c->i2c_regs->txconfig,
+		 ACPI2C_MASTER_OP_CLEAR | ACPI2C_MASTER_MODE);
+
+	out_le32(&i2c->i2c_regs->txconfig,
+		 ACPI2C_MASTER_MODE | ACPI2C_STOP_MODE);
+
+	/* I2C clock frequency and duty cycle */
+	out_le32(&i2c->i2c_regs->clkconfig, ACPI2C_CLK_100KHZ);
+	/* The setup and hold durations for the START condition. */
+	out_le32(&i2c->i2c_regs->startsetup, ACPI2C_MSTSHC);
+	/* The setup and hold durations for the STOP condition. */
+	out_le32(&i2c->i2c_regs->stopsetup, ACPI2C_MSPSHC);
+	/* The setup and hold durations for the data bits. */
+	out_le32(&i2c->i2c_regs->datasetup, ACPI2C_MDSHC);
+	/* Set Dummy Slave Address */
+	out_le32(&i2c->i2c_regs->slaveaddr, 0x7f);
+	/* Disable the actions for which the host requires to be interrupted */
+	out_le32(&i2c->i2c_regs->irqenable, 0);
+	/* Number of bytes 0, clear stop mode */
+	out_le32(&i2c->i2c_regs->txconfig, ACPI2C_MASTER_MODE);
+
+	/* Set Transmit Ready - triggers the transmit transaction */
+	out_le32(&i2c->i2c_regs->txconfig,
+		 (ACPI2C_XFER_START | ACPI2C_MASTER_MODE));
+
+}
+
+/*
+ * Generic master transfer entrypoint.
+ * Returns the number of processed messages or error (<0)
+ */
+static int acp3400_i2c_xfer(struct i2c_adapter *adap,
+		struct i2c_msg *msgs, int num)
+{
+	struct acp3400_i2c *i2c = i2c_get_adapdata(adap);
+	int msg_cnt, ret = 0;
+
+	if (!num)
+		return 0;
+
+#ifdef ACP3400_I2C_DEBUG
+	if (num == 1 && msgs[0].addr == 0x7f && msgs[0].len == 0) {
+		mutex_lock(&i2c->i2c_lock);
+		acp3400_i2c_dummy_xfer(i2c);
+		mutex_unlock(&i2c->i2c_lock);
+		return 0;
+	}
+#endif
+	/*
+	 * Check the sanity of the passed messages.
+	 * Uhh, generic i2c layer is more suitable place for such code...
+	 */
+	if ((msgs[0].addr > 0x3ff) ||
+	    (!(msgs[0].flags & I2C_M_TEN) && (msgs[0].addr > 0x7f)))
+		return -EINVAL;
+
+	for (msg_cnt = 0; msg_cnt < num; ++msg_cnt) {
+		if (msgs[msg_cnt].len <= 0)
+			return -EINVAL;
+		if ((msgs[0].addr != msgs[msg_cnt].addr) ||
+		    ((msgs[0].flags & I2C_M_TEN) !=
+			(msgs[msg_cnt].flags & I2C_M_TEN)))
+			return -EINVAL;
+	}
+
+	/* Do real transfer */
+	for (msg_cnt = 0; msg_cnt < num; msg_cnt++)
+		ret = acp3400_i2c_xfer_bytes(i2c, &msgs[msg_cnt]);
+	return ret < 0 ? ret : num;
+}
+
+static u32 acp3400_i2c_functionality(struct i2c_adapter *adapter)
+{
+	return I2C_FUNC_I2C | I2C_FUNC_10BIT_ADDR;
+}
+
+static const struct i2c_algorithm acp3400_i2c_algo = {
+	.master_xfer = acp3400_i2c_xfer,
+	.functionality = acp3400_i2c_functionality,
+};
+
+static struct i2c_adapter acp3400_i2c_ops = {
+	.owner = THIS_MODULE,
+	.name = "ACP3400 adapter",
+	.class = I2C_CLASS_HWMON | I2C_CLASS_SPD,
+	.algo = &acp3400_i2c_algo,
+	.timeout = HZ,
+};
+
+static int acp34xx_i2c_probe(struct platform_device *dev)
+{
+	struct device_node *np = dev->dev.of_node;
+
+	struct acp3400_i2c *i2c;
+	int result = -ENODEV;
+	const u32 *field;
+
+	if (!np)
+		return -ENODEV;
+
+	field = of_get_property(np, "enabled", NULL);
+	if (!field || (field && (0 == *field)))
+		return -EINVAL;
+
+	i2c = kzalloc(sizeof(*i2c), GFP_KERNEL);
+	if (!i2c)
+		goto err;
+
+	i2c->i2c_regs = of_iomap(np, 0);
+	i2c->timer_regs = of_iomap(np, 1);
+	if (!i2c->i2c_regs || !i2c->timer_regs) {
+		pr_err("%s: failed to map I/O\n", np->full_name);
+		goto err;
+	}
+
+	i2c->adap = acp3400_i2c_ops;
+	i2c_set_adapdata(&i2c->adap, i2c);
+	i2c->adap.dev.of_node = of_node_get(np);
+	mutex_init(&i2c->i2c_lock);
+
+	/* I2C timer setup */
+	acp3400_i2c_timer_setup(i2c);
+	acp3400_i2c_dummy_xfer(i2c);
+	result = i2c_add_adapter(&i2c->adap);
+	if (result < 0) {
+		pr_err("%s: failed to add adapter\n",
+				np->full_name);
+		goto err;
+	}
+
+	pr_info("%s: adapter has been added\n", np->full_name);
+
+	dev_set_drvdata(&dev->dev, i2c);
+	return 0;
+err:
+	if (i2c) {
+		if (i2c->i2c_regs)
+			iounmap(i2c->i2c_regs);
+		if (i2c->timer_regs)
+			iounmap(i2c->timer_regs);
+		kfree(i2c);
+	}
+
+	return result;
+}
+
+
+static int acp34xx_i2c_remove(struct platform_device *dev)
+{
+	struct acp3400_i2c *i2c = dev_get_drvdata(&dev->dev);
+
+	i2c_del_adapter(&i2c->adap);
+	kfree(i2c);
+
+	return 0;
+}
+
+static struct of_device_id acp_i2c_match[] = {
+	{
+		.compatible = "acp-i2c",
+	},
+	{
+		.compatible = "acp, acp3400-i2c",
+	},
+	{ /* end of list */ },
+};
+
+static struct platform_driver acp_i2c_driver = {
+	.driver = {
+		.name = "acp-i2c",
+		.owner = THIS_MODULE,
+		.of_match_table = acp_i2c_match,
+	},
+	.probe		= acp34xx_i2c_probe,
+	.remove		= acp34xx_i2c_remove,
+};
+
+module_platform_driver(acp_i2c_driver);
+
+MODULE_AUTHOR("Andrey Panteleev <andrey.xx.panteleev@ericsson.com>");
+MODULE_DESCRIPTION("I2C adapter for ACP3400");
+MODULE_LICENSE("GPL");
diff --git a/drivers/i2c/busses/i2c-axxia.c b/drivers/i2c/busses/i2c-axxia.c
index 32d8834..6c84826 100644
--- a/drivers/i2c/busses/i2c-axxia.c
+++ b/drivers/i2c/busses/i2c-axxia.c
@@ -1,134 +1,163 @@
 /*
- * This driver implements I2C master functionality using the LSI API2C
- * controller.
- *
- * NOTE: The controller has a limitation in that it can only do transfers of
- * maximum 255 bytes at a time. If a larger transfer is attempted, error code
- * (-EINVAL) is returned.
+ * drivers/i2c/busses/i2c-axxia.c
  *
  * This software is licensed under the terms of the GNU General Public
  * License version 2, as published by the Free Software Foundation, and
  * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
  */
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/platform_device.h>
 #include <linux/clk.h>
 #include <linux/clkdev.h>
 #include <linux/err.h>
 #include <linux/i2c.h>
-#include <linux/init.h>
+#include <linux/io.h>
 #include <linux/interrupt.h>
+#include <linux/delay.h>
+#include <linux/slab.h>
+#include <linux/of_address.h>
+#include <linux/of_irq.h>
 #include <linux/module.h>
-#include <linux/io.h>
-#include <linux/kernel.h>
-#include <linux/platform_device.h>
 
 #define SCL_WAIT_TIMEOUT_NS 25000000
-#define I2C_XFER_TIMEOUT    (msecs_to_jiffies(250))
+#define I2C_XFER_TIMEOUT    (msecs_to_jiffies(500))
 #define I2C_STOP_TIMEOUT    (msecs_to_jiffies(100))
-#define FIFO_SIZE           8
-
-#define GLOBAL_CONTROL		0x00
-#define   GLOBAL_MST_EN         BIT(0)
-#define   GLOBAL_SLV_EN         BIT(1)
-#define   GLOBAL_IBML_EN        BIT(2)
-#define INTERRUPT_STATUS	0x04
-#define INTERRUPT_ENABLE	0x08
-#define   INT_SLV               BIT(1)
-#define   INT_MST               BIT(0)
-#define WAIT_TIMER_CONTROL	0x0c
-#define   WT_EN			BIT(15)
-#define   WT_VALUE(_x)		((_x) & 0x7fff)
-#define IBML_TIMEOUT		0x10
-#define IBML_LOW_MEXT		0x14
-#define IBML_LOW_SEXT		0x18
-#define TIMER_CLOCK_DIV		0x1c
-#define I2C_BUS_MONITOR		0x20
-#define SOFT_RESET		0x24
-#define MST_COMMAND		0x28
-#define   CMD_BUSY		(1<<3)
-#define   CMD_MANUAL		(0x00 | CMD_BUSY)
-#define   CMD_AUTO		(0x01 | CMD_BUSY)
-#define MST_RX_XFER		0x2c
-#define MST_TX_XFER		0x30
-#define MST_ADDR_1		0x34
-#define MST_ADDR_2		0x38
-#define MST_DATA		0x3c
-#define MST_TX_FIFO		0x40
-#define MST_RX_FIFO		0x44
-#define MST_INT_ENABLE		0x48
-#define MST_INT_STATUS		0x4c
-#define   MST_STATUS_RFL	(1 << 13) /* RX FIFO serivce */
-#define   MST_STATUS_TFL	(1 << 12) /* TX FIFO service */
-#define   MST_STATUS_SNS	(1 << 11) /* Manual mode done */
-#define   MST_STATUS_SS		(1 << 10) /* Automatic mode done */
-#define   MST_STATUS_SCC	(1 << 9)  /* Stop complete */
-#define   MST_STATUS_IP		(1 << 8)  /* Invalid parameter */
-#define   MST_STATUS_TSS	(1 << 7)  /* Timeout */
-#define   MST_STATUS_AL		(1 << 6)  /* Arbitration lost */
-#define   MST_STATUS_ND		(1 << 5)  /* NAK on data phase */
-#define   MST_STATUS_NA		(1 << 4)  /* NAK on address phase */
-#define   MST_STATUS_NAK	(MST_STATUS_NA | \
-				 MST_STATUS_ND)
-#define   MST_STATUS_ERR	(MST_STATUS_NAK | \
-				 MST_STATUS_AL  | \
-				 MST_STATUS_IP  | \
-				 MST_STATUS_TSS)
-#define MST_TX_BYTES_XFRD	0x50
-#define MST_RX_BYTES_XFRD	0x54
-#define SCL_HIGH_PERIOD		0x80
-#define SCL_LOW_PERIOD		0x84
-#define SPIKE_FLTR_LEN		0x88
-#define SDA_SETUP_TIME		0x8c
-#define SDA_HOLD_TIME		0x90
+#define TX_FIFO_SIZE        8
+#define RX_FIFO_SIZE        8
+
+struct i2c_regs {
+	__le32 global_control;
+	__le32 interrupt_status;
+	__le32 interrupt_enable;
+	__le32 wait_timer_control;
+	__le32 ibml_timeout;
+	__le32 ibml_low_mext;
+	__le32 ibml_low_sext;
+	__le32 timer_clock_div;
+	__le32 i2c_bus_monitor;
+	__le32 soft_reset;
+	__le32 mst_command;
+#define CMD_MANUAL 0x08
+#define CMD_AUTO   0x09
+	__le32 mst_rx_xfer;
+	__le32 mst_tx_xfer;
+	__le32 mst_addr_1;
+	__le32 mst_addr_2;
+	__le32 mst_data;
+	__le32 mst_tx_fifo;
+	__le32 mst_rx_fifo;
+	__le32 mst_int_enable;
+	__le32 mst_int_status;
+#define MST_STATUS_RFL (1<<13) /* RX FIFO serivce */
+#define MST_STATUS_TFL (1<<12) /* TX FIFO service */
+#define MST_STATUS_SNS (1<<11) /* Manual mode done */
+#define MST_STATUS_SS  (1<<10) /* Automatic mode done */
+#define MST_STATUS_SCC (1<<9)  /* Stop complete */
+#define MST_STATUS_IP  (1<<8)  /* Invalid parameter */
+#define MST_STATUS_TSS (1<<7)  /* Timeout */
+#define MST_STATUS_AL  (1<<6)  /* Arbitration lost */
+#define MST_STATUS_NAK (MST_STATUS_NA | MST_STATUS_ND)
+#define MST_STATUS_ND  (1<<5)  /* NAK on data phase */
+#define MST_STATUS_NA  (1<<4)  /* NAK on address phase */
+#define MST_STATUS_ERR (MST_STATUS_NAK | \
+			MST_STATUS_AL  | \
+			MST_STATUS_IP  | \
+			MST_STATUS_TSS)
+	__le32 mst_tx_bytes_xfrd;
+	__le32 mst_rx_bytes_xfrd;
+	__le32 slv_addr_dec_ctl;
+	__le32 slv_addr_1;
+	__le32 slv_addr_2;
+	__le32 slv_rx_ctl;
+	__le32 slv_data;
+	__le32 slv_rx_fifo;
+	__le32 slv_int_enable;
+	__le32 slv_int_status;
+	__le32 slv_read_dummy;
+	__le32 reserved;
+	__le32 scl_high_period;
+	__le32 scl_low_period;
+	__le32 spike_fltr_len;
+	__le32 sda_setup_time;
+	__le32 sda_hold_time;
+	__le32 smb_alert;
+	__le32 udid_w7;
+	__le32 udid_w6;
+	__le32 udid_w5;
+	__le32 udid_w4;
+	__le32 udid_w3;
+	__le32 udid_w2;
+	__le32 udid_w1;
+	__le32 udid_w0;
+	__le32 arppec_cfg_stat;
+	__le32 slv_arp_int_enable;
+	__le32 slv_arp_int_status;
+	__le32 mst_arp_int_enable;
+	__le32 mst_arp_int_status;
+};
+
 
 /**
- * axxia_i2c_dev - I2C device context
- * @base: pointer to register struct
- * @msg: pointer to current message
- * @msg_xfrd: number of bytes transferred in msg
- * @msg_err: error code for completed message
- * @msg_complete: xfer completion object
- * @dev: device reference
- * @adapter: core i2c abstraction
- * @i2c_clk: clock reference for i2c input clock
- * @bus_clk_rate: current i2c bus clock rate
+ * I2C device context
  */
 struct axxia_i2c_dev {
-	void __iomem *base;
-	struct i2c_msg *msg;
-	size_t msg_xfrd;
-	int msg_err;
-	struct completion msg_complete;
+	/** device reference */
 	struct device *dev;
+	/** core i2c abstraction */
 	struct i2c_adapter adapter;
+	/* clock reference for i2c input clock */
 	struct clk *i2c_clk;
+	/* pointer to registers */
+	struct i2c_regs __iomem *regs;
+	/* xfer completion object */
+	struct completion msg_complete;
+	/* pointer to current message */
+	struct i2c_msg *msg;
+	/* number of bytes transferred in msg */
+	size_t msg_xfrd;
+	/* error code for completed message */
+	int msg_err;
+	/* IRQ number (or 0 if not using interrupt) */
+	int irq;
+	/* current i2c bus clock rate */
 	u32 bus_clk_rate;
 };
 
-static void i2c_int_disable(struct axxia_i2c_dev *idev, u32 mask)
+static void
+i2c_int_disable(struct axxia_i2c_dev *idev, u32 mask)
 {
-	u32 int_en;
+	u32 int_mask = readl(&idev->regs->mst_int_enable);
 
-	int_en = readl(idev->base + MST_INT_ENABLE);
-	writel(int_en & ~mask, idev->base + MST_INT_ENABLE);
+	int_mask &= ~mask;
+	writel(int_mask, &idev->regs->mst_int_enable);
 }
 
-static void i2c_int_enable(struct axxia_i2c_dev *idev, u32 mask)
+static void
+i2c_int_enable(struct axxia_i2c_dev *idev, u32 mask)
 {
-	u32 int_en;
+	u32 int_mask = readl(&idev->regs->mst_int_enable);
 
-	int_en = readl(idev->base + MST_INT_ENABLE);
-	writel(int_en | mask, idev->base + MST_INT_ENABLE);
+	int_mask |= mask;
+	writel(int_mask, &idev->regs->mst_int_enable);
 }
 
 /**
- * ns_to_clk - Convert time (ns) to clock cycles for the given clock frequency.
+ * Convert nanoseconds to clock cycles for the given clock frequency.
  */
-static u32 ns_to_clk(u64 ns, u32 clk_mhz)
+static u32
+ns_to_clk(u64 ns, u32 clk_mhz)
 {
-	return div_u64(ns * clk_mhz, 1000);
+	return div_u64(ns*clk_mhz, 1000);
 }
 
-static int axxia_i2c_init(struct axxia_i2c_dev *idev)
+static int
+axxia_i2c_init(struct axxia_i2c_dev *idev)
 {
 	u32 divisor = clk_get_rate(idev->i2c_clk) / idev->bus_clk_rate;
 	u32 clk_mhz = clk_get_rate(idev->i2c_clk) / 1000000;
@@ -142,9 +171,9 @@ static int axxia_i2c_init(struct axxia_i2c_dev *idev)
 		idev->bus_clk_rate, clk_mhz, divisor);
 
 	/* Reset controller */
-	writel(0x01, idev->base + SOFT_RESET);
+	writel(0x01, &idev->regs->soft_reset);
 	timeout = jiffies + msecs_to_jiffies(100);
-	while (readl(idev->base + SOFT_RESET) & 1) {
+	while (readl(&idev->regs->soft_reset) & 1) {
 		if (time_after(jiffies, timeout)) {
 			dev_warn(idev->dev, "Soft reset failed\n");
 			break;
@@ -152,35 +181,37 @@ static int axxia_i2c_init(struct axxia_i2c_dev *idev)
 	}
 
 	/* Enable Master Mode */
-	writel(0x1, idev->base + GLOBAL_CONTROL);
+	writel(0x1, &idev->regs->global_control);
 
 	if (idev->bus_clk_rate <= 100000) {
 		/* Standard mode SCL 50/50, tSU:DAT = 250 ns */
-		t_high = divisor * 1 / 2;
-		t_low = divisor * 1 / 2;
+		t_high  = divisor*1/2;
+		t_low   = divisor*1/2;
 		t_setup = ns_to_clk(250, clk_mhz);
 	} else {
 		/* Fast mode SCL 33/66, tSU:DAT = 100 ns */
-		t_high = divisor * 1 / 3;
-		t_low = divisor * 2 / 3;
+		t_high  = divisor*1/3;
+		t_low   = divisor*2/3;
 		t_setup = ns_to_clk(100, clk_mhz);
 	}
 
 	/* SCL High Time */
-	writel(t_high, idev->base + SCL_HIGH_PERIOD);
+	writel(t_high, &idev->regs->scl_high_period);
 	/* SCL Low Time */
-	writel(t_low, idev->base + SCL_LOW_PERIOD);
+	writel(t_low, &idev->regs->scl_low_period);
 	/* SDA Setup Time */
-	writel(t_setup, idev->base + SDA_SETUP_TIME);
+	writel(t_setup, &idev->regs->sda_setup_time);
 	/* SDA Hold Time, 300ns */
-	writel(ns_to_clk(300, clk_mhz), idev->base + SDA_HOLD_TIME);
+	writel(ns_to_clk(300, clk_mhz), &idev->regs->sda_hold_time);
 	/* Filter <50ns spikes */
-	writel(ns_to_clk(50, clk_mhz), idev->base + SPIKE_FLTR_LEN);
+	writel(ns_to_clk(50, clk_mhz), &idev->regs->spike_fltr_len);
 
 	/* Configure Time-Out Registers */
 	tmo_clk = ns_to_clk(SCL_WAIT_TIMEOUT_NS, clk_mhz);
 
-	/* Find prescaler value that makes tmo_clk fit in 15-bits counter. */
+	/*
+	   Find the prescaler value that makes tmo_clk fit in 15-bits counter.
+	 */
 	for (prescale = 0; prescale < 15; ++prescale) {
 		if (tmo_clk <= 0x7fff)
 			break;
@@ -190,59 +221,72 @@ static int axxia_i2c_init(struct axxia_i2c_dev *idev)
 		tmo_clk = 0x7fff;
 
 	/* Prescale divider (log2) */
-	writel(prescale, idev->base + TIMER_CLOCK_DIV);
+	writel(prescale, &idev->regs->timer_clock_div);
 	/* Timeout in divided clocks */
-	writel(WT_EN | WT_VALUE(tmo_clk), idev->base + WAIT_TIMER_CONTROL);
+	writel((1<<15) | tmo_clk, &idev->regs->wait_timer_control);
 
 	/* Mask all master interrupt bits */
 	i2c_int_disable(idev, ~0);
 
 	/* Interrupt enable */
-	writel(0x01, idev->base + INTERRUPT_ENABLE);
+	writel(0x01, &idev->regs->interrupt_enable);
+
+	dev_dbg(idev->dev, "SDA_SETUP:        %08x\n",
+		readl(&idev->regs->sda_setup_time));
+	dev_dbg(idev->dev, "SDA_HOLD:         %08x\n",
+		readl(&idev->regs->sda_hold_time));
+	dev_dbg(idev->dev, "SPIKE_FILTER_LEN: %08x\n",
+		readl(&idev->regs->spike_fltr_len));
+	dev_dbg(idev->dev, "TIMER_DIV:        %08x\n",
+		readl(&idev->regs->timer_clock_div));
+	dev_dbg(idev->dev, "WAIT_TIMER:       %08x\n",
+		readl(&idev->regs->wait_timer_control));
 
 	return 0;
 }
 
-static int i2c_m_rd(const struct i2c_msg *msg)
+static int
+i2c_m_rd(const struct i2c_msg *msg)
 {
 	return (msg->flags & I2C_M_RD) != 0;
 }
 
-static int i2c_m_ten(const struct i2c_msg *msg)
+static int
+i2c_m_ten(const struct i2c_msg *msg)
 {
 	return (msg->flags & I2C_M_TEN) != 0;
 }
 
-static int i2c_m_recv_len(const struct i2c_msg *msg)
+static int
+i2c_m_recv_len(const struct i2c_msg *msg)
 {
 	return (msg->flags & I2C_M_RECV_LEN) != 0;
 }
 
-/**
- * axxia_i2c_empty_rx_fifo - Fetch data from RX FIFO and update SMBus block
- * transfer length if this is the first byte of such a transfer.
- */
-static int axxia_i2c_empty_rx_fifo(struct axxia_i2c_dev *idev)
+static int
+axxia_i2c_empty_rx_fifo(struct axxia_i2c_dev *idev)
 {
 	struct i2c_msg *msg = idev->msg;
-	size_t rx_fifo_avail = readl(idev->base + MST_RX_FIFO);
+	size_t rx_fifo_avail = readl(&idev->regs->mst_rx_fifo);
 	int bytes_to_transfer = min(rx_fifo_avail, msg->len - idev->msg_xfrd);
 
-	while (bytes_to_transfer-- > 0) {
-		int c = readl(idev->base + MST_DATA);
+	while (0 < bytes_to_transfer--) {
+		int c = readl(&idev->regs->mst_data);
 
 		if (idev->msg_xfrd == 0 && i2c_m_recv_len(msg)) {
 			/*
 			 * Check length byte for SMBus block read
 			 */
-			if (c <= 0 || c > I2C_SMBUS_BLOCK_MAX) {
+			if (c <= 0) {
 				idev->msg_err = -EPROTO;
 				i2c_int_disable(idev, ~0);
 				complete(&idev->msg_complete);
 				break;
+			} else if (c > I2C_SMBUS_BLOCK_MAX) {
+				c = I2C_SMBUS_BLOCK_MAX;
 			}
 			msg->len = 1 + c;
-			writel(msg->len, idev->base + MST_RX_XFER);
+			writel(msg->len, &idev->regs->mst_rx_xfer);
 		}
 		msg->buf[idev->msg_xfrd++] = c;
 	}
@@ -250,38 +294,51 @@ static int axxia_i2c_empty_rx_fifo(struct axxia_i2c_dev *idev)
 	return 0;
 }
 
-/**
- * axxia_i2c_fill_tx_fifo - Fill TX FIFO from current message buffer.
- * @return: Number of bytes left to transfer.
- */
-static int axxia_i2c_fill_tx_fifo(struct axxia_i2c_dev *idev)
+static int
+axxia_i2c_fill_tx_fifo(struct axxia_i2c_dev *idev)
 {
 	struct i2c_msg *msg = idev->msg;
-	size_t tx_fifo_avail = FIFO_SIZE - readl(idev->base + MST_TX_FIFO);
+	size_t tx_fifo_avail = TX_FIFO_SIZE - readl(&idev->regs->mst_tx_fifo);
 	int bytes_to_transfer = min(tx_fifo_avail, msg->len - idev->msg_xfrd);
-	int ret = msg->len - idev->msg_xfrd - bytes_to_transfer;
 
-	while (bytes_to_transfer-- > 0)
-		writel(msg->buf[idev->msg_xfrd++], idev->base + MST_DATA);
+	while (0 < bytes_to_transfer--)
+		writel(msg->buf[idev->msg_xfrd++], &idev->regs->mst_data);
 
-	return ret;
+	return 0;
 }
 
-static irqreturn_t axxia_i2c_isr(int irq, void *_dev)
+static char *
+status_str(u32 status)
 {
-	struct axxia_i2c_dev *idev = _dev;
-	u32 status;
-
-	if (!(readl(idev->base + INTERRUPT_STATUS) & INT_MST))
-		return IRQ_NONE;
-
-	/* Read interrupt status bits */
-	status = readl(idev->base + MST_INT_STATUS);
+	static char buf[128];
+
+	buf[0] = '\0';
+
+	if (status & MST_STATUS_RFL)
+		strcat(buf, "RFL ");
+	if (status & MST_STATUS_TFL)
+		strcat(buf, "TFL ");
+	if (status & MST_STATUS_SNS)
+		strcat(buf, "SNS ");
+	if (status & MST_STATUS_SS)
+		strcat(buf, "SS ");
+	if (status & MST_STATUS_SCC)
+		strcat(buf, "SCC ");
+	if (status & MST_STATUS_TSS)
+		strcat(buf, "TSS ");
+	if (status & MST_STATUS_AL)
+		strcat(buf, "AL ");
+	if (status & MST_STATUS_ND)
+		strcat(buf, "ND ");
+	if (status & MST_STATUS_NA)
+		strcat(buf, "NA ");
+	return buf;
+}
 
-	if (!idev->msg) {
-		dev_warn(idev->dev, "unexpected interrupt\n");
-		goto out;
-	}
+static void
+axxia_i2c_service_irq(struct axxia_i2c_dev *idev)
+{
+	u32 status = readl(&idev->regs->mst_int_status);
 
 	/* RX FIFO needs service? */
 	if (i2c_m_rd(idev->msg) && (status & MST_STATUS_RFL))
@@ -289,135 +346,165 @@ static irqreturn_t axxia_i2c_isr(int irq, void *_dev)
 
 	/* TX FIFO needs service? */
 	if (!i2c_m_rd(idev->msg) && (status & MST_STATUS_TFL)) {
-		if (axxia_i2c_fill_tx_fifo(idev) == 0)
+		if (idev->msg_xfrd < idev->msg->len)
+			axxia_i2c_fill_tx_fifo(idev);
+		else
 			i2c_int_disable(idev, MST_STATUS_TFL);
 	}
 
 	if (status & MST_STATUS_SCC) {
-		/* Stop completed */
+		/* Stop completed? */
 		i2c_int_disable(idev, ~0);
 		complete(&idev->msg_complete);
-	} else if (status & MST_STATUS_SNS) {
-		/* Transfer done */
-		i2c_int_disable(idev, ~0);
+	} else if (status & (MST_STATUS_SNS | MST_STATUS_SS)) {
+		/* Transfer done? */
 		if (i2c_m_rd(idev->msg) && idev->msg_xfrd < idev->msg->len)
 			axxia_i2c_empty_rx_fifo(idev);
+		i2c_int_disable(idev, ~0);
 		complete(&idev->msg_complete);
 	} else if (unlikely(status & MST_STATUS_ERR)) {
-		/* Transfer error */
+		/* Transfer error? */
+		idev->msg_err = status & MST_STATUS_ERR;
 		i2c_int_disable(idev, ~0);
-		if (status & MST_STATUS_AL)
-			idev->msg_err = -EAGAIN;
-		else if (status & MST_STATUS_NAK)
-			idev->msg_err = -ENXIO;
-		else
-			idev->msg_err = -EIO;
-		dev_dbg(idev->dev, "error %#x, addr=%#x rx=%u/%u tx=%u/%u\n",
-			status,
-			idev->msg->addr,
-			readl(idev->base + MST_RX_BYTES_XFRD),
-			readl(idev->base + MST_RX_XFER),
-			readl(idev->base + MST_TX_BYTES_XFRD),
-			readl(idev->base + MST_TX_XFER));
+		dev_dbg(idev->dev, "error %s, rx=%u/%u tx=%u/%u\n",
+			status_str(status),
+			readl(&idev->regs->mst_rx_bytes_xfrd),
+			readl(&idev->regs->mst_rx_xfer),
+			readl(&idev->regs->mst_tx_bytes_xfrd),
+			readl(&idev->regs->mst_tx_xfer));
 		complete(&idev->msg_complete);
 	}
+}
+
+static irqreturn_t
+axxia_i2c_isr(int irq, void *_dev)
+{
+	struct axxia_i2c_dev *idev = _dev;
+
+	if ((readl(&idev->regs->interrupt_status) & 0x1) == 0)
+		return IRQ_NONE;
+
+	if (!idev->msg)
+		return IRQ_NONE;
+
+	axxia_i2c_service_irq(idev);
 
-out:
 	/* Clear interrupt */
-	writel(INT_MST, idev->base + INTERRUPT_STATUS);
+	writel(0x01, &idev->regs->interrupt_status);
 
 	return IRQ_HANDLED;
 }
 
-static int axxia_i2c_xfer_msg(struct axxia_i2c_dev *idev, struct i2c_msg *msg)
+
+static int
+axxia_i2c_xfer_msg(struct axxia_i2c_dev *idev, struct i2c_msg *msg)
 {
 	u32 int_mask = MST_STATUS_ERR | MST_STATUS_SNS;
-	u32 rx_xfer, tx_xfer;
 	u32 addr_1, addr_2;
-	unsigned long time_left;
+	int ret;
 
-	idev->msg = msg;
+	if (msg->len == 0 || msg->len > 255)
+		return -EINVAL;
+
+	idev->msg      = msg;
 	idev->msg_xfrd = 0;
-	idev->msg_err = 0;
+	idev->msg_err  = 0;
 	reinit_completion(&idev->msg_complete);
 
+	if (i2c_m_rd(msg)) {
+		/* TX 0 bytes */
+		writel(0, &idev->regs->mst_tx_xfer);
+		/* RX # bytes */
+		if (i2c_m_recv_len(msg))
+			writel(I2C_SMBUS_BLOCK_MAX, &idev->regs->mst_rx_xfer);
+		else
+			writel(msg->len, &idev->regs->mst_rx_xfer);
+	} else {
+		/* TX # bytes */
+		writel(msg->len, &idev->regs->mst_tx_xfer);
+		/* RX 0 bytes */
+		writel(0, &idev->regs->mst_rx_xfer);
+	}
+
 	if (i2c_m_ten(msg)) {
 		/* 10-bit address
-		 *   addr_1: 5'b11110 | addr[9:8] | (R/nW)
+		 *   addr_1: 5'b11110 | addr[9:8] | (R/W)
 		 *   addr_2: addr[7:0]
 		 */
 		addr_1 = 0xF0 | ((msg->addr >> 7) & 0x06);
 		addr_2 = msg->addr & 0xFF;
 	} else {
 		/* 7-bit address
-		 *   addr_1: addr[6:0] | (R/nW)
+		 *   addr_1: addr[6:0] | (R/W)
 		 *   addr_2: dont care
 		 */
 		addr_1 = (msg->addr << 1) & 0xFF;
 		addr_2 = 0;
 	}
+	if (i2c_m_rd(msg))
+		addr_1 |= 1;
+	writel(addr_1, &idev->regs->mst_addr_1);
+	writel(addr_2, &idev->regs->mst_addr_2);
 
 	if (i2c_m_rd(msg)) {
-		/* I2C read transfer */
-		rx_xfer = i2c_m_recv_len(msg) ? I2C_SMBUS_BLOCK_MAX : msg->len;
-		tx_xfer = 0;
-		addr_1 |= 1;	/* Set the R/nW bit of the address */
+		int_mask |= MST_STATUS_RFL;
 	} else {
-		/* I2C write transfer */
-		rx_xfer = 0;
-		tx_xfer = msg->len;
+		axxia_i2c_fill_tx_fifo(idev);
+		if (idev->msg_xfrd < msg->len)
+			int_mask |= MST_STATUS_TFL;
 	}
 
-	writel(rx_xfer, idev->base + MST_RX_XFER);
-	writel(tx_xfer, idev->base + MST_TX_XFER);
-	writel(addr_1, idev->base + MST_ADDR_1);
-	writel(addr_2, idev->base + MST_ADDR_2);
-
-	if (i2c_m_rd(msg))
-		int_mask |= MST_STATUS_RFL;
-	else if (axxia_i2c_fill_tx_fifo(idev) != 0)
-		int_mask |= MST_STATUS_TFL;
-
 	/* Start manual mode */
-	writel(CMD_MANUAL, idev->base + MST_COMMAND);
-
-	i2c_int_enable(idev, int_mask);
-
-	time_left = wait_for_completion_timeout(&idev->msg_complete,
-					      I2C_XFER_TIMEOUT);
-
-	i2c_int_disable(idev, int_mask);
+	writel(0x8, &idev->regs->mst_command);
+
+	if (idev->irq > 0) {
+		i2c_int_enable(idev, int_mask);
+		ret = wait_for_completion_timeout(&idev->msg_complete,
+						  I2C_XFER_TIMEOUT);
+		i2c_int_disable(idev, int_mask);
+		WARN_ON(readl(&idev->regs->mst_command) & 0x8);
+	} else {
+		unsigned long tmo = jiffies + I2C_XFER_TIMEOUT;
 
-	if (readl(idev->base + MST_COMMAND) & CMD_BUSY)
-		dev_warn(idev->dev, "busy after xfer\n");
+		do {
+			/* Poll interrupt status */
+			axxia_i2c_service_irq(idev);
+			ret = try_wait_for_completion(&idev->msg_complete);
+		} while (!ret && time_before(jiffies, tmo));
+	}
 
-	if (time_left == 0)
-		idev->msg_err = -ETIMEDOUT;
+	if (ret == 0) {
+		dev_warn(idev->dev, "xfer timeout (%#x)\n", msg->addr);
+		axxia_i2c_init(idev);
+		return -ETIMEDOUT;
+	}
 
-	if (unlikely(idev->msg_err) && idev->msg_err != -ENXIO)
+	if (unlikely(idev->msg_err != 0)) {
 		axxia_i2c_init(idev);
+		return -EIO;
+	}
 
-	return idev->msg_err;
+	return 0;
 }
 
-static int axxia_i2c_stop(struct axxia_i2c_dev *idev)
+static int
+axxia_i2c_stop(struct axxia_i2c_dev *idev)
 {
 	u32 int_mask = MST_STATUS_ERR | MST_STATUS_SCC;
-	unsigned long time_left;
+	int ret;
 
 	reinit_completion(&idev->msg_complete);
 
 	/* Issue stop */
-	writel(0xb, idev->base + MST_COMMAND);
+	writel(0xb, &idev->regs->mst_command);
 	i2c_int_enable(idev, int_mask);
-	time_left = wait_for_completion_timeout(&idev->msg_complete,
-					      I2C_STOP_TIMEOUT);
+	ret = wait_for_completion_timeout(&idev->msg_complete,
+					  I2C_STOP_TIMEOUT);
 	i2c_int_disable(idev, int_mask);
-	if (time_left == 0)
+	if (ret == 0)
 		return -ETIMEDOUT;
 
-	if (readl(idev->base + MST_COMMAND) & CMD_BUSY)
-		dev_warn(idev->dev, "busy after stop\n");
+	WARN_ON(readl(&idev->regs->mst_command) & 0x8);
 
 	return 0;
 }
@@ -429,38 +516,36 @@ axxia_i2c_xfer(struct i2c_adapter *adap, struct i2c_msg msgs[], int num)
 	int i;
 	int ret = 0;
 
-	for (i = 0; ret == 0 && i < num; ++i)
+	for (i = 0; ret == 0 && i < num; i++)
 		ret = axxia_i2c_xfer_msg(idev, &msgs[i]);
 
 	axxia_i2c_stop(idev);
 
-	return ret ? : i;
+	return ret ?: i;
 }
 
-static u32 axxia_i2c_func(struct i2c_adapter *adap)
+static u32
+axxia_i2c_func(struct i2c_adapter *adap)
 {
-	u32 caps = (I2C_FUNC_I2C | I2C_FUNC_10BIT_ADDR |
-		    I2C_FUNC_SMBUS_EMUL | I2C_FUNC_SMBUS_BLOCK_DATA);
+	u32 caps = (I2C_FUNC_I2C |
+		    I2C_FUNC_10BIT_ADDR |
+		    I2C_FUNC_SMBUS_EMUL |
+		    I2C_FUNC_SMBUS_BLOCK_DATA);
 	return caps;
 }
 
 static const struct i2c_algorithm axxia_i2c_algo = {
-	.master_xfer = axxia_i2c_xfer,
-	.functionality = axxia_i2c_func,
+	.master_xfer	= axxia_i2c_xfer,
+	.functionality	= axxia_i2c_func,
 };
 
-static struct i2c_adapter_quirks axxia_i2c_quirks = {
-	.max_read_len = 255,
-	.max_write_len = 255,
-};
-
-static int axxia_i2c_probe(struct platform_device *pdev)
+static int
+axxia_i2c_probe(struct platform_device *pdev)
 {
 	struct device_node *np = pdev->dev.of_node;
 	struct axxia_i2c_dev *idev = NULL;
 	struct resource *res;
 	void __iomem *base;
-	int irq;
 	int ret = 0;
 
 	idev = devm_kzalloc(&pdev->dev, sizeof(*idev), GFP_KERNEL);
@@ -472,83 +557,99 @@ static int axxia_i2c_probe(struct platform_device *pdev)
 	if (IS_ERR(base))
 		return PTR_ERR(base);
 
-	irq = platform_get_irq(pdev, 0);
-	if (irq < 0) {
-		dev_err(&pdev->dev, "missing interrupt resource\n");
-		return irq;
-	}
+	idev->irq = platform_get_irq(pdev, 0);
+	if (idev->irq < 0)
+		dev_info(&pdev->dev, "No IRQ specified, using polling mode\n");
 
 	idev->i2c_clk = devm_clk_get(&pdev->dev, "i2c");
 	if (IS_ERR(idev->i2c_clk)) {
-		dev_err(&pdev->dev, "missing clock\n");
+		dev_err(&pdev->dev, "missing I2C bus clock");
 		return PTR_ERR(idev->i2c_clk);
 	}
 
-	idev->base = base;
+	idev->regs = (struct i2c_regs __iomem *) base;
 	idev->dev = &pdev->dev;
 	init_completion(&idev->msg_complete);
 
 	of_property_read_u32(np, "clock-frequency", &idev->bus_clk_rate);
 	if (idev->bus_clk_rate == 0)
-		idev->bus_clk_rate = 100000;	/* default clock rate */
+		idev->bus_clk_rate = 100000; /* default clock rate */
 
 	ret = axxia_i2c_init(idev);
 	if (ret) {
-		dev_err(&pdev->dev, "failed to initialize\n");
+		dev_err(&pdev->dev, "Failed to initialize i2c controller");
 		return ret;
 	}
 
-	ret = devm_request_irq(&pdev->dev, irq, axxia_i2c_isr, 0,
-			       pdev->name, idev);
-	if (ret) {
-		dev_err(&pdev->dev, "failed to claim IRQ%d\n", irq);
-		return ret;
+	if (idev->irq >= 0) {
+		ret = devm_request_irq(&pdev->dev, idev->irq, axxia_i2c_isr, 0,
+				       pdev->name, idev);
+		if (ret) {
+			dev_err(&pdev->dev, "can't claim irq %d\n", idev->irq);
+			return ret;
+		}
 	}
 
-	clk_prepare_enable(idev->i2c_clk);
+	clk_enable(idev->i2c_clk);
 
 	i2c_set_adapdata(&idev->adapter, idev);
 	strlcpy(idev->adapter.name, pdev->name, sizeof(idev->adapter.name));
 	idev->adapter.owner = THIS_MODULE;
+	idev->adapter.class = I2C_CLASS_HWMON;
 	idev->adapter.algo = &axxia_i2c_algo;
-	idev->adapter.quirks = &axxia_i2c_quirks;
 	idev->adapter.dev.parent = &pdev->dev;
 	idev->adapter.dev.of_node = pdev->dev.of_node;
 
-	platform_set_drvdata(pdev, idev);
-
 	ret = i2c_add_adapter(&idev->adapter);
 	if (ret) {
-		dev_err(&pdev->dev, "failed to add adapter\n");
+		dev_err(&pdev->dev, "Failed to add I2C adapter\n");
 		return ret;
 	}
 
+	platform_set_drvdata(pdev, idev);
+
 	return 0;
 }
 
-static int axxia_i2c_remove(struct platform_device *pdev)
+static int
+axxia_i2c_remove(struct platform_device *pdev)
 {
 	struct axxia_i2c_dev *idev = platform_get_drvdata(pdev);
 
-	clk_disable_unprepare(idev->i2c_clk);
 	i2c_del_adapter(&idev->adapter);
-
 	return 0;
 }
 
+#ifdef CONFIG_PM
+static int axxia_i2c_suspend(struct platform_device *pdev, pm_message_t state)
+{
+	return -EOPNOTSUPP;
+}
+
+static int axxia_i2c_resume(struct platform_device *pdev)
+{
+	return -EOPNOTSUPP;
+}
+#else
+#define axxia_i2c_suspend NULL
+#define axxia_i2c_resume NULL
+#endif
+
 /* Match table for of_platform binding */
 static const struct of_device_id axxia_i2c_of_match[] = {
 	{ .compatible = "lsi,api2c", },
 	{},
 };
-
 MODULE_DEVICE_TABLE(of, axxia_i2c_of_match);
 
 static struct platform_driver axxia_i2c_driver = {
-	.probe = axxia_i2c_probe,
-	.remove = axxia_i2c_remove,
-	.driver = {
-		.name = "axxia-i2c",
+	.probe   = axxia_i2c_probe,
+	.remove  = axxia_i2c_remove,
+	.suspend = axxia_i2c_suspend,
+	.resume  = axxia_i2c_resume,
+	.driver  = {
+		.name  = "axxia-i2c",
+		.owner = THIS_MODULE,
 		.of_match_table = axxia_i2c_of_match,
 	},
 };
diff --git a/include/linux/i2c-axxia.h b/include/linux/i2c-axxia.h
new file mode 100644
index 0000000..435cd33
--- /dev/null
+++ b/include/linux/i2c-axxia.h
@@ -0,0 +1,39 @@
+#ifndef __I2C_AXXIA_H__
+#define __I2C_AXXIA_H__
+
+#include <linux/platform_device.h>
+
+/*
+ * Version 2 of the I2C peripheral unit has a different register
+ * layout and extra registers.  The ID register in the V2 peripheral
+ * unit on the AXXIA4430 reports the same ID as the V1 peripheral
+ * unit on the AXXIA3530, so we must inform the driver which IP
+ * version we know it is running on from platform / cpu-specific
+ * code using these constants in the hwmod class definition.
+ */
+
+#define AXXIA_I2C_IP_VERSION_1 1                /* ACP34xx */
+#define AXXIA_I2C_IP_VERSION_2 2                /* AXM55xx */
+
+/* struct axxia_i2c_bus_platform_data .flags meanings */
+#define AXXIA_I2C_FLAGS_NONE            (0x00000000)
+
+
+/*
+ * Maximum byte size of I2C bus name string including null terminator
+ */
+#define MAX_AXXIA_I2C_HWMOD_NAME_LEN    16
+
+
+struct axxia_i2c_bus_platform_data {
+	struct device_node     *node;
+	char                    name[MAX_AXXIA_I2C_HWMOD_NAME_LEN];
+	u32		        index;
+	u32		        rev;
+	u32		        flags;
+	u32		        bus_nr;
+	struct resource         dev_space;
+	struct resource         int_space;
+};
+
+#endif
-- 
1.7.5.4

