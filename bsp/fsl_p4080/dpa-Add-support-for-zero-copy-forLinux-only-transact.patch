From 45bbb34388e0d6d634ed81408847f2d7650a3332 Mon Sep 17 00:00:00 2001
From: Andy Fleming <afleming@freescale.com>
Date: Fri, 7 Aug 2009 11:49:46 +0800
Subject: [PATCH 025/148] dpa: Add support for zero-copy forLinux-only transactions

Linux does not have a way of returning buffers to arbitrary software
drivers when it is done with them, so we need to allocate new buffers to
replace the ones we give to the stack.  This is the mechanism:

Each pool maintains an array of pointers to the pages in which these
buffers exist.  For each buffer in the pool, there is a page pointer in
the array, which is used to remember the mapping between virtual and
physical addresses for the page.  When a buffer is received, its
corresponding page pointer in the array is found, and used to link up
the skb, and unmap the buffer (wrt to the DPA enet).  The page struct's
_count field is appropriately incremented such that the page will be
freed when the kernel is done with *all* buffers contained therein.

Signed-off-by: Andy Fleming <afleming@freescale.com>
[Cleanly applied the FSL SDK 2.0.3 patch:
"0004-dpa-Add-support-for-zero-copy.patch"]
Integrated-by: Yuri Nedel <Yuri.Nedel@windriver.com>
---
 drivers/net/dpa/dpa.c |  391 ++++++++++++++++++++++++++++++++++---------------
 1 files changed, 273 insertions(+), 118 deletions(-)

diff --git a/drivers/net/dpa/dpa.c b/drivers/net/dpa/dpa.c
index 46c9558..b3c3d10 100644
--- a/drivers/net/dpa/dpa.c
+++ b/drivers/net/dpa/dpa.c
@@ -30,6 +30,7 @@
  * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  */
 
+#define CONFIG_DPA_RX_0_COPY 1
 #include <linux/init.h>
 #include <linux/module.h>
 #include <linux/sort.h>
@@ -42,6 +43,8 @@
 #include <linux/ip.h>		/* struct iphdr */
 #include <linux/udp.h>		/* struct udphdr */
 #include <linux/tcp.h>		/* struct tcphdr */
+#include <linux/spinlock.h>
+#include <linux/highmem.h>
 #endif
 #include <linux/percpu.h>
 
@@ -104,19 +107,180 @@ static const char rtx[][3] = {
 struct dpa_bp {
 	struct bman_pool		*pool;
 	union {
-		struct list_head	 list;
-		uint8_t			 bpid;
+		struct list_head	list;
+		uint8_t			bpid;
+	};
+	size_t				count;
+	size_t				size;
+	unsigned int			bp_kick_thresh;
+	unsigned int			bp_blocks_per_page;
+	char				kernel_pool;  /* kernel-owned pool */
+	spinlock_t			lock;
+	unsigned int			bp_refill_pending;
+	union {
+		struct page		**rxhash;
+		struct {
+			dma_addr_t	paddr;
+			void *		vaddr;
+		};
 	};
-	size_t				 count;
-	size_t				 size;
-	dma_addr_t			 paddr;
-	void				*vaddr;
 };
 
 static const size_t dpa_bp_size[] __devinitconst = {
 	DPA_BP_SIZE(128), DPA_BP_SIZE(512), DPA_BP_SIZE(1536)	/* Keep these sorted */
 };
 
+static unsigned int dpa_hash_rxaddr(const struct dpa_bp *bp, dma_addr_t a)
+{
+	a >>= PAGE_SHIFT;
+	a ^= (a >> ilog2(bp->count));
+
+	return (a % bp->count);
+}
+
+static struct page **dpa_find_rxpage(const struct dpa_bp *bp, dma_addr_t addr)
+{
+	unsigned int h = dpa_hash_rxaddr(bp, addr);
+	int i, j;
+
+	addr &= PAGE_MASK;
+	for (i = h, j = 0; j < bp->count; j++, i = (i + 1) % bp->count)
+		if (bp->rxhash[i] && bp->rxhash[i]->index == addr)
+			return &bp->rxhash[i];
+
+	return NULL;
+}
+
+static void dpa_hash_page(struct dpa_bp *bp, struct page *page, dma_addr_t base)
+{
+	unsigned int h = dpa_hash_rxaddr(bp, base);
+	int i, j;
+
+	page->index = base & PAGE_MASK;
+
+	/* If the entry for this hash is missing, just find the next free one */
+	for (i = h, j = 0; j < bp->count; j++, i = (i + 1) % bp->count) {
+		if (bp->rxhash[i])
+			continue;
+
+		bp->rxhash[i] = page;
+		return;
+	}
+
+	BUG();
+}
+
+static void bmb_free(const struct dpa_priv_s *priv, struct dpa_bp *bp, struct bm_buffer *bmb)
+{
+	int i;
+	/* Go through the bmb array, and free/unmap every buffer remaining in it */
+	for (i = 0; i < 8; i++) {
+		struct page **pageptr;
+		struct page *page;
+		unsigned long flags;
+
+		if (!bmb[i].lo)
+			break;
+
+		spin_lock_irqsave(&bp->lock, flags);
+		pageptr = dpa_find_rxpage(bp, bmb[i].lo);
+
+		BUG_ON(!pageptr);
+
+		page = *pageptr;
+		*pageptr = NULL;
+
+		spin_unlock_irqrestore(&bp->lock, flags);
+
+		dma_unmap_page(priv->net_dev->dev.parent, bmb[i].lo,
+				bp->size, DMA_FROM_DEVICE);
+
+		put_page(page);
+	}
+}
+
+static void dpa_bp_refill(const struct dpa_priv_s *priv, struct dpa_bp *bp,
+			gfp_t mask)
+{
+	struct bm_buffer bmb[8];
+	int err;
+	struct net_device *dev = priv->net_dev;
+	unsigned int blocks;
+	unsigned int blocks_per_page = bp->bp_blocks_per_page;
+	unsigned int block_size = bp->size;
+	struct page *page = NULL;
+	unsigned long flags;
+	int i;
+
+	spin_lock_irqsave(&bp->lock, flags);
+	/* Round down to an integral number of pages */
+	blocks = (bp->bp_refill_pending / blocks_per_page) * blocks_per_page;
+
+	bp->bp_refill_pending -= blocks;
+
+	spin_unlock_irqrestore(&bp->lock, flags);
+
+	for (i = 0; i < blocks; i++) {
+		dma_addr_t addr;
+		unsigned int off = (i % blocks_per_page) * block_size;
+
+		/* Do page allocation every time we need a new page */
+		if ((i % blocks_per_page) == 0) {
+			page = alloc_page(mask);
+
+			if (!page)
+				break;
+
+			/* Update the reference count to reflect # of buffers */
+			if (blocks_per_page > 1)
+				atomic_add(blocks_per_page - 1,
+						&compound_head(page)->_count);
+		}
+
+		addr = dma_map_page(dev->dev.parent, page, off, block_size,
+					DMA_FROM_DEVICE);
+
+		spin_lock_irqsave(&bp->lock, flags);
+		/* Add the page to the hash table */
+		dpa_hash_page(bp, page, addr);
+
+		spin_unlock_irqrestore(&bp->lock, flags);
+
+		/*
+ 		 * Fill the release buffer to release as many at a time as
+ 		 * is possible.
+ 		 */
+		bmb[i % 8].hi = 0;
+		bmb[i % 8].lo = addr;
+
+		/* When we get to the end of the buffer, send it all to bman */
+		if ((i % 8) == 7) {
+			err = bman_release(bp->pool, bmb, 8,
+						BMAN_RELEASE_FLAG_WAIT_INT);
+
+			if (err < 0) {
+				bmb_free(priv, bp, bmb);
+				return;
+			}
+		}
+	}
+
+	/* Take care of the leftovers ('i' will be one past the last block done) */
+	if ((i % 8) != 0) {
+		err = bman_release(bp->pool, bmb, i % 8,
+				BMAN_RELEASE_FLAG_WAIT_INT);
+
+		if (err < 0) {
+			int j;
+
+			for (j = i % 8; j < 8; j++)
+				bmb[j].lo = 0;
+			bmb_free(priv, bp, bmb);
+			return;
+		}
+	}
+}
+
 static void __cold dpa_bp_depletion(struct bman_portal	*portal,
 				    struct bman_pool	*pool,
 				    void		*cb_ctx,
@@ -130,77 +294,60 @@ static void __cold dpa_bp_depletion(struct bman_portal	*portal,
 }
 
 static int __devinit __must_check __cold __attribute__((nonnull))
-_dpa_bp_alloc(struct device *dev, struct list_head *list, struct dpa_bp *dpa_bp)
+_dpa_bp_alloc(struct net_device *dev, struct list_head *list, struct dpa_bp *dpa_bp)
 {
-	int			 _errno, i, j;
+	int			 _errno;
 	struct bman_pool_params	 bp_params;
-	struct bm_buffer	 bmb[8];
+	struct dpa_priv_s *priv = netdev_priv(dev);
 
 	BUG_ON(dpa_bp->size == 0);
 	BUG_ON(dpa_bp->count == 0);
 
-	if (dpa_bp->bpid == 0) {
-		_errno = bm_pool_new(&bp_params.bpid);
-		if (unlikely(_errno < 0)) {
-			cpu_dev_err(dev, "%s:%hu:%s(): bm_pool_new() = %d\n",
-				    __file__, __LINE__, __func__, _errno);
-			goto _return;
-		}
-	} else
-		bp_params.bpid = dpa_bp->bpid;
-
 	bp_params.flags		= BMAN_POOL_FLAG_DEPLETION;
 	bp_params.cb		= dpa_bp_depletion;
 	bp_params.cb_ctx	= dpa_bp;
 
+	if (dpa_bp->bpid == 0)
+		bp_params.flags |= BMAN_POOL_FLAG_DYNAMIC_BPID;
+	else
+		bp_params.bpid = dpa_bp->bpid;
+
 	dpa_bp->pool = bman_new_pool(&bp_params);
 	if (unlikely(dpa_bp->pool == NULL)) {
-		cpu_dev_err(dev, "%s:%hu:%s(): bman_new_pool() failed\n",
+		cpu_dev_err(dev->dev.parent, "%s:%hu:%s(): bman_new_pool() failed\n",
 			    __file__, __LINE__, __func__);
 		_errno = -ENOMEM;
 		goto _return_bm_pool_free;
 	}
 
+	/* paddr is only set for pools that are shared between partitions */
 	if (dpa_bp->paddr == 0) {
-		dpa_bp->vaddr = alloc_pages_exact(dpa_bp->size * dpa_bp->count, GFP_DPA_BP);
-		if (unlikely(dpa_bp->vaddr == NULL)) {
-			cpu_dev_err(dev, "%s:%hu:%s(): alloc_pages_exact() failed\n",
-				    __file__, __LINE__, __func__);
+		dpa_bp->rxhash = kzalloc(dpa_bp->count * sizeof(struct page *),
+					 GFP_KERNEL);
+		if (!dpa_bp->rxhash) {
 			_errno = -ENOMEM;
 			goto _return_bman_free_pool;
 		}
 
-		dpa_bp->paddr = dma_map_single(dev, dpa_bp->vaddr, dpa_bp->size * dpa_bp->count,
-					       DMA_BIDIRECTIONAL);
-		if (unlikely(dpa_bp->paddr == 0)) {
-			cpu_dev_err(dev, "%s:%hu:%s(): dma_map_single() failed\n",
-				    __file__, __LINE__, __func__);
-			_errno = -EIO;
-			goto _return_free_pages_exact;
-		}
-
-		/* Populate the pool */
-		for (i = 0; i < dpa_bp->count;) {
-			for (j = 0; j < ARRAY_SIZE(bmb) && i < dpa_bp->count; i++, j++) {
-				bmb[j].hi = 0;
-				bmb[j].lo = dpa_bp->paddr + i * dpa_bp->size;
-			}
+		dpa_bp->bp_blocks_per_page = PAGE_SIZE / dpa_bp->size;
+		BUG_ON(dpa_bp->count < dpa_bp->bp_blocks_per_page);
+		dpa_bp->bp_kick_thresh = dpa_bp->count - max((unsigned int)8,
+						dpa_bp->bp_blocks_per_page);
+		dpa_bp->bp_refill_pending = dpa_bp->count;
+		dpa_bp->kernel_pool = 1;
 
-			_errno = bman_release(dpa_bp->pool, bmb, j, BMAN_RELEASE_FLAG_WAIT_INT);
-			if (unlikely(_errno < 0)) {
-				cpu_pr_err(KBUILD_MODNAME ": %s:%hu:%s(): bman_release(%hu) = %d\n",
-					   __file__, __LINE__, __func__,
-					   bman_get_params(dpa_bp->pool)->bpid, _errno);
-				goto _return_bman_acquire;
-			}
-		}
+		spin_lock_init(&dpa_bp->lock);
+		dpa_bp_refill(priv, dpa_bp, GFP_KERNEL);
 	} else {
-		devm_request_mem_region(dev, dpa_bp->paddr, dpa_bp->size * dpa_bp->count,
+		/* This is a shared pool, which the kernel doesn't manage */
+		dpa_bp->kernel_pool = 0;
+		devm_request_mem_region(dev->dev.parent, dpa_bp->paddr,
+					dpa_bp->size * dpa_bp->count,
 					KBUILD_MODNAME);
-		dpa_bp->vaddr = devm_ioremap_prot(dev,
-						  dpa_bp->paddr, dpa_bp->size * dpa_bp->count, 0);
+		dpa_bp->vaddr = devm_ioremap_prot(dev->dev.parent, dpa_bp->paddr,
+					dpa_bp->size * dpa_bp->count, 0);
 		if (unlikely(dpa_bp->vaddr == NULL)) {
-			cpu_dev_err(dev, "%s:%hu:%s(): devm_ioremap() failed\n",
+			cpu_dev_err(dev->dev.parent, "%s:%hu:%s(): devm_ioremap() failed\n",
 				    __file__, __LINE__, __func__);
 			_errno = -EIO;
 			goto _return_bman_free_pool;
@@ -209,20 +356,14 @@ _dpa_bp_alloc(struct device *dev, struct list_head *list, struct dpa_bp *dpa_bp)
 
 	list_add_tail(&dpa_bp->list, list);
 
-	_errno = 0;
-	goto _return;
+	return 0;
 
-_return_bman_acquire:
-	/* \todo	Drain the pool */
-	dma_unmap_single(dev, dpa_bp->paddr, dpa_bp->size * dpa_bp->count, DMA_BIDIRECTIONAL);
-_return_free_pages_exact:
-	free_pages_exact(dpa_bp->vaddr, dpa_bp->size * dpa_bp->count);
 _return_bman_free_pool:
 	bman_free_pool(dpa_bp->pool);
 _return_bm_pool_free:
 	if (dpa_bp->bpid == 0)
 		bm_pool_free(bp_params.bpid);
-_return:
+
 	return _errno;
 }
 
@@ -275,10 +416,12 @@ _dpa_bp_free(struct device *dev, struct dpa_bp *dpa_bp)
 {
 	uint8_t	bpid;
 
-	/* \todo	Drain the pool */
-
-	dma_unmap_single(dev, dpa_bp->paddr, dpa_bp->size * dpa_bp->count, DMA_BIDIRECTIONAL);
-	free_pages_exact(dpa_bp->vaddr, dpa_bp->size * dpa_bp->count);
+	if (dpa_bp->kernel_pool) {
+		kfree(dpa_bp->rxhash);
+	} else {
+		dma_unmap_single(dev, dpa_bp->paddr, dpa_bp->size * dpa_bp->count, DMA_BIDIRECTIONAL);
+		free_pages_exact(dpa_bp->vaddr, dpa_bp->size * dpa_bp->count);
+	}
 	bpid = dpa_pool2bpid(dpa_bp);
 	bman_free_pool(dpa_bp->pool);
 	list_del(&dpa_bp->list);
@@ -448,7 +591,7 @@ dpa_fd_release(const struct net_device *net_dev, const struct qm_fd *fd)
 	int				 _errno, __errno, i, j;
 	const struct dpa_priv_s		*priv;
 	const struct qm_sg_entry	*sgt;
-	const struct dpa_bp		*_dpa_bp, *dpa_bp;
+	struct dpa_bp		*_dpa_bp, *dpa_bp;
 	const struct bm_buffer		*_bmb;
 	struct bm_buffer		 bmb[8];
 
@@ -461,7 +604,24 @@ dpa_fd_release(const struct net_device *net_dev, const struct qm_fd *fd)
 
 	_errno = 0;
 	if (fd->format == qm_fd_sg) {
-		sgt = (typeof(sgt))(dpa_phys2virt(_dpa_bp, _bmb) + dpa_fd_offset(fd));
+#ifdef CONFIG_DPA_RX_0_COPY
+		struct page **pageptr;
+		struct page *page = NULL;
+		
+		if (_dpa_bp->kernel_pool) {
+			unsigned long flags;
+			spin_lock_irqsave(&_dpa_bp->lock, flags);
+			pageptr = dpa_find_rxpage(_dpa_bp, _bmb->lo);
+			page = *pageptr;
+			spin_unlock_irqrestore(&_dpa_bp->lock, flags);
+
+			sgt = (typeof(sgt))(kmap(page) + (_bmb->lo & ~PAGE_MASK) + dpa_fd_offset(fd));
+		} else
+#endif
+		{
+			sgt = (typeof(sgt))(dpa_phys2virt(_dpa_bp, _bmb)
+				+ dpa_fd_offset(fd));
+		}
 
 		i = 0;
 		do {
@@ -488,6 +648,11 @@ dpa_fd_release(const struct net_device *net_dev, const struct qm_fd *fd)
 					_errno = __errno;
 			}
 		} while (!sgt[i-1].final);
+
+#ifdef CONFIG_DPA_RX_0_COPY
+		if (_dpa_bp->kernel_pool)
+			kunmap(page);
+#endif
 	}
 
 	__errno = bman_release(_dpa_bp->pool, _bmb, 1, BMAN_RELEASE_FLAG_WAIT_INT);
@@ -1072,39 +1237,6 @@ static const struct qman_fq _egress_fqs __devinitconst = {
 	.cb = {egress_dqrr, egress_ern, egress_dc_ern, egress_fqs}
 };
 
-#ifdef CONFIG_DPA_RX_0_COPY
-static void __hot dpa_skb_destructor(struct sk_buff *skb)
-{
-	int		_errno;
-	struct dpa_bp	*dpa_bp;
-
-	cpu_pr_debug(KBUILD_MODNAME ": -> %s:%s()\n", __file__, __func__);
-
-	if (!skb->cloned ||
-	    !atomic_sub_return(skb->nohdr ? (1 << SKB_DATAREF_SHIFT) + 1 : 1,
-			       &skb_shinfo(skb)->dataref)) {
-
-		dpa_bp = *(typeof(&dpa_bp))(skb->head + sizeof(struct bm_buffer));
-
-#ifdef DEBUG
-		memset(dpa_phys2virt(dpa_bp, (struct bm_buffer *)skb->head), 0, dpa_bp->size);
-#endif
-
-		_errno = bman_release(dpa_bp->pool, (struct bm_buffer *)skb->head, 1,
-				      BMAN_RELEASE_FLAG_WAIT_INT);
-		if (unlikely(_errno < 0)) {
-			cpu_pr_err(KBUILD_MODNAME ": %s:%hu:%s(): bman_release(%hu) = %d\n",
-				   __file__, __LINE__, __func__,
-				   bman_get_params(dpa_bp->pool)->bpid, _errno);
-			dump_stack();
-			panic("Can't release buffer to the BM during RX\n");
-		}
-	}
-
-	cpu_pr_debug(KBUILD_MODNAME ": %s:%s() ->\n", __file__, __func__);
-}
-#endif
-
 static struct net_device_stats * __cold dpa_get_stats(struct net_device *net_dev)
 {
 	cpu_netdev_dbg(net_dev, "-> %s:%s()\n", __file__, __func__);
@@ -1141,12 +1273,13 @@ static void __hot dpa_rx(struct work_struct *fd_work)
 	struct fd_list_head	*fd_list;
 	struct dpa_fd		*dpa_fd, *tmp;
 	const struct bm_buffer	*bmb;
-	const struct dpa_bp	*dpa_bp;
+	struct dpa_bp	*dpa_bp;
 	size_t			 head, size;
 	struct sk_buff		*skb;
-	void			*virt;
 #ifdef CONFIG_DPA_RX_0_COPY
-	struct page		*page;
+	struct page **pageptr;
+	struct page *page;
+	unsigned long flags;
 #endif
 
 	priv = (typeof(priv))container_of(fd_work, struct dpa_priv_s, fd_work);
@@ -1175,6 +1308,7 @@ static void __hot dpa_rx(struct work_struct *fd_work)
 
 		if (dpa_fd->fd.format == qm_fd_sg) {
 			net_dev->stats.rx_dropped++;
+			printk("dropping an SG frame");
 
 			goto _continue_dpa_fd_release;
 		}
@@ -1186,11 +1320,21 @@ static void __hot dpa_rx(struct work_struct *fd_work)
 		dpa_bp = dpa_bpid2pool(&priv->dpa_bp_list, bmb->bpid);
 		BUG_ON(IS_ERR(dpa_bp));
 
-		virt = dpa_phys2virt(dpa_bp, bmb);
-
 #ifdef CONFIG_DPA_RX_0_COPY
-		if (virt_addr_valid(virt)) {
-			head = sizeof(*bmb) + sizeof(dpa_bp) + NET_IP_ALIGN;
+		spin_lock_irqsave(&dpa_bp->lock, flags);
+		pageptr = dpa_find_rxpage(dpa_bp, bmb->lo);
+
+		if (!pageptr)
+			cpu_pr_emerg("Aaaaaaah, no page for addr %x in pool %d!\n", bmb->lo, bmb->bpid);
+
+		page = *pageptr;
+		*pageptr = NULL;
+
+		dpa_bp->bp_refill_pending++;
+		spin_unlock_irqrestore(&dpa_bp->lock, flags);
+
+		if (dpa_bp->kernel_pool) {
+			head = sizeof(*bmb) + NET_IP_ALIGN;
 			size = ETH_HLEN + NN_ALLOCATED_SPACE(net_dev) + TT_ALLOCATED_SPACE(net_dev);
 		} else
 #endif
@@ -1214,22 +1358,25 @@ static void __hot dpa_rx(struct work_struct *fd_work)
 		skb_reserve(skb, head);
 
 #ifdef CONFIG_DPA_RX_0_COPY
-		if (virt_addr_valid(virt)) {
-			skb->destructor = dpa_skb_destructor;
-
+		if (dpa_bp->kernel_pool) {
+			unsigned int off = (bmb->lo +
+				dpa_fd_offset(&dpa_fd->fd)) & ~PAGE_MASK;
+			unsigned int len = dpa_fd_length(&dpa_fd->fd);
 			*(struct bm_buffer *)skb->head = *bmb;
-			*(typeof(&dpa_bp))(skb->head + sizeof(*bmb)) = dpa_bp;
-
-			get_page(page = virt_to_page(virt + dpa_fd_offset(&dpa_fd->fd)));
 
-			skb_fill_page_desc(skb, 0, page,
-					   (uintptr_t)(virt + dpa_fd_offset(&dpa_fd->fd)) & ~PAGE_MASK,
-					   dpa_fd_length(&dpa_fd->fd));
+			skb_fill_page_desc(skb, 0, page, off, len);
 
-			skb->len	+= dpa_fd_length(&dpa_fd->fd);
-			skb->data_len	+= dpa_fd_length(&dpa_fd->fd);
+			skb->len	+= len;
+			skb->data_len	+= len;
 			skb->truesize	+= dpa_bp->size;
 
+			/*
+			 * Unmap this page mapping,
+			 * and remove one instance of the page from the hash
+			 */
+			dma_unmap_page(net_dev->dev.parent, page->index + off,
+					dpa_bp->size, DMA_FROM_DEVICE);
+
 			if (unlikely(!__pskb_pull_tail(skb,
 						       ETH_HLEN +
 						       NN_RESERVED_SPACE(net_dev) +
@@ -1246,7 +1393,10 @@ static void __hot dpa_rx(struct work_struct *fd_work)
 		} else
 #endif
 		{
-			memcpy(skb_put(skb, size), virt + dpa_fd_offset(&dpa_fd->fd), size);
+			memcpy(skb_put(skb, size),
+				dpa_phys2virt(dpa_bp, bmb) +
+					dpa_fd_offset(&dpa_fd->fd),
+				size);
 
 			_errno = dpa_fd_release(net_dev, &dpa_fd->fd);
 			if (unlikely(_errno < 0)) {
@@ -1267,6 +1417,11 @@ static void __hot dpa_rx(struct work_struct *fd_work)
 
 		net_dev->last_rx = jiffies;
 
+#ifdef CONFIG_DPA_RX_0_COPY
+		if (dpa_bp->bp_refill_pending > dpa_bp->bp_kick_thresh)
+			dpa_bp_refill(priv, dpa_bp, GFP_KERNEL);
+#endif
+
 		goto _continue;
 
 _continue_dpa_fd_release:
@@ -1888,7 +2043,7 @@ static int __devinit __cold __attribute__((nonnull)) dpa_probe(struct of_device
 	INIT_LIST_HEAD(&priv->dpa_bp_list);
 
 	for (i = 0; i < count; i++) {
-		_errno = _dpa_bp_alloc(dev, &priv->dpa_bp_list, dpa_bp + i);
+		_errno = _dpa_bp_alloc(net_dev, &priv->dpa_bp_list, dpa_bp + i);
 		if (unlikely(_errno < 0))
 			goto _return_dpa_bp_free;
 	}
-- 
1.6.5.2

