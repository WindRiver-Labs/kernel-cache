From fa88749a205aea5fc704594b8a3f495046acf8d7 Mon Sep 17 00:00:00 2001
From: Jeffrey Ladouceur <Jeffrey.Ladouceur@freescale.com>
Date: Fri, 16 Apr 2010 12:07:13 -0400
Subject: [PATCH 005/148] p4080_1-2-rc1-drivers_match.patch

drivers/match/Kconfig
drivers/match/Makefile
drivers/match/pme2_ctrl.c
drivers/match/pme2_db.c
drivers/match/pme2_high.c
drivers/match/pme2_low.c
drivers/match/pme2_private.h
drivers/match/pme2_regs.h
drivers/match/pme2_sample_db.c
drivers/match/pme2_scan.c
drivers/match/pme2_sys.h
drivers/match/pme2_test.h
drivers/match/pme2_test_high.c
drivers/match/pme2_test_low.c
drivers/match/pme2_test_scan.c

Signed-off-by: Geoff Thorpe <geoff@geoffthorpe.net>
Signed-off-by: Jeffrey Ladouceur <Jeffrey.Ladouceur@freescale.com>
[Cleanly applied Freescale SDK 2.0.3 patch
"p4080_1-2-rc1-drivers_match.patch",
original did not have commit text]
Integrated-by: Yuri Nedel <Yuri.Nedel@windriver.com>
---
 drivers/match/Kconfig          |  249 +++++++
 drivers/match/Makefile         |   10 +
 drivers/match/pme2_ctrl.c      | 1418 ++++++++++++++++++++++++++++++++++++++++
 drivers/match/pme2_db.c        |  470 +++++++++++++
 drivers/match/pme2_high.c      |  912 ++++++++++++++++++++++++++
 drivers/match/pme2_low.c       |  289 ++++++++
 drivers/match/pme2_private.h   |  165 +++++
 drivers/match/pme2_regs.h      |  174 +++++
 drivers/match/pme2_sample_db.c |  206 ++++++
 drivers/match/pme2_scan.c      |  775 ++++++++++++++++++++++
 drivers/match/pme2_sys.h       |   41 ++
 drivers/match/pme2_test.h      |   84 +++
 drivers/match/pme2_test_high.c |  138 ++++
 drivers/match/pme2_test_low.c  |  328 ++++++++++
 drivers/match/pme2_test_scan.c |  478 ++++++++++++++
 15 files changed, 5737 insertions(+), 0 deletions(-)
 create mode 100644 drivers/match/Kconfig
 create mode 100644 drivers/match/Makefile
 create mode 100644 drivers/match/pme2_ctrl.c
 create mode 100644 drivers/match/pme2_db.c
 create mode 100644 drivers/match/pme2_high.c
 create mode 100644 drivers/match/pme2_low.c
 create mode 100644 drivers/match/pme2_private.h
 create mode 100644 drivers/match/pme2_regs.h
 create mode 100644 drivers/match/pme2_sample_db.c
 create mode 100644 drivers/match/pme2_scan.c
 create mode 100644 drivers/match/pme2_sys.h
 create mode 100644 drivers/match/pme2_test.h
 create mode 100644 drivers/match/pme2_test_high.c
 create mode 100644 drivers/match/pme2_test_low.c
 create mode 100644 drivers/match/pme2_test_scan.c

diff --git a/drivers/match/Kconfig b/drivers/match/Kconfig
new file mode 100644
index 0000000..c68c428
--- /dev/null
+++ b/drivers/match/Kconfig
@@ -0,0 +1,249 @@
+menu "Pattern matching support"
+
+menuconfig PATTERN_MATCHING
+	bool "Pattern matching h/w support"
+	default y
+	---help---
+	  Say Y here to see options for pattern matching drivers.
+	  This option alone does not add any kernel code.
+	  If unsure, say Y.
+
+if PATTERN_MATCHING
+
+config FSL_PME2_CTRL
+	bool "Freescale PME2 (p4080, etc) device control"
+	default y
+	---help---
+	  This compiles device support for the Freescale PME2 pattern matching
+	  part contained in datapath-enabled SoCs (ie. accessed via Qman and
+	  Bman portal functionality). At least one guest operating system must
+	  have this driver support, together with the appropriate device-tree
+	  entry, for PME2 functionality to be available. It is responsible for
+	  allocating system memory to the device and configuring it for
+	  operation. For this reason, it must be built into the kernel and will
+	  initialise during early kernel boot.
+
+config FSL_PME2_PDSRSIZE
+	int "Pattern Description and Stateful Rule default table size"
+	depends on FSL_PME2_CTRL
+	range 74240 33554431
+	default 131072
+	help
+	  Select the default size of the Pattern Description and Stateful Rule
+	  table as the number of 128 byte entries. This only takes effect if
+	  the device tree node doesn't have the 'fsl,pme-pdsr' property.
+	  range 74240-33554431 (9.5MB-4GB)
+	  default 131072 (16MB)
+
+if FSL_PME2_CTRL
+comment "Statefule Rule Engine"
+endif
+
+config FSL_PME2_SRESIZE
+	int "SRE Session Context Entries table default table size"
+	depends on FSL_PME2_CTRL
+	range 0 134217727
+	default 327680
+	help
+	  Select the default size of the SRE Context Table as the number of 32
+	  byte entries. This only takes effect if the device tree node doesn't
+	  have the 'fsl,pme-sre' property.
+	  range 0-134217727 (0-4GB)
+	  default 327680 (10MB)
+
+config FSL_PME2_SRE_AIM
+	bool "Alternate Inconclusive Mode"
+	depends on FSL_PME2_CTRL
+	default n
+	help
+	  Select the inconclusive match mode treatment. When true the
+	  “alternate” inconclusive mode is used. When false the “default”
+	  inconclusive mode is used.
+
+config FSL_PME2_SRE_ESR
+	bool "End of SUI Simple Report"
+	depends on FSL_PME2_CTRL
+	default n
+	help
+	  Select if an End of SUI will produce a Simple End of SUI report.
+
+config FSL_PME2_SRE_CTX_SIZE_PER_SESSION
+	int "Default SRE Context Size per Session (16 => 64KB, 17 => 128KB)"
+	depends on FSL_PME2_CTRL
+	range 5 17
+	default 17
+	help
+	  Select SRE context size per session as a power of 2.
+	  range 5-17
+	  Examples:
+	             5  => 32 B
+	             6  => 64 B
+	             7  => 128 B
+	             8  => 256 B
+	             9  => 512 B
+	             10 => 1 KB
+	             11 => 2 KB
+	             12 => 4 KB
+	             13 => 8 KB
+	             14 => 16 KB
+	             15 => 32 KB
+	             16 => 64 KB
+	             17 => 128 KB
+
+config FSL_PME2_SRE_CNR
+	int "Configured Number of Stateful Rules as a multiple of 256 (128 => 32768 )"
+	depends on FSL_PME2_CTRL
+	range 0 128
+	default 128
+	help
+	  Select number of stateful rules as a multiple of 256.
+	  range 0-128
+	  Examples:
+	             0  => 0
+	             1  => 256
+	             2  => 215
+	             ...
+	             127 => 32512
+	             128 => 32768
+
+config FSL_PME2_SRE_MAX_INSTRUCTION_LIMIT
+	int "Maximum number of SRE instructions to be executed per reaction."
+	depends on FSL_PME2_CTRL
+	range 0 65535
+	default 65535
+	help
+	  Select the maximum number of SRE instructions to be executed per
+	  reaction.
+	  range 0 65535
+
+config FSL_PME2_SRE_MAX_BLOCK_NUMBER
+	int "Maximum number of Reaction Head blocks to be traversed per pattern match event"
+	depends on FSL_PME2_CTRL
+	range 0 32767
+	default 32767
+	help
+	  Select the maximum number of reaction head blocks to be traversed per
+	  pattern match event (e.g. a matched pattern or an End of SUI event).
+	  range 0-32767
+
+config FSL_PME2
+	tristate "Freescale PME2 (p4080, etc) device usage"
+	depends on FSL_QMAN_PORTAL
+	default y
+	---help---
+	  This compiles I/O support for the Freescale PME2 pattern matching
+	  part contained in datapath-enabled SoCs (ie. accessed via Qman and
+	  Bman portal functionality).
+
+config FSL_PME2_HIGH
+	bool "High-level support, using affine Qman portals"
+	depends on FSL_PME2 && FSL_QMAN_FQALLOCATOR && \
+		!FSL_QMAN_PORTAL_DISABLEAUTO
+	default y
+	---help---
+	  This compiles the high-level driver for PME2. If in doubt, say yes.
+
+config FSL_PME2_SAMPLE_DB
+	bool "Populate the PME database with sample pattern data"
+	depends on FSL_PME2_HIGH
+	default y
+	---help---
+	  This is a temporary configuration option that builds code into the PME2
+	  driver to prepopulate the PME2 database with sample data. This same data
+	  is used (and required) by the FSL_PME2_TEST_SCAN test.
+
+config FSL_PME2_TEST_LOW
+	tristate "PME2 low-level self-test"
+	depends on FSL_PME2 && FSL_QMAN_FQALLOCATOR && \
+		FSL_QMAN_PORTAL_DISABLEAUTO
+	default y
+	---help---
+	  This takes an unused Qman portal and performs low-level PME2 API
+	  testing with it.
+
+config FSL_PME2_TEST_HIGH
+	tristate "PME2 high-level self-test"
+	depends on FSL_PME2_HIGH
+	default y
+	---help---
+	  This uses the high-level Qman driver (and the cpu-affine portals it
+	  manages) to perform high-level PME2 API testing with it.
+
+config FSL_PME2_TEST_SCAN
+	tristate "PME2 scan self-test"
+	depends on FSL_PME2_SAMPLE_DB
+	default y
+	---help---
+	  This uses the high-level Qman driver (and the cpu-affine portals it
+	  manages) to perform scan PME2 API testing with it.
+
+config FSL_PME2_TEST_SCAN_WITH_BPID
+	bool "PME2 scan self-test with buffer pool"
+	depends on FSL_PME2_TEST_SCAN
+	default y
+	---help---
+	  This uses a buffer pool id for scan test
+
+config FSL_PME2_TEST_SCAN_WITH_BPID_SIZE
+	int "Buffer Pool size."
+	depends on FSL_PME2_TEST_SCAN_WITH_BPID
+	range 0 11
+	default 3
+	---help---
+	  This uses the specified buffer pool size.
+
+config FSL_PME2_DB
+	tristate "PME2 Database support"
+	depends on FSL_PME2 && FSL_PME2_HIGH && FSL_QMAN_FQALLOCATOR && \
+		!FSL_QMAN_PORTAL_DISABLEAUTO
+	default y
+	---help---
+	  This compiles the database driver for PME2.
+
+config FSL_PME2_DB_QOSOUT_PRIORITY
+	int "PME DB output frame queue priority."
+	depends on FSL_PME2_DB
+	range 0 7
+	default 2
+	---help---
+	  The PME DB has a scheduled output frame queue. The qos priority level is configurable.
+	  range 0-7
+		0 => High Priority 0
+		1 => High Priority 1
+		2 => Medium Priority
+		3 => Medium Priority
+		4 => Medium Priority
+		5 => Low Priority
+		6 => Low Priority
+		7 => Low Priority
+
+config FSL_PME2_SCAN
+        tristate "PME2 Scan support"
+        depends on FSL_PME2 && FSL_PME2_HIGH && FSL_QMAN_FQALLOCATOR && \
+                !FSL_QMAN_PORTAL_DISABLEAUTO
+        default y
+        ---help---
+          This compiles the scan driver for PME2.
+
+config FSL_PME2_SCAN_DEBUG
+        bool "Debug Statements"
+        default y
+        depends on FSL_PME2_SCAN
+        ---help---
+          The PME2_SCAN driver can optionally trace with more verbosity
+          of verbosity.
+
+config FSL_PME2_STAT_ACCUMULATOR_UPDATE_INTERVAL
+	int "Configure the pme2 statistics update interval in milliseconds"
+	depends on FSL_PME2_CTRL
+	range 0 10000
+	default 4000
+	help
+	  The pme accumulator reads the current device statistics and add it
+	  to a running counter. The frequency of these updates may be
+	  controlled. If 0 is specified, no automatic updates is done.
+	  range 0-10000
+
+endif # PATTERN_MATCHING
+
+endmenu
diff --git a/drivers/match/Makefile b/drivers/match/Makefile
new file mode 100644
index 0000000..0a97f89
--- /dev/null
+++ b/drivers/match/Makefile
@@ -0,0 +1,10 @@
+obj-$(CONFIG_FSL_PME2_CTRL)	+= pme2_ctrl.o
+obj-$(CONFIG_FSL_PME2)		+= pme2.o
+pme2-y				:= pme2_low.o
+pme2-$(CONFIG_FSL_PME2_HIGH)	+= pme2_high.o
+pme2-$(CONFIG_FSL_PME2_SAMPLE_DB) += pme2_sample_db.o
+obj-$(CONFIG_FSL_PME2_TEST_LOW)	+= pme2_test_low.o
+obj-$(CONFIG_FSL_PME2_TEST_HIGH) += pme2_test_high.o
+obj-$(CONFIG_FSL_PME2_TEST_SCAN) += pme2_test_scan.o
+obj-$(CONFIG_FSL_PME2_DB)       += pme2_db.o
+obj-$(CONFIG_FSL_PME2_SCAN)       += pme2_scan.o
diff --git a/drivers/match/pme2_ctrl.c b/drivers/match/pme2_ctrl.c
new file mode 100644
index 0000000..f781df8
--- /dev/null
+++ b/drivers/match/pme2_ctrl.c
@@ -0,0 +1,1418 @@
+/* Copyright (c) 2008, 2009, Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "pme2_private.h"
+#include "pme2_regs.h"
+
+#define DEFAULT_PDSR_SZ (CONFIG_FSL_PME2_PDSRSIZE << 7)
+#define DEFAULT_SRE_SZ  (CONFIG_FSL_PME2_SRESIZE << 5)
+#define PDSR_TBL_ALIGN  (1 << 7)
+#define SRE_TBL_ALIGN   (1 << 5)
+#define DEFAULT_SRFCC   400
+
+/* Defaults */
+#define DEFAULT_DEC0_MTE   0x3FFF
+#define DEFAULT_DLC_MPM    0xFFFF
+#define DEFAULT_DLC_MPE    0xFFFF
+/* Boot parameters */
+DECLARE_GLOBAL(max_test_line_per_pat, unsigned int, uint,
+		DEFAULT_DEC0_MTE,
+		"Maximum allowed Test Line Executions per pattern, "
+		"scaled by a factor of 8");
+DECLARE_GLOBAL(max_pat_eval_per_sui, unsigned int, uint,
+		DEFAULT_DLC_MPE,
+		"Maximum Pattern Evaluations per SUI, scaled by a factor of 8")
+DECLARE_GLOBAL(max_pat_matches_per_sui, unsigned int, uint,
+		DEFAULT_DLC_MPM,
+		"Maximum Pattern Matches per SUI");
+/* SRE */
+DECLARE_GLOBAL(sre_rule_num, unsigned int, uint,
+		CONFIG_FSL_PME2_SRE_CNR,
+		"Configured Number of Stateful Rules");
+DECLARE_GLOBAL(sre_session_ctx_size, unsigned int, uint,
+		1 << CONFIG_FSL_PME2_SRE_CTX_SIZE_PER_SESSION,
+		"SRE Context Size per Session");
+
+/************
+ * Section 1
+ ************
+ * This code is called during kernel early-boot and could never be made
+ * loadable.
+ */
+static dma_addr_t dxe_a, sre_a;
+static size_t dxe_sz = DEFAULT_PDSR_SZ, sre_sz = DEFAULT_SRE_SZ;
+
+/* Parse the <name> property to extract the memory location and size and
+ * lmb_reserve() it. If it isn't supplied, lmb_alloc() the default size. */
+static __init int parse_mem_property(struct device_node *node, const char *name,
+			dma_addr_t *addr, size_t *sz, u64 align, int zero)
+{
+	const u32 *pint;
+	int ret;
+
+	pint = of_get_property(node, name, &ret);
+	if (!pint || (ret != 16)) {
+		pr_info("pme: No %s property '%s', using lmb_alloc(0x%08x)\n",
+				node->full_name, name, *sz);
+		*addr = lmb_alloc(*sz, align);
+		if (zero)
+			memset(phys_to_virt(*addr), 0, *sz);
+		return 0;
+	}
+	pr_info("pme: Using %s property '%s'\n", node->full_name, name);
+	/* Props are 64-bit, but dma_addr_t is (currently) 32-bit */
+	BUG_ON(sizeof(*addr) != 4);
+	if(pint[0] || pint[2]) {
+		pr_err("pme: Invalid number of dt properties\n");
+		return -EINVAL;
+	}
+	*addr = pint[1];
+	if((u64)*addr & (align - 1)) {
+		pr_err("pme: Invalid alignment, address %016llx\n",(u64)*addr);
+		return -EINVAL;
+	}
+	*sz = pint[3];
+	/* Keep things simple, it's either all in the DRAM range or it's all
+	 * outside. */
+	if (*addr < lmb_end_of_DRAM()) {
+		if ((u64)*addr + (u64)*sz > lmb_end_of_DRAM()){
+			pr_err("pme: outside DRAM range\n");
+			return -EINVAL;
+		}
+		if (lmb_reserve(*addr, *sz) < 0) {
+			pr_err("pme: Failed to reserve %s\n", name);
+			return -ENOMEM;
+		}
+		if (zero)
+			memset(phys_to_virt(*addr), 0, *sz);
+	} else {
+		/* map as cacheable, non-guarded */
+		void *tmpp = ioremap_flags(*addr, *sz, 0);
+		if (zero)
+			memset(tmpp, 0, *sz);
+		iounmap(tmpp);
+	}
+	return 0;
+}
+
+/* No errors/interrupts. Physical addresses are assumed <= 32bits. */
+static int __init fsl_pme2_init(struct device_node *node)
+{
+	int ret = 0;
+
+	/* Check if pdsr memory already allocated */
+	if (dxe_a) {
+		pr_err("pme: Error fsl_pme2_init already done\n");
+		return -EINVAL;
+	}
+	ret = parse_mem_property(node, "fsl,pme-pdsr", &dxe_a, &dxe_sz,
+			PDSR_TBL_ALIGN, 0);
+	if (ret)
+		return ret;
+	ret = parse_mem_property(node, "fsl,pme-sre", &sre_a, &sre_sz,
+			SRE_TBL_ALIGN, 0);
+	return ret;
+}
+
+__init void pme2_init_early(void)
+{
+	struct device_node *dn;
+	int ret;
+	for_each_compatible_node(dn, NULL, "fsl,pme") {
+		ret = fsl_pme2_init(dn);
+		if (ret)
+			pr_err("pme: Error fsl_pme2_init\n");
+	}
+}
+
+/************
+ * Section 2
+ ************
+ * This code is called during driver initialisation. It doesn't do anything with
+ * the device-tree entries nor the PME device, it simply creates the sysfs stuff
+ * and gives the user something to hold. This could be made loadable, if there
+ * was any benefit to doing so - but as the device is already "bound" by static
+ * code, there's little point to hiding the fact.
+ */
+
+MODULE_AUTHOR("Geoff Thorpe");
+MODULE_LICENSE("Dual BSD/GPL");
+MODULE_DESCRIPTION("FSL PME2 (p4080) device control");
+
+/* Opaque pointer target used to represent the PME CCSR map, ... */
+struct pme;
+
+/* ... and the instance of it. */
+static struct pme *global_pme;
+static int pme_err_irq;
+
+static inline void __pme_out(struct pme *p, u32 offset, u32 val)
+{
+	u32 __iomem *regs = (void *)p;
+	out_be32(regs + (offset >> 2), val);
+}
+#define pme_out(p, r, v) __pme_out(p, PME_REG_##r, v)
+static inline u32 __pme_in(struct pme *p, u32 offset)
+{
+	u32 __iomem *regs = (void *)p;
+	return in_be32(regs + (offset >> 2));
+}
+#define pme_in(p, r) __pme_in(p, PME_REG_##r)
+
+#define PME_EFQC(en, fq) \
+	({ \
+		/* Assume a default delay of 64 cycles */ \
+		u8 __i419 = 0x1; \
+		u32 __fq419 = (fq) & 0x00ffffff; \
+		((en) ? 0x80000000 : 0) | (__i419 << 28) | __fq419; \
+	})
+
+#define PME_FACONF_ENABLE   0x00000002
+#define PME_FACONF_RESET    0x00000001
+
+static inline struct pme *pme_create(void *regs)
+{
+	struct pme *res = (struct pme *)regs;
+	pme_out(res, FACONF, 0);
+	pme_out(res, EFQC, PME_EFQC(0, 0));
+	pme_out(res, FACONF, PME_FACONF_ENABLE);
+	return res;
+}
+
+/* pme stats accumulator work */
+static int pme_stat_get(enum pme_attr *stat, u64 *value, int reset);
+static void accumulator_update(struct work_struct *work);
+static void accumulator_update_interval(u32 interval);
+static DECLARE_DELAYED_WORK(accumulator_work, accumulator_update);
+u32 pme_stat_interval = CONFIG_FSL_PME2_STAT_ACCUMULATOR_UPDATE_INTERVAL;
+#define MAX_ACCUMULATOR_INTERVAL 10000
+#define PME_SBE_ERR 0x01000000
+#define PME_DBE_ERR 0x00080000
+#define PME_PME_ERR 0x00000100
+#define PME_ALL_ERR (PME_SBE_ERR | PME_DBE_ERR | PME_PME_ERR)
+
+static ssize_t pme_generic_store(const char *buf, size_t count,
+				enum pme_attr attr)
+{
+	unsigned long val;
+	size_t ret;
+	if (strict_strtoul(buf, 0, &val)) {
+		pr_err("pme: invalid input %s\n",buf);
+		return -EINVAL;
+	}
+	ret = pme_attr_set(attr, val);
+	if (ret) {
+		pr_err("pme: attr_set err attr=%u, val=%lu\n", attr, val);
+		return ret;
+	}
+	return count;
+}
+
+static ssize_t pme_generic_show(char *buf, enum pme_attr attr, const char *fmt)
+{
+	u32 data = pme_attr_get(attr);
+	return snprintf(buf, PAGE_SIZE, fmt, data);
+}
+
+static ssize_t pme_generic_stat_show(char *buf, enum pme_attr attr)
+{
+	u64 data = 0;
+	int ret = 0;
+
+	ret = pme_stat_get(&attr, &data, 0);
+	if (!ret)
+		return snprintf(buf, PAGE_SIZE, "%llu\n", data);
+	else
+		return ret;
+}
+
+static ssize_t pme_generic_stat_store(const char *buf, size_t count,
+				enum pme_attr attr)
+{
+	unsigned long val;
+	u64 data = 0;
+	size_t ret = 0;
+	if (strict_strtoul(buf, 0, &val)) {
+		pr_err("pme: invalid input %s\n", buf);
+		return -EINVAL;
+	}
+	if (val) {
+		pr_err("pme: invalid input %s\n", buf);
+		return -EINVAL;
+	}
+	ret = pme_stat_get(&attr, &data, 1);
+	return count;
+}
+
+#define PME_GENERIC_ATTR(pme_attr, perm, showhex) \
+static ssize_t pme_store_##pme_attr(struct device_driver *pme, const char *buf,\
+					size_t count) \
+{ \
+	return pme_generic_store(buf, count, pme_attr_##pme_attr);\
+} \
+static ssize_t pme_show_##pme_attr(struct device_driver *pme, char *buf) \
+{ \
+	return pme_generic_show(buf, pme_attr_##pme_attr, showhex);\
+} \
+static DRIVER_ATTR( pme_attr, perm, pme_show_##pme_attr, pme_store_##pme_attr);
+
+#define PME_GENERIC_BSC_ATTR(bsc_id, perm, showhex) \
+static ssize_t pme_store_pme_attr_bsc_##bsc_id(struct device_driver *pme,\
+					const char *buf, size_t count) \
+{ \
+	return pme_generic_store(buf, count, pme_attr_bsc(bsc_id));\
+} \
+static ssize_t pme_show_pme_attr_bsc_##bsc_id(struct device_driver *pme,\
+						char *buf) \
+{ \
+	return pme_generic_show(buf, pme_attr_bsc(bsc_id), showhex);\
+} \
+static DRIVER_ATTR(bsc_##bsc_id, perm, pme_show_pme_attr_bsc_##bsc_id, \
+			pme_store_pme_attr_bsc_##bsc_id);
+
+
+#define PME_GENERIC_STAT_ATTR(pme_attr, perm) \
+static ssize_t pme_store_##pme_attr(struct device_driver *pme, const char *buf,\
+					size_t count) \
+{ \
+	return pme_generic_stat_store(buf, count, pme_attr_##pme_attr);\
+} \
+static ssize_t pme_show_##pme_attr(struct device_driver *pme, char *buf) \
+{ \
+	return pme_generic_stat_show(buf, pme_attr_##pme_attr);\
+} \
+static DRIVER_ATTR(pme_attr, perm, pme_show_##pme_attr, pme_store_##pme_attr);
+
+static ssize_t pme_store_update_interval(struct device_driver *pme,
+		const char *buf, size_t count)
+{
+	unsigned long val;
+
+	if (strict_strtoul(buf, 0, &val)) {
+		pr_err("pme: invalid input %s\n", buf);
+		return -EINVAL;
+	}
+	if (val > MAX_ACCUMULATOR_INTERVAL) {
+		pr_err("pme: invalid input %s\n", buf);
+		return -ERANGE;
+	}
+
+	accumulator_update_interval(val);
+	return count;
+}
+static ssize_t pme_show_update_interval(struct device_driver *pme, char *buf)
+{
+	return snprintf(buf, PAGE_SIZE, "%u\n", pme_stat_interval);
+}
+static DRIVER_ATTR(update_interval, (S_IRUSR | S_IWUSR),
+		pme_show_update_interval, pme_store_update_interval);
+
+#define FMT_0HEX "0x%08x\n"
+#define FMT_HEX  "0x%x\n"
+#define FMT_DEC  "%u\n"
+#define PRIV_RO  S_IRUSR
+#define PRIV_RW  (S_IRUSR | S_IWUSR)
+
+/* Register Interfaces */
+/* read-write; */
+PME_GENERIC_ATTR(efqc_int, PRIV_RW, FMT_DEC);
+PME_GENERIC_ATTR(sw_db, PRIV_RW, FMT_DEC);
+PME_GENERIC_ATTR(kvlts, PRIV_RW, FMT_DEC);
+PME_GENERIC_ATTR(max_chain_length, PRIV_RW, FMT_DEC);
+PME_GENERIC_ATTR(pattern_range_counter_idx, PRIV_RW, FMT_0HEX);
+PME_GENERIC_ATTR(pattern_range_counter_mask, PRIV_RW, FMT_0HEX);
+PME_GENERIC_ATTR(max_allowed_test_line_per_pattern, PRIV_RW, FMT_DEC);
+PME_GENERIC_ATTR(max_pattern_matches_per_sui, PRIV_RW, FMT_DEC);
+PME_GENERIC_ATTR(max_pattern_evaluations_per_sui, PRIV_RW, FMT_DEC);
+PME_GENERIC_ATTR(report_length_limit, PRIV_RW, FMT_DEC);
+PME_GENERIC_ATTR(end_of_simple_sui_report, PRIV_RW, FMT_DEC);
+PME_GENERIC_ATTR(aim, PRIV_RW, FMT_DEC);
+PME_GENERIC_ATTR(end_of_sui_reaction_ptr, PRIV_RW, FMT_DEC);
+PME_GENERIC_ATTR(sre_pscl, PRIV_RW, FMT_DEC);
+PME_GENERIC_ATTR(sre_max_block_num, PRIV_RW, FMT_DEC);
+PME_GENERIC_ATTR(sre_max_instruction_limit, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(0, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(1, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(2, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(3, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(4, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(5, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(6, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(7, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(8, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(9, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(10, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(11, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(12, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(13, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(14, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(15, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(16, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(17, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(18, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(19, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(20, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(21, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(22, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(23, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(24, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(25, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(26, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(27, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(28, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(29, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(30, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(31, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(32, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(33, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(34, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(35, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(36, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(37, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(38, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(39, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(40, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(41, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(42, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(43, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(44, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(45, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(46, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(47, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(48, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(49, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(50, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(51, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(52, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(53, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(54, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(55, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(56, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(57, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(58, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(59, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(60, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(61, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(62, PRIV_RW, FMT_DEC);
+PME_GENERIC_BSC_ATTR(63, PRIV_RW, FMT_DEC);
+
+/* read-only; */
+PME_GENERIC_ATTR(max_pdsr_index, PRIV_RO, FMT_DEC);
+PME_GENERIC_ATTR(sre_context_size, PRIV_RO, FMT_DEC);
+PME_GENERIC_ATTR(sre_rule_num, PRIV_RO, FMT_DEC);
+PME_GENERIC_ATTR(sre_session_ctx_num, PRIV_RO, FMT_DEC);
+PME_GENERIC_ATTR(sre_max_index_size, PRIV_RO, FMT_DEC);
+PME_GENERIC_ATTR(sre_max_offset_ctrl, PRIV_RO, FMT_DEC);
+PME_GENERIC_ATTR(src_id, PRIV_RO, FMT_DEC);
+PME_GENERIC_ATTR(liodnr, PRIV_RO, FMT_DEC);
+PME_GENERIC_ATTR(rev1, PRIV_RO, FMT_0HEX);
+PME_GENERIC_ATTR(rev2, PRIV_RO, FMT_0HEX);
+PME_GENERIC_ATTR(isr, PRIV_RO, FMT_0HEX);
+
+/* Stats */
+PME_GENERIC_STAT_ATTR(trunci, PRIV_RW);
+PME_GENERIC_STAT_ATTR(rbc, PRIV_RW);
+PME_GENERIC_STAT_ATTR(tbt0ecc1ec, PRIV_RW);
+PME_GENERIC_STAT_ATTR(tbt1ecc1ec, PRIV_RW);
+PME_GENERIC_STAT_ATTR(vlt0ecc1ec, PRIV_RW);
+PME_GENERIC_STAT_ATTR(vlt1ecc1ec, PRIV_RW);
+PME_GENERIC_STAT_ATTR(cmecc1ec, PRIV_RW);
+PME_GENERIC_STAT_ATTR(dxcmecc1ec, PRIV_RW);
+PME_GENERIC_STAT_ATTR(dxemecc1ec, PRIV_RW);
+PME_GENERIC_STAT_ATTR(stnib, PRIV_RW);
+PME_GENERIC_STAT_ATTR(stnis, PRIV_RW);
+PME_GENERIC_STAT_ATTR(stnth1, PRIV_RW);
+PME_GENERIC_STAT_ATTR(stnth2, PRIV_RW);
+PME_GENERIC_STAT_ATTR(stnthv, PRIV_RW);
+PME_GENERIC_STAT_ATTR(stnths, PRIV_RW);
+PME_GENERIC_STAT_ATTR(stnch, PRIV_RW);
+PME_GENERIC_STAT_ATTR(stnpm, PRIV_RW);
+PME_GENERIC_STAT_ATTR(stns1m, PRIV_RW);
+PME_GENERIC_STAT_ATTR(stnpmr, PRIV_RW);
+PME_GENERIC_STAT_ATTR(stndsr, PRIV_RW);
+PME_GENERIC_STAT_ATTR(stnesr, PRIV_RW);
+PME_GENERIC_STAT_ATTR(stns1r, PRIV_RW);
+PME_GENERIC_STAT_ATTR(stnob, PRIV_RW);
+PME_GENERIC_STAT_ATTR(mia_byc, PRIV_RW);
+PME_GENERIC_STAT_ATTR(mia_blc, PRIV_RW);
+
+static struct attribute *pme_drv_attrs[] = {
+	&driver_attr_efqc_int.attr,
+	&driver_attr_sw_db.attr,
+	&driver_attr_kvlts.attr,
+	&driver_attr_max_chain_length.attr,
+	&driver_attr_pattern_range_counter_idx.attr,
+	&driver_attr_pattern_range_counter_mask.attr,
+	&driver_attr_max_allowed_test_line_per_pattern.attr,
+	&driver_attr_max_pdsr_index.attr,
+	&driver_attr_max_pattern_matches_per_sui.attr,
+	&driver_attr_max_pattern_evaluations_per_sui.attr,
+	&driver_attr_report_length_limit.attr,
+	&driver_attr_end_of_simple_sui_report.attr,
+	&driver_attr_aim.attr,
+	&driver_attr_sre_context_size.attr,
+	&driver_attr_sre_rule_num.attr,
+	&driver_attr_sre_session_ctx_num.attr,
+	&driver_attr_end_of_sui_reaction_ptr.attr,
+	&driver_attr_sre_pscl.attr,
+	&driver_attr_sre_max_block_num.attr,
+	&driver_attr_sre_max_instruction_limit.attr,
+	&driver_attr_sre_max_index_size.attr,
+	&driver_attr_sre_max_offset_ctrl.attr,
+	&driver_attr_src_id.attr,
+	&driver_attr_liodnr.attr,
+	&driver_attr_rev1.attr,
+	&driver_attr_rev2.attr,
+	&driver_attr_isr.attr,
+	&driver_attr_bsc_0.attr,
+	&driver_attr_bsc_1.attr,
+	&driver_attr_bsc_2.attr,
+	&driver_attr_bsc_3.attr,
+	&driver_attr_bsc_4.attr,
+	&driver_attr_bsc_5.attr,
+	&driver_attr_bsc_6.attr,
+	&driver_attr_bsc_7.attr,
+	&driver_attr_bsc_8.attr,
+	&driver_attr_bsc_9.attr,
+	&driver_attr_bsc_10.attr,
+	&driver_attr_bsc_11.attr,
+	&driver_attr_bsc_12.attr,
+	&driver_attr_bsc_13.attr,
+	&driver_attr_bsc_14.attr,
+	&driver_attr_bsc_15.attr,
+	&driver_attr_bsc_16.attr,
+	&driver_attr_bsc_17.attr,
+	&driver_attr_bsc_18.attr,
+	&driver_attr_bsc_19.attr,
+	&driver_attr_bsc_20.attr,
+	&driver_attr_bsc_21.attr,
+	&driver_attr_bsc_22.attr,
+	&driver_attr_bsc_23.attr,
+	&driver_attr_bsc_24.attr,
+	&driver_attr_bsc_25.attr,
+	&driver_attr_bsc_26.attr,
+	&driver_attr_bsc_27.attr,
+	&driver_attr_bsc_28.attr,
+	&driver_attr_bsc_29.attr,
+	&driver_attr_bsc_30.attr,
+	&driver_attr_bsc_31.attr,
+	&driver_attr_bsc_32.attr,
+	&driver_attr_bsc_33.attr,
+	&driver_attr_bsc_34.attr,
+	&driver_attr_bsc_35.attr,
+	&driver_attr_bsc_36.attr,
+	&driver_attr_bsc_37.attr,
+	&driver_attr_bsc_38.attr,
+	&driver_attr_bsc_39.attr,
+	&driver_attr_bsc_40.attr,
+	&driver_attr_bsc_41.attr,
+	&driver_attr_bsc_42.attr,
+	&driver_attr_bsc_43.attr,
+	&driver_attr_bsc_44.attr,
+	&driver_attr_bsc_45.attr,
+	&driver_attr_bsc_46.attr,
+	&driver_attr_bsc_47.attr,
+	&driver_attr_bsc_48.attr,
+	&driver_attr_bsc_49.attr,
+	&driver_attr_bsc_50.attr,
+	&driver_attr_bsc_51.attr,
+	&driver_attr_bsc_52.attr,
+	&driver_attr_bsc_53.attr,
+	&driver_attr_bsc_54.attr,
+	&driver_attr_bsc_55.attr,
+	&driver_attr_bsc_56.attr,
+	&driver_attr_bsc_57.attr,
+	&driver_attr_bsc_58.attr,
+	&driver_attr_bsc_59.attr,
+	&driver_attr_bsc_60.attr,
+	&driver_attr_bsc_61.attr,
+	&driver_attr_bsc_62.attr,
+	&driver_attr_bsc_63.attr,
+	NULL
+};
+
+static struct attribute *pme_drv_stats_attrs[] = {
+	&driver_attr_update_interval.attr,
+	&driver_attr_trunci.attr,
+	&driver_attr_rbc.attr,
+	&driver_attr_tbt0ecc1ec.attr,
+	&driver_attr_tbt1ecc1ec.attr,
+	&driver_attr_vlt0ecc1ec.attr,
+	&driver_attr_vlt1ecc1ec.attr,
+	&driver_attr_cmecc1ec.attr,
+	&driver_attr_dxcmecc1ec.attr,
+	&driver_attr_dxemecc1ec.attr,
+	&driver_attr_stnib.attr,
+	&driver_attr_stnis.attr,
+	&driver_attr_stnth1.attr,
+	&driver_attr_stnth2.attr,
+	&driver_attr_stnthv.attr,
+	&driver_attr_stnths.attr,
+	&driver_attr_stnch.attr,
+	&driver_attr_stnpm.attr,
+	&driver_attr_stns1m.attr,
+	&driver_attr_stnpmr.attr,
+	&driver_attr_stndsr.attr,
+	&driver_attr_stnesr.attr,
+	&driver_attr_stns1r.attr,
+	&driver_attr_stnob.attr,
+	&driver_attr_mia_byc.attr,
+	&driver_attr_mia_blc.attr,
+	NULL
+};
+
+static struct attribute_group pme_drv_attr_grp = {
+	.attrs = pme_drv_attrs
+};
+
+static struct attribute_group pme_drv_stats_attr_grp = {
+	.name  = "stats",
+	.attrs = pme_drv_stats_attrs
+};
+
+static struct attribute_group *pme_drv_attr_groups[] = {
+	&pme_drv_attr_grp,
+	&pme_drv_stats_attr_grp,
+	NULL,
+};
+
+static struct of_device_id of_fsl_pme_ids[] = {
+	{ .compatible = "fsl,pme", },
+	{}
+};
+
+/* Pme interrupt handler */
+static irqreturn_t pme_isr(int irq, void *ptr)
+{
+	static u32 last_isrstate;
+	u32 isrstate = pme_in(global_pme, ISR) ^ last_isrstate;
+
+	/* What new ISR state has been raise */
+	if (!isrstate)
+		return IRQ_NONE;
+	if (isrstate & PME_SBE_ERR)
+		pr_crit("PME: SBE detected\n");
+	if (isrstate & PME_DBE_ERR)
+		pr_crit("PME: DBE detected\n");
+	if (isrstate & PME_PME_ERR)
+		pr_crit("PME: PME serious detected\n");
+	/* Clear the ier interrupt bit */
+	last_isrstate |= isrstate;
+	pme_out(global_pme, IER, ~last_isrstate);
+	return IRQ_HANDLED;
+}
+
+static int of_fsl_pme_remove(struct of_device *ofdev)
+{
+	/* Cancel pme accumulator */
+	accumulator_update_interval(0);
+	cancel_delayed_work_sync(&accumulator_work);
+	/* Disable PME..TODO need to wait till it's quiet */
+	pme_out(global_pme, FACONF, PME_FACONF_RESET);
+
+	/* Release interrupt */
+	free_irq(pme_err_irq, &ofdev->dev);
+
+	/* Unmap controller region */
+	iounmap(global_pme);
+	return 0;
+}
+
+static int __devinit of_fsl_pme_probe(struct of_device *ofdev,
+				const struct of_device_id *match)
+{
+	u32 __iomem *regs;
+	struct device *dev;
+	struct device_node *nprop;
+	u32 clkfreq = DEFAULT_SRFCC * 1000000;
+	const u32 *value;
+	int srec_aim = 0, srec_esr = 0;
+	u32 srecontextsize_code;
+
+	dev = &ofdev->dev;
+	nprop = ofdev->node;
+
+	pme_err_irq = of_irq_to_resource(nprop, 0, NULL);
+	if (pme_err_irq == NO_IRQ) {
+		dev_err(dev, "Can't get %s property '%s'\n", nprop->full_name,
+			"interrupts");
+		return -ENODEV;
+	}
+
+	/* Get configuration properties from device tree */
+	/* First, get register page */
+	regs = of_iomap(nprop, 0);
+	if (regs == NULL) {
+		dev_err(dev, "of_iomap() failed\n");
+		return -EINVAL;
+	}
+	/* Global configuration */
+	global_pme = pme_create(regs);
+
+	/* Register the pme ISR handler */
+	if (request_irq(pme_err_irq, pme_isr, IRQF_SHARED, "pme-err", dev)) {
+		dev_err(dev, "request_irq() failed\n");
+		return -ENODEV;
+	}
+
+#ifdef CONFIG_FSL_PME2_SRE_AIM
+	srec_aim = 1;
+#endif
+#ifdef CONFIG_FSL_PME2_SRE_ESR
+	srec_esr = 1;
+#endif
+	/* Validate some parameters */
+	if (!sre_session_ctx_size || !is_power_of_2(sre_session_ctx_size) ||
+			(sre_session_ctx_size < 32) ||
+			(sre_session_ctx_size > (131072))) {
+		dev_err(dev, "invalid sre_session_ctx_size\n");
+		iounmap(global_pme);
+		return -EINVAL;
+	}
+	srecontextsize_code = ilog2(sre_session_ctx_size);
+	srecontextsize_code -= 4;
+
+	value = of_get_property(nprop, "clock-frequency", NULL);
+	if (value)
+		clkfreq = *value;
+
+	pme_out(global_pme, SFRCC, clkfreq/1000000);
+	BUG_ON(sizeof(dxe_a) != 4);
+	pme_out(global_pme, PDSRBAL, (u32)dxe_a);
+	pme_out(global_pme, SCBARL, (u32)sre_a);
+	/* Maximum allocated index into the PDSR table available to the DXE */
+	pme_out(global_pme, DEC1, (dxe_sz/PDSR_TBL_ALIGN)-1);
+	/* Maximum allocated index into the PDSR table available to the SRE */
+	pme_out(global_pme, SEC2, (dxe_sz/PDSR_TBL_ALIGN)-1);
+	/* Maximum allocated 32-byte offset into SRE Context Table.*/
+	if (sre_sz)
+		pme_out(global_pme, SEC3, (sre_sz/SRE_TBL_ALIGN)-1);
+	/* Max test line execution */
+	pme_out(global_pme, DEC0, max_test_line_per_pat);
+	pme_out(global_pme, DLC,
+		(max_pat_eval_per_sui << 16) | max_pat_matches_per_sui);
+
+	/* SREC - SRE Config */
+	pme_out(global_pme, SREC,
+		/* Number of rules in database */
+		(sre_rule_num << 0) |
+		/* Simple Report Enabled */
+		((srec_esr ? 1 : 0) << 18) |
+		/* Context Size per Session */
+		(srecontextsize_code << 19) |
+		/* Alternate Inclusive Mode */
+		((srec_aim ? 1 : 0) << 29));
+
+	pme_out(global_pme, SEC1,
+		(CONFIG_FSL_PME2_SRE_MAX_INSTRUCTION_LIMIT << 16) |
+		CONFIG_FSL_PME2_SRE_MAX_BLOCK_NUMBER);
+
+	/* Setup Accumulator */
+	if (pme_stat_interval)
+		schedule_delayed_work(&accumulator_work,
+				msecs_to_jiffies(pme_stat_interval));
+
+	/* Enable interrupts */
+	pme_out(global_pme, IER, PME_ALL_ERR);
+
+	dev_info(dev, "ver: 0x%08x\n", pme_in(global_pme, PM_IP_REV1));
+	return 0;
+}
+
+static struct of_platform_driver of_fsl_pme_driver = {
+	.name = "of-fsl-pme",
+	.match_table = of_fsl_pme_ids,
+	.probe = of_fsl_pme_probe,
+	.driver = {
+		.groups = pme_drv_attr_groups,
+	},
+	.remove      = __devexit_p(of_fsl_pme_remove),
+};
+
+static int pme2_ctrl_init(void)
+{
+	return of_register_platform_driver(&of_fsl_pme_driver);
+}
+
+static void pme2_ctrl_exit(void)
+{
+	of_unregister_platform_driver(&of_fsl_pme_driver);
+}
+
+module_init(pme2_ctrl_init);
+module_exit(pme2_ctrl_exit);
+
+/************
+ * Section 3
+ ************
+ * These APIs are the only functional hooks into the control driver, besides the
+ * sysfs attributes.
+ */
+
+int pme2_exclusive_set(struct qman_fq *fq)
+{
+	if (!global_pme)
+		return -ENODEV;
+	pme_out(global_pme, EFQC, PME_EFQC(1, qman_fq_fqid(fq)));
+	return 0;
+}
+EXPORT_SYMBOL(pme2_exclusive_set);
+
+void pme2_exclusive_unset(void)
+{
+	pme_out(global_pme, EFQC, PME_EFQC(0, 0));
+}
+EXPORT_SYMBOL(pme2_exclusive_unset);
+
+int pme_attr_set(enum pme_attr attr, u32 val)
+{
+	u32 mask;
+	u32 attr_val;
+
+	/* Check if Buffer size configuration */
+	if (attr >= pme_attr_bsc_first && attr <= pme_attr_bsc_last) {
+		u32 bsc_pool_id = attr - pme_attr_bsc_first;
+		u32 bsc_pool_offset = bsc_pool_id % 8;
+		u32 bsc_pool_mask = ~(0xF << ((7-bsc_pool_offset)*4));
+		/* range for val 0..0xB */
+		if (val > 0xb)
+			return -EINVAL;
+		/* calculate which sky-blue reg */
+		/* 0..7 -> bsc_(0..7), PME_REG_BSC0 */
+		/* 8..15 -> bsc_(8..15) PME_REG_BSC1*/
+		/* ... */
+		/* 56..63 -> bsc_(56..63) PME_REG_BSC7*/
+		attr_val = pme_in(global_pme, BSC0 + ((bsc_pool_id/8)*4));
+		/* Now mask in the new value */
+		attr_val = attr_val & bsc_pool_mask;
+		attr_val = attr_val | (val << ((7-bsc_pool_offset)*4));
+		pme_out(global_pme,  BSC0 + ((bsc_pool_id/8)*4), attr_val);
+		return 0;
+	}
+
+	switch (attr) {
+	case pme_attr_efqc_int:
+		if (val > 4)
+			return -EINVAL;
+		mask = 0x8FFFFFFF;
+		attr_val = pme_in(global_pme, EFQC);
+		/* clear efqc_int */
+		attr_val &= mask;
+		/* clear unwanted bits in val*/
+		val &= ~mask;
+		val <<= 28;
+		val |= attr_val;
+		pme_out(global_pme, EFQC, val);
+		break;
+
+	case pme_attr_sw_db:
+		pme_out(global_pme, SWDB, val);
+		break;
+
+	case pme_attr_kvlts:
+		if (val < 2 || val > 16)
+			return -EINVAL;
+		/* HW range: 1..15, SW range: 2..16 */
+		pme_out(global_pme, KVLTS, --val);
+		break;
+
+	case pme_attr_max_chain_length:
+		if (val > 0x7FFF)
+			val = 0x7FFF;
+		pme_out(global_pme, KEC, val);
+		break;
+
+	case pme_attr_pattern_range_counter_idx:
+		if (val > 0x1FFFF)
+			val = 0x1FFFF;
+		pme_out(global_pme, DRCIC, val);
+		break;
+
+	case pme_attr_pattern_range_counter_mask:
+		if (val > 0x1FFFF)
+			val = 0x1FFFF;
+		pme_out(global_pme, DRCMC, val);
+		break;
+
+	case pme_attr_max_allowed_test_line_per_pattern:
+		if (val > 0x3FFF)
+			val = 0x3FFF;
+		pme_out(global_pme, DEC0, val);
+		break;
+
+	case pme_attr_max_pattern_matches_per_sui:
+		/* mpe, mpm */
+		if (val > 0xFFFF)
+			val = 0xFFFF;
+		mask = 0xFFFF0000;
+		attr_val = pme_in(global_pme, DLC);
+		/* clear mpm */
+		attr_val &= mask;
+		val &= ~mask;
+		val |= attr_val;
+		pme_out(global_pme, DLC, val);
+		break;
+
+	case pme_attr_max_pattern_evaluations_per_sui:
+		/* mpe, mpm */
+		if (val > 0xFFFF)
+			val = 0xFFFF;
+		mask = 0x0000FFFF;
+		attr_val = pme_in(global_pme, DLC);
+		/* clear mpe */
+		attr_val &= mask;
+		/* clear unwanted bits in val*/
+		val &= mask;
+		val <<= 16;
+		val |= attr_val;
+		pme_out(global_pme, DLC, val);
+		break;
+
+	case pme_attr_report_length_limit:
+		if (val > 0xFFFF)
+			val = 0xFFFF;
+		pme_out(global_pme, RLL, val);
+		break;
+
+	case pme_attr_end_of_simple_sui_report:
+		/* bit 13 */
+		mask = 0x00040000;
+		attr_val = pme_in(global_pme, SREC);
+		if (val)
+			attr_val |= mask;
+		else
+			attr_val &= ~mask;
+		pme_out(global_pme, SREC, attr_val);
+		break;
+
+	case pme_attr_aim:
+		/* bit 2 */
+		mask = 0x20000000;
+		attr_val = pme_in(global_pme, SREC);
+		if (val)
+			attr_val |= mask;
+		else
+			attr_val &= ~mask;
+		pme_out(global_pme, SREC, attr_val);
+		break;
+
+	case pme_attr_end_of_sui_reaction_ptr:
+		if (val > 0xFFFFF)
+			val = 0xFFFFF;
+		pme_out(global_pme, ESRP, val);
+		break;
+
+	case pme_attr_sre_pscl:
+		pme_out(global_pme, SFRCC, val);
+		break;
+
+	case pme_attr_sre_max_block_num:
+		/* bits 17..31 */
+		if (val > 0x7FFF)
+			val = 0x7FFF;
+		mask = 0xFFFF8000;
+		attr_val = pme_in(global_pme, SEC1);
+		/* clear mbn */
+		attr_val &= mask;
+		/* clear unwanted bits in val*/
+		val &= ~mask;
+		val |= attr_val;
+		pme_out(global_pme, SEC1, val);
+		break;
+
+	case pme_attr_sre_max_instruction_limit:
+		/* bits 0..15 */
+		if (val > 0xFFFF)
+			val = 0xFFFF;
+		mask = 0x0000FFFF;
+		attr_val = pme_in(global_pme, SEC1);
+		/* clear mil */
+		attr_val &= mask;
+		/* clear unwanted bits in val*/
+		val &= mask;
+		val <<= 16;
+		val |= attr_val;
+		pme_out(global_pme, SEC1, val);
+		break;
+
+	case pme_attr_srrv0:
+		pme_out(global_pme, SRRV0, val);
+		break;
+	case pme_attr_srrv1:
+		pme_out(global_pme, SRRV1, val);
+		break;
+	case pme_attr_srrv2:
+		pme_out(global_pme, SRRV2, val);
+		break;
+	case pme_attr_srrv3:
+		pme_out(global_pme, SRRV3, val);
+		break;
+	case pme_attr_srrv4:
+		pme_out(global_pme, SRRV4, val);
+		break;
+	case pme_attr_srrv5:
+		pme_out(global_pme, SRRV5, val);
+		break;
+	case pme_attr_srrv6:
+		pme_out(global_pme, SRRV6, val);
+		break;
+	case pme_attr_srrv7:
+		pme_out(global_pme, SRRV7, val);
+		break;
+	case pme_attr_srrfi:
+		pme_out(global_pme, SRRFI, val);
+		break;
+	case pme_attr_srri:
+		pme_out(global_pme, SRRI, val);
+		break;
+	case pme_attr_srrwc:
+		pme_out(global_pme, SRRWC, val);
+		break;
+	case pme_attr_srrr:
+		pme_out(global_pme, SRRR, val);
+		break;
+
+	default:
+		pr_err("pme: Unknown attr %u\n", attr);
+		return -EINVAL;
+	};
+	return 0;
+}
+EXPORT_SYMBOL(pme_attr_set);
+
+u32 pme_attr_get(enum pme_attr attr)
+{
+	u32 mask;
+	u32 attr_val;
+
+	/* Check if Buffer size configuration */
+	if (attr >= pme_attr_bsc_first && attr <= pme_attr_bsc_last) {
+		u32 bsc_pool_id = attr - pme_attr_bsc_first;
+		u32 bsc_pool_offset = bsc_pool_id % 8;
+		/* calculate which sky-blue reg */
+		/* 0..7 -> bsc_(0..7), PME_REG_BSC0 */
+		/* 8..15 -> bsc_(8..15) PME_REG_BSC1*/
+		/* ... */
+		/* 56..63 -> bsc_(56..63) PME_REG_BSC7*/
+		attr_val = pme_in(global_pme, BSC0 + ((bsc_pool_id/8)*4));
+		attr_val = attr_val >> ((7-bsc_pool_offset)*4);
+		attr_val = attr_val & 0x0000000F;
+		return attr_val;
+	}
+
+	switch (attr) {
+	case pme_attr_efqc_int:
+		mask = 0x8FFFFFFF;
+		attr_val = pme_in(global_pme, EFQC);
+		attr_val &= ~mask;
+		attr_val >>= 28;
+		break;
+
+	case pme_attr_sw_db:
+		attr_val = pme_in(global_pme, SWDB);
+		break;
+
+	case pme_attr_kvlts:
+		/* bit 28-31 */
+		attr_val = pme_in(global_pme, KVLTS);
+		attr_val &= 0x0000000F;
+		/* HW range: 1..15, SW range: 2..16 */
+		attr_val += 1;
+		break;
+
+	case pme_attr_max_chain_length:
+		/* bit 17-31 */
+		attr_val = pme_in(global_pme, KEC);
+		attr_val &= 0x00007FFF;
+		break;
+
+	case pme_attr_pattern_range_counter_idx:
+		/* bit 15-31 */
+		attr_val = pme_in(global_pme, DRCIC);
+		attr_val &= 0x0001FFFF;
+		break;
+
+	case pme_attr_pattern_range_counter_mask:
+		/* bit 15-31 */
+		attr_val = pme_in(global_pme, DRCMC);
+		attr_val &= 0x0001FFFF;
+		break;
+
+	case pme_attr_max_allowed_test_line_per_pattern:
+		/* bit 18-31 */
+		attr_val = pme_in(global_pme, DEC0);
+		attr_val &= 0x00003FFF;
+		break;
+
+	case pme_attr_max_pdsr_index:
+		/* bit 12-31 */
+		attr_val = pme_in(global_pme, DEC1);
+		attr_val &= 0x000FFFFF;
+		break;
+
+	case pme_attr_max_pattern_matches_per_sui:
+		attr_val = pme_in(global_pme, DLC);
+		attr_val &= 0x0000FFFF;
+		break;
+
+	case pme_attr_max_pattern_evaluations_per_sui:
+		attr_val = pme_in(global_pme, DLC);
+		attr_val >>= 16;
+		break;
+
+	case pme_attr_report_length_limit:
+		attr_val = pme_in(global_pme, RLL);
+		/* clear unwanted bits in val*/
+		attr_val &= 0x0000FFFF;
+		break;
+
+	case pme_attr_end_of_simple_sui_report:
+		/* bit 13 */
+		attr_val = pme_in(global_pme, SREC);
+		attr_val >>= 18;
+		/* clear unwanted bits in val*/
+		attr_val &= 0x00000001;
+		break;
+
+	case pme_attr_aim:
+		/* bit 2 */
+		attr_val = pme_in(global_pme, SREC);
+		attr_val >>= 29;
+		/* clear unwanted bits in val*/
+		attr_val &= 0x00000001;
+		break;
+
+	case pme_attr_sre_context_size:
+		/* bits 9..12 */
+		attr_val = pme_in(global_pme, SREC);
+		attr_val >>= 19;
+		/* clear unwanted bits in val*/
+		attr_val &= 0x0000000F;
+		attr_val += 4;
+		attr_val = 1 << attr_val;
+		break;
+
+	case pme_attr_sre_rule_num:
+		/* bits 24..31 */
+		attr_val = pme_in(global_pme, SREC);
+		/* clear unwanted bits in val*/
+		attr_val &= 0x000003FF;
+		break;
+
+	case pme_attr_sre_session_ctx_num: {
+			u32 ctx_sz = 0;
+			/* = sre_table_size / sre_session_ctx_size */
+			attr_val = pme_in(global_pme, SEC3);
+			/* clear unwanted bits in val*/
+			attr_val &= 0x07FFFFFF;
+			attr_val += 1;
+			attr_val *= 32;
+			ctx_sz = pme_in(global_pme, SREC);
+			ctx_sz >>= 19;
+			/* clear unwanted bits in val*/
+			ctx_sz &= 0x0000000F;
+			ctx_sz += 4;
+			attr_val /= (1 << ctx_sz);
+		}
+		break;
+
+	case pme_attr_end_of_sui_reaction_ptr:
+		/* bits 12..31 */
+		attr_val = pme_in(global_pme, ESRP);
+		/* clear unwanted bits in val*/
+		attr_val &= 0x000FFFFF;
+		break;
+
+	case pme_attr_sre_pscl:
+		/* bits 22..31 */
+		attr_val = pme_in(global_pme, SFRCC);
+		/* clear unwanted bits in val*/
+		attr_val &= 0x000000FF;
+		break;
+
+	case pme_attr_sre_max_block_num:
+		/* bits 17..31 */
+		attr_val = pme_in(global_pme, SEC1);
+		/* clear unwanted bits in val*/
+		attr_val &= 0x00007FFF;
+		break;
+
+	case pme_attr_sre_max_instruction_limit:
+		/* bits 0..15 */
+		attr_val = pme_in(global_pme, SEC1);
+		attr_val >>= 16;
+		break;
+
+	case pme_attr_sre_max_index_size:
+		/* bits 12..31 */
+		attr_val = pme_in(global_pme, SEC2);
+		/* clear unwanted bits in val*/
+		attr_val &= 0x000FFFFF;
+		break;
+
+	case pme_attr_sre_max_offset_ctrl:
+		/* bits 5..31 */
+		attr_val = pme_in(global_pme, SEC3);
+		/* clear unwanted bits in val*/
+		attr_val &= 0x07FFFFFF;
+		break;
+
+	case pme_attr_src_id:
+		/* bits 24..31 */
+		attr_val = pme_in(global_pme, SRCIDR);
+		/* clear unwanted bits in val*/
+		attr_val &= 0x000000FF;
+		break;
+
+	case pme_attr_liodnr:
+		/* bits 20..31 */
+		attr_val = pme_in(global_pme, LIODNR);
+		/* clear unwanted bits in val*/
+		attr_val &= 0x00000FFF;
+		break;
+
+	case pme_attr_rev1:
+		/* bits 0..31 */
+		attr_val = pme_in(global_pme, PM_IP_REV1);
+		break;
+
+	case pme_attr_rev2:
+		/* bits 0..31 */
+		attr_val = pme_in(global_pme, PM_IP_REV2);
+		break;
+
+	case pme_attr_srrr:
+		attr_val = pme_in(global_pme, SRRR);
+		break;
+
+	case pme_attr_trunci:
+		attr_val = pme_in(global_pme, TRUNCI);
+		break;
+
+	case pme_attr_rbc:
+		attr_val = pme_in(global_pme, RBC);
+		break;
+
+	case pme_attr_tbt0ecc1ec:
+		attr_val = pme_in(global_pme, TBT0ECC1EC);
+		break;
+
+	case pme_attr_tbt1ecc1ec:
+		attr_val = pme_in(global_pme, TBT1ECC1EC);
+		break;
+
+	case pme_attr_vlt0ecc1ec:
+		attr_val = pme_in(global_pme, VLT0ECC1EC);
+		break;
+
+	case pme_attr_vlt1ecc1ec:
+		attr_val = pme_in(global_pme, VLT1ECC1EC);
+		break;
+
+	case pme_attr_cmecc1ec:
+		attr_val = pme_in(global_pme, CMECC1EC);
+		break;
+
+	case pme_attr_dxcmecc1ec:
+		attr_val = pme_in(global_pme, DXCMECC1EC);
+		break;
+
+	case pme_attr_dxemecc1ec:
+		attr_val = pme_in(global_pme, DXEMECC1EC);
+		break;
+
+	case pme_attr_stnib:
+		attr_val = pme_in(global_pme, STNIB);
+		break;
+
+	case pme_attr_stnis:
+		attr_val = pme_in(global_pme, STNIS);
+		break;
+
+	case pme_attr_stnth1:
+		attr_val = pme_in(global_pme, STNTH1);
+		break;
+
+	case pme_attr_stnth2:
+		attr_val = pme_in(global_pme, STNTH2);
+		break;
+
+	case pme_attr_stnthv:
+		attr_val = pme_in(global_pme, STNTHV);
+		break;
+
+	case pme_attr_stnths:
+		attr_val = pme_in(global_pme, STNTHS);
+		break;
+
+	case pme_attr_stnch:
+		attr_val = pme_in(global_pme, STNCH);
+		break;
+
+	case pme_attr_stnpm:
+		attr_val = pme_in(global_pme, STNPM);
+		break;
+
+	case pme_attr_stns1m:
+		attr_val = pme_in(global_pme, STNS1M);
+		break;
+
+	case pme_attr_stnpmr:
+		attr_val = pme_in(global_pme, STNPMR);
+		break;
+
+	case pme_attr_stndsr:
+		attr_val = pme_in(global_pme, STNDSR);
+		break;
+
+	case pme_attr_stnesr:
+		attr_val = pme_in(global_pme, STNESR);
+		break;
+
+	case pme_attr_stns1r:
+		attr_val = pme_in(global_pme, STNS1R);
+		break;
+
+	case pme_attr_stnob:
+		attr_val = pme_in(global_pme, STNOB);
+		break;
+
+	case pme_attr_mia_byc:
+		attr_val = pme_in(global_pme, MIA_BYC);
+		break;
+
+	case pme_attr_mia_blc:
+		attr_val = pme_in(global_pme, MIA_BLC);
+		break;
+
+	case pme_attr_isr:
+		attr_val = pme_in(global_pme, ISR);
+		break;
+
+	default:
+		pr_err("pme: Unknown attr %u\n", attr);
+		return 0;
+	};
+	return attr_val;
+}
+EXPORT_SYMBOL(pme_attr_get);
+
+static enum pme_attr stat_list[] = {
+	pme_attr_trunci,
+	pme_attr_rbc,
+	pme_attr_tbt0ecc1ec,
+	pme_attr_tbt1ecc1ec,
+	pme_attr_vlt0ecc1ec,
+	pme_attr_vlt1ecc1ec,
+	pme_attr_cmecc1ec,
+	pme_attr_dxcmecc1ec,
+	pme_attr_dxemecc1ec,
+	pme_attr_stnib,
+	pme_attr_stnis,
+	pme_attr_stnth1,
+	pme_attr_stnth2,
+	pme_attr_stnthv,
+	pme_attr_stnths,
+	pme_attr_stnch,
+	pme_attr_stnpm,
+	pme_attr_stns1m,
+	pme_attr_stnpmr,
+	pme_attr_stndsr,
+	pme_attr_stnesr,
+	pme_attr_stns1r,
+	pme_attr_stnob,
+	pme_attr_mia_byc,
+	pme_attr_mia_blc
+};
+
+static u64 pme_stats[sizeof(stat_list)/sizeof(enum pme_attr)];
+static DEFINE_SPINLOCK(stat_lock);
+
+int pme_stat_get(enum pme_attr *stat, u64 *value, int reset)
+{
+	int i, ret = 0;
+	int value_set = 0;
+
+	spin_lock_irq(&stat_lock);
+	if (stat == NULL || value == NULL) {
+		pr_err("pme: Invalid stat request %d\n", *stat);
+		ret = -EINVAL;
+	} else {
+		for (i = 0; i < sizeof(stat_list)/sizeof(enum pme_attr); i++) {
+			if (stat_list[i] == *stat) {
+				pme_stats[i] += pme_attr_get(stat_list[i]);
+				*value = pme_stats[i];
+				value_set = 1;
+				if (reset)
+					pme_stats[i] = 0;
+				break;
+			}
+		}
+		if (!value_set) {
+			pr_err("pme: Invalid stat request %d\n", *stat);
+			ret = -EINVAL;
+		}
+	}
+	spin_unlock_irq(&stat_lock);
+	return ret;
+}
+EXPORT_SYMBOL(pme_stat_get);
+
+static void accumulator_update_interval(u32 interval)
+{
+	int schedule = 0;
+
+	spin_lock_irq(&stat_lock);
+	if (!pme_stat_interval && interval)
+		schedule = 1;
+	pme_stat_interval = interval;
+	spin_unlock_irq(&stat_lock);
+	if (schedule)
+		schedule_delayed_work(&accumulator_work,
+				msecs_to_jiffies(interval));
+}
+
+static void accumulator_update(struct work_struct *work)
+{
+	int i;
+	u32 local_interval;
+
+	spin_lock_irq(&stat_lock);
+	local_interval = pme_stat_interval;
+	for (i = 0; i < sizeof(stat_list)/sizeof(enum pme_attr); i++)
+		pme_stats[i] += pme_attr_get(stat_list[i]);
+	spin_unlock_irq(&stat_lock);
+	if (local_interval)
+		schedule_delayed_work(&accumulator_work,
+				msecs_to_jiffies(local_interval));
+}
+
diff --git a/drivers/match/pme2_db.c b/drivers/match/pme2_db.c
new file mode 100644
index 0000000..adccbf4
--- /dev/null
+++ b/drivers/match/pme2_db.c
@@ -0,0 +1,470 @@
+/* Copyright (c) 2009, Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "pme2_private.h"
+
+/* Forward declaration */
+static struct miscdevice fsl_pme2_db_dev;
+
+/* Global spinlock for handling exclusive inc/dec */
+static DEFINE_SPINLOCK(exclusive_lock);
+
+/* Private structure that is allocated for each open that is done on the
+ * pme_db device. This is used to maintain the state of a database session */
+struct db_session {
+	/* The ctx that is needed to communicate with the pme high level */
+	struct pme_ctx ctx;
+	/* Used to track the EXCLUSIVE_INC and EXCLUSIVE_DEC ioctls */
+	unsigned int exclusive_counter;
+};
+
+struct cmd_token {
+	/* pme high level token */
+	struct pme_ctx_token hl_token;
+	/* data */
+	struct qm_fd rx_fd;
+	/* Completion interface */
+	struct completion cb_done;
+};
+
+/* PME Compound Frame Index */
+#define INPUT_FRM	1
+#define OUTPUT_FRM	0
+
+/* Callback for database operations */
+static void db_cb(struct pme_ctx *ctx, const struct qm_fd *fd,
+				struct pme_ctx_token *ctx_token)
+{
+	struct cmd_token *token = (struct cmd_token *)ctx_token;
+	token->rx_fd = *fd;
+	complete(&token->cb_done);
+}
+
+static int exclusive_inc(struct file *fp, struct db_session *db)
+{
+	int ret;
+
+	BUG_ON(!db);
+	BUG_ON(!(db->ctx.flags & PME_CTX_FLAG_EXCLUSIVE));
+	spin_lock(&exclusive_lock);
+	ret = pme_ctx_exclusive_inc(&db->ctx,
+			(PME_CTX_OP_WAIT | PME_CTX_OP_WAIT_INT));
+	if (!ret)
+		db->exclusive_counter++;
+	spin_unlock(&exclusive_lock);
+	return ret;
+}
+
+static int exclusive_dec(struct file *fp, struct db_session *db)
+{
+	int ret = 0;
+
+	BUG_ON(!db);
+	BUG_ON(!(db->ctx.flags & PME_CTX_FLAG_EXCLUSIVE));
+	spin_lock(&exclusive_lock);
+	if (!db->exclusive_counter) {
+		PMEPRERR("exclusivity counter already zero\n");
+		ret = -EINVAL;
+	} else {
+		pme_ctx_exclusive_dec(&db->ctx);
+		db->exclusive_counter--;
+	}
+	spin_unlock(&exclusive_lock);
+	return ret;
+}
+
+static int execute_cmd(struct file *fp, struct db_session *db,
+			unsigned long arg)
+{
+	int ret = 0;
+	struct cmd_token token;
+	/* The kernels copy of the user op structure */
+	struct pme_db kernel_op;
+	struct qm_sg_entry tx_comp[2];
+	struct qm_fd tx_fd;
+	void *tx_data = NULL;
+	void *rx_data = NULL;
+	u32 src_sz, dst_sz;
+	dma_addr_t dma_addr;
+
+	memset(&token, 0, sizeof(struct cmd_token));
+	memset(tx_comp, 0, sizeof(tx_comp));
+	memset(&tx_fd, 0, sizeof(struct qm_fd));
+	init_completion(&token.cb_done);
+
+	/* Copy the command to kernel space */
+	if (copy_from_user(&kernel_op, (struct pme_db __user *)arg,
+			sizeof(struct pme_db)))
+		return -EFAULT;
+	/* Copy the input */
+	PMEPRINFO("Received User Space Contiguous mem \n");
+	PMEPRINFO("length = %d \n", kernel_op.input.size);
+	tx_data = kmalloc(kernel_op.input.size, GFP_KERNEL);
+	if (!tx_data) {
+		PMEPRERR("Err alloc %d byte \n", kernel_op.input.size);
+		return -ENOMEM;
+	}
+	PMEPRINFO("kmalloc tx %p\n", tx_data);
+
+	if (copy_from_user(tx_data,
+			(void __user *)kernel_op.input.data,
+			kernel_op.input.size)) {
+		PMEPRERR("Error copying contigous user data \n");
+		ret = -EFAULT;
+		goto free_tx_data;
+	}
+	PMEPRINFO("Copied contiguous user data\n");
+
+	/* Setup input frame */
+	tx_comp[INPUT_FRM].final = 1;
+	tx_comp[INPUT_FRM].length = kernel_op.input.size;
+	dma_addr = pme_map(tx_data);
+	if (pme_map_error(dma_addr)) {
+		PMEPRERR("Error pme_map_error\n");
+		ret = -EIO;
+		goto free_tx_data;
+	}
+	set_sg_addr(&tx_comp[INPUT_FRM], dma_addr);
+	/* setup output frame, if output is expected */
+	if (kernel_op.output.size) {
+		PMEPRINFO("expect output %d\n", kernel_op.output.size);
+		rx_data = kmalloc(kernel_op.output.size, GFP_KERNEL);
+		if (!rx_data) {
+			PMEPRERR("Err alloc %d byte", kernel_op.output.size);
+			ret = -ENOMEM;
+			goto unmap_input_frame;
+		}
+		PMEPRINFO("kmalloc rx %p, size %d\n", rx_data,
+				kernel_op.output.size);
+		/* Setup output frame */
+		tx_comp[OUTPUT_FRM].length = kernel_op.output.size;
+		dma_addr = pme_map(rx_data);
+		if (pme_map_error(dma_addr)) {
+			PMEPRERR("Error pme_map_error\n");
+			ret = -EIO;
+			goto comp_frame_free_rx;
+		}
+		set_sg_addr(&tx_comp[OUTPUT_FRM], dma_addr);
+		tx_fd.format = qm_fd_compound;
+		/* Build compound frame */
+		dma_addr = pme_map(tx_comp);
+		if (pme_map_error(dma_addr)) {
+			PMEPRERR("Error pme_map_error\n");
+			ret = -EIO;
+			goto comp_frame_unmap_output;
+		}
+		set_fd_addr(&tx_fd, dma_addr);
+	} else {
+		tx_fd.format = qm_fd_sg_big;
+		tx_fd.length29 = kernel_op.input.size;
+		/* Build sg frame */
+		dma_addr = pme_map(&tx_comp[INPUT_FRM]);
+		if (pme_map_error(dma_addr)) {
+			PMEPRERR("Error pme_map_error\n");
+			ret = -EIO;
+			goto unmap_input_frame;
+		}
+		set_fd_addr(&tx_fd, dma_addr);
+	}
+	PMEPRINFO("About to call pme_ctx_pmtcc\n");
+	ret = pme_ctx_pmtcc(&db->ctx, PME_CTX_OP_WAIT, &tx_fd,
+				(struct pme_ctx_token *)&token);
+	if (unlikely(ret)) {
+		PMEPRINFO("pme_ctx_pmtcc error %d\n", ret);
+		goto unmap_frame;
+	}
+	PMEPRINFO("Wait for completion\n");
+	/* Wait for the command to complete */
+	wait_for_completion(&token.cb_done);
+
+	PMEPRINFO("pme2_db: process_completed_token\n");
+	PMEPRINFO("pme2_db: received %d frame type\n", token.rx_fd.format);
+	if (token.rx_fd.format == qm_fd_compound) {
+		/* Need to copy  output */
+		src_sz = tx_comp[OUTPUT_FRM].length;
+		dst_sz = kernel_op.output.size;
+		PMEPRINFO("pme gen %u data, have space for %u\n",
+				src_sz, dst_sz);
+		kernel_op.output.size = min(dst_sz, src_sz);
+		/* Doesn't make sense we generated more than available space
+		 * should have got truncation.
+		 */
+		BUG_ON(dst_sz < src_sz);
+		if (copy_to_user((void __user *)kernel_op.output.data, rx_data,
+				kernel_op.output.size)) {
+			PMEPRERR("Error copying to user data \n");
+			ret = -EFAULT;
+			goto comp_frame_unmap_cf;
+		}
+	} else if (token.rx_fd.format == qm_fd_sg_big)
+		kernel_op.output.size = 0;
+	else
+		panic("unexpected frame type received %d\n",
+				token.rx_fd.format);
+
+	kernel_op.flags = pme_fd_res_flags(&token.rx_fd);
+	kernel_op.status = pme_fd_res_status(&token.rx_fd);
+	PMEPRINFO("process_completed_token, cpy to user\n");
+	/* Update the used values */
+	if (unlikely(copy_to_user((struct pme_db __user *)arg, &kernel_op,
+				sizeof(struct pme_db))))
+		ret = -EFAULT;
+
+unmap_frame:
+	if (token.rx_fd.format == qm_fd_sg_big)
+		goto single_frame_unmap_frame;
+
+comp_frame_unmap_cf:
+comp_frame_unmap_output:
+comp_frame_free_rx:
+	kfree(rx_data);
+	goto unmap_input_frame;
+single_frame_unmap_frame:
+unmap_input_frame:
+free_tx_data:
+	kfree(tx_data);
+
+	return ret;
+}
+
+static int execute_nop(struct file *fp, struct db_session *db)
+{
+	int ret = 0;
+	ret = pme_ctx_ctrl_nop(&db->ctx, PME_CTX_OP_WAIT|PME_CTX_OP_WAIT_INT);
+	/* pme_ctx_ctrl_nop() can be interrupted waiting for the response
+	 * of the NOP. In this scenario, 0 is returned. The only way to
+	 * determine that is was interrupted is to check for signal_pending()
+	 */
+	if (!ret && signal_pending(current))
+		ret = -ERESTARTSYS;
+	return ret;
+}
+
+static atomic_t sre_reset_lock = ATOMIC_INIT(1);
+static int ioctl_sre_reset(unsigned long arg)
+{
+	struct pme_db_sre_reset reset_vals;
+	int i;
+	int srrr_val;
+	int ret = 0;
+
+	if (copy_from_user(&reset_vals, (struct pme_db_sre_reset __user *)arg,
+			sizeof(struct pme_db_sre_reset)))
+		return -EFAULT;
+	PMEPRINFO("sre_reset: \n");
+	PMEPRINFO("  rule_index = 0x%x: \n", reset_vals.rule_index);
+	PMEPRINFO("  rule_increment = 0x%x: \n", reset_vals.rule_increment);
+	PMEPRINFO("  rule_repetitions = 0x%x: \n", reset_vals.rule_repetitions);
+	PMEPRINFO("  rule_reset_interval = 0x%x: \n",
+			reset_vals.rule_reset_interval);
+	PMEPRINFO("  rule_reset_priority = 0x%x: \n",
+			reset_vals.rule_reset_priority);
+
+	/* Validate ranges */
+	if ((reset_vals.rule_index >= PME_PMFA_SRE_INDEX_MAX) ||
+			(reset_vals.rule_increment > PME_PMFA_SRE_INC_MAX) ||
+			(reset_vals.rule_repetitions >= PME_PMFA_SRE_REP_MAX) ||
+			(reset_vals.rule_reset_interval >=
+				PME_PMFA_SRE_INTERVAL_MAX))
+		return -ERANGE;
+	/* Check and make sure only one caller is present */
+	if (!atomic_dec_and_test(&sre_reset_lock)) {
+		/* Someone else is already in this call */
+		atomic_inc(&sre_reset_lock);
+		return -EBUSY;
+	};
+	/* All validated.  Run the command */
+	for (i = 0; i < PME_SRE_RULE_VECTOR_SIZE; i++)
+		pme_attr_set(pme_attr_srrv0 + i, reset_vals.rule_vector[i]);
+	pme_attr_set(pme_attr_srrfi, reset_vals.rule_index);
+	pme_attr_set(pme_attr_srri, reset_vals.rule_increment);
+	pme_attr_set(pme_attr_srrwc,
+			(0xFFF & reset_vals.rule_reset_interval) << 1 |
+			(reset_vals.rule_reset_priority ? 1 : 0));
+	/* Need to set SRRR last */
+	pme_attr_set(pme_attr_srrr, reset_vals.rule_repetitions);
+	do {
+		mdelay(PME_PMFA_SRE_POLL_MS);
+		srrr_val = pme_attr_get(pme_attr_srrr);
+		/* Check for error */
+		if (srrr_val & 0x10000000) {
+			PMEPRERR("pme2: Error in SRRR\n");
+			ret = -EIO;
+		}
+		PMEPRINFO("pme2: srrr count %d\n", srrr_val);
+	} while (srrr_val);
+	atomic_inc(&sre_reset_lock);
+	return ret;
+}
+
+/**
+ * fsl_pme2_db_open - open the driver
+ *
+ * Open the driver and prepare for requests.
+ *
+ * Every time an application opens the driver, we create a db_session object
+ * for that file handle.
+ */
+static int fsl_pme2_db_open(struct inode *node, struct file *fp)
+{
+	int ret;
+	struct db_session *db = NULL;
+
+	db = kzalloc(sizeof(struct db_session), GFP_KERNEL);
+	if (!db)
+		return -ENOMEM;
+	fp->private_data = db;
+	db->ctx.cb = db_cb;
+
+	ret = pme_ctx_init(&db->ctx,
+			PME_CTX_FLAG_EXCLUSIVE |
+			PME_CTX_FLAG_PMTCC |
+			PME_CTX_FLAG_DIRECT|
+			PME_CTX_FLAG_LOCAL,
+			0, 4, CONFIG_FSL_PME2_DB_QOSOUT_PRIORITY, 0, NULL);
+	if (ret) {
+		PMEPRERR("pme_ctx_init %d \n", ret);
+		goto free_data;
+	}
+
+	/* enable the context */
+	ret = pme_ctx_enable(&db->ctx);
+	if (ret) {
+		PMEPRERR("error enabling ctx %d\n", ret);
+		pme_ctx_finish(&db->ctx);
+		goto free_data;
+	}
+	PMEPRINFO("pme2_db: Finish pme_db open %d \n", smp_processor_id());
+	return 0;
+free_data:
+	kfree(fp->private_data);
+	fp->private_data = NULL;
+	return ret;
+}
+
+static int fsl_pme2_db_close(struct inode *node, struct file *fp)
+{
+	int ret = 0;
+	struct db_session *db = fp->private_data;
+
+	PMEPRINFO("Start pme_db close\n");
+	while (db->exclusive_counter) {
+		pme_ctx_exclusive_dec(&db->ctx);
+		db->exclusive_counter--;
+	}
+
+	/* Disable context. */
+	ret = pme_ctx_disable(&db->ctx, PME_CTX_OP_WAIT);
+	if (ret)
+		PMEPRCRIT("Error disabling ctx %d\n", ret);
+	pme_ctx_finish(&db->ctx);
+	kfree(db);
+	PMEPRINFO("Finish pme_db close\n");
+	return 0;
+}
+
+/* Main switch loop for ioctl operations */
+static int fsl_pme2_db_ioctl(struct inode *inode, struct file *fp,
+		unsigned int cmd, unsigned long arg)
+{
+	struct db_session *db = fp->private_data;
+	int ret = 0;
+
+	switch (cmd) {
+
+	case PMEIO_PMTCC:
+		return execute_cmd(fp, db, arg);
+	case PMEIO_EXL_INC:
+		return exclusive_inc(fp, db);
+	case PMEIO_EXL_DEC:
+		return exclusive_dec(fp, db);
+	case PMEIO_EXL_GET:
+		BUG_ON(!db);
+		BUG_ON(!(db->ctx.flags & PME_CTX_FLAG_EXCLUSIVE));
+		if (copy_to_user((void __user *)arg,
+				&db->exclusive_counter,
+				sizeof(db->exclusive_counter)))
+			ret = -EFAULT;
+		return ret;
+	case PMEIO_NOP:
+		return execute_nop(fp, db);
+	case PMEIO_SRE_RESET:
+		return ioctl_sre_reset(arg);
+	}
+
+	return -EINVAL;
+}
+
+static const struct file_operations fsl_pme2_db_fops = {
+	.owner =	THIS_MODULE,
+	.ioctl =	fsl_pme2_db_ioctl,
+	.open = 	fsl_pme2_db_open,
+	.release =	fsl_pme2_db_close,
+};
+
+static struct miscdevice fsl_pme2_db_dev = {
+	.minor = MISC_DYNAMIC_MINOR,
+	.name = PME_DEV_DB_NODE,
+	.fops = &fsl_pme2_db_fops
+};
+
+static int __init fsl_pme2_db_init(void)
+{
+	int err = 0;
+
+	pr_info("Freescale pme2 db driver\n");
+	err = misc_register(&fsl_pme2_db_dev);
+	if (err) {
+		PMEPRERR("cannot register device\n");
+		return err;
+	}
+	PMEPRINFO("device %s registered\n", fsl_pme2_db_dev.name);
+	return 0;
+}
+
+static void __exit fsl_pme2_db_exit(void)
+{
+	int err = misc_deregister(&fsl_pme2_db_dev);
+	if (err) {
+		PMEPRERR("Failed to deregister device %s, "
+			"code %d\n", fsl_pme2_db_dev.name, err);
+		return;
+	}
+	PMEPRINFO("device %s deregistered\n", fsl_pme2_db_dev.name);
+}
+
+module_init(fsl_pme2_db_init);
+module_exit(fsl_pme2_db_exit);
+
+MODULE_AUTHOR("Freescale Semiconductor - OTC");
+MODULE_LICENSE("Dual BSD/GPL");
+MODULE_DESCRIPTION("FSL PME2 db driver");
diff --git a/drivers/match/pme2_high.c b/drivers/match/pme2_high.c
new file mode 100644
index 0000000..74268d4
--- /dev/null
+++ b/drivers/match/pme2_high.c
@@ -0,0 +1,912 @@
+/* Copyright (c) 2008, 2009, Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "pme2_private.h"
+
+/* The pme_ctx state machine is described via the following list of
+ * internal PME_CTX_FLAG_*** bits and cross-referenced to the APIs (and
+ * functionality) they track.
+ *
+ * DEAD: set at any point, an error has been hit, doesn't "cause" disabling or
+ * any autonomous ref-decrement (been there, hit the gotchas, won't do it
+ * again).
+ *
+ * DISABLING: set by pme_ctx_disable() at any point that is not already
+ * disabling, disabled, or in ctrl, and the ref is decremented. DISABLING is
+ * unset by pme_ctx_enable().
+ *
+ * DISABLED: once pme_ctx_disable() has set DISABLING and refs==0, DISABLED is
+ * set before returning. (Any failure will clear DISABLING and increment the ref
+ * count.) DISABLING is unset by pme_ctx_enable().
+ *
+ * ENABLING: set by pme_ctx_enable() provided the context is disabled, not dead,
+ * not in RECONFIG, and not already enabling. Once set, the ref is incremented
+ * and the tx FQ is scheduled (for non-exclusive flows). If this fails, the ref
+ * is decremented and the context is re-disabled. ENABLING is unset once
+ * pme_ctx_enable() completes.
+ *
+ * CTRL: set by pme_ctx_ctrl_***() at any point that is not dead, disabling,
+ * disabled, or already in ctrl (the ctrl state is a one-shot usage that locks
+ * in the caller against other attempts to issue ctrl). The function will then
+ * atomic_dec_and_test() the ref, and wait (if necessary) for refs==0.  If
+ * waiting fails, CTRL is unset and the ref is incremented. The eventual
+ * completion event will clear CTRL and increment the reference count.
+ *
+ * CTRL_ISSUED: set by pme_ctx_ctrl_***() once CTRL is set and refs==0, just
+ * before issuing the ctrl command. The eventual completion event uses
+ * CTRL_ISSUED to know it is handling a ctrl command response, and will clear
+ * CTRL and CTRL_ISSUED. If the enqueue fails, CTRL and CTRL_ISSUED are cleared.
+ * NB, the completion handling can't use CTRL to determine the type of result,
+ * because CTRL is set before the pipeline is cleared of outstanding non-ctrl
+ * work - this is why the CTRL_ISSUED flag exists, it's only set once the
+ * pipeline is empty so completion handling can use it as a code point.
+ *
+ * (Side-note, the pme_ctx_ctrl_***() APIs use a UID in pme_ctx that gets bumped
+ * by every ctrl completion, and the caller can wait for this UID to change to
+ * indicate that its command has completed. Although the completion also clears
+ * CTRL, another caller may be waiting to set CTRL and may wake_up() before the
+ * existing caller can detect it was cleared, so the UID mechanism bypasses this
+ * issue.)
+ *
+ * FCW_DEALLOC: set by pme_ctx_ctrl_update_flow() before the ctrl command is
+ * issued if the completion callback should deallocate residue. (This means the
+ * API can return asynchronously if needed, it's not required to wait.)
+ *
+ * RECONFIG: set by pme_ctx_reconfigure_[rt]x() provided the context is
+ * disabled, not dead, and not already in reconfig. RECONFIG is cleared prior to
+ * the function returning.
+ *
+ * Simplifications: the do_flag() wrapper provides synchronised modifications of
+ * the ctx 'flags', and callers can rely on the following implications to reduce
+ * the number of flags in the masks being passed in;
+ * 	DISABLED implies DISABLING (and enable will clear both)
+ * 	CTRL_ISSUED implies CTRL (and completion will clear both)
+ */
+
+#define PME_CTX_FLAG_DEAD        0x80000000
+#define PME_CTX_FLAG_DISABLING   0x40000000
+#define PME_CTX_FLAG_DISABLED    0x20000000
+#define PME_CTX_FLAG_ENABLING    0x10000000
+#define PME_CTX_FLAG_CTRL        0x08000000
+#define PME_CTX_FLAG_CTRL_ISSUED 0x04000000
+#define PME_CTX_FLAG_FCW_DEALLOC 0x02000000
+#define PME_CTX_FLAG_RECONFIG    0x01000000
+
+#define PME_CTX_FLAG_PRIVATE     0xff000000
+
+/* This wrapper simplifies conditional (and locked) read-modify-writes to
+ * 'flags'. Inlining should allow the compiler to optimise it based on the
+ * parameters, eg. if 'must_be_set'/'must_not_be_set' are zero it will
+ * degenerate to an unconditional read-modify-write, if 'to_set'/'to_unset' are
+ * zero it will degenerate to a read-only flag-check, etc. */
+static inline int do_flags(struct pme_ctx *ctx,
+			u32 must_be_set, u32 must_not_be_set,
+			u32 to_set, u32 to_unset)
+{
+	int err = -EBUSY;
+	spin_lock_irq(&ctx->lock);
+	if (((ctx->flags & must_be_set) == must_be_set) &&
+			!(ctx->flags & must_not_be_set)) {
+		ctx->flags |= to_set;
+		ctx->flags &= ~to_unset;
+		err = 0;
+	}
+	spin_unlock_irq(&ctx->lock);
+	return err;
+}
+
+static enum qman_cb_dqrr_result cb_dqrr(struct qman_portal *, struct qman_fq *,
+				const struct qm_dqrr_entry *);
+static void cb_ern(struct qman_portal *, struct qman_fq *,
+				const struct qm_mr_entry *);
+static void cb_dc_ern(struct qman_portal *, struct qman_fq *,
+				const struct qm_mr_entry *);
+static void cb_fqs(struct qman_portal *, struct qman_fq *,
+				const struct qm_mr_entry *);
+static const struct qman_fq_cb pme_fq_base_in = {
+	.fqs = cb_fqs
+};
+static const struct qman_fq_cb pme_fq_base_out = {
+	.dqrr = cb_dqrr,
+	.ern = cb_ern,
+	.dc_ern = cb_dc_ern,
+	.fqs = cb_fqs
+};
+
+/* Globals related to competition for PME_EFQC, ie. exclusivity */
+static DECLARE_WAIT_QUEUE_HEAD(exclusive_queue);
+static spinlock_t exclusive_lock = SPIN_LOCK_UNLOCKED;
+static unsigned int exclusive_refs;
+static struct pme_ctx *exclusive_ctx;
+
+/* TODO: this is hitting the rx FQ with a large blunt instrument, ie. park()
+ * does a retire, query, oos, and (re)init. It's possible to force-eligible the
+ * rx FQ instead, then use a DCA_PK within the cb_dqrr() callback to park it.
+ * Implement this optimisation later if it's an issue (and incur the additional
+ * complexity in the state-machine). */
+static int park(struct qman_fq *fq, struct qm_mcc_initfq *initfq)
+{
+	int ret;
+	u32 flags;
+
+	ret = qman_retire_fq(fq, &flags);
+	if (ret)
+		return ret;
+	BUG_ON(flags & QMAN_FQ_STATE_BLOCKOOS);
+	/* We can't revert from now on */
+	ret = qman_query_fq(fq, &initfq->fqd);
+	BUG_ON(ret);
+	ret = qman_oos_fq(fq);
+	BUG_ON(ret);
+	initfq->we_mask = QM_INITFQ_WE_DESTWQ | QM_INITFQ_WE_CONTEXTA |
+			QM_INITFQ_WE_CONTEXTA;
+	ret = qman_init_fq(fq, 0, initfq);
+	BUG_ON(ret);
+	return 0;
+}
+
+static inline int reconfigure_rx(struct pme_ctx *ctx, int to_park, u8 qosout,
+				enum qm_channel dest,
+				const struct qm_fqd_stashing *stashing)
+{
+	struct qm_mcc_initfq initfq;
+	u32 flags = QMAN_INITFQ_FLAG_SCHED;
+	int ret;
+
+	ret = do_flags(ctx, PME_CTX_FLAG_DISABLED,
+			PME_CTX_FLAG_DEAD | PME_CTX_FLAG_RECONFIG,
+			PME_CTX_FLAG_RECONFIG, 0);
+	if (ret)
+		return ret;
+	if (to_park) {
+		ret = park(&ctx->fq, &initfq);
+		if (ret)
+			goto done;
+	}
+	initfq.we_mask = QM_INITFQ_WE_DESTWQ | QM_INITFQ_WE_FQCTRL;
+	initfq.fqd.dest.wq = qosout;
+	if (stashing) {
+		initfq.we_mask |= QM_INITFQ_WE_CONTEXTA;
+		initfq.fqd.context_a.stashing = *stashing;
+		initfq.fqd.fq_ctrl = QM_FQCTRL_CTXASTASHING;
+	} else
+		initfq.fqd.fq_ctrl = 0; /* disable stashing */
+	if (ctx->flags & PME_CTX_FLAG_LOCAL)
+		flags |= QMAN_INITFQ_FLAG_LOCAL;
+	else {
+		initfq.fqd.dest.channel = dest;
+		/* Set hold-active *IFF* it's a pool channel */
+		if (dest >= qm_channel_pool1)
+			initfq.fqd.fq_ctrl |= QM_FQCTRL_HOLDACTIVE;
+	}
+	ret = qman_init_fq(&ctx->fq, flags, &initfq);
+done:
+	do_flags(ctx, 0, 0, 0, PME_CTX_FLAG_RECONFIG);
+	return ret;
+}
+
+/* this code is factored out of pme_ctx_disable() and get_ctrl() */
+static int empty_pipeline(struct pme_ctx *ctx, u32 flags)
+{
+	int ret;
+	if (flags & PME_CTX_OP_WAIT) {
+		if (flags & PME_CTX_OP_WAIT_INT) {
+			ret = -EINTR;
+			wait_event_interruptible(ctx->queue,
+				!(ret = atomic_read(&ctx->refs)));
+		} else
+			wait_event(ctx->queue,
+				!(ret = atomic_read(&ctx->refs)));
+	} else
+		ret = atomic_read(&ctx->refs);
+	if (ret)
+		/* convert a +ve ref-count to a -ve error code */
+		ret = -EBUSY;
+	return ret;
+}
+
+int pme_ctx_init(struct pme_ctx *ctx, u32 flags, u32 bpid, u8 qosin,
+			u8 qosout, enum qm_channel dest,
+			const struct qm_fqd_stashing *stashing)
+{
+	u32 fqid_rx, fqid_tx;
+	int rxinit = 0, ret = -ENOMEM, fqin_inited = 0;
+
+	ctx->fq.cb = pme_fq_base_out;
+	atomic_set(&ctx->refs, 0);
+	ctx->flags = (flags & ~PME_CTX_FLAG_PRIVATE) | PME_CTX_FLAG_DISABLED |
+			PME_CTX_FLAG_DISABLING;
+	spin_lock_init(&ctx->lock);
+	init_waitqueue_head(&ctx->queue);
+	INIT_LIST_HEAD(&ctx->tokens);
+	ctx->seq_num = 0;
+	ctx->uid = 0xdeadbeef;
+	ctx->fqin = NULL;
+	ctx->hw_flow = NULL;
+	ctx->hw_residue = NULL;
+
+	fqid_rx = qm_fq_new();
+	fqid_tx = qm_fq_new();
+	ctx->fqin = slabfq_alloc();
+	if (!fqid_rx || !fqid_tx || !ctx->fqin)
+		goto err;
+	ctx->fqin->cb = pme_fq_base_in;
+	if (qman_create_fq(fqid_rx, QMAN_FQ_FLAG_TO_DCPORTAL |
+			((flags & PME_CTX_FLAG_LOCKED) ?
+				QMAN_FQ_FLAG_LOCKED : 0), ctx->fqin))
+		goto err;
+	fqin_inited = 1;
+	if (qman_create_fq(fqid_tx, QMAN_FQ_FLAG_NO_ENQUEUE |
+			((flags & PME_CTX_FLAG_LOCKED) ?
+				QMAN_FQ_FLAG_LOCKED : 0), &ctx->fq))
+		goto err;
+	rxinit = 1;
+	/* Input FQ */
+	if (!(flags & PME_CTX_FLAG_DIRECT)) {
+		ctx->hw_flow = pme_hw_flow_new();
+		if (!ctx->hw_flow)
+			goto err;
+	}
+	ret = pme_ctx_reconfigure_tx(ctx, bpid, qosin);
+	if (ret)
+		goto err;
+	/* Output FQ */
+	ret = reconfigure_rx(ctx, 0, qosout, dest, stashing);
+	if (ret) {
+		/* Need to OOS the FQ before it gets free'd */
+		ret = qman_oos_fq(ctx->fqin);
+		BUG_ON(ret);
+		goto err;
+	}
+	return 0;
+err:
+	if (fqid_rx)
+		qm_fq_free(fqid_rx);
+	if (fqid_tx)
+		qm_fq_free(fqid_tx);
+	if (ctx->hw_flow)
+		pme_hw_flow_free(ctx->hw_flow);
+	if (ctx->fqin) {
+		if (fqin_inited)
+			qman_destroy_fq(ctx->fqin, 0);
+		slabfq_free(ctx->fqin);
+	}
+	if (rxinit)
+		qman_destroy_fq(&ctx->fq, 0);
+	return ret;
+}
+EXPORT_SYMBOL(pme_ctx_init);
+
+/* NB, we don't lock here because there must be no other callers (even if we
+ * locked, what does the loser do after we win?) */
+void pme_ctx_finish(struct pme_ctx *ctx)
+{
+	u32 flags, fqid_rx, fqid_tx;
+	int ret;
+
+	ret = do_flags(ctx, PME_CTX_FLAG_DISABLED, PME_CTX_FLAG_RECONFIG, 0, 0);
+	BUG_ON(ret);
+	/* Rx/Tx are empty (coz ctx is disabled) so retirement should be
+	 * immediate */
+	ret = qman_retire_fq(ctx->fqin, &flags);
+	BUG_ON(ret);
+	BUG_ON(flags & QMAN_FQ_STATE_BLOCKOOS);
+	ret = qman_retire_fq(&ctx->fq, &flags);
+	BUG_ON(ret);
+	BUG_ON(flags & QMAN_FQ_STATE_BLOCKOOS);
+	/* OOS and free (don't kfree fq, it's a static ctx member) */
+	ret = qman_oos_fq(ctx->fqin);
+	BUG_ON(ret);
+	ret = qman_oos_fq(&ctx->fq);
+	BUG_ON(ret);
+	fqid_rx = qman_fq_fqid(ctx->fqin);
+	fqid_tx = qman_fq_fqid(&ctx->fq);
+	qman_destroy_fq(ctx->fqin, 0);
+	qman_destroy_fq(&ctx->fq, 0);
+	qm_fq_free(fqid_rx);
+	qm_fq_free(fqid_tx);
+	slabfq_free(ctx->fqin); /* the fq was dynamically allocated */
+	if (ctx->hw_flow)
+		pme_hw_flow_free(ctx->hw_flow);
+	if (ctx->hw_residue)
+		pme_hw_residue_free(ctx->hw_residue);
+}
+EXPORT_SYMBOL(pme_ctx_finish);
+
+int pme_ctx_is_disabled(struct pme_ctx *ctx)
+{
+	return (ctx->flags & PME_CTX_FLAG_DISABLED);
+}
+EXPORT_SYMBOL(pme_ctx_is_disabled);
+
+int pme_ctx_disable(struct pme_ctx *ctx, u32 flags)
+{
+	struct qm_mcc_initfq initfq;
+	int ret;
+
+	ret = do_flags(ctx, 0, PME_CTX_FLAG_DISABLING | PME_CTX_FLAG_CTRL,
+			PME_CTX_FLAG_DISABLING, 0);
+	if (ret)
+		return ret;
+	/* Make sure the pipeline is empty */
+	atomic_dec(&ctx->refs);
+	ret = empty_pipeline(ctx, flags);
+	if (!ret && !(ctx->flags & PME_CTX_FLAG_EXCLUSIVE))
+		/* Park fqin (exclusive is always parked) */
+		ret = park(ctx->fqin, &initfq);
+	if (ret) {
+		atomic_inc(&ctx->refs);
+		do_flags(ctx, 0, 0, 0, PME_CTX_FLAG_DISABLING);
+		wake_up(&ctx->queue);
+		return ret;
+	}
+	/* Our ORP got reset too, so reset the sequence number */
+	ctx->seq_num = 0;
+	do_flags(ctx, 0, 0, PME_CTX_FLAG_DISABLED, 0);
+	return 0;
+}
+EXPORT_SYMBOL(pme_ctx_disable);
+
+int pme_ctx_enable(struct pme_ctx *ctx)
+{
+	int ret;
+	ret = do_flags(ctx, PME_CTX_FLAG_DISABLED,
+			PME_CTX_FLAG_DEAD | PME_CTX_FLAG_RECONFIG |
+			PME_CTX_FLAG_ENABLING,
+			PME_CTX_FLAG_ENABLING, 0);
+	if (ret)
+		return ret;
+	if (!(ctx->flags & PME_CTX_FLAG_EXCLUSIVE)) {
+		ret = qman_init_fq(ctx->fqin, QMAN_INITFQ_FLAG_SCHED, NULL);
+		if (ret) {
+			do_flags(ctx, 0, 0, 0, PME_CTX_FLAG_ENABLING);
+			return ret;
+		}
+	}
+	atomic_inc(&ctx->refs);
+	do_flags(ctx, 0, 0, 0, PME_CTX_FLAG_DISABLED | PME_CTX_FLAG_DISABLING |
+				PME_CTX_FLAG_ENABLING);
+	return 0;
+}
+EXPORT_SYMBOL(pme_ctx_enable);
+
+int pme_ctx_reconfigure_tx(struct pme_ctx *ctx, u32 bpid, u8 qosin)
+{
+	struct qm_mcc_initfq initfq;
+	int ret;
+
+	ret = do_flags(ctx, PME_CTX_FLAG_DISABLED,
+			PME_CTX_FLAG_DEAD | PME_CTX_FLAG_RECONFIG,
+			PME_CTX_FLAG_RECONFIG, 0);
+	if (ret)
+		return ret;
+	memset(&initfq,0,sizeof(initfq));
+	pme_initfq(&initfq, ctx->hw_flow, qosin, bpid, qman_fq_fqid(&ctx->fq));
+	if (!(ctx->flags & PME_CTX_FLAG_NO_ORP)) {
+		initfq.we_mask |= QM_INITFQ_WE_FQCTRL | QM_INITFQ_WE_ORPC;
+		/* ORPRWS==1 means the ORP window is max 64 frames. Given that
+		 * the out-of-order problem is (usually?) limited to spraying
+		 * over different EQCRs from different cores, it shouldn't be
+		 * possible to get more than this far behind (8 full EQCRs is 56
+		 * frames). */
+		initfq.fqd.orprws = 1;
+		initfq.fqd.oa = 0;
+		initfq.fqd.olws = 0;
+		initfq.fqd.fq_ctrl |= QM_FQCTRL_ORP;
+	}
+	ret = qman_init_fq(ctx->fqin, 0, &initfq);
+	do_flags(ctx, 0, 0, 0, PME_CTX_FLAG_RECONFIG);
+	return ret;
+}
+EXPORT_SYMBOL(pme_ctx_reconfigure_tx);
+
+int pme_ctx_reconfigure_rx(struct pme_ctx *ctx, u8 qosout,
+		enum qm_channel dest, const struct qm_fqd_stashing *stashing)
+{
+	return reconfigure_rx(ctx, 1, qosout, dest, stashing);
+}
+EXPORT_SYMBOL(pme_ctx_reconfigure_rx);
+
+/* Helpers for 'ctrl' and 'work' APIs. These are used when the 'ctx' in question
+ * is EXCLUSIVE. */
+static inline void release_exclusive(struct pme_ctx *ctx)
+{
+	BUG_ON(exclusive_ctx != ctx);
+	BUG_ON(!exclusive_refs);
+	spin_lock_irq(&exclusive_lock);
+	if (!(--exclusive_refs)) {
+		exclusive_ctx = NULL;
+		pme2_exclusive_unset();
+		wake_up(&exclusive_queue);
+	}
+	spin_unlock_irq(&exclusive_lock);
+}
+static int __try_exclusive(struct pme_ctx *ctx)
+{
+	int ret = 0;
+	spin_lock_irq(&exclusive_lock);
+	if (exclusive_refs) {
+		/* exclusivity already held, continue if we're the owner */
+		if (exclusive_ctx != ctx)
+			ret = -EBUSY;
+	} else {
+		/* it's not currently held */
+		ret = pme2_exclusive_set(ctx->fqin);
+		if (!ret)
+			exclusive_ctx = ctx;
+	}
+	if (!ret)
+		exclusive_refs++;
+	spin_unlock_irq(&exclusive_lock);
+	return ret;
+}
+/* Use this macro as the wait expression because we don't want to continue
+ * looping if the reason we're failing is that we don't have CCSR access
+ * (-ENODEV). */
+#define try_exclusive(ret, ctx) \
+	(!(ret = __try_exclusive(ctx)) || (ret == -ENODEV))
+static inline int get_exclusive(struct pme_ctx *ctx, u32 flags)
+{
+	int ret;
+	if (flags & PME_CTX_OP_WAIT) {
+		if (flags & PME_CTX_OP_WAIT_INT) {
+			ret = -EINTR;
+			wait_event_interruptible(exclusive_queue,
+					try_exclusive(ret, ctx));
+		} else
+			wait_event(exclusive_queue,
+					try_exclusive(ret, ctx));
+	} else
+		ret = __try_exclusive(ctx);
+	return ret;
+}
+/* Used for 'ctrl' and 'work' APIs, convert PME->QMAN wait flags. The PME and
+ * QMAN "wait" flags have been aligned so that the below conversion should
+ * compile with good straight-line speed. */
+static inline u32 ctrl2eq(u32 flags)
+{
+	return flags & (QMAN_ENQUEUE_FLAG_WAIT | QMAN_ENQUEUE_FLAG_WAIT_INT);
+}
+
+static inline void release_ctrl(struct pme_ctx *ctx)
+{
+	atomic_inc(&ctx->refs);
+	do_flags(ctx, 0, 0, 0, PME_CTX_FLAG_CTRL | PME_CTX_FLAG_CTRL_ISSUED |
+					PME_CTX_FLAG_FCW_DEALLOC);
+	wake_up(&ctx->queue);
+}
+static int __try_ctrl(struct pme_ctx *ctx)
+{
+	return do_flags(ctx, 0,
+		PME_CTX_FLAG_DEAD | PME_CTX_FLAG_DISABLING | PME_CTX_FLAG_CTRL,
+		PME_CTX_FLAG_CTRL, 0);
+}
+/* Use this macro as the wait expression because we don't want to continue
+ * looping if the DEAD/DISABLING flags are set, we only loop if CTRL is held
+ * because we wait for it to be cleared.
+ * IMPLEMENTATION NOTE: don't use a return code from wait_event_interruptible(),
+ * the key is the return value of the last call to __try_ctrl(). */
+#define try_ctrl(ret, ctx) \
+	(!(ret = __try_ctrl(ctx)) || (ctx->flags & (PME_CTX_FLAG_DEAD | \
+					PME_CTX_FLAG_DISABLING)))
+static int get_ctrl(struct pme_ctx *ctx, u32 flags)
+{
+	int ret;
+	/* Lock the CTRL flag in the context */
+	if (flags & PME_CTX_OP_WAIT) {
+		if (flags & PME_CTX_OP_WAIT_INT) {
+			ret = -EINTR;
+			wait_event_interruptible(ctx->queue,
+					try_ctrl(ret, ctx));
+		} else
+			wait_event(ctx->queue, try_ctrl(ret, ctx));
+	} else
+		ret = __try_ctrl(ctx);
+	if (ret)
+		return ret;
+	/* Make sure the pipeline is empty */
+	atomic_dec(&ctx->refs);
+	ret = empty_pipeline(ctx, flags);
+	if (ret)
+		release_ctrl(ctx);
+	return ret;
+}
+/* unlike do_work() (which encapsulates get_work(), get_exclusive(), and
+ * qman_enqueue()), the do_ctrl() wrapper only encapsulates get_exclusive() and
+ * qman_enqueue(). This is because the ctrl functions need to manipulate 'ctx'
+ * between get_ctrl() and qman_enqueue(), so they peel the outer layer of
+ * the onion (get_ctrl) themselves. */
+static int do_ctrl(struct pme_ctx *ctx, u32 flags, const struct qm_fd *fd,
+			u32 *uid)
+{
+	int ret;
+	if (ctx->flags & PME_CTX_FLAG_EXCLUSIVE) {
+		ret = get_exclusive(ctx, flags);
+		if (ret)
+			return ret;
+	}
+	*uid = ctx->uid;
+	do_flags(ctx, 0, 0, PME_CTX_FLAG_CTRL_ISSUED, 0);
+	ret = qman_enqueue(ctx->fqin, fd, ctrl2eq(flags));
+	if (ret) {
+		if (ctx->flags & PME_CTX_FLAG_EXCLUSIVE)
+			release_exclusive(ctx);
+	}
+	return ret;
+}
+/* Swallow any wait failure, the ctrl command has already been sent and will go
+ * to the PME, so returning a wait error would leave the caller thinking the
+ * command didn't happen. If they care about whether they got interrupted
+ * before completion, they should check signal_pending() on return. */
+static void wait_ctrl_completion(struct pme_ctx *ctx, u32 flags, u32 uid)
+{
+	if (flags & PME_CTX_OP_WAIT) {
+		if (flags & PME_CTX_OP_WAIT_INT)
+			wait_event_interruptible(ctx->queue,
+					ctx->uid != uid);
+		else
+			wait_event(ctx->queue, ctx->uid != uid);
+	}
+}
+
+int pme_ctx_ctrl_update_flow(struct pme_ctx *ctx, u32 flags,
+			struct pme_flow *params)
+{
+	struct qm_fd fd;
+	u32 uid;
+	int ret, allocres = 0;
+
+	BUG_ON(ctx->flags & (PME_CTX_FLAG_DIRECT | PME_CTX_FLAG_PMTCC));
+	ret = get_ctrl(ctx, flags);
+	if (ret)
+		return ret;
+	if (flags & PME_CTX_OP_RESETRESLEN) {
+		if (ctx->hw_residue) {
+			params->ren = 1;
+			flags |= PME_CMD_FCW_RES;
+		} else
+			flags &= ~PME_CMD_FCW_RES;
+	}
+	/* allocate residue memory if it is being added */
+	if ((flags & PME_CMD_FCW_RES) && params->ren && !ctx->hw_residue) {
+		ctx->hw_residue = pme_hw_residue_new();
+		if (!ctx->hw_residue) {
+			release_ctrl(ctx);
+			return -ENOMEM;
+		}
+		allocres = 1;
+	}
+	/* enqueue the FCW command to PME */
+	memset(&fd, 0, sizeof(fd));
+	if (unlikely((flags & PME_CMD_FCW_RES) && !params->ren &&
+						ctx->hw_residue)) {
+		/* cb_dqrr() needs to deallocate residue on completion */
+		do_flags(ctx, 0, 0, PME_CTX_FLAG_FCW_DEALLOC, 0);
+		pme_fd_cmd_fcw(&fd, flags & PME_CMD_FCW_ALL, params, NULL);
+	} else
+		pme_fd_cmd_fcw(&fd, flags & PME_CMD_FCW_ALL, params,
+					ctx->hw_residue);
+	ret = do_ctrl(ctx, flags, &fd, &uid);
+	if (ret) {
+		if (allocres) {
+			pme_hw_residue_free(ctx->hw_residue);
+			ctx->hw_residue = NULL;
+		}
+		release_ctrl(ctx);
+		return ret;
+	}
+	wait_ctrl_completion(ctx, flags, uid);
+	return 0;
+}
+EXPORT_SYMBOL(pme_ctx_ctrl_update_flow);
+
+int pme_ctx_ctrl_read_flow(struct pme_ctx *ctx, u32 flags,
+			struct pme_flow *params)
+{
+	struct qm_fd fd;
+	u32 uid;
+	int ret;
+
+	BUG_ON(ctx->flags & (PME_CTX_FLAG_DIRECT | PME_CTX_FLAG_PMTCC));
+	/* This has to block, we can't accept a read flow context command being
+	 * orphaned in-flight */
+	might_sleep();
+	flags |= PME_CTX_OP_WAIT;
+	flags &= ~PME_CTX_OP_WAIT_INT;
+	ret = get_ctrl(ctx, flags);
+	if (ret)
+		return ret;
+	/* enqueue the FCR command to PME */
+	memset(&fd, 0, sizeof(fd));
+	pme_fd_cmd_fcr(&fd, params);
+	ret = do_ctrl(ctx, flags, &fd, &uid);
+	if (ret) {
+		release_ctrl(ctx);
+		return ret;
+	}
+	wait_ctrl_completion(ctx, flags, uid);
+	return 0;
+}
+EXPORT_SYMBOL(pme_ctx_ctrl_read_flow);
+
+int pme_ctx_ctrl_nop(struct pme_ctx *ctx, u32 flags)
+{
+	struct qm_fd fd;
+	u32 uid;
+	int ret;
+
+	ret = get_ctrl(ctx, flags);
+	if (ret)
+		return ret;
+	/* enqueue the NOP command to PME */
+	memset(&fd, 0, sizeof(fd));
+	pme_fd_cmd_nop(&fd);
+	ret = do_ctrl(ctx, flags, &fd, &uid);
+	if (ret) {
+		release_ctrl(ctx);
+		return ret;
+	}
+	wait_ctrl_completion(ctx, flags, uid);
+	return 0;
+}
+EXPORT_SYMBOL(pme_ctx_ctrl_nop);
+
+int pme_ctx_in_ctrl(struct pme_ctx *ctx)
+{
+	return ctx->flags & PME_CTX_FLAG_CTRL;
+}
+EXPORT_SYMBOL(pme_ctx_in_ctrl);
+
+static inline void release_work(struct pme_ctx *ctx)
+{
+	if (atomic_dec_and_test(&ctx->refs))
+		wake_up(&ctx->queue);
+}
+
+static int __try_work(struct pme_ctx *ctx)
+{
+	atomic_inc(&ctx->refs);
+	if (unlikely(ctx->flags & (PME_CTX_FLAG_DEAD | PME_CTX_FLAG_DISABLING |
+						PME_CTX_FLAG_CTRL))) {
+		release_work(ctx);
+		if (ctx->flags & (PME_CTX_FLAG_DEAD | PME_CTX_FLAG_DISABLING))
+			return -EIO;
+		return -EAGAIN;
+	}
+	return 0;
+}
+/* Exact same comment as for try_ctrl() */
+#define try_work(ret, ctx) \
+	(!(ret = __try_work(ctx)) || (ctx->flags & (PME_CTX_FLAG_DEAD | \
+					PME_CTX_FLAG_DISABLING)))
+static int get_work(struct pme_ctx *ctx, u32 flags)
+{
+	int ret;
+	if (flags & PME_CTX_OP_WAIT) {
+		if (flags & PME_CTX_OP_WAIT_INT) {
+			ret = -EINTR;
+			wait_event_interruptible(ctx->queue,
+					try_work(ret, ctx));
+		} else
+			wait_event(ctx->queue, try_work(ret, ctx));
+	} else
+		ret = __try_work(ctx);
+	return ret;
+}
+
+static int do_work(struct pme_ctx *ctx, u32 flags, struct qm_fd *fd,
+		struct pme_ctx_token *token)
+{
+	u32 seq_num;
+	int ret = get_work(ctx, flags);
+	if (ret)
+		return ret;
+	if (ctx->flags & PME_CTX_FLAG_EXCLUSIVE) {
+		ret = get_exclusive(ctx, flags);
+		if (ret) {
+			release_work(ctx);
+			return ret;
+		}
+	}
+	BUG_ON(sizeof(*fd) != sizeof(token->blob));
+	memcpy(&token->blob, fd, sizeof(*fd));
+	spin_lock_irq(&ctx->lock);
+	seq_num = ctx->seq_num++;
+	ctx->seq_num &= QM_EQCR_SEQNUM_SEQMASK; /* rollover at 2^14 */
+	list_add_tail(&token->node, &ctx->tokens);
+	spin_unlock_irq(&ctx->lock);
+	if (ctx->flags & PME_CTX_FLAG_NO_ORP)
+		ret = qman_enqueue(ctx->fqin, fd, ctrl2eq(flags));
+	else
+		ret = qman_enqueue_orp(ctx->fqin, fd, ctrl2eq(flags),
+					ctx->fqin, seq_num);
+	if (ret) {
+		if (ctx->flags & PME_CTX_FLAG_EXCLUSIVE)
+			release_exclusive(ctx);
+		release_work(ctx);
+	}
+	return ret;
+}
+
+int pme_ctx_scan(struct pme_ctx *ctx, u32 flags, struct qm_fd *fd, u32 args,
+		struct pme_ctx_token *token)
+{
+	BUG_ON(ctx->flags & PME_CTX_FLAG_PMTCC);
+	pme_fd_cmd_scan(fd, args);
+	return do_work(ctx, flags, fd, token);
+}
+EXPORT_SYMBOL(pme_ctx_scan);
+
+int pme_ctx_pmtcc(struct pme_ctx *ctx, u32 flags, struct qm_fd *fd,
+		struct pme_ctx_token *token)
+{
+	BUG_ON(!(ctx->flags & PME_CTX_FLAG_PMTCC));
+	pme_fd_cmd_pmtcc(fd);
+	return do_work(ctx, flags, fd, token);
+}
+EXPORT_SYMBOL(pme_ctx_pmtcc);
+
+int pme_ctx_exclusive_inc(struct pme_ctx *ctx, u32 flags)
+{
+	return get_exclusive(ctx, flags);
+}
+EXPORT_SYMBOL(pme_ctx_exclusive_inc);
+
+void pme_ctx_exclusive_dec(struct pme_ctx *ctx)
+{
+	release_exclusive(ctx);
+}
+EXPORT_SYMBOL(pme_ctx_exclusive_dec);
+
+/* The 99.99% case is that enqueues happen in order or they get order-restored
+ * by the ORP, and so dequeues of responses happen in order too, so our FIFO
+ * linked-list of tokens is append-on-enqueue and pop-on-dequeue, and all's
+ * well.
+ *
+ * *EXCEPT*, if ever an enqueue gets rejected ... what then happens is that we
+ * have dequeues and ERNs to deal with, and the order we see them in is not
+ * necessarily the linked-list order. So we need to handle this in DQRR and MR
+ * callbacks, without sacrificing fast-path performance. Ouch.
+ *
+ * We use pop_matching_token() to take care of the mess (inlined, of course). */
+#define MATCH(fd1,fd2) \
+	(((fd1)->addr_hi == (fd2)->addr_hi) && \
+	((fd1)->addr_lo == (fd2)->addr_lo) && \
+	((fd1)->opaque == (fd2)->opaque))
+static inline struct pme_ctx_token *pop_matching_token(struct pme_ctx *ctx,
+						const struct qm_fd *fd)
+{
+	struct pme_ctx_token *token;
+	const struct qm_fd *t_fd;
+
+	/* The fast-path case is that the for() loop actually degenerates into;
+	 *     token = list_first_entry();
+	 *     if (likely(MATCH()))
+	 *         [done]
+	 * The penalty of the slow-path case is the for() loop plus the fact
+	 * we're optimising for a "likely" match first time, which might hurt
+	 * when that assumption is wrong a few times in succession. */
+	spin_lock_irq(&ctx->lock);
+	list_for_each_entry(token, &ctx->tokens, node) {
+		t_fd = (const struct qm_fd *)&token->blob[0];
+		if (likely(MATCH(t_fd, fd))) {
+			list_del(&token->node);
+			goto found;
+		}
+	}
+	token = NULL;
+found:
+	spin_unlock_irq(&ctx->lock);
+	return token;
+}
+
+static inline void cb_helper(struct qman_portal *portal, struct pme_ctx *ctx,
+				const struct qm_fd *fd, int error)
+{
+	struct pme_ctx_token *token;
+	/* Resist the urge to use "unlikely" - 'error' is a constant param to an
+	 * inline fn, so the compiler can collapse this completely. */
+	if (error)
+		do_flags(ctx, 0, 0, PME_CTX_FLAG_DEAD, 0);
+	/* The 'ctrl' case should be our slow-path */
+	if (unlikely(ctx->flags & PME_CTX_FLAG_CTRL_ISSUED)) {
+		if (ctx->flags & PME_CTX_FLAG_FCW_DEALLOC) {
+			pme_hw_residue_free(ctx->hw_residue);
+			ctx->hw_residue = NULL;
+		}
+		/* The caller will be waiting on this... */
+		ctx->uid++;
+		/* Switch out of CTRL mode */
+		release_ctrl(ctx);
+		if (ctx->flags & PME_CTX_FLAG_EXCLUSIVE)
+			release_exclusive(ctx);
+		return;
+	}
+	/* This is a scan or PMTCC - the fast-path, normal, 99.99% case. We
+	 * detach the token for this command and pass it to the owner. */
+	token = pop_matching_token(ctx, fd);
+	ctx->cb(ctx, fd, token);
+	/* Consume the frame */
+	if (ctx->flags & PME_CTX_FLAG_EXCLUSIVE)
+		release_exclusive(ctx);
+	if (atomic_dec_and_test(&ctx->refs))
+		wake_up(&ctx->queue);
+}
+
+/* TODO: this scheme does not allow PME receivers to use held-active at all. Eg.
+ * there's no configuration of held-active for 'fq', and if there was, there's
+ * (a) nothing in the cb_dqrr() to support "park" or "defer" logic, and (b)
+ * nothing in cb_fqs() to support a delayed FQPN (DCAP_PK) notification. */
+static enum qman_cb_dqrr_result cb_dqrr(struct qman_portal *portal,
+			struct qman_fq *fq, const struct qm_dqrr_entry *dq)
+{
+	enum pme_status status = pme_fd_res_status(&dq->fd);
+	u8 flags = pme_fd_res_flags(&dq->fd);
+	struct pme_ctx *ctx = (struct pme_ctx *)fq;
+
+	if (unlikely((status != pme_status_ok) ||
+			(flags & PME_STATUS_UNRELIABLE)))
+		cb_helper(portal, ctx, &dq->fd, 1);
+	else
+		cb_helper(portal, ctx, &dq->fd, 0);
+	return qman_cb_dqrr_consume;
+}
+
+static void cb_ern(struct qman_portal *portal, struct qman_fq *fq,
+				const struct qm_mr_entry *mr)
+{
+	/* Give this the same handling as the error case through cb_dqrr(). */
+	struct pme_ctx *ctx = (struct pme_ctx *)fq;
+	cb_helper(portal, ctx, &mr->ern.fd, 1);
+}
+
+static void cb_dc_ern(struct qman_portal *portal, struct qman_fq *fq,
+				const struct qm_mr_entry *mr)
+{
+	struct pme_ctx *ctx = (struct pme_ctx *)fq;
+	/* This, umm, *shouldn't* happen. It's pretty bad. Things are expected
+	 * to fall apart here, but we'll continue long enough to get out of
+	 * interrupt context and let the user unwind whatever they can. */
+	pr_err("PME2 h/w enqueue rejection - expect catastrophe!\n");
+	cb_helper(portal, ctx, &mr->dcern.fd, 1);
+}
+
+static void cb_fqs(struct qman_portal *portal, struct qman_fq *fq,
+				const struct qm_mr_entry *mr)
+{
+	u8 verb = mr->verb & QM_MR_VERB_TYPE_MASK;
+	if (verb == QM_MR_VERB_FQRNI)
+		return;
+	/* nothing else is supposed to occur */
+	BUG();
+}
+
diff --git a/drivers/match/pme2_low.c b/drivers/match/pme2_low.c
new file mode 100644
index 0000000..c1ae5bd
--- /dev/null
+++ b/drivers/match/pme2_low.c
@@ -0,0 +1,289 @@
+/* Copyright (c) 2008, 2009, Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "pme2_private.h"
+
+MODULE_AUTHOR("Geoff Thorpe");
+MODULE_LICENSE("Dual BSD/GPL");
+MODULE_DESCRIPTION("FSL PME2 (p4080) device usage");
+
+#define PME_RESIDUE_SIZE	128
+#define PME_RESIDUE_ALIGN	64
+#define PME_FLOW_SIZE		sizeof(struct pme_flow)
+#define PME_FLOW_ALIGN		32
+static struct kmem_cache *slab_residue;
+static struct kmem_cache *slab_flow;
+static struct kmem_cache *slab_fq;
+
+/* Hack to support "pme_map()". The point of this is that dma_map_single() now
+ * requires a non-NULL device, so the idea is that address mapping must be
+ * device-sensitive. Now the PAMU IO-MMU already takes care of this, as can be
+ * seen by the device-tree structure generated by the hypervisor (each portal
+ * node has sub-nodes for each h/w end-point it provides access to, and each
+ * sub-node has its own LIODN configuration). So we just need to map cpu
+ * pointers to (guest-)physical address and the PAMU takes care of the rest, so
+ * this doesn't need to be portal-sensitive nor device-sensitive. */
+static struct platform_device *pdev;
+
+static int pme2_low_init(void)
+{
+	int ret = -ENOMEM;
+
+	slab_residue = kmem_cache_create("pme2_residue", PME_RESIDUE_SIZE,
+			PME_RESIDUE_ALIGN, SLAB_HWCACHE_ALIGN, NULL);
+	if (!slab_residue)
+		goto end;
+	slab_flow = kmem_cache_create("pme2_flow", PME_FLOW_SIZE,
+				PME_FLOW_ALIGN, 0, NULL);
+	if (!slab_flow)
+		goto end;
+	slab_fq = kmem_cache_create("pme2_fqslab", sizeof(struct qman_fq),
+			__alignof__(struct qman_fq), SLAB_HWCACHE_ALIGN, NULL);
+	if (!slab_fq)
+		goto end;
+	ret = -ENODEV;
+	pdev = platform_device_alloc("pme", -1);
+	if (!pdev)
+		goto end;
+	if (platform_device_add(pdev))
+		goto end;
+#ifdef CONFIG_FSL_PME2_SAMPLE_DB
+	pme2_sample_db();
+#endif
+	return 0;
+end:
+	if (pdev) {
+		platform_device_put(pdev);
+		pdev = NULL;
+	}
+	if (slab_flow) {
+		kmem_cache_destroy(slab_flow);
+		slab_flow = NULL;
+	}
+	if (slab_residue) {
+		kmem_cache_destroy(slab_residue);
+		slab_residue = NULL;
+	}
+	if (slab_fq) {
+		kmem_cache_destroy(slab_fq);
+		slab_fq = NULL;
+	}
+	return ret;
+}
+
+static void pme2_low_exit(void)
+{
+	platform_device_del(pdev);
+	platform_device_put(pdev);
+	pdev = NULL;
+	kmem_cache_destroy(slab_fq);
+	kmem_cache_destroy(slab_flow);
+	kmem_cache_destroy(slab_residue);
+	slab_fq = slab_flow = slab_residue = NULL;
+}
+
+module_init(pme2_low_init);
+module_exit(pme2_low_exit);
+
+struct qman_fq *slabfq_alloc(void)
+{
+	return kmem_cache_alloc(slab_fq, GFP_KERNEL);
+}
+
+void slabfq_free(struct qman_fq *fq)
+{
+	kmem_cache_free(slab_fq, fq);
+}
+
+/***********************/
+/* low-level functions */
+/***********************/
+
+struct pme_hw_residue *pme_hw_residue_new(void)
+{
+	return kmem_cache_alloc(slab_residue, GFP_KERNEL);
+}
+EXPORT_SYMBOL(pme_hw_residue_new);
+
+void pme_hw_residue_free(struct pme_hw_residue *p)
+{
+	kmem_cache_free(slab_residue, p);
+}
+EXPORT_SYMBOL(pme_hw_residue_free);
+
+struct pme_hw_flow *pme_hw_flow_new(void)
+{
+	struct pme_flow *flow = kmem_cache_zalloc(slab_flow, GFP_KERNEL);
+	if(likely(flow)){
+		flow->sos=1;
+		flow->esee=1;
+		flow->clim=0xffff;
+		flow->mlim=0xffff;
+		flow_map(flow);
+	}
+	return (struct pme_hw_flow *)flow;
+}
+EXPORT_SYMBOL(pme_hw_flow_new);
+
+void pme_hw_flow_free(struct pme_hw_flow *p)
+{
+	kmem_cache_free(slab_flow, p);
+}
+EXPORT_SYMBOL(pme_hw_flow_free);
+
+struct pme_flow *pme_sw_flow_new(void)
+{
+	return (struct pme_flow *)pme_hw_flow_new();
+}
+EXPORT_SYMBOL(pme_sw_flow_new);
+
+void pme_sw_flow_free(struct pme_flow *p)
+{
+	pme_hw_flow_free((struct pme_hw_flow *)p);
+}
+EXPORT_SYMBOL(pme_sw_flow_free);
+
+void pme_initfq(struct qm_mcc_initfq *initfq, struct pme_hw_flow *flow, u8 qos,
+		u8 rbpid, u32 rfqid)
+{
+	struct pme_context_a *pme_a =
+		(struct pme_context_a *)&initfq->fqd.context_a;
+	struct pme_context_b *pme_b =
+		(struct pme_context_b *)&initfq->fqd.context_b;
+
+	initfq->we_mask = QM_INITFQ_WE_DESTWQ | QM_INITFQ_WE_CONTEXTA |
+				QM_INITFQ_WE_CONTEXTB;
+	initfq->fqd.dest.channel = qm_channel_pme;
+	initfq->fqd.dest.wq = qos;
+	if (flow) {
+		dma_addr_t fcp;
+		fcp = flow_map((struct pme_flow *)flow);
+		pme_a->mode = pme_mode_flow;
+		/* TODO: this is to catch >32-bit physical address
+		 * configurations. */
+		BUG_ON(sizeof(dma_addr_t) > 4);
+		pme_a->flow_hi = 0;
+		pme_a->flow_lo = fcp;
+	} else {
+		pme_a->mode = pme_mode_direct;
+		pme_a->flow_hi = 0;
+		pme_a->flow_lo = 0;
+	}
+	pme_b->rbpid = rbpid;
+	pme_b->rfqid = rfqid;
+}
+EXPORT_SYMBOL(pme_initfq);
+
+void pme_fd_cmd_nop(struct qm_fd *fd)
+{
+	struct pme_cmd_nop *nop = (struct pme_cmd_nop *)&fd->cmd;
+	nop->cmd = pme_cmd_nop;
+}
+EXPORT_SYMBOL(pme_fd_cmd_nop);
+
+void pme_fd_cmd_fcw(struct qm_fd *fd, u8 flags, struct pme_flow *flow,
+		struct pme_hw_residue *residue)
+{
+	dma_addr_t f;
+	struct pme_cmd_flow_write *fcw = (struct pme_cmd_flow_write *)&fd->cmd;
+
+	BUG_ON(!flow);
+	BUG_ON((unsigned long)flow & 31);
+	fcw->cmd = pme_cmd_flow_write;
+	fcw->flags = flags;
+	if (flags & PME_CMD_FCW_RES) {
+		if (flow->ren) {
+			dma_addr_t rptr;
+			rptr = residue_map(residue);
+			BUG_ON(!residue);
+			BUG_ON((unsigned long)residue & 63);
+			/* TODO: when this breaks, the obvious fix (set flow->rptr_hi
+			 * non-zero!) will no longer generate warnings. :-) */
+			BUG_ON(sizeof(dma_addr_t) > 4);
+			flow->rptr_hi = 0;
+			flow->rptr_lo = rptr;
+		} else {
+			BUG_ON(residue);
+			flow->rptr_hi = 0;
+			flow->rptr_lo = 0;
+		}
+	}
+	f = flow_map(flow);
+	fd->addr_hi = 0;
+	fd->addr_lo = f;
+	fd->format = qm_fd_contig;
+	fd->offset = 0;
+	fd->length20 = sizeof(*flow);
+}
+EXPORT_SYMBOL(pme_fd_cmd_fcw);
+
+void pme_fd_cmd_fcr(struct qm_fd *fd, struct pme_flow *flow)
+{
+	dma_addr_t f;
+	struct pme_cmd_flow_read *fcr = (struct pme_cmd_flow_read *)&fd->cmd;
+
+	BUG_ON(!flow);
+	BUG_ON((unsigned long)flow & 31);
+	fcr->cmd = pme_cmd_flow_read;
+	f = flow_map(flow);
+	fd->addr_hi = 0;
+	fd->addr_lo = f;
+	fd->format = qm_fd_contig;
+	fd->offset = 0;
+	fd->length20 = sizeof(*flow);
+}
+EXPORT_SYMBOL(pme_fd_cmd_fcr);
+
+void pme_fd_cmd_pmtcc(struct qm_fd *fd)
+{
+	struct pme_cmd_pmtcc *pmtcc = (struct pme_cmd_pmtcc *)&fd->cmd;
+	pmtcc->cmd = pme_cmd_pmtcc;
+}
+EXPORT_SYMBOL(pme_fd_cmd_pmtcc);
+
+void pme_fd_cmd_scan(struct qm_fd *fd, u32 args)
+{
+	struct pme_cmd_scan *scan = (struct pme_cmd_scan *)&fd->cmd;
+	fd->cmd = args;
+	scan->cmd = pme_cmd_scan;
+}
+EXPORT_SYMBOL(pme_fd_cmd_scan);
+
+dma_addr_t pme_map(void *ptr)
+{
+	return dma_map_single(&pdev->dev, ptr, 1, DMA_BIDIRECTIONAL);
+}
+
+int pme_map_error(dma_addr_t dma_addr)
+{
+	return dma_mapping_error(&pdev->dev, dma_addr);
+}
diff --git a/drivers/match/pme2_private.h b/drivers/match/pme2_private.h
new file mode 100644
index 0000000..e95223e
--- /dev/null
+++ b/drivers/match/pme2_private.h
@@ -0,0 +1,165 @@
+/* Copyright (c) 2008, 2009, Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "pme2_sys.h"
+#include <linux/fsl_pme.h>
+
+#define PME2_DEBUG
+
+#ifdef PME2_DEBUG
+#define PMEPRINFO(fmt, args...) pr_info("PME2: %s: " fmt, __func__, ## args)
+#else
+#define PMEPRINFO(fmt, args...)
+#endif
+
+#define PMEPRERR(fmt, args...) pr_err("PME2: %s: " fmt, __func__, ## args)
+#define PMEPRCRIT(fmt, args...) pr_crit("PME2: %s: " fmt, __func__, ## args)
+
+static inline void set_fd_addr(struct qm_fd *fd, dma_addr_t addr)
+{
+	fd->addr_lo = lower_32_bits(addr);
+	fd->addr_hi = upper_32_bits(addr);
+}
+static inline dma_addr_t get_fd_addr(const struct qm_fd *fd)
+{
+	if ((sizeof(dma_addr_t) > 4))
+		return ((fd->addr_hi << 16) << 16) | fd->addr_lo;
+	else
+		return fd->addr_lo;
+}
+static inline void set_sg_addr(struct qm_sg_entry *sg, dma_addr_t addr)
+{
+	sg->addr_lo = lower_32_bits(addr);
+	sg->addr_hi = upper_32_bits(addr);
+}
+static inline dma_addr_t get_sg_addr(const struct qm_sg_entry *sg)
+{
+	if ((sizeof(dma_addr_t) > 4))
+		return ((sg->addr_hi << 16) << 16) | sg->addr_lo;
+	else
+		return sg->addr_lo;
+}
+
+/******************/
+/* Datapath types */
+/******************/
+
+enum pme_mode {
+	pme_mode_direct = 0x00,
+	pme_mode_flow = 0x80
+};
+
+struct pme_context_a {
+	enum pme_mode mode:8;
+	u8 __reserved;
+	/* Flow Context pointer (48-bit), ignored if mode==direct */
+	u16 flow_hi;
+	u32 flow_lo;
+} __packed;
+
+struct pme_context_b {
+	u32 rbpid:8;
+	u32 rfqid:24;
+} __packed;
+
+enum pme_cmd_type {
+	pme_cmd_nop = 0x7,
+	pme_cmd_flow_read = 0x5,	/* aka FCR */
+	pme_cmd_flow_write = 0x4,	/* aka FCW */
+	pme_cmd_pmtcc = 0x1,
+	pme_cmd_scan = 0
+};
+
+/* This is the 32-bit frame "cmd/status" field, sent to PME */
+union pme_cmd {
+	struct pme_cmd_nop {
+		enum pme_cmd_type cmd:3;
+	} nop;
+	struct pme_cmd_flow_read {
+		enum pme_cmd_type cmd:3;
+	} fcr;
+	struct pme_cmd_flow_write {
+		enum pme_cmd_type cmd:3;
+		u8 __reserved:5;
+		u8 flags;	/* See PME_CMD_FCW_*** */
+	} __packed fcw;
+	struct pme_cmd_pmtcc {
+		enum pme_cmd_type cmd:3;
+	} pmtcc;
+	struct pme_cmd_scan {
+		union {
+			struct {
+				enum pme_cmd_type cmd:3;
+				u8 flags:5; /* See PME_CMD_SCAN_*** */
+			} __packed;
+		};
+		u8 set;
+		u16 subset;
+	} __packed scan;
+};
+
+/* Hook from pme2_high to pme2_low */
+struct qman_fq *slabfq_alloc(void);
+void slabfq_free(struct qman_fq *fq);
+
+/* Hook from pme2_high to pme2_driver */
+int pme2_exclusive_set(struct qman_fq *fq);
+void pme2_exclusive_unset(void);
+
+/* Hook from pme2_low to pme2_sample_db */
+void pme2_sample_db(void);
+
+#define DECLARE_GLOBAL(name, t, mt, def, desc) \
+        static t name = def; \
+        module_param(name, mt, 0644); \
+        MODULE_PARM_DESC(name, desc ", default: " __stringify(def));
+
+/* Constants used by the SRE ioctl. */
+#define PME_PMFA_SRE_POLL_MS		100
+#define PME_PMFA_SRE_INDEX_MAX		(1 << 27)
+#define PME_PMFA_SRE_INC_MAX		(1 << 12)
+#define PME_PMFA_SRE_REP_MAX		(1 << 28)
+#define PME_PMFA_SRE_INTERVAL_MAX	(1 << 12)
+
+/* Encapsulations for mapping */
+#define flow_map(flow) \
+({ \
+	struct pme_flow *__f913 = (flow); \
+	pme_map(__f913); \
+})
+
+#define residue_map(residue) \
+({ \
+	struct pme_hw_residue *__f913 = (residue); \
+	pme_map(__f913); \
+})
+
diff --git a/drivers/match/pme2_regs.h b/drivers/match/pme2_regs.h
new file mode 100644
index 0000000..6ac4f44
--- /dev/null
+++ b/drivers/match/pme2_regs.h
@@ -0,0 +1,174 @@
+/* Copyright (c) 2009, Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef PME2_REGS_H
+#define PME2_REGS_H
+
+#define PME_REG_ISR		0x000
+#define PME_REG_IER		0x004
+#define PME_REG_ISDR		0x008
+#define PME_REG_IIR		0x00C
+#define PME_REG_RLL		0x014
+#define PME_REG_CDCR		0x018
+#define PME_REG_TRUNCI		0x024
+#define PME_REG_RBC		0x028
+#define PME_REG_ESR		0x02C
+#define PME_REG_ECR1		0x030
+#define PME_REG_ECR0		0x034
+#define PME_REG_EFQC		0x050
+#define PME_REG_FACONF		0x060
+#define PME_REG_FAMCR		0x064
+#define PME_REG_PMSTAT		0x068
+#define PME_REG_PMTR		0x06C
+#define PME_REG_PEHD		0x074
+#define PME_REG_BSC0		0x080
+#define PME_REG_BSC1		0x084
+#define PME_REG_BSC2		0x088
+#define PME_REG_BSC3		0x08C
+#define PME_REG_BSC4		0x090
+#define PME_REG_BSC5		0x094
+#define PME_REG_BSC6		0x098
+#define PME_REG_BSC7		0x09C
+#define PME_REG_QMBFD0		0x0E0
+#define PME_REG_QMBFD1		0x0E4
+#define PME_REG_QMBFD2		0x0E8
+#define PME_REG_QMBFD3		0x0EC
+#define PME_REG_QMBCTXTAH	0x0F0
+#define PME_REG_QMBCTXTAL	0x0F4
+#define PME_REG_QMBCTXTB	0x0F8
+#define PME_REG_QMBCTL		0x0FC
+#define PME_REG_ECC1BES		0x100
+#define PME_REG_ECC2BES		0x104
+#define PME_REG_ECCADDR		0x110
+#define PME_REG_ECCCODE		0x118
+#define PME_REG_TBT0ECC1TH	0x180
+#define PME_REG_TBT0ECC1EC	0x184
+#define PME_REG_TBT1ECC1TH	0x188
+#define PME_REG_TBT1ECC1EC	0x18C
+#define PME_REG_VLT0ECC1TH	0x190
+#define PME_REG_VLT0ECC1EC	0x194
+#define PME_REG_VLT1ECC1TH	0x198
+#define PME_REG_VLT1ECC1EC	0x19C
+#define PME_REG_CMECC1TH	0x1A0
+#define PME_REG_CMECC1EC	0x1A4
+#define PME_REG_DXCMECC1TH	0x1B0
+#define PME_REG_DXCMECC1EC	0x1B4
+#define PME_REG_DXEMECC1TH	0x1C0
+#define PME_REG_DXEMECC1EC	0x1C4
+#define PME_REG_STNIB		0x200
+#define PME_REG_STNIS		0x204
+#define PME_REG_STNTH1		0x208
+#define PME_REG_STNTH2		0x20C
+#define PME_REG_STNTHV		0x210
+#define PME_REG_STNTHS		0x214
+#define PME_REG_STNCH		0x218
+#define PME_REG_SWDB		0x21C
+#define PME_REG_KVLTS		0x220
+#define PME_REG_KEC		0x224
+#define PME_REG_STNPM		0x280
+#define PME_REG_STNS1M		0x284
+#define PME_REG_DRCIC		0x288
+#define PME_REG_DRCMC		0x28C
+#define PME_REG_STNPMR		0x290
+#define PME_REG_PDSRBAH		0x2A0
+#define PME_REG_PDSRBAL		0x2A4
+#define PME_REG_DMCR		0x2A8
+#define PME_REG_DEC0		0x2AC
+#define PME_REG_DEC1		0x2B0
+#define PME_REG_DLC		0x2C0
+#define PME_REG_STNDSR		0x300
+#define PME_REG_STNESR		0x304
+#define PME_REG_STNS1R		0x308
+#define PME_REG_STNOB		0x30C
+#define PME_REG_SCBARH		0x310
+#define PME_REG_SCBARL		0x314
+#define PME_REG_SMCR		0x318
+#define PME_REG_SREC		0x320
+#define PME_REG_ESRP		0x328
+#define PME_REG_SRRV0		0x338
+#define PME_REG_SRRV1		0x33C
+#define PME_REG_SRRV2		0x340
+#define PME_REG_SRRV3		0x344
+#define PME_REG_SRRV4		0x348
+#define PME_REG_SRRV5		0x34C
+#define PME_REG_SRRV6		0x350
+#define PME_REG_SRRV7		0x354
+#define PME_REG_SRRFI		0x358
+#define PME_REG_SRRI		0x360
+#define PME_REG_SRRR		0x364
+#define PME_REG_SRRWC		0x368
+#define PME_REG_SFRCC		0x36C
+#define PME_REG_SEC1		0x370
+#define PME_REG_SEC2		0x374
+#define PME_REG_SEC3		0x378
+#define PME_REG_MIA_BYC		0x380
+#define PME_REG_MIA_BLC		0x384
+#define PME_REG_MIA_CE		0x388
+#define PME_REG_MIA_CR		0x390
+#define PME_REG_PPIDMR0		0x800
+#define PME_REG_PPIDMR1		0x804
+#define PME_REG_PPIDMR2		0x808
+#define PME_REG_PPIDMR3		0x80C
+#define PME_REG_PPIDMR4		0x810
+#define PME_REG_PPIDMR5		0x814
+#define PME_REG_PPIDMR6		0x818
+#define PME_REG_PPIDMR7		0x81C
+#define PME_REG_PPIDMR8		0x820
+#define PME_REG_PPIDMR9		0x824
+#define PME_REG_PPIDMR10	0x828
+#define PME_REG_PPIDMR11	0x82C
+#define PME_REG_PPIDMR12	0x830
+#define PME_REG_PPIDMR13	0x834
+#define PME_REG_PPIDMR14	0x838
+#define PME_REG_PPIDMR15	0x83C
+#define PME_REG_PPIDMR16	0x840
+#define PME_REG_PPIDMR17	0x844
+#define PME_REG_PPIDMR18	0x848
+#define PME_REG_PPIDMR19	0x84C
+#define PME_REG_PPIDMR20	0x850
+#define PME_REG_PPIDMR21	0x854
+#define PME_REG_PPIDMR22	0x858
+#define PME_REG_PPIDMR23	0x85C
+#define PME_REG_PPIDMR24	0x860
+#define PME_REG_PPIDMR25	0x864
+#define PME_REG_PPIDMR26	0x868
+#define PME_REG_PPIDMR27	0x86C
+#define PME_REG_PPIDMR28	0x870
+#define PME_REG_PPIDMR29	0x874
+#define PME_REG_PPIDMR30	0x878
+#define PME_REG_PPIDMR31	0x87C
+#define PME_REG_SRCIDR		0xA00
+#define PME_REG_LIODNR		0xA0C
+#define PME_REG_PM_IP_REV1	0xBF8
+#define PME_REG_PM_IP_REV2	0xBFC
+
+#endif /* REGS_H */
diff --git a/drivers/match/pme2_sample_db.c b/drivers/match/pme2_sample_db.c
new file mode 100644
index 0000000..8dc054e
--- /dev/null
+++ b/drivers/match/pme2_sample_db.c
@@ -0,0 +1,206 @@
+/* Copyright (c) 2009, Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/* So long as hexdump() is used, we break ranks and use the test-specific
+ * header. Naughty naughty... */
+#if 0
+#include "pme2_private.h"
+#else
+#include "pme2_test.h"
+#endif
+
+static u8 pme_db[] = {
+	0x01,0x01,0x00,0x00,0x00,0x00,0x00,0x38,0x00,0x00,0x00,0x00,0x00,0x00,
+	0x00,0x02,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,
+	0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,
+	0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,
+	0x01,0x01,0x00,0x00,0x00,0x00,0x00,0x38,0x00,0x00,0x00,0x00,0x00,0x00,
+	0x00,0x05,0x00,0x00,0x00,0x08,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,
+	0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,
+	0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,
+	0x01,0x01,0x00,0x00,0x00,0x00,0x00,0x1c,0x00,0x00,0x00,0x00,0x00,0x00,
+	0x00,0x0c,0x00,0x00,0x00,0x03,0x00,0x01,0x20,0x41,0x40,0x20,0x00,0x11,
+	0x01,0x01,0x00,0x00,0x00,0x00,0x00,0x98,0x00,0x00,0x00,0x00,0x00,0x00,
+	0x00,0x0d,0x00,0x00,0x00,0x04,0x00,0x01,0x20,0x41,0xff,0x81,0x00,0x00,
+	0x00,0x00,0xff,0xff,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,
+	0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,
+	0x00,0x80,0x00,0x01,0x01,0xff,0x80,0x00,0x41,0x00,0x00,0x00,0x00,0x00,
+	0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,
+	0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,
+	0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,
+	0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,
+	0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,
+	0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x01,0x01,
+	0x00,0x00,0x00,0x00,0x00,0x38,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x0e,
+	0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,
+	0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,
+	0x00,0x00,0x00,0x02,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x01,0x01,
+	0x00,0x00,0x00,0x00,0x01,0x18,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x0b,
+	0x00,0x00,0x00,0x06,0x00,0x00,0x00,0x00,0x00,0x01,0x02,0x03,0x04,0x05,
+	0x06,0x07,0x08,0x09,0x0a,0x0b,0x0c,0x0d,0x0e,0x0f,0x10,0x11,0x12,0x13,
+	0x14,0x15,0x16,0x17,0x18,0x19,0x1a,0x1b,0x1c,0x1d,0x1e,0x1f,0x20,0x21,
+	0x22,0x23,0x24,0x25,0x26,0x27,0x28,0x29,0x2a,0x2b,0x2c,0x2d,0x2e,0x2f,
+	0x30,0x31,0x32,0x33,0x34,0x35,0x36,0x37,0x38,0x39,0x3a,0x3b,0x3c,0x3d,
+	0x3e,0x3f,0x40,0x41,0x42,0x43,0x44,0x45,0x46,0x47,0x48,0x49,0x4a,0x4b,
+	0x4c,0x4d,0x4e,0x4f,0x50,0x51,0x52,0x53,0x54,0x55,0x56,0x57,0x58,0x59,
+	0x5a,0x5b,0x5c,0x5d,0x5e,0x5f,0x60,0x41,0x42,0x43,0x44,0x45,0x46,0x47,
+	0x48,0x49,0x4a,0x4b,0x4c,0x4d,0x4e,0x4f,0x50,0x51,0x52,0x53,0x54,0x55,
+	0x56,0x57,0x58,0x59,0x5a,0x7b,0x7c,0x7d,0x7e,0x7f,0x80,0x81,0x82,0x83,
+	0x84,0x85,0x86,0x87,0x88,0x89,0x8a,0x8b,0x8c,0x8d,0x8e,0x8f,0x90,0x91,
+	0x92,0x93,0x94,0x95,0x96,0x97,0x98,0x99,0x9a,0x9b,0x9c,0x9d,0x9e,0x9f,
+	0xa0,0xa1,0xa2,0xa3,0xa4,0xa5,0xa6,0xa7,0xa8,0xa9,0xaa,0xab,0xac,0xad,
+	0xae,0xaf,0xb0,0xb1,0xb2,0xb3,0xb4,0xb5,0xb6,0xb7,0xb8,0xb9,0xba,0xbb,
+	0xbc,0xbd,0xbe,0xbf,0xc0,0xc1,0xc2,0xc3,0xc4,0xc5,0xc6,0xc7,0xc8,0xc9,
+	0xca,0xcb,0xcc,0xcd,0xce,0xcf,0xd0,0xd1,0xd2,0xd3,0xd4,0xd5,0xd6,0xd7,
+	0xd8,0xd9,0xda,0xdb,0xdc,0xdd,0xde,0xdf,0xe0,0xe1,0xe2,0xe3,0xe4,0xe5,
+	0xe6,0xe7,0xe8,0xe9,0xea,0xeb,0xec,0xed,0xee,0xef,0xf0,0xf1,0xf2,0xf3,
+	0xf4,0xf5,0xf6,0xf7,0xf8,0xf9,0xfa,0xfb,0xfc,0xfd,0xfe,0xff
+};
+
+static u8 db_read[] = {
+	0x01,0x00,0x00,0x00,0x00,0x00,0x00,0x18,0x11,0x00,0x00,0x00,0x00,0x00,
+	0x00,0x00,0x00,0x00,0x00,0x03,0x00,0x01,0x20,0x41
+};
+
+static u8 result_read_data[] = {
+	0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,
+	0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,
+	0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00
+};
+
+static u8 db_read_expected_result[] = {
+	0x01,0x00,0x00,0x00,0x00,0x00,0x00,0x1c,0x11,0x00,0x00,0x00,0x00,0x00,
+	0x00,0x00,0x00,0x00,0x00,0x03,0x00,0x01,0x20,0x41,0x40,0x20,0x00,0x11
+};
+
+struct pmtcc_ctx {
+	struct pme_ctx base_ctx;
+	struct qm_fd result_fd;
+	struct completion done;
+};
+
+static void pmtcc_cb(struct pme_ctx *ctx, const struct qm_fd *fd,
+				struct pme_ctx_token *ctx_token)
+{
+	struct pmtcc_ctx *my_ctx = (struct pmtcc_ctx *)ctx;
+	memcpy(&my_ctx->result_fd, fd, sizeof(*fd));
+	complete(&my_ctx->done);
+}
+
+void pme2_sample_db(void)
+{
+	struct pmtcc_ctx ctx = {
+		.base_ctx.cb = pmtcc_cb,
+	};
+	struct qm_fd fd;
+	struct qm_sg_entry sg_table[2];
+	int ret;
+	enum pme_status status;
+	struct pme_ctx_token token;
+
+	pr_info("sample_db: starting pme2 sample DB initialisation\n");
+
+	init_completion(&ctx.done);
+	ret = pme_ctx_init(&ctx.base_ctx,
+		PME_CTX_FLAG_EXCLUSIVE |
+		PME_CTX_FLAG_PMTCC |
+		PME_CTX_FLAG_LOCAL, 0, 4, 4, 0, NULL);
+	BUG_ON(ret);
+	/* enable the context */
+	ret = pme_ctx_enable(&ctx.base_ctx);
+	BUG_ON(ret);
+
+	/* Write the database */
+	memset(&fd, 0, sizeof(struct qm_fd));
+	fd.length20 = sizeof(pme_db);
+	fd.addr_lo = pme_map(pme_db);
+
+	ret = pme_ctx_pmtcc(&ctx.base_ctx, PME_CTX_OP_WAIT, &fd, &token);
+	if (ret == -ENODEV) {
+		pr_err("sample_db: not the control plane, bailing\n");
+		ret = pme_ctx_disable(&ctx.base_ctx,
+			PME_CTX_OP_WAIT | PME_CTX_OP_WAIT_INT);
+		BUG_ON(ret);
+		pme_ctx_finish(&ctx.base_ctx);
+		return;
+	}
+	BUG_ON(ret);
+	wait_for_completion(&ctx.done);
+	status = pme_fd_res_status(&ctx.result_fd);
+	if (status) {
+		pr_info("sample_db: PMTCC write status failed %d\n", status);
+		BUG_ON(1);
+	}
+
+	/* Read back the database */
+	init_completion(&ctx.done);
+	memset(&fd, 0, sizeof(struct qm_fd));
+	memset(&sg_table, 0, sizeof(sg_table));
+	sg_table[0].addr_lo = pme_map(result_read_data);
+	sg_table[0].length = sizeof(result_read_data);
+	sg_table[1].addr_lo = pme_map(db_read);
+	sg_table[1].length = sizeof(db_read);
+	sg_table[1].final = 1;
+	fd.format = qm_fd_compound;
+	fd.addr_lo = pme_map(sg_table);
+	ret = pme_ctx_pmtcc(&ctx.base_ctx, PME_CTX_OP_WAIT, &fd, &token);
+	BUG_ON(ret);
+	wait_for_completion(&ctx.done);
+
+	status = pme_fd_res_status(&ctx.result_fd);
+	if (status) {
+		pr_info("sample_db: PMTCC read status failed %d\n", status);
+		BUG_ON(1);
+	}
+	if (pme_fd_res_flags(&ctx.result_fd)) {
+		pr_err("sample_db: flags result set %x\n",
+			pme_fd_res_flags(&ctx.result_fd));
+		BUG_ON(1);
+	}
+	if (memcmp(db_read_expected_result, result_read_data,
+				sizeof(result_read_data)) != 0) {
+		pr_err("sample_db: DB read result not expected\n");
+		pr_err("Expected\n");
+		hexdump(db_read_expected_result,
+				sizeof(db_read_expected_result));
+		pr_info("Received\n");
+		hexdump(result_read_data, sizeof(result_read_data));
+		BUG_ON(1);
+	}
+	/* Disable */
+	ret = pme_ctx_disable(&ctx.base_ctx,
+		PME_CTX_OP_WAIT | PME_CTX_OP_WAIT_INT);
+	BUG_ON(ret);
+	pme_ctx_finish(&ctx.base_ctx);
+	pr_info("sample_db: pme2 sample DB initialised\n");
+}
+
diff --git a/drivers/match/pme2_scan.c b/drivers/match/pme2_scan.c
new file mode 100644
index 0000000..9b29c80
--- /dev/null
+++ b/drivers/match/pme2_scan.c
@@ -0,0 +1,775 @@
+/* Copyright (c) 2009, Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "pme2_private.h"
+#include <linux/miscdevice.h>
+#include <linux/uaccess.h>
+#include <linux/poll.h>
+
+#define WAIT_AND_INTERRUPTABLE	(PME_CTX_OP_WAIT|PME_CTX_OP_WAIT_INT)
+#define INPUT_FRM	1
+#define OUTPUT_FRM	0
+/* Private structure that is allocated for each open that is done on the
+ * pme_scan device. */
+struct scan_session {
+	/* The ctx that is needed to communicate with the pme high level */
+	struct pme_ctx ctx;
+	/* Locks completed_commands */
+	spinlock_t set_subset_lock;
+	__u8 set;
+	__u16 subset;
+	/* For asynchronous processing */
+	wait_queue_head_t waiting_for_completion;
+	struct list_head completed_commands;
+	/* Locks completed_commands */
+	spinlock_t completed_commands_lock;
+	u32 completed_count;
+};
+
+/* Command Token for scan operations. One of these is created for every
+ * operation on a context. When the context operation is complete cleanup
+ * is done */
+struct cmd_token {
+	/* pme high level token */
+	struct pme_ctx_token hl_token;
+	/* The kernels copy of the user op structure */
+	struct pme_scan_cmd kernel_op;
+	/* Set to non zero if this is a synchronous request */
+	u8 synchronous;
+	/* data */
+	struct qm_fd tx_fd;
+	struct qm_sg_entry tx_comp[2];
+	struct qm_fd rx_fd;
+	void *tx_data;
+	size_t tx_size;
+	void *rx_data;
+	size_t rx_size;
+	/* For blocking requests, we need a wait point and condition */
+	wait_queue_head_t *queue;
+	/* List management for completed async requests */
+	struct list_head completed_list;
+	u8 done;
+};
+
+static inline int scan_data_empty(struct scan_session *session)
+{
+	return list_empty(&session->completed_commands);
+}
+
+/* Cleanup for the execute_cmd method */
+static inline void cleanup_token(struct cmd_token *token_p)
+{
+	kfree(token_p->tx_data);
+	kfree(token_p->rx_data);
+	return;
+}
+
+/* Callback for scan operations */
+static void scan_cb(struct pme_ctx *ctx, const struct qm_fd *fd,
+				struct pme_ctx_token *ctx_token)
+{
+	struct cmd_token *token = (struct cmd_token *)ctx_token;
+	struct scan_session *session = (struct scan_session *)ctx;
+
+	token->rx_fd = *fd;
+	/* If this is a asynchronous command, queue the token */
+	if (!token->synchronous) {
+		spin_lock(&session->completed_commands_lock);
+		list_add_tail(&token->completed_list,
+			      &session->completed_commands);
+		session->completed_count++;
+		spin_unlock(&session->completed_commands_lock);
+	}
+	/* Wake up the thread that's waiting for us */
+	token->done = 1;
+	wake_up(token->queue);
+	return;
+}
+
+static int process_completed_token(struct file *fp,
+				struct cmd_token *token_p,
+				struct pme_scan_result *user_result)
+{
+	int ret = 0;
+	struct pme_scan_result local_result;
+	u32 src_sz, dst_sz;
+
+#ifdef CONFIG_FSL_PME2_SCAN_DEBUG
+	pr_info("pme2_scan: process_completed_token\n");
+	pr_info("pme2_scan: received %d frame type\n", token_p->rx_fd.format);
+#endif
+	memset(&local_result, 0, sizeof(local_result));
+	local_result.output.data = token_p->kernel_op.output.data;
+
+	if (token_p->rx_fd.format == qm_fd_compound) {
+		/* Need to copy  output */
+		src_sz = token_p->tx_comp[OUTPUT_FRM].length;
+		dst_sz = token_p->kernel_op.output.size;
+#ifdef CONFIG_FSL_PME2_SCAN_DEBUG
+		pr_info("pme2_scan: pme gen %u data, have space for %u\n",
+				src_sz, dst_sz);
+#endif
+		local_result.output.size = min(dst_sz, src_sz);
+		/* Doesn't make sense we generated more than available space
+		 * should have got truncation.
+		 */
+		BUG_ON(dst_sz < src_sz);
+		if (copy_to_user(local_result.output.data, token_p->rx_data,
+				local_result.output.size)) {
+			pr_err("Error copying to user data \n");
+			cleanup_token(token_p);
+			return -EFAULT;
+		}
+	} else if (token_p->rx_fd.format == qm_fd_sg_big)
+		local_result.output.size = 0;
+	else
+		pr_err("pme2_scan: unexpected frame type received\n");
+
+	local_result.flags |= pme_fd_res_flags(&token_p->rx_fd);
+	local_result.status |= pme_fd_res_status(&token_p->rx_fd);
+	local_result.opaque = token_p->kernel_op.opaque;
+#ifdef CONFIG_FSL_PME2_SCAN_DEBUG
+	pr_info("pme2_scan: process_completed_token, cpy to user\n");
+#endif
+	/* Update the used values */
+	if (copy_to_user(user_result, &local_result, sizeof(local_result))) {
+		ret = -EFAULT;
+		cleanup_token(token_p);
+	}
+#ifdef CONFIG_FSL_PME2_SCAN_DEBUG
+	pr_info("pme2_scan: process_completed_token, free token\n");
+#endif
+	cleanup_token(token_p);
+	return ret;
+}
+
+static int getscan_cmd(struct file *fp, struct scan_session *session,
+	struct pme_scan_params *user_scan_params)
+{
+	int ret = 0;
+	struct pme_flow *params = NULL;
+	struct pme_scan_params local_scan_params;
+
+	memset(&local_scan_params, 0, sizeof(local_scan_params));
+
+	/* must be enabled */
+	if (pme_ctx_is_disabled(&session->ctx)) {
+		pr_err("pme2_scan: ctx is disabled\n");
+		ret = -EINVAL;
+		goto done;
+	}
+	params = pme_sw_flow_new();
+	if (!params) {
+		pr_err("pme2_scan: unable to allocate flw_ctx\n");
+		ret = -ENOMEM;
+		goto done;
+	}
+	ret = pme_ctx_ctrl_read_flow(&session->ctx, WAIT_AND_INTERRUPTABLE,
+			params);
+	if (ret) {
+		pr_info("pme2_scan: read flow error %d\n", ret);
+		goto done;
+	}
+	local_scan_params.residue.enable = params->ren;
+	local_scan_params.residue.length = params->rlen;
+	local_scan_params.sre.sessionid = params->sessionid;
+	local_scan_params.sre.verbose = params->srvm;
+	local_scan_params.sre.esee = params->esee;
+	local_scan_params.dxe.clim = params->clim;
+	local_scan_params.dxe.mlim = params->mlim;
+	spin_lock(&session->set_subset_lock);
+	local_scan_params.pattern.set = session->set;
+	local_scan_params.pattern.subset = session->subset;
+	spin_unlock(&session->set_subset_lock);
+
+	if (copy_to_user(user_scan_params, &local_scan_params,
+			sizeof(local_scan_params))) {
+		pr_err("Error copying to user data \n");
+		ret = -EFAULT;
+	}
+done:
+	if (params)
+		pme_sw_flow_free(params);
+	return ret;
+}
+
+static int setscan_cmd(struct file *fp, struct scan_session *session,
+	struct pme_scan_params *user_params)
+{
+	int ret = 0;
+	u32 flag = WAIT_AND_INTERRUPTABLE;
+
+	struct pme_flow *params = NULL;
+	struct pme_scan_params local_params;
+
+	if (copy_from_user(&local_params, user_params, sizeof(local_params)))
+		return -EFAULT;
+
+	/* must be enabled */
+	if (pme_ctx_is_disabled(&session->ctx)) {
+		pr_err("pme2_scan: ctx is disabled\n");
+		ret = -EINVAL;
+		goto done;
+	}
+	/* Only send a flw_ctx_w if PME_SCAN_PARAMS_{RESIDUE, SRE or DXE}
+	 * is being done */
+	if (local_params.flags == PME_SCAN_PARAMS_PATTERN)
+		goto set_subset;
+	params = pme_sw_flow_new();
+	if (!params) {
+		pr_err("pme2_scan: unable to allocate flw_ctx\n");
+		ret = -ENOMEM;
+		goto done;
+	}
+	if (local_params.flags & PME_SCAN_PARAMS_RESIDUE)
+		flag |= PME_CMD_FCW_RES;
+	if (local_params.flags & PME_SCAN_PARAMS_SRE)
+		flag |= PME_CMD_FCW_SRE;
+	if (local_params.flags & PME_SCAN_PARAMS_DXE)
+		flag |= PME_CMD_FCW_DXE;
+	params->ren = local_params.residue.enable;
+	params->sessionid = local_params.sre.sessionid;
+	params->srvm = local_params.sre.verbose;
+	params->esee = local_params.sre.esee;
+	params->clim = local_params.dxe.clim;
+	params->mlim = local_params.dxe.mlim;
+
+	ret = pme_ctx_ctrl_update_flow(&session->ctx, flag, params);
+	if (ret) {
+		pr_info("pme2_scan: update flow error %d\n", ret);
+		goto done;
+	}
+
+set_subset:
+	if (local_params.flags & PME_SCAN_PARAMS_PATTERN) {
+		spin_lock(&session->set_subset_lock);
+		session->set = local_params.pattern.set;
+		session->subset = local_params.pattern.subset;
+		spin_unlock(&session->set_subset_lock);
+		goto done;
+	}
+done:
+	if (params)
+		pme_sw_flow_free(params);
+	return ret;
+}
+
+static int resetseq_cmd(struct file *fp, struct scan_session *session)
+{
+	int ret = 0;
+	struct pme_flow *params = NULL;
+
+	/* must be enabled */
+	if (pme_ctx_is_disabled(&session->ctx)) {
+		pr_err("pme2_scan: ctx is disabled\n");
+		ret =  -EINVAL;
+		goto done;
+	}
+	params = pme_sw_flow_new();
+	if (!params) {
+		pr_err("pme2_scan: unable to allocate flw_ctx\n");
+		ret = -ENOMEM;
+		goto done;
+	}
+	params->seqnum_hi = 0;
+	params->seqnum_lo = 0;
+	params->sos = 1;
+
+	ret = pme_ctx_ctrl_update_flow(&session->ctx, PME_CMD_FCW_SEQ, params);
+	if (!ret)
+		pr_info("pme2_scan: update flow error %d\n", ret);
+done:
+	if (params)
+		pme_sw_flow_free(params);
+	return ret;
+}
+
+static int resetresidue_cmd(struct file *fp, struct scan_session *session)
+{
+	int ret = 0;
+	struct pme_flow *params = NULL;
+
+	/* must be enabled */
+	if (pme_ctx_is_disabled(&session->ctx)) {
+		pr_err("pme2_scan: ctx is disabled\n");
+		ret =  -EINVAL;
+		goto done;
+	}
+	params = pme_sw_flow_new();
+	if (!params) {
+		pr_err("pme2_scan: unable to allocate flw_ctx\n");
+		ret = -ENOMEM;
+		goto done;
+	}
+	params->rlen = 0;
+	ret = pme_ctx_ctrl_update_flow(&session->ctx,
+			WAIT_AND_INTERRUPTABLE | PME_CTX_OP_RESETRESLEN,
+			params);
+	if (!ret)
+		pr_info("pme2_scan: update flow error %d\n", ret);
+done:
+	if (params)
+		pme_sw_flow_free(params);
+	return ret;
+}
+
+static int process_scan_cmd(struct file *fp, struct scan_session *session,
+	struct pme_scan_cmd *user_cmd, struct pme_scan_result *user_ret,
+	u8 synchronous)
+{
+	int ret = 0;
+	struct cmd_token local_token;
+	struct cmd_token *token_p = NULL;
+	DECLARE_WAIT_QUEUE_HEAD(local_waitqueue);
+	u8 scan_flags = 0;
+
+	BUG_ON(synchronous && !user_ret);
+
+	/* If synchronous, use a local token (from the stack)
+	 * If asynchronous, allocate a token to use */
+	if (synchronous)
+		token_p = &local_token;
+	else {
+		token_p = kmalloc(sizeof(*token_p), GFP_KERNEL);
+		if (!token_p)
+			return -ENOMEM;
+	}
+	memset(token_p, 0, sizeof(*token_p));
+
+	/* Copy the command to kernel space */
+	if (copy_from_user(&token_p->kernel_op, user_cmd, sizeof(*user_cmd)))
+		return -EFAULT;
+	/* Copy the input */
+#ifdef CONFIG_FSL_PME2_SCAN_DEBUG
+	pr_info("Received User Space Contiguous mem \n");
+	pr_info("length = %d \n", token_p->kernel_op.input.size);
+#endif
+	token_p->synchronous = synchronous;
+	token_p->tx_size = token_p->kernel_op.input.size;
+	token_p->tx_data = kmalloc(token_p->kernel_op.input.size, GFP_KERNEL);
+	if (!token_p->tx_data) {
+		pr_err("pme2_scan: Err alloc %d byte", token_p->tx_size);
+		cleanup_token(token_p);
+		return -ENOMEM;
+	} else {
+#ifdef CONFIG_FSL_PME2_SCAN_DEBUG
+		pr_info("kmalloc tx %p\n", token_p->tx_data);
+#endif
+	}
+	if (copy_from_user(token_p->tx_data,
+			token_p->kernel_op.input.data,
+			token_p->kernel_op.input.size)) {
+		pr_err("Error copying contigous user data \n");
+		cleanup_token(token_p);
+		return -EFAULT;
+	}
+#ifdef CONFIG_FSL_PME2_SCAN_DEBUG
+	pr_info("Copied contiguous user data\n");
+#endif
+
+	/* Setup input frame */
+	token_p->tx_comp[INPUT_FRM].final = 1;
+	token_p->tx_comp[INPUT_FRM].length = token_p->tx_size;
+	token_p->tx_comp[INPUT_FRM].addr_lo = pme_map(token_p->tx_data);
+
+	/* setup output frame, if output is expected */
+	if (token_p->kernel_op.output.size) {
+		token_p->rx_size = token_p->kernel_op.output.size;
+	#ifdef CONFIG_FSL_PME2_SCAN_DEBUG
+		pr_info("pme2_scan: expect output %d\n", token_p->rx_size);
+	#endif
+		token_p->rx_data = kmalloc(token_p->rx_size, GFP_KERNEL);
+		if (!token_p->rx_data) {
+			pr_err("pme2_scan: Err alloc %d byte",
+					token_p->rx_size);
+			cleanup_token(token_p);
+			return -ENOMEM;
+		} else {
+#ifdef CONFIG_FSL_PME2_SCAN_DEBUG
+			pr_info("kmalloc rx %p, size %d\n", token_p->rx_data,
+					token_p->rx_size);
+#endif
+		}
+		/* Setup output frame */
+		token_p->tx_comp[OUTPUT_FRM].length = token_p->rx_size;
+		token_p->tx_comp[OUTPUT_FRM].addr_lo =
+			pme_map(token_p->rx_data);
+		token_p->tx_fd.format = qm_fd_compound;
+		/* Build compound frame */
+		token_p->tx_fd.addr_lo = pme_map(token_p->tx_comp);
+	} else {
+		token_p->tx_fd.format = qm_fd_sg_big;
+		/* Build sg frame */
+		token_p->tx_fd.addr_lo = pme_map(&token_p->tx_comp[INPUT_FRM]);
+		token_p->tx_fd.length29 = token_p->tx_size;
+	}
+
+	/* use the local wait queue if synchronous, the shared
+	 * queue if asynchronous */
+	if (synchronous)
+		token_p->queue = &local_waitqueue;
+	else
+		token_p->queue = &session->waiting_for_completion;
+	token_p->done = 0;
+
+#ifdef CONFIG_FSL_PME2_SCAN_DEBUG
+	pr_info("About to call pme_ctx_scan\n");
+#endif
+	if (token_p->kernel_op.flags & PME_SCAN_CMD_STARTRESET)
+		scan_flags |= PME_CMD_SCAN_SR;
+	if (token_p->kernel_op.flags & PME_SCAN_CMD_END)
+		scan_flags |= PME_CMD_SCAN_E;
+
+	ret = pme_ctx_scan(&session->ctx, WAIT_AND_INTERRUPTABLE,
+		&token_p->tx_fd,
+		PME_SCAN_ARGS(scan_flags, session->set, session->subset),
+		&token_p->hl_token);
+
+	if (unlikely(ret)) {
+		cleanup_token(token_p);
+		return ret;
+	}
+
+	if (!synchronous)
+		/* Don't wait.  The command is away */
+		return 0;
+
+#ifdef CONFIG_FSL_PME2_SCAN_DEBUG
+	pr_info("Wait for completion\n");
+#endif
+
+	/* Wait for the command to complete */
+	/* TODO: Should this be wait_event_interruptible ?
+	 * If so, will need logic to indicate */
+	wait_event(*token_p->queue, token_p->done == 1);
+	return process_completed_token(fp, token_p, user_ret);
+}
+
+/**
+ * fsl_pme2_scan_open - open the driver
+ *
+ * Open the driver and prepare for requests.
+ *
+ * Every time an application opens the driver, we create a scan_session object
+ * for that file handle.
+ */
+static int fsl_pme2_scan_open(struct inode *node, struct file *fp)
+{
+	int ret;
+	struct scan_session *session;
+#ifdef CONFIG_FSL_PME2_SCAN_DEBUG
+	pr_info("pme2_scan: open %d\n", smp_processor_id());
+#endif
+	fp->private_data = kzalloc(sizeof(*session), GFP_KERNEL);
+	if (!fp->private_data)
+		return -ENOMEM;
+	session = (struct scan_session *)fp->private_data;
+	/* Set up the structures used for asynchronous requests */
+	init_waitqueue_head(&session->waiting_for_completion);
+	INIT_LIST_HEAD(&session->completed_commands);
+	spin_lock_init(&session->completed_commands_lock);
+	spin_lock_init(&session->set_subset_lock);
+#ifdef CONFIG_FSL_PME2_SCAN_DEBUG
+	pr_info("kmalloc session %p\n", fp->private_data);
+#endif
+	session = fp->private_data;
+	session->ctx.cb = scan_cb;
+
+	/* qosin, qosout should be driver attributes */
+	ret = pme_ctx_init(&session->ctx, PME_CTX_FLAG_LOCAL, 0, 4, 4, 0, NULL);
+	if (ret) {
+		pr_err("pme2_scan: pme_ctx_init %d \n", ret);
+		goto exit;
+	}
+	/* enable the context */
+	ret = pme_ctx_enable(&session->ctx);
+	if (ret) {
+		pr_info("pme2_scan: error enabling ctx %d\n", ret);
+		pme_ctx_finish(&session->ctx);
+		goto exit;
+	}
+#ifdef CONFIG_FSL_PME2_SCAN_DEBUG
+	/* Set up the structures used for asynchronous requests */
+	pr_info("pme2_scan: Finish pme_scan open %d \n", smp_processor_id());
+#endif
+	return 0;
+exit:
+	kfree(fp->private_data);
+	fp->private_data = NULL;
+	return ret;
+}
+
+static int fsl_pme2_scan_close(struct inode *node, struct file *fp)
+{
+	int ret = 0;
+	struct scan_session *session = fp->private_data;
+
+	/* Before disabling check to see if it's already disabled. This can
+	 * happen if a pme serious error has occurred for instance.*/
+	if (!pme_ctx_is_disabled(&session->ctx)) {
+		ret = pme_ctx_disable(&session->ctx,
+				PME_CTX_OP_WAIT | PME_CTX_OP_WAIT_INT);
+		if (ret) {
+			pr_err("pme2_scan: Error disabling ctx %d\n", ret);
+			return ret;
+		}
+	}
+
+	pme_ctx_finish(&session->ctx);
+	kfree(session);
+#ifdef CONFIG_FSL_PME2_SCAN_DEBUG
+	pr_info("pme2_scan: Finish pme_session close\n");
+#endif
+	return 0;
+}
+
+static unsigned int fsl_pme2_scan_poll(struct file *fp,
+				      struct poll_table_struct *wait)
+{
+	struct scan_session *session;
+	unsigned int mask = POLLOUT | POLLWRNORM;
+
+	if (!fp->private_data)
+		return -EINVAL;
+
+	session = (struct scan_session *)fp->private_data;
+
+	poll_wait(fp, &session->waiting_for_completion, wait);
+
+	if (!scan_data_empty(session))
+		mask |= (POLLIN | POLLRDNORM);
+	return mask;
+}
+
+
+/* Main switch loop for ioctl operations */
+static int fsl_pme2_scan_ioctl(struct inode *inode, struct file *fp,
+		unsigned int cmd, unsigned long arg)
+{
+	struct scan_session *session = fp->private_data;
+	int ret = 0;
+
+	switch (cmd) {
+
+	case PMEIO_GETSCAN:
+		return getscan_cmd(fp, session, (struct pme_scan_params *)arg);
+	break;
+
+	case PMEIO_SETSCAN:
+		return setscan_cmd(fp, session, (struct pme_scan_params *)arg);
+	break;
+
+	case PMEIO_RESETSEQ:
+		return resetseq_cmd(fp, session);
+	break;
+
+	case PMEIO_RESETRES:
+		return resetresidue_cmd(fp, session);
+	break;
+
+	case PMEIO_SCAN:
+	{
+		struct pme_scan *scan_cmd = (struct pme_scan *)arg;
+		return process_scan_cmd(fp, session, &scan_cmd->cmd,
+				&scan_cmd->result, 1);
+	}
+	break;
+
+	case PMEIO_SCAN_W1:
+	{
+		struct pme_scan_cmd *scan_cmd = (struct pme_scan_cmd *)arg;
+		return process_scan_cmd(fp, session, scan_cmd, NULL, 0);
+	}
+	break;
+
+	case PMEIO_SCAN_R1:
+	{
+		struct pme_scan_result *result = (struct pme_scan_result *)arg;
+		struct cmd_token *completed_cmd = NULL;
+		/* Check to see if any results */
+		spin_lock(&session->completed_commands_lock);
+		if (!list_empty(&session->completed_commands)) {
+			completed_cmd = list_first_entry(
+					&session->completed_commands,
+					struct cmd_token,
+					completed_list);
+			list_del(&completed_cmd->completed_list);
+			session->completed_count--;
+		}
+		spin_unlock(&session->completed_commands_lock);
+		if (completed_cmd)
+			return process_completed_token(fp, completed_cmd,
+					result);
+		else
+			return -EIO;
+	}
+	break;
+
+	case PMEIO_SCAN_Wn:
+	{
+		struct pme_scan_cmds scan_cmds;
+		int i = 0;
+		int ret = 0;
+		/* Copy the command to kernel space */
+		if (copy_from_user(&scan_cmds, (void *)arg, sizeof(scan_cmds)))
+			return -EFAULT;
+#ifdef CONFIG_FSL_PME2_SCAN_DEBUG
+		pr_info("pme2_scan: Received Wn for %d cmds \n", scan_cmds.num);
+#endif
+		for (i = 0; i < scan_cmds.num; i++) {
+			ret = process_scan_cmd(fp, session, &scan_cmds.cmds[i],
+					NULL, 0);
+			if (ret) {
+				pr_err("pme2_scan: Err with %d cmd %d\n",
+					i, ret);
+				scan_cmds.num = i;
+				if (copy_to_user((void *)arg, &scan_cmds,
+						sizeof(scan_cmds))) {
+					pr_err("Error copying to user data \n");
+					return -EFAULT;
+				}
+				return -EINTR;
+			}
+		}
+		return ret;
+	}
+	break;
+
+	case PMEIO_SCAN_Rn:
+	{
+		struct pme_scan_results results;
+		int i = 0, ret = 0;
+		struct cmd_token *completed_cmd = NULL;
+		/* Copy the command to kernel space */
+		if (copy_from_user(&results, (void *)arg, sizeof(results)))
+			return -EFAULT;
+#ifdef CONFIG_FSL_PME2_SCAN_DEBUG
+		pr_info("pme2_scan: Received Rn for %d res \n", results.num);
+#endif
+		if (!results.num)
+			return 0;
+		do {
+			completed_cmd = NULL;
+			ret = 0;
+			/* Check to see if any results */
+			spin_lock(&session->completed_commands_lock);
+			if (!list_empty(&session->completed_commands)) {
+				/* Move to a different list */
+#ifdef CONFIG_FSL_PME2_SCAN_DEBUG
+				pr_info("pme2_scan: Pop response\n");
+#endif
+				completed_cmd = list_first_entry(
+						&session->completed_commands,
+						struct cmd_token,
+						completed_list);
+				list_del(&completed_cmd->completed_list);
+				session->completed_count--;
+			}
+			spin_unlock(&session->completed_commands_lock);
+			if (completed_cmd) {
+				ret = process_completed_token(fp, completed_cmd,
+						results.results+i);
+				if (!ret)
+					i++;
+			}
+		} while (!ret && completed_cmd && (i != results.num));
+
+		if (i != results.num) {
+#ifdef CONFIG_FSL_PME2_SCAN_DEBUG
+			pr_info("pme2_scan: Only filled %d responses\n", i);
+#endif
+			results.num = i;
+#ifdef CONFIG_FSL_PME2_SCAN_DEBUG
+			pr_info("pme2_scan: results.num = %d\n", results.num);
+#endif
+			if (copy_to_user((void *)arg, &results,
+					sizeof(results))) {
+				pr_err("Error copying to user data \n");
+				return -EFAULT;
+			}
+		}
+		return ret;
+	}
+	break;
+
+	case PMEIO_RELEASE_BUFS:
+	default:
+		return -EINVAL;
+		break;
+	}
+	return ret;
+}
+
+static const struct file_operations fsl_pme2_scan_fops = {
+	.owner =	THIS_MODULE,
+	.ioctl =	fsl_pme2_scan_ioctl,
+	.open = 	fsl_pme2_scan_open,
+	.release =	fsl_pme2_scan_close,
+	.poll = 	fsl_pme2_scan_poll,
+};
+
+static struct miscdevice fsl_pme2_scan_dev = {
+	.minor = MISC_DYNAMIC_MINOR,
+	.name = PME_DEV_SCAN_NODE,
+	.fops = &fsl_pme2_scan_fops
+};
+
+static int __init fsl_pme2_scan_init(void)
+{
+	int err = 0;
+
+	pr_info("Freescale pme2 scan driver\n");
+	err = misc_register(&fsl_pme2_scan_dev);
+	if (err) {
+		pr_err("fsl-pme2-scan: cannot register device\n");
+		return err;
+	}
+	pr_info("fsl-pme2-san: device %s registered\n", fsl_pme2_scan_dev.name);
+	return 0;
+}
+
+static void __exit fsl_pme2_scan_exit(void)
+{
+	int err = misc_deregister(&fsl_pme2_scan_dev);
+	if (err)
+		pr_err("fsl-pme2-scan: Failed to deregister device %s, "
+				"code %d\n", fsl_pme2_scan_dev.name, err);
+	pr_info("fsl-pme2-scan: device %s deregistered\n",
+			fsl_pme2_scan_dev.name);
+}
+
+module_init(fsl_pme2_scan_init);
+module_exit(fsl_pme2_scan_exit);
+
+MODULE_AUTHOR("Jeffrey Ladouceur <jeffrey.ladouceur@freescale.com>");
+MODULE_LICENSE("Dual BSD/GPL");
+MODULE_DESCRIPTION("Freescale PME2 scan driver");
diff --git a/drivers/match/pme2_sys.h b/drivers/match/pme2_sys.h
new file mode 100644
index 0000000..a935c1e
--- /dev/null
+++ b/drivers/match/pme2_sys.h
@@ -0,0 +1,41 @@
+/* Copyright (c) 2008, 2009 Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/* same comments as for drivers/hwqueue/qman_sys.h */
+#if defined(CONFIG_FSL_QMAN_CHECKING) && !defined(CONFIG_FSL_BMAN_CHECKING)
+#define CONFIG_FSL_BMAN_CHECKING
+#elif !defined(CONFIG_FSL_QMAN_CHECKING) && defined(CONFIG_FSL_BMAN_CHECKING)
+#undef CONFIG_FSL_BMAN_CHECKING
+#endif
+#define PME_ASSERT(x) BM_ASSERT(x)
+#include "../hwalloc/bman_sys.h"
+
diff --git a/drivers/match/pme2_test.h b/drivers/match/pme2_test.h
new file mode 100644
index 0000000..5a6576a
--- /dev/null
+++ b/drivers/match/pme2_test.h
@@ -0,0 +1,84 @@
+/* Copyright (c) 2008, 2009, Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/io.h>
+#include <linux/dma-mapping.h>
+#include <linux/slab.h>
+#include <linux/module.h>
+#include <linux/interrupt.h>
+#include <linux/delay.h>
+#include <linux/completion.h>
+
+#include <linux/fsl_pme.h>
+#include <linux/fsl_bman.h>
+
+static inline void __hexdump(unsigned long start, unsigned long end,
+			unsigned long p, size_t sz, const unsigned char *c)
+{
+	while (start < end) {
+		unsigned int pos = 0;
+		char buf[64];
+		int nl = 0;
+		pos += sprintf(buf + pos, "%08lx: ", start);
+		do {
+			if ((start < p) || (start >= (p + sz)))
+				pos += sprintf(buf + pos, "..");
+			else
+				pos += sprintf(buf + pos, "%02x", *(c++));
+			if (!(++start & 15)) {
+				buf[pos++] = '\n';
+				nl = 1;
+			} else {
+				nl = 0;
+				if(!(start & 1))
+					buf[pos++] = ' ';
+				if(!(start & 3))
+					buf[pos++] = ' ';
+			}
+		} while (start & 15);
+		if (!nl)
+			buf[pos++] = '\n';
+		buf[pos] = '\0';
+		pr_info("%s", buf);
+	}
+}
+static inline void hexdump(const void *ptr, size_t sz)
+{
+	unsigned long p = (unsigned long)ptr;
+	unsigned long start = p & ~(unsigned long)15;
+	unsigned long end = (p + sz + 15) & ~(unsigned long)15;
+	const unsigned char *c = ptr;
+	__hexdump(start, end, p, sz, c);
+}
+
diff --git a/drivers/match/pme2_test_high.c b/drivers/match/pme2_test_high.c
new file mode 100644
index 0000000..2a99fd4
--- /dev/null
+++ b/drivers/match/pme2_test_high.c
@@ -0,0 +1,138 @@
+/* Copyright (c) 2008, 2009, Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "pme2_test.h"
+
+MODULE_AUTHOR("Geoff Thorpe");
+MODULE_LICENSE("Dual BSD/GPL");
+MODULE_DESCRIPTION("FSL PME2 (p4080) high-level self-test");
+
+/* Default Flow Context State */
+static u8 fl_ctx_exp[]={
+	0x88,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,
+	0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0xff,0xff,0xff,0xff,
+	0x00,0x00,0x00,0x00
+};
+
+void scan_cb(struct pme_ctx *ctx, const struct qm_fd *fd,
+		struct pme_ctx_token *token)
+{
+	pr_info("pme2_test_high: scan_cb() invoked, fd;!\n");
+	hexdump(fd, sizeof(*fd));
+}
+
+void pme2_test_high(void)
+{
+	struct pme_flow *flow;
+	struct qm_fqd_stashing stashing;
+	struct pme_ctx ctx = {
+		.cb = scan_cb
+	};
+	int ret;
+
+	flow = pme_sw_flow_new();
+	BUG_ON(!flow);
+	pr_info("PME2: high-level test starting\n");
+
+	ret = pme_ctx_init(&ctx, PME_CTX_FLAG_LOCAL, 0, 4, 4, 0, NULL);
+	BUG_ON(ret);
+	/* enable the context */
+	pme_ctx_enable(&ctx);
+	/* read back flow settings */
+	ret = pme_ctx_ctrl_read_flow(&ctx,
+		PME_CTX_OP_WAIT | PME_CTX_OP_WAIT_INT | PME_CMD_FCW_ALL, flow);
+	BUG_ON(ret);
+	if (memcmp(flow,fl_ctx_exp, sizeof(*flow))!= 0) {
+		pr_info("Default Flow Context Read FAIL\n");
+		pr_info("Expected:\n");
+		hexdump(fl_ctx_exp, sizeof(fl_ctx_exp));
+		pr_info("Received:\n");
+		hexdump(flow, sizeof(*flow));
+		BUG_ON(1);
+	} else
+		pr_info("Default Flow Context Read OK\n");
+	/* start a non-blocking NOP */
+	ret = pme_ctx_ctrl_nop(&ctx, 0);
+	BUG_ON(ret);
+	/* start a blocking update (implicitly blocks on NOP-completion) to add
+	 * residue to the context */
+	flow->ren = 1;
+	ret = pme_ctx_ctrl_update_flow(&ctx,
+		PME_CTX_OP_WAIT | PME_CTX_OP_WAIT_INT | PME_CMD_FCW_RES, flow);
+	BUG_ON(ret);
+	/* start a blocking disable */
+	ret = pme_ctx_disable(&ctx, PME_CTX_OP_WAIT | PME_CTX_OP_WAIT_INT);
+	BUG_ON(ret);
+	/* do some reconfiguration */
+	ret = pme_ctx_reconfigure_tx(&ctx, 63, 7);
+	BUG_ON(ret);
+	stashing.exclusive = 0;
+	stashing.annotation_cl = 0;
+	stashing.data_cl = 2;
+	stashing.context_cl = 2;
+	ret = pme_ctx_reconfigure_rx(&ctx, 7, 0, &stashing);
+	BUG_ON(ret);
+	/* reenable */
+	ret = pme_ctx_enable(&ctx);
+	BUG_ON(ret);
+	/* read back flow settings */
+	ret = pme_ctx_ctrl_read_flow(&ctx,
+		PME_CTX_OP_WAIT | PME_CTX_OP_WAIT_INT | PME_CMD_FCW_RES, flow);
+	BUG_ON(ret);
+	pr_info("read Flow Context;\n");
+	hexdump(flow, sizeof(*flow));
+	/* blocking NOP */
+	ret = pme_ctx_ctrl_nop(&ctx, PME_CTX_OP_WAIT | PME_CTX_OP_WAIT_INT);
+	BUG_ON(ret);
+	/* Disable, and done */
+	ret = pme_ctx_disable(&ctx, PME_CTX_OP_WAIT | PME_CTX_OP_WAIT_INT);
+	BUG_ON(ret);
+	pme_ctx_finish(&ctx);
+	pme_sw_flow_free(flow);
+	pr_info("PME2: high-level test done\n");
+}
+
+static int pme2_test_high_init(void)
+{
+	int big_loop = 2;
+	while (big_loop--)
+		pme2_test_high();
+	return 0;
+}
+
+static void pme2_test_high_exit(void)
+{
+}
+
+module_init(pme2_test_high_init);
+module_exit(pme2_test_high_exit);
+
diff --git a/drivers/match/pme2_test_low.c b/drivers/match/pme2_test_low.c
new file mode 100644
index 0000000..0cea9a2
--- /dev/null
+++ b/drivers/match/pme2_test_low.c
@@ -0,0 +1,328 @@
+/* Copyright (c) 2008, 2009, Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "pme2_test.h"
+
+MODULE_AUTHOR("Geoff Thorpe");
+MODULE_LICENSE("Dual BSD/GPL");
+MODULE_DESCRIPTION("FSL PME2 (p4080) low-level self-test");
+
+#define TTOKEN		0xc3
+
+/* DQRR maxfill, and ring/data stashing booleans */
+#define DQRR_MAXFILL	15
+#define DQRR_STASH_RING	0
+#define DQRR_STASH_DATA	0
+
+static struct qm_portal *portal;
+static struct qm_eqcr_entry *eq;
+static struct qm_dqrr_entry *dq;
+static struct qm_mc_command *mcc;
+static struct qm_mc_result *mcr;
+
+/* Helper for qm_mc_start() that checks the return code */
+static void mc_start(void)
+{
+	mcc = qm_mc_start(portal);
+	BUG_ON(!mcc);
+}
+
+/* Helper for qm_mc_result() that checks the response */
+static void mc_commit(u8 verb)
+{
+	qm_mc_commit(portal, verb);
+	do {
+		mcr = qm_mc_result(portal);
+	} while (!mcr);
+	pr_info("command:\n");
+	hexdump(mcc, sizeof(*mcc));
+	pr_info("response:\n");
+	hexdump(mcr, sizeof(*mcr));
+	BUG_ON((mcr->verb & QM_MCR_VERB_MASK) != verb);
+	BUG_ON(mcr->result != QM_MCR_RESULT_OK);
+}
+
+static struct qm_fd backup;
+
+static void eqcr_update(void)
+{
+	qm_eqcr_cci_update(portal);
+}
+
+static void eqcr_start(void)
+{
+	#define SEED_INC 0xb9
+	static u8 seeder = 0xff;
+	eqcr_update();
+	do {
+		eq = qm_eqcr_start(portal);
+	} while (!eq);
+	memset(&eq->fd, seeder, sizeof(eq->fd));
+	/* Qman apparently pays attention to format/length because of BYTE_CNT,
+	 * so do the minimum to keep it happy (in particular, crop the length so
+	 * that we don't hit overflow). */
+	eq->fd.format = qm_fd_contig_big;
+	eq->fd.length29 &= 0xffff;
+	do {
+		seeder += SEED_INC;
+	} while (!seeder);
+}
+
+static void eqcr_commit(void)
+{
+	backup = eq->fd;
+	hexdump(eq, sizeof(*eq));
+	qm_eqcr_pci_commit(portal, QM_EQCR_VERB_CMD_ENQUEUE);
+}
+
+/* Macro'd so the BUG_ON() gives us the caller location */
+#define dqrr_poll(check) \
+do { \
+	do { \
+		qm_dqrr_pvb_update(portal); \
+		dq = qm_dqrr_current(portal); \
+	} while (!dq); \
+	hexdump(dq, sizeof(*dq)); \
+	if (check) { \
+		backup.pid = dq->fd.pid; \
+		backup.status = dq->fd.status; \
+		BUG_ON(memcmp(&backup, &dq->fd, sizeof(backup))); \
+	} \
+} while(0)
+
+static void dqrr_consume_and_next(void)
+{
+	qm_dqrr_next(portal);
+	qm_dqrr_cci_consume_to_current(portal);
+	dq = qm_dqrr_current(portal);
+}
+
+void pme2_test_low(struct qm_portal *__portal)
+{
+	const struct qm_portal_config *config;
+	struct pme_hw_flow *hwflow = pme_hw_flow_new();
+	struct pme_flow *test_flow = pme_sw_flow_new();
+	struct pme_flow *test_flow2 = pme_sw_flow_new();
+	u32 fqid_rx = qm_fq_new();
+	u32 fqid_tx = qm_fq_new();
+
+	pr_info("PME2: low-level test starting\n");
+	BUG_ON(!hwflow || !test_flow || !test_flow2 || !fqid_rx || !fqid_tx);
+	portal = __portal;
+	config = qm_portal_config(portal);
+	if (qm_eqcr_init(portal, qm_eqcr_pci, qm_eqcr_cci) ||
+		qm_dqrr_init(portal,
+			qm_dqrr_dpush, qm_dqrr_pvb, qm_dqrr_cci,
+			DQRR_MAXFILL, DQRR_STASH_RING, DQRR_STASH_DATA) ||
+		qm_mr_init(portal, qm_mr_pvb, qm_mr_cci) ||
+		qm_mc_init(portal) || qm_isr_init(portal))
+		panic("Portal setup failed");
+	qm_dqrr_sdqcr_set(portal, QM_SDQCR_SOURCE_CHANNELS |
+		QM_SDQCR_TYPE_PRIO_QOS | QM_SDQCR_TOKEN_SET(TTOKEN) |
+		QM_SDQCR_CHANNELS_DEDICATED);
+	/* Initialise output FQ and schedule it */
+	mc_start();
+	mcc->initfq.we_mask = QM_INITFQ_WE_DESTWQ | QM_INITFQ_WE_CONTEXTB;
+	mcc->initfq.fqid = fqid_tx;
+	mcc->initfq.count = 0;
+	mcc->initfq.fqd.dest.channel = config->channel;
+	mcc->initfq.fqd.dest.wq = 3;
+	mcc->initfq.fqd.context_b = 0xdeadbeef;
+	pr_info("Init output FQ\n");
+	mc_commit(QM_MCC_VERB_INITFQ_SCHED);
+	/* Initialise input FQ for Direct Action mode and schedule it */
+	mc_start();
+	pme_initfq(&mcc->initfq, NULL, 3, 0, fqid_tx);
+	mcc->initfq.fqid = fqid_rx;
+	mcc->initfq.count = 0;
+	pr_info("Init input FQ\n");
+	mc_commit(QM_MCC_VERB_INITFQ_SCHED);
+	/* Send a NOP */
+	eqcr_start();
+	eq->fqid = fqid_rx;
+	pme_fd_cmd_nop(&eq->fd);
+	pr_info("Enqueuing NOP\n");
+	eqcr_commit();
+	/* Poll for the NOP */
+	pr_info("Dequeuing NOP\n");
+	dqrr_poll(0);
+	BUG_ON((dq->verb & QM_DQRR_VERB_MASK) != QM_DQRR_VERB_FRAME_DEQUEUE);
+	BUG_ON(dq->stat != (QM_DQRR_STAT_FQ_EMPTY | QM_DQRR_STAT_FD_VALID));
+	BUG_ON(dq->tok != TTOKEN);
+	BUG_ON(dq->fqid != fqid_tx);
+	BUG_ON(dq->contextB != 0xdeadbeef);
+	BUG_ON(pme_fd_res_status(&dq->fd) != pme_status_ok);
+	BUG_ON(pme_fd_res_flags(&dq->fd) & (PME_STATUS_UNRELIABLE |
+				PME_STATUS_TRUNCATED));
+	dqrr_consume_and_next();
+	/* Retire and OOS the input FQ */
+	mc_start();
+	mcc->alterfq.fqid = fqid_rx;
+	pr_info("Retire input FQ\n");
+	mc_commit(QM_MCC_VERB_ALTER_RETIRE);
+	BUG_ON(mcr->alterfq.fqs & (QM_MCR_FQS_ORLPRESENT |
+				QM_MCR_FQS_NOTEMPTY));
+	mc_start();
+	mcc->alterfq.fqid = fqid_rx;
+	pr_info("OOS input FQ\n");
+	mc_commit(QM_MCC_VERB_ALTER_OOS);
+	/* Initialise input FQ for Flow mode and schedule it */
+	mc_start();
+	pme_initfq(&mcc->initfq, hwflow, 3, 0, fqid_tx);
+	mcc->initfq.fqid = fqid_rx;
+	mcc->initfq.count = 0;
+	pr_info("Init input FQ\n");
+	mc_commit(QM_MCC_VERB_INITFQ_SCHED);
+	/* Send a FCW to initialise the flow */
+	memset(test_flow, 0, sizeof(*test_flow));
+	test_flow->srvm = 2;
+	test_flow->seqnum_hi = 0xabba;
+	test_flow->seqnum_lo = 0xdeadbeef;
+	test_flow->clim = 0xabcd;
+	test_flow->mlim = 0x9876;
+	eqcr_start();
+	eq->fqid = fqid_rx;
+	pme_fd_cmd_fcw(&eq->fd, PME_CMD_FCW_ALL, test_flow, NULL);
+	pr_info("Enqueuing FCW\n");
+	eqcr_commit();
+	/* Poll for the FCW response */
+	pr_info("Dequeuing FCW\n");
+	dqrr_poll(1);
+/*	BUG_ON((dq->stat & QM_DQRR_VERB_MASK) != (QM_DQRR_STAT_FQ_EMPTY |
+						QM_DQRR_STAT_FD_VALID)); */
+	BUG_ON(dq->tok != TTOKEN);
+	BUG_ON(dq->fqid != fqid_tx);
+	BUG_ON(dq->contextB != 0xdeadbeef);
+	BUG_ON(pme_fd_res_status(&dq->fd) != pme_status_ok);
+	BUG_ON(pme_fd_res_flags(&dq->fd) & (PME_STATUS_UNRELIABLE |
+				PME_STATUS_TRUNCATED));
+	dqrr_consume_and_next();
+	/* Send a FCR to read the flow back */
+	memset(test_flow2, 0xff, sizeof(*test_flow2));
+	eqcr_start();
+	eq->fqid = fqid_rx;
+	pme_fd_cmd_fcr(&eq->fd, test_flow2);
+	pr_info("Enqueuing FCR\n");
+	eqcr_commit();
+	/* Poll for the FCR response */
+	pr_info("Dequeuing FCR\n");
+	dqrr_poll(1);
+/*	BUG_ON((dq->stat & QM_DQRR_VERB_MASK) != (QM_DQRR_STAT_FQ_EMPTY |
+						QM_DQRR_STAT_FD_VALID)); */
+	BUG_ON(dq->tok != TTOKEN);
+	BUG_ON(dq->fqid != fqid_tx);
+	BUG_ON(dq->contextB != 0xdeadbeef);
+	BUG_ON(pme_fd_res_status(&dq->fd) != pme_status_ok);
+	BUG_ON(pme_fd_res_flags(&dq->fd) & (PME_STATUS_UNRELIABLE |
+				PME_STATUS_TRUNCATED));
+	dqrr_consume_and_next();
+	/* Send a NOP through the flow */
+	eqcr_start();
+	eq->fqid = fqid_rx;
+	pme_fd_cmd_nop(&eq->fd);
+	pr_info("Enqueuing NOP\n");
+	eqcr_commit();
+	/* Poll for the NOP response */
+	pr_info("Dequeuing NOP\n");
+	dqrr_poll(0);
+/*	BUG_ON((dq->stat & QM_DQRR_VERB_MASK) != (QM_DQRR_STAT_FQ_EMPTY |
+						QM_DQRR_STAT_FD_VALID)); */
+	BUG_ON(dq->tok != TTOKEN);
+	BUG_ON(dq->fqid != fqid_tx);
+	BUG_ON(dq->contextB != 0xdeadbeef);
+	BUG_ON(pme_fd_res_status(&dq->fd) != pme_status_ok);
+	BUG_ON(pme_fd_res_flags(&dq->fd) & (PME_STATUS_UNRELIABLE |
+				PME_STATUS_TRUNCATED));
+	dqrr_consume_and_next();
+	/* Retire and OOS the input FQ */
+	mc_start();
+	mcc->alterfq.fqid = fqid_rx;
+	pr_info("Retire input FQ\n");
+	mc_commit(QM_MCC_VERB_ALTER_RETIRE);
+	BUG_ON(mcr->alterfq.fqs & (QM_MCR_FQS_ORLPRESENT |
+				QM_MCR_FQS_NOTEMPTY));
+	mc_start();
+	mcc->alterfq.fqid = fqid_rx;
+	pr_info("OOS input FQ\n");
+	mc_commit(QM_MCC_VERB_ALTER_OOS);
+	/* Retire and OOS the output FQ */
+	mc_start();
+	mcc->alterfq.fqid = fqid_tx;
+	pr_info("Retire output FQ\n");
+	mc_commit(QM_MCC_VERB_ALTER_RETIRE);
+	BUG_ON(mcr->alterfq.fqs & (QM_MCR_FQS_ORLPRESENT |
+				QM_MCR_FQS_NOTEMPTY));
+	mc_start();
+	mcc->alterfq.fqid = fqid_tx;
+	pr_info("OOS output FQ\n");
+	mc_commit(QM_MCC_VERB_ALTER_OOS);
+
+	eqcr_update();
+	qm_eqcr_finish(portal);
+	qm_dqrr_finish(portal);
+	qm_mr_finish(portal);
+	qm_mc_finish(portal);
+	qm_isr_finish(portal);
+	pr_info("PME2: low-level test done\n");
+}
+
+static int pme2_test_low_init(void)
+{
+	int big_loop = 2;
+	u8 num = qm_portal_num();
+	const struct qm_portal_config *config = NULL;
+	struct qm_portal *portal;
+
+	while (!config && (num-- > 0)) {
+		portal = qm_portal_get(num);
+		config = qm_portal_config(portal);
+		if (!config->bound)
+			pr_info("Portal %d is available, using it\n", num);
+		else
+			config = NULL;
+	}
+	if (!config) {
+		pr_err("No Qman portals available!\n");
+		return -ENOSYS;
+	}
+	while (big_loop--)
+		pme2_test_low(portal);
+	return 0;
+}
+
+static void pme2_test_low_exit(void)
+{
+}
+
+module_init(pme2_test_low_init);
+module_exit(pme2_test_low_exit);
+
diff --git a/drivers/match/pme2_test_scan.c b/drivers/match/pme2_test_scan.c
new file mode 100644
index 0000000..2ee815e
--- /dev/null
+++ b/drivers/match/pme2_test_scan.c
@@ -0,0 +1,478 @@
+/* Copyright (c) 2009, Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "pme2_test.h"
+
+static u8 scan_data[] = {
+	0x41,0x42,0x43,0x44,0x45
+};
+
+static u8 result_data[] = {
+	0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,
+	0x00,0x00,0x00
+};
+
+static u8 scan_result_direct_mode_inc_mode[] = {
+	0x01,0x01,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x01,0x00,0x00,
+	0x00,0x00
+};
+
+static u8 fl_ctx_exp[] = {
+	0x88,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,
+	0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0x00,0xff,0xff,0xff,0xff,
+	0x00,0x00,0x00,0x00
+};
+
+struct scan_ctx {
+	struct pme_ctx base_ctx;
+	struct qm_fd result_fd;
+};
+
+static struct bman_pool *pool;
+static u32 pme_bpid;
+static void *bman_buffers_virt_base;
+static dma_addr_t bman_buffers_phys_base;
+static DECLARE_COMPLETION(scan_comp);
+
+static void scan_cb(struct pme_ctx *ctx, const struct qm_fd *fd,
+		struct pme_ctx_token *ctx_token)
+{
+	struct scan_ctx *my_ctx = (struct scan_ctx *)ctx;
+	pr_info("st: scan_cb() invoked, fd;!\n");
+	hexdump(fd, sizeof(*fd));
+	memcpy(&my_ctx->result_fd, fd, sizeof(*fd));
+	complete(&scan_comp);
+}
+
+static void release_buffer(dma_addr_t addr)
+{
+	struct bm_buffer bufs_in;
+
+	memset(&bufs_in, 0, sizeof(struct bm_buffer));
+	bufs_in.lo = addr;
+	if (bman_release(pool, &bufs_in, 1, BMAN_RELEASE_FLAG_WAIT))
+		panic("bman_release() failed\n");
+}
+
+static void empty_buffer(void)
+{
+	struct bm_buffer bufs_in;
+	int ret;
+
+	do {
+		ret = bman_acquire(pool, &bufs_in, 1, 0);
+		if (!ret)
+			pr_info("st: Acquired buffer\n");
+	} while (!ret);
+}
+
+
+void pme2_test_scan(void)
+{
+	struct pme_flow *flow;
+	struct scan_ctx a_scan_ctx = {
+		.base_ctx = {
+			.cb = scan_cb
+		}
+	};
+	struct qm_fd fd;
+	struct qm_sg_entry sg_table[2];
+	int ret;
+	enum pme_status status;
+	struct pme_ctx_token token;
+	u8 *scan_result;
+	u32 scan_result_size;
+
+	scan_result = scan_result_direct_mode_inc_mode;
+	scan_result_size = sizeof(scan_result_direct_mode_inc_mode);
+
+	/**********************************************************************/
+	/**********************************************************************/
+	/********************* Direct Mode ************************************/
+	/**********************************************************************/
+	/**********************************************************************/
+	ret = pme_ctx_init(&a_scan_ctx.base_ctx,
+			PME_CTX_FLAG_DIRECT | PME_CTX_FLAG_LOCAL,
+			0, 4, 4, 0, NULL);
+	BUG_ON(ret);
+	/* enable the context */
+	pme_ctx_enable(&a_scan_ctx.base_ctx);
+
+	/* Do a pre-built output, scan with match test */
+	/* Build a frame descriptor */
+	memset(&fd, 0, sizeof(struct qm_fd));
+	memset(&sg_table, 0, sizeof(sg_table));
+
+	/* build the result */
+	sg_table[0].addr_lo = pme_map(result_data);
+	sg_table[0].length = sizeof(result_data);
+	sg_table[1].addr_lo = pme_map(scan_data);
+	sg_table[1].length = sizeof(scan_data);
+	sg_table[1].final = 1;
+
+	fd._format2 = qm_fd_compound;
+	fd.addr_lo = pme_map(sg_table);
+
+	pr_info("st: Send scan request\n");
+	ret = pme_ctx_scan(&a_scan_ctx.base_ctx, 0, &fd,
+		PME_SCAN_ARGS(PME_CMD_SCAN_SR | PME_CMD_SCAN_E, 0, 0xff00),
+		&token);
+	pr_info("st: Response scan %d\n", ret);
+	wait_for_completion(&scan_comp);
+
+	pr_info("st: Status is fd.status %x\n",
+			a_scan_ctx.result_fd.status);
+
+	status = pme_fd_res_status(&a_scan_ctx.result_fd);
+	if (status) {
+		pr_info("st: Scan status failed %d\n", status);
+		BUG_ON(1);
+	}
+	if (memcmp(scan_result,result_data, scan_result_size) != 0) {
+		pr_info("st: Scan result not expected, Expected\n");
+		hexdump(scan_result, scan_result_size);
+		pr_info("st: Received...\n");
+		hexdump(result_data, sizeof(result_data));
+		BUG_ON(1);
+	}
+	/* Test truncation test */
+
+	/* Build a frame descriptor */
+	memset(&fd, 0, sizeof(struct qm_fd));
+	fd.length20 = sizeof(scan_data);
+	fd.addr_lo = pme_map(scan_data);
+
+	pr_info("st: Send scan request\n");
+
+	ret = pme_ctx_scan(&a_scan_ctx.base_ctx, 0, &fd,
+		PME_SCAN_ARGS(PME_CMD_SCAN_SR | PME_CMD_SCAN_E, 0, 0xff00),
+		&token);
+
+	pr_info("st: Response scan %d\n", ret);
+	wait_for_completion(&scan_comp);
+
+	pr_info("st: Status is fd.status %x\n",
+			a_scan_ctx.result_fd.status);
+
+	status = pme_fd_res_status(&a_scan_ctx.result_fd);
+	pr_info("st: Scan status %x\n", status);
+
+	/* Check the response...expect truncation bit to be set */
+	if (!(pme_fd_res_flags(&a_scan_ctx.result_fd) & PME_STATUS_TRUNCATED)) {
+		pr_info("st: Scan result failed...expected trunc\n");
+		BUG_ON(1);
+	}
+	pr_info("st: Simple scan test Passed\n");
+
+	/* Disable */
+	ret = pme_ctx_disable(&a_scan_ctx.base_ctx, PME_CTX_OP_WAIT);
+	BUG_ON(ret);
+
+	/* Check with bman */
+#if CONFIG_FSL_PME2_TEST_SCAN_WITH_BPID
+	/* reconfigure */
+	ret = pme_ctx_reconfigure_tx(&a_scan_ctx.base_ctx, pme_bpid, 5);
+	BUG_ON(ret);
+	ret = pme_ctx_enable(&a_scan_ctx.base_ctx);
+	BUG_ON(ret);
+	/* Do a pre-built output, scan with match test */
+	/* Build a frame descriptor */
+	memset(&fd, 0, sizeof(struct qm_fd));
+	memset(&sg_table, 0, sizeof(sg_table));
+
+	/* build the result */
+	/* result is all zero...use bman */
+	sg_table[1].addr_lo = pme_map(scan_data);
+	sg_table[1].length = sizeof(scan_data);
+	sg_table[1].final = 1;
+
+	fd._format2 = qm_fd_compound;
+	fd.addr_lo = pme_map(sg_table);
+
+	pr_info("st: Send scan with bpid response request\n");
+	ret = pme_ctx_scan(&a_scan_ctx.base_ctx, 0, &fd,
+		PME_SCAN_ARGS(PME_CMD_SCAN_SR | PME_CMD_SCAN_E, 0, 0xff00),
+		&token);
+	pr_info("st: Response scan %d\n", ret);
+	wait_for_completion(&scan_comp);
+
+	pr_info("st: Status is fd.status %x\n",
+			a_scan_ctx.result_fd.status);
+
+	status = pme_fd_res_status(&a_scan_ctx.result_fd);
+	if (status) {
+		pr_info("st: Scan status failed %d\n", status);
+		BUG_ON(1);
+	}
+
+	/* sg result should point to bman buffer */
+	pr_info("st: sg result should point to bman buffer 0x%x\n",
+			sg_table[0].addr_lo);
+	if (!sg_table[0].addr_lo)
+		BUG_ON(1);
+
+	if (memcmp(scan_result, bman_buffers_virt_base, scan_result_size)
+			!= 0) {
+		pr_info("st: Scan result not expected, Expected\n");
+		hexdump(scan_result, scan_result_size);
+		pr_info("st: Received...\n");
+		hexdump(bman_buffers_virt_base, scan_result_size);
+		BUG_ON(1);
+	}
+
+	release_buffer(sg_table[0].addr_lo);
+	pr_info("st: Released to bman\n");
+
+	/* Disable */
+	ret = pme_ctx_disable(&a_scan_ctx.base_ctx, PME_CTX_OP_WAIT);
+	BUG_ON(ret);
+#endif
+	pme_ctx_finish(&a_scan_ctx.base_ctx);
+
+	/**********************************************************************/
+	/**********************************************************************/
+	/*********************** Flow Mode ************************************/
+	/**********************************************************************/
+	/**********************************************************************/
+	pr_info("st: Start Flow Mode Test\n");
+
+	flow = pme_sw_flow_new();
+	BUG_ON(!flow);
+	ret = pme_ctx_init(&a_scan_ctx.base_ctx,
+		PME_CTX_FLAG_LOCAL, 0, 4, 4, 0, NULL);
+	BUG_ON(ret);
+
+	/* enable the context */
+	pme_ctx_enable(&a_scan_ctx.base_ctx);
+	pr_info("st: Context Enabled\n");
+
+	/* read back flow settings */
+	{
+		struct pme_flow* rb_flow;
+		rb_flow = pme_sw_flow_new();
+		memset(rb_flow, 0, sizeof(struct pme_flow));
+		pr_info("st: Initial rb_flow\n");
+		hexdump(rb_flow, sizeof(*rb_flow));
+
+		ret = pme_ctx_ctrl_read_flow(&a_scan_ctx.base_ctx,
+			PME_CTX_OP_WAIT |
+			PME_CTX_OP_WAIT_INT |
+			PME_CMD_FCW_ALL, rb_flow);
+		BUG_ON(ret);
+		if (memcmp(rb_flow,fl_ctx_exp, sizeof(*rb_flow)) != 0) {
+			pr_info("st: Flow Context Read FAIL\n");
+			pr_info("st: Expected\n");
+			hexdump(fl_ctx_exp, sizeof(fl_ctx_exp));
+			pr_info("st: Received...\n");
+			hexdump(rb_flow, sizeof(*rb_flow));
+			BUG_ON(1);
+		} else {
+			pr_info("st: Flow Context Read OK\n");
+		}
+		pme_sw_flow_free(rb_flow);
+	}
+
+
+	/* Do a pre-built output, scan with match test */
+	/* Build a frame descriptor */
+	memset(&fd, 0, sizeof(struct qm_fd));
+	memset(&sg_table, 0, sizeof(sg_table));
+
+	/* build the result */
+	sg_table[0].addr_lo = pme_map(result_data);
+	sg_table[0].length = sizeof(result_data);
+	sg_table[1].addr_lo = pme_map(scan_data);
+	sg_table[1].length = sizeof(scan_data);
+	sg_table[1].final = 1;
+
+	fd._format2 = qm_fd_compound;
+	fd.addr_lo = pme_map(sg_table);
+
+	pr_info("st: Send scan request\n");
+	ret = pme_ctx_scan(&a_scan_ctx.base_ctx, 0, &fd,
+		PME_SCAN_ARGS(PME_CMD_SCAN_SR | PME_CMD_SCAN_E, 0, 0xff00),
+		&token);
+
+	pr_info("st: Response scan %d\n", ret);
+	wait_for_completion(&scan_comp);
+
+	pr_info("st: Status is fd.status %x\n",
+			a_scan_ctx.result_fd.status);
+
+	status = pme_fd_res_status(&a_scan_ctx.result_fd);
+	if (status) {
+		pr_info("st: Scan status failed %d\n", status);
+		BUG_ON(1);
+	}
+
+	if (memcmp(scan_result,result_data, scan_result_size) != 0) {
+		pr_info("st: Scan result not expected, Expected\n");
+		hexdump(scan_result, scan_result_size);
+		pr_info("st: Received...\n");
+		hexdump(result_data, sizeof(result_data));
+		BUG_ON(1);
+	}
+
+	/* read back flow settings */
+	{
+		struct pme_flow *rb_flow;
+		rb_flow = pme_sw_flow_new();
+		memset(rb_flow, 0, sizeof(struct pme_flow));
+		pr_info("st: Initial rb_flow\n");
+		hexdump(rb_flow, sizeof(*rb_flow));
+
+		ret = pme_ctx_ctrl_read_flow(&a_scan_ctx.base_ctx,
+			PME_CTX_OP_WAIT |
+			PME_CTX_OP_WAIT_INT |
+			PME_CMD_FCW_ALL, rb_flow);
+		BUG_ON(ret);
+		if (memcmp(rb_flow,fl_ctx_exp, sizeof(*rb_flow)) != 0) {
+			pr_info("st: Flow Context Read FAIL\n");
+			pr_info("st: Expected\n");
+			hexdump(fl_ctx_exp, sizeof(fl_ctx_exp));
+			pr_info("st: Received\n");
+			hexdump(rb_flow, sizeof(*rb_flow));
+			BUG_ON(1);
+		} else {
+			pr_info("st: Flow Context Read OK\n");
+		}
+		pme_sw_flow_free(rb_flow);
+	}
+	/* Test truncation test */
+	/* Build a frame descriptor */
+	memset(&fd, 0, sizeof(struct qm_fd));
+
+	fd.length20 = sizeof(scan_data);
+	fd.addr_lo = pme_map(scan_data);
+
+	pr_info("st: Send scan request\n");
+	ret = pme_ctx_scan(&a_scan_ctx.base_ctx, 0, &fd,
+		PME_SCAN_ARGS(PME_CMD_SCAN_SR | PME_CMD_SCAN_E, 0, 0xff00),
+		&token);
+
+	pr_info("st: Response scan %d\n", ret);
+	wait_for_completion(&scan_comp);
+
+	pr_info("st: Status is fd.status %x\n",
+			a_scan_ctx.result_fd.status);
+
+	status = pme_fd_res_status(&a_scan_ctx.result_fd);
+	 pr_info("Scan status %x\n",status);
+
+	/* Check the response...expect truncation bit to be set */
+	if (!(pme_fd_res_flags(&a_scan_ctx.result_fd) & PME_STATUS_TRUNCATED)) {
+		pr_info("st: Scan result failed...expected trunc\n");
+		BUG_ON(1);
+	}
+	pr_info("st: Simple scan test Passed\n");
+
+	/* read back flow settings */
+	{
+		struct pme_flow *rb_flow;
+		rb_flow = kmalloc(sizeof(struct pme_flow), GFP_KERNEL);
+		memset(rb_flow, 0, sizeof(struct pme_flow));
+		ret = pme_ctx_ctrl_read_flow(&a_scan_ctx.base_ctx,
+			PME_CTX_OP_WAIT |
+			PME_CTX_OP_WAIT_INT |
+			PME_CMD_FCW_ALL, rb_flow);
+		BUG_ON(ret);
+		pr_info("st: read Flow Context;\n");
+		hexdump(rb_flow, sizeof(*rb_flow));
+		kfree(rb_flow);
+	}
+
+	/* Disable */
+	ret = pme_ctx_disable(&a_scan_ctx.base_ctx,
+			PME_CTX_OP_WAIT | PME_CTX_OP_WAIT_INT);
+	BUG_ON(ret);
+	pme_ctx_finish(&a_scan_ctx.base_ctx);
+	pme_sw_flow_free(flow);
+
+	pr_info("st: Scan Test Passed\n");
+}
+
+static int pme2_test_scan_init(void)
+{
+	int big_loop = 2;
+#if CONFIG_FSL_PME2_TEST_SCAN_WITH_BPID
+	u32 bpid_size = CONFIG_FSL_PME2_TEST_SCAN_WITH_BPID_SIZE;
+
+	struct bman_pool_params pparams = {
+		.flags = BMAN_POOL_FLAG_DYNAMIC_BPID,
+		.thresholds = {
+			0,
+			0,
+			0,
+			0
+		}
+	};
+	pr_info("st: About to allocate bpool\n");
+	pool = bman_new_pool(&pparams);
+	if (!pool) {
+		pr_err("st: can't allocate buffer pool, not the ctrl-plane\n");
+		return 0;
+	}
+	BUG_ON(!pool);
+	pme_bpid = bman_get_params(pool)->bpid;
+	pr_info("st: Allocate buffer pool id %d\n", pme_bpid);
+	pr_info("st: Allocate buffer of size %d\n", 1<<(bpid_size+5));
+	bman_buffers_virt_base = kmalloc(1<<(bpid_size+5), GFP_KERNEL);
+	bman_buffers_phys_base = pme_map(bman_buffers_virt_base);
+	pr_info("st: virt address %p\n", bman_buffers_virt_base);
+	pr_info("st: physical address 0x%x\n", bman_buffers_phys_base);
+	pr_info("st: Allocate buffer of size 0x%x\n", 1<<(bpid_size+5));
+
+	release_buffer(bman_buffers_phys_base);
+	pr_info("st: Released to bman\n");
+
+	/* Configure the buffer pool */
+	pr_info("st: Config bpid %d with size %u\n", pme_bpid, bpid_size);
+	pme_attr_set(pme_attr_bsc(pme_bpid), bpid_size);
+	/* realease to the specified buffer pool */
+#endif
+	while (big_loop--)
+		pme2_test_scan();
+#if CONFIG_FSL_PME2_TEST_SCAN_WITH_BPID
+	pme_attr_set(pme_attr_bsc(pme_bpid), 0);
+	empty_buffer();
+	bman_free_pool(pool);
+	kfree(bman_buffers_virt_base);
+#endif
+	return 0;
+}
+
+static void pme2_test_scan_exit(void)
+{
+}
+
+module_init(pme2_test_scan_init);
+module_exit(pme2_test_scan_exit);
-- 
1.6.5.2

