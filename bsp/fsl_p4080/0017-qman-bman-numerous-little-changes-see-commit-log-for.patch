From 4a2c242dea6117d38543c70830986e7d80820d0f Mon Sep 17 00:00:00 2001
From: Geoff Thorpe <geoff.thorpe@freescale.com>
Date: Mon, 21 Sep 2009 11:07:05 +0800
Subject: [PATCH 17/77] qman/bman: numerous little changes, see commit log for details

* fix QM_MR_VERB_TYPE_MASK.
* fixing EQCR locking bug, it wasn't 100% irq-safe.
* add missing qman_get_null_cb() API.
* add missing qman_query_fq_np() API.
* rename qman_[alloc/release]_fq_range() functions with s/fq/fqid/.
* add inline qman_[alloc/release]_fq() wrappers (ie. range==1).
* add qman_poll_fq_for_init() assist API. When a FQ is used between
  different s/w, this allows the consumer to determine when the consumer
  has initialised the FQ out of the OOS state.
* add missing qman_rbtree_init(), and tighten up the code slightly.
* remove redundant casts.
* add optional tracing to the FQ-range allocator (disabled by default).
* remove needless check from Bman RCR fast-path.
* replace internal FAST_POLL with Kconfig CONFIG_FSL_QMAN_POLL_LIMIT.

Signed-off-by: Geoff Thorpe <geoff.thorpe@freescale.com>
[KevinHao: Original patch taken from Freescale rev 1.2 board support ISO
image for p4080.]
Integrated-by: Kevin Hao <kexin.hao@windriver.com>
---
 drivers/hwalloc/bman_high.c         |    4 --
 drivers/hwqueue/Kconfig             |    4 ++
 drivers/hwqueue/qman_driver.c       |   14 ++++----
 drivers/hwqueue/qman_fqalloc.c      |   43 +++++++++++++++++++----
 drivers/hwqueue/qman_high.c         |   56 +++++++++++++++++++++++------
 drivers/hwqueue/qman_private.h      |    2 +
 drivers/hwqueue/qman_sys.h          |   14 ++++---
 drivers/hwqueue/qman_test_fqrange.c |   18 +++++-----
 include/linux/fsl_qman.h            |   67 +++++++++++++++++++++++++++++++----
 9 files changed, 170 insertions(+), 52 deletions(-)

diff --git a/drivers/hwalloc/bman_high.c b/drivers/hwalloc/bman_high.c
index 9a0b542..2561509 100644
--- a/drivers/hwalloc/bman_high.c
+++ b/drivers/hwalloc/bman_high.c
@@ -464,10 +464,6 @@ static struct bm_rcr_entry *try_rel_start(struct bman_portal **p)
 {
 	struct bm_rcr_entry *r;
 	*p = get_affine_portal();
-	if (unlikely(!*p)) {
-		put_affine_portal();
-		return NULL;
-	}
 	local_irq_disable();
 	if (bm_rcr_get_avail((*p)->p) < RCR_THRESH)
 		bm_rcr_cci_update((*p)->p);
diff --git a/drivers/hwqueue/Kconfig b/drivers/hwqueue/Kconfig
index 3e167d6..d40c846 100644
--- a/drivers/hwqueue/Kconfig
+++ b/drivers/hwqueue/Kconfig
@@ -59,6 +59,10 @@ config FSL_QMAN_FQRANGE
 	  provides a s/w allocator implementation that supports allocating
 	  and deallocating FQID ranges, with control over alignment.
 
+config FSL_QMAN_POLL_LIMIT
+	int
+	default 15
+
 config FSL_QMAN_CONFIG
 	bool "Qman device management"
 	default y
diff --git a/drivers/hwqueue/qman_driver.c b/drivers/hwqueue/qman_driver.c
index 09989e4..faa0449 100644
--- a/drivers/hwqueue/qman_driver.c
+++ b/drivers/hwqueue/qman_driver.c
@@ -349,15 +349,15 @@ bad_cpu_ph:
 		/* TODO: cgrs ?? */
 		affine_portal = qman_create_portal(portal, flags, NULL,
 						&null_cb);
-		if (!affine_portal)
+		if (!affine_portal) {
 			pr_err("Qman portal auto-initialisation failed\n");
-		else {
-			/* default: enable all (available) pool channels */
-			qman_static_dequeue_add_ex(affine_portal, ~0);
-			pr_info("Qman portal %d auto-initialised\n", cfg.cpu);
-			per_cpu(qman_affine_portal, cfg.cpu) = affine_portal;
-			num_affine_portals++;
+			return 0;
 		}
+		/* default: enable all (available) pool channels */
+		qman_static_dequeue_add_ex(affine_portal, ~0);
+		pr_info("Qman portal %d auto-initialised\n", cfg.cpu);
+		per_cpu(qman_affine_portal, cfg.cpu) = affine_portal;
+		num_affine_portals++;
 	}
 #endif
 	return 0;
diff --git a/drivers/hwqueue/qman_fqalloc.c b/drivers/hwqueue/qman_fqalloc.c
index 73b02d2..07b24b1 100644
--- a/drivers/hwqueue/qman_fqalloc.c
+++ b/drivers/hwqueue/qman_fqalloc.c
@@ -99,22 +99,45 @@ struct alloc_node {
 	u32 num;
 };
 
-int qman_alloc_fq_range(u32 *result, u32 count, u32 align, int partial)
+/* #define FQRANGE_DEBUG */
+
+#ifdef FQRANGE_DEBUG
+#define DPRINT		pr_info
+static void DUMP(void)
 {
-	struct alloc_node *i, *next_best = NULL;
+	int off = 0;
+	char buf[256];
+	struct alloc_node *p;
+	list_for_each_entry(p, &alloc_list, list) {
+		if (off < 255)
+			off += snprintf(buf + off, 255-off, "{%d,%d}",
+				p->base, p->base + p->num - 1);
+	}
+	pr_info("%s\n", buf);
+}
+#else
+#define DPRINT(x...)	do { ; } while(0)
+#define DUMP()		do { ; } while(0)
+#endif
+
+int qman_alloc_fqid_range(u32 *result, u32 count, u32 align, int partial)
+{
+	struct alloc_node *i = NULL, *next_best = NULL;
 	u32 base, next_best_base = 0, num = 0, next_best_num = 0;
 	struct alloc_node *margin_left, *margin_right;
 
+	DPRINT("alloc_range(%d,%d,%d)\n", count, align, partial);
+	DUMP();
 	/* If 'align' is 0, it should behave as though it was 1 */
 	if (!align)
 		align = 1;
 	margin_left = kmalloc(sizeof(*margin_left), GFP_KERNEL);
 	if (!margin_left)
-		return -ENOMEM;
+		goto err;
 	margin_right = kmalloc(sizeof(*margin_right), GFP_KERNEL);
 	if (!margin_right) {
 		kfree(margin_left);
-		return -ENOMEM;
+		goto err;
 	}
 	spin_lock_irq(&alloc_lock);
 	list_for_each_entry(i, &alloc_list, list) {
@@ -161,13 +184,18 @@ done:
 		*result = base;
 	}
 	spin_unlock_irq(&alloc_lock);
+err:
+	DPRINT("returning %d\n", i ? num : -ENOMEM);
+	DUMP();
 	return i ? num : -ENOMEM;
 }
-EXPORT_SYMBOL(qman_alloc_fq_range);
+EXPORT_SYMBOL(qman_alloc_fqid_range);
 
-void qman_release_fq_range(u32 fqid, unsigned int count)
+void qman_release_fqid_range(u32 fqid, u32 count)
 {
 	struct alloc_node *i, *node = kmalloc(sizeof(*node), GFP_KERNEL);
+	DPRINT("release_range(%d,%d)\n", fqid, count);
+	DUMP();
 	spin_lock_irq(&alloc_lock);
 	node->base = fqid;
 	node->num = count;
@@ -197,7 +225,8 @@ done:
 		kfree(i);
 	}
 	spin_unlock_irq(&alloc_lock);
+	DUMP();
 }
-EXPORT_SYMBOL(qman_release_fq_range);
+EXPORT_SYMBOL(qman_release_fqid_range);
 
 #endif /* CONFIG_FSL_QMAN_FQRANGE */
diff --git a/drivers/hwqueue/qman_high.c b/drivers/hwqueue/qman_high.c
index 9674d4e..94eeb6d 100644
--- a/drivers/hwqueue/qman_high.c
+++ b/drivers/hwqueue/qman_high.c
@@ -178,10 +178,6 @@ static inline struct qman_fq *table_find_fq(u32 fqid)
 #define SLOW_POLL_BUSY   10
 static u32 __poll_portal_slow(struct qman_portal *p, u32 is);
 static void __poll_portal_fast(struct qman_portal *p);
-/* For fast-path handling, the DQRR cursor chases ring entries around the ring
- * until it runs out of valid things to process. This decrementer constant
- * places a cap on how much it'll process in a single invocation. */
-#define FAST_POLL        16
 
 /* Portal interrupt handler */
 static irqreturn_t portal_isr(int irq, void *ptr)
@@ -413,6 +409,14 @@ void qman_destroy_portal(struct qman_portal *qm)
 	kfree(qm);
 }
 
+void qman_get_null_cb(struct qman_fq_cb *null_cb)
+{
+	struct qman_portal *p = get_affine_portal();
+	*null_cb = p->null_cb;
+	put_affine_portal();
+}
+EXPORT_SYMBOL(qman_get_null_cb);
+
 void qman_set_null_cb(const struct qman_fq_cb *null_cb)
 {
 	struct qman_portal *p = get_affine_portal();
@@ -532,14 +536,14 @@ mr_loop:
 					panic("unexpected FQ retirement");
 				fq_state_change(fq, msg, verb);
 				if (fq->cb.fqs)
-					fq->cb.fqs(p, (void *)fq, msg);
+					fq->cb.fqs(p, fq, msg);
 			} else if (likely(fq)) {
 				/* As per the header note, this is the way to
 				 * determine if it's a s/w ERN or not. */
 				if (likely(!(verb & QM_MR_VERB_DC_ERN)))
-					fq->cb.ern(p, (void *)fq, msg);
+					fq->cb.ern(p, fq, msg);
 				else
-					fq->cb.dc_ern(p, (void *)fq, msg);
+					fq->cb.dc_ern(p, fq, msg);
 			} else {
 				/* use portal default handlers for 'null's */
 				if (likely(!(verb & QM_MR_VERB_DC_ERN)))
@@ -599,7 +603,7 @@ dqrr_loop:
 	if (qm_dqrr_pvb_update(p->p))
 		qm_dqrr_pvb_prefetch(p->p);
 	dq = qm_dqrr_current(p->p);
-	if (!dq || (num == FAST_POLL))
+	if (!dq || (num == CONFIG_FSL_QMAN_POLL_LIMIT))
 		goto done;
 	fq = (void *)dq->contextB;
 	/* Interpret 'dq' from the owner's perspective. */
@@ -1144,6 +1148,33 @@ int qman_query_fq(struct qman_fq *fq, struct qm_fqd *fqd)
 }
 EXPORT_SYMBOL(qman_query_fq);
 
+int qman_query_fq_np(struct qman_fq *fq, struct qm_mcr_queryfq_np *np)
+{
+	struct qm_mc_command *mcc;
+	struct qm_mc_result *mcr;
+	struct qman_portal *p = get_affine_portal();
+	u8 res;
+
+	local_irq_disable();
+	mcc = qm_mc_start(p->p);
+	mcc->queryfq.fqid = fq->fqid;
+	qm_mc_commit(p->p, QM_MCC_VERB_QUERYFQ_NP);
+	while (!(mcr = qm_mc_result(p->p)))
+		cpu_relax();
+	QM_ASSERT((mcr->verb & QM_MCR_VERB_MASK) == QM_MCR_VERB_QUERYFQ_NP);
+	res = mcr->result;
+	if (res == QM_MCR_RESULT_OK)
+		*np = mcr->queryfq_np;
+	local_irq_enable();
+	put_affine_portal();
+	if (res != QM_MCR_RESULT_OK) {
+		pr_err("QUERYFQ_NP failed: %s\n", mcr_result_str(res));
+		return -EIO;
+	}
+	return 0;
+}
+EXPORT_SYMBOL(qman_query_fq_np);
+
 /* internal function used as a wait_event() expression */
 static int set_vdqcr(struct qman_portal **p, u32 vdqcr, u16 *v)
 {
@@ -1213,14 +1244,13 @@ static struct qm_eqcr_entry *try_eq_start(struct qman_portal **p)
 {
 	struct qm_eqcr_entry *eq;
 	*p = get_affine_portal();
-	if (qm_eqcr_get_avail((*p)->p) < EQCR_THRESH) {
-		local_irq_disable();
+	local_irq_disable();
+	if (qm_eqcr_get_avail((*p)->p) < EQCR_THRESH)
 		(*p)->eq_cons += qm_eqcr_cci_update((*p)->p);
-		local_irq_enable();
-	}
 	eq = qm_eqcr_start((*p)->p);
 	if (unlikely(!eq)) {
 		eqcr_set_thresh(*p, 1);
+		local_irq_enable();
 		put_affine_portal();
 	}
 	return eq;
@@ -1279,6 +1309,7 @@ static inline void eqcr_commit(struct qman_portal *p, u32 flags, int orp)
 	if ((flags & QMAN_ENQUEUE_FLAG_WAIT_SYNC) ==
 			QMAN_ENQUEUE_FLAG_WAIT_SYNC)
 		eqcr_set_thresh(p, 1);
+	local_irq_enable();
 	put_affine_portal();
 	if ((flags & QMAN_ENQUEUE_FLAG_WAIT_SYNC) !=
 			QMAN_ENQUEUE_FLAG_WAIT_SYNC)
@@ -1296,6 +1327,7 @@ static inline void eqcr_commit(struct qman_portal *p, u32 flags, int orp)
 static inline void eqcr_abort(struct qman_portal *p)
 {
 	qm_eqcr_abort(p->p);
+	local_irq_enable();
 	put_affine_portal();
 }
 
diff --git a/drivers/hwqueue/qman_private.h b/drivers/hwqueue/qman_private.h
index 6a7d277..8875321 100644
--- a/drivers/hwqueue/qman_private.h
+++ b/drivers/hwqueue/qman_private.h
@@ -114,7 +114,9 @@ int __qm_portal_bind(struct qm_portal *portal, u8 iface);
 void __qm_portal_unbind(struct qm_portal *portal, u8 iface);
 
 /* Hooks for driver initialisation */
+#ifdef CONFIG_FSL_QMAN_FQALLOCATOR
 __init int __fqalloc_init(void);
+#endif
 
 /* Hooks between qman_driver.c and qman_high.c */
 extern DEFINE_PER_CPU(struct qman_portal *, qman_affine_portal);
diff --git a/drivers/hwqueue/qman_sys.h b/drivers/hwqueue/qman_sys.h
index 96ef503..9c6c72d 100644
--- a/drivers/hwqueue/qman_sys.h
+++ b/drivers/hwqueue/qman_sys.h
@@ -55,11 +55,15 @@ struct qman_rbtree {
 };
 #define QMAN_RBTREE { .root = RB_ROOT }
 
+static inline void qman_rbtree_init(struct qman_rbtree *tree)
+{
+	tree->root = RB_ROOT;
+}
+
 #define IMPLEMENT_QMAN_RBTREE(name, type, node_field, val_field) \
 static inline int name##_push(struct qman_rbtree *tree, type *obj) \
 { \
 	struct rb_node *parent = NULL, **p = &tree->root.rb_node; \
-	int ret = -EBUSY; \
 	while (*p) { \
 		u32 item; \
 		parent = *p; \
@@ -69,7 +73,7 @@ static inline int name##_push(struct qman_rbtree *tree, type *obj) \
 		else if (obj->val_field > item) \
 			p = &parent->rb_right; \
 		else \
-			return ret; \
+			return -EBUSY; \
 	} \
 	rb_link_node(&obj->node_field, parent, p); \
 	rb_insert_color(&obj->node_field, &tree->root); \
@@ -90,10 +94,8 @@ static inline type *name##_find(struct qman_rbtree *tree, u32 val) \
 		else if (val > ret->val_field) \
 			p = p->rb_right; \
 		else \
-			break; \
+			return ret; \
 	} \
-	if (!p) \
-		ret = NULL; \
-	return ret; \
+	return NULL; \
 }
 
diff --git a/drivers/hwqueue/qman_test_fqrange.c b/drivers/hwqueue/qman_test_fqrange.c
index eaf21b2..3ce2bb6 100644
--- a/drivers/hwqueue/qman_test_fqrange.c
+++ b/drivers/hwqueue/qman_test_fqrange.c
@@ -39,26 +39,26 @@ void qman_test_fqrange(void)
 
 	pr_info("Testing \"FQRANGE\" allocator ...\n");
 	/* Seed the allocator*/
-	qman_release_fq_range(0, 1000);
+	qman_release_fqid_range(0, 1000);
 
-	num = qman_alloc_fq_range(&result1, 100, 4, 0);
+	num = qman_alloc_fqid_range(&result1, 100, 4, 0);
 	BUG_ON(result1 % 4);
 
-	num = qman_alloc_fq_range(&result2, 500, 500, 0);
+	num = qman_alloc_fqid_range(&result2, 500, 500, 0);
 	BUG_ON((num != 500) || (result2 != 500));
 
-	num = qman_alloc_fq_range(&result3, 1000, 0, 0);
+	num = qman_alloc_fqid_range(&result3, 1000, 0, 0);
 	BUG_ON(num >= 0);
 
-	num = qman_alloc_fq_range(&result3, 1000, 0, 1);
+	num = qman_alloc_fqid_range(&result3, 1000, 0, 1);
 	BUG_ON(num < 400);
 
-	qman_release_fq_range(result2, 500);
-	qman_release_fq_range(result1, 100);
-	qman_release_fq_range(result3, num);
+	qman_release_fqid_range(result2, 500);
+	qman_release_fqid_range(result1, 100);
+	qman_release_fqid_range(result3, num);
 
 	/* It should now be possible to drain the allocator empty */
-	num = qman_alloc_fq_range(&result1, 1000, 0, 0);
+	num = qman_alloc_fqid_range(&result1, 1000, 0, 0);
 	BUG_ON(num != 1000);
 	pr_info("                              ... SUCCESS!\n");
 }
diff --git a/include/linux/fsl_qman.h b/include/linux/fsl_qman.h
index 3ae33b4..0bb71e6 100644
--- a/include/linux/fsl_qman.h
+++ b/include/linux/fsl_qman.h
@@ -700,7 +700,7 @@ struct qm_mr_entry {
  * originating from direct-connect portals ("dcern") use 0x20 as a verb which
  * would be invalid as a s/w enqueue verb. A s/w ERN can be distinguished from
  * the other MR types by noting if the 0x20 bit is unset. */
-#define QM_MR_VERB_TYPE_MASK		0x23
+#define QM_MR_VERB_TYPE_MASK		0x27
 #define QM_MR_VERB_DC_ERN		0x20
 #define QM_MR_VERB_FQRN			0x21
 #define QM_MR_VERB_FQRNI		0x22
@@ -1297,6 +1297,13 @@ struct qman_fq {
 	/* Portal Management */
 	/* ----------------- */
 /**
+ * qman_get_null_cb - get callbacks currently used for "null" frame queues
+ *
+ * Copies the callbacks used for the affine portal of the current cpu.
+ */
+void qman_get_null_cb(struct qman_fq_cb *null_cb);
+
+/**
  * qman_set_null_cb - set callbacks to use for "null" frame queues
  *
  * Sets the callbacks to use for the affine portal of the current cpu, whenever
@@ -1380,7 +1387,6 @@ u32 qman_static_dequeue_get(void);
  */
 void qman_dca(struct qm_dqrr_entry *dq, int park_request);
 
-
 	/* FQ management */
 	/* ------------- */
 /**
@@ -1521,10 +1527,18 @@ int qman_oos_fq(struct qman_fq *fq);
 /**
  * qman_query_fq - Queries FQD fields (via h/w query command)
  * @fq: the frame queue object to be queried
+ * @fqd: storage for the queried FQD fields
  */
 int qman_query_fq(struct qman_fq *fq, struct qm_fqd *fqd);
 
 /**
+ * qman_query_fq_np - Queries non-programmable FQD fields
+ * @fq: the frame queue object to be queried
+ * @np: storage for the queried FQD fields
+ */
+int qman_query_fq_np(struct qman_fq *fq, struct qm_mcr_queryfq_np *np);
+
+/**
  * qman_volatile_dequeue - Issue a volatile dequeue command
  * @fq: the frame queue object to dequeue from (or NULL)
  * @flags: a bit-mask of QMAN_VOLATILE_FLAG_*** options
@@ -1617,7 +1631,7 @@ int qman_enqueue_orp(struct qman_fq *fq, const struct qm_fd *fd, u32 flags,
 			struct qman_fq *orp, u16 orp_seqnum);
 
 /**
- * qman_alloc_fq_range - Allocate a contiguous range of FQIDs
+ * qman_alloc_fqid_range - Allocate a contiguous range of FQIDs
  * @result: is set by the API to the base FQID of the allocated range
  * @count: the number of FQIDs required
  * @align: required alignment of the allocated range
@@ -1628,17 +1642,56 @@ int qman_enqueue_orp(struct qman_fq *fq, const struct qm_fd *fd, u32 flags,
  * FQs than requested (though alignment will be as requested). If @partial is
  * zero, the return value will either be 'count' or negative.
  */
-int qman_alloc_fq_range(u32 *result, u32 count, u32 align, int partial);
+int qman_alloc_fqid_range(u32 *result, u32 count, u32 align, int partial);
+static inline int qman_alloc_fqid(u32 *result)
+{
+	return qman_alloc_fqid_range(result, 1, 0, 0);
+}
 
 /**
- * qman_release_fq_range - Release the specified range of frame queue IDs
+ * qman_release_fqid_range - Release the specified range of frame queue IDs
  * @fqid: the base FQID of the range to deallocate
  * @count: the number of FQIDs in the range
  *
  * This function can also be used to seed the allocator with ranges of FQIDs
- * that it can subsequently use.
+ * that it can subsequently use. Returns zero for success.
  */
-void qman_release_fq_range(u32 fqid, unsigned int count);
+void qman_release_fqid_range(u32 fqid, unsigned int count);
+static inline void qman_release_fqid(u32 fqid)
+{
+	qman_release_fqid_range(fqid, 1);
+}
+
+	/* Helpers */
+	/* ------- */
+/**
+ * qman_poll_fq_for_init - Check if an FQ has been initialised from OOS
+ * @fqid: the FQID that will be initialised by other s/w
+ *
+ * In many situations, a FQID is provided for communication between s/w
+ * entities, and whilst the consumer is responsible for initialising and
+ * scheduling the FQ, the producer(s) generally create a wrapper FQ object using
+ * and only call qman_enqueue() (no FQ initialisation, scheduling, etc). Ie;
+ *     qman_create_fq(..., QMAN_FQ_FLAG_NO_MODIFY, ...);
+ * However, data can not be enqueued to the FQ until it is initialised out of
+ * the OOS state - this function polls for that condition. It is particularly
+ * useful for users of IPC functions - each endpoint's Rx FQ is the other
+ * endpoint's Tx FQ, so each side can initialise and schedule their Rx FQ object
+ * and then use this API on the (NO_MODIFY) Tx FQ object in order to
+ * synchronise. The function returns zero for success, +1 if the FQ is still in
+ * the OOS state, or negative if there was an error.
+ */
+static inline int qman_poll_fq_for_init(struct qman_fq *fq)
+{
+	struct qm_mcr_queryfq_np np;
+	int err;
+	err = qman_query_fq_np(fq, &np);
+	if (err)
+		return err;
+	if ((np.state & QM_MCR_NP_STATE_MASK) == QM_MCR_NP_STATE_OOS)
+		return 1;
+	return 0;
+}
 
 #endif /* FSL_QMAN_H */
 
-- 
1.6.5.2

