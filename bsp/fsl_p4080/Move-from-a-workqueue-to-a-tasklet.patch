From de95583c752d738356ed61b815413aa5a9225f1b Mon Sep 17 00:00:00 2001
From: Kumar Gala <galak@kernel.crashing.org>
Date: Wed, 24 Feb 2010 10:53:20 -0600
Subject: [PATCH] Move from a workqueue to a tasklet

Clean up bman_release flags.  Tasklets are not allowed to sleep, so we
can't wait.  For the initial releases, we need to add
BMAN_RELEASE_FLAG_WAIT so that it actually *does*

Signed-off-by: Andy Fleming <afleming@freescale.com>
[Kevin: Original patch taken from Freescale moto-patches-4.tgz
tar package, just some context change in order to port to kernel
2.6.27]
Integrated-by: Kevin Hao <kexin.hao@windriver.com>
---
 drivers/net/dpa/dpa.c |   61 ++++++++++++++----------------------------------
 drivers/net/dpa/dpa.h |    2 +-
 2 files changed, 19 insertions(+), 44 deletions(-)

diff --git a/drivers/net/dpa/dpa.c b/drivers/net/dpa/dpa.c
index f2ee83b..be39bce 100644
--- a/drivers/net/dpa/dpa.c
+++ b/drivers/net/dpa/dpa.c
@@ -95,11 +95,9 @@ static const char rtx[][3] = {
 /* BM */
 
 #ifdef DEBUG
-#define GFP_DPA_BP	(GFP_DMA | __GFP_ZERO)
-#define GFP_DPA		(GFP_KERNEL | __GFP_ZERO)
+#define GFP_DPA_BP	(GFP_DMA | __GFP_ZERO | GFP_ATOMIC)
 #else
-#define GFP_DPA_BP	(GFP_DMA)
-#define GFP_DPA		(GFP_KERNEL)
+#define GFP_DPA_BP	(GFP_DMA | GFP_ATOMIC)
 #endif
 
 #define DPA_BP_HEAD	64
@@ -279,8 +277,7 @@ static void dpa_bp_refill(const struct net_device *net_dev, struct dpa_bp *bp,
 
 		/* When we get to the end of the buffer, send it all to bman */
 		if ((i % 8) == 7) {
-			err = bman_release(bp->pool, bmb, 8,
-						BMAN_RELEASE_FLAG_WAIT_INT);
+			err = bman_release(bp->pool, bmb, 8, 0);
 
 			if (err < 0) {
 				bmb_free(net_dev, bp, bmb);
@@ -291,8 +288,7 @@ static void dpa_bp_refill(const struct net_device *net_dev, struct dpa_bp *bp,
 
 	/* Take care of the leftovers ('i' will be one past the last block) */
 	if ((i % 8) != 0) {
-		err = bman_release(bp->pool, bmb, i % 8,
-				BMAN_RELEASE_FLAG_WAIT_INT);
+		err = bman_release(bp->pool, bmb, i % 8, 0);
 
 		if (err < 0) {
 			int j;
@@ -392,7 +388,7 @@ _dpa_bp_alloc(struct net_device *net_dev, struct list_head *list,
 		dpa_bp->kernel_pool = 1;
 
 		spin_lock_init(&dpa_bp->lock);
-		dpa_bp_refill(net_dev, dpa_bp, GFP_KERNEL);
+		dpa_bp_refill(net_dev, dpa_bp, GFP_DPA_BP);
 	} else if (dpa_bp->paddr == 0) {
 		int i;
 		dpa_bp->kernel_pool = 0;
@@ -420,6 +416,7 @@ _dpa_bp_alloc(struct net_device *net_dev, struct list_head *list,
 			bmb.lo = dpa_bp->paddr + i * dpa_bp->size;
 
 			_errno = bman_release(dpa_bp->pool, &bmb, 1,
+						BMAN_RELEASE_FLAG_WAIT |
 						BMAN_RELEASE_FLAG_WAIT_INT);
 			if (_errno < 0) {
 				goto _return_bman_acquire;
@@ -693,8 +690,7 @@ dpa_fd_release(const struct net_device *net_dev, const struct qm_fd *fd)
 					!sgt[i-1].final &&
 					sgt[i-1].bpid == sgt[i].bpid);
 
-			__errno = bman_release(dpa_bp->pool, bmb, j,
-					BMAN_RELEASE_FLAG_WAIT_INT);
+			__errno = bman_release(dpa_bp->pool, bmb, j, 0);
 			if (unlikely(__errno < 0)) {
 				if (netif_msg_drv(priv) && net_ratelimit())
 					cpu_netdev_err(net_dev,	"%s:%hu:%s(): "
@@ -714,15 +710,14 @@ dpa_fd_release(const struct net_device *net_dev, const struct qm_fd *fd)
 	}
 
 	if (!_dpa_bp->kernel_pool) {
-		__errno = bman_release(_dpa_bp->pool, &_bmb, 1,
-				BMAN_RELEASE_FLAG_WAIT_INT);
+		__errno = bman_release(_dpa_bp->pool, &_bmb, 1, 0);
 		if (unlikely(__errno < 0)) {
 			if (netif_msg_drv(priv) && net_ratelimit())
 				cpu_netdev_err(net_dev, "%s:%hu:%s(): "
 					       "bman_release(%hu) = %d\n",
 					       __file__, __LINE__, __func__,
 					       bman_get_params(_dpa_bp->pool)->bpid,
-					       _errno);
+					       __errno);
 			if (_errno >= 0)
 				_errno = __errno;
 		}
@@ -748,7 +743,6 @@ ingress_dqrr(struct qman_portal		*portal,
 	     uint8_t			 _rtx,
 	     uint8_t			 ed)
 {
-	int				 _errno;
 	struct net_device		*net_dev;
 	struct dpa_priv_s		*priv;
 	struct dpa_percpu_priv_s	*percpu_priv;
@@ -790,12 +784,7 @@ ingress_dqrr(struct qman_portal		*portal,
 	if (_rtx == TX && percpu_priv->count[_rtx][ed] > DPA_MAX_TX_BACKLOG)
 		netif_tx_stop_all_queues(net_dev);
 
-	_errno = schedule_work(&percpu_priv->fd_work);
-	if (unlikely(_errno < 0))
-		if (netif_msg_rx_err(priv))
-			cpu_netdev_crit(net_dev,
-					"%s:%hu:%s(): schedule_work() = %d\n",
-					__file__, __LINE__, __func__, _errno);
+	tasklet_schedule(&percpu_priv->t);
 
 _return:
 	if (netif_msg_intr(priv))
@@ -1524,8 +1513,6 @@ static void __hot _dpa_rx(struct net_device		*net_dev,
 					__file__, __LINE__, __func__,
 					dpa_fd->fd.status & FM_FD_STAT_ERRORS);
 
-#warning We need to return the buffer to the pool here, or update the fill count
-
 		net_dev->stats.rx_errors++;
 
 		goto _return_dpa_fd_release;
@@ -1555,7 +1542,7 @@ static void __hot _dpa_rx(struct net_device		*net_dev,
 		size = dpa_fd_length(&dpa_fd->fd);
 
 	if (skb == NULL) {
-		skb = __netdev_alloc_skb(net_dev, DPA_BP_HEAD + NET_IP_ALIGN + size, GFP_DPA);
+		skb = __netdev_alloc_skb(net_dev, DPA_BP_HEAD + NET_IP_ALIGN + size, GFP_ATOMIC);
 		if (unlikely(skb == NULL)) {
 			if (netif_msg_rx_err(priv) && net_ratelimit())
 				cpu_netdev_err(net_dev, "%s:%hu:%s(): "
@@ -1585,13 +1572,8 @@ static void __hot _dpa_rx(struct net_device		*net_dev,
 	fman_test_ip_manip((void *)priv->mac_dev, skb->data);
 #endif /* CONFIG_FSL_FMAN_TEST */
 
-	_errno = netif_rx_ni(skb);
+	_errno = netif_receive_skb(skb);
 	if (unlikely(_errno != NET_RX_SUCCESS)) {
-		if (netif_msg_rx_status(priv) && net_ratelimit())
-			cpu_netdev_warn(net_dev, "%s:%hu:%s(): "
-					"netif_rx_ni() = %d\n",
-					__file__, __LINE__, __func__, _errno);
-
 		net_dev->stats.rx_dropped++;
 
 		goto _return_dev_kfree_skb;
@@ -1600,7 +1582,7 @@ static void __hot _dpa_rx(struct net_device		*net_dev,
 	net_dev->last_rx = jiffies;
 
 	if (dpa_bp->bp_refill_pending > dpa_bp->bp_kick_thresh)
-		dpa_bp_refill(net_dev, dpa_bp, GFP_KERNEL);
+		dpa_bp_refill(net_dev, dpa_bp, GFP_DPA_BP);
 
 	return;
 
@@ -1656,17 +1638,16 @@ static void (*const _dpa_work[][2])(struct net_device		*net_dev,
 	[TX] = {_dpa_tx_error, _dpa_tx}
 };
 
-static void __hot dpa_work(struct work_struct *fd_work)
+static void __hot dpa_tasklet(unsigned long data)
 {
-	int				 _errno, i, j;
+	int				 i, j;
 	struct net_device		*net_dev;
 	const struct dpa_priv_s		*priv;
-	struct dpa_percpu_priv_s	*percpu_priv;
+ 	struct dpa_percpu_priv_s	*percpu_priv = (struct dpa_percpu_priv_s *)data;
 	struct dpa_fd			*dpa_fd, *tmp;
 	unsigned int quota = 0;
 	unsigned int retry = 0;
 
-	percpu_priv = container_of(fd_work, struct dpa_percpu_priv_s, fd_work);
 	net_dev = percpu_priv->net_dev;
 	priv = netdev_priv(net_dev);
 
@@ -1709,13 +1690,7 @@ static void __hot dpa_work(struct work_struct *fd_work)
 
 	/* Try again later if we're not done */
 	if (retry) {
-		_errno = schedule_work(fd_work);
-		if (unlikely(_errno < 0))
-			if (netif_msg_rx_err(priv))
-				cpu_netdev_crit(net_dev, "%s:%hu:%s(): "
-						"schedule_work() = %d\n",
-						__file__, __LINE__, __func__,
-						_errno);
+		tasklet_schedule(&percpu_priv->t);
 	}
 
 	if (netif_msg_drv(priv))
@@ -2586,7 +2561,7 @@ dpa_probe(struct of_device *_of_dev)
 		percpu_priv = per_cpu_ptr(priv->percpu_priv, i);
 
 		percpu_priv->net_dev = net_dev;
-		INIT_WORK(&percpu_priv->fd_work, dpa_work);
+		tasklet_init(&percpu_priv->t, dpa_tasklet, (unsigned long)percpu_priv);
 		for (j = 0; j < ARRAY_SIZE(percpu_priv->fd_list); j++)
 			for (k = 0; k < ARRAY_SIZE(percpu_priv->fd_list[j]);
 			     k++)
diff --git a/drivers/net/dpa/dpa.h b/drivers/net/dpa/dpa.h
index cb66a43..95948c4 100644
--- a/drivers/net/dpa/dpa.h
+++ b/drivers/net/dpa/dpa.h
@@ -55,7 +55,7 @@ struct pcd_range {
 
 struct dpa_percpu_priv_s {
 	struct net_device	*net_dev;
-	struct work_struct	 fd_work;
+	struct tasklet_struct	t;
 	struct list_head	 fd_list[2][2];
 	struct list_head	 free_list;
 	struct sk_buff_head	 rx_recycle;
-- 
1.6.0.4

