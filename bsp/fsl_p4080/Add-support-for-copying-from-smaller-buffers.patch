From 2807a58594a08ade877a939a3494b7a28b27105e Mon Sep 17 00:00:00 2001
From: Andy Fleming <afleming@freescale.com>
Date: Tue, 2 Feb 2010 15:40:33 -0600
Subject: [PATCH] Add support for copying from smaller buffers

Signed-off-by: Kumar Gala <galak@kernel.crashing.org>
[Kevin: Original patch taken from moto-patches.tar.gz, apply cleanly
to kernel 2.6.27]
Integrated-by: Kevin Hao <kexin.hao@windriver.com>
---
 drivers/net/dpa/dpa.c |   45 +++++++++++++++++++++++++++++++++++++++++++--
 1 files changed, 43 insertions(+), 2 deletions(-)

diff --git a/drivers/net/dpa/dpa.c b/drivers/net/dpa/dpa.c
index 1ac497c..5936f4e 100644
--- a/drivers/net/dpa/dpa.c
+++ b/drivers/net/dpa/dpa.c
@@ -367,8 +367,9 @@ _dpa_bp_alloc(struct net_device *net_dev, struct list_head *list,
 		return -ENODEV;
 	}
 
+#define DPA_COPYBREAK DPA_BP_SIZE(1600)
 	/* paddr is only set for pools that are shared between partitions */
-	if (dpa_bp->paddr == 0) {
+	if (dpa_bp->paddr == 0 && dpa_bp->size > DPA_COPYBREAK) {
 		int hashsize = dpa_bp->count * DPA_HASH_MULTIPLE;
 		hashsize = __roundup_pow_of_two(hashsize);
 		dpa_bp->rxhash = kzalloc(hashsize * sizeof(struct page *),
@@ -380,13 +381,45 @@ _dpa_bp_alloc(struct net_device *net_dev, struct list_head *list,
 
 		dpa_bp->bp_blocks_per_page = PAGE_SIZE / dpa_bp->size;
 		BUG_ON(dpa_bp->count < dpa_bp->bp_blocks_per_page);
-		dpa_bp->bp_kick_thresh = dpa_bp->count - max((unsigned int)8,
+		dpa_bp->bp_kick_thresh = dpa_bp->count - max((unsigned int)32,
 						dpa_bp->bp_blocks_per_page);
 		dpa_bp->bp_refill_pending = dpa_bp->count;
 		dpa_bp->kernel_pool = 1;
 
 		spin_lock_init(&dpa_bp->lock);
 		dpa_bp_refill(net_dev, dpa_bp, GFP_KERNEL);
+	} else if (dpa_bp->paddr == 0) {
+		int i;
+		dpa_bp->kernel_pool = 0;
+		dpa_bp->vaddr = alloc_pages_exact(dpa_bp->size * dpa_bp->count,
+						GFP_DPA_BP);
+		if (!dpa_bp->vaddr) {
+			_errno = -ENOMEM;
+			goto _return_bman_free_pool;
+		}
+
+		dpa_bp->paddr = dma_map_single(net_dev->dev.parent,
+						dpa_bp->vaddr,
+						dpa_bp->size * dpa_bp->count,
+						DMA_BIDIRECTIONAL);
+
+		if (dpa_bp->paddr == 0) {
+			_errno = -EIO;
+			goto _return_free_pages_exact;
+		}
+
+		for (i = 0; i < dpa_bp->count; i++) {
+			struct bm_buffer bmb;
+
+			bmb.hi = 0;
+			bmb.lo = dpa_bp->paddr + i * dpa_bp->size;
+
+			_errno = bman_release(dpa_bp->pool, &bmb, 1,
+						BMAN_RELEASE_FLAG_WAIT_INT);
+			if (_errno < 0) {
+				goto _return_bman_acquire;
+			}
+		}
 	} else {
 		/* This is a shared pool, which the kernel doesn't manage */
 		dpa_bp->kernel_pool = 0;
@@ -411,6 +444,12 @@ _dpa_bp_alloc(struct net_device *net_dev, struct list_head *list,
 
 	return 0;
 
+_return_bman_acquire:
+	dma_unmap_single(net_dev->dev.parent, dpa_bp->paddr,
+				dpa_bp->size * dpa_bp->count,
+				DMA_BIDIRECTIONAL);
+_return_free_pages_exact:
+	free_pages_exact(dpa_bp->vaddr, dpa_bp->size * dpa_bp->count);
 _return_bman_free_pool:
 	bman_free_pool(dpa_bp->pool);
 
@@ -1473,6 +1512,8 @@ static void __hot _dpa_rx(struct net_device		*net_dev,
 					__file__, __LINE__, __func__,
 					dpa_fd->fd.status & FM_FD_STAT_ERRORS);
 
+#warning We need to return the buffer to the pool here, or update the fill count
+
 		net_dev->stats.rx_errors++;
 
 		goto _return_dpa_fd_release;
-- 
1.6.0.4

