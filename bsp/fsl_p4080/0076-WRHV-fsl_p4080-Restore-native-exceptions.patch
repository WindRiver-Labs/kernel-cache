From a05b8410f4920af2f866a7f51d7eb163248e70e6 Mon Sep 17 00:00:00 2001
From: Tiejun Chen <tiejun.chen@windriver.com>
Date: Fri, 4 Dec 2009 22:58:34 -0800
Subject: [PATCH 76/77] WRHV/fsl_p4080: Restore native exceptions

We should use some native exceptions since they are ditected to guest OS:
1> DSI
2> ISI
3> DataTLBError
4> InstructionTLBError

Here ome instructions emulation codes should be implemented for the hypervisor.

Signed-off-by: Tiejun Chen <tiejun.chen@windriver.com>
---
 arch/powerpc/kernel/head_wrhv.S |  458 +++++++++++++++++++++++++++++++++++++++
 1 files changed, 458 insertions(+), 0 deletions(-)

diff --git a/arch/powerpc/kernel/head_wrhv.S b/arch/powerpc/kernel/head_wrhv.S
index af28fda..39cc255 100644
--- a/arch/powerpc/kernel/head_wrhv.S
+++ b/arch/powerpc/kernel/head_wrhv.S
@@ -98,6 +98,11 @@ _ENTRY(_start);
  * if needed
  */
 
+#define TLBWE_CODE		0x7C0007A4
+#define TLBSX_CODE		0x7c005724
+#define MAS6_AS1		0x0001
+#define MAS6_AS0		0xFFFE
+
 _ENTRY(__early_start)
 	/*
 	 * This is where the main kernel code starts.
@@ -294,6 +299,166 @@ interrupt_base:
 
 	/* Data Storage Interrupt */
 	START_EXCEPTION(DataStorage)
+#ifdef	CONFIG_PPC85xx_VT_MODE
+	mtspr	SPRN_SPRG0, r10		/* Save some working registers */
+	mtspr	SPRN_SPRG1, r11
+	mtspr	SPRN_SPRG6W, r3
+	mtspr	SPRN_SPRG4W, r12
+	mtspr	SPRN_SPRG5W, r13
+	mfcr	r11
+	mtspr	SPRN_SPRG7W, r11
+
+	/*
+	 * Check if it was a store fault, if not then bail
+	 * because a user tried to access a kernel or
+	 * read-protected page.  Otherwise, get the
+	 * offending address and handle it.
+	 */
+	mfspr	r10, SPRN_ESR
+	andis.	r10, r10, ESR_ST@h
+	beq	2f
+
+	mfspr	r10, SPRN_DEAR		/* Get faulting address */
+
+	/* If we are faulting a kernel address, we have to use the
+	 * kernel page tables.
+	 */
+	lis	r11, PAGE_OFFSET@h
+	cmplw	0, r10, r11
+	bge	2f
+
+	/* Get the PGD for the current thread */
+3:
+	mfspr	r11,SPRN_SPRG3
+	lwz	r11,PGDIR(r11)
+4:
+	FIND_PTE_ADDR
+
+	beq	2f	/* Bail if there's no entry */
+
+5:
+	LOAD_PTE
+
+	/* Are _PAGE_USER & _PAGE_RW set & _PAGE_HWWRITE not? */
+	andi.	r13, r11, _PAGE_RW|_PAGE_USER|_PAGE_HWWRITE
+	cmpwi	0, r13, _PAGE_RW|_PAGE_USER
+	bne	2f			/* Bail if not */
+
+
+#ifndef CONFIG_PPC_E500MC
+	/* update search PID in MAS6, AS = 0 */
+	mfspr	r13, SPRN_PID0
+	slwi	r13, r13, 16
+	mtspr	SPRN_MAS6, r13
+#else
+	/* update search PID in MAS6, AS = 1 */
+	mfspr	r3,SPRN_MAS6
+	mfspr	r13, SPRN_PID0
+	rlwimi	r3,r13,16,8,15	
+
+	ori	r3,r3,MAS6_AS1
+	mfsrr1	r13
+	rlwinm.	r13,r13,0,27,27	
+	bne	1f
+	andi.	r3,r3,MAS6_AS0
+1:	mtspr	SPRN_MAS6, r3
+#endif
+
+	/*  The HY will check which privileged instruction trap privileged 
+	 *  exception via r3. */
+        lis     r3,TLBSX_CODE@h
+        ori     r3,r3,TLBSX_CODE@l
+	tlbsx	0, r10
+
+#ifdef CONFIG_SMP
+	/*
+	 * It's possible another processor kicked out the entry
+	 * before we did our tlbsx, so check if we hit
+	 */
+	mfspr	r13, SPRN_MAS1
+	rlwinm.	r13,r13, 0, 0, 0;	/* Check the Valid bit */
+	beq	2f
+#endif /* CONFIG_SMP */
+
+	/*
+	 * MAS2 not updated as the entry does exist in the tlb, this
+	 * fault taken to detect state transition (eg: COW -> DIRTY)
+	 */
+	andi.	r13, r11, _PAGE_HWEXEC
+	rlwimi	r13, r13, 31, 27, 27	/* SX <- _PAGE_HWEXEC */
+	ori	r13, r13, (MAS3_UW|MAS3_SW|MAS3_UR|MAS3_SR)@l /* set static perms */
+
+	/* only update the perm bits, assume the RPN is fine */
+	mfspr	r10, SPRN_MAS3
+	rlwimi	r10, r13, 0, 20, 31
+	mtspr	SPRN_MAS3,r10
+	/*  The HY will check which privileged instruction trap privileged 
+	 *  exception via r3. */
+        lis     r3,TLBWE_CODE@h
+        ori     r3,r3,TLBWE_CODE@l
+	tlbwe
+
+#ifdef CONFIG_SMP
+	mr	r13, r11
+	LWARX_PTE
+	cmpw	r13, r11
+	bne-	7f
+#endif
+
+	/* Update 'changed'. */
+	ori	r11, r11, _PAGE_DIRTY|_PAGE_ACCESSED|_PAGE_HWWRITE
+	STWCX_PTE	/* r11 and r12 must be PTE and &PTE */
+
+#ifdef CONFIG_SMP
+	/*
+	 * If the stwcx. failed, we invalidate the entry we just wrote,
+	 * and start over
+	 */
+	beq+	6f
+7:	mfspr	r13, SPRN_MAS1
+	rlwinm	r13, r13, 0, 1, 31	/* Clear Valid bit */
+	mtspr	SPRN_MAS1, r13
+	/*  The HY will check which privileged instruction trap privileged 
+	 *  exception via r3. */
+        lis     r3,TLBWE_CODE@h
+        ori     r3,r3,TLBWE_CODE@l
+	tlbwe
+
+	b	5b		/* Try again */
+#endif	/* CONFIG_SMP */
+6:
+
+	/* Done...restore registers and get out of here.  */
+	mfspr	r11, SPRN_SPRG7R
+	mtcr	r11
+	mfspr	r13, SPRN_SPRG5R
+	mfspr	r12, SPRN_SPRG4R
+	mfspr	r3,  SPRN_SPRG6R
+	mfspr	r11, SPRN_SPRG1
+	mfspr	r10, SPRN_SPRG0
+	rfi			/* Force context change */
+
+2:
+#ifdef CONFIG_SMP
+	/* Clear the reservation */
+	lis	r11, dummy_stwcx@h
+	ori	r11,r11, dummy_stwcx@l
+	stwcx.	r11, 0, r11
+#endif
+
+	/*
+	 * The bailout.  Restore registers to pre-exception conditions
+	 * and call the heavyweights to help us out.
+	 */
+	mfspr	r11, SPRN_SPRG7R
+	mtcr	r11
+	mfspr	r13, SPRN_SPRG5R
+	mfspr	r12, SPRN_SPRG4R
+	mfspr	r3,  SPRN_SPRG6R
+	mfspr	r11, SPRN_SPRG1
+	mfspr	r10, SPRN_SPRG0
+	b	data_access
+#else
         /*    only  r3, r4, CR are saved in vb_status */
        	lis	r4,wr_control@ha
        	lwz	r4,wr_control@l(r4)
@@ -427,6 +592,7 @@ interrupt_base:
 	lwz     r10,VB_CONTROL_R10(r11)
 	lwz     r11,VB_CONTROL_R11(r11)
 	b	data_access
+#endif
 
 	/* Instruction Storage Interrupt */
 	INSTRUCTION_STORAGE_EXCEPTION
@@ -476,6 +642,149 @@ interrupt_base:
 
 	/* Data TLB Error Interrupt */
 	START_EXCEPTION(DataTLBError)
+#ifdef	CONFIG_PPC85xx_VT_MODE
+	mtspr	SPRN_SPRG0, r10		/* Save some working registers */
+	mtspr	SPRN_SPRG1, r11
+	mtspr	SPRN_SPRG6W, r3
+	mtspr	SPRN_SPRG4W, r12
+	mtspr	SPRN_SPRG5W, r13
+	mfcr	r11
+	mtspr	SPRN_SPRG7W, r11
+	mfspr	r10, SPRN_DEAR		/* Get faulting address */
+
+	/* If we are faulting a kernel address, we have to use the
+	 * kernel page tables.
+	 */
+	lis	r11, PAGE_OFFSET@h
+	cmplw	5, r10, r11
+	blt	5, 3f
+	lis	r11, swapper_pg_dir@h
+	ori	r11, r11, swapper_pg_dir@l
+
+	mfspr	r12,SPRN_MAS1		/* Set TID to 0 */
+	rlwinm	r12,r12,0,16,1
+	mtspr	SPRN_MAS1,r12
+
+	b	4f
+
+	/* Get the PGD for the current thread */
+3:
+	mfspr	r11,SPRN_SPRG3
+	lwz	r11,PGDIR(r11)
+
+4:
+	FIND_PTE_ADDR
+
+	beq	2f	/* Bail if no entry */
+
+5:
+	LOAD_PTE
+
+	andi.	r13, r11, _PAGE_PRESENT	/* Is the page present? */
+	beq	2f			/* Bail if not present */
+
+	/*
+	 * We set execute, because we don't have the granularity to
+	 * properly set this at the page level (Linux problem).
+	 * Many of these bits are software only.  Bits we don't set
+	 * here we (properly should) assume have the appropriate value.
+	 */
+
+	mfspr	r10, SPRN_MAS2
+#ifdef CONFIG_PTE_64BIT
+	rlwimi	r10, r11, 26, 24, 31	/* extract ...WIMGE from pte */
+#else
+	rlwimi	r10, r11, 26, 27, 31	/* extract WIMGE from pte */
+#endif
+#ifdef CONFIG_SMP
+	ori	r10, r10, (MAS2_M)@l
+#endif
+	mtspr	SPRN_MAS2, r10
+
+	bge	5, 11f
+
+	/* is user addr */
+	andi.	r13, r11, (_PAGE_USER | _PAGE_HWWRITE | _PAGE_HWEXEC)
+	andi.	r10, r11, _PAGE_USER	/* Test for _PAGE_USER */
+	srwi	r10, r13, 1
+	or	r10, r13, r10	/* Copy user perms into supervisor */
+	iseleq	r10, 0, r10
+	b	22f
+
+	/* is kernel addr */
+11:	rlwinm	r10, r11, 31, 29, 29	/* Extract _PAGE_HWWRITE into SW */
+	ori	r10, r10, (MAS3_SX | MAS3_SR)
+
+#ifdef CONFIG_PTE_64BIT
+22:	lwz	r13, 0(r12)
+	rlwimi	r10, r13, 24, 0, 7	/* grab RPN[32:39] */
+	rlwimi	r10, r11, 24, 8, 19	/* grab RPN[40:51] */
+	mtspr	SPRN_MAS3, r10
+BEGIN_FTR_SECTION
+	srwi	r10, r13, 8		/* grab RPN[8:31] */
+	mtspr	SPRN_MAS7, r10
+END_FTR_SECTION_IFSET(CPU_FTR_BIG_PHYS)
+#else
+22:	rlwimi	r10, r11, 0, 0, 19	/* Extract RPN from PTE and
+					 * merge with perms */
+	mtspr	SPRN_MAS3, r10
+#endif
+	/*  The HY will check which privileged instruction trap privileged 
+	 *  exception via r3. */
+        lis     r3,TLBWE_CODE@h
+        ori     r3,r3,TLBWE_CODE@l
+	tlbwe
+
+#ifdef CONFIG_SMP
+	mr	r13, r11
+	LWARX_PTE
+	cmpw	r13, r11
+	bne-	7f
+#endif
+	ori	r11, r11, _PAGE_ACCESSED
+	STWCX_PTE
+
+#ifdef CONFIG_SMP
+	/*
+	 * If the stwcx. failed, we need to invalidate
+	 * the TLB entry, and re-lwarx the pte
+	 */
+	beq	88f
+7:	mfspr	r13, SPRN_MAS1
+	rlwinm	r13, r13, 0, 1, 31	/* Clear Valid bit */
+	mtspr	SPRN_MAS1, r13
+	/*  The HY will check which privileged instruction trap privileged 
+	 *  exception via r3. */
+        lis     r3,TLBWE_CODE@h
+        ori     r3,r3,TLBWE_CODE@l
+	tlbwe
+
+	b	5b
+#endif	/* CONFIG_SMP */
+
+88:
+	/* Done...restore registers and get out of here.  */
+	mfspr	r11, SPRN_SPRG7R
+	mtcr	r11
+	mfspr	r13, SPRN_SPRG5R
+	mfspr	r12, SPRN_SPRG4R
+	mfspr	r3,  SPRN_SPRG6R
+	mfspr	r11, SPRN_SPRG1
+	mfspr	r10, SPRN_SPRG0
+	rfi					/* Force context change */
+
+2:
+	/* The bailout.  Restore registers to pre-exception conditions
+	 * and call the heavyweights to help us out.
+	 */
+	mfspr	r11, SPRN_SPRG7R
+	mtcr	r11
+	mfspr	r13, SPRN_SPRG5R
+	mfspr	r12, SPRN_SPRG4R
+	mfspr	r3,  SPRN_SPRG6R
+	mfspr	r11, SPRN_SPRG1
+	mfspr	r10, SPRN_SPRG0
+#endif
 	b	data_access
 
 	/* Instruction TLB Error Interrupt */
@@ -485,6 +794,149 @@ interrupt_base:
 	 * to a different point.
 	 */
 	START_EXCEPTION(InstructionTLBError)
+#ifdef	CONFIG_PPC85xx_VT_MODE
+	mtspr	SPRN_SPRG0, r10		/* Save some working registers */
+	mtspr	SPRN_SPRG1, r11
+	mtspr	SPRN_SPRG6W, r3
+	mtspr	SPRN_SPRG4W, r12
+	mtspr	SPRN_SPRG5W, r13
+	mfcr	r11
+	mtspr	SPRN_SPRG7W, r11
+	mfspr	r10, SPRN_SRR0		/* Get faulting address */
+
+	/* If we are faulting a kernel address, we have to use the
+	 * kernel page tables.
+	 */
+	lis	r11, PAGE_OFFSET@h
+	cmplw	5, r10, r11
+	blt	5, 3f
+	lis	r11, swapper_pg_dir@h
+	ori	r11, r11, swapper_pg_dir@l
+
+	mfspr	r12,SPRN_MAS1		/* Set TID to 0 */
+	rlwinm	r12,r12,0,16,1
+	mtspr	SPRN_MAS1,r12
+
+	b	4f
+
+	/* Get the PGD for the current thread */
+3:
+	mfspr	r11,SPRN_SPRG3
+	lwz	r11,PGDIR(r11)
+
+4:
+	FIND_PTE_ADDR
+
+	beq	2f	/* Bail if no entry */
+
+5:
+	LOAD_PTE
+
+	andi.	r13, r11, _PAGE_PRESENT	/* Is the page present? */
+	beq	2f			/* Bail if not present */
+
+	/*
+	 * We set execute, because we don't have the granularity to
+	 * properly set this at the page level (Linux problem).
+	 * Many of these bits are software only.  Bits we don't set
+	 * here we (properly should) assume have the appropriate value.
+	 */
+
+	mfspr	r10, SPRN_MAS2
+#ifdef CONFIG_PTE_64BIT
+	rlwimi	r10, r11, 26, 24, 31	/* extract ...WIMGE from pte */
+#else
+	rlwimi	r10, r11, 26, 27, 31	/* extract WIMGE from pte */
+#endif
+#ifdef CONFIG_SMP
+	ori	r10, r10, (MAS2_M)@l
+#endif
+	mtspr	SPRN_MAS2, r10
+
+	bge	5, 11f
+
+	/* is user addr */
+	andi.	r13, r11, (_PAGE_USER | _PAGE_HWWRITE | _PAGE_HWEXEC)
+	andi.	r10, r11, _PAGE_USER	/* Test for _PAGE_USER */
+	srwi	r10, r13, 1
+	or	r10, r13, r10	/* Copy user perms into supervisor */
+	iseleq	r10, 0, r10
+	b	22f
+
+	/* is kernel addr */
+11:	rlwinm	r10, r11, 31, 29, 29	/* Extract _PAGE_HWWRITE into SW */
+	ori	r10, r10, (MAS3_SX | MAS3_SR)
+
+#ifdef CONFIG_PTE_64BIT
+22:	lwz	r13, 0(r12)
+	rlwimi	r10, r13, 24, 0, 7	/* grab RPN[32:39] */
+	rlwimi	r10, r11, 24, 8, 19	/* grab RPN[40:51] */
+	mtspr	SPRN_MAS3, r10
+BEGIN_FTR_SECTION
+	srwi	r10, r13, 8		/* grab RPN[8:31] */
+	mtspr	SPRN_MAS7, r10
+END_FTR_SECTION_IFSET(CPU_FTR_BIG_PHYS)
+#else
+22:	rlwimi	r10, r11, 0, 0, 19	/* Extract RPN from PTE and
+					 * merge with perms */
+	mtspr	SPRN_MAS3, r10
+#endif
+	/*  The HY will check which privileged instruction trap privileged 
+	 *  exception via r3. */
+        lis     r3,TLBWE_CODE@h
+        ori     r3,r3,TLBWE_CODE@l
+	tlbwe
+
+#ifdef CONFIG_SMP
+	mr	r13, r11
+	LWARX_PTE
+	cmpw	r13, r11
+	bne-	7f
+#endif
+	ori	r11, r11, _PAGE_ACCESSED
+	STWCX_PTE
+
+#ifdef CONFIG_SMP
+	/*
+	 * If the stwcx. failed, we need to invalidate
+	 * the TLB entry, and re-lwarx the pte
+	 */
+	beq	88f
+7:	mfspr	r13, SPRN_MAS1
+	rlwinm	r13, r13, 0, 1, 31	/* Clear Valid bit */
+	mtspr	SPRN_MAS1, r13
+	/*  The HY will check which privileged instruction trap privileged 
+	 *  exception via r3. */
+        lis     r3,TLBWE_CODE@h
+        ori     r3,r3,TLBWE_CODE@l
+	tlbwe
+
+	b	5b
+#endif	/* CONFIG_SMP */
+
+88:
+	/* Done...restore registers and get out of here.  */
+	mfspr	r11, SPRN_SPRG7R
+	mtcr	r11
+	mfspr	r13, SPRN_SPRG5R
+	mfspr	r12, SPRN_SPRG4R
+	mfspr	r3,  SPRN_SPRG6R
+	mfspr	r11, SPRN_SPRG1
+	mfspr	r10, SPRN_SPRG0
+	rfi					/* Force context change */
+
+2:
+	/* The bailout.  Restore registers to pre-exception conditions
+	 * and call the heavyweights to help us out.
+	 */
+	mfspr	r11, SPRN_SPRG7R
+	mtcr	r11
+	mfspr	r13, SPRN_SPRG5R
+	mfspr	r12, SPRN_SPRG4R
+	mfspr	r3,  SPRN_SPRG6R
+	mfspr	r11, SPRN_SPRG1
+	mfspr	r10, SPRN_SPRG0
+#endif
 	b	InstructionStorage
 
 	DEBUG_DEBUG_EXCEPTION
@@ -531,11 +983,17 @@ interrupt_base:
 	 */
 data_access:
 	NORMAL_EXCEPTION_PROLOG
+#ifdef	CONFIG_PPC85xx_VT_MODE
+	mfspr	r5,SPRN_ESR		/* Grab the ESR, save it, pass arg3 */
+	stw	r5,_ESR(r11)
+	mfspr	r4,SPRN_DEAR		/* Grab the DEAR, save it, pass arg2 */
+#else
 	lis	r6,wr_status@ha
 	lwz	r6,wr_status@l(r6)
 	lwz	r5, VB_STATUS_ESR(r6)
 	stw	r5,_ESR(r11)
 	lwz	r4, VB_STATUS_DEAR(r6)
+#endif
 	andis.	r10,r5,(ESR_ILK|ESR_DLK)@h
 	bne	1f
 	EXC_XFER_EE_LITE(0x0300, handle_page_fault)
-- 
1.6.5.2

