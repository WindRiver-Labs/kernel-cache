From b3f0b442e6ef07bc7e9fc118a24337bd036e5894 Mon Sep 17 00:00:00 2001
From: Kevin Hao <kexin.hao@windriver.com>
Date: Mon, 21 Sep 2009 11:07:01 +0800
Subject: [PATCH 13/52] add Freescale Queue Manager support

Signed-off-by: Andy Fleming <afleming@freescale.com>
Signed-off-by: Emil Medve <Emilian.Medve@Freescale.com>
Signed-off-by: Geoff Thorpe <geoff@geoffthorpe.net>
Signed-off-by: Kim Phillips <kim.phillips@freescale.com>
Signed-off-by: Kumar Gala <galak@kernel.crashing.org>
Signed-off-by: Roy Pledge <roy.pledge@freescale.com>
Signed-off-by: Scott Wood <scottwood@freescale.com>
Signed-off-by: Timur Tabi <timur@freescale.com>
[KevinHao: Original headerless patch (p4080_1-2-rc1-dpa.patch)
taken from Freescale rev 1.2 board support ISO image for p4080.
Change cpumask_of to get_cpu_mask]
Integrated-by: Kevin Hao <kexin.hao@windriver.com>
---
 drivers/Kconfig                       |    2 +
 drivers/Makefile                      |    1 +
 drivers/hwqueue/Kconfig               |  131 +++
 drivers/hwqueue/Makefile              |    8 +
 drivers/hwqueue/qman_config.c         |  496 ++++++++++++
 drivers/hwqueue/qman_driver.c         |  407 ++++++++++
 drivers/hwqueue/qman_fqalloc.c        |  203 +++++
 drivers/hwqueue/qman_high.c           | 1415 +++++++++++++++++++++++++++++++++
 drivers/hwqueue/qman_low.c            | 1030 ++++++++++++++++++++++++
 drivers/hwqueue/qman_private.h        |  149 ++++
 drivers/hwqueue/qman_sys.h            |   99 +++
 drivers/hwqueue/qman_test.c           |   84 ++
 drivers/hwqueue/qman_test.h           |   86 ++
 drivers/hwqueue/qman_test_fqrange.c   |   65 ++
 drivers/hwqueue/qman_test_high.c      |  233 ++++++
 drivers/hwqueue/qman_test_hotpotato.c |  449 +++++++++++
 drivers/hwqueue/qman_test_low.c       |  480 +++++++++++
 drivers/hwqueue/qman_utility.c        |  132 +++
 18 files changed, 5470 insertions(+), 0 deletions(-)
 create mode 100644 drivers/hwqueue/Kconfig
 create mode 100644 drivers/hwqueue/Makefile
 create mode 100644 drivers/hwqueue/qman_config.c
 create mode 100644 drivers/hwqueue/qman_driver.c
 create mode 100644 drivers/hwqueue/qman_fqalloc.c
 create mode 100644 drivers/hwqueue/qman_high.c
 create mode 100644 drivers/hwqueue/qman_low.c
 create mode 100644 drivers/hwqueue/qman_private.h
 create mode 100644 drivers/hwqueue/qman_sys.h
 create mode 100644 drivers/hwqueue/qman_test.c
 create mode 100644 drivers/hwqueue/qman_test.h
 create mode 100644 drivers/hwqueue/qman_test_fqrange.c
 create mode 100644 drivers/hwqueue/qman_test_high.c
 create mode 100644 drivers/hwqueue/qman_test_hotpotato.c
 create mode 100644 drivers/hwqueue/qman_test_low.c
 create mode 100644 drivers/hwqueue/qman_utility.c

diff --git a/drivers/Kconfig b/drivers/Kconfig
index 59f33fa..8462eee 100644
--- a/drivers/Kconfig
+++ b/drivers/Kconfig
@@ -101,4 +101,6 @@ source "drivers/auxdisplay/Kconfig"
 source "drivers/uio/Kconfig"
 
 source "drivers/xen/Kconfig"
+
+source "drivers/hwqueue/Kconfig"
 endmenu
diff --git a/drivers/Makefile b/drivers/Makefile
index 39cfe40..f98c14b 100644
--- a/drivers/Makefile
+++ b/drivers/Makefile
@@ -103,3 +103,4 @@ obj-$(CONFIG_OF)		+= of/
 obj-$(CONFIG_SSB)		+= ssb/
 obj-$(CONFIG_VIRTIO)		+= virtio/
 obj-$(CONFIG_REGULATOR)		+= regulator/
+obj-y				+= hwqueue/
diff --git a/drivers/hwqueue/Kconfig b/drivers/hwqueue/Kconfig
new file mode 100644
index 0000000..3e167d6
--- /dev/null
+++ b/drivers/hwqueue/Kconfig
@@ -0,0 +1,131 @@
+menu "Hardware queue support"
+
+menuconfig FSL_QMAN
+	bool "Freescale Queue Manager (datapath) support"
+	depends on PPC_E500MC
+	default y
+	---help---
+	  If unsure, say Y.
+
+if FSL_QMAN
+
+config FSL_QMAN_CHECKING
+	bool "additional driver checking"
+	default y
+	---help---
+	  Compiles in additional checks to sanity-check the Qman driver and any
+	  use of it by other code. Not recommended for performance.
+
+config FSL_QMAN_PORTAL
+	bool "Qman portal support"
+	default y
+	---help---
+	  Compiles support to detect and support Qman software corenet portals
+	  (as provided by the device-tree).
+
+config FSL_QMAN_PORTAL_DISABLEAUTO
+	bool "disable auto-initialisation of cpu-affine portals"
+	depends on FSL_QMAN_PORTAL
+	default n
+	---help---
+	  The high-level portal API, in its normal usage, requires that each cpu
+	  have a portal assigned to it that is auto-initialised. If an
+	  application is manually initialising portals in a non-cpu-affine
+	  manner (or you are using the low-level portal API), this may need to
+	  be disabled. If in doubt, say N.
+
+config FSL_QMAN_PORTAL_DISABLEAUTO_DCA
+	bool "disable discrete-consumption support on cpu-affine portals"
+	depends on !FSL_QMAN_PORTAL_DISABLEAUTO
+	default n
+	---help---
+	  By default, auto-initialised cpu-affine portals support
+	  discrete-consumption acknowledgements, but this may be unimplemented
+	  in the simulation model.
+
+config FSL_QMAN_FQALLOCATOR
+	bool "use Bman buffer pool 0 as a Qman FQ allocator"
+	depends on FSL_QMAN_PORTAL && !FSL_BMAN_PORTAL_DISABLEAUTO
+	default y
+	---help---
+	  If enabled, the Qman driver will assume that Bman buffer pool 0 has
+	  been preseeded with frame queue IDs available for dynamic allocation.
+
+config FSL_QMAN_FQRANGE
+	bool "Implement a control-plane only allocator supporting ranges"
+	default y
+	---help---
+	  This allocator does not use Bman or any other h/w resource, it
+	  provides a s/w allocator implementation that supports allocating
+	  and deallocating FQID ranges, with control over alignment.
+
+config FSL_QMAN_CONFIG
+	bool "Qman device management"
+	default y
+	---help---
+	  If this linux image is running natively, you need this option. If this
+	  linux image is running as a guest OS under the hypervisor, only one
+	  guest OS ("the control plane") needs this option.
+
+config FSL_QMAN_BUG_CGR_INIT
+	bool "workaround early-silicon h/w bug"
+	depends on FSL_QMAN_CONFIG && !FSL_QMAN_PORTAL_DISABLEAUTO
+	default y
+	---help---
+	  Only an early revision of Qman h/w, s/w must explicitly initialise all
+	  CGR memories to avoid a possible ECC error. This workaround will
+	  perform the required initialisation on boot, and has no side-effects
+	  thereafter. If in doubt, select Yes.
+
+config FSL_QMAN_TEST
+	tristate "Qman self-tests"
+	depends on FSL_QMAN_PORTAL
+	default n
+	---help---
+	  This option compiles self-test code for Qman.
+
+config FSL_QMAN_TEST_STASH_POTATO
+	bool "Qman 'hot potato' data-stashing self-test"
+	depends on FSL_QMAN_TEST && FSL_QMAN_FQALLOCATOR && !FSL_QMAN_PORTAL_DISABLEAUTO
+	default y
+	---help---
+	  This performs a "hot potato" style test enqueuing/dequeuing a frame
+	  across a series of FQs scheduled to different portals (and cpus), with
+	  DQRR, data and context stashing always on.
+
+config FSL_QMAN_TEST_LOW_BUG_INTERRUPTS
+	bool "ignore interrupts and interrupt registers"
+	depends on FSL_QMAN_TEST_LOW
+	default n
+	---help---
+	  Certain versions of the cycle-accurate model don't support portal
+	  interrupt lines or the associated registers.
+
+config FSL_QMAN_TEST_LOW_BUG_STASHING
+	bool "disable dequeue stashing (of DQRR and data/context)"
+	depends on FSL_QMAN_TEST_LOW
+	default n
+	---help---
+	  Support for DQRR stashing with the cycle-accurate model is still
+	  experimental.
+
+config FSL_QMAN_TEST_HIGH
+	bool "Qman high-level self-test"
+	depends on FSL_QMAN_TEST && !FSL_QMAN_PORTAL_DISABLEAUTO
+	default y
+	---help---
+	  This requires the presence of cpu-affine portals, and performs
+	  high-level API testing with them (whichever portal(s) are affine to
+	  the cpu(s) the test executes on).
+
+config FSL_QMAN_TEST_HIGH_BUG_VDQCR
+	bool "don't test VDQCR"
+	depends on FSL_QMAN_TEST_HIGH
+	default n
+	---help---
+	  Certain (current) versions of the cycle-accurate model don't support
+	  VDQCR.
+
+endif # FSL_QMAN
+
+endmenu
diff --git a/drivers/hwqueue/Makefile b/drivers/hwqueue/Makefile
new file mode 100644
index 0000000..b858520
--- /dev/null
+++ b/drivers/hwqueue/Makefile
@@ -0,0 +1,8 @@
+obj-$(CONFIG_FSL_QMAN)		+= qman_utility.o
+obj-$(CONFIG_FSL_QMAN_CONFIG)	+= qman_config.o
+obj-$(CONFIG_FSL_QMAN_PORTAL)	+= qman_driver.o qman_low.o qman_high.o
+obj-$(CONFIG_FSL_QMAN_FQALLOCATOR) += qman_fqalloc.o
+obj-$(CONFIG_FSL_QMAN_TEST)	+= qman_tester.o
+qman_tester-y			 = qman_test.o qman_test_hotpotato.o \
+				qman_test_high.o qman_test_low.o
+qman_tester-$(CONFIG_FSL_QMAN_FQRANGE) += qman_test_fqrange.o
diff --git a/drivers/hwqueue/qman_config.c b/drivers/hwqueue/qman_config.c
new file mode 100644
index 0000000..62f12b5
--- /dev/null
+++ b/drivers/hwqueue/qman_config.c
@@ -0,0 +1,496 @@
+/* Copyright (c) 2008, 2009 Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef CONFIG_SMP
+#include <linux/smp.h>	/* get_hard_smp_processor_id() */
+#endif
+
+#include "qman_private.h"
+
+/* Last updated for v00.800 of the BG */
+
+/* Register offsets */
+#define REG_QCSP_PID_CFG(n)	(0x0000 + ((n) * 0x10))
+#define REG_QCSP_IO_CFG(n)	(0x0004 + ((n) * 0x10))
+#define REG_QCSP_DD_CFG(n)	(0x000c + ((n) * 0x10))
+#define REG_DD_CFG		0x0200
+#define REG_DCP_CFG(n)		(0x0300 + ((n) * 0x10))
+#define REG_DCP_DD_CFG(n)	(0x0304 + ((n) * 0x10))
+#define REG_PFDR_FPC		0x0400
+#define REG_PFDR_FP_HEAD	0x0404
+#define REG_PFDR_FP_TAIL	0x0408
+#define REG_PFDR_FP_LWIT	0x0410
+#define REG_PFDR_CFG		0x0414
+#define REG_SFDR_CFG		0x0500
+#define REG_WQ_CS_CFG(n)	(0x0600 + ((n) * 0x04))
+#define REG_WQ_DEF_ENC_WQID	0x0630
+#define REG_WQ_SC_DD_CFG(n)	(0x640 + ((n) * 0x04))
+#define REG_WQ_PC_DD_CFG(n)	(0x680 + ((n) * 0x04))
+#define REG_WQ_DC0_DD_CFG(n)	(0x6c0 + ((n) * 0x04))
+#define REG_WQ_DC1_DD_CFG(n)	(0x700 + ((n) * 0x04))
+#define REG_WQ_DCn_DD_CFG(n)	(0x6c0 + ((n) * 0x40)) /* n=2,3 */
+#define REG_CM_CFG		0x0800
+#define REG_MCR			0x0b00
+#define REG_MCP(n)		(0x0b04 + ((n) * 0x04))
+#define REG_IP_REV_1		0x0bf8
+#define REG_IP_REV_2		0x0bfc
+#define REG_FQD_BARE		0x0c00
+#define REG_PFDR_BARE		0x0c20
+#define REG_offset_BAR		0x0004	/* relative to REG_[FQD|PFDR]_BARE */
+#define REG_offset_AR		0x0010	/* relative to REG_[FQD|PFDR]_BARE */
+#define REG_QCSP_BARE		0x0c80
+#define REG_QCSP_BAR		0x0c84
+#define REG_CI_SCHED_CFG	0x0d00
+#define REG_SRCIDR		0x0d04
+#define REG_LIODNR		0x0d08
+#define REG_ERR_ISR		0x0e00	/* + "enum qm_isr_reg" */
+
+/* Assists for QMAN_MCR */
+#define MCR_INIT_PFDR		0x01000000
+#define MCR_get_rslt(v)		(u8)((v) >> 24)
+#define MCR_rslt_idle(r)	(!rslt || (rslt >= 0xf0))
+#define MCR_rslt_ok(r)		(rslt == 0xf0)
+#define MCR_rslt_eaccess(r)	(rslt == 0xf8)
+#define MCR_rslt_inval(r)	(rslt == 0xff)
+
+struct qman;
+
+/* Follows WQ_CS_CFG0-5 */
+enum qm_wq_class {
+	qm_wq_portal = 0,
+	qm_wq_pool = 1,
+	qm_wq_fman0 = 2,
+	qm_wq_fman1 = 3,
+	qm_wq_caam = 4,
+	qm_wq_pme = 5
+};
+
+/* Follows FQD_[BARE|BAR|AR] and PFDR_[BARE|BAR|AR] */
+enum qm_memory {
+	qm_memory_fqd,
+	qm_memory_pfdr
+};
+
+/* Used by all error interrupt registers except 'inhibit' */
+#define QM_EIRQ_CIDE	0x20000000	/* Corenet Initiator Data Error */
+#define QM_EIRQ_CTDE	0x10000000	/* Corenet Target Data Error */
+#define QM_EIRQ_CITT	0x08000000	/* Corenet Invalid Target Transaction */
+#define QM_EIRQ_PLWI	0x04000000	/* PFDR Low Watermark */
+#define QM_EIRQ_MBEI	0x01000000	/* Multi-bit ECC Error */
+#define QM_EIRQ_SBEI	0x00800000	/* Single-bit ECC Error */
+#define QM_EIRQ_ICVI	0x00010000	/* Invalid Command Verb */
+#define QM_EIRQ_IDDI	0x00000800	/* Invalid Dequeue (Direct-connect) */
+#define QM_EIRQ_IDFI	0x00000400	/* Invalid Dequeue FQ */
+#define QM_EIRQ_IDSI	0x00000200	/* Invalid Dequeue Source */
+#define QM_EIRQ_IDQI	0x00000100	/* Invalid Dequeue Queue */
+#define QM_EIRQ_IEOI	0x00000008	/* Invalid Enqueue Overflow */
+#define QM_EIRQ_IESI	0x00000004	/* Invalid Enqueue State */
+#define QM_EIRQ_IECI	0x00000002	/* Invalid Enqueue Channel */
+#define QM_EIRQ_IEQI	0x00000001	/* Invalid Enqueue Queue */
+
+/**
+ * qm_err_isr_<reg>_<verb> - Manipulate global interrupt registers
+ * @v: for accessors that write values, this is the 32-bit value
+ *
+ * Manipulates QMAN_ERR_ISR, QMAN_ERR_IER, QMAN_ERR_ISDR, QMAN_ERR_IIR. All
+ * manipulations except qm_err_isr_[un]inhibit() use 32-bit masks composed of
+ * the QM_EIRQ_*** definitions. Note that "qm_err_isr_enable_write" means
+ * "write the enable register" rather than "enable the write register"!
+ */
+#define qm_err_isr_status_read(qm)	__qm_err_isr_read(qm, qm_isr_status)
+#define qm_err_isr_status_clear(qm, m)	__qm_err_isr_write(qm, qm_isr_status,m)
+#define qm_err_isr_enable_read(qm)	__qm_err_isr_read(qm, qm_isr_enable)
+#define qm_err_isr_enable_write(qm, v)	__qm_err_isr_write(qm, qm_isr_enable,v)
+#define qm_err_isr_disable_read(qm)	__qm_err_isr_read(qm, qm_isr_disable)
+#define qm_err_isr_disable_write(qm, v)	__qm_err_isr_write(qm, qm_isr_disable,v)
+#define qm_err_isr_inhibit(qm)		__qm_err_isr_write(qm, qm_isr_inhibit,1)
+#define qm_err_isr_uninhibit(qm)	__qm_err_isr_write(qm, qm_isr_inhibit,0)
+
+/*
+ * TODO: unimplemented registers
+ *
+ * Keeping a list here of Qman registers I have not yet covered;
+ * QCSP_DD_IHRSR, QCSP_DD_IHRFR, QCSP_DD_HASR,
+ * DCP_DD_IHRSR, DCP_DD_IHRFR, DCP_DD_HASR, CM_CFG,
+ * QMAN_ECSR, QMAN_ECIR, QMAN_EADR, QMAN_EECC, QMAN_EDATA0-7,
+ * QMAN_SBET, QMAN_EINJ, QMAN_SBEC0-12
+ */
+
+/* Encapsulate "struct qman *" as a cast of the register space address. */
+
+static struct qman *qm_create(void *regs)
+{
+	return (struct qman *)regs;
+}
+
+static inline u32 __qm_in(struct qman *qm, u32 offset)
+{
+	return in_be32((void *)qm + offset);
+}
+static inline void __qm_out(struct qman *qm, u32 offset, u32 val)
+{
+	out_be32((void *)qm + offset, val);
+}
+#define qm_in(reg)		__qm_in(qm, REG_##reg)
+#define qm_out(reg, val)	__qm_out(qm, REG_##reg, val)
+
+#if 0
+
+static u32 __qm_err_isr_read(struct qman *qm, enum qm_isr_reg n)
+{
+	return __qm_in(qm, REG_ERR_ISR + (n << 2));
+}
+
+static void __qm_err_isr_write(struct qman *qm, enum qm_isr_reg n, u32 val)
+{
+	__qm_out(qm, REG_ERR_ISR + (n << 2), val);
+}
+
+static void qm_set_portal(struct qman *qm, u8 swportalID,
+			u16 ec_tp_cfg, u16 ecd_tp_cfg)
+{
+	qm_out(QCSP_DD_CFG(swportalID),
+		((ec_tp_cfg & 0x1ff) << 16) | (ecd_tp_cfg & 0x1ff));
+}
+
+static void qm_set_ddebug(struct qman *qm, u8 mdd, u8 m_cfg)
+{
+	qm_out(DD_CFG, ((mdd & 0x3) << 4) | (m_cfg & 0xf));
+}
+
+static void qm_set_dc(struct qman *qm, enum qm_dc_portal portal, int ed, u8 sernd)
+{
+	QM_ASSERT(!ed || (portal == qm_dc_portal_fman0) ||
+			(portal == qm_dc_portal_fman1));
+	qm_out(DCP_CFG(portal), (ed ? 0x100 : 0) | (sernd & 0x1f));
+}
+
+static void qm_set_dc_ddebug(struct qman *qm, enum qm_dc_portal portal, u16 ecd_tp_cfg)
+{
+	qm_out(DCP_DD_CFG(portal), ecd_tp_cfg & 0x1ff);
+}
+
+static u32 qm_get_pfdr_free_pool_count(struct qman *qm)
+{
+	return qm_in(PFDR_FPC);
+}
+
+static void qm_get_pfdr_free_pool(struct qman *qm, u32 *head, u32 *tail)
+{
+	*head = qm_in(PFDR_FP_HEAD);
+	*tail = qm_in(PFDR_FP_TAIL);
+}
+
+static void qm_set_wq_scheduling(struct qman *qm, enum qm_wq_class wq_class,
+			u8 cs_elev, u8 csw2, u8 csw3, u8 csw4, u8 csw5,
+			u8 csw6, u8 csw7)
+{
+	qm_out(WQ_CS_CFG(wq_class), ((cs_elev & 0xff) << 24) |
+		((csw2 & 0x7) << 20) | ((csw3 & 0x7) << 16) |
+		((csw4 & 0x7) << 12) | ((csw5 & 0x7) << 8) |
+		((csw6 & 0x7) << 4) | (csw7 & 0x7));
+}
+
+static void qm_set_default_wq(struct qman *qm, u16 wqid)
+{
+	qm_out(WQ_DEF_ENC_WQID, wqid);
+}
+
+static void qm_set_channel_ddebug(struct qman *qm, enum qm_channel channel,
+				u16 tp_cfg)
+{
+	u32 offset;
+	int upperhalf = 0;
+	if ((channel >= qm_channel_swportal0) &&
+				(channel <= qm_channel_swportal9)) {
+		offset = (channel - qm_channel_swportal0);
+		upperhalf = offset & 0x1;
+		offset = REG_WQ_SC_DD_CFG(offset / 2);
+	} else if ((channel >= qm_channel_pool1) &&
+				(channel <= qm_channel_pool15)) {
+		offset = (channel + 1 - qm_channel_pool1);
+		upperhalf = offset & 0x1;
+		offset = REG_WQ_PC_DD_CFG(offset / 2);
+	} else if ((channel >= qm_channel_fman0_sp0) &&
+				(channel <= qm_channel_fman0_sp11)) {
+		offset = (channel - qm_channel_fman0_sp0);
+		upperhalf = offset & 0x1;
+		offset = REG_WQ_DC0_DD_CFG(offset / 2);
+	}
+	else if ((channel >= qm_channel_fman1_sp0) &&
+				(channel <= qm_channel_fman1_sp11)) {
+		offset = (channel - qm_channel_fman1_sp0);
+		upperhalf = offset & 0x1;
+		offset = REG_WQ_DC1_DD_CFG(offset / 2);
+	}
+	else if (channel == qm_channel_caam)
+		offset = REG_WQ_DCn_DD_CFG(2);
+	else if (channel == qm_channel_pme)
+		offset = REG_WQ_DCn_DD_CFG(3);
+	else {
+		pr_crit("Illegal qm_channel type %d\n", channel);
+		return;
+	}
+	__qm_out(qm, offset, upperhalf ? ((u32)tp_cfg << 16) : tp_cfg);
+}
+
+static void qm_get_details(struct qman *qm, u8 *int_options, u8 *errata,
+			u8 *conf_options)
+{
+	u32 v = qm_in(IP_REV_1);
+	*int_options = (v >> 16) & 0xff;
+	*errata = (v >> 8) & 0xff;
+	*conf_options = v & 0xff;
+}
+
+static void qm_set_corenet_bar(struct qman *qm, u16 eba, u32 ba)
+{
+	/* choke if 'ba' isn't properly aligned */
+	QM_ASSERT(!(ba & 0x001fffff));
+	qm_out(QCSP_BARE, eba);
+	qm_out(QCSP_BAR, ba);
+}
+
+static void qm_set_corenet_initiator(struct qman *qm, int write_srcciv,
+				u8 srcciv, u8 srq_w, u8 rw_w, u8 bman_w)
+{
+	qm_out(CI_SCHED_CFG, (write_srcciv ? 0x80000000 : 0x0) |
+		((srcciv & 0xf) << 24) |
+		((srq_w & 0x7) << 8) | ((rw_w & 0x7) << 4) |
+		(bman_w & 0x7));
+}
+
+static u8 qm_get_corenet_sourceid(struct qman *qm)
+{
+	return qm_in(SRCIDR);
+}
+
+static u16 qm_get_liodn(struct qman *qm)
+{
+	return qm_in(LIODNR);
+}
+
+static void qm_set_congestion_config(struct qman *qm, u16 pres)
+{
+	qm_out(CM_CFG, pres);
+}
+
+#endif
+
+static void qm_get_version(struct qman *qm, u16 *id, u8 *major, u8 *minor)
+{
+	u32 v = qm_in(IP_REV_1);
+	*id = (v >> 16);
+	*major = (v >> 8) & 0xff;
+	*minor = v & 0xff;
+}
+
+static void qm_set_memory(struct qman *qm, enum qm_memory memory, u16 eba,
+			u32 ba, int enable, int prio, int stash, u32 size)
+{
+	u32 offset = (memory == qm_memory_fqd) ? REG_FQD_BARE : REG_PFDR_BARE;
+	u32 exp = ilog2(size);
+	/* choke if size isn't within range */
+	QM_ASSERT((size >= 4096) && (size <= 1073741824) &&
+			is_power_of_2(size));
+	/* choke if 'ba' has lower-alignment than 'size' */
+	QM_ASSERT(!(ba & (size - 1)));
+	__qm_out(qm, offset, eba);
+	__qm_out(qm, offset + REG_offset_BAR, ba);
+	__qm_out(qm, offset + REG_offset_AR,
+		(enable ? 0x80000000 : 0) |
+		(prio ? 0x40000000 : 0) |
+		(stash ? 0x20000000 : 0) |
+		(exp - 1));
+}
+
+static void qm_set_pfdr_threshold(struct qman *qm, u32 th, u8 k)
+{
+	qm_out(PFDR_FP_LWIT, th & 0xffffff);
+	qm_out(PFDR_CFG, k);
+}
+
+static void qm_set_sfdr_threshold(struct qman *qm, u16 th)
+{
+	qm_out(SFDR_CFG, th & 0x3ff);
+}
+
+static int qm_init_pfdr(struct qman *qm, u32 pfdr_start, u32 num)
+{
+	u8 rslt = MCR_get_rslt(qm_in(MCR));
+
+	QM_ASSERT(pfdr_start && !(pfdr_start & 7) && !(num & 7) && num);
+	/* Make sure the command interface is 'idle' */
+	if(!MCR_rslt_idle(rslt))
+		panic("QMAN_MCR isn't idle");
+
+	/* Write the MCR command params then the verb */
+	qm_out(MCP(0), pfdr_start );
+	/* TODO: remove this - it's a workaround for a model bug that is
+	 * corrected in more recent versions. We use the workaround until
+	 * everyone has upgraded. */
+	qm_out(MCP(1), (pfdr_start + num - 16));
+	lwsync();
+	qm_out(MCR, MCR_INIT_PFDR);
+
+	/* Poll for the result */
+	do {
+		rslt = MCR_get_rslt(qm_in(MCR));
+	} while(!MCR_rslt_idle(rslt));
+	if (MCR_rslt_ok(rslt))
+		return 0;
+	if (MCR_rslt_eaccess(rslt))
+		return -EACCES;
+	if (MCR_rslt_inval(rslt))
+		return -EINVAL;
+	pr_crit("Unexpected result from MCR_INIT_PFDR: %02x\n", rslt);
+	return -ENOSYS;
+}
+
+/*****************/
+/* Config driver */
+/*****************/
+
+/* TODO: Kconfig these? */
+#define DEFAULT_FQD_SZ	(PAGE_SIZE << 9)
+#define DEFAULT_PFDR_SZ	(PAGE_SIZE << 12)
+
+/* We support only one of these */
+static struct qman *qm;
+
+/* Parse the <name> property to extract the memory location and size and
+ * lmb_reserve() it. If it isn't supplied, lmb_alloc() the default size. */
+static __init int parse_mem_property(struct device_node *node, const char *name,
+				dma_addr_t *addr, size_t *sz, int zero)
+{
+	const u32 *pint;
+	int ret;
+
+	pint = of_get_property(node, name, &ret);
+	if (!pint || (ret != 16)) {
+		pr_info("No %s property '%s', using lmb_alloc(%08x)\n",
+				node->full_name, name, *sz);
+		*addr = lmb_alloc(*sz, *sz);
+		if (zero)
+			memset(phys_to_virt(*addr), 0, *sz);
+		return 0;
+	}
+	pr_info("Using %s property '%s'\n", node->full_name, name);
+	/* Props are 64-bit, but dma_addr_t is (currently) 32-bit */
+	BUG_ON(sizeof(*addr) != 4);
+	BUG_ON(pint[0] || pint[2]);
+	*addr = pint[1];
+	*sz = pint[3];
+	/* Keep things simple, it's either all in the DRAM range or it's all
+	 * outside. */
+	if (*addr < lmb_end_of_DRAM()) {
+		BUG_ON((u64)*addr + (u64)*sz > lmb_end_of_DRAM());
+		if (lmb_reserve(*addr, *sz) < 0) {
+			pr_err("Failed to reserve %s\n", name);
+			return -ENOMEM;
+		}
+		if (zero)
+			memset(phys_to_virt(*addr), 0, *sz);
+	} else {
+		/* map as cacheable, non-guarded */
+		void *tmpp = ioremap_flags(*addr, *sz, 0);
+		if (zero)
+			memset(tmpp, 0, *sz);
+		iounmap(tmpp);
+	}
+	return 0;
+}
+
+/* TODO:
+ * - there is obviously no handling of errors,
+ * - the calls to qm_set_memory() pass no upper-bits, the physical addresses
+ *   are cast on the assumption that they are <= 32bits. We BUG_ON() to handle
+ *   this for now,
+ * - the calls to qm_set_memory() hard-code the priority and CPC-stashing for
+ *   both memory resources to zero.
+ */
+static int __init fsl_qman_init(struct device_node *node)
+{
+	struct resource res;
+	u32 __iomem *regs;
+	dma_addr_t fqd_a, pfdr_a;
+	size_t fqd_sz = DEFAULT_FQD_SZ, pfdr_sz = DEFAULT_PFDR_SZ;
+	int ret;
+	u16 id;
+	u8 major, minor;
+
+	BUG_ON(sizeof(dma_addr_t) != sizeof(u32));
+	ret = of_address_to_resource(node, 0, &res);
+	if (ret) {
+		pr_err("Can't get %s property '%s'\n", node->full_name, "reg");
+		return ret;
+	}
+	ret = parse_mem_property(node, "fsl,qman-fqd", &fqd_a, &fqd_sz, 1);
+	BUG_ON(ret);
+	ret = parse_mem_property(node, "fsl,qman-pfdr", &pfdr_a, &pfdr_sz, 0);
+	BUG_ON(ret);
+	/* Global configuration */
+	regs = ioremap(res.start, res.end - res.start + 1);
+	qm = qm_create(regs);
+	qm_get_version(qm, &id, &major, &minor);
+	pr_info("Qman ver:%04x,%02x,%02x\n", id, major, minor);
+	/* FQD memory */
+	qm_set_memory(qm, qm_memory_fqd, 0, (u32)fqd_a, 1, 0, 0, fqd_sz);
+	/* PFDR memory */
+	qm_set_memory(qm, qm_memory_pfdr, 0, (u32)pfdr_a, 1, 0, 0, pfdr_sz);
+	qm_init_pfdr(qm, 8, pfdr_sz / 64 - 8);
+	/* thresholds */
+	qm_set_pfdr_threshold(qm, 32, 32);
+	qm_set_sfdr_threshold(qm, 128);
+	/* Workaround for bug 3594: "PAMU Address translation exception during
+	 * qman dqrr stashing". */
+	if (sizeof(dma_addr_t) <= sizeof(u32))
+		qm_out(QCSP_BARE, 0);
+	/* TODO: add interrupt handling here, so that ISR is cleared *after*
+	 * PFDR initialisation. */
+	return 0;
+}
+
+__init void qman_init_early(void)
+{
+	struct device_node *dn;
+	for_each_compatible_node(dn, NULL, "fsl,qman") {
+		if (qm)
+			pr_err("%s: only one 'fsl,qman' allowed\n",
+				dn->full_name);
+		else {
+			int ret = fsl_qman_init(dn);
+			BUG_ON(ret);
+		}
+	}
+}
+
diff --git a/drivers/hwqueue/qman_driver.c b/drivers/hwqueue/qman_driver.c
new file mode 100644
index 0000000..09989e4
--- /dev/null
+++ b/drivers/hwqueue/qman_driver.c
@@ -0,0 +1,407 @@
+/* Copyright (c) 2008, 2009 Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "qman_private.h"
+
+/*****************/
+/* Portal driver */
+/*****************/
+
+#define PORTAL_MAX	10
+#define POOL_MAX	16
+
+/* NB: we waste an entry because idx==0 isn't valid (pool-channels have hardware
+ * indexing from 1..15, as is reflected in the way SDQCR is encoded). However
+ * this scheme lets us use cell-index rather than searching. */
+struct __pool_channel {
+	struct qm_pool_channel cfg;
+	phandle ph;
+};
+static struct __pool_channel pools[POOL_MAX];
+static u32 pools_mask;
+
+static struct qm_portal portals[PORTAL_MAX];
+static u8 num_portals;
+#ifndef CONFIG_FSL_QMAN_PORTAL_DISABLEAUTO
+static u8 num_affine_portals;
+#endif
+static DEFINE_SPINLOCK(bind_lock);
+DEFINE_PER_CPU(struct qman_portal *, qman_affine_portal);
+
+static int __qm_pool_add(u32 idx, enum qm_channel channel, phandle ph)
+{
+	struct __pool_channel *c = &pools[idx];
+	BUG_ON(!idx || (idx >= POOL_MAX));
+	BUG_ON((channel < qm_channel_pool1) || (channel > qm_channel_pool15));
+	if (c->ph)
+		return -EBUSY;
+	c->cfg.pool = QM_SDQCR_CHANNELS_POOL(idx);
+	c->cfg.channel = channel;
+	c->cfg.portals = 0;
+	c->ph = ph;
+	pools_mask |= c->cfg.pool;
+	return 0;
+}
+
+static int __qm_link(struct qm_portal *portal, phandle pool_ph)
+{
+	int idx = 0;
+	struct __pool_channel *pool = &pools[0];
+	while ((idx < POOL_MAX) && (pool->ph != pool_ph)) {
+		idx++;
+		pool++;
+	}
+	if (idx == POOL_MAX)
+		return -EINVAL;
+	/* Link the pool to the portal */
+	pool->cfg.portals |= (1 << portal->index);
+	/* Link the portal to the pool */
+	portal->config.pools |= pool->cfg.pool;
+	return 0;
+}
+
+static struct qm_portal *__qm_portal_add(const struct qm_addr *addr,
+				const struct qm_portal_config *config)
+{
+	struct qm_portal *ret;
+	BUG_ON((num_portals + 1) > PORTAL_MAX);
+	ret = &portals[num_portals];
+	ret->index = num_portals++;
+	ret->addr = *addr;
+	ret->config = *config;
+	ret->config.bound = 0;
+	return ret;
+}
+
+int __qm_portal_bind(struct qm_portal *portal, u8 iface)
+{
+	int ret = -EBUSY;
+	spin_lock(&bind_lock);
+	if (!(portal->config.bound & iface)) {
+		portal->config.bound |= iface;
+		ret = 0;
+	}
+	spin_unlock(&bind_lock);
+	return ret;
+}
+
+void __qm_portal_unbind(struct qm_portal *portal, u8 iface)
+{
+	spin_lock(&bind_lock);
+	QM_ASSERT(portal->config.bound & iface);
+	portal->config.bound &= ~iface;
+	spin_unlock(&bind_lock);
+}
+
+u8 qm_portal_num(void)
+{
+	return num_portals;
+}
+EXPORT_SYMBOL(qm_portal_num);
+
+struct qm_portal *qm_portal_get(u8 idx)
+{
+	if (unlikely(idx >= num_portals))
+		return NULL;
+
+	return &portals[idx];
+}
+EXPORT_SYMBOL(qm_portal_get);
+
+const struct qm_portal_config *qm_portal_config(const struct qm_portal *portal)
+{
+	return &portal->config;
+}
+EXPORT_SYMBOL(qm_portal_config);
+
+u32 qm_pools(void)
+{
+	return pools_mask;
+}
+EXPORT_SYMBOL(qm_pools);
+
+const struct qm_pool_channel *qm_pool_channel(u32 mask)
+{
+	u32 idx = 0, m = 1;
+	struct __pool_channel *c = &pools[0];
+	while ((idx < POOL_MAX) && !(mask & m)) {
+		idx++;
+		m <<= 1;
+		c++;
+	}
+	if (idx == POOL_MAX)
+		return NULL;
+	return &c->cfg;
+}
+EXPORT_SYMBOL(qm_pool_channel);
+
+static int __init fsl_qman_pool_channel_init(struct device_node *node)
+{
+	phandle *ph;
+	int ret;
+	u32 *channel, *index = (u32 *)of_get_property(node, "cell-index", &ret);
+	if (!index || (ret != 4) || !*index || (*index > 15)) {
+		pr_err("Can't get %s property '%s'\n", node->full_name,
+			"cell-index");
+		return -ENODEV;
+	}
+	channel = (u32 *)of_get_property(node, "fsl,qman-channel-id", &ret);
+	if (!channel || (ret != 4)) {
+		pr_err("Can't get %s property '%s'\n", node->full_name,
+			"fsl,qman-channel-id");
+		return -ENODEV;
+	}
+	if (*channel != (*index + qm_channel_pool1 - 1))
+		pr_err("Warning: node %s has mismatched %s and %s\n",
+			node->full_name, "cell-index", "fsl,qman-channel-id");
+	ph = (phandle *)of_get_property(node, "linux,phandle", &ret);
+	if (!ph || (ret != sizeof(phandle))) {
+		pr_err("Can't get %s property '%s'\n", node->full_name,
+			"linux,phandle");
+		return -ENODEV;
+	}
+	ret = __qm_pool_add(*index, *channel, *ph);
+	if (ret)
+		pr_err("Failed to register pool channel %s\n", node->full_name);
+	return ret;
+}
+
+/* Handlers for NULL portal callbacks (ie. where the contextB field, normally
+ * pointing to the corresponding FQ object, is NULL). */
+static enum qman_cb_dqrr_result null_cb_dqrr(struct qman_portal *qm,
+					struct qman_fq *fq,
+					const struct qm_dqrr_entry *dqrr)
+{
+	pr_warning("Ignoring unowned DQRR frame on portal %p.\n", qm);
+	return qman_cb_dqrr_consume;
+}
+static void null_cb_mr(struct qman_portal *qm, struct qman_fq *fq,
+			const struct qm_mr_entry *msg)
+{
+	pr_warning("Ignoring unowned MR msg on portal %p, verb 0x%02x.\n",
+			qm, msg->verb);
+}
+static const struct qman_fq_cb null_cb = {
+	.dqrr = null_cb_dqrr,
+	.ern = null_cb_mr,
+	.dc_ern = null_cb_mr,
+	.fqs = null_cb_mr
+};
+
+static int __init fsl_qman_portal_init(struct device_node *node)
+{
+	struct resource res[2];
+	struct qm_portal_config cfg;
+	struct qm_addr addr;
+	struct qm_portal *portal;
+	const u32 *index, *channel;
+	const phandle *ph;
+#ifndef CONFIG_FSL_QMAN_PORTAL_DISABLEAUTO
+	struct qman_portal *affine_portal;
+#endif
+	int irq, ret, numpools;
+
+	ret = of_address_to_resource(node, 0, &res[0]);
+	if (ret) {
+		pr_err("Can't get %s property '%s'\n", node->full_name,
+			"reg::CE");
+		return ret;
+	}
+	ret = of_address_to_resource(node, 1, &res[1]);
+	if (ret) {
+		pr_err("Can't get %s property '%s'\n", node->full_name,
+			"reg::CI");
+		return ret;
+	}
+	index = of_get_property(node, "cell-index", &ret);
+	if (!index || (ret != 4)) {
+		pr_err("Can't get %s property '%s'\n", node->full_name,
+			"cell-index");
+		return -ENODEV;
+	}
+	channel = of_get_property(node, "fsl,qman-channel-id", &ret);
+	if (!channel || (ret != 4)) {
+		pr_err("Can't get %s property '%s'\n", node->full_name,
+			"fsl,qman-channel-id");
+		return -ENODEV;
+	}
+	if (*channel != (*index + qm_channel_swportal0))
+		pr_err("Warning: node %s has mismatched %s and %s\n",
+			node->full_name, "cell-index", "fsl,qman-channel-id");
+	cfg.channel = *channel;
+	cfg.cpu = -1;
+	ph = of_get_property(node, "cpu-handle", &ret);
+	if (ph && (ret != sizeof(phandle))) {
+		pr_err("Malformed %s property '%s'\n", node->full_name,
+			"cpu-handle");
+		return -ENODEV;
+	}
+	if (ph) {
+		const u32 *cpu_val;
+		struct device_node *cpu_node = of_find_node_by_phandle(*ph);
+		if (!cpu_node) {
+			pr_err("Bad %s property 'cpu-handle'\n",
+				cpu_node->full_name);
+			goto bad_cpu_ph;
+		}
+		cpu_val = of_get_property(cpu_node, "reg", &ret);
+		if (!cpu_val || (ret != sizeof(*cpu_val))) {
+			pr_err("Can't get %s property 'reg'\n",
+				cpu_node->full_name);
+			ret = -ENODEV;
+		} else {
+			int cpu;
+			ret = -ENODEV;
+			for_each_present_cpu(cpu) {
+				if (*cpu_val == get_hard_smp_processor_id(cpu)) {
+					ret = 0;
+					break;
+				}
+			}
+			if (ret)
+				pr_err("Invalid cpu index %d in %s\n", *cpu_val,
+					cpu_node->full_name);
+			else
+				cfg.cpu = cpu;
+		}
+		of_node_put(cpu_node);
+	}
+bad_cpu_ph:
+	ph = of_get_property(node, "fsl,qman-pool-channels", &ret);
+	if (ph && (ret % sizeof(phandle))) {
+		pr_err("Malformed %s property '%s'\n", node->full_name,
+			"fsl,qman-pool-channels");
+		return -ENODEV;
+	}
+	numpools = ph ? (ret / sizeof(phandle)) : 0;
+	irq = irq_of_parse_and_map(node, 0);
+	if (irq == NO_IRQ) {
+		pr_err("Can't get %s property '%s'\n", node->full_name,
+			"interrupts");
+		return -ENODEV;
+	}
+	cfg.irq = irq;
+	if (of_get_property(node, "fsl,hv-dma-handle", &ret))
+		cfg.has_hv_dma = 1;
+	else
+		cfg.has_hv_dma = 0;
+	addr.addr_ce = ioremap_flags(res[0].start,
+				res[0].end - res[0].start + 1, 0);
+	addr.addr_ci = ioremap_flags(res[1].start,
+				res[1].end - res[1].start + 1,
+				_PAGE_GUARDED | _PAGE_NO_CACHE);
+	cfg.pools = 0;
+	cfg.bound = 0;
+	pr_info("Qman portal at %p:%p (%d:%d)\n", addr.addr_ce, addr.addr_ci,
+		cfg.cpu, cfg.channel);
+	portal = __qm_portal_add(&addr, &cfg);
+	while (numpools--) {
+		int tmp = __qm_link(portal, *(ph++));
+		if (tmp)
+			panic("Unrecoverable error linking pool channels");
+	}
+	/* If the portal is affine to a cpu and that cpu has no default affine
+	 * portal, auto-initialise this one for the job. */
+#ifndef CONFIG_FSL_QMAN_PORTAL_DISABLEAUTO
+	if (cfg.cpu == -1)
+		return 0;
+	affine_portal = per_cpu(qman_affine_portal, cfg.cpu);
+	if (!affine_portal) {
+		u32 flags = QMAN_PORTAL_FLAG_IRQ | QMAN_PORTAL_FLAG_IRQ_FAST;
+#ifndef CONFIG_FSL_QMAN_PORTAL_DISABLEAUTO_DCA
+		flags |= QMAN_PORTAL_FLAG_DCA;
+#endif
+#ifdef CONFIG_FSL_QMAN_PORTAL_REQUIRE_ENABLE
+		flags |= QMAN_PORTAL_FLAG_DISABLE;
+#endif
+		if (cfg.has_hv_dma)
+			flags |= QMAN_PORTAL_FLAG_RSTASH |
+				QMAN_PORTAL_FLAG_DSTASH;
+		/* TODO: cgrs ?? */
+		affine_portal = qman_create_portal(portal, flags, NULL,
+						&null_cb);
+		if (!affine_portal)
+			pr_err("Qman portal auto-initialisation failed\n");
+		else {
+			/* default: enable all (available) pool channels */
+			qman_static_dequeue_add_ex(affine_portal, ~0);
+			pr_info("Qman portal %d auto-initialised\n", cfg.cpu);
+			per_cpu(qman_affine_portal, cfg.cpu) = affine_portal;
+			num_affine_portals++;
+		}
+	}
+#endif
+	return 0;
+}
+
+/***************/
+/* Driver load */
+/***************/
+
+static __init int qman_init(void)
+{
+	struct device_node *dn;
+	int ret;
+	for_each_compatible_node(dn, NULL, "fsl,qman-pool-channel") {
+		ret = fsl_qman_pool_channel_init(dn);
+		if (ret)
+			return ret;
+	}
+	for_each_compatible_node(dn, NULL, "fsl,qman-portal") {
+		ret = fsl_qman_portal_init(dn);
+		if (ret)
+			return ret;
+	}
+#ifndef CONFIG_FSL_QMAN_PORTAL_DISABLEAUTO
+	if (num_affine_portals == num_online_cpus()) {
+#ifdef CONFIG_FSL_QMAN_BUG_CGR_INIT
+		u32 cgid;
+		for (cgid = 0; cgid < 256; cgid++)
+			if (qman_init_cgr(cgid))
+				pr_err("CGR BUG workaround failed on CGID %d\n",
+					cgid);
+		pr_info("Qman CGR bug workaround, CGRs initialised\n");
+#endif
+	} else {
+		pr_err("Not all cpus have an affine Qman portal\n");
+		pr_err("Expect Qman-dependent drivers to crash!\n");
+	}
+#endif
+#ifdef CONFIG_FSL_QMAN_FQALLOCATOR
+	ret = __fqalloc_init();
+	if (ret)
+		return ret;
+#endif
+	pr_info("Qman driver initialised\n");
+	return 0;
+}
+subsys_initcall(qman_init);
diff --git a/drivers/hwqueue/qman_fqalloc.c b/drivers/hwqueue/qman_fqalloc.c
new file mode 100644
index 0000000..73b02d2
--- /dev/null
+++ b/drivers/hwqueue/qman_fqalloc.c
@@ -0,0 +1,203 @@
+/* Copyright (c) 2009 Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "qman_private.h"
+
+#ifdef CONFIG_FSL_QMAN_FQALLOCATOR
+
+#include <linux/fsl_bman.h>
+
+/****************/
+/* FQ allocator */
+/****************/
+
+static struct bman_pool *fq_pool;
+static const struct bman_pool_params fq_pool_params;
+
+__init int __fqalloc_init(void)
+{
+	fq_pool = bman_new_pool(&fq_pool_params);
+	if (!fq_pool)
+		return -ENOMEM;
+	return 0;
+}
+
+u32 qm_fq_new(void)
+{
+	struct bm_buffer buf;
+	int ret;
+
+	BUG_ON(!fq_pool);
+	ret = bman_acquire(fq_pool, &buf, 1, 0);
+	if (ret != 1)
+		return 0;
+	return buf.lo;
+}
+EXPORT_SYMBOL(qm_fq_new);
+
+int qm_fq_free_flags(u32 fqid, u32 flags)
+{
+	struct bm_buffer buf = {
+		.hi = 0,
+		.lo = fqid
+	};
+	u32 bflags = 0;
+	int ret;
+	if (flags & QM_FQ_FREE_WAIT) {
+		bflags |= BMAN_RELEASE_FLAG_WAIT;
+		if (flags & BMAN_RELEASE_FLAG_WAIT_INT)
+			bflags |= BMAN_RELEASE_FLAG_WAIT_INT;
+		if (flags & BMAN_RELEASE_FLAG_WAIT_SYNC)
+			bflags |= BMAN_RELEASE_FLAG_WAIT_SYNC;
+	}
+	ret = bman_release(fq_pool, &buf, 1, bflags);
+	return ret;
+}
+EXPORT_SYMBOL(qm_fq_free_flags);
+
+#endif /* CONFIG_FSL_QMAN_FQALLOCATOR */
+
+#ifdef CONFIG_FSL_QMAN_FQRANGE
+
+/* Global state for the allocator */
+static DEFINE_SPINLOCK(alloc_lock);
+static LIST_HEAD(alloc_list);
+
+/* The allocator is a (possibly-empty) list of these; */
+struct alloc_node {
+	struct list_head list;
+	u32 base;
+	u32 num;
+};
+
+int qman_alloc_fq_range(u32 *result, u32 count, u32 align, int partial)
+{
+	struct alloc_node *i, *next_best = NULL;
+	u32 base, next_best_base = 0, num = 0, next_best_num = 0;
+	struct alloc_node *margin_left, *margin_right;
+
+	/* If 'align' is 0, it should behave as though it was 1 */
+	if (!align)
+		align = 1;
+	margin_left = kmalloc(sizeof(*margin_left), GFP_KERNEL);
+	if (!margin_left)
+		return -ENOMEM;
+	margin_right = kmalloc(sizeof(*margin_right), GFP_KERNEL);
+	if (!margin_right) {
+		kfree(margin_left);
+		return -ENOMEM;
+	}
+	spin_lock_irq(&alloc_lock);
+	list_for_each_entry(i, &alloc_list, list) {
+		base = (i->base + align - 1) / align;
+		base *= align;
+		if ((base - i->base) >= i->num)
+			/* alignment is impossible, regardless of count */
+			continue;
+		num = i->num - (base - i->base);
+		if (num >= count) {
+			/* this one will do nicely */
+			num = count;
+			goto done;
+		}
+		if (num > next_best_num) {
+			next_best = i;
+			next_best_base = base;
+			next_best_num = num;
+		}
+	}
+	if (partial && next_best) {
+		i = next_best;
+		base = next_best_base;
+		num = next_best_num;
+	} else
+		i = NULL;
+done:
+	if (i) {
+		if (base != i->base) {
+			margin_left->base = i->base;
+			margin_left->num = base - i->base;
+			list_add_tail(&margin_left->list, &i->list);
+		} else
+			kfree(margin_left);
+		if ((base + num) < (i->base + i->num)) {
+			margin_right->base = base + num;
+			margin_right->num = (i->base + i->num) -
+						(base + num);
+			list_add(&margin_right->list, &i->list);
+		} else
+			kfree(margin_right);
+		list_del(&i->list);
+		kfree(i);
+		*result = base;
+	}
+	spin_unlock_irq(&alloc_lock);
+	return i ? num : -ENOMEM;
+}
+EXPORT_SYMBOL(qman_alloc_fq_range);
+
+void qman_release_fq_range(u32 fqid, unsigned int count)
+{
+	struct alloc_node *i, *node = kmalloc(sizeof(*node), GFP_KERNEL);
+	spin_lock_irq(&alloc_lock);
+	node->base = fqid;
+	node->num = count;
+	list_for_each_entry(i, &alloc_list, list) {
+		if (i->base >= node->base) {
+			list_add_tail(&node->list, &i->list);
+			goto done;
+		}
+	}
+	list_add_tail(&node->list, &alloc_list);
+done:
+	/* Merge to the left */
+	for (i = list_entry(node->list.prev, struct alloc_node, list);
+		(&i->list != &alloc_list) && (i->base + i->num == fqid);
+		i = list_entry(node->list.prev, struct alloc_node, list)) {
+		node->base = i->base;
+		node->num += i->num;
+		list_del(&i->list);
+		kfree(i);
+	}
+	/* Merge to the right */
+	for (i = list_entry(node->list.next, struct alloc_node, list);
+		(&i->list != &alloc_list) && (i->base == fqid + count);
+		i = list_entry(node->list.prev, struct alloc_node, list)) {
+		node->num += i->num;
+		list_del(&i->list);
+		kfree(i);
+	}
+	spin_unlock_irq(&alloc_lock);
+}
+EXPORT_SYMBOL(qman_release_fq_range);
+
+#endif /* CONFIG_FSL_QMAN_FQRANGE */
diff --git a/drivers/hwqueue/qman_high.c b/drivers/hwqueue/qman_high.c
new file mode 100644
index 0000000..9674d4e
--- /dev/null
+++ b/drivers/hwqueue/qman_high.c
@@ -0,0 +1,1415 @@
+/* Copyright (c) 2008, 2009 Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/* TODO:
+ *
+ * - make RECOVER also handle incomplete mgmt-commands
+ */
+
+#include <linux/bitops.h>
+#include "qman_private.h"
+
+/* Compilation constants */
+#define DQRR_MAXFILL	15
+#define DQRR_STASH_RING	0	/* if enabled, we ought to check SDEST */
+#define DQRR_STASH_DATA	0	/* ditto */
+#define EQCR_THRESH	1	/* reread h/w CI when running out of space */
+#define EQCR_ITHRESH	4	/* if EQCR congests, interrupt threshold */
+#define RECOVER_MSLEEP	100	/* DQRR and MR need to be empty for 0.1s */
+
+/* Lock/unlock frame queues, subject to the "LOCKED" flag. This is about
+ * inter-processor locking only. Note, FQLOCK() is always called either under a
+ * local_irq_disable() or from interrupt context - hence there's no need for
+ * spin_lock_irq() (and indeed, the nesting breaks as the "irq" bit isn't
+ * recursive...). */
+#define FQLOCK(fq) \
+	do { \
+		struct qman_fq *__fq478 = (fq); \
+		if (fq_isset(__fq478, QMAN_FQ_FLAG_LOCKED)) \
+			spin_lock(&__fq478->fqlock); \
+	} while(0)
+#define FQUNLOCK(fq) \
+	do { \
+		struct qman_fq *__fq478 = (fq); \
+		if (fq_isset(__fq478, QMAN_FQ_FLAG_LOCKED)) \
+			spin_unlock(&__fq478->fqlock); \
+	} while(0)
+
+static inline void fq_set(struct qman_fq *fq, u32 mask)
+{
+	set_bits(mask, &fq->flags);
+}
+static inline void fq_clear(struct qman_fq *fq, u32 mask)
+{
+	clear_bits(mask, &fq->flags);
+}
+static inline int fq_isset(struct qman_fq *fq, u32 mask)
+{
+	return fq->flags & mask;
+}
+static inline int fq_isclear(struct qman_fq *fq, u32 mask)
+{
+	return !(fq->flags & mask);
+}
+
+/**************/
+/* Portal API */
+/**************/
+
+#define PORTAL_BITS_RECOVER	0x00010000	/* use default callbacks */
+#define PORTAL_BITS_VDQCR	0x00008000	/* VDQCR active */
+#define PORTAL_BITS_MASK_V	0x00007fff
+#define PORTAL_BITS_NON_V	~(PORTAL_BITS_VDQCR | PORTAL_BITS_MASK_V)
+#define PORTAL_BITS_GET_V(p)	((p)->bits & PORTAL_BITS_MASK_V)
+#define PORTAL_BITS_INC_V(p) \
+	do { \
+		struct qman_portal *__p793 = (p); \
+		u32 __r793 = __p793->bits & PORTAL_BITS_NON_V; \
+		__r793 |= ((__p793->bits + 1) & PORTAL_BITS_MASK_V); \
+		__p793->bits = __r793; \
+	} while (0)
+
+struct qman_portal {
+	struct qm_portal *p;
+	/* 2-element array. cgrs[0] is mask, cgrs[1] is snapshot. */
+	struct qman_cgrs *cgrs;
+	/* To avoid overloading the term "flags", we use these 2; */
+	u32 options;	/* QMAN_PORTAL_FLAG_*** - static, caller-provided */
+	u32 bits;	/* PORTAL_BITS_*** - dynamic, strictly internal */
+	u32 slowpoll;	/* only used when interrupts are off */
+	/* The wrap-around eq_[prod|cons] counters are used to support
+	 * QMAN_ENQUEUE_FLAG_WAIT_SYNC. */
+	u32 eq_prod, eq_cons;
+	u32 sdqcr;
+	volatile int disable_count;
+	/* If we receive a DQRR or MR ring entry for a "null" FQ, ie. for which
+	 * FQD::contextB is NULL rather than pointing to a FQ object, we use
+	 * these handlers. (This is not considered a fast-path mechanism.) */
+	struct qman_fq_cb null_cb;
+	/* This is needed for providing a non-NULL device to dma_map_***() */
+	struct platform_device *pdev;
+};
+
+/* This gives a FQID->FQ lookup to cover the fact that we can't directly demux
+ * retirement notifications (the fact they are sometimes h/w-consumed means that
+ * contextB isn't always a s/w demux - and as we can't know which case it is
+ * when looking at the notification, we have to use the slow lookup for all of
+ * them). NB, it's possible to have multiple FQ objects refer to the same FQID
+ * (though at most one of them should be the consumer), so this table isn't for
+ * all FQs - FQs are added when retirement commands are issued, and removed when
+ * they complete, which also massively reduces the size of this table. */
+IMPLEMENT_QMAN_RBTREE(fqtree, struct qman_fq, node, fqid);
+static struct qman_rbtree table = QMAN_RBTREE;
+/* This is only locked with local_irq_disable() held or from interrupt context,
+ * so spin_lock_irq() shouldn't be used (nesting trouble). */
+static spinlock_t table_lock = SPIN_LOCK_UNLOCKED;
+static unsigned int table_num;
+
+/* This is what everything can wait on, even if it migrates to a different cpu
+ * to the one whose affine portal it is waiting on. */
+static DECLARE_WAIT_QUEUE_HEAD(affine_queue);
+
+static inline int table_push_fq(struct qman_fq *fq)
+{
+	int ret;
+	spin_lock(&table_lock);
+	ret = fqtree_push(&table, fq);
+	if (ret)
+		pr_err("ERROR: double FQ-retirement %d\n", fq->fqid);
+	else
+		table_num++;
+	spin_unlock(&table_lock);
+	return ret;
+}
+
+static inline void table_del_fq(struct qman_fq *fq)
+{
+	spin_lock(&table_lock);
+	fqtree_del(&table, fq);
+	spin_unlock(&table_lock);
+}
+
+static inline struct qman_fq *table_find_fq(u32 fqid)
+{
+	struct qman_fq *fq;
+	spin_lock(&table_lock);
+	fq = fqtree_find(&table, fqid);
+	spin_unlock(&table_lock);
+	return fq;
+}
+
+/* In the case that slow- and fast-path handling are both done by qman_poll()
+ * (ie. because there is no interrupt handling), we ought to balance how often
+ * we do the fast-path poll versus the slow-path poll. We'll use two decrementer
+ * sources, so we call the fast poll 'n' times before calling the slow poll
+ * once. The idle decrementer constant is used when the last slow-poll detected
+ * no work to do, and the busy decrementer constant when the last slow-poll had
+ * work to do. */
+#define SLOW_POLL_IDLE   1000
+#define SLOW_POLL_BUSY   10
+static u32 __poll_portal_slow(struct qman_portal *p, u32 is);
+static void __poll_portal_fast(struct qman_portal *p);
+/* For fast-path handling, the DQRR cursor chases ring entries around the ring
+ * until it runs out of valid things to process. This decrementer constant
+ * places a cap on how much it'll process in a single invocation. */
+#define FAST_POLL        16
+
+/* Portal interrupt handler */
+static irqreturn_t portal_isr(int irq, void *ptr)
+{
+	struct qman_portal *p = ptr;
+	u32 clear = QM_PIRQ_DQRI, is = qm_isr_status_read(p->p);
+	if (unlikely(!(p->options & QMAN_PORTAL_FLAG_IRQ))) {
+		pr_crit("Portal interrupt is supposed to be disabled!\n");
+		qm_isr_inhibit(p->p);
+		return IRQ_HANDLED;
+	}
+	/* Only do fast-path handling if it's required */
+	if (p->options & QMAN_PORTAL_FLAG_IRQ_FAST)
+		__poll_portal_fast(p);
+	clear |= __poll_portal_slow(p, is);
+	qm_isr_status_clear(p->p, clear);
+	return IRQ_HANDLED;
+}
+
+/* This inner version is used privately by qman_create_portal(), as well as by
+ * the exported qman_disable_portal(). */
+static inline void qman_disable_portal_ex(struct qman_portal *p)
+{
+	local_irq_disable();
+	if (!(p->disable_count++))
+		qm_dqrr_set_maxfill(p->p, 0);
+	local_irq_enable();
+}
+
+static int int_dqrr_mr_empty(struct qman_portal *p, int can_wait)
+{
+	int ret;
+	might_sleep_if(can_wait);
+	ret = (qm_dqrr_current(p->p) == NULL) &&
+		(qm_mr_current(p->p) == NULL);
+	if (ret && can_wait) {
+		/* Stall and recheck to be sure it has quiesced. */
+		msleep(RECOVER_MSLEEP);
+		ret = (qm_dqrr_current(p->p) == NULL) &&
+			(qm_mr_current(p->p) == NULL);
+	}
+	return ret;
+}
+
+struct qman_portal *qman_create_portal(struct qm_portal *__p, u32 flags,
+			const struct qman_cgrs *cgrs,
+			const struct qman_fq_cb *null_cb)
+{
+	struct qman_portal *portal;
+	const struct qm_portal_config *config = qm_portal_config(__p);
+	char buf[16];
+	int ret;
+	u32 isdr;
+
+	if (!(flags & QMAN_PORTAL_FLAG_NOTAFFINE))
+		/* core-affine portals don't need multi-core locking */
+		flags &= ~QMAN_PORTAL_FLAG_LOCKED;
+	portal = kmalloc(sizeof(*portal), GFP_KERNEL);
+	if (!portal)
+		return NULL;
+	if (qm_eqcr_init(__p, qm_eqcr_pvb, qm_eqcr_cci)) {
+		pr_err("Qman EQCR initialisation failed\n");
+		goto fail_eqcr;
+	}
+	if (qm_dqrr_init(__p, qm_dqrr_dpush, qm_dqrr_pvb,
+			(flags & QMAN_PORTAL_FLAG_DCA) ? qm_dqrr_cdc :
+					qm_dqrr_cci, DQRR_MAXFILL,
+			(flags & QMAN_PORTAL_FLAG_RSTASH) ? 1 : 0,
+			(flags & QMAN_PORTAL_FLAG_DSTASH) ? 1 : 0)) {
+		pr_err("Qman DQRR initialisation failed\n");
+		goto fail_dqrr;
+	}
+	if (qm_mr_init(__p, qm_mr_pvb, qm_mr_cci)) {
+		pr_err("Qman MR initialisation failed\n");
+		goto fail_mr;
+	}
+	if (qm_mc_init(__p)) {
+		pr_err("Qman MC initialisation failed\n");
+		goto fail_mc;
+	}
+	if (qm_isr_init(__p)) {
+		pr_err("Qman ISR initialisation failed\n");
+		goto fail_isr;
+	}
+	portal->p = __p;
+	if (!cgrs)
+		portal->cgrs = NULL;
+	else {
+		portal->cgrs = kmalloc(2 * sizeof(*cgrs), GFP_KERNEL);
+		if (!portal->cgrs)
+			goto fail_cgrs;
+		portal->cgrs[0] = *cgrs;
+		memset(&portal->cgrs[1], 0, sizeof(*cgrs));
+	}
+	portal->options = flags;
+	portal->bits = 0;
+	portal->slowpoll = 0;
+	portal->eq_prod = portal->eq_cons = 0;
+	portal->sdqcr = QM_SDQCR_SOURCE_CHANNELS | QM_SDQCR_COUNT_UPTO3 |
+			QM_SDQCR_DEDICATED_PRECEDENCE | QM_SDQCR_TYPE_PRIO_QOS |
+			QM_SDQCR_TOKEN_SET(0xab) | QM_SDQCR_CHANNELS_DEDICATED;
+	portal->disable_count = 0;
+	if (null_cb)
+		portal->null_cb = *null_cb;
+	else
+		memset(&portal->null_cb, 0, sizeof(*null_cb));
+	sprintf(buf, "qportal-%d", config->channel);
+	portal->pdev = platform_device_alloc(buf, -1);
+	if (!portal->pdev) {
+		ret = -ENOMEM;
+		goto fail_devalloc;
+	}
+	ret = platform_device_add(portal->pdev);
+	if (ret)
+		goto fail_devadd;
+	isdr = 0xffffffff;
+	qm_isr_disable_write(portal->p, isdr);
+	qm_isr_enable_write(portal->p, QM_PIRQ_EQCI | QM_PIRQ_EQRI |
+		((flags & QMAN_PORTAL_FLAG_IRQ_FAST) ? QM_PIRQ_DQRI : 0) |
+		QM_PIRQ_MRI | (cgrs ? QM_PIRQ_CSCI : 0));
+	qm_isr_status_clear(portal->p, 0xffffffff);
+	if (flags & QMAN_PORTAL_FLAG_IRQ) {
+		if (request_irq(config->irq, portal_isr, 0, "Qman portal 0",
+					portal)) {
+			pr_err("request_irq() failed\n");
+			goto fail_irq;
+		}
+		if ((config->cpu != -1) &&
+				irq_can_set_affinity(config->irq) &&
+				irq_set_affinity(config->irq,
+				     *get_cpu_mask(config->cpu))) {
+			pr_err("irq_set_affinity() failed\n");
+			goto fail_affinity;
+		}
+		qm_isr_uninhibit(portal->p);
+	} else
+		/* without IRQ, we can't block */
+		flags &= ~QMAN_PORTAL_FLAG_WAIT;
+	/* Need EQCR to be empty before continuing */
+	isdr ^= QM_PIRQ_EQCI;
+	qm_isr_disable_write(portal->p, isdr);
+	if (!(flags & QMAN_PORTAL_FLAG_RECOVER) ||
+			!(flags & QMAN_PORTAL_FLAG_WAIT))
+		ret = qm_eqcr_get_fill(portal->p);
+	else if (flags & QMAN_PORTAL_FLAG_WAIT_INT)
+		ret = wait_event_interruptible(affine_queue,
+			!qm_eqcr_get_fill(portal->p));
+	else {
+		wait_event(affine_queue, !qm_eqcr_get_fill(portal->p));
+		ret = 0;
+	}
+	if (ret) {
+		pr_err("Qman EQCR unclean, need recovery\n");
+		goto fail_eqcr_empty;
+	}
+	/* Check DQRR and MR are empty too, subject to RECOVERY logic */
+	if (flags & QMAN_PORTAL_FLAG_RECOVER)
+		portal->bits |= PORTAL_BITS_RECOVER;
+	isdr ^= (QM_PIRQ_DQRI | QM_PIRQ_MRI);
+	qm_isr_disable_write(portal->p, isdr);
+	if (!(flags & QMAN_PORTAL_FLAG_RECOVER))
+		ret = !int_dqrr_mr_empty(portal, 0);
+	else {
+		if (!(flags & QMAN_PORTAL_FLAG_WAIT))
+			ret = !int_dqrr_mr_empty(portal, 0);
+		else if (flags & QMAN_PORTAL_FLAG_WAIT_INT)
+			ret = wait_event_interruptible(affine_queue,
+				int_dqrr_mr_empty(portal, 1));
+		else {
+			wait_event(affine_queue,
+				int_dqrr_mr_empty(portal, 1));
+			ret = 0;
+		}
+		qman_disable_portal_ex(portal);
+		portal->bits ^= PORTAL_BITS_RECOVER;
+	}
+	if (ret) {
+		if (flags & QMAN_PORTAL_FLAG_RECOVER)
+			pr_err("Qman DQRR/MR unclean, recovery failed\n");
+		else
+			pr_err("Qman DQRR/MR unclean, need recovery\n");
+		goto fail_dqrr_mr_empty;
+	}
+	qm_isr_disable_write(portal->p, 0);
+	/* Write a sane SDQCR */
+	qm_dqrr_sdqcr_set(portal->p, portal->sdqcr);
+	return portal;
+fail_dqrr_mr_empty:
+fail_eqcr_empty:
+fail_affinity:
+	if (flags & QMAN_PORTAL_FLAG_IRQ)
+		free_irq(config->irq, portal);
+fail_irq:
+	platform_device_del(portal->pdev);
+fail_devadd:
+	platform_device_put(portal->pdev);
+fail_devalloc:
+	if (portal->cgrs)
+		kfree(portal->cgrs);
+fail_cgrs:
+	qm_isr_finish(__p);
+fail_isr:
+	qm_mc_finish(__p);
+fail_mc:
+	qm_mr_finish(__p);
+fail_mr:
+	qm_dqrr_finish(__p);
+fail_dqrr:
+	qm_eqcr_finish(__p);
+fail_eqcr:
+	kfree(portal);
+	return NULL;
+}
+
+void qman_destroy_portal(struct qman_portal *qm)
+{
+	/* NB we do this to "quiesce" EQCR. If we add enqueue-completions or
+	 * something related to QM_PIRQ_EQCI, this may need fixing. */
+	qm_eqcr_cci_update(qm->p);
+	if (qm->options & QMAN_PORTAL_FLAG_IRQ)
+		free_irq(qm_portal_config(qm->p)->irq, qm);
+	if (qm->cgrs)
+		kfree(qm->cgrs);
+	qm_isr_finish(qm->p);
+	qm_mc_finish(qm->p);
+	qm_mr_finish(qm->p);
+	qm_dqrr_finish(qm->p);
+	qm_eqcr_finish(qm->p);
+	kfree(qm);
+}
+
+void qman_set_null_cb(const struct qman_fq_cb *null_cb)
+{
+	struct qman_portal *p = get_affine_portal();
+	p->null_cb = *null_cb;
+	put_affine_portal();
+}
+EXPORT_SYMBOL(qman_set_null_cb);
+
+/* Inline helper to reduce nesting in __poll_portal_slow() */
+static inline void fq_state_change(struct qman_fq *fq, struct qm_mr_entry *msg,
+					u8 verb)
+{
+	FQLOCK(fq);
+	switch(verb) {
+	case QM_MR_VERB_FQRL:
+		QM_ASSERT(fq_isset(fq, QMAN_FQ_STATE_ORL));
+		fq_clear(fq, QMAN_FQ_STATE_ORL);
+		table_del_fq(fq);
+		break;
+	case QM_MR_VERB_FQRN:
+		QM_ASSERT((fq->state == qman_fq_state_parked) ||
+			(fq->state == qman_fq_state_sched));
+		QM_ASSERT(fq_isset(fq, QMAN_FQ_STATE_CHANGING));
+		fq_clear(fq, QMAN_FQ_STATE_CHANGING);
+		if (msg->fq.fqs & QM_MR_FQS_NOTEMPTY)
+			fq_set(fq, QMAN_FQ_STATE_NE);
+		if (msg->fq.fqs & QM_MR_FQS_ORLPRESENT)
+			fq_set(fq, QMAN_FQ_STATE_ORL);
+		else
+			table_del_fq(fq);
+		fq->state = qman_fq_state_retired;
+		break;
+	case QM_MR_VERB_FQRNI:
+		/* we processed retirement with the MC command */
+		if (fq_isclear(fq, QMAN_FQ_STATE_ORL))
+			table_del_fq(fq);
+		break;
+	case QM_MR_VERB_FQPN:
+		QM_ASSERT(fq->state == qman_fq_state_sched);
+		QM_ASSERT(fq_isclear(fq, QMAN_FQ_STATE_CHANGING));
+		fq->state = qman_fq_state_parked;
+	}
+	FQUNLOCK(fq);
+}
+
+static inline void eqcr_set_thresh(struct qman_portal *p, int check)
+{
+	if (!check || !qm_eqcr_get_ithresh(p->p))
+		qm_eqcr_set_ithresh(p->p, EQCR_ITHRESH);
+}
+
+static u32 __poll_portal_slow(struct qman_portal *p, u32 is)
+{
+	struct qm_mr_entry *msg;
+
+	qm_mr_pvb_prefetch(p->p);
+
+	if (is & QM_PIRQ_CSCI) {
+		struct qm_mc_result *mcr;
+		unsigned int i;
+		local_irq_disable();
+		qm_mc_start(p->p);
+		qm_mc_commit(p->p, QM_MCC_VERB_QUERYCONGESTION);
+		while (!(mcr = qm_mc_result(p->p)))
+			cpu_relax();
+		p->cgrs[1].q = mcr->querycongestion.state;
+		local_irq_enable();
+		for (i = 0; i < 8; i++)
+			p->cgrs[1].q.__state[i] &= p->cgrs[0].q.__state[i];
+	}
+
+#if 0
+	/* PIRQ_EQCI serves no meaningful purpose for a high-level interface,
+	 * so you can enable it to force interrupt-processing if you want (ie.
+	 * as a consequence of h/w consuming your EQCR entry, despite this
+	 * being unrelated to the work interrupt-processing needs to do).
+	 * Callbacks for enqueue-completion don't make sense (because they can
+	 * get rejected some time after EQCR-consumption, so you'd have to be
+	 * call it enqueue-incompletion...), and even if they did, you'd have
+	 * to call them irrespective of the EQCI interrupt source because it
+	 * can get coalesced. */
+	if (is & QM_PIRQ_EQCI) { ... }
+
+#endif
+	if (is & QM_PIRQ_EQRI) {
+		local_irq_disable();
+		p->eq_cons += qm_eqcr_cci_update(p->p);
+		qm_eqcr_set_ithresh(p->p, 0);
+		wake_up(&affine_queue);
+		local_irq_enable();
+	}
+
+	if (is & QM_PIRQ_MRI) {
+		u8 num = 0;
+mr_loop:
+		if (qm_mr_pvb_update(p->p))
+			qm_mr_pvb_prefetch(p->p);
+		msg = qm_mr_current(p->p);
+		if (msg) {
+			struct qman_fq *fq = (void *)msg->ern.tag;
+			u8 verb = msg->verb & QM_MR_VERB_TYPE_MASK;
+			if (unlikely(p->bits & PORTAL_BITS_RECOVER)) {
+				/* use portal default handlers for recovery */
+				if (likely(!(verb & QM_MR_VERB_DC_ERN)))
+					p->null_cb.ern(p, NULL, msg);
+				else if (verb == QM_MR_VERB_DC_ERN)
+					p->null_cb.dc_ern(p, NULL, msg);
+				else if (p->null_cb.fqs)
+					p->null_cb.fqs(p, NULL, msg);
+			}
+			if ((verb == QM_MR_VERB_FQRN) ||
+					(verb == QM_MR_VERB_FQRNI) ||
+					(verb == QM_MR_VERB_FQRL)) {
+				/* It's retirement related - need a lookup */
+				fq = table_find_fq(msg->fq.fqid);
+				if (!fq)
+					panic("unexpected FQ retirement");
+				fq_state_change(fq, msg, verb);
+				if (fq->cb.fqs)
+					fq->cb.fqs(p, (void *)fq, msg);
+			} else if (likely(fq)) {
+				/* As per the header note, this is the way to
+				 * determine if it's a s/w ERN or not. */
+				if (likely(!(verb & QM_MR_VERB_DC_ERN)))
+					fq->cb.ern(p, (void *)fq, msg);
+				else
+					fq->cb.dc_ern(p, (void *)fq, msg);
+			} else {
+				/* use portal default handlers for 'null's */
+				if (likely(!(verb & QM_MR_VERB_DC_ERN)))
+					p->null_cb.ern(p, NULL, msg);
+				else if (verb == QM_MR_VERB_DC_ERN)
+					p->null_cb.dc_ern(p, NULL, msg);
+				else if (p->null_cb.fqs)
+					p->null_cb.fqs(p, NULL, msg);
+			}
+			num++;
+			qm_mr_next(p->p);
+			goto mr_loop;
+		}
+		qm_mr_cci_consume(p->p, num);
+	}
+
+	return is & (QM_PIRQ_CSCI | QM_PIRQ_EQCI | QM_PIRQ_EQRI | QM_PIRQ_MRI);
+}
+
+/* Look: no locks, no irq_disable()s, no preempt_disable()s! :-) The only states
+ * that would conflict with other things if they ran at the same time on the
+ * same cpu are;
+ *
+ *   (i) clearing/incrementing PORTAL_BITS_*** stuff related to VDQCR, and
+ *  (ii) clearing the NE (Not Empty) flag.
+ *
+ * Both are safe. Because;
+ *
+ *   (i) this clearing/incrementing can only occur after qman_volatile_dequeue()
+ *       has set the PORTAL_BITS_*** stuff (which it does before setting VDQCR),
+ *       and qman_volatile_dequeue() blocks interrupts and preemption while this
+ *       is done so that we can't interfere.
+ *  (ii) the NE flag is only cleared after qman_retire_fq() has set it, and as
+ *       with (i) that API prevents us from interfering until it's safe.
+ *
+ * The good thing is that qman_volatile_dequeue() and qman_retire_fq() run far
+ * less frequently (ie. per-FQ) than __poll_portal_fast() does, so the nett
+ * advantage comes from this function not having to "lock" anything at all.
+ *
+ * Note also that the callbacks are invoked at points which are safe against the
+ * above potential conflicts, but that this function itself is not re-entrant
+ * (this is because the function tracks one end of each FIFO in the portal and
+ * we do *not* want to lock that). So the consequence is that it is safe for
+ * user callbacks to call into any Qman API *except* qman_poll() (as that's the
+ * sole API that could be invoking the callback through this function).
+ */
+static void __poll_portal_fast(struct qman_portal *p)
+{
+	struct qm_dqrr_entry *dq;
+	struct qman_fq *fq;
+	enum qman_cb_dqrr_result res;
+	u8 num = 0;
+
+	qm_dqrr_pvb_prefetch(p->p);
+
+dqrr_loop:
+	if (qm_dqrr_pvb_update(p->p))
+		qm_dqrr_pvb_prefetch(p->p);
+	dq = qm_dqrr_current(p->p);
+	if (!dq || (num == FAST_POLL))
+		goto done;
+	fq = (void *)dq->contextB;
+	/* Interpret 'dq' from the owner's perspective. */
+	if (unlikely((p->bits & PORTAL_BITS_RECOVER) || !fq)) {
+		/* use portal default handlers */
+		res = p->null_cb.dqrr(p, NULL, dq);
+		QM_ASSERT(res == qman_cb_dqrr_consume);
+		res = qman_cb_dqrr_consume;
+	} else {
+		if (dq->stat & QM_DQRR_STAT_FQ_EMPTY)
+			fq_clear(fq, QMAN_FQ_STATE_NE);
+		/* Now let the callback do its stuff */
+		res = fq->cb.dqrr(p, fq, dq);
+	}
+	/* Interpret 'dq' from a driver perspective. */
+#define VDQCR_DONE (QM_DQRR_STAT_UNSCHEDULED | QM_DQRR_STAT_DQCR_EXPIRED)
+	if ((dq->stat & VDQCR_DONE) == VDQCR_DONE) {
+		PORTAL_BITS_INC_V(p);
+		wake_up(&affine_queue);
+	}
+	/* Parking isn't possible unless HELDACTIVE was set. NB,
+	 * FORCEELIGIBLE implies HELDACTIVE, so we only need to
+	 * check for HELDACTIVE to cover both. */
+	QM_ASSERT((dq->stat & QM_DQRR_STAT_FQ_HELDACTIVE) ||
+		(res != qman_cb_dqrr_park));
+	if (p->options & QMAN_PORTAL_FLAG_DCA) {
+		/* Defer just means "skip it, I'll consume it
+		 * myself later on" */
+		if (res != qman_cb_dqrr_defer)
+			qm_dqrr_cdc_consume_1ptr(p->p, dq,
+				(res == qman_cb_dqrr_park));
+	} else if (res == qman_cb_dqrr_park)
+		/* The only thing to do for non-DCA is the
+		 * park-request */
+		qm_dqrr_park_ci(p->p);
+	/* Move forward */
+	num++;
+	qm_dqrr_next(p->p);
+	goto dqrr_loop;
+done:
+	if (num) {
+		if (!(p->options & QMAN_PORTAL_FLAG_DCA))
+			qm_dqrr_cci_consume(p->p, num);
+	}
+}
+
+void qman_poll(void)
+{
+	struct qman_portal *p = get_affine_portal();
+	if (!(p->options & QMAN_PORTAL_FLAG_IRQ)) {
+		/* we handle slow- and fast-path */
+		__poll_portal_fast(p);
+		if (!(p->slowpoll--)) {
+			u32 is = qm_isr_status_read(p->p);
+			u32 active = __poll_portal_slow(p, is);
+			if (active) {
+				qm_isr_status_clear(p->p, active);
+				p->slowpoll = SLOW_POLL_BUSY;
+			} else
+				p->slowpoll = SLOW_POLL_IDLE;
+		}
+	} else if (!(p->options & QMAN_PORTAL_FLAG_IRQ_FAST))
+		/* we handle fast-path only */
+		__poll_portal_fast(p);
+	put_affine_portal();
+}
+EXPORT_SYMBOL(qman_poll);
+
+void qman_disable_portal(void)
+{
+	struct qman_portal *p = get_affine_portal();
+	qman_disable_portal_ex(p);
+	put_affine_portal();
+}
+EXPORT_SYMBOL(qman_disable_portal);
+
+void qman_enable_portal(void)
+{
+	struct qman_portal *p = get_affine_portal();
+	local_irq_disable();
+	QM_ASSERT(p->disable_count > 0);
+	if (!(--p->disable_count))
+		qm_dqrr_set_maxfill(p->p, DQRR_MAXFILL);
+	local_irq_enable();
+	put_affine_portal();
+}
+EXPORT_SYMBOL(qman_enable_portal);
+
+/* This isn't a fast-path API, and qman_driver.c setup code needs to be able to
+ * set initial SDQCR values to all portals, not just the affine one for the
+ * current cpu, so we suction out the "_ex" version as a private hook. */
+void qman_static_dequeue_add_ex(struct qman_portal *p, u32 pools)
+{
+	local_irq_disable();
+	pools &= p->p->config.pools;
+	p->sdqcr |= pools;
+	qm_dqrr_sdqcr_set(p->p, p->sdqcr);
+	local_irq_enable();
+}
+
+void qman_static_dequeue_add(u32 pools)
+{
+	struct qman_portal *p = get_affine_portal();
+	qman_static_dequeue_add_ex(p, pools);
+	put_affine_portal();
+}
+EXPORT_SYMBOL(qman_static_dequeue_add);
+
+void qman_static_dequeue_del(u32 pools)
+{
+	struct qman_portal *p = get_affine_portal();
+	local_irq_disable();
+	pools &= p->p->config.pools;
+	p->sdqcr &= ~pools;
+	qm_dqrr_sdqcr_set(p->p, p->sdqcr);
+	local_irq_enable();
+	put_affine_portal();
+}
+EXPORT_SYMBOL(qman_static_dequeue_del);
+
+u32 qman_static_dequeue_get(void)
+{
+	struct qman_portal *p = get_affine_portal();
+	u32 ret = p->sdqcr;
+	put_affine_portal();
+	return ret;
+}
+EXPORT_SYMBOL(qman_static_dequeue_get);
+
+void qman_dca(struct qm_dqrr_entry *dq, int park_request)
+{
+	struct qman_portal *p = get_affine_portal();
+	qm_dqrr_cdc_consume_1ptr(p->p, dq, park_request);
+	put_affine_portal();
+}
+EXPORT_SYMBOL(qman_dca);
+
+/*******************/
+/* Frame queue API */
+/*******************/
+
+static const char *mcr_result_str(u8 result)
+{
+	switch (result) {
+	case QM_MCR_RESULT_NULL:
+		return "QM_MCR_RESULT_NULL";
+	case QM_MCR_RESULT_OK:
+		return "QM_MCR_RESULT_OK";
+	case QM_MCR_RESULT_ERR_FQID:
+		return "QM_MCR_RESULT_ERR_FQID";
+	case QM_MCR_RESULT_ERR_FQSTATE:
+		return "QM_MCR_RESULT_ERR_FQSTATE";
+	case QM_MCR_RESULT_ERR_NOTEMPTY:
+		return "QM_MCR_RESULT_ERR_NOTEMPTY";
+	case QM_MCR_RESULT_PENDING:
+		return "QM_MCR_RESULT_PENDING";
+	}
+	return "<unknown MCR result>";
+}
+
+int qman_create_fq(u32 fqid, u32 flags, struct qman_fq *fq)
+{
+	struct qm_fqd fqd;
+	struct qm_mcr_queryfq_np np;
+	struct qm_mc_command *mcc;
+	struct qm_mc_result *mcr;
+	struct qman_portal *p;
+
+	if (flags & QMAN_FQ_FLAG_DYNAMIC_FQID) {
+		fqid = qm_fq_new();
+		if (!fqid)
+			return -ENOMEM;
+	}
+	spin_lock_init(&fq->fqlock);
+	fq->fqid = fqid;
+	fq->flags = flags;
+	fq->state = qman_fq_state_oos;
+	fq->cgr_groupid = 0;
+	if (!(flags & QMAN_FQ_FLAG_RECOVER) ||
+			(flags & QMAN_FQ_FLAG_NO_MODIFY))
+		return 0;
+	/* Everything else is RECOVER support */
+	p = get_affine_portal();
+	local_irq_disable();
+	mcc = qm_mc_start(p->p);
+	mcc->queryfq.fqid = fqid;
+	qm_mc_commit(p->p, QM_MCC_VERB_QUERYFQ);
+	while (!(mcr = qm_mc_result(p->p)))
+		cpu_relax();
+	QM_ASSERT((mcr->verb & QM_MCR_VERB_MASK) == QM_MCC_VERB_QUERYFQ);
+	if (mcr->result != QM_MCR_RESULT_OK) {
+		pr_err("QUERYFQ failed: %s\n", mcr_result_str(mcr->result));
+		goto err;
+	}
+	fqd = mcr->queryfq.fqd;
+	mcc = qm_mc_start(p->p);
+	mcc->queryfq_np.fqid = fqid;
+	qm_mc_commit(p->p, QM_MCC_VERB_QUERYFQ_NP);
+	while (!(mcr = qm_mc_result(p->p)))
+		cpu_relax();
+	QM_ASSERT((mcr->verb & QM_MCR_VERB_MASK) == QM_MCC_VERB_QUERYFQ_NP);
+	if (mcr->result != QM_MCR_RESULT_OK) {
+		pr_err("QUERYFQ_NP failed: %s\n", mcr_result_str(mcr->result));
+		goto err;
+	}
+	np = mcr->queryfq_np;
+	/* Phew, have queryfq and queryfq_np results, stitch together
+	 * the FQ object from those. */
+	fq->cgr_groupid = fqd.cgid;
+	switch (np.state & QM_MCR_NP_STATE_MASK) {
+	case QM_MCR_NP_STATE_OOS:
+		break;
+	case QM_MCR_NP_STATE_RETIRED:
+		fq->state = qman_fq_state_retired;
+		if (np.frm_cnt)
+			fq_set(fq, QMAN_FQ_STATE_NE);
+		break;
+	case QM_MCR_NP_STATE_TEN_SCHED:
+	case QM_MCR_NP_STATE_TRU_SCHED:
+	case QM_MCR_NP_STATE_ACTIVE:
+		fq->state = qman_fq_state_sched;
+		if (np.state & QM_MCR_NP_STATE_R)
+			fq_set(fq, QMAN_FQ_STATE_CHANGING);
+		break;
+	case QM_MCR_NP_STATE_PARKED:
+		fq->state = qman_fq_state_parked;
+		break;
+	default:
+		QM_ASSERT(NULL == "invalid FQ state");
+	}
+	if (fqd.fq_ctrl & QM_FQCTRL_CGE)
+		fq->state |= QMAN_FQ_STATE_CGR_EN;
+	local_irq_enable();
+	put_affine_portal();
+	return 0;
+err:
+	local_irq_enable();
+	put_affine_portal();
+	if (flags & QMAN_FQ_FLAG_DYNAMIC_FQID)
+		qm_fq_free(fqid);
+	return -EIO;
+}
+EXPORT_SYMBOL(qman_create_fq);
+
+void qman_destroy_fq(struct qman_fq *fq, u32 flags)
+{
+	/* We don't need to lock the FQ as it is a pre-condition that the FQ be
+	 * quiesced. Instead, run some checks. */
+	switch (fq->state) {
+	case qman_fq_state_parked:
+		QM_ASSERT(flags & QMAN_FQ_DESTROY_PARKED);
+	case qman_fq_state_oos:
+		if (fq_isset(fq, QMAN_FQ_FLAG_DYNAMIC_FQID))
+			qm_fq_free(fq->fqid);
+		return;
+	default:
+		break;
+	}
+	QM_ASSERT(NULL == "qman_free_fq() on unquiesced FQ!");
+}
+EXPORT_SYMBOL(qman_destroy_fq);
+
+u32 qman_fq_fqid(struct qman_fq *fq)
+{
+	return fq->fqid;
+}
+EXPORT_SYMBOL(qman_fq_fqid);
+
+void qman_fq_state(struct qman_fq *fq, enum qman_fq_state *state, u32 *flags)
+{
+	*state = fq->state;
+	if (flags)
+		*flags = fq->flags;
+}
+EXPORT_SYMBOL(qman_fq_state);
+
+int qman_init_fq(struct qman_fq *fq, u32 flags, struct qm_mcc_initfq *opts)
+{
+	struct qm_mc_command *mcc;
+	struct qm_mc_result *mcr;
+	struct qman_portal *p;
+	u8 res, myverb = (flags & QMAN_INITFQ_FLAG_SCHED) ?
+		QM_MCC_VERB_INITFQ_SCHED : QM_MCC_VERB_INITFQ_PARKED;
+
+	QM_ASSERT((fq->state == qman_fq_state_oos) ||
+		(fq->state == qman_fq_state_parked));
+#ifdef CONFIG_FSL_QMAN_CHECKING
+	if (unlikely(fq_isset(fq, QMAN_FQ_FLAG_NO_MODIFY)))
+		return -EINVAL;
+#endif
+	/* Issue an INITFQ_[PARKED|SCHED] management command */
+	p = get_affine_portal();
+	local_irq_disable();
+	FQLOCK(fq);
+	if (unlikely((fq_isset(fq, QMAN_FQ_STATE_CHANGING)) ||
+			((fq->state != qman_fq_state_oos) &&
+				(fq->state != qman_fq_state_parked)))) {
+		FQUNLOCK(fq);
+		local_irq_enable();
+		put_affine_portal();
+		return -EBUSY;
+	}
+	mcc = qm_mc_start(p->p);
+	if (opts)
+		mcc->initfq = *opts;
+	mcc->initfq.fqid = fq->fqid;
+	mcc->initfq.count = 0;
+	/* If INITFQ_FLAG_NULL is passed, contextB is set to zero. Otherwise,
+	 * if the FQ does *not* have the TO_DCPORTAL flag, contextB is set as a
+	 * demux pointer. Otherwise, TO_DCPORTAL is set, so the caller-provided
+	 * value is allowed to stand, don't overwrite it. */
+	if ((flags & QMAN_INITFQ_FLAG_NULL) ||
+			fq_isclear(fq, QMAN_FQ_FLAG_TO_DCPORTAL)) {
+		dma_addr_t phys_fq;
+		BUG_ON(sizeof(phys_fq) > sizeof(u32));
+		mcc->initfq.we_mask |= QM_INITFQ_WE_CONTEXTB;
+		mcc->initfq.fqd.context_b = (flags & QMAN_INITFQ_FLAG_NULL) ?
+						0 : (u32)fq;
+		/* and the physical address - NB, if the user wasn't trying to
+		 * set CONTEXTA, clear the stashing settings. */
+		if (!(mcc->initfq.we_mask & QM_INITFQ_WE_CONTEXTA)) {
+			mcc->initfq.we_mask |= QM_INITFQ_WE_CONTEXTA;
+			memset(&mcc->initfq.fqd.context_a, 0,
+				sizeof(&mcc->initfq.fqd.context_a));
+		}
+		phys_fq = dma_map_single(&p->pdev->dev, fq, sizeof(*fq),
+					DMA_TO_DEVICE);
+		mcc->initfq.fqd.context_a.context_hi = 0;
+		mcc->initfq.fqd.context_a.context_lo = (u32)phys_fq;
+	}
+	if (flags & QMAN_INITFQ_FLAG_LOCAL) {
+		mcc->initfq.fqd.dest.channel = p->p->config.channel;
+		if (!(mcc->initfq.we_mask & QM_INITFQ_WE_DESTWQ)) {
+			mcc->initfq.we_mask |= QM_INITFQ_WE_DESTWQ;
+			mcc->initfq.fqd.dest.wq = 4;
+		}
+	}
+	qm_mc_commit(p->p, myverb);
+	while (!(mcr = qm_mc_result(p->p)))
+		cpu_relax();
+	QM_ASSERT((mcr->verb & QM_MCR_VERB_MASK) == myverb);
+	res = mcr->result;
+	if (res != QM_MCR_RESULT_OK) {
+		pr_err("INITFQ failed: %s\n", mcr_result_str(res));
+		FQUNLOCK(fq);
+		local_irq_enable();
+		put_affine_portal();
+		return -EIO;
+	}
+	if (mcc->initfq.we_mask & QM_INITFQ_WE_FQCTRL) {
+		if (mcc->initfq.fqd.fq_ctrl & QM_FQCTRL_CGE)
+			fq_set(fq, QMAN_FQ_STATE_CGR_EN);
+		else
+			fq_clear(fq, QMAN_FQ_STATE_CGR_EN);
+	}
+	if (mcc->initfq.we_mask & QM_INITFQ_WE_CGID)
+		fq->cgr_groupid = mcc->initfq.fqd.cgid;
+	fq->state = (flags & QMAN_INITFQ_FLAG_SCHED) ?
+			qman_fq_state_sched : qman_fq_state_parked;
+	FQUNLOCK(fq);
+	local_irq_enable();
+	put_affine_portal();
+	return 0;
+}
+EXPORT_SYMBOL(qman_init_fq);
+
+int qman_schedule_fq(struct qman_fq *fq)
+{
+	struct qm_mc_command *mcc;
+	struct qm_mc_result *mcr;
+	struct qman_portal *p;
+	int ret = 0;
+	u8 res;
+
+	QM_ASSERT(fq->state == qman_fq_state_parked);
+#ifdef CONFIG_FSL_QMAN_CHECKING
+	if (unlikely(fq_isset(fq, QMAN_FQ_FLAG_NO_MODIFY)))
+		return -EINVAL;
+#endif
+	/* Issue a ALTERFQ_SCHED management command */
+	p = get_affine_portal();
+	local_irq_disable();
+	FQLOCK(fq);
+	if (unlikely((fq_isset(fq, QMAN_FQ_STATE_CHANGING)) ||
+			(fq->state != qman_fq_state_parked))) {
+		ret = -EBUSY;
+		goto out;
+	}
+	mcc = qm_mc_start(p->p);
+	mcc->alterfq.fqid = fq->fqid;
+	qm_mc_commit(p->p, QM_MCC_VERB_ALTER_SCHED);
+	while (!(mcr = qm_mc_result(p->p)))
+		cpu_relax();
+	QM_ASSERT((mcr->verb & QM_MCR_VERB_MASK) == QM_MCR_VERB_ALTER_SCHED);
+	res = mcr->result;
+	if (res != QM_MCR_RESULT_OK) {
+		pr_err("ALTER_SCHED failed: %s\n", mcr_result_str(res));
+		ret = -EIO;
+		goto out;
+	}
+	fq->state = qman_fq_state_sched;
+out:
+	FQUNLOCK(fq);
+	local_irq_enable();
+	put_affine_portal();
+	return ret;
+}
+EXPORT_SYMBOL(qman_schedule_fq);
+
+int qman_retire_fq(struct qman_fq *fq, u32 *flags)
+{
+	struct qm_mc_command *mcc;
+	struct qm_mc_result *mcr;
+	struct qman_portal *p;
+	int rval;
+	u8 res;
+
+	QM_ASSERT((fq->state == qman_fq_state_parked) ||
+		(fq->state == qman_fq_state_sched));
+#ifdef CONFIG_FSL_QMAN_CHECKING
+	if (unlikely(fq_isset(fq, QMAN_FQ_FLAG_NO_MODIFY)))
+		return -EINVAL;
+#endif
+	p = get_affine_portal();
+	local_irq_disable();
+	FQLOCK(fq);
+	if (unlikely((fq_isset(fq, QMAN_FQ_STATE_CHANGING)) ||
+			(fq->state == qman_fq_state_retired) ||
+				(fq->state == qman_fq_state_oos))) {
+		rval = -EBUSY;
+		goto out;
+	}
+	rval = table_push_fq(fq);
+	if (rval)
+		goto out;
+	mcc = qm_mc_start(p->p);
+	mcc->alterfq.fqid = fq->fqid;
+	qm_mc_commit(p->p, QM_MCC_VERB_ALTER_RETIRE);
+	while (!(mcr = qm_mc_result(p->p)))
+		cpu_relax();
+	QM_ASSERT((mcr->verb & QM_MCR_VERB_MASK) == QM_MCR_VERB_ALTER_RETIRE);
+	res = mcr->result;
+	/* "Elegant" would be to treat OK/PENDING the same way; set CHANGING,
+	 * and defer the flags until FQRNI or FQRN (respectively) show up. But
+	 * "Friendly" is to process OK immediately, and not set CHANGING. We do
+	 * friendly, otherwise the caller doesn't necessarily have a fully
+	 * "retired" FQ on return even if the retirement was immediate.  However
+	 * this does mean some code duplication between here and
+	 * fq_state_change(). */
+	if (likely(res == QM_MCR_RESULT_OK)) {
+		rval = 0;
+		/* Process 'fq' right away, we'll ignore FQRNI */
+		if (mcr->alterfq.fqs & QM_MCR_FQS_NOTEMPTY)
+			fq_set(fq, QMAN_FQ_STATE_NE);
+		if (mcr->alterfq.fqs & QM_MCR_FQS_ORLPRESENT)
+			fq_set(fq, QMAN_FQ_STATE_ORL);
+		if (flags)
+			*flags = fq->flags;
+		fq->state = qman_fq_state_retired;
+	} else if (res == QM_MCR_RESULT_PENDING) {
+		rval = 1;
+		fq_set(fq, QMAN_FQ_STATE_CHANGING);
+	} else {
+		rval = -EIO;
+		table_del_fq(fq);
+		pr_err("ALTER_RETIRE failed: %s\n", mcr_result_str(res));
+	}
+out:
+	FQUNLOCK(fq);
+	local_irq_enable();
+	put_affine_portal();
+	return rval;
+}
+EXPORT_SYMBOL(qman_retire_fq);
+
+int qman_oos_fq(struct qman_fq *fq)
+{
+	struct qm_mc_command *mcc;
+	struct qm_mc_result *mcr;
+	struct qman_portal *p;
+	int ret = 0;
+	u8 res;
+
+	QM_ASSERT(fq->state == qman_fq_state_retired);
+#ifdef CONFIG_FSL_QMAN_CHECKING
+	if (unlikely(fq_isset(fq, QMAN_FQ_FLAG_NO_MODIFY)))
+		return -EINVAL;
+#endif
+	p = get_affine_portal();
+	local_irq_disable();
+	FQLOCK(fq);
+	if (unlikely((fq_isset(fq, QMAN_FQ_STATE_BLOCKOOS)) ||
+			(fq->state != qman_fq_state_retired))) {
+		ret = -EBUSY;
+		goto out;
+	}
+	mcc = qm_mc_start(p->p);
+	mcc->alterfq.fqid = fq->fqid;
+	qm_mc_commit(p->p, QM_MCC_VERB_ALTER_OOS);
+	while (!(mcr = qm_mc_result(p->p)))
+		cpu_relax();
+	QM_ASSERT((mcr->verb & QM_MCR_VERB_MASK) == QM_MCR_VERB_ALTER_OOS);
+	res = mcr->result;
+	if (res != QM_MCR_RESULT_OK) {
+		pr_err("ALTER_OOS failed: %s\n", mcr_result_str(res));
+		ret = -EIO;
+		goto out;
+	}
+	fq->state = qman_fq_state_oos;
+out:
+	FQUNLOCK(fq);
+	local_irq_enable();
+	put_affine_portal();
+	return ret;
+}
+EXPORT_SYMBOL(qman_oos_fq);
+
+int qman_query_fq(struct qman_fq *fq, struct qm_fqd *fqd)
+{
+	struct qm_mc_command *mcc;
+	struct qm_mc_result *mcr;
+	struct qman_portal *p = get_affine_portal();
+	u8 res;
+
+	local_irq_disable();
+	mcc = qm_mc_start(p->p);
+	mcc->queryfq.fqid = fq->fqid;
+	qm_mc_commit(p->p, QM_MCC_VERB_QUERYFQ);
+	while (!(mcr = qm_mc_result(p->p)))
+		cpu_relax();
+	QM_ASSERT((mcr->verb & QM_MCR_VERB_MASK) == QM_MCR_VERB_QUERYFQ);
+	res = mcr->result;
+	if (res == QM_MCR_RESULT_OK)
+		*fqd = mcr->queryfq.fqd;
+	local_irq_enable();
+	put_affine_portal();
+	if (res != QM_MCR_RESULT_OK) {
+		pr_err("QUERYFQ failed: %s\n", mcr_result_str(res));
+		return -EIO;
+	}
+	return 0;
+}
+EXPORT_SYMBOL(qman_query_fq);
+
+/* internal function used as a wait_event() expression */
+static int set_vdqcr(struct qman_portal **p, u32 vdqcr, u16 *v)
+{
+	int ret = -EBUSY;
+	*p = get_affine_portal();
+	local_irq_disable();
+	if (!((*p)->bits & PORTAL_BITS_VDQCR)) {
+		(*p)->bits |= PORTAL_BITS_VDQCR;
+		ret = 0;
+	}
+	local_irq_enable();
+	if (!ret) {
+		if (v)
+			*v = PORTAL_BITS_GET_V(*p);
+		qm_dqrr_vdqcr_set((*p)->p, vdqcr);
+	}
+	put_affine_portal();
+	return ret;
+}
+
+static inline int wait_vdqcr_start(struct qman_portal **p, u32 vdqcr, u16 *v,
+					u32 flags)
+{
+	int ret = 0;
+	if (flags & QMAN_VOLATILE_FLAG_WAIT_INT)
+		ret = wait_event_interruptible(affine_queue,
+					!(ret = set_vdqcr(p, vdqcr, v)));
+	else
+		wait_event(affine_queue, !(ret = set_vdqcr(p, vdqcr, v)));
+	return ret;
+}
+
+int qman_volatile_dequeue(struct qman_fq *fq, u32 flags, u32 vdqcr)
+{
+	struct qman_portal *p;
+	int ret;
+	u16 v = 0; /* init not needed, but gcc is dumb */
+
+	QM_ASSERT(!fq || (fq->state == qman_fq_state_parked) ||
+			(fq->state == qman_fq_state_retired));
+	QM_ASSERT(!fq || !(vdqcr & QM_VDQCR_FQID_MASK));
+	if (fq)
+		vdqcr = (vdqcr & ~QM_VDQCR_FQID_MASK) | fq->fqid;
+	if (flags & QMAN_VOLATILE_FLAG_WAIT)
+		ret = wait_vdqcr_start(&p, vdqcr, &v, flags);
+	else
+		ret = set_vdqcr(&p, vdqcr, &v);
+	if (ret)
+		return ret;
+	/* VDQCR is set */
+	if (flags & QMAN_VOLATILE_FLAG_FINISH) {
+		if (flags & QMAN_VOLATILE_FLAG_WAIT_INT)
+			/* NB: don't propagate any error - the caller wouldn't
+			 * know whether the VDQCR was issued or not. A signal
+			 * could arrive after returning anyway, so the caller
+			 * can check signal_pending() if that's an issue. */
+			wait_event_interruptible(affine_queue,
+				PORTAL_BITS_GET_V(p) != v);
+		else
+			wait_event(affine_queue, PORTAL_BITS_GET_V(p) != v);
+	}
+	return 0;
+}
+EXPORT_SYMBOL(qman_volatile_dequeue);
+
+static struct qm_eqcr_entry *try_eq_start(struct qman_portal **p)
+{
+	struct qm_eqcr_entry *eq;
+	*p = get_affine_portal();
+	if (qm_eqcr_get_avail((*p)->p) < EQCR_THRESH) {
+		local_irq_disable();
+		(*p)->eq_cons += qm_eqcr_cci_update((*p)->p);
+		local_irq_enable();
+	}
+	eq = qm_eqcr_start((*p)->p);
+	if (unlikely(!eq)) {
+		eqcr_set_thresh(*p, 1);
+		put_affine_portal();
+	}
+	return eq;
+}
+
+static inline int wait_eq_start(struct qman_portal **p,
+			struct qm_eqcr_entry **eq, u32 flags)
+{
+	int ret = 0;
+	if (flags & QMAN_ENQUEUE_FLAG_WAIT_INT)
+		ret = wait_event_interruptible(affine_queue,
+					(*eq = try_eq_start(p)));
+	else
+		wait_event(affine_queue, (*eq = try_eq_start(p)));
+	return ret;
+}
+
+/* Used as a wait_event() condition to determine if eq_cons has caught up to
+ * eq_poll. The complication is that they're wrap-arounds, so we use a cyclic
+ * comparison. This would a lot simpler if it weren't to work around a
+ * theoretical possibility - that the u32 prod/cons counters wrap so fast before
+ * this task is woken that it appears the enqueue never completed. We can't wait
+ * for it to wrap "another time" because activity might have stopped and the
+ * interrupt threshold is no longer set to wake us up (about as improbable as
+ * the scenario we're fixing). What we do then is wait until the cons counter
+ * reaches a safely-completed distance from 'eq_poll' *or* EQCR becomes empty,
+ * and continually reset the interrupt threshold until this happens (and for
+ * qman_poll() to do wakeups *after* unsetting the interrupt threshold). */
+static int eqcr_completed(struct qman_portal *p, u32 eq_poll)
+{
+	u32 tr_cons = p->eq_cons;
+	if (eq_poll & 0xc0000000) {
+		eq_poll &= 0x7fffffff;
+		tr_cons ^= 0x80000000;
+	}
+	if (tr_cons >= eq_poll)
+		return 1;
+	if ((eq_poll - tr_cons) > QM_EQCR_SIZE)
+		return 1;
+	if (!qm_eqcr_get_fill(p->p))
+		/* If EQCR is empty, we must have completed */
+		return 1;
+	eqcr_set_thresh(p, 0);
+	return 0;
+}
+
+static inline void eqcr_commit(struct qman_portal *p, u32 flags, int orp)
+{
+	u32 eq_poll;
+	qm_eqcr_pvb_commit(p->p,
+		(flags & (QM_EQCR_VERB_COLOUR_MASK | QM_EQCR_VERB_INTERRUPT |
+				QM_EQCR_VERB_CMD_ENQUEUE)) |
+		(orp ? QM_EQCR_VERB_ORP : 0));
+	/* increment the producer count and capture it for SYNC */
+	eq_poll = ++p->eq_prod;
+	if ((flags & QMAN_ENQUEUE_FLAG_WAIT_SYNC) ==
+			QMAN_ENQUEUE_FLAG_WAIT_SYNC)
+		eqcr_set_thresh(p, 1);
+	put_affine_portal();
+	if ((flags & QMAN_ENQUEUE_FLAG_WAIT_SYNC) !=
+			QMAN_ENQUEUE_FLAG_WAIT_SYNC)
+		return;
+	/* So we're supposed to wait until the commit is consumed */
+	if (flags & QMAN_ENQUEUE_FLAG_WAIT_INT)
+		/* See __enqueue() (where this inline is called) as to why we're
+		 * ignoring return codes from wait_***(). */
+		wait_event_interruptible(affine_queue,
+					eqcr_completed(p, eq_poll));
+	else
+		wait_event(affine_queue, eqcr_completed(p, eq_poll));
+}
+
+static inline void eqcr_abort(struct qman_portal *p)
+{
+	qm_eqcr_abort(p->p);
+	put_affine_portal();
+}
+
+/* Internal version of enqueue, used by ORP and non-ORP variants. Inlining
+ * should allow each instantiation to optimise appropriately (and this is why
+ * the excess 'orp' parameters are not an issue). */
+static inline int __enqueue(struct qman_fq *fq, const struct qm_fd *fd,
+				u32 flags, struct qman_fq *orp_fq,
+				u16 orp_seqnum, int orp)
+{
+	struct qm_eqcr_entry *eq;
+	struct qman_portal *p;
+
+#ifdef CONFIG_FSL_QMAN_CHECKING
+	if (unlikely(fq_isset(fq, QMAN_FQ_FLAG_NO_ENQUEUE)))
+		return -EINVAL;
+	if (unlikely(fq_isclear(fq, QMAN_FQ_FLAG_NO_MODIFY) &&
+			((fq->state == qman_fq_state_retired) ||
+			(fq->state == qman_fq_state_oos))))
+		return -EBUSY;
+#endif
+
+	eq = try_eq_start(&p);
+	if (unlikely(!eq)) {
+		if (flags & QMAN_ENQUEUE_FLAG_WAIT) {
+			int ret = wait_eq_start(&p, &eq, flags);
+			if (ret)
+				return ret;
+		} else
+			return -EBUSY;
+	}
+	/* If we're using ORP, it's very unwise to back-off because of
+	 * WATCH_CGR - that would leave a hole in the ORP sequence which could
+	 * block (if the caller doesn't retry). The idea is to enqueue via the
+	 * ORP anyway, and let congestion take effect in h/w once
+	 * order-restoration has occurred. */
+	if (unlikely(!orp && (flags & QMAN_ENQUEUE_FLAG_WATCH_CGR) && p->cgrs &&
+			fq_isset(fq, QMAN_FQ_STATE_CGR_EN) &&
+			qman_cgrs_get(&p->cgrs[1], fq->cgr_groupid))) {
+		eqcr_abort(p);
+		return -EAGAIN;
+	}
+	if (flags & QMAN_ENQUEUE_FLAG_DCA) {
+		u8 dca = QM_EQCR_DCA_ENABLE;
+		if (unlikely(flags & QMAN_ENQUEUE_FLAG_DCA_PARK))
+			dca |= QM_EQCR_DCA_PARK;
+		dca |= ((flags >> 8) & QM_EQCR_DCA_IDXMASK);
+		eq->dca = dca;
+	}
+	if (orp) {
+		if (flags & QMAN_ENQUEUE_FLAG_NLIS)
+			orp_seqnum |= QM_EQCR_SEQNUM_NLIS;
+		else {
+			orp_seqnum &= ~QM_EQCR_SEQNUM_NLIS;
+			if (flags & QMAN_ENQUEUE_FLAG_NESN)
+				orp_seqnum |= QM_EQCR_SEQNUM_NESN;
+			else
+				/* No need to check 4 QMAN_ENQUEUE_FLAG_HOLE */
+				orp_seqnum &= ~QM_EQCR_SEQNUM_NESN;
+		}
+		eq->seqnum = orp_seqnum;
+		eq->orp = orp_fq->fqid;
+	}
+	eq->fqid = fq->fqid;
+	eq->tag = (u32)fq;
+	eq->fd = *fd;
+	/* Issue the enqueue command, and wait for sync if requested.
+	 * NB: design choice - the commit can't fail, only waiting can. Don't
+	 * propogate any failure if a signal arrives. Otherwise the caller can't
+	 * distinguish whether the enqueue was issued or not. Code for
+	 * user-space can check signal_pending() after we return. */
+	eqcr_commit(p, flags, orp);
+	return 0;
+}
+
+int qman_enqueue(struct qman_fq *fq, const struct qm_fd *fd, u32 flags)
+{
+	flags |= QM_EQCR_VERB_CMD_ENQUEUE;
+	return __enqueue(fq, fd, flags, NULL, 0, 0);
+}
+EXPORT_SYMBOL(qman_enqueue);
+
+int qman_enqueue_orp(struct qman_fq *fq, const struct qm_fd *fd, u32 flags,
+			struct qman_fq *orp, u16 orp_seqnum)
+{
+	if (flags & (QMAN_ENQUEUE_FLAG_HOLE | QMAN_ENQUEUE_FLAG_NESN))
+		flags &= ~QM_EQCR_VERB_CMD_ENQUEUE;
+	else
+		flags |= QM_EQCR_VERB_CMD_ENQUEUE;
+	return __enqueue(fq, fd, flags, orp, orp_seqnum, 1);
+}
+EXPORT_SYMBOL(qman_enqueue_orp);
+
+int qman_init_cgr(u32 cgid)
+{
+	struct qm_mc_command *mcc;
+	struct qm_mc_result *mcr;
+	struct qman_portal *p = get_affine_portal();
+	u8 res;
+
+	local_irq_disable();
+	mcc = qm_mc_start(p->p);
+	mcc->initcgr.cgid = cgid;
+	qm_mc_commit(p->p, QM_MCC_VERB_INITCGR);
+	while (!(mcr = qm_mc_result(p->p)))
+		cpu_relax();
+	QM_ASSERT((mcr->verb & QM_MCR_VERB_MASK) == QM_MCC_VERB_INITCGR);
+	res = mcr->result;
+	if (res != QM_MCR_RESULT_OK) {
+		pr_err("INITCGR failed: %s\n", mcr_result_str(res));
+	}
+	local_irq_enable();
+	put_affine_portal();
+	return (res == QM_MCR_RESULT_OK) ? 0 : -EIO;
+}
+EXPORT_SYMBOL(qman_init_cgr);
+
diff --git a/drivers/hwqueue/qman_low.c b/drivers/hwqueue/qman_low.c
new file mode 100644
index 0000000..10cde3a
--- /dev/null
+++ b/drivers/hwqueue/qman_low.c
@@ -0,0 +1,1030 @@
+/* Copyright (c) 2008, 2009 Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "qman_private.h"
+
+/***************************/
+/* Portal register assists */
+/***************************/
+
+/* Cache-inhibited register offsets */
+#define REG_EQCR_PI_CINH	(void *)0x0000
+#define REG_EQCR_CI_CINH	(void *)0x0004
+#define REG_EQCR_ITR		(void *)0x0008
+#define REG_DQRR_PI_CINH	(void *)0x0040
+#define REG_DQRR_CI_CINH	(void *)0x0044
+#define REG_DQRR_ITR		(void *)0x0048
+#define REG_DQRR_DCAP		(void *)0x0050
+#define REG_DQRR_SDQCR		(void *)0x0054
+#define REG_DQRR_VDQCR		(void *)0x0058
+#define REG_DQRR_PDQCR		(void *)0x005c
+#define REG_MR_PI_CINH		(void *)0x0080
+#define REG_MR_CI_CINH		(void *)0x0084
+#define REG_MR_ITR		(void *)0x0088
+#define REG_CFG			(void *)0x0100
+#define REG_ISR			(void *)0x0e00
+
+/* Cache-enabled register offsets */
+#define CL_EQCR			(void *)0x0000
+#define CL_DQRR			(void *)0x1000
+#define CL_MR			(void *)0x2000
+#define CL_EQCR_PI_CENA		(void *)0x3000
+#define CL_EQCR_CI_CENA		(void *)0x3100
+#define CL_DQRR_PI_CENA		(void *)0x3200
+#define CL_DQRR_CI_CENA		(void *)0x3300
+#define CL_MR_PI_CENA		(void *)0x3400
+#define CL_MR_CI_CENA		(void *)0x3500
+#define CL_CR			(void *)0x3800
+#define CL_RR0			(void *)0x3900
+#define CL_RR1			(void *)0x3940
+
+/* The h/w design requires mappings to be size-aligned so that "add"s can be
+ * reduced to "or"s. The primitives below do the same for s/w. */
+
+/* Bitwise-OR two pointers */
+static inline void *ptr_OR(void *a, void *b)
+{
+	return (void *)((unsigned long)a | (unsigned long)b);
+}
+
+/* Cache-inhibited register access */
+static inline u32 __qm_in(struct qm_addr *qm, void *offset)
+{
+	return in_be32(ptr_OR(qm->addr_ci, offset));
+}
+static inline void __qm_out(struct qm_addr *qm, void *offset, u32 val)
+{
+	out_be32(ptr_OR(qm->addr_ci, offset), val);
+}
+#define qm_in(reg)		__qm_in(&portal->addr, REG_##reg)
+#define qm_out(reg, val)	__qm_out(&portal->addr, REG_##reg, val)
+
+/* Convert 'n' cachelines to a pointer value for bitwise OR */
+#define qm_cl(n)		(void *)((n) << 6)
+
+/* Cache-enabled (index) register access */
+static inline void __qm_cl_touch_ro(struct qm_addr *qm, void *offset)
+{
+	dcbt_ro(ptr_OR(qm->addr_ce, offset));
+}
+static inline void __qm_cl_touch_rw(struct qm_addr *qm, void *offset)
+{
+	dcbt_rw(ptr_OR(qm->addr_ce, offset));
+}
+static inline u32 __qm_cl_in(struct qm_addr *qm, void *offset)
+{
+	return in_be32(ptr_OR(qm->addr_ce, offset));
+}
+static inline void __qm_cl_out(struct qm_addr *qm, void *offset, u32 val)
+{
+	out_be32(ptr_OR(qm->addr_ce, offset), val);
+	dcbf(ptr_OR(qm->addr_ce, offset));
+}
+static inline void __qm_cl_invalidate(struct qm_addr *qm, void *offset)
+{
+	dcbi(ptr_OR(qm->addr_ce, offset));
+}
+#define qm_cl_touch_ro(reg)	__qm_cl_touch_ro(&portal->addr, CL_##reg##_CENA)
+#define qm_cl_touch_rw(reg)	__qm_cl_touch_rw(&portal->addr, CL_##reg##_CENA)
+#define qm_cl_in(reg)		__qm_cl_in(&portal->addr, CL_##reg##_CENA)
+#define qm_cl_out(reg, val)	__qm_cl_out(&portal->addr, CL_##reg##_CENA, val)
+#define qm_cl_invalidate(reg) __qm_cl_invalidate(&portal->addr, CL_##reg##_CENA)
+
+/* Cyclic helper for rings. FIXME: once we are able to do fine-grain perf
+ * analysis, look at using the "extra" bit in the ring index registers to avoid
+ * cyclic issues. */
+static inline u8 cyc_diff(u8 ringsize, u8 first, u8 last)
+{
+	/* 'first' is included, 'last' is excluded */
+	if (first <= last)
+		return last - first;
+	return ringsize + last - first;
+}
+
+
+/* ---------------- */
+/* --- EQCR API --- */
+
+/* Bit-wise logic to wrap a ring pointer by clearing the "carry bit" */
+#define EQCR_CARRYCLEAR(p) \
+	(void *)((unsigned long)(p) & (~(unsigned long)(QM_EQCR_SIZE << 6)))
+
+/* Bit-wise logic to convert a ring pointer to a ring index */
+static inline u8 EQCR_PTR2IDX(struct qm_eqcr_entry *e)
+{
+	return ((u32)e >> 6) & (QM_EQCR_SIZE - 1);
+}
+
+/* Increment the 'cursor' ring pointer, taking 'vbit' into account */
+static inline void EQCR_INC(struct qm_eqcr *eqcr)
+{
+	/* NB: this is odd-looking, but experiments show that it generates fast
+	 * code with essentially no branching overheads. We increment to the
+	 * next EQCR pointer and handle overflow and 'vbit'. */
+	struct qm_eqcr_entry *partial = eqcr->cursor + 1;
+	eqcr->cursor = EQCR_CARRYCLEAR(partial);
+	if (partial != eqcr->cursor)
+		eqcr->vbit ^= QM_EQCR_VERB_VBIT;
+}
+
+/* It's safer to code in terms of the 'eqcr' object than the 'portal' object,
+ * because the latter runs the risk of copy-n-paste errors from other code where
+ * we could manipulate some other structure within 'portal'. */
+#define eqcr	(&portal->eqcr)
+
+int qm_eqcr_init(struct qm_portal *portal, enum qm_eqcr_pmode pmode,
+		enum qm_eqcr_cmode cmode)
+{
+	u32 cfg;
+	u8 pi;
+
+	if (__qm_portal_bind(portal, QM_BIND_EQCR))
+		return -EBUSY;
+	eqcr->ring = ptr_OR(portal->addr.addr_ce, CL_EQCR);
+	eqcr->ci = qm_in(EQCR_CI_CINH) & (QM_EQCR_SIZE - 1);
+	pi = qm_in(EQCR_PI_CINH) & (QM_EQCR_SIZE - 1);
+	eqcr->cursor = eqcr->ring + pi;
+	eqcr->vbit = (qm_in(EQCR_PI_CINH) & QM_EQCR_SIZE) ?
+			QM_EQCR_VERB_VBIT : 0;
+	eqcr->available = QM_EQCR_SIZE - 1 -
+			cyc_diff(QM_EQCR_SIZE, eqcr->ci, pi);
+	eqcr->ithresh = qm_in(EQCR_ITR);
+#ifdef CONFIG_FSL_QMAN_CHECKING
+	eqcr->busy = 0;
+	eqcr->pmode = pmode;
+	eqcr->cmode = cmode;
+#endif
+	cfg = (qm_in(CFG) & 0x00ffffff) |
+		((pmode & 0x3) << 24);	/* QCSP_CFG::EPM */
+	qm_out(CFG, cfg);
+	return 0;
+}
+EXPORT_SYMBOL(qm_eqcr_init);
+
+void qm_eqcr_finish(struct qm_portal *portal)
+{
+	u8 pi = qm_in(EQCR_PI_CINH) & (QM_EQCR_SIZE - 1);
+	u8 ci = qm_in(EQCR_CI_CINH) & (QM_EQCR_SIZE - 1);
+
+	QM_ASSERT(!eqcr->busy);
+	if (pi != EQCR_PTR2IDX(eqcr->cursor))
+		pr_crit("losing uncommited EQCR entries\n");
+	if (ci != eqcr->ci)
+		pr_crit("missing existing EQCR completions\n");
+	if (eqcr->ci != EQCR_PTR2IDX(eqcr->cursor))
+		pr_crit("EQCR destroyed unquiesced\n");
+	__qm_portal_unbind(portal, QM_BIND_EQCR);
+}
+EXPORT_SYMBOL(qm_eqcr_finish);
+
+struct qm_eqcr_entry *qm_eqcr_start(struct qm_portal *portal)
+{
+	QM_ASSERT(!eqcr->busy);
+	if (!eqcr->available)
+		return NULL;
+#ifdef CONFIG_FSL_QMAN_CHECKING
+	eqcr->busy = 1;
+#endif
+	dcbzl(eqcr->cursor);
+	return eqcr->cursor;
+}
+EXPORT_SYMBOL(qm_eqcr_start);
+
+void qm_eqcr_abort(struct qm_portal *portal)
+{
+	QM_ASSERT(eqcr->busy);
+#ifdef CONFIG_FSL_QMAN_CHECKING
+	eqcr->busy = 0;
+#endif
+}
+EXPORT_SYMBOL(qm_eqcr_abort);
+
+struct qm_eqcr_entry *qm_eqcr_pend_and_next(struct qm_portal *portal, u8 myverb)
+{
+	QM_ASSERT(eqcr->busy);
+	QM_ASSERT(eqcr->pmode != qm_eqcr_pvb);
+	if (eqcr->available == 1)
+		return NULL;
+	eqcr->cursor->__dont_write_directly__verb = myverb | eqcr->vbit;
+	dcbf(eqcr->cursor);
+	EQCR_INC(eqcr);
+	eqcr->available--;
+	dcbzl(eqcr->cursor);
+	return eqcr->cursor;
+}
+EXPORT_SYMBOL(qm_eqcr_pend_and_next);
+
+#define EQCR_COMMIT_CHECKS(eqcr) \
+do { \
+	QM_ASSERT(eqcr->busy); \
+	QM_ASSERT(eqcr->cursor->orp == (eqcr->cursor->orp & 0x00ffffff)); \
+	QM_ASSERT(eqcr->cursor->fqid == (eqcr->cursor->fqid & 0x00ffffff)); \
+} while(0)
+
+void qm_eqcr_pci_commit(struct qm_portal *portal, u8 myverb)
+{
+	EQCR_COMMIT_CHECKS(eqcr);
+	QM_ASSERT(eqcr->pmode == qm_eqcr_pci);
+	eqcr->cursor->__dont_write_directly__verb = myverb | eqcr->vbit;
+	EQCR_INC(eqcr);
+	eqcr->available--;
+	dcbf(eqcr->cursor);
+	hwsync();
+	qm_out(EQCR_PI_CINH, EQCR_PTR2IDX(eqcr->cursor));
+#ifdef CONFIG_FSL_QMAN_CHECKING
+	eqcr->busy = 0;
+#endif
+}
+EXPORT_SYMBOL(qm_eqcr_pci_commit);
+
+void qm_eqcr_pce_prefetch(struct qm_portal *portal)
+{
+	QM_ASSERT(eqcr->pmode == qm_eqcr_pce);
+	qm_cl_invalidate(EQCR_PI);
+	qm_cl_touch_rw(EQCR_PI);
+}
+EXPORT_SYMBOL(qm_eqcr_pce_prefetch);
+
+void qm_eqcr_pce_commit(struct qm_portal *portal, u8 myverb)
+{
+	EQCR_COMMIT_CHECKS(eqcr);
+	QM_ASSERT(eqcr->pmode == qm_eqcr_pce);
+	eqcr->cursor->__dont_write_directly__verb = myverb | eqcr->vbit;
+	EQCR_INC(eqcr);
+	eqcr->available--;
+	dcbf(eqcr->cursor);
+	lwsync();
+	qm_cl_out(EQCR_PI, EQCR_PTR2IDX(eqcr->cursor));
+#ifdef CONFIG_FSL_QMAN_CHECKING
+	eqcr->busy = 0;
+#endif
+}
+EXPORT_SYMBOL(qm_eqcr_pce_commit);
+
+void qm_eqcr_pvb_commit(struct qm_portal *portal, u8 myverb)
+{
+	EQCR_COMMIT_CHECKS(eqcr);
+	QM_ASSERT(eqcr->pmode == qm_eqcr_pvb);
+	lwsync();
+	eqcr->cursor->__dont_write_directly__verb = myverb | eqcr->vbit;
+	dcbf(eqcr->cursor);
+	EQCR_INC(eqcr);
+	eqcr->available--;
+#ifdef CONFIG_FSL_QMAN_CHECKING
+	eqcr->busy = 0;
+#endif
+}
+EXPORT_SYMBOL(qm_eqcr_pvb_commit);
+
+u8 qm_eqcr_cci_update(struct qm_portal *portal)
+{
+	u8 diff, old_ci = eqcr->ci;
+	QM_ASSERT(eqcr->cmode == qm_eqcr_cci);
+	eqcr->ci = qm_in(EQCR_CI_CINH) & (QM_EQCR_SIZE - 1);
+	diff = cyc_diff(QM_EQCR_SIZE, old_ci, eqcr->ci);
+	eqcr->available += diff;
+	return diff;
+}
+EXPORT_SYMBOL(qm_eqcr_cci_update);
+
+void qm_eqcr_cce_prefetch(struct qm_portal *portal)
+{
+	QM_ASSERT(eqcr->cmode == qm_eqcr_cce);
+	qm_cl_invalidate(EQCR_CI);
+	qm_cl_touch_ro(EQCR_CI);
+}
+EXPORT_SYMBOL(qm_eqcr_cce_prefetch);
+
+u8 qm_eqcr_cce_update(struct qm_portal *portal)
+{
+	u8 diff, old_ci = eqcr->ci;
+	QM_ASSERT(eqcr->cmode == qm_eqcr_cce);
+	eqcr->ci = qm_cl_in(EQCR_CI) & (QM_EQCR_SIZE - 1);
+	qm_cl_invalidate(EQCR_CI);
+	diff = cyc_diff(QM_EQCR_SIZE, old_ci, eqcr->ci);
+	eqcr->available += diff;
+	return diff;
+}
+EXPORT_SYMBOL(qm_eqcr_cce_update);
+
+u8 qm_eqcr_get_ithresh(struct qm_portal *portal)
+{
+	return eqcr->ithresh;
+}
+EXPORT_SYMBOL(qm_eqcr_get_ithresh);
+
+void qm_eqcr_set_ithresh(struct qm_portal *portal, u8 ithresh)
+{
+	eqcr->ithresh = ithresh;
+	qm_out(EQCR_ITR, ithresh);
+}
+EXPORT_SYMBOL(qm_eqcr_set_ithresh);
+
+u8 qm_eqcr_get_avail(struct qm_portal *portal)
+{
+	return eqcr->available;
+}
+EXPORT_SYMBOL(qm_eqcr_get_avail);
+
+u8 qm_eqcr_get_fill(struct qm_portal *portal)
+{
+	return QM_EQCR_SIZE - 1 - eqcr->available;
+}
+EXPORT_SYMBOL(qm_eqcr_get_fill);
+
+
+/* ---------------- */
+/* --- DQRR API --- */
+
+/* FIXME: many possible improvements;
+ * - look at changing the API to use pointer rather than index parameters now
+ *   that 'cursor' is a pointer,
+ * - consider moving other parameters to pointer if it could help (ci)
+ */
+
+#define DQRR_CARRYCLEAR(p) \
+	(void *)((unsigned long)(p) & (~(unsigned long)(QM_DQRR_SIZE << 6)))
+
+static inline u8 DQRR_PTR2IDX(struct qm_dqrr_entry *e)
+{
+	return ((u32)e >> 6) & (QM_DQRR_SIZE - 1);
+}
+
+static inline struct qm_dqrr_entry *DQRR_INC(struct qm_dqrr_entry *e)
+{
+	return DQRR_CARRYCLEAR(e + 1);
+}
+
+/* It's safer to code in terms of the 'dqrr' object than the 'portal' object,
+ * because the latter runs the risk of copy-n-paste errors from other code where
+ * we could manipulate some other structure within 'portal'. */
+#define dqrr	(&portal->dqrr)
+
+int qm_dqrr_init(struct qm_portal *portal, enum qm_dqrr_dmode dmode,
+		enum qm_dqrr_pmode pmode, enum qm_dqrr_cmode cmode,
+		u8 max_fill, int stash_ring, int stash_data)
+{
+	const struct qm_portal_config *config = qm_portal_config(portal);
+	u32 cfg;
+
+	if (__qm_portal_bind(portal, QM_BIND_DQRR))
+		return -EBUSY;
+	if ((stash_ring || stash_data) &&
+			((config->cpu == -1) || !config->has_hv_dma))
+		return -EINVAL;
+	/* Make sure the DQRR will be idle when we enable */
+	qm_out(DQRR_SDQCR, 0);
+	qm_out(DQRR_VDQCR, 0);
+	qm_out(DQRR_PDQCR, 0);
+	dqrr->ring = ptr_OR(portal->addr.addr_ce, CL_DQRR);
+	dqrr->pi = qm_in(DQRR_PI_CINH) & (QM_DQRR_SIZE - 1);
+	dqrr->ci = qm_in(DQRR_CI_CINH) & (QM_DQRR_SIZE - 1);
+	dqrr->cursor = dqrr->ring + dqrr->ci;
+	dqrr->fill = cyc_diff(QM_DQRR_SIZE, dqrr->ci, dqrr->pi);
+	dqrr->vbit = (qm_in(DQRR_PI_CINH) & QM_DQRR_SIZE) ?
+			QM_DQRR_VERB_VBIT : 0;
+	dqrr->ithresh = qm_in(DQRR_ITR);
+#ifdef CONFIG_FSL_QMAN_CHECKING
+	dqrr->dmode = dmode;
+	dqrr->pmode = pmode;
+	dqrr->cmode = cmode;
+#endif
+	dqrr->flags = 0;
+	if (stash_ring)
+		dqrr->flags |= QM_DQRR_FLAG_RE;
+	if (stash_data)
+		dqrr->flags |= QM_DQRR_FLAG_SE;
+	cfg = (qm_in(CFG) & 0xff000f00) |
+		((max_fill & (QM_DQRR_SIZE - 1)) << 20) | /* DQRR_MF */
+		((dmode & 1) << 18) |			/* DP */
+		((cmode & 3) << 16) |			/* DCM */
+		(stash_ring ? 0x80 : 0) |		/* RE */
+		(0 ? 0x40 : 0) |			/* Ignore RP */
+		(stash_data ? 0x20 : 0) |		/* SE */
+		(0 ? 0x10 : 0);				/* Ignore SP */
+	qm_out(CFG, cfg);
+	qm_dqrr_set_maxfill(portal, max_fill);
+	return 0;
+}
+EXPORT_SYMBOL(qm_dqrr_init);
+
+void qm_dqrr_finish(struct qm_portal *portal)
+{
+	if (dqrr->ci != DQRR_PTR2IDX(dqrr->cursor))
+		pr_crit("Ignoring completed DQRR entries\n");
+	__qm_portal_unbind(portal, QM_BIND_DQRR);
+}
+EXPORT_SYMBOL(qm_dqrr_finish);
+
+void qm_dqrr_current_prefetch(struct qm_portal *portal)
+{
+	/* If ring entries get stashed, don't invalidate/prefetch */
+	if (!(dqrr->flags & QM_DQRR_FLAG_RE)) {
+		dcbi(dqrr->cursor);
+		dcbt_ro(dqrr->cursor);
+	}
+}
+EXPORT_SYMBOL(qm_dqrr_current_prefetch);
+
+struct qm_dqrr_entry *qm_dqrr_current(struct qm_portal *portal)
+{
+	if (!dqrr->fill)
+		return NULL;
+	return dqrr->cursor;
+}
+EXPORT_SYMBOL(qm_dqrr_current);
+
+u8 qm_dqrr_cursor(struct qm_portal *portal)
+{
+	return DQRR_PTR2IDX(dqrr->cursor);
+}
+EXPORT_SYMBOL(qm_dqrr_cursor);
+
+u8 qm_dqrr_next(struct qm_portal *portal)
+{
+	QM_ASSERT(dqrr->fill);
+	dqrr->cursor = DQRR_INC(dqrr->cursor);
+	return --dqrr->fill;
+}
+EXPORT_SYMBOL(qm_dqrr_next);
+
+u8 qm_dqrr_pci_update(struct qm_portal *portal)
+{
+	u8 diff, old_pi = dqrr->pi;
+	QM_ASSERT(dqrr->pmode == qm_dqrr_pci);
+	dqrr->pi = qm_in(DQRR_PI_CINH) & (QM_DQRR_SIZE - 1);
+	diff = cyc_diff(QM_DQRR_SIZE, old_pi, dqrr->pi);
+	dqrr->fill += diff;
+	return diff;
+}
+EXPORT_SYMBOL(qm_dqrr_pci_update);
+
+void qm_dqrr_pce_prefetch(struct qm_portal *portal)
+{
+	QM_ASSERT(dqrr->pmode == qm_dqrr_pce);
+	qm_cl_invalidate(DQRR_PI);
+	qm_cl_touch_ro(DQRR_PI);
+}
+EXPORT_SYMBOL(qm_dqrr_pce_prefetch);
+
+u8 qm_dqrr_pce_update(struct qm_portal *portal)
+{
+	u8 diff, old_pi = dqrr->pi;
+	QM_ASSERT(dqrr->pmode == qm_dqrr_pce);
+	dqrr->pi = qm_cl_in(DQRR_PI) & (QM_DQRR_SIZE - 1);
+	diff = cyc_diff(QM_DQRR_SIZE, old_pi, dqrr->pi);
+	dqrr->fill += diff;
+	return diff;
+}
+EXPORT_SYMBOL(qm_dqrr_pce_update);
+
+void qm_dqrr_pvb_prefetch(struct qm_portal *portal)
+{
+	QM_ASSERT(dqrr->pmode == qm_dqrr_pvb);
+	/* If ring entries get stashed, don't invalidate/prefetch */
+	if (!(dqrr->flags & QM_DQRR_FLAG_RE)) {
+		dcbi(ptr_OR(dqrr->ring, qm_cl(dqrr->pi)));
+		dcbt_ro(ptr_OR(dqrr->ring, qm_cl(dqrr->pi)));
+	}
+}
+EXPORT_SYMBOL(qm_dqrr_pvb_prefetch);
+
+u8 qm_dqrr_pvb_update(struct qm_portal *portal)
+{
+	struct qm_dqrr_entry *res = ptr_OR(dqrr->ring, qm_cl(dqrr->pi));
+	QM_ASSERT(dqrr->pmode == qm_dqrr_pvb);
+	if ((res->verb & QM_DQRR_VERB_VBIT) == dqrr->vbit) {
+		dqrr->pi = (dqrr->pi + 1) & (QM_DQRR_SIZE - 1);
+		if (!dqrr->pi)
+			dqrr->vbit ^= QM_DQRR_VERB_VBIT;
+		dqrr->fill++;
+		return 1;
+	}
+	return 0;
+}
+EXPORT_SYMBOL(qm_dqrr_pvb_update);
+
+void qm_dqrr_cci_consume(struct qm_portal *portal, u8 num)
+{
+	QM_ASSERT(dqrr->cmode == qm_dqrr_cci);
+	dqrr->ci = (dqrr->ci + num) & (QM_DQRR_SIZE - 1);
+	qm_out(DQRR_CI_CINH, dqrr->ci);
+}
+EXPORT_SYMBOL(qm_dqrr_cci_consume);
+
+void qm_dqrr_cci_consume_to_current(struct qm_portal *portal)
+{
+	QM_ASSERT(dqrr->cmode == qm_dqrr_cci);
+	dqrr->ci = DQRR_PTR2IDX(dqrr->cursor);
+	qm_out(DQRR_CI_CINH, dqrr->ci);
+}
+EXPORT_SYMBOL(qm_dqrr_cci_consume_to_current);
+
+void qm_dqrr_cce_prefetch(struct qm_portal *portal)
+{
+	QM_ASSERT(dqrr->cmode == qm_dqrr_cce);
+	qm_cl_invalidate(DQRR_CI);
+	qm_cl_touch_rw(DQRR_CI);
+}
+EXPORT_SYMBOL(qm_dqrr_cce_prefetch);
+
+void qm_dqrr_cce_consume(struct qm_portal *portal, u8 num)
+{
+	QM_ASSERT(dqrr->cmode == qm_dqrr_cce);
+	dqrr->ci = (dqrr->ci + num) & (QM_DQRR_SIZE - 1);
+	qm_cl_out(DQRR_CI, dqrr->ci);
+}
+EXPORT_SYMBOL(qm_dqrr_cce_consume);
+
+void qm_dqrr_cce_consume_to_current(struct qm_portal *portal)
+{
+	QM_ASSERT(dqrr->cmode == qm_dqrr_cce);
+	dqrr->ci = DQRR_PTR2IDX(dqrr->cursor);
+	qm_cl_out(DQRR_CI, dqrr->ci);
+}
+EXPORT_SYMBOL(qm_dqrr_cce_consume_to_current);
+
+void qm_dqrr_cdc_consume_1(struct qm_portal *portal, u8 idx, int park)
+{
+	QM_ASSERT(dqrr->cmode == qm_dqrr_cdc);
+	QM_ASSERT(idx < QM_DQRR_SIZE);
+	qm_out(DQRR_DCAP, (0 << 8) |	/* S */
+		((park ? 1 : 0) << 6) |	/* PK */
+		idx);			/* DCAP_CI */
+}
+EXPORT_SYMBOL(qm_dqrr_cdc_consume_1);
+
+void qm_dqrr_cdc_consume_1ptr(struct qm_portal *portal, struct qm_dqrr_entry *dq,
+				int park)
+{
+	u8 idx = DQRR_PTR2IDX(dq);
+	QM_ASSERT(dqrr->cmode == qm_dqrr_cdc);
+	QM_ASSERT((dqrr->ring + idx) == dq);
+	QM_ASSERT(idx < QM_DQRR_SIZE);
+	qm_out(DQRR_DCAP, (0 << 8) |		/* DQRR_DCAP::S */
+		((park ? 1 : 0) << 6) |		/* DQRR_DCAP::PK */
+		idx);				/* DQRR_DCAP::DCAP_CI */
+}
+EXPORT_SYMBOL(qm_dqrr_cdc_consume_1ptr);
+
+void qm_dqrr_cdc_consume_n(struct qm_portal *portal, u16 bitmask)
+{
+	QM_ASSERT(dqrr->cmode == qm_dqrr_cdc);
+	qm_out(DQRR_DCAP, (1 << 8) |		/* DQRR_DCAP::S */
+		((u32)bitmask << 16));		/* DQRR_DCAP::DCAP_CI */
+}
+EXPORT_SYMBOL(qm_dqrr_cdc_consume_n);
+
+u8 qm_dqrr_cdc_cci(struct qm_portal *portal)
+{
+	QM_ASSERT(dqrr->cmode == qm_dqrr_cdc);
+	return qm_in(DQRR_CI_CINH) & (QM_DQRR_SIZE - 1);
+}
+EXPORT_SYMBOL(qm_dqrr_cdc_cci);
+
+void qm_dqrr_cdc_cce_prefetch(struct qm_portal *portal)
+{
+	QM_ASSERT(dqrr->cmode == qm_dqrr_cdc);
+	qm_cl_invalidate(DQRR_CI);
+	qm_cl_touch_ro(DQRR_CI);
+}
+EXPORT_SYMBOL(qm_dqrr_cdc_cce_prefetch);
+
+u8 qm_dqrr_cdc_cce(struct qm_portal *portal)
+{
+	QM_ASSERT(dqrr->cmode == qm_dqrr_cdc);
+	return qm_cl_in(DQRR_CI) & (QM_DQRR_SIZE - 1);
+}
+EXPORT_SYMBOL(qm_dqrr_cdc_cce);
+
+u8 qm_dqrr_get_ci(struct qm_portal *portal)
+{
+	QM_ASSERT(dqrr->cmode != qm_dqrr_cdc);
+	return dqrr->ci;
+}
+EXPORT_SYMBOL(qm_dqrr_get_ci);
+
+void qm_dqrr_park(struct qm_portal *portal, u8 idx)
+{
+	QM_ASSERT(dqrr->cmode != qm_dqrr_cdc);
+	qm_out(DQRR_DCAP, (0 << 8) |		/* S */
+		(1 << 6) |			/* PK */
+		(idx & (QM_DQRR_SIZE - 1)));	/* DCAP_CI */
+}
+EXPORT_SYMBOL(qm_dqrr_park);
+
+void qm_dqrr_park_ci(struct qm_portal *portal)
+{
+	QM_ASSERT(dqrr->cmode != qm_dqrr_cdc);
+	qm_out(DQRR_DCAP, (0 << 8) |		/* S */
+		(1 << 6) |			/* PK */
+		(dqrr->ci & (QM_DQRR_SIZE - 1)));/* DCAP_CI */
+}
+EXPORT_SYMBOL(qm_dqrr_park_ci);
+
+void qm_dqrr_sdqcr_set(struct qm_portal *portal, u32 sdqcr)
+{
+	qm_out(DQRR_SDQCR, sdqcr);
+}
+EXPORT_SYMBOL(qm_dqrr_sdqcr_set);
+
+u32 qm_dqrr_sdqcr_get(struct qm_portal *portal)
+{
+	return qm_in(DQRR_SDQCR);
+}
+EXPORT_SYMBOL(qm_dqrr_sdqcr_get);
+
+void qm_dqrr_vdqcr_set(struct qm_portal *portal, u32 vdqcr)
+{
+	qm_out(DQRR_VDQCR, vdqcr);
+}
+EXPORT_SYMBOL(qm_dqrr_vdqcr_set);
+
+u32 qm_dqrr_vdqcr_get(struct qm_portal *portal)
+{
+	return qm_in(DQRR_VDQCR);
+}
+EXPORT_SYMBOL(qm_dqrr_vdqcr_get);
+
+void qm_dqrr_pdqcr_set(struct qm_portal *portal, u32 pdqcr)
+{
+	qm_out(DQRR_PDQCR, pdqcr);
+}
+EXPORT_SYMBOL(qm_dqrr_pdqcr_set);
+
+u32 qm_dqrr_pdqcr_get(struct qm_portal *portal)
+{
+	return qm_in(DQRR_PDQCR);
+}
+EXPORT_SYMBOL(qm_dqrr_pdqcr_get);
+
+u8 qm_dqrr_get_ithresh(struct qm_portal *portal)
+{
+	return dqrr->ithresh;
+}
+EXPORT_SYMBOL(qm_dqrr_get_ithresh);
+
+void qm_dqrr_set_ithresh(struct qm_portal *portal, u8 ithresh)
+{
+	qm_out(DQRR_ITR, ithresh);
+}
+EXPORT_SYMBOL(qm_dqrr_set_ithresh);
+
+u8 qm_dqrr_get_maxfill(struct qm_portal *portal)
+{
+	return (qm_in(CFG) & 0x00f00000) >> 20;
+}
+EXPORT_SYMBOL(qm_dqrr_get_maxfill);
+
+void qm_dqrr_set_maxfill(struct qm_portal *portal, u8 mf)
+{
+	qm_out(CFG, (qm_in(CFG) & 0xff0fffff) |
+		((mf & (QM_DQRR_SIZE - 1)) << 20));
+}
+EXPORT_SYMBOL(qm_dqrr_set_maxfill);
+
+
+/* -------------- */
+/* --- MR API --- */
+
+#define MR_CARRYCLEAR(p) \
+	(void *)((unsigned long)(p) & (~(unsigned long)(QM_MR_SIZE << 6)))
+
+static inline u8 MR_PTR2IDX(struct qm_mr_entry *e)
+{
+	return ((u32)e >> 6) & (QM_MR_SIZE - 1);
+}
+
+static inline struct qm_mr_entry *MR_INC(struct qm_mr_entry *e)
+{
+	return MR_CARRYCLEAR(e + 1);
+}
+
+/* It's safer to code in terms of the 'mr' object than the 'portal' object,
+ * because the latter runs the risk of copy-n-paste errors from other code where
+ * we could manipulate some other structure within 'portal'. */
+#define mr	(&portal->mr)
+
+int qm_mr_init(struct qm_portal *portal, enum qm_mr_pmode pmode,
+		enum qm_mr_cmode cmode)
+{
+	u32 cfg;
+
+	if (__qm_portal_bind(portal, QM_BIND_MR))
+		return -EBUSY;
+	mr->ring = ptr_OR(portal->addr.addr_ce, CL_MR);
+	mr->pi = qm_in(MR_PI_CINH) & (QM_MR_SIZE - 1);
+	mr->ci = qm_in(MR_CI_CINH) & (QM_MR_SIZE - 1);
+	mr->cursor = mr->ring + mr->ci;
+	mr->fill = cyc_diff(QM_MR_SIZE, mr->ci, mr->pi);
+	mr->vbit = (qm_in(MR_PI_CINH) & QM_MR_SIZE) ?QM_MR_VERB_VBIT : 0;
+	mr->ithresh = qm_in(MR_ITR);
+#ifdef CONFIG_FSL_QMAN_CHECKING
+	mr->pmode = pmode;
+	mr->cmode = cmode;
+#endif
+	cfg = (qm_in(CFG) & 0xfffff0ff) |
+		((cmode & 1) << 8);		/* QCSP_CFG:MM */
+	qm_out(CFG, cfg);
+	return 0;
+}
+EXPORT_SYMBOL(qm_mr_init);
+
+void qm_mr_finish(struct qm_portal *portal)
+{
+	if (mr->ci != MR_PTR2IDX(mr->cursor))
+		pr_crit("Ignoring completed MR entries\n");
+	__qm_portal_unbind(portal, QM_BIND_MR);
+}
+EXPORT_SYMBOL(qm_mr_finish);
+
+void qm_mr_current_prefetch(struct qm_portal *portal)
+{
+	dcbi(mr->cursor);
+	dcbt_ro(mr->cursor);
+}
+EXPORT_SYMBOL(qm_mr_current_prefetch);
+
+struct qm_mr_entry *qm_mr_current(struct qm_portal *portal)
+{
+	if (!mr->fill)
+		return NULL;
+	return mr->cursor;
+}
+EXPORT_SYMBOL(qm_mr_current);
+
+u8 qm_mr_cursor(struct qm_portal *portal)
+{
+	return MR_PTR2IDX(mr->cursor);
+}
+EXPORT_SYMBOL(qm_mr_cursor);
+
+u8 qm_mr_next(struct qm_portal *portal)
+{
+	QM_ASSERT(mr->fill);
+	mr->cursor = MR_INC(mr->cursor);
+	return --mr->fill;
+}
+EXPORT_SYMBOL(qm_mr_next);
+
+u8 qm_mr_pci_update(struct qm_portal *portal)
+{
+	u8 diff, old_pi = mr->pi;
+	QM_ASSERT(mr->pmode == qm_mr_pci);
+	mr->pi = qm_in(MR_PI_CINH);
+	diff = cyc_diff(QM_MR_SIZE, old_pi, mr->pi);
+	mr->fill += diff;
+	return diff;
+}
+EXPORT_SYMBOL(qm_mr_pci_update);
+
+void qm_mr_pce_prefetch(struct qm_portal *portal)
+{
+	QM_ASSERT(mr->pmode == qm_mr_pce);
+	qm_cl_invalidate(MR_PI);
+	qm_cl_touch_ro(MR_PI);
+}
+EXPORT_SYMBOL(qm_mr_pce_prefetch);
+
+u8 qm_mr_pce_update(struct qm_portal *portal)
+{
+	u8 diff, old_pi = mr->pi;
+	QM_ASSERT(mr->pmode == qm_mr_pce);
+	mr->pi = qm_cl_in(MR_PI) & (QM_MR_SIZE - 1);
+	diff = cyc_diff(QM_MR_SIZE, old_pi, mr->pi);
+	mr->fill += diff;
+	return diff;
+}
+EXPORT_SYMBOL(qm_mr_pce_update);
+
+void qm_mr_pvb_prefetch(struct qm_portal *portal)
+{
+	QM_ASSERT(mr->pmode == qm_mr_pvb);
+	dcbi(ptr_OR(mr->ring, qm_cl(mr->pi)));
+	dcbt_ro(ptr_OR(mr->ring, qm_cl(mr->pi)));
+}
+EXPORT_SYMBOL(qm_mr_pvb_prefetch);
+
+u8 qm_mr_pvb_update(struct qm_portal *portal)
+{
+	struct qm_mr_entry *res = ptr_OR(mr->ring, qm_cl(mr->pi));
+	QM_ASSERT(mr->pmode == qm_mr_pvb);
+	if ((res->verb & QM_MR_VERB_VBIT) == mr->vbit) {
+		mr->pi = (mr->pi + 1) & (QM_MR_SIZE - 1);
+		if (!mr->pi)
+			mr->vbit ^= QM_MR_VERB_VBIT;
+		mr->fill++;
+		return 1;
+	}
+	return 0;
+}
+EXPORT_SYMBOL(qm_mr_pvb_update);
+
+void qm_mr_cci_consume(struct qm_portal *portal, u8 num)
+{
+	QM_ASSERT(mr->cmode == qm_mr_cci);
+	mr->ci = (mr->ci + num) & (QM_MR_SIZE - 1);
+	qm_out(MR_CI_CINH, mr->ci);
+}
+EXPORT_SYMBOL(qm_mr_cci_consume);
+
+void qm_mr_cci_consume_to_current(struct qm_portal *portal)
+{
+	QM_ASSERT(mr->cmode == qm_mr_cci);
+	mr->ci = MR_PTR2IDX(mr->cursor);
+	qm_out(MR_CI_CINH, mr->ci);
+}
+EXPORT_SYMBOL(qm_mr_cci_consume_to_current);
+
+void qm_mr_cce_prefetch(struct qm_portal *portal)
+{
+	QM_ASSERT(mr->cmode == qm_mr_cce);
+	qm_cl_invalidate(MR_CI);
+	qm_cl_touch_rw(MR_CI);
+}
+EXPORT_SYMBOL(qm_mr_cce_prefetch);
+
+void qm_mr_cce_consume(struct qm_portal *portal, u8 num)
+{
+	QM_ASSERT(mr->cmode == qm_mr_cce);
+	mr->ci = (mr->ci + num) & (QM_MR_SIZE - 1);
+	qm_cl_out(MR_CI, mr->ci);
+}
+EXPORT_SYMBOL(qm_mr_cce_consume);
+
+void qm_mr_cce_consume_to_current(struct qm_portal *portal)
+{
+	QM_ASSERT(mr->cmode == qm_mr_cce);
+	mr->ci = MR_PTR2IDX(mr->cursor);
+	qm_cl_out(MR_CI, mr->ci);
+}
+EXPORT_SYMBOL(qm_mr_cce_consume_to_current);
+
+u8 qm_mr_get_ci(struct qm_portal *portal)
+{
+	return mr->ci;
+}
+EXPORT_SYMBOL(qm_mr_get_ci);
+
+u8 qm_mr_get_ithresh(struct qm_portal *portal)
+{
+	return mr->ithresh;
+}
+EXPORT_SYMBOL(qm_mr_get_ithresh);
+
+void qm_mr_set_ithresh(struct qm_portal *portal, u8 ithresh)
+{
+	qm_out(MR_ITR, ithresh);
+}
+EXPORT_SYMBOL(qm_mr_set_ithresh);
+
+
+/* ------------------------------ */
+/* --- Management command API --- */
+
+/* It's safer to code in terms of the 'mc' object than the 'portal' object,
+ * because the latter runs the risk of copy-n-paste errors from other code where
+ * we could manipulate some other structure within 'portal'. */
+#define mc	(&portal->mc)
+
+int qm_mc_init(struct qm_portal *portal)
+{
+	if (__qm_portal_bind(portal, QM_BIND_MC))
+		return -EBUSY;
+	mc->cr = ptr_OR(portal->addr.addr_ce, CL_CR);
+	mc->rr = ptr_OR(portal->addr.addr_ce, CL_RR0);
+	mc->rridx = (mc->cr->__dont_write_directly__verb & QM_MCC_VERB_VBIT) ?
+			0 : 1;
+	mc->vbit = mc->rridx ? QM_MCC_VERB_VBIT : 0;
+#ifdef CONFIG_FSL_QMAN_CHECKING
+	mc->state = mc_idle;
+#endif
+	return 0;
+}
+EXPORT_SYMBOL(qm_mc_init);
+
+void qm_mc_finish(struct qm_portal *portal)
+{
+	QM_ASSERT(mc->state == mc_idle);
+#ifdef CONFIG_FSL_QMAN_CHECKING
+	if (mc->state != mc_idle)
+		pr_crit("Losing incomplete MC command\n");
+#endif
+	__qm_portal_unbind(portal, QM_BIND_MC);
+}
+EXPORT_SYMBOL(qm_mc_finish);
+
+struct qm_mc_command *qm_mc_start(struct qm_portal *portal)
+{
+	QM_ASSERT(mc->state == mc_idle);
+#ifdef CONFIG_FSL_QMAN_CHECKING
+	mc->state = mc_user;
+#endif
+	dcbzl(mc->cr);
+	return mc->cr;
+}
+EXPORT_SYMBOL(qm_mc_start);
+
+void qm_mc_abort(struct qm_portal *portal)
+{
+	QM_ASSERT(mc->state == mc_user);
+#ifdef CONFIG_FSL_QMAN_CHECKING
+	mc->state = mc_idle;
+#endif
+}
+EXPORT_SYMBOL(qm_mc_abort);
+
+void qm_mc_commit(struct qm_portal *portal, u8 myverb)
+{
+	QM_ASSERT(mc->state == mc_user);
+	dcbi(mc->rr + mc->rridx);
+	lwsync();
+	mc->cr->__dont_write_directly__verb = myverb | mc->vbit;
+	dcbf(mc->cr);
+	dcbt_ro(mc->rr + mc->rridx);
+#ifdef CONFIG_FSL_QMAN_CHECKING
+	mc->state = mc_hw;
+#endif
+}
+EXPORT_SYMBOL(qm_mc_commit);
+
+struct qm_mc_result *qm_mc_result(struct qm_portal *portal)
+{
+	struct qm_mc_result *rr = mc->rr + mc->rridx;
+	QM_ASSERT(mc->state == mc_hw);
+	/* The inactive response register's verb byte always returns zero until
+	 * its command is submitted and completed. This includes the valid-bit,
+	 * in case you were wondering... */
+	if (!rr->verb) {
+		dcbi(rr);
+		dcbt_ro(rr);
+		return NULL;
+	}
+	mc->rridx ^= 1;
+	mc->vbit ^= QM_MCC_VERB_VBIT;
+#ifdef CONFIG_FSL_QMAN_CHECKING
+	mc->state = mc_idle;
+#endif
+	return rr;
+}
+EXPORT_SYMBOL(qm_mc_result);
+
+
+/* ------------------------------------- */
+/* --- Portal interrupt register API --- */
+
+int qm_isr_init(struct qm_portal *portal)
+{
+	if (__qm_portal_bind(portal, QM_BIND_ISR))
+		return -EBUSY;
+	return 0;
+}
+EXPORT_SYMBOL(qm_isr_init);
+
+void qm_isr_finish(struct qm_portal *portal)
+{
+	__qm_portal_unbind(portal, QM_BIND_ISR);
+}
+EXPORT_SYMBOL(qm_isr_finish);
+
+u32 __qm_isr_read(struct qm_portal *portal, enum qm_isr_reg n)
+{
+	return __qm_in(&portal->addr, REG_ISR + (n << 2));
+}
+EXPORT_SYMBOL(__qm_isr_read);
+
+void __qm_isr_write(struct qm_portal *portal, enum qm_isr_reg n, u32 val)
+{
+	__qm_out(&portal->addr, REG_ISR + (n << 2), val);
+}
+EXPORT_SYMBOL(__qm_isr_write);
+
diff --git a/drivers/hwqueue/qman_private.h b/drivers/hwqueue/qman_private.h
new file mode 100644
index 0000000..6a7d277
--- /dev/null
+++ b/drivers/hwqueue/qman_private.h
@@ -0,0 +1,149 @@
+/* Copyright (c) 2008, 2009 Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "qman_sys.h"
+#include <linux/fsl_qman.h>
+
+struct qm_addr {
+	void __iomem *addr_ce;	/* cache-enabled */
+	void __iomem *addr_ci;	/* cache-inhibited */
+};
+
+/* EQCR state */
+struct qm_eqcr {
+	struct qm_eqcr_entry *ring, *cursor;
+	u8 ci, available, ithresh, vbit;
+#ifdef CONFIG_FSL_QMAN_CHECKING
+	u32 busy;
+	enum qm_eqcr_pmode pmode;
+	enum qm_eqcr_cmode cmode;
+#endif
+};
+
+/* DQRR state */
+struct qm_dqrr {
+	struct qm_dqrr_entry *ring, *cursor;
+	u8 pi, ci, fill, ithresh, vbit, flags;
+#ifdef CONFIG_FSL_QMAN_CHECKING
+	enum qm_dqrr_dmode dmode;
+	enum qm_dqrr_pmode pmode;
+	enum qm_dqrr_cmode cmode;
+#endif
+};
+#define QM_DQRR_FLAG_RE 0x01 /* Stash ring entries */
+#define QM_DQRR_FLAG_SE 0x02 /* Stash data */
+
+/* MR state */
+struct qm_mr {
+	struct qm_mr_entry *ring, *cursor;
+	u8 pi, ci, fill, ithresh, vbit;
+#ifdef CONFIG_FSL_QMAN_CHECKING
+	enum qm_mr_pmode pmode;
+	enum qm_mr_cmode cmode;
+#endif
+};
+
+/* MC state */
+struct qm_mc {
+	struct qm_mc_command *cr;
+	struct qm_mc_result *rr;
+	u8 rridx, vbit;
+#ifdef CONFIG_FSL_QMAN_CHECKING
+	enum {
+		/* Can be _mc_start()ed */
+		mc_idle,
+		/* Can be _mc_commit()ed or _mc_abort()ed */
+		mc_user,
+		/* Can only be _mc_retry()ed */
+		mc_hw
+	} state;
+#endif
+};
+
+/********************/
+/* Portal structure */
+/********************/
+
+struct qm_portal {
+	/* In the non-CONFIG_FSL_QMAN_CHECKING case, everything up to and
+	 * including 'mc' fits in a cacheline (yay!). The 'config' part is
+	 * setup-only, so isn't a cause for a concern. In other words, don't
+	 * rearrange this structure on a whim, there be dragons ... */
+	struct qm_addr addr;
+	struct qm_eqcr eqcr;
+	struct qm_dqrr dqrr;
+	struct qm_mr mr;
+	struct qm_mc mc;
+	struct qm_portal_config config;
+	/* Logical index (not cell-index) */
+	int index;
+} ____cacheline_aligned;
+
+/* EQCR/DQRR/[...] code uses this as a locked mechanism to bind/unbind to
+ * qm_portal::bound. */
+int __qm_portal_bind(struct qm_portal *portal, u8 iface);
+void __qm_portal_unbind(struct qm_portal *portal, u8 iface);
+
+/* Hooks for driver initialisation */
+__init int __fqalloc_init(void);
+
+/* Hooks between qman_driver.c and qman_high.c */
+extern DEFINE_PER_CPU(struct qman_portal *, qman_affine_portal);
+static inline struct qman_portal *get_affine_portal(void)
+{
+	return get_cpu_var(qman_affine_portal);
+}
+static inline void put_affine_portal(void)
+{
+	put_cpu_var(qman_affine_portal);
+}
+#define QMAN_PORTAL_FLAG_IRQ         0x00000001 /* use interrupt handler */
+#define QMAN_PORTAL_FLAG_IRQ_FAST    0x00000002 /* ... for fast-path too! */
+#define QMAN_PORTAL_FLAG_DCA         0x00000004 /* use DCA */
+#define QMAN_PORTAL_FLAG_LOCKED      0x00000008 /* multi-core locking */
+#define QMAN_PORTAL_FLAG_NOTAFFINE   0x00000010 /* not cpu-default portal */
+#define QMAN_PORTAL_FLAG_RSTASH      0x00000020 /* enable DQRR entry stashing */
+#define QMAN_PORTAL_FLAG_DSTASH      0x00000040 /* enable data stashing */
+#define QMAN_PORTAL_FLAG_RECOVER     0x00000080 /* recovery mode */
+#define QMAN_PORTAL_FLAG_WAIT        0x00000100 /* for recovery; can wait */
+#define QMAN_PORTAL_FLAG_WAIT_INT    0x00000200 /* for wait; interruptible */
+struct qman_portal *qman_create_portal(struct qm_portal *portal, u32 flags,
+			const struct qman_cgrs *cgrs,
+			const struct qman_fq_cb *null_cb);
+void qman_destroy_portal(struct qman_portal *p);
+void qman_static_dequeue_add_ex(struct qman_portal *p, u32 pools);
+
+/* There are no CGR-related APIs exported so far, but due to the
+ * uninitialised-data ECC issue in rev1.0 Qman, the driver needs to issue "Init
+ * CGR" commands on boot-up. So we're declaring some internal-only APIs to
+ * facilitate this for now. */
+int qman_init_cgr(u32 cgid);
diff --git a/drivers/hwqueue/qman_sys.h b/drivers/hwqueue/qman_sys.h
new file mode 100644
index 0000000..96ef503
--- /dev/null
+++ b/drivers/hwqueue/qman_sys.h
@@ -0,0 +1,99 @@
+/* Copyright (c) 2008, 2009 Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/* this stuff is system-specific but isn't Qman/Bman-specific, so share a single
+ * implementation in Bman's directory (and Qman-ify the stuff that is named
+ * after Bman). */
+#if defined(CONFIG_FSL_QMAN_CHECKING) && !defined(CONFIG_FSL_BMAN_CHECKING)
+#define CONFIG_FSL_BMAN_CHECKING
+#elif !defined(CONFIG_FSL_QMAN_CHECKING) && defined(CONFIG_FSL_BMAN_CHECKING)
+#undef CONFIG_FSL_BMAN_CHECKING
+#endif
+#define QM_ASSERT(x) BM_ASSERT(x)
+#include "../hwalloc/bman_sys.h"
+
+/************/
+/* RB-trees */
+/************/
+
+/* We encapsulate RB-trees so that its easier to use non-linux forms in
+ * non-linux systems. This also encapsulates the extra plumbing that linux code
+ * usually provides when using RB-trees. This encapsulation assumes that the
+ * data type held by the tree is u32. */
+
+struct qman_rbtree {
+	struct rb_root root;
+};
+#define QMAN_RBTREE { .root = RB_ROOT }
+
+#define IMPLEMENT_QMAN_RBTREE(name, type, node_field, val_field) \
+static inline int name##_push(struct qman_rbtree *tree, type *obj) \
+{ \
+	struct rb_node *parent = NULL, **p = &tree->root.rb_node; \
+	int ret = -EBUSY; \
+	while (*p) { \
+		u32 item; \
+		parent = *p; \
+		item = rb_entry(parent, type, node_field)->val_field; \
+		if (obj->val_field < item) \
+			p = &parent->rb_left; \
+		else if (obj->val_field > item) \
+			p = &parent->rb_right; \
+		else \
+			return ret; \
+	} \
+	rb_link_node(&obj->node_field, parent, p); \
+	rb_insert_color(&obj->node_field, &tree->root); \
+	return 0; \
+} \
+static inline void name##_del(struct qman_rbtree *tree, type *obj) \
+{ \
+	rb_erase(&obj->node_field, &tree->root); \
+} \
+static inline type *name##_find(struct qman_rbtree *tree, u32 val) \
+{ \
+	type *ret; \
+	struct rb_node *p = tree->root.rb_node; \
+	while (p) { \
+		ret = rb_entry(p, type, node_field); \
+		if (val < ret->val_field) \
+			p = p->rb_left; \
+		else if (val > ret->val_field) \
+			p = p->rb_right; \
+		else \
+			break; \
+	} \
+	if (!p) \
+		ret = NULL; \
+	return ret; \
+}
+
diff --git a/drivers/hwqueue/qman_test.c b/drivers/hwqueue/qman_test.c
new file mode 100644
index 0000000..bc7e99a
--- /dev/null
+++ b/drivers/hwqueue/qman_test.c
@@ -0,0 +1,84 @@
+/* Copyright (c) 2008, 2009 Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "qman_test.h"
+
+MODULE_AUTHOR("Geoff Thorpe");
+MODULE_LICENSE("Dual BSD/GPL");
+MODULE_DESCRIPTION("Qman testing");
+
+static int test_init(void)
+{
+	int loop;
+#ifdef CONFIG_FSL_QMAN_FQRANGE
+	qman_test_fqrange();
+#endif
+#ifdef CONFIG_FSL_QMAN_TEST_LOW
+	struct qm_portal *portal;
+	const struct qm_portal_config *config = NULL;
+	u8 num = qm_portal_num();
+
+	while (!config && (num-- > 0)) {
+		portal = qm_portal_get(num);
+		config = qm_portal_config(portal);
+		if (!config->bound)
+			pr_info("Portal %d is available, using it\n", num);
+		else
+			config = NULL;
+	}
+	if (!config) {
+		pr_err("No unused Qman portals available!\n");
+		return -ENOSYS;
+	}
+#endif
+	loop = 1;
+	while(loop--) {
+#ifdef CONFIG_FSL_QMAN_TEST_STASH_POTATO
+		qman_test_hotpotato();
+#endif
+#ifdef CONFIG_FSL_QMAN_TEST_LOW
+		qman_test_low(portal);
+#endif
+#ifdef CONFIG_FSL_QMAN_TEST_HIGH
+		qman_test_high();
+#endif
+	}
+	return 0;
+}
+
+static void test_exit(void)
+{
+}
+
+module_init(test_init);
+module_exit(test_exit);
+
diff --git a/drivers/hwqueue/qman_test.h b/drivers/hwqueue/qman_test.h
new file mode 100644
index 0000000..3888583
--- /dev/null
+++ b/drivers/hwqueue/qman_test.h
@@ -0,0 +1,86 @@
+/* Copyright (c) 2008, 2009 Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/io.h>
+#include <linux/slab.h>
+#include <linux/module.h>
+#include <linux/interrupt.h>
+#include <linux/delay.h>
+
+#include <linux/fsl_qman.h>
+
+void qman_test_hotpotato(void);
+void qman_test_low(struct qm_portal *p);
+void qman_test_high(void);
+void qman_test_fqrange(void);
+
+static inline void __hexdump(unsigned long start, unsigned long end,
+			unsigned long p, size_t sz, const unsigned char *c)
+{
+	while (start < end) {
+		unsigned int pos = 0;
+		char buf[64];
+		int nl = 0;
+		pos += sprintf(buf + pos, "%08lx: ", start);
+		do {
+			if ((start < p) || (start >= (p + sz)))
+				pos += sprintf(buf + pos, "..");
+			else
+				pos += sprintf(buf + pos, "%02x", *(c++));
+			if (!(++start & 15)) {
+				buf[pos++] = '\n';
+				nl = 1;
+			} else {
+				nl = 0;
+				if(!(start & 1))
+					buf[pos++] = ' ';
+				if(!(start & 3))
+					buf[pos++] = ' ';
+			}
+		} while (start & 15);
+		if (!nl)
+			buf[pos++] = '\n';
+		buf[pos] = '\0';
+		pr_info("%s", buf);
+	}
+}
+static inline void hexdump(const void *ptr, size_t sz)
+{
+	unsigned long p = (unsigned long)ptr;
+	unsigned long start = p & ~(unsigned long)15;
+	unsigned long end = (p + sz + 15) & ~(unsigned long)15;
+	const unsigned char *c = ptr;
+	__hexdump(start, end, p, sz, c);
+}
+
diff --git a/drivers/hwqueue/qman_test_fqrange.c b/drivers/hwqueue/qman_test_fqrange.c
new file mode 100644
index 0000000..eaf21b2
--- /dev/null
+++ b/drivers/hwqueue/qman_test_fqrange.c
@@ -0,0 +1,65 @@
+/* Copyright (c) 2009 Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "qman_test.h"
+
+void qman_test_fqrange(void)
+{
+	int num;
+	u32 result1, result2, result3;
+
+	pr_info("Testing \"FQRANGE\" allocator ...\n");
+	/* Seed the allocator*/
+	qman_release_fq_range(0, 1000);
+
+	num = qman_alloc_fq_range(&result1, 100, 4, 0);
+	BUG_ON(result1 % 4);
+
+	num = qman_alloc_fq_range(&result2, 500, 500, 0);
+	BUG_ON((num != 500) || (result2 != 500));
+
+	num = qman_alloc_fq_range(&result3, 1000, 0, 0);
+	BUG_ON(num >= 0);
+
+	num = qman_alloc_fq_range(&result3, 1000, 0, 1);
+	BUG_ON(num < 400);
+
+	qman_release_fq_range(result2, 500);
+	qman_release_fq_range(result1, 100);
+	qman_release_fq_range(result3, num);
+
+	/* It should now be possible to drain the allocator empty */
+	num = qman_alloc_fq_range(&result1, 1000, 0, 0);
+	BUG_ON(num != 1000);
+	pr_info("                              ... SUCCESS!\n");
+}
+
diff --git a/drivers/hwqueue/qman_test_high.c b/drivers/hwqueue/qman_test_high.c
new file mode 100644
index 0000000..2427c53
--- /dev/null
+++ b/drivers/hwqueue/qman_test_high.c
@@ -0,0 +1,233 @@
+/* Copyright (c) 2008, 2009 Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "qman_test.h"
+
+/*************/
+/* constants */
+/*************/
+
+#define CGR_ID		27
+#define POOL_ID		12
+#define FQ_FLAGS	QMAN_FQ_FLAG_DYNAMIC_FQID
+#define NUM_ENQUEUES	10
+#define NUM_PARTIAL	4
+#define PORTAL_SDQCR	(QM_SDQCR_SOURCE_CHANNELS | \
+			QM_SDQCR_TYPE_PRIO_QOS | \
+			QM_SDQCR_TOKEN_SET(0x98) | \
+			QM_SDQCR_CHANNELS_DEDICATED | \
+			QM_SDQCR_CHANNELS_POOL(POOL_ID))
+#define PORTAL_OPAQUE	(void *)0xf00dbeef
+#define VDQCR_FLAGS	(QMAN_VOLATILE_FLAG_WAIT | QMAN_VOLATILE_FLAG_FINISH)
+#define PORTAL_FLAGS	QMAN_PORTAL_FLAG_IRQ
+
+/*************************************/
+/* Predeclarations (eg. for fq_base) */
+/*************************************/
+
+static enum qman_cb_dqrr_result cb_dqrr(struct qman_portal *,
+					struct qman_fq *,
+					const struct qm_dqrr_entry *);
+static void cb_ern(struct qman_portal *, struct qman_fq *,
+			const struct qm_mr_entry *);
+static void cb_dc_ern(struct qman_portal *, struct qman_fq *,
+			const struct qm_mr_entry *);
+static void cb_fqs(struct qman_portal *, struct qman_fq *,
+			const struct qm_mr_entry *);
+
+/***************/
+/* global vars */
+/***************/
+
+static struct qm_fd fd, fd_dq;
+static struct qman_fq fq_base = {
+	.cb.dqrr = cb_dqrr,
+	.cb.ern = cb_ern,
+	.cb.dc_ern = cb_dc_ern,
+	.cb.fqs = cb_fqs
+};
+static DECLARE_WAIT_QUEUE_HEAD(waitqueue);
+static int retire_complete, sdqcr_complete;
+
+/**********************/
+/* internal functions */
+/**********************/
+
+/* Helpers for initialising and "incrementing" a frame descriptor */
+static void fd_init(struct qm_fd *__fd)
+{
+	__fd->addr_hi = 0xabba;		/* high 16-bits */
+	__fd->addr_lo = 0xdeadbeef;	/* low 32-bits */
+	__fd->format = qm_fd_contig_big;
+	__fd->length29 = 0x0000ffff;
+	__fd->cmd = 0xfeedf00d;
+}
+
+static void fd_inc(struct qm_fd *__fd)
+{
+	__fd->addr_lo++;
+	__fd->addr_hi--;
+	__fd->length29--;
+	__fd->cmd++;
+}
+
+/* The only part of the 'fd' we can't memcmp() is the ppid */
+static int fd_cmp(const struct qm_fd *a, const struct qm_fd *b)
+{
+	int r = a->addr_hi - b->addr_hi;
+	if (!r)
+		r = a->addr_lo - b->addr_lo;
+	if (!r)
+		r = a->format - b->format;
+	if (!r)
+		r = a->opaque - b->opaque;
+	if (!r)
+		r = a->cmd - b->cmd;
+	return r;
+}
+
+/********/
+/* test */
+/********/
+
+static void do_enqueues(struct qman_fq *fq)
+{
+	unsigned int loop;
+	for (loop = 0; loop < NUM_ENQUEUES; loop++) {
+		pr_info("Enqueue: %08x\n", fd.addr_lo);
+		if (qman_enqueue(fq, &fd, QMAN_ENQUEUE_FLAG_WAIT |
+				(((loop + 1) == NUM_ENQUEUES) ?
+				QMAN_ENQUEUE_FLAG_WAIT_SYNC : 0)))
+			panic("qman_enqueue() failed\n");
+		fd_inc(&fd);
+	}
+}
+
+void qman_test_high(void)
+{
+	struct qman_cgrs cgrs;
+	int flags, res;
+	struct qman_fq *fq = &fq_base;
+
+	pr_info("qman_test_high starting\n");
+	fd_init(&fd);
+	fd_init(&fd_dq);
+	qman_cgrs_init(&cgrs);
+	qman_cgrs_set(&cgrs, CGR_ID);
+
+	pr_info("high-level test, start ccmode\n");
+
+	/* Initialise (parked) FQ */
+	if (qman_create_fq(0, FQ_FLAGS, fq))
+		panic("qman_create_fq() failed\n");
+	if (qman_init_fq(fq, QMAN_INITFQ_FLAG_LOCAL, NULL))
+		panic("qman_init_fq() failed\n");
+
+#ifndef CONFIG_FSL_QMAN_TEST_HIGH_BUG_VDQCR
+	/* Do enqueues + VDQCR, twice. (Parked FQ) */
+	do_enqueues(fq);
+	pr_info("VDQCR (till-empty);\n");
+	if (qman_volatile_dequeue(fq, VDQCR_FLAGS,
+			QM_VDQCR_NUMFRAMES_TILLEMPTY))
+		panic("qman_volatile_dequeue() failed\n");
+	do_enqueues(fq);
+	pr_info("VDQCR (%d of %d);\n", NUM_PARTIAL, NUM_ENQUEUES);
+	if (qman_volatile_dequeue(fq, VDQCR_FLAGS,
+			QM_VDQCR_NUMFRAMES_SET(NUM_PARTIAL)))
+		panic("qman_volatile_dequeue() failed\n");
+	pr_info("VDQCR (%d of %d);\n", NUM_ENQUEUES - NUM_PARTIAL,
+					NUM_ENQUEUES);
+	if (qman_volatile_dequeue(fq, VDQCR_FLAGS,
+			QM_VDQCR_NUMFRAMES_SET(NUM_ENQUEUES - NUM_PARTIAL)))
+		panic("qman_volatile_dequeue() failed\n");
+#endif
+
+	do_enqueues(fq);
+	pr_info("scheduled dequeue (till-empty)\n");
+	if (qman_schedule_fq(fq))
+		panic("qman_schedule_fq() failed\n");
+	wait_event(waitqueue, sdqcr_complete);
+
+	/* Retire and OOS the FQ */
+	res = qman_retire_fq(fq, &flags);
+	if (res < 0)
+		panic("qman_retire_fq() failed\n");
+	wait_event(waitqueue, retire_complete);
+	if (flags & QMAN_FQ_STATE_BLOCKOOS)
+		panic("leaking frames\n");
+	if (qman_oos_fq(fq))
+		panic("qman_oos_fq() failed\n");
+	qman_destroy_fq(fq, 0);
+	pr_info("qman_test_high finished\n");
+}
+
+static enum qman_cb_dqrr_result cb_dqrr(struct qman_portal *p,
+					struct qman_fq *fq,
+					const struct qm_dqrr_entry *dq)
+{
+	pr_info("Dequeue: %08x (%s)\n", dq->fd.addr_lo,
+		(dq->stat & QM_DQRR_STAT_UNSCHEDULED) ? "VDQCR" : "SDQCR");
+	if (fd_cmp(&fd_dq, &dq->fd)) {
+		pr_err("BADNESS: dequeued frame doesn't match;\n");
+		BUG();
+	}
+	fd_inc(&fd_dq);
+	if (!(dq->stat & QM_DQRR_STAT_UNSCHEDULED) && !fd_cmp(&fd_dq, &fd)) {
+		sdqcr_complete = 1;
+		wake_up(&waitqueue);
+	}
+	return qman_cb_dqrr_consume;
+}
+
+static void cb_ern(struct qman_portal *p, struct qman_fq *fq,
+			const struct qm_mr_entry *msg)
+{
+	panic("cb_ern() unimplemented");
+}
+
+static void cb_dc_ern(struct qman_portal *p, struct qman_fq *fq,
+			const struct qm_mr_entry *msg)
+{
+	panic("cb_dc_ern() unimplemented");
+}
+
+static void cb_fqs(struct qman_portal *p, struct qman_fq *fq,
+			const struct qm_mr_entry *msg)
+{
+	u8 verb = (msg->verb & QM_MR_VERB_TYPE_MASK);
+	if ((verb != QM_MR_VERB_FQRN) && (verb != QM_MR_VERB_FQRNI))
+		panic("unexpected FQS message");
+	pr_info("Retirement message received\n");
+	retire_complete = 1;
+	wake_up(&waitqueue);
+}
+
diff --git a/drivers/hwqueue/qman_test_hotpotato.c b/drivers/hwqueue/qman_test_hotpotato.c
new file mode 100644
index 0000000..28b98bc
--- /dev/null
+++ b/drivers/hwqueue/qman_test_hotpotato.c
@@ -0,0 +1,449 @@
+/* Copyright (c) 2009, Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <linux/kthread.h>
+#include <linux/platform_device.h>
+#include <linux/dma-mapping.h>
+#include "qman_test.h"
+
+/* Algorithm:
+ *
+ * Each cpu will have HP_PER_CPU "handlers" set up, each of which incorporates
+ * an rx/tx pair of FQ objects (both of which are stashed on dequeue). The
+ * organisation of FQIDs is such that the HP_PER_CPU*NUM_CPUS handlers will
+ * shuttle a "hot potato" frame around them such that every forwarding action
+ * moves it from one cpu to another. (The use of more than one handler per cpu
+ * is to allow enough handlers/FQs to truly test the significance of cachine -
+ * ie. when cache-expiries are occuring.)
+ *
+ * The "hot potato" frame content will be HP_NUM_WORDS*4 bytes in size, and the
+ * first and last words of the frame data will undergo a transformation step on
+ * each forwarding action. To achieve this, each handler will be assigned a
+ * 32-bit "mixer", that is produced using a 32-bit LFSR. When a frame is
+ * received by a handler, the mixer of the expected sender is XOR'd into all
+ * words of the entire frame, which is then validated against the original
+ * values. Then, before forwarding, the entire frame is XOR'd with the mixer of
+ * the current handler. Apart from validating that the frame is taking the
+ * expected path, this also provides some quasi-realistic overheads to each
+ * forwarding action - dereferencing *all* the frame data, computation, and
+ * conditional branching. There is a "special" handler designated to act as the
+ * instigator of the test by creating an enqueuing the "hot potato" frame, and
+ * to determine when the test has completed by counting HP_LOOPS iterations.
+ *
+ * Init phases:
+ *
+ * 1. prepare each cpu's 'hp_cpu' struct using on_each_cpu(,,1) and link them
+ *    into 'hp_cpu_list'. Specifically, set processor_id, allocate HP_PER_CPU
+ *    handlers and link-list them (but do no other handler setup).
+ *
+ * 2. scan over 'hp_list' HP_PER_CPU times, the first time sets each hp_cpu's
+ *    'iterator' to point to its first handler. With each loop, allocate rx/tx
+ *    FQIDs and mixer values to the hp_cpu's iterator handler and advance the
+ *    iterator for the next loop. This includes a final fixup, which connects
+ *    the last handler to the first (and which is why phase 2 and 3 are
+ *    separate).
+ *
+ * 3. scan over 'hp_list' HP_PER_CPU times, the first time sets each hp_cpu's
+ *    'iterator' to point to its first handler. With each loop, initialise FQ
+ *    objects and advance the iterator for the next loop. Moreover, do this
+ *    initialisation on the cpu it applies to so that Rx FQ initialisation
+ *    targets the correct cpu.
+ */
+
+struct hp_handler {
+
+	/* The following data is stashed when 'rx' is dequeued; */
+	/* -------------- */
+	/* The Rx FQ, dequeues of which will stash the entire hp_handler */
+	struct qman_fq rx;
+	/* The Tx FQ we should forward to */
+	struct qman_fq tx;
+	/* The value we XOR post-dequeue, prior to validating */
+	u32 rx_mixer;
+	/* The value we XOR pre-enqueue, after validating */
+	u32 tx_mixer;
+	/* what the hotpotato address should be on dequeue */
+	dma_addr_t addr;
+	u32 *frame_ptr;
+
+	/* The following data isn't (necessarily) stashed on dequeue; */
+	/* -------------- */
+	u32 fqid_rx, fqid_tx;
+	/* list node for linking us into 'hp_cpu' */
+	struct list_head node;
+	/* Just to check ... */
+	unsigned int processor_id;
+} ____cacheline_aligned;
+
+struct hp_cpu {
+	/* identify the cpu we run on; */
+	unsigned int processor_id;
+	/* root node for the per-cpu list of handlers */
+	struct list_head handlers;
+	/* list node for linking us into 'hp_cpu_list' */
+	struct list_head node;
+	/* when repeatedly scanning 'hp_list', each time linking the n'th
+	 * handlers together, this is used as per-cpu iterator state */
+	struct hp_handler *iterator;
+};
+
+/* Each cpu has one of these */
+static DEFINE_PER_CPU(struct hp_cpu, hp_cpu);
+
+/* links together the hp_cpu structs, in first-come first-serve order. */
+static LIST_HEAD(hp_cpu_list);
+static spinlock_t hp_lock = SPIN_LOCK_UNLOCKED;
+static unsigned int hp_cpu_list_length;
+
+/* the "special" handler, that starts and terminates the test. */
+static struct hp_handler *special_handler;
+static int loop_counter;
+
+/* handlers are allocated out of this, so they're properly aligned. */
+static struct kmem_cache *hp_handler_slab;
+
+/* this is the frame data */
+static void *__frame_ptr;
+static u32 *frame_ptr;
+static dma_addr_t frame_dma;
+
+/* the main function waits on this */
+static DECLARE_WAIT_QUEUE_HEAD(queue);
+
+#define HP_PER_CPU 	16
+#define HP_LOOPS	8
+/* 80 bytes, like a small ethernet frame, and bleeds into a second cacheline */
+#define HP_NUM_WORDS	80
+/* First word of the LFSR-based frame data */
+#define HP_FIRST_WORD	0xabbaf00d
+
+static inline u32 do_lfsr(u32 prev)
+{
+	return (prev >> 1) ^ (-(prev & 1u) & 0xd0000001u);
+}
+
+static void allocate_frame_data(void)
+{
+	u32 lfsr = HP_FIRST_WORD;
+	int loop;
+	struct platform_device *pdev = platform_device_alloc("foobar", -1);
+	if (!pdev)
+		panic("platform_device_alloc() failed");
+	if (platform_device_add(pdev))
+		panic("platform_device_add() failed");
+	__frame_ptr = kmalloc(4 * HP_NUM_WORDS, GFP_KERNEL);
+	if (!__frame_ptr)
+		panic("kmalloc() failed");
+	frame_ptr = (void *)(((unsigned long)__frame_ptr + 63) &
+				~(unsigned long)63);
+	for (loop = 0; loop < HP_NUM_WORDS; loop++) {
+		frame_ptr[loop] = lfsr;
+		lfsr = do_lfsr(lfsr);
+	}
+	frame_dma = dma_map_single(&pdev->dev, frame_ptr, 4 * HP_NUM_WORDS,
+					DMA_BIDIRECTIONAL);
+	platform_device_del(pdev);
+	platform_device_put(pdev);
+}
+
+static void deallocate_frame_data(void)
+{
+	kfree(__frame_ptr);
+}
+
+static inline void process_frame_data(struct hp_handler *handler,
+				const struct qm_fd *fd)
+{
+	u32 *p = handler->frame_ptr;
+	u32 lfsr = HP_FIRST_WORD;
+	int loop;
+	if (fd->addr_lo != (u32)handler->addr)
+		panic("bad frame address");
+	for (loop = 0; loop < HP_NUM_WORDS; loop++, p++) {
+		*p ^= handler->rx_mixer;
+		if (*p != lfsr)
+			panic("corrupt frame data");
+		*p ^= handler->tx_mixer;
+		lfsr = do_lfsr(lfsr);
+	}
+}
+
+static enum qman_cb_dqrr_result normal_dqrr(struct qman_portal *portal,
+					struct qman_fq *fq,
+					const struct qm_dqrr_entry *dqrr)
+{
+	struct hp_handler *handler = (struct hp_handler *)fq;
+
+	process_frame_data(handler, &dqrr->fd);
+	if (qman_enqueue(&handler->tx, &dqrr->fd, 0))
+		panic("qman_enqueue() failed");
+	return qman_cb_dqrr_consume;
+}
+
+static enum qman_cb_dqrr_result special_dqrr(struct qman_portal *portal,
+					struct qman_fq *fq,
+					const struct qm_dqrr_entry *dqrr)
+{
+	struct hp_handler *handler = (struct hp_handler *)fq;
+
+	process_frame_data(handler, &dqrr->fd);
+	if (++loop_counter < HP_LOOPS) {
+		if (qman_enqueue(&handler->tx, &dqrr->fd, 0))
+			panic("qman_enqueue() failed");
+	} else {
+		pr_info("Received final (%dth) frame\n", loop_counter);
+		wake_up(&queue);
+	}
+	return qman_cb_dqrr_consume;
+}
+
+static void create_per_cpu_handlers(void *ignore)
+{
+	struct hp_handler *handler;
+	struct hp_cpu *hp_cpu = &get_cpu_var(hp_cpu);
+	int loop;
+
+	hp_cpu->processor_id = smp_processor_id();
+	spin_lock(&hp_lock);
+	list_add_tail(&hp_cpu->node, &hp_cpu_list);
+	hp_cpu_list_length++;
+	spin_unlock(&hp_lock);
+	INIT_LIST_HEAD(&hp_cpu->handlers);
+	for (loop = 0; loop < HP_PER_CPU; loop++) {
+		handler = kmem_cache_alloc(hp_handler_slab, GFP_KERNEL);
+		if (!handler)
+			panic("kmem_cache_alloc() failed");
+		handler->processor_id = hp_cpu->processor_id;
+		handler->addr = frame_dma;
+		handler->frame_ptr = frame_ptr;
+		list_add_tail(&handler->node, &hp_cpu->handlers);
+	}
+	put_cpu_var(hp_cpu);
+}
+
+static void destroy_per_cpu_handlers(void *ignore)
+{
+	struct list_head *loop, *tmp;
+	struct hp_cpu *hp_cpu = &get_cpu_var(hp_cpu);
+
+	spin_lock(&hp_lock);
+	list_del(&hp_cpu->node);
+	spin_unlock(&hp_lock);
+	list_for_each_safe(loop, tmp, &hp_cpu->handlers) {
+		u32 flags;
+		struct hp_handler *handler = list_entry(loop, struct hp_handler,
+							node);
+		if (qman_retire_fq(&handler->rx, &flags))
+			panic("qman_retire_fq(rx) failed");
+		BUG_ON(flags & QMAN_FQ_STATE_BLOCKOOS);
+		if (qman_oos_fq(&handler->rx))
+			panic("qman_oos_fq(rx) failed");
+		qman_destroy_fq(&handler->rx, 0);
+		qman_destroy_fq(&handler->tx, 0);
+		list_del(&handler->node);
+		kmem_cache_free(hp_handler_slab, handler);
+	}
+	put_cpu_var(hp_cpu);
+}
+
+static inline u8 num_cachelines(u32 offset)
+{
+	u8 res = (offset + 63) / 64;
+	if (res > 3)
+		return 3;
+	return res;
+}
+#define STASH_DATA_CL \
+	num_cachelines(HP_NUM_WORDS * 4)
+#define STASH_CTX_CL \
+	num_cachelines(offsetof(struct hp_handler,fqid_rx))
+
+static void init_handler(void *__handler)
+{
+	struct qm_mcc_initfq opts;
+	struct hp_handler *handler = __handler;
+	BUG_ON(handler->processor_id != smp_processor_id());
+	/* Set up rx */
+	memset(&handler->rx, 0, sizeof(handler->rx));
+	if (handler == special_handler)
+		handler->rx.cb.dqrr = special_dqrr;
+	else
+		handler->rx.cb.dqrr = normal_dqrr;
+	if (qman_create_fq(handler->fqid_rx, 0, &handler->rx))
+		panic("qman_create_fq(rx) failed");
+	memset(&opts, 0, sizeof(opts));
+	opts.we_mask = QM_INITFQ_WE_FQCTRL | QM_INITFQ_WE_CONTEXTA;
+	opts.fqd.fq_ctrl = QM_FQCTRL_CTXASTASHING;
+	opts.fqd.context_a.stashing.data_cl = STASH_DATA_CL;
+	opts.fqd.context_a.stashing.context_cl = STASH_CTX_CL;
+	if (qman_init_fq(&handler->rx, QMAN_INITFQ_FLAG_SCHED |
+				QMAN_INITFQ_FLAG_LOCAL, &opts))
+		panic("qman_init_fq(rx) failed");
+	/* Set up tx */
+	memset(&handler->tx, 0, sizeof(handler->tx));
+	if (qman_create_fq(handler->fqid_tx, QMAN_FQ_FLAG_NO_MODIFY,
+				&handler->tx))
+		panic("qman_create_fq(tx) failed");
+}
+
+static void init_phase2(void)
+{
+	int loop;
+	u32 fqid = 0;
+	u32 lfsr = 0xdeadbeef;
+	struct hp_cpu *hp_cpu;
+	struct hp_handler *handler;
+
+	for (loop = 0; loop < HP_PER_CPU; loop++) {
+		list_for_each_entry(hp_cpu, &hp_cpu_list, node) {
+			if (!loop)
+				hp_cpu->iterator = list_first_entry(
+						&hp_cpu->handlers,
+						struct hp_handler, node);
+			else
+				hp_cpu->iterator = list_entry(
+						hp_cpu->iterator->node.next,
+						struct hp_handler, node);
+			/* Rx FQID is the previous handler's Tx FQID */
+			hp_cpu->iterator->fqid_rx = fqid;
+			/* Allocate new FQID for Tx */
+			fqid = qm_fq_new();
+			if (!fqid)
+				panic("qm_fq_new() failed");
+			hp_cpu->iterator->fqid_tx = fqid;
+			/* Rx mixer is the previous handler's Tx mixer */
+			hp_cpu->iterator->rx_mixer = lfsr;
+			/* Get new mixer for Tx */
+			lfsr = do_lfsr(lfsr);
+			hp_cpu->iterator->tx_mixer = lfsr;
+		}
+	}
+	/* Fix up the first handler (fqid_rx==0, rx_mixer=0xdeadbeef) */
+	hp_cpu = list_first_entry(&hp_cpu_list, struct hp_cpu, node);
+	handler = list_first_entry(&hp_cpu->handlers, struct hp_handler, node);
+	BUG_ON((handler->fqid_rx != 0) || (handler->rx_mixer != 0xdeadbeef));
+	handler->fqid_rx = fqid;
+	handler->rx_mixer = lfsr;
+	/* and tag it as our "special" handler */
+	special_handler = handler;
+}
+
+static void init_phase3(void)
+{
+	int loop;
+	struct hp_cpu *hp_cpu;
+
+	for (loop = 0; loop < HP_PER_CPU; loop++) {
+		list_for_each_entry(hp_cpu, &hp_cpu_list, node) {
+			if (!loop)
+				hp_cpu->iterator = list_first_entry(
+						&hp_cpu->handlers,
+						struct hp_handler, node);
+			else
+				hp_cpu->iterator = list_entry(
+						hp_cpu->iterator->node.next,
+						struct hp_handler, node);
+			preempt_disable();
+			if (hp_cpu->processor_id == smp_processor_id())
+				init_handler(hp_cpu->iterator);
+			else
+				smp_call_function_single(hp_cpu->processor_id,
+					init_handler, hp_cpu->iterator, 1);
+			preempt_enable();
+		}
+	}
+}
+
+static void send_first_frame(void *ignore)
+{
+	u32 *p = special_handler->frame_ptr;
+	u32 lfsr = HP_FIRST_WORD;
+	int loop;
+	struct qm_fd fd;
+
+	BUG_ON(special_handler->processor_id != smp_processor_id());
+	memset(&fd, 0, sizeof(fd));
+	fd.addr_lo = (u32)special_handler->addr;
+	fd.format = qm_fd_contig_big;
+	fd.length29 = HP_NUM_WORDS * 4;
+	for (loop = 0; loop < HP_NUM_WORDS; loop++, p++) {
+		if (*p != lfsr)
+			panic("corrupt frame data");
+		*p ^= special_handler->tx_mixer;
+		lfsr = do_lfsr(lfsr);
+	}
+	pr_info("Sending first frame\n");
+	if (qman_enqueue(&special_handler->tx, &fd, 0))
+		panic("qman_enqueue() failed");
+}
+
+void qman_test_hotpotato(void)
+{
+	pr_info("qman_test_hotpotato starting\n");
+
+	hp_cpu_list_length = 0;
+	loop_counter = 0;
+	hp_handler_slab = kmem_cache_create("hp_handler_slab",
+			sizeof(struct hp_handler), 64,
+			SLAB_HWCACHE_ALIGN, NULL);
+	if (!hp_handler_slab)
+		panic("kmem_cache_create() failed");
+
+	allocate_frame_data();
+
+	/* Init phase 1 */
+	pr_info("Creating %d handlers per cpu...\n", HP_PER_CPU);
+	if (on_each_cpu(create_per_cpu_handlers, NULL, 1))
+		panic("on_each_cpu() failed");
+	pr_info("Number of cpus: %d, total of %d handlers\n",
+		hp_cpu_list_length, hp_cpu_list_length * HP_PER_CPU);
+
+	init_phase2();
+
+	init_phase3();
+
+	preempt_disable();
+	if (special_handler->processor_id == smp_processor_id())
+		send_first_frame(NULL);
+	else
+		smp_call_function_single(special_handler->processor_id,
+			send_first_frame, NULL, 1);
+	preempt_enable();
+
+	wait_event(queue, loop_counter == HP_LOOPS);
+	deallocate_frame_data();
+	if (on_each_cpu(destroy_per_cpu_handlers, NULL, 1))
+		panic("on_each_cpu() failed");
+	kmem_cache_destroy(hp_handler_slab);
+	pr_info("qman_test_hotpotato finished\n");
+}
+
diff --git a/drivers/hwqueue/qman_test_low.c b/drivers/hwqueue/qman_test_low.c
new file mode 100644
index 0000000..e5d718d
--- /dev/null
+++ b/drivers/hwqueue/qman_test_low.c
@@ -0,0 +1,480 @@
+/* Copyright (c) 2008, 2009 Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "qman_test.h"
+
+/* Bug or missing-feature workarounds */
+#define BUG_NO_EQCR_ITR
+#define BUG_NO_REASSERT_DQAVAIL
+
+/* DQRR maxfill, and ring/data stashing booleans */
+#define DQRR_MAXFILL	15
+
+/* Test constants */
+#define TWQ		3	/* 0..7, ie. within the channel */
+#define TCONTEXTB	0xabcd9876
+#define TFRAMES		3	/* Number enqueued */
+#define TEQVERB		QM_EQCR_VERB_CMD_ENQUEUE
+#define TTOKEN		0xab
+
+/* Global variables make life simpler */
+static struct qm_portal *portal;
+static struct qm_eqcr_entry *eq;
+static struct qm_dqrr_entry *dq;
+static struct qm_mr_entry *mr;
+static struct qm_mc_command *mcc;
+static struct qm_mc_result *mcr;
+static DECLARE_WAIT_QUEUE_HEAD(queue);
+static int isr_count;
+
+#ifdef CONFIG_FSL_QMAN_FQALLOCATOR
+
+/* Use Bman for dynamic FQ allocation (BPID==0) */
+#include <linux/fsl_bman.h>
+static struct bman_pool *fq_pool;
+static const struct bman_pool_params fq_pool_params;
+static u32 fqid;
+static void alloc_fqid(void)
+{
+	struct bm_buffer buf;
+	int ret;
+	fq_pool = bman_new_pool(&fq_pool_params);
+	BUG_ON(!fq_pool);
+	ret = bman_acquire(fq_pool, &buf, 1, 0);
+	BUG_ON(ret != 1);
+	fqid = buf.lo;
+}
+static void free_fqid(void)
+{
+	struct bm_buffer buf = {
+		.hi = 0,
+		.lo = fqid
+	};
+	int ret = bman_release(fq_pool, &buf, 1, BMAN_RELEASE_FLAG_WAIT);
+	BUG_ON(ret);
+	bman_free_pool(fq_pool);
+}
+
+#else /* !defined(CONFIG_FSL_QMAN_FQALLOCATOR) */
+
+/* No dynamic allocation, use FQID==1 */
+static u32 fqid = 1;
+#define alloc_fqid()	0
+#define free_fqid()	do { ; } while(0)
+
+#endif /* !defined(CONFIG_FSL_QMAN_FQALLOCATOR) */
+
+/* Boolean switch for handling EQCR_ITR */
+#ifndef BUG_NO_EQCR_ITR
+static int eqcr_thresh_on;
+#endif
+
+/* Test frame-descriptor, and another one to track dequeues */
+static struct qm_fd fd, fd_dq;
+
+/* Helpers for initialising and "incrementing" a frame descriptor */
+static void fd_init(struct qm_fd *__fd)
+{
+	__fd->addr_hi = 0xabba;		/* high 16-bits */
+	__fd->addr_lo = 0xdeadbeef;	/* low 32-bits */
+	__fd->format = qm_fd_contig_big;
+	__fd->length29 = 0x0000ffff;
+	__fd->cmd = 0xfeedf00d;
+}
+
+static void fd_inc(struct qm_fd *__fd)
+{
+	__fd->addr_lo++;
+	__fd->addr_hi--;
+	__fd->length29--;
+	__fd->cmd++;
+}
+
+/* Helper for qm_mc_start() that checks the return code */
+static void mc_start(void)
+{
+	mcc = qm_mc_start(portal);
+	BUG_ON(!mcc);
+}
+
+/* Helper for qm_mc_result() that checks the response */
+#define mc_commit(v) \
+do { \
+	qm_mc_commit(portal, v); \
+	do { \
+		mcr = qm_mc_result(portal); \
+	} while (!mcr); \
+	BUG_ON((mcr->verb & QM_MCR_VERB_MASK) != v); \
+	BUG_ON(mcr->result != QM_MCR_RESULT_OK); \
+} while(0)
+
+/* Track EQCR consumption */
+static void eqcr_update(void)
+{
+#ifndef BUG_NO_EQCR_ITR
+	u32 status = qm_isr_status_read(portal);
+	if (status & QM_PIRQ_EQRI) {
+		int progress = qm_eqcr_cci_update(portal);
+		BUG_ON(!progress);
+		BUG_ON(!eqcr_thresh_on);
+		qm_eqcr_set_ithresh(portal, 0);
+		eqcr_thresh_on = 0;
+		pr_info("Auto-update of EQCR consumption\n");
+		qm_isr_status_clear(portal, progress & QM_PIRQ_EQRI);
+	}
+#else
+	qm_eqcr_cci_update(portal);
+#endif
+}
+
+/* Helper for qm_eqcr_start() that tracks ring consumption and checks the
+ * return code */
+static void eqcr_start(void)
+{
+	/* If there are consumed EQCR entries, track them now. The alternative
+	 * is to catch an error in qm_eqcr_start(), track consume entries then,
+	 * and then retry qm_eqcr_start(). */
+	do {
+		eqcr_update();
+		eq = qm_eqcr_start(portal);
+	} while (!eq);
+}
+
+/* Wait for EQCR to empty */
+static void eqcr_empty(void)
+{
+	do {
+		eqcr_update();
+	} while (qm_eqcr_get_fill(portal));
+}
+
+/* Helper for qm_eqcr_pvb_commit() */
+static void eqcr_commit(void)
+{
+	qm_eqcr_pvb_commit(portal, TEQVERB);
+#ifndef BUG_NO_EQCR_ITR
+	if (!eqcr_thresh_on && (qm_eqcr_get_avail(portal) < 2)) {
+		eqcr_thresh_on = 1;
+		qm_eqcr_set_ithresh(portal, 1);
+	}
+#endif
+}
+
+/* Track DQRR consumption */
+static void dqrr_update(void)
+{
+	do {
+		qm_dqrr_pvb_update(portal);
+		dq = qm_dqrr_current(portal);
+		if (!dq)
+			qm_dqrr_pvb_prefetch(portal);
+	} while (!dq);
+}
+
+/* Helper for qm_dqrr_cci_consume() */
+static void dqrr_consume_and_next(void)
+{
+	qm_dqrr_next(portal);
+	qm_dqrr_cci_consume_to_current(portal);
+	qm_dqrr_pvb_prefetch(portal);
+}
+
+static void mr_update(void)
+{
+	do {
+		qm_mr_pvb_update(portal);
+		mr = qm_mr_current(portal);
+	} while (!mr);
+}
+
+static void mr_consume_and_next(void)
+{
+	qm_mr_next(portal);
+	qm_mr_cci_consume_to_current(portal);
+}
+
+#ifndef CONFIG_FSL_QMAN_TEST_LOW_BUG_INTERRUPTS
+static irqreturn_t portal_isr(int irq, void *ptr)
+{
+	pr_info("QMAN portal interrupt, isr_count=%d->%d\n", isr_count,
+		isr_count + 1);
+	isr_count++;
+	qm_isr_inhibit(portal);
+	wake_up(&queue);
+	return IRQ_HANDLED;
+}
+#endif
+
+void qman_test_low(struct qm_portal *__p)
+{
+	cpumask_t oldmask = current->cpus_allowed, newmask = CPU_MASK_NONE;
+	const struct qm_portal_config *config = qm_portal_config(__p);
+	int i;
+#ifndef CONFIG_FSL_QMAN_TEST_LOW_BUG_INTERRUPTS
+	u32 status;
+#endif
+
+	portal = __p;
+	fd_init(&fd);
+	fd_init(&fd_dq);
+	isr_count = 0;
+
+	pr_info("qman_test_low starting\n");
+
+	/* If the portal is affine, run on the corresponding cpu */
+	if (config->cpu != -1) {
+		cpu_set(config->cpu, newmask);
+		if (set_cpus_allowed(current, newmask))
+			panic("can't schedule to affine cpu");
+	}
+
+	alloc_fqid();
+	/*********************/
+	/* Initialise portal */
+	/*********************/
+	if (qm_eqcr_init(portal, qm_eqcr_pvb, qm_eqcr_cci) ||
+#ifdef CONFIG_FSL_QMAN_TEST_LOW_BUG_STASHING
+		qm_dqrr_init(portal,
+			qm_dqrr_dpush, qm_dqrr_pvb, qm_dqrr_cci,
+			DQRR_MAXFILL, 0, 0) ||
+#else
+		qm_dqrr_init(portal,
+			qm_dqrr_dpush, qm_dqrr_pvb, qm_dqrr_cci,
+			DQRR_MAXFILL,
+			(config->cpu == -1) ? 0 : 1,
+			(config->cpu == -1) ? 0 : 1) ||
+#endif
+		qm_mr_init(portal, qm_mr_pvb, qm_mr_cci) ||
+		qm_mc_init(portal) || qm_isr_init(portal))
+		panic("Portal setup failed");
+	/* Set interrupt register values (eg. for post reboot) */
+	qm_isr_enable_write(portal, 0);
+	qm_isr_disable_write(portal, 0);
+	qm_isr_uninhibit(portal);
+	qm_isr_status_clear(portal, 0xffffffff);
+
+	pr_info("low-level test, start ccmode\n");
+
+#ifndef CONFIG_FSL_QMAN_TEST_LOW_BUG_INTERRUPTS
+	if (request_irq(config->irq, portal_isr, 0, "Qman portal 0", NULL))
+		panic("Can't register Qman portal 0 IRQ");
+#endif
+	pr_info("Portal %d channel i/faces initialised\n", config->channel);
+
+	/*****************/
+	/* Initialise FQ */
+	/*****************/
+	mc_start();
+	mcc->initfq.we_mask = QM_INITFQ_WE_DESTWQ | QM_INITFQ_WE_CONTEXTB;
+	mcc->initfq.fqid = fqid;
+	mcc->initfq.count = 0;
+	/* mcc->initfq.fqd.fq_ctrl = 0; */ /* (QM_FQCTRL_***) */
+	mcc->initfq.fqd.dest.channel = config->channel;
+	mcc->initfq.fqd.dest.wq = TWQ;
+	mcc->initfq.fqd.context_b = TCONTEXTB;
+	mc_commit(QM_MCC_VERB_INITFQ_PARKED);
+	pr_info("FQ %d initialised for channel %d, wq %d\n", fqid,
+		config->channel, TWQ);
+
+#ifndef CONFIG_FSL_QMAN_TEST_LOW_BUG_INTERRUPTS
+	/* The portal's (interrupt) status register should be zero */
+	status = qm_isr_status_read(portal);
+	BUG_ON(status);
+#endif
+
+	/**************************/
+	/* Enqueue TFRAMES frames */
+	/**************************/
+	for (i = 0; i < TFRAMES; i++) {
+		/* Enqueue the test frame-descriptor */
+		eqcr_start();
+		eq->fqid = fqid;
+		memcpy(&eq->fd, &fd, sizeof(fd));
+		eqcr_commit();
+		pr_info("Enqueued frame %d: %08x\n", i, fd.addr_lo);
+		/* Modify the frame-descriptor for next time */
+		fd_inc(&fd);
+	}
+	eqcr_empty();
+
+#ifndef CONFIG_FSL_QMAN_TEST_LOW_BUG_INTERRUPTS
+	/* The portal's (interrupt) status register should be zero */
+	status = qm_isr_status_read(portal);
+	BUG_ON(status);
+#endif
+
+	/***************/
+	/* Schedule FQ */
+	/***************/
+#ifndef CONFIG_FSL_QMAN_TEST_LOW_BUG_INTERRUPTS
+	/* enable the interrupt source before it asserts */
+	qm_isr_enable_write(portal, QM_DQAVAIL_PORTAL);
+	qm_isr_uninhibit(portal);
+#endif
+	mc_start();
+	mcc->alterfq.fqid = fqid;
+	mc_commit(QM_MCC_VERB_ALTER_SCHED);
+	pr_info("FQ %d scheduled\n", fqid);
+#ifndef CONFIG_FSL_QMAN_TEST_LOW_BUG_INTERRUPTS
+	/* The interrupt should have fired immediately. */
+	wait_event(queue, isr_count == 1);
+	/* The status register should show DQAVAIL. */
+	do {
+		status = qm_isr_status_read(portal);
+	} while (status != QM_DQAVAIL_PORTAL);
+	/* Writing to clear should fail due to an immediate reassertion */
+#ifndef BUG_NO_REASSERT_DQAVAIL
+	qm_isr_status_clear(portal, status);
+	do {
+		status = qm_isr_status_read(portal);
+	} while (status != QM_DQAVAIL_PORTAL);
+#endif
+	/* Zeroing the enable register before unihibiting should prevent any
+	 * interrupt */
+	qm_isr_enable_write(portal, 0);
+	qm_isr_uninhibit(portal);
+	BUG_ON(isr_count != 1);
+#endif
+
+	/******************************/
+	/* SDQCR the remaining frames */
+	/******************************/
+
+	/* Initiate SDQCR (static dequeue command) */
+	qm_dqrr_sdqcr_set(portal, QM_SDQCR_SOURCE_CHANNELS |
+		QM_SDQCR_TYPE_PRIO_QOS | QM_SDQCR_TOKEN_SET(TTOKEN) |
+		QM_SDQCR_CHANNELS_DEDICATED);
+
+#ifndef CONFIG_FSL_QMAN_TEST_LOW_BUG_INTERRUPTS
+	/* The status register should have the sticky 'DQAVAIL' from before, and
+	 * maybe DQRI (it's on its way). */
+	status = qm_isr_status_read(portal);
+	BUG_ON(!(status & QM_DQAVAIL_PORTAL));
+	qm_isr_enable_write(portal, QM_PIRQ_DQRI);
+	wait_event(queue, isr_count == 2);
+	do {
+		status = qm_isr_status_read(portal);
+	} while (!(status & QM_PIRQ_DQRI));
+	/* Writing to clear should fail due to an immediate reassertion. NB
+	 * also, DQAVAIL_PORTAL mightn't disappear right away, the dequeues are
+	 * delayed so the FQ may stay truly-scheduled for a bit... */
+	qm_isr_status_clear(portal, status);
+	do {
+		status = qm_isr_status_read(portal);
+	} while (!(status & QM_PIRQ_DQRI));
+	/* Zeroing the enable register before unihibiting should prevent any
+	 * interrupt */
+	qm_isr_enable_write(portal, 0);
+	qm_isr_uninhibit(portal);
+	BUG_ON(isr_count != 2);
+#endif
+	for (i = 0; i < TFRAMES; i++)
+	{
+		dqrr_update();
+		BUG_ON((dq->verb & QM_DQRR_VERB_MASK) !=
+				QM_DQRR_VERB_FRAME_DEQUEUE);
+		pr_info("Dequeued SDQCR frame %d: %08x\n", i, dq->fd.addr_lo);
+		if ((i + 1) == TFRAMES)
+			BUG_ON(!(dq->stat & QM_DQRR_STAT_FQ_EMPTY));
+		else
+			BUG_ON(dq->stat & QM_DQRR_STAT_FQ_EMPTY);
+		BUG_ON(dq->stat & QM_DQRR_STAT_FQ_HELDACTIVE);
+		BUG_ON(dq->stat & QM_DQRR_STAT_FQ_FORCEELIGIBLE);
+		BUG_ON(!(dq->stat & QM_DQRR_STAT_FD_VALID));
+		BUG_ON(dq->stat & QM_DQRR_STAT_UNSCHEDULED);
+		BUG_ON(dq->tok != TTOKEN);
+		BUG_ON(dq->fqid != fqid);
+		BUG_ON(dq->contextB != TCONTEXTB);
+		fd_dq.pid = dq->fd.pid;
+		BUG_ON(memcmp(&dq->fd, &fd_dq, sizeof(fd_dq)));
+		/* Modify the frame-descriptor for next time */
+		fd_inc(&fd_dq);
+		dqrr_consume_and_next();
+	}
+#ifndef CONFIG_FSL_QMAN_TEST_LOW_BUG_INTERRUPTS
+	/* Clear stick bits from the status register, and it should
+	 * now remain zero. */
+	status = qm_isr_status_read(portal);
+	qm_isr_status_clear(portal, status);
+	status = qm_isr_status_read(portal);
+	BUG_ON(status);
+#endif
+
+	pr_info("low-level test, end ccmode\n");
+
+	/*************/
+	/* Retire FQ */
+	/*************/
+	mc_start();
+	mcc->alterfq.fqid = fqid;
+	mc_commit(QM_MCC_VERB_ALTER_RETIRE);
+	pr_info("FQ %d retired\n", fqid);
+
+	/*****************/
+	/* Consume FQRNI */
+	/*****************/
+	mr_update();
+	BUG_ON((mr->verb & QM_MR_VERB_TYPE_MASK) != QM_MR_VERB_FQRNI);
+	pr_info("Received FQRNI message\n");
+	hexdump(mr, 32);
+	BUG_ON(mr->fq.fqid != fqid);
+	BUG_ON(mr->fq.fqs);
+	BUG_ON(mr->fq.contextB != TCONTEXTB);
+	mr_consume_and_next();
+
+	/**********/
+	/* OOS FQ */
+	/**********/
+	mc_start();
+	mcc->alterfq.fqid = fqid;
+	mc_commit(QM_MCC_VERB_ALTER_OOS);
+	pr_info("FQ %d OOS'd\n", fqid);
+
+	/************/
+	/* Teardown */
+	/************/
+	eqcr_empty();
+#ifndef CONFIG_FSL_QMAN_TEST_LOW_BUG_INTERRUPTS
+	free_irq(config->irq, NULL);
+#endif
+	qm_eqcr_finish(portal);
+	qm_dqrr_finish(portal);
+	qm_mr_finish(portal);
+	qm_mc_finish(portal);
+	qm_isr_finish(portal);
+	free_fqid();
+	if (config->cpu != -1)
+		if (set_cpus_allowed(current, oldmask))
+			panic("can't restore old cpu mask");
+	pr_info("qman_test_low finished\n");
+}
+
diff --git a/drivers/hwqueue/qman_utility.c b/drivers/hwqueue/qman_utility.c
new file mode 100644
index 0000000..d1cf16e
--- /dev/null
+++ b/drivers/hwqueue/qman_utility.c
@@ -0,0 +1,132 @@
+/* Copyright (c) 2008, 2009 Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "qman_private.h"
+
+/* ----------------- */
+/* --- FQID Pool --- */
+
+struct qman_fqid_pool {
+	/* Base and size of the FQID range */
+	u32 fqid_base;
+	u32 total;
+	/* Number of FQIDs currently "allocated" */
+	u32 used;
+	/* Allocation optimisation. When 'used<total', it is the index of an
+	 * available FQID. Otherwise there are no available FQIDs, and this
+	 * will be set when the next deallocation occurs. */
+	u32 next;
+	/* A bit-field representation of the FQID range. */
+	unsigned long *bits;
+};
+
+#define QLONG_BYTES	sizeof(unsigned long)
+#define QLONG_BITS	(QLONG_BYTES * 8)
+/* Number of 'longs' required for the given number of bits */
+#define QNUM_LONGS(b)	(((b) + QLONG_BITS - 1) / QLONG_BITS)
+/* Shorthand for the number of bytes of same (kmalloc, memset, etc) */
+#define QNUM_BYTES(b)	(QNUM_LONGS(b) * QLONG_BYTES)
+/* And in bits */
+#define QNUM_BITS(b)	(QNUM_LONGS(b) * QLONG_BITS)
+
+struct qman_fqid_pool *qman_fqid_pool_create(u32 fqid_start, u32 num)
+{
+	struct qman_fqid_pool *pool = kmalloc(sizeof(*pool), GFP_KERNEL);
+	unsigned int i;
+
+	BUG_ON(!num);
+	if (!pool)
+		return NULL;
+	pool->fqid_base = fqid_start;
+	pool->total = num;
+	pool->used = 0;
+	pool->next = 0;
+	pool->bits = kmalloc(QNUM_BYTES(num), GFP_KERNEL);
+	if (!pool->bits) {
+		kfree(pool);
+		return NULL;
+	}
+	memset(pool->bits, 0, QNUM_BYTES(num));
+	/* If num is not an even multiple of QLONG_BITS (or even 8, for
+	 * byte-oriented searching) then we fill the trailing bits with 1, to
+	 * make them look allocated (permanently). */
+	for (i = num + 1; i < QNUM_BITS(num); i++)
+		set_bit(i, pool->bits);
+	return pool;
+}
+EXPORT_SYMBOL(qman_fqid_pool_create);
+
+int qman_fqid_pool_destroy(struct qman_fqid_pool *pool)
+{
+	int ret = pool->used;
+	kfree(pool->bits);
+	kfree(pool);
+	return ret;
+}
+EXPORT_SYMBOL(qman_fqid_pool_destroy);
+
+int qman_fqid_pool_alloc(struct qman_fqid_pool *pool, u32 *fqid)
+{
+	int ret;
+	if (pool->used == pool->total)
+		return -ENOMEM;
+	*fqid = pool->fqid_base + pool->next;
+	ret = test_and_set_bit(pool->next, pool->bits);
+	BUG_ON(ret);
+	if (++pool->used == pool->total)
+		return 0;
+	pool->next = find_next_zero_bit(pool->bits, pool->total, pool->next);
+	if (pool->next >= pool->total)
+		pool->next = find_first_zero_bit(pool->bits, pool->total);
+	BUG_ON(pool->next >= pool->total);
+	return 0;
+}
+EXPORT_SYMBOL(qman_fqid_pool_alloc);
+
+void qman_fqid_pool_free(struct qman_fqid_pool *pool, u32 fqid)
+{
+	int ret;
+
+	fqid -= pool->fqid_base;
+	ret = test_and_clear_bit(fqid, pool->bits);
+	BUG_ON(!ret);
+	if (pool->used-- == pool->total)
+		pool->next = fqid;
+}
+EXPORT_SYMBOL(qman_fqid_pool_free);
+
+u32 qman_fqid_pool_used(struct qman_fqid_pool *pool)
+{
+	return pool->used;
+}
+EXPORT_SYMBOL(qman_fqid_pool_used);
+
-- 
1.6.3.3

