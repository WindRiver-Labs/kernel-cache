From 05edaa8f865b0acc477af730207c5a835f54470d Mon Sep 17 00:00:00 2001
From: Andrew Liu <shengping.liu@windriver.com>
Date: Wed, 4 Aug 2010 14:57:50 +0800
Subject: [PATCH 11/22] dpa: Move to a tasklet for packet processing

It is from FSL vendor SDK 2.x.

The network rx code is invoked inside softirq context.
If we are not in softirq context, we have to switch to
softirq context before the stack can process each packet.
This is costly. Instead, we switch from using a workqueue
to a tasklet, which runs in softirq context.  This allows
us to call netif_receive_skb() directly.

Also, clean up bman_release flags. Tasklets are not allowed
to sleep, so we can't wait, and for the initial bman_release
calls, we need to add BMAN_RELEASE_FLAG_WAIT so that it
actually *does* wait.

Signed-off-by: Andy Fleming <afleming@freescale.com>
Integrated-by: Andrew Liu <shengping.liu@windriver.com>
---
 drivers/net/dpa/dpa.c |   50 +++++++++++++++++-------------------------------
 drivers/net/dpa/dpa.h |    4 +-
 2 files changed, 20 insertions(+), 34 deletions(-)

diff --git a/drivers/net/dpa/dpa.c b/drivers/net/dpa/dpa.c
index 6520b92..025ffc9 100644
--- a/drivers/net/dpa/dpa.c
+++ b/drivers/net/dpa/dpa.c
@@ -106,11 +106,9 @@ static u64 guest_phy_offset;
 /* BM */
 
 #ifdef DEBUG
-#define GFP_DPA_BP	(GFP_DMA | __GFP_ZERO)
-#define GFP_DPA		(GFP_KERNEL | __GFP_ZERO)
+#define GFP_DPA_BP     (GFP_DMA | __GFP_ZERO | GFP_ATOMIC)
 #else
-#define GFP_DPA_BP	(GFP_DMA)
-#define GFP_DPA		(GFP_KERNEL)
+#define GFP_DPA_BP     (GFP_DMA | GFP_ATOMIC)
 #endif
 
 #define DPA_BP_HEAD	64
@@ -486,8 +484,7 @@ dpa_fd_release(const struct net_device *net_dev, const struct qm_fd *fd)
 					!sgt[i-1].final &&
 					sgt[i-1].bpid == sgt[i].bpid);
 
-			__errno = bman_release(dpa_bp->pool, bmb, j,
-					BMAN_RELEASE_FLAG_WAIT_INT);
+			__errno = bman_release(dpa_bp->pool, bmb, j, 0);
 			if (unlikely(__errno < 0)) {
 				if (netif_msg_drv(priv) && net_ratelimit())
 					cpu_netdev_err(net_dev,	"%s:%hu:%s(): "
@@ -533,7 +530,6 @@ ingress_dqrr(struct qman_portal		*portal,
 	     uint8_t			 _rtx,
 	     uint8_t			 ed)
 {
-	int				 _errno;
 	struct net_device		*net_dev;
 	struct dpa_priv_s		*priv;
 	struct dpa_percpu_priv_s	*percpu_priv;
@@ -575,12 +571,7 @@ ingress_dqrr(struct qman_portal		*portal,
 	if (_rtx == TX && percpu_priv->count[_rtx][ed] > DPA_MAX_TX_BACKLOG)
 		netif_tx_stop_all_queues(net_dev);
 
-	_errno = schedule_work(&percpu_priv->fd_work);
-	if (unlikely(_errno < 0))
-		if (netif_msg_rx_err(priv))
-			cpu_netdev_crit(net_dev,
-					"%s:%hu:%s(): schedule_work() = %d\n",
-					__file__, __LINE__, __func__, _errno);
+	tasklet_schedule(&percpu_priv->dpa_task);
 
 _return:
 	if (netif_msg_intr(priv))
@@ -850,7 +841,6 @@ static int dpa_process_one(struct net_device *net_dev, struct sk_buff *skb,
 				struct dpa_bp *dpa_bp, const struct qm_fd *fd)
 {
 	int err;
-	const struct dpa_priv_s	*priv;
 
  	memcpy(skb_put(skb, dpa_fd_length(fd)),
  			dpa_phys2virt(dpa_bp, (struct bm_buffer *)fd) +
@@ -957,7 +947,8 @@ static void __hot _dpa_rx(struct net_device		*net_dev,
 	size = dpa_fd_length(&dpa_fd->fd);
 
 	if (skb == NULL) {
-		skb = __netdev_alloc_skb(net_dev, DPA_BP_HEAD + NET_IP_ALIGN + size, GFP_DPA);
+		skb = __netdev_alloc_skb(net_dev,
+				DPA_BP_HEAD + NET_IP_ALIGN + size, GFP_ATOMIC);
 		if (unlikely(skb == NULL)) {
 			if (netif_msg_rx_err(priv) && net_ratelimit())
 				cpu_netdev_err(net_dev, "%s:%hu:%s(): "
@@ -1045,17 +1036,17 @@ static void (*const _dpa_work[][2])(struct net_device		*net_dev,
 	[TX] = {_dpa_tx_error, _dpa_tx}
 };
 
-static void __hot dpa_work(struct work_struct *fd_work)
+static void __hot dpa_tasklet(unsigned long data)
 {
-	int				 _errno, i, j;
-	struct net_device		*net_dev;
-	const struct dpa_priv_s		*priv;
-	struct dpa_percpu_priv_s	*percpu_priv;
-	struct dpa_fd			*dpa_fd, *tmp;
+	int i, j;
+	struct net_device *net_dev;
+	const struct dpa_priv_s *priv;
+	struct dpa_percpu_priv_s *percpu_priv =
+				(struct dpa_percpu_priv_s *)data;
+	struct dpa_fd *dpa_fd, *tmp;
 	unsigned int quota = 0;
 	unsigned int retry = 0;
 
-	percpu_priv = container_of(fd_work, struct dpa_percpu_priv_s, fd_work);
 	net_dev = percpu_priv->net_dev;
 	priv = netdev_priv(net_dev);
 
@@ -1097,15 +1088,8 @@ static void __hot dpa_work(struct work_struct *fd_work)
 		netif_tx_wake_all_queues(net_dev);
 
 	/* Try again later if we're not done */
-	if (retry) {
-		_errno = schedule_work(fd_work);
-		if (unlikely(_errno < 0))
-			if (netif_msg_rx_err(priv))
-				cpu_netdev_crit(net_dev, "%s:%hu:%s(): "
-						"schedule_work() = %d\n",
-						__file__, __LINE__, __func__,
-						_errno);
-	}
+	if (retry)
+		tasklet_schedule(&percpu_priv->dpa_task);
 
 	if (netif_msg_drv(priv))
 		cpu_netdev_dbg(net_dev, "%s:%s() ->\n", __file__, __func__);
@@ -1990,7 +1974,9 @@ dpa_probe(struct of_device *_of_dev)
 		percpu_priv = per_cpu_ptr(priv->percpu_priv, i);
 
 		percpu_priv->net_dev = net_dev;
-		INIT_WORK(&percpu_priv->fd_work, dpa_work);
+		tasklet_init(&percpu_priv->dpa_task, dpa_tasklet,
+				(unsigned long)percpu_priv);
+
 		for (j = 0; j < ARRAY_SIZE(percpu_priv->fd_list); j++)
 			for (k = 0; k < ARRAY_SIZE(percpu_priv->fd_list[j]);
 			     k++)
diff --git a/drivers/net/dpa/dpa.h b/drivers/net/dpa/dpa.h
index af36894..695e3da 100644
--- a/drivers/net/dpa/dpa.h
+++ b/drivers/net/dpa/dpa.h
@@ -1,4 +1,4 @@
-/* Copyright (c) 2008 - 2009, Freescale Semiconductor, Inc.
+/* Copyright (c) 2008 - 2010, Freescale Semiconductor, Inc.
  * All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
@@ -55,7 +55,7 @@ struct pcd_range {
 
 struct dpa_percpu_priv_s {
 	struct net_device	*net_dev;
-	struct work_struct	 fd_work;
+	struct tasklet_struct    dpa_task;
 	struct list_head	 fd_list[2][2];
 	struct list_head	 free_list;
 	struct sk_buff_head	 rx_recycle;
-- 
1.6.5.2

