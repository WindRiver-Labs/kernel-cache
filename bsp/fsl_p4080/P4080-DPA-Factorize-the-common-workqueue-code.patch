From 1a89a233a2c5692a798971d6c28c5e8efd920966 Mon Sep 17 00:00:00 2001
From: Emil Medve <Emilian.Medve@Freescale.com>
Date: Mon, 14 Dec 2009 11:36:33 -0600
Subject: [PATCH] P4080/DPA: Factorize the common workqueue code

Signed-off-by: Emil Medve <Emilian.Medve@Freescale.com>
[Kevin: Original patch taken from Freescale p4080 SDK 2.0.2 tar
package, apply cleanly to kernel 2.6.27]
Integrated-by: Kevin Hao <kexin.hao@windriver.com>
---
 drivers/net/dpa/dpa.c |  339 +++++++++++++++++++++++++++----------------------
 drivers/net/dpa/dpa.h |    9 +-
 2 files changed, 192 insertions(+), 156 deletions(-)

diff --git a/drivers/net/dpa/dpa.c b/drivers/net/dpa/dpa.c
index 5955e37..a879c58 100644
--- a/drivers/net/dpa/dpa.c
+++ b/drivers/net/dpa/dpa.c
@@ -816,10 +816,10 @@ ingress_rx_default_dqrr(struct qman_portal		*portal,
 			struct qman_fq			*fq,
 			const struct qm_dqrr_entry	*dq)
 {
-	const struct net_device	*net_dev;
-	struct dpa_priv_s	*priv;
-	struct dpa_fd		*dpa_fd;
+	const struct net_device		*net_dev;
+	const struct dpa_priv_s		*priv;
 	struct dpa_percpu_priv_s	*percpu_priv;
+	struct dpa_fd			*dpa_fd;
 
 	net_dev = ((struct dpa_fq *)fq)->net_dev;
 	priv = (typeof(priv))netdev_priv(net_dev);
@@ -899,10 +899,11 @@ ingress_rx_default_dqrr(struct qman_portal		*portal,
 
 	dpa_fd->fd = dq->fd;
 
-	list_add_tail(&dpa_fd->list, &percpu_priv->fd_list);
+	list_add_tail(&dpa_fd->list, &percpu_priv->fd_list[RX]);
 #ifdef CONFIG_DEBUG_FS
-	percpu_priv->count++;
-	percpu_priv->max = max(percpu_priv->max, percpu_priv->count);
+	percpu_priv->count[RX]++;
+	percpu_priv->max[RX] = max(percpu_priv->max[RX],
+				   percpu_priv->count[RX]);
 #endif
 
 	schedule_work(&percpu_priv->fd_work);
@@ -1062,20 +1063,20 @@ static void ingress_tx_error_fqs(struct qman_portal		*portal,
 		cpu_netdev_dbg(net_dev, "%s:%s() ->\n", __file__, __func__);
 }
 
-static enum qman_cb_dqrr_result
+static enum qman_cb_dqrr_result __hot
 ingress_tx_default_dqrr(struct qman_portal		*portal,
 			struct qman_fq			*fq,
 			const struct qm_dqrr_entry	*dq)
 {
-	struct net_device	*net_dev;
-	const struct dpa_priv_s	*priv;
-	struct dpa_percpu_priv_s *percpu_priv;
-	struct dpa_fd *dpa_fd;
+	struct net_device		*net_dev;
+	const struct dpa_priv_s		*priv;
+	struct dpa_percpu_priv_s	*percpu_priv;
+	struct dpa_fd			*dpa_fd;
 
 	net_dev = ((struct dpa_fq *)fq)->net_dev;
 	priv = netdev_priv(net_dev);
 
-	if (netif_msg_tx_err(priv))
+	if (netif_msg_intr(priv))
 		cpu_netdev_dbg(net_dev, "-> %s:%s()\n", __file__, __func__);
 
 #ifdef CONFIG_FSL_FMAN_TEST
@@ -1100,11 +1101,11 @@ ingress_tx_default_dqrr(struct qman_portal		*portal,
 		dpa_fd = devm_kzalloc(net_dev->dev.parent,
 				sizeof(*dpa_fd), GFP_ATOMIC);
 		if (unlikely(dpa_fd == NULL)) {
-			if (netif_msg_rx_err(priv))
+			if (netif_msg_tx_err(priv))
 				cpu_netdev_err(net_dev,
 					"%s:%hu:%s(): devm_kzalloc() failed\n",
 					__file__, __LINE__, __func__);
-			return qman_cb_dqrr_consume;
+			goto _return;
 		}
 	} else {
 		dpa_fd = list_first_entry(&percpu_priv->free_list,
@@ -1114,11 +1115,17 @@ ingress_tx_default_dqrr(struct qman_portal		*portal,
 
 	dpa_fd->fd = dq->fd;
 
-	list_add_tail(&dpa_fd->list, &percpu_priv->tx_fd_list);
+	list_add_tail(&dpa_fd->list, &percpu_priv->fd_list[TX]);
+#ifdef CONFIG_DEBUG_FS
+	percpu_priv->count[TX]++;
+	percpu_priv->max[TX] = max(percpu_priv->max[TX],
+				   percpu_priv->count[TX]);
+#endif
 
 	schedule_work(&percpu_priv->fd_work);
 
-	if (netif_msg_tx_err(priv))
+_return:
+	if (netif_msg_intr(priv))
 		cpu_netdev_dbg(net_dev, "%s:%s() ->\n", __file__, __func__);
 
 	return qman_cb_dqrr_consume;
@@ -1414,7 +1421,7 @@ err_bpid2pool_failed:
 }
 
 static int dpa_process_one(struct net_device *net_dev, struct sk_buff *skb,
-				struct dpa_bp *dpa_bp, struct qm_fd *fd)
+				struct dpa_bp *dpa_bp, const struct qm_fd *fd)
 {
 	int err;
 
@@ -1455,98 +1462,51 @@ static int dpa_process_one(struct net_device *net_dev, struct sk_buff *skb,
 	return 0;
 }
 
-static void __hot dpa_rx(struct work_struct *fd_work)
+static void __hot _dpa_rx(struct net_device		*net_dev,
+			  const struct dpa_priv_s	*priv,
+			  struct dpa_percpu_priv_s	*percpu_priv,
+			  const struct dpa_fd		*dpa_fd)
 {
 	int _errno;
-	struct net_device *net_dev;
-	const struct dpa_priv_s	*priv;
-	struct dpa_percpu_priv_s *percpu_priv;
-	struct dpa_fd *dpa_fd, *tmp;
 	struct dpa_bp *dpa_bp;
 	size_t size;
 	struct sk_buff *skb;
 
-	percpu_priv = (typeof(percpu_priv))container_of(
-		fd_work, struct dpa_percpu_priv_s, fd_work);
-	net_dev = percpu_priv->net_dev;
-	priv = netdev_priv(net_dev);
-
-	if (netif_msg_rx_status(priv))
-		cpu_netdev_dbg(net_dev, "-> %s:%s()\n", __file__, __func__);
-
-	BUG_ON(percpu_priv != per_cpu_ptr(priv->percpu_priv,
-					  smp_processor_id()));
-
-#ifdef CONFIG_DEBUG_FS
-	percpu_priv->swi++;
-#endif
-
-	/* Clean up any finished TX packets */
-	list_for_each_entry_safe(dpa_fd, tmp, &percpu_priv->tx_fd_list, list) {
-
-		skb = *(typeof(&skb))bus_to_virt(dpa_fd->fd.addr_lo);
-
-		BUG_ON(net_dev != skb->dev);
-
-		dma_unmap_single(net_dev->dev.parent, dpa_fd->fd.addr_lo,
-				skb_headlen(skb), DMA_TO_DEVICE);
-
-		if (skb_queue_len(&percpu_priv->rx_recycle) < DEFAULT_COUNT &&
-				skb_recycle_check(skb,
-					NET_IP_ALIGN +
-					ETH_HLEN +
-					NN_ALLOCATED_SPACE(net_dev) +
-					TT_ALLOCATED_SPACE(net_dev)))
-			__skb_queue_head(&percpu_priv->rx_recycle, skb);
-		else
-			dev_kfree_skb_any(skb);
-
-		local_irq_disable();
-		list_del(&dpa_fd->list);
-		list_add_tail(&dpa_fd->list, &percpu_priv->free_list);
-		local_irq_enable();
-
-	}
-
-	list_for_each_entry_safe(dpa_fd, tmp, &percpu_priv->fd_list, list) {
-		skb = NULL;
-
-		if (unlikely(dpa_fd->fd.status & FM_FD_STAT_ERRORS) != 0) {
-			if (netif_msg_rx_err(priv))
-				cpu_netdev_warn(net_dev,
+	if (unlikely(dpa_fd->fd.status & FM_FD_STAT_ERRORS) != 0) {
+		if (netif_msg_rx_err(priv))
+			cpu_netdev_warn(net_dev,
 					"%s:%hu:%s(): FD status = 0x%08x\n",
 					__file__, __LINE__, __func__,
 					dpa_fd->fd.status & FM_FD_STAT_ERRORS);
 
-			net_dev->stats.rx_errors++;
+		net_dev->stats.rx_errors++;
 
-			goto _continue_dpa_fd_release;
-		}
-
-		net_dev->stats.rx_packets++;
-		net_dev->stats.rx_bytes += dpa_fd_length(&dpa_fd->fd);
+		goto _return_dpa_fd_release;
+	}
 
-		dpa_bp = dpa_bpid2pool(dpa_fd->fd.bpid);
-		BUG_ON(IS_ERR(dpa_bp));
+	net_dev->stats.rx_packets++;
+	net_dev->stats.rx_bytes += dpa_fd_length(&dpa_fd->fd);
 
-		if (dpa_fd->fd.format == qm_fd_sg && !dpa_bp->kernel_pool) {
-			net_dev->stats.rx_dropped++;
-			cpu_netdev_err(net_dev, "%s:%hu:%s(): "
-				       "Dropping a SG frame\n",
-				       __file__, __LINE__, __func__);
-			goto _continue_dpa_fd_release;
-		}
+	dpa_bp = dpa_bpid2pool(dpa_fd->fd.bpid);
+	BUG_ON(IS_ERR(dpa_bp));
 
-		if (dpa_bp->kernel_pool) {
-			size = ETH_HLEN + NN_ALLOCATED_SPACE(net_dev) +
-				TT_ALLOCATED_SPACE(net_dev);
-			skb = __skb_dequeue(&percpu_priv->rx_recycle);
-		} else
-			size = dpa_fd_length(&dpa_fd->fd);
+	if (dpa_fd->fd.format == qm_fd_sg && !dpa_bp->kernel_pool) {
+		net_dev->stats.rx_dropped++;
+		cpu_netdev_err(net_dev, "%s:%hu:%s(): Dropping a SG frame\n",
+			       __file__, __LINE__, __func__);
+		goto _return_dpa_fd_release;
+	}
 
-		if (!skb)
-			skb = __netdev_alloc_skb(net_dev,
-					NET_IP_ALIGN + size, GFP_DPA);
+	skb = NULL;
+	if (dpa_bp->kernel_pool) {
+		size = ETH_HLEN + NN_ALLOCATED_SPACE(net_dev) +
+			TT_ALLOCATED_SPACE(net_dev);
+		skb = __skb_dequeue(&percpu_priv->rx_recycle);
+	} else
+		size = dpa_fd_length(&dpa_fd->fd);
+
+	if (skb == NULL) {
+		skb = __netdev_alloc_skb(net_dev, NET_IP_ALIGN + size, GFP_DPA);
 		if (unlikely(skb == NULL)) {
 			if (netif_msg_rx_err(priv))
 				cpu_netdev_err(net_dev, "%s:%hu:%s(): "
@@ -1555,65 +1515,138 @@ static void __hot dpa_rx(struct work_struct *fd_work)
 
 			net_dev->stats.rx_dropped++;
 
-			goto _continue_dpa_fd_release;
+			goto _return_dpa_fd_release;
 		}
+	}
 
-		skb_reserve(skb, NET_IP_ALIGN);
+	skb_reserve(skb, NET_IP_ALIGN);
 
-		/* Fill the SKB */
-		if (dpa_fd->fd.format == qm_fd_sg)
-			_errno = dpa_process_sg(net_dev, skb, dpa_bp,
-						&dpa_fd->fd);
-		else
-			_errno = dpa_process_one(net_dev, skb, dpa_bp,
-						 &dpa_fd->fd);
+	/* Fill the SKB */
+	if (dpa_fd->fd.format == qm_fd_sg)
+		_errno = dpa_process_sg(net_dev, skb, dpa_bp, &dpa_fd->fd);
+	else
+		_errno = dpa_process_one(net_dev, skb, dpa_bp, &dpa_fd->fd);
 
-		if (_errno)
-			goto _continue_dev_kfree_skb;
+	if (unlikely(_errno < 0))
+		goto _return_dev_kfree_skb;
 
-		skb->protocol = eth_type_trans(skb, net_dev);
+	skb->protocol = eth_type_trans(skb, net_dev);
 
 #ifdef CONFIG_FSL_FMAN_TEST
         fman_test_ip_manip((void *)priv->mac_dev, skb->data);
 #endif /* CONFIG_FSL_FMAN_TEST */
 
-		_errno = netif_rx_ni(skb);
-		if (unlikely(_errno != NET_RX_SUCCESS)) {
-			if (netif_msg_rx_status(priv))
-				cpu_netdev_warn(net_dev, "%s:%hu:%s(): netif_rx_ni() = %d\n",
-						__file__, __LINE__, __func__, _errno);
-			net_dev->stats.rx_dropped++;
-			goto _continue_dev_kfree_skb;
-		}
+	_errno = netif_rx_ni(skb);
+	if (unlikely(_errno != NET_RX_SUCCESS)) {
+		if (netif_msg_rx_status(priv))
+			cpu_netdev_warn(net_dev, "%s:%hu:%s(): "
+					"netif_rx_ni() = %d\n",
+					__file__, __LINE__, __func__, _errno);
 
-		net_dev->last_rx = jiffies;
+		net_dev->stats.rx_dropped++;
 
-		if (dpa_bp->bp_refill_pending > dpa_bp->bp_kick_thresh)
-			dpa_bp_refill(net_dev, dpa_bp, GFP_KERNEL);
+		goto _return_dev_kfree_skb;
+	}
 
-		goto _continue;
+	net_dev->last_rx = jiffies;
 
-_continue_dev_kfree_skb:
+	if (dpa_bp->bp_refill_pending > dpa_bp->bp_kick_thresh)
+		dpa_bp_refill(net_dev, dpa_bp, GFP_KERNEL);
+
+	return;
+
+_return_dev_kfree_skb:
+	dev_kfree_skb(skb);
+_return_dpa_fd_release:
+	_errno = dpa_fd_release(net_dev, &dpa_fd->fd);
+	if (unlikely(_errno < 0)) {
+		dump_stack();
+		panic("Can't release buffer to the BM during RX\n");
+	}
+}
+
+static void __hot _dpa_tx(struct net_device		*net_dev,
+			  const struct dpa_priv_s	*priv,
+			  struct dpa_percpu_priv_s	*percpu_priv,
+			  const struct dpa_fd		*dpa_fd)
+{
+	struct sk_buff	*skb;
+
+	if (unlikely(dpa_fd->fd.status & FM_FD_STAT_ERRORS) != 0) {
+		if (netif_msg_tx_err(priv))
+			cpu_netdev_warn(net_dev,
+					"%s:%hu:%s(): FD status = 0x%08x\n",
+					__file__, __LINE__, __func__,
+					dpa_fd->fd.status & FM_FD_STAT_ERRORS);
+
+		net_dev->stats.tx_errors++;
+	}
+
+	skb = *(typeof(&skb))bus_to_virt(dpa_fd->fd.addr_lo);
+
+	BUG_ON(net_dev != skb->dev);
+
+	dma_unmap_single(net_dev->dev.parent, dpa_fd->fd.addr_lo,
+			 skb_headlen(skb), DMA_TO_DEVICE);
+
+	if (skb_queue_len(&percpu_priv->rx_recycle) < DEFAULT_COUNT &&
+	    skb_recycle_check(skb, NET_IP_ALIGN + ETH_HLEN +
+			      NN_ALLOCATED_SPACE(net_dev) +
+			      TT_ALLOCATED_SPACE(net_dev)))
+		__skb_queue_head(&percpu_priv->rx_recycle, skb);
+	else
 		dev_kfree_skb(skb);
-_continue_dpa_fd_release:
-		_errno = dpa_fd_release(net_dev, &dpa_fd->fd);
-		if (unlikely(_errno < 0)) {
-			dump_stack();
-			panic("Can't release buffer to the BM during RX\n");
-		}
+}
+
+static void (*const _dpa_work[])(struct net_device		*net_dev,
+				 const struct dpa_priv_s	*priv,
+				 struct dpa_percpu_priv_s	*percpu_priv,
+				 const struct dpa_fd		*dpa_fd) =
+{
+	[RX] = _dpa_rx,
+	[TX] = _dpa_tx
+};
+
+static void __hot dpa_work(struct work_struct *fd_work)
+{
+	int				 i;
+	struct net_device		*net_dev;
+	const struct dpa_priv_s		*priv;
+	struct dpa_percpu_priv_s	*percpu_priv;
+	struct dpa_fd			*dpa_fd, *tmp;
+
+	percpu_priv = (typeof(percpu_priv))container_of(
+		fd_work, struct dpa_percpu_priv_s, fd_work);
+	net_dev = percpu_priv->net_dev;
+	priv = netdev_priv(net_dev);
+
+	if (netif_msg_intr(priv))
+		cpu_netdev_dbg(net_dev, "-> %s:%s()\n", __file__, __func__);
+
+	BUG_ON(percpu_priv != per_cpu_ptr(priv->percpu_priv,
+					  smp_processor_id()));
 
-_continue:
-		local_irq_disable();
-		list_del(&dpa_fd->list);
 #ifdef CONFIG_DEBUG_FS
-		percpu_priv->count--;
-		percpu_priv->total++;
+	percpu_priv->swi++;
 #endif
-		list_add_tail(&dpa_fd->list, &percpu_priv->free_list);
-		local_irq_enable();
-	}
 
-	if (netif_msg_rx_status(priv))
+	/* RX, TX */
+	for (i = 0; i < ARRAY_SIZE(percpu_priv->fd_list); i++)
+		list_for_each_entry_safe(dpa_fd, tmp,
+					 &percpu_priv->fd_list[i], list) {
+			_dpa_work[i](net_dev, priv, percpu_priv, dpa_fd);
+
+			local_irq_disable();
+			list_del(&dpa_fd->list);
+#ifdef CONFIG_DEBUG_FS
+			percpu_priv->count[i]--;
+			percpu_priv->total[i]++;
+#endif
+			list_add_tail(&dpa_fd->list, &percpu_priv->free_list);
+			local_irq_enable();
+		}
+
+	if (netif_msg_intr(priv))
 		cpu_netdev_dbg(net_dev, "%s:%s() ->\n", __file__, __func__);
 }
 
@@ -2268,27 +2301,31 @@ static int __cold dpa_debugfs_show(struct seq_file *file, void *offset)
 
 	memset(&total, 0, sizeof(total));
 
-	seq_printf(file, "hardware/logical cpu\thwi[RX]\t\tswi\t\ttotal\t\tmax"
-		   "\t\thwi[TX]\n");
+	seq_printf(file, "      hwi[RX]    hwi[TX]    swi        "
+		   "total[RX]  max[RX]    total[TX]  max[TX]\n");
 	for_each_online_cpu(i) {
 		percpu_priv = per_cpu_ptr(priv->percpu_priv, i);
 
-		total.total	+= percpu_priv->total;
-		total.max	+= percpu_priv->max;
+		for (j = 0; j < ARRAY_SIZE(total.total); j++) {
+			total.total[j]	+= percpu_priv->total[j];
+			total.max[j]	+= percpu_priv->max[j];
+			total.hwi[j]	+= percpu_priv->hwi[j];
+		}
 		total.swi	+= percpu_priv->swi;
-		for (j = 0; j < ARRAY_SIZE(total.hwi); j++)
-			total.hwi[j] += percpu_priv->hwi[j];
 
-		seq_printf(file, "%hu/%hu\t\t\t0x%08x\t0x%08x\t0x%08x\t0x%08x\t"
-			   "0x%08x\n",
+		seq_printf(file, "%hu/%hu   0x%08x 0x%08x 0x%08x "
+			   "0x%08x 0x%08x 0x%08x 0x%08x\n",
 			   get_hard_smp_processor_id(i), i,
-			   percpu_priv->hwi[RX], percpu_priv->swi,
-			   percpu_priv->total, percpu_priv->max,
-			   percpu_priv->hwi[TX]);
+			   percpu_priv->hwi[RX], percpu_priv->hwi[TX],
+			   percpu_priv->swi,
+			   percpu_priv->total[RX], percpu_priv->max[RX],
+			   percpu_priv->total[TX], percpu_priv->max[TX]);
 	}
-	seq_printf(file, "Total\t\t\t0x%08x\t0x%08x\t0x%08x\t0x%08x\t0x%08x\n",
-		   total.hwi[RX], total.swi, total.total, total.max,
-		   total.hwi[TX]);
+	seq_printf(file, "Total 0x%08x 0x%08x 0x%08x "
+		   "0x%08x 0x%08x 0x%08x 0x%08x\n",
+		   total.hwi[RX], total.hwi[TX], total.swi,
+		   total.total[RX], total.max[RX],
+		   total.total[TX], total.max[TX]);
 
 	return 0;
 }
@@ -2460,9 +2497,9 @@ dpa_probe(struct of_device *_of_dev)
 		percpu_priv = per_cpu_ptr(priv->percpu_priv, i);
 
 		percpu_priv->net_dev = net_dev;
-		INIT_WORK(&percpu_priv->fd_work, dpa_rx);
-		INIT_LIST_HEAD(&percpu_priv->fd_list);
-		INIT_LIST_HEAD(&percpu_priv->tx_fd_list);
+		INIT_WORK(&percpu_priv->fd_work, dpa_work);
+		for (j = 0; j < ARRAY_SIZE(percpu_priv->fd_list); j++)
+			INIT_LIST_HEAD(&percpu_priv->fd_list[j]);
 		INIT_LIST_HEAD(&percpu_priv->free_list);
 		skb_queue_head_init(&percpu_priv->rx_recycle);
 	}
diff --git a/drivers/net/dpa/dpa.h b/drivers/net/dpa/dpa.h
index d59688c..70faacb 100644
--- a/drivers/net/dpa/dpa.h
+++ b/drivers/net/dpa/dpa.h
@@ -56,12 +56,11 @@ struct pcd_range {
 struct dpa_percpu_priv_s {
 	struct net_device	*net_dev;
 	struct work_struct	 fd_work;
-	struct list_head	fd_list;
-	struct list_head	tx_fd_list;
-	struct list_head	free_list;
-	struct sk_buff_head	rx_recycle;
+	struct list_head	 fd_list[2];
+	struct list_head	 free_list;
+	struct sk_buff_head	 rx_recycle;
 #ifdef CONFIG_DEBUG_FS
-	size_t			 count, total, max, hwi[2], swi;
+	size_t			 count[2], total[2], max[2], hwi[2], swi;
 #endif
 };
 
-- 
1.6.0.4

