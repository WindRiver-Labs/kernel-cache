From c34c1770618777e55ed59ae3b027937988db9d22 Mon Sep 17 00:00:00 2001
From: Kevin Hao <kexin.hao@windriver.com>
Date: Mon, 21 Sep 2009 11:07:02 +0800
Subject: [PATCH 14/77] add Freescale Buffer Manager support

Signed-off-by: Emil Medve <Emilian.Medve@Freescale.com>
Signed-off-by: Geoff Thorpe <geoff@geoffthorpe.net>
Signed-off-by: Jeffrey Ladouceur <Jeffrey.Ladouceur@freescale.com>
Signed-off-by: Kim Phillips <kim.phillips@freescale.com>
Signed-off-by: Kumar Gala <galak@kernel.crashing.org>
Signed-off-by: Scott Wood <scottwood@freescale.com>
[KevinHao: Original headerless patch
(p4080_1-2-rc1-drivers_hwalloc.patch) taken from Freescale rev 1.2
board support ISO image for p4080. Change cpumask_of to get_cpu_mask]
Integrated-by: Kevin Hao <kexin.hao@windriver.com>
---
 drivers/Kconfig                  |    2 +
 drivers/Makefile                 |    1 +
 drivers/hwalloc/Kconfig          |   71 ++++
 drivers/hwalloc/Makefile         |    4 +
 drivers/hwalloc/bman_config.c    |  299 +++++++++++++++++
 drivers/hwalloc/bman_driver.c    |  356 ++++++++++++++++++++
 drivers/hwalloc/bman_high.c      |  674 ++++++++++++++++++++++++++++++++++++++
 drivers/hwalloc/bman_low.c       |  471 ++++++++++++++++++++++++++
 drivers/hwalloc/bman_private.h   |  133 ++++++++
 drivers/hwalloc/bman_sys.h       |   97 ++++++
 drivers/hwalloc/bman_test.c      |   78 +++++
 drivers/hwalloc/bman_test.h      |   92 ++++++
 drivers/hwalloc/bman_test_high.c |  183 +++++++++++
 drivers/hwalloc/bman_test_low.c  |  292 ++++++++++++++++
 14 files changed, 2753 insertions(+), 0 deletions(-)
 create mode 100644 drivers/hwalloc/Kconfig
 create mode 100644 drivers/hwalloc/Makefile
 create mode 100644 drivers/hwalloc/bman_config.c
 create mode 100644 drivers/hwalloc/bman_driver.c
 create mode 100644 drivers/hwalloc/bman_high.c
 create mode 100644 drivers/hwalloc/bman_low.c
 create mode 100644 drivers/hwalloc/bman_private.h
 create mode 100644 drivers/hwalloc/bman_sys.h
 create mode 100644 drivers/hwalloc/bman_test.c
 create mode 100644 drivers/hwalloc/bman_test.h
 create mode 100644 drivers/hwalloc/bman_test_high.c
 create mode 100644 drivers/hwalloc/bman_test_low.c

diff --git a/drivers/Kconfig b/drivers/Kconfig
index 8462eee..dc298a1 100644
--- a/drivers/Kconfig
+++ b/drivers/Kconfig
@@ -102,5 +102,7 @@ source "drivers/uio/Kconfig"
 
 source "drivers/xen/Kconfig"
 
+source "drivers/hwalloc/Kconfig"
+
 source "drivers/hwqueue/Kconfig"
 endmenu
diff --git a/drivers/Makefile b/drivers/Makefile
index f98c14b..e6968ce 100644
--- a/drivers/Makefile
+++ b/drivers/Makefile
@@ -103,4 +103,5 @@ obj-$(CONFIG_OF)		+= of/
 obj-$(CONFIG_SSB)		+= ssb/
 obj-$(CONFIG_VIRTIO)		+= virtio/
 obj-$(CONFIG_REGULATOR)		+= regulator/
+obj-y				+= hwalloc/
 obj-y				+= hwqueue/
diff --git a/drivers/hwalloc/Kconfig b/drivers/hwalloc/Kconfig
new file mode 100644
index 0000000..7939fee
--- /dev/null
+++ b/drivers/hwalloc/Kconfig
@@ -0,0 +1,71 @@
+menu "Hardware allocator support"
+
+menuconfig FSL_BMAN
+	bool "Freescale Buffer Manager (datapath) support"
+	depends on PPC_E500MC
+	default y
+	---help---
+	  If unsure, say Y.
+
+if FSL_BMAN
+
+config FSL_BMAN_CHECKING
+	bool "additional driver checking"
+	default y
+	---help---
+	  Compiles in additional checks to sanity-check the Bman driver and any
+	  use of it by other code. Not recommended for performance.
+
+config FSL_BMAN_PORTAL
+	bool "Bman portal support"
+	default y
+	---help---
+	  Compiles support to detect and support Bman software corenet portals
+	  (as provided by the device-tree).
+
+config FSL_BMAN_PORTAL_DISABLEAUTO
+	bool "disable auto-initialisation of cpu-affine portals"
+	depends on FSL_BMAN_PORTAL
+	default n
+	---help---
+	  The high-level portal API, in its normal usage, requires that each cpu
+	  have a portal assigned to it that is auto-initialised. If an
+	  application is manually initialising portals in a non-cpu-affine
+	  manner (or you are using the low-level portal API), this may need to
+	  be disabled. If in doubt, say N.
+
+config FSL_BMAN_CONFIG
+	bool "Bman device management"
+	default y
+	---help---
+	  If this linux image is running natively, you need this option. If this
+	  linux image is running as a guest OS under the hypervisor, only one
+	  guest OS ("the control plane") needs this option.
+
+config FSL_BMAN_TEST
+	tristate "Bman self-tests"
+	depends on FSL_BMAN_PORTAL
+	default n
+	---help---
+	  This option compiles self-test code for Bman.
+
+config FSL_BMAN_TEST_LOW
+	bool "Bman low-level self-test"
+	depends on FSL_BMAN_TEST
+	default y
+	---help---
+	  This takes an unused portal and portal and performs low-level
+	  API testing with it.
+
+config FSL_BMAN_TEST_HIGH
+	bool "Bman high-level self-test"
+	depends on FSL_BMAN_TEST && !FSL_BMAN_PORTAL_DISABLEAUTO
+	default y
+	---help---
+	  This requires the presence of cpu-affine portals, and performs
+	  high-level API testing with them (whichever portal(s) are affine to
+	  the cpu(s) the test executes on).
+
+endif # FSL_BMAN
+
+endmenu
diff --git a/drivers/hwalloc/Makefile b/drivers/hwalloc/Makefile
new file mode 100644
index 0000000..e157d46
--- /dev/null
+++ b/drivers/hwalloc/Makefile
@@ -0,0 +1,4 @@
+obj-$(CONFIG_FSL_BMAN_CONFIG)	+= bman_config.o
+obj-$(CONFIG_FSL_BMAN_PORTAL)	+= bman_driver.o bman_low.o bman_high.o
+obj-$(CONFIG_FSL_BMAN_TEST)	+= bman_tester.o
+bman_tester-y			 = bman_test.o bman_test_high.o bman_test_low.o
diff --git a/drivers/hwalloc/bman_config.c b/drivers/hwalloc/bman_config.c
new file mode 100644
index 0000000..f76094a
--- /dev/null
+++ b/drivers/hwalloc/bman_config.c
@@ -0,0 +1,299 @@
+/* Copyright (c) 2009 Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef CONFIG_SMP
+#include <linux/smp.h>	/* get_hard_smp_processor_id() */
+#endif
+
+#include "bman_private.h"
+
+/* Last updated for v00.79 of the BG */
+
+struct bman;
+
+/* Register offsets */
+#define REG_POOL_SWDET(n)	(0x0000 + ((n) * 0x04))
+#define REG_POOL_HWDET(n)	(0x0100 + ((n) * 0x04))
+#define REG_POOL_SWDXT(n)	(0x0200 + ((n) * 0x04))
+#define REG_POOL_HWDXT(n)	(0x0300 + ((n) * 0x04))
+#define REG_IP_REV_1		0x0bf8
+#define REG_IP_REV_2		0x0bfc
+#define REG_FBPR_BARE		0x0c00
+#define REG_FBPR_BAR		0x0c04
+#define REG_FBPR_AR		0x0c10
+#define REG_SRCIDR		0x0d04
+#define REG_LIODNR		0x0d08
+#define REG_ERR_ISR		0x0e00	/* + "enum bm_isr_reg" */
+
+/* Used by all error interrupt registers except 'inhibit' */
+#define BM_EIRQ_IVCI	0x00000010	/* Invalid Command Verb */
+#define BM_EIRQ_FLWI	0x00000008	/* FBPR Low Watermark */
+#define BM_EIRQ_MBEI	0x00000004	/* Multi-bit ECC Error */
+#define BM_EIRQ_SBEI	0x00000002	/* Single-bit ECC Error */
+#define BM_EIRQ_BSCN	0x00000001	/* pool State Change Notification */
+
+/**
+ * bm_err_isr_<reg>_<verb> - Manipulate global interrupt registers
+ * @v: for accessors that write values, this is the 32-bit value
+ *
+ * Manipulates BMAN_ERR_ISR, BMAN_ERR_IER, BMAN_ERR_ISDR, BMAN_ERR_IIR. All
+ * manipulations except bm_err_isr_[un]inhibit() use 32-bit masks composed of
+ * the BM_EIRQ_*** definitions. Note that "bm_err_isr_enable_write" means
+ * "write the enable register" rather than "enable the write register"!
+ */
+#define bm_err_isr_status_read(bm)	__bm_err_isr_read(bm, bm_isr_status)
+#define bm_err_isr_status_clear(bm, m)	__bm_err_isr_write(bm, bm_isr_status,m)
+#define bm_err_isr_enable_read(bm)	__bm_err_isr_read(bm, bm_isr_enable)
+#define bm_err_isr_enable_write(bm, v)	__bm_err_isr_write(bm, bm_isr_enable,v)
+#define bm_err_isr_disable_read(bm)	__bm_err_isr_read(bm, bm_isr_disable)
+#define bm_err_isr_disable_write(bm, v)	__bm_err_isr_write(bm, bm_isr_disable,v)
+#define bm_err_isr_inhibit(bm)		__bm_err_isr_write(bm, bm_isr_inhibit,1)
+#define bm_err_isr_uninhibit(bm)	__bm_err_isr_write(bm, bm_isr_inhibit,0)
+
+/*
+ * TODO: unimplemented registers
+ *
+ * BMAN_POOLk_SDCNT, BMAN_POOLk_HDCNT, BMAN_POOLk_CONTENT, BMAN_FULT,
+ * BMAN_VLDPL, BMAN_ECSR, BMAN_ECIR, BMAN_EADR, BMAN_EECC, BMAN_EDATA<n>,
+ * BMAN_SBET, BMAN_EINJ, BMAN_SBEC[0|1]
+ */
+
+/* Encapsulate "struct bman *" as a cast of the register space address. */
+
+static struct bman *bm_create(void *regs)
+{
+	return (struct bman *)regs;
+}
+
+static inline u32 __bm_in(struct bman *bm, u32 offset)
+{
+	return in_be32((void *)bm + offset);
+}
+static inline void __bm_out(struct bman *bm, u32 offset, u32 val)
+{
+	out_be32((void *)bm + offset, val);
+}
+#define bm_in(reg)		__bm_in(bm, REG_##reg)
+#define bm_out(reg, val)	__bm_out(bm, REG_##reg, val)
+
+#if 0
+
+static u32 __bm_err_isr_read(struct bman *bm, enum bm_isr_reg n)
+{
+	return __bm_in(bm, REG_ERR_ISR + (n << 2));
+}
+
+static void __bm_err_isr_write(struct bman *bm, enum bm_isr_reg n, u32 val)
+{
+	__bm_out(bm, REG_ERR_ISR + (n << 2), val);
+}
+
+static void bm_get_details(struct bman *bm, u8 *int_options, u8 *errata,
+			u8 *conf_options)
+{
+	u32 v = bm_in(IP_REV_1);
+	*int_options = (v >> 16) & 0xff;
+	*errata = (v >> 8) & 0xff;
+	*conf_options = v & 0xff;
+}
+
+static u8 bm_get_corenet_sourceid(struct bman *bm)
+{
+	return bm_in(SRCIDR);
+}
+
+static void bm_set_liodn(struct bman *bm, u16 liodn)
+{
+	bm_out(LIODNR, liodn & 0xfff);
+}
+
+#endif
+
+static void bm_get_version(struct bman *bm, u16 *id, u8 *major, u8 *minor)
+{
+	u32 v = bm_in(IP_REV_1);
+	*id = (v >> 16);
+	*major = (v >> 8) & 0xff;
+	*minor = v & 0xff;
+}
+
+static u32 __generate_thresh(u32 val, int roundup)
+{
+	u32 e = 0;	/* co-efficient, exponent */
+	int oddbit = 0;
+	while(val > 0xff) {
+		oddbit = val & 1;
+		val >>= 1;
+		e++;
+		if(roundup && oddbit)
+			val++;
+	}
+	BM_ASSERT(e < 0x10);
+	return (val | (e << 8));
+}
+
+static void bm_set_pool(struct bman *bm, u8 pool, u32 swdet, u32 swdxt,
+			u32 hwdet, u32 hwdxt)
+{
+	BM_ASSERT(pool < 64);
+	bm_out(POOL_SWDET(pool), __generate_thresh(swdet, 0));
+	bm_out(POOL_SWDXT(pool), __generate_thresh(swdxt, 1));
+	bm_out(POOL_HWDET(pool), __generate_thresh(hwdet, 0));
+	bm_out(POOL_HWDXT(pool), __generate_thresh(hwdxt, 1));
+}
+
+static void bm_set_memory(struct bman *bm, u16 eba, u32 ba, int prio, u32 size)
+{
+	u32 exp = ilog2(size);
+	/* choke if size isn't within range */
+	BM_ASSERT((size >= 4096) && (size <= 1073741824) &&
+			is_power_of_2(size));
+	/* choke if '[e]ba' has lower-alignment than 'size' */
+	BM_ASSERT(!(ba & (size - 1)));
+	bm_out(FBPR_BARE, eba);
+	bm_out(FBPR_BAR, ba);
+	bm_out(FBPR_AR, (prio ? 0x40000000 : 0) | (exp - 1));
+}
+
+/*****************/
+/* Config driver */
+/*****************/
+
+/* We support only one of these. */
+static struct bman *bm;
+
+/* TODO: Kconfig these? */
+#define DEFAULT_FBPR_SZ	(PAGE_SIZE << 12)
+
+/* Parse the <name> property to extract the memory location and size and
+ * lmb_reserve() it. If it isn't supplied, lmb_alloc() the default size. */
+static __init int parse_mem_property(struct device_node *node, const char *name,
+				dma_addr_t *addr, size_t *sz, int zero)
+{
+	const u32 *pint;
+	int ret;
+
+	pint = of_get_property(node, name, &ret);
+	if (!pint || (ret != 16)) {
+		pr_info("No %s property '%s', using lmb_alloc(%08x)\n",
+				node->full_name, name, *sz);
+		*addr = lmb_alloc(*sz, *sz);
+		if (zero)
+			memset(phys_to_virt(*addr), 0, *sz);
+		return 0;
+	}
+	pr_info("Using %s property '%s'\n", node->full_name, name);
+	/* Props are 64-bit, but dma_addr_t is (currently) 32-bit */
+	BUG_ON(sizeof(*addr) != 4);
+	BUG_ON(pint[0] || pint[2]);
+	*addr = pint[1];
+	*sz = pint[3];
+	/* Keep things simple, it's either all in the DRAM range or it's all
+	 * outside. */
+	if (*addr < lmb_end_of_DRAM()) {
+		BUG_ON((u64)*addr + (u64)*sz > lmb_end_of_DRAM());
+		if (lmb_reserve(*addr, *sz) < 0) {
+			pr_err("Failed to reserve %s\n", name);
+			return -ENOMEM;
+		}
+		if (zero)
+			memset(phys_to_virt(*addr), 0, *sz);
+	} else {
+		/* map as cacheable, non-guarded */
+		void *tmpp = ioremap_flags(*addr, *sz, 0);
+		if (zero)
+			memset(tmpp, 0, *sz);
+		iounmap(tmpp);
+	}
+	return 0;
+}
+
+static int __init fsl_bman_init(struct device_node *node)
+{
+	struct resource res;
+	u32 __iomem *regs;
+	dma_addr_t fbpr_a;
+	size_t fbpr_sz = DEFAULT_FBPR_SZ;
+	int ret;
+	u16 id;
+	u8 major, minor;
+
+	ret = of_address_to_resource(node, 0, &res);
+	if (ret) {
+		pr_err("Can't get %s property 'reg'\n",
+				node->full_name);
+		return ret;
+	}
+	ret = parse_mem_property(node, "fsl,bman-fbpr", &fbpr_a, &fbpr_sz, 0);
+	BUG_ON(ret);
+	/* Global configuration */
+	regs = ioremap(res.start, res.end - res.start + 1);
+	bm = bm_create(regs);
+	BUG_ON(!bm);
+	bm_get_version(bm, &id, &major, &minor);
+	pr_info("Bman ver:%04x,%02x,%02x\n", id, major, minor);
+	/* FBPR memory */
+	bm_set_memory(bm, 0, (u32)fbpr_a, 0, fbpr_sz);
+	/* TODO: add interrupt handling here, so that ISR is cleared *after*
+	 * FBPR initialisation. */
+	return 0;
+}
+
+int bman_have_ccsr(void)
+{
+	return (bm ? 1 : 0);
+}
+
+int bm_pool_set(u32 bpid, const u32 *thresholds)
+{
+	if (!bm)
+		return -ENODEV;
+	bm_set_pool(bm, bpid, thresholds[0], thresholds[1],
+		thresholds[2], thresholds[3]);
+	return 0;
+}
+EXPORT_SYMBOL(bm_pool_set);
+
+__init void bman_init_early(void)
+{
+	struct device_node *dn;
+	for_each_compatible_node(dn, NULL, "fsl,bman") {
+		if (bm)
+			pr_err("%s: only one 'fsl,bman' allowed\n",
+				dn->full_name);
+		else {
+			int ret = fsl_bman_init(dn);
+			BUG_ON(ret);
+		}
+	}
+}
+
diff --git a/drivers/hwalloc/bman_driver.c b/drivers/hwalloc/bman_driver.c
new file mode 100644
index 0000000..2455370
--- /dev/null
+++ b/drivers/hwalloc/bman_driver.c
@@ -0,0 +1,356 @@
+/* Copyright (c) 2008, 2009 Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "bman_private.h"
+
+/*****************/
+/* Portal driver */
+/*****************/
+
+#define PORTAL_MAX	10
+#define POOL_MAX	64
+
+static struct bm_portal portals[PORTAL_MAX];
+static u8 num_portals;
+#ifndef CONFIG_FSL_BMAN_PORTAL_DISABLEAUTO
+static u8 num_affine_portals;
+#endif
+static DEFINE_SPINLOCK(bind_lock);
+DEFINE_PER_CPU(struct bman_portal *, bman_affine_portal);
+
+/* The bman_depletion type is a bitfield representation of the 64 BPIDs as
+ * booleans. We're not using it here to represent depletion state though, it's
+ * to represent reservations. */
+static struct bman_depletion pools;
+static u8 num_pools;
+
+static struct bm_portal *__bm_portal_add(const struct bm_addr *addr,
+				const struct bm_portal_config *config)
+{
+	struct bm_portal *ret;
+	BUG_ON((num_portals + 1) > PORTAL_MAX);
+	ret = &portals[num_portals++];
+	ret->addr = *addr;
+	ret->config = *config;
+	ret->config.bound = 0;
+	return ret;
+}
+
+static int __bm_pool_add(u32 bpid, u32 *cfg, int triplets)
+{
+	u64 total = 0;
+	BUG_ON((bpid + 1) > POOL_MAX);
+	if (bman_depletion_get(&pools, bpid)) {
+		pr_err("Duplicate pool for bpid %d\n", bpid);
+		return -EBUSY;
+	}
+	while (triplets--) {
+		struct bman_pool_params params = {
+			.bpid = bpid,
+			.flags = BMAN_POOL_FLAG_ONLY_RELEASE
+		};
+		u64 c = ((u64)cfg[0] << 32) | cfg[1];
+		u64 d = ((u64)cfg[2] << 32) | cfg[3];
+		u64 b = ((u64)cfg[4] << 32) | cfg[5];
+		struct bman_pool *pobj = bman_new_pool(&params);
+		if (!pobj)
+			return -ENOMEM;
+		while (c) {
+			struct bm_buffer bufs[8];
+			int ret, num_bufs = 0;
+			do {
+				BUG_ON(b > 0xffffffffffffull);
+				bufs[num_bufs].bpid = bpid;
+				bufs[num_bufs].hi = (b >> 32);
+				bufs[num_bufs++].lo = b & 0xffffffff;
+				b += d;
+			} while (--c && (num_bufs < 8));
+			ret = bman_release(pobj, bufs, num_bufs,
+					BMAN_RELEASE_FLAG_WAIT);
+			if (ret)
+				panic("Seeding reserved buffer pool failed\n");
+			total += num_bufs;
+		}
+		bman_free_pool(pobj);
+		cfg += 6;
+	}
+	bman_depletion_set(&pools, bpid);
+	num_pools++;
+	if (total)
+		pr_info("Bman: reserved bpid %d, seeded %lld items\n", bpid,
+			total);
+	else
+		pr_info("Bman: reserved bpid %d\n", bpid);
+	return 0;
+}
+
+int __bm_portal_bind(struct bm_portal *portal, u8 iface)
+{
+	int ret = -EBUSY;
+	spin_lock(&bind_lock);
+	if (!(portal->config.bound & iface)) {
+		portal->config.bound |= iface;
+		ret = 0;
+	}
+	spin_unlock(&bind_lock);
+	return ret;
+}
+
+void __bm_portal_unbind(struct bm_portal *portal, u8 iface)
+{
+	spin_lock(&bind_lock);
+	BM_ASSERT(portal->config.bound & iface);
+	portal->config.bound &= ~iface;
+	spin_unlock(&bind_lock);
+}
+
+u8 bm_portal_num(void)
+{
+	return num_portals;
+}
+EXPORT_SYMBOL(bm_portal_num);
+
+struct bm_portal *bm_portal_get(u8 idx)
+{
+	if (unlikely(idx >= num_portals))
+		return NULL;
+
+	return &portals[idx];
+}
+EXPORT_SYMBOL(bm_portal_get);
+
+const struct bm_portal_config *bm_portal_config(const struct bm_portal *portal)
+{
+	return &portal->config;
+}
+EXPORT_SYMBOL(bm_portal_config);
+
+int bm_pool_new(u32 *bpid)
+{
+	int ret = 0, b = 64;
+	spin_lock(&bind_lock);
+	if (num_pools > 63)
+		ret = -ENOMEM;
+	else {
+		while (b-- && bman_depletion_get(&pools, b))
+			;
+		BUG_ON(b < 0);
+		bman_depletion_set(&pools, b);
+		*bpid = b;
+		num_pools++;
+	}
+	spin_unlock(&bind_lock);
+	return ret;
+}
+EXPORT_SYMBOL(bm_pool_new);
+
+void bm_pool_free(u32 bpid)
+{
+	spin_lock(&bind_lock);
+	BUG_ON(bpid > 63);
+	BUG_ON(!bman_depletion_get(&pools, bpid));
+	bman_depletion_unset(&pools, bpid);
+	num_pools--;
+	spin_unlock(&bind_lock);
+}
+EXPORT_SYMBOL(bm_pool_free);
+
+static int __init fsl_bman_portal_init(struct device_node *node)
+{
+	struct resource res[2];
+	struct bm_portal_config cfg;
+	struct bm_addr addr;
+	struct bm_portal *portal;
+	const phandle *cpu_ph = NULL;
+#ifndef CONFIG_FSL_BMAN_PORTAL_DISABLEAUTO
+	struct bman_portal *affine_portal;
+#endif
+	int irq, ret;
+
+	ret = of_address_to_resource(node, 0, &res[0]);
+	if (ret) {
+		pr_err("Can't get %s property 'reg::CE'\n", node->full_name);
+		return ret;
+	}
+	ret = of_address_to_resource(node, 1, &res[1]);
+	if (ret) {
+		pr_err("Can't get %s property 'reg::CI'\n", node->full_name);
+		return ret;
+	}
+	irq = irq_of_parse_and_map(node, 0);
+	if (irq == NO_IRQ) {
+		pr_err("Can't get %s property 'interrupts'\n", node->full_name);
+		return -ENODEV;
+	}
+	addr.addr_ce = ioremap_flags(res[0].start,
+				res[0].end - res[0].start + 1, 0);
+	addr.addr_ci = ioremap_flags(res[1].start,
+				res[1].end - res[1].start + 1,
+				_PAGE_GUARDED | _PAGE_NO_CACHE);
+	cfg.irq = irq;
+	cfg.cpu = -1;
+	cpu_ph = of_get_property(node, "cpu-handle", &ret);
+	if (cpu_ph && (ret == sizeof(phandle))) {
+		const u32 *cpu_val;
+		struct device_node *cpu_node = of_find_node_by_phandle(*cpu_ph);
+		if (!cpu_node) {
+			pr_err("Bad %s property 'cpu-handle'\n",
+				cpu_node->full_name);
+			goto bad_cpu_ph;
+		}
+		cpu_val = of_get_property(cpu_node, "reg", &ret);
+		if (!cpu_val || (ret != sizeof(*cpu_val)))
+			pr_err("Can't get %s property 'reg'\n",
+				cpu_node->full_name);
+		else {
+			int cpu;
+			bool invalid = true;
+
+			for_each_present_cpu(cpu)
+				if (*cpu_val == get_hard_smp_processor_id(cpu)) {
+					invalid = false;
+					break;
+				}
+
+			if (invalid)
+				pr_err("Invalid cpu index %d in %s\n", *cpu_val,
+					cpu_node->full_name);
+			else
+				cfg.cpu = cpu;
+		}
+		of_node_put(cpu_node);
+	}
+bad_cpu_ph:
+	bman_depletion_fill(&cfg.mask);
+	cfg.bound = 0;
+	pr_info("Bman portal at %p:%p (%d)\n", addr.addr_ce, addr.addr_ci,
+		cfg.cpu);
+	portal = __bm_portal_add(&addr, &cfg);
+	/* If the portal is affine to a cpu and that cpu has no default affine
+	 * portal, auto-initialise this one for the job. */
+#ifndef CONFIG_FSL_BMAN_PORTAL_DISABLEAUTO
+	if (cfg.cpu == -1)
+		return 0;
+	affine_portal = per_cpu(bman_affine_portal, cfg.cpu);
+	if (!affine_portal) {
+		affine_portal = bman_create_portal(portal,
+			BMAN_PORTAL_FLAG_IRQ | BMAN_PORTAL_FLAG_IRQ_FAST,
+			&cfg.mask);
+		if (!affine_portal)
+			pr_err("Bman portal auto-initialisation failed\n");
+		else {
+			pr_info("Bman portal %d auto-initialised\n", cfg.cpu);
+			per_cpu(bman_affine_portal, cfg.cpu) = affine_portal;
+			num_affine_portals++;
+		}
+	}
+#endif
+	return 0;
+}
+
+static int __init fsl_bpool_init(struct device_node *node)
+{
+	int ret;
+	u32 *cfg, *thresh;
+	u32 *bpid = (u32 *)of_get_property(node, "fsl,bpid", &ret);
+	if (!bpid || (ret!= 4)) {
+		pr_err("Can't get %s property 'fsl,bpid'\n", node->full_name);
+		return -ENODEV;
+	}
+	thresh = (u32 *)of_get_property(node, "fsl,bpool-thresholds", &ret);
+	if (thresh) {
+		if (ret != 16) {
+			pr_err("Invalid %s property '%s'\n",
+				node->full_name, "fsl,bpool-thresholds");
+			return -ENODEV;
+		}
+#ifndef CONFIG_FSL_BMAN_CONFIG
+		pr_err("Ignoring %s property '%s', no CCSR support\n",
+			node->full_name, "fsl,bpool-thresholds");
+#endif
+	}
+	cfg = (u32 *)of_get_property(node, "fsl,bpool-cfg", &ret);
+	if (cfg && (!ret || (ret % 24))) {
+		pr_err("Invalid %s property '%s'\n", node->full_name,
+			"fsl,bpool-cfg");
+		return -ENODEV;
+	}
+#ifdef CONFIG_FSL_BMAN_CONFIG
+	if (cfg)
+		ret = __bm_pool_add(*bpid, cfg, ret / 24);
+	else
+		ret = __bm_pool_add(*bpid, NULL, 0);
+	if (ret) {
+		pr_err("Can't reserve bpid %d from node %s\n", *bpid,
+			node->full_name);
+		return ret;
+	}
+	if (thresh) {
+		ret = bm_pool_set(*bpid, thresh);
+		if (ret)
+			pr_err("No CCSR node for %s property '%s'\n",
+				node->full_name, "fsl,bpool-thresholds");
+	}
+#endif
+	return ret;
+}
+
+static __init int bman_init(void)
+{
+	struct device_node *dn;
+	if (!bman_have_ccsr()) {
+		/* If there's no CCSR, our bpid allocator is empty */
+		bman_depletion_fill(&pools);
+		num_pools = 64;
+	}
+	for_each_compatible_node(dn, NULL, "fsl,bman-portal") {
+		int ret = fsl_bman_portal_init(dn);
+		if (ret)
+			return ret;
+	}
+#ifndef CONFIG_FSL_BMAN_PORTAL_DISABLEAUTO
+	if (num_affine_portals == num_online_cpus()) {
+		for_each_compatible_node(dn, NULL, "fsl,bpool") {
+			int ret = fsl_bpool_init(dn);
+			if (ret)
+				return ret;
+		}
+	} else {
+		pr_err("Not all cpus have an affine Bman portal\n");
+		pr_err("Ignoring buffer pools\n");
+		pr_err("Expect Bman-dependent drivers to crash!\n");
+	}
+#endif
+	return 0;
+	pr_info("Bman driver initialised\n");
+}
+subsys_initcall(bman_init);
diff --git a/drivers/hwalloc/bman_high.c b/drivers/hwalloc/bman_high.c
new file mode 100644
index 0000000..9a0b542
--- /dev/null
+++ b/drivers/hwalloc/bman_high.c
@@ -0,0 +1,674 @@
+/* Copyright (c) 2008, 2009 Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/* TODO:
+ *
+ * - make RECOVER also handle incomplete mgmt-commands
+ */
+
+#include "bman_private.h"
+
+/* Compilation constants */
+#define RCR_THRESH	2	/* reread h/w CI when running out of space */
+#define RCR_ITHRESH	4	/* if RCR congests, interrupt threshold */
+
+/**************/
+/* Portal API */
+/**************/
+
+struct bman_portal {
+	struct bm_portal *p;
+	/* 2-element array. pools[0] is mask, pools[1] is snapshot. */
+	struct bman_depletion *pools;
+	u32 flags;	/* BMAN_PORTAL_FLAG_*** - static, caller-provided */
+	int thresh_set;
+	u32 slowpoll;	/* only used when interrupts are off */
+	wait_queue_head_t queue;
+	/* The wrap-around rcr_[prod|cons] counters are used to support
+	 * BMAN_RELEASE_FLAG_WAIT_SYNC. */
+	u32 rcr_prod, rcr_cons;
+	/* 64-entry hash-table of pool objects that are tracking depletion
+	 * entry/exit (ie. BMAN_POOL_FLAG_DEPLETION). This isn't fast-path, so
+	 * we're not fussy about cache-misses and so forth - whereas the above
+	 * members should all fit in one cacheline.
+	 * BTW, with 64 entries in the hash table and 64 buffer pools to track,
+	 * you'll never guess the hash-function ... */
+	struct bman_pool *cb[64];
+};
+
+/* GOTCHA: this object type refers to a pool, it isn't *the* pool. There may be
+ * more than one such object per Bman buffer pool, eg. if different users of the
+ * pool are operating via different portals. */
+struct bman_pool {
+	struct bman_pool_params params;
+	/* Used for hash-table admin when using depletion notifications. */
+	struct bman_portal *portal;
+	struct bman_pool *next;
+	/* stockpile state - NULL unless BMAN_POOL_FLAG_STOCKPILE is set */
+	struct bm_buffer *sp;
+	unsigned int sp_fill;
+};
+
+/* (De)Registration of depletion notification callbacks */
+static void depletion_link(struct bman_portal *portal, struct bman_pool *pool)
+{
+	pool->portal = portal;
+	local_irq_disable();
+	pool->next = portal->cb[pool->params.bpid];
+	portal->cb[pool->params.bpid] = pool;
+	if (!pool->next)
+		/* First object for that bpid on this portal, enable the BSCN
+		 * mask bit. */
+		bm_isr_bscn_mask(portal->p, pool->params.bpid, 1);
+	local_irq_enable();
+}
+static void depletion_unlink(struct bman_pool *pool)
+{
+	struct bman_pool *it, *last = NULL;
+	struct bman_pool **base = &pool->portal->cb[pool->params.bpid];
+	local_irq_disable();
+	it = *base;	/* <-- gotcha, don't do this prior to the irq_disable */
+	while (it != pool) {
+		last = it;
+		it = it->next;
+	}
+	if (!last)
+		*base = pool->next;
+	else
+		last->next = pool->next;
+	if (!last && !pool->next)
+		/* Last object for that bpid on this portal, disable the BSCN
+		 * mask bit. */
+		bm_isr_bscn_mask(pool->portal->p, pool->params.bpid, 0);
+	local_irq_enable();
+}
+
+static u32 __poll_portal_slow(struct bman_portal *p);
+static void __poll_portal_fast(struct bman_portal *p);
+
+/* Portal interrupt handler */
+static irqreturn_t portal_isr(int irq, void *ptr)
+{
+	struct bman_portal *p = ptr;
+#ifdef CONFIG_FSL_BMAN_CHECKING
+	if (unlikely(!(p->flags & BMAN_PORTAL_FLAG_IRQ))) {
+		pr_crit("Portal interrupt is supposed to be disabled!\n");
+		bm_isr_inhibit(p->p);
+		return IRQ_HANDLED;
+	}
+#endif
+	/* Only do fast-path handling if it's required */
+	if (p->flags & BMAN_PORTAL_FLAG_IRQ_FAST)
+		__poll_portal_fast(p);
+	__poll_portal_slow(p);
+	return IRQ_HANDLED;
+}
+
+struct bman_portal *bman_create_portal(struct bm_portal *__p,
+			u32 flags, const struct bman_depletion *pools)
+{
+	struct bman_portal *portal;
+	const struct bm_portal_config *config = bm_portal_config(__p);
+	int ret;
+
+	portal = kmalloc(sizeof(*portal), GFP_KERNEL);
+	if (!portal)
+		return NULL;
+	if (bm_rcr_init(__p, bm_rcr_pvb, bm_rcr_cci)) {
+		pr_err("Bman RCR initialisation failed\n");
+		goto fail_rcr;
+	}
+	if (bm_mc_init(__p)) {
+		pr_err("Bman MC initialisation failed\n");
+		goto fail_mc;
+	}
+	if (bm_isr_init(__p)) {
+		pr_err("Bman ISR initialisation failed\n");
+		goto fail_isr;
+	}
+	portal->p = __p;
+	if (!pools)
+		portal->pools = NULL;
+	else {
+		u8 bpid = 0;
+		portal->pools = kmalloc(2 * sizeof(*pools), GFP_KERNEL);
+		if (!portal->pools)
+			goto fail_pools;
+		portal->pools[0] = *pools;
+		bman_depletion_init(portal->pools + 1);
+		while (bpid < 64) {
+			/* Default to all BPIDs disabled, we enable as required
+			 * at run-time. */
+			bm_isr_bscn_mask(__p, bpid, 0);
+			bpid++;
+		}
+	}
+	portal->flags = flags;
+	portal->slowpoll = 0;
+	init_waitqueue_head(&portal->queue);
+	portal->rcr_prod = portal->rcr_cons = 0;
+	memset(&portal->cb, 0, sizeof(portal->cb));
+	/* Write-to-clear any stale interrupt status bits */
+	bm_isr_disable_write(portal->p, 0xffffffff);
+	bm_isr_enable_write(portal->p, BM_PIRQ_RCRI | BM_PIRQ_BSCN);
+	bm_isr_status_clear(portal->p, 0xffffffff);
+	if (flags & BMAN_PORTAL_FLAG_IRQ) {
+		if (request_irq(config->irq, portal_isr, 0, "Bman portal 0", portal)) {
+			pr_err("request_irq() failed\n");
+			goto fail_irq;
+		}
+		if ((config->cpu != -1) &&
+				irq_can_set_affinity(config->irq) &&
+				irq_set_affinity(config->irq,
+				     *get_cpu_mask(config->cpu))) {
+			pr_err("irq_set_affinity() failed\n");
+			goto fail_affinity;
+		}
+		/* Enable the bits that make sense */
+		bm_isr_uninhibit(portal->p);
+	} else
+		/* without IRQ, we can't block */
+		flags &= ~BMAN_PORTAL_FLAG_WAIT;
+	/* Need RCR to be empty before continuing */
+	bm_isr_disable_write(portal->p, ~BM_PIRQ_RCRI);
+	if (!(flags & BMAN_PORTAL_FLAG_RECOVER) ||
+			!(flags & BMAN_PORTAL_FLAG_WAIT))
+		ret = bm_rcr_get_fill(portal->p);
+	else if (flags & BMAN_PORTAL_FLAG_WAIT_INT)
+		ret = wait_event_interruptible(portal->queue,
+			!bm_rcr_get_fill(portal->p));
+	else {
+		wait_event(portal->queue, !bm_rcr_get_fill(portal->p));
+		ret = 0;
+	}
+	if (ret) {
+		pr_err("Bman RCR unclean, need recovery\n");
+		goto fail_rcr_empty;
+	}
+	bm_isr_disable_write(portal->p, 0);
+	return portal;
+fail_rcr_empty:
+fail_affinity:
+	if (flags & BMAN_PORTAL_FLAG_IRQ)
+		free_irq(config->irq, portal);
+fail_irq:
+	if (portal->pools)
+		kfree(portal->pools);
+fail_pools:
+	bm_isr_finish(__p);
+fail_isr:
+	bm_mc_finish(__p);
+fail_mc:
+	bm_rcr_finish(__p);
+fail_rcr:
+	kfree(portal);
+	return NULL;
+}
+
+void bman_destroy_portal(struct bman_portal *bm)
+{
+	bm_rcr_cci_update(bm->p);
+	if (bm->flags & BMAN_PORTAL_FLAG_IRQ)
+		free_irq(bm_portal_config(bm->p)->irq, bm);
+	if (bm->pools)
+		kfree(bm->pools);
+	bm_isr_finish(bm->p);
+	bm_mc_finish(bm->p);
+	bm_rcr_finish(bm->p);
+	kfree(bm);
+}
+
+/* When release logic waits on available RCR space, we need a global waitqueue
+ * in the case of "affine" use (as the waits wake on different cpus which means
+ * different portals - so we can't wait on any per-portal waitqueue). */
+static DECLARE_WAIT_QUEUE_HEAD(affine_queue);
+
+static u32 __poll_portal_slow(struct bman_portal *p)
+{
+	struct bman_depletion tmp;
+	u32 ret, is = bm_isr_status_read(p->p);
+	ret = is;
+
+	/* There is a gotcha to be aware of. If we do the query before clearing
+	 * the status register, we may miss state changes that occur between the
+	 * two. If we write to clear the status register before the query, the
+	 * cache-enabled query command may overtake the status register write
+	 * unless we use a heavyweight sync (which we don't want). Instead, we
+	 * write-to-clear the status register then *read it back* before doing
+	 * the query, hence the odd while loop with the 'is' accumulation. */
+	if (is & BM_PIRQ_BSCN) {
+		struct bm_mc_result *mcr;
+		unsigned int i, j;
+		u32 __is;
+		bm_isr_status_clear(p->p, BM_PIRQ_BSCN);
+		while ((__is = bm_isr_status_read(p->p)) & BM_PIRQ_BSCN) {
+			is |= __is;
+			bm_isr_status_clear(p->p, BM_PIRQ_BSCN);
+		}
+		is &= ~BM_PIRQ_BSCN;
+		local_irq_disable();
+		bm_mc_start(p->p);
+		bm_mc_commit(p->p, BM_MCC_VERB_CMD_QUERY);
+		while (!(mcr = bm_mc_result(p->p)))
+			cpu_relax();
+		tmp = mcr->query.ds.state;
+		local_irq_enable();
+		for (i = 0; i < 2; i++) {
+			int idx = i * 32;
+			/* tmp is a mask of currently-depleted pools.
+			 * pools[0] is mask of those we care about.
+			 * pools[1] is our previous view (we only want to
+			 * be told about changes). */
+			tmp.__state[i] &= p->pools[0].__state[i];
+			if (tmp.__state[i] == p->pools[1].__state[i])
+				/* fast-path, nothing to see, move along */
+				continue;
+			for (j = 0; j <= 31; j++, idx++) {
+				struct bman_pool *pool = p->cb[idx];
+				int b4 = bman_depletion_get(&p->pools[1], idx);
+				int af = bman_depletion_get(&tmp, idx);
+				if (b4 == af)
+					continue;
+				while (pool) {
+					pool->params.cb(p, pool,
+						pool->params.cb_ctx, af);
+					pool = pool->next;
+				}
+			}
+		}
+		p->pools[1] = tmp;
+	}
+
+	if (is & BM_PIRQ_RCRI) {
+		local_irq_disable();
+		p->rcr_cons += bm_rcr_cci_update(p->p);
+		bm_rcr_set_ithresh(p->p, 0);
+		wake_up(&p->queue);
+		local_irq_enable();
+		bm_isr_status_clear(p->p, BM_PIRQ_RCRI);
+		is &= ~BM_PIRQ_RCRI;
+	}
+
+	/* There should be no status register bits left undefined */
+	BM_ASSERT(!is);
+	return ret;
+}
+
+static void __poll_portal_fast(struct bman_portal *p)
+{
+	/* nothing yet, this is where we'll put optimised RCR consumption
+	 * tracking */
+}
+
+/* In the case that slow- and fast-path handling are both done by bman_poll()
+ * (ie. because there is no interrupt handling), we ought to balance how often
+ * we do the fast-path poll versus the slow-path poll. We'll use two decrementer
+ * sources, so we call the fast poll 'n' times before calling the slow poll
+ * once. The idle decrementer constant is used when the last slow-poll detected
+ * no work to do, and the busy decrementer constant when the last slow-poll had
+ * work to do. */
+#define SLOW_POLL_IDLE   1000
+#define SLOW_POLL_BUSY   10
+void bman_poll(void)
+{
+	struct bman_portal *p = get_affine_portal();
+	if (!(p->flags & BMAN_PORTAL_FLAG_IRQ)) {
+		/* we handle slow- and fast-path */
+		__poll_portal_fast(p);
+		if (!(p->slowpoll--)) {
+			u32 active = __poll_portal_slow(p);
+			if (active)
+				p->slowpoll = SLOW_POLL_BUSY;
+			else
+				p->slowpoll = SLOW_POLL_IDLE;
+		}
+	} else if (!(p->flags & BMAN_PORTAL_FLAG_IRQ_FAST))
+		/* we handle fast-path only */
+		__poll_portal_fast(p);
+	put_affine_portal();
+}
+EXPORT_SYMBOL(bman_poll);
+
+static const u32 zero_thresholds[4] = {0, 0, 0, 0};
+
+struct bman_pool *bman_new_pool(const struct bman_pool_params *params)
+{
+	struct bman_pool *pool = NULL;
+	u32 bpid;
+
+	if (params->flags & BMAN_POOL_FLAG_DYNAMIC_BPID) {
+#ifdef CONFIG_FSL_BMAN_CONFIG
+		int ret = bm_pool_new(&bpid);
+		if (ret)
+			return NULL;
+#else
+		pr_err("No dynamic BPID allocator available\n");
+		return NULL;
+#endif
+	} else
+		bpid = params->bpid;
+#ifdef CONFIG_FSL_BMAN_CONFIG
+	if (params->flags & BMAN_POOL_FLAG_THRESH) {
+		int ret;
+		BUG_ON(!(params->flags & BMAN_POOL_FLAG_DYNAMIC_BPID));
+		ret = bm_pool_set(bpid, params->thresholds);
+		if (ret)
+			goto err;
+	} else
+		/* ignore result, if it fails, there was no CCSR */
+		bm_pool_set(bpid, zero_thresholds);
+#else
+	if (params->flags & BMAN_POOL_FLAG_THRESH)
+		goto err;
+#endif
+	pool = kmalloc(sizeof(*pool), GFP_KERNEL);
+	if (!pool)
+		goto err;
+	pool->sp = NULL;
+	pool->sp_fill = 0;
+	pool->params = *params;
+	if (params->flags & BMAN_POOL_FLAG_DYNAMIC_BPID)
+		pool->params.bpid = bpid;
+	if (params->flags & BMAN_POOL_FLAG_STOCKPILE) {
+		pool->sp = kmalloc(sizeof(struct bm_buffer) * BMAN_STOCKPILE_SZ,
+					GFP_KERNEL);
+		if (!pool->sp)
+			goto err;
+	}
+	if (pool->params.flags & BMAN_POOL_FLAG_DEPLETION) {
+		struct bman_portal *p = get_affine_portal();
+		if (!p->pools || !bman_depletion_get(&p->pools[0], bpid)) {
+			pr_err("Depletion events disabled for bpid %d\n", bpid);
+			goto err;
+		}
+		depletion_link(p, pool);
+		put_affine_portal();
+	}
+	return pool;
+err:
+#ifdef CONFIG_FSL_BMAN_CONFIG
+	if (params->flags & BMAN_POOL_FLAG_THRESH)
+		bm_pool_set(bpid, zero_thresholds);
+	if (params->flags & BMAN_POOL_FLAG_DYNAMIC_BPID)
+		bm_pool_free(bpid);
+#endif
+	if (pool) {
+		if (pool->sp)
+			kfree(pool->sp);
+		kfree(pool);
+	}
+	return NULL;
+}
+EXPORT_SYMBOL(bman_new_pool);
+
+void bman_free_pool(struct bman_pool *pool)
+{
+#ifdef CONFIG_FSL_BMAN_CONFIG
+	if (pool->params.flags & BMAN_POOL_FLAG_THRESH)
+		bm_pool_set(pool->params.bpid, zero_thresholds);
+#endif
+	if (pool->params.flags & BMAN_POOL_FLAG_DEPLETION)
+		depletion_unlink(pool);
+#ifdef CONFIG_FSL_BMAN_CONFIG
+	if (pool->params.flags & BMAN_POOL_FLAG_DYNAMIC_BPID)
+		bm_pool_free(pool->params.bpid);
+#endif
+	kfree(pool);
+}
+EXPORT_SYMBOL(bman_free_pool);
+
+const struct bman_pool_params *bman_get_params(const struct bman_pool *pool)
+{
+	return &pool->params;
+}
+EXPORT_SYMBOL(bman_get_params);
+
+static inline void rel_set_thresh(struct bman_portal *p, int check)
+{
+	if (!check || !bm_rcr_get_ithresh(p->p))
+		bm_rcr_set_ithresh(p->p, RCR_ITHRESH);
+}
+
+/* Used as a wait_event() expression. If it returns non-NULL, any lock will
+ * remain held. */
+static struct bm_rcr_entry *try_rel_start(struct bman_portal **p)
+{
+	struct bm_rcr_entry *r;
+	*p = get_affine_portal();
+	if (unlikely(!*p)) {
+		put_affine_portal();
+		return NULL;
+	}
+	local_irq_disable();
+	if (bm_rcr_get_avail((*p)->p) < RCR_THRESH)
+		bm_rcr_cci_update((*p)->p);
+	r = bm_rcr_start((*p)->p);
+	if (unlikely(!r)) {
+		rel_set_thresh(*p, 1);
+		local_irq_enable();
+		put_affine_portal();
+	}
+	return r;
+}
+
+static inline int wait_rel_start(struct bman_portal **p,
+			struct bm_rcr_entry **rel, u32 flags)
+{
+	int ret = 0;
+	if (flags & BMAN_RELEASE_FLAG_WAIT_INT)
+		ret = wait_event_interruptible(affine_queue,
+				(*rel = try_rel_start(p)));
+	else
+		wait_event(affine_queue, (*rel = try_rel_start(p)));
+	return ret;
+}
+
+/* This copies Qman's eqcr_completed() routine, see that for details */
+static int rel_completed(struct bman_portal *p, u32 rcr_poll)
+{
+	u32 tr_cons = p->rcr_cons;
+	if (rcr_poll & 0xc0000000) {
+		rcr_poll &= 0x7fffffff;
+		tr_cons ^= 0x80000000;
+	}
+	if (tr_cons >= rcr_poll)
+		return 1;
+	if ((rcr_poll - tr_cons) > BM_RCR_SIZE)
+		return 1;
+	if (!bm_rcr_get_fill(p->p))
+		/* If RCR is empty, we must have completed */
+		return 1;
+	rel_set_thresh(p, 0);
+	return 0;
+}
+
+static inline void rel_commit(struct bman_portal *p, u32 flags, u8 num)
+{
+	u32 rcr_poll;
+	bm_rcr_pvb_commit(p->p, BM_RCR_VERB_CMD_BPID_SINGLE |
+			(num & BM_RCR_VERB_BUFCOUNT_MASK));
+	/* increment the producer count and capture it for SYNC */
+	rcr_poll = ++p->rcr_prod;
+	if ((flags & BMAN_RELEASE_FLAG_WAIT_SYNC) ==
+			BMAN_RELEASE_FLAG_WAIT_SYNC)
+		rel_set_thresh(p, 1);
+	local_irq_enable();
+	put_affine_portal();
+	if ((flags & BMAN_RELEASE_FLAG_WAIT_SYNC) !=
+			BMAN_RELEASE_FLAG_WAIT_SYNC)
+		return;
+	/* So we're supposed to wait until the commit is consumed */
+	if (flags & BMAN_RELEASE_FLAG_WAIT_INT)
+		/* See bman_release() as to why we're ignoring return codes
+		 * from wait_***(). */
+		wait_event_interruptible(affine_queue,
+					rel_completed(p, rcr_poll));
+	else
+		wait_event(affine_queue, rel_completed(p, rcr_poll));
+}
+
+static inline int __bman_release(struct bman_pool *pool,
+			const struct bm_buffer *bufs, u8 num, u32 flags)
+{
+	struct bman_portal *p;
+	struct bm_rcr_entry *r;
+	u8 i;
+
+	/* FIXME: I'm ignoring BMAN_PORTAL_FLAG_COMPACT for now. */
+	r = try_rel_start(&p);
+	if (unlikely(!r)) {
+		if (flags & BMAN_RELEASE_FLAG_WAIT) {
+			int ret = wait_rel_start(&p, &r, flags);
+			if (ret)
+				return ret;
+		} else
+			return -EBUSY;
+		BM_ASSERT(r != NULL);
+	}
+	r->bpid = pool->params.bpid;
+	for (i = 0; i < num; i++) {
+		r->bufs[i].hi = bufs[i].hi;
+		r->bufs[i].lo = bufs[i].lo;
+	}
+	/* Issue the release command and wait for sync if requested. NB: the
+	 * commit can't fail, only waiting can. Don't propogate any failure if a
+	 * signal arrives, otherwise the caller can't distinguish whether the
+	 * release was issued or not. Code for user-space can check
+	 * signal_pending() after we return. */
+	rel_commit(p, flags, num);
+	return 0;
+}
+
+int bman_release(struct bman_pool *pool, const struct bm_buffer *bufs, u8 num,
+			u32 flags)
+{
+#ifdef CONFIG_FSL_BMAN_CHECKING
+	if (!num || (num > 8))
+		return -EINVAL;
+	if (pool->params.flags & BMAN_POOL_FLAG_NO_RELEASE)
+		return -EINVAL;
+#endif
+	/* Without stockpile, this API is a pass-through to the h/w operation */
+	if (!(pool->params.flags & BMAN_POOL_FLAG_STOCKPILE))
+		return __bman_release(pool, bufs, num, flags);
+	/* This needs some explanation. Adding the given buffers may take the
+	 * stockpile over the threshold, but in fact the stockpile may already
+	 * *be* over the threshold if a previous release-to-hw attempt had
+	 * failed. So we have 3 cases to cover;
+	 *   1. we add to the stockpile and don't hit the threshold,
+	 *   2. we add to the stockpile, hit the threshold and release-to-hw,
+	 *   3. we have to release-to-hw before adding to the stockpile
+	 *      (not enough room in the stockpile for case 2).
+	 * Our constraints on thresholds guarantee that in case 3, there must be
+	 * at least 8 bufs already in the stockpile, so all release-to-hw ops
+	 * are for 8 bufs. Despite all this, the API must indicate whether the
+	 * given buffers were taken off the caller's hands, irrespective of
+	 * whether a release-to-hw was attempted. */
+	while (num) {
+		/* Add buffers to stockpile if they fit */
+		if ((pool->sp_fill + num) < BMAN_STOCKPILE_SZ) {
+			memcpy(pool->sp + pool->sp_fill, bufs,
+				sizeof(struct bm_buffer) * num);
+			pool->sp_fill += num;
+			num = 0; /* --> will return success no matter what */
+		}
+		/* Do hw op if hitting the high-water threshold */
+		if ((pool->sp_fill + num) >= BMAN_STOCKPILE_HIGH) {
+			u8 ret = __bman_release(pool,
+				pool->sp + (pool->sp_fill - 8), 8, flags);
+			if (ret)
+				return (num ? ret : 0);
+			pool->sp_fill -= 8;
+		}
+	}
+	return 0;
+}
+EXPORT_SYMBOL(bman_release);
+
+static inline int __bman_acquire(struct bman_pool *pool, struct bm_buffer *bufs,
+					u8 num)
+{
+	struct bman_portal *p = get_affine_portal();
+	struct bm_mc_command *mcc;
+	struct bm_mc_result *mcr;
+	u8 ret;
+
+	local_irq_disable();
+	mcc = bm_mc_start(p->p);
+	mcc->acquire.bpid = pool->params.bpid;
+	bm_mc_commit(p->p, BM_MCC_VERB_CMD_ACQUIRE |
+			(num & BM_MCC_VERB_ACQUIRE_BUFCOUNT));
+	while (!(mcr = bm_mc_result(p->p)))
+		cpu_relax();
+	ret = num = mcr->verb & BM_MCR_VERB_ACQUIRE_BUFCOUNT;
+	while (num--) {
+		bufs[num].bpid = pool->params.bpid;
+		bufs[num].hi = mcr->acquire.bufs[num].hi;
+		bufs[num].lo = mcr->acquire.bufs[num].lo;
+	}
+	local_irq_enable();
+	put_affine_portal();
+	return ret;
+}
+
+int bman_acquire(struct bman_pool *pool, struct bm_buffer *bufs, u8 num,
+			u32 flags)
+{
+#ifdef CONFIG_FSL_BMAN_CHECKING
+	if (!num || (num > 8))
+		return -EINVAL;
+	if (pool->params.flags & BMAN_POOL_FLAG_ONLY_RELEASE)
+		return -EINVAL;
+#endif
+	/* Without stockpile, this API is a pass-through to the h/w operation */
+	if (!(pool->params.flags & BMAN_POOL_FLAG_STOCKPILE))
+		return __bman_acquire(pool, bufs, num);
+#ifdef CONFIG_SMP
+	panic("Bman stockpiles are not SMP-safe!");
+#endif
+	/* Only need a h/w op if we'll hit the low-water thresh */
+	if (!(flags & BMAN_ACQUIRE_FLAG_STOCKPILE) &&
+			(pool->sp_fill <= (BMAN_STOCKPILE_LOW + num))) {
+		u8 ret = __bman_acquire(pool, pool->sp + pool->sp_fill, 8);
+		if (!ret)
+			return -ENOMEM;
+		BUG_ON(ret != 8);
+		pool->sp_fill += 8;
+	} else if (pool->sp_fill < num)
+		return -ENOMEM;
+	memcpy(bufs, pool->sp + (pool->sp_fill - num),
+		sizeof(struct bm_buffer) * num);
+	pool->sp_fill -= num;
+	return num;
+}
+EXPORT_SYMBOL(bman_acquire);
+
diff --git a/drivers/hwalloc/bman_low.c b/drivers/hwalloc/bman_low.c
new file mode 100644
index 0000000..84a293d
--- /dev/null
+++ b/drivers/hwalloc/bman_low.c
@@ -0,0 +1,471 @@
+/* Copyright (c) 2008, 2009 Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "bman_private.h"
+
+/***************************/
+/* Portal register assists */
+/***************************/
+
+/* Cache-inhibited register offsets */
+#define REG_RCR_PI_CINH		(void *)0x0000
+#define REG_RCR_CI_CINH		(void *)0x0004
+#define REG_RCR_ITR		(void *)0x0008
+#define REG_CFG			(void *)0x0100
+#define REG_SCN(n)		((void *)(0x0200 + ((n) << 2)))
+#define REG_ISR			(void *)0x0e00
+
+/* Cache-enabled register offsets */
+#define CL_CR			(void *)0x0000
+#define CL_RR0			(void *)0x0100
+#define CL_RR1			(void *)0x0140
+#define CL_RCR			(void *)0x1000
+#define CL_RCR_PI_CENA		(void *)0x3000
+#define CL_RCR_CI_CENA		(void *)0x3100
+
+/* The h/w design requires mappings to be size-aligned so that "add"s can be
+ * reduced to "or"s. The primitives below do the same for s/w. */
+
+/* Bitwise-OR two pointers */
+static inline void *ptr_OR(void *a, void *b)
+{
+	return (void *)((unsigned long)a | (unsigned long)b);
+}
+
+/* Cache-inhibited register access */
+static inline u32 __bm_in(struct bm_addr *bm, void *offset)
+{
+	return in_be32(ptr_OR(bm->addr_ci, offset));
+}
+static inline void __bm_out(struct bm_addr *bm, void *offset, u32 val)
+{
+	out_be32(ptr_OR(bm->addr_ci, offset), val);
+}
+#define bm_in(reg)		__bm_in(&portal->addr, REG_##reg)
+#define bm_out(reg, val)	__bm_out(&portal->addr, REG_##reg, val)
+
+/* Convert 'n' cachelines to a pointer value for bitwise OR */
+#define bm_cl(n)		(void *)((n) << 6)
+
+/* Cache-enabled (index) register access */
+static inline void __bm_cl_touch_ro(struct bm_addr *bm, void *offset)
+{
+	dcbt_ro(ptr_OR(bm->addr_ce, offset));
+}
+static inline void __bm_cl_touch_rw(struct bm_addr *bm, void *offset)
+{
+	dcbt_rw(ptr_OR(bm->addr_ce, offset));
+}
+static inline u32 __bm_cl_in(struct bm_addr *bm, void *offset)
+{
+	return in_be32(ptr_OR(bm->addr_ce, offset));
+}
+static inline void __bm_cl_out(struct bm_addr *bm, void *offset, u32 val)
+{
+	out_be32(ptr_OR(bm->addr_ce, offset), val);
+	dcbf(ptr_OR(bm->addr_ce, offset));
+}
+static inline void __bm_cl_invalidate(struct bm_addr *bm, void *offset)
+{
+	dcbi(ptr_OR(bm->addr_ce, offset));
+}
+#define bm_cl_touch_ro(reg)	__bm_cl_touch_ro(&portal->addr, CL_##reg##_CENA)
+#define bm_cl_touch_rw(reg)	__bm_cl_touch_rw(&portal->addr, CL_##reg##_CENA)
+#define bm_cl_in(reg)		__bm_cl_in(&portal->addr, CL_##reg##_CENA)
+#define bm_cl_out(reg, val)	__bm_cl_out(&portal->addr, CL_##reg##_CENA, val)
+#define bm_cl_invalidate(reg) __bm_cl_invalidate(&portal->addr, CL_##reg##_CENA)
+
+/* Cyclic helper for rings. FIXME: once we are able to do fine-grain perf
+ * analysis, look at using the "extra" bit in the ring index registers to avoid
+ * cyclic issues. */
+static inline u8 cyc_diff(u8 ringsize, u8 first, u8 last)
+{
+	/* 'first' is included, 'last' is excluded */
+	if (first <= last)
+		return last - first;
+	return ringsize + last - first;
+}
+
+/* --------------- */
+/* --- RCR API --- */
+
+/* Bit-wise logic to wrap a ring pointer by clearing the "carry bit" */
+#define RCR_CARRYCLEAR(p) \
+	(void *)((unsigned long)(p) & (~(unsigned long)(BM_RCR_SIZE << 6)))
+
+/* Bit-wise logic to convert a ring pointer to a ring index */
+static inline u8 RCR_PTR2IDX(struct bm_rcr_entry *e)
+{
+	return ((u32)e >> 6) & (BM_RCR_SIZE - 1);
+}
+
+/* Increment the 'cursor' ring pointer, taking 'vbit' into account */
+static inline void RCR_INC(struct bm_rcr *rcr)
+{
+	/* NB: this is odd-looking, but experiments show that it generates
+	 * fast code with essentially no branching overheads. We increment to
+	 * the next RCR pointer and handle overflow and 'vbit'. */
+	struct bm_rcr_entry *partial = rcr->cursor + 1;
+	rcr->cursor = RCR_CARRYCLEAR(partial);
+	if (partial != rcr->cursor)
+		rcr->vbit ^= BM_RCR_VERB_VBIT;
+}
+
+/* It's safer to code in terms of the 'rcr' object than the 'portal' object,
+ * because the latter runs the risk of copy-n-paste errors from other code where
+ * we could manipulate some other structure within 'portal'. */
+#define rcr	(&portal->rcr)
+
+int bm_rcr_init(struct bm_portal *portal, enum bm_rcr_pmode pmode,
+		enum bm_rcr_cmode cmode)
+{
+	u32 cfg;
+	u8 pi;
+
+	if (__bm_portal_bind(portal, BM_BIND_RCR))
+		return -EBUSY;
+	rcr->ring = ptr_OR(portal->addr.addr_ce, CL_RCR);
+	rcr->ci = bm_in(RCR_CI_CINH) & (BM_RCR_SIZE - 1);
+	pi = bm_in(RCR_PI_CINH) & (BM_RCR_SIZE - 1);
+	rcr->cursor = rcr->ring + pi;
+	rcr->vbit = (bm_in(RCR_PI_CINH) & BM_RCR_SIZE) ?  BM_RCR_VERB_VBIT : 0;
+	rcr->available = BM_RCR_SIZE - 1 - cyc_diff(BM_RCR_SIZE, rcr->ci, pi);
+	rcr->ithresh = bm_in(RCR_ITR);
+#ifdef CONFIG_FSL_BMAN_CHECKING
+	rcr->busy = 0;
+	rcr->pmode = pmode;
+	rcr->cmode = cmode;
+#endif
+	cfg = (bm_in(CFG) & 0xffffffe0) | (pmode & 0x3); /* BCSP_CFG::RPM */
+	bm_out(CFG, cfg);
+	return 0;
+}
+EXPORT_SYMBOL(bm_rcr_init);
+
+void bm_rcr_finish(struct bm_portal *portal)
+{
+	u8 pi = bm_in(RCR_PI_CINH) & (BM_RCR_SIZE - 1);
+	u8 ci = bm_in(RCR_CI_CINH) & (BM_RCR_SIZE - 1);
+	BM_ASSERT(!rcr->busy);
+	if (pi != RCR_PTR2IDX(rcr->cursor))
+		pr_crit("losing uncommited RCR entries\n");
+	if (ci != rcr->ci)
+		pr_crit("missing existing RCR completions\n");
+	if (rcr->ci != RCR_PTR2IDX(rcr->cursor))
+		pr_crit("RCR destroyed unquiesced\n");
+	__bm_portal_unbind(portal, BM_BIND_RCR);
+}
+EXPORT_SYMBOL(bm_rcr_finish);
+
+struct bm_rcr_entry *bm_rcr_start(struct bm_portal *portal)
+{
+	BM_ASSERT(!rcr->busy);
+	if (!rcr->available)
+		return NULL;
+#ifdef CONFIG_FSL_BMAN_CHECKING
+	rcr->busy = 1;
+#endif
+	dcbzl(rcr->cursor);
+	return rcr->cursor;
+}
+EXPORT_SYMBOL(bm_rcr_start);
+
+void bm_rcr_abort(struct bm_portal *portal)
+{
+	BM_ASSERT(rcr->busy);
+#ifdef CONFIG_FSL_BMAN_CHECKING
+	rcr->busy = 0;
+#endif
+}
+EXPORT_SYMBOL(bm_rcr_abort);
+
+struct bm_rcr_entry *bm_rcr_pend_and_next(struct bm_portal *portal, u8 myverb)
+{
+	BM_ASSERT(rcr->busy);
+	BM_ASSERT(rcr->pmode != bm_rcr_pvb);
+	if (rcr->available == 1)
+		return NULL;
+	rcr->cursor->__dont_write_directly__verb = myverb | rcr->vbit;
+	dcbf(rcr->cursor);
+	RCR_INC(rcr);
+	rcr->available--;
+	dcbzl(rcr->cursor);
+	return rcr->cursor;
+}
+EXPORT_SYMBOL(bm_rcr_pend_and_next);
+
+void bm_rcr_pci_commit(struct bm_portal *portal, u8 myverb)
+{
+	BM_ASSERT(rcr->busy);
+	BM_ASSERT(rcr->pmode == bm_rcr_pci);
+	rcr->cursor->__dont_write_directly__verb = myverb | rcr->vbit;
+	RCR_INC(rcr);
+	rcr->available--;
+	hwsync();
+	bm_out(RCR_PI_CINH, RCR_PTR2IDX(rcr->cursor));
+#ifdef CONFIG_FSL_BMAN_CHECKING
+	rcr->busy = 0;
+#endif
+}
+EXPORT_SYMBOL(bm_rcr_pci_commit);
+
+void bm_rcr_pce_prefetch(struct bm_portal *portal)
+{
+	BM_ASSERT(rcr->pmode == bm_rcr_pce);
+	bm_cl_invalidate(RCR_PI);
+	bm_cl_touch_rw(RCR_PI);
+}
+EXPORT_SYMBOL(bm_rcr_pce_prefetch);
+
+void bm_rcr_pce_commit(struct bm_portal *portal, u8 myverb)
+{
+	BM_ASSERT(rcr->busy);
+	BM_ASSERT(rcr->pmode == bm_rcr_pce);
+	rcr->cursor->__dont_write_directly__verb = myverb | rcr->vbit;
+	RCR_INC(rcr);
+	rcr->available--;
+	lwsync();
+	bm_cl_out(RCR_PI, RCR_PTR2IDX(rcr->cursor));
+#ifdef CONFIG_FSL_BMAN_CHECKING
+	rcr->busy = 0;
+#endif
+}
+EXPORT_SYMBOL(bm_rcr_pce_commit);
+
+void bm_rcr_pvb_commit(struct bm_portal *portal, u8 myverb)
+{
+	BM_ASSERT(rcr->busy);
+	BM_ASSERT(rcr->pmode == bm_rcr_pvb);
+	lwsync();
+	rcr->cursor->__dont_write_directly__verb = myverb | rcr->vbit;
+	dcbf(rcr->cursor);
+	RCR_INC(rcr);
+	rcr->available--;
+#ifdef CONFIG_FSL_BMAN_CHECKING
+	rcr->busy = 0;
+#endif
+}
+EXPORT_SYMBOL(bm_rcr_pvb_commit);
+
+u8 bm_rcr_cci_update(struct bm_portal *portal)
+{
+	u8 diff, old_ci = rcr->ci;
+	BM_ASSERT(rcr->cmode == bm_rcr_cci);
+	rcr->ci = bm_in(RCR_CI_CINH) & (BM_RCR_SIZE - 1);
+	diff = cyc_diff(BM_RCR_SIZE, old_ci, rcr->ci);
+	rcr->available += diff;
+	return diff;
+}
+EXPORT_SYMBOL(bm_rcr_cci_update);
+
+void bm_rcr_cce_prefetch(struct bm_portal *portal)
+{
+	BM_ASSERT(rcr->cmode == bm_rcr_cce);
+	bm_cl_invalidate(RCR_CI);
+	bm_cl_touch_ro(RCR_CI);
+}
+EXPORT_SYMBOL(bm_rcr_cce_prefetch);
+
+u8 bm_rcr_cce_update(struct bm_portal *portal)
+{
+	u8 diff, old_ci = rcr->ci;
+	BM_ASSERT(rcr->cmode == bm_rcr_cce);
+	rcr->ci = bm_cl_in(RCR_CI) & (BM_RCR_SIZE - 1);
+	bm_cl_invalidate(RCR_CI);
+	diff = cyc_diff(BM_RCR_SIZE, old_ci, rcr->ci);
+	rcr->available += diff;
+	return diff;
+}
+EXPORT_SYMBOL(bm_rcr_cce_update);
+
+u8 bm_rcr_get_ithresh(struct bm_portal *portal)
+{
+	return rcr->ithresh;
+}
+EXPORT_SYMBOL(bm_rcr_get_ithresh);
+
+void bm_rcr_set_ithresh(struct bm_portal *portal, u8 ithresh)
+{
+	rcr->ithresh = ithresh;
+	bm_out(RCR_ITR, ithresh);
+}
+EXPORT_SYMBOL(bm_rcr_set_ithresh);
+
+u8 bm_rcr_get_avail(struct bm_portal *portal)
+{
+	return rcr->available;
+}
+EXPORT_SYMBOL(bm_rcr_get_avail);
+
+u8 bm_rcr_get_fill(struct bm_portal *portal)
+{
+	return BM_RCR_SIZE - 1 - rcr->available;
+}
+EXPORT_SYMBOL(bm_rcr_get_fill);
+
+
+/* ------------------------------ */
+/* --- Management command API --- */
+
+/* It's safer to code in terms of the 'mc' object than the 'portal' object,
+ * because the latter runs the risk of copy-n-paste errors from other code where
+ * we could manipulate some other structure within 'portal'. */
+#define mc	(&portal->mc)
+
+int bm_mc_init(struct bm_portal *portal)
+{
+	if (__bm_portal_bind(portal, BM_BIND_MC))
+		return -EBUSY;
+	mc->cr = ptr_OR(portal->addr.addr_ce, CL_CR);
+	mc->rr = ptr_OR(portal->addr.addr_ce, CL_RR0);
+	mc->rridx = (mc->cr->__dont_write_directly__verb & BM_MCC_VERB_VBIT) ?
+			0 : 1;
+	mc->vbit = mc->rridx ? BM_MCC_VERB_VBIT : 0;
+#ifdef CONFIG_FSL_BMAN_CHECKING
+	mc->state = mc_idle;
+#endif
+	return 0;
+}
+EXPORT_SYMBOL(bm_mc_init);
+
+void bm_mc_finish(struct bm_portal *portal)
+{
+	BM_ASSERT(mc->state == mc_idle);
+#ifdef CONFIG_FSL_BMAN_CHECKING
+	if (mc->state != mc_idle)
+		pr_crit("Losing incomplete MC command\n");
+#endif
+	__bm_portal_unbind(portal, BM_BIND_MC);
+}
+EXPORT_SYMBOL(bm_mc_finish);
+
+struct bm_mc_command *bm_mc_start(struct bm_portal *portal)
+{
+	BM_ASSERT(mc->state == mc_idle);
+#ifdef CONFIG_FSL_BMAN_CHECKING
+	mc->state = mc_user;
+#endif
+	dcbzl(mc->cr);
+	return mc->cr;
+}
+EXPORT_SYMBOL(bm_mc_start);
+
+void bm_mc_abort(struct bm_portal *portal)
+{
+	BM_ASSERT(mc->state == mc_user);
+#ifdef CONFIG_FSL_BMAN_CHECKING
+	mc->state = mc_idle;
+#endif
+}
+EXPORT_SYMBOL(bm_mc_abort);
+
+void bm_mc_commit(struct bm_portal *portal, u8 myverb)
+{
+	BM_ASSERT(mc->state == mc_user);
+	dcbi(mc->rr + mc->rridx);
+	lwsync();
+	mc->cr->__dont_write_directly__verb = myverb | mc->vbit;
+	dcbf(mc->cr);
+	dcbt_ro(mc->rr + mc->rridx);
+#ifdef CONFIG_FSL_BMAN_CHECKING
+	mc->state = mc_hw;
+#endif
+}
+EXPORT_SYMBOL(bm_mc_commit);
+
+struct bm_mc_result *bm_mc_result(struct bm_portal *portal)
+{
+	struct bm_mc_result *rr = mc->rr + mc->rridx;
+	BM_ASSERT(mc->state == mc_hw);
+	/* The inactive response register's verb byte always returns zero until
+	 * its command is submitted and completed. This includes the valid-bit,
+	 * in case you were wondering... */
+	if (!rr->verb) {
+		dcbi(rr);
+		dcbt_ro(rr);
+		return NULL;
+	}
+	mc->rridx ^= 1;
+	mc->vbit ^= BM_MCC_VERB_VBIT;
+#ifdef CONFIG_FSL_BMAN_CHECKING
+	mc->state = mc_idle;
+#endif
+	return rr;
+}
+EXPORT_SYMBOL(bm_mc_result);
+
+
+/* ------------------------------------- */
+/* --- Portal interrupt register API --- */
+
+int bm_isr_init(struct bm_portal *portal)
+{
+	if (__bm_portal_bind(portal, BM_BIND_ISR))
+		return -EBUSY;
+	return 0;
+}
+EXPORT_SYMBOL(bm_isr_init);
+
+void bm_isr_finish(struct bm_portal *portal)
+{
+	__bm_portal_unbind(portal, BM_BIND_ISR);
+}
+EXPORT_SYMBOL(bm_isr_finish);
+
+#define SCN_REG(bpid) REG_SCN((bpid) / 32)
+#define SCN_BIT(bpid) (0x80000000 >> (bpid & 31))
+void bm_isr_bscn_mask(struct bm_portal *portal, u8 bpid, int enable)
+{
+	u32 val;
+	BM_ASSERT(bpid < 64);
+	/* REG_SCN for bpid=0..31, REG_SCN+4 for bpid=32..63 */
+	val = __bm_in(&portal->addr, SCN_REG(bpid));
+	if (enable)
+		val |= SCN_BIT(bpid);
+	else
+		val &= ~SCN_BIT(bpid);
+	__bm_out(&portal->addr, SCN_REG(bpid), val);
+}
+EXPORT_SYMBOL(bm_isr_bscn_mask);
+
+u32 __bm_isr_read(struct bm_portal *portal, enum bm_isr_reg n)
+{
+	return __bm_in(&portal->addr, REG_ISR + (n << 2));
+}
+EXPORT_SYMBOL(__bm_isr_read);
+
+void __bm_isr_write(struct bm_portal *portal, enum bm_isr_reg n, u32 val)
+{
+	__bm_out(&portal->addr, REG_ISR + (n << 2), val);
+}
+EXPORT_SYMBOL(__bm_isr_write);
+
diff --git a/drivers/hwalloc/bman_private.h b/drivers/hwalloc/bman_private.h
new file mode 100644
index 0000000..5f41426
--- /dev/null
+++ b/drivers/hwalloc/bman_private.h
@@ -0,0 +1,133 @@
+/* Copyright (c) 2008, 2009 Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "bman_sys.h"
+#include <linux/fsl_bman.h>
+
+struct bm_addr {
+	void __iomem *addr_ce;	/* cache-enabled */
+	void __iomem *addr_ci;	/* cache-inhibited */
+};
+
+/* RCR state */
+struct bm_rcr {
+	struct bm_rcr_entry *ring, *cursor;
+	u8 ci, available, ithresh, vbit;
+#ifdef CONFIG_FSL_BMAN_CHECKING
+	u32 busy;
+	enum bm_rcr_pmode pmode;
+	enum bm_rcr_cmode cmode;
+#endif
+};
+
+/* MC state */
+struct bm_mc {
+	struct bm_mc_command *cr;
+	struct bm_mc_result *rr;
+	u8 rridx, vbit;
+#ifdef CONFIG_FSL_BMAN_CHECKING
+	enum {
+		/* Can only be _mc_start()ed */
+		mc_idle,
+		/* Can only be _mc_commit()ed or _mc_abort()ed */
+		mc_user,
+		/* Can only be _mc_retry()ed */
+		mc_hw
+	} state;
+#endif
+};
+
+/********************/
+/* Portal structure */
+/********************/
+
+struct bm_portal {
+	struct bm_addr addr;
+	struct bm_rcr rcr;
+	struct bm_mc mc;
+	struct bm_portal_config config;
+} ____cacheline_aligned;
+
+/* RCR/MC/ISR code uses this as a locked mechanism to bind/unbind to
+ * bm_portal::config::bound. */
+int __bm_portal_bind(struct bm_portal *portal, u8 iface);
+void __bm_portal_unbind(struct bm_portal *portal, u8 iface);
+
+/* Hooks between qman_driver.c and qman_high.c */
+extern DEFINE_PER_CPU(struct bman_portal *, bman_affine_portal);
+static inline struct bman_portal *get_affine_portal(void)
+{
+	return get_cpu_var(bman_affine_portal);
+}
+static inline void put_affine_portal(void)
+{
+	put_cpu_var(bman_affine_portal);
+}
+#define BMAN_PORTAL_FLAG_IRQ         0x00000001 /* use interrupt handler */
+#define BMAN_PORTAL_FLAG_IRQ_FAST    0x00000002 /* ... for fast-path too! */
+#define BMAN_PORTAL_FLAG_COMPACT     0x00000004 /* use compaction algorithm */
+#define BMAN_PORTAL_FLAG_RECOVER     0x00000008 /* recovery mode */
+#define BMAN_PORTAL_FLAG_WAIT        0x00000010 /* wait if RCR is full */
+#define BMAN_PORTAL_FLAG_WAIT_INT    0x00000020 /* if wait, interruptible? */
+struct bman_portal *bman_create_portal(struct bm_portal *portal, u32 flags,
+					const struct bman_depletion *pools);
+void bman_destroy_portal(struct bman_portal *p);
+
+/* Pool logic in the portal driver, during initialisation, needs to know if
+ * there's access to CCSR or not (if not, it'll cripple the pool allocator). */
+#ifdef CONFIG_FSL_BMAN_CONFIG
+int bman_have_ccsr(void);
+#else
+#define bman_have_ccsr() 0
+#endif
+
+/* Stockpile build constants. The _LOW value: when bman_acquire() is called and
+ * the stockpile fill-level is <= _LOW, an acquire is attempted from h/w but it
+ * might fail (if the buffer pool is depleted). So this value provides some
+ * "stagger" in that the bman_acquire() function will only fail if lots of bufs
+ * are requested at once or if h/w has been tested a couple of times without
+ * luck. The _HIGH value: when bman_release() is called and the stockpile
+ * fill-level is >= _HIGH, a release is attempted to h/w but it might fail (if
+ * the release ring is full). So this value provides some "stagger" so that
+ * ring-access is retried a couple of times prior to the API returning a
+ * failure. The following *must* be true;
+ *   BMAN_STOCKPILE_HIGH-BMAN_STOCKPILE_LOW > 8
+ *     (to avoid thrashing)
+ *   BMAN_STOCKPILE_SZ >= 16
+ *     (as the release logic expects to either send 8 buffers to hw prior to
+ *     adding the given buffers to the stockpile or add the buffers to the
+ *     stockpile before sending 8 to hw, as the API must be an all-or-nothing
+ *     success/fail.)
+ */
+#define BMAN_STOCKPILE_SZ   16u /* number of bufs in per-pool cache */
+#define BMAN_STOCKPILE_LOW  2u  /* when fill is <= this, acquire from hw */
+#define BMAN_STOCKPILE_HIGH 14u /* when fill is >= this, release to hw */
diff --git a/drivers/hwalloc/bman_sys.h b/drivers/hwalloc/bman_sys.h
new file mode 100644
index 0000000..470d793
--- /dev/null
+++ b/drivers/hwalloc/bman_sys.h
@@ -0,0 +1,97 @@
+/* Copyright (c) 2008, 2009 Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/io.h>
+#include <linux/dma-mapping.h>
+#include <linux/bootmem.h>
+#include <linux/slab.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/interrupt.h>
+#include <linux/delay.h>
+#include <linux/of_platform.h>
+#include <linux/kthread.h>
+#include <linux/lmb.h>
+#include <linux/completion.h>
+#include <linux/log2.h>
+#include <linux/types.h>
+#include <linux/ioctl.h>
+#include <linux/miscdevice.h>
+#include <linux/uaccess.h>
+
+/* TODO: NB, we currently assume that hwsync() and lwsync() imply compiler
+ * barriers and that dcb*() won't fall victim to compiler or execution
+ * reordering with respect to other code/instructions that manipulate the same
+ * cacheline. */
+#define hwsync() \
+	do { \
+		__asm__ __volatile__ ("sync" : : : "memory"); \
+	} while(0)
+#define lwsync() \
+	do { \
+		__asm__ __volatile__ ("lwsync" : : : "memory"); \
+	} while(0)
+#define dcbzl(p) \
+	do { \
+		__asm__ __volatile__ ("dcbzl 0,%0" : : "r" (p)); \
+	} while(0)
+#define dcbf(p) \
+	do { \
+		__asm__ __volatile__ ("dcbf 0,%0" : : "r" (p)); \
+	} while(0)
+#define dcbt_ro(p) \
+	do { \
+		__asm__ __volatile__ ("dcbt 0,%0" : : "r" (p)); \
+	} while(0)
+#define dcbt_rw(p) \
+	do { \
+		__asm__ __volatile__ ("dcbtst 0,%0" : : "r" (p)); \
+	} while(0)
+#define dcbi(p) dcbf(p)
+
+#ifdef CONFIG_FSL_BMAN_CHECKING
+#define BM_ASSERT(x) \
+	do { \
+		if (!(x)) { \
+			pr_crit("ASSERT: (%s:%d) %s\n", __FILE__, __LINE__, \
+				__stringify_1(x)); \
+			dump_stack(); \
+			panic("assertion failure"); \
+		} \
+	} while(0)
+#else
+#define BM_ASSERT(x)
+#endif
+
+
diff --git a/drivers/hwalloc/bman_test.c b/drivers/hwalloc/bman_test.c
new file mode 100644
index 0000000..dc5ebfb
--- /dev/null
+++ b/drivers/hwalloc/bman_test.c
@@ -0,0 +1,78 @@
+/* Copyright (c) 2008, 2009 Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "bman_test.h"
+
+MODULE_AUTHOR("Geoff Thorpe");
+MODULE_LICENSE("Dual BSD/GPL");
+MODULE_DESCRIPTION("Bman testing");
+
+static int test_init(void)
+{
+	int loop;
+#ifdef CONFIG_FSL_BMAN_TEST_LOW
+	struct bm_portal *portal;
+	const struct bm_portal_config *config = NULL;
+	u8 num = bm_portal_num();
+
+	while (!config && (num-- > 0)) {
+		portal = bm_portal_get(num);
+		config = bm_portal_config(portal);
+		if (!config->bound)
+			pr_info("Portal %d is available, using it\n", num);
+		else
+			config = NULL;
+	}
+	if (!config) {
+		pr_err("No Bman portals available!\n");
+		return -ENOSYS;
+	}
+#endif
+	loop = 1;
+	while (loop--) {
+#ifdef CONFIG_FSL_BMAN_TEST_LOW
+		bman_test_low(portal);
+#endif
+#ifdef CONFIG_FSL_BMAN_TEST_HIGH
+		bman_test_high();
+#endif
+	}
+	return 0;
+}
+
+static void test_exit(void)
+{
+}
+
+module_init(test_init);
+module_exit(test_exit);
+
diff --git a/drivers/hwalloc/bman_test.h b/drivers/hwalloc/bman_test.h
new file mode 100644
index 0000000..ddb1e11
--- /dev/null
+++ b/drivers/hwalloc/bman_test.h
@@ -0,0 +1,92 @@
+/* Copyright (c) 2008, 2009 Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/io.h>
+#include <linux/slab.h>
+#include <linux/module.h>
+#include <linux/interrupt.h>
+#include <linux/delay.h>
+
+#include <linux/fsl_bman.h>
+
+void bman_test_low(struct bm_portal *portal);
+void bman_test_high(void);
+
+static inline void __hexdump(unsigned long start, unsigned long end,
+			unsigned long p, size_t sz, unsigned char *c)
+{
+	while (start < end) {
+		unsigned int pos = 0;
+		char buf[64];
+		int nl = 0;
+		pos += sprintf(buf + pos, "%08lx: ", start);
+		do {
+			if ((start < p) || (start >= (p + sz)))
+				pos += sprintf(buf + pos, "..");
+			else
+				pos += sprintf(buf + pos, "%02x", *(c++));
+			if (!(++start & 15)) {
+				buf[pos++] = '\n';
+				nl = 1;
+			} else {
+				nl = 0;
+				if(!(start & 1))
+					buf[pos++] = ' ';
+				if(!(start & 3))
+					buf[pos++] = ' ';
+			}
+		} while (start & 15);
+		if (!nl)
+			buf[pos++] = '\n';
+		buf[pos] = '\0';
+		pr_info("%s", buf);
+	}
+}
+static inline void hexdump(void *ptr, size_t sz)
+{
+	unsigned long p = (unsigned long)ptr;
+	unsigned long start = p & ~(unsigned long)15;
+	unsigned long end = (p + sz + 15) & ~(unsigned long)15;
+	unsigned char *c = ptr;
+	__hexdump(start, end, p, sz, c);
+}
+static inline void hexdump_by_cl(void *ptr, size_t sz)
+{
+	unsigned long p = (unsigned long)ptr;
+	unsigned long start = p & ~(unsigned long)63;
+	unsigned long end = (p + sz + 63) & ~(unsigned long)63;
+	unsigned char *c = ptr;
+	__hexdump(start, end, p, sz, c);
+}
+
diff --git a/drivers/hwalloc/bman_test_high.c b/drivers/hwalloc/bman_test_high.c
new file mode 100644
index 0000000..fa1a9c8
--- /dev/null
+++ b/drivers/hwalloc/bman_test_high.c
@@ -0,0 +1,183 @@
+/* Copyright (c) 2008, 2009 Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "bman_test.h"
+
+/*************/
+/* constants */
+/*************/
+
+#define PORTAL_OPAQUE	(void *)0xf00dbeef
+#define POOL_OPAQUE	(void *)0xdeadabba
+#define NUM_BUFS	93
+#define DEPLETION_ENTRY	40
+#define DEPLETION_EXIT	80
+#define LOOPS		3
+
+/***************/
+/* global vars */
+/***************/
+
+static struct bman_pool *pool;
+static int depleted;
+static struct bm_buffer bufs_in[NUM_BUFS] ____cacheline_aligned;
+static struct bm_buffer bufs_out[NUM_BUFS] ____cacheline_aligned;
+static int bufs_received;
+
+/* Predeclare the callback so we can instantiate pool parameters */
+static void depletion_cb(struct bman_portal *, struct bman_pool *, void *, int);
+
+/**********************/
+/* internal functions */
+/**********************/
+
+static void bufs_init(void)
+{
+	int i;
+	for (i = 0; i < NUM_BUFS; i++) {
+		bufs_in[i].hi = 0xfedc - i;
+		bufs_in[i].lo = 0xcccccccc + (0x11111111 * i);
+	}
+	bufs_received = 0;
+}
+
+static inline int bufs_cmp(const struct bm_buffer *a, const struct bm_buffer *b)
+{
+	if (a->hi < b->hi)
+		return -1;
+	if (a->hi > b->hi)
+		return 1;
+	if (a->lo < b->lo)
+		return -1;
+	if (a->lo > b->lo)
+		return 1;
+	return 0;
+}
+
+static void bufs_confirm(void)
+{
+	int i, j;
+	for (i = 0; i < NUM_BUFS; i++) {
+		int matches = 0;
+		for (j = 0; j < NUM_BUFS; j++)
+			if (!bufs_cmp(&bufs_in[i], &bufs_out[j]))
+				matches++;
+		BUG_ON(matches != 1);
+	}
+}
+
+/********/
+/* test */
+/********/
+
+static void depletion_cb(struct bman_portal *__portal, struct bman_pool *__pool,
+			void *pool_ctx, int __depleted)
+{
+	BUG_ON(__pool != pool);
+	BUG_ON(pool_ctx != POOL_OPAQUE);
+	depleted = __depleted;
+	pr_info("BMAN: depletion_cb: depleted=%d\n", depleted);
+}
+
+void bman_test_high(void)
+{
+	struct bman_pool_params pparams = {
+		.flags = BMAN_POOL_FLAG_DEPLETION |
+			BMAN_POOL_FLAG_DYNAMIC_BPID |
+			BMAN_POOL_FLAG_THRESH,
+		.cb = depletion_cb,
+		.cb_ctx = POOL_OPAQUE,
+		.thresholds = {
+			DEPLETION_ENTRY,
+			DEPLETION_EXIT,
+			0,
+			0
+		}
+	};
+	int i, loops = LOOPS;
+
+	bufs_init();
+
+	pr_info("BMAN:  --------------------------------\n");
+	pr_info("BMAN:  --- starting high-level test ---\n");
+	pr_info("BMAN:  --------------------------------\n");
+
+	pool = bman_new_pool(&pparams);
+	BUG_ON(!pool);
+
+	/*******************/
+	/* Release buffers */
+	/*******************/
+do_loop:
+	i = 0;
+	while (i < NUM_BUFS) {
+		u32 flags = BMAN_RELEASE_FLAG_WAIT;
+		int num = 8;
+		if ((i + num) > NUM_BUFS)
+			num = NUM_BUFS - i;
+		if ((i + num) == NUM_BUFS)
+			flags |= BMAN_RELEASE_FLAG_WAIT_SYNC;
+		if (bman_release(pool, bufs_in + i, num, flags))
+			panic("bman_release() failed\n");
+		i += num;
+		pr_info("BMAN: released %d buffers, total->%d\n", num, i);
+	}
+
+	/*******************/
+	/* Acquire buffers */
+	/*******************/
+	while (i > 0) {
+		int tmp, num = 8;
+		if (num > i)
+			num = i;
+		tmp = bman_acquire(pool, bufs_out + i - num, num, 0);
+		BUG_ON(tmp != num);
+		i -= num;
+		pr_info("BMAN: acquired %d buffers, total->%d\n", num, i);
+	}
+	i = bman_acquire(pool, NULL, 1, 0);
+	BUG_ON(i);
+
+	bufs_confirm();
+
+	if (--loops)
+		goto do_loop;
+
+	/************/
+	/* Clean up */
+	/************/
+	bman_free_pool(pool);
+	pr_info("BMAN:  --------------------------------\n");
+	pr_info("BMAN:  --- finished high-level test ---\n");
+	pr_info("BMAN:  --------------------------------\n");
+}
+
diff --git a/drivers/hwalloc/bman_test_low.c b/drivers/hwalloc/bman_test_low.c
new file mode 100644
index 0000000..17ae792
--- /dev/null
+++ b/drivers/hwalloc/bman_test_low.c
@@ -0,0 +1,292 @@
+/* Copyright (c) 2008, 2009 Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "bman_test.h"
+
+/* Assume this is broken, enable it afterwards */
+#define BUG_NO_RCR_ITR
+
+/* Test constants */
+#define TPORTAL		0
+#define TBUFS		10	/* Number released */
+#define TRELEASEMAX	8	/* Maximum released at once */
+#define TACQUIREMAX	8	/* Maximum acquired at once */
+#define TDENTRY		2
+#define TDEXIT		8
+#define TADDRHI		0xabba
+#define TADDRLO		0xdeadbeef
+
+/* Global variables make life simpler */
+static struct bm_portal *portal;
+static struct bm_rcr_entry *rc;
+static struct bm_mc_command *mcc;
+static struct bm_mc_result *mcr;
+static DEFINE_SPINLOCK(mc_lock);
+static const u32 test_thresholds[4] = {TDENTRY, TDEXIT, 0, 0};
+static DECLARE_WAIT_QUEUE_HEAD(queue);
+static int isr_count;
+
+/* Boolean switch for handling RCR_ITR */
+#ifndef BUG_NO_RCR_ITR
+static int rcr_thresh_on;
+#endif
+
+/* Bit-arrray representing releases to check against acquires. */
+static u32 bufs[(TBUFS + 31) / 32];
+static inline int BUFS_get(int idx)
+{
+	return (bufs[idx / 32] & (1 << (idx & 31))) ? 1 : 0;
+}
+static inline void BUFS_set(int idx)
+{
+	bufs[idx / 32] |= (1 << (idx & 31));
+}
+static inline void BUFS_unset(int idx)
+{
+	bufs[idx / 32] &= ~(u32)(1 << (idx & 31));
+}
+
+/* Helper for bm_mc_start() that checks the return code */
+static void mc_start(void)
+{
+	mcc = bm_mc_start(portal);
+	BUG_ON(!mcc);
+}
+
+/* Helper for bm_mc_result() that checks the response */
+static void mc_commit(u8 verb)
+{
+	bm_mc_commit(portal, verb);
+	do {
+		mcr = bm_mc_result(portal);
+	} while (!mcr);
+	BUG_ON((mcr->verb & BM_MCR_VERB_CMD_MASK) !=
+			(verb & BM_MCR_VERB_CMD_MASK));
+}
+
+/* Track RCR consumption */
+static void rcr_update(void)
+{
+#ifndef BUG_NO_RCR_ITR
+	u32 status;
+	u8 fill;
+	while ((fill = bm_rcr_get_fill(portal))) {
+		pr_info("rcr_update: fill==%d\n", fill);
+		bm_rcr_cci_update(portal);
+	}
+	status = bm_isr_status_read(portal);
+	if (status & BM_PIRQ_RCRI) {
+		BUG_ON(!rcr_thresh_on);
+		bm_rcr_set_ithresh(portal, 0);
+		rcr_thresh_on = 0;
+		pr_info("Auto-update of RCR consumption\n");
+		bm_isr_status_clear(portal, progress & BM_PIRQ_RCRI);
+	}
+#else
+	bm_rcr_cci_update(portal);
+#endif
+}
+
+/* Helper for bm_eqcr_start() that tracks ring consumption and checks the
+ * return code */
+static void rcr_start(void)
+{
+	/* If there are consumed RCR entries, track them now. The alternative
+	 * is to catch an error in bm_rcr_start(), track consume entries then,
+	 * and then retry bm_rcr_start(). */
+	do {
+		rcr_update();
+		rc = bm_rcr_start(portal);
+	} while (!rc);
+}
+
+/* Helper for bm_rcr_pvb_commit() */
+static void rcr_commit(u8 numbufs)
+{
+	bm_rcr_pvb_commit(portal, BM_RCR_VERB_CMD_BPID_SINGLE | numbufs);
+#ifndef BUG_NO_RCR_ITR
+	if (!rcr_thresh_on && (bm_rcr_get_avail(portal) < 2)) {
+		rcr_thresh_on = 1;
+		bm_rcr_set_ithresh(portal, 1);
+	}
+#endif
+}
+
+static irqreturn_t portal_isr(int irq, void *ptr)
+{
+	pr_info("BMAN portal interrupt, isr_count=%d->%d\n", isr_count,
+		isr_count + 1);
+	isr_count++;
+	bm_isr_inhibit(portal);
+	wake_up(&queue);
+	return IRQ_HANDLED;
+}
+
+void bman_test_low(struct bm_portal *__p)
+{
+	const struct bm_portal_config *config = bm_portal_config(__p);
+	u32 bpid;
+	int i, big_loop = 2;
+	u32 status;
+	int depleted = 1, last_count = 0;
+#define WAIT_ISR() \
+do { \
+	last_count++; \
+	wait_event(queue, isr_count == last_count); \
+} while(0)
+
+	portal = __p;
+
+	i = bm_pool_new(&bpid);
+	if (i)
+		panic("can't allocate bpid");
+	i = bm_pool_set(bpid, test_thresholds);
+	if (i)
+		panic("can't set thresholds");
+
+	/*********************/
+	/* Initialise portal */
+	/*********************/
+	if (bm_rcr_init(portal, bm_rcr_pvb, bm_rcr_cci) ||
+			bm_mc_init(portal) || bm_isr_init(portal))
+		panic("Portal setup failed");
+	bm_isr_enable_write(portal, -1);
+	bm_isr_disable_write(portal, 0);
+	bm_isr_uninhibit(portal);
+	bm_isr_status_clear(portal, 0xffffffff);
+
+	pr_info("low-level test, start ccmode\n");
+
+	if (request_irq(config->irq, portal_isr, 0, "Bman portal 0", NULL))
+		panic("Can't register Bman portal 0 IRQ");
+	/* Enable the BSCN mask bit corresponding to our bpid */
+	bm_isr_bscn_mask(portal, bpid, 1);
+
+	pr_info("Portal %d i/faces initialised\n", TPORTAL);
+
+again:
+	/* The portal's (interrupt) status register should be zero */
+	status = bm_isr_status_read(portal);
+	BUG_ON(status);
+
+	/*************************/
+	/* Release TBUFS buffers */
+	/*************************/
+	for (i = 0; i < TBUFS;) {
+		int j = 0;
+		rcr_start();
+		rc->bpid = bpid;
+		while ((j < TRELEASEMAX) && (i < TBUFS)) {
+			rc->bufs[j].hi = TADDRHI;
+			rc->bufs[j].lo = TADDRLO + i;
+			BUFS_set(i);
+			i++;
+			j++;
+		}
+		rcr_commit(j);
+		if (depleted && (i > TDEXIT)) {
+			WAIT_ISR();
+			do {
+				status = bm_isr_status_read(portal);
+			} while (status != BM_PIRQ_BSCN);
+			bm_isr_status_clear(portal, BM_PIRQ_BSCN);
+			status = bm_isr_status_read(portal);
+			BUG_ON(status);
+			bm_isr_uninhibit(portal);
+			depleted = 0;
+		}
+		pr_info("Releasing %d bufs (%d-%d)\n", j, i - j, i - 1);
+	}
+	rcr_update(); /* RCR should be empty now */
+
+	/****************/
+	/* Acquire bufs */
+	/****************/
+	while (i) {
+		int j;
+		spin_lock_irq(&mc_lock);
+		mc_start();
+		mcc->acquire.bpid = bpid;
+		mc_commit(BM_MCC_VERB_CMD_ACQUIRE |
+			((i < TRELEASEMAX) ? i : TRELEASEMAX));
+		j = mcr->verb & BM_MCR_VERB_ACQUIRE_BUFCOUNT;
+		pr_info("Acquired %d bufs (%d remain)\n", j, i - j);
+		BUG_ON(!j);
+		BUG_ON(j > i);
+		while (j--) {
+			unsigned int idx;
+			BUG_ON(mcr->acquire.bufs[j].hi != TADDRHI);
+			idx = mcr->acquire.bufs[j].lo - TADDRLO;
+			BUG_ON(idx >= TBUFS);
+			BUG_ON(!BUFS_get(idx));
+			BUFS_unset(idx);
+			i--;
+		}
+		spin_unlock_irq(&mc_lock);
+		if (!depleted && (i < TDENTRY)) {
+			WAIT_ISR();
+			do {
+				status = bm_isr_status_read(portal);
+			} while (status != BM_PIRQ_BSCN);
+			bm_isr_status_clear(portal, BM_PIRQ_BSCN);
+			status = bm_isr_status_read(portal);
+			BUG_ON(status);
+			bm_isr_uninhibit(portal);
+			depleted = 1;
+		}
+	}
+
+	/****************************/
+	/* Acquire should give zero */
+	/****************************/
+	mc_start();
+	mcc->acquire.bpid = bpid;
+	mc_commit(BM_MCC_VERB_CMD_ACQUIRE | 1);
+	pr_info("Acquiring 1 buf when pool is empty\n");
+	BUG_ON((mcr->verb & BM_MCR_VERB_ACQUIRE_BUFCOUNT));
+	pr_info("Acquire gave %d buffers when pool is empty\n",
+		mcr->verb & BM_MCR_VERB_ACQUIRE_BUFCOUNT);
+
+	if (big_loop--)
+		goto again;
+
+	bm_isr_bscn_mask(portal, bpid, 0);
+	free_irq(config->irq, NULL);
+	bm_rcr_finish(portal);
+	bm_mc_finish(portal);
+	bm_isr_status_clear(portal, -1);
+	bm_isr_enable_write(portal, 0);
+	bm_isr_finish(portal);
+
+	bm_pool_free(bpid);
+}
+
-- 
1.6.5.2

