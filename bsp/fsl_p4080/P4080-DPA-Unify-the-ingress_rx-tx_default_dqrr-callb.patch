From 5e43a12c18b45644165194931042f7b1cf7d108e Mon Sep 17 00:00:00 2001
From: Emil Medve <Emilian.Medve@Freescale.com>
Date: Tue, 15 Dec 2009 09:48:00 -0600
Subject: [PATCH 121/148] P4080/DPA: Unify the ingress_rx/tx_default_dqrr() callbacks

Signed-off-by: Emil Medve <Emilian.Medve@Freescale.com>
[Cleanly applied the FSL SDK 2.0.3 patch:
"kernel-2.6.30-P4080-DPA-Unify-the-ingress_rx-tx_default_dq.patch"]
Integrated-by: Yuri Nedel <Yuri.Nedel@windriver.com>
---
 drivers/net/dpa/dpa.c |  177 +++++++++++++++++++++----------------------------
 1 files changed, 76 insertions(+), 101 deletions(-)

diff --git a/drivers/net/dpa/dpa.c b/drivers/net/dpa/dpa.c
index 4c8c43f..18be459 100644
--- a/drivers/net/dpa/dpa.c
+++ b/drivers/net/dpa/dpa.c
@@ -705,6 +705,64 @@ dpa_fd_release(const struct net_device *net_dev, const struct qm_fd *fd)
 #define TT_RESERVED_SPACE(net_dev)	\
 	min(sizeof(struct icmphdr), min(sizeof(struct udphdr), sizeof(struct tcphdr)))
 
+static enum qman_cb_dqrr_result __hot
+ingress_dqrr(struct qman_portal		*portal,
+	     struct qman_fq		*fq,
+	     const struct qm_dqrr_entry	*dq,
+	     uint8_t			 _rtx)
+{
+	const struct net_device		*net_dev;
+	const struct dpa_priv_s		*priv;
+	struct dpa_percpu_priv_s	*percpu_priv;
+	struct dpa_fd			*dpa_fd;
+
+	net_dev = ((struct dpa_fq *)fq)->net_dev;
+	priv = (typeof(priv))netdev_priv(net_dev);
+
+	if (netif_msg_intr(priv))
+		cpu_netdev_dbg(net_dev, "-> %s:%s[%s]()\n",
+			       __file__, __func__, rtx[_rtx]);
+
+	percpu_priv = per_cpu_ptr(priv->percpu_priv, smp_processor_id());
+#ifdef CONFIG_DEBUG_FS
+	percpu_priv->hwi[_rtx]++;
+#endif
+
+	if (list_empty(&percpu_priv->free_list)) {
+		dpa_fd = devm_kzalloc(net_dev->dev.parent,
+				sizeof(*dpa_fd), GFP_ATOMIC);
+		if (unlikely(dpa_fd == NULL)) {
+			if (netif_msg_rx_err(priv))
+				cpu_netdev_err(net_dev,
+					"%s:%hu:%s(): devm_kzalloc() failed\n",
+					__file__, __LINE__, __func__);
+			goto _return;
+		}
+	} else {
+		dpa_fd = list_first_entry(&percpu_priv->free_list,
+					typeof(*dpa_fd), list);
+		list_del(&dpa_fd->list);
+	}
+
+	dpa_fd->fd = dq->fd;
+
+	list_add_tail(&dpa_fd->list, &percpu_priv->fd_list[_rtx]);
+#ifdef CONFIG_DEBUG_FS
+	percpu_priv->count[_rtx]++;
+	percpu_priv->max[_rtx] = max(percpu_priv->max[_rtx],
+				    percpu_priv->count[_rtx]);
+#endif
+
+	schedule_work(&percpu_priv->fd_work);
+
+_return:
+	if (netif_msg_intr(priv))
+		cpu_netdev_dbg(net_dev, "%s:%s[%s]() ->\n",
+			       __file__, __func__, rtx[_rtx]);
+
+	return qman_cb_dqrr_consume;
+}
+
 static enum qman_cb_dqrr_result
 ingress_rx_error_dqrr(struct qman_portal *portal, struct qman_fq *fq,
 		const struct qm_dqrr_entry *dq)
@@ -816,24 +874,18 @@ ingress_rx_default_dqrr(struct qman_portal		*portal,
 			struct qman_fq			*fq,
 			const struct qm_dqrr_entry	*dq)
 {
+#ifdef CONFIG_FSL_FMAN_TEST
+{
+	int				 _errno, i;
 	const struct net_device		*net_dev;
 	const struct dpa_priv_s		*priv;
-	struct dpa_percpu_priv_s	*percpu_priv;
-	struct dpa_fd			*dpa_fd;
+	struct dpa_fq			*dpa_fq;
+	void				*virt;
+	uint32_t			 fqid = 0;
 
 	net_dev = ((struct dpa_fq *)fq)->net_dev;
 	priv = (typeof(priv))netdev_priv(net_dev);
 
-	if (netif_msg_intr(priv))
-		cpu_netdev_dbg(net_dev, "-> %s:%s()\n", __file__, __func__);
-
-#ifdef CONFIG_FSL_FMAN_TEST
-{
-    void   					*virt;
-    int						 _errno, i;
-	struct dpa_fq			*dpa_fq;
-	uint32_t				fqid=0;
-
 	for(i=0; i<priv->num; i++) {
 		if((dq->fqid >= priv->ranges[i].base) &&
 		   (dq->fqid < (priv->ranges[i].base+priv->ranges[i].count))) {
@@ -871,48 +923,12 @@ ingress_rx_default_dqrr(struct qman_portal		*portal,
             dump_stack();
             panic("Can't release buffer to the BM during RX\n");
         }
-        goto _return;
+	return qman_cb_dqrr_consume;
     }
 }
 #endif /* CONFIG_FSL_FMAN_TEST */
 
-	percpu_priv = per_cpu_ptr(priv->percpu_priv, smp_processor_id());
-#ifdef CONFIG_DEBUG_FS
-	percpu_priv->hwi[RX]++;
-#endif
-
-	if (list_empty(&percpu_priv->free_list)) {
-		dpa_fd = devm_kzalloc(net_dev->dev.parent,
-				sizeof(*dpa_fd), GFP_ATOMIC);
-		if (unlikely(dpa_fd == NULL)) {
-			if (netif_msg_rx_err(priv))
-				cpu_netdev_err(net_dev,
-					"%s:%hu:%s(): devm_kzalloc() failed\n",
-					__file__, __LINE__, __func__);
-			goto _return;
-		}
-	} else {
-		dpa_fd = list_first_entry(&percpu_priv->free_list,
-					typeof(*dpa_fd), list);
-		list_del(&dpa_fd->list);
-	}
-
-	dpa_fd->fd = dq->fd;
-
-	list_add_tail(&dpa_fd->list, &percpu_priv->fd_list[RX]);
-#ifdef CONFIG_DEBUG_FS
-	percpu_priv->count[RX]++;
-	percpu_priv->max[RX] = max(percpu_priv->max[RX],
-				   percpu_priv->count[RX]);
-#endif
-
-	schedule_work(&percpu_priv->fd_work);
-
-_return:
-	if (netif_msg_intr(priv))
-		cpu_netdev_dbg(net_dev, "%s:%s() ->\n", __file__, __func__);
-
-	return qman_cb_dqrr_consume;
+	return ingress_dqrr(portal, fq, dq, RX);
 }
 
 static void ingress_rx_default_ern(struct qman_portal		*portal,
@@ -1068,67 +1084,26 @@ ingress_tx_default_dqrr(struct qman_portal		*portal,
 			struct qman_fq			*fq,
 			const struct qm_dqrr_entry	*dq)
 {
+#ifdef CONFIG_FSL_FMAN_TEST
+{
 	struct net_device		*net_dev;
 	const struct dpa_priv_s		*priv;
-	struct dpa_percpu_priv_s	*percpu_priv;
-	struct dpa_fd			*dpa_fd;
+	void   *virt = bus_to_virt(dq->fd.addr_lo);
 
 	net_dev = ((struct dpa_fq *)fq)->net_dev;
 	priv = netdev_priv(net_dev);
 
-	if (netif_msg_intr(priv))
-		cpu_netdev_dbg(net_dev, "-> %s:%s()\n", __file__, __func__);
-
-#ifdef CONFIG_FSL_FMAN_TEST
-{
-    void   *virt = bus_to_virt(dq->fd.addr_lo);
     /* No support yet for more than 32 bit address */
-    BUG_ON(dq->fd.addr_hi);
-    if (is_fman_test((void *)priv->mac_dev,
-                     FMT_TX_CONF_Q,
-                     virt,
-                     dq->fd.length20 + dq->fd.offset))
-        return qman_cb_dqrr_consume;
+	BUG_ON(dq->fd.addr_hi);
+	if (is_fman_test((void *)priv->mac_dev,
+			 FMT_TX_CONF_Q,
+			 virt,
+			 dq->fd.length20 + dq->fd.offset))
+		return qman_cb_dqrr_consume;
 }
 #endif /* CONFIG_FSL_FMAN_TEST */
 
-	percpu_priv = per_cpu_ptr(priv->percpu_priv, smp_processor_id());
-#ifdef CONFIG_DEBUG_FS
-	percpu_priv->hwi[TX]++;
-#endif
-
-	if (list_empty(&percpu_priv->free_list)) {
-		dpa_fd = devm_kzalloc(net_dev->dev.parent,
-				sizeof(*dpa_fd), GFP_ATOMIC);
-		if (unlikely(dpa_fd == NULL)) {
-			if (netif_msg_tx_err(priv))
-				cpu_netdev_err(net_dev,
-					"%s:%hu:%s(): devm_kzalloc() failed\n",
-					__file__, __LINE__, __func__);
-			goto _return;
-		}
-	} else {
-		dpa_fd = list_first_entry(&percpu_priv->free_list,
-					typeof(*dpa_fd), list);
-		list_del(&dpa_fd->list);
-	}
-
-	dpa_fd->fd = dq->fd;
-
-	list_add_tail(&dpa_fd->list, &percpu_priv->fd_list[TX]);
-#ifdef CONFIG_DEBUG_FS
-	percpu_priv->count[TX]++;
-	percpu_priv->max[TX] = max(percpu_priv->max[TX],
-				   percpu_priv->count[TX]);
-#endif
-
-	schedule_work(&percpu_priv->fd_work);
-
-_return:
-	if (netif_msg_intr(priv))
-		cpu_netdev_dbg(net_dev, "%s:%s() ->\n", __file__, __func__);
-
-	return qman_cb_dqrr_consume;
+	return ingress_dqrr(portal, fq, dq, TX);
 }
 
 static void ingress_tx_default_ern(struct qman_portal		*portal,
-- 
1.6.5.2

