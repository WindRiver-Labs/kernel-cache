From 618a61253e22fa77b705a66a41e2a3ac63246379 Mon Sep 17 00:00:00 2001
From: Kevin Hao <kexin.hao@windriver.com>
Date: Wed, 24 Mar 2010 15:24:48 +0800
Subject: [PATCH] crypto: update FSL caam driver

[Kevin: Original headless
patch(kernel-2.6.30-initial-CAAM-driver-updates-4080-sdk2.0-gpl.patch)
taken from Freescale p4080 SDK 2.0 ISO image. Apply cleanly to
kernel 2.6.27]
Signed-off-by: Kevin Hao <kexin.hao@windriver.com>
---
 drivers/crypto/caam/Kconfig  |   27 ++
 drivers/crypto/caam/Makefile |    2 +-
 drivers/crypto/caam/algapi.c |  573 +++++++++++++++++++++++++++---------------
 3 files changed, 392 insertions(+), 210 deletions(-)

diff --git a/drivers/crypto/caam/Kconfig b/drivers/crypto/caam/Kconfig
index a126969..6912ec5 100644
--- a/drivers/crypto/caam/Kconfig
+++ b/drivers/crypto/caam/Kconfig
@@ -25,6 +25,33 @@ config CRYPTO_DEV_FSL_CAAM_RINGSIZE
 		9 => 512
 		10 => 1024
 
+config CRYPTO_DEV_FSL_CAAM_INTC
+	bool "Job Queue interrupt coalescing"
+	depends on CRYPTO_DEV_FSL_CAAM
+	help
+	  Enable the Job Queue's interrupt coalescing feature.
+
+config CRYPTO_DEV_FSL_CAAM_INTC_COUNT_THLD
+	int "Job Queue interrupt coalescing count threshold"
+	depends on CRYPTO_DEV_FSL_CAAM_INTC
+	range 1 255
+	default 8
+	help
+	  Select number of descriptor completions to queue before
+	  raising an interrupt, in the range 1-255. Note that a selection
+	  of 1 functionally defeats the coalescing feature, and a selection
+	  equal or greater than the job ring size will force timeouts.
+
+config CRYPTO_DEV_FSL_CAAM_INTC_TIME_THLD
+	int "Job Queue interrupt coalescing timer threshold"
+	depends on CRYPTO_DEV_FSL_CAAM_INTC
+	range 1 65535
+	default 8
+	help
+	  Select number of bus clocks/64 to timeout in the case that one or
+	  more descriptor completions are queued without reaching the count
+	  threshold.
+
 config CRYPTO_DEV_FSL_CAAM_DC_LIB
 	tristate "Freescale CAAM Descriptor Library (EXPERIMENTAL)"
 	depends on CRYPTO_DEV_FSL_CAAM
diff --git a/drivers/crypto/caam/Makefile b/drivers/crypto/caam/Makefile
index ceddfed..ae3425f 100644
--- a/drivers/crypto/caam/Makefile
+++ b/drivers/crypto/caam/Makefile
@@ -11,4 +11,4 @@ obj-$(CONFIG_CRYPTO_DEV_FSL_CAAM_DC_LIB) += dcl/
 obj-$(CONFIG_CRYPTO_DEV_FSL_CAAM_CRYPTO_API) += algapi.o
 obj-$(CONFIG_CRYPTO_DEV_FSL_CAAM_JQ_TEST) += jq_test/
 
-caam-objs := ctrl.o jq.o
+caam-objs := ctrl.o jq.o error.o
diff --git a/drivers/crypto/caam/algapi.c b/drivers/crypto/caam/algapi.c
index 0a18a32..073e3ef 100644
--- a/drivers/crypto/caam/algapi.c
+++ b/drivers/crypto/caam/algapi.c
@@ -61,14 +61,18 @@
 #include "regs.h"
 #include "intern.h"
 #include "desc.h"
+#include "pdb.h"
 #include "jq.h"
+#include "error.h"
 #include "dcl/dcl.h"
 
 /*
  * crypto alg
  */
 #define CAAM_CRA_PRIORITY		3000
-#define CAAM_MAX_KEY_SIZE		64
+/* max key is sum of AES_MAX_KEY_SIZE, max split key size */
+#define CAAM_MAX_KEY_SIZE		(AES_MAX_KEY_SIZE + \
+					 SHA512_DIGEST_SIZE * 2)
 /* max IV is max of AES_BLOCK_SIZE, DES3_EDE_BLOCK_SIZE */
 #define CAAM_MAX_IV_LENGTH		16
 
@@ -89,50 +93,19 @@ struct caam_ctx {
 	int class1_alg_type;
 	int class2_alg_type;
 	u8 key[CAAM_MAX_KEY_SIZE];
+	u32 key_phys; /* FIXME: dma_addr_t */
 	unsigned int keylen;
 	unsigned int enckeylen;
 	unsigned int authkeylen;
+	unsigned int split_key_len;
 	unsigned int authsize;
 	u32 *shared_desc;
-	u32 shared_desc_phys;
+	u32 shared_desc_phys; /* FIXME: dma_addr_t */
 	int shared_desc_len;
 	spinlock_t first_lock;
 };
 
 /*
- * IPSec shared protocol descriptor/PDB - encapsulation
- */
-struct pdb_proto_ipsec_cbc_encap {
-	__be32 desc_hdr;
-	u8 res1;
-	u8 ip_nh; /* next header */
-	u8 ip_nh_offset; /* its offset within packet */
-	u8 options;
-	__be32 seq_num_ext_hi;
-	__be32 seq_num;
-	__be32 iv[4];
-	__be32 spi;
-	__be16 res2;
-	__be16 ip_hdr_len;
-	__be32 ip_hdr[0]; /* optional IP Header content */
-} __packed;
-
-/*
- * IPSec shared protocol descriptor/PDB- decapsulation
- */
-struct pdb_proto_ipsec_cbc_decap {
-	__be32 desc_hdr;
-	__be16 ip_hdr_len;
-	u8 ip_nh_offset;
-	u8 options;
-	__be32 res1[2];
-	__be32 seq_num_ext_hi;
-	__be32 seq_num;
-	__be32 anti_replay[2]; /* anti-replay scorecard */
-	__be32 end_index[0];
-} __packed;
-
-/*
  * IPSec ESP Datapath Protocol Override Register (DPOVRD)
  */
 struct ipsec_deco_dpovrd {
@@ -150,10 +123,12 @@ static int aead_authenc_setauthsize(struct crypto_aead *authenc,
 	debug("setauthsize: authsize %d\n", authsize);
 
 	switch (authsize) {
-	case 12: ctx->class2_alg_type = AUTH_TYPE_IPSEC_SHA1HMAC_96;
-		 break;
-	case 20: ctx->class2_alg_type = AUTH_TYPE_IPSEC_SHA1HMAC_160;
-		 break;
+	case 12:
+		ctx->class2_alg_type = AUTH_TYPE_IPSEC_SHA1HMAC_96;
+		break;
+	case 20:
+		ctx->class2_alg_type = AUTH_TYPE_IPSEC_SHA1HMAC_160;
+		break;
 	}
 
 	ctx->authsize = authsize;
@@ -164,31 +139,31 @@ static int aead_authenc_setauthsize(struct crypto_aead *authenc,
 static int build_protocol_desc_ipsec_decap(struct caam_ctx *ctx,
 					   struct aead_request *req)
 {
+	struct device *dev = ctx->dev;
 	gfp_t flags = req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL :
 		      GFP_ATOMIC;
 	struct crypto_async_request *req_base = &req->base;
 	struct sk_buff *skb = req_base->data;
 	struct xfrm_state *x = xfrm_input_state(skb);
-
-	struct pdb_proto_ipsec_cbc_decap *sh_desc;
-	void *sh_desc_pos;
-	int endidx;
+	struct ipsec_decap_pdb *sh_desc;
 	int seq_no_offset = offsetof(struct ip_esp_hdr, seq_no);
+	void *sh_desc_ptr;
+	int endidx;
 
 	/* build shared descriptor for this session */
-	sh_desc = kzalloc(sizeof(struct pdb_proto_ipsec_cbc_decap) +
+	sh_desc = kzalloc(sizeof(struct ipsec_decap_pdb) +
 			 (sizeof(u32) + CAAM_MAX_KEY_SIZE) * 2 +
 			 sizeof(struct iphdr), GFP_DMA | flags);
 	if (!sh_desc) {
-		dev_err(ctx->dev, "could not allocate shared descriptor\n");
+		dev_err(dev, "could not allocate shared descriptor\n");
 		return -ENOMEM;
 	}
 
 	/* ip hdr len currently fixed */
 	sh_desc->ip_hdr_len = cpu_to_be16(sizeof(struct iphdr));
 
-	/* next hdr offset (9 bytes in) */
-	sh_desc->ip_nh_offset = 9;
+	/* we don't have a next hdr offset */
+	sh_desc->ip_nh_offset = 0;
 
 	/*
 	 * options: ipv4, only the decapsulated output if tunnel mode
@@ -198,7 +173,8 @@ static int build_protocol_desc_ipsec_decap(struct caam_ctx *ctx,
 	debug("xfrm is in %s mode\n", x->props.mode == XFRM_MODE_TUNNEL ?
 	      "tunnel" : "transport");
 	sh_desc->options = ((x->props.mode == XFRM_MODE_TUNNEL) ?
-			    (PDBOPTS_ESPCBC_TUNNEL | PDBOPTS_ESPCBC_OUTFMT) : 0);
+			    (PDBOPTS_ESPCBC_TUNNEL | PDBOPTS_ESPCBC_OUTFMT) :
+			    0);
 	/* copy Sequence Number
 	 * equivalent to:
 	 * *spi = *(__be32*)(skb_transport_header(skb) + offset);
@@ -206,55 +182,61 @@ static int build_protocol_desc_ipsec_decap(struct caam_ctx *ctx,
 	sh_desc->seq_num = *(__be32 *)((char *)sg_virt(req->assoc) +
 			    seq_no_offset);
 
-	/* Save current location for computing start index */
-	sh_desc_pos = &sh_desc->end_index[0];
+	/* insert keys, leaving space here for the jump instruction */
+	sh_desc_ptr = &sh_desc->end_index[1];
 
 	/*
 	 * process keys, starting with class 2/authentication
 	 * This is assuming keys are immediate for sharedesc
 	 */
-	sh_desc_pos = cmd_insert_key(sh_desc_pos, ctx->key, ctx->authkeylen * 8,
+	sh_desc_ptr = cmd_insert_key(sh_desc_ptr, (char *)ctx->key_phys,
+				     ctx->authkeylen * 8 * 2,
+				     PTR_DIRECT, KEYDST_MD_SPLIT, KEY_COVERED,
+				     ITEM_REFERENCE, ITEM_CLASS2);
+
+	sh_desc_ptr = cmd_insert_key(sh_desc_ptr, (char *)(ctx->key_phys +
+				     ctx->split_key_len), ctx->enckeylen * 8,
 				     PTR_DIRECT, KEYDST_KEYREG, KEY_CLEAR,
-				     ITEM_INLINE, ITEM_CLASS2);
+				     ITEM_REFERENCE, ITEM_CLASS1);
 
-	/* class 1/cipher key */
-	sh_desc_pos = cmd_insert_key(sh_desc_pos, ctx->key + ctx->authkeylen,
-				     ctx->enckeylen * 8, PTR_DIRECT,
-				     KEYDST_KEYREG, KEY_CLEAR, ITEM_INLINE,
-				     ITEM_CLASS1);
+	/* insert jump instruction now that we are at the jump target */
+	cmd_insert_jump((u32 *)&sh_desc->end_index[0], JUMP_TYPE_LOCAL, CLASS_2,
+			JUMP_TEST_ALL, JUMP_COND_SHRD | JUMP_COND_SELF,
+			(u32 *)sh_desc_ptr - (u32 *)(&sh_desc->end_index[0]),
+			NULL);
 
 	/* insert the operation command */
-	sh_desc_pos = cmd_insert_proto_op_ipsec(sh_desc_pos,
+	sh_desc_ptr = cmd_insert_proto_op_ipsec(sh_desc_ptr,
 						ctx->class1_alg_type,
 						ctx->class2_alg_type,
 						DIR_DECAP);
 
-	/* update the header with size/offsets */
-	endidx = (sh_desc_pos - (void *)sh_desc ) / sizeof(char *) + 1;
-	/* add 1 to include header */
-
-	cmd_insert_shared_hdr((u_int32_t *)sh_desc,
-			      sizeof(struct pdb_proto_ipsec_cbc_decap) /
-			      sizeof(u32), endidx, CTX_ERASE, SHR_SERIAL);
+	/*
+	 * update the header with size/offsets
+	 * add 1 to include header
+	 */
+	endidx = (sh_desc_ptr - (void *)sh_desc) / sizeof(char *) + 1;
+	cmd_insert_shared_hdr((u32 *)sh_desc, sizeof(struct ipsec_decap_pdb) /
+			      sizeof(u32), endidx, CTX_SAVE, SHR_SERIAL);
 
 #ifdef DEBUG
 	print_hex_dump(KERN_ERR, "shrdesc@"xstr(__LINE__)": ",
 		       DUMP_PREFIX_ADDRESS, 16, 4, sh_desc,
-		       (sh_desc_pos - (void *)sh_desc ) * 4+ 1, 1);
-	caam_desc_disasm((u_int32_t *)sh_desc);
+		       (sh_desc_ptr - (void *)sh_desc) * 4 + 1, 1);
+	caam_desc_disasm((u_int32_t *)sh_desc, DISASM_SHOW_OFFSETS |
+			 DISASM_SHOW_RAW);
 #endif
 
 	ctx->shared_desc_len = endidx * sizeof(u32);
 
 	/* now we know the length, stop wasting preallocated sh_desc space */
 	ctx->shared_desc = krealloc(sh_desc, ctx->shared_desc_len,
-					  GFP_DMA | flags);
+				    GFP_DMA | flags);
 
-	ctx->shared_desc_phys = dma_map_single(ctx->dev, sh_desc,
-						     endidx * sizeof(u32),
-						     DMA_BIDIRECTIONAL);
-	if (dma_mapping_error(ctx->dev, ctx->shared_desc_phys)) {
-		dev_err(ctx->dev, "unable to map shared descriptor\n");
+	ctx->shared_desc_phys = dma_map_single(dev, sh_desc, endidx *
+					       sizeof(u32), DMA_BIDIRECTIONAL);
+	if (dma_mapping_error(dev, ctx->shared_desc_phys)) {
+		dev_err(dev, "unable to map shared descriptor\n");
 		kfree(ctx->shared_desc);
 		return -ENOMEM;
 	}
@@ -267,42 +249,38 @@ static int build_protocol_desc_ipsec_encap(struct caam_ctx *ctx,
 {
 	gfp_t flags = areq->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL :
 		      GFP_ATOMIC;
+	struct device *dev = ctx->dev;
 	struct crypto_async_request *req_base = &areq->base;
 	struct sk_buff *skb = req_base->data;
 	struct dst_entry *dst = skb->dst;
 	struct xfrm_state *x = dst->xfrm;
-	int  startidx, endidx;
-	u32 *shdesc, *shdescptr;
+	struct ipsec_encap_pdb *sh_desc;
+	int endidx;
+	void *sh_desc_ptr;
 
 	/* build shared descriptor for this session */
-	shdesc = kzalloc(sizeof(struct pdb_proto_ipsec_cbc_encap) +
-			  (sizeof(u32) + CAAM_MAX_KEY_SIZE) * 2 +
-			  sizeof(struct iphdr), GFP_DMA | flags);
-	if (!shdesc) {
-		dev_err(ctx->dev, "could not allocate shared descriptor\n");
+	sh_desc = kzalloc(sizeof(struct ipsec_encap_pdb) +
+			 (sizeof(u32) + CAAM_MAX_KEY_SIZE) * 2 +
+			 52 /*sizeof(struct iphdr)*/, GFP_DMA | flags);
+	if (!sh_desc) {
+		dev_err(dev, "could not allocate shared descriptor\n");
 		return -ENOMEM;
 	}
 
-	shdescptr = shdesc;
-
-	/* skip shared header (filled in last) */
-	shdescptr++;
-
 	/*
 	 * options byte: IVsrc is RNG
 	 * we do not Prepend IP header to output frame
 	 */
-#if !defined(DEBUG)
-	*shdescptr |= PDBOPTS_ESPCBC_IVSRC; /* IV src is RNG */
-#endif
-	debug("xfrm is in %s mode\n", x->props.mode== XFRM_MODE_TUNNEL ?
+	debug("xfrm is in %s mode\n", x->props.mode == XFRM_MODE_TUNNEL ?
 	      "tunnel" : "transport");
 
-	if (x->props.mode == XFRM_MODE_TUNNEL)
-		*shdescptr |= 4 << 16 | /* next hdr = IPv4 */
-			      9 << 8 | /* next hdr offset (9 bytes in) */
-			      PDBOPTS_ESPCBC_TUNNEL;
-	shdescptr++;
+	if (x->props.mode == XFRM_MODE_TUNNEL) {
+		sh_desc->ip_nh = IPPROTO_IPIP; /* next hdr = IPv4 */
+		sh_desc->options = PDBOPTS_ESPCBC_TUNNEL;
+	}
+#if !defined(DEBUG)
+	sh_desc->options |= PDBOPTS_ESPCBC_IVSRC; /* IV src is RNG */
+#endif
 
 	/*
 	 * need to pretend we have a full fledged pdb, otherwise get:
@@ -310,70 +288,75 @@ static int build_protocol_desc_ipsec_encap(struct caam_ctx *ctx,
 	 * expected at least 36 bytes
 	 */
 
-	/* Skip sequence numbers */
-	shdescptr += 2;
-
-	/* Skip IV */
 #ifdef DEBUG
-	memcpy(shdesc + 4, "myivmyivmyivmyiv", 16);
+	memcpy(&sh_desc.cbc.iv, "myivmyivmyivmyiv", sizeof(sh_desc.cbc.iv));
 #endif
-	shdescptr += 4;
 
-	/* Skip SPI */
-	shdescptr++;
-
-	/* fixed IP header length */
-	*shdescptr++ = sizeof(struct iphdr);
+	/* indicate no IP header,
+	 * rather a jump instruction and key specification follow
+	 */
+	sh_desc->ip_hdr_len = 0;
 
-	shdescptr += sizeof(struct iphdr) / sizeof(u32);
+	/* insert keys, leaving space here for the jump instruction */
+	sh_desc_ptr = &sh_desc->ip_hdr[1];
 
-	/* </pretention> */
+	/*
+	 * process keys, starting with class 2/authentication
+	 * This is assuming keys are immediate for sharedesc
+	 * class 1/cipher key
+	 * n.b., h/w triggers:
+	 * "of_platform jq.31: DECO: desc idx 11: Invalid KEY Command..."
+	 * if split_key_len is used here instead of authkeylen * 2
+	 * for len of class 2 key
+	 */
 
-	/* Save current location for computing start index */
-	startidx = shdescptr - shdesc;
+	sh_desc_ptr = cmd_insert_key(sh_desc_ptr, (char *)ctx->key_phys,
+				     ctx->authkeylen * 8 * 2, PTR_DIRECT,
+				     KEYDST_MD_SPLIT, KEY_COVERED,
+				     ITEM_REFERENCE, ITEM_CLASS2);
 
-	/* process keys, starting with class 2/authentication */
-	/* This is assuming keys are immediate for sharedesc */
-	shdescptr = cmd_insert_key(shdescptr, ctx->key, ctx->authkeylen * 8,
-				   PTR_DIRECT, KEYDST_KEYREG, KEY_CLEAR,
-				   ITEM_INLINE, ITEM_CLASS2);
+	sh_desc_ptr = cmd_insert_key(sh_desc_ptr, (char *)ctx->key_phys +
+				     ctx->split_key_len, ctx->enckeylen * 8,
+				     PTR_DIRECT, KEYDST_KEYREG, KEY_CLEAR,
+				     ITEM_REFERENCE, ITEM_CLASS1);
 
-	/* class 1/cipher key */
-	shdescptr = cmd_insert_key(shdescptr, ctx->key + ctx->authkeylen,
-				   ctx->enckeylen * 8, PTR_DIRECT,
-				   KEYDST_KEYREG, KEY_CLEAR, ITEM_INLINE,
-				   ITEM_CLASS1);
+	/* insert jump instruction now that we are at the jump target */
+	cmd_insert_jump((u32 *)&sh_desc->ip_hdr[0], JUMP_TYPE_LOCAL, CLASS_BOTH,
+			JUMP_TEST_ALL, JUMP_COND_SHRD | JUMP_COND_SELF,
+			(u32 *)sh_desc_ptr - (u32 *)(&sh_desc->ip_hdr[0]),
+			NULL);
 
 	/* insert the operation command */
-	shdescptr = cmd_insert_proto_op_ipsec(shdescptr, ctx->class1_alg_type,
-					      ctx->class2_alg_type, DIR_ENCAP);
-
-	/* update the header with size/offsets */
-	endidx = shdescptr - shdesc + 1; /* add 1 to include header */
+	sh_desc_ptr = cmd_insert_proto_op_ipsec(sh_desc_ptr,
+						ctx->class1_alg_type,
+						ctx->class2_alg_type,
+						DIR_ENCAP);
 
-	/* get spec-viol with CTX_SAVE here:
-	 * [caam spec-viol] shared descriptor with INIT=1
+	/*
+	 * update the header with size/offsets
+	 * add 1 to include header
 	 */
-	cmd_insert_shared_hdr(shdesc, startidx, endidx, CTX_ERASE,
-			      SHR_SERIAL);
+	endidx = (sh_desc_ptr - (void *)sh_desc) / sizeof(char *) + 1;
+	cmd_insert_shared_hdr((u32 *)sh_desc, sizeof(struct ipsec_encap_pdb) /
+			      sizeof(u32), endidx, CTX_SAVE, SHR_SERIAL);
 
 #ifdef DEBUG
 	print_hex_dump(KERN_ERR, "shrdesc@"xstr(__LINE__)": ",
-		       DUMP_PREFIX_ADDRESS, 16, 4, shdesc,
-		       (shdescptr - shdesc + 1) * 4, 1);
-	caam_desc_disasm(shdesc);
+		       DUMP_PREFIX_ADDRESS, 16, 4, sh_desc,
+		       (sh_desc_ptr - sh_desc + 1 + 4) * 4, 1);
+	caam_desc_disasm(sh_desc, DISASM_SHOW_OFFSETS | DISASM_SHOW_RAW);
 #endif
 
 	ctx->shared_desc_len = endidx * sizeof(u32);
 
-	/* now we know the length, stop wasting preallocated shdesc space */
-	ctx->shared_desc = krealloc(shdesc, ctx->shared_desc_len,
+	/* now we know the length, stop wasting preallocated sh_desc space */
+	ctx->shared_desc = krealloc(sh_desc, ctx->shared_desc_len,
 				    GFP_DMA | flags);
 
-	ctx->shared_desc_phys = dma_map_single(ctx->dev, shdesc,
+	ctx->shared_desc_phys = dma_map_single(dev, sh_desc,
 					       endidx * sizeof(u32),
 					       DMA_BIDIRECTIONAL);
-	if (dma_mapping_error(ctx->dev, ctx->shared_desc_phys)) {
+	if (dma_mapping_error(dev, ctx->shared_desc_phys)) {
 		dev_err(ctx->dev, "unable to map shared descriptor\n");
 		kfree(ctx->shared_desc);
 		return -ENOMEM;
@@ -382,14 +365,152 @@ static int build_protocol_desc_ipsec_encap(struct caam_ctx *ctx,
 	return 0;
 }
 
+struct split_key_result {
+	struct completion completion;
+	int err;
+};
+
+static void split_key_done(struct device *dev, u32 *desc, u32 err,
+			   void *context)
+{
+	struct split_key_result *res = context;
+
+#ifdef DEBUG
+	dev_err(dev, "%s %d: err 0x%x\n", __func__, __LINE__, err);
+#endif
+	if (err) {
+		char tmp[256];
+
+		dev_err(dev, "%s\n", caam_jq_strstatus(tmp, err));
+	}
+
+	res->err = err;
+
+	complete(&res->completion);
+}
+
+/*
+get a split ipad/opad key
+
+Split key generation-----------------------------------------------
+
+[00] 0xb0810008    jobdesc: stidx=1 share=never len=8
+[01] 0x04000014        key: class2->keyreg len=20
+			@0xffe01000
+[03] 0x84410014  operation: cls2-op sha1 hmac init dec
+[04] 0x24940000     fifold: class2 msgdata-last2 len=0 imm
+[05] 0xa4000001       jump: class2 local all ->1 [06]
+[06] 0x64260028    fifostr: class2 mdsplit-jdk len=40
+			@0xffe04000
+*/
+static u32 gen_split_key(struct device *dev, struct caam_ctx *ctx,
+			 const u8 *key_in, u32 authkeylen)
+{
+	u32 *desc, *desc_pos;
+	struct split_key_result result;
+	dma_addr_t dma_addr_in, dma_addr_out;
+	int ret = 0;
+
+	/* FIXME: may need to look at keyspec instead of basing on inkeysize */
+	ctx->split_key_len = ALIGN(authkeylen * 2, 16);
+
+	desc = kzalloc(MAX_CAAM_DESCSIZE, GFP_KERNEL | GFP_DMA);
+	desc_pos = desc;
+
+	/* skip header; done last */
+	desc_pos++;
+
+	dma_addr_in = dma_map_single(dev, (void *)key_in, authkeylen,
+				     DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, dma_addr_in)) {
+		dev_err(dev, "unable to map key input memory\n");
+		kfree(desc);
+		return -ENOMEM;
+	}
+	desc_pos = cmd_insert_key(desc_pos, (void *)dma_addr_in, authkeylen * 8,
+				  PTR_DIRECT, KEYDST_KEYREG, KEY_CLEAR,
+				  ITEM_REFERENCE, ITEM_CLASS2);
+
+	/* Sets MDHA up into an HMAC-INIT */
+	desc_pos = cmd_insert_alg_op(desc_pos, OP_TYPE_CLASS2_ALG,
+				     OP_ALG_ALGSEL_SHA1, OP_ALG_AAI_HMAC,
+				     MDSTATE_INIT, ICV_CHECK_OFF, DIR_DECRYPT);
+
+	/*
+	 * do a FIFO_LOAD of zero, this will trigger the internal key expansion
+	   into both pads inside MDHA
+	 */
+	desc_pos = cmd_insert_fifo_load(desc_pos, NULL, 0, LDST_CLASS_2_CCB,
+					0, FIFOLD_IMM, 0,
+					FIFOLD_TYPE_MSG | FIFOLD_TYPE_LAST2);
+
+	/* jump to next insn only necessary due to erratum? */
+	desc_pos = cmd_insert_jump(desc_pos, JUMP_TYPE_LOCAL, CLASS_2,
+				  JUMP_TEST_ALL, 0, 1, NULL);
+
+	/*
+	 * FIFO_STORE with the explicit split-key content store
+	 * (0x26 output type)
+	 */
+	dma_addr_out = dma_map_single(dev, &ctx->key, ctx->split_key_len,
+				      DMA_FROM_DEVICE);
+	if (dma_mapping_error(dev, dma_addr_out)) {
+		dev_err(dev, "unable to map key output memory\n");
+		kfree(desc);
+		return -ENOMEM;
+	}
+	desc_pos = cmd_insert_fifo_store(desc_pos, (void *)dma_addr_out,
+					 authkeylen * 2, LDST_CLASS_2_CCB, 0, 0,
+					 0, FIFOST_TYPE_SPLIT_KEK);
+
+	/* insert job descriptor header */
+	cmd_insert_hdr(desc, 0, desc_pos - desc, SHR_NEVER, SHRNXT_LENGTH,
+		       ORDER_FORWARD, DESC_STD);
+
+#ifdef DEBUG
+	print_hex_dump(KERN_ERR, "ctx.key@"xstr(__LINE__)": ",
+		       DUMP_PREFIX_ADDRESS, 16, 4, &ctx->key,
+		       CAAM_MAX_KEY_SIZE, 1);
+	print_hex_dump(KERN_ERR, "jobdesc@"xstr(__LINE__)": ",
+		       DUMP_PREFIX_ADDRESS, 16, 4, desc,
+		       (desc_pos - desc + 1) * 4, 1);
+	caam_desc_disasm(desc, DISASM_SHOW_OFFSETS | DISASM_SHOW_RAW);
+#endif
+
+	result.err = 0;
+	init_completion(&result.completion);
+
+	ret = caam_jq_enqueue(dev, desc, split_key_done, &result);
+	if (!ret) {
+		/* in progress */
+		wait_for_completion_interruptible(&result.completion);
+		ret = result.err;
+#ifdef DEBUG
+		print_hex_dump(KERN_ERR, "ctx.key@"xstr(__LINE__)": ",
+			       DUMP_PREFIX_ADDRESS, 16, 4, &ctx->key,
+			       CAAM_MAX_KEY_SIZE, 1);
+#endif
+	}
+
+	dma_unmap_single(dev, dma_addr_out, ctx->split_key_len,
+			 DMA_FROM_DEVICE);
+	dma_unmap_single(dev, dma_addr_in, authkeylen, DMA_TO_DEVICE);
+
+	kfree(desc);
+
+	return ret;
+}
+
 static int aead_authenc_setkey(struct crypto_aead *aead,
 			       const u8 *key, unsigned int keylen)
 {
 	struct caam_ctx *ctx = crypto_aead_ctx(aead);
+	struct device *dev = ctx->dev;
 	struct rtattr *rta = (void *)key;
 	struct crypto_authenc_key_param *param;
 	unsigned int authkeylen;
 	unsigned int enckeylen;
+	int ret = 0;
 
 	if (!RTA_OK(rta, keylen))
 		goto badkey;
@@ -414,13 +535,39 @@ static int aead_authenc_setkey(struct crypto_aead *aead,
 	if (keylen > CAAM_MAX_KEY_SIZE)
 		goto badkey;
 
-	memcpy(&ctx->key, key, keylen);
+#ifdef DEBUG
+	printk(KERN_ERR "keylen %d enckeylen %d authkeylen %d\n",
+	       keylen, enckeylen, authkeylen);
+	print_hex_dump(KERN_ERR, "key in @"xstr(__LINE__)": ",
+		       DUMP_PREFIX_ADDRESS, 16, 4, key,
+		       CAAM_MAX_KEY_SIZE, 1);
+#endif
+	ret = gen_split_key(dev, ctx, key, authkeylen);
+	if (ret)
+		goto badkey;
+
+	/* postpend encryption key to auth split key */
+	memcpy((char *)&ctx->key + ctx->split_key_len,
+	       key + authkeylen, enckeylen);
+
+	ctx->key_phys = dma_map_single(dev, ctx->key,
+				       ctx->split_key_len + enckeylen,
+				       DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, ctx->key_phys)) {
+		dev_err(dev, "unable to map key i/o memory\n");
+		return -ENOMEM;
+	}
+#ifdef DEBUG
+	print_hex_dump(KERN_ERR, "ctx.key@"xstr(__LINE__)": ",
+		       DUMP_PREFIX_ADDRESS, 16, 4, &ctx->key,
+		       CAAM_MAX_KEY_SIZE, 1);
+#endif
 
 	ctx->keylen = keylen;
 	ctx->enckeylen = enckeylen;
 	ctx->authkeylen = authkeylen;
 
-	return 0;
+	return ret;
 badkey:
 	crypto_aead_set_flags(aead, CRYPTO_TFM_RES_BAD_KEY_LEN);
 	return -EINVAL;
@@ -436,6 +583,7 @@ struct link_tbl_entry {
 
 /*
  * ipsec_esp_edesc - s/w-extended ipsec_esp descriptor
+ * @hw_desc: the h/w job descriptor
  * @src_nents: number of segments in input scatterlist
  * @dst_nents: number of segments in output scatterlist
  * @assoc_nents: number of segments in associated data (SPI+Seq) scatterlist
@@ -446,10 +594,10 @@ struct link_tbl_entry {
  *           (until s-g support added)
  */
 struct ipsec_esp_edesc {
+	u32 hw_desc[MAX_CAAM_DESCSIZE];
 	int src_nents;
 	int dst_nents;
 	int assoc_nents;
-	u32 desc[MAX_CAAM_DESCSIZE];
 	int dma_len;
 	dma_addr_t link_tbl_phys;
 	struct link_tbl_entry link_tbl[0];
@@ -472,12 +620,23 @@ static void ipsec_esp_unmap(struct device *dev,
 /*
  * ipsec_esp descriptor callbacks
  */
-static void ipsec_esp_encrypt_done(struct device *dev, u32 *desc,
-				   u32 err, void *context)
+static void ipsec_esp_encrypt_done(struct device *dev, u32 *desc, u32 err,
+				   void *context)
 {
-	struct ipsec_esp_edesc *edesc =
-		 container_of(desc, struct ipsec_esp_edesc, desc);
 	struct aead_request *areq = context;
+	struct ipsec_esp_edesc *edesc = (struct ipsec_esp_edesc *)desc;
+#ifdef DEBUG
+	struct crypto_aead *aead = crypto_aead_reqtfm(areq);
+	struct caam_ctx *ctx = crypto_aead_ctx(aead);
+
+	dev_err(dev, "%s %d: err 0x%x\n", __func__, __LINE__, err);
+#endif
+
+	if (err) {
+		char tmp[256];
+
+		dev_err(dev, "%s\n", caam_jq_strstatus(tmp, err));
+	}
 
 	ipsec_esp_unmap(dev, edesc, areq);
 
@@ -503,9 +662,19 @@ static void ipsec_esp_encrypt_done(struct device *dev, u32 *desc,
 static void ipsec_esp_decrypt_done(struct device *dev, u32 *desc, u32 err,
 				   void *context)
 {
-	struct ipsec_esp_edesc *edesc =
-		 container_of(desc, struct ipsec_esp_edesc, desc);
 	struct aead_request *areq = context;
+	struct ipsec_esp_edesc *edesc = (struct ipsec_esp_edesc *)desc;
+#ifdef DEBUG
+	struct crypto_aead *aead = crypto_aead_reqtfm(areq);
+	struct caam_ctx *ctx = crypto_aead_ctx(aead);
+
+	dev_err(dev, "%s %d: err 0x%x\n", __func__, __LINE__, err);
+#endif
+	if (err) {
+		char tmp[256];
+
+		dev_err(dev, "%s\n", caam_jq_strstatus(tmp, err));
+	}
 
 	ipsec_esp_unmap(dev, edesc, areq);
 
@@ -516,21 +685,22 @@ static void ipsec_esp_decrypt_done(struct device *dev, u32 *desc, u32 err,
 	if ((err & JQSTA_CCBERR_ERRID_MASK) == JQSTA_CCBERR_ERRID_ICVCHK)
 		err = -EBADMSG;
 
-	kfree(edesc);
-
 #ifdef DEBUG
 	print_hex_dump(KERN_ERR, "iphdrout@"xstr(__LINE__)": ",
 		       DUMP_PREFIX_ADDRESS, 16, 4,
 		       ((char *)sg_virt(areq->assoc) - sizeof(struct iphdr)),
-		       sizeof(struct iphdr) + areq->assoclen + areq->cryptlen +
+		       sizeof(struct iphdr) + areq->assoclen +
+		       ((areq->cryptlen > 1500) ? 1500 : areq->cryptlen) +
 		       ctx->authsize + 36, 1);
 	if (!err && edesc->dma_len) {
 		struct scatterlist *sg = sg_last(areq->src, edesc->src_nents);
-		print_hex_dump(KERN_ERR, "sglastin@"xstr(__LINE__)": ",
+		print_hex_dump(KERN_ERR, "sglastout@"xstr(__LINE__)": ",
 			       DUMP_PREFIX_ADDRESS, 16, 4, sg_virt(sg),
 			sg->length + ctx->authsize + 16, 1);
 	}
 #endif
+	kfree(edesc);
+
 	aead_request_complete(areq, err);
 }
 
@@ -539,7 +709,7 @@ static void ipsec_esp_decrypt_done(struct device *dev, u32 *desc, u32 err,
  * scatterlist must have been previously dma mapped
  */
 static void sg_to_link_tbl(struct scatterlist *sg, int sg_count,
-                           struct link_tbl_entry *link_tbl_ptr, int offset)
+			   struct link_tbl_entry *link_tbl_ptr, int offset)
 {
 	while (sg_count) {
 		link_tbl_ptr->ptr = sg_dma_address(sg);
@@ -554,7 +724,7 @@ static void sg_to_link_tbl(struct scatterlist *sg, int sg_count,
 
 	/* set Final bit (marks end of link table) */
 	link_tbl_ptr--;
-	link_tbl_ptr->len |= 0x40000000;
+	link_tbl_ptr->len |= cpu_to_be32(0x40000000);
 }
 
 /*
@@ -569,24 +739,23 @@ static int ipsec_esp(struct ipsec_esp_edesc *edesc, struct aead_request *areq,
 	struct caam_ctx *ctx = crypto_aead_ctx(aead);
 	struct device *dev = ctx->dev;
 	struct scatterlist *sg;
-	u32 *desc = &edesc->desc[0];
+	u32 *desc = edesc->hw_desc;
 	u32 *descptr = desc;
 	struct link_tbl_entry *link_tbl_ptr = &edesc->link_tbl[0];
 	int startidx, endidx, ret, sg_count, assoc_sg_count, len, padlen;
 	int ivsize = crypto_aead_ivsize(aead);
 	dma_addr_t ptr;
 	/* defaults; may be overwritten */
-	struct ipsec_deco_dpovrd dpovrd = { sizeof(struct iphdr), 9,
-					    IPPROTO_IPIP};
+	struct ipsec_deco_dpovrd dpovrd = { 0, 0, IPPROTO_IPIP};
 
 #ifdef DEBUG
 	debug("assoclen %d cryptlen %d authsize %d\n",
-	      areq->assoclen, areq->cryptlen,ctx->authsize);
+	      areq->assoclen, areq->cryptlen, ctx->authsize);
 	print_hex_dump(KERN_ERR, "iphdrin@"xstr(__LINE__)": ",
 		       DUMP_PREFIX_ADDRESS, 16, 4,
 		       ((char *)sg_virt(areq->assoc) - sizeof(struct iphdr)),
 		       sizeof(struct iphdr) + areq->assoclen + ivsize +
-		       areq->cryptlen + ctx->authsize  + 16, 1);
+		       areq->cryptlen + ctx->authsize + 16, 1);
 #endif
 
 	/* skip job header (filled in last) */
@@ -621,11 +790,9 @@ static int ipsec_esp(struct ipsec_esp_edesc *edesc, struct aead_request *areq,
 			len = areq->cryptlen - padlen - 2;
 
 			if (!(ctx->shared_desc[1] & PDBOPTS_ESPCBC_TUNNEL)) {
-				len += sizeof(struct iphdr);
-				ptr -= sizeof(struct iphdr);
 				dpovrd.next_header = *(u8 *)((u8 *)sg_virt
 							     (areq->src) +
-							     areq->cryptlen -1);
+							     areq->cryptlen-1);
 			}
 		} else {
 			sg_to_link_tbl(areq->src, sg_count, link_tbl_ptr, 0);
@@ -644,10 +811,6 @@ static int ipsec_esp(struct ipsec_esp_edesc *edesc, struct aead_request *areq,
 			/* cryptlen includes padlen / is blocksize aligned */
 			len = areq->cryptlen - padlen - 2;
 			if (!(ctx->shared_desc[1] & PDBOPTS_ESPCBC_TUNNEL)) {
-				len += sizeof(struct iphdr);
-				link_tbl_ptr->ptr -= sizeof(struct iphdr);
-				link_tbl_ptr->len += cpu_to_be32
-						     (sizeof(struct iphdr));
 				dpovrd.next_header = *(u8 *)((u8 *)sg_virt(sg) +
 							     sg->length -
 							     ctx->authsize - 1);
@@ -665,18 +828,17 @@ static int ipsec_esp(struct ipsec_esp_edesc *edesc, struct aead_request *areq,
 	} else { /* DECAP */
 		debug("seq.num %d\n",
 		      *(u32 *)((u32 *)sg_virt(areq->assoc) + 1));
-/* #define INJECT_ICV_CHECK_FAILURE to verify ICV check correctness */
+/* #define INJECT_ICV_CHECK_FAILURE to verify correctness of ICV check */
 #ifdef INJECT_ICV_CHECK_FAILURE
 		{
-		u32 *foil_ptr;
-		/*
-		 * intentionally tamper with every 13th packet's data
-		 * to verify proper ICV check result propagation
-		 */
-		foil_ptr = sg_virt(areq->assoc) + 1;
-		if (*foil_ptr && (((*foil_ptr % 13) == 0))) {
+			/*
+			 * intentionally tamper with packet's data
+			 * to verify proper ICV check result propagation
+			 */
+			u32 *foil_ptr;
 			struct scatterlist *foil_sg;
-			foil_sg = sg_last(areq->src, edesc->assoc_nents ? : 1);
+
+			foil_sg = sg_last(areq->src, edesc->src_nents ? : 1);
 			foil_ptr = sg_virt(foil_sg) +
 				   (areq->src->length - 26) / 4;
 
@@ -686,7 +848,6 @@ static int ipsec_esp(struct ipsec_esp_edesc *edesc, struct aead_request *areq,
 			dev_warn(dev, " AFTER FOILING PACKET DATA: addr 0x%p"
 				 "  data 0x%x\n", foil_ptr, *foil_ptr);
 		}
-		}
 #endif
 		/* h/w wants ip hdr + assoc + iv data in input */
 		if (!edesc->dma_len) {
@@ -694,13 +855,13 @@ static int ipsec_esp(struct ipsec_esp_edesc *edesc, struct aead_request *areq,
 			      areq->assoclen - sizeof(struct iphdr);
 		} else {
 			sg_to_link_tbl(areq->src, sg_count, link_tbl_ptr, 0);
-			link_tbl_ptr->ptr = cpu_to_be64(link_tbl_ptr->ptr
-							- sizeof(struct iphdr)
-							- areq->assoclen
-							- ivsize);
-			link_tbl_ptr->len += cpu_to_be32(sizeof(struct iphdr)
-							 + areq->assoclen
-							 + ivsize);
+			link_tbl_ptr->ptr = cpu_to_be64(link_tbl_ptr->ptr -
+							sizeof(struct iphdr) -
+							areq->assoclen -
+							ivsize);
+			link_tbl_ptr->len += cpu_to_be32(sizeof(struct iphdr) +
+							 areq->assoclen +
+							 ivsize);
 #ifdef DEBUG
 			print_hex_dump(KERN_ERR, "link_tbl@"xstr(__LINE__)": ",
 			       DUMP_PREFIX_ADDRESS, 16, 4, &edesc->link_tbl[0],
@@ -739,10 +900,6 @@ static int ipsec_esp(struct ipsec_esp_edesc *edesc, struct aead_request *areq,
 		} else {
 			link_tbl_ptr->ptr -= areq->assoclen + ivsize;
 			link_tbl_ptr->len += areq->assoclen + ivsize;
-			if (!(ctx->shared_desc[1] & PDBOPTS_ESPCBC_TUNNEL)) {
-				link_tbl_ptr->ptr += sizeof(struct iphdr);
-				link_tbl_ptr->len -= sizeof(struct iphdr);
-			}
 #ifdef DEBUG
 			print_hex_dump(KERN_ERR, "link_tbl@"xstr(__LINE__)": ",
 			       DUMP_PREFIX_ADDRESS, 16, 4, &edesc->link_tbl[0],
@@ -766,13 +923,17 @@ static int ipsec_esp(struct ipsec_esp_edesc *edesc, struct aead_request *areq,
 						     areq->assoclen + ivsize;
 				link_tbl_ptr->len -= sizeof(struct iphdr) +
 						     areq->assoclen + ivsize;
-			}
-			else { /* transport mode */
+			} else { /* transport mode */
 				len += sizeof(struct iphdr) + areq->assoclen +
 				       ivsize + ctx->authsize;
 			}
 			ptr += sg_count * sizeof(struct link_tbl_entry);
 			/* FIXME: need dma_sync since post-map adjustments? */
+#ifdef DEBUG
+			print_hex_dump(KERN_ERR, "link_tbl@"xstr(__LINE__)": ",
+			       DUMP_PREFIX_ADDRESS, 16, 4, &edesc->link_tbl[0],
+			       edesc->dma_len, 1);
+#endif
 		}
 	}
 
@@ -803,10 +964,10 @@ static int ipsec_esp(struct ipsec_esp_edesc *edesc, struct aead_request *areq,
 	print_hex_dump(KERN_ERR, "jobdesc@"xstr(__LINE__)": ",
 		       DUMP_PREFIX_ADDRESS, 16, 4, desc,
 		       (descptr - desc + 1) * 4, 1);
-	caam_desc_disasm(desc);
+	caam_desc_disasm(desc, DISASM_SHOW_OFFSETS | DISASM_SHOW_RAW);
 #endif
 
-	ret = caam_jq_enqueue(ctx->dev, desc, callback, areq);
+	ret = caam_jq_enqueue(dev, desc, callback, areq);
 	if (!ret)
 		ret = -EINPROGRESS;
 	else {
@@ -820,15 +981,18 @@ static int ipsec_esp(struct ipsec_esp_edesc *edesc, struct aead_request *areq,
 /*
  * derive number of elements in scatterlist
  */
-static int sg_count(struct scatterlist *sg_list, int nbytes)
+static int sg_count(struct scatterlist *sg_list, int nbytes, int *chained)
 {
 	struct scatterlist *sg = sg_list;
 	int sg_nents = 0;
 
-	while (nbytes) {
+	*chained = 0;
+	while (nbytes > 0) {
 		sg_nents++;
 		nbytes -= sg->length;
-		sg = sg_next(sg);
+		if (!sg_is_last(sg) && (sg + 1)->length == 0)
+			*chained = 1;
+		sg = scatterwalk_sg_next(sg);
 	}
 
 	return sg_nents;
@@ -844,24 +1008,27 @@ static struct ipsec_esp_edesc *ipsec_esp_edesc_alloc(struct aead_request *areq,
 	struct caam_ctx *ctx = crypto_aead_ctx(aead);
 	gfp_t flags = areq->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL :
 		      GFP_ATOMIC;
-	int assoc_nents, src_nents, dst_nents, dma_len = 0;
+	int assoc_nents, src_nents, dst_nents, chained, dma_len = 0;
 	struct ipsec_esp_edesc *edesc;
 
 	BUG_ON(areq->dst != areq->src);
 
-	assoc_nents = sg_count(areq->assoc, areq->assoclen);
+	assoc_nents = sg_count(areq->assoc, areq->assoclen, &chained);
+	BUG_ON(chained);
 	assoc_nents = (assoc_nents == 1) ? 0 : assoc_nents;
 
-	src_nents = sg_count(areq->src, areq->cryptlen + ctx->authsize);
+	src_nents = sg_count(areq->src, areq->cryptlen + ctx->authsize,
+			     &chained);
+	BUG_ON(chained);
 	src_nents = (src_nents == 1) ? 0 : src_nents;
 
 	/* + 1 for the IV, which is not included in assoc data */
 	if (assoc_nents || src_nents)
-		dma_len = ((assoc_nents ? : 1) + 1 + (src_nents ? : 1))
-			  * sizeof(struct link_tbl_entry);
+		dma_len = ((assoc_nents ? : 1) + 1 + (src_nents ? : 1)) *
+			  sizeof(struct link_tbl_entry);
 
 	dst_nents = src_nents;
-	dma_len *= 2;
+	dma_len *= 2; /* because we assume src == dst */
 
 	/*
 	 * allocate space for base edesc plus the two link tables
@@ -881,16 +1048,9 @@ static struct ipsec_esp_edesc *ipsec_esp_edesc_alloc(struct aead_request *areq,
 	return edesc;
 }
 
-static int aead_authenc_encrypt(struct aead_request *req)
-{
-	printk("%s unimplemented\n", __FUNCTION__);
-
-	return -EINVAL;
-}
-
 static int aead_authenc_encrypt_first(struct aead_request *req)
 {
-	printk("%s unimplemented\n", __FUNCTION__);
+	printk(KERN_ERR "%s unimplemented\n", __func__);
 
 	return -EINVAL;
 }
@@ -1065,11 +1225,6 @@ static void caam_cra_exit(struct crypto_tfm *tfm)
 		dma_unmap_single(ctx->dev, ctx->shared_desc_phys,
 				 ctx->shared_desc_len, DMA_BIDIRECTIONAL);
 	kfree(ctx->shared_desc);
-
-	if (!dma_mapping_error(ctx->dev, ctx->shared_desc_phys))
-		dma_unmap_single(ctx->dev, ctx->shared_desc_phys,
-				 ctx->shared_desc_len, DMA_BIDIRECTIONAL);
-	kfree(ctx->shared_desc);
 }
 
 void caam_algapi_remove(struct device *dev)
-- 
1.6.0.4

